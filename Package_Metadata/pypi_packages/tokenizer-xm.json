{"info": {"author": "Xiao Ma", "author_email": "Marshalma0923@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Software Development :: Build Tools"], "description": "# Introduction \n\nThis package is an aggregation of several packages I found useful for text pre-processing including gensim and ntlk. I put them together to create a more comprehensive and convenient pipeline. \n\n# Installation\n\n```\npip install tokenizer_xm\n```\n\n# Usage\n\n## Processing a single text string\n\n\n```python\nfrom tokenizer_xm import text_tokenizer_xm, contractions\n\n# An example text\nexample_text = \"This is an amazing product! I've been using it for almost a year now and it's clearly better\\\n than any other products I've used.\"\n```\n\n\n```python\nprint(\"Original text:\")\nprint(example_text)\nprint(\"---\")\n\nprint(\"Simple Preprocessed:\")\nprint(\"---\")\ntk = text_tokenizer_xm(text = example_text, lemma_flag= False, stem_flag = False, stopwords = [])\nprint(tk.txt_pre_pros())\nprint(\"---\")\n\nprint(\"Pre-processing with regular contractions (e.g. I've -> I have):\")\n# In this package, I included a dictionary of regular contractions for your convenience\ntk = text_tokenizer_xm(text = example_text, lemma_flag= False, stem_flag = False, \\\n                       contractions = contractions, stopwords=[])\nprint(tk.txt_pre_pros())\nprint(\"---\")\n\nprint(\"Pre-processing with lemmatization:\")\ntk = text_tokenizer_xm(text = example_text, lemma_flag= True, stem_flag = False, \\\n                       contractions = contractions, stopwords=[])\nprint(tk.txt_pre_pros())\nprint(\"---\")\n\nprint(\"Pre-processing with lemmatization and stemming:\")\n# This package uses the SnowballStemmer from ntlk.stem. I will try to make it customizable later\ntk = text_tokenizer_xm(text = example_text, lemma_flag= True, stem_flag = True, \\\n                       contractions = contractions, stopwords=[])\nprint(tk.txt_pre_pros())\nprint(\"---\")\n\nprint(\"Adding stop words\")\n# This package uses the SnowballStemmer from ntlk.stem. I will try to make it customizable later\ntk = text_tokenizer_xm(text = example_text, lemma_flag= True, stem_flag = True, \\\n                       contractions = contractions, stopwords=[\"this\",'be',\"an\",'it'])\nprint(tk.txt_pre_pros())\nprint(\"---\")\n```\n\n    Original text:\n    This is an amazing product! I've been using it for almost a year now and it's clearly better than any other products I've used.\n    ---\n    Simple Preprocessed:\n    ---\n    ['this', 'is', 'an', 'amazing', 'product', 've', 'been', 'using', 'it', 'for', 'almost', 'year', 'now', 'and', 'it', 'clearly', 'better', 'than', 'any', 'other', 'products', 've', 'used']\n    ---\n    Pre-processing with regular contractions (e.g. I've -> I have):\n    ['this', 'is', 'an', 'amazing', 'product', 'have', 'been', 'using', 'it', 'for', 'almost', 'year', 'now', 'and', 'it', 'has', 'it', 'is', 'clearly', 'better', 'than', 'any', 'other', 'products', 'have', 'used']\n    ---\n    Pre-processing with lemmatization:\n    ['this', 'be', 'an', 'amaze', 'product', 'have', 'be', 'use', 'it', 'for', 'almost', 'year', 'now', 'and', 'it', 'have', 'it', 'be', 'clearly', 'better', 'than', 'any', 'other', 'product', 'have', 'use']\n    ---\n    Pre-processing with lemmatization and stemming:\n    ['this', 'be', 'an', 'amaz', 'product', 'have', 'be', 'use', 'it', 'for', 'almost', 'year', 'now', 'and', 'it', 'have', 'it', 'be', 'clear', 'better', 'than', 'ani', 'other', 'product', 'have', 'use']\n    ---\n    Adding stop words\n    ['amaz', 'product', 'have', 'use', 'for', 'almost', 'year', 'now', 'and', 'have', 'clear', 'better', 'than', 'ani', 'other', 'product', 'have', 'use']\n    ---\n\n\n## Processing a list of text\n\n\n```python\ntext_list = ['I am ready',\"This is great\",\"I love it\"]\ntk = text_tokenizer_xm(text = text_list, lemma_flag= True, stem_flag = True, \\\n                       contractions = contractions, stopwords=[])\n# Use the .txt_pre_pros_all method instead when the input is a corpus\nprint(tk.txt_pre_pros_all())\nprint(\"---\")\n```\n\n    0          [be, readi]\n    1    [this, be, great]\n    2           [love, it]\n    dtype: object\n    ---\n\n\n## The order of stop words removal and lemmatization/stemming\n\nThe current algorithm **performs lemmatization and stem before stop-words removal**. Thus,\n\n1. You need to be carefull when defining a list of stop words. For example, including the term \"product\" will also remove the term \"production\" if you set the stem_flag to True or the term \"products\" if you set lemma_flag to True.\n\n2. When the lemma_flag is set to True, terms like \"is\" and \"are\" will be lemmatized to \"be\". And if \"be\" is not in the list of stopwords, it will remain. It is recommended that you process the list of stop-words as well if you decide to perform lemmatization\n\n\n```python\n\"\"\"\nExample\n\"\"\"\n\ntext = \"products, production, is\"\nstop_words = ['product','is']\ntk = text_tokenizer_xm(text = text, lemma_flag= False, stem_flag = False, \\\n                       contractions = contractions, stopwords=stop_words)\n# Use the .txt_pre_pros_all method instead when the input is a corpus\nprint(tk.txt_pre_pros())\n```\n\n    ['products', 'production']\n\n\n\n```python\ntk = text_tokenizer_xm(text = text, lemma_flag= True, stem_flag = False, \\\n                       contractions = contractions, stopwords=stop_words)\n# Use the .txt_pre_pros_all method instead when the input is a corpus\nprint(tk.txt_pre_pros())\n```\n\n    ['production', 'be']\n\n\n\n```python\ntk = text_tokenizer_xm(text = text, lemma_flag= True, stem_flag = True, \\\n                       contractions = contractions, stopwords=stop_words)\n# Use the .txt_pre_pros_all method instead when the input is a corpus\nprint(tk.txt_pre_pros())\n```\n\n    ['be']", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/ALaughingHorse/tokenizer_xm/archive/v_05.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ALaughingHorse/tokenizer_xm", "keywords": "text preprocessing,tokenize,NLP", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "tokenizer-xm", "package_url": "https://pypi.org/project/tokenizer-xm/", "platform": "", "project_url": "https://pypi.org/project/tokenizer-xm/", "project_urls": {"Download": "https://github.com/ALaughingHorse/tokenizer_xm/archive/v_05.tar.gz", "Homepage": "https://github.com/ALaughingHorse/tokenizer_xm"}, "release_url": "https://pypi.org/project/tokenizer-xm/0.5/", "requires_dist": null, "requires_python": "", "summary": "Tokenizing with options to include contractions, lemmatize and stem.", "version": "0.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Introduction</h1>\n<p>This package is an aggregation of several packages I found useful for text pre-processing including gensim and ntlk. I put them together to create a more comprehensive and convenient pipeline.</p>\n<h1>Installation</h1>\n<pre><code>pip install tokenizer_xm\n</code></pre>\n<h1>Usage</h1>\n<h2>Processing a single text string</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tokenizer_xm</span> <span class=\"kn\">import</span> <span class=\"n\">text_tokenizer_xm</span><span class=\"p\">,</span> <span class=\"n\">contractions</span>\n\n<span class=\"c1\"># An example text</span>\n<span class=\"n\">example_text</span> <span class=\"o\">=</span> <span class=\"s2\">\"This is an amazing product! I've been using it for almost a year now and it's clearly better</span><span class=\"se\">\\</span>\n<span class=\"s2\"> than any other products I've used.\"</span>\n</pre>\n<pre><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Original text:\"</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">example_text</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"---\"</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Simple Preprocessed:\"</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"---\"</span><span class=\"p\">)</span>\n<span class=\"n\">tk</span> <span class=\"o\">=</span> <span class=\"n\">text_tokenizer_xm</span><span class=\"p\">(</span><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">example_text</span><span class=\"p\">,</span> <span class=\"n\">lemma_flag</span><span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">stem_flag</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">stopwords</span> <span class=\"o\">=</span> <span class=\"p\">[])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tk</span><span class=\"o\">.</span><span class=\"n\">txt_pre_pros</span><span class=\"p\">())</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"---\"</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Pre-processing with regular contractions (e.g. I've -&gt; I have):\"</span><span class=\"p\">)</span>\n<span class=\"c1\"># In this package, I included a dictionary of regular contractions for your convenience</span>\n<span class=\"n\">tk</span> <span class=\"o\">=</span> <span class=\"n\">text_tokenizer_xm</span><span class=\"p\">(</span><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">example_text</span><span class=\"p\">,</span> <span class=\"n\">lemma_flag</span><span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">stem_flag</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> \\\n                       <span class=\"n\">contractions</span> <span class=\"o\">=</span> <span class=\"n\">contractions</span><span class=\"p\">,</span> <span class=\"n\">stopwords</span><span class=\"o\">=</span><span class=\"p\">[])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tk</span><span class=\"o\">.</span><span class=\"n\">txt_pre_pros</span><span class=\"p\">())</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"---\"</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Pre-processing with lemmatization:\"</span><span class=\"p\">)</span>\n<span class=\"n\">tk</span> <span class=\"o\">=</span> <span class=\"n\">text_tokenizer_xm</span><span class=\"p\">(</span><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">example_text</span><span class=\"p\">,</span> <span class=\"n\">lemma_flag</span><span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">stem_flag</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> \\\n                       <span class=\"n\">contractions</span> <span class=\"o\">=</span> <span class=\"n\">contractions</span><span class=\"p\">,</span> <span class=\"n\">stopwords</span><span class=\"o\">=</span><span class=\"p\">[])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tk</span><span class=\"o\">.</span><span class=\"n\">txt_pre_pros</span><span class=\"p\">())</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"---\"</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Pre-processing with lemmatization and stemming:\"</span><span class=\"p\">)</span>\n<span class=\"c1\"># This package uses the SnowballStemmer from ntlk.stem. I will try to make it customizable later</span>\n<span class=\"n\">tk</span> <span class=\"o\">=</span> <span class=\"n\">text_tokenizer_xm</span><span class=\"p\">(</span><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">example_text</span><span class=\"p\">,</span> <span class=\"n\">lemma_flag</span><span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">stem_flag</span> <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> \\\n                       <span class=\"n\">contractions</span> <span class=\"o\">=</span> <span class=\"n\">contractions</span><span class=\"p\">,</span> <span class=\"n\">stopwords</span><span class=\"o\">=</span><span class=\"p\">[])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tk</span><span class=\"o\">.</span><span class=\"n\">txt_pre_pros</span><span class=\"p\">())</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"---\"</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Adding stop words\"</span><span class=\"p\">)</span>\n<span class=\"c1\"># This package uses the SnowballStemmer from ntlk.stem. I will try to make it customizable later</span>\n<span class=\"n\">tk</span> <span class=\"o\">=</span> <span class=\"n\">text_tokenizer_xm</span><span class=\"p\">(</span><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">example_text</span><span class=\"p\">,</span> <span class=\"n\">lemma_flag</span><span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">stem_flag</span> <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> \\\n                       <span class=\"n\">contractions</span> <span class=\"o\">=</span> <span class=\"n\">contractions</span><span class=\"p\">,</span> <span class=\"n\">stopwords</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"this\"</span><span class=\"p\">,</span><span class=\"s1\">'be'</span><span class=\"p\">,</span><span class=\"s2\">\"an\"</span><span class=\"p\">,</span><span class=\"s1\">'it'</span><span class=\"p\">])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tk</span><span class=\"o\">.</span><span class=\"n\">txt_pre_pros</span><span class=\"p\">())</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"---\"</span><span class=\"p\">)</span>\n</pre>\n<pre><code>Original text:\nThis is an amazing product! I've been using it for almost a year now and it's clearly better than any other products I've used.\n---\nSimple Preprocessed:\n---\n['this', 'is', 'an', 'amazing', 'product', 've', 'been', 'using', 'it', 'for', 'almost', 'year', 'now', 'and', 'it', 'clearly', 'better', 'than', 'any', 'other', 'products', 've', 'used']\n---\nPre-processing with regular contractions (e.g. I've -&gt; I have):\n['this', 'is', 'an', 'amazing', 'product', 'have', 'been', 'using', 'it', 'for', 'almost', 'year', 'now', 'and', 'it', 'has', 'it', 'is', 'clearly', 'better', 'than', 'any', 'other', 'products', 'have', 'used']\n---\nPre-processing with lemmatization:\n['this', 'be', 'an', 'amaze', 'product', 'have', 'be', 'use', 'it', 'for', 'almost', 'year', 'now', 'and', 'it', 'have', 'it', 'be', 'clearly', 'better', 'than', 'any', 'other', 'product', 'have', 'use']\n---\nPre-processing with lemmatization and stemming:\n['this', 'be', 'an', 'amaz', 'product', 'have', 'be', 'use', 'it', 'for', 'almost', 'year', 'now', 'and', 'it', 'have', 'it', 'be', 'clear', 'better', 'than', 'ani', 'other', 'product', 'have', 'use']\n---\nAdding stop words\n['amaz', 'product', 'have', 'use', 'for', 'almost', 'year', 'now', 'and', 'have', 'clear', 'better', 'than', 'ani', 'other', 'product', 'have', 'use']\n---\n</code></pre>\n<h2>Processing a list of text</h2>\n<pre><span class=\"n\">text_list</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'I am ready'</span><span class=\"p\">,</span><span class=\"s2\">\"This is great\"</span><span class=\"p\">,</span><span class=\"s2\">\"I love it\"</span><span class=\"p\">]</span>\n<span class=\"n\">tk</span> <span class=\"o\">=</span> <span class=\"n\">text_tokenizer_xm</span><span class=\"p\">(</span><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">text_list</span><span class=\"p\">,</span> <span class=\"n\">lemma_flag</span><span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">stem_flag</span> <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> \\\n                       <span class=\"n\">contractions</span> <span class=\"o\">=</span> <span class=\"n\">contractions</span><span class=\"p\">,</span> <span class=\"n\">stopwords</span><span class=\"o\">=</span><span class=\"p\">[])</span>\n<span class=\"c1\"># Use the .txt_pre_pros_all method instead when the input is a corpus</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tk</span><span class=\"o\">.</span><span class=\"n\">txt_pre_pros_all</span><span class=\"p\">())</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"---\"</span><span class=\"p\">)</span>\n</pre>\n<pre><code>0          [be, readi]\n1    [this, be, great]\n2           [love, it]\ndtype: object\n---\n</code></pre>\n<h2>The order of stop words removal and lemmatization/stemming</h2>\n<p>The current algorithm <strong>performs lemmatization and stem before stop-words removal</strong>. Thus,</p>\n<ol>\n<li>\n<p>You need to be carefull when defining a list of stop words. For example, including the term \"product\" will also remove the term \"production\" if you set the stem_flag to True or the term \"products\" if you set lemma_flag to True.</p>\n</li>\n<li>\n<p>When the lemma_flag is set to True, terms like \"is\" and \"are\" will be lemmatized to \"be\". And if \"be\" is not in the list of stopwords, it will remain. It is recommended that you process the list of stop-words as well if you decide to perform lemmatization</p>\n</li>\n</ol>\n<pre><span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">Example</span>\n<span class=\"sd\">\"\"\"</span>\n\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s2\">\"products, production, is\"</span>\n<span class=\"n\">stop_words</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'product'</span><span class=\"p\">,</span><span class=\"s1\">'is'</span><span class=\"p\">]</span>\n<span class=\"n\">tk</span> <span class=\"o\">=</span> <span class=\"n\">text_tokenizer_xm</span><span class=\"p\">(</span><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">lemma_flag</span><span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">stem_flag</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> \\\n                       <span class=\"n\">contractions</span> <span class=\"o\">=</span> <span class=\"n\">contractions</span><span class=\"p\">,</span> <span class=\"n\">stopwords</span><span class=\"o\">=</span><span class=\"n\">stop_words</span><span class=\"p\">)</span>\n<span class=\"c1\"># Use the .txt_pre_pros_all method instead when the input is a corpus</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tk</span><span class=\"o\">.</span><span class=\"n\">txt_pre_pros</span><span class=\"p\">())</span>\n</pre>\n<pre><code>['products', 'production']\n</code></pre>\n<pre><span class=\"n\">tk</span> <span class=\"o\">=</span> <span class=\"n\">text_tokenizer_xm</span><span class=\"p\">(</span><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">lemma_flag</span><span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">stem_flag</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> \\\n                       <span class=\"n\">contractions</span> <span class=\"o\">=</span> <span class=\"n\">contractions</span><span class=\"p\">,</span> <span class=\"n\">stopwords</span><span class=\"o\">=</span><span class=\"n\">stop_words</span><span class=\"p\">)</span>\n<span class=\"c1\"># Use the .txt_pre_pros_all method instead when the input is a corpus</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tk</span><span class=\"o\">.</span><span class=\"n\">txt_pre_pros</span><span class=\"p\">())</span>\n</pre>\n<pre><code>['production', 'be']\n</code></pre>\n<pre><span class=\"n\">tk</span> <span class=\"o\">=</span> <span class=\"n\">text_tokenizer_xm</span><span class=\"p\">(</span><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">lemma_flag</span><span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">stem_flag</span> <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> \\\n                       <span class=\"n\">contractions</span> <span class=\"o\">=</span> <span class=\"n\">contractions</span><span class=\"p\">,</span> <span class=\"n\">stopwords</span><span class=\"o\">=</span><span class=\"n\">stop_words</span><span class=\"p\">)</span>\n<span class=\"c1\"># Use the .txt_pre_pros_all method instead when the input is a corpus</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tk</span><span class=\"o\">.</span><span class=\"n\">txt_pre_pros</span><span class=\"p\">())</span>\n</pre>\n<pre><code>['be']\n</code></pre>\n\n          </div>"}, "last_serial": 6263835, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "a0edfa44bf4e72e7c86162d61cf8727d", "sha256": "ede096ef449268c8bf955f48e495430ca1e4616f5f58a6d7761a75cc19a157aa"}, "downloads": -1, "filename": "tokenizer_xm-0.1.tar.gz", "has_sig": false, "md5_digest": "a0edfa44bf4e72e7c86162d61cf8727d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2994, "upload_time": "2019-09-20T19:44:48", "upload_time_iso_8601": "2019-09-20T19:44:48.717767Z", "url": "https://files.pythonhosted.org/packages/55/2c/4d19ddee2423395aeb70dca5413716e4028665b354c5b489df460fb7da53/tokenizer_xm-0.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "7d6e59a9959d14539d308d4c53c1099e", "sha256": "e6cb18dceaa547eef85fe7d472b82536e3a024d77737b629ed223953c91ec454"}, "downloads": -1, "filename": "tokenizer_xm-0.2.tar.gz", "has_sig": false, "md5_digest": "7d6e59a9959d14539d308d4c53c1099e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3013, "upload_time": "2019-09-20T20:12:08", "upload_time_iso_8601": "2019-09-20T20:12:08.952991Z", "url": "https://files.pythonhosted.org/packages/b7/c8/8b2ce0816e126c9b3661ba921ff3616d10e080ad007b83b775a98d383601/tokenizer_xm-0.2.tar.gz", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "d37ecab1f64f52f86679afca0f6af758", "sha256": "d2200dee67cb2fbafcf1bbf7eb7c806db2ec812db538f0bae8dfa8f96e6d3bed"}, "downloads": -1, "filename": "tokenizer_xm-0.4.tar.gz", "has_sig": false, "md5_digest": "d37ecab1f64f52f86679afca0f6af758", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3006, "upload_time": "2019-11-19T00:05:44", "upload_time_iso_8601": "2019-11-19T00:05:44.774289Z", "url": "https://files.pythonhosted.org/packages/51/96/f6a80fa7cabbc2c2c1f7b3bdfa50a201cf7015c09ebff93ea5c5787906a2/tokenizer_xm-0.4.tar.gz", "yanked": false}], "0.5": [{"comment_text": "", "digests": {"md5": "09051bb2c83ccc373e1819d9806544ea", "sha256": "9175769ae7df5e914c083229a95776b10db0de566fbff9fd245b3187f69cf25d"}, "downloads": -1, "filename": "tokenizer_xm-0.5.tar.gz", "has_sig": false, "md5_digest": "09051bb2c83ccc373e1819d9806544ea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4945, "upload_time": "2019-12-09T00:23:28", "upload_time_iso_8601": "2019-12-09T00:23:28.977935Z", "url": "https://files.pythonhosted.org/packages/85/f5/ba28eb6fe7c743f1cf6afd7a62467d2916efedbbcab1698e0e03c712249f/tokenizer_xm-0.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "09051bb2c83ccc373e1819d9806544ea", "sha256": "9175769ae7df5e914c083229a95776b10db0de566fbff9fd245b3187f69cf25d"}, "downloads": -1, "filename": "tokenizer_xm-0.5.tar.gz", "has_sig": false, "md5_digest": "09051bb2c83ccc373e1819d9806544ea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4945, "upload_time": "2019-12-09T00:23:28", "upload_time_iso_8601": "2019-12-09T00:23:28.977935Z", "url": "https://files.pythonhosted.org/packages/85/f5/ba28eb6fe7c743f1cf6afd7a62467d2916efedbbcab1698e0e03c712249f/tokenizer_xm-0.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:51:24 2020"}