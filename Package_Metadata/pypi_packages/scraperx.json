{"info": {"author": "Eddy Hintze", "author_email": "eddy@hintze.co", "bugtrack_url": null, "classifiers": [], "description": "# ScraperX  SDK\n\n[![PyPI](https://img.shields.io/pypi/v/scraperx.svg)](https://pypi.org/project/scraperx/)  \n[![PyPI](https://img.shields.io/pypi/l/scraperx.svg)](https://pypi.org/project/scraperx/)  \n\n\n### Getting Started\n\n1. Create a new directory where the scraper will live and add the following files:\n    - A config file: `config.yaml` [(Example)](./examples/config.yaml)\n    - The scraper file: `your_scraper.py` [(Example)](./examples/minimal.py)\n1. Next install this library from pypi: `pip install scraperx`\n1. Run the full scraper by running `python your_scraper.py dispatch`\n    - To see the arguments for the command: `python your_scraper.py dispatch -h`\n    - See all the commands available: `python your_scraper.py -h`\n\n#### Sample scrapers\nSample scrapers can be found in the [examples](./examples) folder of this repo\n\n\n## Developing\n\nAny time the scraper needs to override the bases `__init__`, always pass in `*args` & `**kwargs` like so:  \n```python\ndef __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n```\n\n### Dispatching\n\n#### Task data\nThis is a dict of values that is passed to each step of the process. The scraper can put anything it wants here that it may need. But here are a few build in values that are not required, but are used if you do supply them:\n\n- `headers`: Dict of headers to use each request\n- `proxy`: Full proxy string to be used\n- `proxy_country`: Used to get a proxy for this region, if this and `proxy` are not set, a random proxy will be used.\n- `device_type`: used when setting a user-agent if one was not set. Options are `desktop` or `mobile`\n\n### Downloading\n\nUses a `requests.Session` to make get and post requests.\nThe `__init__` of the `BaseDownload` class can take the following args:\n- task: Required. The task from the dispatcher\n- headers: (Named arg) dict to set headers for the whole session. default: random User-Agent for the device type, will use desktop if no device type is set\n- proxy: (Named arg) Proxy string to use for the requests\n- ignore_codes: (Named arg) List of HTTP Status codes to not retry on. If these codes are seen, it will treat the request as any other success. \n\n\nWhen using BaseDownloader, a requests session is created under `self.session`, so every get/post you make will use the same session per task.\nHeaders can also be set per call by passing the keyword args to `self.request_get()` and `self.request_post()`. Any kwargs you pass to self.request_get/post will be passed to the sessions get/post methods.\n\nWhen using BaseDownloader's get & post functions, it will use the requests session created in __init__ and a python `requests` response object.\n\nA request will retry _n_ times (3 by default) to get a successful status code, each retry it will try and trigger a function called `new_profile()` where you have the chance to switch the headers/proxy the request is using (will only update for that request?). If that function does not exist, it will try again with the same data.\n\nThere are a few custom arguments that can be passed into the `self.request_*` functions that this sdk will use. All others will be passed to the `requests` methods call.  \nNamed arguments:\n- **max_tries**: Default=3. Type int. The number of tries a request will be tried, each try it will try and get a new proxy and User-Agent\n- **custom_source_checks**: Default=None. Type list of lists. Used to set the request to a set status code based on a regex that runs on the page source.\n    - This will look to see if the words _captcha_ are in the source page and set that response status code to a 403, with the status message being _Capacha Found_. The status message is there so you know if it is a real 403 or your custom status.\n        -  `[(re.compile(r'captcha', re.I), 403, 'Capacha Found')]`\n\nWhen using `self.request_*`, it will return a normal requests.request response, If using custom source checks, `response.reason` will be set to the custom message passed in. This is useful if you have multiple ways a custom 403 happens and you need to do different actions depending on why.\n\n#### Saving the source\nThis is required for the extractor to run on the downloaded data. Inside of `self.download()` just call `self.save_request(r)` on the request that was made. This will add the source file to a list of saved sources that will be passed to the extractor for parsing.  \nSome keyword arguments that can be passed into `self.save_request`  \n- **template_values** _{dict}_ - Additional keys to use in the template\n- **filename** _{str}_ - Override the filename from the template_name in the config\n\n#### Download Exceptions\nThese exceptions will be raised when calling `self.request_*`. They will be caught safely so the scraper does not need to catch them. But if the scraper wanted to do something based on the exception, there can be a `try/except` around the scrapers `self.request_*`.  \n\n - `scraperx.exceptions.DownloadValueError`: If there is an exception that is not caught by the others\n - `scraperx.exceptions.HTTPIgnoreCodeError`: When the status code of the request is found in the `ignore_codes` argument of BaseDownload\n - `requests.exceptions.HTTPError`: When the requests returns a non successful status code and was not found in `ignore_codes` \n\n#### Setting headers/proxies\n\nThe ones set in the `self.request_get/request_post` will be combined with the ones set in the `__init__` and override if the key is the same.\n\nself.request_get/request_post kwargs headers/proxy  \nwill override  \nself.task[headers/proxy]  \nwill override  \n__init__ kwargs headers/proxy  \n\nAny header/proxy set on the request (get/post/etc) will only be set for that single request. For those values to be set in the session they must be set from the init or be in the task data.\n\n#### Proxies\nIf you have a list of proxies that the downloader should auto rotate between they can be saved in a csv in the following format:\n```csv\nproxy,country\nhttp://user:pass@some-server.com:5500,US\nhttp://user:pass@some-server.com:5501,US\nhttp://user:pass@some-server.com:6501,DE\n```\nSet the env var `PROXY_FILE` to the path of the above csv for the scraper to load it in.  \nIf you have not passed in a proxy directly in the task and this proxy csv exists, then it will pull a random proxy from this file. It will use the `proxy_country` if set in the task data to select the correct country to proxy to.\n\n#### User-Agent\nIf you have not directly set a user-agent, a random one will be pulled based on the `device_type` in the task data.  \nIf `device_type` is not set, it will default to use a desktop user-agent.\nTo set your own list of user-agents to choose from, create a csv in the following format:  \n```csv\ndevice_type,user_agent\ndesktop,\"Some User Agent for desktop\"\ndesktop,\"Another User Agent for desktop\"\nmobile,\"Now one for mobile\"\n```\nSet the env var `UA_FILE` to the path of the above csv for the scraper to load it in.  \n\n\n### Extracting\n\n[Parsel documentation](https://parsel.readthedocs.io/en/latest/)  \n\n#### Data extraction helpers\n`self.find_css_elements(source, css_selectors)`  \n  - `source` - Parsel object to run the css selectors on\n  - `css_selectors` - A list of css selectors to try and extract the data\n\nReturns a Parsel element from the first css selector that returns data.  \n\nThis snippet would be in the scrapers `MyScraperExtract(Extract)` class, used in the method that is extracting the data.\n```python\ntitle_selectors = ['h3',\n                   'span.title',\n                   ]\nresult['title'] = self.find_css_elements(element, title_selectors)\\\n                      .xpath('string()').extract_first().strip()\n```\n\nThere are a few built in parsers that can assist with extracting some types of data \n```python\nfrom scraperx import parsers\n\n###\n# Price\n###\n# This will parse the price out of a string and return the low and high values as floats\nraw_p1_str = '15,48\u20ac'\np1 = parsers.price(raw_p1_str)\n# p1 = {'low': 15.48, 'high': None}\n\nraw_p2_str = '1,999'\np2 = parsers.price(raw_p2_str)\n# p2 = {'low': 1999.0, 'high': None}\n\nraw_p3_str = '$49.95 - $99.99'\np3 = parsers.price(raw_p3_str)\n# p3 = {'low': 49.95, 'high': 99.99}\n\n###\n# Rating\n###\n# Parse the rating from a string\n# Examples: https://regex101.com/r/ChmgmF/3\nraw_r1_str = '4.4 out of 5 stars'\nr1 = parsers.rating(raw_r1_str)\n# r1 = 4.4\n\nraw_r2_str = 'An average of 4.1 star'\nr2 = parsers.rating(raw_r2_str)\n# r2 = 4.1\n```\n\nIf there are more cases you would like these parsers to catch please open up an issue with the use case you are trying to parse.\n\n\n### Testing\nWhen updating the extractors there is a chance that it will not work with the previous source files. So having a source and its QA'd data file is useful to test against to verify that data is still extracting correctly.\n\n#### Creating test files\n1. Run `python your_scraper.py create-test path_to/metadata_source_file`\n    - The input file is the `*_metadata.json` file that gets created when you run the scraper and it downloads the source files.\n2. This will copy the metadata file and the sources into the directory `tests/sample_data/your_scraper/` using the time the source was downloaded (from the metadata) as the file name.\n    - It also creates extracted qa files for each of the sources based on your extractors.\n    - it extracts the data in json format to make it easy to qa and read.\n3. The QA files it created will have `_extracted_(qa)_` in the file name. What you have to do it confirm that all values are correct in that file. If everything looks good then fix the file name from having `_extracted_(qa)_` to `_extracted_qa_`. This will let the system know that the file has been checked and that is the data it will use to compare when testing.\n4. Create an empty file `tests/__init__.py`. This is needed for the tests to run.\n5. Next is to create the code that will run the tests. Create the file `tests/tests.py` with the contents below\n```python\nimport unittest  # The testing frame work to use\nfrom scraperx.test import ExtractorBaseTest  # Does all the heavy lifting for the test\nfrom your_scraper import scraper as my_scraper  # The scrapers Scraper class\n# If you have multiple scrapers, then import their extract classes here as well\n\n# This test will loop through all the test files for the scraper\nclass YourScraper(ExtractorBaseTest.TestCase):\n\n    def __init__(self, *args, **kwargs):\n        # The directory that the test files for your scraper are in\n        data_dir = 'tests/sample_data/your_scraper'\n        # ignore_keys will not test the qa values to the current extracted test value. This is most useful when dealing with timestamps or other values that will change on each time the data is extracted\n        super().__init__(data_dir, my_scraper, ignore_keys=['time_extracted'], *args, **kwargs)\n\n# If you have multiple scrapers, then create a class for each\n\n# Feel free to include any other unit tests you may want to run as well\n```\n6. Running the tests `python -m unittest discover -vv`\n\n\n## Config\n\n3 Ways of setting config values:\n- CLI Argument: Will override any other type of config value. Use `-h` to see available options\n- Environment variable: Will override a config value in the yaml\n- Yaml file: Will use these values if no other way is set for a key\n\n### Config values\n\n```yaml\n# config.yaml\n# This is a config file with all config values\n# Required fields are marked as such\n\ndefault:\n  dispatch:\n    limit: 5  # Default None. Max number of tasks to dispatch. If not set, all tasks will run\n    service:\n      # This is where both the download and extractor services will run\n      name: local  # (local, sns) Default: local\n      sns_arn: sns:arn:of:service:to:trigger  # Required if `name` is sns, if local this is not needed\n    ratelimit:\n      type: qps  # (qps, period) Required. `qps`: Queries per second to dispatch the tasks at. `period`: The time in hours to dispatch all of the tasks in.\n      value: 1  # Required. Can be an int or a float. When using period, value is in hours\n  \n  downloader:\n    save_metadata: true  # (true, false) Default: true. If false, a metadata file will NOT be saved with the downloaded source.\n    save_data:\n      service: local  # (local, s3) Default: local\n      bucket_name: my-downloaded-data  # Required if `service` is s3, if local this is not needed\n    file_template: test_output/{scraper_name}/{id}_source.html  # Optional, Default is \"output/extracted.json\"\n\n  extractor:\n    save_data:\n      service: local  # (local, s3) Default: local\n      bucket_name: my-extracted-data  # Required if `service` is s3, if local this is not needed\n    file_template: test_output/{scraper_name}/{id}_extracted.json  # Optional, Default is \"output/source.html\"\n```\n\nIf you are using the `file_template` config, a python `.format()` runs on this string so you can use `{key_name}` to make it dynamic. The keys you will have direct access to are the following:\n  - All keys in your task that was dispatched\n  - Any thing you pass into the `template_values={}` kwarg for the `.save()` fn\n  - `time_downloaded`: time (utc) passed from the downloader (in both the downloader and extractor)\n  - `date_downloaded`: date (utc) passed from the downloader (in both the downloader and extractor)\n  - `time_extracted`: time (utc) passed from the extractor (just in the extractor)\n  - `date_extracted`: date (utc) passed from the extractor (just in the extractor)\n\nAnything under the `default` section can also have its own value per scraper. So if we have a scraper named `search` and we want it to use a different rate limit then all the other scrapers you can do:\n```yaml\n# Name of the python file\nsearch:\n  dispatch:\n    ratelimit:\n      type: period\n      value: 5\n```\n\nTo override the `value` in the above snippet using an environment variable, set `DISPATCH_RATELIMIT_VALUE=1`. This will override all dispatch ratelimit values in default and custom.\n\n\n## Issues\nIf you run into the error `may have been in progress in another thread when fork() was called.` when running the scraper locally on a mac. Then set the env var `export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES`  \nThis is because of a security setting on macs when spawning threads from threads https://github.com/ansible/ansible/issues/32499#issuecomment-341578864", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ScraperX/scraperx", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "scraperx", "package_url": "https://pypi.org/project/scraperx/", "platform": "", "project_url": "https://pypi.org/project/scraperx/", "project_urls": {"Homepage": "https://github.com/ScraperX/scraperx"}, "release_url": "https://pypi.org/project/scraperx/0.4.6/", "requires_dist": null, "requires_python": ">=3.6.0", "summary": "ScraperX SDK", "version": "0.4.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>ScraperX  SDK</h1>\n<p><a href=\"https://pypi.org/project/scraperx/\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/120d3463700fd9da7445c30a1e6cc63b4eda4c1b/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f73637261706572782e737667\"></a><br>\n<a href=\"https://pypi.org/project/scraperx/\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/47103f034e50b90437c306145d4c6b5f3dc2b5c0/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f73637261706572782e737667\"></a></p>\n<h3>Getting Started</h3>\n<ol>\n<li>Create a new directory where the scraper will live and add the following files:\n<ul>\n<li>A config file: <code>config.yaml</code> <a href=\"./examples/config.yaml\" rel=\"nofollow\">(Example)</a></li>\n<li>The scraper file: <code>your_scraper.py</code> <a href=\"./examples/minimal.py\" rel=\"nofollow\">(Example)</a></li>\n</ul>\n</li>\n<li>Next install this library from pypi: <code>pip install scraperx</code></li>\n<li>Run the full scraper by running <code>python your_scraper.py dispatch</code>\n<ul>\n<li>To see the arguments for the command: <code>python your_scraper.py dispatch -h</code></li>\n<li>See all the commands available: <code>python your_scraper.py -h</code></li>\n</ul>\n</li>\n</ol>\n<h4>Sample scrapers</h4>\n<p>Sample scrapers can be found in the <a href=\"./examples\" rel=\"nofollow\">examples</a> folder of this repo</p>\n<h2>Developing</h2>\n<p>Any time the scraper needs to override the bases <code>__init__</code>, always pass in <code>*args</code> &amp; <code>**kwargs</code> like so:</p>\n<pre><span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n    <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n</pre>\n<h3>Dispatching</h3>\n<h4>Task data</h4>\n<p>This is a dict of values that is passed to each step of the process. The scraper can put anything it wants here that it may need. But here are a few build in values that are not required, but are used if you do supply them:</p>\n<ul>\n<li><code>headers</code>: Dict of headers to use each request</li>\n<li><code>proxy</code>: Full proxy string to be used</li>\n<li><code>proxy_country</code>: Used to get a proxy for this region, if this and <code>proxy</code> are not set, a random proxy will be used.</li>\n<li><code>device_type</code>: used when setting a user-agent if one was not set. Options are <code>desktop</code> or <code>mobile</code></li>\n</ul>\n<h3>Downloading</h3>\n<p>Uses a <code>requests.Session</code> to make get and post requests.\nThe <code>__init__</code> of the <code>BaseDownload</code> class can take the following args:</p>\n<ul>\n<li>task: Required. The task from the dispatcher</li>\n<li>headers: (Named arg) dict to set headers for the whole session. default: random User-Agent for the device type, will use desktop if no device type is set</li>\n<li>proxy: (Named arg) Proxy string to use for the requests</li>\n<li>ignore_codes: (Named arg) List of HTTP Status codes to not retry on. If these codes are seen, it will treat the request as any other success.</li>\n</ul>\n<p>When using BaseDownloader, a requests session is created under <code>self.session</code>, so every get/post you make will use the same session per task.\nHeaders can also be set per call by passing the keyword args to <code>self.request_get()</code> and <code>self.request_post()</code>. Any kwargs you pass to self.request_get/post will be passed to the sessions get/post methods.</p>\n<p>When using BaseDownloader's get &amp; post functions, it will use the requests session created in <strong>init</strong> and a python <code>requests</code> response object.</p>\n<p>A request will retry <em>n</em> times (3 by default) to get a successful status code, each retry it will try and trigger a function called <code>new_profile()</code> where you have the chance to switch the headers/proxy the request is using (will only update for that request?). If that function does not exist, it will try again with the same data.</p>\n<p>There are a few custom arguments that can be passed into the <code>self.request_*</code> functions that this sdk will use. All others will be passed to the <code>requests</code> methods call.<br>\nNamed arguments:</p>\n<ul>\n<li><strong>max_tries</strong>: Default=3. Type int. The number of tries a request will be tried, each try it will try and get a new proxy and User-Agent</li>\n<li><strong>custom_source_checks</strong>: Default=None. Type list of lists. Used to set the request to a set status code based on a regex that runs on the page source.\n<ul>\n<li>This will look to see if the words <em>captcha</em> are in the source page and set that response status code to a 403, with the status message being <em>Capacha Found</em>. The status message is there so you know if it is a real 403 or your custom status.\n<ul>\n<li><code>[(re.compile(r'captcha', re.I), 403, 'Capacha Found')]</code></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>When using <code>self.request_*</code>, it will return a normal requests.request response, If using custom source checks, <code>response.reason</code> will be set to the custom message passed in. This is useful if you have multiple ways a custom 403 happens and you need to do different actions depending on why.</p>\n<h4>Saving the source</h4>\n<p>This is required for the extractor to run on the downloaded data. Inside of <code>self.download()</code> just call <code>self.save_request(r)</code> on the request that was made. This will add the source file to a list of saved sources that will be passed to the extractor for parsing.<br>\nSome keyword arguments that can be passed into <code>self.save_request</code></p>\n<ul>\n<li><strong>template_values</strong> <em>{dict}</em> - Additional keys to use in the template</li>\n<li><strong>filename</strong> <em>{str}</em> - Override the filename from the template_name in the config</li>\n</ul>\n<h4>Download Exceptions</h4>\n<p>These exceptions will be raised when calling <code>self.request_*</code>. They will be caught safely so the scraper does not need to catch them. But if the scraper wanted to do something based on the exception, there can be a <code>try/except</code> around the scrapers <code>self.request_*</code>.</p>\n<ul>\n<li><code>scraperx.exceptions.DownloadValueError</code>: If there is an exception that is not caught by the others</li>\n<li><code>scraperx.exceptions.HTTPIgnoreCodeError</code>: When the status code of the request is found in the <code>ignore_codes</code> argument of BaseDownload</li>\n<li><code>requests.exceptions.HTTPError</code>: When the requests returns a non successful status code and was not found in <code>ignore_codes</code></li>\n</ul>\n<h4>Setting headers/proxies</h4>\n<p>The ones set in the <code>self.request_get/request_post</code> will be combined with the ones set in the <code>__init__</code> and override if the key is the same.</p>\n<p>self.request_get/request_post kwargs headers/proxy<br>\nwill override<br>\nself.task[headers/proxy]<br>\nwill override<br>\n<strong>init</strong> kwargs headers/proxy</p>\n<p>Any header/proxy set on the request (get/post/etc) will only be set for that single request. For those values to be set in the session they must be set from the init or be in the task data.</p>\n<h4>Proxies</h4>\n<p>If you have a list of proxies that the downloader should auto rotate between they can be saved in a csv in the following format:</p>\n<pre>proxy,country\nhttp://user:pass@some-server.com:5500,US\nhttp://user:pass@some-server.com:5501,US\nhttp://user:pass@some-server.com:6501,DE\n</pre>\n<p>Set the env var <code>PROXY_FILE</code> to the path of the above csv for the scraper to load it in.<br>\nIf you have not passed in a proxy directly in the task and this proxy csv exists, then it will pull a random proxy from this file. It will use the <code>proxy_country</code> if set in the task data to select the correct country to proxy to.</p>\n<h4>User-Agent</h4>\n<p>If you have not directly set a user-agent, a random one will be pulled based on the <code>device_type</code> in the task data.<br>\nIf <code>device_type</code> is not set, it will default to use a desktop user-agent.\nTo set your own list of user-agents to choose from, create a csv in the following format:</p>\n<pre>device_type,user_agent\ndesktop,\"Some User Agent for desktop\"\ndesktop,\"Another User Agent for desktop\"\nmobile,\"Now one for mobile\"\n</pre>\n<p>Set the env var <code>UA_FILE</code> to the path of the above csv for the scraper to load it in.</p>\n<h3>Extracting</h3>\n<p><a href=\"https://parsel.readthedocs.io/en/latest/\" rel=\"nofollow\">Parsel documentation</a></p>\n<h4>Data extraction helpers</h4>\n<p><code>self.find_css_elements(source, css_selectors)</code></p>\n<ul>\n<li><code>source</code> - Parsel object to run the css selectors on</li>\n<li><code>css_selectors</code> - A list of css selectors to try and extract the data</li>\n</ul>\n<p>Returns a Parsel element from the first css selector that returns data.</p>\n<p>This snippet would be in the scrapers <code>MyScraperExtract(Extract)</code> class, used in the method that is extracting the data.</p>\n<pre><span class=\"n\">title_selectors</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'h3'</span><span class=\"p\">,</span>\n                   <span class=\"s1\">'span.title'</span><span class=\"p\">,</span>\n                   <span class=\"p\">]</span>\n<span class=\"n\">result</span><span class=\"p\">[</span><span class=\"s1\">'title'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">find_css_elements</span><span class=\"p\">(</span><span class=\"n\">element</span><span class=\"p\">,</span> <span class=\"n\">title_selectors</span><span class=\"p\">)</span>\\\n                      <span class=\"o\">.</span><span class=\"n\">xpath</span><span class=\"p\">(</span><span class=\"s1\">'string()'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">extract_first</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span>\n</pre>\n<p>There are a few built in parsers that can assist with extracting some types of data</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">scraperx</span> <span class=\"kn\">import</span> <span class=\"n\">parsers</span>\n\n<span class=\"c1\">###</span>\n<span class=\"c1\"># Price</span>\n<span class=\"c1\">###</span>\n<span class=\"c1\"># This will parse the price out of a string and return the low and high values as floats</span>\n<span class=\"n\">raw_p1_str</span> <span class=\"o\">=</span> <span class=\"s1\">'15,48\u20ac'</span>\n<span class=\"n\">p1</span> <span class=\"o\">=</span> <span class=\"n\">parsers</span><span class=\"o\">.</span><span class=\"n\">price</span><span class=\"p\">(</span><span class=\"n\">raw_p1_str</span><span class=\"p\">)</span>\n<span class=\"c1\"># p1 = {'low': 15.48, 'high': None}</span>\n\n<span class=\"n\">raw_p2_str</span> <span class=\"o\">=</span> <span class=\"s1\">'1,999'</span>\n<span class=\"n\">p2</span> <span class=\"o\">=</span> <span class=\"n\">parsers</span><span class=\"o\">.</span><span class=\"n\">price</span><span class=\"p\">(</span><span class=\"n\">raw_p2_str</span><span class=\"p\">)</span>\n<span class=\"c1\"># p2 = {'low': 1999.0, 'high': None}</span>\n\n<span class=\"n\">raw_p3_str</span> <span class=\"o\">=</span> <span class=\"s1\">'$49.95 - $99.99'</span>\n<span class=\"n\">p3</span> <span class=\"o\">=</span> <span class=\"n\">parsers</span><span class=\"o\">.</span><span class=\"n\">price</span><span class=\"p\">(</span><span class=\"n\">raw_p3_str</span><span class=\"p\">)</span>\n<span class=\"c1\"># p3 = {'low': 49.95, 'high': 99.99}</span>\n\n<span class=\"c1\">###</span>\n<span class=\"c1\"># Rating</span>\n<span class=\"c1\">###</span>\n<span class=\"c1\"># Parse the rating from a string</span>\n<span class=\"c1\"># Examples: https://regex101.com/r/ChmgmF/3</span>\n<span class=\"n\">raw_r1_str</span> <span class=\"o\">=</span> <span class=\"s1\">'4.4 out of 5 stars'</span>\n<span class=\"n\">r1</span> <span class=\"o\">=</span> <span class=\"n\">parsers</span><span class=\"o\">.</span><span class=\"n\">rating</span><span class=\"p\">(</span><span class=\"n\">raw_r1_str</span><span class=\"p\">)</span>\n<span class=\"c1\"># r1 = 4.4</span>\n\n<span class=\"n\">raw_r2_str</span> <span class=\"o\">=</span> <span class=\"s1\">'An average of 4.1 star'</span>\n<span class=\"n\">r2</span> <span class=\"o\">=</span> <span class=\"n\">parsers</span><span class=\"o\">.</span><span class=\"n\">rating</span><span class=\"p\">(</span><span class=\"n\">raw_r2_str</span><span class=\"p\">)</span>\n<span class=\"c1\"># r2 = 4.1</span>\n</pre>\n<p>If there are more cases you would like these parsers to catch please open up an issue with the use case you are trying to parse.</p>\n<h3>Testing</h3>\n<p>When updating the extractors there is a chance that it will not work with the previous source files. So having a source and its QA'd data file is useful to test against to verify that data is still extracting correctly.</p>\n<h4>Creating test files</h4>\n<ol>\n<li>Run <code>python your_scraper.py create-test path_to/metadata_source_file</code>\n<ul>\n<li>The input file is the <code>*_metadata.json</code> file that gets created when you run the scraper and it downloads the source files.</li>\n</ul>\n</li>\n<li>This will copy the metadata file and the sources into the directory <code>tests/sample_data/your_scraper/</code> using the time the source was downloaded (from the metadata) as the file name.\n<ul>\n<li>It also creates extracted qa files for each of the sources based on your extractors.</li>\n<li>it extracts the data in json format to make it easy to qa and read.</li>\n</ul>\n</li>\n<li>The QA files it created will have <code>_extracted_(qa)_</code> in the file name. What you have to do it confirm that all values are correct in that file. If everything looks good then fix the file name from having <code>_extracted_(qa)_</code> to <code>_extracted_qa_</code>. This will let the system know that the file has been checked and that is the data it will use to compare when testing.</li>\n<li>Create an empty file <code>tests/__init__.py</code>. This is needed for the tests to run.</li>\n<li>Next is to create the code that will run the tests. Create the file <code>tests/tests.py</code> with the contents below</li>\n</ol>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">unittest</span>  <span class=\"c1\"># The testing frame work to use</span>\n<span class=\"kn\">from</span> <span class=\"nn\">scraperx.test</span> <span class=\"kn\">import</span> <span class=\"n\">ExtractorBaseTest</span>  <span class=\"c1\"># Does all the heavy lifting for the test</span>\n<span class=\"kn\">from</span> <span class=\"nn\">your_scraper</span> <span class=\"kn\">import</span> <span class=\"n\">scraper</span> <span class=\"k\">as</span> <span class=\"n\">my_scraper</span>  <span class=\"c1\"># The scrapers Scraper class</span>\n<span class=\"c1\"># If you have multiple scrapers, then import their extract classes here as well</span>\n\n<span class=\"c1\"># This test will loop through all the test files for the scraper</span>\n<span class=\"k\">class</span> <span class=\"nc\">YourScraper</span><span class=\"p\">(</span><span class=\"n\">ExtractorBaseTest</span><span class=\"o\">.</span><span class=\"n\">TestCase</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n        <span class=\"c1\"># The directory that the test files for your scraper are in</span>\n        <span class=\"n\">data_dir</span> <span class=\"o\">=</span> <span class=\"s1\">'tests/sample_data/your_scraper'</span>\n        <span class=\"c1\"># ignore_keys will not test the qa values to the current extracted test value. This is most useful when dealing with timestamps or other values that will change on each time the data is extracted</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"n\">data_dir</span><span class=\"p\">,</span> <span class=\"n\">my_scraper</span><span class=\"p\">,</span> <span class=\"n\">ignore_keys</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'time_extracted'</span><span class=\"p\">],</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># If you have multiple scrapers, then create a class for each</span>\n\n<span class=\"c1\"># Feel free to include any other unit tests you may want to run as well</span>\n</pre>\n<ol>\n<li>Running the tests <code>python -m unittest discover -vv</code></li>\n</ol>\n<h2>Config</h2>\n<p>3 Ways of setting config values:</p>\n<ul>\n<li>CLI Argument: Will override any other type of config value. Use <code>-h</code> to see available options</li>\n<li>Environment variable: Will override a config value in the yaml</li>\n<li>Yaml file: Will use these values if no other way is set for a key</li>\n</ul>\n<h3>Config values</h3>\n<pre><span class=\"c1\"># config.yaml</span>\n<span class=\"c1\"># This is a config file with all config values</span>\n<span class=\"c1\"># Required fields are marked as such</span>\n\n<span class=\"nt\">default</span><span class=\"p\">:</span>\n  <span class=\"nt\">dispatch</span><span class=\"p\">:</span>\n    <span class=\"nt\">limit</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">5</span>  <span class=\"c1\"># Default None. Max number of tasks to dispatch. If not set, all tasks will run</span>\n    <span class=\"nt\">service</span><span class=\"p\">:</span>\n      <span class=\"c1\"># This is where both the download and extractor services will run</span>\n      <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">local</span>  <span class=\"c1\"># (local, sns) Default: local</span>\n      <span class=\"nt\">sns_arn</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">sns:arn:of:service:to:trigger</span>  <span class=\"c1\"># Required if `name` is sns, if local this is not needed</span>\n    <span class=\"nt\">ratelimit</span><span class=\"p\">:</span>\n      <span class=\"nt\">type</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">qps</span>  <span class=\"c1\"># (qps, period) Required. `qps`: Queries per second to dispatch the tasks at. `period`: The time in hours to dispatch all of the tasks in.</span>\n      <span class=\"nt\">value</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">1</span>  <span class=\"c1\"># Required. Can be an int or a float. When using period, value is in hours</span>\n  \n  <span class=\"nt\">downloader</span><span class=\"p\">:</span>\n    <span class=\"nt\">save_metadata</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">true</span>  <span class=\"c1\"># (true, false) Default: true. If false, a metadata file will NOT be saved with the downloaded source.</span>\n    <span class=\"nt\">save_data</span><span class=\"p\">:</span>\n      <span class=\"nt\">service</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">local</span>  <span class=\"c1\"># (local, s3) Default: local</span>\n      <span class=\"nt\">bucket_name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">my-downloaded-data</span>  <span class=\"c1\"># Required if `service` is s3, if local this is not needed</span>\n    <span class=\"nt\">file_template</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">test_output/{scraper_name}/{id}_source.html</span>  <span class=\"c1\"># Optional, Default is \"output/extracted.json\"</span>\n\n  <span class=\"nt\">extractor</span><span class=\"p\">:</span>\n    <span class=\"nt\">save_data</span><span class=\"p\">:</span>\n      <span class=\"nt\">service</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">local</span>  <span class=\"c1\"># (local, s3) Default: local</span>\n      <span class=\"nt\">bucket_name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">my-extracted-data</span>  <span class=\"c1\"># Required if `service` is s3, if local this is not needed</span>\n    <span class=\"nt\">file_template</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">test_output/{scraper_name}/{id}_extracted.json</span>  <span class=\"c1\"># Optional, Default is \"output/source.html\"</span>\n</pre>\n<p>If you are using the <code>file_template</code> config, a python <code>.format()</code> runs on this string so you can use <code>{key_name}</code> to make it dynamic. The keys you will have direct access to are the following:</p>\n<ul>\n<li>All keys in your task that was dispatched</li>\n<li>Any thing you pass into the <code>template_values={}</code> kwarg for the <code>.save()</code> fn</li>\n<li><code>time_downloaded</code>: time (utc) passed from the downloader (in both the downloader and extractor)</li>\n<li><code>date_downloaded</code>: date (utc) passed from the downloader (in both the downloader and extractor)</li>\n<li><code>time_extracted</code>: time (utc) passed from the extractor (just in the extractor)</li>\n<li><code>date_extracted</code>: date (utc) passed from the extractor (just in the extractor)</li>\n</ul>\n<p>Anything under the <code>default</code> section can also have its own value per scraper. So if we have a scraper named <code>search</code> and we want it to use a different rate limit then all the other scrapers you can do:</p>\n<pre><span class=\"c1\"># Name of the python file</span>\n<span class=\"nt\">search</span><span class=\"p\">:</span>\n  <span class=\"nt\">dispatch</span><span class=\"p\">:</span>\n    <span class=\"nt\">ratelimit</span><span class=\"p\">:</span>\n      <span class=\"nt\">type</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">period</span>\n      <span class=\"nt\">value</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">5</span>\n</pre>\n<p>To override the <code>value</code> in the above snippet using an environment variable, set <code>DISPATCH_RATELIMIT_VALUE=1</code>. This will override all dispatch ratelimit values in default and custom.</p>\n<h2>Issues</h2>\n<p>If you run into the error <code>may have been in progress in another thread when fork() was called.</code> when running the scraper locally on a mac. Then set the env var <code>export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES</code><br>\nThis is because of a security setting on macs when spawning threads from threads <a href=\"https://github.com/ansible/ansible/issues/32499#issuecomment-341578864\" rel=\"nofollow\">https://github.com/ansible/ansible/issues/32499#issuecomment-341578864</a></p>\n\n          </div>"}, "last_serial": 7100930, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "544ab9b3a7e805a6122ea71ef74541f6", "sha256": "a6bd19f500368808819bc42fd30c22f4b7690f7f240dd756632187df92742bf8"}, "downloads": -1, "filename": "scraperx-0.0.1.tar.gz", "has_sig": false, "md5_digest": "544ab9b3a7e805a6122ea71ef74541f6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22899, "upload_time": "2019-02-15T21:46:27", "upload_time_iso_8601": "2019-02-15T21:46:27.705663Z", "url": "https://files.pythonhosted.org/packages/c1/1f/1ed83225a0d94225a72174fb8243ef2312bcd7b9079a0f65062045638d68/scraperx-0.0.1.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "cdaf7d6a6f5f51bf132c0f35e11c44dd", "sha256": "20295458d887490fabbebfa6d62b11b244c0dfdcb5db70333249eb3a9bdf7214"}, "downloads": -1, "filename": "scraperx-0.0.3.tar.gz", "has_sig": false, "md5_digest": "cdaf7d6a6f5f51bf132c0f35e11c44dd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22637, "upload_time": "2019-04-07T21:03:47", "upload_time_iso_8601": "2019-04-07T21:03:47.057416Z", "url": "https://files.pythonhosted.org/packages/9e/3b/afa54c4dc66581def90a6a9d4fe6054634900926f42591d7f43bc7210086/scraperx-0.0.3.tar.gz", "yanked": false}], "0.0.4rc1": [{"comment_text": "", "digests": {"md5": "7b99afb87d32b1701e1edcd16de2047e", "sha256": "1d0a6da47809d1195593b2146e3f2fb0932595fc69bbcc6f264aa26b826b7e71"}, "downloads": -1, "filename": "scraperx-0.0.4rc1.tar.gz", "has_sig": false, "md5_digest": "7b99afb87d32b1701e1edcd16de2047e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22521, "upload_time": "2019-04-08T18:49:43", "upload_time_iso_8601": "2019-04-08T18:49:43.196051Z", "url": "https://files.pythonhosted.org/packages/5c/77/413d3e0299e0ce50c17453937370ee3a440cc8c4f4458ba45c5b659356e9/scraperx-0.0.4rc1.tar.gz", "yanked": false}], "0.0.4rc10": [{"comment_text": "", "digests": {"md5": "0b4088e2bfbd47b355a92d8e1819c15c", "sha256": "f5f731bfbb686a68f198ba6a385952b00ae2824577c831d976bb6e96202eecf1"}, "downloads": -1, "filename": "scraperx-0.0.4rc10.tar.gz", "has_sig": false, "md5_digest": "0b4088e2bfbd47b355a92d8e1819c15c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 31563, "upload_time": "2019-06-18T18:11:34", "upload_time_iso_8601": "2019-06-18T18:11:34.182783Z", "url": "https://files.pythonhosted.org/packages/aa/b4/67e6c71ae3647bbba49ecf5785baf05d40d26079b5acc460200862d036e8/scraperx-0.0.4rc10.tar.gz", "yanked": false}], "0.0.4rc11": [{"comment_text": "", "digests": {"md5": "9c27da559eb663caf094359a33c1db01", "sha256": "3a9672c043cd51523c0e9e477a8969f6dc2b793578554369ab0110c3b193a8b4"}, "downloads": -1, "filename": "scraperx-0.0.4rc11.tar.gz", "has_sig": false, "md5_digest": "9c27da559eb663caf094359a33c1db01", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 31669, "upload_time": "2019-06-29T21:04:16", "upload_time_iso_8601": "2019-06-29T21:04:16.105847Z", "url": "https://files.pythonhosted.org/packages/76/a7/6d844b2a6c47f4c44911404c0efa9b932de7ed62e994476c382a5121e1b0/scraperx-0.0.4rc11.tar.gz", "yanked": false}], "0.0.4rc12": [{"comment_text": "", "digests": {"md5": "aa7c4d5b8ef08614c1469caf77bfac8d", "sha256": "705e01ce0d4236c7bbc9f44b81a79614bd082487ae8973fe32d6c9f25aecf3ff"}, "downloads": -1, "filename": "scraperx-0.0.4rc12.tar.gz", "has_sig": false, "md5_digest": "aa7c4d5b8ef08614c1469caf77bfac8d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 31700, "upload_time": "2019-06-30T01:54:53", "upload_time_iso_8601": "2019-06-30T01:54:53.170096Z", "url": "https://files.pythonhosted.org/packages/2d/61/075713c798b6edac8ba09975262f943656a451b13c6642bf2e72b1086848/scraperx-0.0.4rc12.tar.gz", "yanked": false}], "0.0.4rc2": [{"comment_text": "", "digests": {"md5": "a5c8e86c6a99c5c1ca68abe923c996ff", "sha256": "6b507078693b6420d11fc982dbc95ff6ca5fca36f20963c6a03cd18af1d82f67"}, "downloads": -1, "filename": "scraperx-0.0.4rc2.tar.gz", "has_sig": false, "md5_digest": "a5c8e86c6a99c5c1ca68abe923c996ff", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22513, "upload_time": "2019-04-08T19:09:05", "upload_time_iso_8601": "2019-04-08T19:09:05.142899Z", "url": "https://files.pythonhosted.org/packages/b2/8c/5ba79db047ebd3a08072544caa68430053cbc0dc04bf249c01ab44a80a72/scraperx-0.0.4rc2.tar.gz", "yanked": false}], "0.0.4rc3": [{"comment_text": "", "digests": {"md5": "3b175812b3d089dfaf68ad2f754b20e5", "sha256": "1b9032a79bc15f8633d786b914be612e2bd9e83bc1828816436960a25af23ff0"}, "downloads": -1, "filename": "scraperx-0.0.4rc3.tar.gz", "has_sig": false, "md5_digest": "3b175812b3d089dfaf68ad2f754b20e5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22532, "upload_time": "2019-04-08T19:50:05", "upload_time_iso_8601": "2019-04-08T19:50:05.543078Z", "url": "https://files.pythonhosted.org/packages/05/92/b26c3a1ad63a454b5e644c370830e2edcd3bc19b9e3d162126a27991b632/scraperx-0.0.4rc3.tar.gz", "yanked": false}], "0.0.4rc4": [{"comment_text": "", "digests": {"md5": "4de35f1cd75fe31b4f2414357bd4877a", "sha256": "93d04a5be7b15f7dbd5238413792565a4d90ed9dd217725d0dd1ee44d4afb91f"}, "downloads": -1, "filename": "scraperx-0.0.4rc4.tar.gz", "has_sig": false, "md5_digest": "4de35f1cd75fe31b4f2414357bd4877a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 27292, "upload_time": "2019-05-30T18:40:04", "upload_time_iso_8601": "2019-05-30T18:40:04.498571Z", "url": "https://files.pythonhosted.org/packages/50/92/5483a2a71efe0baade7ded461c5b17ddefe8768d11a8c19fbca220adc937/scraperx-0.0.4rc4.tar.gz", "yanked": false}], "0.0.4rc5": [{"comment_text": "", "digests": {"md5": "a5b72851d2a1fc026a3c788bf583f084", "sha256": "59edc91e4d164ed6be50efb0477a843c2388fc1b0ab74df3bffb807a98f273bc"}, "downloads": -1, "filename": "scraperx-0.0.4rc5.tar.gz", "has_sig": false, "md5_digest": "a5b72851d2a1fc026a3c788bf583f084", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 28627, "upload_time": "2019-06-02T17:23:37", "upload_time_iso_8601": "2019-06-02T17:23:37.853876Z", "url": "https://files.pythonhosted.org/packages/65/fe/08b52c2b9c6a8381d279430e7f2dc32524eb8714fbd63806275a4ce59116/scraperx-0.0.4rc5.tar.gz", "yanked": false}], "0.0.4rc6": [{"comment_text": "", "digests": {"md5": "98c397dd320b54cbcfd0fd4c4fa903b2", "sha256": "25d1434cf3f3825f505702d1a8ebb622f5976ee80df2c60764e0b0f845e42313"}, "downloads": -1, "filename": "scraperx-0.0.4rc6.tar.gz", "has_sig": false, "md5_digest": "98c397dd320b54cbcfd0fd4c4fa903b2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 30334, "upload_time": "2019-06-11T13:04:39", "upload_time_iso_8601": "2019-06-11T13:04:39.907484Z", "url": "https://files.pythonhosted.org/packages/3f/c2/68e0a9fc39e2ff57a8c92ce18b23df92e5d528d6f99bcf9b3cf0d4d2a4ae/scraperx-0.0.4rc6.tar.gz", "yanked": false}], "0.0.4rc7": [{"comment_text": "", "digests": {"md5": "91d7c501a1d1bc3e309784eab0df9192", "sha256": "c401b521b60582fa74b68df0cc172fb880c0a6e1e448d10eaa8b856f844e4e78"}, "downloads": -1, "filename": "scraperx-0.0.4rc7.tar.gz", "has_sig": false, "md5_digest": "91d7c501a1d1bc3e309784eab0df9192", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 30339, "upload_time": "2019-06-11T16:01:50", "upload_time_iso_8601": "2019-06-11T16:01:50.452637Z", "url": "https://files.pythonhosted.org/packages/ac/80/02de4746fd7da60d32f2773e868f8db79f3266dac1bcfd666cab10465540/scraperx-0.0.4rc7.tar.gz", "yanked": false}], "0.0.4rc8": [{"comment_text": "", "digests": {"md5": "8d5010666b9dd6a6ea5a69e8e2f25f50", "sha256": "9418c2732f944bff7afdf68ee0e48b4220749e0914b6b016f84a73175adee9a5"}, "downloads": -1, "filename": "scraperx-0.0.4rc8.tar.gz", "has_sig": false, "md5_digest": "8d5010666b9dd6a6ea5a69e8e2f25f50", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 30356, "upload_time": "2019-06-13T22:32:47", "upload_time_iso_8601": "2019-06-13T22:32:47.352332Z", "url": "https://files.pythonhosted.org/packages/4e/ce/7891e0fd4de3768612bd614118fc299d0e5beab0fc63577439e84ddaa431/scraperx-0.0.4rc8.tar.gz", "yanked": false}], "0.0.4rc9": [{"comment_text": "", "digests": {"md5": "08f98c53336415df825003c0e889764f", "sha256": "05fa71e967d94147c7425f23fd83cf4d74f424d5573f102c1714a7dbaa9c5913"}, "downloads": -1, "filename": "scraperx-0.0.4rc9.tar.gz", "has_sig": false, "md5_digest": "08f98c53336415df825003c0e889764f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 30794, "upload_time": "2019-06-17T19:26:05", "upload_time_iso_8601": "2019-06-17T19:26:05.824168Z", "url": "https://files.pythonhosted.org/packages/9a/de/1b9e56baec68d6263728d56ef45e8be2533d59fcdebd18873c7945bdc184/scraperx-0.0.4rc9.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "eec1316fd1e2a19585b5746e8450ef3b", "sha256": "e0c489ff4869797b628b01dccc8b2a83b1cca90c5657d80bb88ace2d1661d515"}, "downloads": -1, "filename": "scraperx-0.1.0.tar.gz", "has_sig": false, "md5_digest": "eec1316fd1e2a19585b5746e8450ef3b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32259, "upload_time": "2019-07-01T14:37:15", "upload_time_iso_8601": "2019-07-01T14:37:15.763677Z", "url": "https://files.pythonhosted.org/packages/1b/fe/b4211100677372d329402d5eac17bd3cebd5b8541827b2fe1ba5d3089778/scraperx-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "1369fd204bb73051f1d6964066eebe0f", "sha256": "1f03b0b9c5fc418868df4f4fb6c3133798ee2b85ada32231685cc88073d42bf3"}, "downloads": -1, "filename": "scraperx-0.1.1.tar.gz", "has_sig": false, "md5_digest": "1369fd204bb73051f1d6964066eebe0f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32373, "upload_time": "2019-07-04T16:30:04", "upload_time_iso_8601": "2019-07-04T16:30:04.340936Z", "url": "https://files.pythonhosted.org/packages/2a/64/a2e468584c6b906a210511b0ff0d030f4d2a4783fe343f9c2c5649f6e090/scraperx-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "9cf37ee9a4b4ef88b0b09f64379f1861", "sha256": "858b53bacef2858af1bb6c55f880cb38af2d24f6bf25211dbe7cd61c71632b43"}, "downloads": -1, "filename": "scraperx-0.1.2.tar.gz", "has_sig": false, "md5_digest": "9cf37ee9a4b4ef88b0b09f64379f1861", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32344, "upload_time": "2019-07-08T16:03:49", "upload_time_iso_8601": "2019-07-08T16:03:49.649231Z", "url": "https://files.pythonhosted.org/packages/fe/2c/69a98117fa7bac16b85852197221c058349397f92fee81779353d6ca5b24/scraperx-0.1.2.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "fc6696b97138f5c1cffb3ab084a0eea2", "sha256": "c2acbdd223b8f58ee1bd382bf7c160609ead7c5344bf57c32065082a201b4a5a"}, "downloads": -1, "filename": "scraperx-0.2.0.tar.gz", "has_sig": false, "md5_digest": "fc6696b97138f5c1cffb3ab084a0eea2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32344, "upload_time": "2019-07-11T19:32:23", "upload_time_iso_8601": "2019-07-11T19:32:23.734971Z", "url": "https://files.pythonhosted.org/packages/da/03/b4820f70db120adac2f98b52cc213bb9af03a6b002732c8913da7aaa9d39/scraperx-0.2.0.tar.gz", "yanked": false}], "0.2.0rc1": [{"comment_text": "", "digests": {"md5": "8a9e6ddac67c62127fb6d5e60bc14af8", "sha256": "ae41ed4694e44f9071c14d58774b12ee14fe359c4b3d195b84ef02f8a451695d"}, "downloads": -1, "filename": "scraperx-0.2.0rc1.tar.gz", "has_sig": false, "md5_digest": "8a9e6ddac67c62127fb6d5e60bc14af8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32653, "upload_time": "2019-07-07T02:29:57", "upload_time_iso_8601": "2019-07-07T02:29:57.408587Z", "url": "https://files.pythonhosted.org/packages/92/0b/2bc6f8b0ab368370f28b9af9ba0adc3a1547b38a9df4cc31def7823f71a0/scraperx-0.2.0rc1.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "899367ceffac5584521ff05536d80cec", "sha256": "0f64f7a5d46767971ab8b2d643ed982962918081e2eda7016e58c2996c7796f7"}, "downloads": -1, "filename": "scraperx-0.2.1.tar.gz", "has_sig": false, "md5_digest": "899367ceffac5584521ff05536d80cec", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32363, "upload_time": "2019-07-11T19:49:35", "upload_time_iso_8601": "2019-07-11T19:49:35.501166Z", "url": "https://files.pythonhosted.org/packages/02/b5/09fc866af9f6830a8d78ca9495c2949d7977d6a53210596396a8005b6e11/scraperx-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "c337caf341be1409b9c34594dce455f2", "sha256": "4a803fe38bf951f0a3af0a39b35882a1230fba678300bb054ecb6c3cd1f25bc9"}, "downloads": -1, "filename": "scraperx-0.2.2.tar.gz", "has_sig": false, "md5_digest": "c337caf341be1409b9c34594dce455f2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32397, "upload_time": "2019-07-11T20:11:33", "upload_time_iso_8601": "2019-07-11T20:11:33.430375Z", "url": "https://files.pythonhosted.org/packages/14/83/95af505978dc5a5a7f35bbc5f7d15471990d66395a755b084541abcd5061/scraperx-0.2.2.tar.gz", "yanked": false}], "0.2.3": [{"comment_text": "", "digests": {"md5": "fc5bcae30c455d7cf3cc7f382deaa503", "sha256": "f3405b09da2360ea104701ed06b67c3956b26e55f4a97d0710b69ed97b1abd28"}, "downloads": -1, "filename": "scraperx-0.2.3.tar.gz", "has_sig": false, "md5_digest": "fc5bcae30c455d7cf3cc7f382deaa503", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32390, "upload_time": "2019-07-11T20:54:26", "upload_time_iso_8601": "2019-07-11T20:54:26.563742Z", "url": "https://files.pythonhosted.org/packages/a9/9d/12f59b5981d87e7ea636dd3ce5a65f2e281669671aed34e71cca318652bf/scraperx-0.2.3.tar.gz", "yanked": false}], "0.2.4": [{"comment_text": "", "digests": {"md5": "79258a44a2040bbbb7619d87dd078584", "sha256": "2c7cbc97db019bc3b90a295fab0d0436ecb46cb98fa9b87c17ee5714c2108e9c"}, "downloads": -1, "filename": "scraperx-0.2.4.tar.gz", "has_sig": false, "md5_digest": "79258a44a2040bbbb7619d87dd078584", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32415, "upload_time": "2019-07-12T13:22:35", "upload_time_iso_8601": "2019-07-12T13:22:35.863631Z", "url": "https://files.pythonhosted.org/packages/1d/a7/d4cb56552a12c58c035082fa636c3026acfeb5bcfa846cc7b102cc93df35/scraperx-0.2.4.tar.gz", "yanked": false}], "0.2.5": [{"comment_text": "", "digests": {"md5": "20c3ff29b4795ab98765c71be25e89c9", "sha256": "54609aef80b5d6196ebeab26f09858f6411c6c44da3df91b4f00e35d3a2f6a51"}, "downloads": -1, "filename": "scraperx-0.2.5.tar.gz", "has_sig": false, "md5_digest": "20c3ff29b4795ab98765c71be25e89c9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32447, "upload_time": "2019-07-12T16:19:47", "upload_time_iso_8601": "2019-07-12T16:19:47.212845Z", "url": "https://files.pythonhosted.org/packages/b4/a2/3f65d19ce89f85610a7a290b169cc53547614ea190d489872e5e55a4d857/scraperx-0.2.5.tar.gz", "yanked": false}], "0.2.6": [{"comment_text": "", "digests": {"md5": "31e982c991e807da1539032fba93cea6", "sha256": "84b39d7e93aa34788f0b5198655a0f7da95af7b1bc0cdcd104c9b78552956686"}, "downloads": -1, "filename": "scraperx-0.2.6.tar.gz", "has_sig": false, "md5_digest": "31e982c991e807da1539032fba93cea6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32446, "upload_time": "2019-07-12T16:57:08", "upload_time_iso_8601": "2019-07-12T16:57:08.742149Z", "url": "https://files.pythonhosted.org/packages/ab/bd/3e795fc48ee7cca5fabc29ebca161e86fc916a093020b4aaab3dd36c9074/scraperx-0.2.6.tar.gz", "yanked": false}], "0.2.7": [{"comment_text": "", "digests": {"md5": "5a251818c52c057c765ef6cf8253dd57", "sha256": "3d86c657f7128517108e0256c64b2ee5446b50c0b60100eb4bd3c1f511bc4246"}, "downloads": -1, "filename": "scraperx-0.2.7.tar.gz", "has_sig": false, "md5_digest": "5a251818c52c057c765ef6cf8253dd57", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32369, "upload_time": "2019-07-18T13:21:13", "upload_time_iso_8601": "2019-07-18T13:21:13.242115Z", "url": "https://files.pythonhosted.org/packages/67/3f/96f2b25938ae1ab3bea4622a14cc70537f52437137a5739e66bf76f09d6c/scraperx-0.2.7.tar.gz", "yanked": false}], "0.2.8": [{"comment_text": "", "digests": {"md5": "fd879e4ffe7dbb62780eb9e4b7c0f9d2", "sha256": "c8cad812a519d1fce9bb7cd35f6978d2244b4fbbff8e3910810db48041594d3c"}, "downloads": -1, "filename": "scraperx-0.2.8.tar.gz", "has_sig": false, "md5_digest": "fd879e4ffe7dbb62780eb9e4b7c0f9d2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32416, "upload_time": "2019-07-19T15:32:23", "upload_time_iso_8601": "2019-07-19T15:32:23.090795Z", "url": "https://files.pythonhosted.org/packages/1c/81/021f71f5e7b8f476127c6e2b4b1903be1d39b48b9f8638b6fef0bd12127b/scraperx-0.2.8.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "9479df8df7f8f832544d3b5b887b90c0", "sha256": "0e7ba19aea631985efc038750836bfe20f657d5fc7cba8b09f01b8ed81be36f7"}, "downloads": -1, "filename": "scraperx-0.3.0.tar.gz", "has_sig": false, "md5_digest": "9479df8df7f8f832544d3b5b887b90c0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 32446, "upload_time": "2019-07-21T21:05:19", "upload_time_iso_8601": "2019-07-21T21:05:19.841457Z", "url": "https://files.pythonhosted.org/packages/0f/1b/8b6c55977ae77b3bab6e32b1e3d953a3001849699dc53cb92d8c4beb4a53/scraperx-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "f537caaadf7ee1fb50a076d521e57ba7", "sha256": "6de51740968bde4ecea449555fa95ff1e3873d7e2e3a2b3967fb9163f6eb9612"}, "downloads": -1, "filename": "scraperx-0.3.1.tar.gz", "has_sig": false, "md5_digest": "f537caaadf7ee1fb50a076d521e57ba7", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34517, "upload_time": "2019-07-31T19:52:00", "upload_time_iso_8601": "2019-07-31T19:52:00.647678Z", "url": "https://files.pythonhosted.org/packages/b3/79/ab3181a26ba7a5a736bcc9558c22531ba5a37f756d280ef51b53c46cbad1/scraperx-0.3.1.tar.gz", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "3cc4090394d0969c9685f2d16a57b706", "sha256": "28eac23068481b2192a19916d8391f20f782af040c91ae8726644257f576125b"}, "downloads": -1, "filename": "scraperx-0.3.2.tar.gz", "has_sig": false, "md5_digest": "3cc4090394d0969c9685f2d16a57b706", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34608, "upload_time": "2019-08-21T16:02:05", "upload_time_iso_8601": "2019-08-21T16:02:05.287680Z", "url": "https://files.pythonhosted.org/packages/2e/38/decb693a66ab483e3481b8d666294c5ff7bfba0460f3953265f9b96cc3bf/scraperx-0.3.2.tar.gz", "yanked": false}], "0.3.3": [{"comment_text": "", "digests": {"md5": "f24f73412bc2def5be8c082a3901a9bd", "sha256": "cc6d899521ec52c88161ba7186c98940cfa9d14b9ff437bd6dd4d7a854999109"}, "downloads": -1, "filename": "scraperx-0.3.3.tar.gz", "has_sig": false, "md5_digest": "f24f73412bc2def5be8c082a3901a9bd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34405, "upload_time": "2019-09-23T20:05:28", "upload_time_iso_8601": "2019-09-23T20:05:28.453180Z", "url": "https://files.pythonhosted.org/packages/45/6e/8ba858cc8ab44bd5f48526c5731f0cc93643b3f0d3f996702b190bba3deb/scraperx-0.3.3.tar.gz", "yanked": false}], "0.3.4": [{"comment_text": "", "digests": {"md5": "8dcd9ac5a7f05e3e035fa7d897b89546", "sha256": "7dbb02c84024d225cd5cd253e2a7bd2097ed9ecf2b3534a0a200d91d9dd29f20"}, "downloads": -1, "filename": "scraperx-0.3.4.tar.gz", "has_sig": false, "md5_digest": "8dcd9ac5a7f05e3e035fa7d897b89546", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34082, "upload_time": "2019-09-24T17:50:44", "upload_time_iso_8601": "2019-09-24T17:50:44.703375Z", "url": "https://files.pythonhosted.org/packages/03/ba/f311b258916b4fe8ec0897e2648f690b13549cd0d49296833d41a0308ec4/scraperx-0.3.4.tar.gz", "yanked": false}], "0.3.5": [{"comment_text": "", "digests": {"md5": "122e28989adbef5bd5dcfbbfd7e033b2", "sha256": "4faeead10423328acf16217dd7ac27db699447fbce01ec7abf7ac255ebc7b188"}, "downloads": -1, "filename": "scraperx-0.3.5.tar.gz", "has_sig": false, "md5_digest": "122e28989adbef5bd5dcfbbfd7e033b2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34147, "upload_time": "2019-09-26T16:08:21", "upload_time_iso_8601": "2019-09-26T16:08:21.943145Z", "url": "https://files.pythonhosted.org/packages/bb/f0/528064615ffd9df25e79aff9e7f532d3c23e9631863df39b8c5051364875/scraperx-0.3.5.tar.gz", "yanked": false}], "0.3.6": [{"comment_text": "", "digests": {"md5": "f1303f0dbc0ee40d0446cb38c1fe63c0", "sha256": "5548bab7545fe3cebcf2cc5c3f6727d39cc29327a8ab448046c632a9213d75de"}, "downloads": -1, "filename": "scraperx-0.3.6.tar.gz", "has_sig": false, "md5_digest": "f1303f0dbc0ee40d0446cb38c1fe63c0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34173, "upload_time": "2019-10-02T19:19:28", "upload_time_iso_8601": "2019-10-02T19:19:28.303734Z", "url": "https://files.pythonhosted.org/packages/09/a0/416ae2fd28657b248543248fe1a32635c212665c5b5ab3dea96d4f71afea/scraperx-0.3.6.tar.gz", "yanked": false}], "0.3.7": [{"comment_text": "", "digests": {"md5": "b5f0f92df2414662d8605cf6229bcbfd", "sha256": "061b2d4738b9b9d2777f8583a853e184810699a5c530ec88565e9727f3d7c75b"}, "downloads": -1, "filename": "scraperx-0.3.7.tar.gz", "has_sig": false, "md5_digest": "b5f0f92df2414662d8605cf6229bcbfd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34239, "upload_time": "2019-10-02T20:59:15", "upload_time_iso_8601": "2019-10-02T20:59:15.038526Z", "url": "https://files.pythonhosted.org/packages/d3/94/b1a03d1def222d6f74462c99c70bb902c51d341f2dd9457dbcdb32699fda/scraperx-0.3.7.tar.gz", "yanked": false}], "0.3.8": [{"comment_text": "", "digests": {"md5": "b2258427f4d0280e9eb787b9d1bae833", "sha256": "f6f3fb156d35043a9d2c9e07b2d9409b5e17eefd552a998ef8e7a7790f9f7df3"}, "downloads": -1, "filename": "scraperx-0.3.8.tar.gz", "has_sig": false, "md5_digest": "b2258427f4d0280e9eb787b9d1bae833", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34283, "upload_time": "2019-10-16T14:30:44", "upload_time_iso_8601": "2019-10-16T14:30:44.342744Z", "url": "https://files.pythonhosted.org/packages/ff/9f/edecdf68b2a2e774ebd686221a91a7432ac94a1bb289ead346424ee1ce2b/scraperx-0.3.8.tar.gz", "yanked": false}], "0.3.9": [{"comment_text": "", "digests": {"md5": "fbd2c83a67ebcaada2c8cebdf49be696", "sha256": "d2d79c3766f844d733e8219cb692b9a50972ffb6a2a4bb79766eee5bc38f7245"}, "downloads": -1, "filename": "scraperx-0.3.9.tar.gz", "has_sig": false, "md5_digest": "fbd2c83a67ebcaada2c8cebdf49be696", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34447, "upload_time": "2019-10-24T11:30:48", "upload_time_iso_8601": "2019-10-24T11:30:48.114023Z", "url": "https://files.pythonhosted.org/packages/05/95/c65fd55697f66bc879538533c0ea7a6ecae70469ba111036cb8f161f512b/scraperx-0.3.9.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "ce2bc9120dc77af38f5ce807dcdcc65f", "sha256": "eaf94cfe941d6c8bed8227d3b9dcd686aff36be557bb4f64f3dcfdc8af533a21"}, "downloads": -1, "filename": "scraperx-0.4.0.tar.gz", "has_sig": false, "md5_digest": "ce2bc9120dc77af38f5ce807dcdcc65f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34390, "upload_time": "2019-10-24T14:31:06", "upload_time_iso_8601": "2019-10-24T14:31:06.949356Z", "url": "https://files.pythonhosted.org/packages/80/0a/66302cc6d8b814ab1c86fbe0ff755d725fb4fa43bbe9f9dfea9254405eea/scraperx-0.4.0.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "9e5f001a82b4d9baed14992695788247", "sha256": "822718deb59e401cbfb4bd0c4c2d068ffbae1b73edc3316622d49bc5e139120e"}, "downloads": -1, "filename": "scraperx-0.4.1.tar.gz", "has_sig": false, "md5_digest": "9e5f001a82b4d9baed14992695788247", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34376, "upload_time": "2019-10-30T13:56:51", "upload_time_iso_8601": "2019-10-30T13:56:51.203060Z", "url": "https://files.pythonhosted.org/packages/c7/98/2595c3cf05304acf6e01e8524ce21594a6a6ab951bcc3c90d7f0a374d612/scraperx-0.4.1.tar.gz", "yanked": false}], "0.4.2": [{"comment_text": "", "digests": {"md5": "87b7ed653b0b20f7e43e636d59133de6", "sha256": "c2a1b03cb7db24f62b206b8dfdace8ac680c82258a60a059f318e5682f0122d5"}, "downloads": -1, "filename": "scraperx-0.4.2.tar.gz", "has_sig": false, "md5_digest": "87b7ed653b0b20f7e43e636d59133de6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34414, "upload_time": "2019-11-09T20:05:25", "upload_time_iso_8601": "2019-11-09T20:05:25.492689Z", "url": "https://files.pythonhosted.org/packages/0f/a9/266aa4e28452ddc444149e9b424c97f9ab45d77e04869495b0ccaa9aae79/scraperx-0.4.2.tar.gz", "yanked": false}], "0.4.3": [{"comment_text": "", "digests": {"md5": "6103d575079d5e73b4f8f9d55794d94e", "sha256": "59cc452ad57ce2dc81e6cffa580760d75330a011302a22563b2b9ebb3d74cec0"}, "downloads": -1, "filename": "scraperx-0.4.3.tar.gz", "has_sig": false, "md5_digest": "6103d575079d5e73b4f8f9d55794d94e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34435, "upload_time": "2020-02-17T02:04:54", "upload_time_iso_8601": "2020-02-17T02:04:54.482805Z", "url": "https://files.pythonhosted.org/packages/ce/06/25cbab45c8acbc250f7fb5cf29f03fcd429e14115e0871c571faffd03764/scraperx-0.4.3.tar.gz", "yanked": false}], "0.4.4": [{"comment_text": "", "digests": {"md5": "7e57c72f93cb126b89c845b90dd56510", "sha256": "dfb18d5ba2b3bd28c417c597ad4c15dbe480a1f4aef2c320c21579f54a945a69"}, "downloads": -1, "filename": "scraperx-0.4.4.tar.gz", "has_sig": false, "md5_digest": "7e57c72f93cb126b89c845b90dd56510", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34492, "upload_time": "2020-03-24T22:52:37", "upload_time_iso_8601": "2020-03-24T22:52:37.090079Z", "url": "https://files.pythonhosted.org/packages/64/c0/a015c725144312c545db9a90187bda32b79c1c84b405df6e62944aa9b0a4/scraperx-0.4.4.tar.gz", "yanked": false}], "0.4.5": [{"comment_text": "", "digests": {"md5": "ba41260c4638ceceaa021eb7b1bcfc4d", "sha256": "bf49acee85fcca8aa8176eb91b4c8cc99677965f78d6b89fd6bb5ed82515617e"}, "downloads": -1, "filename": "scraperx-0.4.5.tar.gz", "has_sig": false, "md5_digest": "ba41260c4638ceceaa021eb7b1bcfc4d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34513, "upload_time": "2020-03-26T21:27:41", "upload_time_iso_8601": "2020-03-26T21:27:41.004658Z", "url": "https://files.pythonhosted.org/packages/df/34/3175ba1cb36a2f46f31329d481eb460321763f5e5ae447e19fa49f5ea847/scraperx-0.4.5.tar.gz", "yanked": false}], "0.4.6": [{"comment_text": "", "digests": {"md5": "9e543160243aa8e37cdcf35f2386434a", "sha256": "8668dc7c60057514db29d0b50e7e83fc9cf39502ddb6d468a8c15640f1028fdc"}, "downloads": -1, "filename": "scraperx-0.4.6.tar.gz", "has_sig": false, "md5_digest": "9e543160243aa8e37cdcf35f2386434a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34542, "upload_time": "2020-04-25T19:29:18", "upload_time_iso_8601": "2020-04-25T19:29:18.416897Z", "url": "https://files.pythonhosted.org/packages/fd/3e/72b2a1dbd404343b0236f3026b5b662a7c6c7bfbd88ed848671bb0c4f956/scraperx-0.4.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "9e543160243aa8e37cdcf35f2386434a", "sha256": "8668dc7c60057514db29d0b50e7e83fc9cf39502ddb6d468a8c15640f1028fdc"}, "downloads": -1, "filename": "scraperx-0.4.6.tar.gz", "has_sig": false, "md5_digest": "9e543160243aa8e37cdcf35f2386434a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 34542, "upload_time": "2020-04-25T19:29:18", "upload_time_iso_8601": "2020-04-25T19:29:18.416897Z", "url": "https://files.pythonhosted.org/packages/fd/3e/72b2a1dbd404343b0236f3026b5b662a7c6c7bfbd88ed848671bb0c4f956/scraperx-0.4.6.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:53 2020"}