{"info": {"author": "Thomas Breuel", "author_email": "tmbdev+removeme@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "License :: OSI Approved :: BSD License", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "# Tensorcom\n\nTensorcom is a way of loading training data into deep learning frameworks\nquickly and portably. You can write a single data loading/augmentation\npipeline and train one or more jobs in the same or different frameworks\nwith it.\n\nBoth Keras and PyTorch can use the Python `Connection` object for input,\nbut MessagePack and ZMQ libraries exist in all major languages, making\nit easy to write servers and input operators for any framework.\n\nTensorcom replaces the use of `multiprocessing` in Python for that purpose.\nBoth use separate processes for loading and augmentation, but by making\nthe processes and communications explicit, you gain some significant advantages:\n\n - the same augmentation pipeline can be used with different DL frameworks\n - augmentation processes can easily be run on multiple machines\n - output from a single automentation pipeline can be shared by many training jobs\n - you can start up and test the augmentation pipeline before you start the Dl jobs\n - DL frameworks wanting to use `tensorcom` only need a small library to handle input\n\nUsing `tensorcom` for training is very simple. First, start up a data server;\nfor Imagenet, there are two example jobs. The `serve-imagenet-dir` program\nillustrates how to use the standard PyTorch Imagenet `DataLoader` to serve\ntraining data:\n\n```\n    $ serve-imagenet-dir -d /data/imagenet -b 64 zpub://127.0.0.1:7880\n```\n\nThe server will give you information about the rate at which it serves image batches.\nYour training loop then becomes very simple:\n\n```\n    training = tensorcom.Connection(\"zsub://127.0.0.1:7880\", epoch=1000000)\n    for xs, ys in training:\n        train_batch(xs, ys)\n```\n\nIf you want multiple jobs for augmentation, just use more publishers using\nBash-style brace notation: `zpub://127.0.0.1:788{0..3}` and `zsub://127.0.0.1:788{0..3}`.\n\nNote that you can start up multiple training jobs connecting to the same server.\n\n# Command Line Tools\n\nThere are some command line programs to help with developing and debugging these\njobs:\n\n - tensormon -- connect to a data server and monitor throughput\n - tensorshow -- show images from input batches\n - tensorstat -- compute statistics over input data samples\n\n# Examples\n\n - serve-imagenet-dir -- serve Imagenet data from a file system using PyTorch\n - serve-imagenet-shards -- serve Imagenet from shards using `webloader`\n - keras.ipynb -- simple example of using Keras with tensorcom\n - pytorch.ipynb -- simple example of using PyTorch with tensorcom\n\n# ZMQ URLs\n\nThere is no official standard for ZMQ URLs. This library uses the following notation:\n\nSocket types:\n\n - zpush / zpull -- standard PUSH/PULL sockets\n - zrpush / zrpull -- reverse PUSH/PULL connections (PUSH socket is server / PULL socket connects)\n - zpub / zsub -- standard PUB/SUB sockets\n - zrpub / zrsub -- reverse PUB/SUB connections\n\n The pub/sub servers allow the same augmentation pipeline to be shared by multiple\n learning jobs.\n\n Default transport is TCP/IP, but you can choose IPC as in `zpush+ipc://mypath`.\n\n# Connection Objects\n\nThe major way of interacting with the library is through the `Connection` object.\nIt simply gives you an iterator over training samples.\n\n# Encodings\n\nData is encoded in a simple binary tensor format; see `codec.py` for details.\nThe same format can also be used for saving and loading lists of\ntensors from disk (extension: `.ten`).\nData is encoded on 64 byte aligned boundaries to allow easy memory\nmapping and direct use by CPUs and GPUs.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/tmbdev/tensorcom", "keywords": "object store,client,deep learning", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "tensorcom", "package_url": "https://pypi.org/project/tensorcom/", "platform": "", "project_url": "https://pypi.org/project/tensorcom/", "project_urls": {"Homepage": "http://github.com/tmbdev/tensorcom"}, "release_url": "https://pypi.org/project/tensorcom/0.1.0/", "requires_dist": ["msgpack", "pyzmq", "torch", "webdataset"], "requires_python": ">=3.6", "summary": "Distributed preprocessing for deep learning.", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Tensorcom</h1>\n<p>Tensorcom is a way of loading training data into deep learning frameworks\nquickly and portably. You can write a single data loading/augmentation\npipeline and train one or more jobs in the same or different frameworks\nwith it.</p>\n<p>Both Keras and PyTorch can use the Python <code>Connection</code> object for input,\nbut MessagePack and ZMQ libraries exist in all major languages, making\nit easy to write servers and input operators for any framework.</p>\n<p>Tensorcom replaces the use of <code>multiprocessing</code> in Python for that purpose.\nBoth use separate processes for loading and augmentation, but by making\nthe processes and communications explicit, you gain some significant advantages:</p>\n<ul>\n<li>the same augmentation pipeline can be used with different DL frameworks</li>\n<li>augmentation processes can easily be run on multiple machines</li>\n<li>output from a single automentation pipeline can be shared by many training jobs</li>\n<li>you can start up and test the augmentation pipeline before you start the Dl jobs</li>\n<li>DL frameworks wanting to use <code>tensorcom</code> only need a small library to handle input</li>\n</ul>\n<p>Using <code>tensorcom</code> for training is very simple. First, start up a data server;\nfor Imagenet, there are two example jobs. The <code>serve-imagenet-dir</code> program\nillustrates how to use the standard PyTorch Imagenet <code>DataLoader</code> to serve\ntraining data:</p>\n<pre><code>    $ serve-imagenet-dir -d /data/imagenet -b 64 zpub://127.0.0.1:7880\n</code></pre>\n<p>The server will give you information about the rate at which it serves image batches.\nYour training loop then becomes very simple:</p>\n<pre><code>    training = tensorcom.Connection(\"zsub://127.0.0.1:7880\", epoch=1000000)\n    for xs, ys in training:\n        train_batch(xs, ys)\n</code></pre>\n<p>If you want multiple jobs for augmentation, just use more publishers using\nBash-style brace notation: <code>zpub://127.0.0.1:788{0..3}</code> and <code>zsub://127.0.0.1:788{0..3}</code>.</p>\n<p>Note that you can start up multiple training jobs connecting to the same server.</p>\n<h1>Command Line Tools</h1>\n<p>There are some command line programs to help with developing and debugging these\njobs:</p>\n<ul>\n<li>tensormon -- connect to a data server and monitor throughput</li>\n<li>tensorshow -- show images from input batches</li>\n<li>tensorstat -- compute statistics over input data samples</li>\n</ul>\n<h1>Examples</h1>\n<ul>\n<li>serve-imagenet-dir -- serve Imagenet data from a file system using PyTorch</li>\n<li>serve-imagenet-shards -- serve Imagenet from shards using <code>webloader</code></li>\n<li>keras.ipynb -- simple example of using Keras with tensorcom</li>\n<li>pytorch.ipynb -- simple example of using PyTorch with tensorcom</li>\n</ul>\n<h1>ZMQ URLs</h1>\n<p>There is no official standard for ZMQ URLs. This library uses the following notation:</p>\n<p>Socket types:</p>\n<ul>\n<li>zpush / zpull -- standard PUSH/PULL sockets</li>\n<li>zrpush / zrpull -- reverse PUSH/PULL connections (PUSH socket is server / PULL socket connects)</li>\n<li>zpub / zsub -- standard PUB/SUB sockets</li>\n<li>zrpub / zrsub -- reverse PUB/SUB connections</li>\n</ul>\n<p>The pub/sub servers allow the same augmentation pipeline to be shared by multiple\nlearning jobs.</p>\n<p>Default transport is TCP/IP, but you can choose IPC as in <code>zpush+ipc://mypath</code>.</p>\n<h1>Connection Objects</h1>\n<p>The major way of interacting with the library is through the <code>Connection</code> object.\nIt simply gives you an iterator over training samples.</p>\n<h1>Encodings</h1>\n<p>Data is encoded in a simple binary tensor format; see <code>codec.py</code> for details.\nThe same format can also be used for saving and loading lists of\ntensors from disk (extension: <code>.ten</code>).\nData is encoded on 64 byte aligned boundaries to allow easy memory\nmapping and direct use by CPUs and GPUs.</p>\n\n          </div>"}, "last_serial": 6686070, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "58a36eeb8188b439f2788c6aff9d4ce6", "sha256": "0a94d03fea46c52808b23d18abe77ce1653241b0ec31a81dd2526fcf26431b2b"}, "downloads": -1, "filename": "tensorcom-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "58a36eeb8188b439f2788c6aff9d4ce6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 16243, "upload_time": "2020-02-24T02:14:59", "upload_time_iso_8601": "2020-02-24T02:14:59.023559Z", "url": "https://files.pythonhosted.org/packages/7a/29/3cf37ad769c2b3dcb1015417114788e7c3f27499aa2ad2a2ee748c4269d7/tensorcom-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5574319703f92d2865d1adaabb4278c0", "sha256": "62046fff717f81448eaefb44498bd87ed8b2aa7eeac1b25810d838586bc635b4"}, "downloads": -1, "filename": "tensorcom-0.1.0.tar.gz", "has_sig": false, "md5_digest": "5574319703f92d2865d1adaabb4278c0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 14367, "upload_time": "2020-02-24T02:15:01", "upload_time_iso_8601": "2020-02-24T02:15:01.299372Z", "url": "https://files.pythonhosted.org/packages/a5/6c/6abff6ed384567956930d3f75bf1b9755714c59b3792c57971e91fb4e4d7/tensorcom-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "58a36eeb8188b439f2788c6aff9d4ce6", "sha256": "0a94d03fea46c52808b23d18abe77ce1653241b0ec31a81dd2526fcf26431b2b"}, "downloads": -1, "filename": "tensorcom-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "58a36eeb8188b439f2788c6aff9d4ce6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 16243, "upload_time": "2020-02-24T02:14:59", "upload_time_iso_8601": "2020-02-24T02:14:59.023559Z", "url": "https://files.pythonhosted.org/packages/7a/29/3cf37ad769c2b3dcb1015417114788e7c3f27499aa2ad2a2ee748c4269d7/tensorcom-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5574319703f92d2865d1adaabb4278c0", "sha256": "62046fff717f81448eaefb44498bd87ed8b2aa7eeac1b25810d838586bc635b4"}, "downloads": -1, "filename": "tensorcom-0.1.0.tar.gz", "has_sig": false, "md5_digest": "5574319703f92d2865d1adaabb4278c0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 14367, "upload_time": "2020-02-24T02:15:01", "upload_time_iso_8601": "2020-02-24T02:15:01.299372Z", "url": "https://files.pythonhosted.org/packages/a5/6c/6abff6ed384567956930d3f75bf1b9755714c59b3792c57971e91fb4e4d7/tensorcom-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:27 2020"}