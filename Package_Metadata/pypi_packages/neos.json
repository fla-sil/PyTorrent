{"info": {"author": "Nathan Simpson", "author_email": "nathan.simpson@hep.lu.se", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "# neos\n> nice end-to-end optimized statistics ;)\n\n\n<img src=\"neos_logo.png\" alt=\"logo\" width=\"200\" align=\"middle\">\n\n![](training.gif)\n\n## Install\n`pip install neos`\n\n## How to use (and reproduce the results from the cool animation)\n\n```python\nimport jax\nimport neos.makers as makers\nimport neos.cls as cls\nimport numpy as np\nimport jax.experimental.stax as stax\nimport jax.experimental.optimizers as optimizers\nimport jax.random\nimport time\n```\n\n### Initialise network using `jax.experimental.stax`\n\n```python\ninit_random_params, predict = stax.serial(\n    stax.Dense(1024),\n    stax.Relu,\n    stax.Dense(1024),\n    stax.Relu,\n    stax.Dense(2),\n    stax.Softmax,\n)\n```\n\n### Initialse tools from `neos`:\n\nThe way we initialise in `neos` is to define functions that make a statistical model from histograms, which in turn are themselves made from a predictive model, such as a neural network. Here's some detail on the unctions used below:\n\n- `hists_from_nn_three_blobs(predict)` uses the nn decision function `predict` defined in the cell above to form histograms from signal and background data, all drawn from multivariate normal distributions with different means. Two background distributions are sampled from, which is meant to mimic the situation in particle physics where one has a 'nominal' prediction for a nuisance parameter and then an alternate value (e.g. from varying up/down by one standard deviation), which then modifies the background pdf. Here, we take that effect to be a shift of the mean of the distribution. The value for the background histogram is then the mean of the resulting counts of the two modes, and the uncertainty can be quantified through the count standard deviation.\n- `nn_hepdata_like(hmaker)` uses `hmaker` to construct histograms, then feeds them into the `neos.models.hepdata_like` function that constructs a pyhf-like model. This can then be used to call things like `logpdf` and `expected_data` downstream.\n- `cls_maker` takes a model-making function as it's primary argument, which is fed into functions from `neos.fit` that minimise the `logpdf` of the model in both a constrained (fixed parameter of interest) and a global way. Moreover, these fits are wrapped in a function that allows us to calculate gradients through the fits using *fixed-point differentiation*. This allows for the calculation of both the profile likelihood and its gradient, and then the same for cls :)\n\nAll three of these methods return functions. in particular, `cls_maker` returns a function that differentiably calculates cls values, which is our desired objective to minimise.\n\n```python\nhmaker = makers.hists_from_nn_three_blobs(predict)\nnnm = makers.nn_hepdata_like(hmaker)\nloss = cls.cls_maker(nnm, solver_kwargs=dict(pdf_transform=True))\n```\n\n```python\n_, network = init_random_params(jax.random.PRNGKey(2), (-1, 2))\n```\n\n    /home/phinate/envs/neos/lib/python3.7/site-packages/jax-0.1.59-py3.7.egg/jax/lib/xla_bridge.py:122: UserWarning: No GPU/TPU found, falling back to CPU.\n\n\n### Define training loop!\n\n```python\nopt_init, opt_update, opt_params = optimizers.adam(1e-3)\n\ndef update_and_value(i, opt_state, mu):\n    net = opt_params(opt_state)\n    value, grad = jax.value_and_grad(loss)(net, mu)\n    return opt_update(i, grad, opt_state), value, net\n\ndef train_network(N):\n    cls_vals = []\n    _, network = init_random_params(jax.random.PRNGKey(1), (-1, 2))\n    state = opt_init(network)\n    losses = []\n\n    for i in range(N):\n        start_time = time.time()\n        state, value, network = update_and_value(i,state,1.0)\n        epoch_time = time.time() - start_time\n        losses.append(value)\n        metrics = {\"loss\": losses}\n        yield network, metrics, epoch_time\n```\n\n### Let's run it!!\n\n```python\nmaxN = 20 # make me bigger for better results!\n\n# Training\nfor i, (network, metrics, epoch_time) in enumerate(train_network(maxN)):\n    print(f\"epoch {i}:\", f'CLs = {metrics[\"loss\"][-1]}, took {epoch_time}s') \n```\n\n    epoch 0: CLs = 0.06680655092981347, took 5.355436325073242s\n    epoch 1: CLs = 0.4853891149072429, took 1.5733795166015625s\n    epoch 2: CLs = 0.3379355596004474, took 1.5171947479248047s\n    epoch 3: CLs = 0.1821927415636535, took 1.5081253051757812s\n    epoch 4: CLs = 0.09119136931683047, took 1.5193650722503662s\n    epoch 5: CLs = 0.04530559823843272, took 1.5008423328399658s\n    epoch 6: CLs = 0.022572851867672883, took 1.499192476272583s\n    epoch 7: CLs = 0.013835564056077887, took 1.5843737125396729s\n    epoch 8: CLs = 0.01322058601444187, took 1.520324468612671s\n    epoch 9: CLs = 0.013407422454837725, took 1.5050244331359863s\n    epoch 10: CLs = 0.011836452218993765, took 1.509469985961914s\n    epoch 11: CLs = 0.00948507486266359, took 1.5089364051818848s\n    epoch 12: CLs = 0.007350505632595539, took 1.5106918811798096s\n    epoch 13: CLs = 0.005755974539907838, took 1.5267891883850098s\n    epoch 14: CLs = 0.0046464301411786035, took 1.5851080417633057s\n    epoch 15: CLs = 0.0038756402968267434, took 1.8452086448669434s\n    epoch 16: CLs = 0.003323640670405803, took 1.9116990566253662s\n    epoch 17: CLs = 0.0029133909840759475, took 1.7648999691009521s\n    epoch 18: CLs = 0.002596946123608612, took 1.6314191818237305s\n    epoch 19: CLs = 0.0023454051342963744, took 1.5911424160003662s\n\n\nAnd there we go!! We discovered a new signal (depending on your arbitrary thershold) ;)\n\nIf you want to reproduce the full animation, a version of this code with plotting helpers can be found in `demo_training.ipynb`! :D\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/phinate/neos", "keywords": "inference machine learning particle physics", "license": "Apache Software License 2.0", "maintainer": "", "maintainer_email": "", "name": "neos", "package_url": "https://pypi.org/project/neos/", "platform": "", "project_url": "https://pypi.org/project/neos/", "project_urls": {"Homepage": "https://github.com/phinate/neos"}, "release_url": "https://pypi.org/project/neos/0.0.1/", "requires_dist": ["jax", "jaxlib", "matplotlib", "numpy", "pyhf", "celluloid"], "requires_python": ">=3.6", "summary": "Library for optimally representing data with neural networks with respect to statistical inference in a particle physics context.", "version": "0.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>neos</h1>\n<blockquote>\n<p>nice end-to-end optimized statistics ;)</p>\n</blockquote>\n<img align=\"middle\" alt=\"logo\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a38651ab7af237d96ca91427322e6ace5f636402/6e656f735f6c6f676f2e706e67\" width=\"200\">\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4c5365d098264f6e3297f43a15c30d7476609554/747261696e696e672e676966\"></p>\n<h2>Install</h2>\n<p><code>pip install neos</code></p>\n<h2>How to use (and reproduce the results from the cool animation)</h2>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">jax</span>\n<span class=\"kn\">import</span> <span class=\"nn\">neos.makers</span> <span class=\"k\">as</span> <span class=\"nn\">makers</span>\n<span class=\"kn\">import</span> <span class=\"nn\">neos.cls</span> <span class=\"k\">as</span> <span class=\"nn\">cls</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">jax.experimental.stax</span> <span class=\"k\">as</span> <span class=\"nn\">stax</span>\n<span class=\"kn\">import</span> <span class=\"nn\">jax.experimental.optimizers</span> <span class=\"k\">as</span> <span class=\"nn\">optimizers</span>\n<span class=\"kn\">import</span> <span class=\"nn\">jax.random</span>\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n</pre>\n<h3>Initialise network using <code>jax.experimental.stax</code></h3>\n<pre><span class=\"n\">init_random_params</span><span class=\"p\">,</span> <span class=\"n\">predict</span> <span class=\"o\">=</span> <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">serial</span><span class=\"p\">(</span>\n    <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1024</span><span class=\"p\">),</span>\n    <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Relu</span><span class=\"p\">,</span>\n    <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1024</span><span class=\"p\">),</span>\n    <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Relu</span><span class=\"p\">,</span>\n    <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n    <span class=\"n\">stax</span><span class=\"o\">.</span><span class=\"n\">Softmax</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</pre>\n<h3>Initialse tools from <code>neos</code>:</h3>\n<p>The way we initialise in <code>neos</code> is to define functions that make a statistical model from histograms, which in turn are themselves made from a predictive model, such as a neural network. Here's some detail on the unctions used below:</p>\n<ul>\n<li><code>hists_from_nn_three_blobs(predict)</code> uses the nn decision function <code>predict</code> defined in the cell above to form histograms from signal and background data, all drawn from multivariate normal distributions with different means. Two background distributions are sampled from, which is meant to mimic the situation in particle physics where one has a 'nominal' prediction for a nuisance parameter and then an alternate value (e.g. from varying up/down by one standard deviation), which then modifies the background pdf. Here, we take that effect to be a shift of the mean of the distribution. The value for the background histogram is then the mean of the resulting counts of the two modes, and the uncertainty can be quantified through the count standard deviation.</li>\n<li><code>nn_hepdata_like(hmaker)</code> uses <code>hmaker</code> to construct histograms, then feeds them into the <code>neos.models.hepdata_like</code> function that constructs a pyhf-like model. This can then be used to call things like <code>logpdf</code> and <code>expected_data</code> downstream.</li>\n<li><code>cls_maker</code> takes a model-making function as it's primary argument, which is fed into functions from <code>neos.fit</code> that minimise the <code>logpdf</code> of the model in both a constrained (fixed parameter of interest) and a global way. Moreover, these fits are wrapped in a function that allows us to calculate gradients through the fits using <em>fixed-point differentiation</em>. This allows for the calculation of both the profile likelihood and its gradient, and then the same for cls :)</li>\n</ul>\n<p>All three of these methods return functions. in particular, <code>cls_maker</code> returns a function that differentiably calculates cls values, which is our desired objective to minimise.</p>\n<pre><span class=\"n\">hmaker</span> <span class=\"o\">=</span> <span class=\"n\">makers</span><span class=\"o\">.</span><span class=\"n\">hists_from_nn_three_blobs</span><span class=\"p\">(</span><span class=\"n\">predict</span><span class=\"p\">)</span>\n<span class=\"n\">nnm</span> <span class=\"o\">=</span> <span class=\"n\">makers</span><span class=\"o\">.</span><span class=\"n\">nn_hepdata_like</span><span class=\"p\">(</span><span class=\"n\">hmaker</span><span class=\"p\">)</span>\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"bp\">cls</span><span class=\"o\">.</span><span class=\"n\">cls_maker</span><span class=\"p\">(</span><span class=\"n\">nnm</span><span class=\"p\">,</span> <span class=\"n\">solver_kwargs</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">pdf_transform</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">))</span>\n</pre>\n<pre><span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">network</span> <span class=\"o\">=</span> <span class=\"n\">init_random_params</span><span class=\"p\">(</span><span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">PRNGKey</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n</pre>\n<pre><code>/home/phinate/envs/neos/lib/python3.7/site-packages/jax-0.1.59-py3.7.egg/jax/lib/xla_bridge.py:122: UserWarning: No GPU/TPU found, falling back to CPU.\n</code></pre>\n<h3>Define training loop!</h3>\n<pre><span class=\"n\">opt_init</span><span class=\"p\">,</span> <span class=\"n\">opt_update</span><span class=\"p\">,</span> <span class=\"n\">opt_params</span> <span class=\"o\">=</span> <span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">adam</span><span class=\"p\">(</span><span class=\"mf\">1e-3</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">update_and_value</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">opt_state</span><span class=\"p\">,</span> <span class=\"n\">mu</span><span class=\"p\">):</span>\n    <span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">opt_params</span><span class=\"p\">(</span><span class=\"n\">opt_state</span><span class=\"p\">)</span>\n    <span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">value_and_grad</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"p\">)(</span><span class=\"n\">net</span><span class=\"p\">,</span> <span class=\"n\">mu</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">opt_update</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">,</span> <span class=\"n\">opt_state</span><span class=\"p\">),</span> <span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"n\">net</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">train_network</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">):</span>\n    <span class=\"n\">cls_vals</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">network</span> <span class=\"o\">=</span> <span class=\"n\">init_random_params</span><span class=\"p\">(</span><span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">PRNGKey</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n    <span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"n\">opt_init</span><span class=\"p\">(</span><span class=\"n\">network</span><span class=\"p\">)</span>\n    <span class=\"n\">losses</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">):</span>\n        <span class=\"n\">start_time</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n        <span class=\"n\">state</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"n\">network</span> <span class=\"o\">=</span> <span class=\"n\">update_and_value</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"n\">state</span><span class=\"p\">,</span><span class=\"mf\">1.0</span><span class=\"p\">)</span>\n        <span class=\"n\">epoch_time</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">start_time</span>\n        <span class=\"n\">losses</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">)</span>\n        <span class=\"n\">metrics</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"loss\"</span><span class=\"p\">:</span> <span class=\"n\">losses</span><span class=\"p\">}</span>\n        <span class=\"k\">yield</span> <span class=\"n\">network</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"p\">,</span> <span class=\"n\">epoch_time</span>\n</pre>\n<h3>Let's run it!!</h3>\n<pre><span class=\"n\">maxN</span> <span class=\"o\">=</span> <span class=\"mi\">20</span> <span class=\"c1\"># make me bigger for better results!</span>\n\n<span class=\"c1\"># Training</span>\n<span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">network</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"p\">,</span> <span class=\"n\">epoch_time</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">train_network</span><span class=\"p\">(</span><span class=\"n\">maxN</span><span class=\"p\">)):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"epoch </span><span class=\"si\">{</span><span class=\"n\">i</span><span class=\"si\">}</span><span class=\"s2\">:\"</span><span class=\"p\">,</span> <span class=\"sa\">f</span><span class=\"s1\">'CLs = </span><span class=\"si\">{</span><span class=\"n\">metrics</span><span class=\"p\">[</span><span class=\"s2\">\"loss\"</span><span class=\"p\">][</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"si\">}</span><span class=\"s1\">, took </span><span class=\"si\">{</span><span class=\"n\">epoch_time</span><span class=\"si\">}</span><span class=\"s1\">s'</span><span class=\"p\">)</span> \n</pre>\n<pre><code>epoch 0: CLs = 0.06680655092981347, took 5.355436325073242s\nepoch 1: CLs = 0.4853891149072429, took 1.5733795166015625s\nepoch 2: CLs = 0.3379355596004474, took 1.5171947479248047s\nepoch 3: CLs = 0.1821927415636535, took 1.5081253051757812s\nepoch 4: CLs = 0.09119136931683047, took 1.5193650722503662s\nepoch 5: CLs = 0.04530559823843272, took 1.5008423328399658s\nepoch 6: CLs = 0.022572851867672883, took 1.499192476272583s\nepoch 7: CLs = 0.013835564056077887, took 1.5843737125396729s\nepoch 8: CLs = 0.01322058601444187, took 1.520324468612671s\nepoch 9: CLs = 0.013407422454837725, took 1.5050244331359863s\nepoch 10: CLs = 0.011836452218993765, took 1.509469985961914s\nepoch 11: CLs = 0.00948507486266359, took 1.5089364051818848s\nepoch 12: CLs = 0.007350505632595539, took 1.5106918811798096s\nepoch 13: CLs = 0.005755974539907838, took 1.5267891883850098s\nepoch 14: CLs = 0.0046464301411786035, took 1.5851080417633057s\nepoch 15: CLs = 0.0038756402968267434, took 1.8452086448669434s\nepoch 16: CLs = 0.003323640670405803, took 1.9116990566253662s\nepoch 17: CLs = 0.0029133909840759475, took 1.7648999691009521s\nepoch 18: CLs = 0.002596946123608612, took 1.6314191818237305s\nepoch 19: CLs = 0.0023454051342963744, took 1.5911424160003662s\n</code></pre>\n<p>And there we go!! We discovered a new signal (depending on your arbitrary thershold) ;)</p>\n<p>If you want to reproduce the full animation, a version of this code with plotting helpers can be found in <code>demo_training.ipynb</code>! :D</p>\n\n          </div>"}, "last_serial": 6754380, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "c228b478628f07b2497a3dfd98b35d3a", "sha256": "a6128efa36b0b49088e800a6d1ae4a8db691ea57c827a29ba83d7bcb94cce279"}, "downloads": -1, "filename": "neos-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "c228b478628f07b2497a3dfd98b35d3a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18055, "upload_time": "2020-03-05T10:16:28", "upload_time_iso_8601": "2020-03-05T10:16:28.416856Z", "url": "https://files.pythonhosted.org/packages/d3/c6/33de4405dd0e85e747d54bc2b16a23d64b0a028bca1a9589c10443219a8f/neos-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ee859b84bb1172261d1979194659c2e2", "sha256": "9e07ce206a17220334a56dd7acc84063a9c9ee6d47ae7ee50a3c251262e58421"}, "downloads": -1, "filename": "neos-0.0.1.tar.gz", "has_sig": false, "md5_digest": "ee859b84bb1172261d1979194659c2e2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 14614, "upload_time": "2020-03-05T10:16:31", "upload_time_iso_8601": "2020-03-05T10:16:31.550087Z", "url": "https://files.pythonhosted.org/packages/63/f0/3654153f964c34843398139be22635e608320e8b7a5041a96912d5c8dc20/neos-0.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c228b478628f07b2497a3dfd98b35d3a", "sha256": "a6128efa36b0b49088e800a6d1ae4a8db691ea57c827a29ba83d7bcb94cce279"}, "downloads": -1, "filename": "neos-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "c228b478628f07b2497a3dfd98b35d3a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18055, "upload_time": "2020-03-05T10:16:28", "upload_time_iso_8601": "2020-03-05T10:16:28.416856Z", "url": "https://files.pythonhosted.org/packages/d3/c6/33de4405dd0e85e747d54bc2b16a23d64b0a028bca1a9589c10443219a8f/neos-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ee859b84bb1172261d1979194659c2e2", "sha256": "9e07ce206a17220334a56dd7acc84063a9c9ee6d47ae7ee50a3c251262e58421"}, "downloads": -1, "filename": "neos-0.0.1.tar.gz", "has_sig": false, "md5_digest": "ee859b84bb1172261d1979194659c2e2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 14614, "upload_time": "2020-03-05T10:16:31", "upload_time_iso_8601": "2020-03-05T10:16:31.550087Z", "url": "https://files.pythonhosted.org/packages/63/f0/3654153f964c34843398139be22635e608320e8b7a5041a96912d5c8dc20/neos-0.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:46:28 2020"}