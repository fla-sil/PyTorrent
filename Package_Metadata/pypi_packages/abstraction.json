{"info": {"author": "Will Breaden Madden", "author_email": "wbm@protonmail.ch", "bugtrack_url": null, "classifiers": [], "description": "|project abstraction|\n\nNOTE\n====\n\nPlease note that abstraction is a *project*, not a finished product.\n\nsetup\n=====\n\nThe following Bash commands, that have been tested on Ubuntu 15.10,\nshould install prerequisites and check out abstraction.\n\n.. code:: bash\n\n    # Install ROOT.\n    sudo apt-get -y install festival\n    sudo apt-get -y install pylint\n    sudo apt-get -y install snakefood\n    sudo apt-get -y install sqlite\n    sudo apt-get -y install python-nltk\n    sudo python -m nltk.downloader all\n    sudo easy_install -U gensim\n    sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n    sudo pip install git+git://github.com/google/skflow.git\n    sudo pip install abstraction\n    git clone https://github.com/wdbm/abstraction.git\n\nThe function ``abstraction.setup()`` should be run.\n\nupcoming\n========\n\nUnder consideration is a requirement for arcodex to ensure the existence\nof a response to an utterance before saving to database.\n\nlogging\n=======\n\nUpdating logging procedures is under consideration because of possible\nlogging conflicts. It could be beneficial currently to run using Bash\nanonymous pipes, in a way like the following:\n\n.. code:: bash\n\n    python script.py 2> >(grep -E -v \"INFO|DEBUG\")\n\ndata\n====\n\nfeature scaling\n---------------\n\nStandardization of datasets is a common requirement for many machine\nlearning estimators implemented in the scikit; they might behave badly\nif the individual features do not more or less look like standard\nnormally-distributed data: Gaussian with zero mean and unit variance --\noften called a standard scores. Many machine learning algorithms assume\nthat all features are centered around zero and have variance of the same\norder. A feature with a variance that is orders of magnitude larger that\nothers might dominate the objective function and make the estimator\nunable to learn from other features. The scikit function ``scale``\nprovides a quick way to perform this operation on a single array-like\ndataset.\n\nSUSY Data Set\n-------------\n\n-  https://archive.ics.uci.edu/ml/datasets/SUSY\n-  http://arxiv.org/abs/1402.4735\n\nThe SUSY Data Set is a classification problem to distinguish between a\nsignal process which produces supersymmetric particles and a background\nprocess which does not. In the data, the first column is the class label\n(1 for signal, 0 for background), followed by 18 features (8 low-level\nfeatures and 10 high-level features):\n\n-  lepton 1 pT\n-  lepton 1 eta\n-  lepton 1 phi\n-  lepton 2 pT\n-  lepton 2 eta\n-  lepton 2 phi\n-  missing energy magnitude\n-  missing energy phi\n-  MET\\_rel\n-  axial MET\n-  M\\_R\n-  M\\_TR\\_2\n-  R\n-  MT2\n-  S\\_R\n-  M\\_Delta\\_R\n-  dPhi\\_r\\_b\n-  cos(theta\\_r1)\n\nThis data has been produced by MadGraph5 Monte Carlo simulations of 8\nTeV proton collisions, with showering and hadronisation performed by\nPythia 6 and detector response simulated by Delphes. The first 8\nfeatures are kinematic properties measured by simulated particle\ndetectors. The next 10 features are functions of the first 8 features;\nthey are high-level features derived by physicists to help discriminate\nbetween the two classes. There are 46% positive examples in the SUSY\ndata set. The features were standardised over the entire\ntraining/testing sets with mean zero and standard deviation one, except\nfor those features with values strictly greater than zero; these were\nscaled such that the mean value was one.\n\nCaffe\n=====\n\nintroduction\n------------\n\nCaffe is a deep learning framework developed by the Berkeley Vision and\nLearning Center (BVLC) with cleanliness, readability and speed in mind.\nIt has a clean architecture which enables rapid deployment. It is\nreadable and modifiable, encouraging active development. It is a fast\nCNN implementation. It has command line, Python and MATLAB interfaces\nfor day-to-day usage, interfacing with research code and rapid\nprototyping. While Caffe is essentially a C++ library, it has a modular\ninterface for development with cmdcaffe, pycaffe and matcaffe.\n\nThe Caffe core software packages are as follows:\n\n-  Caffe\n-  CUDA\n-  cuDNN\n-  OpenBLAS\n-  OpenCV\n-  Boost\n\nCaffe other dependencies are as follows:\n\n-  protobuf\n-  google-glog\n-  gflags\n-  snappy\n-  leveldb\n-  lmdb\n-  hdf5\n\nThe Caffe build tools are CMake and make.\n\ncommand line\n------------\n\nThe command line interface cmdcaffe is a Caffe tool for model training,\nscoring and diagnostics. Run it without arguments for help. It is at\ndirectory ``caffe/build/tools``.\n\ntrain\n~~~~~\n\n``caffe train`` learns models from scratch, resumes learning from saved\nsnapshots and fine-tunes models to new data and tasks. All training\nrequires a solver configuration through the option\n``-solver solver.prototxt``. Resuming requires the option\n``snapshot model_item_1000.solverstate`` argument to load the solver\nsnapshot.\n\n.. code:: bash\n\n    # train LeNet\n    caffe train -solver examples/mnist/lenet_solver.prototxt\n    # train on GPU 2\n    caffe train -solver examples/mnist/lenet_solver .prototxt -gpu 2\n\ntest\n~~~~\n\n``caffe test`` scores models by running them in the test phase and\nresport the network output as its score. The network architecture must\nbe defined properly to output an accuracy measure or loss as its output.\nThe per-batch score is reported and then the grand average is reported\nlast.\n\n.. code:: bash\n\n    # score the learned LeNet model on the validation set\n    as defined in the model architecture lenet_train_test.prototxt\n    caffe test - model examples/mnist/lenet_train_test.prototxt -weights examples/mnist/lenet_iter_10000 -gpu 0 -iterations 100\n\nbenchmark\n~~~~~~~~~\n\n``caffe time`` benchmarks model execution layer-by-layer through timing\nand synchronisation. This is useful to check system performance and\nmeasure relative execution times for models.\n\n.. code:: bash\n\n    # time LeNet training on CPU for 10 iterations\n    caffe time -model examples/mnist/lenet_train_test.prototxt -iterations 10\n    # time LeNet training on GPU for the default 50 iterations\n    caffe time -model examples/mnist/lenet_train_test.prototxt - gpu 0\n\ndiagnose\n~~~~~~~~\n\n``caffe device_query`` reports GPU details for reference and checking\ndevice ordinals for running on a device in multi-GPU machines.\n\n.. code:: bash\n\n    # query the first device\n    caffe device_query -gpu 0\n\npycaffe\n-------\n\nThe Python interface ``pycaffe`` is the caffe module and its scripts are\nat the directory ``caffe/python``. Run ``import caffe`` to load models,\ndo forward and backward, handle IO, visualise networks and instrument\nmodel-solving. All model data, derivatives and parameters are exposed\nfor reading and writing.\n\n``caffe.Net`` is the central interface for loading, configuring and\nrunning models. ``caffe.Classifier`` and ``caffe.Detector`` provide\nconvenience interfaces for common tasks. ``caffe.SGDSolver`` exposes the\nsolving interface. ``caffe.io`` handles input and output with\npreprocessing and protocol buffers. ``caffe.draw`` visualises network\narchitectures. Caffe blobs are exposed as numpy ndarrays for ease-of-use\nand efficiency.\n\nMATLAB\n------\n\nThe MATLAB interface ``matcaffe`` is the Caffe MATLAB MEX file and its\nhelper m-files are at the directory caffe/matlab. There is example code\n``caffe/matlab/caffe/matcaffe_demo.m``.\n\nmodels\n------\n\nThe directory structure of models is as follows:\n\n.. code:: bash\n\n    .\n    \u251c\u2500\u2500 bvlc_alexnet\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 deploy.prototxt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 readme.md\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 solver.prototxt\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 train_val.prototxt\n    \u251c\u2500\u2500 bvlc_googlenet\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 bvlc_googlenet.caffemodel\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 deploy.prototxt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 quick_solver.prototxt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 readme.md\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 solver.prototxt\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 train_val.prototxt\n    \u251c\u2500\u2500 bvlc_reference_caffenet\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 deploy.prototxt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 readme.md\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 solver.prototxt\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 train_val.prototxt\n    \u251c\u2500\u2500 bvlc_reference_rcnn_ilsvrc13\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 deploy.prototxt\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 readme.md\n    \u2514\u2500\u2500 finetune_flickr_style\n        \u251c\u2500\u2500 deploy.prototxt\n        \u251c\u2500\u2500 readme.md\n        \u251c\u2500\u2500 solver.prototxt\n        \u2514\u2500\u2500 train_val.prototxt\n\ndraw a graph of network architecture\n------------------------------------\n\n.. code:: bash\n\n    \"${CAFFE}\"/python/draw_net.py \"${CAFFE}\"/models/bvlc_googlenet/deploy.prototxt bvlc_googlenet_deploy.png\n\nsetup\n-----\n\n.. code:: bash\n\n    sudo apt-get -y install libprotobuf-dev\n    sudo apt-get -y install libleveldb-dev\n    sudo apt-get -y install libsnappy-dev\n    sudo apt-get -y install libopencv-dev\n    sudo apt-get -y install libhdf5-dev\n    sudo apt-get -y install libhdf5-serial-dev\n    sudo apt-get -y install protobuf-compiler\n    sudo apt-get -y install --no-install-recommends libboost-all-dev\n    sudo apt-get -y install libatlas-base-dev\n    sudo apt-get -y install python-dev\n    sudo apt-get -y install libgflags-dev\n    sudo apt-get -y install libgoogle-glog-dev\n    sudo apt-get -y install liblmdb-dev\n    sudo apt-get -y install python-pydot\n\n.. code:: bash\n\n    sudo pip install protobuf\n    sudo pip install scikit-image\n\n.. code:: bash\n\n    cd\n    git clone https://github.com/BVLC/caffe.git\n    cd caffe\n    cp Makefile.config.example Makefile.config\n\nEdit the makefile. Uncomment ``CPU_ONLY := 1`` for a non-GPU compilation\n(without CUDA). It may be necessary to include the following lines:\n\n::\n\n    INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/\n    LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu/hdf5/serial\n\n.. code:: bash\n\n    time make all\n    time make test\n    time make runtest\n    time make pycaffe\n\n.. code:: bash\n\n    PYTHONPATH=\"/home/\"${USER}\"/caffe/python:${PYTHONPATH}\"\n    CAFFE=\"/home/\"${USER}\"/caffe\"\n\nDownload Caffe models from the Model Zoo.\n\n-  http://caffe.berkeleyvision.org/model_zoo.html\n-  https://github.com/BVLC/caffe/wiki/Model-Zoo\n\n.. code:: bash\n\n    ~/caffe/scripts/download_model_binary.py models/bvlc_googlenet\n\nTorch\n=====\n\nsetup\n-----\n\n.. code:: bash\n\n    curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps | bash\n    git clone https://github.com/torch/distro.git ~/torch --recursive\n    cd ~/torch; ./install.sh\n\nCPU versus GPU for deep learning\n================================\n\nRoelof Pieters set some benchmarks in 2015-07 for deep dreaming video\nprocessing using CPU and GPU hardware. The CPU hardware was Amazon EC2\ng2.2xlarge Intel Xeon E5-2670 (Sandy Bridge) 8 cores 2.6 GHz/3.3 GHz\nturbo and the GPU hardware was Amazon EC2 g2.2xlarge 2 x 4 Gb GPU.\n\n+------+------+------+------+------+\n| **in | **CP | **GP | **CP | **GP |\n| put  | U    | U    | U    | U    |\n| imag | proc | proc | proc | proc |\n| e    | essi | essi | essi | essi |\n| reso | ng   | ng   | ng   | ng   |\n| luti | time | time | time | time |\n| on   | for  | for  | for  | for  |\n| (pix | 1    | 1    | 2    | 2    |\n| els) | imag | imag | minu | minu |\n| **   | e**  | e**  | te   | te   |\n|      |      |      | vide | vide |\n|      |      |      | o**  | o**  |\n+======+======+======+======+======+\n| 540  | 45 s | 1 s  | 1 d  | 60   |\n| x    |      |      | 21 h | minu |\n| 360  |      |      |      | tes  |\n+------+------+------+------+------+\n| 1024 | 144  | 3 s  | 6 d  | 3 h  |\n| x    | s    |      |      |      |\n| 768  |      |      |      |      |\n+------+------+------+------+------+\n\nSo, the GPU hardware was ~45 -- ~48 times faster than the CPU hardware.\n\nintroduction\n============\n\nProject abstraction is a natural language processing project utilising\ncurated conversation data as neural network training data.\n\nbags of words, skip-grams and word vectors\n==========================================\n\nWord vectors are an efficient implementation of bag-of-words and\nskip-gram architectures for computing vector representations of words.\nThese representations can be used in natural language processing\napplications and research.\n\nAn n-gram is a contiguous sequence of n items from a sequence of text or\nspeech. The items can be phonemes, syllabels, letters, words or base\npairs depending on the application. Skip-grams are a generalisation of\nn-grams in which the components (typically words) need not be\nconsecutive in the text under consideration, but may have gaps that are\nskipped. They are one way of overcoming the data sparsity problem found\nin conventional n-gram analysis.\n\nFormally, an n-gram is a consecutive subsequence of length n of some\nsequence of tokens w\\_n. A k-skip-n-gram is a length-n subsequence in\nwhich components occur at a distance of at most k from each other. For\nexample, in the text\n\n::\n\n    the rain in Spain falls mainly on the plain\n\nthe set of 1-skip-2-grams includes all of the 2-grams and, in addition,\nthe following sequences:\n\n::\n\n    the in,\n    rain Spain,\n    in falls,\n    Spain mainly,\n    mainly the,\n    on plain\n\nIt has been demonstrated that skip-gram language models can be trained\nsuch that it is possible to perform 'word arithmetic'. For example, with\nan appropriate model, the expression ``king - man + woman`` evaluates to\nvery close to ``queen``.\n\n-  \"Efficient Estimation of Word Representations in Vector Space\", Tomas\n   Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean\n   http://arxiv.org/abs/1301.3781\n\nThe bag-of-words model is a simplifying representation used in natural\nlanguage processing. In this model, a text is represented as a bag\n(multiset -- a set in which members can appear more than once) of its\nwords, disregarding grammar and word order but keeping multiplicity. The\nbag-of-words model is used commonly in methods of document\nclassification, for which the frequency of occurrence of each word is\nused as a feature for training a classifier.\n\nWord vectors are continuous distributed representations of words. The\ntool word2vec takes a text corpus as input and produces word vectors as\noutput. It constructs a vocabulary from the training text data and then\nlearns vector representations of words. A word2vec model is formed by\ntraining on raw text. It records the context, or usage, of each word\nencoded as word vectors. The significance of a word vector is defined as\nits usefulness as an indicator of certain larger meanings or labels.\n\ncurated conversation data\n=========================\n\nCurated conversation data sourced from Reddit is used for the\nconversation analysis and modelling. Specifically, conversational\nexchanges on Reddit are recorded. An exchange consists of an utterance\nand a response to the utterance, together with associated data, such as\nreferences and timestamps. A submission to Reddit is considered as an\nutterance and a comment on the submission is considered as a response to\nthe utterance. The utterance is assumed to be of good quality and the\nresponse is assumed to be appropriate to the utterance based on the\ncrowd-curated quality assessment inherent in Reddit.\n\ntranslation with word vectors\n=============================\n\nIn the paper `\"Exploiting Similarities among Languages for Machine\nTranslation\" <http://arxiv.org/abs/1309.4168>`__, Tomas Milokov\ndescribes how after training two monolingual modes, a translation matrix\nis generated on the most frequently occurring 5000 words. Using this\ntranslation matrix, the accuracy of the translations was tested on 1000\nwords. A description Milokov gave of the general procedure is as\nfollows:\n\n-  Create matrix ``M`` with dimensionality ``I`` times ``O``, where\n   ``I`` is the size of input vectors and ``O`` is the size of the\n   output vectors.\n-  Iterate over the training set several times with decreasing learning\n   rate and update ``M``.\n\n   -  For each training sample, compute outputs by multiplying the input\n      vector by ``M``.\n   -  Compute the gradient of the error (target vector - output vector).\n   -  Update the weights in ``M`` (with reference to how the weights are\n      updated between the hidden layer and the output layer in word2vec\n      code).\n\nabstraction code picture\n========================\n\n.. figure:: packages_abstraction.png\n   :alt: \n\nmodule abstraction\n==================\n\nThe module abstraction contains functions used generally for project\nabstraction. Many of the programs of the project use its functions.\n\narcodex: archive collated exchanges\n===================================\n\nThe program arcodex is a data collation and archiving program\nspecialised to conversational exchanges. It can be used to archive to\ndatabase exchanges on Reddit.\n\nThe following example accesses 2 utterances from the subreddit\n\"worldnews\" with verbosity:\n\n.. code:: bash\n\n    arcodex.py --numberOfUtterances 2 --subreddits=worldnews --verbose\n\nThe following example accesses 2 utterances from each of the subreddits\n\"changemyview\" and \"worldnews\" with verbosity:\n\n.. code:: bash\n\n    arcodex.py --numberOfUtterances 2 --subreddits=changemyview,worldnews --verbose\n\nThe following example accesses 30 utterances from all of the listed\nsubreddits with verbosity:\n\n.. code:: bash\n\n    arcodex.py --numberOfUtterances 30 --subreddits=askreddit,changemyview,lgbt,machinelearning,particlephysics,technology,worldnews --verbose\n\nThe standard run 2014-10-28T202832Z is as follows:\n\n.. code:: bash\n\n    arcodex.py --numberOfUtterances 200 --subreddits=askreddit,changemyview,lgbt,machinelearning,particlephysics,technology,worldnews --verbose\n\nvicodex, vicodex\\_2: view collated exchanges\n============================================\n\nThe program vicodex\\_2 (and vicodex) is a viewing program specialised to\nconversational exchanges. It can be used to access and view a database\nof exchanges.\n\nThe following example accesses database \"database.db\" and displays its\nexchanges data:\n\n.. code:: bash\n\n    vicodex_2.py --database=\"database.db\"\n\ninspect-database: quick printout of database\n============================================\n\nThe program inspect-database provides a simple, comprehensive printout\nof the contents of a database. Specifically, for every table in the\ndatabase it prints all of the column contents for every entry.\n\n.. code:: bash\n\n    inspect-database.py --database=\"database.db\"\n\nThe program Sqliteman can be used to provide a view of database\ninformation:\n\n.. code:: bash\n\n    sqliteman database.db\n\n::\n\n    SELECT * FROM exchanges;\n\nvcodex: word vectors\n====================\n\nThe program vcodex converts conversational exchanges in an abstraction\ndatabase to word vector representations and adds or updates an\nabstraction database with these vectors.\n\n.. code:: bash\n\n    vcodex.py --database=\"database.db\" --wordvectormodel=Brown_corpus.wvm\n\nThe program vcodex increases the file size of abstraction database\nversion 2015-01-06T172242Z by a factor of ~5.49. On an i7-5500U CPU\nrunning at 2.40 GHz, the conversion rate is ~25 exchanges per second.\n\nreducodex: remove duplicate collated exchanges\n==============================================\n\nThe program reducodex inspects an existing database of conversational\nexchanges, removes duplicate entries, creates simplified identifiers for\nentries and then writes a new database of these entries.\n\nThe following examples access database \"database.db\", remove duplicate\nentries, create simplified identifiers for entries and output database\n\"database\\_1.db\":\n\n.. code:: bash\n\n    reducodex.py --inputdatabase=\"database.db\"\n\n.. code:: bash\n\n    reducodex.py --inputdatabase=\"database.db\" --outputdatabase=\"database_1.db\"\n\nfix\\_database: fix the data structures of database entries\n==========================================================\n\n.. code:: bash\n\n    fix_database.py --verbose 2> >(grep -E -v \"INFO|DEBUG\")\n\nabstraction development testing\n===============================\n\n.. code:: bash\n\n    ./arcodex.py --numberOfUtterances 10 --subreddits=askreddit,changemyview,lgbt,machinelearning,particlephysics,technology,worldnews --database=2015-10-12T1612Z.db --verbose\n\n.. code:: bash\n\n    ./vicodex.py --database=2015-10-12T1612Z.db\n\nsaving models\n=============\n\nNote that the file ``checkpoint`` in the saved model directory contains\nfull paths.\n\n.. |project abstraction| image:: http://img.youtube.com/vi/v9zJ9noLeok/0.jpg\n   :target: https://www.youtube.com/watch?v=v9zJ9noLeok", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/wdbm/abstraction", "keywords": null, "license": "GPLv3", "maintainer": null, "maintainer_email": null, "name": "abstraction", "package_url": "https://pypi.org/project/abstraction/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/abstraction/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/wdbm/abstraction"}, "release_url": "https://pypi.org/project/abstraction/2017.1.16.1534/", "requires_dist": null, "requires_python": null, "summary": "machine learning framework", "version": "2017.1.16.1534", "yanked": false, "html_description": "<div class=\"project-description\">\n            |project abstraction|<br><br>NOTE<br>====<br><br>Please note that abstraction is a *project*, not a finished product.<br><br>setup<br>=====<br><br>The following Bash commands, that have been tested on Ubuntu 15.10,<br>should install prerequisites and check out abstraction.<br><br>.. code:: bash<br><br>    # Install ROOT.<br>    sudo apt-get -y install festival<br>    sudo apt-get -y install pylint<br>    sudo apt-get -y install snakefood<br>    sudo apt-get -y install sqlite<br>    sudo apt-get -y install python-nltk<br>    sudo python -m nltk.downloader all<br>    sudo easy_install -U gensim<br>    sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl<br>    sudo pip install git+git://github.com/google/skflow.git<br>    sudo pip install abstraction<br>    git clone https://github.com/wdbm/abstraction.git<br><br>The function ``abstraction.setup()`` should be run.<br><br>upcoming<br>========<br><br>Under consideration is a requirement for arcodex to ensure the existence<br>of a response to an utterance before saving to database.<br><br>logging<br>=======<br><br>Updating logging procedures is under consideration because of possible<br>logging conflicts. It could be beneficial currently to run using Bash<br>anonymous pipes, in a way like the following:<br><br>.. code:: bash<br><br>    python script.py 2&gt; &gt;(grep -E -v \"INFO|DEBUG\")<br><br>data<br>====<br><br>feature scaling<br>---------------<br><br>Standardization of datasets is a common requirement for many machine<br>learning estimators implemented in the scikit; they might behave badly<br>if the individual features do not more or less look like standard<br>normally-distributed data: Gaussian with zero mean and unit variance --<br>often called a standard scores. Many machine learning algorithms assume<br>that all features are centered around zero and have variance of the same<br>order. A feature with a variance that is orders of magnitude larger that<br>others might dominate the objective function and make the estimator<br>unable to learn from other features. The scikit function ``scale``<br>provides a quick way to perform this operation on a single array-like<br>dataset.<br><br>SUSY Data Set<br>-------------<br><br>-  https://archive.ics.uci.edu/ml/datasets/SUSY<br>-  http://arxiv.org/abs/1402.4735<br><br>The SUSY Data Set is a classification problem to distinguish between a<br>signal process which produces supersymmetric particles and a background<br>process which does not. In the data, the first column is the class label<br>(1 for signal, 0 for background), followed by 18 features (8 low-level<br>features and 10 high-level features):<br><br>-  lepton 1 pT<br>-  lepton 1 eta<br>-  lepton 1 phi<br>-  lepton 2 pT<br>-  lepton 2 eta<br>-  lepton 2 phi<br>-  missing energy magnitude<br>-  missing energy phi<br>-  MET\\_rel<br>-  axial MET<br>-  M\\_R<br>-  M\\_TR\\_2<br>-  R<br>-  MT2<br>-  S\\_R<br>-  M\\_Delta\\_R<br>-  dPhi\\_r\\_b<br>-  cos(theta\\_r1)<br><br>This data has been produced by MadGraph5 Monte Carlo simulations of 8<br>TeV proton collisions, with showering and hadronisation performed by<br>Pythia 6 and detector response simulated by Delphes. The first 8<br>features are kinematic properties measured by simulated particle<br>detectors. The next 10 features are functions of the first 8 features;<br>they are high-level features derived by physicists to help discriminate<br>between the two classes. There are 46% positive examples in the SUSY<br>data set. The features were standardised over the entire<br>training/testing sets with mean zero and standard deviation one, except<br>for those features with values strictly greater than zero; these were<br>scaled such that the mean value was one.<br><br>Caffe<br>=====<br><br>introduction<br>------------<br><br>Caffe is a deep learning framework developed by the Berkeley Vision and<br>Learning Center (BVLC) with cleanliness, readability and speed in mind.<br>It has a clean architecture which enables rapid deployment. It is<br>readable and modifiable, encouraging active development. It is a fast<br>CNN implementation. It has command line, Python and MATLAB interfaces<br>for day-to-day usage, interfacing with research code and rapid<br>prototyping. While Caffe is essentially a C++ library, it has a modular<br>interface for development with cmdcaffe, pycaffe and matcaffe.<br><br>The Caffe core software packages are as follows:<br><br>-  Caffe<br>-  CUDA<br>-  cuDNN<br>-  OpenBLAS<br>-  OpenCV<br>-  Boost<br><br>Caffe other dependencies are as follows:<br><br>-  protobuf<br>-  google-glog<br>-  gflags<br>-  snappy<br>-  leveldb<br>-  lmdb<br>-  hdf5<br><br>The Caffe build tools are CMake and make.<br><br>command line<br>------------<br><br>The command line interface cmdcaffe is a Caffe tool for model training,<br>scoring and diagnostics. Run it without arguments for help. It is at<br>directory ``caffe/build/tools``.<br><br>train<br>~~~~~<br><br>``caffe train`` learns models from scratch, resumes learning from saved<br>snapshots and fine-tunes models to new data and tasks. All training<br>requires a solver configuration through the option<br>``-solver solver.prototxt``. Resuming requires the option<br>``snapshot model_item_1000.solverstate`` argument to load the solver<br>snapshot.<br><br>.. code:: bash<br><br>    # train LeNet<br>    caffe train -solver examples/mnist/lenet_solver.prototxt<br>    # train on GPU 2<br>    caffe train -solver examples/mnist/lenet_solver .prototxt -gpu 2<br><br>test<br>~~~~<br><br>``caffe test`` scores models by running them in the test phase and<br>resport the network output as its score. The network architecture must<br>be defined properly to output an accuracy measure or loss as its output.<br>The per-batch score is reported and then the grand average is reported<br>last.<br><br>.. code:: bash<br><br>    # score the learned LeNet model on the validation set<br>    as defined in the model architecture lenet_train_test.prototxt<br>    caffe test - model examples/mnist/lenet_train_test.prototxt -weights examples/mnist/lenet_iter_10000 -gpu 0 -iterations 100<br><br>benchmark<br>~~~~~~~~~<br><br>``caffe time`` benchmarks model execution layer-by-layer through timing<br>and synchronisation. This is useful to check system performance and<br>measure relative execution times for models.<br><br>.. code:: bash<br><br>    # time LeNet training on CPU for 10 iterations<br>    caffe time -model examples/mnist/lenet_train_test.prototxt -iterations 10<br>    # time LeNet training on GPU for the default 50 iterations<br>    caffe time -model examples/mnist/lenet_train_test.prototxt - gpu 0<br><br>diagnose<br>~~~~~~~~<br><br>``caffe device_query`` reports GPU details for reference and checking<br>device ordinals for running on a device in multi-GPU machines.<br><br>.. code:: bash<br><br>    # query the first device<br>    caffe device_query -gpu 0<br><br>pycaffe<br>-------<br><br>The Python interface ``pycaffe`` is the caffe module and its scripts are<br>at the directory ``caffe/python``. Run ``import caffe`` to load models,<br>do forward and backward, handle IO, visualise networks and instrument<br>model-solving. All model data, derivatives and parameters are exposed<br>for reading and writing.<br><br>``caffe.Net`` is the central interface for loading, configuring and<br>running models. ``caffe.Classifier`` and ``caffe.Detector`` provide<br>convenience interfaces for common tasks. ``caffe.SGDSolver`` exposes the<br>solving interface. ``caffe.io`` handles input and output with<br>preprocessing and protocol buffers. ``caffe.draw`` visualises network<br>architectures. Caffe blobs are exposed as numpy ndarrays for ease-of-use<br>and efficiency.<br><br>MATLAB<br>------<br><br>The MATLAB interface ``matcaffe`` is the Caffe MATLAB MEX file and its<br>helper m-files are at the directory caffe/matlab. There is example code<br>``caffe/matlab/caffe/matcaffe_demo.m``.<br><br>models<br>------<br><br>The directory structure of models is as follows:<br><br>.. code:: bash<br><br>    .<br>    \u251c\u2500\u2500 bvlc_alexnet<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 deploy.prototxt<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 readme.md<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 solver.prototxt<br>    \u2502\u00a0\u00a0 \u2514\u2500\u2500 train_val.prototxt<br>    \u251c\u2500\u2500 bvlc_googlenet<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 bvlc_googlenet.caffemodel<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 deploy.prototxt<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 quick_solver.prototxt<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 readme.md<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 solver.prototxt<br>    \u2502\u00a0\u00a0 \u2514\u2500\u2500 train_val.prototxt<br>    \u251c\u2500\u2500 bvlc_reference_caffenet<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 deploy.prototxt<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 readme.md<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 solver.prototxt<br>    \u2502\u00a0\u00a0 \u2514\u2500\u2500 train_val.prototxt<br>    \u251c\u2500\u2500 bvlc_reference_rcnn_ilsvrc13<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 deploy.prototxt<br>    \u2502\u00a0\u00a0 \u2514\u2500\u2500 readme.md<br>    \u2514\u2500\u2500 finetune_flickr_style<br>        \u251c\u2500\u2500 deploy.prototxt<br>        \u251c\u2500\u2500 readme.md<br>        \u251c\u2500\u2500 solver.prototxt<br>        \u2514\u2500\u2500 train_val.prototxt<br><br>draw a graph of network architecture<br>------------------------------------<br><br>.. code:: bash<br><br>    \"${CAFFE}\"/python/draw_net.py \"${CAFFE}\"/models/bvlc_googlenet/deploy.prototxt bvlc_googlenet_deploy.png<br><br>setup<br>-----<br><br>.. code:: bash<br><br>    sudo apt-get -y install libprotobuf-dev<br>    sudo apt-get -y install libleveldb-dev<br>    sudo apt-get -y install libsnappy-dev<br>    sudo apt-get -y install libopencv-dev<br>    sudo apt-get -y install libhdf5-dev<br>    sudo apt-get -y install libhdf5-serial-dev<br>    sudo apt-get -y install protobuf-compiler<br>    sudo apt-get -y install --no-install-recommends libboost-all-dev<br>    sudo apt-get -y install libatlas-base-dev<br>    sudo apt-get -y install python-dev<br>    sudo apt-get -y install libgflags-dev<br>    sudo apt-get -y install libgoogle-glog-dev<br>    sudo apt-get -y install liblmdb-dev<br>    sudo apt-get -y install python-pydot<br><br>.. code:: bash<br><br>    sudo pip install protobuf<br>    sudo pip install scikit-image<br><br>.. code:: bash<br><br>    cd<br>    git clone https://github.com/BVLC/caffe.git<br>    cd caffe<br>    cp Makefile.config.example Makefile.config<br><br>Edit the makefile. Uncomment ``CPU_ONLY := 1`` for a non-GPU compilation<br>(without CUDA). It may be necessary to include the following lines:<br><br>::<br><br>    INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/<br>    LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu/hdf5/serial<br><br>.. code:: bash<br><br>    time make all<br>    time make test<br>    time make runtest<br>    time make pycaffe<br><br>.. code:: bash<br><br>    PYTHONPATH=\"/home/\"${USER}\"/caffe/python:${PYTHONPATH}\"<br>    CAFFE=\"/home/\"${USER}\"/caffe\"<br><br>Download Caffe models from the Model Zoo.<br><br>-  http://caffe.berkeleyvision.org/model_zoo.html<br>-  https://github.com/BVLC/caffe/wiki/Model-Zoo<br><br>.. code:: bash<br><br>    ~/caffe/scripts/download_model_binary.py models/bvlc_googlenet<br><br>Torch<br>=====<br><br>setup<br>-----<br><br>.. code:: bash<br><br>    curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps | bash<br>    git clone https://github.com/torch/distro.git ~/torch --recursive<br>    cd ~/torch; ./install.sh<br><br>CPU versus GPU for deep learning<br>================================<br><br>Roelof Pieters set some benchmarks in 2015-07 for deep dreaming video<br>processing using CPU and GPU hardware. The CPU hardware was Amazon EC2<br>g2.2xlarge Intel Xeon E5-2670 (Sandy Bridge) 8 cores 2.6 GHz/3.3 GHz<br>turbo and the GPU hardware was Amazon EC2 g2.2xlarge 2 x 4 Gb GPU.<br><br>+------+------+------+------+------+<br>| **in | **CP | **GP | **CP | **GP |<br>| put  | U    | U    | U    | U    |<br>| imag | proc | proc | proc | proc |<br>| e    | essi | essi | essi | essi |<br>| reso | ng   | ng   | ng   | ng   |<br>| luti | time | time | time | time |<br>| on   | for  | for  | for  | for  |<br>| (pix | 1    | 1    | 2    | 2    |<br>| els) | imag | imag | minu | minu |<br>| **   | e**  | e**  | te   | te   |<br>|      |      |      | vide | vide |<br>|      |      |      | o**  | o**  |<br>+======+======+======+======+======+<br>| 540  | 45 s | 1 s  | 1 d  | 60   |<br>| x    |      |      | 21 h | minu |<br>| 360  |      |      |      | tes  |<br>+------+------+------+------+------+<br>| 1024 | 144  | 3 s  | 6 d  | 3 h  |<br>| x    | s    |      |      |      |<br>| 768  |      |      |      |      |<br>+------+------+------+------+------+<br><br>So, the GPU hardware was ~45 -- ~48 times faster than the CPU hardware.<br><br>introduction<br>============<br><br>Project abstraction is a natural language processing project utilising<br>curated conversation data as neural network training data.<br><br>bags of words, skip-grams and word vectors<br>==========================================<br><br>Word vectors are an efficient implementation of bag-of-words and<br>skip-gram architectures for computing vector representations of words.<br>These representations can be used in natural language processing<br>applications and research.<br><br>An n-gram is a contiguous sequence of n items from a sequence of text or<br>speech. The items can be phonemes, syllabels, letters, words or base<br>pairs depending on the application. Skip-grams are a generalisation of<br>n-grams in which the components (typically words) need not be<br>consecutive in the text under consideration, but may have gaps that are<br>skipped. They are one way of overcoming the data sparsity problem found<br>in conventional n-gram analysis.<br><br>Formally, an n-gram is a consecutive subsequence of length n of some<br>sequence of tokens w\\_n. A k-skip-n-gram is a length-n subsequence in<br>which components occur at a distance of at most k from each other. For<br>example, in the text<br><br>::<br><br>    the rain in Spain falls mainly on the plain<br><br>the set of 1-skip-2-grams includes all of the 2-grams and, in addition,<br>the following sequences:<br><br>::<br><br>    the in,<br>    rain Spain,<br>    in falls,<br>    Spain mainly,<br>    mainly the,<br>    on plain<br><br>It has been demonstrated that skip-gram language models can be trained<br>such that it is possible to perform 'word arithmetic'. For example, with<br>an appropriate model, the expression ``king - man + woman`` evaluates to<br>very close to ``queen``.<br><br>-  \"Efficient Estimation of Word Representations in Vector Space\", Tomas<br>   Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean<br>   http://arxiv.org/abs/1301.3781<br><br>The bag-of-words model is a simplifying representation used in natural<br>language processing. In this model, a text is represented as a bag<br>(multiset -- a set in which members can appear more than once) of its<br>words, disregarding grammar and word order but keeping multiplicity. The<br>bag-of-words model is used commonly in methods of document<br>classification, for which the frequency of occurrence of each word is<br>used as a feature for training a classifier.<br><br>Word vectors are continuous distributed representations of words. The<br>tool word2vec takes a text corpus as input and produces word vectors as<br>output. It constructs a vocabulary from the training text data and then<br>learns vector representations of words. A word2vec model is formed by<br>training on raw text. It records the context, or usage, of each word<br>encoded as word vectors. The significance of a word vector is defined as<br>its usefulness as an indicator of certain larger meanings or labels.<br><br>curated conversation data<br>=========================<br><br>Curated conversation data sourced from Reddit is used for the<br>conversation analysis and modelling. Specifically, conversational<br>exchanges on Reddit are recorded. An exchange consists of an utterance<br>and a response to the utterance, together with associated data, such as<br>references and timestamps. A submission to Reddit is considered as an<br>utterance and a comment on the submission is considered as a response to<br>the utterance. The utterance is assumed to be of good quality and the<br>response is assumed to be appropriate to the utterance based on the<br>crowd-curated quality assessment inherent in Reddit.<br><br>translation with word vectors<br>=============================<br><br>In the paper `\"Exploiting Similarities among Languages for Machine<br>Translation\" &lt;http://arxiv.org/abs/1309.4168&gt;`__, Tomas Milokov<br>describes how after training two monolingual modes, a translation matrix<br>is generated on the most frequently occurring 5000 words. Using this<br>translation matrix, the accuracy of the translations was tested on 1000<br>words. A description Milokov gave of the general procedure is as<br>follows:<br><br>-  Create matrix ``M`` with dimensionality ``I`` times ``O``, where<br>   ``I`` is the size of input vectors and ``O`` is the size of the<br>   output vectors.<br>-  Iterate over the training set several times with decreasing learning<br>   rate and update ``M``.<br><br>   -  For each training sample, compute outputs by multiplying the input<br>      vector by ``M``.<br>   -  Compute the gradient of the error (target vector - output vector).<br>   -  Update the weights in ``M`` (with reference to how the weights are<br>      updated between the hidden layer and the output layer in word2vec<br>      code).<br><br>abstraction code picture<br>========================<br><br>.. figure:: packages_abstraction.png<br>   :alt: <br><br>module abstraction<br>==================<br><br>The module abstraction contains functions used generally for project<br>abstraction. Many of the programs of the project use its functions.<br><br>arcodex: archive collated exchanges<br>===================================<br><br>The program arcodex is a data collation and archiving program<br>specialised to conversational exchanges. It can be used to archive to<br>database exchanges on Reddit.<br><br>The following example accesses 2 utterances from the subreddit<br>\"worldnews\" with verbosity:<br><br>.. code:: bash<br><br>    arcodex.py --numberOfUtterances 2 --subreddits=worldnews --verbose<br><br>The following example accesses 2 utterances from each of the subreddits<br>\"changemyview\" and \"worldnews\" with verbosity:<br><br>.. code:: bash<br><br>    arcodex.py --numberOfUtterances 2 --subreddits=changemyview,worldnews --verbose<br><br>The following example accesses 30 utterances from all of the listed<br>subreddits with verbosity:<br><br>.. code:: bash<br><br>    arcodex.py --numberOfUtterances 30 --subreddits=askreddit,changemyview,lgbt,machinelearning,particlephysics,technology,worldnews --verbose<br><br>The standard run 2014-10-28T202832Z is as follows:<br><br>.. code:: bash<br><br>    arcodex.py --numberOfUtterances 200 --subreddits=askreddit,changemyview,lgbt,machinelearning,particlephysics,technology,worldnews --verbose<br><br>vicodex, vicodex\\_2: view collated exchanges<br>============================================<br><br>The program vicodex\\_2 (and vicodex) is a viewing program specialised to<br>conversational exchanges. It can be used to access and view a database<br>of exchanges.<br><br>The following example accesses database \"database.db\" and displays its<br>exchanges data:<br><br>.. code:: bash<br><br>    vicodex_2.py --database=\"database.db\"<br><br>inspect-database: quick printout of database<br>============================================<br><br>The program inspect-database provides a simple, comprehensive printout<br>of the contents of a database. Specifically, for every table in the<br>database it prints all of the column contents for every entry.<br><br>.. code:: bash<br><br>    inspect-database.py --database=\"database.db\"<br><br>The program Sqliteman can be used to provide a view of database<br>information:<br><br>.. code:: bash<br><br>    sqliteman database.db<br><br>::<br><br>    SELECT * FROM exchanges;<br><br>vcodex: word vectors<br>====================<br><br>The program vcodex converts conversational exchanges in an abstraction<br>database to word vector representations and adds or updates an<br>abstraction database with these vectors.<br><br>.. code:: bash<br><br>    vcodex.py --database=\"database.db\" --wordvectormodel=Brown_corpus.wvm<br><br>The program vcodex increases the file size of abstraction database<br>version 2015-01-06T172242Z by a factor of ~5.49. On an i7-5500U CPU<br>running at 2.40 GHz, the conversion rate is ~25 exchanges per second.<br><br>reducodex: remove duplicate collated exchanges<br>==============================================<br><br>The program reducodex inspects an existing database of conversational<br>exchanges, removes duplicate entries, creates simplified identifiers for<br>entries and then writes a new database of these entries.<br><br>The following examples access database \"database.db\", remove duplicate<br>entries, create simplified identifiers for entries and output database<br>\"database\\_1.db\":<br><br>.. code:: bash<br><br>    reducodex.py --inputdatabase=\"database.db\"<br><br>.. code:: bash<br><br>    reducodex.py --inputdatabase=\"database.db\" --outputdatabase=\"database_1.db\"<br><br>fix\\_database: fix the data structures of database entries<br>==========================================================<br><br>.. code:: bash<br><br>    fix_database.py --verbose 2&gt; &gt;(grep -E -v \"INFO|DEBUG\")<br><br>abstraction development testing<br>===============================<br><br>.. code:: bash<br><br>    ./arcodex.py --numberOfUtterances 10 --subreddits=askreddit,changemyview,lgbt,machinelearning,particlephysics,technology,worldnews --database=2015-10-12T1612Z.db --verbose<br><br>.. code:: bash<br><br>    ./vicodex.py --database=2015-10-12T1612Z.db<br><br>saving models<br>=============<br><br>Note that the file ``checkpoint`` in the saved model directory contains<br>full paths.<br><br>.. |project abstraction| image:: http://img.youtube.com/vi/v9zJ9noLeok/0.jpg<br>   :target: https://www.youtube.com/watch?v=v9zJ9noLeok\n          </div>"}, "last_serial": 2577482, "releases": {"2015.10.30.2039": [{"comment_text": "", "digests": {"md5": "5bc71d4322426459db0d8eca6c275bff", "sha256": "3a7aec52c478a57f2d3757cc9a367148693c4429627d5563f6b003a241897ed7"}, "downloads": -1, "filename": "abstraction-2015.10.30.2039.tar.gz", "has_sig": false, "md5_digest": "5bc71d4322426459db0d8eca6c275bff", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26689, "upload_time": "2015-10-30T20:49:32", "upload_time_iso_8601": "2015-10-30T20:49:32.498519Z", "url": "https://files.pythonhosted.org/packages/74/a6/f89328ea7ea0c923b2185d0d1c92cfc4d3d2f32323cdadb062d71bc0fbd2/abstraction-2015.10.30.2039.tar.gz", "yanked": false}], "2015.12.2.1425": [{"comment_text": "", "digests": {"md5": "c4f1946073a111a92376b7a2eb430711", "sha256": "9858e565552a1fd957e5c396eef922c7fdfd4388dbfd787e9903423e45895eb5"}, "downloads": -1, "filename": "abstraction-2015.12.2.1425.tar.gz", "has_sig": false, "md5_digest": "c4f1946073a111a92376b7a2eb430711", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 27765, "upload_time": "2015-12-02T15:17:49", "upload_time_iso_8601": "2015-12-02T15:17:49.439481Z", "url": "https://files.pythonhosted.org/packages/2f/54/1cba4556f355e528faca0cd1c694c55a8bf64850e17848d34df327e2b3af/abstraction-2015.12.2.1425.tar.gz", "yanked": false}], "2016.2.8.2250": [{"comment_text": "", "digests": {"md5": "7687edc653c811373143b76c195cb9eb", "sha256": "ee1163b8831b49954e24f6b5390be30f15ff215aec1b81b91613eb5201351daf"}, "downloads": -1, "filename": "abstraction-2016.2.8.2250.tar.gz", "has_sig": false, "md5_digest": "7687edc653c811373143b76c195cb9eb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 41245, "upload_time": "2016-02-18T13:11:15", "upload_time_iso_8601": "2016-02-18T13:11:15.153699Z", "url": "https://files.pythonhosted.org/packages/fd/c0/cb4d0a8d0222311cdd61b2b52c86bb5575c45813f951a724367dd6325bb6/abstraction-2016.2.8.2250.tar.gz", "yanked": false}], "2016.3.2.1123": [{"comment_text": "", "digests": {"md5": "8f6b0f18150dd92adad54cac49e4889a", "sha256": "a1fb43791c395aa9f1d107de7d34dd98bcaa2b5d1680daab5a08f1f03097a5c3"}, "downloads": -1, "filename": "abstraction-2016.3.2.1123.tar.gz", "has_sig": false, "md5_digest": "8f6b0f18150dd92adad54cac49e4889a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 41455, "upload_time": "2016-03-02T13:32:43", "upload_time_iso_8601": "2016-03-02T13:32:43.071541Z", "url": "https://files.pythonhosted.org/packages/f8/55/18d77a558e89e3581f43e92ddcdf8d5f8cb29fe68d6a10a61134ff78dc92/abstraction-2016.3.2.1123.tar.gz", "yanked": false}], "2016.6.1.1602": [{"comment_text": "", "digests": {"md5": "45298767b20822580b06ce7cef678a12", "sha256": "d45bdf272b7a0217baa97f01f2450aa787153ea6f91b859c6a9c556fef50c04a"}, "downloads": -1, "filename": "abstraction-2016.6.1.1602.tar.gz", "has_sig": false, "md5_digest": "45298767b20822580b06ce7cef678a12", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39944, "upload_time": "2016-06-01T16:04:37", "upload_time_iso_8601": "2016-06-01T16:04:37.044618Z", "url": "https://files.pythonhosted.org/packages/06/13/8564bee8f95e895445cf4033aff6e41185e60771aa1ea7ee0a96da18eb29/abstraction-2016.6.1.1602.tar.gz", "yanked": false}], "2016.6.15.1537": [{"comment_text": "", "digests": {"md5": "43e2de249ee550292836e22ec9ce2cb8", "sha256": "2f6227e42e8b138332c8568d5205d8ad04c769ea7182421effb356a4ff1edc7e"}, "downloads": -1, "filename": "abstraction-2016.6.15.1537.tar.gz", "has_sig": false, "md5_digest": "43e2de249ee550292836e22ec9ce2cb8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2292673, "upload_time": "2016-06-15T15:55:45", "upload_time_iso_8601": "2016-06-15T15:55:45.194275Z", "url": "https://files.pythonhosted.org/packages/25/9e/d5fb418658d840cc013470e6f73f938c6b9ba0896f564ccd1b67c9c523e8/abstraction-2016.6.15.1537.tar.gz", "yanked": false}], "2016.6.17.1558": [{"comment_text": "", "digests": {"md5": "2115960d5fb00b16416a3a0526787ca9", "sha256": "c7d332c8fceb8cbaf3f8525054d396caed9f2d43da466ba139479489f2779ca4"}, "downloads": -1, "filename": "abstraction-2016.6.17.1558.tar.gz", "has_sig": false, "md5_digest": "2115960d5fb00b16416a3a0526787ca9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2293425, "upload_time": "2016-06-17T16:26:34", "upload_time_iso_8601": "2016-06-17T16:26:34.827944Z", "url": "https://files.pythonhosted.org/packages/c7/fb/6e0c05d2d3db01e938ce16e7d98c518f67ce20bea854c4579fa6c84adcdf/abstraction-2016.6.17.1558.tar.gz", "yanked": false}], "2016.6.2.1354": [{"comment_text": "", "digests": {"md5": "602acbaa59eef52483bea0d78d824fe5", "sha256": "1d08b902d93bc1319fd6479b13111fce86e223a52300e57d9e61f97f1e2c1ff1"}, "downloads": -1, "filename": "abstraction-2016.6.2.1354.tar.gz", "has_sig": false, "md5_digest": "602acbaa59eef52483bea0d78d824fe5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39991, "upload_time": "2016-06-02T14:01:54", "upload_time_iso_8601": "2016-06-02T14:01:54.383149Z", "url": "https://files.pythonhosted.org/packages/f0/fa/6c34ae8961bab29a874845c9a9e6f268aad3f71d69e08020c19c804cbda6/abstraction-2016.6.2.1354.tar.gz", "yanked": false}], "2016.6.22.1658": [{"comment_text": "", "digests": {"md5": "1d5508bc097d11a78ae681971da9bd82", "sha256": "6014a178d066c4b944737aa13a42c4ab303d39e186416bdcc9128cbae41bb0c5"}, "downloads": -1, "filename": "abstraction-2016.6.22.1658.tar.gz", "has_sig": false, "md5_digest": "1d5508bc097d11a78ae681971da9bd82", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2293866, "upload_time": "2016-06-22T17:22:48", "upload_time_iso_8601": "2016-06-22T17:22:48.375068Z", "url": "https://files.pythonhosted.org/packages/99/56/ee498f3c837faf6f4c273c40aae194efc475954145a03ba0efbbd6c87f3c/abstraction-2016.6.22.1658.tar.gz", "yanked": false}], "2017.1.16.1534": [{"comment_text": "", "digests": {"md5": "2bdc81605247ee31674b816049c2302a", "sha256": "5ee47f76f2fdb95e2d1e398905606441d932d11f26b482d5d1fab91153127def"}, "downloads": -1, "filename": "abstraction-2017.1.16.1534.tar.gz", "has_sig": false, "md5_digest": "2bdc81605247ee31674b816049c2302a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2290703, "upload_time": "2017-01-16T15:46:00", "upload_time_iso_8601": "2017-01-16T15:46:00.391696Z", "url": "https://files.pythonhosted.org/packages/c6/b6/f0176ec95410e23bdd2b8e8e1f63f0e66bf3f78fa3c29c257f0613c646c9/abstraction-2017.1.16.1534.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2bdc81605247ee31674b816049c2302a", "sha256": "5ee47f76f2fdb95e2d1e398905606441d932d11f26b482d5d1fab91153127def"}, "downloads": -1, "filename": "abstraction-2017.1.16.1534.tar.gz", "has_sig": false, "md5_digest": "2bdc81605247ee31674b816049c2302a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2290703, "upload_time": "2017-01-16T15:46:00", "upload_time_iso_8601": "2017-01-16T15:46:00.391696Z", "url": "https://files.pythonhosted.org/packages/c6/b6/f0176ec95410e23bdd2b8e8e1f63f0e66bf3f78fa3c29c257f0613c646c9/abstraction-2017.1.16.1534.tar.gz", "yanked": false}], "timestamp": "Thu May  7 16:24:57 2020"}