{"info": {"author": "Kyle Wilcox", "author_email": "kyle@axiomdatascience.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Operating System :: OS Independent", "Programming Language :: Python", "Topic :: Scientific/Engineering"], "description": "# thredds_crawler\n\n[![Build Status](https://travis-ci.org/ioos/thredds_crawler.svg?branch=master)](https://travis-ci.org/ioos/thredds_crawler)\n\nA simple crawler/parser for THREDDS catalogs\n\n## Installation\n\n```bash\npip install thredds_crawler\n```\n\nor\n\n```bash\nconda install -c conda-forge thredds_crawler\n```\n\n## Usage\n\n\n### Select\n\nYou can select datasets based on their THREDDS ID using the 'select' parameter.  Python regex is supported.\n\n```python\nfrom thredds_crawler.crawl import Crawl\nc = Crawl('http://tds.maracoos.org/thredds/MODIS.xml', select=[\".*-Agg\"])\nprint c.datasets\n[\n  <LeafDataset id: MODIS-Agg, name: MODIS-Complete Aggregation, services: ['OPENDAP', 'ISO']>,\n  <LeafDataset id: MODIS-2009-Agg, name: MODIS-2009 Aggregation, services: ['OPENDAP', 'ISO']>,\n  <LeafDataset id: MODIS-2010-Agg, name: MODIS-2010 Aggregation, services: ['OPENDAP', 'ISO']>,\n  <LeafDataset id: MODIS-2011-Agg, name: MODIS-2011 Aggregation, services: ['OPENDAP', 'ISO']>,\n  <LeafDataset id: MODIS-2012-Agg, name: MODIS-2012 Aggregation, services: ['OPENDAP', 'ISO']>,\n  <LeafDataset id: MODIS-2013-Agg, name: MODIS-2013 Aggregation, services: ['OPENDAP', 'ISO']>,\n  <LeafDataset id: MODIS-One-Agg, name: 1-Day-Aggregation, services: ['OPENDAP', 'ISO']>,\n  <LeafDataset id: MODIS-Three-Agg, name: 3-Day-Aggregation, services: ['OPENDAP', 'ISO']>,\n  <LeafDataset id: MODIS-Seven-Agg, name: 7-Day-Aggregation, services: ['OPENDAP', 'ISO']>\n]\n```\n\n### Skip\n\nYou can skip datasets based on their `name` and catalogRefs based on their `xlink:title`.  By default, the crawler uses some common regular expressions to skip lists of thousands upon thousands of individual files that are part of aggregations or FMRCs:\n\n*  `.*files.*`\n*  `.*Individual Files.*`\n*  `.*File_Access.*`\n*  `.*Forecast Model Run.*`\n*  `.*Constant Forecast Offset.*`\n*  `.*Constant Forecast Date.*`\n\nBy setting the `skip` parameter to anything other than a superset of the default you run the risk of having some angry system admins after you.\n\nYou can access the default `skip` list through the Crawl.SKIPS class variable\n\n```python\nfrom thredds_crawler.crawl import Crawl\nprint Crawl.SKIPS\n[\n  '.*files.*',\n  '.*Individual Files.*',\n  '.*File_Access.*',\n  '.*Forecast Model Run.*',\n  '.*Constant Forecast Offset.*',\n  '.*Constant Forecast Date.*'\n]\n```\n\nIf you need to remove or add a new `skip`, it is **strongly** encouraged you use the `SKIPS` class variable as a starting point!\n\n```python\nfrom thredds_crawler.crawl import Crawl\nskips = Crawl.SKIPS + [\".*-Day-Aggregation\"]\nc = Crawl(\n  'http://tds.maracoos.org/thredds/MODIS.xml',\n  select=[\".*-Agg\"],\n  skip=skips\n)\nprint c.datasets\n\n[\n  <LeafDataset id: MODIS-Agg, name: MODIS-Complete Aggregation, services: ['OPENDAP', 'ISO']>,\n  <LeafDataset id: MODIS-2009-Agg, name: MODIS-2009 Aggregation, services: ['OPENDAP', 'ISO']>,\n  <LeafDataset id: MODIS-2010-Agg, name: MODIS-2010 Aggregation, services: ['OPENDAP', 'ISO']>,\n  <LeafDataset id: MODIS-2011-Agg, name: MODIS-2011 Aggregation, services: ['OPENDAP', 'ISO']>,\n  <LeafDataset id: MODIS-2012-Agg, name: MODIS-2012 Aggregation, services: ['OPENDAP', 'ISO']>,\n  <LeafDataset id: MODIS-2013-Agg, name: MODIS-2013 Aggregation, services: ['OPENDAP', 'ISO']>,\n]\n```\n\n### Workers\n\nBy default there are `4` worker threads used in the crawling. You can change this by specifying a `workers` parameter.\n\n```python\nimport time\nfrom contextlib import contextmanager\nfrom thredds_crawler.crawl import Crawl\n\n@contextmanager\ndef timeit(name):\n    startTime = time.time()\n    yield\n    elapsedTime = time.time() - startTime\n    print('[{}] finished in {} ms'.format(name, int(elapsedTime * 1000)))\n\nfor x in range(1, 11):\n    with timeit('{} workers'.format(x)):\n        Crawl(\"http://tds.maracoos.org/thredds/MODIS.xml\", workers=x)\n\n[1 workers] finished in 872 ms\n[2 workers] finished in 397 ms\n[3 workers] finished in 329 ms\n[4 workers] finished in 260 ms\n[5 workers] finished in 264 ms\n[6 workers] finished in 219 ms\n[7 workers] finished in 212 ms\n[8 workers] finished in 185 ms\n[9 workers] finished in 217 ms\n[10 workers] finished in 205 ms\n```\n\n\n### Modified Time\n\nYou can select data by the THREDDS `modified_time` by using a the `before` and `after` parameters. Keep in mind that the modified time is only avaiable for individual files hosted in THREDDS (not aggregations).\n\n```python\nimport pytz\nfrom thredds_crawler.crawl import Crawl\n\nbf = datetime(2016, 1, 5, 0, 0)\naf = datetime(2015, 12, 30, 0, 0, tzinfo=pytz.utc)\nurl = 'http://tds.maracoos.org/thredds/catalog/MODIS-Chesapeake-Salinity/raw/2016/catalog.xml'\n\n# after\nc = Crawl(url, after=af)\nassert len(c.datasets) == 3\n\n# before\nc = Crawl(url, before=bf)\nassert len(c.datasets) == 3\n\n# both\naf = datetime(2016, 1, 20, 0, 0)\nbf = datetime(2016, 2, 1, 0, 0)\nc = Crawl(url, before=bf, after=af)\nassert len(c.datasets) == 11\n```\n\n\n### Authentication\n\nYou can pass an auth parameter as needed. It needs to be a [requests compatible auth object](http://docs.python-requests.org/en/latest/user/authentication/).\n\n```python\nfrom thredds_crawler.crawl import Crawl\nauth = ('user', 'password')\nc = Crawl(\n  'http://tds.maracoos.org/thredds/MODIS.xml',\n  select=['.*-Agg'],\n  skip=Crawl.SKIPS,\n  auth=auth\n)\n```\n\n\n### Debugging\n\nYou can pass in a `debug=True` parameter to Crawl to log to STDOUT what is actually happening.\n\n```python\nfrom thredds_crawler.crawl import Crawl\nskips = Crawl.SKIPS + [\".*-Day-Aggregation\"]\nc = Crawl(\n  'http://tds.maracoos.org/thredds/MODIS.xml',\n  select=['.*-Agg'],\n  skip=skips,\n  debug=True\n)\n\nCrawling: http://tds.maracoos.org/thredds/MODIS.xml\nSkipping catalogRef based on 'skips'.  Title: MODIS Individual Files\nSkipping catalogRef based on 'skips'.  Title: 1-Day Individual Files\nSkipping catalogRef based on 'skips'.  Title: 3-Day Individual Files\nSkipping catalogRef based on 'skips'.  Title: 8-Day Individual Files\nProcessing MODIS-Agg\nProcessing MODIS-2009-Agg\nProcessing MODIS-2010-Agg\nProcessing MODIS-2011-Agg\nProcessing MODIS-2012-Agg\nProcessing MODIS-2013-Agg\nSkipping dataset based on 'skips'.  Name: 1-Day-Aggregation\n```\n\n\n### Logging\n\nIf you are using `thredds_crawler` from inside of another program, you can access its logs\nusing the named logger `thredds_crawler` to control the log level.  If you access to the named\nlogger, **do not** include `debug=True` when initializing the Crawl object.\n\n```python\nimport logging\ncrawl_log = logging.getLogger('thredds_crawler')\ncrawl_log.setLevel(logging.WARNING)\n```\n\n\n## Dataset\n\nYou can get some basic information about a LeafDataset, including the services available.\n\n```python\nfrom thredds_crawler.crawl import Crawl\nc = Crawl('http://tds.maracoos.org/thredds/MODIS.xml', select=['.*-Agg'])\ndataset = c.datasets[0]\nprint dataset.id\nMODIS-Agg\nprint dataset.name\nMODIS-Complete Aggregation\nprint dataset.services\n[\n  {\n    'url': 'http://tds.maracoos.org/thredds/dodsC/MODIS-Agg.nc',\n    'name': 'odap',\n    'service': 'OPENDAP'\n  },\n  {\n    'url': 'http://tds.maracoos.org/thredds/iso/MODIS-Agg.nc',\n    'name': 'iso',\n    'service': 'ISO'\n  }\n]\n```\n\nIf you have a list of datasets you can easily return all endpoints of a certain type:\n\n```python\nfrom thredds_crawler.crawl import Crawl\nc = Crawl('http://tds.maracoos.org/thredds/MODIS.xml', select=['.*-Agg'])\nurls = [s.get(\"url\") for d in c.datasets for s in d.services if s.get(\"service\").lower() == \"opendap\"]\nprint urls\n[\n  'http://tds.maracoos.org/thredds/dodsC/MODIS-Agg.nc',\n  'http://tds.maracoos.org/thredds/dodsC/MODIS-2009-Agg.nc',\n  'http://tds.maracoos.org/thredds/dodsC/MODIS-2010-Agg.nc',\n  'http://tds.maracoos.org/thredds/dodsC/MODIS-2011-Agg.nc',\n  'http://tds.maracoos.org/thredds/dodsC/MODIS-2012-Agg.nc',\n  'http://tds.maracoos.org/thredds/dodsC/MODIS-2013-Agg.nc',\n  'http://tds.maracoos.org/thredds/dodsC/MODIS-One-Agg.nc',\n  'http://tds.maracoos.org/thredds/dodsC/MODIS-Three-Agg.nc',\n  'http://tds.maracoos.org/thredds/dodsC/MODIS-Seven-Agg.nc'\n]\n```\n\nYou can also obtain the dataset size.  This returns the size on disk if the informaton is available in the TDS\ncatalog.  If it is not available and a DAP endpoint is available, it returns the theoretical size of all of thh variables.\nThis isn't necessarialy the size on disk, because it does not account for `missing_value` and `_FillValue` space.\n\n```python\nfrom thredds_crawler.crawl import Crawl\nc = Crawl(\n  'http://thredds.axiomalaska.com/thredds/catalogs/cencoos.html',\n  select=['MB_.*']\n)\nsizes = [d.size for d in c.datasets]\nprint sizes\n[29247.410283999998, 72166.289680000002]\n```\n\n\n## Metadata\n\nThe entire THREDDS catalog metadata record is saved along with the dataset object.  It is an etree Element object ready for you to pull information out of.  See the [THREDDS metadata spec](http://www.unidata.ucar.edu/projects/THREDDS/tech/catalog/v1.0.2/InvCatalogSpec.html#metadata)\n\n```python\nfrom thredds_crawler.crawl import Crawl\nc = Crawl('http://tds.maracoos.org/thredds/MODIS.xml', select=['.*-Agg'])\ndataset = c.datasets[0]\nprint dataset.metadata.find(\"{http://www.unidata.ucar.edu/namespaces/thredds/InvCatalog/v1.0}documentation\").text\nOcean Color data are provided as a service to the broader community, and can be\ninfluenced by sensor degradation and or algorithm changes. We make efforts to keep\nthis dataset updated and calibrated. The products in these files are experimental.\nAggregations are simple means of available data over the specified time frame. Use at\nyour own discretion.\n```\n\n\n## Use Case\n\nBelow is a python script that can be used to harvest THEDDS catalogs and save the ISO metadata files\nto a local directory\n\n```python\nimport os\nimport urllib\nfrom thredds_crawler.crawl import Crawl\n\nimport logging\nimport logging.handlers\nlogger = logging.getLogger('thredds_crawler')\nfh = logging.handlers.RotatingFileHandler('/var/log/iso_harvest/iso_harvest.log', maxBytes=1024*1024*10, backupCount=5)\nfh.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nfh.setFormatter(formatter)\nch.setFormatter(formatter)\nlogger.addHandler(fh)\nlogger.addHandler(ch)\nlogger.setLevel(logging.DEBUG)\n\nSAVE_DIR=\"/srv/http/iso\"\n\nTHREDDS_SERVERS = {\n    \"aoos\":      \"http://thredds.axiomalaska.com/thredds/catalogs/aoos.html\",\n    \"cencoos\":   \"http://thredds.axiomalaska.com/thredds/catalogs/cencoos.html\",\n    \"maracoos\" : \"http://tds.maracoos.org/thredds/catalog.html\",\n    \"glos\":      \"http://tds.glos.us/thredds/catalog.html\"\n}\n\nfor subfolder, thredds_url in THREDDS_SERVERS.items():\n  logger.info(\"Crawling %s (%s)\" % (subfolder, thredds_url))\n  crawler = Crawl(thredds_url, debug=True)\n  isos = [(d.id, s.get(\"url\")) for d in crawler.datasets for s in d.services if s.get(\"service\").lower() == \"iso\"]\n  filefolder = os.path.join(SAVE_DIR, subfolder)\n  if not os.path.exists(filefolder):\n    os.makedirs(filefolder)\n  for iso in isos:\n    try:\n      filename = iso[0].replace(\"/\", \"_\") + \".iso.xml\"\n      filepath = os.path.join(filefolder, filename)\n      logger.info(\"Downloading/Saving %s\" % filepath)\n      urllib.urlretrieve(iso[1], filepath)\n    except BaseException:\n      logger.exception(\"Error!\")\n```\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ioos/thredds_crawler", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "thredds_crawler", "package_url": "https://pypi.org/project/thredds_crawler/", "platform": "", "project_url": "https://pypi.org/project/thredds_crawler/", "project_urls": {"Homepage": "https://github.com/ioos/thredds_crawler"}, "release_url": "https://pypi.org/project/thredds_crawler/1.5.4/", "requires_dist": ["requests", "lxml", "pytz", "python-dateutil"], "requires_python": "", "summary": "A Python library for crawling THREDDS servers", "version": "1.5.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            # thredds_crawler<br><br>[![Build Status](https://travis-ci.org/ioos/thredds_crawler.svg?branch=master)](https://travis-ci.org/ioos/thredds_crawler)<br><br>A simple crawler/parser for THREDDS catalogs<br><br>## Installation<br><br>```bash<br>pip install thredds_crawler<br>```<br><br>or<br><br>```bash<br>conda install -c conda-forge thredds_crawler<br>```<br><br>## Usage<br><br><br>### Select<br><br>You can select datasets based on their THREDDS ID using the 'select' parameter.  Python regex is supported.<br><br>```python<br>from thredds_crawler.crawl import Crawl<br>c = Crawl('http://tds.maracoos.org/thredds/MODIS.xml', select=[\".*-Agg\"])<br>print c.datasets<br>[<br>  &lt;LeafDataset id: MODIS-Agg, name: MODIS-Complete Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>  &lt;LeafDataset id: MODIS-2009-Agg, name: MODIS-2009 Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>  &lt;LeafDataset id: MODIS-2010-Agg, name: MODIS-2010 Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>  &lt;LeafDataset id: MODIS-2011-Agg, name: MODIS-2011 Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>  &lt;LeafDataset id: MODIS-2012-Agg, name: MODIS-2012 Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>  &lt;LeafDataset id: MODIS-2013-Agg, name: MODIS-2013 Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>  &lt;LeafDataset id: MODIS-One-Agg, name: 1-Day-Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>  &lt;LeafDataset id: MODIS-Three-Agg, name: 3-Day-Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>  &lt;LeafDataset id: MODIS-Seven-Agg, name: 7-Day-Aggregation, services: ['OPENDAP', 'ISO']&gt;<br>]<br>```<br><br>### Skip<br><br>You can skip datasets based on their `name` and catalogRefs based on their `xlink:title`.  By default, the crawler uses some common regular expressions to skip lists of thousands upon thousands of individual files that are part of aggregations or FMRCs:<br><br>*  `.*files.*`<br>*  `.*Individual Files.*`<br>*  `.*File_Access.*`<br>*  `.*Forecast Model Run.*`<br>*  `.*Constant Forecast Offset.*`<br>*  `.*Constant Forecast Date.*`<br><br>By setting the `skip` parameter to anything other than a superset of the default you run the risk of having some angry system admins after you.<br><br>You can access the default `skip` list through the Crawl.SKIPS class variable<br><br>```python<br>from thredds_crawler.crawl import Crawl<br>print Crawl.SKIPS<br>[<br>  '.*files.*',<br>  '.*Individual Files.*',<br>  '.*File_Access.*',<br>  '.*Forecast Model Run.*',<br>  '.*Constant Forecast Offset.*',<br>  '.*Constant Forecast Date.*'<br>]<br>```<br><br>If you need to remove or add a new `skip`, it is **strongly** encouraged you use the `SKIPS` class variable as a starting point!<br><br>```python<br>from thredds_crawler.crawl import Crawl<br>skips = Crawl.SKIPS + [\".*-Day-Aggregation\"]<br>c = Crawl(<br>  'http://tds.maracoos.org/thredds/MODIS.xml',<br>  select=[\".*-Agg\"],<br>  skip=skips<br>)<br>print c.datasets<br><br>[<br>  &lt;LeafDataset id: MODIS-Agg, name: MODIS-Complete Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>  &lt;LeafDataset id: MODIS-2009-Agg, name: MODIS-2009 Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>  &lt;LeafDataset id: MODIS-2010-Agg, name: MODIS-2010 Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>  &lt;LeafDataset id: MODIS-2011-Agg, name: MODIS-2011 Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>  &lt;LeafDataset id: MODIS-2012-Agg, name: MODIS-2012 Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>  &lt;LeafDataset id: MODIS-2013-Agg, name: MODIS-2013 Aggregation, services: ['OPENDAP', 'ISO']&gt;,<br>]<br>```<br><br>### Workers<br><br>By default there are `4` worker threads used in the crawling. You can change this by specifying a `workers` parameter.<br><br>```python<br>import time<br>from contextlib import contextmanager<br>from thredds_crawler.crawl import Crawl<br><br>@contextmanager<br>def timeit(name):<br>    startTime = time.time()<br>    yield<br>    elapsedTime = time.time() - startTime<br>    print('[{}] finished in {} ms'.format(name, int(elapsedTime * 1000)))<br><br>for x in range(1, 11):<br>    with timeit('{} workers'.format(x)):<br>        Crawl(\"http://tds.maracoos.org/thredds/MODIS.xml\", workers=x)<br><br>[1 workers] finished in 872 ms<br>[2 workers] finished in 397 ms<br>[3 workers] finished in 329 ms<br>[4 workers] finished in 260 ms<br>[5 workers] finished in 264 ms<br>[6 workers] finished in 219 ms<br>[7 workers] finished in 212 ms<br>[8 workers] finished in 185 ms<br>[9 workers] finished in 217 ms<br>[10 workers] finished in 205 ms<br>```<br><br><br>### Modified Time<br><br>You can select data by the THREDDS `modified_time` by using a the `before` and `after` parameters. Keep in mind that the modified time is only avaiable for individual files hosted in THREDDS (not aggregations).<br><br>```python<br>import pytz<br>from thredds_crawler.crawl import Crawl<br><br>bf = datetime(2016, 1, 5, 0, 0)<br>af = datetime(2015, 12, 30, 0, 0, tzinfo=pytz.utc)<br>url = 'http://tds.maracoos.org/thredds/catalog/MODIS-Chesapeake-Salinity/raw/2016/catalog.xml'<br><br># after<br>c = Crawl(url, after=af)<br>assert len(c.datasets) == 3<br><br># before<br>c = Crawl(url, before=bf)<br>assert len(c.datasets) == 3<br><br># both<br>af = datetime(2016, 1, 20, 0, 0)<br>bf = datetime(2016, 2, 1, 0, 0)<br>c = Crawl(url, before=bf, after=af)<br>assert len(c.datasets) == 11<br>```<br><br><br>### Authentication<br><br>You can pass an auth parameter as needed. It needs to be a [requests compatible auth object](http://docs.python-requests.org/en/latest/user/authentication/).<br><br>```python<br>from thredds_crawler.crawl import Crawl<br>auth = ('user', 'password')<br>c = Crawl(<br>  'http://tds.maracoos.org/thredds/MODIS.xml',<br>  select=['.*-Agg'],<br>  skip=Crawl.SKIPS,<br>  auth=auth<br>)<br>```<br><br><br>### Debugging<br><br>You can pass in a `debug=True` parameter to Crawl to log to STDOUT what is actually happening.<br><br>```python<br>from thredds_crawler.crawl import Crawl<br>skips = Crawl.SKIPS + [\".*-Day-Aggregation\"]<br>c = Crawl(<br>  'http://tds.maracoos.org/thredds/MODIS.xml',<br>  select=['.*-Agg'],<br>  skip=skips,<br>  debug=True<br>)<br><br>Crawling: http://tds.maracoos.org/thredds/MODIS.xml<br>Skipping catalogRef based on 'skips'.  Title: MODIS Individual Files<br>Skipping catalogRef based on 'skips'.  Title: 1-Day Individual Files<br>Skipping catalogRef based on 'skips'.  Title: 3-Day Individual Files<br>Skipping catalogRef based on 'skips'.  Title: 8-Day Individual Files<br>Processing MODIS-Agg<br>Processing MODIS-2009-Agg<br>Processing MODIS-2010-Agg<br>Processing MODIS-2011-Agg<br>Processing MODIS-2012-Agg<br>Processing MODIS-2013-Agg<br>Skipping dataset based on 'skips'.  Name: 1-Day-Aggregation<br>```<br><br><br>### Logging<br><br>If you are using `thredds_crawler` from inside of another program, you can access its logs<br>using the named logger `thredds_crawler` to control the log level.  If you access to the named<br>logger, **do not** include `debug=True` when initializing the Crawl object.<br><br>```python<br>import logging<br>crawl_log = logging.getLogger('thredds_crawler')<br>crawl_log.setLevel(logging.WARNING)<br>```<br><br><br>## Dataset<br><br>You can get some basic information about a LeafDataset, including the services available.<br><br>```python<br>from thredds_crawler.crawl import Crawl<br>c = Crawl('http://tds.maracoos.org/thredds/MODIS.xml', select=['.*-Agg'])<br>dataset = c.datasets[0]<br>print dataset.id<br>MODIS-Agg<br>print dataset.name<br>MODIS-Complete Aggregation<br>print dataset.services<br>[<br>  {<br>    'url': 'http://tds.maracoos.org/thredds/dodsC/MODIS-Agg.nc',<br>    'name': 'odap',<br>    'service': 'OPENDAP'<br>  },<br>  {<br>    'url': 'http://tds.maracoos.org/thredds/iso/MODIS-Agg.nc',<br>    'name': 'iso',<br>    'service': 'ISO'<br>  }<br>]<br>```<br><br>If you have a list of datasets you can easily return all endpoints of a certain type:<br><br>```python<br>from thredds_crawler.crawl import Crawl<br>c = Crawl('http://tds.maracoos.org/thredds/MODIS.xml', select=['.*-Agg'])<br>urls = [s.get(\"url\") for d in c.datasets for s in d.services if s.get(\"service\").lower() == \"opendap\"]<br>print urls<br>[<br>  'http://tds.maracoos.org/thredds/dodsC/MODIS-Agg.nc',<br>  'http://tds.maracoos.org/thredds/dodsC/MODIS-2009-Agg.nc',<br>  'http://tds.maracoos.org/thredds/dodsC/MODIS-2010-Agg.nc',<br>  'http://tds.maracoos.org/thredds/dodsC/MODIS-2011-Agg.nc',<br>  'http://tds.maracoos.org/thredds/dodsC/MODIS-2012-Agg.nc',<br>  'http://tds.maracoos.org/thredds/dodsC/MODIS-2013-Agg.nc',<br>  'http://tds.maracoos.org/thredds/dodsC/MODIS-One-Agg.nc',<br>  'http://tds.maracoos.org/thredds/dodsC/MODIS-Three-Agg.nc',<br>  'http://tds.maracoos.org/thredds/dodsC/MODIS-Seven-Agg.nc'<br>]<br>```<br><br>You can also obtain the dataset size.  This returns the size on disk if the informaton is available in the TDS<br>catalog.  If it is not available and a DAP endpoint is available, it returns the theoretical size of all of thh variables.<br>This isn't necessarialy the size on disk, because it does not account for `missing_value` and `_FillValue` space.<br><br>```python<br>from thredds_crawler.crawl import Crawl<br>c = Crawl(<br>  'http://thredds.axiomalaska.com/thredds/catalogs/cencoos.html',<br>  select=['MB_.*']<br>)<br>sizes = [d.size for d in c.datasets]<br>print sizes<br>[29247.410283999998, 72166.289680000002]<br>```<br><br><br>## Metadata<br><br>The entire THREDDS catalog metadata record is saved along with the dataset object.  It is an etree Element object ready for you to pull information out of.  See the [THREDDS metadata spec](http://www.unidata.ucar.edu/projects/THREDDS/tech/catalog/v1.0.2/InvCatalogSpec.html#metadata)<br><br>```python<br>from thredds_crawler.crawl import Crawl<br>c = Crawl('http://tds.maracoos.org/thredds/MODIS.xml', select=['.*-Agg'])<br>dataset = c.datasets[0]<br>print dataset.metadata.find(\"{http://www.unidata.ucar.edu/namespaces/thredds/InvCatalog/v1.0}documentation\").text<br>Ocean Color data are provided as a service to the broader community, and can be<br>influenced by sensor degradation and or algorithm changes. We make efforts to keep<br>this dataset updated and calibrated. The products in these files are experimental.<br>Aggregations are simple means of available data over the specified time frame. Use at<br>your own discretion.<br>```<br><br><br>## Use Case<br><br>Below is a python script that can be used to harvest THEDDS catalogs and save the ISO metadata files<br>to a local directory<br><br>```python<br>import os<br>import urllib<br>from thredds_crawler.crawl import Crawl<br><br>import logging<br>import logging.handlers<br>logger = logging.getLogger('thredds_crawler')<br>fh = logging.handlers.RotatingFileHandler('/var/log/iso_harvest/iso_harvest.log', maxBytes=1024*1024*10, backupCount=5)<br>fh.setLevel(logging.DEBUG)<br>ch = logging.StreamHandler()<br>ch.setLevel(logging.DEBUG)<br>formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')<br>fh.setFormatter(formatter)<br>ch.setFormatter(formatter)<br>logger.addHandler(fh)<br>logger.addHandler(ch)<br>logger.setLevel(logging.DEBUG)<br><br>SAVE_DIR=\"/srv/http/iso\"<br><br>THREDDS_SERVERS = {<br>    \"aoos\":      \"http://thredds.axiomalaska.com/thredds/catalogs/aoos.html\",<br>    \"cencoos\":   \"http://thredds.axiomalaska.com/thredds/catalogs/cencoos.html\",<br>    \"maracoos\" : \"http://tds.maracoos.org/thredds/catalog.html\",<br>    \"glos\":      \"http://tds.glos.us/thredds/catalog.html\"<br>}<br><br>for subfolder, thredds_url in THREDDS_SERVERS.items():<br>  logger.info(\"Crawling %s (%s)\" % (subfolder, thredds_url))<br>  crawler = Crawl(thredds_url, debug=True)<br>  isos = [(d.id, s.get(\"url\")) for d in crawler.datasets for s in d.services if s.get(\"service\").lower() == \"iso\"]<br>  filefolder = os.path.join(SAVE_DIR, subfolder)<br>  if not os.path.exists(filefolder):<br>    os.makedirs(filefolder)<br>  for iso in isos:<br>    try:<br>      filename = iso[0].replace(\"/\", \"_\") + \".iso.xml\"<br>      filepath = os.path.join(filefolder, filename)<br>      logger.info(\"Downloading/Saving %s\" % filepath)<br>      urllib.urlretrieve(iso[1], filepath)<br>    except BaseException:<br>      logger.exception(\"Error!\")<br>```<br><br><br>\n          </div>"}, "last_serial": 3936084, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "e3f1fb818e5032b7e487a2faae97e7cc", "sha256": "61eb658ce31422c3bfb446b744eed431403fb639e2721ff3ed273e8a53797684"}, "downloads": -1, "filename": "thredds_crawler-0.1.tar.gz", "has_sig": false, "md5_digest": "e3f1fb818e5032b7e487a2faae97e7cc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4652, "upload_time": "2013-07-25T17:26:26", "upload_time_iso_8601": "2013-07-25T17:26:26.223669Z", "url": "https://files.pythonhosted.org/packages/ce/d0/9e88cb555ea3fde631a04e7ccfd8e2041ab87d6044a5b3c38bbc9ac835fb/thredds_crawler-0.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "6d74ae7e9ad4ef07e3f2105d964be0b1", "sha256": "a86f4771f10b0c1e2f789f4e327521f652d97b73b0f60677cdb2ecb012fa6e18"}, "downloads": -1, "filename": "thredds_crawler-0.2.tar.gz", "has_sig": false, "md5_digest": "6d74ae7e9ad4ef07e3f2105d964be0b1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5070, "upload_time": "2013-07-25T17:50:21", "upload_time_iso_8601": "2013-07-25T17:50:21.221507Z", "url": "https://files.pythonhosted.org/packages/10/03/b94e5937c57570c5421d1efbbe97001422a3eddff547f985dd85fb834706/thredds_crawler-0.2.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "1604b78d3a84a59d7c0f6b83cd30f674", "sha256": "8c06be9fce064e5e9d857e4512b6087ff5c5e44ed89fd8e024f5a607539e1f00"}, "downloads": -1, "filename": "thredds_crawler-0.3.tar.gz", "has_sig": false, "md5_digest": "1604b78d3a84a59d7c0f6b83cd30f674", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5084, "upload_time": "2013-07-29T17:19:27", "upload_time_iso_8601": "2013-07-29T17:19:27.459507Z", "url": "https://files.pythonhosted.org/packages/3e/3e/2dc918fb5492f62f8cb00bd81101fdceda718346ef912e5a15fd943be97c/thredds_crawler-0.3.tar.gz", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "5f898895f6229cabdd0965761953ba2f", "sha256": "396d0bb73a8682f3ddc107b11fa6d9235b66a353185fc35b44c317370ad84818"}, "downloads": -1, "filename": "thredds_crawler-0.4.tar.gz", "has_sig": false, "md5_digest": "5f898895f6229cabdd0965761953ba2f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5090, "upload_time": "2013-08-02T17:43:15", "upload_time_iso_8601": "2013-08-02T17:43:15.143681Z", "url": "https://files.pythonhosted.org/packages/75/c7/8ff5ad88ba5e42f9c3c87978b28d2eefeb761a60196cc24b745e91ac78f1/thredds_crawler-0.4.tar.gz", "yanked": false}], "0.5": [{"comment_text": "", "digests": {"md5": "e03de54766811a0c5a96ea2a73ea3eb0", "sha256": "62579c15691e48880bd1c02c9a79deecce39c7a53a2180565bddfc04c68e3b58"}, "downloads": -1, "filename": "thredds_crawler-0.5.tar.gz", "has_sig": false, "md5_digest": "e03de54766811a0c5a96ea2a73ea3eb0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5269, "upload_time": "2013-08-08T16:51:07", "upload_time_iso_8601": "2013-08-08T16:51:07.204021Z", "url": "https://files.pythonhosted.org/packages/04/61/563c9354bec5ebd569c1953c1c471c2fa2f359f5b17f48fe220cbd0aa04c/thredds_crawler-0.5.tar.gz", "yanked": false}], "0.6": [{"comment_text": "", "digests": {"md5": "fd75a6446b6c0f0cab9e6eccc24a409f", "sha256": "93e8b24d74ecc996ad9f7104abb2fa029870bb7229a9583be9ad1b41bb7e4938"}, "downloads": -1, "filename": "thredds_crawler-0.6.tar.gz", "has_sig": false, "md5_digest": "fd75a6446b6c0f0cab9e6eccc24a409f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6470, "upload_time": "2014-05-16T15:21:54", "upload_time_iso_8601": "2014-05-16T15:21:54.675920Z", "url": "https://files.pythonhosted.org/packages/0f/53/3f7a1a5039ad71fb7ffcef970bf05f350c2f52c2bc1bdffb8f5f56b43430/thredds_crawler-0.6.tar.gz", "yanked": false}], "0.7": [{"comment_text": "", "digests": {"md5": "f6a815c2d3fdd1a1ef36c79aaf8730cb", "sha256": "c36046d7622291febf636b3f3d79dc64301e6a046cbae07ab88d0a934f8f5499"}, "downloads": -1, "filename": "thredds_crawler-0.7.tar.gz", "has_sig": false, "md5_digest": "f6a815c2d3fdd1a1ef36c79aaf8730cb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6994, "upload_time": "2014-10-28T20:40:25", "upload_time_iso_8601": "2014-10-28T20:40:25.579503Z", "url": "https://files.pythonhosted.org/packages/aa/eb/148cf1dd97e59e0d465345a563ee0205e61ff2003a281093937ed51698dd/thredds_crawler-0.7.tar.gz", "yanked": false}], "0.8": [{"comment_text": "", "digests": {"md5": "f9cba61069b4b1c40d31630253da1fb5", "sha256": "0bc1fb9405080e087ac15e513e97d92be09f2f9aeec1037c52eae008ee0e359d"}, "downloads": -1, "filename": "thredds_crawler-0.8.tar.gz", "has_sig": false, "md5_digest": "f9cba61069b4b1c40d31630253da1fb5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7286, "upload_time": "2014-11-03T15:38:36", "upload_time_iso_8601": "2014-11-03T15:38:36.552377Z", "url": "https://files.pythonhosted.org/packages/af/a1/785f6b43336a5fceec9941006ff2d835552eb39e9a1c295ea9b7d0684109/thredds_crawler-0.8.tar.gz", "yanked": false}], "0.9": [{"comment_text": "", "digests": {"md5": "19490219b0aba2b9b9c8d038a2c5c273", "sha256": "2728ba1d40ef0d7ca07019e351c481037d70e04da1a594bfee5a71b60ce7bddf"}, "downloads": -1, "filename": "thredds_crawler-0.9.tar.gz", "has_sig": false, "md5_digest": "19490219b0aba2b9b9c8d038a2c5c273", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7924, "upload_time": "2015-01-07T20:32:45", "upload_time_iso_8601": "2015-01-07T20:32:45.100809Z", "url": "https://files.pythonhosted.org/packages/d7/b5/c7ce8ea8680dab4641fd0a945b9b9c4b10b346a8d7ad1e8cdaa31f89020d/thredds_crawler-0.9.tar.gz", "yanked": false}], "1.0.0": [{"comment_text": "", "digests": {"md5": "0556daa86fe954639b7f50b024f92289", "sha256": "35877610d42f2c843e3635965173b09cb77b1eeafe3055f96e169565de20ee9b"}, "downloads": -1, "filename": "thredds_crawler-1.0.0.tar.gz", "has_sig": false, "md5_digest": "0556daa86fe954639b7f50b024f92289", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10236, "upload_time": "2015-03-20T13:48:12", "upload_time_iso_8601": "2015-03-20T13:48:12.560229Z", "url": "https://files.pythonhosted.org/packages/81/70/587fa1b3e8572e77ced93adb2c06fba8674be785e56096d07a155833cc15/thredds_crawler-1.0.0.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "81e499f791c7bcc8a01f95f94db01fef", "sha256": "34a620f1e598993a25fed18d00e0948d7de79870f620e9b1f129412642dce141"}, "downloads": -1, "filename": "thredds_crawler-1.1.0.tar.gz", "has_sig": false, "md5_digest": "81e499f791c7bcc8a01f95f94db01fef", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7841, "upload_time": "2015-10-26T14:49:42", "upload_time_iso_8601": "2015-10-26T14:49:42.106067Z", "url": "https://files.pythonhosted.org/packages/71/8a/64c14d604d26db30052ccc4f683cee5de8d6d7987c2d9afff2e61ed70e40/thredds_crawler-1.1.0.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "36e610681ed4023d26d24d3a55c03a6b", "sha256": "d3c6aa09f76ad8cfea3f3c794f3506da856d9ff6b868be7070307a94c2ec0432"}, "downloads": -1, "filename": "thredds_crawler-1.2.0.tar.gz", "has_sig": false, "md5_digest": "36e610681ed4023d26d24d3a55c03a6b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8645, "upload_time": "2016-02-09T22:00:55", "upload_time_iso_8601": "2016-02-09T22:00:55.822278Z", "url": "https://files.pythonhosted.org/packages/46/57/f4d66a71e12798c946e793d2814f2f3448a56438c741d5cf6200c89918dd/thredds_crawler-1.2.0.tar.gz", "yanked": false}], "1.3.0": [{"comment_text": "", "digests": {"md5": "0dcab4a4783e0567a8ed8cb0b776bcc4", "sha256": "258f3e0d73b0ee15f0c8a2c863b845990f8acaf7bb73b9b6909b669c9fd72edc"}, "downloads": -1, "filename": "thredds_crawler-1.3.0.tar.gz", "has_sig": false, "md5_digest": "0dcab4a4783e0567a8ed8cb0b776bcc4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9712, "upload_time": "2016-05-25T15:21:32", "upload_time_iso_8601": "2016-05-25T15:21:32.298723Z", "url": "https://files.pythonhosted.org/packages/ac/46/eb5367b1619259d87cb34c15b188be2bceb4a2c094b7c9dd2a539725801b/thredds_crawler-1.3.0.tar.gz", "yanked": false}], "1.4.0": [{"comment_text": "", "digests": {"md5": "74984e6e77314fd6b466c5e6f853c9fe", "sha256": "d19cea6399e314ed43c6110ba8181bb782e581e1c9f06080782aae5ebb7683eb"}, "downloads": -1, "filename": "thredds_crawler-1.4.0.tar.gz", "has_sig": false, "md5_digest": "74984e6e77314fd6b466c5e6f853c9fe", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21794, "upload_time": "2016-05-25T15:41:41", "upload_time_iso_8601": "2016-05-25T15:41:41.742979Z", "url": "https://files.pythonhosted.org/packages/ef/72/cd479deb55378e96ce3a354e7815e66fa630ead3dd1daa272b7dea0f736f/thredds_crawler-1.4.0.tar.gz", "yanked": false}], "1.5.0": [{"comment_text": "", "digests": {"md5": "26e51efc904b3c6e05ff611e601715af", "sha256": "a879cad31bd55a5dcbde2fe1ff269a5aed77e0a317c0eb0c4e359ed24bd7537d"}, "downloads": -1, "filename": "thredds_crawler-1.5.0.tar.gz", "has_sig": false, "md5_digest": "26e51efc904b3c6e05ff611e601715af", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21876, "upload_time": "2016-07-21T13:43:12", "upload_time_iso_8601": "2016-07-21T13:43:12.463696Z", "url": "https://files.pythonhosted.org/packages/a8/00/bcebf21e1d5bd2f019b3441ed3bcdd501c85f920a04f7155ca0dade03a33/thredds_crawler-1.5.0.tar.gz", "yanked": false}], "1.5.1": [{"comment_text": "", "digests": {"md5": "2185177376e8252e3122c303668ecf06", "sha256": "aff58803a8cc7a8c2201dff9c32a7848ced7ee8ff5fc1d86635a415cc2c3c83e"}, "downloads": -1, "filename": "thredds_crawler-1.5.1.tar.gz", "has_sig": false, "md5_digest": "2185177376e8252e3122c303668ecf06", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13255, "upload_time": "2016-09-28T12:36:54", "upload_time_iso_8601": "2016-09-28T12:36:54.631007Z", "url": "https://files.pythonhosted.org/packages/3b/0a/f4302e29459b873cc268e6239dcf0548ce0896f1c31eea6c714731355a78/thredds_crawler-1.5.1.tar.gz", "yanked": false}], "1.5.2": [{"comment_text": "", "digests": {"md5": "82202453f4e8b7e37d03c349144c2f57", "sha256": "55f7b46cdc8b0267235c0e19946a8ca8e35d953843e3b2bf0ed2da662f54545b"}, "downloads": -1, "filename": "thredds_crawler-1.5.2.tar.gz", "has_sig": false, "md5_digest": "82202453f4e8b7e37d03c349144c2f57", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11044, "upload_time": "2016-10-11T17:50:50", "upload_time_iso_8601": "2016-10-11T17:50:50.397358Z", "url": "https://files.pythonhosted.org/packages/25/71/302f479e57905a707c1d4c22551c85f9d58113441e9a2afcf9c967a0c957/thredds_crawler-1.5.2.tar.gz", "yanked": false}], "1.5.3": [{"comment_text": "", "digests": {"md5": "212c96545b8ab8af1ab0343683d1b073", "sha256": "f58871bd23501230e979e93f1815a79e62348f616008fbc48e51f4dfd0dbda66"}, "downloads": -1, "filename": "thredds_crawler-1.5.3.tar.gz", "has_sig": false, "md5_digest": "212c96545b8ab8af1ab0343683d1b073", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11145, "upload_time": "2016-12-06T15:47:00", "upload_time_iso_8601": "2016-12-06T15:47:00.936390Z", "url": "https://files.pythonhosted.org/packages/89/02/e860711cb3620d9c237f3eb257115db7c3e214f5bca1218c6d18742f57de/thredds_crawler-1.5.3.tar.gz", "yanked": false}], "1.5.4": [{"comment_text": "", "digests": {"md5": "f8fc3388aa79f0e841820c5b8fd7973f", "sha256": "ca184551e0176b7c29647743066e395962ff7dc393124f4def7d6fc2763ceb51"}, "downloads": -1, "filename": "thredds_crawler-1.5.4-py3-none-any.whl", "has_sig": false, "md5_digest": "f8fc3388aa79f0e841820c5b8fd7973f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10867, "upload_time": "2018-06-06T14:01:52", "upload_time_iso_8601": "2018-06-06T14:01:52.311953Z", "url": "https://files.pythonhosted.org/packages/98/88/8cc14bf9d9581268ef7f98e29f2add238517d987646ff22ec296e7e7d062/thredds_crawler-1.5.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "59247b3b176345e531a8956d505eb05e", "sha256": "68c7440723269d23d1c98fcb138cee6a29a68cdc552c1014811085b794e0379e"}, "downloads": -1, "filename": "thredds_crawler-1.5.4.tar.gz", "has_sig": false, "md5_digest": "59247b3b176345e531a8956d505eb05e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13130, "upload_time": "2018-06-06T14:01:53", "upload_time_iso_8601": "2018-06-06T14:01:53.669251Z", "url": "https://files.pythonhosted.org/packages/61/b3/6bf1c08b0dd847cadd21b2aa0f692af8d8b13013c7a0a8df6a810ffd4684/thredds_crawler-1.5.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f8fc3388aa79f0e841820c5b8fd7973f", "sha256": "ca184551e0176b7c29647743066e395962ff7dc393124f4def7d6fc2763ceb51"}, "downloads": -1, "filename": "thredds_crawler-1.5.4-py3-none-any.whl", "has_sig": false, "md5_digest": "f8fc3388aa79f0e841820c5b8fd7973f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10867, "upload_time": "2018-06-06T14:01:52", "upload_time_iso_8601": "2018-06-06T14:01:52.311953Z", "url": "https://files.pythonhosted.org/packages/98/88/8cc14bf9d9581268ef7f98e29f2add238517d987646ff22ec296e7e7d062/thredds_crawler-1.5.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "59247b3b176345e531a8956d505eb05e", "sha256": "68c7440723269d23d1c98fcb138cee6a29a68cdc552c1014811085b794e0379e"}, "downloads": -1, "filename": "thredds_crawler-1.5.4.tar.gz", "has_sig": false, "md5_digest": "59247b3b176345e531a8956d505eb05e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13130, "upload_time": "2018-06-06T14:01:53", "upload_time_iso_8601": "2018-06-06T14:01:53.669251Z", "url": "https://files.pythonhosted.org/packages/61/b3/6bf1c08b0dd847cadd21b2aa0f692af8d8b13013c7a0a8df6a810ffd4684/thredds_crawler-1.5.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:53:44 2020"}