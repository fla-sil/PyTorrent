{"info": {"author": "Michael Hirsch, Ph.D.", "author_email": "scivision@users.noreply.github.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Intended Audience :: Science/Research", "Operating System :: OS Independent", "Programming Language :: Fortran", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Topic :: Scientific/Engineering :: Atmospheric Science"], "description": "# GEMINI\n\n[![DOI](https://zenodo.org/badge/146920930.svg)](https://zenodo.org/badge/latestdoi/146920930)\n\n![ci_python](https://github.com/gemini3d/gemini/workflows/ci_python/badge.svg)\n![ci_debug_build](https://github.com/gemini3d/gemini/workflows/ci_debug_build/badge.svg)\n![ci_linux](https://github.com/gemini3d/gemini/workflows/ci_linux/badge.svg)\n![ci_macos](https://github.com/gemini3d/gemini/workflows/ci_macos/badge.svg)\n\nThe GEMINI model (*G*eospace *E*nvironment *M*odel of *I*on-*N*eutral *I*nteractions) is a three-dimensional ionospheric fluid-electrodynamic model used for various scientific studies including effects of auroras on the terrestrial ionosphere, natural hazard effects on the space environment, and effects of ionospheric fluid instabilities on radio propagation (see references section of this document for details).\nThe detailed mathematical formulation of GEMINI is included in `doc/`.\nA subroutine-level set of documentation describing functions of individual program units is given via source code comments which are\n[rendered as webpages](https://gemini3d.github.io/gemini/index.html).\nGEMINI uses generalized orthogonal curvilinear coordinates and has been tested with dipole and Cartesian coordinates.\n\nPlease open a\n[GitHub Issue](https://github.com/gemini3d/gemini/issues)\nif you experience difficulty building GEMINI.\n\nGenerally, the Git `master` branch has the current development version and is the best place to start, while more thoroughly-tested releases happen occasionally.\nSpecific commits corresponding to published results will also be noted, where appropriate, in the corresponding journal article.\n\n## Quickest start\n\nGemini can run on most modern computers using Linux, MacOS, or Windows.\nGemini even runs on the Raspberry Pi 4 with 1 GByte of RAM.\nWindows users should use\n[Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/install-win10).\nTo build Gemini and run self-tests takes about 10 minutes on a typical laptop.\n\nRequirements:\n\n* Fortran 2008 compiler (Gfortran &ge; 6 or Intel `ifort`)\n  * MacOS / [Homebrew](https://brew.sh): `brew install gcc`\n  * Ubuntu / Debian / Windows Subsystem for Linux: `apt install gfortran`\n  * CentOS: `yum install gcc-gfortran`\n* OpenMPI or IntelMPI\n  * MacOS / [Homebrew](https://brew.sh): `brew install openmpi`\n  * Ubuntu / Debian / Windows Subsystem for Linux: `apt install libopenmpi-dev openmpi-bin`\n  * CentOS: `yum install openmpi-devel`\n* [CMake &ge; 3.14](https://cmake.org/download/)\n* [Python &ge; 3.6](https://docs.conda.io/en/latest/miniconda.html)\n\n1. get the Gemini code\n\n  ```sh\n  git clone https://github.com/gemini3d/gemini\n\n  cd gemini\n  ```\n2. Setup Gemini and prereqs\n\n  ```sh\n  python3 setup.py develop --user\n  ```\n3. Build Gemini\n\n  ```sh\n  cmake -B build\n\n  cmake --build build -j\n\n  cd build\n\n  ctest --output-on-failure\n  ```\n\n## Prerequisites\n\nGemini uses CMake build system to automatically build the entire software library stack,\nchecking for compatibility of pre-installed libraries such as Lapack, Scalapack and MUMPS.\nGemini scripts used to load and check data use Python.\n\n### Compilers\n\nThe object-oriented Fortran 2008 code used in GEMINI requires a Fortran compiler that handles Fortran 2008 syntax including\n\n* submodule\n* block\n\nMost currently maintained compilers can do this.\nGfortran and Intel Fortran are two compilers known to work easily with GEMINI.\n\n#### Gfortran\n\nGfortran is the recommended compiler to run Gemini, as most libraries are already compiled for it.\nGfortran 6 or newer works well.\nWe regularly work with Gfortran 7 and 9 on MacOS and Linux (CentOS / Ubuntu)\nDue to OpenMPI not supporting native Windows, Gfortran is usable on Windows via Windows Subsystem for Linux.\n\n#### Intel Fortran\n\nIntel Fortran `ifort` for Linux--need to have \"Parallel Studio XE\" so that it has MPI.\nIfort versions 19.0.5 and 19.1.0 are among the versions known to work with Gemini.\nIntel Fortran for Linux\n[currently supported versions](https://software.intel.com/en-us/articles/intel-parallel-studio-xe-supported-and-unsupported-product-versions)\nare generally targeted for Gemini support.\n\nIntel Fortran for Windows is in general a tricky compiler to work with, and it is not supported for Gemini.\n\n### Libraries\n\nTested versions include:\n\n* OpenMPI 1.10, 2.1 - 4.0  |  IntelMPI 19.0, 19.1\n* MUMPS 4.10, 5.1.2, 5.2.1\n  * Mumps &ge; 5.2 recommended to have vastly less verbose console output\n* SCALAPACK 2.0 / 2.1\n* LAPACK95 3.0  (optional)\n* HDF5 1.8.16 / 1.10 / 1.12\n\n### build-time options\n\nbuild-time options in general are enabled or disabled like\n\n```sh\ncmake -B build -Doption=true\n\ncmake -B build -Doption=false\n```\n\nNCAR GLOW is automatically installed, but optional in general.\nAuroral emissions use GLOW.\nEnable or disable GLOW with `-Dglow=` option\n\nBy default, Python and/or GNU Octave are used for the self-tests.\nMatlab is considerably slower, but may be enabled for self-tests with `-Dmatlab=enabled`\n\nHDF5 is enabled / disabled by `-Dhdf5=` option\n\n### Analysis of simulation output\n\nGEMINI uses Python for essential interfaces, plotting and analysis.\nA lot of legacy analysis was done with Matlab scripts that we continue to maintain.\n\nThe Matlab .m scripts generally require Matlab &ge; R2019a and are being transitioned to Python.\nOnly the essential scripts needed to setup a simple example, and plot the results are included in the main GEMINI respository.\nThe [Gemini-scripts](https://github.com/gemini3d/GEMINI-scripts) and\n[Gemini-examples](https://github.com/gemini3d/GEMINI-examples)\ncontain scripts used for various published and ongoing analyses.\n\n### Document generation\n\nThe documentation is Markdown-based, using FORD. If source code or documentation edits are made, the documentation is regenerated from the top-level gemini directory by:\n\n```sh\nford ford.md\n```\n\nThe output is under `docs/` and upon `git push` will appear at the [GEMINI docs website](https://mattzett.github.io/gemini/index.html).\n\nFORD is a Python program, installed via:\n\n```sh\npip install ford\n```\n\nNote: leave the LaTeX files and other non-autogenerated documents in `doc/` so they don't get disturbed by FORD.\n\n## License\n\nGEMINI is distributed under the Apache 2.0 license.\n\n## Suggested hardware\n\nGEMINI can run on hardware ranging from a modest laptop to a high-performance computing (HPC) cluster.\n\nFor large 3D simulations (more than 20M grid points), GEMINI should be run in a cluster environment or a \"large\" multicore workstation (e.g. 12 or more cores).  Runtime depends heavily on the grid spacing used, which determines the time step needed to insure stability,  For example we have found that a 20M grid point simulations takes about  4 hours on 72 Xeon E5 cores.  200M grid point simulations can take up to a week on 256 cores.  It has generally been found that acceptable performance requires > 1GB memory per core; moreover, a large amount of storage (hundreds of GB to several TB) is needed to store results from large simulations.\n\nOne could run large 2D or very small 3D simulations (not exceeding a few million grid points) on a quad-core workstation, but may take quite a while to complete.\n\n### Compiler configure\n\nIf using Homebrew, be sure Homebrew's GCC is used instead of AppleClang or other non-Homebrew compilers so that the Homebrew library ABIs match the compiler ABI.\n\n```sh\nFC=gfortran-9 CC=gcc-9 cmake -B build\n```\n\nIf you need to specify MPI compiler wrappers, do like:\n\n```sh\ncmake -B build -DMPI_ROOT=~/lib_gcc/openmpi-3.1.4\n```\n\n### input directory\n\nThe example `config.ini` in `initialize/` looks for input grid data in `tests/data`.\nIf you plan to push back to the repository, please don't edit those example `.ini` file paths, instead use softlinks `ln -s` to point somewhere else if needed.\nNote that any `config.ini` you create yourself in `initialize/` will not be included in the repository since that directory is in `.gitignore` (viz. not tracked by git).\n\n#### MUMPS verbosity\n\nMUMPS initialization ICNTL flags are set in `numerical/potential/potential_mumps.f90`.\nICNTL 1-4 concern print output unit and verbosity level, see MUMPS\n[User Manual](http://mumps.enseeiht.fr/index.php?page=doc)\n\n#### Build tips\n\nLibraries are auto-built by Gemini when building gemini.bin.\nIf it's desired to use system libraries, consider [install_prereqs.py](./scripts/install_prereqs.py)\n\n### self-tests\n\nGEMINI has self tests that compare the output from a \"known\" test problem to a reference output.\nUse `ctest -V` to reveal the commands being sent if you want to run them manually.\n\n### OS-specific tips\n\nCMake will automatically download and build necessary libraries for Gemini.\n\n* Ubuntu: tested on Ubuntu 18.04 / 16.04\n* CentOS 7\n\n    ```sh\n    module load git cmake openmpi\n\n    module load gcc\n    export CC=gcc CXX=g++ FC=gfortran\n    ```\n* MacOS / Linux: use Homebrew to install Gfortran and OpenMPI like:\n\n    ```sh\n    brew install cmake gcc openmpi lapack scalapack\n    ```\n\n## Known limitations and issues of GEMINI\n\n1. Generating equilibrium conditions can be a bit tricky with curvilinear grids.  A low-res run can be done, but it will not necessary interpolate properly onto a finer grid due to some issue with the way the grids are made with ghost cells etc.  A workaround is to use a slightly narrower (x2) grid in the high-res run (quarter of a degree seems to work most of the time).\n2. Magnetic field calculations on an open 2D grid do not appear completely consistent with MATLAB model prototype results; although there are quite close.  This may have been related to sign errors in the FAC calculations - these tests should be retried at some point.\n3. Occasionally MUMPS will throw an error because it underestimated the amount of memory needed for a solve.  If this happens a workaround is to add this line of code to the potential solver being used for your simulations.  If the problem persists try changing the number to 100.\n\n    ```fortran\n    mumps_par%ICNTL(14)=50\n    ```\n4. There are potentially some issues with the way the stability condition is evaluated, i.e. it is computed before the perp. drifts are solved so it is possible when using input data to overrun this especially if your target CFL number is &gt; 0.8 or so.  Some code has been added as of 8/20/2018 to throttle how much dt is allowed to change between time steps and this seems to completely fix this issue, but theoretically it could still happen; however this is probably very unlikely.\n5. Occasionally one will see edge artifacts in either the field -aligned currents or other parameters for non-periodic in x3 solves.  This may be related to the divergence calculations needed for the parallel current (under EFL formulation) and for compression calculations in the multifluid module, but this needs to be investigated further...  This do not appear to affect solutions in the interior of the grid domain and can probably be safely ignored if your region of interest is sufficiently far from the boundary (which is alway good practice anyway).\n\n## To do list\n\nSee [TODO.md](./TODO.md).\n\n## Standard and style\n\nGEMINI is Fortran 2008 compliant and uses two-space indents throughout (to accommodate the many, deeply nested loops).  Some of our developers are avid VIM users so please do not use tabs if you plan to push back to the repository or merge.\n\n## Debug text\n\nThe gemini.bin command line option `-d` or `-debug` prints a large amount to text to console, perhaps gigabytes worth for medium simulations. By default, only the current simulation time and a few other messages are shown.\n\n## Manually set number of MPI processes\n\n```sh\nmpiexec -np <number of processors>  build/gemini.bin <input config file> <output directory>\n```\n\nfor example:\n\n```sh\nmpiexec -np 4 build/gemini.bin initialize/2Dtest/config.ini /tmp/2d\n```\n\nNote that the output *base* directory must already exist (e.g. `/tmp/2d`).\nThe source code consists of about ten module source files encapsulating various functionalities used in the model.\nA diagram all of the modules and their function is shown in figure 1.\nCMake can generate a graphical [dependency graph](https://cmake.org/cmake/help/latest/module/CMakeGraphVizOptions.html).\n\nTwo of the log files created are:\n\n* gitrev.log: the Git branch and hash revision at runtime (changed filenames beyond this revision are logged)\n* compiler.log: compiler name, version and options used to compile the executable.\n\n![Figure 1](doc/figure1.png)\n\n<!-- ![Figure 2](doc/figure2.png) -->\n\n## Auxiliary fortran program\n\nNote that there is also a utility that can compute magnetic fields from the currents calculated by GEMINI.\nThis can be run by:\n\n```sh\nmpirun -np 4 ./magcalc /tmp/3d tests/data/test3d/input/magfieldpoints.dat\n```\n\nThis will compute magnetic fields over a grid at ground level using currents computed from the 3Dtest simulation.  In order to run this program, you will need to create a set of field points at which the magnetic perturbations will be calculated.  For example, this could be a list of ground stations, a regular mesh, or a set of satellite tracks.\n\n## Number of MPI processes\n\nIn general for MPI programs and associated simulations, there may be a minimum number of MPI processes and/or integer multiples that must be met (for example, number of processes must be factor of 4).\nThe build system generation process automatically sets the maximum number of processes possible based on your CPU core count and grid size.\n\n## Input file format\n\nEach simulation needs an input file that specifies location of initial conditions and other pertinent information for the simulation.  A coupled of very basic examples of these are included in the ./initialize directory; each subdirectory is a separate example usage of GEMINI for a particular problem.  The basic template for an input file (config.ini) file follows (please note that most use cases will not have all options activated as this example does).\n\n```\n16,9,2015                             !dmy:  day,month,year\n82473.0                               !UTsec0:  start time, UT seconds\n1800.0                                !tdur:  duration of simulation in seconds\n15.0                                  !dtout: how often (s) to do output\n109.0,109.0,5.0                       !activ:  f107a,f107,Ap (81 day averaged f10.7, daily f10.7, and average Ap index)\n0.9                                   !tcfl:  target cfl number (dimensionless - must be less than 1.0 to insure stability)\n1500.0                                !Teinf:  exospheric electron temperature, K (only used in open-grid simulations)\n0                             \t  !potsolve:  are we solving potential? (0=no; 1=-yes)\n0                                     !flagperiodic:  do we interpret the grid as being periodic in the x3-direction?  (0=no; 1=yes)\n2                                     !flagoutput:  what type of output do we do?  (2=ISR-like species-averaged plasma parameters; 3=electron density only; anything else nonzero=full output)\n0                                     !flagcapacitance:  include inertial capacitance? (0=no; 1=yes; 2=yes+m'spheric contribution)\n../simulations/input/chile20153D_0.5_medhighres/chile20153D_0.5_medhighres_simsize.dat\n../simulations/input/chile20153D_0.5_medhighres/chile20153D_0.5_medhighres_simgrid.dat\n../simulations/input/chile20153D_0.5_medhighres/chile20153D_0.5_medhighres_ICs.dat\n1                                     !are we applying neutral perturbations? (0=no; 1=yes).  If 0, the next five entries are skipped while reading this input file\n1                                     !how doe we interpret the input neutral file geometry?  (0=Cartesian; anything else=axisymmetric)\n-20.5706d0,359.4048d0                 !source mlat,mlon of disturbance (degrees magnetic lat,lon)\n4d0                                   !time step between neutral input files\n2d3,2d3                               !spatial resolutions in radial and vertical directions\n../simulations/chile2015_neutrals/\n1                                     !flagprecfileinput:  for precipitation file input (0=no; 1=yes).  If 0, then next two entries of input file are skipped\n1.0                                   !dtprec:  time (s) between precipitation input files\n../simulations/isinglass_precipitation/\n1                                     !flagE0fileinput:  flag for electric field file input (0-no; 1=yes).  If 0, next two entries of input file are skipped\n10.0                                  !dtE0:  time (s) between electric field input files\n../simulations/isinglass_fields/\n```\n\nA large number of examples (in addition to those included in the main repo) are included in the GEMINI-script repository.\n\n## Running with different boundary and initial conditions:\n\nGEMINI requires both initial and boundary conditions to run properly.  Specifically the user must provide a complete initial ionospheric state (density, drift, and temperature for all ionospheric species), along with boundary conditions for the electric potential (in 2D this are the top, bottom, and side potentials; in 3D the topside current density and side wave potentials).  Fluid state variables are given free-flow boundary conditions at the edges of the simulation grid.  The `io` module contains code dealing with input of initial state from file and the `potential_comm` and `potentialBCs_mumps` modules contains contains code dealing with boundary condition input.\n\nThere are presently two ways in which the boundary and initial conditions can be set for GEMINI:  subroutine-based input and file-based input.\n\nPLEASE NOTE that future releases will use Fortran 2008 `submodule`, likely completely removing the option for subroutine-based initial and boundary conditions.\n\n### Subroutine-based input (*not recommended* and to be deprecated in a future release):\n\nThere are two subroutines that can be modified by the user to provide boundary conditions to the code; these are described below. Note that, if any of these are changed, the code needs to be recompiled.\n\n`./ionization/boundary_conditions/precipBCs_mod.f90` - the function `precipBCs' specifies the pattern of electron precipitation, including characteristic energy and total energy flux, over top of grid.  If the user does not specify an input file for precipitation boundary conditions in `config.ini`, then this subroutine will be called to set the boundary.\n\n`./numerical/potential/boundary_conditions/potentialBCs_mumps.f90` - boundary conditions for the electric potential or field-aligned current.  The type of input that is being used is specified by the flags in the `config.ini` file for the simulation.  This subroutine will only be called if the user has not specified an input file containing boundary conditions.\n\nBy default these subroutines will be used for boundary conditions if file input is not specified in the config.ini input file.  The base GEMINI sets these to be zero potential (or current) and some negligible amount of precipitation.  Note that if you write over these subroutines then the code will use whatever you have put into them if file input is not specified.  This can lead to unintended behavior if ones modifies these and then forgets since the code will continue to use the modifications instead of some baseline.  Because of this issue, and the fact that GEMINI must be rebuilt every time these subroutines are changed, this method of boudnary condition input is going to be removed.\n\n### File-based input (*recommended*)\n\nAn alternative is to use the file input option, which needs to be set up using MATLAB (or other) scripts.  To enable this type of input, the appropriate flags (flagprecfileinput and flagE0fileinput) need to be set in the input `config.ini` file (see Section entitled \"Input file format\" above).  All examples included in `initialize/` in both the GEMINI and GEMINI-scripts repositories use this method for setting boundary conditions.  Note that the user can specify the boundary condition on a different grid from what the simulation is to be run with; in this case GEMINI will just interpolate the given boundary data onto the current simulation grid.\n\n### Initial conditions\n\nGEMINI needs density, drift, and temperature for each species that it simulations over the entire grid for which the simulation is being run as input.  Generally one will use the results of another GEMINI simulation that has been initialized in an arbitrary way but run for a full day to a proper ionospheric equilibrium as this input.  Any equilibrium simulation run this way must use full output (flagoutput=1 in the `config.ini`).  A useful approach for these equilibrium runs is to use a coarser grid so that the simulation completes quickly and then interpolate the results up to fine grid resolution.  An example of an equilibrium setup is given in `./initialize/2Dtest_eq`; note that this basically makes up an initial conditions (using `eqICs.m`) and runs until initial transients have settled.  An example of a script that interpolates the output of an equilibrium run to a finer grid is included with `./initialize/2Dtest`.\n\n## Running one of the premade examples\n\nCurrently the main repo only includes the very basic 2Dtest and 3Dtest examples\n\n## Creating a simulation\n\n1)  Create initial conditions for equilibrium simulation -  Several examples of equilibrium setups are included in the ./initialize directory; these end with `_eq`.  These are all based off of the general scripts `./setup/model_setup.m` and related scripts.  In general this equilbrium simulation will set the date, location, and geomagnetic conditions for the background ionospheric state for later perturbation simulations.\n2)  Run an equilibrium simulation at low resolution to obtain a background ionosphere.  See examples in ./initialize ending in `_eq`\n3)  Generate a grid - Several examples of grid generation scripts adapted to particular problems are given in the `initialize/` directory of the repo (see list above for an example).  In particular, for 2Dtest and 3Dtest there is a script that reads in an equilbirum simulation, creates a fine resolution mesh, and then interpolates the equilibrium data onto that fine mesh.\n4)  Interpolate the equilibrium results on to a high resolution grid and create new input files for full resolution - See examples in the ./initialize/ directories not ending in `_eq`.  These are all based off of the general `./setup/model_setup_interp.m` script.\n5)  Set up boundary conditions for potential, if required - see section of this document on boundary conditions\n6)  Set up precipitation boundary conditions, if required -  see section of this document on boundary conditions\n7)  Recompile the code with make *only if you are using subroutine based input and boundary conditions* (please note that this functionality will be removed in a later release).  If you are using file-based input then a rebuild is not necessary (this is another benefit of using file-based input)\n8)  Run your new simulation\n\n## Running in two dimensions\n\nThe code determines 2D vs. 3D runs by the number of x2 or x3 grid points specified in the `config.ini` input file.  If the number of x2 grid points is 1, then a 2D run is executed (since message passing in the x3 direction will work normally).  If the number of x3 grid points is 1, the simulation will swap array dimensions and vector components between the x2 and x3 directions so that message passing parallelization still provides performance benefits.  The data will be swapped again before output so that the output files are structured normally and the user who is not modifying the source code need not concern themselves with this reordering.\n\n## Loading and plotting output\n\nMATLAB is required to load the output file via scripts in the ./vis directory (these scripts generally work on both 2D and 3D simulation results).\nGNU Octave is not reliable at plotting in general, and might crash or make wierd looking plots.\nThe results for an entire simulation can be plotted using [plotall.m](./vis/plotall.m)\n\n```matlab\nplotall('/tmp/mysim')\n```\n\nThese also illustrates how to read in a sequence of files from a simulation.  This script prints a copy of the output plots into the simulation output directory.  Finer-level output control can be achieve by using the 'plotframe.m' and 'loadframe.m' scripts to plot and load data from individual simulation output frames, respectively.\n\nThe particular format of the output files is specified by the user in the input config.ini file.  There are three options:\n1)  full output - output all state variables; very large file sizes will results, but this is required for building initial conditions and for some analysis that require detailed composition and temperature information.\n2)  average state parameter output - species averaged temperature and velocity; electron density.  Probably best for most uses\n3)  density only output - only electron density output.  Best for high-res instability runs where only the density is needed and the output cadence is high\n\nThe organization of the data in the Matlab workspace, after a single frame is loaded (via 'loadframe.m'), is as follows (MKSA units throughout):\n\n### Time variables:\n\nsimdate - a six element vector containing year, month, day, UT hour, UT minute, and UT seconds of the present frame\n\n### Grid variables:\n\n<!--x1,x2,x3 - x1 is altitude (z in plots), x2 is east (x in plots), x3 north (y in plots); the sizes of these variables are stored in lxs by the MATLAB script.-->\n\nstructure xg - members xg.x1,2,3 are the position variables, `xg.h*` are the metric factors, `xg.dx*` are the finite differences,\n\nxg.glat,glon are the latitudes and longitudes (degrees geographic) of each grid point, xg.alt is the altitude of each grid point.\n\nxg.r,theta,phi - for each grid point:  radial distance (from ctr of Earth), magnetic colatitude (rads.), and magnetic longitude (rads.)\n\nThe grid structure, by itself, can be read in by the MATLAB function 'readgrid.m'; this is automatically invoked with 'loadframe.m' so there is not need to separately load the grid and output frame data.\n\n### Temperature variable:\n\nTs (first three dimensions have size lxs; 4th dimension is species index:  1=O+,2=NO+,3=N2+,4=O2+,5=N+, 6=H+,7=e-)\n\n### Density variable:\n\nns (same indexing as temperature)\n\n### Drifts:\n\nvs1 (same indexing as temperature)\n\nx2-drift component:  v2 (same for all species, so this is just size lxs and is a 3D array)\nx3-drift component:  v3\n\n### Electromagnetic variables:\n\ncurrent density:  J1, J2, J3\npotential:  Phitop (EFL potential)\n\nNote that the electric field is not included in the output file, but that it can be calculated from this output by taking -vxB at an altitude above about 200 km or by differentiating the top boundary electric potential 'Phitop' with respect to the x2 and x3 variables; however, note that if a curvilinear grid is used the derivatives must include the appropriate metric factors.\n\n## Computing total electron content (TEC)\n\nTEC and magnetic field variations can be calculated as a post-processing step in which the simulation data are read in and interpolated onto a regular geographic grid and then integrated accordingly using scripts in the './vis' directory - see `TECcalc.m`.\nAn example of how to plot TEC computed by this script is included in `TECplot_map.m` (requires MATLAB mapping toolbox).\n\n## Visualizing magnetic field perturbations computed by magcalc.f90\n\nThe example script `magplot_fort_map.m` shows an example of how to load the results of running magcalc.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/gemini3d/gemini", "keywords": "mesosphere,stratosphere,thermosphere", "license": "", "maintainer": "", "maintainer_email": "", "name": "gemini3d", "package_url": "https://pypi.org/project/gemini3d/", "platform": "", "project_url": "https://pypi.org/project/gemini3d/", "project_urls": {"Homepage": "https://github.com/gemini3d/gemini"}, "release_url": "https://pypi.org/project/gemini3d/0.3.1/", "requires_dist": null, "requires_python": ">=3.6", "summary": "3-D ionospheric model plotting suite", "version": "0.3.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>GEMINI</h1>\n<p><a href=\"https://zenodo.org/badge/latestdoi/146920930\" rel=\"nofollow\"><img alt=\"DOI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c8c12b3ade47dd8db4b340e6e6526027bc5fa1fd/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3134363932303933302e737667\"></a></p>\n<p><img alt=\"ci_python\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bbef2f646eb0b01f3dbe406dada3b255c5aa8d30/68747470733a2f2f6769746875622e636f6d2f67656d696e6933642f67656d696e692f776f726b666c6f77732f63695f707974686f6e2f62616467652e737667\">\n<img alt=\"ci_debug_build\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/22449049b6db4d06316ac5de067d54e3d28417ea/68747470733a2f2f6769746875622e636f6d2f67656d696e6933642f67656d696e692f776f726b666c6f77732f63695f64656275675f6275696c642f62616467652e737667\">\n<img alt=\"ci_linux\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6910955781ed22f173415198e4f4b28529c36adb/68747470733a2f2f6769746875622e636f6d2f67656d696e6933642f67656d696e692f776f726b666c6f77732f63695f6c696e75782f62616467652e737667\">\n<img alt=\"ci_macos\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ee45d22fab07b4dcc2b7d82fcb7a2c542a055482/68747470733a2f2f6769746875622e636f6d2f67656d696e6933642f67656d696e692f776f726b666c6f77732f63695f6d61636f732f62616467652e737667\"></p>\n<p>The GEMINI model (<em>G</em>eospace <em>E</em>nvironment <em>M</em>odel of <em>I</em>on-<em>N</em>eutral <em>I</em>nteractions) is a three-dimensional ionospheric fluid-electrodynamic model used for various scientific studies including effects of auroras on the terrestrial ionosphere, natural hazard effects on the space environment, and effects of ionospheric fluid instabilities on radio propagation (see references section of this document for details).\nThe detailed mathematical formulation of GEMINI is included in <code>doc/</code>.\nA subroutine-level set of documentation describing functions of individual program units is given via source code comments which are\n<a href=\"https://gemini3d.github.io/gemini/index.html\" rel=\"nofollow\">rendered as webpages</a>.\nGEMINI uses generalized orthogonal curvilinear coordinates and has been tested with dipole and Cartesian coordinates.</p>\n<p>Please open a\n<a href=\"https://github.com/gemini3d/gemini/issues\" rel=\"nofollow\">GitHub Issue</a>\nif you experience difficulty building GEMINI.</p>\n<p>Generally, the Git <code>master</code> branch has the current development version and is the best place to start, while more thoroughly-tested releases happen occasionally.\nSpecific commits corresponding to published results will also be noted, where appropriate, in the corresponding journal article.</p>\n<h2>Quickest start</h2>\n<p>Gemini can run on most modern computers using Linux, MacOS, or Windows.\nGemini even runs on the Raspberry Pi 4 with 1 GByte of RAM.\nWindows users should use\n<a href=\"https://docs.microsoft.com/en-us/windows/wsl/install-win10\" rel=\"nofollow\">Windows Subsystem for Linux</a>.\nTo build Gemini and run self-tests takes about 10 minutes on a typical laptop.</p>\n<p>Requirements:</p>\n<ul>\n<li>Fortran 2008 compiler (Gfortran \u2265 6 or Intel <code>ifort</code>)\n<ul>\n<li>MacOS / <a href=\"https://brew.sh\" rel=\"nofollow\">Homebrew</a>: <code>brew install gcc</code></li>\n<li>Ubuntu / Debian / Windows Subsystem for Linux: <code>apt install gfortran</code></li>\n<li>CentOS: <code>yum install gcc-gfortran</code></li>\n</ul>\n</li>\n<li>OpenMPI or IntelMPI\n<ul>\n<li>MacOS / <a href=\"https://brew.sh\" rel=\"nofollow\">Homebrew</a>: <code>brew install openmpi</code></li>\n<li>Ubuntu / Debian / Windows Subsystem for Linux: <code>apt install libopenmpi-dev openmpi-bin</code></li>\n<li>CentOS: <code>yum install openmpi-devel</code></li>\n</ul>\n</li>\n<li><a href=\"https://cmake.org/download/\" rel=\"nofollow\">CMake \u2265 3.14</a></li>\n<li><a href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\">Python \u2265 3.6</a></li>\n</ul>\n<ol>\n<li>get the Gemini code</li>\n</ol>\n<pre>git clone https://github.com/gemini3d/gemini\n\n<span class=\"nb\">cd</span> gemini\n</pre>\n<ol>\n<li>Setup Gemini and prereqs</li>\n</ol>\n<pre>python3 setup.py develop --user\n</pre>\n<ol>\n<li>Build Gemini</li>\n</ol>\n<pre>cmake -B build\n\ncmake --build build -j\n\n<span class=\"nb\">cd</span> build\n\nctest --output-on-failure\n</pre>\n<h2>Prerequisites</h2>\n<p>Gemini uses CMake build system to automatically build the entire software library stack,\nchecking for compatibility of pre-installed libraries such as Lapack, Scalapack and MUMPS.\nGemini scripts used to load and check data use Python.</p>\n<h3>Compilers</h3>\n<p>The object-oriented Fortran 2008 code used in GEMINI requires a Fortran compiler that handles Fortran 2008 syntax including</p>\n<ul>\n<li>submodule</li>\n<li>block</li>\n</ul>\n<p>Most currently maintained compilers can do this.\nGfortran and Intel Fortran are two compilers known to work easily with GEMINI.</p>\n<h4>Gfortran</h4>\n<p>Gfortran is the recommended compiler to run Gemini, as most libraries are already compiled for it.\nGfortran 6 or newer works well.\nWe regularly work with Gfortran 7 and 9 on MacOS and Linux (CentOS / Ubuntu)\nDue to OpenMPI not supporting native Windows, Gfortran is usable on Windows via Windows Subsystem for Linux.</p>\n<h4>Intel Fortran</h4>\n<p>Intel Fortran <code>ifort</code> for Linux--need to have \"Parallel Studio XE\" so that it has MPI.\nIfort versions 19.0.5 and 19.1.0 are among the versions known to work with Gemini.\nIntel Fortran for Linux\n<a href=\"https://software.intel.com/en-us/articles/intel-parallel-studio-xe-supported-and-unsupported-product-versions\" rel=\"nofollow\">currently supported versions</a>\nare generally targeted for Gemini support.</p>\n<p>Intel Fortran for Windows is in general a tricky compiler to work with, and it is not supported for Gemini.</p>\n<h3>Libraries</h3>\n<p>Tested versions include:</p>\n<ul>\n<li>OpenMPI 1.10, 2.1 - 4.0  |  IntelMPI 19.0, 19.1</li>\n<li>MUMPS 4.10, 5.1.2, 5.2.1\n<ul>\n<li>Mumps \u2265 5.2 recommended to have vastly less verbose console output</li>\n</ul>\n</li>\n<li>SCALAPACK 2.0 / 2.1</li>\n<li>LAPACK95 3.0  (optional)</li>\n<li>HDF5 1.8.16 / 1.10 / 1.12</li>\n</ul>\n<h3>build-time options</h3>\n<p>build-time options in general are enabled or disabled like</p>\n<pre>cmake -B build -Doption<span class=\"o\">=</span><span class=\"nb\">true</span>\n\ncmake -B build -Doption<span class=\"o\">=</span><span class=\"nb\">false</span>\n</pre>\n<p>NCAR GLOW is automatically installed, but optional in general.\nAuroral emissions use GLOW.\nEnable or disable GLOW with <code>-Dglow=</code> option</p>\n<p>By default, Python and/or GNU Octave are used for the self-tests.\nMatlab is considerably slower, but may be enabled for self-tests with <code>-Dmatlab=enabled</code></p>\n<p>HDF5 is enabled / disabled by <code>-Dhdf5=</code> option</p>\n<h3>Analysis of simulation output</h3>\n<p>GEMINI uses Python for essential interfaces, plotting and analysis.\nA lot of legacy analysis was done with Matlab scripts that we continue to maintain.</p>\n<p>The Matlab .m scripts generally require Matlab \u2265 R2019a and are being transitioned to Python.\nOnly the essential scripts needed to setup a simple example, and plot the results are included in the main GEMINI respository.\nThe <a href=\"https://github.com/gemini3d/GEMINI-scripts\" rel=\"nofollow\">Gemini-scripts</a> and\n<a href=\"https://github.com/gemini3d/GEMINI-examples\" rel=\"nofollow\">Gemini-examples</a>\ncontain scripts used for various published and ongoing analyses.</p>\n<h3>Document generation</h3>\n<p>The documentation is Markdown-based, using FORD. If source code or documentation edits are made, the documentation is regenerated from the top-level gemini directory by:</p>\n<pre>ford ford.md\n</pre>\n<p>The output is under <code>docs/</code> and upon <code>git push</code> will appear at the <a href=\"https://mattzett.github.io/gemini/index.html\" rel=\"nofollow\">GEMINI docs website</a>.</p>\n<p>FORD is a Python program, installed via:</p>\n<pre>pip install ford\n</pre>\n<p>Note: leave the LaTeX files and other non-autogenerated documents in <code>doc/</code> so they don't get disturbed by FORD.</p>\n<h2>License</h2>\n<p>GEMINI is distributed under the Apache 2.0 license.</p>\n<h2>Suggested hardware</h2>\n<p>GEMINI can run on hardware ranging from a modest laptop to a high-performance computing (HPC) cluster.</p>\n<p>For large 3D simulations (more than 20M grid points), GEMINI should be run in a cluster environment or a \"large\" multicore workstation (e.g. 12 or more cores).  Runtime depends heavily on the grid spacing used, which determines the time step needed to insure stability,  For example we have found that a 20M grid point simulations takes about  4 hours on 72 Xeon E5 cores.  200M grid point simulations can take up to a week on 256 cores.  It has generally been found that acceptable performance requires &gt; 1GB memory per core; moreover, a large amount of storage (hundreds of GB to several TB) is needed to store results from large simulations.</p>\n<p>One could run large 2D or very small 3D simulations (not exceeding a few million grid points) on a quad-core workstation, but may take quite a while to complete.</p>\n<h3>Compiler configure</h3>\n<p>If using Homebrew, be sure Homebrew's GCC is used instead of AppleClang or other non-Homebrew compilers so that the Homebrew library ABIs match the compiler ABI.</p>\n<pre><span class=\"nv\">FC</span><span class=\"o\">=</span>gfortran-9 <span class=\"nv\">CC</span><span class=\"o\">=</span>gcc-9 cmake -B build\n</pre>\n<p>If you need to specify MPI compiler wrappers, do like:</p>\n<pre>cmake -B build -DMPI_ROOT<span class=\"o\">=</span>~/lib_gcc/openmpi-3.1.4\n</pre>\n<h3>input directory</h3>\n<p>The example <code>config.ini</code> in <code>initialize/</code> looks for input grid data in <code>tests/data</code>.\nIf you plan to push back to the repository, please don't edit those example <code>.ini</code> file paths, instead use softlinks <code>ln -s</code> to point somewhere else if needed.\nNote that any <code>config.ini</code> you create yourself in <code>initialize/</code> will not be included in the repository since that directory is in <code>.gitignore</code> (viz. not tracked by git).</p>\n<h4>MUMPS verbosity</h4>\n<p>MUMPS initialization ICNTL flags are set in <code>numerical/potential/potential_mumps.f90</code>.\nICNTL 1-4 concern print output unit and verbosity level, see MUMPS\n<a href=\"http://mumps.enseeiht.fr/index.php?page=doc\" rel=\"nofollow\">User Manual</a></p>\n<h4>Build tips</h4>\n<p>Libraries are auto-built by Gemini when building gemini.bin.\nIf it's desired to use system libraries, consider <a href=\"./scripts/install_prereqs.py\" rel=\"nofollow\">install_prereqs.py</a></p>\n<h3>self-tests</h3>\n<p>GEMINI has self tests that compare the output from a \"known\" test problem to a reference output.\nUse <code>ctest -V</code> to reveal the commands being sent if you want to run them manually.</p>\n<h3>OS-specific tips</h3>\n<p>CMake will automatically download and build necessary libraries for Gemini.</p>\n<ul>\n<li>\n<p>Ubuntu: tested on Ubuntu 18.04 / 16.04</p>\n</li>\n<li>\n<p>CentOS 7</p>\n<pre>module load git cmake openmpi\n\nmodule load gcc\n<span class=\"nb\">export</span> <span class=\"nv\">CC</span><span class=\"o\">=</span>gcc <span class=\"nv\">CXX</span><span class=\"o\">=</span>g++ <span class=\"nv\">FC</span><span class=\"o\">=</span>gfortran\n</pre>\n</li>\n<li>\n<p>MacOS / Linux: use Homebrew to install Gfortran and OpenMPI like:</p>\n<pre>brew install cmake gcc openmpi lapack scalapack\n</pre>\n</li>\n</ul>\n<h2>Known limitations and issues of GEMINI</h2>\n<ol>\n<li>\n<p>Generating equilibrium conditions can be a bit tricky with curvilinear grids.  A low-res run can be done, but it will not necessary interpolate properly onto a finer grid due to some issue with the way the grids are made with ghost cells etc.  A workaround is to use a slightly narrower (x2) grid in the high-res run (quarter of a degree seems to work most of the time).</p>\n</li>\n<li>\n<p>Magnetic field calculations on an open 2D grid do not appear completely consistent with MATLAB model prototype results; although there are quite close.  This may have been related to sign errors in the FAC calculations - these tests should be retried at some point.</p>\n</li>\n<li>\n<p>Occasionally MUMPS will throw an error because it underestimated the amount of memory needed for a solve.  If this happens a workaround is to add this line of code to the potential solver being used for your simulations.  If the problem persists try changing the number to 100.</p>\n<pre><span class=\"n\">mumps_par</span><span class=\"p\">%</span><span class=\"n\">ICNTL</span><span class=\"p\">(</span><span class=\"mi\">14</span><span class=\"p\">)</span><span class=\"o\">=</span><span class=\"mi\">50</span>\n</pre>\n</li>\n<li>\n<p>There are potentially some issues with the way the stability condition is evaluated, i.e. it is computed before the perp. drifts are solved so it is possible when using input data to overrun this especially if your target CFL number is &gt; 0.8 or so.  Some code has been added as of 8/20/2018 to throttle how much dt is allowed to change between time steps and this seems to completely fix this issue, but theoretically it could still happen; however this is probably very unlikely.</p>\n</li>\n<li>\n<p>Occasionally one will see edge artifacts in either the field -aligned currents or other parameters for non-periodic in x3 solves.  This may be related to the divergence calculations needed for the parallel current (under EFL formulation) and for compression calculations in the multifluid module, but this needs to be investigated further...  This do not appear to affect solutions in the interior of the grid domain and can probably be safely ignored if your region of interest is sufficiently far from the boundary (which is alway good practice anyway).</p>\n</li>\n</ol>\n<h2>To do list</h2>\n<p>See <a href=\"./TODO.md\" rel=\"nofollow\">TODO.md</a>.</p>\n<h2>Standard and style</h2>\n<p>GEMINI is Fortran 2008 compliant and uses two-space indents throughout (to accommodate the many, deeply nested loops).  Some of our developers are avid VIM users so please do not use tabs if you plan to push back to the repository or merge.</p>\n<h2>Debug text</h2>\n<p>The gemini.bin command line option <code>-d</code> or <code>-debug</code> prints a large amount to text to console, perhaps gigabytes worth for medium simulations. By default, only the current simulation time and a few other messages are shown.</p>\n<h2>Manually set number of MPI processes</h2>\n<pre>mpiexec -np &lt;number of processors&gt;  build/gemini.bin &lt;input config file&gt; &lt;output directory&gt;\n</pre>\n<p>for example:</p>\n<pre>mpiexec -np <span class=\"m\">4</span> build/gemini.bin initialize/2Dtest/config.ini /tmp/2d\n</pre>\n<p>Note that the output <em>base</em> directory must already exist (e.g. <code>/tmp/2d</code>).\nThe source code consists of about ten module source files encapsulating various functionalities used in the model.\nA diagram all of the modules and their function is shown in figure 1.\nCMake can generate a graphical <a href=\"https://cmake.org/cmake/help/latest/module/CMakeGraphVizOptions.html\" rel=\"nofollow\">dependency graph</a>.</p>\n<p>Two of the log files created are:</p>\n<ul>\n<li>gitrev.log: the Git branch and hash revision at runtime (changed filenames beyond this revision are logged)</li>\n<li>compiler.log: compiler name, version and options used to compile the executable.</li>\n</ul>\n<p><img alt=\"Figure 1\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/48d105df6233c1bbc6d53ff8cb3323a2a6c32c99/646f632f666967757265312e706e67\"></p>\n\n<h2>Auxiliary fortran program</h2>\n<p>Note that there is also a utility that can compute magnetic fields from the currents calculated by GEMINI.\nThis can be run by:</p>\n<pre>mpirun -np <span class=\"m\">4</span> ./magcalc /tmp/3d tests/data/test3d/input/magfieldpoints.dat\n</pre>\n<p>This will compute magnetic fields over a grid at ground level using currents computed from the 3Dtest simulation.  In order to run this program, you will need to create a set of field points at which the magnetic perturbations will be calculated.  For example, this could be a list of ground stations, a regular mesh, or a set of satellite tracks.</p>\n<h2>Number of MPI processes</h2>\n<p>In general for MPI programs and associated simulations, there may be a minimum number of MPI processes and/or integer multiples that must be met (for example, number of processes must be factor of 4).\nThe build system generation process automatically sets the maximum number of processes possible based on your CPU core count and grid size.</p>\n<h2>Input file format</h2>\n<p>Each simulation needs an input file that specifies location of initial conditions and other pertinent information for the simulation.  A coupled of very basic examples of these are included in the ./initialize directory; each subdirectory is a separate example usage of GEMINI for a particular problem.  The basic template for an input file (config.ini) file follows (please note that most use cases will not have all options activated as this example does).</p>\n<pre><code>16,9,2015                             !dmy:  day,month,year\n82473.0                               !UTsec0:  start time, UT seconds\n1800.0                                !tdur:  duration of simulation in seconds\n15.0                                  !dtout: how often (s) to do output\n109.0,109.0,5.0                       !activ:  f107a,f107,Ap (81 day averaged f10.7, daily f10.7, and average Ap index)\n0.9                                   !tcfl:  target cfl number (dimensionless - must be less than 1.0 to insure stability)\n1500.0                                !Teinf:  exospheric electron temperature, K (only used in open-grid simulations)\n0                             \t  !potsolve:  are we solving potential? (0=no; 1=-yes)\n0                                     !flagperiodic:  do we interpret the grid as being periodic in the x3-direction?  (0=no; 1=yes)\n2                                     !flagoutput:  what type of output do we do?  (2=ISR-like species-averaged plasma parameters; 3=electron density only; anything else nonzero=full output)\n0                                     !flagcapacitance:  include inertial capacitance? (0=no; 1=yes; 2=yes+m'spheric contribution)\n../simulations/input/chile20153D_0.5_medhighres/chile20153D_0.5_medhighres_simsize.dat\n../simulations/input/chile20153D_0.5_medhighres/chile20153D_0.5_medhighres_simgrid.dat\n../simulations/input/chile20153D_0.5_medhighres/chile20153D_0.5_medhighres_ICs.dat\n1                                     !are we applying neutral perturbations? (0=no; 1=yes).  If 0, the next five entries are skipped while reading this input file\n1                                     !how doe we interpret the input neutral file geometry?  (0=Cartesian; anything else=axisymmetric)\n-20.5706d0,359.4048d0                 !source mlat,mlon of disturbance (degrees magnetic lat,lon)\n4d0                                   !time step between neutral input files\n2d3,2d3                               !spatial resolutions in radial and vertical directions\n../simulations/chile2015_neutrals/\n1                                     !flagprecfileinput:  for precipitation file input (0=no; 1=yes).  If 0, then next two entries of input file are skipped\n1.0                                   !dtprec:  time (s) between precipitation input files\n../simulations/isinglass_precipitation/\n1                                     !flagE0fileinput:  flag for electric field file input (0-no; 1=yes).  If 0, next two entries of input file are skipped\n10.0                                  !dtE0:  time (s) between electric field input files\n../simulations/isinglass_fields/\n</code></pre>\n<p>A large number of examples (in addition to those included in the main repo) are included in the GEMINI-script repository.</p>\n<h2>Running with different boundary and initial conditions:</h2>\n<p>GEMINI requires both initial and boundary conditions to run properly.  Specifically the user must provide a complete initial ionospheric state (density, drift, and temperature for all ionospheric species), along with boundary conditions for the electric potential (in 2D this are the top, bottom, and side potentials; in 3D the topside current density and side wave potentials).  Fluid state variables are given free-flow boundary conditions at the edges of the simulation grid.  The <code>io</code> module contains code dealing with input of initial state from file and the <code>potential_comm</code> and <code>potentialBCs_mumps</code> modules contains contains code dealing with boundary condition input.</p>\n<p>There are presently two ways in which the boundary and initial conditions can be set for GEMINI:  subroutine-based input and file-based input.</p>\n<p>PLEASE NOTE that future releases will use Fortran 2008 <code>submodule</code>, likely completely removing the option for subroutine-based initial and boundary conditions.</p>\n<h3>Subroutine-based input (<em>not recommended</em> and to be deprecated in a future release):</h3>\n<p>There are two subroutines that can be modified by the user to provide boundary conditions to the code; these are described below. Note that, if any of these are changed, the code needs to be recompiled.</p>\n<p><code>./ionization/boundary_conditions/precipBCs_mod.f90</code> - the function <code>precipBCs' specifies the pattern of electron precipitation, including characteristic energy and total energy flux, over top of grid. If the user does not specify an input file for precipitation boundary conditions in</code>config.ini`, then this subroutine will be called to set the boundary.</p>\n<p><code>./numerical/potential/boundary_conditions/potentialBCs_mumps.f90</code> - boundary conditions for the electric potential or field-aligned current.  The type of input that is being used is specified by the flags in the <code>config.ini</code> file for the simulation.  This subroutine will only be called if the user has not specified an input file containing boundary conditions.</p>\n<p>By default these subroutines will be used for boundary conditions if file input is not specified in the config.ini input file.  The base GEMINI sets these to be zero potential (or current) and some negligible amount of precipitation.  Note that if you write over these subroutines then the code will use whatever you have put into them if file input is not specified.  This can lead to unintended behavior if ones modifies these and then forgets since the code will continue to use the modifications instead of some baseline.  Because of this issue, and the fact that GEMINI must be rebuilt every time these subroutines are changed, this method of boudnary condition input is going to be removed.</p>\n<h3>File-based input (<em>recommended</em>)</h3>\n<p>An alternative is to use the file input option, which needs to be set up using MATLAB (or other) scripts.  To enable this type of input, the appropriate flags (flagprecfileinput and flagE0fileinput) need to be set in the input <code>config.ini</code> file (see Section entitled \"Input file format\" above).  All examples included in <code>initialize/</code> in both the GEMINI and GEMINI-scripts repositories use this method for setting boundary conditions.  Note that the user can specify the boundary condition on a different grid from what the simulation is to be run with; in this case GEMINI will just interpolate the given boundary data onto the current simulation grid.</p>\n<h3>Initial conditions</h3>\n<p>GEMINI needs density, drift, and temperature for each species that it simulations over the entire grid for which the simulation is being run as input.  Generally one will use the results of another GEMINI simulation that has been initialized in an arbitrary way but run for a full day to a proper ionospheric equilibrium as this input.  Any equilibrium simulation run this way must use full output (flagoutput=1 in the <code>config.ini</code>).  A useful approach for these equilibrium runs is to use a coarser grid so that the simulation completes quickly and then interpolate the results up to fine grid resolution.  An example of an equilibrium setup is given in <code>./initialize/2Dtest_eq</code>; note that this basically makes up an initial conditions (using <code>eqICs.m</code>) and runs until initial transients have settled.  An example of a script that interpolates the output of an equilibrium run to a finer grid is included with <code>./initialize/2Dtest</code>.</p>\n<h2>Running one of the premade examples</h2>\n<p>Currently the main repo only includes the very basic 2Dtest and 3Dtest examples</p>\n<h2>Creating a simulation</h2>\n<ol>\n<li>Create initial conditions for equilibrium simulation -  Several examples of equilibrium setups are included in the ./initialize directory; these end with <code>_eq</code>.  These are all based off of the general scripts <code>./setup/model_setup.m</code> and related scripts.  In general this equilbrium simulation will set the date, location, and geomagnetic conditions for the background ionospheric state for later perturbation simulations.</li>\n<li>Run an equilibrium simulation at low resolution to obtain a background ionosphere.  See examples in ./initialize ending in <code>_eq</code></li>\n<li>Generate a grid - Several examples of grid generation scripts adapted to particular problems are given in the <code>initialize/</code> directory of the repo (see list above for an example).  In particular, for 2Dtest and 3Dtest there is a script that reads in an equilbirum simulation, creates a fine resolution mesh, and then interpolates the equilibrium data onto that fine mesh.</li>\n<li>Interpolate the equilibrium results on to a high resolution grid and create new input files for full resolution - See examples in the ./initialize/ directories not ending in <code>_eq</code>.  These are all based off of the general <code>./setup/model_setup_interp.m</code> script.</li>\n<li>Set up boundary conditions for potential, if required - see section of this document on boundary conditions</li>\n<li>Set up precipitation boundary conditions, if required -  see section of this document on boundary conditions</li>\n<li>Recompile the code with make <em>only if you are using subroutine based input and boundary conditions</em> (please note that this functionality will be removed in a later release).  If you are using file-based input then a rebuild is not necessary (this is another benefit of using file-based input)</li>\n<li>Run your new simulation</li>\n</ol>\n<h2>Running in two dimensions</h2>\n<p>The code determines 2D vs. 3D runs by the number of x2 or x3 grid points specified in the <code>config.ini</code> input file.  If the number of x2 grid points is 1, then a 2D run is executed (since message passing in the x3 direction will work normally).  If the number of x3 grid points is 1, the simulation will swap array dimensions and vector components between the x2 and x3 directions so that message passing parallelization still provides performance benefits.  The data will be swapped again before output so that the output files are structured normally and the user who is not modifying the source code need not concern themselves with this reordering.</p>\n<h2>Loading and plotting output</h2>\n<p>MATLAB is required to load the output file via scripts in the ./vis directory (these scripts generally work on both 2D and 3D simulation results).\nGNU Octave is not reliable at plotting in general, and might crash or make wierd looking plots.\nThe results for an entire simulation can be plotted using <a href=\"./vis/plotall.m\" rel=\"nofollow\">plotall.m</a></p>\n<pre><span class=\"n\">plotall</span><span class=\"p\">(</span><span class=\"s\">'/tmp/mysim'</span><span class=\"p\">)</span>\n</pre>\n<p>These also illustrates how to read in a sequence of files from a simulation.  This script prints a copy of the output plots into the simulation output directory.  Finer-level output control can be achieve by using the 'plotframe.m' and 'loadframe.m' scripts to plot and load data from individual simulation output frames, respectively.</p>\n<p>The particular format of the output files is specified by the user in the input config.ini file.  There are three options:</p>\n<ol>\n<li>full output - output all state variables; very large file sizes will results, but this is required for building initial conditions and for some analysis that require detailed composition and temperature information.</li>\n<li>average state parameter output - species averaged temperature and velocity; electron density.  Probably best for most uses</li>\n<li>density only output - only electron density output.  Best for high-res instability runs where only the density is needed and the output cadence is high</li>\n</ol>\n<p>The organization of the data in the Matlab workspace, after a single frame is loaded (via 'loadframe.m'), is as follows (MKSA units throughout):</p>\n<h3>Time variables:</h3>\n<p>simdate - a six element vector containing year, month, day, UT hour, UT minute, and UT seconds of the present frame</p>\n<h3>Grid variables:</h3>\n\n<p>structure xg - members xg.x1,2,3 are the position variables, <code>xg.h*</code> are the metric factors, <code>xg.dx*</code> are the finite differences,</p>\n<p>xg.glat,glon are the latitudes and longitudes (degrees geographic) of each grid point, xg.alt is the altitude of each grid point.</p>\n<p>xg.r,theta,phi - for each grid point:  radial distance (from ctr of Earth), magnetic colatitude (rads.), and magnetic longitude (rads.)</p>\n<p>The grid structure, by itself, can be read in by the MATLAB function 'readgrid.m'; this is automatically invoked with 'loadframe.m' so there is not need to separately load the grid and output frame data.</p>\n<h3>Temperature variable:</h3>\n<p>Ts (first three dimensions have size lxs; 4th dimension is species index:  1=O+,2=NO+,3=N2+,4=O2+,5=N+, 6=H+,7=e-)</p>\n<h3>Density variable:</h3>\n<p>ns (same indexing as temperature)</p>\n<h3>Drifts:</h3>\n<p>vs1 (same indexing as temperature)</p>\n<p>x2-drift component:  v2 (same for all species, so this is just size lxs and is a 3D array)\nx3-drift component:  v3</p>\n<h3>Electromagnetic variables:</h3>\n<p>current density:  J1, J2, J3\npotential:  Phitop (EFL potential)</p>\n<p>Note that the electric field is not included in the output file, but that it can be calculated from this output by taking -vxB at an altitude above about 200 km or by differentiating the top boundary electric potential 'Phitop' with respect to the x2 and x3 variables; however, note that if a curvilinear grid is used the derivatives must include the appropriate metric factors.</p>\n<h2>Computing total electron content (TEC)</h2>\n<p>TEC and magnetic field variations can be calculated as a post-processing step in which the simulation data are read in and interpolated onto a regular geographic grid and then integrated accordingly using scripts in the './vis' directory - see <code>TECcalc.m</code>.\nAn example of how to plot TEC computed by this script is included in <code>TECplot_map.m</code> (requires MATLAB mapping toolbox).</p>\n<h2>Visualizing magnetic field perturbations computed by magcalc.f90</h2>\n<p>The example script <code>magplot_fort_map.m</code> shows an example of how to load the results of running magcalc.</p>\n\n          </div>"}, "last_serial": 6762348, "releases": {"0.3.0": [{"comment_text": "", "digests": {"md5": "485eaa99bd7fbde5e732f30c1b5e09b1", "sha256": "f46c33187d6de7a0bb21de8592ac28f2ac15fc03f2f9c6c3a97d7b5307822bad"}, "downloads": -1, "filename": "gemini3d-0.3.0.tar.gz", "has_sig": false, "md5_digest": "485eaa99bd7fbde5e732f30c1b5e09b1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 60518, "upload_time": "2020-03-05T20:17:28", "upload_time_iso_8601": "2020-03-05T20:17:28.468668Z", "url": "https://files.pythonhosted.org/packages/fa/3e/5bd5ae7c5d5ecef38ccfc82d1d724f876632782d83c08e8590d68820303d/gemini3d-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "c7245ad0d68889f1f7ddf10af951e69e", "sha256": "7a1118ca57467a52633db7304943f951aa055ea8c38f7feda0afe04bc49ac0ef"}, "downloads": -1, "filename": "gemini3d-0.3.1.tar.gz", "has_sig": false, "md5_digest": "c7245ad0d68889f1f7ddf10af951e69e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 59855, "upload_time": "2020-03-06T14:40:50", "upload_time_iso_8601": "2020-03-06T14:40:50.081555Z", "url": "https://files.pythonhosted.org/packages/4e/dd/23a74214b4d7a9a9d9a787e68343526941376cbdeedeed7bfb5e5ac142f3/gemini3d-0.3.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c7245ad0d68889f1f7ddf10af951e69e", "sha256": "7a1118ca57467a52633db7304943f951aa055ea8c38f7feda0afe04bc49ac0ef"}, "downloads": -1, "filename": "gemini3d-0.3.1.tar.gz", "has_sig": false, "md5_digest": "c7245ad0d68889f1f7ddf10af951e69e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 59855, "upload_time": "2020-03-06T14:40:50", "upload_time_iso_8601": "2020-03-06T14:40:50.081555Z", "url": "https://files.pythonhosted.org/packages/4e/dd/23a74214b4d7a9a9d9a787e68343526941376cbdeedeed7bfb5e5ac142f3/gemini3d-0.3.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:58:31 2020"}