{"info": {"author": "Anthony Scopatz", "author_email": "scopatz@gmail.com", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 3"], "description": "# bitemporal-h5\nA generic bitemporal model built on HDF5 (h5py)\n\n## Model\nThe basic model for a bitemporal is an HDF5 dataset that is extensible\nalong a single dimension with named columns and different dtypes for\neach column. In-memory, this will be represented by a numpy structured array.\nWe will call this structure a `Table`, for purposes here.\n\nNote that HDF5 has its own Table data structure in the high-level\ninterface (hdf5-hl). We will not be using the high-level table here for\na couple of reasons. The first is that `h5py` does not support HDF5's\nhigh-level constructs. The second is that we plan on eventually swapping out\nthe value column with a deduplicated cache. Relying on low-level HDF5 constructs\ngrants us this flexibility in the future.\n\nThe columns present in the table are as follows:\n\n* `transaction_id (uint64)`: This is a monotonic integer that represents the\n  precise write action that caused this row to be written. Multiple rows may\n  be written at the same time, so this value is not unique among rows, though\n  presumably all rows with a given transaction id are contiguous in the table.\n  This value is zero-indexed. The current largest transaction id should be\n  written to the table's attributes as `max_transaction_id` (also uint64).\n  Write operations should bump the `max_transaction_id` by one.\n* `transaction_time (datetime64)`: This is a timestamp (sec since epoch). Any metadata\n  about the timezones should be stored as a string attribute of the dataset as\n  `transaction_time_zone`. This represents the time at which the data was\n  recorded by the write operation. All rows with the same `transaction_id` should\n  have the same value here.\n* `valid_time (datetime64)`: This is a timestamp (sec since epoch). Any metadata\n  about the timetzones should be stored as a string attribute of the dataset as\n  `valid_time_zone`. This is the primary axis of the time series. It represents\n  the data stored in the `value` column.\n* `value ((I,J,K,...)<scalar-type>|)`: This column represents the actual values\n  of a time series. This may be an N-dimensional array of any valid dtype.\n  It is likely sufficient to restrict ourselves to floats and ints, but the model\n  should be general enough to accept any scalar dtypes. Additionally, the typical\n  usecase will be for this column to be a scalar float value.\n\nTherefor an example numpy dtype with float values and a shape of `(1, 2, 3)` is:\n\n```python\nnp.dtype([\n    ('transaction_id', '<uint64'),\n    ('transaction_time', '<M8'),\n    ('valid_time', '<M8'),\n    ('value', '<f8', (1, 2, 3))\n])\n```\n\n## Quickstart API\nThe interface for writing to the bitemporal HDF5 storage is as follows:\n\n```python\nimport bth5\n\nwith bth5.open(\"/path/to/file.h5\", \"/path/to/group\", \"a+\") as ds:\n    # all writes are staged into a single transaction and then\n    # written when the context exits. Transaction times and IDs\n    # are automatically applied. The first write call determines\n    # the dtype & shape of value, if the data set does not already\n    # exist.\n    ds.write(t1, p1)\n    ds.write(valid_time=t2, value=p2)\n    ds.write(valid_time=[t3, t4, t5], value=[p3, p4, p5])\n    ds.write((t6, p6))\n    ds.write([(t7, p7), (t8, p8)])\n```\n\nReading from the data set should follow the normal numpy indexing:\n\n```python\n# opened in read-only mode\n>>> ds = bth5.open(\"/path/to/file.h5\", \"/path/to/group\")\n>>> in_mem = ds[:]\n>>> in_mem.dtype\nnp.dtype([\n    ('valid_time', '<M8'),\n    ('value', '<f8', (1, 2, 3))\n])\n```\n\nThis again should return the latest valid time and value which is present in the\nreduced dataset. Furthermore, there is an escape valve to reach the actual\ndataset as represented on-disk. This is the `raw` attribute, that returns\na reference to the h5py dataset.\n\n```python\n>>> ds = bth5.open(\"/path/to/file.h5\", \"/path/to/group\")\n>>> ds.raw\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/quansight/bitemporal-h5", "keywords": "", "license": "BSD-3-Clause", "maintainer": "Quansight", "maintainer_email": "", "name": "bth5", "package_url": "https://pypi.org/project/bth5/", "platform": "Cross Platform", "project_url": "https://pypi.org/project/bth5/", "project_urls": {"Homepage": "https://github.com/quansight/bitemporal-h5"}, "release_url": "https://pypi.org/project/bth5/0.0.3/", "requires_dist": null, "requires_python": "", "summary": "Bitemporal HDF5", "version": "0.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>bitemporal-h5</h1>\n<p>A generic bitemporal model built on HDF5 (h5py)</p>\n<h2>Model</h2>\n<p>The basic model for a bitemporal is an HDF5 dataset that is extensible\nalong a single dimension with named columns and different dtypes for\neach column. In-memory, this will be represented by a numpy structured array.\nWe will call this structure a <code>Table</code>, for purposes here.</p>\n<p>Note that HDF5 has its own Table data structure in the high-level\ninterface (hdf5-hl). We will not be using the high-level table here for\na couple of reasons. The first is that <code>h5py</code> does not support HDF5's\nhigh-level constructs. The second is that we plan on eventually swapping out\nthe value column with a deduplicated cache. Relying on low-level HDF5 constructs\ngrants us this flexibility in the future.</p>\n<p>The columns present in the table are as follows:</p>\n<ul>\n<li><code>transaction_id (uint64)</code>: This is a monotonic integer that represents the\nprecise write action that caused this row to be written. Multiple rows may\nbe written at the same time, so this value is not unique among rows, though\npresumably all rows with a given transaction id are contiguous in the table.\nThis value is zero-indexed. The current largest transaction id should be\nwritten to the table's attributes as <code>max_transaction_id</code> (also uint64).\nWrite operations should bump the <code>max_transaction_id</code> by one.</li>\n<li><code>transaction_time (datetime64)</code>: This is a timestamp (sec since epoch). Any metadata\nabout the timezones should be stored as a string attribute of the dataset as\n<code>transaction_time_zone</code>. This represents the time at which the data was\nrecorded by the write operation. All rows with the same <code>transaction_id</code> should\nhave the same value here.</li>\n<li><code>valid_time (datetime64)</code>: This is a timestamp (sec since epoch). Any metadata\nabout the timetzones should be stored as a string attribute of the dataset as\n<code>valid_time_zone</code>. This is the primary axis of the time series. It represents\nthe data stored in the <code>value</code> column.</li>\n<li><code>value ((I,J,K,...)&lt;scalar-type&gt;|)</code>: This column represents the actual values\nof a time series. This may be an N-dimensional array of any valid dtype.\nIt is likely sufficient to restrict ourselves to floats and ints, but the model\nshould be general enough to accept any scalar dtypes. Additionally, the typical\nusecase will be for this column to be a scalar float value.</li>\n</ul>\n<p>Therefor an example numpy dtype with float values and a shape of <code>(1, 2, 3)</code> is:</p>\n<pre><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">([</span>\n    <span class=\"p\">(</span><span class=\"s1\">'transaction_id'</span><span class=\"p\">,</span> <span class=\"s1\">'&lt;uint64'</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"s1\">'transaction_time'</span><span class=\"p\">,</span> <span class=\"s1\">'&lt;M8'</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"s1\">'valid_time'</span><span class=\"p\">,</span> <span class=\"s1\">'&lt;M8'</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"s1\">'value'</span><span class=\"p\">,</span> <span class=\"s1\">'&lt;f8'</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">))</span>\n<span class=\"p\">])</span>\n</pre>\n<h2>Quickstart API</h2>\n<p>The interface for writing to the bitemporal HDF5 storage is as follows:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">bth5</span>\n\n<span class=\"k\">with</span> <span class=\"n\">bth5</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"s2\">\"/path/to/file.h5\"</span><span class=\"p\">,</span> <span class=\"s2\">\"/path/to/group\"</span><span class=\"p\">,</span> <span class=\"s2\">\"a+\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">ds</span><span class=\"p\">:</span>\n    <span class=\"c1\"># all writes are staged into a single transaction and then</span>\n    <span class=\"c1\"># written when the context exits. Transaction times and IDs</span>\n    <span class=\"c1\"># are automatically applied. The first write call determines</span>\n    <span class=\"c1\"># the dtype &amp; shape of value, if the data set does not already</span>\n    <span class=\"c1\"># exist.</span>\n    <span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">t1</span><span class=\"p\">,</span> <span class=\"n\">p1</span><span class=\"p\">)</span>\n    <span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">valid_time</span><span class=\"o\">=</span><span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"o\">=</span><span class=\"n\">p2</span><span class=\"p\">)</span>\n    <span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">valid_time</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">t3</span><span class=\"p\">,</span> <span class=\"n\">t4</span><span class=\"p\">,</span> <span class=\"n\">t5</span><span class=\"p\">],</span> <span class=\"n\">value</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">p3</span><span class=\"p\">,</span> <span class=\"n\">p4</span><span class=\"p\">,</span> <span class=\"n\">p5</span><span class=\"p\">])</span>\n    <span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">((</span><span class=\"n\">t6</span><span class=\"p\">,</span> <span class=\"n\">p6</span><span class=\"p\">))</span>\n    <span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">([(</span><span class=\"n\">t7</span><span class=\"p\">,</span> <span class=\"n\">p7</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">t8</span><span class=\"p\">,</span> <span class=\"n\">p8</span><span class=\"p\">)])</span>\n</pre>\n<p>Reading from the data set should follow the normal numpy indexing:</p>\n<pre><span class=\"c1\"># opened in read-only mode</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"n\">bth5</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"s2\">\"/path/to/file.h5\"</span><span class=\"p\">,</span> <span class=\"s2\">\"/path/to/group\"</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">in_mem</span> <span class=\"o\">=</span> <span class=\"n\">ds</span><span class=\"p\">[:]</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">in_mem</span><span class=\"o\">.</span><span class=\"n\">dtype</span>\n<span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">([</span>\n    <span class=\"p\">(</span><span class=\"s1\">'valid_time'</span><span class=\"p\">,</span> <span class=\"s1\">'&lt;M8'</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"s1\">'value'</span><span class=\"p\">,</span> <span class=\"s1\">'&lt;f8'</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">))</span>\n<span class=\"p\">])</span>\n</pre>\n<p>This again should return the latest valid time and value which is present in the\nreduced dataset. Furthermore, there is an escape valve to reach the actual\ndataset as represented on-disk. This is the <code>raw</code> attribute, that returns\na reference to the h5py dataset.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"n\">bth5</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"s2\">\"/path/to/file.h5\"</span><span class=\"p\">,</span> <span class=\"s2\">\"/path/to/group\"</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">raw</span>\n</pre>\n\n          </div>"}, "last_serial": 5956555, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "a971997b826d67591ee1df7338944425", "sha256": "32491ba5349a55189d2975265110ad72aa7e8a208d1905741edf0394084fa34f"}, "downloads": -1, "filename": "bth5-0.0.1.tar.gz", "has_sig": false, "md5_digest": "a971997b826d67591ee1df7338944425", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7267, "upload_time": "2019-09-13T18:49:05", "upload_time_iso_8601": "2019-09-13T18:49:05.544192Z", "url": "https://files.pythonhosted.org/packages/52/be/aa0160320567d634bfc0dd349284646b31c7492e62277974ac36cc34eda2/bth5-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "8898f3ad762faa8b55a62ace9ec67b20", "sha256": "3dd737c322b18df85f3b0bf030b7b6d87ca9870970336b6339fff9955506fd13"}, "downloads": -1, "filename": "bth5-0.0.2.tar.gz", "has_sig": false, "md5_digest": "8898f3ad762faa8b55a62ace9ec67b20", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9612, "upload_time": "2019-10-10T18:47:52", "upload_time_iso_8601": "2019-10-10T18:47:52.913305Z", "url": "https://files.pythonhosted.org/packages/60/d9/02dd4841bebfcb49276b059131047576284ac2d75cceeaa8f96e4120f39e/bth5-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "2979f59f517ad3057122331d41d4f188", "sha256": "4defac62a6d857e12b75cbf6179a6ef3aa652f6cec36bbc81d222d0b0dce6ce2"}, "downloads": -1, "filename": "bth5-0.0.3.tar.gz", "has_sig": false, "md5_digest": "2979f59f517ad3057122331d41d4f188", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9611, "upload_time": "2019-10-10T19:33:34", "upload_time_iso_8601": "2019-10-10T19:33:34.044256Z", "url": "https://files.pythonhosted.org/packages/81/7e/80f87068eff0c58361895a6ba15a90362201c4277631c9e3c781f7bb9207/bth5-0.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2979f59f517ad3057122331d41d4f188", "sha256": "4defac62a6d857e12b75cbf6179a6ef3aa652f6cec36bbc81d222d0b0dce6ce2"}, "downloads": -1, "filename": "bth5-0.0.3.tar.gz", "has_sig": false, "md5_digest": "2979f59f517ad3057122331d41d4f188", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9611, "upload_time": "2019-10-10T19:33:34", "upload_time_iso_8601": "2019-10-10T19:33:34.044256Z", "url": "https://files.pythonhosted.org/packages/81/7e/80f87068eff0c58361895a6ba15a90362201c4277631c9e3c781f7bb9207/bth5-0.0.3.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:36:09 2020"}