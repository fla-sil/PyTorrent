{"info": {"author": "cardsurf", "author_email": "cardsurf@email.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: GNU General Public License v2 (GPLv2)", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Topic :: Internet :: WWW/HTTP", "Topic :: Internet :: WWW/HTTP :: Dynamic Content"], "description": "xcrawler\n========\nA multi-threaded, open source web crawler\n\nFeatures\n---------\n* Use multiple threads to visit web pages\n* Extract web page data using XPath expressions or CSS selectors\n* Extract urls from a web page and visit extracted urls\n* Write extracted data to an output file\n* Set HTTP session parameters such as: cookies, SSL certificates, proxies\n* Set HTTP request parameters such as: header, body, authentication\n* Download files from the urls\n* Supports Python 2 and Python 3\n\nInstallation\n------------\n::\n\n    pip install xcrawler\n\n| \n| When installing ``lxml`` library on Windows you may encounter ``Microsoft Visual C++ is required`` errors.\n| To install ``lxml`` library on Windows:\n\n#. Download and install Microsoft Windows SDK:\n\n   * For Python 2.6, 2.7, 3.0, 3.1, 3.2: `Microsoft Windows SDK for .NET Framework 3.5 SP1 <http://www.microsoft.com/en-us/download/confirmation.aspx?id=3138>`_\n   * For Python 3.3, 3.4: `Microsoft Windows SDK for .NET Framework 4.0 <http://www.microsoft.com/en-us/download/confirmation.aspx?id=8279>`_\n\n#. Click the Start Menu, search for and open the command prompt:\n\n   * For Python 2.6, 2.7, 3.0, 3.1, 3.2: ``CMD Shell``\n   * For Python 3.3, 3.4: ``Windows SDK 7.1 Command Prompt``\n\n#. Install ``lxml``\n\n::\n\n    setenv /x86 /release && SET DISTUTILS_USE_SDK=1 && set STATICBUILD=true && pip install lxml\n\nUsage\n-----\n| Data and urls are extracted from a web page by a page scraper.\n| To extract data and urls from a web page use the following methods:\n| ``extract`` returns data extracted from a web page\n| ``visit`` returns next Pages to be visited\n| \n| A crawler can be configured before crawling web pages. A user can configure such settings of the crawler as:\n| * the number of threads used to visit web pages\n| * the name of an output file\n| * the request timeout\n| To run the crawler call:\n| ``crawler.run()``\n| \n| Examples how to use xcrawler can be found at: https://github.com/cardsurf/xcrawler/tree/master/examples\n| \n\nXPath Example\n-------------\n.. code:: python\n\n    from xcrawler import XCrawler, Page, PageScraper\n\n\n    class Scraper(PageScraper):\n        def extract(self, page):\n            topics = page.xpath(\"//a[@class='question-hyperlink']/text()\")\n            return topics\n\n\n    start_pages = [ Page(\"http://stackoverflow.com/questions/16622802/center-image-within-div\", Scraper()) ]\n    crawler = XCrawler(start_pages)\n    crawler.config.output_file_name = \"stackoverflow_example_crawler_output.csv\"\n    crawler.run()\n\nCSS Example\n-------------\n.. code:: python\n\n    from xcrawler import XCrawler, Page, PageScraper\n\n\n    class StackOverflowItem:\n        def __init__(self):\n            self.title = None\n            self.votes = None\n            self.tags = None\n            self.url = None\n\n\n    class UrlsScraper(PageScraper):\n        def visit(self, page):\n            hrefs = page.css_attr(\".question-summary h3 a\", \"href\")\n            urls = page.to_urls(hrefs)\n            return [Page(url, QuestionScraper()) for url in urls]\n\n\n    class QuestionScraper(PageScraper):\n        def extract(self, page):\n            item = StackOverflowItem()\n            item.title = page.css_text(\"h1 a\")[0]\n            item.votes = page.css_text(\".question .vote-count-post\")[0].strip()\n            item.tags = page.css_text(\".question .post-tag\")[0]\n            item.url = page.url\n            return item\n\n\n    start_pages = [ Page(\"http://stackoverflow.com/questions?sort=votes\", UrlsScraper()) ]\n    crawler = XCrawler(start_pages)\n    crawler.config.output_file_name = \"stackoverflow_css_crawler_output.csv\"\n    crawler.config.number_of_threads = 3\n    crawler.run()\n\nFile Example\n-------------\n.. code:: python\n\n    from xcrawler import XCrawler, Page, PageScraper\n\n\n    class WikimediaItem:\n        def __init__(self):\n            self.name = None\n            self.base64 = None\n\n\n    class EncodedScraper(PageScraper):\n        def extract(self, page):\n            url = page.xpath(\"//div[@class='fullImageLink']/a/@href\")[0]\n            item = WikimediaItem()\n            item.name = url.split(\"/\")[-1]\n            item.base64 = page.file(url)\n            return item\n\n\n    start_pages = [ Page(\"https://commons.wikimedia.org/wiki/File:Records.svg\", EncodedScraper()) ]\n    crawler = XCrawler(start_pages)\n    crawler.config.output_file_name = \"wikimedia_file_example_output.csv\"\n    crawler.run()\n\nSession Example\n----------------\n.. code:: python\n\n    from xcrawler import XCrawler, Page, PageScraper\n    from requests.auth import HTTPBasicAuth\n\n\n    class Scraper(PageScraper):\n        def extract(self, page):\n            return page.__str__()\n\n\n    start_pages = [ Page(\"http://192.168.1.1/\", Scraper()) ]\n    crawler = XCrawler(start_pages)\n    crawler.config.output_file_name = \"router_session_example_output.csv\"\n    crawler.config.session.headers = {\"User-Agent\": \"Custom User Agent\",\n                                      \"Accept-Language\": \"fr\"}\n    crawler.config.session.auth = HTTPBasicAuth('admin', 'admin')\n    crawler.run()\n\nRequest Example\n----------------\n.. code:: python\n\n    from xcrawler import XCrawler, Page, PageScraper\n\n\n    class Scraper(PageScraper):\n        def extract(self, page):\n            return page.__str__()\n\n\n    start_page = Page(\"http://192.168.5.5\", Scraper())\n    start_page.request.cookies = {\"theme\": \"classic\"}\n    crawler = XCrawler([start_page])\n    crawler.config.request_timeout = (5, 5)\n    crawler.config.output_file_name = \"router_request_example_output.csv\"\n    crawler.run()\n\nDocumentation\n--------------\n| For more information about xcrawler see the source code and Python Docstrings: `source code <https://github.com/cardsurf/xcrawler/tree/master/xcrawler/core/>`_\n| The documentation can also be accessed at runtime with Python's built-in ``help`` function:\n\n.. code:: python\n\n    >>> import xcrawler\n    >>> help(xcrawler.Config)\n        # Information about the Config class\n    >>> help(xcrawler.PageScraper.extract)\n        # Information about the extract method of the PageScraper class\n\nLicence\n-------\nGNU GPL v2.0", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/cardsurf/xcrawler", "keywords": "crawler,spider,scraper", "license": "GNU GPL v2.0", "maintainer": "", "maintainer_email": "", "name": "xcrawler", "package_url": "https://pypi.org/project/xcrawler/", "platform": "Any", "project_url": "https://pypi.org/project/xcrawler/", "project_urls": {"Homepage": "https://github.com/cardsurf/xcrawler"}, "release_url": "https://pypi.org/project/xcrawler/1.3.0/", "requires_dist": null, "requires_python": "", "summary": "A multi-threaded, open source web crawler", "version": "1.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>A multi-threaded, open source web crawler</p>\n<div id=\"features\">\n<h2>Features</h2>\n<ul>\n<li>Use multiple threads to visit web pages</li>\n<li>Extract web page data using XPath expressions or CSS selectors</li>\n<li>Extract urls from a web page and visit extracted urls</li>\n<li>Write extracted data to an output file</li>\n<li>Set HTTP session parameters such as: cookies, SSL certificates, proxies</li>\n<li>Set HTTP request parameters such as: header, body, authentication</li>\n<li>Download files from the urls</li>\n<li>Supports Python 2 and Python 3</li>\n</ul>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<pre>pip install xcrawler\n</pre>\n<div>\n<div><br></div>\n<div>When installing <tt>lxml</tt> library on Windows you may encounter <tt>Microsoft Visual C++ is required</tt> errors.</div>\n<div>To install <tt>lxml</tt> library on Windows:</div>\n</div>\n<ol>\n<li>Download and install Microsoft Windows SDK:<ul>\n<li>For Python 2.6, 2.7, 3.0, 3.1, 3.2: <a href=\"http://www.microsoft.com/en-us/download/confirmation.aspx?id=3138\" rel=\"nofollow\">Microsoft Windows SDK for .NET Framework 3.5 SP1</a></li>\n<li>For Python 3.3, 3.4: <a href=\"http://www.microsoft.com/en-us/download/confirmation.aspx?id=8279\" rel=\"nofollow\">Microsoft Windows SDK for .NET Framework 4.0</a></li>\n</ul>\n</li>\n<li>Click the Start Menu, search for and open the command prompt:<ul>\n<li>For Python 2.6, 2.7, 3.0, 3.1, 3.2: <tt>CMD Shell</tt></li>\n<li>For Python 3.3, 3.4: <tt>Windows SDK 7.1 Command Prompt</tt></li>\n</ul>\n</li>\n<li>Install <tt>lxml</tt></li>\n</ol>\n<pre>setenv /x86 /release &amp;&amp; SET DISTUTILS_USE_SDK=1 &amp;&amp; set STATICBUILD=true &amp;&amp; pip install lxml\n</pre>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<div>\n<div>Data and urls are extracted from a web page by a page scraper.</div>\n<div>To extract data and urls from a web page use the following methods:</div>\n<div><tt>extract</tt> returns data extracted from a web page</div>\n<div><tt>visit</tt> returns next Pages to be visited</div>\n<div><br></div>\n<div>A crawler can be configured before crawling web pages. A user can configure such settings of the crawler as:</div>\n<div>* the number of threads used to visit web pages</div>\n<div>* the name of an output file</div>\n<div>* the request timeout</div>\n<div>To run the crawler call:</div>\n<div><tt>crawler.run()</tt></div>\n<div><br></div>\n<div>Examples how to use xcrawler can be found at: <a href=\"https://github.com/cardsurf/xcrawler/tree/master/examples\" rel=\"nofollow\">https://github.com/cardsurf/xcrawler/tree/master/examples</a></div>\n<div><br></div>\n</div>\n</div>\n<div id=\"xpath-example\">\n<h2>XPath Example</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">xcrawler</span> <span class=\"kn\">import</span> <span class=\"n\">XCrawler</span><span class=\"p\">,</span> <span class=\"n\">Page</span><span class=\"p\">,</span> <span class=\"n\">PageScraper</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">Scraper</span><span class=\"p\">(</span><span class=\"n\">PageScraper</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">extract</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">page</span><span class=\"p\">):</span>\n        <span class=\"n\">topics</span> <span class=\"o\">=</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">xpath</span><span class=\"p\">(</span><span class=\"s2\">\"//a[@class='question-hyperlink']/text()\"</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">topics</span>\n\n\n<span class=\"n\">start_pages</span> <span class=\"o\">=</span> <span class=\"p\">[</span> <span class=\"n\">Page</span><span class=\"p\">(</span><span class=\"s2\">\"http://stackoverflow.com/questions/16622802/center-image-within-div\"</span><span class=\"p\">,</span> <span class=\"n\">Scraper</span><span class=\"p\">())</span> <span class=\"p\">]</span>\n<span class=\"n\">crawler</span> <span class=\"o\">=</span> <span class=\"n\">XCrawler</span><span class=\"p\">(</span><span class=\"n\">start_pages</span><span class=\"p\">)</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">output_file_name</span> <span class=\"o\">=</span> <span class=\"s2\">\"stackoverflow_example_crawler_output.csv\"</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n</pre>\n</div>\n<div id=\"css-example\">\n<h2>CSS Example</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">xcrawler</span> <span class=\"kn\">import</span> <span class=\"n\">XCrawler</span><span class=\"p\">,</span> <span class=\"n\">Page</span><span class=\"p\">,</span> <span class=\"n\">PageScraper</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">StackOverflowItem</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">title</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">votes</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">tags</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">UrlsScraper</span><span class=\"p\">(</span><span class=\"n\">PageScraper</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">visit</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">page</span><span class=\"p\">):</span>\n        <span class=\"n\">hrefs</span> <span class=\"o\">=</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">css_attr</span><span class=\"p\">(</span><span class=\"s2\">\".question-summary h3 a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"href\"</span><span class=\"p\">)</span>\n        <span class=\"n\">urls</span> <span class=\"o\">=</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">to_urls</span><span class=\"p\">(</span><span class=\"n\">hrefs</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"n\">Page</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">QuestionScraper</span><span class=\"p\">())</span> <span class=\"k\">for</span> <span class=\"n\">url</span> <span class=\"ow\">in</span> <span class=\"n\">urls</span><span class=\"p\">]</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">QuestionScraper</span><span class=\"p\">(</span><span class=\"n\">PageScraper</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">extract</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">page</span><span class=\"p\">):</span>\n        <span class=\"n\">item</span> <span class=\"o\">=</span> <span class=\"n\">StackOverflowItem</span><span class=\"p\">()</span>\n        <span class=\"n\">item</span><span class=\"o\">.</span><span class=\"n\">title</span> <span class=\"o\">=</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">css_text</span><span class=\"p\">(</span><span class=\"s2\">\"h1 a\"</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n        <span class=\"n\">item</span><span class=\"o\">.</span><span class=\"n\">votes</span> <span class=\"o\">=</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">css_text</span><span class=\"p\">(</span><span class=\"s2\">\".question .vote-count-post\"</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span>\n        <span class=\"n\">item</span><span class=\"o\">.</span><span class=\"n\">tags</span> <span class=\"o\">=</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">css_text</span><span class=\"p\">(</span><span class=\"s2\">\".question .post-tag\"</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n        <span class=\"n\">item</span><span class=\"o\">.</span><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">url</span>\n        <span class=\"k\">return</span> <span class=\"n\">item</span>\n\n\n<span class=\"n\">start_pages</span> <span class=\"o\">=</span> <span class=\"p\">[</span> <span class=\"n\">Page</span><span class=\"p\">(</span><span class=\"s2\">\"http://stackoverflow.com/questions?sort=votes\"</span><span class=\"p\">,</span> <span class=\"n\">UrlsScraper</span><span class=\"p\">())</span> <span class=\"p\">]</span>\n<span class=\"n\">crawler</span> <span class=\"o\">=</span> <span class=\"n\">XCrawler</span><span class=\"p\">(</span><span class=\"n\">start_pages</span><span class=\"p\">)</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">output_file_name</span> <span class=\"o\">=</span> <span class=\"s2\">\"stackoverflow_css_crawler_output.csv\"</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">number_of_threads</span> <span class=\"o\">=</span> <span class=\"mi\">3</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n</pre>\n</div>\n<div id=\"file-example\">\n<h2>File Example</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">xcrawler</span> <span class=\"kn\">import</span> <span class=\"n\">XCrawler</span><span class=\"p\">,</span> <span class=\"n\">Page</span><span class=\"p\">,</span> <span class=\"n\">PageScraper</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">WikimediaItem</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">base64</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">EncodedScraper</span><span class=\"p\">(</span><span class=\"n\">PageScraper</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">extract</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">page</span><span class=\"p\">):</span>\n        <span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">xpath</span><span class=\"p\">(</span><span class=\"s2\">\"//div[@class='fullImageLink']/a/@href\"</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n        <span class=\"n\">item</span> <span class=\"o\">=</span> <span class=\"n\">WikimediaItem</span><span class=\"p\">()</span>\n        <span class=\"n\">item</span><span class=\"o\">.</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"n\">url</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s2\">\"/\"</span><span class=\"p\">)[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n        <span class=\"n\">item</span><span class=\"o\">.</span><span class=\"n\">base64</span> <span class=\"o\">=</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"n\">file</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">item</span>\n\n\n<span class=\"n\">start_pages</span> <span class=\"o\">=</span> <span class=\"p\">[</span> <span class=\"n\">Page</span><span class=\"p\">(</span><span class=\"s2\">\"https://commons.wikimedia.org/wiki/File:Records.svg\"</span><span class=\"p\">,</span> <span class=\"n\">EncodedScraper</span><span class=\"p\">())</span> <span class=\"p\">]</span>\n<span class=\"n\">crawler</span> <span class=\"o\">=</span> <span class=\"n\">XCrawler</span><span class=\"p\">(</span><span class=\"n\">start_pages</span><span class=\"p\">)</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">output_file_name</span> <span class=\"o\">=</span> <span class=\"s2\">\"wikimedia_file_example_output.csv\"</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n</pre>\n</div>\n<div id=\"session-example\">\n<h2>Session Example</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">xcrawler</span> <span class=\"kn\">import</span> <span class=\"n\">XCrawler</span><span class=\"p\">,</span> <span class=\"n\">Page</span><span class=\"p\">,</span> <span class=\"n\">PageScraper</span>\n<span class=\"kn\">from</span> <span class=\"nn\">requests.auth</span> <span class=\"kn\">import</span> <span class=\"n\">HTTPBasicAuth</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">Scraper</span><span class=\"p\">(</span><span class=\"n\">PageScraper</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">extract</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">page</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"fm\">__str__</span><span class=\"p\">()</span>\n\n\n<span class=\"n\">start_pages</span> <span class=\"o\">=</span> <span class=\"p\">[</span> <span class=\"n\">Page</span><span class=\"p\">(</span><span class=\"s2\">\"http://192.168.1.1/\"</span><span class=\"p\">,</span> <span class=\"n\">Scraper</span><span class=\"p\">())</span> <span class=\"p\">]</span>\n<span class=\"n\">crawler</span> <span class=\"o\">=</span> <span class=\"n\">XCrawler</span><span class=\"p\">(</span><span class=\"n\">start_pages</span><span class=\"p\">)</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">output_file_name</span> <span class=\"o\">=</span> <span class=\"s2\">\"router_session_example_output.csv\"</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">headers</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"User-Agent\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Custom User Agent\"</span><span class=\"p\">,</span>\n                                  <span class=\"s2\">\"Accept-Language\"</span><span class=\"p\">:</span> <span class=\"s2\">\"fr\"</span><span class=\"p\">}</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">auth</span> <span class=\"o\">=</span> <span class=\"n\">HTTPBasicAuth</span><span class=\"p\">(</span><span class=\"s1\">'admin'</span><span class=\"p\">,</span> <span class=\"s1\">'admin'</span><span class=\"p\">)</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n</pre>\n</div>\n<div id=\"request-example\">\n<h2>Request Example</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">xcrawler</span> <span class=\"kn\">import</span> <span class=\"n\">XCrawler</span><span class=\"p\">,</span> <span class=\"n\">Page</span><span class=\"p\">,</span> <span class=\"n\">PageScraper</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">Scraper</span><span class=\"p\">(</span><span class=\"n\">PageScraper</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">extract</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">page</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">page</span><span class=\"o\">.</span><span class=\"fm\">__str__</span><span class=\"p\">()</span>\n\n\n<span class=\"n\">start_page</span> <span class=\"o\">=</span> <span class=\"n\">Page</span><span class=\"p\">(</span><span class=\"s2\">\"http://192.168.5.5\"</span><span class=\"p\">,</span> <span class=\"n\">Scraper</span><span class=\"p\">())</span>\n<span class=\"n\">start_page</span><span class=\"o\">.</span><span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">cookies</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"theme\"</span><span class=\"p\">:</span> <span class=\"s2\">\"classic\"</span><span class=\"p\">}</span>\n<span class=\"n\">crawler</span> <span class=\"o\">=</span> <span class=\"n\">XCrawler</span><span class=\"p\">([</span><span class=\"n\">start_page</span><span class=\"p\">])</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">request_timeout</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">output_file_name</span> <span class=\"o\">=</span> <span class=\"s2\">\"router_request_example_output.csv\"</span>\n<span class=\"n\">crawler</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n</pre>\n</div>\n<div id=\"documentation\">\n<h2>Documentation</h2>\n<div>\n<div>For more information about xcrawler see the source code and Python Docstrings: <a href=\"https://github.com/cardsurf/xcrawler/tree/master/xcrawler/core/\" rel=\"nofollow\">source code</a></div>\n<div>The documentation can also be accessed at runtime with Python\u2019s built-in <tt>help</tt> function:</div>\n</div>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">xcrawler</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">help</span><span class=\"p\">(</span><span class=\"n\">xcrawler</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">)</span>\n    <span class=\"c1\"># Information about the Config class</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">help</span><span class=\"p\">(</span><span class=\"n\">xcrawler</span><span class=\"o\">.</span><span class=\"n\">PageScraper</span><span class=\"o\">.</span><span class=\"n\">extract</span><span class=\"p\">)</span>\n    <span class=\"c1\"># Information about the extract method of the PageScraper class</span>\n</pre>\n</div>\n<div id=\"licence\">\n<h2>Licence</h2>\n<p>GNU GPL v2.0</p>\n</div>\n\n          </div>"}, "last_serial": 1910351, "releases": {"1.1.0": [{"comment_text": "", "digests": {"md5": "a856999ff97fe28ee39c1bd9c669ccfc", "sha256": "7be5885e847a90b620b44fa464f0605e268b8bae646dfc3cec54c49924d7cffb"}, "downloads": -1, "filename": "xcrawler-1.1.0.zip", "has_sig": false, "md5_digest": "a856999ff97fe28ee39c1bd9c669ccfc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 30251, "upload_time": "2015-09-29T11:15:22", "upload_time_iso_8601": "2015-09-29T11:15:22.369890Z", "url": "https://files.pythonhosted.org/packages/25/02/4d7af4f5ab913539cc64bb82dc903c8d784414fcb8ccbf7a5ec5d6babdd9/xcrawler-1.1.0.zip", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "f5f45e88212a69f8918a25e8cf01d992", "sha256": "f665a36cdef3103dbe46b75533a44ec5aa06b11e78b42cf80e15921f0b9742e5"}, "downloads": -1, "filename": "xcrawler-1.2.0.zip", "has_sig": false, "md5_digest": "f5f45e88212a69f8918a25e8cf01d992", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 71169, "upload_time": "2015-10-14T10:34:43", "upload_time_iso_8601": "2015-10-14T10:34:43.694160Z", "url": "https://files.pythonhosted.org/packages/3d/72/e167eb7c69a6d24a0f77199324164fc71317d688f775c3a88122f518c79a/xcrawler-1.2.0.zip", "yanked": false}], "1.3.0": [{"comment_text": "", "digests": {"md5": "e0585a2c4d97eb13d70e631a706d2719", "sha256": "2536c3a903384fc727f35b4a73ab6a1b9e153a9e5586eabfa0b13bee55a7e2c4"}, "downloads": -1, "filename": "xcrawler-1.3.0.tar.gz", "has_sig": false, "md5_digest": "e0585a2c4d97eb13d70e631a706d2719", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29971, "upload_time": "2016-01-18T19:00:28", "upload_time_iso_8601": "2016-01-18T19:00:28.739317Z", "url": "https://files.pythonhosted.org/packages/e6/22/c42c3907f45bc36c3e8c04f8ec95eed62659e7c720e4db0933ad5e120a4d/xcrawler-1.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e0585a2c4d97eb13d70e631a706d2719", "sha256": "2536c3a903384fc727f35b4a73ab6a1b9e153a9e5586eabfa0b13bee55a7e2c4"}, "downloads": -1, "filename": "xcrawler-1.3.0.tar.gz", "has_sig": false, "md5_digest": "e0585a2c4d97eb13d70e631a706d2719", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29971, "upload_time": "2016-01-18T19:00:28", "upload_time_iso_8601": "2016-01-18T19:00:28.739317Z", "url": "https://files.pythonhosted.org/packages/e6/22/c42c3907f45bc36c3e8c04f8ec95eed62659e7c720e4db0933ad5e120a4d/xcrawler-1.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:26:00 2020"}