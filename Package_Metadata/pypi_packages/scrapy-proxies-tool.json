{"info": {"author": "AnJia", "author_email": "anjia0532@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "Random proxy middleware for Scrapy (http://scrapy.org/)\n=======================================================\n\n**base on https://github.com/aivarsk/scrapy-proxies , support  load proxies from https://github.com/qiyeboy/IPProxyPool**\n\nProcesses Scrapy requests using a random proxy from list to avoid IP ban and\nimprove crawling speed.\n\nGet your proxy list from sites like http://www.hidemyass.com/ (copy-paste into text file\nand reformat to http://host:port format)\n\nInstall\n--------\n\nThe quick way:\n\n```bash\npip install scrapy-proxies-tool\n```\n\nOr checkout the source and run\n\n```bash\npython setup.py install\n```\n\nsettings.py\n-----------\n\n```python\n\n# Retry many times since proxies often fail\nRETRY_TIMES = 10\n# Retry on most error codes since proxies fail for different reasons\nRETRY_HTTP_CODES = [500, 503, 504, 400, 403, 404, 408]\n\nDOWNLOADER_MIDDLEWARES = {\n  'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,\n  'scrapy_proxies.RandomProxy': 100,\n  'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,\n}\n\nPROXY_SETTINGS = {\n  # Proxy list containing entries like\n  # http://host1:port\n  # http://username:password@host2:port\n  # http://host3:port\n  # ...\n  # if PROXY_SETTINGS[from_proxies_server] = True , proxy_list is server address (ref https://github.com/qiyeboy/IPProxyPool and https://github.com/awolfly9/IPProxyTool )\n  # Only support http(ref https://github.com/qiyeboy/IPProxyPool#%E5%8F%82%E6%95%B0)\n  # list : ['http://localhost:8000?protocol=0'],\n  'list':['/path/to/proxy/list.txt'],\n\n  # disable proxy settings and  use real ip when all proxies are unusable\n  'use_real_when_empty':False,\n  'from_proxies_server':False,\n\n  # If proxy mode is 2 uncomment this sentence :\n  # 'custom_proxy': \"http://host1:port\",\n\n  # Proxy mode\n  # 0 = Every requests have different proxy\n  # 1 = Take only one proxy from the list and assign it to every requests\n  # 2 = Put a custom proxy to use in the settings\n  'mode':0\n}\n```\n\nFor older versions of Scrapy (before 1.0.0) you have to use\n`scrapy.contrib.downloadermiddleware.retry.RetryMiddleware` and\n`scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware`\nmiddlewares instead.\n\n\nYour spider\n-----------\n\nIn each callback ensure that proxy /really/ returned your target page by\nchecking for site logo or some other significant element.\nIf not - retry request with dont_filter=True\n\n```python\n  if not hxs.select('//get/site/logo'):\n    yield Request(url=response.url, dont_filter=True)\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/anjia0532/scrapy-proxies", "keywords": "Scrapy,scrapy-proxies,proxies,IPProxyTool", "license": "MIT Licence", "maintainer": "", "maintainer_email": "", "name": "scrapy-proxies-tool", "package_url": "https://pypi.org/project/scrapy-proxies-tool/", "platform": "", "project_url": "https://pypi.org/project/scrapy-proxies-tool/", "project_urls": {"Homepage": "https://github.com/anjia0532/scrapy-proxies"}, "release_url": "https://pypi.org/project/scrapy-proxies-tool/0.4.0/", "requires_dist": null, "requires_python": "", "summary": "Scrapy Proxies: random proxy middleware for Scrapy(support load proxies from IPProxyTool)", "version": "0.4.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Random proxy middleware for Scrapy (<a href=\"http://scrapy.org/\" rel=\"nofollow\">http://scrapy.org/</a>)</h1>\n<p><strong>base on <a href=\"https://github.com/aivarsk/scrapy-proxies\" rel=\"nofollow\">https://github.com/aivarsk/scrapy-proxies</a> , support  load proxies from <a href=\"https://github.com/qiyeboy/IPProxyPool\" rel=\"nofollow\">https://github.com/qiyeboy/IPProxyPool</a></strong></p>\n<p>Processes Scrapy requests using a random proxy from list to avoid IP ban and\nimprove crawling speed.</p>\n<p>Get your proxy list from sites like <a href=\"http://www.hidemyass.com/\" rel=\"nofollow\">http://www.hidemyass.com/</a> (copy-paste into text file\nand reformat to <a href=\"http://host:port\" rel=\"nofollow\">http://host:port</a> format)</p>\n<h2>Install</h2>\n<p>The quick way:</p>\n<pre>pip install scrapy-proxies-tool\n</pre>\n<p>Or checkout the source and run</p>\n<pre>python setup.py install\n</pre>\n<h2>settings.py</h2>\n<pre><span class=\"c1\"># Retry many times since proxies often fail</span>\n<span class=\"n\">RETRY_TIMES</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n<span class=\"c1\"># Retry on most error codes since proxies fail for different reasons</span>\n<span class=\"n\">RETRY_HTTP_CODES</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">500</span><span class=\"p\">,</span> <span class=\"mi\">503</span><span class=\"p\">,</span> <span class=\"mi\">504</span><span class=\"p\">,</span> <span class=\"mi\">400</span><span class=\"p\">,</span> <span class=\"mi\">403</span><span class=\"p\">,</span> <span class=\"mi\">404</span><span class=\"p\">,</span> <span class=\"mi\">408</span><span class=\"p\">]</span>\n\n<span class=\"n\">DOWNLOADER_MIDDLEWARES</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"s1\">'scrapy.downloadermiddlewares.retry.RetryMiddleware'</span><span class=\"p\">:</span> <span class=\"mi\">90</span><span class=\"p\">,</span>\n  <span class=\"s1\">'scrapy_proxies.RandomProxy'</span><span class=\"p\">:</span> <span class=\"mi\">100</span><span class=\"p\">,</span>\n  <span class=\"s1\">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware'</span><span class=\"p\">:</span> <span class=\"mi\">110</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">PROXY_SETTINGS</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"c1\"># Proxy list containing entries like</span>\n  <span class=\"c1\"># http://host1:port</span>\n  <span class=\"c1\"># http://username:password@host2:port</span>\n  <span class=\"c1\"># http://host3:port</span>\n  <span class=\"c1\"># ...</span>\n  <span class=\"c1\"># if PROXY_SETTINGS[from_proxies_server] = True , proxy_list is server address (ref https://github.com/qiyeboy/IPProxyPool and https://github.com/awolfly9/IPProxyTool )</span>\n  <span class=\"c1\"># Only support http(ref https://github.com/qiyeboy/IPProxyPool#%E5%8F%82%E6%95%B0)</span>\n  <span class=\"c1\"># list : ['http://localhost:8000?protocol=0'],</span>\n  <span class=\"s1\">'list'</span><span class=\"p\">:[</span><span class=\"s1\">'/path/to/proxy/list.txt'</span><span class=\"p\">],</span>\n\n  <span class=\"c1\"># disable proxy settings and  use real ip when all proxies are unusable</span>\n  <span class=\"s1\">'use_real_when_empty'</span><span class=\"p\">:</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n  <span class=\"s1\">'from_proxies_server'</span><span class=\"p\">:</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n\n  <span class=\"c1\"># If proxy mode is 2 uncomment this sentence :</span>\n  <span class=\"c1\"># 'custom_proxy': \"http://host1:port\",</span>\n\n  <span class=\"c1\"># Proxy mode</span>\n  <span class=\"c1\"># 0 = Every requests have different proxy</span>\n  <span class=\"c1\"># 1 = Take only one proxy from the list and assign it to every requests</span>\n  <span class=\"c1\"># 2 = Put a custom proxy to use in the settings</span>\n  <span class=\"s1\">'mode'</span><span class=\"p\">:</span><span class=\"mi\">0</span>\n<span class=\"p\">}</span>\n</pre>\n<p>For older versions of Scrapy (before 1.0.0) you have to use\n<code>scrapy.contrib.downloadermiddleware.retry.RetryMiddleware</code> and\n<code>scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware</code>\nmiddlewares instead.</p>\n<h2>Your spider</h2>\n<p>In each callback ensure that proxy /really/ returned your target page by\nchecking for site logo or some other significant element.\nIf not - retry request with dont_filter=True</p>\n<pre>  <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">hxs</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"s1\">'//get/site/logo'</span><span class=\"p\">):</span>\n    <span class=\"k\">yield</span> <span class=\"n\">Request</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"o\">=</span><span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">dont_filter</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n\n          </div>"}, "last_serial": 4275306, "releases": {"0.3.0": [{"comment_text": "", "digests": {"md5": "5c009894d847dcdc0baa5f1f9d8800af", "sha256": "2292cf063a1f00db93bf0733c06d632e60de88c4933b68207b62f81581da2539"}, "downloads": -1, "filename": "scrapy-proxies-tool-0.3.0.tar.gz", "has_sig": false, "md5_digest": "5c009894d847dcdc0baa5f1f9d8800af", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4291, "upload_time": "2018-09-15T17:06:08", "upload_time_iso_8601": "2018-09-15T17:06:08.724374Z", "url": "https://files.pythonhosted.org/packages/06/f8/98a376ef4efb28401dd06ad3b561122a73e31fd0e3e3d0d7e82facf6dfb0/scrapy-proxies-tool-0.3.0.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "42faf711cde6fd74f28394bdca1b93a8", "sha256": "1b5e2614b91625f52906b6f990b86efcafc3141eb3886f0de5978bbe63523915"}, "downloads": -1, "filename": "scrapy-proxies-tool-0.4.0.tar.gz", "has_sig": false, "md5_digest": "42faf711cde6fd74f28394bdca1b93a8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4332, "upload_time": "2018-09-15T18:05:42", "upload_time_iso_8601": "2018-09-15T18:05:42.438551Z", "url": "https://files.pythonhosted.org/packages/d6/9a/529796b44339608ff26026831bf8ebf123a804e001535ef8239f75329416/scrapy-proxies-tool-0.4.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "42faf711cde6fd74f28394bdca1b93a8", "sha256": "1b5e2614b91625f52906b6f990b86efcafc3141eb3886f0de5978bbe63523915"}, "downloads": -1, "filename": "scrapy-proxies-tool-0.4.0.tar.gz", "has_sig": false, "md5_digest": "42faf711cde6fd74f28394bdca1b93a8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4332, "upload_time": "2018-09-15T18:05:42", "upload_time_iso_8601": "2018-09-15T18:05:42.438551Z", "url": "https://files.pythonhosted.org/packages/d6/9a/529796b44339608ff26026831bf8ebf123a804e001535ef8239f75329416/scrapy-proxies-tool-0.4.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:44 2020"}