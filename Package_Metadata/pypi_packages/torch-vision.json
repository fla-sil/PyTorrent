{"info": {"author": "PyTorch Core Team", "author_email": "soumith@pytorch.org", "bugtrack_url": null, "classifiers": [], "description": "torch-vision\n============\n\nThis repository consists of:\n\n-  `vision.datasets <#datasets>`__ : Data loaders for popular vision\n   datasets\n-  `vision.models <#models>`__ : Definitions for popular model\n   architectures, such as AlexNet, VGG, and ResNet and pre-trained\n   models.\n-  `vision.transforms <#transforms>`__ : Common image transformations\n   such as random crop, rotations etc.\n-  `vision.utils <#utils>`__ : Useful stuff such as saving tensor (3 x H\n   x W) as image to disk, given a mini-batch creating a grid of images,\n   etc.\n\nInstallation\n============\n\nBinaries:\n\n.. code:: bash\n\n    conda install torchvision -c https://conda.anaconda.org/t/6N-MsQ4WZ7jo/soumith\n\nFrom Source:\n\n.. code:: bash\n\n    pip install -r requirements.txt\n    pip install .\n\nDatasets\n========\n\nThe following dataset loaders are available:\n\n-  `COCO (Captioning and Detection) <#coco>`__\n-  `LSUN Classification <#lsun>`__\n-  `ImageFolder <#imagefolder>`__\n-  `Imagenet-12 <#imagenet-12>`__\n-  `CIFAR10 and CIFAR100 <#cifar>`__\n\nDatasets have the API: - ``__getitem__`` - ``__len__`` They all subclass\nfrom ``torch.utils.data.Dataset`` Hence, they can all be multi-threaded\n(python multiprocessing) using standard torch.utils.data.DataLoader.\n\nFor example:\n\n``torch.utils.data.DataLoader(coco_cap, batch_size=args.batchSize, shuffle=True, num_workers=args.nThreads)``\n\nIn the constructor, each dataset has a slightly different API as needed,\nbut they all take the keyword args:\n\n-  ``transform`` - a function that takes in an image and returns a\n   transformed version\n-  common stuff like ``ToTensor``, ``RandomCrop``, etc. These can be\n   composed together with ``transforms.Compose`` (see transforms section\n   below)\n-  ``target_transform`` - a function that takes in the target and\n   transforms it. For example, take in the caption string and return a\n   tensor of word indices.\n\nCOCO\n~~~~\n\nThis requires the `COCO API to be\ninstalled <https://github.com/pdollar/coco/tree/master/PythonAPI>`__\n\nCaptions:\n^^^^^^^^^\n\n``dset.CocoCaptions(root=\"dir where images are\", annFile=\"json annotation file\", [transform, target_transform])``\n\nExample:\n\n.. code:: python\n\n    import torchvision.datasets as dset\n    import torchvision.transforms as transforms\n    cap = dset.CocoCaptions(root = 'dir where images are',\n                            annFile = 'json annotation file',\n                            transform=transforms.ToTensor())\n\n    print('Number of samples: ', len(cap))\n    img, target = cap[3] # load 4th sample\n\n    print(\"Image Size: \", img.size())\n    print(target)\n\nOutput:\n\n::\n\n    Number of samples: 82783\n    Image Size: (3L, 427L, 640L)\n    [u'A plane emitting smoke stream flying over a mountain.',\n    u'A plane darts across a bright blue sky behind a mountain covered in snow',\n    u'A plane leaves a contrail above the snowy mountain top.',\n    u'A mountain that has a plane flying overheard in the distance.',\n    u'A mountain view with a plume of smoke in the background']\n\nDetection:\n^^^^^^^^^^\n\n``dset.CocoDetection(root=\"dir where images are\", annFile=\"json annotation file\", [transform, target_transform])``\n\nLSUN\n~~~~\n\n``dset.LSUN(db_path, classes='train', [transform, target_transform])``\n\n-  db\\_path = root directory for the database files\n-  classes =\n-  'train' - all categories, training set\n-  'val' - all categories, validation set\n-  'test' - all categories, test set\n-  ['bedroom\\_train', 'church\\_train', ...] : a list of categories to\n   load\n\nCIFAR\n~~~~~\n\n``dset.CIFAR10(root, train=True, transform=None, target_transform=None, download=False)``\n\n``dset.CIFAR100(root, train=True, transform=None, target_transform=None, download=False)``\n\n-  ``root`` : root directory of dataset where there is folder\n   ``cifar-10-batches-py``\n-  ``train`` : ``True`` = Training set, ``False`` = Test set\n-  ``download`` : ``True`` = downloads the dataset from the internet and\n   puts it in root directory. If dataset already downloaded, does not do\n   anything.\n\nImageFolder\n~~~~~~~~~~~\n\nA generic data loader where the images are arranged in this way:\n\n::\n\n    root/dog/xxx.png\n    root/dog/xxy.png\n    root/dog/xxz.png\n\n    root/cat/123.png\n    root/cat/nsdf3.png\n    root/cat/asd932_.png\n\n``dset.ImageFolder(root=\"root folder path\", [transform, target_transform])``\n\nIt has the members:\n\n-  ``self.classes`` - The class names as a list\n-  ``self.class_to_idx`` - Corresponding class indices\n-  ``self.imgs`` - The list of (image path, class-index) tuples\n\nImagenet-12\n~~~~~~~~~~~\n\nThis is simply implemented with an ImageFolder dataset.\n\nThe data is preprocessed `as described\nhere <https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset>`__\n\n`Here is an\nexample <https://github.com/pytorch/examples/blob/27e2a46c1d1505324032b1d94fc6ce24d5b67e97/imagenet/main.py#L48-L62>`__.\n\nModels\n======\n\nThe models subpackage contains definitions for the following model\narchitectures:\n\n-  `AlexNet <https://arxiv.org/abs/1404.5997>`__: AlexNet variant from\n   the \"One weird trick\" paper.\n-  `VGG <https://arxiv.org/abs/1409.1556>`__: VGG-11, VGG-13, VGG-16,\n   VGG-19 (with and without batch normalization)\n-  `ResNet <https://arxiv.org/abs/1512.03385>`__: ResNet-18, ResNet-34,\n   ResNet-50, ResNet-101, ResNet-152\n\nYou can construct a model with random weights by calling its\nconstructor:\n\n.. code:: python\n\n    import torchvision.models as models\n    resnet18 = models.resnet18()\n    alexnet = models.alexnet()\n\nWe provide pre-trained models for the ResNet variants and AlexNet, using\nthe PyTorch `model zoo <http://pytorch.org/docs/model_zoo.html>`__.\nThese can be constructed by passing ``pretrained=True``:\n\n``python  import torchvision.models as models  resnet18 = models.resnet18(pretrained=True)  alexnet = models.alexnet(pretrained=True)``\n\nTransforms\n==========\n\nTransforms are common image transforms. They can be chained together\nusing ``transforms.Compose``\n\n``transforms.Compose``\n~~~~~~~~~~~~~~~~~~~~~~\n\nOne can compose several transforms together. For example.\n\n.. code:: python\n\n    transform = transforms.Compose([\n        transforms.RandomSizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n                              std = [ 0.229, 0.224, 0.225 ]),\n    ])\n\nTransforms on PIL.Image\n~~~~~~~~~~~~~~~~~~~~~~~\n\n``Scale(size, interpolation=Image.BILINEAR)``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRescales the input PIL.Image to the given 'size'. 'size' will be the\nsize of the smaller edge.\n\nFor example, if height > width, then image will be rescaled to (size \\*\nheight / width, size) - size: size of the smaller edge - interpolation:\nDefault: PIL.Image.BILINEAR\n\n``CenterCrop(size)`` - center-crops the image to the given size\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nCrops the given PIL.Image at the center to have a region of the given\nsize. size can be a tuple (target\\_height, target\\_width) or an integer,\nin which case the target will be of a square shape (size, size)\n\n``RandomCrop(size, padding=0)``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nCrops the given PIL.Image at a random location to have a region of the\ngiven size. size can be a tuple (target\\_height, target\\_width) or an\ninteger, in which case the target will be of a square shape (size, size)\nIf ``padding`` is non-zero, then the image is first zero-padded on each\nside with ``padding`` pixels.\n\n``RandomHorizontalFlip()``\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRandomly horizontally flips the given PIL.Image with a probability of\n0.5\n\n``RandomSizedCrop(size, interpolation=Image.BILINEAR)``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRandom crop the given PIL.Image to a random size of (0.08 to 1.0) of the\noriginal size and and a random aspect ratio of 3/4 to 4/3 of the\noriginal aspect ratio\n\nThis is popularly used to train the Inception networks - size: size of\nthe smaller edge - interpolation: Default: PIL.Image.BILINEAR\n\n``Pad(padding, fill=0)``\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nPads the given image on each side with ``padding`` number of pixels, and\nthe padding pixels are filled with pixel value ``fill``. If a ``5x5``\nimage is padded with ``padding=1`` then it becomes ``7x7``\n\nTransforms on torch.\\*Tensor\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``Normalize(mean, std)``\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nGiven mean: (R, G, B) and std: (R, G, B), will normalize each channel of\nthe torch.\\*Tensor, i.e. channel = (channel - mean) / std\n\nConversion Transforms\n~~~~~~~~~~~~~~~~~~~~~\n\n-  ``ToTensor()`` - Converts a PIL.Image (RGB) or numpy.ndarray (H x W x\n   C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W)\n   in the range [0.0, 1.0]\n-  ``ToPILImage()`` - Converts a torch.\\*Tensor of range [0, 1] and\n   shape C x H x W or numpy ndarray of dtype=uint8, range[0, 255] and\n   shape H x W x C to a PIL.Image of range [0, 255]\n\nGeneric Transofrms\n~~~~~~~~~~~~~~~~~~\n\n``Lambda(lambda)``\n^^^^^^^^^^^^^^^^^^\n\nGiven a Python lambda, applies it to the input ``img`` and returns it.\nFor example:\n\n.. code:: python\n\n    transforms.Lambda(lambda x: x.add(10))\n\nUtils\n=====\n\nmake\\_grid(tensor, nrow=8, padding=2)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nGiven a 4D mini-batch Tensor of shape (B x C x H x W), makes a grid of\nimages\n\nsave\\_image(tensor, filename, nrow=8, padding=2)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSaves a given Tensor into an image file.\n\nIf given a mini-batch tensor, will save the tensor as a grid of images.\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/pytorch/vision", "keywords": "", "license": "BSD", "maintainer": "", "maintainer_email": "", "name": "torch-vision", "package_url": "https://pypi.org/project/torch-vision/", "platform": "", "project_url": "https://pypi.org/project/torch-vision/", "project_urls": {"Homepage": "https://github.com/pytorch/vision"}, "release_url": "https://pypi.org/project/torch-vision/0.1.6.dev0/", "requires_dist": null, "requires_python": "", "summary": "image and video datasets and models for torch deep learning", "version": "0.1.6.dev0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"torch-vision\">\n<h2>torch-vision</h2>\n<p>This repository consists of:</p>\n<ul>\n<li><a href=\"#datasets\" rel=\"nofollow\">vision.datasets</a> : Data loaders for popular vision\ndatasets</li>\n<li><a href=\"#models\" rel=\"nofollow\">vision.models</a> : Definitions for popular model\narchitectures, such as AlexNet, VGG, and ResNet and pre-trained\nmodels.</li>\n<li><a href=\"#transforms\" rel=\"nofollow\">vision.transforms</a> : Common image transformations\nsuch as random crop, rotations etc.</li>\n<li><a href=\"#utils\" rel=\"nofollow\">vision.utils</a> : Useful stuff such as saving tensor (3 x H\nx W) as image to disk, given a mini-batch creating a grid of images,\netc.</li>\n</ul>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>Binaries:</p>\n<pre>conda install torchvision -c https://conda.anaconda.org/t/6N-MsQ4WZ7jo/soumith\n</pre>\n<p>From Source:</p>\n<pre>pip install -r requirements.txt\npip install .\n</pre>\n</div>\n<div id=\"datasets\">\n<h2>Datasets</h2>\n<p>The following dataset loaders are available:</p>\n<ul>\n<li><a href=\"#coco\" rel=\"nofollow\">COCO (Captioning and Detection)</a></li>\n<li><a href=\"#lsun\" rel=\"nofollow\">LSUN Classification</a></li>\n<li><a href=\"#imagefolder\" rel=\"nofollow\">ImageFolder</a></li>\n<li><a href=\"#imagenet-12\" rel=\"nofollow\">Imagenet-12</a></li>\n<li><a href=\"#cifar\" rel=\"nofollow\">CIFAR10 and CIFAR100</a></li>\n</ul>\n<p>Datasets have the API: - <tt>__getitem__</tt> - <tt>__len__</tt> They all subclass\nfrom <tt>torch.utils.data.Dataset</tt> Hence, they can all be multi-threaded\n(python multiprocessing) using standard torch.utils.data.DataLoader.</p>\n<p>For example:</p>\n<p><tt>torch.utils.data.DataLoader(coco_cap, batch_size=args.batchSize, shuffle=True, num_workers=args.nThreads)</tt></p>\n<p>In the constructor, each dataset has a slightly different API as needed,\nbut they all take the keyword args:</p>\n<ul>\n<li><tt>transform</tt> - a function that takes in an image and returns a\ntransformed version</li>\n<li>common stuff like <tt>ToTensor</tt>, <tt>RandomCrop</tt>, etc. These can be\ncomposed together with <tt>transforms.Compose</tt> (see transforms section\nbelow)</li>\n<li><tt>target_transform</tt> - a function that takes in the target and\ntransforms it. For example, take in the caption string and return a\ntensor of word indices.</li>\n</ul>\n<div id=\"coco\">\n<h3>COCO</h3>\n<p>This requires the <a href=\"https://github.com/pdollar/coco/tree/master/PythonAPI\" rel=\"nofollow\">COCO API to be\ninstalled</a></p>\n<div id=\"captions\">\n<h4>Captions:</h4>\n<p><tt><span class=\"pre\">dset.CocoCaptions(root=\"dir</span> where images are\", <span class=\"pre\">annFile=\"json</span> annotation file\", [transform, target_transform])</tt></p>\n<p>Example:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torchvision.datasets</span> <span class=\"k\">as</span> <span class=\"nn\">dset</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision.transforms</span> <span class=\"k\">as</span> <span class=\"nn\">transforms</span>\n<span class=\"n\">cap</span> <span class=\"o\">=</span> <span class=\"n\">dset</span><span class=\"o\">.</span><span class=\"n\">CocoCaptions</span><span class=\"p\">(</span><span class=\"n\">root</span> <span class=\"o\">=</span> <span class=\"s1\">'dir where images are'</span><span class=\"p\">,</span>\n                        <span class=\"n\">annFile</span> <span class=\"o\">=</span> <span class=\"s1\">'json annotation file'</span><span class=\"p\">,</span>\n                        <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">())</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'Number of samples: '</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">cap</span><span class=\"p\">))</span>\n<span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"n\">cap</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]</span> <span class=\"c1\"># load 4th sample</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Image Size: \"</span><span class=\"p\">,</span> <span class=\"n\">img</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"p\">)</span>\n</pre>\n<p>Output:</p>\n<pre>Number of samples: 82783\nImage Size: (3L, 427L, 640L)\n[u'A plane emitting smoke stream flying over a mountain.',\nu'A plane darts across a bright blue sky behind a mountain covered in snow',\nu'A plane leaves a contrail above the snowy mountain top.',\nu'A mountain that has a plane flying overheard in the distance.',\nu'A mountain view with a plume of smoke in the background']\n</pre>\n</div>\n<div id=\"detection\">\n<h4>Detection:</h4>\n<p><tt><span class=\"pre\">dset.CocoDetection(root=\"dir</span> where images are\", <span class=\"pre\">annFile=\"json</span> annotation file\", [transform, target_transform])</tt></p>\n</div>\n</div>\n<div id=\"lsun\">\n<h3>LSUN</h3>\n<p><tt>dset.LSUN(db_path, <span class=\"pre\">classes='train',</span> [transform, target_transform])</tt></p>\n<ul>\n<li>db_path = root directory for the database files</li>\n<li>classes =</li>\n<li>\u2018train\u2019 - all categories, training set</li>\n<li>\u2018val\u2019 - all categories, validation set</li>\n<li>\u2018test\u2019 - all categories, test set</li>\n<li>[\u2018bedroom_train\u2019, \u2018church_train\u2019, \u2026] : a list of categories to\nload</li>\n</ul>\n</div>\n<div id=\"cifar\">\n<h3>CIFAR</h3>\n<p><tt>dset.CIFAR10(root, train=True, transform=None, target_transform=None, download=False)</tt></p>\n<p><tt>dset.CIFAR100(root, train=True, transform=None, target_transform=None, download=False)</tt></p>\n<ul>\n<li><tt>root</tt> : root directory of dataset where there is folder\n<tt><span class=\"pre\">cifar-10-batches-py</span></tt></li>\n<li><tt>train</tt> : <tt>True</tt> = Training set, <tt>False</tt> = Test set</li>\n<li><tt>download</tt> : <tt>True</tt> = downloads the dataset from the internet and\nputs it in root directory. If dataset already downloaded, does not do\nanything.</li>\n</ul>\n</div>\n<div id=\"imagefolder\">\n<h3>ImageFolder</h3>\n<p>A generic data loader where the images are arranged in this way:</p>\n<pre>root/dog/xxx.png\nroot/dog/xxy.png\nroot/dog/xxz.png\n\nroot/cat/123.png\nroot/cat/nsdf3.png\nroot/cat/asd932_.png\n</pre>\n<p><tt><span class=\"pre\">dset.ImageFolder(root=\"root</span> folder path\", [transform, target_transform])</tt></p>\n<p>It has the members:</p>\n<ul>\n<li><tt>self.classes</tt> - The class names as a list</li>\n<li><tt>self.class_to_idx</tt> - Corresponding class indices</li>\n<li><tt>self.imgs</tt> - The list of (image path, class-index) tuples</li>\n</ul>\n</div>\n<div id=\"imagenet-12\">\n<h3>Imagenet-12</h3>\n<p>This is simply implemented with an ImageFolder dataset.</p>\n<p>The data is preprocessed <a href=\"https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset\" rel=\"nofollow\">as described\nhere</a></p>\n<p><a href=\"https://github.com/pytorch/examples/blob/27e2a46c1d1505324032b1d94fc6ce24d5b67e97/imagenet/main.py#L48-L62\" rel=\"nofollow\">Here is an\nexample</a>.</p>\n</div>\n</div>\n<div id=\"models\">\n<h2>Models</h2>\n<p>The models subpackage contains definitions for the following model\narchitectures:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1404.5997\" rel=\"nofollow\">AlexNet</a>: AlexNet variant from\nthe \u201cOne weird trick\u201d paper.</li>\n<li><a href=\"https://arxiv.org/abs/1409.1556\" rel=\"nofollow\">VGG</a>: VGG-11, VGG-13, VGG-16,\nVGG-19 (with and without batch normalization)</li>\n<li><a href=\"https://arxiv.org/abs/1512.03385\" rel=\"nofollow\">ResNet</a>: ResNet-18, ResNet-34,\nResNet-50, ResNet-101, ResNet-152</li>\n</ul>\n<p>You can construct a model with random weights by calling its\nconstructor:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torchvision.models</span> <span class=\"k\">as</span> <span class=\"nn\">models</span>\n<span class=\"n\">resnet18</span> <span class=\"o\">=</span> <span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">resnet18</span><span class=\"p\">()</span>\n<span class=\"n\">alexnet</span> <span class=\"o\">=</span> <span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">alexnet</span><span class=\"p\">()</span>\n</pre>\n<p>We provide pre-trained models for the ResNet variants and AlexNet, using\nthe PyTorch <a href=\"http://pytorch.org/docs/model_zoo.html\" rel=\"nofollow\">model zoo</a>.\nThese can be constructed by passing <tt>pretrained=True</tt>:</p>\n<p><tt>python\u00a0 import torchvision.models as models\u00a0 resnet18 = models.resnet18(pretrained=True)\u00a0 alexnet = models.alexnet(pretrained=True)</tt></p>\n</div>\n<div id=\"transforms\">\n<h2>Transforms</h2>\n<p>Transforms are common image transforms. They can be chained together\nusing <tt>transforms.Compose</tt></p>\n<div id=\"transforms-compose\">\n<h3><tt>transforms.Compose</tt></h3>\n<p>One can compose several transforms together. For example.</p>\n<pre><span class=\"n\">transform</span> <span class=\"o\">=</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">([</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">RandomSizedCrop</span><span class=\"p\">(</span><span class=\"mi\">224</span><span class=\"p\">),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">RandomHorizontalFlip</span><span class=\"p\">(),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">(),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Normalize</span><span class=\"p\">(</span><span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"p\">[</span> <span class=\"mf\">0.485</span><span class=\"p\">,</span> <span class=\"mf\">0.456</span><span class=\"p\">,</span> <span class=\"mf\">0.406</span> <span class=\"p\">],</span>\n                          <span class=\"n\">std</span> <span class=\"o\">=</span> <span class=\"p\">[</span> <span class=\"mf\">0.229</span><span class=\"p\">,</span> <span class=\"mf\">0.224</span><span class=\"p\">,</span> <span class=\"mf\">0.225</span> <span class=\"p\">]),</span>\n<span class=\"p\">])</span>\n</pre>\n</div>\n<div id=\"transforms-on-pil-image\">\n<h3>Transforms on PIL.Image</h3>\n<div id=\"scale-size-interpolation-image-bilinear\">\n<h4><tt>Scale(size, interpolation=Image.BILINEAR)</tt></h4>\n<p>Rescales the input PIL.Image to the given \u2018size\u2019. \u2018size\u2019 will be the\nsize of the smaller edge.</p>\n<p>For example, if height &gt; width, then image will be rescaled to (size *\nheight / width, size) - size: size of the smaller edge - interpolation:\nDefault: PIL.Image.BILINEAR</p>\n</div>\n<div id=\"centercrop-size-center-crops-the-image-to-the-given-size\">\n<h4><tt>CenterCrop(size)</tt> - center-crops the image to the given size</h4>\n<p>Crops the given PIL.Image at the center to have a region of the given\nsize. size can be a tuple (target_height, target_width) or an integer,\nin which case the target will be of a square shape (size, size)</p>\n</div>\n<div id=\"randomcrop-size-padding-0\">\n<h4><tt>RandomCrop(size, padding=0)</tt></h4>\n<p>Crops the given PIL.Image at a random location to have a region of the\ngiven size. size can be a tuple (target_height, target_width) or an\ninteger, in which case the target will be of a square shape (size, size)\nIf <tt>padding</tt> is non-zero, then the image is first zero-padded on each\nside with <tt>padding</tt> pixels.</p>\n</div>\n<div id=\"randomhorizontalflip\">\n<h4><tt>RandomHorizontalFlip()</tt></h4>\n<p>Randomly horizontally flips the given PIL.Image with a probability of\n0.5</p>\n</div>\n<div id=\"randomsizedcrop-size-interpolation-image-bilinear\">\n<h4><tt>RandomSizedCrop(size, interpolation=Image.BILINEAR)</tt></h4>\n<p>Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the\noriginal size and and a random aspect ratio of 3/4 to 4/3 of the\noriginal aspect ratio</p>\n<p>This is popularly used to train the Inception networks - size: size of\nthe smaller edge - interpolation: Default: PIL.Image.BILINEAR</p>\n</div>\n<div id=\"pad-padding-fill-0\">\n<h4><tt>Pad(padding, fill=0)</tt></h4>\n<p>Pads the given image on each side with <tt>padding</tt> number of pixels, and\nthe padding pixels are filled with pixel value <tt>fill</tt>. If a <tt>5x5</tt>\nimage is padded with <tt>padding=1</tt> then it becomes <tt>7x7</tt></p>\n</div>\n</div>\n<div id=\"transforms-on-torch-tensor\">\n<h3>Transforms on torch.*Tensor</h3>\n<h3 id=\"normalize-mean-std\"><span class=\"section-subtitle\"><tt>Normalize(mean, std)</tt></span></h3>\n<p>Given mean: (R, G, B) and std: (R, G, B), will normalize each channel of\nthe torch.*Tensor, i.e. channel = (channel - mean) / std</p>\n</div>\n<div id=\"conversion-transforms\">\n<h3>Conversion Transforms</h3>\n<ul>\n<li><tt>ToTensor()</tt> - Converts a PIL.Image (RGB) or numpy.ndarray (H x W x\nC) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W)\nin the range [0.0, 1.0]</li>\n<li><tt>ToPILImage()</tt> - Converts a torch.*Tensor of range [0, 1] and\nshape C x H x W or numpy ndarray of dtype=uint8, range[0, 255] and\nshape H x W x C to a PIL.Image of range [0, 255]</li>\n</ul>\n</div>\n<div id=\"generic-transofrms\">\n<h3>Generic Transofrms</h3>\n<h3 id=\"lambda-lambda\"><span class=\"section-subtitle\"><tt>Lambda(lambda)</tt></span></h3>\n<p>Given a Python lambda, applies it to the input <tt>img</tt> and returns it.\nFor example:</p>\n<pre><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Lambda</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">))</span>\n</pre>\n</div>\n</div>\n<div id=\"utils\">\n<h2>Utils</h2>\n<div id=\"make-grid-tensor-nrow-8-padding-2\">\n<h3>make_grid(tensor, nrow=8, padding=2)</h3>\n<p>Given a 4D mini-batch Tensor of shape (B x C x H x W), makes a grid of\nimages</p>\n</div>\n<div id=\"save-image-tensor-filename-nrow-8-padding-2\">\n<h3>save_image(tensor, filename, nrow=8, padding=2)</h3>\n<p>Saves a given Tensor into an image file.</p>\n<p>If given a mini-batch tensor, will save the tensor as a grid of images.</p>\n</div>\n</div>\n\n          </div>"}, "last_serial": 2583772, "releases": {"0.1.6.dev0": [{"comment_text": "", "digests": {"md5": "ebcdb6e89dbad46c39514023e6c24464", "sha256": "e5292127b8d9e4b211fdf1a030e61fa56a2f537fe8306963618be6b61d65a80d"}, "downloads": -1, "filename": "torch_vision-0.1.6.dev0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "ebcdb6e89dbad46c39514023e6c24464", "packagetype": "bdist_wheel", "python_version": "3.5", "requires_python": null, "size": 23322, "upload_time": "2017-01-19T00:25:18", "upload_time_iso_8601": "2017-01-19T00:25:18.694631Z", "url": "https://files.pythonhosted.org/packages/ea/13/4942860c32f6877def97c0b432348adce870ae613ed4eb1de10cae0bb018/torch_vision-0.1.6.dev0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "726f4a66c3953ba3971f938cf42b20ea", "sha256": "c3f682bc21ef59da0543aeba8191c6a1bbe95ccf18747b636769078ab3fe86d1"}, "downloads": -1, "filename": "torch-vision-0.1.6.dev0.tar.gz", "has_sig": false, "md5_digest": "726f4a66c3953ba3971f938cf42b20ea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18495, "upload_time": "2017-01-19T00:25:17", "upload_time_iso_8601": "2017-01-19T00:25:17.043872Z", "url": "https://files.pythonhosted.org/packages/e9/5f/c3f0dcafaffd9a481b009eaafd076e7d09cf7e044e204bbd79910672e918/torch-vision-0.1.6.dev0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ebcdb6e89dbad46c39514023e6c24464", "sha256": "e5292127b8d9e4b211fdf1a030e61fa56a2f537fe8306963618be6b61d65a80d"}, "downloads": -1, "filename": "torch_vision-0.1.6.dev0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "ebcdb6e89dbad46c39514023e6c24464", "packagetype": "bdist_wheel", "python_version": "3.5", "requires_python": null, "size": 23322, "upload_time": "2017-01-19T00:25:18", "upload_time_iso_8601": "2017-01-19T00:25:18.694631Z", "url": "https://files.pythonhosted.org/packages/ea/13/4942860c32f6877def97c0b432348adce870ae613ed4eb1de10cae0bb018/torch_vision-0.1.6.dev0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "726f4a66c3953ba3971f938cf42b20ea", "sha256": "c3f682bc21ef59da0543aeba8191c6a1bbe95ccf18747b636769078ab3fe86d1"}, "downloads": -1, "filename": "torch-vision-0.1.6.dev0.tar.gz", "has_sig": false, "md5_digest": "726f4a66c3953ba3971f938cf42b20ea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18495, "upload_time": "2017-01-19T00:25:17", "upload_time_iso_8601": "2017-01-19T00:25:17.043872Z", "url": "https://files.pythonhosted.org/packages/e9/5f/c3f0dcafaffd9a481b009eaafd076e7d09cf7e044e204bbd79910672e918/torch-vision-0.1.6.dev0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:03 2020"}