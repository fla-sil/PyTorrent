{"info": {"author": "Mi\u00f0eind ehf.", "author_email": "mideind@mideind.is", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: Icelandic", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX", "Operating System :: Unix", "Programming Language :: Python", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Text Processing :: Linguistic", "Topic :: Utilities"], "description": "-----------------------------------------\nTokenizer: A tokenizer for Icelandic text\n-----------------------------------------\n\n.. image:: https://travis-ci.com/mideind/Tokenizer.svg?branch=master\n   :target: https://travis-ci.com/mideind/Tokenizer\n\n\nOverview\n--------\n\nTokenization is a necessary first step in many natural language processing\ntasks, such as word counting, parsing, spell checking, corpus generation, and\nstatistical analysis of text.\n\n**Tokenizer** is a compact pure-Python (2 and 3) executable program and module\nfor tokenizing Icelandic text. It converts input text to streams of *tokens*,\nwhere each token is a separate word, punctuation sign, number/amount, date,\ne-mail, URL/URI, etc. It also segments the token stream into sentences,\nconsidering corner cases such as abbreviations and dates in the middle\nof sentences.\n\nThe package contains a dictionary of common Icelandic abbreviations,\nin the file ``src/tokenizer/Abbrev.conf``.\n\nTokenizer is an independent spinoff from the `Greynir project <https://greynir.is>`_\n(GitHub repository `here <https://github.com/mideind/Greynir>`_), by the same authors.\nThe `Greynir natural language parser for Icelandic <https://github.com/mideind/ReynirPackage>`_\nuses Tokenizer on its input.\n\nNote that Tokenizer is licensed under the *MIT* license\nwhile Greynir is licensed under *GPLv3*.\n\n\nDeep vs. shallow tokenization\n-----------------------------\n\nTokenizer can do both *deep* and *shallow* tokenization.\n\n*Shallow* tokenization simply returns each sentence as a string (or as a line\nof text in an output file), where the individual tokens are separated\nby spaces.\n\n*Deep* tokenization returns token objects that have been annotated with\nthe token type and further information extracted from the token, for example\na *(year, month, day)* tuple in the case of date tokens.\n\nIn shallow tokenization, tokens are in most cases kept intact, although\nconsecutive white space is always coalesced. The input strings\n``\"800 MW\"``, ``\"21. jan\u00faar\"`` and ``\"800 7000\"`` thus become\ntwo tokens each, output with a single space between them.\n\nIn deep tokenization, the same strings are represented by single token objects,\nof type ``TOK.MEASUREMENT``, ``TOK.DATEREL`` and ``TOK.TELNO``, respectively.\nThe text associated with a single token object may contain one or more spaces,\nalthough consecutive space is always coalesced.\n\nBy default, the command line tool performs shallow tokenization. If you\nwant deep tokenization with the command line tool, use the ``--json`` or\n``--csv`` switches.\n\n>From Python code, call ``split_into_sentences()`` for shallow tokenization,\nor ``tokenize()`` for deep tokenization. These functions are documented with\nexamples below.\n\n\nInstallation\n------------\n\nTo install:\n\n.. code-block:: console\n\n    $ pip install tokenizer\n\n\nCommand line tool\n-----------------\n\nAfter installation, the tokenizer can be invoked directly from\nthe command line:\n\n.. code-block:: console\n\n    $ tokenize input.txt output.txt\n\nInput and output files are encoded in UTF-8. If the files are not\ngiven explicitly, ``stdin`` and ``stdout`` are used for input and output,\nrespectively.\n\nEmpty lines in the input are treated as sentence boundaries.\n\nBy default, the output consists of one sentence per line, where each\nline ends with a single newline character (ASCII LF, ``chr(10)``, ``\"\\n\"``).\nWithin each line, tokens are separated by spaces.\n\nThe following (mutually exclusive) options can be specified\non the command line:\n\n+-------------------+---------------------------------------------------+\n| | ``--csv``       | Deep tokenization. Output token objects in CSV    |\n|                   | format, one per line. Sentences are separated by  |\n|                   | lines containing ``0,\"\",\"\"``                      |\n+-------------------+---------------------------------------------------+\n| | ``--json``      | Deep tokenization. Output token objects in JSON   |\n|                   | format, one per line.                             |\n+-------------------+---------------------------------------------------+\n| | ``--normalize`` | Normalize punctuation, causing e.g. quotes to be  |\n|                   | output in Icelandic form and hyphens to be        |\n|                   | regularized. This option is only applicable to    |\n|                   | shallow tokenization.                             |\n+-------------------+---------------------------------------------------+\n\nType ``tokenize -h`` or ``tokenize --help`` to get a short help message.\n\nExample\n=======\n\n.. code-block:: console\n\n    $ echo \"3.jan\u00faar sl. keypti   \u00e9g 64kWst rafb\u00edl. Hann kosta\u00f0i \u20ac 30.000.\" | tokenize\n    3. jan\u00faar sl. keypti \u00e9g 64kWst rafb\u00edl .\n    Hann kosta\u00f0i \u20ac30.000 .\n\n    $ echo \"3.jan\u00faar sl. keypti   \u00e9g 64kWst rafb\u00edl. Hann kosta\u00f0i \u20ac 30.000.\" | tokenize --csv\n    19,\"3. jan\u00faar\",\"0|1|3\"\n    6,\"sl.\",\"s\u00ed\u00f0astli\u00f0inn\"\n    6,\"keypti\",\"\"\n    6,\"\u00e9g\",\"\"\n    22,\"64kWst\",\"J|230400000.0\"\n    6,\"rafb\u00edl\",\"\"\n    1,\".\",\".\"\n    0,\"\",\"\"\n    6,\"Hann\",\"\"\n    6,\"kosta\u00f0i\",\"\"\n    13,\"\u20ac30.000\",\"30000|EUR\"\n    1,\".\",\".\"\n    0,\"\",\"\"\n\n    $ echo \"3.jan\u00faar sl. keypti   \u00e9g 64kWst rafb\u00edl. Hann kosta\u00f0i \u20ac 30.000.\" | tokenize --json\n    {\"k\":\"BEGIN SENT\"}\n    {\"k\":\"DATEREL\",\"t\":\"3. jan\u00faar\",\"v\":[0,1,3]}\n    {\"k\":\"WORD\",\"t\":\"sl.\",\"v\":[\"s\u00ed\u00f0astli\u00f0inn\"]}\n    {\"k\":\"WORD\",\"t\":\"keypti\"}\n    {\"k\":\"WORD\",\"t\":\"\u00e9g\"}\n    {\"k\":\"MEASUREMENT\",\"t\":\"64kWst\",\"v\":[\"J\",230400000.0]}\n    {\"k\":\"WORD\",\"t\":\"rafb\u00edl\"}\n    {\"k\":\"PUNCTUATION\",\"t\":\".\",\"v\":\".\"}\n    {\"k\":\"END SENT\"}\n    {\"k\":\"BEGIN SENT\"}\n    {\"k\":\"WORD\",\"t\":\"Hann\"}\n    {\"k\":\"WORD\",\"t\":\"kosta\u00f0i\"}\n    {\"k\":\"AMOUNT\",\"t\":\"\u20ac30.000\",\"v\":[30000,\"EUR\"]}\n    {\"k\":\"PUNCTUATION\",\"t\":\".\",\"v\":\".\"}\n    {\"k\":\"END SENT\"}\n\nPython module\n-------------\n\nShallow tokenization example\n============================\n\nAn example of shallow tokenization from Python code goes something like this:\n\n.. code-block:: python\n\n    from __future__ import print_function\n    # The following import is optional but convenient under Python 2.7\n    from __future__ import unicode_literals\n\n    from tokenizer import split_into_sentences\n\n    # A string to be tokenized, containing two sentences\n    s = \"3.jan\u00faar sl. keypti   \u00e9g 64kWst rafb\u00edl. Hann kosta\u00f0i \u20ac 30.000.\"\n\n    # Obtain a generator of sentence strings\n    g = split_into_sentences(s)\n\n    # Loop through the sentences\n    for sentence in g:\n\n        # Obtain the individual token strings\n        tokens = sentence.split()\n\n        # Print the tokens, comma-separated\n        print(\", \".join(tokens))\n\nThe program outputs::\n\n    3., jan\u00faar, sl., keypti, \u00e9g, 64kWst, rafb\u00edl, .\n    Hann, kosta\u00f0i, \u20ac30.000, .\n\nDeep tokenization example\n=========================\n\nTo do deep tokenization from within Python code:\n\n.. code-block:: python\n\n    # The following import is optional but convenient under Python 2.7\n    from __future__ import unicode_literals\n    from tokenizer import tokenize, TOK\n\n    text = (\"M\u00e1linu var v\u00edsa\u00f0 til stj\u00f3rnskipunar- og eftirlitsnefndar \"\n        \"skv. 3. gr. XVII. kafla laga nr. 10/2007 \u00feann 3. jan\u00faar 2010.\")\n\n    for token in tokenize(text):\n\n        print(\"{0}: '{1}' {2}\".format(\n            TOK.descr[token.kind],\n            token.txt or \"-\",\n            token.val or \"\"))\n\nOutput::\n\n    BEGIN SENT: '-' (0, None)\n    WORD: 'M\u00e1linu'\n    WORD: 'var'\n    WORD: 'v\u00edsa\u00f0'\n    WORD: 'til'\n    WORD: 'stj\u00f3rnskipunar- og eftirlitsnefndar'\n    WORD: 'skv.' [('samkv\u00e6mt', 0, 'fs', 'skst', 'skv.', '-')]\n    ORDINAL: '3.' 3\n    WORD: 'gr.' [('grein', 0, 'kvk', 'skst', 'gr.', '-')]\n    ORDINAL: 'XVII.' 17\n    WORD: 'kafla'\n    WORD: 'laga'\n    WORD: 'nr.' [('n\u00famer', 0, 'hk', 'skst', 'nr.', '-')]\n    NUMBER: '10' (10, None, None)\n    PUNCTUATION: '/' (4, '/')\n    YEAR: '2007' 2007\n    WORD: '\u00feann'\n    DATEABS: '3. jan\u00faar 2010' (2010, 1, 3)\n    PUNCTUATION: '.' (3, '.')\n    END SENT: '-'\n\nNote the following:\n\n- Sentences are delimited by ``TOK.S_BEGIN`` and ``TOK.S_END`` tokens.\n- Composite words, such as *stj\u00f3rnskipunar- og eftirlitsnefndar*,\n  are coalesced into one token.\n- Well-known abbreviations are recognized and their full expansion\n  is available in the ``token.val`` field.\n- Ordinal numbers (*3., XVII.*) are recognized and their value (*3, 17*)\n  is available in the ``token.val``  field.\n- Dates, years and times, both absolute and relative, are recognized and\n  the respective year, month, day, hour, minute and second\n  values are included as a tuple in ``token.val``.\n- Numbers, both integer and real, are recognized and their value\n  is available in the ``token.val`` field.\n- Further details of how Tokenizer processes text can be inferred from the\n  `test module <https://github.com/mideind/Tokenizer/blob/master/test/test_tokenizer.py>`_\n  in the project's `GitHub repository <https://github.com/mideind/Tokenizer>`_.\n\n\nThe ``tokenize()`` function\n---------------------------\n\nTo deep-tokenize a text string, call ``tokenizer.tokenize(text, **options)``.\nThe ``text`` parameter can be a string, or an iterable that yields strings\n(such as a text file object).\n\nThe function returns a Python *generator* of token objects.\nEach token object is a simple ``namedtuple`` with three\nfields: ``(kind, txt, val)`` (further documented below).\n\nThe ``tokenizer.tokenize()`` function is typically called in a ``for`` loop:\n\n.. code-block:: python\n\n    import tokenizer\n    for token in tokenizer.tokenize(mystring):\n        kind, txt, val = token\n        if kind == tokenizer.TOK.WORD:\n            # Do something with word tokens\n            pass\n        else:\n            # Do something else\n            pass\n\nAlternatively, create a token list from the returned generator::\n\n    token_list = list(tokenizer.tokenize(mystring))\n\nIn Python 2.7, you can pass either ``unicode`` strings or ``str``\nbyte strings to ``tokenizer.tokenize()``. In the latter case, the\nbyte string is assumed to be encoded in UTF-8.\n\n\nThe ``split_into_sentences()`` function\n---------------------------------------\n\nTo shallow-tokenize a text string, call\n``tokenizer.split_into_sentences(text_or_gen, **options)``.\nThe ``text_or_gen`` parameter can be a string, or an iterable that yields\nstrings (such as a text file object).\n\nThis function returns a Python *generator* of strings, yielding a string\nfor each sentence in the input. Within a sentence, the tokens are\nseparated by spaces.\n\nYou can pass the option ``normalize=True`` to the function if you want\nthe normalized form of punctuation tokens. Normalization outputs\nIcelandic single and double quotes (\u201ethese\u201c) instead of English-style\nones (\"these\"), converts three-dot ellipsis ... to single character\nellipsis \u2026, and casts en-dashes \u2013 and em-dashes \u2014 to regular hyphens.\n\nThe ``tokenizer.split_into_sentences()`` function is typically called\nin a ``for`` loop:\n\n.. code-block:: python\n\n    import tokenizer\n    with open(\"example.txt\", \"r\", encoding=\"utf-8\") as f:\n        # You can pass a file object directly to split_into_sentences()\n        for sentence in tokenizer.split_into_sentences(f):\n            # sentence is a string of space-separated tokens\n            tokens = sentence.split()\n            # Now, tokens is a list of strings, one for each token\n            for t in tokens:\n                # Do something with the token t\n                pass\n\n\nThe ``correct_spaces()`` function\n---------------------------------\n\nThe ``tokenizer.correct_spaces(text)`` function returns a string after\nsplitting it up and re-joining it with correct whitespace around\npunctuation tokens. Example::\n\n    >>> import tokenizer\n    >>> tokenizer.correct_spaces(\n    ... \"Fr\u00e9tt \\n  dagsins:J\u00f3n\\t ,Fri\u00f0geir og P\u00e1ll ! 100  /  2  =   50\"\n    ... )\n    'Fr\u00e9tt dagsins: J\u00f3n, Fri\u00f0geir og P\u00e1ll! 100/2 = 50'\n\n\nThe ``detokenize()`` function\n---------------------------------\n\nThe ``tokenizer.detokenize(tokens, normalize=False)`` function\ntakes an iterable of token objects and returns a corresponding, correctly\nspaced text string, composed from the tokens' text. If the\n``normalize`` parameter is set to ``True``,\nthe function uses the normalized form of any punctuation tokens, such\nas proper Icelandic single and double quotes instead of English-type\nquotes. Example::\n\n    >>> import tokenizer\n    >>> toklist = list(tokenizer.tokenize(\"Hann sag\u00f0i: \u201e\u00de\u00fa ert \u00e1g\u00e6t!\u201c.\"))\n    >>> tokenizer.detokenize(toklist, normalize=True)\n    'Hann sag\u00f0i: \u201e\u00de\u00fa ert \u00e1g\u00e6t!\u201c.'\n\n\nThe ``normalized_text()`` function\n----------------------------------\n\nThe ``tokenizer.normalized_text(token)`` function\nreturns the normalized text for a token. This means that the original\ntoken text is returned except for certain punctuation tokens, where a\nnormalized form is returned instead. Specifically, English-type quotes\nare converted to Icelandic ones, and en- and em-dashes are converted\nto regular hyphens.\n\n\nThe ``text_from_tokens()`` function\n-----------------------------------\n\nThe ``tokenizer.text_from_tokens(tokens)`` function\nreturns a concatenation of the text contents of the given token list,\nwith spaces between tokens. Example::\n\n    >>> import tokenizer\n    >>> toklist = list(tokenizer.tokenize(\"Hann sag\u00f0i: \\\"\u00de\u00fa ert \u00e1g\u00e6t!\\\".\"))\n    >>> tokenizer.text_from_tokens(toklist)\n    'Hann sag\u00f0i : \" \u00de\u00fa ert \u00e1g\u00e6t ! \" .'\n\n\nThe ``normalized_text_from_tokens()`` function\n----------------------------------------------\n\nThe ``tokenizer.normalized_text_from_tokens(tokens)`` function\nreturns a concatenation of the normalized text contents of the given\ntoken list, with spaces between tokens. Example (note the double quotes)::\n\n    >>> import tokenizer\n    >>> toklist = list(tokenizer.tokenize(\"Hann sag\u00f0i: \\\"\u00de\u00fa ert \u00e1g\u00e6t!\\\".\"))\n    >>> tokenizer.normalized_text_from_tokens(toklist)\n    'Hann sag\u00f0i : \u201e \u00de\u00fa ert \u00e1g\u00e6t ! \u201c .'\n\n\nTokenization options\n--------------------\n\nYou can optionally pass one or more of the following options as\nkeyword parameters to the ``tokenize()`` and ``split_into_sentences()``\nfunctions:\n\n\n* ``convert_numbers=[bool]``\n\n  Setting this option to ``True`` causes the tokenizer to convert numbers\n  and amounts with\n  English-style decimal points (``.``) and thousands separators (``,``)\n  to Icelandic format, where the decimal separator is a comma (``,``)\n  and the thousands separator is a period (``.``). ``$1,234.56`` is thus\n  converted to a token whose text is ``$1.234,56``.\n\n  The default value for the ``convert_numbers`` option is ``False``.\n\n  Note that in versions of Tokenizer prior to 1.4, ``convert_numbers``\n  was ``True``.\n\n\n* ``handle_kludgy_ordinals=[value]``\n\n  This options controls the way Tokenizer handles 'kludgy' ordinals, such as\n  *1sti*, *4\u00f0u*, or *2ja*. By default, such ordinals are returned unmodified\n  ('passed through') as word tokens (``TOK.WORD``).\n  However, this can be modified as follows:\n\n  * ``tokenizer.KLUDGY_ORDINALS_MODIFY``: Kludgy ordinals are corrected\n    to become 'proper' word tokens, i.e. *1sti* becomes *fyrsti* and\n    *2ja* becomes *tveggja*.\n\n  * ``tokenizer.KLUDGY_ORDINALS_TRANSLATE``: Kludgy ordinals that represent\n    proper ordinal numbers are translated to ordinal tokens (``TOK.ORDINAL``),\n    with their original text and their ordinal value. *1sti* thus\n    becomes a ``TOK.ORDINAL`` token with a value of 1, and *3ja* becomes\n    a ``TOK.ORDINAL`` with a value of 3.\n\n  * ``tokenizer.KLUDGY_ORDINALS_PASS_THROUGH`` is the default value of\n    the option. It causes kludgy ordinals to be returned unmodified as\n    word tokens.\n\n  Note that versions of Tokenizer prior to 1.4 behaved as if\n  ``handle_kludgy_ordinals`` were set to\n  ``tokenizer.KLUDGY_ORDINALS_TRANSLATE``.\n\n\nThe token object\n----------------\n\nEach token is represented by a ``namedtuple`` with three fields:\n``(kind, txt, val)``.\n\n\nThe ``kind`` field\n==================\n\nThe ``kind`` field contains one of the following integer constants,\ndefined within the ``TOK`` class:\n\n+---------------+---------+---------------------+---------------------------+\n| Constant      |  Value  | Explanation         | Examples                  |\n+===============+=========+=====================+===========================+\n| PUNCTUATION   |    1    | Punctuation         | . ! ; % &                 |\n+---------------+---------+---------------------+---------------------------+\n| TIME          |    2    | Time (h, m, s)      | | 11:35:40                |\n|               |         |                     | | kl. 7:05                |\n|               |         |                     | | klukkan 23:35           |\n+---------------+---------+---------------------+---------------------------+\n| DATE *        |    3    | Date (y, m, d)      | [Unused, see DATEABS and  |\n|               |         |                     | DATEREL]                  |\n+---------------+---------+---------------------+---------------------------+\n| YEAR          |    4    | Year                | | \u00e1ri\u00f0 874 e.Kr.          |\n|               |         |                     | | 1965                    |\n|               |         |                     | | 44 f.Kr.                |\n+---------------+---------+---------------------+---------------------------+\n| NUMBER        |    5    | Number              | | 100                     |\n|               |         |                     | | 1.965                   |\n|               |         |                     | | 1.965,34                |\n|               |         |                     | | 1,965.34                |\n|               |         |                     | | 2\u215e                      |\n+---------------+---------+---------------------+---------------------------+\n| WORD          |    6    | Word                | | kattaeftirlit           |\n|               |         |                     | | hunda- og kattaeftirlit |\n+---------------+---------+---------------------+---------------------------+\n| TELNO         |    7    | Telephone number    | | 5254764                 |\n|               |         |                     | | 699-4244                |\n|               |         |                     | | 410 4000                |\n+---------------+---------+---------------------+---------------------------+\n| PERCENT       |    8    | Percentage          | 78%                       |\n+---------------+---------+---------------------+---------------------------+\n| URL           |    9    | URL                 | | https://greynir.is      |\n|               |         |                     | | http://tiny.cc/28695y   |\n+---------------+---------+---------------------+---------------------------+\n| ORDINAL       |    10   | Ordinal number      | | 30.                     |\n|               |         |                     | | XVIII.                  |\n+---------------+---------+---------------------+---------------------------+\n| TIMESTAMP *   |    11   | Timestamp           | [Unused, see              |\n|               |         |                     | TIMESTAMPABS and          |\n|               |         |                     | TIMESTAMPREL]             |\n+---------------+---------+---------------------+---------------------------+\n| CURRENCY *    |    12   | Currency name       | [Unused]                  |\n+---------------+---------+---------------------+---------------------------+\n| AMOUNT        |    13   | Amount              | | \u20ac2.345,67               |\n|               |         |                     | | 750 \u00fe\u00fas.kr.             |\n|               |         |                     | | 2,7 mr\u00f0. USD            |\n|               |         |                     | | kr. 9.900               |\n|               |         |                     | | EUR 200                 |\n+---------------+---------+---------------------+---------------------------+\n| PERSON *      |    14   | Person name         | [Unused]                  |\n+---------------+---------+---------------------+---------------------------+\n| EMAIL         |    15   | E-mail              | ``fake@news.is``          |\n+---------------+---------+---------------------+---------------------------+\n| ENTITY *      |    16   | Named entity        | [Unused]                  |\n+---------------+---------+---------------------+---------------------------+\n| UNKNOWN       |    17   | Unknown token       |                           |\n+---------------+---------+---------------------+---------------------------+\n| DATEABS       |    18   | Absolute date       | | 30. desember 1965       |\n|               |         |                     | | 30/12/1965              |\n|               |         |                     | | 1965-12-30              |\n|               |         |                     | | 1965/12/30              |\n+---------------+---------+---------------------+---------------------------+\n| DATEREL       |    19   | Relative date       | | 15. mars                |\n|               |         |                     | | 15/3                    |\n|               |         |                     | | 15.3.                   |\n|               |         |                     | | mars 1911               |\n+---------------+---------+---------------------+---------------------------+\n| TIMESTAMPABS  |    20   | Absolute timestamp  | | 30. desember 1965 11:34 |\n|               |         |                     | | 1965-12-30 kl. 13:00    |\n+---------------+---------+---------------------+---------------------------+\n| TIMESTAMPREL  |    21   | Relative timestamp  | | 30. desember kl. 13:00  |\n+---------------+---------+---------------------+---------------------------+\n| MEASUREMENT   |    22   | Value with a        | | 690 MW                  |\n|               |         | measurement unit    | | 1.010 hPa               |\n|               |         |                     | | 220 m\u00b2                  |\n|               |         |                     | | 80\u00b0 C                   |\n+---------------+---------+---------------------+---------------------------+\n| NUMWLETTER    |    23   | Number followed by  | | 14a                     |\n|               |         | a single letter     | | 7B                      |\n+---------------+---------+---------------------+---------------------------+\n| DOMAIN        |    24   | Domain name         | | greynir.is              |\n|               |         |                     | | Reddit.com              |\n|               |         |                     | | www.wikipedia.org       |\n+---------------+---------+---------------------+---------------------------+\n| HASHTAG       |    25   | Hashtag             | | #MeToo                  |\n|               |         |                     | | #12stig                 |\n+---------------+---------+---------------------+---------------------------+\n| MOLECULE      |    26   | Molecular formula   | | H2SO4                   |\n|               |         |                     | | CO2                     |\n+---------------+---------+---------------------+---------------------------+\n| SSN           |    27   | Social security     | | 591213-1480             |\n|               |         | number (*kennitala*)|                           |\n+---------------+---------+---------------------+---------------------------+\n| USERNAME      |    28   | Twitter user handle | | @username_123           |\n|               |         |                     |                           |\n+---------------+---------+---------------------+---------------------------+\n| SERIALNUMBER  |    29   | Serial number       | | 394-5388                |\n|               |         |                     | | 12-345-6789             |\n+---------------+---------+---------------------+---------------------------+\n| S_BEGIN       |  11001  | Start of sentence   |                           |\n+---------------+---------+---------------------+---------------------------+\n| S_END         |  11002  | End of sentence     |                           |\n+---------------+---------+---------------------+---------------------------+\n\n(*) The token types marked with an asterisk are reserved for the Reynir package\nand not currently returned by the tokenizer.\n\nTo obtain a descriptive text for a token kind, use\n``TOK.descr[token.kind]`` (see example above).\n\n\nThe ``txt`` field\n==================\n\nThe ``txt`` field contains the original source text for the token,\nwith the following exceptions:\n\n* All contiguous whitespace (spaces, tabs, newlines) is coalesced\n  into single spaces (``\" \"``) within the ``txt`` field. A date\n  token that is parsed from a source text of ``\"29.  \\n   jan\u00faar\"``\n  thus has a ``txt`` of ``\"29. jan\u00faar\"``.\n\n* Tokenizer automatically merges Unicode ``COMBINING ACUTE ACCENT``\n  (code point 769) and ``COMBINING DIAERESIS`` (code point 776)\n  with vowels to form single code points for the Icelandic letters\n  \u00e1, \u00e9, \u00ed, \u00f3, \u00fa, \u00fd and \u00f6, in both lower and upper case.\n\n* If the appropriate options are specified (see above), it converts\n  kludgy ordinals (*3ja*) to proper ones (*\u00feri\u00f0ja*), and English-style\n  thousand and decimal separators to Icelandic ones\n  (*10,345.67* becomes *10.345,67*).\n\nIn the case of abbreviations that end a sentence, the final period\n``\".\"`` is a separate token, and it is consequently omitted from the\nabbreviation token's ``txt`` field. A sentence ending in *o.s.frv.*\nwill thus end with two tokens, the next-to-last one being the tuple\n``(TOK.WORD, \"o.s.frv\", \"og svo framvegis\")`` - note the omitted\nperiod in the ``txt`` field - and the last one being\n``(TOK.PUNCTUATION, \".\", (3, \".\"))`` (this tuple form is further\nexplained below).\n\n\nThe ``val`` field\n==================\n\nThe ``val`` field contains auxiliary information, corresponding to\nthe token kind, as follows:\n\n- For ``TOK.PUNCTUATION``, the ``val`` field contains a tuple with\n  two items: ``(whitespace, normalform)``. The first item (``token.val[0]``)\n  specifies the whitespace normally found around the symbol in question,\n  as an integer::\n\n    TP_LEFT = 1   # Whitespace to the left\n    TP_CENTER = 2 # Whitespace to the left and right\n    TP_RIGHT = 3  # Whitespace to the right\n    TP_NONE = 4   # No whitespace\n\n  The second item (``token.val[1]``) contains a normalized representation of the\n  punctuation. For instance, various forms of single and double\n  quotes are represented as Icelandic ones (i.e. \u201ethese\u201c or \u201athese\u2018) in\n  normalized form, and ellipsis (\"...\") are represented as the single\n  character \"\u2026\".\n- For ``TOK.TIME``, the ``val`` field contains an\n  ``(hour, minute, second)`` tuple.\n- For ``TOK.DATEABS``, the ``val`` field contains a\n  ``(year, month, day)`` tuple (all 1-based).\n- For ``TOK.DATEREL``, the ``val`` field contains a\n  ``(year, month, day)`` tuple (all 1-based),\n  except that a least one of the tuple fields is missing and set to 0.\n  Example: *3. j\u00fan\u00ed* becomes ``TOK.DATEREL`` with the fields ``(0, 6, 3)``\n  as the year is missing.\n- For ``TOK.YEAR``, the ``val`` field contains the year as an integer.\n  A negative number indicates that the year is BCE (*fyrir Krist*),\n  specified with the suffix *f.Kr.* (e.g. *\u00e1ri\u00f0 33 f.Kr.*).\n- For ``TOK.NUMBER``, the ``val`` field contains a tuple\n  ``(number, None, None)``.\n  (The two empty fields are included for compatibility with Greynir.)\n- For ``TOK.WORD``, the ``val`` field contains the full expansion\n  of an abbreviation, as a list containing a single tuple, or ``None``\n  if the word is not abbreviated.\n- For ``TOK.PERCENT``, the ``val`` field contains a tuple\n  of ``(percentage, None, None)``.\n- For ``TOK.ORDINAL``, the ``val`` field contains the ordinal value\n  as an integer. The original ordinal may be a decimal number\n  or a Roman numeral.\n- For ``TOK.TIMESTAMP``, the ``val`` field contains\n  a ``(year, month, day, hour, minute, second)`` tuple.\n- For ``TOK.AMOUNT``, the ``val`` field contains\n  an ``(amount, currency, None, None)`` tuple. The amount is a float, and\n  the currency is an ISO currency code, e.g. *USD* for dollars ($ sign),\n  *EUR* for euros (\u20ac sign) or *ISK* for Icelandic kr\u00f3na\n  (*kr.* abbreviation). (The two empty fields are included for\n  compatibility with Greynir.)\n- For ``TOK.MEASUREMENT``, the ``val`` field contains a ``(unit, value)``\n  tuple, where ``unit`` is a base SI unit (such as ``g``, ``m``,\n  ``m\u00b2``, ``s``, ``W``, ``Hz``, ``K`` for temperature in Kelvin).\n- For ``TOK.TELNO``, the ``val`` field contains a tuple: ``(number, cc)``\n  where the first item is the phone number\n  in a normalized ``NNN-NNNN`` format, i.e. always including a hyphen,\n  and the second item is the country code, eventually prefixed by ``+``.\n  The country code defaults to ``354`` (Iceland).\n\n\nAbbreviations\n-------------\n\nAbbreviations recognized by Tokenizer are defined in the ``Abbrev.conf``\nfile, found in the ``src/tokenizer/`` directory. This is a text file with\nabbreviations, their definitions and explanatory comments.\n\nWhen an abbreviation is encountered, it is recognized as a word token\n(i.e. having its ``kind`` field equal to ``TOK.WORD``).\nIts expansion(s) are included in the token's\n``val`` field as a list containing tuples of the format\n``(ordmynd, utg, ordfl, fl, stofn, beyging)``.\nAn example is *o.s.frv.*, which results in a ``val`` field equal to\n``[('og svo framvegis', 0, 'ao', 'frasi', 'o.s.frv.', '-')]``.\n\nThe tuple format is designed to be compatible with the\n*Database of Modern Icelandic Inflection* (*DMII*),\n*Beygingarl\u00fdsing \u00edslensks n\u00fat\u00edmam\u00e1ls*.\n\n\nDevelopment installation\n------------------------\n\nTo install Tokenizer in development mode, where you can easily\nmodify the source files (assuming you have ``git`` available):\n\n.. code-block:: console\n\n    $ git clone https://github.com/mideind/Tokenizer\n    $ cd Tokenizer\n    $ # [ Activate your virtualenv here, if you have one ]\n    $ pip install -e .\n\n\nTest suite\n----------\n\nTokenizer comes with a large test suite.\nThe file ``test/test_tokenizer.py`` contains built-in tests that\nrun under ``pytest``.\n\nTo run the built-in tests, install `pytest <https://docs.pytest.org/en/latest/>`_,\n``cd`` to your ``Tokenizer`` subdirectory (and optionally\nactivate your virtualenv), then run:\n\n.. code-block:: console\n\n    $ python -m pytest\n\nThe file ``test/toktest_large.txt`` contains a test set of 13,075 lines.\nThe lines test sentence detection, token detection and token classification.\nFor analysis, ``test/toktest_large_gold_perfect.txt`` contains\nthe expected output of a perfect shallow tokenization, and\n``test/toktest_large_gold_acceptable.txt`` contains the current output of the\nshallow tokenization.\n\nThe file ``test/Overview.txt`` (only in Icelandic) contains a description\nof the test set, including line numbers for each part in both\n``test/toktest_large.txt`` and ``test/toktest_large_gold_acceptable.txt``,\nand a tag describing what is being tested in each part.\n\nIt also contains a description of a perfect shallow tokenization for each part,\nacceptable tokenization and the current behaviour.\nAs such, the description is an analysis of which edge cases the tokenizer\ncan handle and which it can not.\n\nTo test the tokenizer on the large test set the following needs to be typed\nin the command line:\n\n.. code-block:: console\n\n    $ tokenize test/toktest_large.txt test/toktest_large_out.txt\n\nTo compare it to the acceptable behaviour:\n\n.. code-block:: console\n\n    $ diff test/toktest_large_out.txt test/toktest_large_gold_acceptable.txt > diff.txt\n\nThe file ``test/toktest_normal.txt`` contains a running text from recent\nnews articles, containing no edge cases. The gold standard for that file\ncan be found in the file ``test/toktest_normal_gold_expected.txt``.\n\n\nChangelog\n---------\n\n* Version 2.0.5: Fixed bug where single uppercase letters were erroneously\n  being recognized as abbreviations, causing prepositions such as '\u00cd' and '\u00c1'\n  at the beginning of sentences to be misunderstood in ReynirPackage\n* Version 2.0.4: Added imperfect abbreviations (*amk.*, *osfrv.*); recognized\n  *klukkan h\u00e1lf tv\u00f6* as a ``TOK.TIME``\n* Version 2.0.3: Fixed bug in ``detokenize()`` where abbreviations, domains\n  and e-mails containing periods were wrongly split\n* Version 2.0.2: Spelled-out day ordinals are no longer included as a part of\n  ``TOK.DATEREL`` tokens. Thus, *\u00feri\u00f0ji j\u00fan\u00ed* is now a ``TOK.WORD``\n  followed by a ``TOK.DATEREL``. *3. j\u00fan\u00ed* continues to be parsed as\n  a single ``TOK.DATEREL``\n* Version 2.0.1: Order of abbreviation meanings within the ``token.val`` field\n  made deterministic; fixed bug in measurement unit handling\n* Version 2.0.0: Added command line tool; added ``split_into_sentences()``\n  and ``detokenize()`` functions; removed ``convert_telno`` option;\n  splitting of coalesced tokens made more robust;\n  added ``TOK.SSN``, ``TOK.MOLECULE``, ``TOK.USERNAME`` and\n  ``TOK.SERIALNUMBER`` token kinds; abbreviations can now have multiple\n  meanings\n* Version 1.4.0: Added the ``**options`` parameter to the\n  ``tokenize()`` function, giving control over the handling of numbers,\n  telephone numbers, and 'kludgy' ordinals\n* Version 1.3.0: Added ``TOK.DOMAIN`` and ``TOK.HASHTAG`` token types;\n  improved handling of capitalized month name *\u00c1g\u00fast*, which is\n  now recognized when following an ordinal number; improved recognition\n  of telephone numbers; added abbreviations\n* Version 1.2.3: Added abbreviations; updated GitHub URLs\n* Version 1.2.2: Added support for composites with more than two parts, i.e.\n  *\u201ed\u00f3msm\u00e1la-, fer\u00f0am\u00e1la-, i\u00f0na\u00f0ar- og n\u00fdsk\u00f6punarr\u00e1\u00f0herra\u201c*; added support for\n  ``\u00b1`` sign; added several abbreviations\n* Version 1.2.1: Fixed bug where the name *\u00c1g\u00fast* was recognized\n  as a month name; Unicode nonbreaking and invisible space characters\n  are now removed before tokenization\n* Version 1.2.0: Added support for Unicode fraction characters;\n  enhanced handing of degrees (\u00b0, \u00b0C, \u00b0F); fixed bug in cubic meter\n  measurement unit; more abbreviations\n* Version 1.1.2: Fixed bug in liter (``l`` and ``ltr``) measurement units\n* Version 1.1.1: Added ``mark_paragraphs()`` function\n* Version 1.1.0: All abbreviations in ``Abbrev.conf`` are now\n  returned with their meaning in a tuple in ``token.val``;\n  handling of 'mbl.is' fixed\n* Version 1.0.9: Added abbreviation 'MAST'; harmonized copyright headers\n* Version 1.0.8: Bug fixes in ``DATEREL``, ``MEASUREMENT`` and ``NUMWLETTER``\n  token handling; added 'kWst' and 'MWst' measurement units; blackened\n* Version 1.0.7: Added ``TOK.NUMWLETTER`` token type\n* Version 1.0.6: Automatic merging of Unicode ``COMBINING ACUTE ACCENT`` and\n  ``COMBINING DIAERESIS`` code points with vowels\n* Version 1.0.5: Date/time and amount tokens coalesced to a further extent\n* Version 1.0.4: Added ``TOK.DATEABS``, ``TOK.TIMESTAMPABS``,\n  ``TOK.MEASUREMENT``\n\n\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/mideind/Tokenizer", "keywords": "nlp,tokenizer,icelandic", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "tokenizer", "package_url": "https://pypi.org/project/tokenizer/", "platform": "", "project_url": "https://pypi.org/project/tokenizer/", "project_urls": {"Homepage": "https://github.com/mideind/Tokenizer"}, "release_url": "https://pypi.org/project/tokenizer/2.0.5/", "requires_dist": null, "requires_python": "", "summary": "A tokenizer for Icelandic text", "version": "2.0.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://travis-ci.com/mideind/Tokenizer\" rel=\"nofollow\"><img alt=\"https://travis-ci.com/mideind/Tokenizer.svg?branch=master\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d6b9a2e7fb35a684e082aab2c8ad0b8dcc550f34/68747470733a2f2f7472617669732d63692e636f6d2f6d696465696e642f546f6b656e697a65722e7376673f6272616e63683d6d6173746572\"></a>\n<div id=\"overview\">\n<h2>Overview</h2>\n<p>Tokenization is a necessary first step in many natural language processing\ntasks, such as word counting, parsing, spell checking, corpus generation, and\nstatistical analysis of text.</p>\n<p><strong>Tokenizer</strong> is a compact pure-Python (2 and 3) executable program and module\nfor tokenizing Icelandic text. It converts input text to streams of <em>tokens</em>,\nwhere each token is a separate word, punctuation sign, number/amount, date,\ne-mail, URL/URI, etc. It also segments the token stream into sentences,\nconsidering corner cases such as abbreviations and dates in the middle\nof sentences.</p>\n<p>The package contains a dictionary of common Icelandic abbreviations,\nin the file <tt>src/tokenizer/Abbrev.conf</tt>.</p>\n<p>Tokenizer is an independent spinoff from the <a href=\"https://greynir.is\" rel=\"nofollow\">Greynir project</a>\n(GitHub repository <a href=\"https://github.com/mideind/Greynir\" rel=\"nofollow\">here</a>), by the same authors.\nThe <a href=\"https://github.com/mideind/ReynirPackage\" rel=\"nofollow\">Greynir natural language parser for Icelandic</a>\nuses Tokenizer on its input.</p>\n<p>Note that Tokenizer is licensed under the <em>MIT</em> license\nwhile Greynir is licensed under <em>GPLv3</em>.</p>\n</div>\n<div id=\"deep-vs-shallow-tokenization\">\n<h2>Deep vs. shallow tokenization</h2>\n<p>Tokenizer can do both <em>deep</em> and <em>shallow</em> tokenization.</p>\n<p><em>Shallow</em> tokenization simply returns each sentence as a string (or as a line\nof text in an output file), where the individual tokens are separated\nby spaces.</p>\n<p><em>Deep</em> tokenization returns token objects that have been annotated with\nthe token type and further information extracted from the token, for example\na <em>(year, month, day)</em> tuple in the case of date tokens.</p>\n<p>In shallow tokenization, tokens are in most cases kept intact, although\nconsecutive white space is always coalesced. The input strings\n<tt>\"800 MW\"</tt>, <tt>\"21. jan\u00faar\"</tt> and <tt>\"800 7000\"</tt> thus become\ntwo tokens each, output with a single space between them.</p>\n<p>In deep tokenization, the same strings are represented by single token objects,\nof type <tt>TOK.MEASUREMENT</tt>, <tt>TOK.DATEREL</tt> and <tt>TOK.TELNO</tt>, respectively.\nThe text associated with a single token object may contain one or more spaces,\nalthough consecutive space is always coalesced.</p>\n<p>By default, the command line tool performs shallow tokenization. If you\nwant deep tokenization with the command line tool, use the <tt><span class=\"pre\">--json</span></tt> or\n<tt><span class=\"pre\">--csv</span></tt> switches.</p>\n<p>&gt;From Python code, call <tt>split_into_sentences()</tt> for shallow tokenization,\nor <tt>tokenize()</tt> for deep tokenization. These functions are documented with\nexamples below.</p>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>To install:</p>\n<pre><span class=\"gp\">$</span> pip install tokenizer\n</pre>\n</div>\n<div id=\"command-line-tool\">\n<h2>Command line tool</h2>\n<p>After installation, the tokenizer can be invoked directly from\nthe command line:</p>\n<pre><span class=\"gp\">$</span> tokenize input.txt output.txt\n</pre>\n<p>Input and output files are encoded in UTF-8. If the files are not\ngiven explicitly, <tt>stdin</tt> and <tt>stdout</tt> are used for input and output,\nrespectively.</p>\n<p>Empty lines in the input are treated as sentence boundaries.</p>\n<p>By default, the output consists of one sentence per line, where each\nline ends with a single newline character (ASCII LF, <tt>chr(10)</tt>, <tt>\"\\n\"</tt>).\nWithin each line, tokens are separated by spaces.</p>\n<p>The following (mutually exclusive) options can be specified\non the command line:</p>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><div>\n<div><tt><span class=\"pre\">--csv</span></tt></div>\n</div>\n</td>\n<td>Deep tokenization. Output token objects in CSV\nformat, one per line. Sentences are separated by\nlines containing <tt><span class=\"pre\">0,\"\",\"\"</span></tt></td>\n</tr>\n<tr><td><div>\n<div><tt><span class=\"pre\">--json</span></tt></div>\n</div>\n</td>\n<td>Deep tokenization. Output token objects in JSON\nformat, one per line.</td>\n</tr>\n<tr><td><div>\n<div><tt><span class=\"pre\">--normalize</span></tt></div>\n</div>\n</td>\n<td>Normalize punctuation, causing e.g. quotes to be\noutput in Icelandic form and hyphens to be\nregularized. This option is only applicable to\nshallow tokenization.</td>\n</tr>\n</tbody>\n</table>\n<p>Type <tt>tokenize <span class=\"pre\">-h</span></tt> or <tt>tokenize <span class=\"pre\">--help</span></tt> to get a short help message.</p>\n<div id=\"example\">\n<h3>Example</h3>\n<pre><span class=\"gp\">$</span> <span class=\"nb\">echo</span> <span class=\"s2\">\"3.jan\u00faar sl. keypti   \u00e9g 64kWst rafb\u00edl. Hann kosta\u00f0i \u20ac 30.000.\"</span> <span class=\"p\">|</span> tokenize\n<span class=\"go\">3. jan\u00faar sl. keypti \u00e9g 64kWst rafb\u00edl .\nHann kosta\u00f0i \u20ac30.000 .\n\n</span><span class=\"gp\">$</span> <span class=\"nb\">echo</span> <span class=\"s2\">\"3.jan\u00faar sl. keypti   \u00e9g 64kWst rafb\u00edl. Hann kosta\u00f0i \u20ac 30.000.\"</span> <span class=\"p\">|</span> tokenize --csv\n<span class=\"go\">19,\"3. jan\u00faar\",\"0|1|3\"\n6,\"sl.\",\"s\u00ed\u00f0astli\u00f0inn\"\n6,\"keypti\",\"\"\n6,\"\u00e9g\",\"\"\n22,\"64kWst\",\"J|230400000.0\"\n6,\"rafb\u00edl\",\"\"\n1,\".\",\".\"\n0,\"\",\"\"\n6,\"Hann\",\"\"\n6,\"kosta\u00f0i\",\"\"\n13,\"\u20ac30.000\",\"30000|EUR\"\n1,\".\",\".\"\n0,\"\",\"\"\n\n</span><span class=\"gp\">$</span> <span class=\"nb\">echo</span> <span class=\"s2\">\"3.jan\u00faar sl. keypti   \u00e9g 64kWst rafb\u00edl. Hann kosta\u00f0i \u20ac 30.000.\"</span> <span class=\"p\">|</span> tokenize --json\n<span class=\"go\">{\"k\":\"BEGIN SENT\"}\n{\"k\":\"DATEREL\",\"t\":\"3. jan\u00faar\",\"v\":[0,1,3]}\n{\"k\":\"WORD\",\"t\":\"sl.\",\"v\":[\"s\u00ed\u00f0astli\u00f0inn\"]}\n{\"k\":\"WORD\",\"t\":\"keypti\"}\n{\"k\":\"WORD\",\"t\":\"\u00e9g\"}\n{\"k\":\"MEASUREMENT\",\"t\":\"64kWst\",\"v\":[\"J\",230400000.0]}\n{\"k\":\"WORD\",\"t\":\"rafb\u00edl\"}\n{\"k\":\"PUNCTUATION\",\"t\":\".\",\"v\":\".\"}\n{\"k\":\"END SENT\"}\n{\"k\":\"BEGIN SENT\"}\n{\"k\":\"WORD\",\"t\":\"Hann\"}\n{\"k\":\"WORD\",\"t\":\"kosta\u00f0i\"}\n{\"k\":\"AMOUNT\",\"t\":\"\u20ac30.000\",\"v\":[30000,\"EUR\"]}\n{\"k\":\"PUNCTUATION\",\"t\":\".\",\"v\":\".\"}\n{\"k\":\"END SENT\"}</span>\n</pre>\n</div>\n</div>\n<div id=\"python-module\">\n<h2>Python module</h2>\n<div id=\"shallow-tokenization-example\">\n<h3>Shallow tokenization example</h3>\n<p>An example of shallow tokenization from Python code goes something like this:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">__future__</span> <span class=\"kn\">import</span> <span class=\"n\">print_function</span>\n<span class=\"c1\"># The following import is optional but convenient under Python 2.7</span>\n<span class=\"kn\">from</span> <span class=\"nn\">__future__</span> <span class=\"kn\">import</span> <span class=\"n\">unicode_literals</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">tokenizer</span> <span class=\"kn\">import</span> <span class=\"n\">split_into_sentences</span>\n\n<span class=\"c1\"># A string to be tokenized, containing two sentences</span>\n<span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"s2\">\"3.jan\u00faar sl. keypti   \u00e9g 64kWst rafb\u00edl. Hann kosta\u00f0i \u20ac 30.000.\"</span>\n\n<span class=\"c1\"># Obtain a generator of sentence strings</span>\n<span class=\"n\">g</span> <span class=\"o\">=</span> <span class=\"n\">split_into_sentences</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Loop through the sentences</span>\n<span class=\"k\">for</span> <span class=\"n\">sentence</span> <span class=\"ow\">in</span> <span class=\"n\">g</span><span class=\"p\">:</span>\n\n    <span class=\"c1\"># Obtain the individual token strings</span>\n    <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">sentence</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># Print the tokens, comma-separated</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\", \"</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">tokens</span><span class=\"p\">))</span>\n</pre>\n<p>The program outputs:</p>\n<pre>3., jan\u00faar, sl., keypti, \u00e9g, 64kWst, rafb\u00edl, .\nHann, kosta\u00f0i, \u20ac30.000, .\n</pre>\n</div>\n<div id=\"deep-tokenization-example\">\n<h3>Deep tokenization example</h3>\n<p>To do deep tokenization from within Python code:</p>\n<pre><span class=\"c1\"># The following import is optional but convenient under Python 2.7</span>\n<span class=\"kn\">from</span> <span class=\"nn\">__future__</span> <span class=\"kn\">import</span> <span class=\"n\">unicode_literals</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tokenizer</span> <span class=\"kn\">import</span> <span class=\"n\">tokenize</span><span class=\"p\">,</span> <span class=\"n\">TOK</span>\n\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"s2\">\"M\u00e1linu var v\u00edsa\u00f0 til stj\u00f3rnskipunar- og eftirlitsnefndar \"</span>\n    <span class=\"s2\">\"skv. 3. gr. XVII. kafla laga nr. 10/2007 \u00feann 3. jan\u00faar 2010.\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">token</span> <span class=\"ow\">in</span> <span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">):</span>\n\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"</span><span class=\"si\">{0}</span><span class=\"s2\">: '</span><span class=\"si\">{1}</span><span class=\"s2\">' </span><span class=\"si\">{2}</span><span class=\"s2\">\"</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span>\n        <span class=\"n\">TOK</span><span class=\"o\">.</span><span class=\"n\">descr</span><span class=\"p\">[</span><span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">kind</span><span class=\"p\">],</span>\n        <span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">txt</span> <span class=\"ow\">or</span> <span class=\"s2\">\"-\"</span><span class=\"p\">,</span>\n        <span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">val</span> <span class=\"ow\">or</span> <span class=\"s2\">\"\"</span><span class=\"p\">))</span>\n</pre>\n<p>Output:</p>\n<pre>BEGIN SENT: '-' (0, None)\nWORD: 'M\u00e1linu'\nWORD: 'var'\nWORD: 'v\u00edsa\u00f0'\nWORD: 'til'\nWORD: 'stj\u00f3rnskipunar- og eftirlitsnefndar'\nWORD: 'skv.' [('samkv\u00e6mt', 0, 'fs', 'skst', 'skv.', '-')]\nORDINAL: '3.' 3\nWORD: 'gr.' [('grein', 0, 'kvk', 'skst', 'gr.', '-')]\nORDINAL: 'XVII.' 17\nWORD: 'kafla'\nWORD: 'laga'\nWORD: 'nr.' [('n\u00famer', 0, 'hk', 'skst', 'nr.', '-')]\nNUMBER: '10' (10, None, None)\nPUNCTUATION: '/' (4, '/')\nYEAR: '2007' 2007\nWORD: '\u00feann'\nDATEABS: '3. jan\u00faar 2010' (2010, 1, 3)\nPUNCTUATION: '.' (3, '.')\nEND SENT: '-'\n</pre>\n<p>Note the following:</p>\n<ul>\n<li>Sentences are delimited by <tt>TOK.S_BEGIN</tt> and <tt>TOK.S_END</tt> tokens.</li>\n<li>Composite words, such as <em>stj\u00f3rnskipunar- og eftirlitsnefndar</em>,\nare coalesced into one token.</li>\n<li>Well-known abbreviations are recognized and their full expansion\nis available in the <tt>token.val</tt> field.</li>\n<li>Ordinal numbers (<em>3., XVII.</em>) are recognized and their value (<em>3, 17</em>)\nis available in the <tt>token.val</tt>  field.</li>\n<li>Dates, years and times, both absolute and relative, are recognized and\nthe respective year, month, day, hour, minute and second\nvalues are included as a tuple in <tt>token.val</tt>.</li>\n<li>Numbers, both integer and real, are recognized and their value\nis available in the <tt>token.val</tt> field.</li>\n<li>Further details of how Tokenizer processes text can be inferred from the\n<a href=\"https://github.com/mideind/Tokenizer/blob/master/test/test_tokenizer.py\" rel=\"nofollow\">test module</a>\nin the project\u2019s <a href=\"https://github.com/mideind/Tokenizer\" rel=\"nofollow\">GitHub repository</a>.</li>\n</ul>\n</div>\n</div>\n<div id=\"the-tokenize-function\">\n<h2>The <tt>tokenize()</tt> function</h2>\n<p>To deep-tokenize a text string, call <tt>tokenizer.tokenize(text, **options)</tt>.\nThe <tt>text</tt> parameter can be a string, or an iterable that yields strings\n(such as a text file object).</p>\n<p>The function returns a Python <em>generator</em> of token objects.\nEach token object is a simple <tt>namedtuple</tt> with three\nfields: <tt>(kind, txt, val)</tt> (further documented below).</p>\n<p>The <tt>tokenizer.tokenize()</tt> function is typically called in a <tt>for</tt> loop:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">tokenizer</span>\n<span class=\"k\">for</span> <span class=\"n\">token</span> <span class=\"ow\">in</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"n\">mystring</span><span class=\"p\">):</span>\n    <span class=\"n\">kind</span><span class=\"p\">,</span> <span class=\"n\">txt</span><span class=\"p\">,</span> <span class=\"n\">val</span> <span class=\"o\">=</span> <span class=\"n\">token</span>\n    <span class=\"k\">if</span> <span class=\"n\">kind</span> <span class=\"o\">==</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">TOK</span><span class=\"o\">.</span><span class=\"n\">WORD</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Do something with word tokens</span>\n        <span class=\"k\">pass</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Do something else</span>\n        <span class=\"k\">pass</span>\n</pre>\n<p>Alternatively, create a token list from the returned generator:</p>\n<pre>token_list = list(tokenizer.tokenize(mystring))\n</pre>\n<p>In Python 2.7, you can pass either <tt>unicode</tt> strings or <tt>str</tt>\nbyte strings to <tt>tokenizer.tokenize()</tt>. In the latter case, the\nbyte string is assumed to be encoded in UTF-8.</p>\n</div>\n<div id=\"the-split-into-sentences-function\">\n<h2>The <tt>split_into_sentences()</tt> function</h2>\n<p>To shallow-tokenize a text string, call\n<tt>tokenizer.split_into_sentences(text_or_gen, **options)</tt>.\nThe <tt>text_or_gen</tt> parameter can be a string, or an iterable that yields\nstrings (such as a text file object).</p>\n<p>This function returns a Python <em>generator</em> of strings, yielding a string\nfor each sentence in the input. Within a sentence, the tokens are\nseparated by spaces.</p>\n<p>You can pass the option <tt>normalize=True</tt> to the function if you want\nthe normalized form of punctuation tokens. Normalization outputs\nIcelandic single and double quotes (\u201ethese\u201c) instead of English-style\nones (\u201cthese\u201d), converts three-dot ellipsis \u2026 to single character\nellipsis \u2026, and casts en-dashes \u2013 and em-dashes \u2014 to regular hyphens.</p>\n<p>The <tt>tokenizer.split_into_sentences()</tt> function is typically called\nin a <tt>for</tt> loop:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">tokenizer</span>\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">\"example.txt\"</span><span class=\"p\">,</span> <span class=\"s2\">\"r\"</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s2\">\"utf-8\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"c1\"># You can pass a file object directly to split_into_sentences()</span>\n    <span class=\"k\">for</span> <span class=\"n\">sentence</span> <span class=\"ow\">in</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">split_into_sentences</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">):</span>\n        <span class=\"c1\"># sentence is a string of space-separated tokens</span>\n        <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">sentence</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">()</span>\n        <span class=\"c1\"># Now, tokens is a list of strings, one for each token</span>\n        <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">tokens</span><span class=\"p\">:</span>\n            <span class=\"c1\"># Do something with the token t</span>\n            <span class=\"k\">pass</span>\n</pre>\n</div>\n<div id=\"the-correct-spaces-function\">\n<h2>The <tt>correct_spaces()</tt> function</h2>\n<p>The <tt>tokenizer.correct_spaces(text)</tt> function returns a string after\nsplitting it up and re-joining it with correct whitespace around\npunctuation tokens. Example:</p>\n<pre>&gt;&gt;&gt; import tokenizer\n&gt;&gt;&gt; tokenizer.correct_spaces(\n... \"Fr\u00e9tt \\n  dagsins:J\u00f3n\\t ,Fri\u00f0geir og P\u00e1ll ! 100  /  2  =   50\"\n... )\n'Fr\u00e9tt dagsins: J\u00f3n, Fri\u00f0geir og P\u00e1ll! 100/2 = 50'\n</pre>\n</div>\n<div id=\"the-detokenize-function\">\n<h2>The <tt>detokenize()</tt> function</h2>\n<p>The <tt>tokenizer.detokenize(tokens, normalize=False)</tt> function\ntakes an iterable of token objects and returns a corresponding, correctly\nspaced text string, composed from the tokens\u2019 text. If the\n<tt>normalize</tt> parameter is set to <tt>True</tt>,\nthe function uses the normalized form of any punctuation tokens, such\nas proper Icelandic single and double quotes instead of English-type\nquotes. Example:</p>\n<pre>&gt;&gt;&gt; import tokenizer\n&gt;&gt;&gt; toklist = list(tokenizer.tokenize(\"Hann sag\u00f0i: \u201e\u00de\u00fa ert \u00e1g\u00e6t!\u201c.\"))\n&gt;&gt;&gt; tokenizer.detokenize(toklist, normalize=True)\n'Hann sag\u00f0i: \u201e\u00de\u00fa ert \u00e1g\u00e6t!\u201c.'\n</pre>\n</div>\n<div id=\"the-normalized-text-function\">\n<h2>The <tt>normalized_text()</tt> function</h2>\n<p>The <tt>tokenizer.normalized_text(token)</tt> function\nreturns the normalized text for a token. This means that the original\ntoken text is returned except for certain punctuation tokens, where a\nnormalized form is returned instead. Specifically, English-type quotes\nare converted to Icelandic ones, and en- and em-dashes are converted\nto regular hyphens.</p>\n</div>\n<div id=\"the-text-from-tokens-function\">\n<h2>The <tt>text_from_tokens()</tt> function</h2>\n<p>The <tt>tokenizer.text_from_tokens(tokens)</tt> function\nreturns a concatenation of the text contents of the given token list,\nwith spaces between tokens. Example:</p>\n<pre>&gt;&gt;&gt; import tokenizer\n&gt;&gt;&gt; toklist = list(tokenizer.tokenize(\"Hann sag\u00f0i: \\\"\u00de\u00fa ert \u00e1g\u00e6t!\\\".\"))\n&gt;&gt;&gt; tokenizer.text_from_tokens(toklist)\n'Hann sag\u00f0i : \" \u00de\u00fa ert \u00e1g\u00e6t ! \" .'\n</pre>\n</div>\n<div id=\"the-normalized-text-from-tokens-function\">\n<h2>The <tt>normalized_text_from_tokens()</tt> function</h2>\n<p>The <tt>tokenizer.normalized_text_from_tokens(tokens)</tt> function\nreturns a concatenation of the normalized text contents of the given\ntoken list, with spaces between tokens. Example (note the double quotes):</p>\n<pre>&gt;&gt;&gt; import tokenizer\n&gt;&gt;&gt; toklist = list(tokenizer.tokenize(\"Hann sag\u00f0i: \\\"\u00de\u00fa ert \u00e1g\u00e6t!\\\".\"))\n&gt;&gt;&gt; tokenizer.normalized_text_from_tokens(toklist)\n'Hann sag\u00f0i : \u201e \u00de\u00fa ert \u00e1g\u00e6t ! \u201c .'\n</pre>\n</div>\n<div id=\"tokenization-options\">\n<h2>Tokenization options</h2>\n<p>You can optionally pass one or more of the following options as\nkeyword parameters to the <tt>tokenize()</tt> and <tt>split_into_sentences()</tt>\nfunctions:</p>\n<ul>\n<li><p><tt><span class=\"pre\">convert_numbers=[bool]</span></tt></p>\n<p>Setting this option to <tt>True</tt> causes the tokenizer to convert numbers\nand amounts with\nEnglish-style decimal points (<tt>.</tt>) and thousands separators (<tt>,</tt>)\nto Icelandic format, where the decimal separator is a comma (<tt>,</tt>)\nand the thousands separator is a period (<tt>.</tt>). <tt>$1,234.56</tt> is thus\nconverted to a token whose text is <tt>$1.234,56</tt>.</p>\n<p>The default value for the <tt>convert_numbers</tt> option is <tt>False</tt>.</p>\n<p>Note that in versions of Tokenizer prior to 1.4, <tt>convert_numbers</tt>\nwas <tt>True</tt>.</p>\n</li>\n<li><p><tt><span class=\"pre\">handle_kludgy_ordinals=[value]</span></tt></p>\n<p>This options controls the way Tokenizer handles \u2018kludgy\u2019 ordinals, such as\n<em>1sti</em>, <em>4\u00f0u</em>, or <em>2ja</em>. By default, such ordinals are returned unmodified\n(\u2018passed through\u2019) as word tokens (<tt>TOK.WORD</tt>).\nHowever, this can be modified as follows:</p>\n<ul>\n<li><tt>tokenizer.KLUDGY_ORDINALS_MODIFY</tt>: Kludgy ordinals are corrected\nto become \u2018proper\u2019 word tokens, i.e. <em>1sti</em> becomes <em>fyrsti</em> and\n<em>2ja</em> becomes <em>tveggja</em>.</li>\n<li><tt>tokenizer.KLUDGY_ORDINALS_TRANSLATE</tt>: Kludgy ordinals that represent\nproper ordinal numbers are translated to ordinal tokens (<tt>TOK.ORDINAL</tt>),\nwith their original text and their ordinal value. <em>1sti</em> thus\nbecomes a <tt>TOK.ORDINAL</tt> token with a value of 1, and <em>3ja</em> becomes\na <tt>TOK.ORDINAL</tt> with a value of 3.</li>\n<li><tt>tokenizer.KLUDGY_ORDINALS_PASS_THROUGH</tt> is the default value of\nthe option. It causes kludgy ordinals to be returned unmodified as\nword tokens.</li>\n</ul>\n<p>Note that versions of Tokenizer prior to 1.4 behaved as if\n<tt>handle_kludgy_ordinals</tt> were set to\n<tt>tokenizer.KLUDGY_ORDINALS_TRANSLATE</tt>.</p>\n</li>\n</ul>\n</div>\n<div id=\"the-token-object\">\n<h2>The token object</h2>\n<p>Each token is represented by a <tt>namedtuple</tt> with three fields:\n<tt>(kind, txt, val)</tt>.</p>\n<div id=\"the-kind-field\">\n<h3>The <tt>kind</tt> field</h3>\n<p>The <tt>kind</tt> field contains one of the following integer constants,\ndefined within the <tt>TOK</tt> class:</p>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>Constant</th>\n<th>Value</th>\n<th>Explanation</th>\n<th>Examples</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>PUNCTUATION</td>\n<td>1</td>\n<td>Punctuation</td>\n<td>. ! ; % &amp;</td>\n</tr>\n<tr><td>TIME</td>\n<td>2</td>\n<td>Time (h, m, s)</td>\n<td><div>\n<div>11:35:40</div>\n<div>kl. 7:05</div>\n<div>klukkan 23:35</div>\n</div>\n</td>\n</tr>\n<tr><td>DATE *</td>\n<td>3</td>\n<td>Date (y, m, d)</td>\n<td>[Unused, see DATEABS and\nDATEREL]</td>\n</tr>\n<tr><td>YEAR</td>\n<td>4</td>\n<td>Year</td>\n<td><div>\n<div>\u00e1ri\u00f0 874 e.Kr.</div>\n<div>1965</div>\n<div>44 f.Kr.</div>\n</div>\n</td>\n</tr>\n<tr><td>NUMBER</td>\n<td>5</td>\n<td>Number</td>\n<td><div>\n<div>100</div>\n<div>1.965</div>\n<div>1.965,34</div>\n<div>1,965.34</div>\n<div>2\u215e</div>\n</div>\n</td>\n</tr>\n<tr><td>WORD</td>\n<td>6</td>\n<td>Word</td>\n<td><div>\n<div>kattaeftirlit</div>\n<div>hunda- og kattaeftirlit</div>\n</div>\n</td>\n</tr>\n<tr><td>TELNO</td>\n<td>7</td>\n<td>Telephone number</td>\n<td><div>\n<div>5254764</div>\n<div>699-4244</div>\n<div>410 4000</div>\n</div>\n</td>\n</tr>\n<tr><td>PERCENT</td>\n<td>8</td>\n<td>Percentage</td>\n<td>78%</td>\n</tr>\n<tr><td>URL</td>\n<td>9</td>\n<td>URL</td>\n<td><div>\n<div><a href=\"https://greynir.is\" rel=\"nofollow\">https://greynir.is</a></div>\n<div><a href=\"http://tiny.cc/28695y\" rel=\"nofollow\">http://tiny.cc/28695y</a></div>\n</div>\n</td>\n</tr>\n<tr><td>ORDINAL</td>\n<td>10</td>\n<td>Ordinal number</td>\n<td><div>\n<div>30.</div>\n<div>XVIII.</div>\n</div>\n</td>\n</tr>\n<tr><td>TIMESTAMP *</td>\n<td>11</td>\n<td>Timestamp</td>\n<td>[Unused, see\nTIMESTAMPABS and\nTIMESTAMPREL]</td>\n</tr>\n<tr><td>CURRENCY *</td>\n<td>12</td>\n<td>Currency name</td>\n<td>[Unused]</td>\n</tr>\n<tr><td>AMOUNT</td>\n<td>13</td>\n<td>Amount</td>\n<td><div>\n<div>\u20ac2.345,67</div>\n<div>750 \u00fe\u00fas.kr.</div>\n<div>2,7 mr\u00f0. USD</div>\n<div>kr. 9.900</div>\n<div>EUR 200</div>\n</div>\n</td>\n</tr>\n<tr><td>PERSON *</td>\n<td>14</td>\n<td>Person name</td>\n<td>[Unused]</td>\n</tr>\n<tr><td>EMAIL</td>\n<td>15</td>\n<td>E-mail</td>\n<td><tt>fake@news.is</tt></td>\n</tr>\n<tr><td>ENTITY *</td>\n<td>16</td>\n<td>Named entity</td>\n<td>[Unused]</td>\n</tr>\n<tr><td>UNKNOWN</td>\n<td>17</td>\n<td>Unknown token</td>\n<td>\u00a0</td>\n</tr>\n<tr><td>DATEABS</td>\n<td>18</td>\n<td>Absolute date</td>\n<td><div>\n<div>30. desember 1965</div>\n<div>30/12/1965</div>\n<div>1965-12-30</div>\n<div>1965/12/30</div>\n</div>\n</td>\n</tr>\n<tr><td>DATEREL</td>\n<td>19</td>\n<td>Relative date</td>\n<td><div>\n<div>15. mars</div>\n<div>15/3</div>\n<div>15.3.</div>\n<div>mars 1911</div>\n</div>\n</td>\n</tr>\n<tr><td>TIMESTAMPABS</td>\n<td>20</td>\n<td>Absolute timestamp</td>\n<td><div>\n<div>30. desember 1965 11:34</div>\n<div>1965-12-30 kl. 13:00</div>\n</div>\n</td>\n</tr>\n<tr><td>TIMESTAMPREL</td>\n<td>21</td>\n<td>Relative timestamp</td>\n<td><div>\n<div>30. desember kl. 13:00</div>\n</div>\n</td>\n</tr>\n<tr><td>MEASUREMENT</td>\n<td>22</td>\n<td>Value with a\nmeasurement unit</td>\n<td><div>\n<div>690 MW</div>\n<div>1.010 hPa</div>\n<div>220 m\u00b2</div>\n<div>80\u00b0 C</div>\n</div>\n</td>\n</tr>\n<tr><td>NUMWLETTER</td>\n<td>23</td>\n<td>Number followed by\na single letter</td>\n<td><div>\n<div>14a</div>\n<div>7B</div>\n</div>\n</td>\n</tr>\n<tr><td>DOMAIN</td>\n<td>24</td>\n<td>Domain name</td>\n<td><div>\n<div>greynir.is</div>\n<div>Reddit.com</div>\n<div>www.wikipedia.org</div>\n</div>\n</td>\n</tr>\n<tr><td>HASHTAG</td>\n<td>25</td>\n<td>Hashtag</td>\n<td><div>\n<div>#MeToo</div>\n<div>#12stig</div>\n</div>\n</td>\n</tr>\n<tr><td>MOLECULE</td>\n<td>26</td>\n<td>Molecular formula</td>\n<td><div>\n<div>H2SO4</div>\n<div>CO2</div>\n</div>\n</td>\n</tr>\n<tr><td>SSN</td>\n<td>27</td>\n<td>Social security\nnumber (<em>kennitala</em>)</td>\n<td><div>\n<div>591213-1480</div>\n</div>\n</td>\n</tr>\n<tr><td>USERNAME</td>\n<td>28</td>\n<td>Twitter user handle</td>\n<td><div>\n<div>@username_123</div>\n</div>\n</td>\n</tr>\n<tr><td>SERIALNUMBER</td>\n<td>29</td>\n<td>Serial number</td>\n<td><div>\n<div>394-5388</div>\n<div>12-345-6789</div>\n</div>\n</td>\n</tr>\n<tr><td>S_BEGIN</td>\n<td>11001</td>\n<td>Start of sentence</td>\n<td>\u00a0</td>\n</tr>\n<tr><td>S_END</td>\n<td>11002</td>\n<td>End of sentence</td>\n<td>\u00a0</td>\n</tr>\n</tbody>\n</table>\n<p>(*) The token types marked with an asterisk are reserved for the Reynir package\nand not currently returned by the tokenizer.</p>\n<p>To obtain a descriptive text for a token kind, use\n<tt>TOK.descr[token.kind]</tt> (see example above).</p>\n</div>\n<div id=\"the-txt-field\">\n<h3>The <tt>txt</tt> field</h3>\n<p>The <tt>txt</tt> field contains the original source text for the token,\nwith the following exceptions:</p>\n<ul>\n<li>All contiguous whitespace (spaces, tabs, newlines) is coalesced\ninto single spaces (<tt>\" \"</tt>) within the <tt>txt</tt> field. A date\ntoken that is parsed from a source text of <tt>\"29.\u00a0 \\n\u00a0\u00a0 jan\u00faar\"</tt>\nthus has a <tt>txt</tt> of <tt>\"29. jan\u00faar\"</tt>.</li>\n<li>Tokenizer automatically merges Unicode <tt>COMBINING ACUTE ACCENT</tt>\n(code point 769) and <tt>COMBINING DIAERESIS</tt> (code point 776)\nwith vowels to form single code points for the Icelandic letters\n\u00e1, \u00e9, \u00ed, \u00f3, \u00fa, \u00fd and \u00f6, in both lower and upper case.</li>\n<li>If the appropriate options are specified (see above), it converts\nkludgy ordinals (<em>3ja</em>) to proper ones (<em>\u00feri\u00f0ja</em>), and English-style\nthousand and decimal separators to Icelandic ones\n(<em>10,345.67</em> becomes <em>10.345,67</em>).</li>\n</ul>\n<p>In the case of abbreviations that end a sentence, the final period\n<tt>\".\"</tt> is a separate token, and it is consequently omitted from the\nabbreviation token\u2019s <tt>txt</tt> field. A sentence ending in <em>o.s.frv.</em>\nwill thus end with two tokens, the next-to-last one being the tuple\n<tt>(TOK.WORD, \"o.s.frv\", \"og svo framvegis\")</tt> - note the omitted\nperiod in the <tt>txt</tt> field - and the last one being\n<tt>(TOK.PUNCTUATION, <span class=\"pre\">\".\",</span> (3, <span class=\"pre\">\".\"))</span></tt> (this tuple form is further\nexplained below).</p>\n</div>\n<div id=\"the-val-field\">\n<h3>The <tt>val</tt> field</h3>\n<p>The <tt>val</tt> field contains auxiliary information, corresponding to\nthe token kind, as follows:</p>\n<ul>\n<li><p>For <tt>TOK.PUNCTUATION</tt>, the <tt>val</tt> field contains a tuple with\ntwo items: <tt>(whitespace, normalform)</tt>. The first item (<tt>token.val[0]</tt>)\nspecifies the whitespace normally found around the symbol in question,\nas an integer:</p>\n<pre>TP_LEFT = 1   # Whitespace to the left\nTP_CENTER = 2 # Whitespace to the left and right\nTP_RIGHT = 3  # Whitespace to the right\nTP_NONE = 4   # No whitespace\n</pre>\n<p>The second item (<tt>token.val[1]</tt>) contains a normalized representation of the\npunctuation. For instance, various forms of single and double\nquotes are represented as Icelandic ones (i.e. \u201ethese\u201c or \u201athese\u2018) in\nnormalized form, and ellipsis (\u201c\u2026\u201d) are represented as the single\ncharacter \u201c\u2026\u201d.</p>\n</li>\n<li><p>For <tt>TOK.TIME</tt>, the <tt>val</tt> field contains an\n<tt>(hour, minute, second)</tt> tuple.</p>\n</li>\n<li><p>For <tt>TOK.DATEABS</tt>, the <tt>val</tt> field contains a\n<tt>(year, month, day)</tt> tuple (all 1-based).</p>\n</li>\n<li><p>For <tt>TOK.DATEREL</tt>, the <tt>val</tt> field contains a\n<tt>(year, month, day)</tt> tuple (all 1-based),\nexcept that a least one of the tuple fields is missing and set to 0.\nExample: <em>3. j\u00fan\u00ed</em> becomes <tt>TOK.DATEREL</tt> with the fields <tt>(0, 6, 3)</tt>\nas the year is missing.</p>\n</li>\n<li><p>For <tt>TOK.YEAR</tt>, the <tt>val</tt> field contains the year as an integer.\nA negative number indicates that the year is BCE (<em>fyrir Krist</em>),\nspecified with the suffix <em>f.Kr.</em> (e.g. <em>\u00e1ri\u00f0 33 f.Kr.</em>).</p>\n</li>\n<li><p>For <tt>TOK.NUMBER</tt>, the <tt>val</tt> field contains a tuple\n<tt>(number, None, None)</tt>.\n(The two empty fields are included for compatibility with Greynir.)</p>\n</li>\n<li><p>For <tt>TOK.WORD</tt>, the <tt>val</tt> field contains the full expansion\nof an abbreviation, as a list containing a single tuple, or <tt>None</tt>\nif the word is not abbreviated.</p>\n</li>\n<li><p>For <tt>TOK.PERCENT</tt>, the <tt>val</tt> field contains a tuple\nof <tt>(percentage, None, None)</tt>.</p>\n</li>\n<li><p>For <tt>TOK.ORDINAL</tt>, the <tt>val</tt> field contains the ordinal value\nas an integer. The original ordinal may be a decimal number\nor a Roman numeral.</p>\n</li>\n<li><p>For <tt>TOK.TIMESTAMP</tt>, the <tt>val</tt> field contains\na <tt>(year, month, day, hour, minute, second)</tt> tuple.</p>\n</li>\n<li><p>For <tt>TOK.AMOUNT</tt>, the <tt>val</tt> field contains\nan <tt>(amount, currency, None, None)</tt> tuple. The amount is a float, and\nthe currency is an ISO currency code, e.g. <em>USD</em> for dollars ($ sign),\n<em>EUR</em> for euros (\u20ac sign) or <em>ISK</em> for Icelandic kr\u00f3na\n(<em>kr.</em> abbreviation). (The two empty fields are included for\ncompatibility with Greynir.)</p>\n</li>\n<li><p>For <tt>TOK.MEASUREMENT</tt>, the <tt>val</tt> field contains a <tt>(unit, value)</tt>\ntuple, where <tt>unit</tt> is a base SI unit (such as <tt>g</tt>, <tt>m</tt>,\n<tt>m\u00b2</tt>, <tt>s</tt>, <tt>W</tt>, <tt>Hz</tt>, <tt>K</tt> for temperature in Kelvin).</p>\n</li>\n<li><p>For <tt>TOK.TELNO</tt>, the <tt>val</tt> field contains a tuple: <tt>(number, cc)</tt>\nwhere the first item is the phone number\nin a normalized <tt><span class=\"pre\">NNN-NNNN</span></tt> format, i.e. always including a hyphen,\nand the second item is the country code, eventually prefixed by <tt>+</tt>.\nThe country code defaults to <tt>354</tt> (Iceland).</p>\n</li>\n</ul>\n</div>\n</div>\n<div id=\"abbreviations\">\n<h2>Abbreviations</h2>\n<p>Abbreviations recognized by Tokenizer are defined in the <tt>Abbrev.conf</tt>\nfile, found in the <tt>src/tokenizer/</tt> directory. This is a text file with\nabbreviations, their definitions and explanatory comments.</p>\n<p>When an abbreviation is encountered, it is recognized as a word token\n(i.e. having its <tt>kind</tt> field equal to <tt>TOK.WORD</tt>).\nIts expansion(s) are included in the token\u2019s\n<tt>val</tt> field as a list containing tuples of the format\n<tt>(ordmynd, utg, ordfl, fl, stofn, beyging)</tt>.\nAn example is <em>o.s.frv.</em>, which results in a <tt>val</tt> field equal to\n<tt><span class=\"pre\">[('og</span> svo framvegis', 0, 'ao', 'frasi', <span class=\"pre\">'o.s.frv.',</span> <span class=\"pre\">'-')]</span></tt>.</p>\n<p>The tuple format is designed to be compatible with the\n<em>Database of Modern Icelandic Inflection</em> (<em>DMII</em>),\n<em>Beygingarl\u00fdsing \u00edslensks n\u00fat\u00edmam\u00e1ls</em>.</p>\n</div>\n<div id=\"development-installation\">\n<h2>Development installation</h2>\n<p>To install Tokenizer in development mode, where you can easily\nmodify the source files (assuming you have <tt>git</tt> available):</p>\n<pre><span class=\"gp\">$</span> git clone https://github.com/mideind/Tokenizer\n<span class=\"gp\">$</span> <span class=\"nb\">cd</span> Tokenizer\n<span class=\"gp\">$</span> <span class=\"c1\"># [ Activate your virtualenv here, if you have one ]\n</span><span class=\"gp\">$</span><span class=\"c1\"></span> pip install -e .\n</pre>\n</div>\n<div id=\"test-suite\">\n<h2>Test suite</h2>\n<p>Tokenizer comes with a large test suite.\nThe file <tt>test/test_tokenizer.py</tt> contains built-in tests that\nrun under <tt>pytest</tt>.</p>\n<p>To run the built-in tests, install <a href=\"https://docs.pytest.org/en/latest/\" rel=\"nofollow\">pytest</a>,\n<tt>cd</tt> to your <tt>Tokenizer</tt> subdirectory (and optionally\nactivate your virtualenv), then run:</p>\n<pre><span class=\"gp\">$</span> python -m pytest\n</pre>\n<p>The file <tt>test/toktest_large.txt</tt> contains a test set of 13,075 lines.\nThe lines test sentence detection, token detection and token classification.\nFor analysis, <tt>test/toktest_large_gold_perfect.txt</tt> contains\nthe expected output of a perfect shallow tokenization, and\n<tt>test/toktest_large_gold_acceptable.txt</tt> contains the current output of the\nshallow tokenization.</p>\n<p>The file <tt>test/Overview.txt</tt> (only in Icelandic) contains a description\nof the test set, including line numbers for each part in both\n<tt>test/toktest_large.txt</tt> and <tt>test/toktest_large_gold_acceptable.txt</tt>,\nand a tag describing what is being tested in each part.</p>\n<p>It also contains a description of a perfect shallow tokenization for each part,\nacceptable tokenization and the current behaviour.\nAs such, the description is an analysis of which edge cases the tokenizer\ncan handle and which it can not.</p>\n<p>To test the tokenizer on the large test set the following needs to be typed\nin the command line:</p>\n<pre><span class=\"gp\">$</span> tokenize test/toktest_large.txt test/toktest_large_out.txt\n</pre>\n<p>To compare it to the acceptable behaviour:</p>\n<pre><span class=\"gp\">$</span> diff test/toktest_large_out.txt test/toktest_large_gold_acceptable.txt &gt; diff.txt\n</pre>\n<p>The file <tt>test/toktest_normal.txt</tt> contains a running text from recent\nnews articles, containing no edge cases. The gold standard for that file\ncan be found in the file <tt>test/toktest_normal_gold_expected.txt</tt>.</p>\n</div>\n<div id=\"changelog\">\n<h2>Changelog</h2>\n<ul>\n<li>Version 2.0.5: Fixed bug where single uppercase letters were erroneously\nbeing recognized as abbreviations, causing prepositions such as \u2018\u00cd\u2019 and \u2018\u00c1\u2019\nat the beginning of sentences to be misunderstood in ReynirPackage</li>\n<li>Version 2.0.4: Added imperfect abbreviations (<em>amk.</em>, <em>osfrv.</em>); recognized\n<em>klukkan h\u00e1lf tv\u00f6</em> as a <tt>TOK.TIME</tt></li>\n<li>Version 2.0.3: Fixed bug in <tt>detokenize()</tt> where abbreviations, domains\nand e-mails containing periods were wrongly split</li>\n<li>Version 2.0.2: Spelled-out day ordinals are no longer included as a part of\n<tt>TOK.DATEREL</tt> tokens. Thus, <em>\u00feri\u00f0ji j\u00fan\u00ed</em> is now a <tt>TOK.WORD</tt>\nfollowed by a <tt>TOK.DATEREL</tt>. <em>3. j\u00fan\u00ed</em> continues to be parsed as\na single <tt>TOK.DATEREL</tt></li>\n<li>Version 2.0.1: Order of abbreviation meanings within the <tt>token.val</tt> field\nmade deterministic; fixed bug in measurement unit handling</li>\n<li>Version 2.0.0: Added command line tool; added <tt>split_into_sentences()</tt>\nand <tt>detokenize()</tt> functions; removed <tt>convert_telno</tt> option;\nsplitting of coalesced tokens made more robust;\nadded <tt>TOK.SSN</tt>, <tt>TOK.MOLECULE</tt>, <tt>TOK.USERNAME</tt> and\n<tt>TOK.SERIALNUMBER</tt> token kinds; abbreviations can now have multiple\nmeanings</li>\n<li>Version 1.4.0: Added the <tt>**options</tt> parameter to the\n<tt>tokenize()</tt> function, giving control over the handling of numbers,\ntelephone numbers, and \u2018kludgy\u2019 ordinals</li>\n<li>Version 1.3.0: Added <tt>TOK.DOMAIN</tt> and <tt>TOK.HASHTAG</tt> token types;\nimproved handling of capitalized month name <em>\u00c1g\u00fast</em>, which is\nnow recognized when following an ordinal number; improved recognition\nof telephone numbers; added abbreviations</li>\n<li>Version 1.2.3: Added abbreviations; updated GitHub URLs</li>\n<li>Version 1.2.2: Added support for composites with more than two parts, i.e.\n<em>\u201ed\u00f3msm\u00e1la-, fer\u00f0am\u00e1la-, i\u00f0na\u00f0ar- og n\u00fdsk\u00f6punarr\u00e1\u00f0herra\u201c</em>; added support for\n<tt>\u00b1</tt> sign; added several abbreviations</li>\n<li>Version 1.2.1: Fixed bug where the name <em>\u00c1g\u00fast</em> was recognized\nas a month name; Unicode nonbreaking and invisible space characters\nare now removed before tokenization</li>\n<li>Version 1.2.0: Added support for Unicode fraction characters;\nenhanced handing of degrees (\u00b0, \u00b0C, \u00b0F); fixed bug in cubic meter\nmeasurement unit; more abbreviations</li>\n<li>Version 1.1.2: Fixed bug in liter (<tt>l</tt> and <tt>ltr</tt>) measurement units</li>\n<li>Version 1.1.1: Added <tt>mark_paragraphs()</tt> function</li>\n<li>Version 1.1.0: All abbreviations in <tt>Abbrev.conf</tt> are now\nreturned with their meaning in a tuple in <tt>token.val</tt>;\nhandling of \u2018mbl.is\u2019 fixed</li>\n<li>Version 1.0.9: Added abbreviation \u2018MAST\u2019; harmonized copyright headers</li>\n<li>Version 1.0.8: Bug fixes in <tt>DATEREL</tt>, <tt>MEASUREMENT</tt> and <tt>NUMWLETTER</tt>\ntoken handling; added \u2018kWst\u2019 and \u2018MWst\u2019 measurement units; blackened</li>\n<li>Version 1.0.7: Added <tt>TOK.NUMWLETTER</tt> token type</li>\n<li>Version 1.0.6: Automatic merging of Unicode <tt>COMBINING ACUTE ACCENT</tt> and\n<tt>COMBINING DIAERESIS</tt> code points with vowels</li>\n<li>Version 1.0.5: Date/time and amount tokens coalesced to a further extent</li>\n<li>Version 1.0.4: Added <tt>TOK.DATEABS</tt>, <tt>TOK.TIMESTAMPABS</tt>,\n<tt>TOK.MEASUREMENT</tt></li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 6892590, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "51f8450a721544630aef1e6fb6a25b5c", "sha256": "e24eed67a141272d0cb88dfd3fa730082ca1aa119a38c9df57b87fda4fdc30f8"}, "downloads": -1, "filename": "tokenizer-0.1.1.tar.gz", "has_sig": false, "md5_digest": "51f8450a721544630aef1e6fb6a25b5c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 49706, "upload_time": "2017-10-01T16:08:19", "upload_time_iso_8601": "2017-10-01T16:08:19.234402Z", "url": "https://files.pythonhosted.org/packages/61/50/b35a23d91503fa7ba757129c1bc04d37cfb7e0ead2b19729307f2a6dafb2/tokenizer-0.1.1.tar.gz", "yanked": false}], "0.1.10": [{"comment_text": "", "digests": {"md5": "a2f38d4f1eb5117315ce3af84cf0f14f", "sha256": "79bc0eb2771bc7883333affc4275ae4bb818d97056bb8d1188b29cffdfa07a7d"}, "downloads": -1, "filename": "tokenizer-0.1.10-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "a2f38d4f1eb5117315ce3af84cf0f14f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 57425, "upload_time": "2018-03-13T11:36:36", "upload_time_iso_8601": "2018-03-13T11:36:36.586163Z", "url": "https://files.pythonhosted.org/packages/6f/f7/eaafb5a04f3f116d7b9ffc6a14a8ac86d8a835ae7c1f416562e15e9cd35a/tokenizer-0.1.10-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c690540e9f85ce46709157156cd48d6a", "sha256": "2f3db1d5889d6cf1e49112e664043285efec2f33fd3f4cb1dfe68ff90b739024"}, "downloads": -1, "filename": "tokenizer-0.1.10.tar.gz", "has_sig": false, "md5_digest": "c690540e9f85ce46709157156cd48d6a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 37558, "upload_time": "2018-03-13T11:36:37", "upload_time_iso_8601": "2018-03-13T11:36:37.835101Z", "url": "https://files.pythonhosted.org/packages/36/e2/cb7682af6888cef2853024b20a937cf7c26174a98140f2bbac836603d361/tokenizer-0.1.10.tar.gz", "yanked": false}], "0.1.11": [{"comment_text": "", "digests": {"md5": "75e9601df29e6bb4a14bad0a17fab077", "sha256": "fae881c189cf22d141fca48f0820b16a75fa004e953ccc96ce7ecbb8ef861def"}, "downloads": -1, "filename": "tokenizer-0.1.11-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "75e9601df29e6bb4a14bad0a17fab077", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 44084, "upload_time": "2018-03-27T16:32:27", "upload_time_iso_8601": "2018-03-27T16:32:27.304299Z", "url": "https://files.pythonhosted.org/packages/3f/0e/54313c2acb3fe97935a34620840d876886bf2a672199b74a71015c0caa8d/tokenizer-0.1.11-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "def6926838a7c7fbdc92598ec8f9883c", "sha256": "c18b968218f68aeac2c1c042fbba05d535e9de3958f7a1f224f4a172469bf504"}, "downloads": -1, "filename": "tokenizer-0.1.11.tar.gz", "has_sig": false, "md5_digest": "def6926838a7c7fbdc92598ec8f9883c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39475, "upload_time": "2018-03-27T16:32:28", "upload_time_iso_8601": "2018-03-27T16:32:28.616837Z", "url": "https://files.pythonhosted.org/packages/98/73/162789b761ad9ccd53717d092d1272a10cbbfcae2f48efd3d45fd3d423ae/tokenizer-0.1.11.tar.gz", "yanked": false}], "0.1.12": [{"comment_text": "", "digests": {"md5": "790e599aed2b5826f421b84ebc2c8a40", "sha256": "8f66449e46c6d9f5e609179eb6fcafca13ec115a934945e8412f30b19da529a1"}, "downloads": -1, "filename": "tokenizer-0.1.12-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "790e599aed2b5826f421b84ebc2c8a40", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 44084, "upload_time": "2018-04-05T16:30:27", "upload_time_iso_8601": "2018-04-05T16:30:27.437479Z", "url": "https://files.pythonhosted.org/packages/82/9a/a353c42e59c38402f55fe052510bc06d7427dba51b6a7503387bd734227c/tokenizer-0.1.12-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "467a869d471614155c0c8a3039bbbac6", "sha256": "fc2fdf6eea920ca78647f207af036c2ea8dc1c651603ccba3ad6fd4bcb88fa81"}, "downloads": -1, "filename": "tokenizer-0.1.12.tar.gz", "has_sig": false, "md5_digest": "467a869d471614155c0c8a3039bbbac6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39484, "upload_time": "2018-04-05T16:30:28", "upload_time_iso_8601": "2018-04-05T16:30:28.709703Z", "url": "https://files.pythonhosted.org/packages/c3/a7/051d352e0534a87681b72e9af96ef38e27f0f553b0470c36416bd3ff5088/tokenizer-0.1.12.tar.gz", "yanked": false}], "0.1.14": [{"comment_text": "", "digests": {"md5": "746ab3b7eb77d126b37e96368f887d0b", "sha256": "9dc8f8ba760261fdc1b19ccca400ac7eb2353228a35258c2a27b07b8ee2c37ee"}, "downloads": -1, "filename": "tokenizer-0.1.14-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "746ab3b7eb77d126b37e96368f887d0b", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 44324, "upload_time": "2018-04-11T22:17:49", "upload_time_iso_8601": "2018-04-11T22:17:49.982268Z", "url": "https://files.pythonhosted.org/packages/88/4b/99f6342b53797effd801560dd46b6294b77c26b91d57124a935fbf1faecb/tokenizer-0.1.14-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "021f88b2912e71dd18d4d3b5112b772d", "sha256": "4ded9ba1c0bbd4da325028a675ef960e0b1a4017f857e6c6a004ca084edc56d5"}, "downloads": -1, "filename": "tokenizer-0.1.14.tar.gz", "has_sig": false, "md5_digest": "021f88b2912e71dd18d4d3b5112b772d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39726, "upload_time": "2018-04-11T22:17:51", "upload_time_iso_8601": "2018-04-11T22:17:51.224002Z", "url": "https://files.pythonhosted.org/packages/b7/58/47e66e5fd526955f0f81a08a60b6712a41fada6b7075c6a512c621e144cb/tokenizer-0.1.14.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "bc8ad18886c03b320edff4280e1144f8", "sha256": "427b2aa6acedb3c8e89d78503ef4f22a9545cb455e1b48700084121d625d86e4"}, "downloads": -1, "filename": "tokenizer-0.1.2.tar.gz", "has_sig": false, "md5_digest": "bc8ad18886c03b320edff4280e1144f8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 49676, "upload_time": "2017-10-01T16:30:42", "upload_time_iso_8601": "2017-10-01T16:30:42.490305Z", "url": "https://files.pythonhosted.org/packages/43/92/bf087046d1ee49258c35448cc34608660a705ce5e01c0924b02518da3ed6/tokenizer-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "fb16da94ce5a387c80d2cb00a2cd53eb", "sha256": "9c4da27fcb68f7c5959923f78765f4b775fe337d5d955dfeb74a58214c11d86c"}, "downloads": -1, "filename": "tokenizer-0.1.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "fb16da94ce5a387c80d2cb00a2cd53eb", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 58150, "upload_time": "2017-10-01T18:41:08", "upload_time_iso_8601": "2017-10-01T18:41:08.312004Z", "url": "https://files.pythonhosted.org/packages/37/5e/35ddaa5e3fac34d0a440efd09cd7237fa94ef48047e7b4eca067e8ad51e2/tokenizer-0.1.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0f2d98d55110d35ca63b45ba4b853878", "sha256": "eb1cf95c6f4fcfa87bcb8addc2acf5955b284647398b5fd216e3bcfdf94be4d8"}, "downloads": -1, "filename": "tokenizer-0.1.3.tar.gz", "has_sig": false, "md5_digest": "0f2d98d55110d35ca63b45ba4b853878", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 50373, "upload_time": "2017-10-01T18:27:19", "upload_time_iso_8601": "2017-10-01T18:27:19.538457Z", "url": "https://files.pythonhosted.org/packages/2d/8e/38a33f2e6ed3f39eab6d6d0074ac292386afb431ffc09fd9327069451425/tokenizer-0.1.3.tar.gz", "yanked": false}], "1.0.0": [{"comment_text": "", "digests": {"md5": "453703199bea7e5cf41e5c545b6a9d0a", "sha256": "4669489d162d809e33d4edacbeaa2eacb437cad5e3e9392e0724c1a816cc3183"}, "downloads": -1, "filename": "tokenizer-1.0.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "453703199bea7e5cf41e5c545b6a9d0a", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 44402, "upload_time": "2018-04-20T12:09:39", "upload_time_iso_8601": "2018-04-20T12:09:39.753523Z", "url": "https://files.pythonhosted.org/packages/46/88/02b2c7b9e2fb680e8cea4203a2ef3ed2aaa73330e44d3448804aa8ae2ead/tokenizer-1.0.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "20c3d88f6c36fcf67002ec39751e8ff3", "sha256": "f80f57dfb1c8977411069963c8fc0113534a37d08a30745a83770119f71028ce"}, "downloads": -1, "filename": "tokenizer-1.0.0.tar.gz", "has_sig": false, "md5_digest": "20c3d88f6c36fcf67002ec39751e8ff3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39800, "upload_time": "2018-04-20T12:09:41", "upload_time_iso_8601": "2018-04-20T12:09:41.408446Z", "url": "https://files.pythonhosted.org/packages/1e/f2/32830931d4fe86996f3a4c895288063997941100fdf0a0bbbc596c024df5/tokenizer-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "923b667f809b9e0fec4f3a56a0a2a77d", "sha256": "fb0a5c329ddb400919e8409bea724357b27b965a8b020e248bff425453d3170f"}, "downloads": -1, "filename": "tokenizer-1.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "923b667f809b9e0fec4f3a56a0a2a77d", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 44898, "upload_time": "2018-04-24T15:03:01", "upload_time_iso_8601": "2018-04-24T15:03:01.539689Z", "url": "https://files.pythonhosted.org/packages/c4/a7/ef324aedcaf4b4f7f5fe3244e70e6df569ab12b742b030fbdc6b80d7f108/tokenizer-1.0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "16c52b54690264bc2d4844c2cdf23ca4", "sha256": "1cc9a9d238ba9c5612a7a769cbb2b8501f83faf3c263adbaedb922a953db7d9c"}, "downloads": -1, "filename": "tokenizer-1.0.1.tar.gz", "has_sig": false, "md5_digest": "16c52b54690264bc2d4844c2cdf23ca4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 40136, "upload_time": "2018-04-24T15:03:02", "upload_time_iso_8601": "2018-04-24T15:03:02.999513Z", "url": "https://files.pythonhosted.org/packages/9a/ec/5d409842b01318dbdf6912a4c64962092b7e977bce2cf7ea9d50bcd93c93/tokenizer-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "8445af548b6ca05b4953a2f562067bcf", "sha256": "5cbab5a776babf070b90a444044180cfc405567d416d0b9506f85213870d9549"}, "downloads": -1, "filename": "tokenizer-1.0.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "8445af548b6ca05b4953a2f562067bcf", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 44885, "upload_time": "2018-04-25T14:02:25", "upload_time_iso_8601": "2018-04-25T14:02:25.029319Z", "url": "https://files.pythonhosted.org/packages/98/6f/f2518190080a83ce452f17d78c347f89c29b0efe522521db6302cb27e515/tokenizer-1.0.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "200011e7e8c52bbeb5fde02617703dfd", "sha256": "dfc9431549c0bfab0f55e2dd52d76580d5304e85df028925839ed4b7b362de93"}, "downloads": -1, "filename": "tokenizer-1.0.2.tar.gz", "has_sig": false, "md5_digest": "200011e7e8c52bbeb5fde02617703dfd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 40193, "upload_time": "2018-04-25T14:02:26", "upload_time_iso_8601": "2018-04-25T14:02:26.450333Z", "url": "https://files.pythonhosted.org/packages/da/58/1ed6a7a021d11ca1675513b56f07ebede69be69600e3329fd4832b5fa2f8/tokenizer-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "7da12128e7925b68519609ab88ded473", "sha256": "e6a5a9812c0efa7a8de676d06a0f5c26b4feaea53ca536b1aef13a20072dee71"}, "downloads": -1, "filename": "tokenizer-1.0.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "7da12128e7925b68519609ab88ded473", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 45309, "upload_time": "2018-04-30T13:51:40", "upload_time_iso_8601": "2018-04-30T13:51:40.056805Z", "url": "https://files.pythonhosted.org/packages/01/f4/c3d461945ef3dce6f725a01639176122b25d72bd33a8746c61438c7f8f33/tokenizer-1.0.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9fa4da39315201d69959ae3564a29da3", "sha256": "d2c4f4d356d1a82b8755d7b7e730b1060e9f82e1f6bcf4752781f7133bf409e8"}, "downloads": -1, "filename": "tokenizer-1.0.3.tar.gz", "has_sig": false, "md5_digest": "9fa4da39315201d69959ae3564a29da3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 40493, "upload_time": "2018-04-30T13:51:41", "upload_time_iso_8601": "2018-04-30T13:51:41.388895Z", "url": "https://files.pythonhosted.org/packages/5a/83/e2584026ae01b2eade0be1779d75c5d2ac7f1c9f2557387473c22b1e0b2d/tokenizer-1.0.3.tar.gz", "yanked": false}], "1.0.4": [{"comment_text": "", "digests": {"md5": "ecc37be1f81e799c68c51fb19a14f0f7", "sha256": "573d4f754949c96661e7ba250e637cda3106064580cea6404d1e4d3b4aff3034"}, "downloads": -1, "filename": "tokenizer-1.0.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "ecc37be1f81e799c68c51fb19a14f0f7", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 55258, "upload_time": "2018-06-08T17:48:32", "upload_time_iso_8601": "2018-06-08T17:48:32.123278Z", "url": "https://files.pythonhosted.org/packages/45/88/b623f12bb31f4cb54c406596128060458cfe9b8a16832b69a20468cfe0f4/tokenizer-1.0.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9f2ffd544902591bb5fdaae91f9b7c0c", "sha256": "dc2e64a3c9d8068a77b737a94bd48b2c7053dd39bce426d70e68ef95be027163"}, "downloads": -1, "filename": "tokenizer-1.0.4.tar.gz", "has_sig": false, "md5_digest": "9f2ffd544902591bb5fdaae91f9b7c0c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 54462, "upload_time": "2018-06-08T17:48:33", "upload_time_iso_8601": "2018-06-08T17:48:33.599661Z", "url": "https://files.pythonhosted.org/packages/0c/93/0b982ee194128528fa1a3aebb371fbd461ce1479323a0c4b3e55d2a2c692/tokenizer-1.0.4.tar.gz", "yanked": false}], "1.0.5": [{"comment_text": "", "digests": {"md5": "0c85816a67fb172fae9af62fa24d25d6", "sha256": "386a8ccab98cff94752ac92736f35fead6d122ba63ad9d770ca73672e22b0b15"}, "downloads": -1, "filename": "tokenizer-1.0.5-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "0c85816a67fb172fae9af62fa24d25d6", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 60633, "upload_time": "2018-07-23T15:50:40", "upload_time_iso_8601": "2018-07-23T15:50:40.306607Z", "url": "https://files.pythonhosted.org/packages/58/21/4b8618eecc77b2aa60505dfcd660e1148682c2a8c101e37dd0667550c0b1/tokenizer-1.0.5-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6f71ba3996e65e33852ae776ecfb9eaf", "sha256": "aebcc8b4bd3da2323ef4965558e27d5cb11682376a7897f096feb0a8a4fa5fe8"}, "downloads": -1, "filename": "tokenizer-1.0.5.tar.gz", "has_sig": false, "md5_digest": "6f71ba3996e65e33852ae776ecfb9eaf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 45680, "upload_time": "2018-07-23T15:50:41", "upload_time_iso_8601": "2018-07-23T15:50:41.494234Z", "url": "https://files.pythonhosted.org/packages/7d/50/d57447db0eea1ab161f53b7b23119426ed6d26399d9f70e33b2520b7a8b2/tokenizer-1.0.5.tar.gz", "yanked": false}], "1.0.6": [{"comment_text": "", "digests": {"md5": "fde46bf8210ac46ff033aaa85760ad65", "sha256": "77ae1fd23e3823509dd089eef361932c588aaf4b7842dbc1d84eb15b66564466"}, "downloads": -1, "filename": "tokenizer-1.0.6-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "fde46bf8210ac46ff033aaa85760ad65", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 68110, "upload_time": "2018-08-23T17:09:54", "upload_time_iso_8601": "2018-08-23T17:09:54.167380Z", "url": "https://files.pythonhosted.org/packages/96/a3/7ed7c578ed2ce3ce466863c73acb8c6d580edf3985d81e79401e01538ee5/tokenizer-1.0.6-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "aa47d826904a99ba59ca6cd4eaee7957", "sha256": "50cc5858da1832aefbd31b7ee8cc94866eee7d9af792bdea5971ae3114a757a0"}, "downloads": -1, "filename": "tokenizer-1.0.6.tar.gz", "has_sig": false, "md5_digest": "aa47d826904a99ba59ca6cd4eaee7957", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 54501, "upload_time": "2018-08-23T17:09:55", "upload_time_iso_8601": "2018-08-23T17:09:55.983566Z", "url": "https://files.pythonhosted.org/packages/16/a9/a9b0aa81bdacb9486305c96d6ef2e9f94819cc9892304ec425986a73b30b/tokenizer-1.0.6.tar.gz", "yanked": false}], "1.0.7": [{"comment_text": "", "digests": {"md5": "452ba09e534b5ae07666ee276e109626", "sha256": "c651c3343be5d33f5a4c33a6064b2c8abbf28695459717a00be35fac2491230e"}, "downloads": -1, "filename": "tokenizer-1.0.7-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "452ba09e534b5ae07666ee276e109626", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 68324, "upload_time": "2018-09-25T12:13:01", "upload_time_iso_8601": "2018-09-25T12:13:01.215018Z", "url": "https://files.pythonhosted.org/packages/b4/3c/6f444db3966704a76403feffd393803c3234295a213e6d5b680cfc6b9785/tokenizer-1.0.7-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "aa21d580f468cbabd38ba937bafba06f", "sha256": "0d597a5c13be013b83d3f7291d3e454402252df9e92068940f7b0a0419bb8476"}, "downloads": -1, "filename": "tokenizer-1.0.7.tar.gz", "has_sig": false, "md5_digest": "aa21d580f468cbabd38ba937bafba06f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 54965, "upload_time": "2018-09-25T12:13:03", "upload_time_iso_8601": "2018-09-25T12:13:03.297483Z", "url": "https://files.pythonhosted.org/packages/bb/11/7130c7f478207147514ff3e9456c5f64b7999125e3731ede797c1b363303/tokenizer-1.0.7.tar.gz", "yanked": false}], "1.0.8": [{"comment_text": "", "digests": {"md5": "c8601eecb9d2dc008a12647921b8edee", "sha256": "daddad477333c46c55f5f6e9c9e0f9a8cf3cf10d1624a1e1b1073812778c5d97"}, "downloads": -1, "filename": "tokenizer-1.0.8-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c8601eecb9d2dc008a12647921b8edee", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 60704, "upload_time": "2018-10-30T18:33:10", "upload_time_iso_8601": "2018-10-30T18:33:10.670692Z", "url": "https://files.pythonhosted.org/packages/af/8f/fc44a4ce5268382b87df43638a83d41a729cf3b0821a9c1b2494f7113e35/tokenizer-1.0.8-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "555fe52af91f4edc0071631f4841635b", "sha256": "282ef136ab122c58a1c16b4f2926928f23c6f9e106f8b0c92a932c4c001eafaa"}, "downloads": -1, "filename": "tokenizer-1.0.8.tar.gz", "has_sig": false, "md5_digest": "555fe52af91f4edc0071631f4841635b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 55783, "upload_time": "2018-10-30T18:33:12", "upload_time_iso_8601": "2018-10-30T18:33:12.559897Z", "url": "https://files.pythonhosted.org/packages/fe/49/da364494aee3526490bfea378f6c14c6a05bdbca030f4966addc4a9e6504/tokenizer-1.0.8.tar.gz", "yanked": false}], "1.0.9": [{"comment_text": "", "digests": {"md5": "2ade3b4246258baa37a743db78cb38a0", "sha256": "a31a7e3983a0ecdc362ec50c494649698bc2602c5e43ba3d81612d2212688168"}, "downloads": -1, "filename": "tokenizer-1.0.9-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "2ade3b4246258baa37a743db78cb38a0", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 61221, "upload_time": "2018-12-29T13:04:15", "upload_time_iso_8601": "2018-12-29T13:04:15.862248Z", "url": "https://files.pythonhosted.org/packages/d8/98/989d2fd86f8e6e92ffb19ff990ec2f0a3a908b731a540d1794b549c65cad/tokenizer-1.0.9-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c5b13e81b84aecc46a7904eed108bcf0", "sha256": "244c8c7e7606e9410f2dedf1e359fb73ab30f131eb3c8350a16ed8248313ded1"}, "downloads": -1, "filename": "tokenizer-1.0.9.tar.gz", "has_sig": false, "md5_digest": "c5b13e81b84aecc46a7904eed108bcf0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59879, "upload_time": "2018-12-29T13:04:17", "upload_time_iso_8601": "2018-12-29T13:04:17.658148Z", "url": "https://files.pythonhosted.org/packages/b1/cf/a542f3972f28ca5017ac77cf6a8168288bbfc93075c94b93c04fb0c8b0eb/tokenizer-1.0.9.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "9ea98409a3285355b1c04d37196e861c", "sha256": "538d28ab767eb417a8b2318d4e1b4b437f7bf2699381042ec4a96bb3af00d13f"}, "downloads": -1, "filename": "tokenizer-1.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "9ea98409a3285355b1c04d37196e861c", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 61732, "upload_time": "2019-01-02T14:37:20", "upload_time_iso_8601": "2019-01-02T14:37:20.070458Z", "url": "https://files.pythonhosted.org/packages/1f/50/d27c3fea1f3e367d1e4eab7600b1ee3797eba230f06baaf2d77f13bedb95/tokenizer-1.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1f3921126b2b2c8627f8c3c8dd796d58", "sha256": "4dcbecc913eac115d7c72c9a58bd8a33978a55559492eb1b8af8a4acd539abbd"}, "downloads": -1, "filename": "tokenizer-1.1.0.tar.gz", "has_sig": false, "md5_digest": "1f3921126b2b2c8627f8c3c8dd796d58", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 60674, "upload_time": "2019-01-02T14:37:22", "upload_time_iso_8601": "2019-01-02T14:37:22.125265Z", "url": "https://files.pythonhosted.org/packages/dc/7e/8322e7b1c8ba315813eb81fbcc91e8e26b576ba53433af955c4859ab772d/tokenizer-1.1.0.tar.gz", "yanked": false}], "1.1.1": [{"comment_text": "", "digests": {"md5": "f098d23c3f8e0ccc1a86ab89e97804b8", "sha256": "e382dbb0f0f3d936817aa100ca02e90e976ce11ee2635df9208d1dc0d6cd8a05"}, "downloads": -1, "filename": "tokenizer-1.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "f098d23c3f8e0ccc1a86ab89e97804b8", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 61993, "upload_time": "2019-01-04T18:24:20", "upload_time_iso_8601": "2019-01-04T18:24:20.016852Z", "url": "https://files.pythonhosted.org/packages/ad/e1/543445781df1b7b6a40332f522d19b857bd004a51717a4495062eb5bf7fe/tokenizer-1.1.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c97d9dbbbfd8c942229267ce7c98e768", "sha256": "845a47213fe4accdcb7b8bd4ff7652fb53b9475b7d9cfc5f07d54143ff512dfe"}, "downloads": -1, "filename": "tokenizer-1.1.1.tar.gz", "has_sig": false, "md5_digest": "c97d9dbbbfd8c942229267ce7c98e768", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 60925, "upload_time": "2019-01-04T18:24:22", "upload_time_iso_8601": "2019-01-04T18:24:22.044407Z", "url": "https://files.pythonhosted.org/packages/8e/3d/971d40b67ceeb8ae48b54e95dfa7251c021d5b39645aa393bcf28fd1b50f/tokenizer-1.1.1.tar.gz", "yanked": false}], "1.1.2": [{"comment_text": "", "digests": {"md5": "64bf5770c6aaa9156fb017c904e84b64", "sha256": "d7889a728852f95dfe93ecd1c6e07f4dec99d0767a4b062ef01b5ddc89e972e9"}, "downloads": -1, "filename": "tokenizer-1.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "64bf5770c6aaa9156fb017c904e84b64", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 62053, "upload_time": "2019-01-10T11:36:20", "upload_time_iso_8601": "2019-01-10T11:36:20.219682Z", "url": "https://files.pythonhosted.org/packages/e1/ad/b927b6d4a3af94416a5e5598ddbc73daa3e2ab7dc8570643254e619ad18d/tokenizer-1.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "231e865e2eb4d899f283e916a5e69de4", "sha256": "16014d50c6e80489bc36de7a76de10b5ce7eca6a286979cdb9bd015699ece319"}, "downloads": -1, "filename": "tokenizer-1.1.2.tar.gz", "has_sig": false, "md5_digest": "231e865e2eb4d899f283e916a5e69de4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 61049, "upload_time": "2019-01-10T11:36:22", "upload_time_iso_8601": "2019-01-10T11:36:22.330985Z", "url": "https://files.pythonhosted.org/packages/44/3e/0da2f03137478903dd588f20167d7ae56d45a2ab195ab3a67c61436a7b88/tokenizer-1.1.2.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "54298593197adaf685d8c50b83063747", "sha256": "6c794eb5e288d5b976772625a5d5b8b2aca83b3fea4b3a3e10c08c1304338cbe"}, "downloads": -1, "filename": "tokenizer-1.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "54298593197adaf685d8c50b83063747", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 63043, "upload_time": "2019-02-07T16:35:11", "upload_time_iso_8601": "2019-02-07T16:35:11.381587Z", "url": "https://files.pythonhosted.org/packages/11/76/50c693df95bcec3f28203d7837d5ce0e5a7e28c3f31fb379e743843fb292/tokenizer-1.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c2d3013d3ba32a204c49a2e92768b9ce", "sha256": "464d6a0a6c0adb3ccb2c616c7fe18bbe817fed25534f34a6b6895464c7d59afa"}, "downloads": -1, "filename": "tokenizer-1.2.0.tar.gz", "has_sig": false, "md5_digest": "c2d3013d3ba32a204c49a2e92768b9ce", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 62460, "upload_time": "2019-02-07T16:35:13", "upload_time_iso_8601": "2019-02-07T16:35:13.562615Z", "url": "https://files.pythonhosted.org/packages/39/29/ae65b522395357a91d92d6d857fd899eb5e4171dd2f2f0fe1d32bef979b2/tokenizer-1.2.0.tar.gz", "yanked": false}], "1.2.1": [{"comment_text": "", "digests": {"md5": "aacb37c62ea6b13051c5f44edee14c0c", "sha256": "cdc859ba3c732dd9f076fc0ad3ea970d4092fe362b802586e4dd47d04f79ac48"}, "downloads": -1, "filename": "tokenizer-1.2.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "aacb37c62ea6b13051c5f44edee14c0c", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 63783, "upload_time": "2019-02-18T19:17:49", "upload_time_iso_8601": "2019-02-18T19:17:49.756457Z", "url": "https://files.pythonhosted.org/packages/1b/46/dee3e5628effb7efc13fcbad082097fd29c6c6c1960b3d757d529d90720b/tokenizer-1.2.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "98c01956cdf35315835b59d07cbdc186", "sha256": "e361c713641c32f91659e8e4771dfa974c0b5f170f08145074142bfc1fbdab63"}, "downloads": -1, "filename": "tokenizer-1.2.1.tar.gz", "has_sig": false, "md5_digest": "98c01956cdf35315835b59d07cbdc186", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 63605, "upload_time": "2019-02-18T19:17:51", "upload_time_iso_8601": "2019-02-18T19:17:51.567305Z", "url": "https://files.pythonhosted.org/packages/7a/18/36770f34671bfc8ba4c3c3829f51485eea502b1c4b3c0cb13972f780f7ea/tokenizer-1.2.1.tar.gz", "yanked": false}], "1.2.2": [{"comment_text": "", "digests": {"md5": "d8b1e652565dd3ca5fa5e8e3bbe8353f", "sha256": "e6f0deb2f5322b71b82326eaf9d52249b8acca6d0fd7d8087197f3e2d32622b0"}, "downloads": -1, "filename": "tokenizer-1.2.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "d8b1e652565dd3ca5fa5e8e3bbe8353f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 64478, "upload_time": "2019-04-26T13:15:51", "upload_time_iso_8601": "2019-04-26T13:15:51.798885Z", "url": "https://files.pythonhosted.org/packages/1c/aa/be419418313c47c4b6eaa6666ebbb164c896e33a70e74faa45f0e473a619/tokenizer-1.2.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ce22c0d23ffc9ce14505e4e49a4b2637", "sha256": "d0f4550ddcabf617e904f3d429195df4a623912dfbf97e7780e0d0088703f5a8"}, "downloads": -1, "filename": "tokenizer-1.2.2.tar.gz", "has_sig": false, "md5_digest": "ce22c0d23ffc9ce14505e4e49a4b2637", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59840, "upload_time": "2019-04-26T13:15:55", "upload_time_iso_8601": "2019-04-26T13:15:55.374881Z", "url": "https://files.pythonhosted.org/packages/bd/7e/ecb493d085189baa8ab0987d74ed882fb72ff8e8351cf36eaca9540c751d/tokenizer-1.2.2.tar.gz", "yanked": false}], "1.2.3": [{"comment_text": "", "digests": {"md5": "c03c445508dddcb97470028dff8b27ec", "sha256": "5132f0ccee0a613e9a48106a816f1abdafd90e86656bac3f2e4277ab3eeb301a"}, "downloads": -1, "filename": "tokenizer-1.2.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c03c445508dddcb97470028dff8b27ec", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 72466, "upload_time": "2019-05-03T11:42:01", "upload_time_iso_8601": "2019-05-03T11:42:01.007615Z", "url": "https://files.pythonhosted.org/packages/f3/b1/eb1715a312b949ac1001e23d387f39833e4683f136239e193ae4fa2f73ec/tokenizer-1.2.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "845f4770a1a7e5b3985918d6cdec5c67", "sha256": "44195dbc1c312773bcc2476cfdb32ff5ea2ea57244edd58d2857034cd3cc37e1"}, "downloads": -1, "filename": "tokenizer-1.2.3.tar.gz", "has_sig": false, "md5_digest": "845f4770a1a7e5b3985918d6cdec5c67", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59921, "upload_time": "2019-05-03T11:42:02", "upload_time_iso_8601": "2019-05-03T11:42:02.898999Z", "url": "https://files.pythonhosted.org/packages/f5/7e/a157345a652781fcfdb4dfc6e06eb9b8514499968c6fef8779f28c35cee2/tokenizer-1.2.3.tar.gz", "yanked": false}], "1.3.0": [{"comment_text": "", "digests": {"md5": "c7d85265e66e07b940621b083fb5392f", "sha256": "65e20f53974b936018a4a9f55af8b5bf45927f84e22f723d6cb8b95c8120d961"}, "downloads": -1, "filename": "tokenizer-1.3.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c7d85265e66e07b940621b083fb5392f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 75955, "upload_time": "2019-05-21T11:19:42", "upload_time_iso_8601": "2019-05-21T11:19:42.335699Z", "url": "https://files.pythonhosted.org/packages/08/ad/bdf7cac9415855ce61249be3549c8d32f781c3d3b54ad754d42078f152ce/tokenizer-1.3.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ce22cb92db7eb619ac7c3a16978fc3ee", "sha256": "b6b68b3873dbb946d97331b2d43c0198530a4b4ca4d42e18e2bb9122ffefed02"}, "downloads": -1, "filename": "tokenizer-1.3.0.tar.gz", "has_sig": false, "md5_digest": "ce22cb92db7eb619ac7c3a16978fc3ee", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 63563, "upload_time": "2019-05-21T11:19:44", "upload_time_iso_8601": "2019-05-21T11:19:44.682776Z", "url": "https://files.pythonhosted.org/packages/e1/8c/039292bb0855fb7543e20ac028dca0cc593a819a26128d8cb4c8106af22e/tokenizer-1.3.0.tar.gz", "yanked": false}], "1.4.0": [{"comment_text": "", "digests": {"md5": "7a0b61472bb1f11913eafca4eedbf9b1", "sha256": "53545f701e257e409014f995976d06f2183432fceb1c9a80bc18b708510ef8a0"}, "downloads": -1, "filename": "tokenizer-1.4.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "7a0b61472bb1f11913eafca4eedbf9b1", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 78231, "upload_time": "2019-07-16T17:05:41", "upload_time_iso_8601": "2019-07-16T17:05:41.673451Z", "url": "https://files.pythonhosted.org/packages/4d/af/46342e59df9dbb195f24554b26e082e40a8bc2689a33d72a011c1a3b78df/tokenizer-1.4.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "09145bed4265e30b02340ea97acc8772", "sha256": "9da08dce06565a5fca3f3ab472cd6608a98830c1d5165dd53ec13a340f555b12"}, "downloads": -1, "filename": "tokenizer-1.4.0.tar.gz", "has_sig": false, "md5_digest": "09145bed4265e30b02340ea97acc8772", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 67144, "upload_time": "2019-07-16T17:05:43", "upload_time_iso_8601": "2019-07-16T17:05:43.549996Z", "url": "https://files.pythonhosted.org/packages/cf/00/20a5bb856e781ba5305e46ba134b1597b52caea4ca5dc5fe62210a8ad987/tokenizer-1.4.0.tar.gz", "yanked": false}], "1.4.1": [{"comment_text": "", "digests": {"md5": "0571cb711d00f3c575f4c9b3923bc6c3", "sha256": "1ec6325327d45a82841356e343b434c89df96484abe9c6ba2109f85ec65cba7f"}, "downloads": -1, "filename": "tokenizer-1.4.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "0571cb711d00f3c575f4c9b3923bc6c3", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 80019, "upload_time": "2019-10-22T17:18:46", "upload_time_iso_8601": "2019-10-22T17:18:46.923408Z", "url": "https://files.pythonhosted.org/packages/1a/ef/2e9a3f764907a46e965d0c311eab8312290dfe723ae10ed2d776583174e0/tokenizer-1.4.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "49a70198793bc9c1d75bf58c6f2e7d0b", "sha256": "75e9ebc0f6dadad3cb82ba6076a418fcea0b8370d879b5f6b9e3317c8484ebe8"}, "downloads": -1, "filename": "tokenizer-1.4.1.tar.gz", "has_sig": false, "md5_digest": "49a70198793bc9c1d75bf58c6f2e7d0b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 68905, "upload_time": "2019-10-22T17:18:48", "upload_time_iso_8601": "2019-10-22T17:18:48.903658Z", "url": "https://files.pythonhosted.org/packages/4a/3f/ac6ae2595e2ddba612d5815bc8c86527cd8d865719f7b9ff3a2db8e43f7c/tokenizer-1.4.1.tar.gz", "yanked": false}], "2.0.0": [{"comment_text": "", "digests": {"md5": "af3f7c81bffa6621dffa63b7b2a7f9a1", "sha256": "6728d0eccc1f5386817bf28859d70e16512f2b9fb8053b2159e5f54c5b071214"}, "downloads": -1, "filename": "tokenizer-2.0.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "af3f7c81bffa6621dffa63b7b2a7f9a1", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 106484, "upload_time": "2019-12-04T16:05:24", "upload_time_iso_8601": "2019-12-04T16:05:24.068148Z", "url": "https://files.pythonhosted.org/packages/5e/0e/1e253c56542934b053c6021d7e5464206c8c2445fbcd72e76b4168b9499d/tokenizer-2.0.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7af130f4f4bc4b107c7f4d4f6bb99ae0", "sha256": "ef4005840a573ebb4787dbaedc0bb6b4aecf6b10fa359bb2d1d12d41b34b5c15"}, "downloads": -1, "filename": "tokenizer-2.0.0.tar.gz", "has_sig": false, "md5_digest": "7af130f4f4bc4b107c7f4d4f6bb99ae0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 112974, "upload_time": "2019-12-04T16:05:26", "upload_time_iso_8601": "2019-12-04T16:05:26.390468Z", "url": "https://files.pythonhosted.org/packages/7d/48/597b086edab63dfc38a34c0853244f740587c7cee8edadf3d160691aa2e1/tokenizer-2.0.0.tar.gz", "yanked": false}], "2.0.1": [{"comment_text": "", "digests": {"md5": "41e297b3304195a76b6c29e89def559b", "sha256": "790b938d2b509e832ae4d89a0bfb9f45548d05c9e0de6e48b60a581d0294ff7e"}, "downloads": -1, "filename": "tokenizer-2.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "41e297b3304195a76b6c29e89def559b", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 107344, "upload_time": "2019-12-09T15:33:20", "upload_time_iso_8601": "2019-12-09T15:33:20.518929Z", "url": "https://files.pythonhosted.org/packages/6f/4e/613393a8ba3154f8b58cef5af1585dab33cc08430fdd0cc72896c131b79a/tokenizer-2.0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "da5e6685af94114cdfc5d45a60d272d8", "sha256": "e0773953ca9a47bdedb10863539e41ba623519e0341a9a89541fedc91ce85963"}, "downloads": -1, "filename": "tokenizer-2.0.1.tar.gz", "has_sig": false, "md5_digest": "da5e6685af94114cdfc5d45a60d272d8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 114828, "upload_time": "2019-12-09T15:33:23", "upload_time_iso_8601": "2019-12-09T15:33:23.180765Z", "url": "https://files.pythonhosted.org/packages/08/36/3e75dd86ec5beb11c8f9e07b4e446f7c4372ba9edb123d7c73ec8d8bfc46/tokenizer-2.0.1.tar.gz", "yanked": false}], "2.0.2": [{"comment_text": "", "digests": {"md5": "03da0d25ed265200c8133ed4f3c91fa9", "sha256": "1a169066ca9f34e89fa646089f34656e69d983186b0af2fafd9a421f68174808"}, "downloads": -1, "filename": "tokenizer-2.0.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "03da0d25ed265200c8133ed4f3c91fa9", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 107466, "upload_time": "2019-12-11T11:34:19", "upload_time_iso_8601": "2019-12-11T11:34:19.938985Z", "url": "https://files.pythonhosted.org/packages/97/bd/83b997aa8f360b813ddab004adff36f711812f0a498f957871b0c281bb84/tokenizer-2.0.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "963ee4dfa402cece0c595fb1ee9467de", "sha256": "0d53b9035af9b217f4cc9c31beb52c15e75fe6bfbef250125e588c05645d7731"}, "downloads": -1, "filename": "tokenizer-2.0.2.tar.gz", "has_sig": false, "md5_digest": "963ee4dfa402cece0c595fb1ee9467de", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 115211, "upload_time": "2019-12-11T11:34:22", "upload_time_iso_8601": "2019-12-11T11:34:22.689694Z", "url": "https://files.pythonhosted.org/packages/c0/0c/0733cd853170fb3fd6cccd180ef54cde6c4c2e9667e8e733605bc2e7d386/tokenizer-2.0.2.tar.gz", "yanked": false}], "2.0.3": [{"comment_text": "", "digests": {"md5": "eaffcb4f0d1449e2e56bccbd3ab2938c", "sha256": "49693ea39fd4bcd74bff4087d62c7020e75b6e30950b6c02a597438e6c3ce908"}, "downloads": -1, "filename": "tokenizer-2.0.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "eaffcb4f0d1449e2e56bccbd3ab2938c", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 107557, "upload_time": "2019-12-17T15:54:23", "upload_time_iso_8601": "2019-12-17T15:54:23.714034Z", "url": "https://files.pythonhosted.org/packages/65/48/49fd393348a4dcb796e901732ebab32d7a37e826fa35b72702eeea2e2f77/tokenizer-2.0.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e7ee17809fda7563a56ad89ed707a926", "sha256": "ff9ff525c32cbec46e254bf343ed1f5ba0e39436bbca33a5912277b0c463e0b1"}, "downloads": -1, "filename": "tokenizer-2.0.3.tar.gz", "has_sig": false, "md5_digest": "e7ee17809fda7563a56ad89ed707a926", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 115677, "upload_time": "2019-12-17T15:54:26", "upload_time_iso_8601": "2019-12-17T15:54:26.383162Z", "url": "https://files.pythonhosted.org/packages/f0/3a/8f391d16cf47bc3847d5212c88dfa7ad3e27d071d6ae8e088a307c1c62db/tokenizer-2.0.3.tar.gz", "yanked": false}], "2.0.4": [{"comment_text": "", "digests": {"md5": "9a5a2d7b06b46a41b100922abbf296b0", "sha256": "3085d93464967b6d79b4eaa5e06707cdbcef0543951cc0f2e444eab9d6d539f8"}, "downloads": -1, "filename": "tokenizer-2.0.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "9a5a2d7b06b46a41b100922abbf296b0", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 104908, "upload_time": "2020-02-17T16:17:02", "upload_time_iso_8601": "2020-02-17T16:17:02.906031Z", "url": "https://files.pythonhosted.org/packages/b2/ec/d049556036ba76c3002d08410c3864b1b6daa175562156a6d0431dc7e0cd/tokenizer-2.0.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8c7ee8954a2d87cc42d6e17161aa094e", "sha256": "5a99d8ca4673786b0ca4e4802e7ee2937da5a091932eb6c45894fa893da7779e"}, "downloads": -1, "filename": "tokenizer-2.0.4.tar.gz", "has_sig": false, "md5_digest": "8c7ee8954a2d87cc42d6e17161aa094e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 116228, "upload_time": "2020-02-17T16:17:05", "upload_time_iso_8601": "2020-02-17T16:17:05.382783Z", "url": "https://files.pythonhosted.org/packages/36/a0/0efac5c15129829815016056a21db306b04fd74ec44a22f653d619ff663f/tokenizer-2.0.4.tar.gz", "yanked": false}], "2.0.5": [{"comment_text": "", "digests": {"md5": "aaa66bf866477127fe0705bbe11e5f73", "sha256": "7c1ac32ef74e1da0be0a1b864b78cf5e13b002f494139faec21562b49a9997a4"}, "downloads": -1, "filename": "tokenizer-2.0.5-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "aaa66bf866477127fe0705bbe11e5f73", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 105378, "upload_time": "2020-03-26T23:28:27", "upload_time_iso_8601": "2020-03-26T23:28:27.872018Z", "url": "https://files.pythonhosted.org/packages/a5/22/440325f97a442a2d5e2d01703dbedc80deb168f8f8f5a5480fc2e7a8979e/tokenizer-2.0.5-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0dc2f1137db2c347dbcf1135498a21c7", "sha256": "75961c560e6f8baed2128598220cc0177da5d0733d6a33c26922e5e4e59582e4"}, "downloads": -1, "filename": "tokenizer-2.0.5.tar.gz", "has_sig": false, "md5_digest": "0dc2f1137db2c347dbcf1135498a21c7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 116863, "upload_time": "2020-03-26T23:28:30", "upload_time_iso_8601": "2020-03-26T23:28:30.374174Z", "url": "https://files.pythonhosted.org/packages/4a/bb/82268b7bf0dee4a27a0db53b62d8c60af295ebb37396b2f4910079a2df7c/tokenizer-2.0.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "aaa66bf866477127fe0705bbe11e5f73", "sha256": "7c1ac32ef74e1da0be0a1b864b78cf5e13b002f494139faec21562b49a9997a4"}, "downloads": -1, "filename": "tokenizer-2.0.5-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "aaa66bf866477127fe0705bbe11e5f73", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 105378, "upload_time": "2020-03-26T23:28:27", "upload_time_iso_8601": "2020-03-26T23:28:27.872018Z", "url": "https://files.pythonhosted.org/packages/a5/22/440325f97a442a2d5e2d01703dbedc80deb168f8f8f5a5480fc2e7a8979e/tokenizer-2.0.5-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0dc2f1137db2c347dbcf1135498a21c7", "sha256": "75961c560e6f8baed2128598220cc0177da5d0733d6a33c26922e5e4e59582e4"}, "downloads": -1, "filename": "tokenizer-2.0.5.tar.gz", "has_sig": false, "md5_digest": "0dc2f1137db2c347dbcf1135498a21c7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 116863, "upload_time": "2020-03-26T23:28:30", "upload_time_iso_8601": "2020-03-26T23:28:30.374174Z", "url": "https://files.pythonhosted.org/packages/4a/bb/82268b7bf0dee4a27a0db53b62d8c60af295ebb37396b2f4910079a2df7c/tokenizer-2.0.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:51:27 2020"}