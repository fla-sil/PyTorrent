{"info": {"author": "Microsoft Corporation", "author_email": "swiftkey-deep@service.microsoft.com", "bugtrack_url": null, "classifiers": [], "description": "# Language Model Challenge (LM Challenge)\n\n_A library & tools to evaluate predictive language models._ This is a guide for users of LM Challenge; you may also want to see:\n\n - [data formats](doc/formats.md) for integrators\n - [dev notes](doc/dev.md) for developers wishing to extend LM Challenge\n\n\n## What is LM Challenge for?\n\nIt is hard to compare language model performance in general. Some models output probabilities, others scores; some model words, others morphemes, characters or bytes. Vocabulary coverage varies. Comparing them in a fair way is therefore difficult. So in LM Challenge we have some very simple 'challenge games' that evaluate (and help compare) language models over a test corpus.\n\nLM Challenge is for researchers and engineers who wish to set a standard for fair comparison of very different language model architectures. It requires a little work to wrap your model in a standard API, but we believe this is often better than writing & testing evaluation tools afresh for each project/investigation.\n\nNote: most of LM Challenge tools are word-based (although all can be applied to sub-word \"character compositional\" word models). Additionally, our assumption is that the language model is \"forward contextual\" - so it predicts a word or character based only on preceding words/characters.\n\n\n## Getting Started\n\nInstall LM Challenge from the published Python package:\n\n    pip3 install --user lmchallenge\n\n(Or from this repository `python3 setup.py install --user`.)\n\n**Setup:** LM Challenge needs a model to evaluate. We include an example ngram model implementation in `sample`. Download data & build models (this may take a couple of minutes):\n\n    cd sample/\n    ./prepare.sh\n\n**Model REPL:** Now you can use the example script to evaluate a very basic ngram model (see [ngram.py](sample/ngram.py), which you may find useful if integrating your own prediction model). _Note that this command will not terminate, as it launches an interactive program:_\n\n    python3 ngram.py words data/words.3gram\n\nThis starts an interactive program which can accept commands of a single word followed by a hard `TAB` character and any arguments, for example:\n\n    > predict<TAB>\n    =    0.0000    The    -1.0000    In    -2.0000...\n\nThis produces start-of-line predictions, each with an attached score. To query with word context, try the following (making sure you leave a trailing space at the end of the query, after \"favourite\"):\n\n    > predict<TAB>My favourite \n    of    0.0000    song    -1.0000    the    -2.0000...\n\nThis provides next-word-prediction based on a context. There is more to the API (see [formats](doc/formats.md) for more details), but since you won't usually be using the API directly, let's move on to running LM Challenge over this model (so exit the predictor using `Ctrl+D`, back to your shell).\n\n**Evaluation:** To run LM Challenge for this model, we'll pipe some text into `lmc run`, and save the result:\n\n    mkdir out\n    head -n 10 data/wiki.test.tokens | lmc run \"python3 ngram.py words data/words.3gram\" wc > out/w3.wc.log\n\nThe resulting log contains all of the original text, and can be queried using the `lmc` utilities. Note: `jq` here is optional, but a very convenient program for working with JSON.\n\n    lmc stats out/w3.wc.log | jq .\n\nYou should see some statistics about the model - in particular `completion` & `prediction`. Now let's try comparing with a less powerful model:\n\n    head -n 10 data/wiki.test.tokens | lmc run \"python3 ngram.py words data/words.2gram\" wc > out/w2.wc.log\n    lmc stats out/*.wc.log | jq .\n\nThe aggregated level prediction and completion stats should be slightly different for the two models. But we can get a better picture from inspecting the logs in detail:\n\n    lmc pretty out/w3.wc.log\n\nThis shows a pretty-printed dump of the data, according to how well the model performed on each token. We can also pretty-print the difference between two models:\n\n    lmc diff out/w3.wc.log out/w2.wc.log\n\nFilter the log for only capitalized words, and print summary statistics:\n\n    lmc grep \"^[A-Z][a-z]+$\" out/w3.wc.log | lmc stats | jq .\n\nYou should notice that capitalized words are (in this small, statistically insignificant example), much harder to predict than words in general.\n\n**Other challenges:** Other LM challenges can be run & inspected in a similar way, see `lmc run --help`.\n\n\n## Running LM Challenge\n\nLM Challenge is quite flexible - it can be used in a variety of ways:\n\n 1. Command Line Interface\n 2. Python API\n 3. Log file format\n\n### 1. Command Line Interface\n\nThis is the simplest way of using LM Challenge, and works if your model is implemented in any language supporting piped stdout/stdin. See the [Getting Started](#getting-started) guide above, and the CLI help:\n\n    lmc --help\n    lmc run --help\n\n### 2. Python API\n\nIf your model runs in Python 3, and you wish to script evaluation in Python, you can use the API directly:\n\n    import lmchallenge as lmc\n    help(lmc)\n\nOur documentation (as in `help(lmc)`) includes a tutorial for getting started with Python. We don't yet publish the HTML, but it has been tested with `pdoc`:\n\n    $ pdoc --http\n    # use your browser to view generated documentation\n\n### 3. Log file format\n\nIf you require batching or distribution for sufficient evaluation speed, you can write the LM Challenge log files yourself. This means you can use LM Challenge to process & analyse the data, without imposing a particular execution model. To do this:\n\n 1. Write JSONlines files that contain lmchallenge log data:\n    - See [data formats](doc/formats.md) notes that describe the log format.\n    - (Optionally) use the [JSON schema](lmchallenge/log.schema) that formally describes an acceptable log datum.\n    - (Optionally) use the CLI `lmc validate` (or Python API `lmchallenge.validate.validate`) to check that your log conforms to the schema.\n    - Note that log files can often be concatenated if they were generated in parallel.\n 2. Use the lmchallenge tools to analyse the logs (everything except `lmc run`).\n\n\n## The details\n\nAn _LM challenge game_ is a runnable Python module that evaluates one or more _language models_ on some task, over some _test text_.\n\nThe **challenge games** we have are:\n\n - `wc` - Word Completion Challenge - a Next Word Prediction / Completion task (generates Hit@N & completion ratios)\n - `we|ce` - Word|Character Entropy Challenges - a language probability distribution task (generates cross entropy given a defined vocabulary)\n - `wr` - Word Reranking Challenge - a correction task (generates accuracy)\n\n**Test text** is pure text data (as typed & understood by real actual humans!) LM Challenge does not define test text - we expect it to be provided. This is the other thing you need to decide on in order to evaluate a _language model_.\n\nA **language model** is an executable process that responds to commands from a _LM challenge game_ in a specific text format, usually comprising of a pre-trained model of the same language as the _test text_.\n\n### Word Completion `wc`\n\nThe Word Completion task scans through words in the test text, at each point querying the language model for next-word predictions & word completions.\n\n    cat DATA | lmc run \"PREDICTOR\" wc > LOG\n\nThe model should aim to predict the correct next word before other words (i.e. with as low a rank as possible), or failing that to predict it in the top two completions, given as short a typed prefix as possible. Statistics available from `wc` include:\n\n - next-word-prediction\n   - `Hit@N` - ratio of correct predictions obtained with rank below `N`\n   - `MRR` (Mean Reciprocal Rank) - the sum total of `1/rank` over all words\n - completion\n   - `characters` - ratio of characters that were completed (e.g. if typing `\"hello\"`, and it is predicted after you type `\"he\"`, the ratio of completed characters would be `0.5`)\n   - `tokens` - ratio of tokens that were completed before they were fully typed\n\nNote that the flag `--next-word-only` may be used to speed up evaluation, by skipping all prefixes, only evaluating the model's next-word-prediction performance (so that completion stats are not generated).\n\n### Word/Character Entropy `we|ce`\n\nThe Word/Character Entropy task produces stats that are analogous to the standard cross-entropy/perplexity measures used for evaluating language models. These evaluators scan through text, at each point querying the language model for a normalized log-probability for the current word.\n\n    cat DATA | lmc run \"PREDICTOR\" we > LOG\n    cat DATA | lmc run \"PREDICTOR\" ce > LOG\n\nIt is important to note that the entropy metric can only be compared between models that share a common vocabulary. If the vocabulary is different, the entropy task is different, and models should not be compared. Therefore, a model must generate a \"fair\" normalized log-probability over its vocabulary (and if a word is not in the vocabulary, to omit the score from the results). It should not merge \"equivalence classes\" of words (except by general agreement with every other model being evaluated). An example of this would be example normalizing capitalization to give \"fish\" the same score as \"Fish\", or giving many words an \"out of vocabulary\" score (such that, if you were to calculate `p(\"fish\") + p(\"Fish\") + p(everything else)` it would not sum to one). Simply ommiting any words that cannot be scored (e.g. OOV words) is safe, as this contributes to a special \"entropy fingerprint\", which checks that two models successfully scored the same set of words, and are therefore comparable under the entropy metric.\n\n### Word Reranking `wr`\n\nThe Word Reranking task emulates a sloppy typist entering text, using the language model to correct input after it has been typed. This challenge requires a list of words to use as correction candidates for corrupted words (which should be a large set of valid words in the target language.) Text from the data source is first corrupted (as if by a sloppy typist). The corrupted text is fed into a search for nearby candidate words, which are scored according to the language model under evaluation. The evaluator measures corrected, un-corrected and mis-corrected results.\n\n    cat DATA | lmc run \"PREDICTOR\" wr VOCAB > LOG\n\nThe aim of the model is to assign high score to the correct word, and low score to all other words. We evaluate this by mixing the score from the language model with an _input score_ for each word (using a minimum score for words that are not scored by the lanugage model), then ranking based on that. If the top-ranked prediction is the correct word, this example was a success, otherwise it counts as a failure. The _input score_ is the log-probability of the particular corrupted text being produced from this word, in the same error model that was used to corrupt the true word. In order to be robust against different ranges of scores from language models, we optimize the _input_ and _language model_ mixing parameters before counting statistics (this is done automatically, but requires the optional dependency `scipy`). The accuracy aggregate measures the maximum proportion of correct top predictions, using the optimum mixing proportions.\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Microsoft/LMChallenge", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "lmchallenge", "package_url": "https://pypi.org/project/lmchallenge/", "platform": "", "project_url": "https://pypi.org/project/lmchallenge/", "project_urls": {"Homepage": "https://github.com/Microsoft/LMChallenge"}, "release_url": "https://pypi.org/project/lmchallenge/5.2/", "requires_dist": null, "requires_python": "", "summary": "LM Challenge - A library & tools to evaluate predictive language models.", "version": "5.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Language Model Challenge (LM Challenge)</h1>\n<p><em>A library &amp; tools to evaluate predictive language models.</em> This is a guide for users of LM Challenge; you may also want to see:</p>\n<ul>\n<li><a href=\"doc/formats.md\" rel=\"nofollow\">data formats</a> for integrators</li>\n<li><a href=\"doc/dev.md\" rel=\"nofollow\">dev notes</a> for developers wishing to extend LM Challenge</li>\n</ul>\n<h2>What is LM Challenge for?</h2>\n<p>It is hard to compare language model performance in general. Some models output probabilities, others scores; some model words, others morphemes, characters or bytes. Vocabulary coverage varies. Comparing them in a fair way is therefore difficult. So in LM Challenge we have some very simple 'challenge games' that evaluate (and help compare) language models over a test corpus.</p>\n<p>LM Challenge is for researchers and engineers who wish to set a standard for fair comparison of very different language model architectures. It requires a little work to wrap your model in a standard API, but we believe this is often better than writing &amp; testing evaluation tools afresh for each project/investigation.</p>\n<p>Note: most of LM Challenge tools are word-based (although all can be applied to sub-word \"character compositional\" word models). Additionally, our assumption is that the language model is \"forward contextual\" - so it predicts a word or character based only on preceding words/characters.</p>\n<h2>Getting Started</h2>\n<p>Install LM Challenge from the published Python package:</p>\n<pre><code>pip3 install --user lmchallenge\n</code></pre>\n<p>(Or from this repository <code>python3 setup.py install --user</code>.)</p>\n<p><strong>Setup:</strong> LM Challenge needs a model to evaluate. We include an example ngram model implementation in <code>sample</code>. Download data &amp; build models (this may take a couple of minutes):</p>\n<pre><code>cd sample/\n./prepare.sh\n</code></pre>\n<p><strong>Model REPL:</strong> Now you can use the example script to evaluate a very basic ngram model (see <a href=\"sample/ngram.py\" rel=\"nofollow\">ngram.py</a>, which you may find useful if integrating your own prediction model). <em>Note that this command will not terminate, as it launches an interactive program:</em></p>\n<pre><code>python3 ngram.py words data/words.3gram\n</code></pre>\n<p>This starts an interactive program which can accept commands of a single word followed by a hard <code>TAB</code> character and any arguments, for example:</p>\n<pre><code>&gt; predict&lt;TAB&gt;\n=    0.0000    The    -1.0000    In    -2.0000...\n</code></pre>\n<p>This produces start-of-line predictions, each with an attached score. To query with word context, try the following (making sure you leave a trailing space at the end of the query, after \"favourite\"):</p>\n<pre><code>&gt; predict&lt;TAB&gt;My favourite \nof    0.0000    song    -1.0000    the    -2.0000...\n</code></pre>\n<p>This provides next-word-prediction based on a context. There is more to the API (see <a href=\"doc/formats.md\" rel=\"nofollow\">formats</a> for more details), but since you won't usually be using the API directly, let's move on to running LM Challenge over this model (so exit the predictor using <code>Ctrl+D</code>, back to your shell).</p>\n<p><strong>Evaluation:</strong> To run LM Challenge for this model, we'll pipe some text into <code>lmc run</code>, and save the result:</p>\n<pre><code>mkdir out\nhead -n 10 data/wiki.test.tokens | lmc run \"python3 ngram.py words data/words.3gram\" wc &gt; out/w3.wc.log\n</code></pre>\n<p>The resulting log contains all of the original text, and can be queried using the <code>lmc</code> utilities. Note: <code>jq</code> here is optional, but a very convenient program for working with JSON.</p>\n<pre><code>lmc stats out/w3.wc.log | jq .\n</code></pre>\n<p>You should see some statistics about the model - in particular <code>completion</code> &amp; <code>prediction</code>. Now let's try comparing with a less powerful model:</p>\n<pre><code>head -n 10 data/wiki.test.tokens | lmc run \"python3 ngram.py words data/words.2gram\" wc &gt; out/w2.wc.log\nlmc stats out/*.wc.log | jq .\n</code></pre>\n<p>The aggregated level prediction and completion stats should be slightly different for the two models. But we can get a better picture from inspecting the logs in detail:</p>\n<pre><code>lmc pretty out/w3.wc.log\n</code></pre>\n<p>This shows a pretty-printed dump of the data, according to how well the model performed on each token. We can also pretty-print the difference between two models:</p>\n<pre><code>lmc diff out/w3.wc.log out/w2.wc.log\n</code></pre>\n<p>Filter the log for only capitalized words, and print summary statistics:</p>\n<pre><code>lmc grep \"^[A-Z][a-z]+$\" out/w3.wc.log | lmc stats | jq .\n</code></pre>\n<p>You should notice that capitalized words are (in this small, statistically insignificant example), much harder to predict than words in general.</p>\n<p><strong>Other challenges:</strong> Other LM challenges can be run &amp; inspected in a similar way, see <code>lmc run --help</code>.</p>\n<h2>Running LM Challenge</h2>\n<p>LM Challenge is quite flexible - it can be used in a variety of ways:</p>\n<ol>\n<li>Command Line Interface</li>\n<li>Python API</li>\n<li>Log file format</li>\n</ol>\n<h3>1. Command Line Interface</h3>\n<p>This is the simplest way of using LM Challenge, and works if your model is implemented in any language supporting piped stdout/stdin. See the <a href=\"#getting-started\" rel=\"nofollow\">Getting Started</a> guide above, and the CLI help:</p>\n<pre><code>lmc --help\nlmc run --help\n</code></pre>\n<h3>2. Python API</h3>\n<p>If your model runs in Python 3, and you wish to script evaluation in Python, you can use the API directly:</p>\n<pre><code>import lmchallenge as lmc\nhelp(lmc)\n</code></pre>\n<p>Our documentation (as in <code>help(lmc)</code>) includes a tutorial for getting started with Python. We don't yet publish the HTML, but it has been tested with <code>pdoc</code>:</p>\n<pre><code>$ pdoc --http\n# use your browser to view generated documentation\n</code></pre>\n<h3>3. Log file format</h3>\n<p>If you require batching or distribution for sufficient evaluation speed, you can write the LM Challenge log files yourself. This means you can use LM Challenge to process &amp; analyse the data, without imposing a particular execution model. To do this:</p>\n<ol>\n<li>Write JSONlines files that contain lmchallenge log data:\n<ul>\n<li>See <a href=\"doc/formats.md\" rel=\"nofollow\">data formats</a> notes that describe the log format.</li>\n<li>(Optionally) use the <a href=\"lmchallenge/log.schema\" rel=\"nofollow\">JSON schema</a> that formally describes an acceptable log datum.</li>\n<li>(Optionally) use the CLI <code>lmc validate</code> (or Python API <code>lmchallenge.validate.validate</code>) to check that your log conforms to the schema.</li>\n<li>Note that log files can often be concatenated if they were generated in parallel.</li>\n</ul>\n</li>\n<li>Use the lmchallenge tools to analyse the logs (everything except <code>lmc run</code>).</li>\n</ol>\n<h2>The details</h2>\n<p>An <em>LM challenge game</em> is a runnable Python module that evaluates one or more <em>language models</em> on some task, over some <em>test text</em>.</p>\n<p>The <strong>challenge games</strong> we have are:</p>\n<ul>\n<li><code>wc</code> - Word Completion Challenge - a Next Word Prediction / Completion task (generates Hit@N &amp; completion ratios)</li>\n<li><code>we|ce</code> - Word|Character Entropy Challenges - a language probability distribution task (generates cross entropy given a defined vocabulary)</li>\n<li><code>wr</code> - Word Reranking Challenge - a correction task (generates accuracy)</li>\n</ul>\n<p><strong>Test text</strong> is pure text data (as typed &amp; understood by real actual humans!) LM Challenge does not define test text - we expect it to be provided. This is the other thing you need to decide on in order to evaluate a <em>language model</em>.</p>\n<p>A <strong>language model</strong> is an executable process that responds to commands from a <em>LM challenge game</em> in a specific text format, usually comprising of a pre-trained model of the same language as the <em>test text</em>.</p>\n<h3>Word Completion <code>wc</code></h3>\n<p>The Word Completion task scans through words in the test text, at each point querying the language model for next-word predictions &amp; word completions.</p>\n<pre><code>cat DATA | lmc run \"PREDICTOR\" wc &gt; LOG\n</code></pre>\n<p>The model should aim to predict the correct next word before other words (i.e. with as low a rank as possible), or failing that to predict it in the top two completions, given as short a typed prefix as possible. Statistics available from <code>wc</code> include:</p>\n<ul>\n<li>next-word-prediction\n<ul>\n<li><code>Hit@N</code> - ratio of correct predictions obtained with rank below <code>N</code></li>\n<li><code>MRR</code> (Mean Reciprocal Rank) - the sum total of <code>1/rank</code> over all words</li>\n</ul>\n</li>\n<li>completion\n<ul>\n<li><code>characters</code> - ratio of characters that were completed (e.g. if typing <code>\"hello\"</code>, and it is predicted after you type <code>\"he\"</code>, the ratio of completed characters would be <code>0.5</code>)</li>\n<li><code>tokens</code> - ratio of tokens that were completed before they were fully typed</li>\n</ul>\n</li>\n</ul>\n<p>Note that the flag <code>--next-word-only</code> may be used to speed up evaluation, by skipping all prefixes, only evaluating the model's next-word-prediction performance (so that completion stats are not generated).</p>\n<h3>Word/Character Entropy <code>we|ce</code></h3>\n<p>The Word/Character Entropy task produces stats that are analogous to the standard cross-entropy/perplexity measures used for evaluating language models. These evaluators scan through text, at each point querying the language model for a normalized log-probability for the current word.</p>\n<pre><code>cat DATA | lmc run \"PREDICTOR\" we &gt; LOG\ncat DATA | lmc run \"PREDICTOR\" ce &gt; LOG\n</code></pre>\n<p>It is important to note that the entropy metric can only be compared between models that share a common vocabulary. If the vocabulary is different, the entropy task is different, and models should not be compared. Therefore, a model must generate a \"fair\" normalized log-probability over its vocabulary (and if a word is not in the vocabulary, to omit the score from the results). It should not merge \"equivalence classes\" of words (except by general agreement with every other model being evaluated). An example of this would be example normalizing capitalization to give \"fish\" the same score as \"Fish\", or giving many words an \"out of vocabulary\" score (such that, if you were to calculate <code>p(\"fish\") + p(\"Fish\") + p(everything else)</code> it would not sum to one). Simply ommiting any words that cannot be scored (e.g. OOV words) is safe, as this contributes to a special \"entropy fingerprint\", which checks that two models successfully scored the same set of words, and are therefore comparable under the entropy metric.</p>\n<h3>Word Reranking <code>wr</code></h3>\n<p>The Word Reranking task emulates a sloppy typist entering text, using the language model to correct input after it has been typed. This challenge requires a list of words to use as correction candidates for corrupted words (which should be a large set of valid words in the target language.) Text from the data source is first corrupted (as if by a sloppy typist). The corrupted text is fed into a search for nearby candidate words, which are scored according to the language model under evaluation. The evaluator measures corrected, un-corrected and mis-corrected results.</p>\n<pre><code>cat DATA | lmc run \"PREDICTOR\" wr VOCAB &gt; LOG\n</code></pre>\n<p>The aim of the model is to assign high score to the correct word, and low score to all other words. We evaluate this by mixing the score from the language model with an <em>input score</em> for each word (using a minimum score for words that are not scored by the lanugage model), then ranking based on that. If the top-ranked prediction is the correct word, this example was a success, otherwise it counts as a failure. The <em>input score</em> is the log-probability of the particular corrupted text being produced from this word, in the same error model that was used to corrupt the true word. In order to be robust against different ranges of scores from language models, we optimize the <em>input</em> and <em>language model</em> mixing parameters before counting statistics (this is done automatically, but requires the optional dependency <code>scipy</code>). The accuracy aggregate measures the maximum proportion of correct top predictions, using the optimum mixing proportions.</p>\n<h2>Contributing</h2>\n<p>This project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <a href=\"https://cla.microsoft.com\" rel=\"nofollow\">https://cla.microsoft.com</a>.</p>\n<p>When you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.</p>\n<p>This project has adopted the <a href=\"https://opensource.microsoft.com/codeofconduct/\" rel=\"nofollow\">Microsoft Open Source Code of Conduct</a>.\nFor more information see the <a href=\"https://opensource.microsoft.com/codeofconduct/faq/\" rel=\"nofollow\">Code of Conduct FAQ</a> or\ncontact <a href=\"mailto:opencode@microsoft.com\">opencode@microsoft.com</a> with any additional questions or comments.</p>\n\n          </div>"}, "last_serial": 6294719, "releases": {"4.4": [{"comment_text": "", "digests": {"md5": "868ca7e5e48862536660b13a716d88e7", "sha256": "2d68f80bca503112f4de75a3d34a8aae0e4f1c1f5a768b4ffbf72865a11cc4fb"}, "downloads": -1, "filename": "lmchallenge-4.4.tar.gz", "has_sig": true, "md5_digest": "868ca7e5e48862536660b13a716d88e7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 30293, "upload_time": "2017-10-06T13:29:40", "upload_time_iso_8601": "2017-10-06T13:29:40.999405Z", "url": "https://files.pythonhosted.org/packages/c6/b2/531568d931719adfa5ec2f7774260ed63e1a9eef89d421818d793de159c1/lmchallenge-4.4.tar.gz", "yanked": false}], "5.0": [{"comment_text": "", "digests": {"md5": "12c1e03306a1766c79fac6288abb6c60", "sha256": "f7bbe2ab7105e477d1b7aba795a66c86f147e558adb83fb8a4fbfd953ab19d15"}, "downloads": -1, "filename": "lmchallenge-5.0.tar.gz", "has_sig": true, "md5_digest": "12c1e03306a1766c79fac6288abb6c60", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 45152, "upload_time": "2017-12-20T16:21:28", "upload_time_iso_8601": "2017-12-20T16:21:28.440717Z", "url": "https://files.pythonhosted.org/packages/f4/00/5058a9fccb5497d7aedf0a0290bca16f30db662d6391990e20cd7a540d74/lmchallenge-5.0.tar.gz", "yanked": false}], "5.1": [{"comment_text": "", "digests": {"md5": "952b5982a1e1d57e853bea6fcc9ac167", "sha256": "ee25ca45cf397613ea950623f69be571cc5627dbec8ad3d2dab0c4bda13ad25e"}, "downloads": -1, "filename": "lmchallenge-5.1.tar.gz", "has_sig": true, "md5_digest": "952b5982a1e1d57e853bea6fcc9ac167", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 40486, "upload_time": "2018-08-03T15:29:25", "upload_time_iso_8601": "2018-08-03T15:29:25.211728Z", "url": "https://files.pythonhosted.org/packages/48/b8/9dd3e014d0394ffada93a3164b2645081a177469de3ab0f7effc142c3e5b/lmchallenge-5.1.tar.gz", "yanked": false}], "5.2": [{"comment_text": "", "digests": {"md5": "f6b22b38c55b868de7315d9a88b68784", "sha256": "581fad3e5c6208faa99a5965c90c185e7e6b18f93f5cb9237848787832702e04"}, "downloads": -1, "filename": "lmchallenge-5.2.tar.gz", "has_sig": true, "md5_digest": "f6b22b38c55b868de7315d9a88b68784", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 44818, "upload_time": "2019-12-13T18:48:39", "upload_time_iso_8601": "2019-12-13T18:48:39.892337Z", "url": "https://files.pythonhosted.org/packages/bc/af/2b1cac70d374e2e3a2af2d904d748da21b18add1a3c3f7799c81989647bd/lmchallenge-5.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f6b22b38c55b868de7315d9a88b68784", "sha256": "581fad3e5c6208faa99a5965c90c185e7e6b18f93f5cb9237848787832702e04"}, "downloads": -1, "filename": "lmchallenge-5.2.tar.gz", "has_sig": true, "md5_digest": "f6b22b38c55b868de7315d9a88b68784", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 44818, "upload_time": "2019-12-13T18:48:39", "upload_time_iso_8601": "2019-12-13T18:48:39.892337Z", "url": "https://files.pythonhosted.org/packages/bc/af/2b1cac70d374e2e3a2af2d904d748da21b18add1a3c3f7799c81989647bd/lmchallenge-5.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:44:50 2020"}