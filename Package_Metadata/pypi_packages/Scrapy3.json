{"info": {"author": "BobDu", "author_email": "i@bobdu.cc", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Console", "Framework :: Scrapy", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy", "Topic :: Internet :: WWW/HTTP", "Topic :: Software Development :: Libraries :: Application Frameworks", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "=======\nScrapy3\n=======\n\n.. image:: https://img.shields.io/pypi/v/Scrapy.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: PyPI Version\n\n.. image:: https://img.shields.io/pypi/pyversions/Scrapy.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: Supported Python Versions\n\n.. image:: https://img.shields.io/travis/scrapy/scrapy/master.svg\n   :target: https://travis-ci.org/scrapy/scrapy\n   :alt: Build Status\n\n.. image:: https://img.shields.io/badge/wheel-yes-brightgreen.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: Wheel Status\n\n.. image:: https://img.shields.io/codecov/c/github/scrapy/scrapy/master.svg\n   :target: https://codecov.io/github/scrapy/scrapy?branch=master\n   :alt: Coverage report\n\n.. image:: https://anaconda.org/conda-forge/scrapy/badges/version.svg\n   :target: https://anaconda.org/conda-forge/scrapy\n   :alt: Conda Version\n\nAbstract\n========\n\nThis is a fork for Scrapy. For apply best in python3\n\nOverview\n========\n\nScrapy is a fast high-level web crawling and web scraping framework, used to\ncrawl websites and extract structured data from their pages. It can be used for\na wide range of purposes, from data mining to monitoring and automated testing.\n\nFor more information including a list of features check the Scrapy homepage at:\nhttps://scrapy.org\n\nRequirements\n============\n\n* Python 3.4+\n* Works on Linux, Windows, Mac OSX, BSD\n\nInstall\n=======\n\nThe quick way::\n\n    pip install scrapy\n\nFor more details see the install section in the documentation:\nhttps://doc.scrapy.org/en/latest/intro/install.html\n\nDocumentation\n=============\n\nDocumentation is available online at https://doc.scrapy.org/ and in the ``docs``\ndirectory.\n\nReleases\n========\n\nYou can find release notes at https://doc.scrapy.org/en/latest/news.html\n\nCommunity (blog, twitter, mail list, IRC)\n=========================================\n\nSee https://scrapy.org/community/\n\nContributing\n============\n\nSee https://doc.scrapy.org/en/master/contributing.html\n\nCode of Conduct\n---------------\n\nPlease note that this project is released with a Contributor Code of Conduct\n(see https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md).\n\nBy participating in this project you agree to abide by its terms.\nPlease report unacceptable behavior to opensource@scrapinghub.com.\n\nCompanies using Scrapy\n======================\n\nSee https://scrapy.org/companies/\n\nCommercial Support\n==================\n\nSee https://scrapy.org/support/\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Bob-Du/scrapy3", "keywords": "", "license": "BSD", "maintainer": "", "maintainer_email": "", "name": "Scrapy3", "package_url": "https://pypi.org/project/Scrapy3/", "platform": "", "project_url": "https://pypi.org/project/Scrapy3/", "project_urls": {"Homepage": "https://github.com/Bob-Du/scrapy3"}, "release_url": "https://pypi.org/project/Scrapy3/1.0.1/", "requires_dist": ["Twisted (>=13.1.0)", "w3lib (>=1.17.0)", "queuelib", "lxml", "pyOpenSSL", "cssselect (>=0.9)", "six (>=1.5.2)", "parsel (>=1.4)", "PyDispatcher (>=2.0.5)", "service-identity", "PyPyDispatcher (>=2.1.0); platform_python_implementation == \"PyPy\""], "requires_python": ">=3.4", "summary": "A high-level Web Crawling and Web Scraping framework", "version": "1.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://pypi.python.org/pypi/Scrapy\" rel=\"nofollow\"><img alt=\"PyPI Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c32f7e728d29da096100985c2ef0de5c416b3ba2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f5363726170792e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/Scrapy\" rel=\"nofollow\"><img alt=\"Supported Python Versions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/88f611d591633c2fb86c760de065d196c4024bc5/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f5363726170792e737667\"></a>\n<a href=\"https://travis-ci.org/scrapy/scrapy\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/912110d08cc8b03a9003feaaf9f94825513b4e0e/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f7363726170792f7363726170792f6d61737465722e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/Scrapy\" rel=\"nofollow\"><img alt=\"Wheel Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/36f0977ff012856a4c1d90260124b61d42daea02/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f776865656c2d7965732d627269676874677265656e2e737667\"></a>\n<a href=\"https://codecov.io/github/scrapy/scrapy?branch=master\" rel=\"nofollow\"><img alt=\"Coverage report\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/29794fdbb42dbb17c1705fce84779e0ae5a53224/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f7363726170792f7363726170792f6d61737465722e737667\"></a>\n<a href=\"https://anaconda.org/conda-forge/scrapy\" rel=\"nofollow\"><img alt=\"Conda Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8aa303b34b8f52263d3ecbce0e338c0766925860/68747470733a2f2f616e61636f6e64612e6f72672f636f6e64612d666f7267652f7363726170792f6261646765732f76657273696f6e2e737667\"></a>\n<div id=\"abstract\">\n<h2>Abstract</h2>\n<p>This is a fork for Scrapy. For apply best in python3</p>\n</div>\n<div id=\"overview\">\n<h2>Overview</h2>\n<p>Scrapy is a fast high-level web crawling and web scraping framework, used to\ncrawl websites and extract structured data from their pages. It can be used for\na wide range of purposes, from data mining to monitoring and automated testing.</p>\n<p>For more information including a list of features check the Scrapy homepage at:\n<a href=\"https://scrapy.org\" rel=\"nofollow\">https://scrapy.org</a></p>\n</div>\n<div id=\"requirements\">\n<h2>Requirements</h2>\n<ul>\n<li>Python 3.4+</li>\n<li>Works on Linux, Windows, Mac OSX, BSD</li>\n</ul>\n</div>\n<div id=\"install\">\n<h2>Install</h2>\n<p>The quick way:</p>\n<pre>pip install scrapy\n</pre>\n<p>For more details see the install section in the documentation:\n<a href=\"https://doc.scrapy.org/en/latest/intro/install.html\" rel=\"nofollow\">https://doc.scrapy.org/en/latest/intro/install.html</a></p>\n</div>\n<div id=\"documentation\">\n<h2>Documentation</h2>\n<p>Documentation is available online at <a href=\"https://doc.scrapy.org/\" rel=\"nofollow\">https://doc.scrapy.org/</a> and in the <tt>docs</tt>\ndirectory.</p>\n</div>\n<div id=\"releases\">\n<h2>Releases</h2>\n<p>You can find release notes at <a href=\"https://doc.scrapy.org/en/latest/news.html\" rel=\"nofollow\">https://doc.scrapy.org/en/latest/news.html</a></p>\n</div>\n<div id=\"community-blog-twitter-mail-list-irc\">\n<h2>Community (blog, twitter, mail list, IRC)</h2>\n<p>See <a href=\"https://scrapy.org/community/\" rel=\"nofollow\">https://scrapy.org/community/</a></p>\n</div>\n<div id=\"contributing\">\n<h2>Contributing</h2>\n<p>See <a href=\"https://doc.scrapy.org/en/master/contributing.html\" rel=\"nofollow\">https://doc.scrapy.org/en/master/contributing.html</a></p>\n<div id=\"code-of-conduct\">\n<h3>Code of Conduct</h3>\n<p>Please note that this project is released with a Contributor Code of Conduct\n(see <a href=\"https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\" rel=\"nofollow\">https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md</a>).</p>\n<p>By participating in this project you agree to abide by its terms.\nPlease report unacceptable behavior to <a href=\"mailto:opensource%40scrapinghub.com\">opensource<span>@</span>scrapinghub<span>.</span>com</a>.</p>\n</div>\n</div>\n<div id=\"companies-using-scrapy\">\n<h2>Companies using Scrapy</h2>\n<p>See <a href=\"https://scrapy.org/companies/\" rel=\"nofollow\">https://scrapy.org/companies/</a></p>\n</div>\n<div id=\"commercial-support\">\n<h2>Commercial Support</h2>\n<p>See <a href=\"https://scrapy.org/support/\" rel=\"nofollow\">https://scrapy.org/support/</a></p>\n</div>\n\n          </div>"}, "last_serial": 4203044, "releases": {"1.0.1": [{"comment_text": "", "digests": {"md5": "c67318401181c58f70cdd035eca6b31f", "sha256": "7da6f2c055682436ef3612dff3d469261e3c72dba72b71707abdbcc95e3356ed"}, "downloads": -1, "filename": "Scrapy3-1.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c67318401181c58f70cdd035eca6b31f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.4", "size": 230725, "upload_time": "2018-08-24T09:35:14", "upload_time_iso_8601": "2018-08-24T09:35:14.998136Z", "url": "https://files.pythonhosted.org/packages/d1/8c/8063b0138fa0d98cf44eec91335e54f359c5079518afc3c191a4553eae8c/Scrapy3-1.0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d8d482b7d317620a31ec1bd297a4ce90", "sha256": "886c25830514d59bdcfe015bb681e74efc945e0d9032ae30cfbe702116224684"}, "downloads": -1, "filename": "Scrapy3-1.0.1.tar.gz", "has_sig": false, "md5_digest": "d8d482b7d317620a31ec1bd297a4ce90", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 912681, "upload_time": "2018-08-24T09:37:08", "upload_time_iso_8601": "2018-08-24T09:37:08.261510Z", "url": "https://files.pythonhosted.org/packages/9a/fa/6a0512089f4b8238e4d56dd9b21d5b07591a1747197796b48898896b3408/Scrapy3-1.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c67318401181c58f70cdd035eca6b31f", "sha256": "7da6f2c055682436ef3612dff3d469261e3c72dba72b71707abdbcc95e3356ed"}, "downloads": -1, "filename": "Scrapy3-1.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c67318401181c58f70cdd035eca6b31f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.4", "size": 230725, "upload_time": "2018-08-24T09:35:14", "upload_time_iso_8601": "2018-08-24T09:35:14.998136Z", "url": "https://files.pythonhosted.org/packages/d1/8c/8063b0138fa0d98cf44eec91335e54f359c5079518afc3c191a4553eae8c/Scrapy3-1.0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d8d482b7d317620a31ec1bd297a4ce90", "sha256": "886c25830514d59bdcfe015bb681e74efc945e0d9032ae30cfbe702116224684"}, "downloads": -1, "filename": "Scrapy3-1.0.1.tar.gz", "has_sig": false, "md5_digest": "d8d482b7d317620a31ec1bd297a4ce90", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 912681, "upload_time": "2018-08-24T09:37:08", "upload_time_iso_8601": "2018-08-24T09:37:08.261510Z", "url": "https://files.pythonhosted.org/packages/9a/fa/6a0512089f4b8238e4d56dd9b21d5b07591a1747197796b48898896b3408/Scrapy3-1.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:50 2020"}