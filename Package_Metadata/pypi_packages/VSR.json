{"info": {"author": "Wenyi Tang", "author_email": "wenyitang@outlook.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Video Super Resolution\nA collection of state-of-the-art video or single-image super-resolution architectures, reimplemented in tensorflow.\n\n**Project uploaded to PyPI now. Try install from PyPI:**\n```shell script\npip install VSR\n```\n\n**Pretrained weights is uploading now.**\n\n**Several referenced PyTorch implementations are also included now.**\n\n**Quick Link:**\n- [Installation](#install)\n- [Getting Started](#Getting-Started)\n- [Benchmark](https://github.com/LoSealL/VideoSuperResolution/blob/master/Docs/Benchmark%20(reproduce).md)\n\n## Network list and reference (Updating)\nThe hyperlink directs to paper site, follows the official codes if the authors open sources.\n\nAll these models are implemented in **ONE** framework.\n\n|Model |Published |Code* |VSR (TF)**|VSR (Torch)|Keywords|Pretrained|\n|:-----|:---------|:-----|:---------|:----------|:-------|:---------|\n|SRCNN|[ECCV14](https://arxiv.org/abs/1501.00092)|-, [Keras](https://github.com/qobilidop/srcnn)|Y|Y| Kaiming |[\u221a](https://github.com/LoSealL/Model/releases)|\n|RAISR|[arXiv](https://arxiv.org/abs/1606.01299)|-|-|-| Google, Pixel 3 ||\n|ESPCN|[CVPR16](https://arxiv.org/abs/1609.05158)|-, [Keras](https://github.com/qobilidop/srcnn)|Y|Y| Real time |[\u221a](https://github.com/LoSealL/Model/releases)|\n|VDSR|[CVPR16](https://arxiv.org/abs/1511.04587)|-|Y|Y| Deep, Residual |[\u221a](https://drive.google.com/open?id=1hW5YDxXpmjO2IfAy8f29O7yf1M3fPIg1)|\n|DRCN|[CVPR16](https://arxiv.org/abs/1511.04491)|-|Y|Y| Recurrent ||\n|DRRN|[CVPR17](http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf)|[Caffe](https://github.com/tyshiwo/DRRN_CVPR17), [PyTorch](https://github.com/jt827859032/DRRN-pytorch)|Y|Y| Recurrent ||\n|LapSRN|[CVPR17](http://vllab.ucmerced.edu/wlai24/LapSRN/)|[Matlab](https://github.com/phoenix104104/LapSRN)|Y|-| Huber loss ||\n|EDSR|[CVPR17](https://arxiv.org/abs/1707.02921)|-|Y|Y| NTIRE17 Champion |[\u221a](https://github.com/LoSealL/Model/releases)|\n|SRGAN|[CVPR17](https://arxiv.org/abs/1609.04802)|-|Y|-| 1st proposed GAN ||\n|VESPCN|[CVPR17](https://arxiv.org/abs/1611.05250)|-|Y|Y| VideoSR |[\u221a](https://drive.google.com/open?id=19u4YpsyThxW5dv4fhpMj7c5gZeEDKthm)|\n|MemNet|[ICCV17](https://arxiv.org/abs/1708.02209)|[Caffe](https://github.com/tyshiwo/MemNet)|Y|-|||\n|SRDenseNet|[ICCV17](http://openaccess.thecvf.com/content_ICCV_2017/papers/Tong_Image_Super-Resolution_Using_ICCV_2017_paper.pdf)|-, [PyTorch](https://github.com/wxywhu/SRDenseNet-pytorch)|Y|-| Dense |[\u221a](https://drive.google.com/open?id=1aXAfRqZieY6mTfZUnErG84-9NfkQSeDw)|\n|SPMC|[ICCV17](https://arxiv.org/abs/1704.02738)|[Tensorflow](https://github.com/jiangsutx/SPMC_VideoSR)|T|Y| VideoSR ||\n|DnCNN|[TIP17](http://ieeexplore.ieee.org/document/7839189/)|[Matlab](https://github.com/cszn/DnCNN)|Y|Y| Denoise |[\u221a](https://github.com/LoSealL/Model/releases)|\n|DCSCN|[arXiv](https://arxiv.org/abs/1707.05425)|[Tensorflow](https://github.com/jiny2001/dcscn-super-resolution)|Y|-|||\n|IDN|[CVPR18](https://arxiv.org/abs/1803.09454)|[Caffe](https://github.com/Zheng222/IDN-Caffe)|Y|-| Fast |[\u221a](https://drive.google.com/open?id=1Fh3rtvrKKLAK27r518T1M_JET_LWZAFQ)|\n|RDN|[CVPR18](https://arxiv.org/abs/1802.08797)|[Torch](https://github.com/yulunzhang/RDN)|Y|-| Deep, BI-BD-DN ||\n|SRMD|[CVPR18](https://arxiv.org/abs/1712.06116)|[Matlab](https://github.com/cszn/SRMD)|-|Y| Denoise/Deblur/SR |[\u221a](https://drive.google.com/open?id=1ORKH05-aLSbQaWB4qQulIm2INoRufuD_)|\n|DBPN|[CVPR18](https://arxiv.org/abs/1803.02735)|[PyTorch](https://github.com/alterzero/DBPN-Pytorch)|Y|Y| NTIRE18 Champion |[\u221a](https://drive.google.com/open?id=1ymtlOjhkGmad-od0zw7yTf17nWD4KMVi)|\n|ZSSR|[CVPR18](http://www.wisdom.weizmann.ac.il/~vision/zssr/)|[Tensorflow](https://github.com/assafshocher/ZSSR)|-|-| Zero-shot ||\n|FRVSR|[CVPR18](https://arxiv.org/abs/1801.04590)|[PDF](https://github.com/msmsajjadi/FRVSR)|T|Y| VideoSR |[\u221a](https://github.com/LoSealL/Model/releases)|\n|DUF|[CVPR18](http://openaccess.thecvf.com/content_cvpr_2018/papers/Jo_Deep_Video_Super-Resolution_CVPR_2018_paper.pdf)|[Tensorflow](https://github.com/yhjo09/VSR-DUF)|T|-| VideoSR ||\n|CARN|[ECCV18](https://arxiv.org/abs/1803.08664)|[PyTorch](https://github.com/nmhkahn/CARN-pytorch)|Y|Y| Fast |[\u221a](https://github.com/LoSealL/Model/releases/carn)|\n|RCAN|[ECCV18](https://arxiv.org/abs/1807.02758)|[PyTorch](https://github.com/yulunzhang/RCAN)|Y|Y| Deep, BI-BD-DN ||\n|MSRN|[ECCV18](http://openaccess.thecvf.com/content_ECCV_2018/papers/Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper.pdf)|[PyTorch](https://github.com/MIVRC/MSRN-PyTorch)|Y|Y| |[\u221a](https://drive.google.com/open?id=1A0LoY3oB_VnArP3GzI1ILUNJbLAEjdtJ)|\n|SRFeat|[ECCV18](http://openaccess.thecvf.com/content_ECCV_2018/papers/Seong-Jin_Park_SRFeat_Single_Image_ECCV_2018_paper.pdf)|[Tensorflow](https://github.com/HyeongseokSon1/SRFeat)|Y|Y| GAN ||\n|NLRN|[NIPS18](https://papers.nips.cc/paper/7439-non-local-recurrent-network-for-image-restoration.pdf)|[Tensorflow](https://github.com/Ding-Liu/NLRN)|T|-| Non-local, Recurrent ||\n|SRCliqueNet|[NIPS18](https://arxiv.org/abs/1809.04508)|-|-|-| Wavelet ||\n|FFDNet|[TIP18](https://ieeexplore.ieee.org/document/8365806/)|[Matlab](https://github.com/cszn/FFDNet)|Y|Y| Conditional denoise||\n|CBDNet|[CVPR19](https://arxiv.org/abs/1807.04686)|[Matlab](https://github.com/GuoShi28/CBDNet)|T|-| Blind-denoise ||\n|SOFVSR|[ACCV18](http://arxiv.org/abs/1809.08573)|[PyTorch](https://github.com/LongguangWang/SOF-VSR)|-|Y| VideoSR |[\u221a](https://github.com/LoSealL/Model/releases/download/sofvsr/SOFVSR_x4.zip)|\n|ESRGAN|[ECCVW18](http://arxiv.org/abs/1809.00219)|[PyTorch](https://github.com/xinntao/ESRGAN)|-|Y|1st place PIRM 2018|[\u221a](https://github.com/LoSealL/Model/releases/download/esrgan/esrgan.zip)|\n|TecoGAN|[arXiv](http://arxiv.org/abs/1811.09393)|[Tensorflow](https://github.com/thunil/TecoGAN)|-|T| VideoSR GAN|[\u221a](https://github.com/LoSealL/Model/releases/download/tecogan/tecogan.zip)|\n|RBPN|[CVPR19](https://arxiv.org/abs/1903.10128)|[PyTorch](https://github.com/alterzero/RBPN-PyTorch)|-|Y| VideoSR |[\u221a](https://drive.google.com/open?id=1Ozp5j-DBWJSpXY5GvxiEPKdfCaAbOXqu)|\n|DPSR|[CVPR19](https://arxiv.org/abs/1903.12529)|[Pytorch](https://github.com/cszn/DPSR)|-|-|||\n|SRFBN|[CVPR19](https://arxiv.org/abs/1903.09814)|[Pytorch](https://github.com/Paper99/SRFBN_CVPR19)|-|-||||\n|SRNTT|[CVPR19](https://arxiv.org/abs/1903.00834)|[Tensorflow](https://github.com/ZZUTK/SRNTT)|-|-|Adobe||\n|SAN|[CVPR19](http://openaccess.thecvf.com/content_CVPR_2019/papers/Dai_Second-Order_Attention_Network_for_Single_Image_Super-Resolution_CVPR_2019_paper.pdf)|[empty](https://github.com/daitao/SAN)|-|-| AliDAMO SOTA ||\n|AdaFM|[CVPR19](https://arxiv.org/abs/1904.08118)|[Pytorch](https://github.com/hejingwenhejingwen/AdaFM)|-|-| SenseTime Oral ||\n\n\\*The 1st repo is by paper author.\n\n\\**__Y__: included; __-__: not included; __T__: under-testing. \n\nYou can download pre-trained weights through [`prepare_data`](./prepare_data.py), or visit the hyperlink at **\u221a**.\n\n## Link of datasets\n*(please contact me if any of links offend you or any one disabled)*\n\n|Name|Usage|#|Site|Comments|\n|:---|:----|:----|:---|:-----|\n|SET5|Test|5|[download](https://uofi.box.com/shared/static/kfahv87nfe8ax910l85dksyl2q212voc.zip)|[jbhuang0604](https://github.com/jbhuang0604/SelfExSR)|\n|SET14|Test|14|[download](https://uofi.box.com/shared/static/igsnfieh4lz68l926l8xbklwsnnk8we9.zip)|[jbhuang0604](https://github.com/jbhuang0604/SelfExSR)|\n|SunHay80|Test|80|[download](https://uofi.box.com/shared/static/rirohj4773jl7ef752r330rtqw23djt8.zip)|[jbhuang0604](https://github.com/jbhuang0604/SelfExSR)|\n|Urban100|Test|100|[download](https://uofi.box.com/shared/static/65upg43jjd0a4cwsiqgl6o6ixube6klm.zip)|[jbhuang0604](https://github.com/jbhuang0604/SelfExSR)|\n|VID4|Test|4|[download](https://people.csail.mit.edu/celiu/CVPR2011/videoSR.zip)|4 videos|\n|BSD100|Train|300|[download](https://uofi.box.com/shared/static/qgctsplb8txrksm9to9x01zfa4m61ngq.zip)|[jbhuang0604](https://github.com/jbhuang0604/SelfExSR)|\n|BSD300|Train/Val|300|[download](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/BSDS300-images.tgz)|-|\n|BSD500|Train/Val|500|[download](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz)|-|\n|91-Image|Train|91|[download](http://www.ifp.illinois.edu/~jyang29/codes/ScSR.rar)|Yang|\n|DIV2K|Train/Val|900|[website](https://data.vision.ee.ethz.ch/cvl/DIV2K/)|NTIRE17|\n|Waterloo|Train|4741|[website](https://ece.uwaterloo.ca/~k29ma/exploration/)|-|\n|MCL-V|Train|12|[website](http://mcl.usc.edu/mcl-v-database/)|12 videos|\n|GOPRO|Train/Val|33|[website](https://github.com/SeungjunNah/DeepDeblur_release)|33 videos, deblur|\n|CelebA|Train|202599|[website](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)|Human faces|\n|Sintel|Train/Val|35|[website](http://sintel.is.tue.mpg.de/downloads)|Optical flow|\n|FlyingChairs|Train|22872|[website](https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html#flyingchairs)|Optical flow|\n|DND|Test|50|[website](https://noise.visinf.tu-darmstadt.de/)|Real noisy photos|\n|RENOIR|Train|120|[website](http://ani.stat.fsu.edu/~abarbu/Renoir.html)|Real noisy photos|\n|NC|Test|60|[website](http://demo.ipol.im/demo/125/)|Noisy photos|\n|SIDD(M)|Train/Val|200|[website](https://www.eecs.yorku.ca/~kamel/sidd/)|NTIRE 2019 Real Denoise|\n|RSR|Train/Val|80|[download]()|NTIRE 2019 Real SR|\n|Vimeo-90k|Train/Test|89800|[website](http://toflow.csail.mit.edu/)|90k HQ videos|\n\nOther open datasets:\n[Kaggle](https://www.kaggle.com/datasets)\n[ImageNet](http://www.image-net.org/)\n[COCO](http://cocodataset.org/)\n\n## VSR package\nThis package offers a training and data processing framework based on [TF](https://www.tensorflow.org).\nWhat I made is a simple, easy-to-use framework without lots of encapulations and abstractions.\nMoreover, VSR can handle raw NV12/YUV as well as a sequence of images as inputs.\n\n### Install\n\n1. Prepare proper tensorflow and pytorch(optional). For example, GPU and CUDA10.0 (recommend to use `conda`):\n\n   ```shell\n   conda install tensorflow-gpu==1.15.0\n   # optional\n   # conda install pytorch\n   ```\n\n2. Install VSR package\n\n   ```bash\n   # For someone see this doc online\n   # git clone https://github.com/loseall/VideoSuperResolution && cd VideoSuperResolution\n   pip install -e .\n   ```\n### Getting Started\n\n1. Download pre-trained weights and (optinal) training datasets. For instance, let\\'s begin with VESPCN and vid4 test data:\n   ```shell\n   python prepare_data.py --filter vespcn vid4\n   ```\n\n2. Customize backend\n   cd ~/.vsr/\n   touch config.yml\n   ```yaml\n   backend: tensorflow  # (tensorflow, pytorch)\n   verbose: info        # (debug, info, warning, error)\n   ```\n\n3. Evaluate\n   ```shell\n   cd Train\n   python eval.py srcnn -t vid4 --pretrain=/path/srcnn.pth\n   ```\n\n4. Train\n   ```shell\n   python prepare_data.py --filter mcl-v\n   cd Train\n   python train.py vespcn --dataset mcl-v --memory_limit 1GB --epochs 100\n   ```\n\nOK, that's all you need. For more details, use `--help` to get more information.\n\n----\nMore documents can be found at [Docs](https://github.com/LoSealL/VideoSuperResolution/tree/master/Docs).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/LoSealL/VideoSuperResolution", "keywords": "super-resolution sr vsr cnn srcnn vespcn", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "VSR", "package_url": "https://pypi.org/project/VSR/", "platform": "", "project_url": "https://pypi.org/project/VSR/", "project_urls": {"Homepage": "https://github.com/LoSealL/VideoSuperResolution"}, "release_url": "https://pypi.org/project/VSR/1.0.4/", "requires_dist": ["numpy", "scipy", "scikit-image", "matplotlib", "pillow", "pypng", "pytest", "PyYAML", "psutil", "tqdm", "h5py", "easydict (>=1.9)"], "requires_python": ">=3.6", "summary": "Video Super-Resolution Framework", "version": "1.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Video Super Resolution</h1>\n<p>A collection of state-of-the-art video or single-image super-resolution architectures, reimplemented in tensorflow.</p>\n<p><strong>Project uploaded to PyPI now. Try install from PyPI:</strong></p>\n<pre>pip install VSR\n</pre>\n<p><strong>Pretrained weights is uploading now.</strong></p>\n<p><strong>Several referenced PyTorch implementations are also included now.</strong></p>\n<p><strong>Quick Link:</strong></p>\n<ul>\n<li><a href=\"#install\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#Getting-Started\" rel=\"nofollow\">Getting Started</a></li>\n<li><a href=\"https://github.com/LoSealL/VideoSuperResolution/blob/master/Docs/Benchmark%20(reproduce).md\" rel=\"nofollow\">Benchmark</a></li>\n</ul>\n<h2>Network list and reference (Updating)</h2>\n<p>The hyperlink directs to paper site, follows the official codes if the authors open sources.</p>\n<p>All these models are implemented in <strong>ONE</strong> framework.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Model</th>\n<th align=\"left\">Published</th>\n<th align=\"left\">Code*</th>\n<th align=\"left\">VSR (TF)**</th>\n<th align=\"left\">VSR (Torch)</th>\n<th align=\"left\">Keywords</th>\n<th align=\"left\">Pretrained</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">SRCNN</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1501.00092\" rel=\"nofollow\">ECCV14</a></td>\n<td align=\"left\">-, <a href=\"https://github.com/qobilidop/srcnn\" rel=\"nofollow\">Keras</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Kaiming</td>\n<td align=\"left\"><a href=\"https://github.com/LoSealL/Model/releases\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">RAISR</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1606.01299\" rel=\"nofollow\">arXiv</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">-</td>\n<td align=\"left\">-</td>\n<td align=\"left\">Google, Pixel 3</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">ESPCN</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1609.05158\" rel=\"nofollow\">CVPR16</a></td>\n<td align=\"left\">-, <a href=\"https://github.com/qobilidop/srcnn\" rel=\"nofollow\">Keras</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Real time</td>\n<td align=\"left\"><a href=\"https://github.com/LoSealL/Model/releases\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">VDSR</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1511.04587\" rel=\"nofollow\">CVPR16</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Deep, Residual</td>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1hW5YDxXpmjO2IfAy8f29O7yf1M3fPIg1\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">DRCN</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1511.04491\" rel=\"nofollow\">CVPR16</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Recurrent</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">DRRN</td>\n<td align=\"left\"><a href=\"http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf\" rel=\"nofollow\">CVPR17</a></td>\n<td align=\"left\"><a href=\"https://github.com/tyshiwo/DRRN_CVPR17\" rel=\"nofollow\">Caffe</a>, <a href=\"https://github.com/jt827859032/DRRN-pytorch\" rel=\"nofollow\">PyTorch</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Recurrent</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">LapSRN</td>\n<td align=\"left\"><a href=\"http://vllab.ucmerced.edu/wlai24/LapSRN/\" rel=\"nofollow\">CVPR17</a></td>\n<td align=\"left\"><a href=\"https://github.com/phoenix104104/LapSRN\" rel=\"nofollow\">Matlab</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">-</td>\n<td align=\"left\">Huber loss</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">EDSR</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1707.02921\" rel=\"nofollow\">CVPR17</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">NTIRE17 Champion</td>\n<td align=\"left\"><a href=\"https://github.com/LoSealL/Model/releases\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">SRGAN</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1609.04802\" rel=\"nofollow\">CVPR17</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">-</td>\n<td align=\"left\">1st proposed GAN</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">VESPCN</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1611.05250\" rel=\"nofollow\">CVPR17</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">VideoSR</td>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=19u4YpsyThxW5dv4fhpMj7c5gZeEDKthm\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">MemNet</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1708.02209\" rel=\"nofollow\">ICCV17</a></td>\n<td align=\"left\"><a href=\"https://github.com/tyshiwo/MemNet\" rel=\"nofollow\">Caffe</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">-</td>\n<td align=\"left\"></td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">SRDenseNet</td>\n<td align=\"left\"><a href=\"http://openaccess.thecvf.com/content_ICCV_2017/papers/Tong_Image_Super-Resolution_Using_ICCV_2017_paper.pdf\" rel=\"nofollow\">ICCV17</a></td>\n<td align=\"left\">-, <a href=\"https://github.com/wxywhu/SRDenseNet-pytorch\" rel=\"nofollow\">PyTorch</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">-</td>\n<td align=\"left\">Dense</td>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1aXAfRqZieY6mTfZUnErG84-9NfkQSeDw\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">SPMC</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1704.02738\" rel=\"nofollow\">ICCV17</a></td>\n<td align=\"left\"><a href=\"https://github.com/jiangsutx/SPMC_VideoSR\" rel=\"nofollow\">Tensorflow</a></td>\n<td align=\"left\">T</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">VideoSR</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">DnCNN</td>\n<td align=\"left\"><a href=\"http://ieeexplore.ieee.org/document/7839189/\" rel=\"nofollow\">TIP17</a></td>\n<td align=\"left\"><a href=\"https://github.com/cszn/DnCNN\" rel=\"nofollow\">Matlab</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Denoise</td>\n<td align=\"left\"><a href=\"https://github.com/LoSealL/Model/releases\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">DCSCN</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1707.05425\" rel=\"nofollow\">arXiv</a></td>\n<td align=\"left\"><a href=\"https://github.com/jiny2001/dcscn-super-resolution\" rel=\"nofollow\">Tensorflow</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">-</td>\n<td align=\"left\"></td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">IDN</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1803.09454\" rel=\"nofollow\">CVPR18</a></td>\n<td align=\"left\"><a href=\"https://github.com/Zheng222/IDN-Caffe\" rel=\"nofollow\">Caffe</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">-</td>\n<td align=\"left\">Fast</td>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1Fh3rtvrKKLAK27r518T1M_JET_LWZAFQ\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">RDN</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1802.08797\" rel=\"nofollow\">CVPR18</a></td>\n<td align=\"left\"><a href=\"https://github.com/yulunzhang/RDN\" rel=\"nofollow\">Torch</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">-</td>\n<td align=\"left\">Deep, BI-BD-DN</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">SRMD</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1712.06116\" rel=\"nofollow\">CVPR18</a></td>\n<td align=\"left\"><a href=\"https://github.com/cszn/SRMD\" rel=\"nofollow\">Matlab</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Denoise/Deblur/SR</td>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1ORKH05-aLSbQaWB4qQulIm2INoRufuD_\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">DBPN</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1803.02735\" rel=\"nofollow\">CVPR18</a></td>\n<td align=\"left\"><a href=\"https://github.com/alterzero/DBPN-Pytorch\" rel=\"nofollow\">PyTorch</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">NTIRE18 Champion</td>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1ymtlOjhkGmad-od0zw7yTf17nWD4KMVi\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">ZSSR</td>\n<td align=\"left\"><a href=\"http://www.wisdom.weizmann.ac.il/%7Evision/zssr/\" rel=\"nofollow\">CVPR18</a></td>\n<td align=\"left\"><a href=\"https://github.com/assafshocher/ZSSR\" rel=\"nofollow\">Tensorflow</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">-</td>\n<td align=\"left\">Zero-shot</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">FRVSR</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1801.04590\" rel=\"nofollow\">CVPR18</a></td>\n<td align=\"left\"><a href=\"https://github.com/msmsajjadi/FRVSR\" rel=\"nofollow\">PDF</a></td>\n<td align=\"left\">T</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">VideoSR</td>\n<td align=\"left\"><a href=\"https://github.com/LoSealL/Model/releases\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">DUF</td>\n<td align=\"left\"><a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Jo_Deep_Video_Super-Resolution_CVPR_2018_paper.pdf\" rel=\"nofollow\">CVPR18</a></td>\n<td align=\"left\"><a href=\"https://github.com/yhjo09/VSR-DUF\" rel=\"nofollow\">Tensorflow</a></td>\n<td align=\"left\">T</td>\n<td align=\"left\">-</td>\n<td align=\"left\">VideoSR</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">CARN</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1803.08664\" rel=\"nofollow\">ECCV18</a></td>\n<td align=\"left\"><a href=\"https://github.com/nmhkahn/CARN-pytorch\" rel=\"nofollow\">PyTorch</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Fast</td>\n<td align=\"left\"><a href=\"https://github.com/LoSealL/Model/releases/carn\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">RCAN</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1807.02758\" rel=\"nofollow\">ECCV18</a></td>\n<td align=\"left\"><a href=\"https://github.com/yulunzhang/RCAN\" rel=\"nofollow\">PyTorch</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Deep, BI-BD-DN</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">MSRN</td>\n<td align=\"left\"><a href=\"http://openaccess.thecvf.com/content_ECCV_2018/papers/Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper.pdf\" rel=\"nofollow\">ECCV18</a></td>\n<td align=\"left\"><a href=\"https://github.com/MIVRC/MSRN-PyTorch\" rel=\"nofollow\">PyTorch</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\"></td>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1A0LoY3oB_VnArP3GzI1ILUNJbLAEjdtJ\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">SRFeat</td>\n<td align=\"left\"><a href=\"http://openaccess.thecvf.com/content_ECCV_2018/papers/Seong-Jin_Park_SRFeat_Single_Image_ECCV_2018_paper.pdf\" rel=\"nofollow\">ECCV18</a></td>\n<td align=\"left\"><a href=\"https://github.com/HyeongseokSon1/SRFeat\" rel=\"nofollow\">Tensorflow</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">GAN</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">NLRN</td>\n<td align=\"left\"><a href=\"https://papers.nips.cc/paper/7439-non-local-recurrent-network-for-image-restoration.pdf\" rel=\"nofollow\">NIPS18</a></td>\n<td align=\"left\"><a href=\"https://github.com/Ding-Liu/NLRN\" rel=\"nofollow\">Tensorflow</a></td>\n<td align=\"left\">T</td>\n<td align=\"left\">-</td>\n<td align=\"left\">Non-local, Recurrent</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">SRCliqueNet</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1809.04508\" rel=\"nofollow\">NIPS18</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">-</td>\n<td align=\"left\">-</td>\n<td align=\"left\">Wavelet</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">FFDNet</td>\n<td align=\"left\"><a href=\"https://ieeexplore.ieee.org/document/8365806/\" rel=\"nofollow\">TIP18</a></td>\n<td align=\"left\"><a href=\"https://github.com/cszn/FFDNet\" rel=\"nofollow\">Matlab</a></td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">Conditional denoise</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">CBDNet</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1807.04686\" rel=\"nofollow\">CVPR19</a></td>\n<td align=\"left\"><a href=\"https://github.com/GuoShi28/CBDNet\" rel=\"nofollow\">Matlab</a></td>\n<td align=\"left\">T</td>\n<td align=\"left\">-</td>\n<td align=\"left\">Blind-denoise</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">SOFVSR</td>\n<td align=\"left\"><a href=\"http://arxiv.org/abs/1809.08573\" rel=\"nofollow\">ACCV18</a></td>\n<td align=\"left\"><a href=\"https://github.com/LongguangWang/SOF-VSR\" rel=\"nofollow\">PyTorch</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">VideoSR</td>\n<td align=\"left\"><a href=\"https://github.com/LoSealL/Model/releases/download/sofvsr/SOFVSR_x4.zip\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">ESRGAN</td>\n<td align=\"left\"><a href=\"http://arxiv.org/abs/1809.00219\" rel=\"nofollow\">ECCVW18</a></td>\n<td align=\"left\"><a href=\"https://github.com/xinntao/ESRGAN\" rel=\"nofollow\">PyTorch</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">1st place PIRM 2018</td>\n<td align=\"left\"><a href=\"https://github.com/LoSealL/Model/releases/download/esrgan/esrgan.zip\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">TecoGAN</td>\n<td align=\"left\"><a href=\"http://arxiv.org/abs/1811.09393\" rel=\"nofollow\">arXiv</a></td>\n<td align=\"left\"><a href=\"https://github.com/thunil/TecoGAN\" rel=\"nofollow\">Tensorflow</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">T</td>\n<td align=\"left\">VideoSR GAN</td>\n<td align=\"left\"><a href=\"https://github.com/LoSealL/Model/releases/download/tecogan/tecogan.zip\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">RBPN</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1903.10128\" rel=\"nofollow\">CVPR19</a></td>\n<td align=\"left\"><a href=\"https://github.com/alterzero/RBPN-PyTorch\" rel=\"nofollow\">PyTorch</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">Y</td>\n<td align=\"left\">VideoSR</td>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1Ozp5j-DBWJSpXY5GvxiEPKdfCaAbOXqu\" rel=\"nofollow\">\u221a</a></td>\n</tr>\n<tr>\n<td align=\"left\">DPSR</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1903.12529\" rel=\"nofollow\">CVPR19</a></td>\n<td align=\"left\"><a href=\"https://github.com/cszn/DPSR\" rel=\"nofollow\">Pytorch</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">-</td>\n<td align=\"left\"></td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">SRFBN</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1903.09814\" rel=\"nofollow\">CVPR19</a></td>\n<td align=\"left\"><a href=\"https://github.com/Paper99/SRFBN_CVPR19\" rel=\"nofollow\">Pytorch</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">-</td>\n<td align=\"left\"></td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">SRNTT</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1903.00834\" rel=\"nofollow\">CVPR19</a></td>\n<td align=\"left\"><a href=\"https://github.com/ZZUTK/SRNTT\" rel=\"nofollow\">Tensorflow</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">-</td>\n<td align=\"left\">Adobe</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">SAN</td>\n<td align=\"left\"><a href=\"http://openaccess.thecvf.com/content_CVPR_2019/papers/Dai_Second-Order_Attention_Network_for_Single_Image_Super-Resolution_CVPR_2019_paper.pdf\" rel=\"nofollow\">CVPR19</a></td>\n<td align=\"left\"><a href=\"https://github.com/daitao/SAN\" rel=\"nofollow\">empty</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">-</td>\n<td align=\"left\">AliDAMO SOTA</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">AdaFM</td>\n<td align=\"left\"><a href=\"https://arxiv.org/abs/1904.08118\" rel=\"nofollow\">CVPR19</a></td>\n<td align=\"left\"><a href=\"https://github.com/hejingwenhejingwen/AdaFM\" rel=\"nofollow\">Pytorch</a></td>\n<td align=\"left\">-</td>\n<td align=\"left\">-</td>\n<td align=\"left\">SenseTime Oral</td>\n<td align=\"left\"></td>\n</tr></tbody></table>\n<p>*The 1st repo is by paper author.</p>\n<p>**<strong>Y</strong>: included; <strong>-</strong>: not included; <strong>T</strong>: under-testing.</p>\n<p>You can download pre-trained weights through <a href=\"./prepare_data.py\" rel=\"nofollow\"><code>prepare_data</code></a>, or visit the hyperlink at <strong>\u221a</strong>.</p>\n<h2>Link of datasets</h2>\n<p><em>(please contact me if any of links offend you or any one disabled)</em></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Name</th>\n<th align=\"left\">Usage</th>\n<th align=\"left\">#</th>\n<th align=\"left\">Site</th>\n<th align=\"left\">Comments</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">SET5</td>\n<td align=\"left\">Test</td>\n<td align=\"left\">5</td>\n<td align=\"left\"><a href=\"https://uofi.box.com/shared/static/kfahv87nfe8ax910l85dksyl2q212voc.zip\" rel=\"nofollow\">download</a></td>\n<td align=\"left\"><a href=\"https://github.com/jbhuang0604/SelfExSR\" rel=\"nofollow\">jbhuang0604</a></td>\n</tr>\n<tr>\n<td align=\"left\">SET14</td>\n<td align=\"left\">Test</td>\n<td align=\"left\">14</td>\n<td align=\"left\"><a href=\"https://uofi.box.com/shared/static/igsnfieh4lz68l926l8xbklwsnnk8we9.zip\" rel=\"nofollow\">download</a></td>\n<td align=\"left\"><a href=\"https://github.com/jbhuang0604/SelfExSR\" rel=\"nofollow\">jbhuang0604</a></td>\n</tr>\n<tr>\n<td align=\"left\">SunHay80</td>\n<td align=\"left\">Test</td>\n<td align=\"left\">80</td>\n<td align=\"left\"><a href=\"https://uofi.box.com/shared/static/rirohj4773jl7ef752r330rtqw23djt8.zip\" rel=\"nofollow\">download</a></td>\n<td align=\"left\"><a href=\"https://github.com/jbhuang0604/SelfExSR\" rel=\"nofollow\">jbhuang0604</a></td>\n</tr>\n<tr>\n<td align=\"left\">Urban100</td>\n<td align=\"left\">Test</td>\n<td align=\"left\">100</td>\n<td align=\"left\"><a href=\"https://uofi.box.com/shared/static/65upg43jjd0a4cwsiqgl6o6ixube6klm.zip\" rel=\"nofollow\">download</a></td>\n<td align=\"left\"><a href=\"https://github.com/jbhuang0604/SelfExSR\" rel=\"nofollow\">jbhuang0604</a></td>\n</tr>\n<tr>\n<td align=\"left\">VID4</td>\n<td align=\"left\">Test</td>\n<td align=\"left\">4</td>\n<td align=\"left\"><a href=\"https://people.csail.mit.edu/celiu/CVPR2011/videoSR.zip\" rel=\"nofollow\">download</a></td>\n<td align=\"left\">4 videos</td>\n</tr>\n<tr>\n<td align=\"left\">BSD100</td>\n<td align=\"left\">Train</td>\n<td align=\"left\">300</td>\n<td align=\"left\"><a href=\"https://uofi.box.com/shared/static/qgctsplb8txrksm9to9x01zfa4m61ngq.zip\" rel=\"nofollow\">download</a></td>\n<td align=\"left\"><a href=\"https://github.com/jbhuang0604/SelfExSR\" rel=\"nofollow\">jbhuang0604</a></td>\n</tr>\n<tr>\n<td align=\"left\">BSD300</td>\n<td align=\"left\">Train/Val</td>\n<td align=\"left\">300</td>\n<td align=\"left\"><a href=\"https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/BSDS300-images.tgz\" rel=\"nofollow\">download</a></td>\n<td align=\"left\">-</td>\n</tr>\n<tr>\n<td align=\"left\">BSD500</td>\n<td align=\"left\">Train/Val</td>\n<td align=\"left\">500</td>\n<td align=\"left\"><a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz\" rel=\"nofollow\">download</a></td>\n<td align=\"left\">-</td>\n</tr>\n<tr>\n<td align=\"left\">91-Image</td>\n<td align=\"left\">Train</td>\n<td align=\"left\">91</td>\n<td align=\"left\"><a href=\"http://www.ifp.illinois.edu/%7Ejyang29/codes/ScSR.rar\" rel=\"nofollow\">download</a></td>\n<td align=\"left\">Yang</td>\n</tr>\n<tr>\n<td align=\"left\">DIV2K</td>\n<td align=\"left\">Train/Val</td>\n<td align=\"left\">900</td>\n<td align=\"left\"><a href=\"https://data.vision.ee.ethz.ch/cvl/DIV2K/\" rel=\"nofollow\">website</a></td>\n<td align=\"left\">NTIRE17</td>\n</tr>\n<tr>\n<td align=\"left\">Waterloo</td>\n<td align=\"left\">Train</td>\n<td align=\"left\">4741</td>\n<td align=\"left\"><a href=\"https://ece.uwaterloo.ca/%7Ek29ma/exploration/\" rel=\"nofollow\">website</a></td>\n<td align=\"left\">-</td>\n</tr>\n<tr>\n<td align=\"left\">MCL-V</td>\n<td align=\"left\">Train</td>\n<td align=\"left\">12</td>\n<td align=\"left\"><a href=\"http://mcl.usc.edu/mcl-v-database/\" rel=\"nofollow\">website</a></td>\n<td align=\"left\">12 videos</td>\n</tr>\n<tr>\n<td align=\"left\">GOPRO</td>\n<td align=\"left\">Train/Val</td>\n<td align=\"left\">33</td>\n<td align=\"left\"><a href=\"https://github.com/SeungjunNah/DeepDeblur_release\" rel=\"nofollow\">website</a></td>\n<td align=\"left\">33 videos, deblur</td>\n</tr>\n<tr>\n<td align=\"left\">CelebA</td>\n<td align=\"left\">Train</td>\n<td align=\"left\">202599</td>\n<td align=\"left\"><a href=\"http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\" rel=\"nofollow\">website</a></td>\n<td align=\"left\">Human faces</td>\n</tr>\n<tr>\n<td align=\"left\">Sintel</td>\n<td align=\"left\">Train/Val</td>\n<td align=\"left\">35</td>\n<td align=\"left\"><a href=\"http://sintel.is.tue.mpg.de/downloads\" rel=\"nofollow\">website</a></td>\n<td align=\"left\">Optical flow</td>\n</tr>\n<tr>\n<td align=\"left\">FlyingChairs</td>\n<td align=\"left\">Train</td>\n<td align=\"left\">22872</td>\n<td align=\"left\"><a href=\"https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html#flyingchairs\" rel=\"nofollow\">website</a></td>\n<td align=\"left\">Optical flow</td>\n</tr>\n<tr>\n<td align=\"left\">DND</td>\n<td align=\"left\">Test</td>\n<td align=\"left\">50</td>\n<td align=\"left\"><a href=\"https://noise.visinf.tu-darmstadt.de/\" rel=\"nofollow\">website</a></td>\n<td align=\"left\">Real noisy photos</td>\n</tr>\n<tr>\n<td align=\"left\">RENOIR</td>\n<td align=\"left\">Train</td>\n<td align=\"left\">120</td>\n<td align=\"left\"><a href=\"http://ani.stat.fsu.edu/%7Eabarbu/Renoir.html\" rel=\"nofollow\">website</a></td>\n<td align=\"left\">Real noisy photos</td>\n</tr>\n<tr>\n<td align=\"left\">NC</td>\n<td align=\"left\">Test</td>\n<td align=\"left\">60</td>\n<td align=\"left\"><a href=\"http://demo.ipol.im/demo/125/\" rel=\"nofollow\">website</a></td>\n<td align=\"left\">Noisy photos</td>\n</tr>\n<tr>\n<td align=\"left\">SIDD(M)</td>\n<td align=\"left\">Train/Val</td>\n<td align=\"left\">200</td>\n<td align=\"left\"><a href=\"https://www.eecs.yorku.ca/%7Ekamel/sidd/\" rel=\"nofollow\">website</a></td>\n<td align=\"left\">NTIRE 2019 Real Denoise</td>\n</tr>\n<tr>\n<td align=\"left\">RSR</td>\n<td align=\"left\">Train/Val</td>\n<td align=\"left\">80</td>\n<td align=\"left\"><a href=\"\" rel=\"nofollow\">download</a></td>\n<td align=\"left\">NTIRE 2019 Real SR</td>\n</tr>\n<tr>\n<td align=\"left\">Vimeo-90k</td>\n<td align=\"left\">Train/Test</td>\n<td align=\"left\">89800</td>\n<td align=\"left\"><a href=\"http://toflow.csail.mit.edu/\" rel=\"nofollow\">website</a></td>\n<td align=\"left\">90k HQ videos</td>\n</tr></tbody></table>\n<p>Other open datasets:\n<a href=\"https://www.kaggle.com/datasets\" rel=\"nofollow\">Kaggle</a>\n<a href=\"http://www.image-net.org/\" rel=\"nofollow\">ImageNet</a>\n<a href=\"http://cocodataset.org/\" rel=\"nofollow\">COCO</a></p>\n<h2>VSR package</h2>\n<p>This package offers a training and data processing framework based on <a href=\"https://www.tensorflow.org\" rel=\"nofollow\">TF</a>.\nWhat I made is a simple, easy-to-use framework without lots of encapulations and abstractions.\nMoreover, VSR can handle raw NV12/YUV as well as a sequence of images as inputs.</p>\n<h3>Install</h3>\n<ol>\n<li>\n<p>Prepare proper tensorflow and pytorch(optional). For example, GPU and CUDA10.0 (recommend to use <code>conda</code>):</p>\n<pre>conda install tensorflow-gpu<span class=\"o\">==</span><span class=\"m\">1</span>.15.0\n<span class=\"c1\"># optional</span>\n<span class=\"c1\"># conda install pytorch</span>\n</pre>\n</li>\n<li>\n<p>Install VSR package</p>\n<pre><span class=\"c1\"># For someone see this doc online</span>\n<span class=\"c1\"># git clone https://github.com/loseall/VideoSuperResolution &amp;&amp; cd VideoSuperResolution</span>\npip install -e .\n</pre>\n</li>\n</ol>\n<h3>Getting Started</h3>\n<ol>\n<li>\n<p>Download pre-trained weights and (optinal) training datasets. For instance, let's begin with VESPCN and vid4 test data:</p>\n<pre>python prepare_data.py --filter vespcn vid4\n</pre>\n</li>\n<li>\n<p>Customize backend\ncd ~/.vsr/\ntouch config.yml</p>\n<pre><span class=\"nt\">backend</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">tensorflow</span>  <span class=\"c1\"># (tensorflow, pytorch)</span>\n<span class=\"nt\">verbose</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">info</span>        <span class=\"c1\"># (debug, info, warning, error)</span>\n</pre>\n</li>\n<li>\n<p>Evaluate</p>\n<pre><span class=\"nb\">cd</span> Train\npython eval.py srcnn -t vid4 --pretrain<span class=\"o\">=</span>/path/srcnn.pth\n</pre>\n</li>\n<li>\n<p>Train</p>\n<pre>python prepare_data.py --filter mcl-v\n<span class=\"nb\">cd</span> Train\npython train.py vespcn --dataset mcl-v --memory_limit 1GB --epochs <span class=\"m\">100</span>\n</pre>\n</li>\n</ol>\n<p>OK, that's all you need. For more details, use <code>--help</code> to get more information.</p>\n<hr>\n<p>More documents can be found at <a href=\"https://github.com/LoSealL/VideoSuperResolution/tree/master/Docs\" rel=\"nofollow\">Docs</a>.</p>\n\n          </div>"}, "last_serial": 7037382, "releases": {"1.0.1": [{"comment_text": "", "digests": {"md5": "60c62575e14a9f62989d86aead852bc4", "sha256": "fc4d66244ddc2d40319a046889f72aa164194cf399865086be9671b7b1db6db3"}, "downloads": -1, "filename": "VSR-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "60c62575e14a9f62989d86aead852bc4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 35775, "upload_time": "2020-02-16T05:41:36", "upload_time_iso_8601": "2020-02-16T05:41:36.104447Z", "url": "https://files.pythonhosted.org/packages/59/3c/d879aa20d5e38090baec91188ed6673416ac7dad4857957e843d91803ce2/VSR-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7e7ef0f9345883ec0635c6ad9779815a", "sha256": "64cdc7dade7e92755e6ddfdb3bee8b7ad7c332529cccaa347123d28af51a8971"}, "downloads": -1, "filename": "VSR-1.0.1.tar.gz", "has_sig": false, "md5_digest": "7e7ef0f9345883ec0635c6ad9779815a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32314, "upload_time": "2020-02-16T05:41:40", "upload_time_iso_8601": "2020-02-16T05:41:40.065684Z", "url": "https://files.pythonhosted.org/packages/5d/c0/8fbbd6565b8067d49f4aa293d6afdfb3662ae6e7e200e96bc15a7d142dc9/VSR-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "07e8ab963a162983798c141333d62d46", "sha256": "aaae1df808583e1abcbb56f996ee99066d257f66eadd0cf6098c408dd409db85"}, "downloads": -1, "filename": "VSR-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "07e8ab963a162983798c141333d62d46", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 35810, "upload_time": "2020-02-16T09:09:06", "upload_time_iso_8601": "2020-02-16T09:09:06.105545Z", "url": "https://files.pythonhosted.org/packages/6e/ad/facebbc7b97e7a274ccec3b2c51d5d2e7955f2e0c7c27c4a013e4fe2f95a/VSR-1.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4f0c2e06e5fff039b24f33927678c9c4", "sha256": "a454bc65e2644186ceb125bf26d660b69ca04091b0598b2d6d0406fa19684ca1"}, "downloads": -1, "filename": "VSR-1.0.2.tar.gz", "has_sig": false, "md5_digest": "4f0c2e06e5fff039b24f33927678c9c4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32356, "upload_time": "2020-02-16T09:09:07", "upload_time_iso_8601": "2020-02-16T09:09:07.762722Z", "url": "https://files.pythonhosted.org/packages/db/02/9d43c8514759842c8e7a0c0085f3f914418ea38afe7da5c06acebb8f8a70/VSR-1.0.2.tar.gz", "yanked": false}], "1.0.4": [{"comment_text": "", "digests": {"md5": "ae720ec46b8443aef1967f0594afe5d0", "sha256": "906edb9fff3bf04103736fa5104b5c50aa4d6ee7eec295a07c2320d3d88b1b30"}, "downloads": -1, "filename": "VSR-1.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "ae720ec46b8443aef1967f0594afe5d0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 35762, "upload_time": "2020-04-17T04:34:49", "upload_time_iso_8601": "2020-04-17T04:34:49.557295Z", "url": "https://files.pythonhosted.org/packages/16/29/842c919e73131fa0039d3ebf67c739d4d381059db1464b445755a22e2362/VSR-1.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8055fce4c379bb52913d06cbf5ec815f", "sha256": "452d4fb11b8b70145d4a46a2126775e669c1919346421ba304125db3b3eec5b1"}, "downloads": -1, "filename": "VSR-1.0.4.tar.gz", "has_sig": false, "md5_digest": "8055fce4c379bb52913d06cbf5ec815f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32484, "upload_time": "2020-04-17T04:34:51", "upload_time_iso_8601": "2020-04-17T04:34:51.175513Z", "url": "https://files.pythonhosted.org/packages/3e/11/a56bef05630f33d8c92a764630e0763bb5ec8c624dc683b248b95768c92d/VSR-1.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ae720ec46b8443aef1967f0594afe5d0", "sha256": "906edb9fff3bf04103736fa5104b5c50aa4d6ee7eec295a07c2320d3d88b1b30"}, "downloads": -1, "filename": "VSR-1.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "ae720ec46b8443aef1967f0594afe5d0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 35762, "upload_time": "2020-04-17T04:34:49", "upload_time_iso_8601": "2020-04-17T04:34:49.557295Z", "url": "https://files.pythonhosted.org/packages/16/29/842c919e73131fa0039d3ebf67c739d4d381059db1464b445755a22e2362/VSR-1.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8055fce4c379bb52913d06cbf5ec815f", "sha256": "452d4fb11b8b70145d4a46a2126775e669c1919346421ba304125db3b3eec5b1"}, "downloads": -1, "filename": "VSR-1.0.4.tar.gz", "has_sig": false, "md5_digest": "8055fce4c379bb52913d06cbf5ec815f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32484, "upload_time": "2020-04-17T04:34:51", "upload_time_iso_8601": "2020-04-17T04:34:51.175513Z", "url": "https://files.pythonhosted.org/packages/3e/11/a56bef05630f33d8c92a764630e0763bb5ec8c624dc683b248b95768c92d/VSR-1.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:33:54 2020"}