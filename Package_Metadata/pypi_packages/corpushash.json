{"info": {"author": "bruno cuconato", "author_email": "bcclaro+corpushash@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Topic :: Software Development :: Build Tools"], "description": "##########\r\ncorpushash\r\n##########\r\n\r\nThe ``corpushash`` library enables the performance of common NLP tasks on\r\nsensitive documents without disclosing their contents. This is done by\r\nhashing every token in the corpus along with a salt.\r\n\r\nprotocol\r\n========\r\n\r\n1. the client --- in her own secure environment --- takes her classified \r\ndocuments and does the linguistic pre-processing (removal of stopwords, \r\ntokenization, etc.)\r\n\r\n2. the client then hashes the tokens: she creates random salts for every \r\ndifferent token and hashes the concatenation of token+salt. the client keeps a \r\ndictionary of the salts used for every token.\r\n\r\n3. the client sends the hashed tokens to the analyst for linguistic processing. \r\nas there is a biunivocal correspondence between every hashed token and \r\nplaintext token, NLP can occur in the same way as with any plaintext corpus.\r\n\r\n4. once the NLP is over, the analyst sends the results to the client, who then \r\nuses the dictionary to recover the plaintext tokens and thus the results' \r\nmeaning.\r\n\r\n\r\nthe library\r\n===========\r\n\r\nThe library requires as input:\r\n\r\n- a tokenized ``corpus`` as a nested list, whose elements are themselves\r\n  nested lists of the tokens of each document in the corpus.\r\n\r\n  each list corresponds to a document structure: its chapters,\r\n  paragraphs, sentences. you decide how the nested list is to be\r\n  created or structured, as long as the input is a nested list with\r\n  strings as their bottom-most elements..\r\n\r\n- ``corpus_path``, a path to a directory where the output files are to\r\n  be stored.\r\n\r\nThe output includes:\r\n\r\n- a ``.json`` file for every document in the ``corpus``, named sequentially as \r\n  positive integers, e.g., the first document being ``0.json``, stored in \r\n  ``corpus_path/public/$(timestamp-of-hash)/``.\r\n\r\n- two ``.json`` dictionaries stored in ``corpus_path/private``. they are\r\n  used to decode the ``.json`` files or the NLP results.\r\n\r\ninstall\r\n=======\r\n\r\nthe package demands python \u2a7e3, but has no external dependencies.\r\n\r\nusing pip:\r\n\r\n.. code-block:: shell\r\n\r\n    pip3 install corpushash\r\n\r\nor from repository (most up-to-date):\r\n\r\n.. code-block:: bash\r\n\r\n    pip3 install git+https://github.com/NAMD/corpushash.git\r\n\r\nor manually:\r\n\r\n.. code-block:: shell\r\n\r\n    git clone https://github.com/NAMD/corpushash.git\r\n    cd corpushash\r\n    python3 setup.py install\r\n\r\nusage\r\n=====\r\n\r\nthis will hash each word in the first four verses of the `zen of python \r\n<https://www.python.org/dev/peps/pep-0020/>`_ to the same .json document:\r\n\r\n.. code-block:: python\r\n\r\n    import corpushash as ch\r\n    example_corpus = [[\r\n                     ['Beautiful', 'is', 'better', 'than', 'ugly'],\r\n                     ['Explicit', 'is', 'better', 'than', 'implicit'],\r\n                     ['Simple', 'is', 'better', 'than', 'complex'],\r\n                     ['Complex', 'is', 'better', 'than', 'complicated'],\r\n                     ['Flat', 'is', 'better', 'than', 'nested']\r\n                     ]]\r\n    hashed_corpus = ch.CorpusHash(example_corpus, 'output_directory')\r\n\r\nthis will hash each word in the first four verses of the zen of python to **four** \r\ndifferent .json documents, as if they were different documents:\r\n\r\n.. code-block:: python\r\n\r\n    import corpushash as ch\r\n    example_corpus = [\r\n                     ['Beautiful', 'is', 'better', 'than', 'ugly'],\r\n                     ['Explicit', 'is', 'better', 'than', 'implicit'],\r\n                     ['Simple', 'is', 'better', 'than', 'complex'],\r\n                     ['Complex', 'is', 'better', 'than', 'complicated'],\r\n                     ['Flat', 'is', 'better', 'than', 'nested']\r\n                     ]\r\n    hashed_corpus = ch.CorpusHash(example_corpus, 'output_directory')\r\n\r\nso be careful when constructing your nested lists! check the tutorial at \r\n``notebooks/tutorial.ipynb``.\r\n\r\nnotes\r\n=====\r\n\r\n- probability of collision is extremely low (check the `preprint <https://peerj.com/preprints/2994/>`_), but \r\n  still we check for them, so they are not an issue.\r\n\r\n- hashing the tokens is not the same as encrypting them. as the same token \r\n  always maps to the same hash, the resulting hashed corpus is subject to \r\n  frequency analysis. even if a pre-processed text is almost uncomprehensible to \r\n  a reader (specially if stopwords are removed), there probably is still a \r\n  degree of trust in the analyst. she is usually someone who has no incentive \r\n  to attempt a decipherment of the text or someone who has a lesser (but by no \r\n  means inexisting) security clearance. this vulnerability will be investigated \r\n  in the future.\r\n\r\n- memory complexity is estimated to be at most double the size of the biggest \r\n  document in the corpus.\r\n\r\ncredits\r\n=======\r\n\r\n@odanoburu & @fccoelho\r\n\r\nlicense\r\n=======\r\n\r\nLGPL 3, check the ``LICENSE.md`` file for full content.", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/NAMD/corpushash", "keywords": "python 3,nlp,hash,hashing", "license": "", "maintainer": "", "maintainer_email": "", "name": "corpushash", "package_url": "https://pypi.org/project/corpushash/", "platform": "", "project_url": "https://pypi.org/project/corpushash/", "project_urls": {"Homepage": "https://github.com/NAMD/corpushash"}, "release_url": "https://pypi.org/project/corpushash/0.1.0/", "requires_dist": null, "requires_python": "", "summary": "Cryptographic hasher of text document corpora", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>The <tt>corpushash</tt> library enables the performance of common NLP tasks on\nsensitive documents without disclosing their contents. This is done by\nhashing every token in the corpus along with a salt.</p>\n<div id=\"protocol\">\n<h2>protocol</h2>\n<p>1. the client \u2014 in her own secure environment \u2014 takes her classified\ndocuments and does the linguistic pre-processing (removal of stopwords,\ntokenization, etc.)</p>\n<p>2. the client then hashes the tokens: she creates random salts for every\ndifferent token and hashes the concatenation of token+salt. the client keeps a\ndictionary of the salts used for every token.</p>\n<p>3. the client sends the hashed tokens to the analyst for linguistic processing.\nas there is a biunivocal correspondence between every hashed token and\nplaintext token, NLP can occur in the same way as with any plaintext corpus.</p>\n<p>4. once the NLP is over, the analyst sends the results to the client, who then\nuses the dictionary to recover the plaintext tokens and thus the results\u2019\nmeaning.</p>\n</div>\n<div id=\"the-library\">\n<h2>the library</h2>\n<p>The library requires as input:</p>\n<ul>\n<li><p>a tokenized <tt>corpus</tt> as a nested list, whose elements are themselves\nnested lists of the tokens of each document in the corpus.</p>\n<p>each list corresponds to a document structure: its chapters,\nparagraphs, sentences. you decide how the nested list is to be\ncreated or structured, as long as the input is a nested list with\nstrings as their bottom-most elements..</p>\n</li>\n<li><p><tt>corpus_path</tt>, a path to a directory where the output files are to\nbe stored.</p>\n</li>\n</ul>\n<p>The output includes:</p>\n<ul>\n<li>a <tt>.json</tt> file for every document in the <tt>corpus</tt>, named sequentially as\npositive integers, e.g., the first document being <tt>0.json</tt>, stored in\n<tt><span class=\"pre\">corpus_path/public/$(timestamp-of-hash)/</span></tt>.</li>\n<li>two <tt>.json</tt> dictionaries stored in <tt>corpus_path/private</tt>. they are\nused to decode the <tt>.json</tt> files or the NLP results.</li>\n</ul>\n</div>\n<div id=\"install\">\n<h2>install</h2>\n<p>the package demands python \u2a7e3, but has no external dependencies.</p>\n<p>using pip:</p>\n<pre>pip3 install corpushash\n</pre>\n<p>or from repository (most up-to-date):</p>\n<pre>pip3 install git+https://github.com/NAMD/corpushash.git\n</pre>\n<p>or manually:</p>\n<pre>git clone https://github.com/NAMD/corpushash.git\n<span class=\"nb\">cd</span> corpushash\npython3 setup.py install\n</pre>\n</div>\n<div id=\"usage\">\n<h2>usage</h2>\n<p>this will hash each word in the first four verses of the <a href=\"https://www.python.org/dev/peps/pep-0020/\" rel=\"nofollow\">zen of python</a> to the same .json document:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">corpushash</span> <span class=\"k\">as</span> <span class=\"nn\">ch</span>\n<span class=\"n\">example_corpus</span> <span class=\"o\">=</span> <span class=\"p\">[[</span>\n                 <span class=\"p\">[</span><span class=\"s1\">'Beautiful'</span><span class=\"p\">,</span> <span class=\"s1\">'is'</span><span class=\"p\">,</span> <span class=\"s1\">'better'</span><span class=\"p\">,</span> <span class=\"s1\">'than'</span><span class=\"p\">,</span> <span class=\"s1\">'ugly'</span><span class=\"p\">],</span>\n                 <span class=\"p\">[</span><span class=\"s1\">'Explicit'</span><span class=\"p\">,</span> <span class=\"s1\">'is'</span><span class=\"p\">,</span> <span class=\"s1\">'better'</span><span class=\"p\">,</span> <span class=\"s1\">'than'</span><span class=\"p\">,</span> <span class=\"s1\">'implicit'</span><span class=\"p\">],</span>\n                 <span class=\"p\">[</span><span class=\"s1\">'Simple'</span><span class=\"p\">,</span> <span class=\"s1\">'is'</span><span class=\"p\">,</span> <span class=\"s1\">'better'</span><span class=\"p\">,</span> <span class=\"s1\">'than'</span><span class=\"p\">,</span> <span class=\"s1\">'complex'</span><span class=\"p\">],</span>\n                 <span class=\"p\">[</span><span class=\"s1\">'Complex'</span><span class=\"p\">,</span> <span class=\"s1\">'is'</span><span class=\"p\">,</span> <span class=\"s1\">'better'</span><span class=\"p\">,</span> <span class=\"s1\">'than'</span><span class=\"p\">,</span> <span class=\"s1\">'complicated'</span><span class=\"p\">],</span>\n                 <span class=\"p\">[</span><span class=\"s1\">'Flat'</span><span class=\"p\">,</span> <span class=\"s1\">'is'</span><span class=\"p\">,</span> <span class=\"s1\">'better'</span><span class=\"p\">,</span> <span class=\"s1\">'than'</span><span class=\"p\">,</span> <span class=\"s1\">'nested'</span><span class=\"p\">]</span>\n                 <span class=\"p\">]]</span>\n<span class=\"n\">hashed_corpus</span> <span class=\"o\">=</span> <span class=\"n\">ch</span><span class=\"o\">.</span><span class=\"n\">CorpusHash</span><span class=\"p\">(</span><span class=\"n\">example_corpus</span><span class=\"p\">,</span> <span class=\"s1\">'output_directory'</span><span class=\"p\">)</span>\n</pre>\n<p>this will hash each word in the first four verses of the zen of python to <strong>four</strong>\ndifferent .json documents, as if they were different documents:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">corpushash</span> <span class=\"k\">as</span> <span class=\"nn\">ch</span>\n<span class=\"n\">example_corpus</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n                 <span class=\"p\">[</span><span class=\"s1\">'Beautiful'</span><span class=\"p\">,</span> <span class=\"s1\">'is'</span><span class=\"p\">,</span> <span class=\"s1\">'better'</span><span class=\"p\">,</span> <span class=\"s1\">'than'</span><span class=\"p\">,</span> <span class=\"s1\">'ugly'</span><span class=\"p\">],</span>\n                 <span class=\"p\">[</span><span class=\"s1\">'Explicit'</span><span class=\"p\">,</span> <span class=\"s1\">'is'</span><span class=\"p\">,</span> <span class=\"s1\">'better'</span><span class=\"p\">,</span> <span class=\"s1\">'than'</span><span class=\"p\">,</span> <span class=\"s1\">'implicit'</span><span class=\"p\">],</span>\n                 <span class=\"p\">[</span><span class=\"s1\">'Simple'</span><span class=\"p\">,</span> <span class=\"s1\">'is'</span><span class=\"p\">,</span> <span class=\"s1\">'better'</span><span class=\"p\">,</span> <span class=\"s1\">'than'</span><span class=\"p\">,</span> <span class=\"s1\">'complex'</span><span class=\"p\">],</span>\n                 <span class=\"p\">[</span><span class=\"s1\">'Complex'</span><span class=\"p\">,</span> <span class=\"s1\">'is'</span><span class=\"p\">,</span> <span class=\"s1\">'better'</span><span class=\"p\">,</span> <span class=\"s1\">'than'</span><span class=\"p\">,</span> <span class=\"s1\">'complicated'</span><span class=\"p\">],</span>\n                 <span class=\"p\">[</span><span class=\"s1\">'Flat'</span><span class=\"p\">,</span> <span class=\"s1\">'is'</span><span class=\"p\">,</span> <span class=\"s1\">'better'</span><span class=\"p\">,</span> <span class=\"s1\">'than'</span><span class=\"p\">,</span> <span class=\"s1\">'nested'</span><span class=\"p\">]</span>\n                 <span class=\"p\">]</span>\n<span class=\"n\">hashed_corpus</span> <span class=\"o\">=</span> <span class=\"n\">ch</span><span class=\"o\">.</span><span class=\"n\">CorpusHash</span><span class=\"p\">(</span><span class=\"n\">example_corpus</span><span class=\"p\">,</span> <span class=\"s1\">'output_directory'</span><span class=\"p\">)</span>\n</pre>\n<p>so be careful when constructing your nested lists! check the tutorial at\n<tt>notebooks/tutorial.ipynb</tt>.</p>\n</div>\n<div id=\"notes\">\n<h2>notes</h2>\n<ul>\n<li>probability of collision is extremely low (check the <a href=\"https://peerj.com/preprints/2994/\" rel=\"nofollow\">preprint</a>), but\nstill we check for them, so they are not an issue.</li>\n<li>hashing the tokens is not the same as encrypting them. as the same token\nalways maps to the same hash, the resulting hashed corpus is subject to\nfrequency analysis. even if a pre-processed text is almost uncomprehensible to\na reader (specially if stopwords are removed), there probably is still a\ndegree of trust in the analyst. she is usually someone who has no incentive\nto attempt a decipherment of the text or someone who has a lesser (but by no\nmeans inexisting) security clearance. this vulnerability will be investigated\nin the future.</li>\n<li>memory complexity is estimated to be at most double the size of the biggest\ndocument in the corpus.</li>\n</ul>\n</div>\n<div id=\"credits\">\n<h2>credits</h2>\n<p>@odanoburu &amp; @fccoelho</p>\n</div>\n<div id=\"license\">\n<h2>license</h2>\n<p>LGPL 3, check the <tt>LICENSE.md</tt> file for full content.</p>\n</div>\n\n          </div>"}, "last_serial": 2901250, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "a23f267cb01970ce9af0e0ce85a51eaa", "sha256": "a4e2f8c303b6bf351eb9bdfe3ae9294dea32a02199948a1a6cf80f280305ed47"}, "downloads": -1, "filename": "corpushash-0.1.0.tar.gz", "has_sig": false, "md5_digest": "a23f267cb01970ce9af0e0ce85a51eaa", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1327093, "upload_time": "2017-05-24T16:50:04", "upload_time_iso_8601": "2017-05-24T16:50:04.415237Z", "url": "https://files.pythonhosted.org/packages/bf/24/b9d8a9f156ffac5d37254a99f6398c3df670b0f530ad8fd539fec486d7f6/corpushash-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a23f267cb01970ce9af0e0ce85a51eaa", "sha256": "a4e2f8c303b6bf351eb9bdfe3ae9294dea32a02199948a1a6cf80f280305ed47"}, "downloads": -1, "filename": "corpushash-0.1.0.tar.gz", "has_sig": false, "md5_digest": "a23f267cb01970ce9af0e0ce85a51eaa", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1327093, "upload_time": "2017-05-24T16:50:04", "upload_time_iso_8601": "2017-05-24T16:50:04.415237Z", "url": "https://files.pythonhosted.org/packages/bf/24/b9d8a9f156ffac5d37254a99f6398c3df670b0f530ad8fd539fec486d7f6/corpushash-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:06 2020"}