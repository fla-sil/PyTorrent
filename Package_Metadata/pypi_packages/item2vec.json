{"info": {"author": "Johan Berdat", "author_email": "jojolebarjos@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3 :: Only", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Information Analysis", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Text Processing :: Linguistic", "Topic :: Utilities"], "description": "\n# Itemset embeddings\n\n_Disclaimer: This project is named _item2vec_, is not related to the work of\n[Barkan and Koenigstein](#ref_item2vec) and has been developed independently,\nthough the core idea is similar. Any further reference to _item2vec_ is to be\nassociated to the current project._\n\nThis is yet another variation of the well-known _word2vec_ method, proposed by\n[Mikolov et al.](#ref_word2vec), applied to unordered sequences, which commonly\nreferred as itemsets. The contribution of _item2vec_ is twofold:\n\n 1. Modifying the base algorithm to handle unordered sequences, which has an\n    impact on the definition of context windows;\n 2. Using the two embedding sets introduced in _word2vec_ for supervised\n    learning.\n\nA similar philosophy is described by [Wu et al.](#ref_starspace) in\n_StarSpace_.\n\nMore technical details are available in `./doc/main.pdf`.\n\n\n## Installation\n\nInstall from [PyPI](https://pypi.org/project/item2vec/):\n\n```\npip install item2vec\n```\n\nOr install from source, to ensure latest version:\n\n```\npip install git+https://gitlab.com/jojolebarjos/item2vec.git\n```\n\n\n## Getting started\n\nItemsets must be provided as so-called packed arrays, i.e. a pair of integer\narrays describing _indices_ and _offsets_. The index array is defined as the\nconcatenation of all N itemsets. The offset array contains the N+1 boundaries.\n\n```python\nimport numpy as np\n\nindices = np.array([\n    0, 1, 4, 7,\n    0, 1, 6,\n    2, 3, 5, 6, 7,\n], dtype=np.int32)\n\noffsets = np.array([\n    0, 4, 7, 12\n])\n```\n\nThis is similar to [compressed sparse matrices](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html):\n\n```python\nfrom scipy.sparse import csr_matrix\n\ndense = np.array([\n    [1, 1, 0, 0, 1, 0, 0, 1],\n    [1, 1, 0, 0, 0, 0, 1, 0],\n    [0, 0, 1, 1, 0, 1, 1, 1],\n], dtype=np.int32)\n\nsparse = csr_matrix(dense)\n\nassert (indices == sparse.indices).all()\nassert (offsets == sparse.indptr).all()\n\n```\n\nTraining methods do not handle other data types. Also note that:\n\n * indices start at 0;\n * item order in an itemset is not important;\n * an itemset can contain duplicated items;\n * itemsets order is not important;\n * there is no weight associated to items, nor itemsets.\n\nHowever, a small helper is provided for simple cases:\n\n```python\nfrom item2vec import pack_itemsets\n\nitemsets = [\n    ['apple', 'sugar', 'flour'],\n    ['pear', 'sugar', 'flour', 'butter'],\n    ['apple', 'pear', 'sugar', 'buffer', 'cinnamon'],\n    ['salt', 'flour', 'oil'],\n    # ...\n]\n\nlabels, indices, offsets = pack_itemsets(itemsets, min_count=2)\nnum_label = len(labels)\n```\n\nThe next step is to define at least one task. For now, let us stick to the\nunsupervised case, where co-occurrence is used as knowledge source. This is\nsimilar to the continuous bag-of-word and continuous skip-gram tasks defined\nin _word2vec_.\n\nFirst, two embedding sets must be allocated. Both should capture the same\ninformation, and one is the complement of the other. This is a not-so\ndocumented question of _word2vec_, but empirical results have shown that it is\nbetter than reusing the same set twice.\n\n```python\nfrom item2vec import initialize_syn\n\nnum_dimension = 64\nsyn0 = initialize_syn(num_label, num_dimension)\nsyn1 = initialize_syn(num_label, num_dimension)\n```\n\nSecond, define a task object that holds all the descriptors:\n\n```python\nfrom item2vec import UnsupervisedTask\n\ntask = UnsupervisedTask(indices, offsets, syn0, syn1, num_negative=5)\n```\n\nThird, the `do_batch`method must be invoked multiple times, until convergence.\nAnother helper is provided to handle the training loop. Note that, due to a\ndifferent sampling strategy, a larger number of iteration is needed.\n\n```python\nfrom item2vec import train\n\ntrain(task, num_epoch=100)\n```\n\nThe full code is therefore as follows:\n\n```python\nimport numpy as np\n\nfrom item2vec import (\n    pack_itemsets,\n    initialize_syn,\n    UnsupervisedTask,\n    train,\n)\n\n# Get your own itemsets\nitemsets = [\n    ['apple', 'sugar', 'flour'],\n    ['pear', 'sugar', 'flour', 'butter'],\n    ['apple', 'pear', 'sugar', 'buffer', 'cinnamon'],\n    ['salt', 'flour', 'oil'],\n    # ...\n]\n\n# Pack itemsets into contiguous arrays\nlabels, indices, offsets = pack_itemsets(itemsets, min_count=2)\nnum_label = len(labels)\n\n# Initialize embeddings sets from uniform distribution\nnum_dimension = 64\nsyn0 = initialize_syn(num_label, num_dimension)\nsyn1 = initialize_syn(num_label, num_dimension)\n\n# Define unsupervised task, i.e. using co-occurrences\ntask = UnsupervisedTask(indices, offsets, syn0, syn1, num_negative=5)\n\n# Do training\n# Note: due to a different sampling strategy, more epochs than word2vec are needed\ntrain(task, num_epoch=100)\n\n# Both embedding sets are equivalent, just choose one of them\nsyn = syn0\n```\n\nMore examples can be found in `./example/`.\n\n\n## Performance improvement\n\nAs [suggested](https://numba.pydata.org/numba-doc/dev/user/performance-tips.html#intel-svml) in Numba's documentation, Intel's short vector math library can be used to increase performances:\n\n```\nconda install -c numba icc_rt\n```\n\n\n## References\n\n<ol>\n    <li id=\"ref_word2vec\">\n        <i>Efficient Estimation of Word Representations in Vector Space</i>,\n        2013,\n        Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean,\n        https://arxiv.org/abs/1301.3781\n    </li>\n    <li id=\"ref_starspace\">\n        <i>StarSpace: Embed All The Things!</i>,\n        2017,\n        Ledell Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, Jason Weston,\n        https://arxiv.org/abs/1709.03856\n    </li>\n    <li id=\"ref_item2vec\">\n        <i>Item2Vec: Neural Item Embedding for Collaborative Filtering</i>,\n        Oren Barkan, Noam Koenigstein,\n        https://arxiv.org/abs/1603.04259\n    </li>\n</ol>\n\n\n## Changelog\n\n * 0.4.0 - 2020-05-04\n    * Refactor to make training task explicit\n    * Add supervised task\n * 0.3.0 - 2020-03-26\n    * Complete refactor to increase performances and reusability\n * 0.2.1 - 2020-03-24\n    * Allow keyboard interruption\n    * Fix label count argument\n    * Fix learning rate issue\n    * Add optimization flags to Numba JIT\n * 0.2.0 - 2019-11-08\n    * Clean and refactor\n    * Allow training from plain arrays\n * 0.1.0 - 2019-09-13\n    * Initial version\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://gitlab.com/jojolebarjos/item2vec", "keywords": "itemset,word2vec,embedding", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "item2vec", "package_url": "https://pypi.org/project/item2vec/", "platform": "", "project_url": "https://pypi.org/project/item2vec/", "project_urls": {"Homepage": "https://gitlab.com/jojolebarjos/item2vec"}, "release_url": "https://pypi.org/project/item2vec/0.4.0/", "requires_dist": ["numba", "numpy", "tqdm"], "requires_python": "", "summary": "word2vec for itemsets", "version": "0.4.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Itemset embeddings</h1>\n<p><em>Disclaimer: This project is named <em>item2vec</em>, is not related to the work of\n<a href=\"#ref_item2vec\" rel=\"nofollow\">Barkan and Koenigstein</a> and has been developed independently,\nthough the core idea is similar. Any further reference to <em>item2vec</em> is to be\nassociated to the current project.</em></p>\n<p>This is yet another variation of the well-known <em>word2vec</em> method, proposed by\n<a href=\"#ref_word2vec\" rel=\"nofollow\">Mikolov et al.</a>, applied to unordered sequences, which commonly\nreferred as itemsets. The contribution of <em>item2vec</em> is twofold:</p>\n<ol>\n<li>Modifying the base algorithm to handle unordered sequences, which has an\nimpact on the definition of context windows;</li>\n<li>Using the two embedding sets introduced in <em>word2vec</em> for supervised\nlearning.</li>\n</ol>\n<p>A similar philosophy is described by <a href=\"#ref_starspace\" rel=\"nofollow\">Wu et al.</a> in\n<em>StarSpace</em>.</p>\n<p>More technical details are available in <code>./doc/main.pdf</code>.</p>\n<h2>Installation</h2>\n<p>Install from <a href=\"https://pypi.org/project/item2vec/\" rel=\"nofollow\">PyPI</a>:</p>\n<pre><code>pip install item2vec\n</code></pre>\n<p>Or install from source, to ensure latest version:</p>\n<pre><code>pip install git+https://gitlab.com/jojolebarjos/item2vec.git\n</code></pre>\n<h2>Getting started</h2>\n<p>Itemsets must be provided as so-called packed arrays, i.e. a pair of integer\narrays describing <em>indices</em> and <em>offsets</em>. The index array is defined as the\nconcatenation of all N itemsets. The offset array contains the N+1 boundaries.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n\n<span class=\"n\">indices</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span>\n    <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">,</span>\n    <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span>\n    <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">,</span>\n<span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span>\n\n<span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span>\n    <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">12</span>\n<span class=\"p\">])</span>\n</pre>\n<p>This is similar to <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\" rel=\"nofollow\">compressed sparse matrices</a>:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">scipy.sparse</span> <span class=\"kn\">import</span> <span class=\"n\">csr_matrix</span>\n\n<span class=\"n\">dense</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span>\n    <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n<span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span>\n\n<span class=\"n\">sparse</span> <span class=\"o\">=</span> <span class=\"n\">csr_matrix</span><span class=\"p\">(</span><span class=\"n\">dense</span><span class=\"p\">)</span>\n\n<span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">indices</span> <span class=\"o\">==</span> <span class=\"n\">sparse</span><span class=\"o\">.</span><span class=\"n\">indices</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">all</span><span class=\"p\">()</span>\n<span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">offsets</span> <span class=\"o\">==</span> <span class=\"n\">sparse</span><span class=\"o\">.</span><span class=\"n\">indptr</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">all</span><span class=\"p\">()</span>\n</pre>\n<p>Training methods do not handle other data types. Also note that:</p>\n<ul>\n<li>indices start at 0;</li>\n<li>item order in an itemset is not important;</li>\n<li>an itemset can contain duplicated items;</li>\n<li>itemsets order is not important;</li>\n<li>there is no weight associated to items, nor itemsets.</li>\n</ul>\n<p>However, a small helper is provided for simple cases:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">item2vec</span> <span class=\"kn\">import</span> <span class=\"n\">pack_itemsets</span>\n\n<span class=\"n\">itemsets</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s1\">'apple'</span><span class=\"p\">,</span> <span class=\"s1\">'sugar'</span><span class=\"p\">,</span> <span class=\"s1\">'flour'</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'pear'</span><span class=\"p\">,</span> <span class=\"s1\">'sugar'</span><span class=\"p\">,</span> <span class=\"s1\">'flour'</span><span class=\"p\">,</span> <span class=\"s1\">'butter'</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'apple'</span><span class=\"p\">,</span> <span class=\"s1\">'pear'</span><span class=\"p\">,</span> <span class=\"s1\">'sugar'</span><span class=\"p\">,</span> <span class=\"s1\">'buffer'</span><span class=\"p\">,</span> <span class=\"s1\">'cinnamon'</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'salt'</span><span class=\"p\">,</span> <span class=\"s1\">'flour'</span><span class=\"p\">,</span> <span class=\"s1\">'oil'</span><span class=\"p\">],</span>\n    <span class=\"c1\"># ...</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">indices</span><span class=\"p\">,</span> <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">pack_itemsets</span><span class=\"p\">(</span><span class=\"n\">itemsets</span><span class=\"p\">,</span> <span class=\"n\">min_count</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">num_label</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">)</span>\n</pre>\n<p>The next step is to define at least one task. For now, let us stick to the\nunsupervised case, where co-occurrence is used as knowledge source. This is\nsimilar to the continuous bag-of-word and continuous skip-gram tasks defined\nin <em>word2vec</em>.</p>\n<p>First, two embedding sets must be allocated. Both should capture the same\ninformation, and one is the complement of the other. This is a not-so\ndocumented question of <em>word2vec</em>, but empirical results have shown that it is\nbetter than reusing the same set twice.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">item2vec</span> <span class=\"kn\">import</span> <span class=\"n\">initialize_syn</span>\n\n<span class=\"n\">num_dimension</span> <span class=\"o\">=</span> <span class=\"mi\">64</span>\n<span class=\"n\">syn0</span> <span class=\"o\">=</span> <span class=\"n\">initialize_syn</span><span class=\"p\">(</span><span class=\"n\">num_label</span><span class=\"p\">,</span> <span class=\"n\">num_dimension</span><span class=\"p\">)</span>\n<span class=\"n\">syn1</span> <span class=\"o\">=</span> <span class=\"n\">initialize_syn</span><span class=\"p\">(</span><span class=\"n\">num_label</span><span class=\"p\">,</span> <span class=\"n\">num_dimension</span><span class=\"p\">)</span>\n</pre>\n<p>Second, define a task object that holds all the descriptors:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">item2vec</span> <span class=\"kn\">import</span> <span class=\"n\">UnsupervisedTask</span>\n\n<span class=\"n\">task</span> <span class=\"o\">=</span> <span class=\"n\">UnsupervisedTask</span><span class=\"p\">(</span><span class=\"n\">indices</span><span class=\"p\">,</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">syn0</span><span class=\"p\">,</span> <span class=\"n\">syn1</span><span class=\"p\">,</span> <span class=\"n\">num_negative</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n</pre>\n<p>Third, the <code>do_batch</code>method must be invoked multiple times, until convergence.\nAnother helper is provided to handle the training loop. Note that, due to a\ndifferent sampling strategy, a larger number of iteration is needed.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">item2vec</span> <span class=\"kn\">import</span> <span class=\"n\">train</span>\n\n<span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">task</span><span class=\"p\">,</span> <span class=\"n\">num_epoch</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n</pre>\n<p>The full code is therefore as follows:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">item2vec</span> <span class=\"kn\">import</span> <span class=\"p\">(</span>\n    <span class=\"n\">pack_itemsets</span><span class=\"p\">,</span>\n    <span class=\"n\">initialize_syn</span><span class=\"p\">,</span>\n    <span class=\"n\">UnsupervisedTask</span><span class=\"p\">,</span>\n    <span class=\"n\">train</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Get your own itemsets</span>\n<span class=\"n\">itemsets</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s1\">'apple'</span><span class=\"p\">,</span> <span class=\"s1\">'sugar'</span><span class=\"p\">,</span> <span class=\"s1\">'flour'</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'pear'</span><span class=\"p\">,</span> <span class=\"s1\">'sugar'</span><span class=\"p\">,</span> <span class=\"s1\">'flour'</span><span class=\"p\">,</span> <span class=\"s1\">'butter'</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'apple'</span><span class=\"p\">,</span> <span class=\"s1\">'pear'</span><span class=\"p\">,</span> <span class=\"s1\">'sugar'</span><span class=\"p\">,</span> <span class=\"s1\">'buffer'</span><span class=\"p\">,</span> <span class=\"s1\">'cinnamon'</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'salt'</span><span class=\"p\">,</span> <span class=\"s1\">'flour'</span><span class=\"p\">,</span> <span class=\"s1\">'oil'</span><span class=\"p\">],</span>\n    <span class=\"c1\"># ...</span>\n<span class=\"p\">]</span>\n\n<span class=\"c1\"># Pack itemsets into contiguous arrays</span>\n<span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">indices</span><span class=\"p\">,</span> <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">pack_itemsets</span><span class=\"p\">(</span><span class=\"n\">itemsets</span><span class=\"p\">,</span> <span class=\"n\">min_count</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">num_label</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Initialize embeddings sets from uniform distribution</span>\n<span class=\"n\">num_dimension</span> <span class=\"o\">=</span> <span class=\"mi\">64</span>\n<span class=\"n\">syn0</span> <span class=\"o\">=</span> <span class=\"n\">initialize_syn</span><span class=\"p\">(</span><span class=\"n\">num_label</span><span class=\"p\">,</span> <span class=\"n\">num_dimension</span><span class=\"p\">)</span>\n<span class=\"n\">syn1</span> <span class=\"o\">=</span> <span class=\"n\">initialize_syn</span><span class=\"p\">(</span><span class=\"n\">num_label</span><span class=\"p\">,</span> <span class=\"n\">num_dimension</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Define unsupervised task, i.e. using co-occurrences</span>\n<span class=\"n\">task</span> <span class=\"o\">=</span> <span class=\"n\">UnsupervisedTask</span><span class=\"p\">(</span><span class=\"n\">indices</span><span class=\"p\">,</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">syn0</span><span class=\"p\">,</span> <span class=\"n\">syn1</span><span class=\"p\">,</span> <span class=\"n\">num_negative</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Do training</span>\n<span class=\"c1\"># Note: due to a different sampling strategy, more epochs than word2vec are needed</span>\n<span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">task</span><span class=\"p\">,</span> <span class=\"n\">num_epoch</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Both embedding sets are equivalent, just choose one of them</span>\n<span class=\"n\">syn</span> <span class=\"o\">=</span> <span class=\"n\">syn0</span>\n</pre>\n<p>More examples can be found in <code>./example/</code>.</p>\n<h2>Performance improvement</h2>\n<p>As <a href=\"https://numba.pydata.org/numba-doc/dev/user/performance-tips.html#intel-svml\" rel=\"nofollow\">suggested</a> in Numba's documentation, Intel's short vector math library can be used to increase performances:</p>\n<pre><code>conda install -c numba icc_rt\n</code></pre>\n<h2>References</h2>\n<ol>\n    <li id=\"ref_word2vec\">\n        <i>Efficient Estimation of Word Representations in Vector Space</i>,\n        2013,\n        Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean,\n        https://arxiv.org/abs/1301.3781\n    </li>\n    <li id=\"ref_starspace\">\n        <i>StarSpace: Embed All The Things!</i>,\n        2017,\n        Ledell Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, Jason Weston,\n        https://arxiv.org/abs/1709.03856\n    </li>\n    <li id=\"ref_item2vec\">\n        <i>Item2Vec: Neural Item Embedding for Collaborative Filtering</i>,\n        Oren Barkan, Noam Koenigstein,\n        https://arxiv.org/abs/1603.04259\n    </li>\n</ol>\n<h2>Changelog</h2>\n<ul>\n<li>0.4.0 - 2020-05-04\n<ul>\n<li>Refactor to make training task explicit</li>\n<li>Add supervised task</li>\n</ul>\n</li>\n<li>0.3.0 - 2020-03-26\n<ul>\n<li>Complete refactor to increase performances and reusability</li>\n</ul>\n</li>\n<li>0.2.1 - 2020-03-24\n<ul>\n<li>Allow keyboard interruption</li>\n<li>Fix label count argument</li>\n<li>Fix learning rate issue</li>\n<li>Add optimization flags to Numba JIT</li>\n</ul>\n</li>\n<li>0.2.0 - 2019-11-08\n<ul>\n<li>Clean and refactor</li>\n<li>Allow training from plain arrays</li>\n</ul>\n</li>\n<li>0.1.0 - 2019-09-13\n<ul>\n<li>Initial version</li>\n</ul>\n</li>\n</ul>\n\n          </div>"}, "last_serial": 7170880, "releases": {"0.4.0": [{"comment_text": "", "digests": {"md5": "dfd63b68c941de8a371e05e3fe845821", "sha256": "6de16b558e59cbafe40d9ad51c9e50c81f2c4b9fc384733a05fe35553c385f48"}, "downloads": -1, "filename": "item2vec-0.4.0-py3-none-any.whl", "has_sig": false, "md5_digest": "dfd63b68c941de8a371e05e3fe845821", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9963, "upload_time": "2020-05-05T10:08:40", "upload_time_iso_8601": "2020-05-05T10:08:40.524306Z", "url": "https://files.pythonhosted.org/packages/a6/09/cfc2f61db46f1d12efc8ae448ecd103bd20a0401d610f92e633cb4822cbf/item2vec-0.4.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "78ebd6afaa69a4899e4f07bf9b2cd214", "sha256": "408ef1829f41ffc7e2b5537924f7986d86ffb071c57381d77941f8794431fa87"}, "downloads": -1, "filename": "item2vec-0.4.0.tar.gz", "has_sig": false, "md5_digest": "78ebd6afaa69a4899e4f07bf9b2cd214", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8665, "upload_time": "2020-05-05T10:08:42", "upload_time_iso_8601": "2020-05-05T10:08:42.273664Z", "url": "https://files.pythonhosted.org/packages/9c/d7/bab915f77e4fa22f252a550ac96e837b11c18bfe549de32869cfd1bfcded/item2vec-0.4.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "dfd63b68c941de8a371e05e3fe845821", "sha256": "6de16b558e59cbafe40d9ad51c9e50c81f2c4b9fc384733a05fe35553c385f48"}, "downloads": -1, "filename": "item2vec-0.4.0-py3-none-any.whl", "has_sig": false, "md5_digest": "dfd63b68c941de8a371e05e3fe845821", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9963, "upload_time": "2020-05-05T10:08:40", "upload_time_iso_8601": "2020-05-05T10:08:40.524306Z", "url": "https://files.pythonhosted.org/packages/a6/09/cfc2f61db46f1d12efc8ae448ecd103bd20a0401d610f92e633cb4822cbf/item2vec-0.4.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "78ebd6afaa69a4899e4f07bf9b2cd214", "sha256": "408ef1829f41ffc7e2b5537924f7986d86ffb071c57381d77941f8794431fa87"}, "downloads": -1, "filename": "item2vec-0.4.0.tar.gz", "has_sig": false, "md5_digest": "78ebd6afaa69a4899e4f07bf9b2cd214", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8665, "upload_time": "2020-05-05T10:08:42", "upload_time_iso_8601": "2020-05-05T10:08:42.273664Z", "url": "https://files.pythonhosted.org/packages/9c/d7/bab915f77e4fa22f252a550ac96e837b11c18bfe549de32869cfd1bfcded/item2vec-0.4.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:53:49 2020"}