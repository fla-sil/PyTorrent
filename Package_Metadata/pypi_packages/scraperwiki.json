{"info": {"author": "Francis Irving", "author_email": "francis@scraperwiki.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.4", "Programming Language :: SQL", "Topic :: Database :: Front-Ends"], "description": "ScraperWiki Python library\n==========================\n\n.. image:: https://travis-ci.org/scraperwiki/scraperwiki-python.png?branch=master\n   :alt: Build Status\n   :target: https://travis-ci.org/scraperwiki/scraperwiki-python\n\nThis is a Python library for scraping web pages and saving data.\nIt is the easiest way to save data on the ScraperWiki platform, and it\ncan also be used locally or on your own servers.\n\nInstalling\n----------\n\n::\n\n   pip install scraperwiki\n\nScraping\n--------\n\nscraperwiki.scrape(url[, params][,user_agent])\n  Returns the downloaded string from the given url.\n\n  ``params`` are sent as a POST if set.\n\n  ``user_agent`` sets the user-agent string if provided.\n\nSaving data\n-----------\n\nHelper functions for saving and querying an SQL database. Updates\nthe schema automatically according to the data you save.\n\nCurrently only supports SQLite. It will make a local SQLite database.\nIt is based on [SQLAlchemy](https://pypi.python.org/pypi/SQLAlchemy).\nYou should expect it to support other SQL databases at a later date.\n\nscraperwiki.sql.save(unique_keys, data[, table_name=\"swdata\"])\n  Saves a data record into the datastore into the table given by ``table_name``.\n\n  ``data`` is a dict object with field names as keys; ``unique_keys`` is a subset of data.keys() which determines when a record is overwritten. For large numbers of records `data` can be a list of dicts.\n\n  ``scraperwiki.sql.save`` is entitled to buffer an arbitrary number of\n  rows until the next read via the ScraperWiki API, an exception is hit,\n  or until process exit. An effort is made to do a timely periodic flush.\n  Records can be lost if the process experiences a hard-crash, power outage\n  or SIGKILL due to high memory usage during an out-of-memory condition. The\n  buffer can be manually flushed with ``scraperwiki.sql.flush()``.\n\nscraperwiki.sql.execute(sql[, vars])\n  Executes any arbitrary SQL command. For example CREATE, DELETE, INSERT or DROP.\n\n  ``vars`` is an optional list of parameters, inserted when the SQL command contains \u2018?\u2019s. For example::\n\n    scraperwiki.sql.execute(\"INSERT INTO swdata VALUES (?,?,?)\", [a,b,c])\n\n  The \u2018?\u2019 convention is like \"paramstyle qmark\" from `Python's DB API 2.0 <http://www.python.org/dev/peps/pep-0249/>`_ (but note that the API to the datastore is nothing like Python's DB API).  In particular the \u2018?\u2019 does not itself need quoting, and can in general only be used where a literal would appear. (Note that you cannot substitute in, for example, table or column names.)\n\nscraperwiki.sql.select(sqlfrag[, vars])\n  Executes a select command on the datastore.  For example::\n\n    scraperwiki.sql.select(\"* FROM swdata LIMIT 10\")\n\n  Returns a list of dicts that have been selected.\n\n  ``vars`` is an optional list of parameters, inserted when the select command contains \u2018?\u2019s.  This is like the feature in the ``.execute`` command, above.\n\nscraperwiki.sql.commit()\n  Commits to the file after a series of execute commands. (sql.save auto-commits after every action).\n\nscraperwiki.sql.show_tables([dbname])\n  Returns an array of tables and their schemas in the current database.\n\nscraperwiki.sql.table_info(name)\n  Returns an array of attributes for each element of the table.\n\nscraperwiki.sql.save_var(key, value)\n  Saves an arbitrary single-value into a table called ``swvariables``. Intended to store scraper state so that a scraper can continue after an interruption.\n\nscraperwiki.sql.get_var(key[, default])\n  Retrieves a single value that was saved by ``save_var``. Only works for string, float, or int types. For anything else, use the `pickle library <http://docs.python.org/library/pickle.html>`_ to turn it into a string.\n\nMiscellaneous\n-------------\n\nscraperwiki.status(type, message=None)\n  If run on the ScraperWiki platform (the new one, not Classic), updates the visible status of the dataset.  If not on the platform, does nothing. ``params`` can be 'ok' or 'error'. If no ``message`` is given, it will show the time since the update. See `dataset status API <https://scraperwiki.com/help/developer#boxes-status>`_ in the documentation for details.\n\nscraperwiki.pdftoxml(pdfdata)\n  Convert a byte string containing a PDF file into an XML file containing the coordinates and font of each text string (see `the pdftohtml documentation <http://linux.die.net/man/1/pdftohtml>`_ for details).\n\nEnvironment Variables\n---------------------\n\nSCRAPERWIKI_DATABASE_NAME\n  default: ``scraperwiki.sqlite`` - name of database\n\nSCRAPERWIKI_DATABASE_TIMEOUT\n  default: ``300`` - number of seconds database will wait for a lock\n", "description_content_type": null, "docs_url": null, "download_url": null, "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/scraperwiki/scraperwiki-python", "keywords": null, "license": "GPL", "maintainer": null, "maintainer_email": null, "name": "scraperwiki", "package_url": "https://pypi.org/project/scraperwiki/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/scraperwiki/", "project_urls": {"Homepage": "https://github.com/scraperwiki/scraperwiki-python"}, "release_url": "https://pypi.org/project/scraperwiki/0.5.1/", "requires_dist": null, "requires_python": null, "summary": "Local version of ScraperWiki libraries", "version": "0.5.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://travis-ci.org/scraperwiki/scraperwiki-python\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/24abbba5fc13a04c11d0eeca653bd3005f1fb055/68747470733a2f2f7472617669732d63692e6f72672f7363726170657277696b692f7363726170657277696b692d707974686f6e2e706e673f6272616e63683d6d6173746572\"></a>\n<p>This is a Python library for scraping web pages and saving data.\nIt is the easiest way to save data on the ScraperWiki platform, and it\ncan also be used locally or on your own servers.</p>\n<div id=\"installing\">\n<h2>Installing</h2>\n<pre>pip install scraperwiki\n</pre>\n</div>\n<div id=\"scraping\">\n<h2>Scraping</h2>\n<dl>\n<dt>scraperwiki.scrape(url[, params][,user_agent])</dt>\n<dd><p>Returns the downloaded string from the given url.</p>\n<p><tt>params</tt> are sent as a POST if set.</p>\n<p><tt>user_agent</tt> sets the user-agent string if provided.</p>\n</dd>\n</dl>\n</div>\n<div id=\"saving-data\">\n<h2>Saving data</h2>\n<p>Helper functions for saving and querying an SQL database. Updates\nthe schema automatically according to the data you save.</p>\n<p>Currently only supports SQLite. It will make a local SQLite database.\nIt is based on [SQLAlchemy](<a href=\"https://pypi.python.org/pypi/SQLAlchemy\" rel=\"nofollow\">https://pypi.python.org/pypi/SQLAlchemy</a>).\nYou should expect it to support other SQL databases at a later date.</p>\n<dl>\n<dt>scraperwiki.sql.save(unique_keys, data[, table_name=\u201dswdata\u201d])</dt>\n<dd><p>Saves a data record into the datastore into the table given by <tt>table_name</tt>.</p>\n<p><tt>data</tt> is a dict object with field names as keys; <tt>unique_keys</tt> is a subset of data.keys() which determines when a record is overwritten. For large numbers of records <cite>data</cite> can be a list of dicts.</p>\n<p><tt>scraperwiki.sql.save</tt> is entitled to buffer an arbitrary number of\nrows until the next read via the ScraperWiki API, an exception is hit,\nor until process exit. An effort is made to do a timely periodic flush.\nRecords can be lost if the process experiences a hard-crash, power outage\nor SIGKILL due to high memory usage during an out-of-memory condition. The\nbuffer can be manually flushed with <tt>scraperwiki.sql.flush()</tt>.</p>\n</dd>\n<dt>scraperwiki.sql.execute(sql[, vars])</dt>\n<dd><p>Executes any arbitrary SQL command. For example CREATE, DELETE, INSERT or DROP.</p>\n<p><tt>vars</tt> is an optional list of parameters, inserted when the SQL command contains \u2018?\u2019s. For example:</p>\n<pre>scraperwiki.sql.execute(\"INSERT INTO swdata VALUES (?,?,?)\", [a,b,c])\n</pre>\n<p>The \u2018?\u2019 convention is like \u201cparamstyle qmark\u201d from <a href=\"http://www.python.org/dev/peps/pep-0249/\" rel=\"nofollow\">Python\u2019s DB API 2.0</a> (but note that the API to the datastore is nothing like Python\u2019s DB API).  In particular the \u2018?\u2019 does not itself need quoting, and can in general only be used where a literal would appear. (Note that you cannot substitute in, for example, table or column names.)</p>\n</dd>\n<dt>scraperwiki.sql.select(sqlfrag[, vars])</dt>\n<dd><p>Executes a select command on the datastore.  For example:</p>\n<pre>scraperwiki.sql.select(\"* FROM swdata LIMIT 10\")\n</pre>\n<p>Returns a list of dicts that have been selected.</p>\n<p><tt>vars</tt> is an optional list of parameters, inserted when the select command contains \u2018?\u2019s.  This is like the feature in the <tt>.execute</tt> command, above.</p>\n</dd>\n<dt>scraperwiki.sql.commit()</dt>\n<dd>Commits to the file after a series of execute commands. (sql.save auto-commits after every action).</dd>\n<dt>scraperwiki.sql.show_tables([dbname])</dt>\n<dd>Returns an array of tables and their schemas in the current database.</dd>\n<dt>scraperwiki.sql.table_info(name)</dt>\n<dd>Returns an array of attributes for each element of the table.</dd>\n<dt>scraperwiki.sql.save_var(key, value)</dt>\n<dd>Saves an arbitrary single-value into a table called <tt>swvariables</tt>. Intended to store scraper state so that a scraper can continue after an interruption.</dd>\n<dt>scraperwiki.sql.get_var(key[, default])</dt>\n<dd>Retrieves a single value that was saved by <tt>save_var</tt>. Only works for string, float, or int types. For anything else, use the <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow\">pickle library</a> to turn it into a string.</dd>\n</dl>\n</div>\n<div id=\"miscellaneous\">\n<h2>Miscellaneous</h2>\n<dl>\n<dt>scraperwiki.status(type, message=None)</dt>\n<dd>If run on the ScraperWiki platform (the new one, not Classic), updates the visible status of the dataset.  If not on the platform, does nothing. <tt>params</tt> can be \u2018ok\u2019 or \u2018error\u2019. If no <tt>message</tt> is given, it will show the time since the update. See <a href=\"https://scraperwiki.com/help/developer#boxes-status\" rel=\"nofollow\">dataset status API</a> in the documentation for details.</dd>\n<dt>scraperwiki.pdftoxml(pdfdata)</dt>\n<dd>Convert a byte string containing a PDF file into an XML file containing the coordinates and font of each text string (see <a href=\"http://linux.die.net/man/1/pdftohtml\" rel=\"nofollow\">the pdftohtml documentation</a> for details).</dd>\n</dl>\n</div>\n<div id=\"environment-variables\">\n<h2>Environment Variables</h2>\n<dl>\n<dt>SCRAPERWIKI_DATABASE_NAME</dt>\n<dd>default: <tt>scraperwiki.sqlite</tt> - name of database</dd>\n<dt>SCRAPERWIKI_DATABASE_TIMEOUT</dt>\n<dd>default: <tt>300</tt> - number of seconds database will wait for a lock</dd>\n</dl>\n</div>\n\n          </div>"}, "last_serial": 5209160, "releases": {"0.2.0": [{"comment_text": "", "digests": {"md5": "6ad35a16530a7112739f6e8f24a280f5", "sha256": "a281d24757de0da8e357763f4fe35e40e18e2832637a19c5e1be837823aab509"}, "downloads": -1, "filename": "scraperwiki-0.2.0.tar.gz", "has_sig": false, "md5_digest": "6ad35a16530a7112739f6e8f24a280f5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15125, "upload_time": "2013-01-29T11:09:20", "upload_time_iso_8601": "2013-01-29T11:09:20.291750Z", "url": "https://files.pythonhosted.org/packages/05/80/1e371f46fe32f86e1bde1c6f6d583465df636aee247d014259ba19d6a700/scraperwiki-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "6fa9407c20044f298fcd303306f6ab66", "sha256": "b38a8a53f3415fa9594edb06c7913e728780ae0b259268cba499c732b48c256b"}, "downloads": -1, "filename": "scraperwiki-0.2.1.tar.gz", "has_sig": false, "md5_digest": "6fa9407c20044f298fcd303306f6ab66", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15093, "upload_time": "2013-02-18T17:32:35", "upload_time_iso_8601": "2013-02-18T17:32:35.530105Z", "url": "https://files.pythonhosted.org/packages/2d/ab/0c46b546bec66827b1d6ad58893d6e687790cdf5f92e49854bd28888d691/scraperwiki-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "b0f5abbe5bcd5b85f18644092554d9ca", "sha256": "654cbf1145fd3a9bf44ece891692a158be6d8c7d978ce49ef1c0b39ccb35bfec"}, "downloads": -1, "filename": "scraperwiki-0.2.2.tar.gz", "has_sig": false, "md5_digest": "b0f5abbe5bcd5b85f18644092554d9ca", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15093, "upload_time": "2013-02-18T17:52:43", "upload_time_iso_8601": "2013-02-18T17:52:43.455431Z", "url": "https://files.pythonhosted.org/packages/0a/04/c215d5ff91319464dfbc318ca2fb8cb4b277f548d6b54a36f4c97610ae4d/scraperwiki-0.2.2.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "450277f935566b5486ab76d1cc0ec04f", "sha256": "e8b61c1e5d184b4d3e8def663298bcdf8bad97279664f4913398da5aa8d8c5ba"}, "downloads": -1, "filename": "scraperwiki-0.3.0.tar.gz", "has_sig": false, "md5_digest": "450277f935566b5486ab76d1cc0ec04f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4937, "upload_time": "2013-03-28T12:00:29", "upload_time_iso_8601": "2013-03-28T12:00:29.163303Z", "url": "https://files.pythonhosted.org/packages/97/4c/a33533d2af20dd99b9bb866f4083d5d4179d97cd04dffd90fa46d8b41e68/scraperwiki-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "d2e8540b81681f22d3d398293f4dda84", "sha256": "bdc199aef3ea45ec884f1584ec897e633d4d23cb11efd3ebf62623e37c059e1b"}, "downloads": -1, "filename": "scraperwiki-0.3.1.tar.gz", "has_sig": false, "md5_digest": "d2e8540b81681f22d3d398293f4dda84", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4759, "upload_time": "2013-04-02T11:10:40", "upload_time_iso_8601": "2013-04-02T11:10:40.751350Z", "url": "https://files.pythonhosted.org/packages/80/d2/31ca6f2fd08de6c0cc7179beea0b2089f6409ae0d19a28ccf97802275d3a/scraperwiki-0.3.1.tar.gz", "yanked": false}], "0.3.10": [{"comment_text": "", "digests": {"md5": "9d235a38eed2d265767c2ac67a2c8eeb", "sha256": "b4477ad8f2fc6f9ba2d1b29322e8cbe81a9539cb100581bad02833de4cddb297"}, "downloads": -1, "filename": "scraperwiki-0.3.10.tar.gz", "has_sig": false, "md5_digest": "9d235a38eed2d265767c2ac67a2c8eeb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6294, "upload_time": "2014-05-19T09:17:43", "upload_time_iso_8601": "2014-05-19T09:17:43.966269Z", "url": "https://files.pythonhosted.org/packages/ca/f0/b5c2b5bdc84c80966644e883735dffb737f96462e94e41eda3f33f005064/scraperwiki-0.3.10.tar.gz", "yanked": false}], "0.3.11": [{"comment_text": "", "digests": {"md5": "c6ca445bc50096607ec6875b9b52b3f8", "sha256": "42eeb305bb4f27b3dc41f0f9d70a930ba90b26905bc96ab988c07ff7e23c4212"}, "downloads": -1, "filename": "scraperwiki-0.3.11.tar.gz", "has_sig": false, "md5_digest": "c6ca445bc50096607ec6875b9b52b3f8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5868, "upload_time": "2014-07-11T09:58:44", "upload_time_iso_8601": "2014-07-11T09:58:44.087564Z", "url": "https://files.pythonhosted.org/packages/65/e4/50d4a0e8cb6c001cf7b06e9c6cc7420e97c26bfbf01ef667a88d201b6636/scraperwiki-0.3.11.tar.gz", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "26c2a6ae4acdb9712e75e3ae94312c0d", "sha256": "ba282ecafe734b03e9e88e51dd80162d99206fba945178387e0e635ed4c5de9a"}, "downloads": -1, "filename": "scraperwiki-0.3.2.tar.gz", "has_sig": false, "md5_digest": "26c2a6ae4acdb9712e75e3ae94312c0d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5300, "upload_time": "2013-05-23T10:59:22", "upload_time_iso_8601": "2013-05-23T10:59:22.064233Z", "url": "https://files.pythonhosted.org/packages/b5/7d/b22b2a689ee530bd0d718e2cb69d9ffde8260f26b6035eae4cac505c778a/scraperwiki-0.3.2.tar.gz", "yanked": false}], "0.3.3": [{"comment_text": "", "digests": {"md5": "d19dc2b3121d3de1c19cfc9c1ea55a65", "sha256": "645cb81ba99ebc341d5e7ee05e96f3b990dbee0e85fdec4d5e6401dfb83fec4e"}, "downloads": -1, "filename": "scraperwiki-0.3.3.tar.gz", "has_sig": false, "md5_digest": "d19dc2b3121d3de1c19cfc9c1ea55a65", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5285, "upload_time": "2013-07-12T13:25:11", "upload_time_iso_8601": "2013-07-12T13:25:11.937883Z", "url": "https://files.pythonhosted.org/packages/26/a6/5aaae7e352fff092f40ded8c9cb06a00413d2cb6bcc8295811598dce40cd/scraperwiki-0.3.3.tar.gz", "yanked": false}], "0.3.4": [{"comment_text": "", "digests": {"md5": "584c3fe408aae5fbe12fbc64679095db", "sha256": "7f4b0bf5b4a411de7bb5de298a1553c83feac72490bdfbfc73231e9cbc47358b"}, "downloads": -1, "filename": "scraperwiki-0.3.4.tar.gz", "has_sig": false, "md5_digest": "584c3fe408aae5fbe12fbc64679095db", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5308, "upload_time": "2013-07-12T15:23:17", "upload_time_iso_8601": "2013-07-12T15:23:17.418206Z", "url": "https://files.pythonhosted.org/packages/42/18/b86cf1cbd0efd2b2d5ee8c0d1ac069897d5eb747a9f007643c798a62b32a/scraperwiki-0.3.4.tar.gz", "yanked": false}], "0.3.5": [{"comment_text": "", "digests": {"md5": "cfdd5dd0ee3d0c35fd7f6cbfd31b0de2", "sha256": "0efc28312ae2e54d2d2ae60dee97a236a4bc8038cf2993fca216de5f184d697e"}, "downloads": -1, "filename": "scraperwiki-0.3.5.tar.gz", "has_sig": false, "md5_digest": "cfdd5dd0ee3d0c35fd7f6cbfd31b0de2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5346, "upload_time": "2013-08-20T14:34:21", "upload_time_iso_8601": "2013-08-20T14:34:21.157172Z", "url": "https://files.pythonhosted.org/packages/a4/ab/2ee303fab9be2490b90d6fe9dc8f9fb27cc5046c2e4140f4c31dabc1e2bc/scraperwiki-0.3.5.tar.gz", "yanked": false}], "0.3.6": [{"comment_text": "", "digests": {"md5": "5c8f90f6f2cd536a1353f06b91da1cbf", "sha256": "7c11f0ecf6208a2e74d8558587d2546dfa78679d1767c097ac741440de6048f1"}, "downloads": -1, "filename": "scraperwiki-0.3.6.tar.gz", "has_sig": false, "md5_digest": "5c8f90f6f2cd536a1353f06b91da1cbf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6061, "upload_time": "2013-08-20T16:23:18", "upload_time_iso_8601": "2013-08-20T16:23:18.960959Z", "url": "https://files.pythonhosted.org/packages/6f/e5/f35a14382965e2f89a6ed2daeed267f8e7bf92368d7b0c94d87e601568ab/scraperwiki-0.3.6.tar.gz", "yanked": false}], "0.3.7": [{"comment_text": "", "digests": {"md5": "a1a5fd790290c19c1398aa283cb6044c", "sha256": "abd16a107ce44d94899ea53ea05b0d20974fdf873ef9e2e8b79c867df42a7440"}, "downloads": -1, "filename": "scraperwiki-0.3.7.tar.gz", "has_sig": false, "md5_digest": "a1a5fd790290c19c1398aa283cb6044c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6199, "upload_time": "2013-10-08T16:13:27", "upload_time_iso_8601": "2013-10-08T16:13:27.273921Z", "url": "https://files.pythonhosted.org/packages/e0/ae/d30f414307b4098d951cef809fd8ebd6406af698db2fbb8e2bbbad45c62c/scraperwiki-0.3.7.tar.gz", "yanked": false}], "0.3.8": [{"comment_text": "", "digests": {"md5": "8ee7c9ab18efe2806b26c312286836fc", "sha256": "56fa77264675e89488d522adb7305198b6e847ad17bff5c980f2b62081a6e474"}, "downloads": -1, "filename": "scraperwiki-0.3.8.tar.gz", "has_sig": false, "md5_digest": "8ee7c9ab18efe2806b26c312286836fc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6297, "upload_time": "2014-01-29T16:11:56", "upload_time_iso_8601": "2014-01-29T16:11:56.629910Z", "url": "https://files.pythonhosted.org/packages/81/30/ea2ca52da35d63b668eda43b992043e4309f0e15179fc9482d28121963f8/scraperwiki-0.3.8.tar.gz", "yanked": false}], "0.3.9": [{"comment_text": "", "digests": {"md5": "0780469a86eee7be2bf0fb0bb3b21182", "sha256": "2f90d47ebc5e8e3272dfeae44ec0e20e68941045fc788e47af08c6a723e56517"}, "downloads": -1, "filename": "scraperwiki-0.3.9.tar.gz", "has_sig": false, "md5_digest": "0780469a86eee7be2bf0fb0bb3b21182", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6298, "upload_time": "2014-05-09T17:21:02", "upload_time_iso_8601": "2014-05-09T17:21:02.179558Z", "url": "https://files.pythonhosted.org/packages/0c/7d/530cf0a2c6d04847ce55da0d110725a50ef27f1b6825783100f9d19f8c53/scraperwiki-0.3.9.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "85d617100938d46b8e9bf2796e7c43ec", "sha256": "ead9dea8c6e9b8f5e8b46c9db0f17cccdc2639765d6d6c1199020f1fabb8f6ed"}, "downloads": -1, "filename": "scraperwiki-0.4.0.tar.gz", "has_sig": false, "md5_digest": "85d617100938d46b8e9bf2796e7c43ec", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8097, "upload_time": "2014-09-23T12:50:12", "upload_time_iso_8601": "2014-09-23T12:50:12.648608Z", "url": "https://files.pythonhosted.org/packages/7d/77/dcabaa7f9754bf629d32b14fba07c0026d726fc8459b7cc93241b8e4ba89/scraperwiki-0.4.0.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "8ac0524eaa8c1bca355515ae182ecb0b", "sha256": "a8392f22c0b97ddea04d711e4b8e47ba4abf930f7110677c460b93c46c0b6ae5"}, "downloads": -1, "filename": "scraperwiki-0.4.1.tar.gz", "has_sig": false, "md5_digest": "8ac0524eaa8c1bca355515ae182ecb0b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8117, "upload_time": "2014-09-30T08:41:39", "upload_time_iso_8601": "2014-09-30T08:41:39.119401Z", "url": "https://files.pythonhosted.org/packages/80/e5/4a50dc3c382a8f9ad21e72d143f8c0a2a7fd24d0e5a43f425e3d31edd1fe/scraperwiki-0.4.1.tar.gz", "yanked": false}], "0.5": [{"comment_text": "", "digests": {"md5": "3bcfb652412452786ac7962c96953aab", "sha256": "9bfe5f2002b0f31f7cd402768d310a71ea2729ea76363c6003134ef6e01c2a14"}, "downloads": -1, "filename": "scraperwiki-0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "3bcfb652412452786ac7962c96953aab", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7371, "upload_time": "2015-06-17T11:11:30", "upload_time_iso_8601": "2015-06-17T11:11:30.234804Z", "url": "https://files.pythonhosted.org/packages/a5/c1/d5fd1f468662a5f08f9efed5747410aed9b4fe377daf108c226c788b7881/scraperwiki-0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1ead9125f6e087edf31d7b3a9b26fa9c", "sha256": "00955854729d0faa2c19f55afc4aae8861936caf6de8d76a7516a0a06e673486"}, "downloads": -1, "filename": "scraperwiki-0.5.tar.gz", "has_sig": false, "md5_digest": "1ead9125f6e087edf31d7b3a9b26fa9c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7701, "upload_time": "2015-06-17T11:11:33", "upload_time_iso_8601": "2015-06-17T11:11:33.842281Z", "url": "https://files.pythonhosted.org/packages/23/81/3c52c1ac5bb81bb61eacf4598a4ac2d468f206120fe8172b6dec74f7097f/scraperwiki-0.5.tar.gz", "yanked": false}], "0.5.1": [{"comment_text": "", "digests": {"md5": "81f64b30b4535d0f8f40c1cd53e0fd02", "sha256": "2ad4c5500c436afb0de13aae7ce6c553b9933029aad06e2bf7497fa728d21b98"}, "downloads": -1, "filename": "scraperwiki-0.5.1.tar.gz", "has_sig": false, "md5_digest": "81f64b30b4535d0f8f40c1cd53e0fd02", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7725, "upload_time": "2015-07-07T14:34:59", "upload_time_iso_8601": "2015-07-07T14:34:59.117962Z", "url": "https://files.pythonhosted.org/packages/30/84/d874847baad89f03e6984fcd87505a37bf924b66519d1e07bf76e2369af0/scraperwiki-0.5.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "81f64b30b4535d0f8f40c1cd53e0fd02", "sha256": "2ad4c5500c436afb0de13aae7ce6c553b9933029aad06e2bf7497fa728d21b98"}, "downloads": -1, "filename": "scraperwiki-0.5.1.tar.gz", "has_sig": false, "md5_digest": "81f64b30b4535d0f8f40c1cd53e0fd02", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7725, "upload_time": "2015-07-07T14:34:59", "upload_time_iso_8601": "2015-07-07T14:34:59.117962Z", "url": "https://files.pythonhosted.org/packages/30/84/d874847baad89f03e6984fcd87505a37bf924b66519d1e07bf76e2369af0/scraperwiki-0.5.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:53 2020"}