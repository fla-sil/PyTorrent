{"info": {"author": "Robert Sachunsky, Konstantin Baierer, Kay-Michael W\u00fcrzner", "author_email": "sachunsky@informatik.uni-leipzig.de, unixprog@gmail.com, wuerzner@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# ocrd_keraslm\n    character-level language modelling using Keras\n\n\n## Introduction\n\nThis is a tool for statistical _language modelling_ (predicting text from context) with recurrent neural networks. It models probabilities not on the word level but the _character level_ so as to allow open vocabulary processing (avoiding morphology, historic orthography and word segmentation problems). It manages a vocabulary of mapped characters, which can be easily extended by training on more text. Above that, unmapped characters are treated with underspecification.\n\nIn addition to character sequences, (meta-data) context variables can be configured as extra input. \n\n### Architecture\n\nThe model consists of:\n\n0. an input layer: characters are represented as indexes from the vocabulary mapping, in windows of a number `length` of characters,\n1. a character embedding layer: window sequences are converted into dense vectors by looking up the indexes in an embedding weight matrix,\n2. a context embedding layer: context variables are converted into dense vectors by looking up the indexes in an embedding weight matrix, \n3. character and context vector sequences are concatenated,\n4. a number `depth` of hidden layers: each with a number `width` of hidden recurrent units of _LSTM cells_ (Long Short-term Memory) connected on top of each other,\n5. an output layer derived from the transposed character embedding matrix (weight tying): hidden activations are projected linearly to vectors of dimensionality equal to the character vocabulary size, then softmax is applied returning a probability for each possible value of the next character, respectively.\n\n![model graph depiction](model-graph.png \"graph with 1 context variable\")\n\nThe model is trained by feeding windows of text in index representation to the input layer, calculating output and comparing it to the same text shifted backward by 1 character, and represented as unit vectors (\"one-hot coding\") as target. The loss is calculated as the (unweighted) cross-entropy between target and output. Backpropagation yields error gradients for each layer, which is used to iteratively update the weights (stochastic gradient descent).\n\nThis is implemented in [Keras](https://keras.io) with [Tensorflow](https://www.tensorflow.org/) as backend. It automatically uses a fast CUDA-optimized LSTM implementation (Nividia GPU and Tensorflow installation with GPU support, see below), both in learning and in prediction phase, if available.\n\n\n### Modes of operation\n\nNotably, this model (by default) runs _statefully_, i.e. by implicitly passing hidden state from one window (batch of samples) to the next. That way, the context available for predictions can be arbitrarily long (above `length`, e.g. the complete document up to that point), or short (below `length`, e.g. at the start of a text). (However, this is a passive perspective above `length`, because errors are never back-propagated any further in time during gradient-descent training.) This is favourable to stateless mode because all characters can be output in parallel, and no partial windows need to be presented during training (which slows down).\n\nBesides stateful mode, the model can also be run _incrementally_, i.e. by explicitly passing hidden state from the caller. That way, multiple alternative hypotheses can be processed together. This is used for generation (sampling from the model) and alternative decoding (finding the best path through a sequence of alternatives).\n\n### Context conditioning\n\nEvery text has meta-data like time, author, text type, genre, production features (e.g. print vs typewriter vs digital born rich text, OCR version), language, structural element (e.g. title vs heading vs paragraph vs footer vs marginalia), font family (e.g. Antiqua vs Fraktura) and font shape (e.g. bold vs letter-spaced vs italic vs normal) etc. \n\nThis information (however noisy) can be very useful to facilitate stochastic modelling, since language has an extreme diversity and complexity. To that end, models can be conditioned on extra inputs here, termed _context variables_. The model learns to represent these high-dimensional discrete values as low-dimensional continuous vectors (embeddings), also entering the recurrent hidden layers (as a form of simple additive adaptation).\n\n### Underspecification\n\nIndex zero is reserved for unmapped characters (unseen contexts). During training, its embedding vector is regularised to occupy a center position of all mapped characters (all other contexts), and the hidden layers get to see it every now and then by random degradation. At runtime, therefore, some unknown character (some unknown context) represented as zero does not disturb follow-up predictions too much.\n\n\n## Installation\n\nRequired Ubuntu packages:\n\n* Python (``python`` or ``python3``)\n* pip (``python-pip`` or ``python3-pip``)\n* virtualenv (``python-virtualenv`` or ``python3-virtualenv``)\n\nCreate and activate a virtualenv as usual.\n\nIf you need a custom version of ``keras`` or ``tensorflow`` (like [GPU support](https://www.tensorflow.org/install/install_sources)), install them via `pip` now.\n\nTo install Python dependencies and this module, then do:\n```shell\nmake deps install\n```\nWhich is the equivalent of:\n```shell\npip install -r requirements.txt\npip install -e .\n```\n\nUseful environment variables are:\n- ``TF_CPP_MIN_LOG_LEVEL`` (set to `1` to suppress most of Tensorflow's messages\n- ``CUDA_VISIBLE_DEVICES`` (set empty to force CPU even in a GPU installation)\n\n\n## Usage\n\nThis packages has two user interfaces:\n\n### command line interface `keraslm-rate`\n\nTo be used with string arguments and plain-text files.\n\n```shell\nUsage: keraslm-rate [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  train                           train a language model\n  test                            get overall perplexity from language model\n  apply                           get individual probabilities from language model\n  generate                        sample characters from language model\n  print-charset                   Print the mapped characters\n  prune-charset                   Delete one character from mapping\n  plot-char-embeddings-similarity\n                                  Paint a heat map of character embeddings\n  plot-context-embeddings-similarity\n                                  Paint a heat map of context embeddings\n  plot-context-embeddings-projection\n                                  Paint a 2-d PCA projection of context embeddings\n```\n\nExamples:\n```shell\nkeraslm-rate train --width 64 --depth 4 --length 256 --model model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/*.tcf.txt\nkeraslm-rate generate -m model_dta_64_4_256.h5 --number 6 \"f\u00fcr die Wi\u017f\u017fen\"\nkeraslm-rate apply -m model_dta_64_4_256.h5 \"so sch\u00e4dlich ist es Borkickheile zu pflanzen\"\nkeraslm-rate test -m model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/grimm_*.tcf.txt\n```\n\n### [OCR-D processor](https://github.com/OCR-D/core) interface `ocrd-keraslm-rate`\n\nTo be used with [PageXML](https://www.primaresearch.org/tools/PAGELibraries) documents in an [OCR-D](https://github.com/OCR-D/spec/) annotation workflow. Input could be anything with a textual annotation (`TextEquiv` on the given `textequiv_level`). The LM rater could be used for both quality control (without alternative decoding, using only each first index `TextEquiv`) and part of post-correction (with `alternative_decoding=True`, finding the best path among `TextEquiv` indexes).\n\n```json\n  \"tools\": {\n    \"ocrd-keraslm-rate\": {\n      \"executable\": \"ocrd-keraslm-rate\",\n      \"categories\": [\n        \"Text recognition and optimization\"\n      ],\n      \"steps\": [\n        \"recognition/text-recognition\"\n      ],\n      \"description\": \"Rate elements of the text with a character-level LSTM language model in Keras\",\n      \"input_file_grp\": [\n        \"OCR-D-OCR-TESS\",\n        \"OCR-D-OCR-KRAK\",\n        \"OCR-D-OCR-OCRO\",\n        \"OCR-D-OCR-CALA\",\n        \"OCR-D-OCR-ANY\",\n        \"OCR-D-COR-CIS\",\n        \"OCR-D-COR-ASV\"\n      ],\n      \"output_file_grp\": [\n        \"OCR-D-COR-LM\"\n      ],\n      \"parameters\": {\n        \"model_file\": {\n          \"type\": \"string\",\n          \"format\": \"uri\",\n          \"content-type\": \"application/x-hdf;subtype=bag\",\n          \"description\": \"path of h5py weight/config file for model trained with keraslm\",\n          \"required\": true,\n          \"cacheable\": true\n        },\n        \"textequiv_level\": {\n          \"type\": \"string\",\n          \"enum\": [\"region\", \"line\", \"word\", \"glyph\"],\n          \"default\": \"glyph\",\n          \"description\": \"PAGE XML hierarchy level to evaluate TextEquiv sequences on\"\n        },\n        \"alternative_decoding\": {\n          \"type\": \"boolean\",\n          \"description\": \"whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative\",\n          \"default\": true\n        },\n        \"beam_width\": {\n          \"type\": \"number\",\n          \"format\": \"integer\",\n          \"description\": \"maximum number of best partial paths to consider during search with alternative_decoding\",\n          \"default\": 100\n        }\n      }\n    }\n  }\n```\n\nExamples:\n```shell\nmake deps-test # installs ocrd_tesserocr\nmake test/assets # downloads GT, imports PageXML, builds workspaces\nocrd workspace clone -a test/assets/kant_aufklaerung_1784/mets.xml ws1\ncd ws1\nocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCK\nocrd-tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINE\nocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-WORD -p '{ \"textequiv_level\" : \"word\", \"model\" : \"Fraktur\" }'\nocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-GLYPH -p '{ \"textequiv_level\" : \"glyph\", \"model\" : \"deu-frak\" }'\n# get confidences and perplexity:\nocrd-keraslm-rate -I OCR-D-OCR-TESS-WORD -O OCR-D-OCR-LM-WORD -p '{ \"model_file\": \"model_dta_64_4_256.h5\", \"textequiv_level\": \"word\", \"alternative_decoding\": false }'\n# also get best path:\nocrd-keraslm-rate -I OCR-D-OCR-TESS-GLYPH -O OCR-D-OCR-LM-GLYPH -p '{ \"model_file\": \"model_dta_64_4_256.h5\", \"textequiv_level\": \"glyph\", \"alternative_decoding\": true, \"beam_width\": 10 }'\n```\n\n## Testing\n\n```shell\nmake deps-test test\n```\nWhich is the equivalent of:\n```shell\npip install -r requirements_test.txt\ntest -e test/assets || test/prepare_gt.bash test/assets\ntest -f model_dta_test.h5 || keraslm-rate train -m model_dta_test.h5 test/assets/*.txt\nkeraslm-rate test -m model_dta_test.h5 test/assets/*.txt\npython -m pytest test $(PYTEST_ARGS)\n```\n\nSet `PYTEST_ARGS=\"-s --verbose\"` to see log output (`-s`) and individual test results (`--verbose`).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/OCR-D/ocrd_keraslm", "keywords": "", "license": "Apache License 2.0", "maintainer": "", "maintainer_email": "", "name": "ocrd-keraslm", "package_url": "https://pypi.org/project/ocrd-keraslm/", "platform": "", "project_url": "https://pypi.org/project/ocrd-keraslm/", "project_urls": {"Homepage": "https://github.com/OCR-D/ocrd_keraslm"}, "release_url": "https://pypi.org/project/ocrd-keraslm/0.3.2/", "requires_dist": ["ocrd (>=2.0)", "click", "keras (>=2.2.4)", "numpy", "tensorflow (<2.0)", "h5py", "networkx (>=2.0)", "sklearn; extra == 'plotting'", "matplotlib; extra == 'plotting'"], "requires_python": "", "summary": "character-level language modelling in Keras", "version": "0.3.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>ocrd_keraslm</h1>\n<pre><code>character-level language modelling using Keras\n</code></pre>\n<h2>Introduction</h2>\n<p>This is a tool for statistical <em>language modelling</em> (predicting text from context) with recurrent neural networks. It models probabilities not on the word level but the <em>character level</em> so as to allow open vocabulary processing (avoiding morphology, historic orthography and word segmentation problems). It manages a vocabulary of mapped characters, which can be easily extended by training on more text. Above that, unmapped characters are treated with underspecification.</p>\n<p>In addition to character sequences, (meta-data) context variables can be configured as extra input.</p>\n<h3>Architecture</h3>\n<p>The model consists of:</p>\n<ol>\n<li>an input layer: characters are represented as indexes from the vocabulary mapping, in windows of a number <code>length</code> of characters,</li>\n<li>a character embedding layer: window sequences are converted into dense vectors by looking up the indexes in an embedding weight matrix,</li>\n<li>a context embedding layer: context variables are converted into dense vectors by looking up the indexes in an embedding weight matrix,</li>\n<li>character and context vector sequences are concatenated,</li>\n<li>a number <code>depth</code> of hidden layers: each with a number <code>width</code> of hidden recurrent units of <em>LSTM cells</em> (Long Short-term Memory) connected on top of each other,</li>\n<li>an output layer derived from the transposed character embedding matrix (weight tying): hidden activations are projected linearly to vectors of dimensionality equal to the character vocabulary size, then softmax is applied returning a probability for each possible value of the next character, respectively.</li>\n</ol>\n<p><img alt=\"model graph depiction\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4a3d3e94425bd68684a64f0588439931413c94ed/6d6f64656c2d67726170682e706e67\"></p>\n<p>The model is trained by feeding windows of text in index representation to the input layer, calculating output and comparing it to the same text shifted backward by 1 character, and represented as unit vectors (\"one-hot coding\") as target. The loss is calculated as the (unweighted) cross-entropy between target and output. Backpropagation yields error gradients for each layer, which is used to iteratively update the weights (stochastic gradient descent).</p>\n<p>This is implemented in <a href=\"https://keras.io\" rel=\"nofollow\">Keras</a> with <a href=\"https://www.tensorflow.org/\" rel=\"nofollow\">Tensorflow</a> as backend. It automatically uses a fast CUDA-optimized LSTM implementation (Nividia GPU and Tensorflow installation with GPU support, see below), both in learning and in prediction phase, if available.</p>\n<h3>Modes of operation</h3>\n<p>Notably, this model (by default) runs <em>statefully</em>, i.e. by implicitly passing hidden state from one window (batch of samples) to the next. That way, the context available for predictions can be arbitrarily long (above <code>length</code>, e.g. the complete document up to that point), or short (below <code>length</code>, e.g. at the start of a text). (However, this is a passive perspective above <code>length</code>, because errors are never back-propagated any further in time during gradient-descent training.) This is favourable to stateless mode because all characters can be output in parallel, and no partial windows need to be presented during training (which slows down).</p>\n<p>Besides stateful mode, the model can also be run <em>incrementally</em>, i.e. by explicitly passing hidden state from the caller. That way, multiple alternative hypotheses can be processed together. This is used for generation (sampling from the model) and alternative decoding (finding the best path through a sequence of alternatives).</p>\n<h3>Context conditioning</h3>\n<p>Every text has meta-data like time, author, text type, genre, production features (e.g. print vs typewriter vs digital born rich text, OCR version), language, structural element (e.g. title vs heading vs paragraph vs footer vs marginalia), font family (e.g. Antiqua vs Fraktura) and font shape (e.g. bold vs letter-spaced vs italic vs normal) etc.</p>\n<p>This information (however noisy) can be very useful to facilitate stochastic modelling, since language has an extreme diversity and complexity. To that end, models can be conditioned on extra inputs here, termed <em>context variables</em>. The model learns to represent these high-dimensional discrete values as low-dimensional continuous vectors (embeddings), also entering the recurrent hidden layers (as a form of simple additive adaptation).</p>\n<h3>Underspecification</h3>\n<p>Index zero is reserved for unmapped characters (unseen contexts). During training, its embedding vector is regularised to occupy a center position of all mapped characters (all other contexts), and the hidden layers get to see it every now and then by random degradation. At runtime, therefore, some unknown character (some unknown context) represented as zero does not disturb follow-up predictions too much.</p>\n<h2>Installation</h2>\n<p>Required Ubuntu packages:</p>\n<ul>\n<li>Python (<code>python</code> or <code>python3</code>)</li>\n<li>pip (<code>python-pip</code> or <code>python3-pip</code>)</li>\n<li>virtualenv (<code>python-virtualenv</code> or <code>python3-virtualenv</code>)</li>\n</ul>\n<p>Create and activate a virtualenv as usual.</p>\n<p>If you need a custom version of <code>keras</code> or <code>tensorflow</code> (like <a href=\"https://www.tensorflow.org/install/install_sources\" rel=\"nofollow\">GPU support</a>), install them via <code>pip</code> now.</p>\n<p>To install Python dependencies and this module, then do:</p>\n<pre>make deps install\n</pre>\n<p>Which is the equivalent of:</p>\n<pre>pip install -r requirements.txt\npip install -e .\n</pre>\n<p>Useful environment variables are:</p>\n<ul>\n<li><code>TF_CPP_MIN_LOG_LEVEL</code> (set to <code>1</code> to suppress most of Tensorflow's messages</li>\n<li><code>CUDA_VISIBLE_DEVICES</code> (set empty to force CPU even in a GPU installation)</li>\n</ul>\n<h2>Usage</h2>\n<p>This packages has two user interfaces:</p>\n<h3>command line interface <code>keraslm-rate</code></h3>\n<p>To be used with string arguments and plain-text files.</p>\n<pre>Usage: keraslm-rate <span class=\"o\">[</span>OPTIONS<span class=\"o\">]</span> COMMAND <span class=\"o\">[</span>ARGS<span class=\"o\">]</span>...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  train                           train a language model\n  <span class=\"nb\">test</span>                            get overall perplexity from language model\n  apply                           get individual probabilities from language model\n  generate                        sample characters from language model\n  print-charset                   Print the mapped characters\n  prune-charset                   Delete one character from mapping\n  plot-char-embeddings-similarity\n                                  Paint a heat map of character embeddings\n  plot-context-embeddings-similarity\n                                  Paint a heat map of context embeddings\n  plot-context-embeddings-projection\n                                  Paint a <span class=\"m\">2</span>-d PCA projection of context embeddings\n</pre>\n<p>Examples:</p>\n<pre>keraslm-rate train --width <span class=\"m\">64</span> --depth <span class=\"m\">4</span> --length <span class=\"m\">256</span> --model model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/*.tcf.txt\nkeraslm-rate generate -m model_dta_64_4_256.h5 --number <span class=\"m\">6</span> <span class=\"s2\">\"f\u00fcr die Wi\u017f\u017fen\"</span>\nkeraslm-rate apply -m model_dta_64_4_256.h5 <span class=\"s2\">\"so sch\u00e4dlich ist es Borkickheile zu pflanzen\"</span>\nkeraslm-rate <span class=\"nb\">test</span> -m model_dta_64_4_256.h5 dta_komplett_2017-09-01/txt/grimm_*.tcf.txt\n</pre>\n<h3><a href=\"https://github.com/OCR-D/core\" rel=\"nofollow\">OCR-D processor</a> interface <code>ocrd-keraslm-rate</code></h3>\n<p>To be used with <a href=\"https://www.primaresearch.org/tools/PAGELibraries\" rel=\"nofollow\">PageXML</a> documents in an <a href=\"https://github.com/OCR-D/spec/\" rel=\"nofollow\">OCR-D</a> annotation workflow. Input could be anything with a textual annotation (<code>TextEquiv</code> on the given <code>textequiv_level</code>). The LM rater could be used for both quality control (without alternative decoding, using only each first index <code>TextEquiv</code>) and part of post-correction (with <code>alternative_decoding=True</code>, finding the best path among <code>TextEquiv</code> indexes).</p>\n<pre>  <span class=\"s2\">\"tools\"</span><span class=\"err\">:</span> <span class=\"p\">{</span>\n    <span class=\"nt\">\"ocrd-keraslm-rate\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n      <span class=\"nt\">\"executable\"</span><span class=\"p\">:</span> <span class=\"s2\">\"ocrd-keraslm-rate\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"categories\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"Text recognition and optimization\"</span>\n      <span class=\"p\">],</span>\n      <span class=\"nt\">\"steps\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"recognition/text-recognition\"</span>\n      <span class=\"p\">],</span>\n      <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Rate elements of the text with a character-level LSTM language model in Keras\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"input_file_grp\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"OCR-D-OCR-TESS\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-OCR-KRAK\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-OCR-OCRO\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-OCR-CALA\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-OCR-ANY\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-COR-CIS\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-COR-ASV\"</span>\n      <span class=\"p\">],</span>\n      <span class=\"nt\">\"output_file_grp\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"OCR-D-COR-LM\"</span>\n      <span class=\"p\">],</span>\n      <span class=\"nt\">\"parameters\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"model_file\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"string\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"format\"</span><span class=\"p\">:</span> <span class=\"s2\">\"uri\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"content-type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"application/x-hdf;subtype=bag\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"path of h5py weight/config file for model trained with keraslm\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"required\"</span><span class=\"p\">:</span> <span class=\"kc\">true</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"cacheable\"</span><span class=\"p\">:</span> <span class=\"kc\">true</span>\n        <span class=\"p\">},</span>\n        <span class=\"nt\">\"textequiv_level\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"string\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"enum\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"region\"</span><span class=\"p\">,</span> <span class=\"s2\">\"line\"</span><span class=\"p\">,</span> <span class=\"s2\">\"word\"</span><span class=\"p\">,</span> <span class=\"s2\">\"glyph\"</span><span class=\"p\">],</span>\n          <span class=\"nt\">\"default\"</span><span class=\"p\">:</span> <span class=\"s2\">\"glyph\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"PAGE XML hierarchy level to evaluate TextEquiv sequences on\"</span>\n        <span class=\"p\">},</span>\n        <span class=\"nt\">\"alternative_decoding\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"boolean\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"whether to process all TextEquiv alternatives, finding the best path via beam search, and delete each non-best alternative\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"default\"</span><span class=\"p\">:</span> <span class=\"kc\">true</span>\n        <span class=\"p\">},</span>\n        <span class=\"nt\">\"beam_width\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"number\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"format\"</span><span class=\"p\">:</span> <span class=\"s2\">\"integer\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"maximum number of best partial paths to consider during search with alternative_decoding\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"default\"</span><span class=\"p\">:</span> <span class=\"mi\">100</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">}</span>\n</pre>\n<p>Examples:</p>\n<pre>make deps-test <span class=\"c1\"># installs ocrd_tesserocr</span>\nmake test/assets <span class=\"c1\"># downloads GT, imports PageXML, builds workspaces</span>\nocrd workspace clone -a test/assets/kant_aufklaerung_1784/mets.xml ws1\n<span class=\"nb\">cd</span> ws1\nocrd-tesserocr-segment-region -I OCR-D-IMG -O OCR-D-SEG-BLOCK\nocrd-tesserocr-segment-line -I OCR-D-SEG-BLOCK -O OCR-D-SEG-LINE\nocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-WORD -p <span class=\"s1\">'{ \"textequiv_level\" : \"word\", \"model\" : \"Fraktur\" }'</span>\nocrd-tesserocr-recognize -I OCR-D-SEG-LINE -O OCR-D-OCR-TESS-GLYPH -p <span class=\"s1\">'{ \"textequiv_level\" : \"glyph\", \"model\" : \"deu-frak\" }'</span>\n<span class=\"c1\"># get confidences and perplexity:</span>\nocrd-keraslm-rate -I OCR-D-OCR-TESS-WORD -O OCR-D-OCR-LM-WORD -p <span class=\"s1\">'{ \"model_file\": \"model_dta_64_4_256.h5\", \"textequiv_level\": \"word\", \"alternative_decoding\": false }'</span>\n<span class=\"c1\"># also get best path:</span>\nocrd-keraslm-rate -I OCR-D-OCR-TESS-GLYPH -O OCR-D-OCR-LM-GLYPH -p <span class=\"s1\">'{ \"model_file\": \"model_dta_64_4_256.h5\", \"textequiv_level\": \"glyph\", \"alternative_decoding\": true, \"beam_width\": 10 }'</span>\n</pre>\n<h2>Testing</h2>\n<pre>make deps-test <span class=\"nb\">test</span>\n</pre>\n<p>Which is the equivalent of:</p>\n<pre>pip install -r requirements_test.txt\n<span class=\"nb\">test</span> -e test/assets <span class=\"o\">||</span> test/prepare_gt.bash test/assets\n<span class=\"nb\">test</span> -f model_dta_test.h5 <span class=\"o\">||</span> keraslm-rate train -m model_dta_test.h5 test/assets/*.txt\nkeraslm-rate <span class=\"nb\">test</span> -m model_dta_test.h5 test/assets/*.txt\npython -m pytest <span class=\"nb\">test</span> <span class=\"k\">$(</span>PYTEST_ARGS<span class=\"k\">)</span>\n</pre>\n<p>Set <code>PYTEST_ARGS=\"-s --verbose\"</code> to see log output (<code>-s</code>) and individual test results (<code>--verbose</code>).</p>\n\n          </div>"}, "last_serial": 6158523, "releases": {"0.3.1": [{"comment_text": "", "digests": {"md5": "0da1139d7b62ee27b9bb3af2b4e38929", "sha256": "f3ec82a615434e90028722586c6123e4a1887e36b0a57f06566a291892280e88"}, "downloads": -1, "filename": "ocrd_keraslm-0.3.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "0da1139d7b62ee27b9bb3af2b4e38929", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 34192, "upload_time": "2019-10-25T22:53:09", "upload_time_iso_8601": "2019-10-25T22:53:09.567407Z", "url": "https://files.pythonhosted.org/packages/eb/ba/8f5f0f1801ea99221c772357e2c79d9935a88e89873924e557e24aea6c33/ocrd_keraslm-0.3.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e8d597a8dbf64e45dcbf19196e73bbf8", "sha256": "665a9bf1d7bc46f497d71638b2d33608062edd16ac11b9cff05be56eacda53c9"}, "downloads": -1, "filename": "ocrd_keraslm-0.3.1.tar.gz", "has_sig": false, "md5_digest": "e8d597a8dbf64e45dcbf19196e73bbf8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32287, "upload_time": "2019-10-25T22:53:12", "upload_time_iso_8601": "2019-10-25T22:53:12.437293Z", "url": "https://files.pythonhosted.org/packages/79/0e/744edc5497d706ac558b90d8d85b2e52ad5fb6b794c6f9cb44fc0aaa341a/ocrd_keraslm-0.3.1.tar.gz", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "9e8927b5ca560a990cb924c7a01e7280", "sha256": "45c4af95f531e3a2c9528e401d368dad10e4b8f9cdba9a67ef6f816afc682d3b"}, "downloads": -1, "filename": "ocrd_keraslm-0.3.2-py3-none-any.whl", "has_sig": false, "md5_digest": "9e8927b5ca560a990cb924c7a01e7280", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 34190, "upload_time": "2019-11-18T22:03:01", "upload_time_iso_8601": "2019-11-18T22:03:01.036117Z", "url": "https://files.pythonhosted.org/packages/15/10/690a290322b84e6c4cba17dbff7e0fb570916810371b1b48020f75504d49/ocrd_keraslm-0.3.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7eb11946732e6410d4ba18dad3fbaf20", "sha256": "ba56b149a68c9f351052e62cc247d4074514a66c5dee99e7ef6a78cca497e5e9"}, "downloads": -1, "filename": "ocrd_keraslm-0.3.2.tar.gz", "has_sig": false, "md5_digest": "7eb11946732e6410d4ba18dad3fbaf20", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32294, "upload_time": "2019-11-18T22:03:06", "upload_time_iso_8601": "2019-11-18T22:03:06.384019Z", "url": "https://files.pythonhosted.org/packages/0e/75/b3875f685ba4d02c8cce12b86200e139617acde417fab40df2e462d85673/ocrd_keraslm-0.3.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "9e8927b5ca560a990cb924c7a01e7280", "sha256": "45c4af95f531e3a2c9528e401d368dad10e4b8f9cdba9a67ef6f816afc682d3b"}, "downloads": -1, "filename": "ocrd_keraslm-0.3.2-py3-none-any.whl", "has_sig": false, "md5_digest": "9e8927b5ca560a990cb924c7a01e7280", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 34190, "upload_time": "2019-11-18T22:03:01", "upload_time_iso_8601": "2019-11-18T22:03:01.036117Z", "url": "https://files.pythonhosted.org/packages/15/10/690a290322b84e6c4cba17dbff7e0fb570916810371b1b48020f75504d49/ocrd_keraslm-0.3.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7eb11946732e6410d4ba18dad3fbaf20", "sha256": "ba56b149a68c9f351052e62cc247d4074514a66c5dee99e7ef6a78cca497e5e9"}, "downloads": -1, "filename": "ocrd_keraslm-0.3.2.tar.gz", "has_sig": false, "md5_digest": "7eb11946732e6410d4ba18dad3fbaf20", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32294, "upload_time": "2019-11-18T22:03:06", "upload_time_iso_8601": "2019-11-18T22:03:06.384019Z", "url": "https://files.pythonhosted.org/packages/0e/75/b3875f685ba4d02c8cce12b86200e139617acde417fab40df2e462d85673/ocrd_keraslm-0.3.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:40 2020"}