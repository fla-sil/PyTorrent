{"info": {"author": "Mustafa Guler", "author_email": "mguler@ucsd.edu", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: MacOS :: MacOS X", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering :: Bio-Informatics"], "description": "# Cirrus-NGS\n\nCloud-optimized primary analysis pipelines for RNA-seq, miRNA-seq, ChIP-seq, and variant calling in whole-genome/whole-exome DNA-Seq.\n\n## Introduction\n\nBionformatic analysis of large-scale next-generation sequencing (NGS) data requires significant compute resources. While cloud computing makes such processing power available on-demand, the administration of dynamic compute clusters is a daunting task for most working biologists.  To address this pain point, the Center for Computational Biology and Bioinformatics at the University of California, San Diego, has developed Cirrus-NGS, a turn-key solution for common NGS analyses using Amazon Web Services (AWS).\n\nCirrus-NGS provides primary analysis pipelines for RNA-Seq, miRNA-Seq, ChIP-Seq, and whole-genome/whole-exome sequencing data.  Cirrus users need not have any bioinformatics tools for these pipelines installed on their local machine, as all computation is performed using AWS clusters and all results are uploaded to AWS's S3 remote storage.  The clusters are created dynamically through Cirrus-NGS, based on a custom machine image with all the necessary software pre-installed.  Users manage clusters and run pipelines from within a web browser, using flexible but light-weight Jupyter notebooks.\n\n## Installation\n\n### Getting an AWS Account\n\nBecause Cirrus-NGS employs AWS for computation and storage, users must have an active AWS account.  If you do not have such an account, visit [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html) for guidance on how to create one, and execute the steps it describes. \n\n### Installing `pip`\n`pip` can be according to instructions from [https://pip.pypa.io/en/stable/installing/](https://pip.pypa.io/en/stable/installing/)\n\n### Installing `conda`\n\nWhile Cirrus-NGS itself is provied through github, its supporting libraries are best installed through conda, a cross-platform package manager.  If you do not have conda installed already, download the appropriate Python-3.6-based installer from the links below:\n\n* linux: [https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh](https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh)\n* OSX: [https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh](https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh)\n\nThen run the installer with the following script, replacing `<miniconda_py3.sh>` with the name of the install script downloaded above:\n\n\tbash <miniconda_py3.sh> -b -p $HOME/miniconda3\n\techo \"export PATH=\\\"$HOME/miniconda3/bin:\\$PATH\\\"\" >>$HOME/.bashrc\n\tsource $HOME/.bashrc\t\n\n### Installing Cirrus-NGS\n\nCirrus-NGS is available for linux-64 or osx-64 platforms, and requires python 3.5 or higher.\n\nIn the directory in which you would like to install Cirrus-NGS, run the following commands:\n\n\tconda install paramiko pyyaml git jupyter notebook\n\tpip install scp cfncluster\n\tgit clone https://github.com/ucsd-ccbb/Cirrus-NGS.git\n\nThese commands install the necessary supporting libraries and create a new directory called `Cirrus-NGS` that holds the Cirrus software.\n\nAlternatively, Cirrus-NGS can be installed with `pip` with the following commands:\n\n\tpip install cirrus-ngs\n\n\n### Starting the notebook server\n\nSince the Cirrus-NGS interface is provided through Jupyter notebooks, the first step of running Cirrus is to start a local Jupyter notebook server. From the `Cirrus-NGS` directory, run:\n\n\tcd notebooks/cirrus-ngs\n\tjupyter notebook\n\nYou will receive a message like\n\n\t[I 10:48:39.779 NotebookApp] Serving notebooks from local directory: /Users/<YourName>/Cirrus-NGS/notebooks\n\t[I 10:48:39.779 NotebookApp] 0 active kernels \n\t[I 10:48:39.779 NotebookApp] The Jupyter Notebook is running at: http://localhost:8889/?token=8dcb989e6852e3dfd679307470e17696c32771432c881573\n\t[I 10:48:39.779 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n\n**Note the URL at which the server is running**.  It is usually http://localhost:8888, but if your system already has something running on port 8888, it may default so something else (e.g., in the example above, it is http://localhost:8889).\n\n**Do not shut down the Jupyter notebook server** or close the terminal window in which it is running until you are done using Cirrus-NGS, as it must be running in order for the Cirrus-NGS notebooks to function.  When you are done using Cirrus and ready to shut down the notebook server, simply type `Control-C` in the terminal window where it is running and follow the prompts.\n\n## Running Cirrus-NGS\n\n### General Overview\n\nA user begins by defining a compute cluster.  After this, sh/e decides upon the pipeline to run, which determines the exact inputs required, and then creates (in a spreadsheet program or text editor) a design file that specifies the details of the input sequencing data.  S/he starts the Jupyter notebook for the chosen pipeline and inputs the path to the design file and other required inputs (such as AWS credentials, etc), then executes the notebook code to start the pipeline processing.  \n\nThe notebook creates a yaml file summarizing all of the user input and transfers that file to the cluster, where it directs cluster-native code to sequentially execute the analysis steps specified by the user in a distributed fashion. Upon completion of each step, the output is uploaded to the user's specified S3 output bucket, from which it can be accessed at any point. During processing, the user can check the status of the pipeline run from within the notebook.\n\n**A Note About Terminology**: Throughout, the term \"pipeline\" is used to describe functionality for analyzing a general type of sequencing (such as RNA-Seq or whole-genome sequencing).  Each pipeline can have multiple, more granular \"workflows\" that support different approaches to processing the same type of sequencing data; for instance, the RNA-Seq pipeline contains four different workflows (star_gatk, star_htseq, star_rsem, and kallisto).\n\n### Gathering Required Inputs\n\nThe following pieces of information are required by all functionality in Cirrus-NGS:\n\n1. The AWS access key ID for your AWS account\n\t* See [https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html](https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html) to find your existing access key or create a new one)\n2. The AWS secret access key for your AWS account (see above link for more information)\n3. A valid key pair (.pem) file for your AWS account\n\t* If you encounter a `Permission denied (publickey) error`, note that the permissions on your key (.pem) file **must** be set so that it is not public, e.g. by running `chmod 0400 <mypemfilename>.pem` in the directory where the .pem file is located.\n\t* See [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair) for further details.\n\nAdditional inputs are required for specific actions, and are discussed in the relevant sections below.\n\n### Defining a Cluster\n\nRun the following steps to define a compute cluster.  You may then use that cluster for all Cirrus-NGS work going forward without having to rerun these steps (although you may also rerun them to create additional clusters, if desired).\n\n1. Gather the required settings information; in addition to the valued described above in the [Gathering Required Inputs](#Gathering-Required-Inputs) section, you will need:\n\t* The AWS region in which you want to run your cluster\n\t\t* A table of available region values is shown at [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions).  **Note that different regions charge different prices for computing**, so do not choose one blindly.  In general, you will want to select the closest region to you geographically that has an acceptable price for the virtual computing nodes (called \"instances\") that you wish to run (see below for more details).\n\t* The instance type to use for the master node of your cluster\n\t\t* This node runs as long as the cluster is active, and directs the other nodes; because of its central role, it will always be run with AWS's standard, \"on-demand\" pricing.\n\t\t* Visit [https://aws.amazon.com/ec2/pricing/on-demand/](https://aws.amazon.com/ec2/pricing/on-demand/) and select a region relevant to you in the \"Region\" dropdown box in order to to view the per-hour prices for available instance types in that region.\n\t\t* Generally speaking, the master node will need to run on an instance with at least 2 vCPUs and at least 8 GiB of memory (such as an `m4.large`).\n\t* The instance type to use for the compute nodes of your cluster\n\t\t* These nodes are dynamically added to and removed from the cluster to accommodate the amount of calculation the cluster is asked to do.\n\t\t* Generally speaking, the compute nodes will need to have CPU and memory capacity greater than or equal to that of the master node.\n\t\t* These nodes are run with AWS's discounted \"spot\" pricing (see below for details).\n\t* The spot price for the compute nodes of your cluster\n\t\t* If AWS has left-over capacity of a particular instance type after filling all on-demand requests for that type, it sells the remaining capacity at a dynamically-set lower price; the \"spot price\" you specify is the maximum hourly cost, in US dollars, that you are willing to pay for each such instance (see [https://aws.amazon.com/ec2/faqs/](https://aws.amazon.com/ec2/faqs/) for details). \n\t\t* The going spot rate for a particular instance type in a particular region is constantly changing bsaed on demand.  Visit [https://aws.amazon.com/ec2/spot/pricing/](https://aws.amazon.com/ec2/spot/pricing/) (remember to select the correct region for you, as described above!) to view the current spot prices for instances available in your region.\n\t\t* If the going spot rate for your chosen compute instance type goes above your specified spot price, you will not be charged more, but your compute nodes will shut down without warning as they are re-allocated to another user.  Therefore, it is wise to choose a spot price that represents the maximum you are willing to pay, rather than under-bidding.\n\t* The number of compute nodes in your cluster when it is first started\n\t\t* If you do not have a good idea of how many you will need, it is fine to use zero by default.\n\t* The size in GiB of the virtual storage disk for the cluster\n\t\t* <!--TODO: Get more detail from Guorong-->  \n\t* The VPC (Virtual Private Cloud) id for your AWS account\n\t\t* If you do not know the VPC id, you can find it by going part-way through the process of launching an AWS compute instance.  Follow the instructions at [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launching-instance.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launching-instance.html) up until step 6 (it does not matter what values you select during these steps, as you won't need to actually launch the instance).  Step 6 brings you to the \"Configure Instance Details\" screen, on which you can find your VPC id, as well as your subnet ids (see next item):\n\n\t\t![Configure Instance Details screen](docs/configure_instance_details.png)\n\n\t* The subnet id for your AWS account for the region in which you wish to create the cluster (see above for how to find this value)\n2. In your browser, visit the address at which the Jupyter notebook server is running\n\t* This is usually http://localhost:8888, but see the [Starting the Notebook Server](#Starting-the-notebook-server) section for additional details.\n\t* You will see a list of all the Cirrus-NGS notebooks:\n\n\t![Cirrus-NGS notebooks list](docs/notebooks_list.png) \n2. Click on the `BasicCFNClusterSetup.ipynb` notebook to start it.\n3. Fill in the parameters in the first two cells in the notebook, using the values identified above in the [Gathering Required Inputs](#Gathering-Required-Inputs) section where relevant. <!--TODO:expand-->\n4. Run the notebook.<!--TODO:expand-->\n\n\n### Running a Pipeline\n\n1. Choose the pipeline you wish to run.\n\t* Cirrus-NGS currently offers pipelines for RNA-Seq, miRNA-seq, ChIP-seq, and variant calling in whole-genome/whole-exome DNA-Seq.\n2. Create a design file for your data.\n\t* The design file is a tab-separated text file with either two or three columns (depending on the workflow chosen) that specifies the names of the sequence files to process and the necessary metadata describing them.\n\t* See the [Building a Design File](#Building-a-Design-File) section below for full specifications of the design file format.\n3. Gather the required settings information; in addition to the valued described above in the [Gathering Required Inputs](#Gathering-Required-Inputs) section, you will need:\n\t* The URL of an existing AWS S3 remote storage bucket where you have placed your sequencing data.\n\t\t* Visit [https://docs.aws.amazon.com/AmazonS3/latest/gsg/GetStartedWithS3.html](https://docs.aws.amazon.com/AmazonS3/latest/gsg/GetStartedWithS3.html) and execute the \"Sign Up for Amazon S3\" and \"Create a Bucket\" steps (click on the boxes in the workflow diagram for detailed instructions) if you do not already have a bucket set up.\n\t\t* To place your sequencing files into the bucket, follow the directions for the \"Add an Object to a Bucket\" step, or use an S3-enabled transfer client like [Cyberduck](https://cyberduck.io/).\n\t* (Optional) The URL of a second existing AWS S3 remote storage bucket for Cirrus-NGS outputs.\n3. In your browser, visit the address at which the Jupyter notebook server is running\n\t* This is usually http://localhost:8888, but see the [Starting the Notebook Server](#Starting-the-notebook-server) section for additional details.\n\t* You will see a list of all the Cirrus-NGS notebooks:\n\n\t![Cirrus-NGS notebooks list](docs/notebooks_list.png)\n4. Click on the appropriate notebook for your chosen pipeline to start it.\n5. Fill in the parameters in the first two cells in the notebook, using the values identified above in the [Gathering Required Inputs](#Gathering-Required-Inputs) section where relevant.<!--TODO:expand-->\n6. Run the notebook.<!--TODO:expand-->\n7. After the pipeline processing is complete, download the output files from your S3 bucket for further analysis locally.\n\t* To download the outputs from your bucket, follow the directions for the \"View an Object\" step at [https://docs.aws.amazon.com/AmazonS3/latest/gsg/OpeningAnObject.html](https://docs.aws.amazon.com/AmazonS3/latest/gsg/OpeningAnObject.html) or use an S3-enabled transfer client like [Cyberduck](https://cyberduck.io/).\n\n## Building a Design File\nThe design file is the primary user input to Cirrus-NGS. It is a tab-separated text file that specifies the names of the sequence files to process and the necessary metadata describing them.  For the RNA-Seq and miRNA-Seq pipelines, it contains two columns, while for the ChIP-Seq and whole-exome/whole-genome sequencing pipelines, it contains three columns (of which the first two are the same as in the two-column format).  The design file has no header line.\n\n### Two-column format\nIn the two-column format, the **first column** is the filename of the sample (with extensions: e.g. fastq.gz), while the **second column** is the name of the group associated with that sample.  (Group names are up to the user, but they are generally set to experimental conditions. For example, all control samples can be given a group named \"control\".)\n\nFor example, a two-column design file for two single-end-sequenced samples named `mysample1` and `mysample2` might look like:\n\n```\n\tmysample1.fastq.gz\t\tgroupA\n\tmysample2.fastq.gz\t\tgroupB\n```\n\nIf the sequencing data is paired-end, the first column includes the name of the forward sequencing file, followed by a comma, followed by the name of the reverse sequencing file (note that there must **not** be any spaces between these two file names--only a comma!)  An example two-column design file for two paired-end-sequenced samples named `mysample1` and `mysample2` might look like:\n\n```\n\tmysample1_fwd.fastq.gz,mysample1_rev.fastq.gz\t\tgroupA\n\tmysample2_fwd.fastq.gz,mysample2_rev.fastq.gz\t\tgroupB\n```\n\n### Three-column format\nThe three-column format has the same first two columns as the two-column format, but adds a third column containing a pipeline-specific identifier that differentiates sample types for the ChIP-Seq and whole-genome/whole-exome sequencing pipelines.\n\n* For the ChIP-Seq pipeline, each sample is identified as either Chip or Input.\n* For the whole-genome/whole-exome sequencing pipeline, each sample is identified as either Tumor or Normal.\n\nThe following constraints apply to three-column design files:\n\n* The sample type identifiers are **case-sensitive**.\n\t* For example, a three-column design file for two paired-end-sequenced ChIP-Seq samples named `mysample1` and `mysample2` might look like \n\n\t```\n\tmysample1_fwd.fastq.gz,mysample1_rev.fastq.gz\t\tgroupA\t\tChip\n\tmysample2_fwd.fastq.gz,mysample2_rev.fastq.gz\t\tgroupB\t\tInput\n\t```\n\n* Each three-column design file must use **either** Chip and Input sample type identifiers **or** Normal and Tumor sample type identifiers, but not both.\n\t* A design file like the example below would thus be **invalid**:\n\n\t```\n\tmysample1_fwd.fastq.gz,mysample1_rev.fastq.gz\t\tbadExample1\t\tChip\n\tmysample2_fwd.fastq.gz,mysample2_rev.fastq.gz\t\tbadExample1\t\tTumor\n\t```\n* Each group must have exactly one sample designed with each of the two relevant sample type identifiers.\t* A design file like the example below would thus be **invalid**:\n\n\t```\n\tmysample1_fwd.fastq.gz,mysample1_rev.fastq.gz\t\tbadExample2\t\tNormal\n\tmysample2_fwd.fastq.gz,mysample2_rev.fastq.gz\t\tbadExample2\t\tNormal\n\t```\t\n* If two samples are designated as forming a Chip/Input or Normal/Tumor pair, they must both have the same group.\n\t* A design file like the example below would thus be **invalid**:\n\n\t```\n\tmysample1_fwd.fastq.gz,mysample1_rev.fastq.gz\t\tbadExample3\t\tNormal\n\tmysample2_fwd.fastq.gz,mysample2_rev.fastq.gz\t\tbadExample4\t\tTumor\n\t```\n\n\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ucsd-ccbb/cirrus-ngs", "keywords": "bioinformatics,NGS,jupyter", "license": "MIT", "maintainer": "Mustafa Guler", "maintainer_email": "mguler@ucsd.edu", "name": "cirrus-ngs", "package_url": "https://pypi.org/project/cirrus-ngs/", "platform": "", "project_url": "https://pypi.org/project/cirrus-ngs/", "project_urls": {"Homepage": "https://github.com/ucsd-ccbb/cirrus-ngs"}, "release_url": "https://pypi.org/project/cirrus-ngs/1.0.0a1/", "requires_dist": ["paramiko", "pyyaml", "jupyter", "notebook", "scp", "cfncluster"], "requires_python": ">=3.5", "summary": "Cloud-optimized primary analysis pipelines for RNA-seq, miRNA-seq, ChIP-seq, and variant calling in whole-genome/whole-exome DNA-seq", "version": "1.0.0a1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Cirrus-NGS</h1>\n<p>Cloud-optimized primary analysis pipelines for RNA-seq, miRNA-seq, ChIP-seq, and variant calling in whole-genome/whole-exome DNA-Seq.</p>\n<h2>Introduction</h2>\n<p>Bionformatic analysis of large-scale next-generation sequencing (NGS) data requires significant compute resources. While cloud computing makes such processing power available on-demand, the administration of dynamic compute clusters is a daunting task for most working biologists.  To address this pain point, the Center for Computational Biology and Bioinformatics at the University of California, San Diego, has developed Cirrus-NGS, a turn-key solution for common NGS analyses using Amazon Web Services (AWS).</p>\n<p>Cirrus-NGS provides primary analysis pipelines for RNA-Seq, miRNA-Seq, ChIP-Seq, and whole-genome/whole-exome sequencing data.  Cirrus users need not have any bioinformatics tools for these pipelines installed on their local machine, as all computation is performed using AWS clusters and all results are uploaded to AWS's S3 remote storage.  The clusters are created dynamically through Cirrus-NGS, based on a custom machine image with all the necessary software pre-installed.  Users manage clusters and run pipelines from within a web browser, using flexible but light-weight Jupyter notebooks.</p>\n<h2>Installation</h2>\n<h3>Getting an AWS Account</h3>\n<p>Because Cirrus-NGS employs AWS for computation and storage, users must have an active AWS account.  If you do not have such an account, visit <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html\" rel=\"nofollow\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html</a> for guidance on how to create one, and execute the steps it describes.</p>\n<h3>Installing <code>pip</code></h3>\n<p><code>pip</code> can be according to instructions from <a href=\"https://pip.pypa.io/en/stable/installing/\" rel=\"nofollow\">https://pip.pypa.io/en/stable/installing/</a></p>\n<h3>Installing <code>conda</code></h3>\n<p>While Cirrus-NGS itself is provied through github, its supporting libraries are best installed through conda, a cross-platform package manager.  If you do not have conda installed already, download the appropriate Python-3.6-based installer from the links below:</p>\n<ul>\n<li>linux: <a href=\"https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\" rel=\"nofollow\">https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh</a></li>\n<li>OSX: <a href=\"https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\" rel=\"nofollow\">https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh</a></li>\n</ul>\n<p>Then run the installer with the following script, replacing <code>&lt;miniconda_py3.sh&gt;</code> with the name of the install script downloaded above:</p>\n<pre><code>bash &lt;miniconda_py3.sh&gt; -b -p $HOME/miniconda3\necho \"export PATH=\\\"$HOME/miniconda3/bin:\\$PATH\\\"\" &gt;&gt;$HOME/.bashrc\nsource $HOME/.bashrc\t\n</code></pre>\n<h3>Installing Cirrus-NGS</h3>\n<p>Cirrus-NGS is available for linux-64 or osx-64 platforms, and requires python 3.5 or higher.</p>\n<p>In the directory in which you would like to install Cirrus-NGS, run the following commands:</p>\n<pre><code>conda install paramiko pyyaml git jupyter notebook\npip install scp cfncluster\ngit clone https://github.com/ucsd-ccbb/Cirrus-NGS.git\n</code></pre>\n<p>These commands install the necessary supporting libraries and create a new directory called <code>Cirrus-NGS</code> that holds the Cirrus software.</p>\n<p>Alternatively, Cirrus-NGS can be installed with <code>pip</code> with the following commands:</p>\n<pre><code>pip install cirrus-ngs\n</code></pre>\n<h3>Starting the notebook server</h3>\n<p>Since the Cirrus-NGS interface is provided through Jupyter notebooks, the first step of running Cirrus is to start a local Jupyter notebook server. From the <code>Cirrus-NGS</code> directory, run:</p>\n<pre><code>cd notebooks/cirrus-ngs\njupyter notebook\n</code></pre>\n<p>You will receive a message like</p>\n<pre><code>[I 10:48:39.779 NotebookApp] Serving notebooks from local directory: /Users/&lt;YourName&gt;/Cirrus-NGS/notebooks\n[I 10:48:39.779 NotebookApp] 0 active kernels \n[I 10:48:39.779 NotebookApp] The Jupyter Notebook is running at: http://localhost:8889/?token=8dcb989e6852e3dfd679307470e17696c32771432c881573\n[I 10:48:39.779 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n</code></pre>\n<p><strong>Note the URL at which the server is running</strong>.  It is usually <a href=\"http://localhost:8888\" rel=\"nofollow\">http://localhost:8888</a>, but if your system already has something running on port 8888, it may default so something else (e.g., in the example above, it is <a href=\"http://localhost:8889\" rel=\"nofollow\">http://localhost:8889</a>).</p>\n<p><strong>Do not shut down the Jupyter notebook server</strong> or close the terminal window in which it is running until you are done using Cirrus-NGS, as it must be running in order for the Cirrus-NGS notebooks to function.  When you are done using Cirrus and ready to shut down the notebook server, simply type <code>Control-C</code> in the terminal window where it is running and follow the prompts.</p>\n<h2>Running Cirrus-NGS</h2>\n<h3>General Overview</h3>\n<p>A user begins by defining a compute cluster.  After this, sh/e decides upon the pipeline to run, which determines the exact inputs required, and then creates (in a spreadsheet program or text editor) a design file that specifies the details of the input sequencing data.  S/he starts the Jupyter notebook for the chosen pipeline and inputs the path to the design file and other required inputs (such as AWS credentials, etc), then executes the notebook code to start the pipeline processing.</p>\n<p>The notebook creates a yaml file summarizing all of the user input and transfers that file to the cluster, where it directs cluster-native code to sequentially execute the analysis steps specified by the user in a distributed fashion. Upon completion of each step, the output is uploaded to the user's specified S3 output bucket, from which it can be accessed at any point. During processing, the user can check the status of the pipeline run from within the notebook.</p>\n<p><strong>A Note About Terminology</strong>: Throughout, the term \"pipeline\" is used to describe functionality for analyzing a general type of sequencing (such as RNA-Seq or whole-genome sequencing).  Each pipeline can have multiple, more granular \"workflows\" that support different approaches to processing the same type of sequencing data; for instance, the RNA-Seq pipeline contains four different workflows (star_gatk, star_htseq, star_rsem, and kallisto).</p>\n<h3>Gathering Required Inputs</h3>\n<p>The following pieces of information are required by all functionality in Cirrus-NGS:</p>\n<ol>\n<li>The AWS access key ID for your AWS account\n<ul>\n<li>See <a href=\"https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html\" rel=\"nofollow\">https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html</a> to find your existing access key or create a new one)</li>\n</ul>\n</li>\n<li>The AWS secret access key for your AWS account (see above link for more information)</li>\n<li>A valid key pair (.pem) file for your AWS account\n<ul>\n<li>If you encounter a <code>Permission denied (publickey) error</code>, note that the permissions on your key (.pem) file <strong>must</strong> be set so that it is not public, e.g. by running <code>chmod 0400 &lt;mypemfilename&gt;.pem</code> in the directory where the .pem file is located.</li>\n<li>See <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair\" rel=\"nofollow\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair</a> for further details.</li>\n</ul>\n</li>\n</ol>\n<p>Additional inputs are required for specific actions, and are discussed in the relevant sections below.</p>\n<h3>Defining a Cluster</h3>\n<p>Run the following steps to define a compute cluster.  You may then use that cluster for all Cirrus-NGS work going forward without having to rerun these steps (although you may also rerun them to create additional clusters, if desired).</p>\n<ol>\n<li>\n<p>Gather the required settings information; in addition to the valued described above in the <a href=\"#Gathering-Required-Inputs\" rel=\"nofollow\">Gathering Required Inputs</a> section, you will need:</p>\n<ul>\n<li>\n<p>The AWS region in which you want to run your cluster</p>\n<ul>\n<li>A table of available region values is shown at <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions\" rel=\"nofollow\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions</a>.  <strong>Note that different regions charge different prices for computing</strong>, so do not choose one blindly.  In general, you will want to select the closest region to you geographically that has an acceptable price for the virtual computing nodes (called \"instances\") that you wish to run (see below for more details).</li>\n</ul>\n</li>\n<li>\n<p>The instance type to use for the master node of your cluster</p>\n<ul>\n<li>This node runs as long as the cluster is active, and directs the other nodes; because of its central role, it will always be run with AWS's standard, \"on-demand\" pricing.</li>\n<li>Visit <a href=\"https://aws.amazon.com/ec2/pricing/on-demand/\" rel=\"nofollow\">https://aws.amazon.com/ec2/pricing/on-demand/</a> and select a region relevant to you in the \"Region\" dropdown box in order to to view the per-hour prices for available instance types in that region.</li>\n<li>Generally speaking, the master node will need to run on an instance with at least 2 vCPUs and at least 8 GiB of memory (such as an <code>m4.large</code>).</li>\n</ul>\n</li>\n<li>\n<p>The instance type to use for the compute nodes of your cluster</p>\n<ul>\n<li>These nodes are dynamically added to and removed from the cluster to accommodate the amount of calculation the cluster is asked to do.</li>\n<li>Generally speaking, the compute nodes will need to have CPU and memory capacity greater than or equal to that of the master node.</li>\n<li>These nodes are run with AWS's discounted \"spot\" pricing (see below for details).</li>\n</ul>\n</li>\n<li>\n<p>The spot price for the compute nodes of your cluster</p>\n<ul>\n<li>If AWS has left-over capacity of a particular instance type after filling all on-demand requests for that type, it sells the remaining capacity at a dynamically-set lower price; the \"spot price\" you specify is the maximum hourly cost, in US dollars, that you are willing to pay for each such instance (see <a href=\"https://aws.amazon.com/ec2/faqs/\" rel=\"nofollow\">https://aws.amazon.com/ec2/faqs/</a> for details).</li>\n<li>The going spot rate for a particular instance type in a particular region is constantly changing bsaed on demand.  Visit <a href=\"https://aws.amazon.com/ec2/spot/pricing/\" rel=\"nofollow\">https://aws.amazon.com/ec2/spot/pricing/</a> (remember to select the correct region for you, as described above!) to view the current spot prices for instances available in your region.</li>\n<li>If the going spot rate for your chosen compute instance type goes above your specified spot price, you will not be charged more, but your compute nodes will shut down without warning as they are re-allocated to another user.  Therefore, it is wise to choose a spot price that represents the maximum you are willing to pay, rather than under-bidding.</li>\n</ul>\n</li>\n<li>\n<p>The number of compute nodes in your cluster when it is first started</p>\n<ul>\n<li>If you do not have a good idea of how many you will need, it is fine to use zero by default.</li>\n</ul>\n</li>\n<li>\n<p>The size in GiB of the virtual storage disk for the cluster</p>\n<ul>\n<li>\n  \n</li>\n</ul>\n</li>\n<li>\n<p>The VPC (Virtual Private Cloud) id for your AWS account</p>\n<ul>\n<li>If you do not know the VPC id, you can find it by going part-way through the process of launching an AWS compute instance.  Follow the instructions at <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launching-instance.html\" rel=\"nofollow\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launching-instance.html</a> up until step 6 (it does not matter what values you select during these steps, as you won't need to actually launch the instance).  Step 6 brings you to the \"Configure Instance Details\" screen, on which you can find your VPC id, as well as your subnet ids (see next item):</li>\n</ul>\n<p><img alt=\"Configure Instance Details screen\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0cf0492561532796676db1aec04590008ed911c8/646f63732f636f6e6669677572655f696e7374616e63655f64657461696c732e706e67\"></p>\n</li>\n<li>\n<p>The subnet id for your AWS account for the region in which you wish to create the cluster (see above for how to find this value)</p>\n</li>\n</ul>\n</li>\n<li>\n<p>In your browser, visit the address at which the Jupyter notebook server is running</p>\n<ul>\n<li>This is usually <a href=\"http://localhost:8888\" rel=\"nofollow\">http://localhost:8888</a>, but see the <a href=\"#Starting-the-notebook-server\" rel=\"nofollow\">Starting the Notebook Server</a> section for additional details.</li>\n<li>You will see a list of all the Cirrus-NGS notebooks:</li>\n</ul>\n<p><img alt=\"Cirrus-NGS notebooks list\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/dbb3489993d87e7a50e6509af260e762967f0fed/646f63732f6e6f7465626f6f6b735f6c6973742e706e67\"></p>\n</li>\n<li>\n<p>Click on the <code>BasicCFNClusterSetup.ipynb</code> notebook to start it.</p>\n</li>\n<li>\n<p>Fill in the parameters in the first two cells in the notebook, using the values identified above in the <a href=\"#Gathering-Required-Inputs\" rel=\"nofollow\">Gathering Required Inputs</a> section where relevant. </p>\n</li>\n<li>\n<p>Run the notebook.</p>\n</li>\n</ol>\n<h3>Running a Pipeline</h3>\n<ol>\n<li>\n<p>Choose the pipeline you wish to run.</p>\n<ul>\n<li>Cirrus-NGS currently offers pipelines for RNA-Seq, miRNA-seq, ChIP-seq, and variant calling in whole-genome/whole-exome DNA-Seq.</li>\n</ul>\n</li>\n<li>\n<p>Create a design file for your data.</p>\n<ul>\n<li>The design file is a tab-separated text file with either two or three columns (depending on the workflow chosen) that specifies the names of the sequence files to process and the necessary metadata describing them.</li>\n<li>See the <a href=\"#Building-a-Design-File\" rel=\"nofollow\">Building a Design File</a> section below for full specifications of the design file format.</li>\n</ul>\n</li>\n<li>\n<p>Gather the required settings information; in addition to the valued described above in the <a href=\"#Gathering-Required-Inputs\" rel=\"nofollow\">Gathering Required Inputs</a> section, you will need:</p>\n<ul>\n<li>The URL of an existing AWS S3 remote storage bucket where you have placed your sequencing data.\n<ul>\n<li>Visit <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/gsg/GetStartedWithS3.html\" rel=\"nofollow\">https://docs.aws.amazon.com/AmazonS3/latest/gsg/GetStartedWithS3.html</a> and execute the \"Sign Up for Amazon S3\" and \"Create a Bucket\" steps (click on the boxes in the workflow diagram for detailed instructions) if you do not already have a bucket set up.</li>\n<li>To place your sequencing files into the bucket, follow the directions for the \"Add an Object to a Bucket\" step, or use an S3-enabled transfer client like <a href=\"https://cyberduck.io/\" rel=\"nofollow\">Cyberduck</a>.</li>\n</ul>\n</li>\n<li>(Optional) The URL of a second existing AWS S3 remote storage bucket for Cirrus-NGS outputs.</li>\n</ul>\n</li>\n<li>\n<p>In your browser, visit the address at which the Jupyter notebook server is running</p>\n<ul>\n<li>This is usually <a href=\"http://localhost:8888\" rel=\"nofollow\">http://localhost:8888</a>, but see the <a href=\"#Starting-the-notebook-server\" rel=\"nofollow\">Starting the Notebook Server</a> section for additional details.</li>\n<li>You will see a list of all the Cirrus-NGS notebooks:</li>\n</ul>\n<p><img alt=\"Cirrus-NGS notebooks list\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/dbb3489993d87e7a50e6509af260e762967f0fed/646f63732f6e6f7465626f6f6b735f6c6973742e706e67\"></p>\n</li>\n<li>\n<p>Click on the appropriate notebook for your chosen pipeline to start it.</p>\n</li>\n<li>\n<p>Fill in the parameters in the first two cells in the notebook, using the values identified above in the <a href=\"#Gathering-Required-Inputs\" rel=\"nofollow\">Gathering Required Inputs</a> section where relevant.</p>\n</li>\n<li>\n<p>Run the notebook.</p>\n</li>\n<li>\n<p>After the pipeline processing is complete, download the output files from your S3 bucket for further analysis locally.</p>\n<ul>\n<li>To download the outputs from your bucket, follow the directions for the \"View an Object\" step at <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/gsg/OpeningAnObject.html\" rel=\"nofollow\">https://docs.aws.amazon.com/AmazonS3/latest/gsg/OpeningAnObject.html</a> or use an S3-enabled transfer client like <a href=\"https://cyberduck.io/\" rel=\"nofollow\">Cyberduck</a>.</li>\n</ul>\n</li>\n</ol>\n<h2>Building a Design File</h2>\n<p>The design file is the primary user input to Cirrus-NGS. It is a tab-separated text file that specifies the names of the sequence files to process and the necessary metadata describing them.  For the RNA-Seq and miRNA-Seq pipelines, it contains two columns, while for the ChIP-Seq and whole-exome/whole-genome sequencing pipelines, it contains three columns (of which the first two are the same as in the two-column format).  The design file has no header line.</p>\n<h3>Two-column format</h3>\n<p>In the two-column format, the <strong>first column</strong> is the filename of the sample (with extensions: e.g. fastq.gz), while the <strong>second column</strong> is the name of the group associated with that sample.  (Group names are up to the user, but they are generally set to experimental conditions. For example, all control samples can be given a group named \"control\".)</p>\n<p>For example, a two-column design file for two single-end-sequenced samples named <code>mysample1</code> and <code>mysample2</code> might look like:</p>\n<pre><code>\tmysample1.fastq.gz\t\tgroupA\n\tmysample2.fastq.gz\t\tgroupB\n</code></pre>\n<p>If the sequencing data is paired-end, the first column includes the name of the forward sequencing file, followed by a comma, followed by the name of the reverse sequencing file (note that there must <strong>not</strong> be any spaces between these two file names--only a comma!)  An example two-column design file for two paired-end-sequenced samples named <code>mysample1</code> and <code>mysample2</code> might look like:</p>\n<pre><code>\tmysample1_fwd.fastq.gz,mysample1_rev.fastq.gz\t\tgroupA\n\tmysample2_fwd.fastq.gz,mysample2_rev.fastq.gz\t\tgroupB\n</code></pre>\n<h3>Three-column format</h3>\n<p>The three-column format has the same first two columns as the two-column format, but adds a third column containing a pipeline-specific identifier that differentiates sample types for the ChIP-Seq and whole-genome/whole-exome sequencing pipelines.</p>\n<ul>\n<li>For the ChIP-Seq pipeline, each sample is identified as either Chip or Input.</li>\n<li>For the whole-genome/whole-exome sequencing pipeline, each sample is identified as either Tumor or Normal.</li>\n</ul>\n<p>The following constraints apply to three-column design files:</p>\n<ul>\n<li>\n<p>The sample type identifiers are <strong>case-sensitive</strong>.</p>\n<ul>\n<li>For example, a three-column design file for two paired-end-sequenced ChIP-Seq samples named <code>mysample1</code> and <code>mysample2</code> might look like</li>\n</ul>\n<pre><code> mysample1_fwd.fastq.gz,mysample1_rev.fastq.gz\t\tgroupA\t\tChip\n mysample2_fwd.fastq.gz,mysample2_rev.fastq.gz\t\tgroupB\t\tInput\n</code></pre>\n</li>\n<li>\n<p>Each three-column design file must use <strong>either</strong> Chip and Input sample type identifiers <strong>or</strong> Normal and Tumor sample type identifiers, but not both.</p>\n<ul>\n<li>A design file like the example below would thus be <strong>invalid</strong>:</li>\n</ul>\n<pre><code> mysample1_fwd.fastq.gz,mysample1_rev.fastq.gz\t\tbadExample1\t\tChip\n mysample2_fwd.fastq.gz,mysample2_rev.fastq.gz\t\tbadExample1\t\tTumor\n</code></pre>\n</li>\n<li>\n<p>Each group must have exactly one sample designed with each of the two relevant sample type identifiers.\t* A design file like the example below would thus be <strong>invalid</strong>:</p>\n<pre><code> mysample1_fwd.fastq.gz,mysample1_rev.fastq.gz\t\tbadExample2\t\tNormal\n mysample2_fwd.fastq.gz,mysample2_rev.fastq.gz\t\tbadExample2\t\tNormal\n</code></pre>\n</li>\n<li>\n<p>If two samples are designated as forming a Chip/Input or Normal/Tumor pair, they must both have the same group.</p>\n<ul>\n<li>A design file like the example below would thus be <strong>invalid</strong>:</li>\n</ul>\n<pre><code> mysample1_fwd.fastq.gz,mysample1_rev.fastq.gz\t\tbadExample3\t\tNormal\n mysample2_fwd.fastq.gz,mysample2_rev.fastq.gz\t\tbadExample4\t\tTumor\n</code></pre>\n</li>\n</ul>\n\n          </div>"}, "last_serial": 3898572, "releases": {"1.0.0a1": [{"comment_text": "", "digests": {"md5": "2c31b4826573d7f8267eaff85acff058", "sha256": "ed64f282cee081ea1786a87328a64df645db018bcb831c00a4c0912a7f545ae6"}, "downloads": -1, "filename": "cirrus_ngs-1.0.0a1-py3-none-any.whl", "has_sig": false, "md5_digest": "2c31b4826573d7f8267eaff85acff058", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 110197, "upload_time": "2018-05-25T12:00:12", "upload_time_iso_8601": "2018-05-25T12:00:12.741562Z", "url": "https://files.pythonhosted.org/packages/eb/fb/34d98ecf2b2bcf9a4219a13ca4647efa11d1b72d1d62c63195a23714d8f6/cirrus_ngs-1.0.0a1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e6f227657af48462fcf08c0c14c5d662", "sha256": "70d97589494709d581bd4acff3c23b5843beb5ca75a8f896ccba61840947e0f4"}, "downloads": -1, "filename": "cirrus-ngs-1.0.0a1.tar.gz", "has_sig": false, "md5_digest": "e6f227657af48462fcf08c0c14c5d662", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 59127, "upload_time": "2018-05-25T12:00:14", "upload_time_iso_8601": "2018-05-25T12:00:14.152963Z", "url": "https://files.pythonhosted.org/packages/4c/30/1365c50321b0d1f3fe229f92cd4138769677d4506991ec99bd1412e3701a/cirrus-ngs-1.0.0a1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2c31b4826573d7f8267eaff85acff058", "sha256": "ed64f282cee081ea1786a87328a64df645db018bcb831c00a4c0912a7f545ae6"}, "downloads": -1, "filename": "cirrus_ngs-1.0.0a1-py3-none-any.whl", "has_sig": false, "md5_digest": "2c31b4826573d7f8267eaff85acff058", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 110197, "upload_time": "2018-05-25T12:00:12", "upload_time_iso_8601": "2018-05-25T12:00:12.741562Z", "url": "https://files.pythonhosted.org/packages/eb/fb/34d98ecf2b2bcf9a4219a13ca4647efa11d1b72d1d62c63195a23714d8f6/cirrus_ngs-1.0.0a1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e6f227657af48462fcf08c0c14c5d662", "sha256": "70d97589494709d581bd4acff3c23b5843beb5ca75a8f896ccba61840947e0f4"}, "downloads": -1, "filename": "cirrus-ngs-1.0.0a1.tar.gz", "has_sig": false, "md5_digest": "e6f227657af48462fcf08c0c14c5d662", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 59127, "upload_time": "2018-05-25T12:00:14", "upload_time_iso_8601": "2018-05-25T12:00:14.152963Z", "url": "https://files.pythonhosted.org/packages/4c/30/1365c50321b0d1f3fe229f92cd4138769677d4506991ec99bd1412e3701a/cirrus-ngs-1.0.0a1.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:19:19 2020"}