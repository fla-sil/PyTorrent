{"info": {"author": "David Reiman", "author_email": "dreiman@ucsc.edu", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Programming Language :: Python :: 3"], "description": "<p align=\"center\">\n  <img src=\"docs/images/logo.png\">\n</p>\n\n---\n\n<p align=\"center\">\nA deep learning project template for TensorFlow. Utilities for TFDataset handling and hyperparameter optimization.\n</p>\n\n## Getting Started\n\nFork the repository, navigate to the local directory and follow the quick start checklist below. Data should be divided into training, validation and testing sets and placed in .tfrecords file format in separate directories. Utilities for converting NumPy arrays to .tfrecords files are included in the utils.py module.\n\n### Quick Start Checklist\n\n- [ ] Build neural network models in models.py\n    - **Note:** hyperparameters that will be optimized later need to be specified. See **Model Selection** section below.\n- [ ] Connect models, define loss, evaluation metrics, optimizer, etc. in graph.py\n    - **Note:** hyperparameters that will be optimized later need to be specified. See **Model Selection** section below.\n- [ ] Specify input data shapes in data_shapes dictionary\n    - **Note:** Data shapes should not include batch size\u00e2\u20ac\u201dthis information is passed to the sampler instead\n    - **Additional note:** the order in data_shapes should correspond to the order you retrieve the tensors in the graph\n      - In **main.py:** ```data_shapes = {'lowres': (32, 32, 3), 'highres': (128, 128, 3)}```\n      - In **graph.py:** ```lowres, highres = data.get_batch()```\n- [ ] Create a DataSampler object with the filepaths to your train/valid/test sets and the data shapes dictionary.\n- [ ] Define hyperparameters to optimize over and their corresponding domain ranges in dictionary\n- [ ] Pass your graph object and hyperparameter dictionary to Sherpa via ```gilgalad.opt.bayesian_optimization```\n- [ ] (Optional) Add custom plotting functionality in plotting.py. Send any matplotlib figure to TensorBoard via tfplot.\n\n\n## Model Selection\n\nGil-Galad model selection is employed via Sherpa's Bayesian optimization suite which utilizes sklearn's Gaussian process module. Bayesian optimization specifies a distribution over functions via a kernel function and prior. Here, the mean function corresponds to a surrogate objective function whose predictor variables are the model hyperparameters. The prior distribution over functions is updated via Bayes' rule to account for trial runs wherein the independent variables specify the model and the dependent variable is the evaluation of such a model on the validation dataset.\n\nWith Gil-Galad, we specify which hyperparameters we will optimize by passing a parameter dictionary to our graph class while also defining default hyperparameters during graph and model construction as follows:\n\n### Graph-level\n\n```python\nclass Graph(BaseGraph):\n\n    def __init__(self, network, sampler, logdir=None, ckptdir=None):\n\n        self.network = network\n        self.data = sampler\n        self.logdir = logdir\n        self.ckptdir = ckptdir\n\n        self.build_graph()\n\n    def build_graph(self, params=None):\n\n        tf.reset_default_graph()\n        self.data.initialize()\n\n        self.x, self.y, self.z = self.data.get_batch()\n\n        self.y_ = self.network(self.x, params=params)\n\n        self.loss = tf.losses.mean_squared_error(self.y, self.y_)\n\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            opt = tf.train.AdamOptimizer(\n                learning_rate=params['lr'] if params else 0.001\n            )\n\n            self.update = opt.minimize(\n                loss=self.loss,\n                var_list=self.network.vars,\n                global_step=self.global_step\n            )\n\n```\n\n### Model-level\n\n```python\nclass Model(BaseModel):\n  def __init__(self, name):\n    self.name = name\n\n  def __call__(self, x, params):\n    with tf.variable_scope(self.name) as vs:\n      y = conv_2d(\n        x=x,\n        filters=params['filters'] if params else 64,\n        kernel_size=params['kernel_size'] if params else 3,\n        strides=2,\n        activation=params['activation'] if params else 'relu'\n      )\n\n      return y\n\n```\n\nWe then define the hyperparameter domain type and ranges in a dictionary. This information accompanies the graph object as arguments for the Bayesian optimization function.\n\n```python\nimport gilgalad as gg\n\nhyperparameters = {\n    'Discrete':\n        {'filters': [64, 128],\n         'kernel_size': [3, 5]},\n    'Continuous':\n        {'lr': [1e-5, 1e-3]},\n    'Choice':\n        {'activation': ['relu', 'prelu']}\n}\n\nbest_model = gg.opt.bayesian_optimization(\n    graph=graph,\n    params=hyperparameters,\n    max_trials=50\n)\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/davidreiman/Gil-Galad", "keywords": "deep-learning tensorflow machine-learning", "license": "GPLv3", "maintainer": "David Reiman", "maintainer_email": "dreiman@ucsc.edu", "name": "gil-galad", "package_url": "https://pypi.org/project/gil-galad/", "platform": "", "project_url": "https://pypi.org/project/gil-galad/", "project_urls": {"Homepage": "https://github.com/davidreiman/Gil-Galad"}, "release_url": "https://pypi.org/project/gil-galad/0.1/", "requires_dist": ["tensorflow (>=1.7.0)", "numpy (>=1.13.3)", "parameter-sherpa", "tqdm", "progress"], "requires_python": "", "summary": "A deep learning project template for TensorFlow", "version": "0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p align=\"center\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3931901145dfb5ab5151ad368576e6e623c1a22a/646f63732f696d616765732f6c6f676f2e706e67\">\n</p>\n<hr>\n<p align=\"center\">\nA deep learning project template for TensorFlow. Utilities for TFDataset handling and hyperparameter optimization.\n</p>\n<h2>Getting Started</h2>\n<p>Fork the repository, navigate to the local directory and follow the quick start checklist below. Data should be divided into training, validation and testing sets and placed in .tfrecords file format in separate directories. Utilities for converting NumPy arrays to .tfrecords files are included in the utils.py module.</p>\n<h3>Quick Start Checklist</h3>\n<ul>\n<li>[ ] Build neural network models in models.py\n<ul>\n<li><strong>Note:</strong> hyperparameters that will be optimized later need to be specified. See <strong>Model Selection</strong> section below.</li>\n</ul>\n</li>\n<li>[ ] Connect models, define loss, evaluation metrics, optimizer, etc. in graph.py\n<ul>\n<li><strong>Note:</strong> hyperparameters that will be optimized later need to be specified. See <strong>Model Selection</strong> section below.</li>\n</ul>\n</li>\n<li>[ ] Specify input data shapes in data_shapes dictionary\n<ul>\n<li><strong>Note:</strong> Data shapes should not include batch size\u00e2\u20ac\u201dthis information is passed to the sampler instead</li>\n<li><strong>Additional note:</strong> the order in data_shapes should correspond to the order you retrieve the tensors in the graph\n<ul>\n<li>In <strong>main.py:</strong> <code>data_shapes = {'lowres': (32, 32, 3), 'highres': (128, 128, 3)}</code></li>\n<li>In <strong>graph.py:</strong> <code>lowres, highres = data.get_batch()</code></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>[ ] Create a DataSampler object with the filepaths to your train/valid/test sets and the data shapes dictionary.</li>\n<li>[ ] Define hyperparameters to optimize over and their corresponding domain ranges in dictionary</li>\n<li>[ ] Pass your graph object and hyperparameter dictionary to Sherpa via <code>gilgalad.opt.bayesian_optimization</code></li>\n<li>[ ] (Optional) Add custom plotting functionality in plotting.py. Send any matplotlib figure to TensorBoard via tfplot.</li>\n</ul>\n<h2>Model Selection</h2>\n<p>Gil-Galad model selection is employed via Sherpa's Bayesian optimization suite which utilizes sklearn's Gaussian process module. Bayesian optimization specifies a distribution over functions via a kernel function and prior. Here, the mean function corresponds to a surrogate objective function whose predictor variables are the model hyperparameters. The prior distribution over functions is updated via Bayes' rule to account for trial runs wherein the independent variables specify the model and the dependent variable is the evaluation of such a model on the validation dataset.</p>\n<p>With Gil-Galad, we specify which hyperparameters we will optimize by passing a parameter dictionary to our graph class while also defining default hyperparameters during graph and model construction as follows:</p>\n<h3>Graph-level</h3>\n<pre><span class=\"k\">class</span> <span class=\"nc\">Graph</span><span class=\"p\">(</span><span class=\"n\">BaseGraph</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">network</span><span class=\"p\">,</span> <span class=\"n\">sampler</span><span class=\"p\">,</span> <span class=\"n\">logdir</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">ckptdir</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">network</span> <span class=\"o\">=</span> <span class=\"n\">network</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">sampler</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">logdir</span> <span class=\"o\">=</span> <span class=\"n\">logdir</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ckptdir</span> <span class=\"o\">=</span> <span class=\"n\">ckptdir</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">build_graph</span><span class=\"p\">()</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">build_graph</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n\n        <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reset_default_graph</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">initialize</span><span class=\"p\">()</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">get_batch</span><span class=\"p\">()</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">y_</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">network</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"n\">params</span><span class=\"p\">)</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">losses</span><span class=\"o\">.</span><span class=\"n\">mean_squared_error</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">y_</span><span class=\"p\">)</span>\n\n        <span class=\"n\">update_ops</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_collection</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">GraphKeys</span><span class=\"o\">.</span><span class=\"n\">UPDATE_OPS</span><span class=\"p\">)</span>\n        <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">control_dependencies</span><span class=\"p\">(</span><span class=\"n\">update_ops</span><span class=\"p\">):</span>\n            <span class=\"n\">opt</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">AdamOptimizer</span><span class=\"p\">(</span>\n                <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"n\">params</span><span class=\"p\">[</span><span class=\"s1\">'lr'</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">params</span> <span class=\"k\">else</span> <span class=\"mf\">0.001</span>\n            <span class=\"p\">)</span>\n\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">update</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">minimize</span><span class=\"p\">(</span>\n                <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">loss</span><span class=\"p\">,</span>\n                <span class=\"n\">var_list</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">network</span><span class=\"o\">.</span><span class=\"n\">vars</span><span class=\"p\">,</span>\n                <span class=\"n\">global_step</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">global_step</span>\n            <span class=\"p\">)</span>\n</pre>\n<h3>Model-level</h3>\n<pre><span class=\"k\">class</span> <span class=\"nc\">Model</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n  <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"p\">):</span>\n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"n\">name</span>\n\n  <span class=\"k\">def</span> <span class=\"fm\">__call__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"p\">):</span>\n    <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">variable_scope</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">vs</span><span class=\"p\">:</span>\n      <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">conv_2d</span><span class=\"p\">(</span>\n        <span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"p\">,</span>\n        <span class=\"n\">filters</span><span class=\"o\">=</span><span class=\"n\">params</span><span class=\"p\">[</span><span class=\"s1\">'filters'</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">params</span> <span class=\"k\">else</span> <span class=\"mi\">64</span><span class=\"p\">,</span>\n        <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"n\">params</span><span class=\"p\">[</span><span class=\"s1\">'kernel_size'</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">params</span> <span class=\"k\">else</span> <span class=\"mi\">3</span><span class=\"p\">,</span>\n        <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n        <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"n\">params</span><span class=\"p\">[</span><span class=\"s1\">'activation'</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">params</span> <span class=\"k\">else</span> <span class=\"s1\">'relu'</span>\n      <span class=\"p\">)</span>\n\n      <span class=\"k\">return</span> <span class=\"n\">y</span>\n</pre>\n<p>We then define the hyperparameter domain type and ranges in a dictionary. This information accompanies the graph object as arguments for the Bayesian optimization function.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">gilgalad</span> <span class=\"k\">as</span> <span class=\"nn\">gg</span>\n\n<span class=\"n\">hyperparameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s1\">'Discrete'</span><span class=\"p\">:</span>\n        <span class=\"p\">{</span><span class=\"s1\">'filters'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">],</span>\n         <span class=\"s1\">'kernel_size'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">]},</span>\n    <span class=\"s1\">'Continuous'</span><span class=\"p\">:</span>\n        <span class=\"p\">{</span><span class=\"s1\">'lr'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">1e-5</span><span class=\"p\">,</span> <span class=\"mf\">1e-3</span><span class=\"p\">]},</span>\n    <span class=\"s1\">'Choice'</span><span class=\"p\">:</span>\n        <span class=\"p\">{</span><span class=\"s1\">'activation'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">'relu'</span><span class=\"p\">,</span> <span class=\"s1\">'prelu'</span><span class=\"p\">]}</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">best_model</span> <span class=\"o\">=</span> <span class=\"n\">gg</span><span class=\"o\">.</span><span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">bayesian_optimization</span><span class=\"p\">(</span>\n    <span class=\"n\">graph</span><span class=\"o\">=</span><span class=\"n\">graph</span><span class=\"p\">,</span>\n    <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"n\">hyperparameters</span><span class=\"p\">,</span>\n    <span class=\"n\">max_trials</span><span class=\"o\">=</span><span class=\"mi\">50</span>\n<span class=\"p\">)</span>\n</pre>\n\n          </div>"}, "last_serial": 4852270, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "ec24a8d253d769878131371b767ada50", "sha256": "acb1f343e3b66dc2c7775c94596f1b5b8b2e4c9c9bce45095c545dfda205c3c5"}, "downloads": -1, "filename": "gil_galad-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "ec24a8d253d769878131371b767ada50", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23599, "upload_time": "2019-02-21T23:09:01", "upload_time_iso_8601": "2019-02-21T23:09:01.789865Z", "url": "https://files.pythonhosted.org/packages/05/6b/f98b15843e20f39a03c8c6b4838e30cd33a07175354b33e9e81acb1427a4/gil_galad-0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fd67270676f8c93ccc45c18953b19f4e", "sha256": "aa87e5cfca12370466b396435d7ff099816db11bb7f36d317248b067a4d3f856"}, "downloads": -1, "filename": "Gil_Galad-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "fd67270676f8c93ccc45c18953b19f4e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23606, "upload_time": "2019-02-21T23:03:07", "upload_time_iso_8601": "2019-02-21T23:03:07.724680Z", "url": "https://files.pythonhosted.org/packages/58/dd/4061a3b035e9d2109ca50d6637d53b210c1793b8cc67c84364c57b040456/Gil_Galad-0.1-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ec24a8d253d769878131371b767ada50", "sha256": "acb1f343e3b66dc2c7775c94596f1b5b8b2e4c9c9bce45095c545dfda205c3c5"}, "downloads": -1, "filename": "gil_galad-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "ec24a8d253d769878131371b767ada50", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23599, "upload_time": "2019-02-21T23:09:01", "upload_time_iso_8601": "2019-02-21T23:09:01.789865Z", "url": "https://files.pythonhosted.org/packages/05/6b/f98b15843e20f39a03c8c6b4838e30cd33a07175354b33e9e81acb1427a4/gil_galad-0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fd67270676f8c93ccc45c18953b19f4e", "sha256": "aa87e5cfca12370466b396435d7ff099816db11bb7f36d317248b067a4d3f856"}, "downloads": -1, "filename": "Gil_Galad-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "fd67270676f8c93ccc45c18953b19f4e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23606, "upload_time": "2019-02-21T23:03:07", "upload_time_iso_8601": "2019-02-21T23:03:07.724680Z", "url": "https://files.pythonhosted.org/packages/58/dd/4061a3b035e9d2109ca50d6637d53b210c1793b8cc67c84364c57b040456/Gil_Galad-0.1-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 00:57:27 2020"}