{"info": {"author": "Determined AI", "author_email": "hello@determined.ai", "bugtrack_url": null, "classifiers": [], "description": "# Yoga Data Layer: The _Flexible_ Data Layer\n\nA better approach to data loading for Deep Learning.  API-transparent caching to disk, GCS, or S3.\n\n## Why `yogadl`?\n\nAt Determined AI, we help many customers perform high-performance data loading for deep learning\nmodels.  We believe every data loader should have two layers: the **random-access layer** and the\n**sequential layer**.\n\nThe **random-access layer** is critical for good training infrastructure.  Direct random access to\nany record enables:\n\n  - Shuffling (potentially every epoch)\n  - Pausing/continuing training mid-epoch\n  - Sharding the dataset efficiently for distributed training\n\nThe **sequential layer** starts as soon as you decide the order in which you will access the records in\nthe dataset.  Often the transition is implicit, in which case it starts as soon as you are done\nmodifying the order of access (i.e. via shuffling, sharding, or splitting).  This layer is vital to\nperformance optimizations because it enables:\n\n  - Prefetching data loading to hide latency costs\n  - Parallelizing data loading to hide compute costs\n\nHere is a simple code snippet to illustrate what the transition from random-access layer to\nsequential layer looks like:\n\n```python\n# Start of random-access layer.\nindices = list(range(100))\nindices = indices[skip:]\nindices=np.random.shuffle(indices)\n\n# Start of sequential layer.\n\ndef record_gen():\n    for i in indices:\n        yield read_file_at_index(i)\n\nrecord_ds = tf.data.Dataset.from_generator(record_gen, ...)\nfinal_ds = record_ds.prefetch(...)\n\n```\n\nNotice that in the above example, the `tf.data` API is used, but only in the sequential layer.\nThis is because `tf.data` has no concept of the random access layer.  As a result:\n\n  - `tf.data.Dataset.shuffle()` can only approximate a shuffle.  Calling `.shuffle(N)` will read\n    `N` records into a buffer and choose samples randomly from **those `N` records**, while a true\n    shuffle chooses samples randomly from the **entire dataset**.  This shortcoming forces you\n    to choose between memory footprint and the quality of your shuffle.  The only true\n    shuffle with tf.data.Dataset.shuffle() is to read the entire dataset into memory.\n  - `tf.data.Dataset.skip(N)` is as inefficient as possible.  Each of the `N` skipped records will\n    still be read from disk and processed normally, according to all of the operations preceeding\n    the `.skip()` call, making `.skip()` prohibitively expensive for most use cases.\n  - Pausing and continuing training is only possible by saving the state of a `tf.data.Iterator`.\n    However, saving a `tf.data.Iterator` does not work with all datasets.  In particular, it does\n    not work with datasets created using `from_generator()`, which is the easiest way to create a\n    `tf.data.Dataset`.\n\nWe have seen countless instances where `tf.data.Dataset` shortcomings have made life harder for\ndeep learning practitioners, so we set out to build something better.  We set out to build a new\ndata layer which could augment an existing `tf.data.Dataset` data loader with the properties should\ncome standard with every data loader.\n\nAt the same time, we wanted this new data layer to relieve another key pain point: high-performance\ndataset caching and dataset versioning.\n\n## What is `yogadl`?\n\nWe designed `yogadl` to be two things: a standalone caching layer to imbue existing data loaders\nwith the properties that come from a random-access layer, and a better interface for defining data\nloaders in general.\n\n### A standalone caching tool\n\nSince `tf.data.Dataset`-based datasets have no random-access layer, `yogadl` caches them to disk in\na random-access-friendly way.  The storage mechanism is, in fact, nearly identical to how\n[TensorPack caches datasets to disk](https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.LMDBSerializer),\nonly with some additional abstractions to allow dataset versioning, cloud storage, and all of the\nwonderful features that a data loader with a random-access layer ought to have.\n\nWhat does all this do for you?  A few things:\n\n - **Better training**: A `yogadl`-cached `tf.data.Dataset` will have better shuffling than a\n   native `tf.data.Dataset`.  Additionally, pausing and continuing training mid-epoch will be\n   simple and robust, and efficient sharding for distributed training comes standard.\n - **Faster data loading**: Slow data loader?  Don't waste your time optimizing it.  `yogadl` will\n   save it in a high-performance cache the first time it is used, and all future uses will be\n   fast and efficient.\n - **API-transparent**: Not all operations in the data loader are cacheable.  Data augmentation\n   must be done at run time.  `yogadl` allows you to keep your existing data augmentation code.\n\n### A better interface\n\nAt the core of `yogadl` is the `DataRef` interface, which creates an explicit boundary between the\nrandom-access layer and the sequential layer.\n\nWe are not the first people to think of this: PyTorch separates the `DataSet` (the random-access\nlayer) from the `Sampler` (which defines the sequential layer).  Keras has a `Sequence` object\nwhich defines the random-access layer, leaving the order of access (the sequential layer) to be\ndecided by the arguments to `model.fit()`.  Both `DataSet` and `Sequence` are already 100%\ncompatible with `yogadl`'s `DataRef` interface (although `yogadl` does not yet include those\nadapters).\n\nAnd yet, the world is still full of data loaders which are lacking.  At Determined AI, we are\ndedicated to advancing the state of the art for training Deep Learning models, and we believe that\na better interface for data loading is a critical piece of that goal.  Any data loader which\nimplements the `DataRef` interface is capable of proper shuffling, pausing and continuing training\nmid-epoch, and efficient multi-machine distributed training.\n\n## What is `yogadl` _not_?\n\n`yogadl` is not a data manipulation API.\n[This](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n[world](https://tensorpack.readthedocs.io/tutorial/dataflow.html)\n[has](https://keras.io/preprocessing/image/)\n[more](https://pytorch.org/docs/stable/torchvision/ops.html)\n[than](https://numpy.org/)\n[enough](https://pandas.pydata.org/)\n[of](https://docs.nvidia.com/deeplearning/sdk/dali-developer-guide/docs/index.html)\n[those](https://opencv-python-tutroals.readthedocs.io/en/latest/).\nInstead, `yogadl` seeks to be API-transparent so that you can continue to use your existing data\nloading code, but with all the benefits of a high-performance, random-access cache.  If you have\ndata augmentation steps which cannot be cached, that code should continue to work without any\nmodifications.\n\n`yogadl` does not (at this time) work with any data frameworks other than `tf.data.Dataset.`\nFirst-class support for (tf.)Keras `Sequence` objects, PyTorch `DataSet` objects, and TensorPack\n`DataFlow` objects is on the near-term roadmap.\n\n`yogadl` offers basic dataset versioning, but it is not (at this time) a full-blown version control\nfor datasets.  Offering something like version control for datasets is on the roadmap as well.\n\n<!-- ## How do I use `yogadl`? -->\n\n<!-- TODO: code examples here. -->\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://www.github.com/determined-ai/yogadl/", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "yogadl", "package_url": "https://pypi.org/project/yogadl/", "platform": "", "project_url": "https://pypi.org/project/yogadl/", "project_urls": {"Homepage": "https://www.github.com/determined-ai/yogadl/"}, "release_url": "https://pypi.org/project/yogadl/0.1.0/", "requires_dist": ["async-generator", "boto3", "filelock", "google-cloud-storage", "lmdb", "lomond", "websockets", "tensorflow ; extra == 'tf'"], "requires_python": ">=3.6.0", "summary": "Yoga Data Layer, a flexible data layer for machine learning", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Yoga Data Layer: The <em>Flexible</em> Data Layer</h1>\n<p>A better approach to data loading for Deep Learning.  API-transparent caching to disk, GCS, or S3.</p>\n<h2>Why <code>yogadl</code>?</h2>\n<p>At Determined AI, we help many customers perform high-performance data loading for deep learning\nmodels.  We believe every data loader should have two layers: the <strong>random-access layer</strong> and the\n<strong>sequential layer</strong>.</p>\n<p>The <strong>random-access layer</strong> is critical for good training infrastructure.  Direct random access to\nany record enables:</p>\n<ul>\n<li>Shuffling (potentially every epoch)</li>\n<li>Pausing/continuing training mid-epoch</li>\n<li>Sharding the dataset efficiently for distributed training</li>\n</ul>\n<p>The <strong>sequential layer</strong> starts as soon as you decide the order in which you will access the records in\nthe dataset.  Often the transition is implicit, in which case it starts as soon as you are done\nmodifying the order of access (i.e. via shuffling, sharding, or splitting).  This layer is vital to\nperformance optimizations because it enables:</p>\n<ul>\n<li>Prefetching data loading to hide latency costs</li>\n<li>Parallelizing data loading to hide compute costs</li>\n</ul>\n<p>Here is a simple code snippet to illustrate what the transition from random-access layer to\nsequential layer looks like:</p>\n<pre><span class=\"c1\"># Start of random-access layer.</span>\n<span class=\"n\">indices</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">))</span>\n<span class=\"n\">indices</span> <span class=\"o\">=</span> <span class=\"n\">indices</span><span class=\"p\">[</span><span class=\"n\">skip</span><span class=\"p\">:]</span>\n<span class=\"n\">indices</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"n\">indices</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Start of sequential layer.</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">record_gen</span><span class=\"p\">():</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">indices</span><span class=\"p\">:</span>\n        <span class=\"k\">yield</span> <span class=\"n\">read_file_at_index</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span>\n\n<span class=\"n\">record_ds</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"o\">.</span><span class=\"n\">from_generator</span><span class=\"p\">(</span><span class=\"n\">record_gen</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">)</span>\n<span class=\"n\">final_ds</span> <span class=\"o\">=</span> <span class=\"n\">record_ds</span><span class=\"o\">.</span><span class=\"n\">prefetch</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">)</span>\n</pre>\n<p>Notice that in the above example, the <code>tf.data</code> API is used, but only in the sequential layer.\nThis is because <code>tf.data</code> has no concept of the random access layer.  As a result:</p>\n<ul>\n<li><code>tf.data.Dataset.shuffle()</code> can only approximate a shuffle.  Calling <code>.shuffle(N)</code> will read\n<code>N</code> records into a buffer and choose samples randomly from <strong>those <code>N</code> records</strong>, while a true\nshuffle chooses samples randomly from the <strong>entire dataset</strong>.  This shortcoming forces you\nto choose between memory footprint and the quality of your shuffle.  The only true\nshuffle with tf.data.Dataset.shuffle() is to read the entire dataset into memory.</li>\n<li><code>tf.data.Dataset.skip(N)</code> is as inefficient as possible.  Each of the <code>N</code> skipped records will\nstill be read from disk and processed normally, according to all of the operations preceeding\nthe <code>.skip()</code> call, making <code>.skip()</code> prohibitively expensive for most use cases.</li>\n<li>Pausing and continuing training is only possible by saving the state of a <code>tf.data.Iterator</code>.\nHowever, saving a <code>tf.data.Iterator</code> does not work with all datasets.  In particular, it does\nnot work with datasets created using <code>from_generator()</code>, which is the easiest way to create a\n<code>tf.data.Dataset</code>.</li>\n</ul>\n<p>We have seen countless instances where <code>tf.data.Dataset</code> shortcomings have made life harder for\ndeep learning practitioners, so we set out to build something better.  We set out to build a new\ndata layer which could augment an existing <code>tf.data.Dataset</code> data loader with the properties should\ncome standard with every data loader.</p>\n<p>At the same time, we wanted this new data layer to relieve another key pain point: high-performance\ndataset caching and dataset versioning.</p>\n<h2>What is <code>yogadl</code>?</h2>\n<p>We designed <code>yogadl</code> to be two things: a standalone caching layer to imbue existing data loaders\nwith the properties that come from a random-access layer, and a better interface for defining data\nloaders in general.</p>\n<h3>A standalone caching tool</h3>\n<p>Since <code>tf.data.Dataset</code>-based datasets have no random-access layer, <code>yogadl</code> caches them to disk in\na random-access-friendly way.  The storage mechanism is, in fact, nearly identical to how\n<a href=\"https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.LMDBSerializer\" rel=\"nofollow\">TensorPack caches datasets to disk</a>,\nonly with some additional abstractions to allow dataset versioning, cloud storage, and all of the\nwonderful features that a data loader with a random-access layer ought to have.</p>\n<p>What does all this do for you?  A few things:</p>\n<ul>\n<li><strong>Better training</strong>: A <code>yogadl</code>-cached <code>tf.data.Dataset</code> will have better shuffling than a\nnative <code>tf.data.Dataset</code>.  Additionally, pausing and continuing training mid-epoch will be\nsimple and robust, and efficient sharding for distributed training comes standard.</li>\n<li><strong>Faster data loading</strong>: Slow data loader?  Don't waste your time optimizing it.  <code>yogadl</code> will\nsave it in a high-performance cache the first time it is used, and all future uses will be\nfast and efficient.</li>\n<li><strong>API-transparent</strong>: Not all operations in the data loader are cacheable.  Data augmentation\nmust be done at run time.  <code>yogadl</code> allows you to keep your existing data augmentation code.</li>\n</ul>\n<h3>A better interface</h3>\n<p>At the core of <code>yogadl</code> is the <code>DataRef</code> interface, which creates an explicit boundary between the\nrandom-access layer and the sequential layer.</p>\n<p>We are not the first people to think of this: PyTorch separates the <code>DataSet</code> (the random-access\nlayer) from the <code>Sampler</code> (which defines the sequential layer).  Keras has a <code>Sequence</code> object\nwhich defines the random-access layer, leaving the order of access (the sequential layer) to be\ndecided by the arguments to <code>model.fit()</code>.  Both <code>DataSet</code> and <code>Sequence</code> are already 100%\ncompatible with <code>yogadl</code>'s <code>DataRef</code> interface (although <code>yogadl</code> does not yet include those\nadapters).</p>\n<p>And yet, the world is still full of data loaders which are lacking.  At Determined AI, we are\ndedicated to advancing the state of the art for training Deep Learning models, and we believe that\na better interface for data loading is a critical piece of that goal.  Any data loader which\nimplements the <code>DataRef</code> interface is capable of proper shuffling, pausing and continuing training\nmid-epoch, and efficient multi-machine distributed training.</p>\n<h2>What is <code>yogadl</code> <em>not</em>?</h2>\n<p><code>yogadl</code> is not a data manipulation API.\n<a href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset\" rel=\"nofollow\">This</a>\n<a href=\"https://tensorpack.readthedocs.io/tutorial/dataflow.html\" rel=\"nofollow\">world</a>\n<a href=\"https://keras.io/preprocessing/image/\" rel=\"nofollow\">has</a>\n<a href=\"https://pytorch.org/docs/stable/torchvision/ops.html\" rel=\"nofollow\">more</a>\n<a href=\"https://numpy.org/\" rel=\"nofollow\">than</a>\n<a href=\"https://pandas.pydata.org/\" rel=\"nofollow\">enough</a>\n<a href=\"https://docs.nvidia.com/deeplearning/sdk/dali-developer-guide/docs/index.html\" rel=\"nofollow\">of</a>\n<a href=\"https://opencv-python-tutroals.readthedocs.io/en/latest/\" rel=\"nofollow\">those</a>.\nInstead, <code>yogadl</code> seeks to be API-transparent so that you can continue to use your existing data\nloading code, but with all the benefits of a high-performance, random-access cache.  If you have\ndata augmentation steps which cannot be cached, that code should continue to work without any\nmodifications.</p>\n<p><code>yogadl</code> does not (at this time) work with any data frameworks other than <code>tf.data.Dataset.</code>\nFirst-class support for (tf.)Keras <code>Sequence</code> objects, PyTorch <code>DataSet</code> objects, and TensorPack\n<code>DataFlow</code> objects is on the near-term roadmap.</p>\n<p><code>yogadl</code> offers basic dataset versioning, but it is not (at this time) a full-blown version control\nfor datasets.  Offering something like version control for datasets is on the roadmap as well.</p>\n\n\n\n          </div>"}, "last_serial": 7067806, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "a240f3767b80fb186a1af95dac0e82db", "sha256": "f0b017071763424cffc706ff4a26e62688692841d2415b877421cfc13af024d2"}, "downloads": -1, "filename": "yogadl-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "a240f3767b80fb186a1af95dac0e82db", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 32072, "upload_time": "2020-04-21T13:08:12", "upload_time_iso_8601": "2020-04-21T13:08:12.120808Z", "url": "https://files.pythonhosted.org/packages/96/bf/028012dfaa973245ca9992a8afed953785eaa72bcbf137253431fbbb2b1d/yogadl-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "311c73299578b95b7e293702bf6534bc", "sha256": "f95e9715ab9f8e8bf56ade363b429ff1b02702a11ad290354c9b06f08243fa8b"}, "downloads": -1, "filename": "yogadl-0.1.0.tar.gz", "has_sig": false, "md5_digest": "311c73299578b95b7e293702bf6534bc", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 22502, "upload_time": "2020-04-21T13:08:14", "upload_time_iso_8601": "2020-04-21T13:08:14.011689Z", "url": "https://files.pythonhosted.org/packages/75/76/9aec4ed9af2bda5f8cc6936c8c95cd0567dad135f925f582f442b1f21a2e/yogadl-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a240f3767b80fb186a1af95dac0e82db", "sha256": "f0b017071763424cffc706ff4a26e62688692841d2415b877421cfc13af024d2"}, "downloads": -1, "filename": "yogadl-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "a240f3767b80fb186a1af95dac0e82db", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 32072, "upload_time": "2020-04-21T13:08:12", "upload_time_iso_8601": "2020-04-21T13:08:12.120808Z", "url": "https://files.pythonhosted.org/packages/96/bf/028012dfaa973245ca9992a8afed953785eaa72bcbf137253431fbbb2b1d/yogadl-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "311c73299578b95b7e293702bf6534bc", "sha256": "f95e9715ab9f8e8bf56ade363b429ff1b02702a11ad290354c9b06f08243fa8b"}, "downloads": -1, "filename": "yogadl-0.1.0.tar.gz", "has_sig": false, "md5_digest": "311c73299578b95b7e293702bf6534bc", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 22502, "upload_time": "2020-04-21T13:08:14", "upload_time_iso_8601": "2020-04-21T13:08:14.011689Z", "url": "https://files.pythonhosted.org/packages/75/76/9aec4ed9af2bda5f8cc6936c8c95cd0567dad135f925f582f442b1f21a2e/yogadl-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:22:03 2020"}