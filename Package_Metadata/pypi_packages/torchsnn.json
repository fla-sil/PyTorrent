{"info": {"author": "Federico A. Galatolo", "author_email": "galatolo.federico@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Science/Research", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# torchsnn\npytorch implementation of the Stigmergic Neural Networks as presented in the paper [*Using stigmergy to incorporate the time into artificial neural networks*]().\n\nThis package was wrote with the intent to make as **easy** as possible to integrate the Stigmergic Neural Networks into the existing models.\n\nYou can safely **mix** native pytorch Modules with ours.  \nThe only **catch** is that you should use *StigmergicModule* (which extends pytorch's *Module*) as base class for your models in order to be able to *tick()* and *reset()* them.\n\nImplementing our [proposed architecture to solve MNIST]() becomes as easy as:\n```python\nimport torch\nimport torchsnn\n\nnet = torchsnn.Sequential(\n    torchsnn.SimpleLayer(28, 10),\n    torchsnn.FullLayer(10, 10),\n    torchsnn.TemporalAdapter(10, 28),\n    torch.nn.Linear(10*28, 10),\n    torch.nn.Sigmoid()\n)\n```\n\nYou can train a *StigmergicModule* as you would do with a pytorch's *Module*, but don't forget to *reset()* and *tick()* it!\n\n```python\noptimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\n\nfor i in range(0,N):\n    for X,Y in zip(dataset_X, dataset_Y):\n        net.reset()\n        out = None\n        for xi in X:\n            out = net(torch.tensor(xi, dtype=torch.float32))\n            net.tick()\n        \n        loss = (Y-out)**2\n        \n        loss.backward()\n        optimizer.step()\n```\n\n### Does it support batch inputs?\n\nYes! if you forward into a StigmergicModule a batch of inputs it will return a batch of outputs\n\n```python\nfor t in range(0, num_ticks):\n    batch_out[0], batch_out[1], ... = net(torch.tensor([batch_in[0][t], batch_in[1][t], ...]))\n    net.tick()\n\n```\n### Can it run on CUDA?\n\nYes and as you will expect from a pytorch Module!  \nYou just need to call the *to(device)* method on a model to move it in the GPU memory\n\n```python\ndevice = torch.device(\"cuda\")\n\nnet = net.to(device)\n\nnet.forward(torch.tensor(..., device=device))\n```\n## Documentation\n\n### torchsnn.StigmergicModule\n\nBase class for a stigmergic network or layer.  \nIf you are writing your own *StigmergicModule* you have to implement the functions\n* tick()\n* reset()\n\nIf you are using others *StigmergicModule* it will propagate these calls in its subtree.  \nFor example if you want to build a network with a *Linear* and a *SimpleLayer* you can do something like:\n\n```python\nimport torch\nimport torchsnn\n\nclass TestNetwork(torchsnn.StigmergicModule):\n    def __init__(self):\n        torchsnn.StigmergicModule.__init__(self)\n        self.linear = torch.nn.Linear(2,5)\n        self.stigmergic = torchsnn.SimpleLayer(5,2)\n\n    def forward(self, input):\n        l1 = torch.sigmoid(self.linear(input))\n        l2 = self.stigmergic(l1)\n        return l2\n\nnet = TestNetwork()\n```\n\n### torchsnn.Sequential\n\nFunction with the same interface of *torch.nn.Sequential* for building sequential networks.  \nThe same network of the previous example can be built with:\n```python\nimport torch\nimport torchsnn\n\nnet = torchsnn.Sequential(\n    torch.nn.Linear(2,5),\n    torch.nn.Sigmoid(),\n    torchsnn.SimpleLayer(5,2)\n)\n```\n\n### torchsnn.SimpleLayer\n\nIt this layer only the thresholds are stigmergic variables and their *stimuli* are the output values.  \n\n![](https://latex.codecogs.com/gif.latex?x%28t%29%20%3D%20%5Ctext%7Binput%20at%20time%20t%7D)  \n\n![](https://latex.codecogs.com/gif.latex?y%28t%29%20%3D%20%5Ctext%7Boutput%20at%20time%20t%7D)  \n\n![](https://latex.codecogs.com/gif.latex?th%28t%29%20%3D%20%5Ctext%7Bthreshold%20at%20time%20t%7D)\n\n<br>\n\n![](https://latex.codecogs.com/gif.latex?y%28t%29%20%3D%20%5Csigma%28Wx%28t%29%20&plus;%20b%20-%20th%28t%29%29)  \n\n![](https://latex.codecogs.com/gif.latex?th%28t%29%20%3D%20C%28th%28t-1%29%20&plus;%20My%28t-1%29%20-%20%5Ctau%2C%20min%2C%20max%29) \n\n### torchsnn.FullLayer\n\nIn this layer both thresholds and weights are stigmergic variables and their *stimuli* are respectively the output values and the input ones.  \n\n![](https://latex.codecogs.com/gif.latex?x%28t%29%20%3D%20%5Ctext%7Binput%20at%20time%20t%7D)  \n\n![](https://latex.codecogs.com/gif.latex?y%28t%29%20%3D%20%5Ctext%7Boutput%20at%20time%20t%7D)  \n\n![](https://latex.codecogs.com/gif.latex?th%28t%29%20%3D%20%5Ctext%7Bthreshold%20at%20time%20t%7D) \n\n![](https://latex.codecogs.com/gif.latex?W%28t%29%20%3D%20%5Ctext%7Bweights%20at%20time%20t%7D)  \n\n<br>\n\n![](https://latex.codecogs.com/gif.latex?y%28t%29%20%3D%20%5Csigma%28Wx%28t%29%20&plus;%20b%20-%20th%28t%29%29)  \n\n![](https://latex.codecogs.com/gif.latex?th%28t%29%20%3D%20C%28th%28t-1%29%20&plus;%20My%28t-1%29%20-%20%5Ctau%2C%20min%2C%20max%29)  \n\n![](https://latex.codecogs.com/gif.latex?J%5Em%28v%29%20%3D%20%5Cbegin%7Bbmatrix%7D%20v_%7B0%7D%20%26%20v_%7B0%7D%20%26%20%5Cdots%20%26%20v_%7B0%7D%20%5C%5C%20v_%7B1%7D%20%26%20v_%7B1%7D%20%26%20%5Cdots%20%26%20v_%7B1%7D%20%5C%5C%20%5Cvdots%20%26%20%5Cvdots%20%26%20%5Cddots%20%26%20%5Cvdots%20%5C%5C%20v_%7Bn-1%7D%20%26%20v_%7Bn-1%7D%20%26%20%5Cdots%20%26%20v_%7Bn-1%7D%20%5Cend%7Bbmatrix%7D)  \n\n![](https://latex.codecogs.com/gif.latex?W%28t%29%20%3D%20C%28W%28t-1%29%20&plus;%20J%5Em%28X%28t-1%29%29M_w%20-%20%5Ctau_w%2C%20min%2C%20max%29) \n\n## Citing\n\nWe can't wait to see what you will build with the Stigmergic Neural Networks!  \nWhen you will publish your work you can use this BibTex to cite us :)\n\n```\n@article{galatolo_snn\n,\tauthor\t= {Galatolo, Federico A and Cimino, Mario GCA and Vaglini, Gigliola}\n,\ttitle\t= {Using stigmergy to incorporate the time into artificial neural networks}\n,\tjournal\t= {MIKE 2018}\n,\tyear\t= {2018}\n}\n```\n\n## Contributing\n\nThis code is released under GNU/GPLv3 so feel free to fork it and submit your changes, every PR helps.  \nIf you need help using it or for any question please reach me at [galatolo.federico@gmail.com](mailto:galatolo.federico@gmail.com) or on Telegram  [@galatolo](https://t.me/galatolo)", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/galatolofederico/torchsnn", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "torchsnn", "package_url": "https://pypi.org/project/torchsnn/", "platform": "", "project_url": "https://pypi.org/project/torchsnn/", "project_urls": {"Homepage": "https://github.com/galatolofederico/torchsnn"}, "release_url": "https://pypi.org/project/torchsnn/0.1.0/", "requires_dist": null, "requires_python": "", "summary": "pytorch implementation of Stigmergic Neural Netowrks", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>torchsnn</h1>\n<p>pytorch implementation of the Stigmergic Neural Networks as presented in the paper <a href=\"\" rel=\"nofollow\"><em>Using stigmergy to incorporate the time into artificial neural networks</em></a>.</p>\n<p>This package was wrote with the intent to make as <strong>easy</strong> as possible to integrate the Stigmergic Neural Networks into the existing models.</p>\n<p>You can safely <strong>mix</strong> native pytorch Modules with ours.<br>\nThe only <strong>catch</strong> is that you should use <em>StigmergicModule</em> (which extends pytorch's <em>Module</em>) as base class for your models in order to be able to <em>tick()</em> and <em>reset()</em> them.</p>\n<p>Implementing our <a href=\"\" rel=\"nofollow\">proposed architecture to solve MNIST</a> becomes as easy as:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchsnn</span>\n\n<span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">torchsnn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n    <span class=\"n\">torchsnn</span><span class=\"o\">.</span><span class=\"n\">SimpleLayer</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>\n    <span class=\"n\">torchsnn</span><span class=\"o\">.</span><span class=\"n\">FullLayer</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>\n    <span class=\"n\">torchsnn</span><span class=\"o\">.</span><span class=\"n\">TemporalAdapter</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">),</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"o\">*</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sigmoid</span><span class=\"p\">()</span>\n<span class=\"p\">)</span>\n</pre>\n<p>You can train a <em>StigmergicModule</em> as you would do with a pytorch's <em>Module</em>, but don't forget to <em>reset()</em> and <em>tick()</em> it!</p>\n<pre><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"n\">N</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">Y</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">dataset_X</span><span class=\"p\">,</span> <span class=\"n\">dataset_Y</span><span class=\"p\">):</span>\n        <span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n        <span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n        <span class=\"k\">for</span> <span class=\"n\">xi</span> <span class=\"ow\">in</span> <span class=\"n\">X</span><span class=\"p\">:</span>\n            <span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">xi</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">))</span>\n            <span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">tick</span><span class=\"p\">()</span>\n        \n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"o\">-</span><span class=\"n\">out</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span>\n        \n        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<h3>Does it support batch inputs?</h3>\n<p>Yes! if you forward into a StigmergicModule a batch of inputs it will return a batch of outputs</p>\n<pre><span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_ticks</span><span class=\"p\">):</span>\n    <span class=\"n\">batch_out</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">batch_out</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"o\">...</span> <span class=\"o\">=</span> <span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"n\">batch_in</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"n\">t</span><span class=\"p\">],</span> <span class=\"n\">batch_in</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">][</span><span class=\"n\">t</span><span class=\"p\">],</span> <span class=\"o\">...</span><span class=\"p\">]))</span>\n    <span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">tick</span><span class=\"p\">()</span>\n</pre>\n<h3>Can it run on CUDA?</h3>\n<p>Yes and as you will expect from a pytorch Module!<br>\nYou just need to call the <em>to(device)</em> method on a model to move it in the GPU memory</p>\n<pre><span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">\"cuda\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n\n<span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">forward</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">))</span>\n</pre>\n<h2>Documentation</h2>\n<h3>torchsnn.StigmergicModule</h3>\n<p>Base class for a stigmergic network or layer.<br>\nIf you are writing your own <em>StigmergicModule</em> you have to implement the functions</p>\n<ul>\n<li>tick()</li>\n<li>reset()</li>\n</ul>\n<p>If you are using others <em>StigmergicModule</em> it will propagate these calls in its subtree.<br>\nFor example if you want to build a network with a <em>Linear</em> and a <em>SimpleLayer</em> you can do something like:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchsnn</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">TestNetwork</span><span class=\"p\">(</span><span class=\"n\">torchsnn</span><span class=\"o\">.</span><span class=\"n\">StigmergicModule</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"n\">torchsnn</span><span class=\"o\">.</span><span class=\"n\">StigmergicModule</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">linear</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">stigmergic</span> <span class=\"o\">=</span> <span class=\"n\">torchsnn</span><span class=\"o\">.</span><span class=\"n\">SimpleLayer</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"nb\">input</span><span class=\"p\">):</span>\n        <span class=\"n\">l1</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">))</span>\n        <span class=\"n\">l2</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">stigmergic</span><span class=\"p\">(</span><span class=\"n\">l1</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">l2</span>\n\n<span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">TestNetwork</span><span class=\"p\">()</span>\n</pre>\n<h3>torchsnn.Sequential</h3>\n<p>Function with the same interface of <em>torch.nn.Sequential</em> for building sequential networks.<br>\nThe same network of the previous example can be built with:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchsnn</span>\n\n<span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">torchsnn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">),</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sigmoid</span><span class=\"p\">(),</span>\n    <span class=\"n\">torchsnn</span><span class=\"o\">.</span><span class=\"n\">SimpleLayer</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n</pre>\n<h3>torchsnn.SimpleLayer</h3>\n<p>It this layer only the thresholds are stigmergic variables and their <em>stimuli</em> are the output values.</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d8d275da9f7f2652c20b0baa2055b2837c60a51e/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f782532387425323925323025334425323025354374657874253742696e707574253230617425323074696d6525323074253744\"></p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5d22e2a7ae25eb8e82ae5e3a6ae4c629cf69b55b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f7925323874253239253230253344253230253543746578742537426f7574707574253230617425323074696d6525323074253744\"></p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ff749d3f27aa116da183178b5d8c3d391aa9e9d4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f746825323874253239253230253344253230253543746578742537427468726573686f6c64253230617425323074696d6525323074253744\"></p>\n<br>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/20aa0252cbb205aa9eb626caf4ed042eb1eb46a1/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f79253238742532392532302533442532302535437369676d612532385778253238742532392532302b253230622532302d253230746825323874253239253239\"></p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/afd2cb52546aae59ea293cf709dc337789a648f8/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f746825323874253239253230253344253230432532387468253238742d312532392532302b2532304d79253238742d312532392532302d2532302535437461752532432532306d696e2532432532306d6178253239\"></p>\n<h3>torchsnn.FullLayer</h3>\n<p>In this layer both thresholds and weights are stigmergic variables and their <em>stimuli</em> are respectively the output values and the input ones.</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d8d275da9f7f2652c20b0baa2055b2837c60a51e/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f782532387425323925323025334425323025354374657874253742696e707574253230617425323074696d6525323074253744\"></p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5d22e2a7ae25eb8e82ae5e3a6ae4c629cf69b55b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f7925323874253239253230253344253230253543746578742537426f7574707574253230617425323074696d6525323074253744\"></p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ff749d3f27aa116da183178b5d8c3d391aa9e9d4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f746825323874253239253230253344253230253543746578742537427468726573686f6c64253230617425323074696d6525323074253744\"></p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/70d480cfbd0cd795ecb063c641bf75bf8cf2292b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f57253238742532392532302533442532302535437465787425374277656967687473253230617425323074696d6525323074253744\"></p>\n<br>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/20aa0252cbb205aa9eb626caf4ed042eb1eb46a1/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f79253238742532392532302533442532302535437369676d612532385778253238742532392532302b253230622532302d253230746825323874253239253239\"></p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/afd2cb52546aae59ea293cf709dc337789a648f8/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f746825323874253239253230253344253230432532387468253238742d312532392532302b2532304d79253238742d312532392532302d2532302535437461752532432532306d696e2532432532306d6178253239\"></p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9946f7554c19d3c95fd876ad1926c56204686532/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f4a2535456d25323876253239253230253344253230253543626567696e253742626d6174726978253744253230765f25374230253744253230253236253230765f25374230253744253230253236253230253543646f7473253230253236253230765f25374230253744253230253543253543253230765f25374231253744253230253236253230765f25374231253744253230253236253230253543646f7473253230253236253230765f2537423125374425323025354325354325323025354376646f747325323025323625323025354376646f747325323025323625323025354364646f747325323025323625323025354376646f7473253230253543253543253230765f2537426e2d31253744253230253236253230765f2537426e2d31253744253230253236253230253543646f7473253230253236253230765f2537426e2d31253744253230253543656e64253742626d6174726978253744\"></p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8f9776928de5d6a870be357b2717f5264cc04f9f/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f57253238742532392532302533442532304325323857253238742d312532392532302b2532304a2535456d25323858253238742d312532392532394d5f772532302d2532302535437461755f772532432532306d696e2532432532306d6178253239\"></p>\n<h2>Citing</h2>\n<p>We can't wait to see what you will build with the Stigmergic Neural Networks!<br>\nWhen you will publish your work you can use this BibTex to cite us :)</p>\n<pre><code>@article{galatolo_snn\n,\tauthor\t= {Galatolo, Federico A and Cimino, Mario GCA and Vaglini, Gigliola}\n,\ttitle\t= {Using stigmergy to incorporate the time into artificial neural networks}\n,\tjournal\t= {MIKE 2018}\n,\tyear\t= {2018}\n}\n</code></pre>\n<h2>Contributing</h2>\n<p>This code is released under GNU/GPLv3 so feel free to fork it and submit your changes, every PR helps.<br>\nIf you need help using it or for any question please reach me at <a href=\"mailto:galatolo.federico@gmail.com\">galatolo.federico@gmail.com</a> or on Telegram  <a href=\"https://t.me/galatolo\" rel=\"nofollow\">@galatolo</a></p>\n\n          </div>"}, "last_serial": 4417210, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "c102862853536338df022aaf16a32e51", "sha256": "40e78cece7bb1d11a0cce5e833e971653fa5f2752447ae79ad6e45bf018106d3"}, "downloads": -1, "filename": "torchsnn-0.1.0.tar.gz", "has_sig": false, "md5_digest": "c102862853536338df022aaf16a32e51", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5619, "upload_time": "2018-10-25T23:29:28", "upload_time_iso_8601": "2018-10-25T23:29:28.532329Z", "url": "https://files.pythonhosted.org/packages/f2/38/d6852634b5d8c0805c6bb699f4423fa621443a277cf127356ad915640e44/torchsnn-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c102862853536338df022aaf16a32e51", "sha256": "40e78cece7bb1d11a0cce5e833e971653fa5f2752447ae79ad6e45bf018106d3"}, "downloads": -1, "filename": "torchsnn-0.1.0.tar.gz", "has_sig": false, "md5_digest": "c102862853536338df022aaf16a32e51", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5619, "upload_time": "2018-10-25T23:29:28", "upload_time_iso_8601": "2018-10-25T23:29:28.532329Z", "url": "https://files.pythonhosted.org/packages/f2/38/d6852634b5d8c0805c6bb699f4423fa621443a277cf127356ad915640e44/torchsnn-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:10 2020"}