{"info": {"author": "Alex Rogozhnikov", "author_email": "", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "Programming Language :: Python :: 3"], "description": "<a href='http://arogozhnikov.github.io/images/einops/einops_video.mp4' >\n<div align=\"center\">\n  <img src=\"http://arogozhnikov.github.io/images/einops/einops_video.gif\" alt=\"einops package examples\" />\n  <br>\n  <small><a href='http://arogozhnikov.github.io/images/einops/einops_video.mp4'>This video in better quality.</a></small>\n  <br><br>\n</div>\n</a>\n\n# einops \n[![Build Status](https://travis-ci.org/arogozhnikov/einops.svg?branch=master)](https://travis-ci.org/arogozhnikov/einops)  [![PyPI version](https://badge.fury.io/py/einops.svg)](https://badge.fury.io/py/einops)\n\nFlexible and powerful tensor operations for readable and reliable code. \nSupports numpy, pytorch, tensorflow, and [others](#supported-frameworks).\n\n\n<!--<div align=\"center\">\n  <img src=\"http://arogozhnikov.github.io/images/einops/einops_logo_350x350.png\" \n  alt=\"einops package logo\" width=\"250\" height=\"250\" />\n  <br><br>\n</div> -->\n\n## Contents\n\n- [Tutorial](#Tutorial--Documentation) \n- [API micro-reference](#API)\n- [Installation](#Installation)\n- [Naming](#Naming)\n- [Why using einops](#Why-using-einops-notation)\n- [Supported frameworks](#Supported-frameworks)\n- [Contributing](#Contributing)\n- [Github repository (for issues/questions)](https://github.com/arogozhnikov/einops)\n\n\n## Tutorial / Documentation \n\nTutorial is the most convenient way to see `einops` in action (and right now works as a documentation)\n\n- part 1: [einops fundamentals](https://github.com/arogozhnikov/einops/blob/master/docs/1-einops-basics.ipynb) \n- part 2: [einops for deep learning](https://github.com/arogozhnikov/einops/blob/master/docs/2-einops-for-deep-learning.ipynb)\n- part 3: [real code fragments improved with einops](https://arogozhnikov.github.io/einops/pytorch-examples.html) (so far only for pytorch)   \n\n## Installation\n\nPlain and simple:\n```bash\npip install einops\n```\n\n`einops` has no mandatory dependencies (code examples also require jupyter, pillow + backends). \nTo obtain the latest github version \n\n```bash\npip install https://github.com/arogozhnikov/einops/archive/master.zip\n```\n\n## API \n\n`einops` has minimalistic and powerful API.\n\nTwo operations provided (see [einops tutorial](https://github.com/arogozhnikov/einops/blob/master/docs/) for examples)\n```python\nfrom einops import rearrange, reduce\n# rearrange elements according to the pattern\noutput_tensor = rearrange(input_tensor, 't b c -> b c t')\n# combine rearrangement and reduction\noutput_tensor = reduce(input_tensor, 'b c (h h2) (w w2) -> b h w c', 'mean', h2=2, w2=2)\n```\nAnd two corresponding layers (`einops` keeps separate version for each framework) with the same API.\n\n```python\nfrom einops.layers.chainer import Rearrange, Reduce\nfrom einops.layers.gluon import Rearrange, Reduce\nfrom einops.layers.keras import Rearrange, Reduce\nfrom einops.layers.torch import Rearrange, Reduce\nfrom einops.layers.tensorflow import Rearrange, Reduce\n```\n\nLayers behave similarly to operations and have same parameters \n(for the exception of first argument, which is passed during call)\n\n```python\nlayer = Rearrange(pattern, **axes_lengths)\nlayer = Reduce(pattern, reduction, **axes_lengths)\n\n# apply created layer to a tensor / variable\nx = layer(x)\n```\n\nExample of using layers within a model:\n```python\n# example given for pytorch, but code in other frameworks is almost identical  \nfrom torch.nn import Sequential, Conv2d, MaxPool2d, Linear, ReLU\nfrom einops.layers.torch import Rearrange\n\nmodel = Sequential(\n    Conv2d(3, 6, kernel_size=5),\n    MaxPool2d(kernel_size=2),\n    Conv2d(6, 16, kernel_size=5),\n    MaxPool2d(kernel_size=2),\n    # flattening\n    Rearrange('b c h w -> b (c h w)'),  \n    Linear(16*5*5, 120), \n    ReLU(),\n    Linear(120, 10), \n)\n```\n\nAdditionally two auxiliary functions provided\n```python\nfrom einops import asnumpy, parse_shape\n# einops.asnumpy converts tensors of imperative frameworks to numpy\nnumpy_tensor = asnumpy(input_tensor)\n# einops.parse_shape gives a shape of axes of interest \nparse_shape(input_tensor, 'batch _ h w') # e.g {'batch': 64, 'h': 128, 'w': 160}\n```\n\n## Naming\n\n`einops` stays for Einstein-Inspired Notation for operations \n(though \"Einstein operations\" is more attractive and easier to remember).\n\nNotation was loosely inspired by Einstein summation (in particular by `numpy.einsum` operation).\n\n## Why using `einops` notation\n\n\n### Semantic information (being verbose in expectations)\n\n```python\ny = x.view(x.shape[0], -1)\ny = rearrange(x, 'b c h w -> b (c h w)')\n```\nwhile these two lines are doing the same job in some context,\nsecond one provides information about input and output.\nIn other words, `einops` focuses on interface: *what is input and output*, not *how* output is computed.\n\nThe next operation looks similar:\n```python\ny = rearrange(x, 'time c h w -> time (c h w)')\n```\nBut it gives reader a hint: \nthis is not an independent batch of images we are processing, \nbut rather a sequence (video). \n\nSemantic information makes code easier to read and maintain. \n\n### More checks\n\nReconsider the same example:\n```python\ny = x.view(x.shape[0], -1) # x: (batch, 256, 19, 19)\ny = rearrange(x, 'b c h w -> b (c h w)')\n```\nsecond line checks that input has four dimensions, \nbut you can also specify particular dimensions. \nThat's opposed to just writing comments about shapes since \n[comments don't work](https://medium.freecodecamp.org/code-comments-the-good-the-bad-and-the-ugly-be9cc65fbf83)\nas we know   \n```python\ny = x.view(x.shape[0], -1) # x: (batch, 256, 19, 19)\ny = rearrange(x, 'b c h w -> b (c h w)', c=256, h=19, w=19)\n```\n\n### Result is strictly determined\n\nBelow we have at least two ways to define depth-to-space operation\n```python\n# depth-to-space\nrearrange(x, 'b c (h h2) (w w2) -> b (c h2 w2) h w', h2=2, w2=2)\nrearrange(x, 'b c (h h2) (w w2) -> b (h2 w2 c) h w', h2=2, w2=2)\n```\nthere are at least four more ways to do it. Which one is used by the framework?\n\nThese details are ignored, since *usually* it makes no difference, \nbut it can make a big difference (e.g. if you use grouped convolutions on the next stage), \nand you'd like to specify this in your code.\n\n<!-- TODO add example with 1d elements? -->\n\n### Uniformity\n\n```python\nreduce(x, 'b c (x dx) -> b c x', 'max', dx=2)\nreduce(x, 'b c (x dx) (y dx) -> b c x y', 'max', dx=2, dy=3)\nreduce(x, 'b c (x dx) (y dx) (z dz)-> b c x y z', 'max', dx=2, dy=3, dz=4)\n```\nThese examples demonstrated that we don't use separate operations for 1d/2d/3d pooling, \nthose all are defined in a uniform way. \n\nSpace-to-depth and depth-to space are defined in many frameworks. But how about width-to-height?\n```python\nrearrange(x, 'b c h (w w2) -> b c (h w2) w', w2=2)\n```\n\n### Framework independent behavior\n\nEven simple functions are defined differently by different frameworks\n\n```python\ny = x.flatten() # or flatten(x)\n```\n\nSuppose `x` shape was `(3, 4, 5)`, then `y` has shape ...\n- numpy, cupy, chainer: `(60,)`\n- keras, tensorflow.layers, mxnet and gluon: `(3, 20)`\n- pytorch: no such function\n\n<!-- TODO examples for depth-to-space and pixel shuffle? transpose vs permute? torch.repeat is numpy.tile -->\n\n## Supported frameworks\n\nEinops works with ...\n\n- [numpy](http://www.numpy.org/)\n- [pytorch](https://pytorch.org/)\n- [tensorflow eager](https://www.tensorflow.org/guide/eager)\n- [cupy](https://cupy.chainer.org/)\n- [chainer](https://chainer.org/)\n- [gluon](https://mxnet.apache.org/)\n- [tensorflow](https://www.tensorflow.org/)\n- [keras](https://keras.io/) and [tf.keras](https://www.tensorflow.org/guide/keras)\n- [mxnet](https://gluon.mxnet.io/) (experimental)\n- [jax](https://github.com/google/jax) (experimental)\n\n## Contributing \n\nBest ways to contribute are\n\n- share your feedback. Experimental APIs currently require third-party testing.\n- spread the word about `einops`\n- if you like explaining things, alternative tutorials can be helpful\n- translating examples in languages other than English is also a good idea \n- finally, use `einops` notation in your papers to strictly define used operations!\n\n## Supported python versions\n\n`einops` works with python 3.5 or later. \n\nThere is nothing specific to python 3 in the code, \nwe simply [need to move further](http://github.com/arogozhnikov/python3_with_pleasure) \nand the decision is not to support python 2.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/arogozhnikov/einops", "keywords": "deep learning,neural networks,tensor manipulation,machine learning,scientific computations,einops", "license": "", "maintainer": "", "maintainer_email": "", "name": "einops", "package_url": "https://pypi.org/project/einops/", "platform": "", "project_url": "https://pypi.org/project/einops/", "project_urls": {"Homepage": "https://github.com/arogozhnikov/einops"}, "release_url": "https://pypi.org/project/einops/0.2.0/", "requires_dist": null, "requires_python": "", "summary": "A new flavour of deep learning operations", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"http://arogozhnikov.github.io/images/einops/einops_video.mp4\" rel=\"nofollow\">\n</a><div><a href=\"http://arogozhnikov.github.io/images/einops/einops_video.mp4\" rel=\"nofollow\">\n  <img alt=\"einops package examples\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9c0bdb374345ab83ed79072bb645ae86f14969f7/687474703a2f2f61726f676f7a686e696b6f762e6769746875622e696f2f696d616765732f65696e6f70732f65696e6f70735f766964656f2e676966\">\n  <br>\n  &lt;small&gt;</a><a href=\"http://arogozhnikov.github.io/images/einops/einops_video.mp4\" rel=\"nofollow\">This video in better quality.</a>&lt;/small&gt;\n  <br><br>\n</div>\n\n<h1>einops</h1>\n<p><a href=\"https://travis-ci.org/arogozhnikov/einops\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/97ad52587c0fe00d8a0fda1087737df7f6f2ff97/68747470733a2f2f7472617669732d63692e6f72672f61726f676f7a686e696b6f762f65696e6f70732e7376673f6272616e63683d6d6173746572\"></a>  <a href=\"https://badge.fury.io/py/einops\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9e065cca36caed219522ecbcb93ca9d8c9160ac4/68747470733a2f2f62616467652e667572792e696f2f70792f65696e6f70732e737667\"></a></p>\n<p>Flexible and powerful tensor operations for readable and reliable code.\nSupports numpy, pytorch, tensorflow, and <a href=\"#supported-frameworks\" rel=\"nofollow\">others</a>.</p>\n\n<h2>Contents</h2>\n<ul>\n<li><a href=\"#Tutorial--Documentation\" rel=\"nofollow\">Tutorial</a></li>\n<li><a href=\"#API\" rel=\"nofollow\">API micro-reference</a></li>\n<li><a href=\"#Installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#Naming\" rel=\"nofollow\">Naming</a></li>\n<li><a href=\"#Why-using-einops-notation\" rel=\"nofollow\">Why using einops</a></li>\n<li><a href=\"#Supported-frameworks\" rel=\"nofollow\">Supported frameworks</a></li>\n<li><a href=\"#Contributing\" rel=\"nofollow\">Contributing</a></li>\n<li><a href=\"https://github.com/arogozhnikov/einops\" rel=\"nofollow\">Github repository (for issues/questions)</a></li>\n</ul>\n<h2>Tutorial / Documentation</h2>\n<p>Tutorial is the most convenient way to see <code>einops</code> in action (and right now works as a documentation)</p>\n<ul>\n<li>part 1: <a href=\"https://github.com/arogozhnikov/einops/blob/master/docs/1-einops-basics.ipynb\" rel=\"nofollow\">einops fundamentals</a></li>\n<li>part 2: <a href=\"https://github.com/arogozhnikov/einops/blob/master/docs/2-einops-for-deep-learning.ipynb\" rel=\"nofollow\">einops for deep learning</a></li>\n<li>part 3: <a href=\"https://arogozhnikov.github.io/einops/pytorch-examples.html\" rel=\"nofollow\">real code fragments improved with einops</a> (so far only for pytorch)</li>\n</ul>\n<h2>Installation</h2>\n<p>Plain and simple:</p>\n<pre>pip install einops\n</pre>\n<p><code>einops</code> has no mandatory dependencies (code examples also require jupyter, pillow + backends).\nTo obtain the latest github version</p>\n<pre>pip install https://github.com/arogozhnikov/einops/archive/master.zip\n</pre>\n<h2>API</h2>\n<p><code>einops</code> has minimalistic and powerful API.</p>\n<p>Two operations provided (see <a href=\"https://github.com/arogozhnikov/einops/blob/master/docs/\" rel=\"nofollow\">einops tutorial</a> for examples)</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">einops</span> <span class=\"kn\">import</span> <span class=\"n\">rearrange</span><span class=\"p\">,</span> <span class=\"n\">reduce</span>\n<span class=\"c1\"># rearrange elements according to the pattern</span>\n<span class=\"n\">output_tensor</span> <span class=\"o\">=</span> <span class=\"n\">rearrange</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">,</span> <span class=\"s1\">'t b c -&gt; b c t'</span><span class=\"p\">)</span>\n<span class=\"c1\"># combine rearrangement and reduction</span>\n<span class=\"n\">output_tensor</span> <span class=\"o\">=</span> <span class=\"n\">reduce</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">,</span> <span class=\"s1\">'b c (h h2) (w w2) -&gt; b h w c'</span><span class=\"p\">,</span> <span class=\"s1\">'mean'</span><span class=\"p\">,</span> <span class=\"n\">h2</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">w2</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n</pre>\n<p>And two corresponding layers (<code>einops</code> keeps separate version for each framework) with the same API.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">einops.layers.chainer</span> <span class=\"kn\">import</span> <span class=\"n\">Rearrange</span><span class=\"p\">,</span> <span class=\"n\">Reduce</span>\n<span class=\"kn\">from</span> <span class=\"nn\">einops.layers.gluon</span> <span class=\"kn\">import</span> <span class=\"n\">Rearrange</span><span class=\"p\">,</span> <span class=\"n\">Reduce</span>\n<span class=\"kn\">from</span> <span class=\"nn\">einops.layers.keras</span> <span class=\"kn\">import</span> <span class=\"n\">Rearrange</span><span class=\"p\">,</span> <span class=\"n\">Reduce</span>\n<span class=\"kn\">from</span> <span class=\"nn\">einops.layers.torch</span> <span class=\"kn\">import</span> <span class=\"n\">Rearrange</span><span class=\"p\">,</span> <span class=\"n\">Reduce</span>\n<span class=\"kn\">from</span> <span class=\"nn\">einops.layers.tensorflow</span> <span class=\"kn\">import</span> <span class=\"n\">Rearrange</span><span class=\"p\">,</span> <span class=\"n\">Reduce</span>\n</pre>\n<p>Layers behave similarly to operations and have same parameters\n(for the exception of first argument, which is passed during call)</p>\n<pre><span class=\"n\">layer</span> <span class=\"o\">=</span> <span class=\"n\">Rearrange</span><span class=\"p\">(</span><span class=\"n\">pattern</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">axes_lengths</span><span class=\"p\">)</span>\n<span class=\"n\">layer</span> <span class=\"o\">=</span> <span class=\"n\">Reduce</span><span class=\"p\">(</span><span class=\"n\">pattern</span><span class=\"p\">,</span> <span class=\"n\">reduction</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">axes_lengths</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># apply created layer to a tensor / variable</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">layer</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n</pre>\n<p>Example of using layers within a model:</p>\n<pre><span class=\"c1\"># example given for pytorch, but code in other frameworks is almost identical  </span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.nn</span> <span class=\"kn\">import</span> <span class=\"n\">Sequential</span><span class=\"p\">,</span> <span class=\"n\">Conv2d</span><span class=\"p\">,</span> <span class=\"n\">MaxPool2d</span><span class=\"p\">,</span> <span class=\"n\">Linear</span><span class=\"p\">,</span> <span class=\"n\">ReLU</span>\n<span class=\"kn\">from</span> <span class=\"nn\">einops.layers.torch</span> <span class=\"kn\">import</span> <span class=\"n\">Rearrange</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">(</span>\n    <span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">),</span>\n    <span class=\"n\">MaxPool2d</span><span class=\"p\">(</span><span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n    <span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">),</span>\n    <span class=\"n\">MaxPool2d</span><span class=\"p\">(</span><span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n    <span class=\"c1\"># flattening</span>\n    <span class=\"n\">Rearrange</span><span class=\"p\">(</span><span class=\"s1\">'b c h w -&gt; b (c h w)'</span><span class=\"p\">),</span>  \n    <span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"o\">*</span><span class=\"mi\">5</span><span class=\"o\">*</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">120</span><span class=\"p\">),</span> \n    <span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n    <span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">120</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span> \n<span class=\"p\">)</span>\n</pre>\n<p>Additionally two auxiliary functions provided</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">einops</span> <span class=\"kn\">import</span> <span class=\"n\">asnumpy</span><span class=\"p\">,</span> <span class=\"n\">parse_shape</span>\n<span class=\"c1\"># einops.asnumpy converts tensors of imperative frameworks to numpy</span>\n<span class=\"n\">numpy_tensor</span> <span class=\"o\">=</span> <span class=\"n\">asnumpy</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">)</span>\n<span class=\"c1\"># einops.parse_shape gives a shape of axes of interest </span>\n<span class=\"n\">parse_shape</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">,</span> <span class=\"s1\">'batch _ h w'</span><span class=\"p\">)</span> <span class=\"c1\"># e.g {'batch': 64, 'h': 128, 'w': 160}</span>\n</pre>\n<h2>Naming</h2>\n<p><code>einops</code> stays for Einstein-Inspired Notation for operations\n(though \"Einstein operations\" is more attractive and easier to remember).</p>\n<p>Notation was loosely inspired by Einstein summation (in particular by <code>numpy.einsum</code> operation).</p>\n<h2>Why using <code>einops</code> notation</h2>\n<h3>Semantic information (being verbose in expectations)</h3>\n<pre><span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">rearrange</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">'b c h w -&gt; b (c h w)'</span><span class=\"p\">)</span>\n</pre>\n<p>while these two lines are doing the same job in some context,\nsecond one provides information about input and output.\nIn other words, <code>einops</code> focuses on interface: <em>what is input and output</em>, not <em>how</em> output is computed.</p>\n<p>The next operation looks similar:</p>\n<pre><span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">rearrange</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">'time c h w -&gt; time (c h w)'</span><span class=\"p\">)</span>\n</pre>\n<p>But it gives reader a hint:\nthis is not an independent batch of images we are processing,\nbut rather a sequence (video).</p>\n<p>Semantic information makes code easier to read and maintain.</p>\n<h3>More checks</h3>\n<p>Reconsider the same example:</p>\n<pre><span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"c1\"># x: (batch, 256, 19, 19)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">rearrange</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">'b c h w -&gt; b (c h w)'</span><span class=\"p\">)</span>\n</pre>\n<p>second line checks that input has four dimensions,\nbut you can also specify particular dimensions.\nThat's opposed to just writing comments about shapes since\n<a href=\"https://medium.freecodecamp.org/code-comments-the-good-the-bad-and-the-ugly-be9cc65fbf83\" rel=\"nofollow\">comments don't work</a>\nas we know</p>\n<pre><span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"c1\"># x: (batch, 256, 19, 19)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">rearrange</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">'b c h w -&gt; b (c h w)'</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"o\">=</span><span class=\"mi\">19</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"o\">=</span><span class=\"mi\">19</span><span class=\"p\">)</span>\n</pre>\n<h3>Result is strictly determined</h3>\n<p>Below we have at least two ways to define depth-to-space operation</p>\n<pre><span class=\"c1\"># depth-to-space</span>\n<span class=\"n\">rearrange</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">'b c (h h2) (w w2) -&gt; b (c h2 w2) h w'</span><span class=\"p\">,</span> <span class=\"n\">h2</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">w2</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">rearrange</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">'b c (h h2) (w w2) -&gt; b (h2 w2 c) h w'</span><span class=\"p\">,</span> <span class=\"n\">h2</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">w2</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n</pre>\n<p>there are at least four more ways to do it. Which one is used by the framework?</p>\n<p>These details are ignored, since <em>usually</em> it makes no difference,\nbut it can make a big difference (e.g. if you use grouped convolutions on the next stage),\nand you'd like to specify this in your code.</p>\n\n<h3>Uniformity</h3>\n<pre><span class=\"n\">reduce</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">'b c (x dx) -&gt; b c x'</span><span class=\"p\">,</span> <span class=\"s1\">'max'</span><span class=\"p\">,</span> <span class=\"n\">dx</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">reduce</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">'b c (x dx) (y dx) -&gt; b c x y'</span><span class=\"p\">,</span> <span class=\"s1\">'max'</span><span class=\"p\">,</span> <span class=\"n\">dx</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">dy</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n<span class=\"n\">reduce</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">'b c (x dx) (y dx) (z dz)-&gt; b c x y z'</span><span class=\"p\">,</span> <span class=\"s1\">'max'</span><span class=\"p\">,</span> <span class=\"n\">dx</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">dy</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">dz</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n</pre>\n<p>These examples demonstrated that we don't use separate operations for 1d/2d/3d pooling,\nthose all are defined in a uniform way.</p>\n<p>Space-to-depth and depth-to space are defined in many frameworks. But how about width-to-height?</p>\n<pre><span class=\"n\">rearrange</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">'b c h (w w2) -&gt; b c (h w2) w'</span><span class=\"p\">,</span> <span class=\"n\">w2</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n</pre>\n<h3>Framework independent behavior</h3>\n<p>Even simple functions are defined differently by different frameworks</p>\n<pre><span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">flatten</span><span class=\"p\">()</span> <span class=\"c1\"># or flatten(x)</span>\n</pre>\n<p>Suppose <code>x</code> shape was <code>(3, 4, 5)</code>, then <code>y</code> has shape ...</p>\n<ul>\n<li>numpy, cupy, chainer: <code>(60,)</code></li>\n<li>keras, tensorflow.layers, mxnet and gluon: <code>(3, 20)</code></li>\n<li>pytorch: no such function</li>\n</ul>\n\n<h2>Supported frameworks</h2>\n<p>Einops works with ...</p>\n<ul>\n<li><a href=\"http://www.numpy.org/\" rel=\"nofollow\">numpy</a></li>\n<li><a href=\"https://pytorch.org/\" rel=\"nofollow\">pytorch</a></li>\n<li><a href=\"https://www.tensorflow.org/guide/eager\" rel=\"nofollow\">tensorflow eager</a></li>\n<li><a href=\"https://cupy.chainer.org/\" rel=\"nofollow\">cupy</a></li>\n<li><a href=\"https://chainer.org/\" rel=\"nofollow\">chainer</a></li>\n<li><a href=\"https://mxnet.apache.org/\" rel=\"nofollow\">gluon</a></li>\n<li><a href=\"https://www.tensorflow.org/\" rel=\"nofollow\">tensorflow</a></li>\n<li><a href=\"https://keras.io/\" rel=\"nofollow\">keras</a> and <a href=\"https://www.tensorflow.org/guide/keras\" rel=\"nofollow\">tf.keras</a></li>\n<li><a href=\"https://gluon.mxnet.io/\" rel=\"nofollow\">mxnet</a> (experimental)</li>\n<li><a href=\"https://github.com/google/jax\" rel=\"nofollow\">jax</a> (experimental)</li>\n</ul>\n<h2>Contributing</h2>\n<p>Best ways to contribute are</p>\n<ul>\n<li>share your feedback. Experimental APIs currently require third-party testing.</li>\n<li>spread the word about <code>einops</code></li>\n<li>if you like explaining things, alternative tutorials can be helpful</li>\n<li>translating examples in languages other than English is also a good idea</li>\n<li>finally, use <code>einops</code> notation in your papers to strictly define used operations!</li>\n</ul>\n<h2>Supported python versions</h2>\n<p><code>einops</code> works with python 3.5 or later.</p>\n<p>There is nothing specific to python 3 in the code,\nwe simply <a href=\"http://github.com/arogozhnikov/python3_with_pleasure\" rel=\"nofollow\">need to move further</a>\nand the decision is not to support python 2.</p>\n\n          </div>"}, "last_serial": 6635992, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "489ed693686d3b474d44e2959f0e9025", "sha256": "4ab512fe059c0841e1a315449ca9d7f35eaa05c8c095a14f2c1b92b2b77684d2"}, "downloads": -1, "filename": "einops-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "489ed693686d3b474d44e2959f0e9025", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 16829, "upload_time": "2018-10-30T08:07:28", "upload_time_iso_8601": "2018-10-30T08:07:28.849184Z", "url": "https://files.pythonhosted.org/packages/36/1f/a5e6496a167b6e892123b201ccce251184457ce7a1b6c8e06662c09bfbab/einops-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9ab814121e666c0cf50944dd1a892ee3", "sha256": "4fd64864fcb8159074da3213b9327c242536784416cbf423745ef8579850d30b"}, "downloads": -1, "filename": "einops-0.1.0.tar.gz", "has_sig": false, "md5_digest": "9ab814121e666c0cf50944dd1a892ee3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15083, "upload_time": "2018-10-30T08:07:30", "upload_time_iso_8601": "2018-10-30T08:07:30.154116Z", "url": "https://files.pythonhosted.org/packages/32/d3/efa7cd2f496ebe32b7b7e17bb0d91f2fb216b032ce572e63bab767f46e32/einops-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "16af6b8b243d200a1c301c01d059713b", "sha256": "96b1bac57ddb591cccb927d24934d7601c3cdf3343a79a43d316a118d66e1043"}, "downloads": -1, "filename": "einops-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "16af6b8b243d200a1c301c01d059713b", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 18682, "upload_time": "2020-02-15T11:00:31", "upload_time_iso_8601": "2020-02-15T11:00:31.687176Z", "url": "https://files.pythonhosted.org/packages/89/32/5ded0a73d2e14ef5a6908a930c3e1e9f92ffead482a2f153182b7429066e/einops-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1528f3cea8a32f309abd5c89c4a351c3", "sha256": "165ee28bcb60e5c2cbb801b5c78e181548ff8daa7c8fcabae5b251e55f7fe614"}, "downloads": -1, "filename": "einops-0.2.0.tar.gz", "has_sig": false, "md5_digest": "1528f3cea8a32f309abd5c89c4a351c3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15557, "upload_time": "2020-02-15T11:00:33", "upload_time_iso_8601": "2020-02-15T11:00:33.418861Z", "url": "https://files.pythonhosted.org/packages/d1/20/df4006976ecb81f5e0bea3f5cbdbd90c4a70d5552b7541a82419449b9481/einops-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "16af6b8b243d200a1c301c01d059713b", "sha256": "96b1bac57ddb591cccb927d24934d7601c3cdf3343a79a43d316a118d66e1043"}, "downloads": -1, "filename": "einops-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "16af6b8b243d200a1c301c01d059713b", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 18682, "upload_time": "2020-02-15T11:00:31", "upload_time_iso_8601": "2020-02-15T11:00:31.687176Z", "url": "https://files.pythonhosted.org/packages/89/32/5ded0a73d2e14ef5a6908a930c3e1e9f92ffead482a2f153182b7429066e/einops-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1528f3cea8a32f309abd5c89c4a351c3", "sha256": "165ee28bcb60e5c2cbb801b5c78e181548ff8daa7c8fcabae5b251e55f7fe614"}, "downloads": -1, "filename": "einops-0.2.0.tar.gz", "has_sig": false, "md5_digest": "1528f3cea8a32f309abd5c89c4a351c3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15557, "upload_time": "2020-02-15T11:00:33", "upload_time_iso_8601": "2020-02-15T11:00:33.418861Z", "url": "https://files.pythonhosted.org/packages/d1/20/df4006976ecb81f5e0bea3f5cbdbd90c4a70d5552b7541a82419449b9481/einops-0.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:47:13 2020"}