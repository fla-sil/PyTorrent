{"info": {"author": "Poppin", "author_email": "technology@poppin.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Intended Audience :: Information Technology", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3 :: Only"], "description": "Mr. Plow\n========\n\nMr. Plow is Poppin's ETL system to persist data from third-party APIs into a\nSnowflake database for later business analysis.\n\nWe use Python to:\n\n#. Call said APIs and store the data in AWS S3 (\"Extract\")\n#. Issue Snowflake commands to import the data from AWS S3 (\"Stage\")\n#. Issue Snowflake commands to transform the new data from its original\n   unstructured form to the tabular form used for analysis (\"Transform\")\n#. Issue Snowflake commands to load the new tabular data to our main store,\n   eliminating any duplicates (\"Load\")\n\nMr. Plow can be run from the command line. In the future we will add support\nfor running its functions as AWS Lambda functions.\n\nWhy that ridiculous name?\n-------------------------\n\nTwo reasons.\n\nFirst, this is primarily a Snowflake client.\n\nSecond: https://www.youtube.com/watch?v=uYXEt7xOh1M\n\nInstallation\n============\n\nCurrently, we support only Python 3.6. This is the highest current version.\nThere is no reason to suspect 3.7 and up will not work to run Mr. Plow.\n\nLower Python 3.x versions may work, but they are not tested. You are free to\ntry them. If you succeed running Mr. Plow in a 3.3, 3.4, or 3.5 environment,\nplease tell us on the Github issue tracker and we will add testing support.\n\nFirst run ``python3 --version`` to ensure you have the required Python version.\nIt may also be installed as ``python3.6``.\n\nWe recommend you install using a virtual python environment to manage library\ndependencies. `Virtualenv <https://virtualenv.pypa.io/en/stable/>`_ is a great\ntool for this.\n\nSuggested usage:\n\n#. `Install virtualenv <https://virtualenv.pypa.io/en/stable/installation/>`_\n   if not already present\n#. Create a virtual environment inside the clone: ``virtualenv -p python3.6 venv``\n\n   * This creates a directory ``venv/``, which contains an entire python\n     environment.\n\n#. Activate your virtual environment: ``source venv/bin/activate``\n\n   * This adds the virtual environment's ``python`` and ``pip`` executables to your\n     ``PATH``.\n\n#. Install dependencies: ``pip install mr-plow``\n\nNow you should have the executable ``plow`` in your ``PATH``. Simply run\n``plow`` at the command line to see available commands and an example workflow.\n\nAll commands accept the ``--help`` option and print a helpful usage guide.\n\nConfiguration\n=============\n\nSince we access both Snowflake and AWS S3, credentials are required. These may\nbe provided via a config file. It can be provided to the ``plow`` CLI tool\neither via the ``-c`` option or by exporting the environment variable\n``MR_PLOW_CONFIG``.\n\nThe supported formats are JSON, YAML, and INI. INI files must have the\nappropriate settings in the ``[mr-plow]`` section. Run ``plow generate-config``\nto have a minimal example generated for you to fill in.\n\nMr. Plow fills in configuration it does not find by searching environment\nvariables.\n\nTo see more including a full list of config options, install Mr. Plow and run\n``python -m pydoc plow.config.Config``.\n\nSnowflake\n---------\n\nThose commands that require Snowflake access draw the credentials from the\nexecution environment. Mr. Plow uses the following environment variables to\ncreate a Snowflake connection. If both ``SNOWSQL_FOO`` and ``SNOWFLAKE_FOO``\nare present as environment variables,  Mr. Plow uses ``SNOWFLAKE_FOO``.\n\n* ``SNOWFLAKE_ACCOUNT`` or ``SNOWSQL_ACCOUNT``\n\n  * visible in the URL you use to log in to Snowflake. For the author and his\n    peers, this is \"poppin\".\n\n* ``SNOWFLAKE_USER`` or ``SNOWSQL_USER``\n\n  * Same as your login on the Snowflake website.\n\n* ``SNOWFLAKE_PASSWORD`` or ``SNOWSQL_PASSWORD``\n\n  * This is the password for that login. With the command line, this is\n    optional as it can be provided as a console prompt.\n\n* ``SNOWFLAKE_ROLE``  or ``SNOWSQL_ROLE``\n\n* ``SNOWFLAKE_DATABASE`` or ``SNOWSQL_DATABASE``\n\n* ``SNOWFLAKE_SCHEMA`` or ``SNOWSQL_SCHEMA``\n\n  * E.g. \"public\". However please keep in mind that Mr. Plow creates its own\n    schemas and never references tables without explicitly providing a schema.\n\n* ``SNOWFLAKE_WAREHOUSE`` or ``SNOWSQL_WAREHOUSE``\n\n.. warning::\n    **Caution**: Choose carefully as many of these commands spin up a warehouse\n    instance. You may incur charges.\n\nAWS\n----\n\nWe use `boto3 <https://boto3.readthedocs.io/en/latest/>`_ to connect to AWS S3.\nIt has a rich system for specifying credentials, which can be used in its\nentirety by omitting AWS settings from your config file. This is appropriate in\nAWS Lambda, where credentials are taken care of in the background, or if you\nare an `AWS CLI <https://aws.amazon.com/documentation/cli/>`_ user and have\nexisting configs. See `boto3 documentation\n<https://boto3.readthedocs.io/en/latest/guide/quickstart.html#configuration>`_\nfor more detail.\n\nTo specify config directly, you may either give the ``aws_access_key_id`` and\n``aws_secret_access_key`` settings directly, or if you have an existing `AWS\nCLI configuration\n<https://docs.aws.amazon.com/cli/latest/reference/configure/index.html>`_, you\nmay simply specify ``aws_profile``.\n\nWhen Snowflake reads your API data from S3, it requires you to provide AWS\ncredentials with the appropriate S3 read permissions. Mr. Plow picks these up\nas distinct config options: ``staging_aws_access_key_id`` and\n``staging_aws_secret_access_key``. As with the direct S3 credentials,\n``staging_aws_profile`` may also be provided.\n\nThird-party API's\n-----------------\n\nWe currently provide integrations for Livechat and Snapfulfil.\n\nThe Livechat integration requires an API login, which is specified as config\noptions ``livechat_login`` and ``livechat_api_key``.\n\nThe Snapfulfil integration requires an API login (config ``snapfulfil_login`` and\n``snapfulfil_password``) as well as an explicit designation of which Snap domain\nyou want to make requests to (config ``snapfulfil_api_domain``). Typically the\nlatter is either ``https://treltestapi.snapfulfil.net`` or\n``https://trelliveapi.snapfulfil.net``.\n\nExtending Mr. Plow\n==================\n\nMr. Plow is designed to be broadly useful and easily extensible. We currently\nprovide integrations for Livechat and Snapfulfil, and more will be added, but\nyou can easily add your own.\n\nYou must create your own implementation of ``plow.op.extract.Extractor`` to\ndefine how to fetch data. In a pinch you can use a RestExtractor; to allow\nfor automatic fetching of subsequent pages, you'll have to subclass it and\nimplement ``postprocess_response()``. See documentation of\n``plow.op.Extractor``; also see ``plow.vendors.*`` for examples.\n\nYou must furthermore create your own instances of ``plow.queries.Table`` to\nspecify how to translate data from the documents you fetch using the Extractor\ninto Snowflake DB tables. See documentation of ``plow.queries.Table``, and see\n``plow.queries.livechat`` and ``plow.queries.snapfulfil`` for tested examples.\n\nFinally, you must create a ``plow.cli.Source`` pointing to all of these.\n\nIf you write your own adapter, we'd love to include it. Please send a pull\nrequest!\n\nExample\n-------\n\nIf you have the following files in your Python project and ``mr-plow`` installed\nwith ``pip``::\n\n    # mymodule/plow/extract.py\n    from plow.op.extract import RestExtractor\n    class Extractor(RestExtractor):\n        ...\n\n::\n\n    # mymodule/plow/tables.py\n    from plow.queries import Table\n    class Table1(Table):\n        select = \"...\"\n        # etc...\n\n    class Table2(Table):\n        ...\n\n    class Table3(Table):\n        ...\n\n::\n\n    # mymodule/plow/cli.py\n    from plow.cli import Source\n    from mymodule.plow.extract import Extractor\n    from mymodule.plow.tables import Table1, Table2, Table3\n\n    extractor = Extractor()\n    tables = {t.name: t for t in (Table1(), Table2(), Table3())}\n    source = Source(extractor=extractor, tables=tables)\n\n\nThen you can invoke the ``plow`` CLI tool, using the ``--source`` option to point\nto your code::\n\n    $ plow -c mr-plow.ini extract --source mymodule.plow.cli:source [options]...\n\n\nDevelopment\n===========\n\nDeveloper installation\n----------------------\n\nTo develop on Mr. Plow, clone this repository, set up and activate a virtualenv\n(see Installation) in the new working copy, and run ``pip install -e .[dev]``.\nThis installs the ``plow`` executable as well as development dependencies like\nFlake8 (the linter we use) and pytest.\n\nTesting\n-------\n\nMr. Plow is tested using `pytest <https://docs.pytest.org/en/latest/>`_. If you\nclone the source and install using ``pip install -e .[dev]``, it is installed\nautomatically along with several other test dependencies. Run ``pytest`` to run\nthe unit tests; add ``--cov-report=term-missing`` or ``--cov-report=html`` to\nsee detailed coverage information.\n\nTesting is separated into two section, unit tests and integration tests.\nIntegration tests are disabled by default: specify ``pytest --integration`` to\nrun integration tests as well, or ``pytest --no-unit`` to disable unit tests and\nrun only integration tests.\n\nThe unit tests use mocking for all external functionality, including Snowflake,\nS3, and third-party API's, and so may be run without an internet connection or\nany of the service-specific configuration specified above. However, at this\ntime, with very few exceptions these tests do not verify any specific SQL\nqueries, nor almost any vendor-specific logic.\n\nThe integration tests run all Snowflake setup operations and a full run of ETL\noperations through S3 and Snowflake, so you must do some setup in order to run\nthem. We mock access to the third party API, so that we can simulate the\nprocessing of a constant dataset and verify the result with precision. To run\nintegration tests, you must supply a configuration file by exporting its path\nto ``PLOW_TEST_CONFIG``.\n\n.. warning::\n    **Caution**: Since these run real Snowflake operations, you may incur charges by running\n    these tests.\n\nGit hooks\n---------\n\nThis project adheres to several standards including a style guide and unit\ntests. To aid developers in complying, we include hooks that can be run upon\na commit. Install them as follows:\n\n* Include whatever hooks you wish in your own .git/hooks/pre-commit::\n\n    $ echo '#!/usr/bin/env bash' >> .git/hooks/pre-commit\n    $ echo 'flake8-hook.py || exit $?' >> .git/hooks/pre-commit\n    $ echo 'unittest-hook.sh || exit $?' >> .git/hooks/pre-commit\n    $ chmod u+x .git/hooks/pre-commit\n\n* Add the appropriate options to your git config::\n\n    $ git config flake8.strict true\n    $ git config plow.unit.strict true\n\nNow, the linter and the unit tests will run every time you commit and you will\nbe prompted to fix any deficiencies before committing. These checks can be\ndisabled temporarily using environment variables. To avoid a linting check::\n\n    $ git commit\n    Style errors found!\n    $ FLAKE8_STRICT=false git commit\n    [...] Success!\n\nTo skip unit tests::\n\n    $ PLOW_UNIT_STRICT=false git commit\n\n\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Poppin-Tech/mr-plow", "keywords": "Snowflake,Database,Amazon Web Services,AWS,S3,API,ETL", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "mr-plow", "package_url": "https://pypi.org/project/mr-plow/", "platform": "", "project_url": "https://pypi.org/project/mr-plow/", "project_urls": {"Homepage": "https://github.com/Poppin-Tech/mr-plow"}, "release_url": "https://pypi.org/project/mr-plow/0.1.5/", "requires_dist": ["boto3 (>=1.4.6)", "click (>=6.7)", "ijson (>=2.3)", "requests (>=2.18.4)", "snowflake-connector-python (>=1.4.10)", "pyyaml (>=3.12)", "pytest; extra == 'dev'", "pytest-cov; extra == 'dev'", "pytest-mock; extra == 'dev'", "flake8; extra == 'dev'", "pytest-profiling; extra == 'dev'"], "requires_python": "~=3.6", "summary": "ETL from third-party APIs into Snowflake", "version": "0.1.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"mr-plow\">\n<h2>Mr. Plow</h2>\n<p>Mr. Plow is Poppin\u2019s ETL system to persist data from third-party APIs into a\nSnowflake database for later business analysis.</p>\n<p>We use Python to:</p>\n<ol>\n<li>Call said APIs and store the data in AWS S3 (\u201cExtract\u201d)</li>\n<li>Issue Snowflake commands to import the data from AWS S3 (\u201cStage\u201d)</li>\n<li>Issue Snowflake commands to transform the new data from its original\nunstructured form to the tabular form used for analysis (\u201cTransform\u201d)</li>\n<li>Issue Snowflake commands to load the new tabular data to our main store,\neliminating any duplicates (\u201cLoad\u201d)</li>\n</ol>\n<p>Mr. Plow can be run from the command line. In the future we will add support\nfor running its functions as AWS Lambda functions.</p>\n<div id=\"why-that-ridiculous-name\">\n<h3>Why that ridiculous name?</h3>\n<p>Two reasons.</p>\n<p>First, this is primarily a Snowflake client.</p>\n<p>Second: <a href=\"https://www.youtube.com/watch?v=uYXEt7xOh1M\" rel=\"nofollow\">https://www.youtube.com/watch?v=uYXEt7xOh1M</a></p>\n</div>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>Currently, we support only Python 3.6. This is the highest current version.\nThere is no reason to suspect 3.7 and up will not work to run Mr. Plow.</p>\n<p>Lower Python 3.x versions may work, but they are not tested. You are free to\ntry them. If you succeed running Mr. Plow in a 3.3, 3.4, or 3.5 environment,\nplease tell us on the Github issue tracker and we will add testing support.</p>\n<p>First run <tt>python3 <span class=\"pre\">--version</span></tt> to ensure you have the required Python version.\nIt may also be installed as <tt>python3.6</tt>.</p>\n<p>We recommend you install using a virtual python environment to manage library\ndependencies. <a href=\"https://virtualenv.pypa.io/en/stable/\" rel=\"nofollow\">Virtualenv</a> is a great\ntool for this.</p>\n<p>Suggested usage:</p>\n<ol>\n<li><a href=\"https://virtualenv.pypa.io/en/stable/installation/\" rel=\"nofollow\">Install virtualenv</a>\nif not already present</li>\n<li>Create a virtual environment inside the clone: <tt>virtualenv <span class=\"pre\">-p</span> python3.6 venv</tt><ul>\n<li>This creates a directory <tt>venv/</tt>, which contains an entire python\nenvironment.</li>\n</ul>\n</li>\n<li>Activate your virtual environment: <tt>source venv/bin/activate</tt><ul>\n<li>This adds the virtual environment\u2019s <tt>python</tt> and <tt>pip</tt> executables to your\n<tt>PATH</tt>.</li>\n</ul>\n</li>\n<li>Install dependencies: <tt>pip install <span class=\"pre\">mr-plow</span></tt></li>\n</ol>\n<p>Now you should have the executable <tt>plow</tt> in your <tt>PATH</tt>. Simply run\n<tt>plow</tt> at the command line to see available commands and an example workflow.</p>\n<p>All commands accept the <tt><span class=\"pre\">--help</span></tt> option and print a helpful usage guide.</p>\n</div>\n<div id=\"configuration\">\n<h2>Configuration</h2>\n<p>Since we access both Snowflake and AWS S3, credentials are required. These may\nbe provided via a config file. It can be provided to the <tt>plow</tt> CLI tool\neither via the <tt><span class=\"pre\">-c</span></tt> option or by exporting the environment variable\n<tt>MR_PLOW_CONFIG</tt>.</p>\n<p>The supported formats are JSON, YAML, and INI. INI files must have the\nappropriate settings in the <tt><span class=\"pre\">[mr-plow]</span></tt> section. Run <tt>plow <span class=\"pre\">generate-config</span></tt>\nto have a minimal example generated for you to fill in.</p>\n<p>Mr. Plow fills in configuration it does not find by searching environment\nvariables.</p>\n<p>To see more including a full list of config options, install Mr. Plow and run\n<tt>python <span class=\"pre\">-m</span> pydoc plow.config.Config</tt>.</p>\n<div id=\"snowflake\">\n<h3>Snowflake</h3>\n<p>Those commands that require Snowflake access draw the credentials from the\nexecution environment. Mr. Plow uses the following environment variables to\ncreate a Snowflake connection. If both <tt>SNOWSQL_FOO</tt> and <tt>SNOWFLAKE_FOO</tt>\nare present as environment variables,  Mr. Plow uses <tt>SNOWFLAKE_FOO</tt>.</p>\n<ul>\n<li><tt>SNOWFLAKE_ACCOUNT</tt> or <tt>SNOWSQL_ACCOUNT</tt><ul>\n<li>visible in the URL you use to log in to Snowflake. For the author and his\npeers, this is \u201cpoppin\u201d.</li>\n</ul>\n</li>\n<li><tt>SNOWFLAKE_USER</tt> or <tt>SNOWSQL_USER</tt><ul>\n<li>Same as your login on the Snowflake website.</li>\n</ul>\n</li>\n<li><tt>SNOWFLAKE_PASSWORD</tt> or <tt>SNOWSQL_PASSWORD</tt><ul>\n<li>This is the password for that login. With the command line, this is\noptional as it can be provided as a console prompt.</li>\n</ul>\n</li>\n<li><tt>SNOWFLAKE_ROLE</tt>  or <tt>SNOWSQL_ROLE</tt></li>\n<li><tt>SNOWFLAKE_DATABASE</tt> or <tt>SNOWSQL_DATABASE</tt></li>\n<li><tt>SNOWFLAKE_SCHEMA</tt> or <tt>SNOWSQL_SCHEMA</tt><ul>\n<li>E.g. \u201cpublic\u201d. However please keep in mind that Mr. Plow creates its own\nschemas and never references tables without explicitly providing a schema.</li>\n</ul>\n</li>\n<li><tt>SNOWFLAKE_WAREHOUSE</tt> or <tt>SNOWSQL_WAREHOUSE</tt></li>\n</ul>\n<div>\n<p>Warning</p>\n<p><strong>Caution</strong>: Choose carefully as many of these commands spin up a warehouse\ninstance. You may incur charges.</p>\n</div>\n</div>\n<div id=\"aws\">\n<h3>AWS</h3>\n<p>We use <a href=\"https://boto3.readthedocs.io/en/latest/\" rel=\"nofollow\">boto3</a> to connect to AWS S3.\nIt has a rich system for specifying credentials, which can be used in its\nentirety by omitting AWS settings from your config file. This is appropriate in\nAWS Lambda, where credentials are taken care of in the background, or if you\nare an <a href=\"https://aws.amazon.com/documentation/cli/\" rel=\"nofollow\">AWS CLI</a> user and have\nexisting configs. See <a href=\"https://boto3.readthedocs.io/en/latest/guide/quickstart.html#configuration\" rel=\"nofollow\">boto3 documentation</a>\nfor more detail.</p>\n<p>To specify config directly, you may either give the <tt>aws_access_key_id</tt> and\n<tt>aws_secret_access_key</tt> settings directly, or if you have an existing <a href=\"https://docs.aws.amazon.com/cli/latest/reference/configure/index.html\" rel=\"nofollow\">AWS\nCLI configuration</a>, you\nmay simply specify <tt>aws_profile</tt>.</p>\n<p>When Snowflake reads your API data from S3, it requires you to provide AWS\ncredentials with the appropriate S3 read permissions. Mr. Plow picks these up\nas distinct config options: <tt>staging_aws_access_key_id</tt> and\n<tt>staging_aws_secret_access_key</tt>. As with the direct S3 credentials,\n<tt>staging_aws_profile</tt> may also be provided.</p>\n</div>\n<div id=\"third-party-api-s\">\n<h3>Third-party API\u2019s</h3>\n<p>We currently provide integrations for Livechat and Snapfulfil.</p>\n<p>The Livechat integration requires an API login, which is specified as config\noptions <tt>livechat_login</tt> and <tt>livechat_api_key</tt>.</p>\n<p>The Snapfulfil integration requires an API login (config <tt>snapfulfil_login</tt> and\n<tt>snapfulfil_password</tt>) as well as an explicit designation of which Snap domain\nyou want to make requests to (config <tt>snapfulfil_api_domain</tt>). Typically the\nlatter is either <tt><span class=\"pre\">https://treltestapi.snapfulfil.net</span></tt> or\n<tt><span class=\"pre\">https://trelliveapi.snapfulfil.net</span></tt>.</p>\n</div>\n</div>\n<div id=\"extending-mr-plow\">\n<h2>Extending Mr. Plow</h2>\n<p>Mr. Plow is designed to be broadly useful and easily extensible. We currently\nprovide integrations for Livechat and Snapfulfil, and more will be added, but\nyou can easily add your own.</p>\n<p>You must create your own implementation of <tt>plow.op.extract.Extractor</tt> to\ndefine how to fetch data. In a pinch you can use a RestExtractor; to allow\nfor automatic fetching of subsequent pages, you\u2019ll have to subclass it and\nimplement <tt>postprocess_response()</tt>. See documentation of\n<tt>plow.op.Extractor</tt>; also see <tt>plow.vendors.*</tt> for examples.</p>\n<p>You must furthermore create your own instances of <tt>plow.queries.Table</tt> to\nspecify how to translate data from the documents you fetch using the Extractor\ninto Snowflake DB tables. See documentation of <tt>plow.queries.Table</tt>, and see\n<tt>plow.queries.livechat</tt> and <tt>plow.queries.snapfulfil</tt> for tested examples.</p>\n<p>Finally, you must create a <tt>plow.cli.Source</tt> pointing to all of these.</p>\n<p>If you write your own adapter, we\u2019d love to include it. Please send a pull\nrequest!</p>\n<div id=\"example\">\n<h3>Example</h3>\n<p>If you have the following files in your Python project and <tt><span class=\"pre\">mr-plow</span></tt> installed\nwith <tt>pip</tt>:</p>\n<pre># mymodule/plow/extract.py\nfrom plow.op.extract import RestExtractor\nclass Extractor(RestExtractor):\n    ...\n</pre>\n<pre># mymodule/plow/tables.py\nfrom plow.queries import Table\nclass Table1(Table):\n    select = \"...\"\n    # etc...\n\nclass Table2(Table):\n    ...\n\nclass Table3(Table):\n    ...\n</pre>\n<pre># mymodule/plow/cli.py\nfrom plow.cli import Source\nfrom mymodule.plow.extract import Extractor\nfrom mymodule.plow.tables import Table1, Table2, Table3\n\nextractor = Extractor()\ntables = {t.name: t for t in (Table1(), Table2(), Table3())}\nsource = Source(extractor=extractor, tables=tables)\n</pre>\n<p>Then you can invoke the <tt>plow</tt> CLI tool, using the <tt><span class=\"pre\">--source</span></tt> option to point\nto your code:</p>\n<pre>$ plow -c mr-plow.ini extract --source mymodule.plow.cli:source [options]...\n</pre>\n</div>\n</div>\n<div id=\"development\">\n<h2>Development</h2>\n<div id=\"developer-installation\">\n<h3>Developer installation</h3>\n<p>To develop on Mr. Plow, clone this repository, set up and activate a virtualenv\n(see Installation) in the new working copy, and run <tt>pip install <span class=\"pre\">-e</span> .[dev]</tt>.\nThis installs the <tt>plow</tt> executable as well as development dependencies like\nFlake8 (the linter we use) and pytest.</p>\n</div>\n<div id=\"testing\">\n<h3>Testing</h3>\n<p>Mr. Plow is tested using <a href=\"https://docs.pytest.org/en/latest/\" rel=\"nofollow\">pytest</a>. If you\nclone the source and install using <tt>pip install <span class=\"pre\">-e</span> .[dev]</tt>, it is installed\nautomatically along with several other test dependencies. Run <tt>pytest</tt> to run\nthe unit tests; add <tt><span class=\"pre\">--cov-report=term-missing</span></tt> or <tt><span class=\"pre\">--cov-report=html</span></tt> to\nsee detailed coverage information.</p>\n<p>Testing is separated into two section, unit tests and integration tests.\nIntegration tests are disabled by default: specify <tt>pytest <span class=\"pre\">--integration</span></tt> to\nrun integration tests as well, or <tt>pytest <span class=\"pre\">--no-unit</span></tt> to disable unit tests and\nrun only integration tests.</p>\n<p>The unit tests use mocking for all external functionality, including Snowflake,\nS3, and third-party API\u2019s, and so may be run without an internet connection or\nany of the service-specific configuration specified above. However, at this\ntime, with very few exceptions these tests do not verify any specific SQL\nqueries, nor almost any vendor-specific logic.</p>\n<p>The integration tests run all Snowflake setup operations and a full run of ETL\noperations through S3 and Snowflake, so you must do some setup in order to run\nthem. We mock access to the third party API, so that we can simulate the\nprocessing of a constant dataset and verify the result with precision. To run\nintegration tests, you must supply a configuration file by exporting its path\nto <tt>PLOW_TEST_CONFIG</tt>.</p>\n<div>\n<p>Warning</p>\n<p><strong>Caution</strong>: Since these run real Snowflake operations, you may incur charges by running\nthese tests.</p>\n</div>\n</div>\n<div id=\"git-hooks\">\n<h3>Git hooks</h3>\n<p>This project adheres to several standards including a style guide and unit\ntests. To aid developers in complying, we include hooks that can be run upon\na commit. Install them as follows:</p>\n<ul>\n<li><p>Include whatever hooks you wish in your own .git/hooks/pre-commit:</p>\n<pre>$ echo '#!/usr/bin/env bash' &gt;&gt; .git/hooks/pre-commit\n$ echo 'flake8-hook.py || exit $?' &gt;&gt; .git/hooks/pre-commit\n$ echo 'unittest-hook.sh || exit $?' &gt;&gt; .git/hooks/pre-commit\n$ chmod u+x .git/hooks/pre-commit\n</pre>\n</li>\n<li><p>Add the appropriate options to your git config:</p>\n<pre>$ git config flake8.strict true\n$ git config plow.unit.strict true\n</pre>\n</li>\n</ul>\n<p>Now, the linter and the unit tests will run every time you commit and you will\nbe prompted to fix any deficiencies before committing. These checks can be\ndisabled temporarily using environment variables. To avoid a linting check:</p>\n<pre>$ git commit\nStyle errors found!\n$ FLAKE8_STRICT=false git commit\n[...] Success!\n</pre>\n<p>To skip unit tests:</p>\n<pre>$ PLOW_UNIT_STRICT=false git commit\n</pre>\n</div>\n</div>\n\n          </div>"}, "last_serial": 3417715, "releases": {"0.1.5": [{"comment_text": "", "digests": {"md5": "bc3752956e66e81d921849ca5828c41e", "sha256": "aff26362bc6ded91ec2488fa023b691a4f35410130924aad1a692ad4c94ca862"}, "downloads": -1, "filename": "mr_plow-0.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "bc3752956e66e81d921849ca5828c41e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "~=3.6", "size": 43476, "upload_time": "2017-12-14T19:40:14", "upload_time_iso_8601": "2017-12-14T19:40:14.107555Z", "url": "https://files.pythonhosted.org/packages/c6/3e/3353ada63955c7210da796e5b035a86f02a0001ec0bc15f7814f6ac71513/mr_plow-0.1.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3cc613eeb2b9e263887889b2eaa92be5", "sha256": "fd44e6a7caad9549e6ad5a583f02c00a3b2f6ada461ad54155db70a65172b106"}, "downloads": -1, "filename": "mr-plow-0.1.5.tar.gz", "has_sig": false, "md5_digest": "3cc613eeb2b9e263887889b2eaa92be5", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.6", "size": 35823, "upload_time": "2017-12-14T19:40:15", "upload_time_iso_8601": "2017-12-14T19:40:15.648138Z", "url": "https://files.pythonhosted.org/packages/39/b7/8663717a9f7b762cd0a4b01a220083fbb3d276909515cc8c36d030c2634f/mr-plow-0.1.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "bc3752956e66e81d921849ca5828c41e", "sha256": "aff26362bc6ded91ec2488fa023b691a4f35410130924aad1a692ad4c94ca862"}, "downloads": -1, "filename": "mr_plow-0.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "bc3752956e66e81d921849ca5828c41e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": "~=3.6", "size": 43476, "upload_time": "2017-12-14T19:40:14", "upload_time_iso_8601": "2017-12-14T19:40:14.107555Z", "url": "https://files.pythonhosted.org/packages/c6/3e/3353ada63955c7210da796e5b035a86f02a0001ec0bc15f7814f6ac71513/mr_plow-0.1.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3cc613eeb2b9e263887889b2eaa92be5", "sha256": "fd44e6a7caad9549e6ad5a583f02c00a3b2f6ada461ad54155db70a65172b106"}, "downloads": -1, "filename": "mr-plow-0.1.5.tar.gz", "has_sig": false, "md5_digest": "3cc613eeb2b9e263887889b2eaa92be5", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.6", "size": 35823, "upload_time": "2017-12-14T19:40:15", "upload_time_iso_8601": "2017-12-14T19:40:15.648138Z", "url": "https://files.pythonhosted.org/packages/39/b7/8663717a9f7b762cd0a4b01a220083fbb3d276909515cc8c36d030c2634f/mr-plow-0.1.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:59 2020"}