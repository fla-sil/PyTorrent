{"info": {"author": "Grammy Jiang", "author_email": "grammy.jiang@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Environment :: Plugins", "Framework :: Scrapy", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Internet :: WWW/HTTP", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "=======================\nScrapy-Proxy-Validation\n=======================\n\n.. image:: https://img.shields.io/pypi/v/scrapy-proxy-validation.svg\n   :target: https://pypi.python.org/pypi/scrapy-proxy-validation\n   :alt: PyPI Version\n\n.. image:: https://img.shields.io/travis/grammy-jiang/scrapy-proxy-validation/master.svg\n   :target: http://travis-ci.org/grammy-jiang/scrapy-proxy-validation\n   :alt: Build Status\n\n.. image:: https://img.shields.io/badge/wheel-yes-brightgreen.svg\n   :target: https://pypi.python.org/pypi/scrapy-proxy-validation\n   :alt: Wheel Status\n\n.. image:: https://img.shields.io/codecov/c/github/grammy-jiang/scrapy-proxy-validation/master.svg\n   :target: http://codecov.io/github/grammy-jiang/scrapy-proxy-validation?branch=master\n   :alt: Coverage report\n\nOverview\n========\n\nScrapy is a great framework for web crawling. This package provides a highly\ncustomized way to deal with the exceptions happening in the downloader\nmiddleware because of the proxy, and uses a signal to note relatives to treat\nthe invalidated proxies (e.g. moving to blacklist, renew the proxy pool).\n\nThere are two types of signals this package support:\n\n* traditional signal, sync\n\n* deferred signal, async\n\nPlease refer to the scrapy and twisted documents:\n\n* `Core API \u2014 Scrapy 1.4.0 documentation`_\n\n.. _`Core API \u2014 Scrapy 1.4.0 documentation`: https://doc.scrapy.org/en/latest/topics/api.html#topics-api-signals\n\n* `Signals \u2014 Scrapy 1.4.0 documentation`_\n\n.. _`Signals \u2014 Scrapy 1.4.0 documentation`: https://doc.scrapy.org/en/latest/topics/signals.html\n\n* `Deferred Reference \u2014 Twisted 17.9.0 documentation`_\n\n.. _`Deferred Reference \u2014 Twisted 17.9.0 documentation`: https://twistedmatrix.com/documents/current/core/howto/defer.html\n\nRequirements\n============\n\n* Scrapy\n\n* Tests on Python 3.5\n\n* Tests on Linux, but it is a pure python module, should work on any other\n  platforms with official python and twisted support\n\nInstallation\n============\n\nThe quick way::\n\n   pip install -U scrapy-proxy-validation\n\nOr put this middleware just beside the scrapy project.\n\nDocumentation\n=============\n\nSet this middleware in ``ITEMPIPELINES`` in ``settings.py``, for example::\n\n    from scrapy_proxy_validation.downloadermiddlewares.proxy_validation import Validation\n\n    DOWNLOADER_MIDDLEWARES.update({\n        'scrapy_proxy_validation.downloadmiddlewares.proxy_validation.ProxyValidation': 751\n    })\n\n    SIGNALS = [Validation(exception='twisted.internet.error.ConnectionRefusedError',\n                          signal='scrapy.signals.spider_closed'),\n               Validation(exception='twisted.internet.error.ConnectionLost',\n                          signal='scrapy.signals.spider_closed',\n                          signal_deferred='scrapy.signals.spider_closed',\n                          limit=5)]\n\n    RECYCLE_REQUEST = 'scrapy_proxy_validation.utils.recycle_request.recycle_request'\n\n\nSettings Reference\n==================\n\nSIGNALS\n-------\n\nA list of the class Validation with the exception it wants to deal with, the\nsync signal it sends, the async signal it sends and the limit it touches.\n\nRECYCLE_REQUEST\n---------------\n\nA function to recycle the request which have trouble with the proxy, the input\nargument is ``request``, and the output is ``request`` too.\n\n**Note: remember to set ``dont_filter`` to be ``True``, or the middleware\n``duplicate_fitler`` will remove this request.**\n\nBuilt-in Functions\n==================\n\nscrapy_proxy_validation.utils.recycle_request.recycle_request\n-------------------------------------------------------------\n\nThis is a built-in function to recycle the request which has a problem with\nthe proxy.\n\nThis function will remove the proxy keyword in meta and set ``dont_filter`` to\nbe ``True``.\n\nTo use this function, in ``settings.py``::\n\n    RECYCLE_REQUEST = 'scrapy_proxy_validation.utils.recycle_request.recycle_request'\n\nNote\n====\n\nThere could be many different problems about the proxy, thus it will take some\nto collect them all and add to ``SIGNALS``. Please be patient, this is not a\nonce-time solution middleware for this case.\n\nTODO\n====\n\nNo idea, please let me know if you have!\n\n\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/grammy-jiang/scrapy-proxy-validation", "keywords": "", "license": "BSD", "maintainer": "", "maintainer_email": "", "name": "scrapy-proxy-validation", "package_url": "https://pypi.org/project/scrapy-proxy-validation/", "platform": "", "project_url": "https://pypi.org/project/scrapy-proxy-validation/", "project_urls": {"Homepage": "https://github.com/grammy-jiang/scrapy-proxy-validation"}, "release_url": "https://pypi.org/project/scrapy-proxy-validation/0.0.4/", "requires_dist": ["txmongo", "scrapy (>=1.4.0)"], "requires_python": "", "summary": "", "version": "0.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://pypi.python.org/pypi/scrapy-proxy-validation\" rel=\"nofollow\"><img alt=\"PyPI Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c2e179fd6fab6094a1f0b139ca892a0155d663b4/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7363726170792d70726f78792d76616c69646174696f6e2e737667\"></a>\n<a href=\"http://travis-ci.org/grammy-jiang/scrapy-proxy-validation\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7e2d2bc3ba809a7f4279050e751520dcb400d386/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f6772616d6d792d6a69616e672f7363726170792d70726f78792d76616c69646174696f6e2f6d61737465722e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/scrapy-proxy-validation\" rel=\"nofollow\"><img alt=\"Wheel Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/36f0977ff012856a4c1d90260124b61d42daea02/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f776865656c2d7965732d627269676874677265656e2e737667\"></a>\n<a href=\"http://codecov.io/github/grammy-jiang/scrapy-proxy-validation?branch=master\" rel=\"nofollow\"><img alt=\"Coverage report\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/778f9d00c855428ae2985beda9fe74c663ae00f5/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f6772616d6d792d6a69616e672f7363726170792d70726f78792d76616c69646174696f6e2f6d61737465722e737667\"></a>\n<div id=\"overview\">\n<h2>Overview</h2>\n<p>Scrapy is a great framework for web crawling. This package provides a highly\ncustomized way to deal with the exceptions happening in the downloader\nmiddleware because of the proxy, and uses a signal to note relatives to treat\nthe invalidated proxies (e.g. moving to blacklist, renew the proxy pool).</p>\n<p>There are two types of signals this package support:</p>\n<ul>\n<li>traditional signal, sync</li>\n<li>deferred signal, async</li>\n</ul>\n<p>Please refer to the scrapy and twisted documents:</p>\n<ul>\n<li><a href=\"https://doc.scrapy.org/en/latest/topics/api.html#topics-api-signals\" rel=\"nofollow\">Core API \u2014 Scrapy 1.4.0 documentation</a></li>\n</ul>\n<ul>\n<li><a href=\"https://doc.scrapy.org/en/latest/topics/signals.html\" rel=\"nofollow\">Signals \u2014 Scrapy 1.4.0 documentation</a></li>\n</ul>\n<ul>\n<li><a href=\"https://twistedmatrix.com/documents/current/core/howto/defer.html\" rel=\"nofollow\">Deferred Reference \u2014 Twisted 17.9.0 documentation</a></li>\n</ul>\n</div>\n<div id=\"requirements\">\n<h2>Requirements</h2>\n<ul>\n<li>Scrapy</li>\n<li>Tests on Python 3.5</li>\n<li>Tests on Linux, but it is a pure python module, should work on any other\nplatforms with official python and twisted support</li>\n</ul>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>The quick way:</p>\n<pre>pip install -U scrapy-proxy-validation\n</pre>\n<p>Or put this middleware just beside the scrapy project.</p>\n</div>\n<div id=\"documentation\">\n<h2>Documentation</h2>\n<p>Set this middleware in <tt>ITEMPIPELINES</tt> in <tt>settings.py</tt>, for example:</p>\n<pre>from scrapy_proxy_validation.downloadermiddlewares.proxy_validation import Validation\n\nDOWNLOADER_MIDDLEWARES.update({\n    'scrapy_proxy_validation.downloadmiddlewares.proxy_validation.ProxyValidation': 751\n})\n\nSIGNALS = [Validation(exception='twisted.internet.error.ConnectionRefusedError',\n                      signal='scrapy.signals.spider_closed'),\n           Validation(exception='twisted.internet.error.ConnectionLost',\n                      signal='scrapy.signals.spider_closed',\n                      signal_deferred='scrapy.signals.spider_closed',\n                      limit=5)]\n\nRECYCLE_REQUEST = 'scrapy_proxy_validation.utils.recycle_request.recycle_request'\n</pre>\n</div>\n<div id=\"settings-reference\">\n<h2>Settings Reference</h2>\n<div id=\"signals\">\n<h3>SIGNALS</h3>\n<p>A list of the class Validation with the exception it wants to deal with, the\nsync signal it sends, the async signal it sends and the limit it touches.</p>\n</div>\n<div id=\"recycle-request\">\n<h3>RECYCLE_REQUEST</h3>\n<p>A function to recycle the request which have trouble with the proxy, the input\nargument is <tt>request</tt>, and the output is <tt>request</tt> too.</p>\n<p><strong>Note: remember to set ``dont_filter`` to be ``True``, or the middleware\n``duplicate_fitler`` will remove this request.</strong></p>\n</div>\n</div>\n<div id=\"built-in-functions\">\n<h2>Built-in Functions</h2>\n<h2 id=\"scrapy-proxy-validation-utils-recycle-request-recycle-request\"><span class=\"section-subtitle\">scrapy_proxy_validation.utils.recycle_request.recycle_request</span></h2>\n<p>This is a built-in function to recycle the request which has a problem with\nthe proxy.</p>\n<p>This function will remove the proxy keyword in meta and set <tt>dont_filter</tt> to\nbe <tt>True</tt>.</p>\n<p>To use this function, in <tt>settings.py</tt>:</p>\n<pre>RECYCLE_REQUEST = 'scrapy_proxy_validation.utils.recycle_request.recycle_request'\n</pre>\n</div>\n<div id=\"note\">\n<h2>Note</h2>\n<p>There could be many different problems about the proxy, thus it will take some\nto collect them all and add to <tt>SIGNALS</tt>. Please be patient, this is not a\nonce-time solution middleware for this case.</p>\n</div>\n<div id=\"todo\">\n<h2>TODO</h2>\n<p>No idea, please let me know if you have!</p>\n</div>\n\n          </div>"}, "last_serial": 3299174, "releases": {"0.0.3": [{"comment_text": "", "digests": {"md5": "d90b92aca73b5838515db7c3f1449a2d", "sha256": "a79f273da847f8f2f974e4c14d9e47242dd532a6625086403bba490ea4e85d5a"}, "downloads": -1, "filename": "scrapy_proxy_validation-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "d90b92aca73b5838515db7c3f1449a2d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8508, "upload_time": "2017-11-02T03:25:38", "upload_time_iso_8601": "2017-11-02T03:25:38.810021Z", "url": "https://files.pythonhosted.org/packages/de/4b/f757846d840092592f31d4db8e03dc1a641b474168cf49eb20909f46fa94/scrapy_proxy_validation-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "43094d2b17648308e81f7595f27286fe", "sha256": "50f6f9e02861c6310b5961203fd2c2a5371cfc44db72adb7f96f85621d8adaef"}, "downloads": -1, "filename": "scrapy-proxy-validation-0.0.3.tar.gz", "has_sig": false, "md5_digest": "43094d2b17648308e81f7595f27286fe", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21948, "upload_time": "2017-11-02T03:25:40", "upload_time_iso_8601": "2017-11-02T03:25:40.161842Z", "url": "https://files.pythonhosted.org/packages/b9/2c/e939f6a0200f3dae4115a13ae945b657f96b35de818f722b0626d2be48c6/scrapy-proxy-validation-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "e6e8eada3ede813ddd969688e7cf4e8e", "sha256": "7ebc6c9882d5e3ca47cab4d78a504aed8ae80915af0715310c82baf0e426a589"}, "downloads": -1, "filename": "scrapy_proxy_validation-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "e6e8eada3ede813ddd969688e7cf4e8e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8515, "upload_time": "2017-11-02T03:43:04", "upload_time_iso_8601": "2017-11-02T03:43:04.113252Z", "url": "https://files.pythonhosted.org/packages/13/0b/a436a8ea64215bf1b14ffc8c1b3097b01301e96011b78a9c33f136f2de72/scrapy_proxy_validation-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "51ec3a913dfe488420607213fa3749b7", "sha256": "3f6e3d72bf3458fd712b4693073ad4f98c4e4db567c705a9106f3c1053c4369f"}, "downloads": -1, "filename": "scrapy-proxy-validation-0.0.4.tar.gz", "has_sig": false, "md5_digest": "51ec3a913dfe488420607213fa3749b7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21947, "upload_time": "2017-11-02T03:43:04", "upload_time_iso_8601": "2017-11-02T03:43:04.848888Z", "url": "https://files.pythonhosted.org/packages/a3/9c/df6b9e97074b47405427ffc93e48746ec44e2004810a976fc695b60bcc07/scrapy-proxy-validation-0.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e6e8eada3ede813ddd969688e7cf4e8e", "sha256": "7ebc6c9882d5e3ca47cab4d78a504aed8ae80915af0715310c82baf0e426a589"}, "downloads": -1, "filename": "scrapy_proxy_validation-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "e6e8eada3ede813ddd969688e7cf4e8e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8515, "upload_time": "2017-11-02T03:43:04", "upload_time_iso_8601": "2017-11-02T03:43:04.113252Z", "url": "https://files.pythonhosted.org/packages/13/0b/a436a8ea64215bf1b14ffc8c1b3097b01301e96011b78a9c33f136f2de72/scrapy_proxy_validation-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "51ec3a913dfe488420607213fa3749b7", "sha256": "3f6e3d72bf3458fd712b4693073ad4f98c4e4db567c705a9106f3c1053c4369f"}, "downloads": -1, "filename": "scrapy-proxy-validation-0.0.4.tar.gz", "has_sig": false, "md5_digest": "51ec3a913dfe488420607213fa3749b7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21947, "upload_time": "2017-11-02T03:43:04", "upload_time_iso_8601": "2017-11-02T03:43:04.848888Z", "url": "https://files.pythonhosted.org/packages/a3/9c/df6b9e97074b47405427ffc93e48746ec44e2004810a976fc695b60bcc07/scrapy-proxy-validation-0.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:43 2020"}