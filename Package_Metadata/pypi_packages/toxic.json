{"info": {"author": "KakaoBrain", "author_email": "kwk236@nyu.edu", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Science/Research", "Operating System :: MacOS :: MacOS X", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3.4"], "description": "\n.. raw:: html\n\n   <div align=\"center\">\n\n.. raw:: html\n\n   </div>\n\nIntro\n-----\n\n**Toxic** is an open source software library for machine learning\nsecurity. It contains tools for adversarial example generation and\nprovides a framework for building new types of attack methods.\n\nCurrently in the dev stage.\n\nAttacks\n-------\n\nAvailable attack algorithms implemented in Toxic:\n\n-  Fast Gradient Methods (FGM/FGSM)\n   ```Tutorial`` </tutorial/source/fgsm.ipynb>`__\n-  Basic Iterative\n   ```Tutorial`` </tutorial/source/basic_iterative.ipynb>`__\n-  Momentum Iterative\n   ```Tutorial`` </tutorial/source/momentum_iterative.ipynb>`__\n-  DeepFool\n-  Universal Adversarial Perturbation (UAP)\n-  Jacobian-based Saliency Map Approach (JSMA)\n-  One Pixel Attack\n-  LBFGS\n-  Carlini Wagner L2\n-  Carlini Wagner L-inf\n-  Feature Adversaries\n-  Boundary Attack\n-  Elastic Net\n-  Natural Adversarial Examples (NAE)\n\nThe Team\n~~~~~~~~\n\nToxic is a community driven project. The project was initiated by\nmachine learning security team @ `KakaoBrain <kakaobrain.com>`__.\n\n.. |license| image:: https://img.shields.io/github/license/mashape/apistatus.svg\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/KakaoBrain", "keywords": "machine learning security adversarial attackdefense deep learning pytorch", "license": "MIT License", "maintainer": "", "maintainer_email": "", "name": "toxic", "package_url": "https://pypi.org/project/toxic/", "platform": "", "project_url": "https://pypi.org/project/toxic/", "project_urls": {"Homepage": "https://github.com/KakaoBrain"}, "release_url": "https://pypi.org/project/toxic/0.1.0/", "requires_dist": ["six (>=1.10.0)", "torchvision; extra == 'dev'", "flake8; extra == 'dev'", "yapf; extra == 'dev'", "isort; extra == 'dev'", "pytest; extra == 'dev'", "pytest-xdist; extra == 'dev'", "nbval; extra == 'dev'", "nbstripout; extra == 'dev'", "pypandoc; extra == 'dev'", "sphinx; extra == 'dev'", "sphinx-rtd-theme; extra == 'dev'", "jupyter (>=1.0.0); extra == 'notebooks'", "prettytable; extra == 'profile'", "pytest; extra == 'test'", "pytest-cov; extra == 'test'", "nbval; extra == 'test'", "visdom; extra == 'test'", "torchvision; extra == 'test'", "matplotlib (>=1.3); extra == 'visualization'", "visdom (>=0.1.4); extra == 'visualization'", "pillow; extra == 'visualization'", "numpy (>=1.7)", "scipy (>=0.19.0)", "torch"], "requires_python": "", "summary": "TODO", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <br>.. raw:: html<br><br>   &lt;div align=\"center\"&gt;<br><br>.. raw:: html<br><br>   &lt;/div&gt;<br><br>Intro<br>-----<br><br>**Toxic** is an open source software library for machine learning<br>security. It contains tools for adversarial example generation and<br>provides a framework for building new types of attack methods.<br><br>Currently in the dev stage.<br><br>Attacks<br>-------<br><br>Available attack algorithms implemented in Toxic:<br><br>-  Fast Gradient Methods (FGM/FGSM)<br>   ```Tutorial`` &lt;/tutorial/source/fgsm.ipynb&gt;`__<br>-  Basic Iterative<br>   ```Tutorial`` &lt;/tutorial/source/basic_iterative.ipynb&gt;`__<br>-  Momentum Iterative<br>   ```Tutorial`` &lt;/tutorial/source/momentum_iterative.ipynb&gt;`__<br>-  DeepFool<br>-  Universal Adversarial Perturbation (UAP)<br>-  Jacobian-based Saliency Map Approach (JSMA)<br>-  One Pixel Attack<br>-  LBFGS<br>-  Carlini Wagner L2<br>-  Carlini Wagner L-inf<br>-  Feature Adversaries<br>-  Boundary Attack<br>-  Elastic Net<br>-  Natural Adversarial Examples (NAE)<br><br>The Team<br>~~~~~~~~<br><br>Toxic is a community driven project. The project was initiated by<br>machine learning security team @ `KakaoBrain &lt;kakaobrain.com&gt;`__.<br><br>.. |license| image:: https://img.shields.io/github/license/mashape/apistatus.svg<br><br><br>\n          </div>"}, "last_serial": 3941890, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "36a1461b8c49a1d165dc245b59114042", "sha256": "d22e460d9ac2af1c4ba4bf3e0936de35f9e6811d111bf0d52d65c4e32838a5f9"}, "downloads": -1, "filename": "toxic-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "36a1461b8c49a1d165dc245b59114042", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 50335, "upload_time": "2018-06-08T05:17:01", "upload_time_iso_8601": "2018-06-08T05:17:01.042372Z", "url": "https://files.pythonhosted.org/packages/26/de/377b8fde6125b0c33bd0086961416584e352a893fe43dee8f8b551946582/toxic-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "02217da4117c5cfd821951e368562b99", "sha256": "c7c7470f5e57626614f085af8ef55d5155640f3bbeda2fea8b917d213f672716"}, "downloads": -1, "filename": "toxic-0.1.0.tar.gz", "has_sig": false, "md5_digest": "02217da4117c5cfd821951e368562b99", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17275, "upload_time": "2018-06-08T05:17:02", "upload_time_iso_8601": "2018-06-08T05:17:02.749418Z", "url": "https://files.pythonhosted.org/packages/94/3f/8f65c19e8496df804984e434b142333a9d76ac648d2d55ffb3d59c045ea3/toxic-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "36a1461b8c49a1d165dc245b59114042", "sha256": "d22e460d9ac2af1c4ba4bf3e0936de35f9e6811d111bf0d52d65c4e32838a5f9"}, "downloads": -1, "filename": "toxic-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "36a1461b8c49a1d165dc245b59114042", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 50335, "upload_time": "2018-06-08T05:17:01", "upload_time_iso_8601": "2018-06-08T05:17:01.042372Z", "url": "https://files.pythonhosted.org/packages/26/de/377b8fde6125b0c33bd0086961416584e352a893fe43dee8f8b551946582/toxic-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "02217da4117c5cfd821951e368562b99", "sha256": "c7c7470f5e57626614f085af8ef55d5155640f3bbeda2fea8b917d213f672716"}, "downloads": -1, "filename": "toxic-0.1.0.tar.gz", "has_sig": false, "md5_digest": "02217da4117c5cfd821951e368562b99", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17275, "upload_time": "2018-06-08T05:17:02", "upload_time_iso_8601": "2018-06-08T05:17:02.749418Z", "url": "https://files.pythonhosted.org/packages/94/3f/8f65c19e8496df804984e434b142333a9d76ac648d2d55ffb3d59c045ea3/toxic-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:49:20 2020"}