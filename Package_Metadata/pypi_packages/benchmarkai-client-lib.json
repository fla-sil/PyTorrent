{"info": {"author": "Edison Gustavo Muenz", "author_email": "edisongustavo@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: Apache Software License", "Operating System :: Unix", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: Implementation :: CPython"], "description": "A small library for publishing metrics when running a workflow using the Benchmark AI infrastructure.\n\n# Installation\n\n```\npip install benchmarkai-client-lib\n```\n\n# API\n\nThe library provides a single method:\n\n```\ndef emit(metrics: Dict[str, float])\n```\n\nPS: This library also works with Python 2, which has no type system.\n\n## Example usage\n\n```python\nfrom benchmarkai import emit\n\ndef my_fancy_benchmark():\n    import time\n    begin = time.time()\n    for epoch in range(100):\n        execute_training()\n    elapsed_time = time.time() - begin\n    emit({\"alexnet.batchsize16.latency\": elapsed_time})\n```\n\n## Performance\n\nAs the author you don't need to worry about `emit()` being slow since it does 2 things:\n\n- serializes `metrics` into JSON\n- writes this string into a file (see the internal details)\n\nSo it will be as slow as these operations can be, which are quite fast.\n\n## Accepted values\n\nThe dictionary provided by the `metrics` parameter is in the form:\n\n`\"metric-name\" => value`\n\n- `metric-name` is a `string`\n- `value` is a `float`\n\n# Running tests\n\nTo set up the environment to run the tests, the file `test-environment.yml` is provided. This conda environment file\nhas all dependencies required to run the tests.\n\n```\n$ conda env create -f test-environment.yml\n$ conda activate benchmark-ai-client-library-tests\n$ tox\n```\n\n# Internal details\n\n## Modes\n\nThe library can be used in these modes as of now:\n\n1. Stdout printing (this is the **default** mode)\n1. Benchmark AI infrastructure\n\nTo control which behavior is desired, the environment variable `BENCHMARK_AI` can be set to one of the values:\n\n- `stdout`: The **default** mode. In this configuration, the `emit()` method is equivalent to:\n    ```python\n    def emit(metrics):\n        import json\n        print(json.dumps(metrics))\n    ```\n\n- `fifo`: When running inside Benchmark AI infrastructure this is the mode used.\n\n### Fifo mode\n\nIn this mode, the `emit()` method writes JSON string to a file that was created as a \n[named pipe](https://docs.python.org/3.7/library/os.html#os.mkfifo). It does **NOT** write to `stdout`.\n\nKeep in mind that the client library does **NOT** create the *named pipe* file, this is delegated to the\n\"daemon process\", which will be running alongside the benchmark process in the Benchmark AI infrastructure.\n\nThe daemon process is responsible to:\n\n- create the *named pipe*\n- read the contents from this pipe\n- post the contents of this pipe to the metrics servers (eg.: Elastic Search, Cloudwatch, etc.)\n\nThe first time that `emit()` is called, it checks for the existence of the file and uses that. If it doesn't exist, it\nwaits for some time before giving up and throwing an exception `FifoNotCreatedInTimeError`.\n\n#### Fifo mode configurations\n\nThese environment variables control the behavior of the client library:\n\n- `BENCHMARK_AI_FIFO_FILEPATH`: The path to the file that will be created as a *named pipe*.\n                                Default value: `/tmp/benchmark-ai-fifo`\n- `BENCHMARK_AI_FIFO_MAX_WAIT_TIME`: The maximum amount of time that the client library will wait for the\n                                     *named pipe file* to be created.\n                                     Type is **float**, unit is **seconds**, default value: `10`.\n- `BENCHMARK_AI_FIFO_WAIT_TIME_STEP`: The amount of time between each check that the client library will wait before\n                                      checking for the existence of the *named pipe file* again.\n                                      Type is **float**, unit is **seconds**, default value: `0.5`.\n\n**IMPORTANT**: These environment variables are for internal usage of the Benchmark AI infrastructure. They are listed\n               here for documentation purposes only.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/MXNetEdge/benchmark-ai", "keywords": "", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "benchmarkai-client-lib", "package_url": "https://pypi.org/project/benchmarkai-client-lib/", "platform": "", "project_url": "https://pypi.org/project/benchmarkai-client-lib/", "project_urls": {"Homepage": "https://github.com/MXNetEdge/benchmark-ai"}, "release_url": "https://pypi.org/project/benchmarkai-client-lib/0.1/", "requires_dist": null, "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "summary": "A client library for posting metrics that uses the Benchmark AI framework.", "version": "0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>A small library for publishing metrics when running a workflow using the Benchmark AI infrastructure.</p>\n<h1>Installation</h1>\n<pre><code>pip install benchmarkai-client-lib\n</code></pre>\n<h1>API</h1>\n<p>The library provides a single method:</p>\n<pre><code>def emit(metrics: Dict[str, float])\n</code></pre>\n<p>PS: This library also works with Python 2, which has no type system.</p>\n<h2>Example usage</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">benchmarkai</span> <span class=\"kn\">import</span> <span class=\"n\">emit</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">my_fancy_benchmark</span><span class=\"p\">():</span>\n    <span class=\"kn\">import</span> <span class=\"nn\">time</span>\n    <span class=\"n\">begin</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n    <span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n        <span class=\"n\">execute_training</span><span class=\"p\">()</span>\n    <span class=\"n\">elapsed_time</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">begin</span>\n    <span class=\"n\">emit</span><span class=\"p\">({</span><span class=\"s2\">\"alexnet.batchsize16.latency\"</span><span class=\"p\">:</span> <span class=\"n\">elapsed_time</span><span class=\"p\">})</span>\n</pre>\n<h2>Performance</h2>\n<p>As the author you don't need to worry about <code>emit()</code> being slow since it does 2 things:</p>\n<ul>\n<li>serializes <code>metrics</code> into JSON</li>\n<li>writes this string into a file (see the internal details)</li>\n</ul>\n<p>So it will be as slow as these operations can be, which are quite fast.</p>\n<h2>Accepted values</h2>\n<p>The dictionary provided by the <code>metrics</code> parameter is in the form:</p>\n<p><code>\"metric-name\" =&gt; value</code></p>\n<ul>\n<li><code>metric-name</code> is a <code>string</code></li>\n<li><code>value</code> is a <code>float</code></li>\n</ul>\n<h1>Running tests</h1>\n<p>To set up the environment to run the tests, the file <code>test-environment.yml</code> is provided. This conda environment file\nhas all dependencies required to run the tests.</p>\n<pre><code>$ conda env create -f test-environment.yml\n$ conda activate benchmark-ai-client-library-tests\n$ tox\n</code></pre>\n<h1>Internal details</h1>\n<h2>Modes</h2>\n<p>The library can be used in these modes as of now:</p>\n<ol>\n<li>Stdout printing (this is the <strong>default</strong> mode)</li>\n<li>Benchmark AI infrastructure</li>\n</ol>\n<p>To control which behavior is desired, the environment variable <code>BENCHMARK_AI</code> can be set to one of the values:</p>\n<ul>\n<li>\n<p><code>stdout</code>: The <strong>default</strong> mode. In this configuration, the <code>emit()</code> method is equivalent to:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">emit</span><span class=\"p\">(</span><span class=\"n\">metrics</span><span class=\"p\">):</span>\n    <span class=\"kn\">import</span> <span class=\"nn\">json</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">(</span><span class=\"n\">metrics</span><span class=\"p\">))</span>\n</pre>\n</li>\n<li>\n<p><code>fifo</code>: When running inside Benchmark AI infrastructure this is the mode used.</p>\n</li>\n</ul>\n<h3>Fifo mode</h3>\n<p>In this mode, the <code>emit()</code> method writes JSON string to a file that was created as a\n<a href=\"https://docs.python.org/3.7/library/os.html#os.mkfifo\" rel=\"nofollow\">named pipe</a>. It does <strong>NOT</strong> write to <code>stdout</code>.</p>\n<p>Keep in mind that the client library does <strong>NOT</strong> create the <em>named pipe</em> file, this is delegated to the\n\"daemon process\", which will be running alongside the benchmark process in the Benchmark AI infrastructure.</p>\n<p>The daemon process is responsible to:</p>\n<ul>\n<li>create the <em>named pipe</em></li>\n<li>read the contents from this pipe</li>\n<li>post the contents of this pipe to the metrics servers (eg.: Elastic Search, Cloudwatch, etc.)</li>\n</ul>\n<p>The first time that <code>emit()</code> is called, it checks for the existence of the file and uses that. If it doesn't exist, it\nwaits for some time before giving up and throwing an exception <code>FifoNotCreatedInTimeError</code>.</p>\n<h4>Fifo mode configurations</h4>\n<p>These environment variables control the behavior of the client library:</p>\n<ul>\n<li><code>BENCHMARK_AI_FIFO_FILEPATH</code>: The path to the file that will be created as a <em>named pipe</em>.\nDefault value: <code>/tmp/benchmark-ai-fifo</code></li>\n<li><code>BENCHMARK_AI_FIFO_MAX_WAIT_TIME</code>: The maximum amount of time that the client library will wait for the\n<em>named pipe file</em> to be created.\nType is <strong>float</strong>, unit is <strong>seconds</strong>, default value: <code>10</code>.</li>\n<li><code>BENCHMARK_AI_FIFO_WAIT_TIME_STEP</code>: The amount of time between each check that the client library will wait before\nchecking for the existence of the <em>named pipe file</em> again.\nType is <strong>float</strong>, unit is <strong>seconds</strong>, default value: <code>0.5</code>.</li>\n</ul>\n<p><strong>IMPORTANT</strong>: These environment variables are for internal usage of the Benchmark AI infrastructure. They are listed\nhere for documentation purposes only.</p>\n\n          </div>"}, "last_serial": 4873932, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "1efa1e34a87adf0aa69390e09df275b9", "sha256": "7d5118375a2f81b7d8bb7bbe1e3ad7319f17e79cdcb1bdb30c41d2cd9d28c566"}, "downloads": -1, "filename": "benchmarkai-client-lib-0.1.tar.gz", "has_sig": false, "md5_digest": "1efa1e34a87adf0aa69390e09df275b9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 4304, "upload_time": "2019-02-27T12:13:09", "upload_time_iso_8601": "2019-02-27T12:13:09.988146Z", "url": "https://files.pythonhosted.org/packages/93/ff/3acf083f8c8c25fe8e8043e98fc2d3fb61d1b55f23a470ac210954c4e82f/benchmarkai-client-lib-0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1efa1e34a87adf0aa69390e09df275b9", "sha256": "7d5118375a2f81b7d8bb7bbe1e3ad7319f17e79cdcb1bdb30c41d2cd9d28c566"}, "downloads": -1, "filename": "benchmarkai-client-lib-0.1.tar.gz", "has_sig": false, "md5_digest": "1efa1e34a87adf0aa69390e09df275b9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*", "size": 4304, "upload_time": "2019-02-27T12:13:09", "upload_time_iso_8601": "2019-02-27T12:13:09.988146Z", "url": "https://files.pythonhosted.org/packages/93/ff/3acf083f8c8c25fe8e8043e98fc2d3fb61d1b55f23a470ac210954c4e82f/benchmarkai-client-lib-0.1.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:37:49 2020"}