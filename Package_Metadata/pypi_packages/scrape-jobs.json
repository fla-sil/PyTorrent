{"info": {"author": "Hrissimir", "author_email": "hrisimir.dakov@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Programming Language :: Python"], "description": "===========\nscrape-jobs\n===========\n\n\n    Simple CLI jobs scraper\n\n\n\nDISCLAIMER\n==========\n\n\n    * Made as POC\n\n    * USE AT YOUR OWN RISK\n\n\n\nWorkflow\n--------\n\n\n    1. Scrape jobs matching certain criteria\n\n        * linkedin.com - [ keywords , location ]\n\n        * seek.com.au - [ what , where ]\n\n\n    2. Store the scraped data\n\n        * upload to Google Sheets.\n\n        * or just store it locally to CSV file.\n\n\n\nInstallation\n============\n\n\nPrerequisites:\n\n    * Google Chrome installed\n\n    * chromedriver binary executable in PATH\n\n    * Python 3.6+\n\n    * Prepared Google Spreadsheet (detailed instructions bellow)\n\n\nInstall command:\n\n    `pip install --force -U scrape-jobs`\n\n\nInstalls the following CLI bindings:\n\n\n    * `scrape-jobs-init-config`\n\n    * `scrape-jobs`\n\n\n\nBasic Instructions\n==================\n\n\n1. Open terminal/CMD and 'cd' into working dir.\n\n2. Run `scrape-jobs-init-config` to populate sample config file in the current work dir\n\n    usage: scrape-jobs-init-config [-h] [--version] [-v] [-vv] [-f FILE]\n\n    initialize sample 'scrape-jobs' config file\n\n    optional arguments:\n      -h, --help           show this help message and exit\n      --version            show program's version number and exit\n      -v, --verbose        set loglevel to INFO\n      -vv, --very-verbose  set loglevel to DEBUG\n      -f FILE              defaults to '/current/work/dir/scrape-jobs.ini'\n\n\n3. Edit the config file as per your needs\n\n4. Run `scrape-jobs` to trigger execution\n\n    usage: scrape-jobs [-h] [--version] [-v] [-vv] [-c CONFIG_FILE]\n                       {linkedin.com,seek.com.au}\n\n    Scrape jobs and store results.\n\n    positional arguments:\n      {linkedin.com,seek.com.au}\n                            site to scrape\n\n    optional arguments:\n      -h, --help            show this help message and exit\n      --version             show program's version number and exit\n      -v, --verbose         set loglevel to INFO\n      -vv, --very-verbose   set loglevel to DEBUG\n      -c CONFIG_FILE        defaults to '/current/work/dir/scrape-jobs.ini'\n\n\nMore Detailed Instructions:\n---------------------------\n\n- Prepare the spreadsheet and the spreadsheet's auth.json (Spreadsheet instructions at the bottom)\n\n    - For `seek.com.au` the Worksheet's columns are:\n\n        [\"scraped_time\", \"posted_time\", \"location\", \"area\", \"classification\", \"sub_classification\", \"title\", \"salary\", \"company\", \"url\"]\n\n    - For `linkedin.com` the Worksheet's columns are:\n\n         [\"scraped_time\", \"posted_time\", \"location\", \"title\", \"company\", \"url\"]\n\n- Init empty config file by calling `scrape-jobs-init-config`\n\n- Edit the newly created `CWD\\\\scrape-jobs.ini` with params of your choice\n\n    - set the path to the AUTH.JSON\n\n    - set the spreadsheet name\n\n    - set the worksheet name  (it will be automatically created if it doesn't exist)\n\n    - set the search params\n\n- Trigger execution:\n\n    - run `scrape-jobs linkedin.com` or `scrape-jobs seek.com.au`\n\n    - you will see output in the console, but a scrape-jobs.log will be created too\n\n    - to have more detailed output add `-vv` execution param\n\n- After the scrape is complete you should see the newly discovered jobs in your spreadsheet\n\n- Alternatively you can init a config at a known place and just pass it's path:\n\n    `scrape-jobs-init-config -f /custom/path/to/config.ini`\n\n    `scrape-jobs -c /custom/path/to/config.ini seek.com.au`\n\n\n\nNote\n====\n\n\nYou need to prepare AUTH.JSON file in advance that is to be used for authentication with GoogleSheets\n\nThe term 'Spreadsheet' refers to a single document that is shown in the GoogleSpreadsheets landing page\n\nA single 'Spreadsheet' can contain one or more 'Worksheets'\n\nUsually a newly created 'Spreadsheet' contains a single 'Worksheet' named 'Sheet1'\n\nIf you don't provide a valid path to AUTH.JSON the collected data will be saved as .csv in the current work dir\n\n\n\nInstructions for preparing Google Spreadsheet AUTH.JSON:\n--------------------------------------------------------\n\n\n    1. Go to https://console.developers.google.com/\n\n    2. Login with the google account that is to be owner of the 'Spreadsheet'.\n\n    3. At the top-left corner, there is a drop-down right next to the \"Google APIs\" text\n\n    4. Click the drop-down and a modal-dialog will appear, then click \"NEW PROJECT\" at it's top-right\n\n    5. Name the project relevant to how the sheet is to be used, don't select 'Location*', just press 'CREATE'\n\n    6. Open the newly created project from the same drop-down as in step 3.\n\n    7. There should be 'APIs' area with a \"-> Go to APIs overview\" at it's bottom - click it\n\n    8. A new page will load having '+ ENABLE APIS AND SERVICES' button at the top side's middle - click it\n\n    9. A new page will load having a 'Search for APIs & Services' input - use it to find and open 'Google Drive API'\n\n    10. In the 'Google Drive API' page click \"ENABLE\" - you'll be redirected back to the project's page\n\n    11. There will be a new 'CREATE CREDENTIALS' button at the top - click it\n\n    12. Setup the new credentials as follows:\n\n        - Which API are you using? -> 'Google Drive API'\n\n        - Where will you be calling the API from? -> 'Web server (e.g. node.js, Tomcat)\n\n        - What data will you be accessing? -> 'Application data'\n\n        - Are you planning to use this API with App Engine or Compute Engine? -> No, I'm not using them.\n\n    13. Click the blue button 'What credentials do I need', will take you to 'Add credentials to you project' page\n\n    14. Setup the credentials as follows:\n\n        - Service account name:  {whatever name you type is OK, as long the input accepts it}\n\n        - Role: Project->Editor\n\n        - Key type: JSON\n\n    15. Press the blue 'Continue' button, and a download of the AUTH.JSON file will begin (store it safe)\n\n    16. Close the modal and go back to the project 'Dashboard' using the left-side navigation panel\n\n    17. Repeat step 8.\n\n    18. Search for 'Google Sheets API', then open the result and click the blue 'ENABLE' button\n\n    19. Open the downloaded auth.json file and copy the value of the 'client_email'\n\n    20. Using the same google account as in step 2. , go to the normal google sheets and create & open the 'Spreadsheet'\n\n        - do a final renaming to the spreadsheet now to avoid issues in future\n\n    21. 'Share' the document with the email copied in step 19., giving it 'Edit' permissions\n\n        - you might want to un-tick 'Notify people' before clicking 'Send' as it's a service email you're sharing with\n\n        - 'Send' will change to 'OK' upon un-tick, but we're cool with that - just click it.\n\n    You are now ready to use this class for retrieving 'Spreadsheet' handle in the code!\n\n\n", "description_content_type": "text/x-rst; charset=UTF-8", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Hrissimir/scrape_jobs", "keywords": "", "license": "mit", "maintainer": "", "maintainer_email": "", "name": "scrape-jobs", "package_url": "https://pypi.org/project/scrape-jobs/", "platform": "any", "project_url": "https://pypi.org/project/scrape-jobs/", "project_urls": {"Code": "https://github.com/Hrissimir/scrape_jobs", "Homepage": "https://github.com/Hrissimir/scrape_jobs"}, "release_url": "https://pypi.org/project/scrape-jobs/3.0.2/", "requires_dist": ["hed-utils (==4.1.2)", "coverage (==5.0) ; extra == 'testing'", "flake8 (==3.7.9) ; extra == 'testing'", "pylint (==2.4.4) ; extra == 'testing'", "pytest (==5.3.2) ; extra == 'testing'", "pytest-cov (==2.8.1) ; extra == 'testing'", "pyscaffold[all] (==3.2.3) ; extra == 'testing'", "twine (==3.1.1) ; extra == 'testing'"], "requires_python": ">=3.6", "summary": "CLI jobs scraper targeting multiple sites", "version": "3.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <blockquote>\nSimple CLI jobs scraper</blockquote>\n<div id=\"disclaimer\">\n<h2>DISCLAIMER</h2>\n<blockquote>\n<ul>\n<li>Made as POC</li>\n<li>USE AT YOUR OWN RISK</li>\n</ul>\n</blockquote>\n<div id=\"workflow\">\n<h3>Workflow</h3>\n<blockquote>\n<ol>\n<li><p>Scrape jobs matching certain criteria</p>\n<blockquote>\n<ul>\n<li>linkedin.com - [ keywords , location ]</li>\n<li>seek.com.au - [ what , where ]</li>\n</ul>\n</blockquote>\n</li>\n<li><p>Store the scraped data</p>\n<blockquote>\n<ul>\n<li>upload to Google Sheets.</li>\n<li>or just store it locally to CSV file.</li>\n</ul>\n</blockquote>\n</li>\n</ol>\n</blockquote>\n</div>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>Prerequisites:</p>\n<blockquote>\n<ul>\n<li>Google Chrome installed</li>\n<li>chromedriver binary executable in PATH</li>\n<li>Python 3.6+</li>\n<li>Prepared Google Spreadsheet (detailed instructions bellow)</li>\n</ul>\n</blockquote>\n<p>Install command:</p>\n<blockquote>\n<cite>pip install \u2013force -U scrape-jobs</cite></blockquote>\n<p>Installs the following CLI bindings:</p>\n<blockquote>\n<ul>\n<li><cite>scrape-jobs-init-config</cite></li>\n<li><cite>scrape-jobs</cite></li>\n</ul>\n</blockquote>\n</div>\n<div id=\"basic-instructions\">\n<h2>Basic Instructions</h2>\n<ol>\n<li><p>Open terminal/CMD and \u2018cd\u2019 into working dir.</p>\n</li>\n<li><p>Run <cite>scrape-jobs-init-config</cite> to populate sample config file in the current work dir</p>\n<blockquote>\n<p>usage: scrape-jobs-init-config [-h] [\u2013version] [-v] [-vv] [-f FILE]</p>\n<p>initialize sample \u2018scrape-jobs\u2019 config file</p>\n<dl>\n<dt>optional arguments:</dt>\n<dd><table>\n<col>\n<col>\n<tbody>\n<tr><td>\n<kbd><span class=\"option\">-h</span>, <span class=\"option\">--help</span></kbd></td>\n<td><p>show this help message and exit</p>\n</td></tr>\n<tr><td>\n<kbd><span class=\"option\">--version</span></kbd></td>\n<td><p>show program\u2019s version number and exit</p>\n</td></tr>\n<tr><td>\n<kbd><span class=\"option\">-v</span>, <span class=\"option\">--verbose</span></kbd></td>\n<td><p>set loglevel to INFO</p>\n</td></tr>\n<tr><td>\n<kbd><span class=\"option\">-v<var>v</var></span>, <span class=\"option\">--very-verbose</span></kbd></td>\n</tr>\n<tr><td>\u00a0</td><td><p>set loglevel to DEBUG</p>\n</td></tr>\n<tr><td>\n<kbd><span class=\"option\">-f <var>FILE</var></span></kbd></td>\n<td><p>defaults to \u2018/current/work/dir/scrape-jobs.ini\u2019</p>\n</td></tr>\n</tbody>\n</table>\n</dd>\n</dl>\n</blockquote>\n</li>\n<li><p>Edit the config file as per your needs</p>\n</li>\n<li><p>Run <cite>scrape-jobs</cite> to trigger execution</p>\n<blockquote>\n<dl>\n<dt>usage: scrape-jobs [-h] [\u2013version] [-v] [-vv] [-c CONFIG_FILE]</dt>\n<dd><p>{linkedin.com,seek.com.au}</p>\n</dd>\n</dl>\n<p>Scrape jobs and store results.</p>\n<dl>\n<dt>positional arguments:</dt>\n<dd><dl>\n<dt>{linkedin.com,seek.com.au}</dt>\n<dd><p>site to scrape</p>\n</dd>\n</dl>\n</dd>\n<dt>optional arguments:</dt>\n<dd><table>\n<col>\n<col>\n<tbody>\n<tr><td>\n<kbd><span class=\"option\">-h</span>, <span class=\"option\">--help</span></kbd></td>\n<td><p>show this help message and exit</p>\n</td></tr>\n<tr><td>\n<kbd><span class=\"option\">--version</span></kbd></td>\n<td><p>show program\u2019s version number and exit</p>\n</td></tr>\n<tr><td>\n<kbd><span class=\"option\">-v</span>, <span class=\"option\">--verbose</span></kbd></td>\n<td><p>set loglevel to INFO</p>\n</td></tr>\n<tr><td>\n<kbd><span class=\"option\">-v<var>v</var></span>, <span class=\"option\">--very-verbose</span></kbd></td>\n</tr>\n<tr><td>\u00a0</td><td><p>set loglevel to DEBUG</p>\n</td></tr>\n<tr><td>\n<kbd><span class=\"option\">-c <var>CONFIG_FILE</var></span></kbd></td>\n<td><p>defaults to \u2018/current/work/dir/scrape-jobs.ini\u2019</p>\n</td></tr>\n</tbody>\n</table>\n</dd>\n</dl>\n</blockquote>\n</li>\n</ol>\n<div id=\"more-detailed-instructions\">\n<h3>More Detailed Instructions:</h3>\n<ul>\n<li><p>Prepare the spreadsheet and the spreadsheet\u2019s auth.json (Spreadsheet instructions at the bottom)</p>\n<blockquote>\n<ul>\n<li><p>For <cite>seek.com.au</cite> the Worksheet\u2019s columns are:</p>\n<blockquote>\n<p>[\u201cscraped_time\u201d, \u201cposted_time\u201d, \u201clocation\u201d, \u201carea\u201d, \u201cclassification\u201d, \u201csub_classification\u201d, \u201ctitle\u201d, \u201csalary\u201d, \u201ccompany\u201d, \u201curl\u201d]</p>\n</blockquote>\n</li>\n<li><p>For <cite>linkedin.com</cite> the Worksheet\u2019s columns are:</p>\n<blockquote>\n<p>[\u201cscraped_time\u201d, \u201cposted_time\u201d, \u201clocation\u201d, \u201ctitle\u201d, \u201ccompany\u201d, \u201curl\u201d]</p>\n</blockquote>\n</li>\n</ul>\n</blockquote>\n</li>\n<li><p>Init empty config file by calling <cite>scrape-jobs-init-config</cite></p>\n</li>\n<li><p>Edit the newly created <cite>CWD\\scrape-jobs.ini</cite> with params of your choice</p>\n<blockquote>\n<ul>\n<li>set the path to the AUTH.JSON</li>\n<li>set the spreadsheet name</li>\n<li>set the worksheet name  (it will be automatically created if it doesn\u2019t exist)</li>\n<li>set the search params</li>\n</ul>\n</blockquote>\n</li>\n<li><p>Trigger execution:</p>\n<blockquote>\n<ul>\n<li>run <cite>scrape-jobs linkedin.com</cite> or <cite>scrape-jobs seek.com.au</cite></li>\n<li>you will see output in the console, but a scrape-jobs.log will be created too</li>\n<li>to have more detailed output add <cite>-vv</cite> execution param</li>\n</ul>\n</blockquote>\n</li>\n<li><p>After the scrape is complete you should see the newly discovered jobs in your spreadsheet</p>\n</li>\n<li><p>Alternatively you can init a config at a known place and just pass it\u2019s path:</p>\n<blockquote>\n<p><cite>scrape-jobs-init-config -f /custom/path/to/config.ini</cite></p>\n<p><cite>scrape-jobs -c /custom/path/to/config.ini seek.com.au</cite></p>\n</blockquote>\n</li>\n</ul>\n</div>\n</div>\n<div id=\"note\">\n<h2>Note</h2>\n<p>You need to prepare AUTH.JSON file in advance that is to be used for authentication with GoogleSheets</p>\n<p>The term \u2018Spreadsheet\u2019 refers to a single document that is shown in the GoogleSpreadsheets landing page</p>\n<p>A single \u2018Spreadsheet\u2019 can contain one or more \u2018Worksheets\u2019</p>\n<p>Usually a newly created \u2018Spreadsheet\u2019 contains a single \u2018Worksheet\u2019 named \u2018Sheet1\u2019</p>\n<p>If you don\u2019t provide a valid path to AUTH.JSON the collected data will be saved as .csv in the current work dir</p>\n<div id=\"instructions-for-preparing-google-spreadsheet-auth-json\">\n<h3>Instructions for preparing Google Spreadsheet AUTH.JSON:</h3>\n<blockquote>\n<ol>\n<li>Go to <a href=\"https://console.developers.google.com/\" rel=\"nofollow\">https://console.developers.google.com/</a></li>\n<li>Login with the google account that is to be owner of the \u2018Spreadsheet\u2019.</li>\n<li>At the top-left corner, there is a drop-down right next to the \u201cGoogle APIs\u201d text</li>\n<li>Click the drop-down and a modal-dialog will appear, then click \u201cNEW PROJECT\u201d at it\u2019s top-right</li>\n<li>Name the project relevant to how the sheet is to be used, don\u2019t select \u2018Location*\u2019, just press \u2018CREATE\u2019</li>\n<li>Open the newly created project from the same drop-down as in step 3.</li>\n<li>There should be \u2018APIs\u2019 area with a \u201c-&gt; Go to APIs overview\u201d at it\u2019s bottom - click it</li>\n<li>A new page will load having \u2018+ ENABLE APIS AND SERVICES\u2019 button at the top side\u2019s middle - click it</li>\n<li>A new page will load having a \u2018Search for APIs &amp; Services\u2019 input - use it to find and open \u2018Google Drive API\u2019</li>\n<li>In the \u2018Google Drive API\u2019 page click \u201cENABLE\u201d - you\u2019ll be redirected back to the project\u2019s page</li>\n<li>There will be a new \u2018CREATE CREDENTIALS\u2019 button at the top - click it</li>\n<li>Setup the new credentials as follows:<ul>\n<li>Which API are you using? -&gt; \u2018Google Drive API\u2019</li>\n<li>Where will you be calling the API from? -&gt; \u2018Web server (e.g. node.js, Tomcat)</li>\n<li>What data will you be accessing? -&gt; \u2018Application data\u2019</li>\n<li>Are you planning to use this API with App Engine or Compute Engine? -&gt; No, I\u2019m not using them.</li>\n</ul>\n</li>\n<li>Click the blue button \u2018What credentials do I need\u2019, will take you to \u2018Add credentials to you project\u2019 page</li>\n<li>Setup the credentials as follows:<ul>\n<li>Service account name:  {whatever name you type is OK, as long the input accepts it}</li>\n<li>Role: Project-&gt;Editor</li>\n<li>Key type: JSON</li>\n</ul>\n</li>\n<li>Press the blue \u2018Continue\u2019 button, and a download of the AUTH.JSON file will begin (store it safe)</li>\n<li>Close the modal and go back to the project \u2018Dashboard\u2019 using the left-side navigation panel</li>\n<li>Repeat step 8.</li>\n<li>Search for \u2018Google Sheets API\u2019, then open the result and click the blue \u2018ENABLE\u2019 button</li>\n<li>Open the downloaded auth.json file and copy the value of the \u2018client_email\u2019</li>\n<li>Using the same google account as in step 2. , go to the normal google sheets and create &amp; open the \u2018Spreadsheet\u2019<ul>\n<li>do a final renaming to the spreadsheet now to avoid issues in future</li>\n</ul>\n</li>\n<li>\u2018Share\u2019 the document with the email copied in step 19., giving it \u2018Edit\u2019 permissions<ul>\n<li>you might want to un-tick \u2018Notify people\u2019 before clicking \u2018Send\u2019 as it\u2019s a service email you\u2019re sharing with</li>\n<li>\u2018Send\u2019 will change to \u2018OK\u2019 upon un-tick, but we\u2019re cool with that - just click it.</li>\n</ul>\n</li>\n</ol>\n<p>You are now ready to use this class for retrieving \u2018Spreadsheet\u2019 handle in the code!</p>\n</blockquote>\n</div>\n</div>\n\n          </div>"}, "last_serial": 6523026, "releases": {"3.0.2": [{"comment_text": "", "digests": {"md5": "0e515f516d9d10d51dca7998d040718a", "sha256": "0dae267e1046e3036c39f0ce3f00e0c4ceb029df5ff22a58e7bc0c39779d0a9b"}, "downloads": -1, "filename": "scrape_jobs-3.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "0e515f516d9d10d51dca7998d040718a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 23064, "upload_time": "2020-01-26T19:20:16", "upload_time_iso_8601": "2020-01-26T19:20:16.690607Z", "url": "https://files.pythonhosted.org/packages/f6/16/38a804832f87fc1e94110e18e4aadf93aa0ca029092ffd6ea54e7c8e4f22/scrape_jobs-3.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b5683adbaaa1c6fa3faf45b0630d77ef", "sha256": "a1e570c594d38abadfe282c0cae341d35ec91453a0d13a0445efd811d6bbba72"}, "downloads": -1, "filename": "scrape_jobs-3.0.2.zip", "has_sig": false, "md5_digest": "b5683adbaaa1c6fa3faf45b0630d77ef", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 43187, "upload_time": "2020-01-26T19:20:18", "upload_time_iso_8601": "2020-01-26T19:20:18.726818Z", "url": "https://files.pythonhosted.org/packages/64/78/e9e2ad6f830a6882dc1be97ec608d46e0c3350aecc56ce06c2ddc213cdb7/scrape_jobs-3.0.2.zip", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "0e515f516d9d10d51dca7998d040718a", "sha256": "0dae267e1046e3036c39f0ce3f00e0c4ceb029df5ff22a58e7bc0c39779d0a9b"}, "downloads": -1, "filename": "scrape_jobs-3.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "0e515f516d9d10d51dca7998d040718a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 23064, "upload_time": "2020-01-26T19:20:16", "upload_time_iso_8601": "2020-01-26T19:20:16.690607Z", "url": "https://files.pythonhosted.org/packages/f6/16/38a804832f87fc1e94110e18e4aadf93aa0ca029092ffd6ea54e7c8e4f22/scrape_jobs-3.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b5683adbaaa1c6fa3faf45b0630d77ef", "sha256": "a1e570c594d38abadfe282c0cae341d35ec91453a0d13a0445efd811d6bbba72"}, "downloads": -1, "filename": "scrape_jobs-3.0.2.zip", "has_sig": false, "md5_digest": "b5683adbaaa1c6fa3faf45b0630d77ef", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 43187, "upload_time": "2020-01-26T19:20:18", "upload_time_iso_8601": "2020-01-26T19:20:18.726818Z", "url": "https://files.pythonhosted.org/packages/64/78/e9e2ad6f830a6882dc1be97ec608d46e0c3350aecc56ce06c2ddc213cdb7/scrape_jobs-3.0.2.zip", "yanked": false}], "timestamp": "Fri May  8 02:56:54 2020"}