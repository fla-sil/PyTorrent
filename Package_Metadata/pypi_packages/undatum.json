{"info": {"author": "Ivan Begtin", "author_email": "ivan@begtin.tech", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Console", "Intended Audience :: Developers", "Intended Audience :: System Administrators", "License :: OSI Approved :: BSD License", "Programming Language :: Python", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3 :: Only", "Topic :: Software Development", "Topic :: System :: Networking", "Topic :: Terminals", "Topic :: Text Processing", "Topic :: Utilities"], "description": "undatum: a command-line tool for data processing\n################################################\n\nundatum (pronounced *un-da-tum*) is a command line data processing tool.\nIts goal is to make CLI interaction with huge datasets so easy as possible.\nIt provides a simple ``undatum`` command that allows to convert, split, calculate frequency, statistics and to validate\ndata in CSV, JSON lines, BSON files.\n\n\n.. contents::\n\n.. section-numbering::\n\n\n\nMain features\n=============\n\n\n* Common data operations against CSV, JSON lines and BSON files\n* Built-in data filtering\n* Conversion between CSV, JSONl, BSON, XML, XLS, XLSX file types\n* Low memory footprint\n* Support for compressed datasets\n* Advanced statistics calculations\n* Date/datetime fields automatic recognition\n* Data validation\n* Documentation\n* Test coverage\n\n\n\n\nInstallation\n============\n\n\nmacOS\n-----\n\n\nOn macOS, undatum can be installed via `Homebrew <https://brew.sh/>`_\n(recommended):\n\n.. code-block:: bash\n\n    $ brew install undatum\n\n\nA MacPorts *port* is also available:\n\n.. code-block:: bash\n\n    $ port install undatum\n\nLinux\n-----\n\nMost Linux distributions provide a package that can be installed using the\nsystem package manager, for example:\n\n.. code-block:: bash\n\n    # Debian, Ubuntu, etc.\n    $ apt install undatum\n\n.. code-block:: bash\n\n    # Fedora\n    $ dnf install undatum\n\n.. code-block:: bash\n\n    # CentOS, RHEL, ...\n    $ yum install undatum\n\n.. code-block:: bash\n\n    # Arch Linux\n    $ pacman -S undatum\n\n\nWindows, etc.\n-------------\n\nA universal installation method (that works on Windows, Mac OS X, Linux, \u2026,\nand always provides the latest version) is to use pip:\n\n\n.. code-block:: bash\n\n    # Make sure we have an up-to-date version of pip and setuptools:\n    $ pip install --upgrade pip setuptools\n\n    $ pip install --upgrade undatum\n\n\n(If ``pip`` installation fails for some reason, you can try\n``easy_install undatum`` as a fallback.)\n\n\nPython version\n--------------\n\nPython version 3.6 or greater is required.\n\n\n\nUsage\n=====\n\n\nSynopsis:\n\n.. code-block:: bash\n\n    $ undatum [flags] [command] inputfile\n\n\nSee also ``undatum --help``.\n\n\nExamples\n--------\n\nGet headers from file as `headers command`_,  `JSONl`_ data:\n\n.. code-block:: bash\n\n    $ undatum headers examples/ausgovdir.jsonl\n\n\nAnalyze file and generate statistics `stats command`_:\n\n.. code-block:: bash\n\n    $ undatum stats examples/ausgovdir.jsonl\n\n\nGet `frequency command`_ of values for field GovSystem in the list of Russian federal government domains from  `govdomains repository <https://github.com/infoculture/govdomains/tree/master/refined>`_\n\n.. code-block:: bash\n\n    $ undatum frequency examples/feddomains.csv --fields GovSystem\n\n\nGet all unique values using `uniq command`_ of the *item.type* field\n\n.. code-block:: bash\n\n    $ undatum uniq --fields item.type examples/ausgovdir.jsonl\n\n`convert command`_ from XML to JSON lines file on tag *item*:\n\n.. code-block:: bash\n\n    $ undatum convert --tagname item examples/ausgovdir.xml examples/ausgovdir.jsonl\n\n\nValidate data with `validate command`_ against validation rule *ru.org.inn* and field *VendorINN* in  data file. Output is statistcs only :\n\n.. code-block:: bash\n\n    $ undatum validate -r ru.org.inn --mode stats --fields VendorINN examples/roszdravvendors_final.jsonl > inn_stats.json\n\nValidate data with `validate command`_ against validation rule *ru.org.inn* and field *VendorINN* in  data file. Output all invalid records :\n\n.. code-block:: bash\n\n    $ undatum validate -r ru.org.inn --mode invalid --fields VendorINN examples/roszdravvendors_final.jsonl > inn_invalid.json\n\nCommands\n========\n\nFrequency command\n-----------------\nField value frequency calculator. Returns frequency table for certain field\n\nGet frequencies of values for field *GovSystem* in the list of Russian federal government domains from  `govdomains repository <https://github.com/infoculture/govdomains/tree/master/refined>`_\n\n.. code-block:: bash\n\n    $ undatum frequency examples/feddomains.csv --fields GovSystem\n\n\n\n\nUniq command\n-------------\n\nReturns all unique files of certain field(s). Accepts parameter *fields* with comma separated fields to gets it unique values.\nProvide single field name to get unique values of this field or provide list of fields to get combined unique values.\n\n\nReturns all unique values of field *regions* in selected JSONl file\n\n.. code-block:: bash\n\n    $ undatum uniq --fields region examples/reestrgp_final.jsonl\n\nReturns all unique combinations of fields *status* and *regions* in selected JSONl file\n\n.. code-block:: bash\n\n    $ undatum uniq --fields status,region examples/reestrgp_final.jsonl\n\n\nConvert command\n---------------\n\nConverts data from one format to another.\nSupports conversions:\n\n* XML to JSON lines\n* CSV to JSON lines\n* XLS to JSON lines\n* XLSX to JSON lines\n* XLS to CSV\n* CSV to BSON\n* XLS to BSON\n\nConversion between XML and JSON lines require flag *tagname* with name of tag which should be converted into single JSON record.\n\nConverts XML ausgovdir.xml with tag named *item* to ausgovdir.jsonl\n\n.. code-block:: bash\n\n    $ undatum convert --tagname item examples/ausgovdir.xml examples/ausgovdir.jsonl\n\n\nValidate command\n----------------\n\n*Validate* command used to check every value of of field against validation rules like rule to validate email or url.\n\nCurrent supported rules:\n\n* *common.email* - checks if value is email\n* *common.url* - checks if value is url\n* *ru.org.inn* - checks if value is russian organization INN identifier\n* *ru.org.ogrn* - checks if value if russian organization OGRN identifier\n\nValidate data with `validate command`_ against validation rule *ru.org.inn* and field *VendorINN* in  data file. Output all invalid records :\n\n.. code-block:: bash\n\n    $ undatum validate -r ru.org.inn --mode invalid --fields VendorINN examples/roszdravvendors_final.jsonl > inn_invalid.json\n\n\nHeaders command\n---------------\nReturns fieldnames of the file. Supports CSV, JSON, BSON file types.\nFor CSV file it takes first line of the file and for JSON lines and BSON files it processes number of records provided as *limit* parameter with default value 10000.\n\nReturns headers of JSON lines file with top 10 000 records (default value)\n\n.. code-block:: bash\n\n    $ undatum headers examples/ausgovdir.jsonl\n\n\nReturns headers of JSON lines file using top 50 000 records\n\n.. code-block:: bash\n\n    $ undatum headers --limit 50000 examples/ausgovdir.jsonl\n\nStats command\n-------------\nCollects statistics about data in dataset. Right now supports only JSON lines files\n\nReturns table with following data:\n\n* *key* - name of the key\n* *ftype* - data type of the values with this key\n* *is_dictkey* - if True, than this key is identified as dictionary value\n* *is_uniq* - if True, identified as unique field\n* *n_uniq* - number of unique values\n* *share_uniq* - share of unique values among all values\n* *minlen* - minimal length of the field\n* *maxlen* - maximum length of the field\n* *avglen* - average length of the field\n\nReturns stats for JSON lines file\n\n.. code-block:: bash\n\n    $ undatum stats examples/ausgovdir.jsonl\n\nAnalysis of JSON lines file and verifies each field that it's date field, detects date format:\n\n.. code-block:: bash\n\n    $ undatum stats --checkdates examples/ausgovdir.jsonl\n\n\n\nSplit command\n-------------\nSplits dataset into number of datasets based on number of records or field value.\nChunksize parameter *-c* used to set size of chunk if dataset should be splitted by chunk size rule.\nIf dataset should be splitted by field value than *--fields* parameter used.\n\nSplit dataset as 10000 records chunks, procuces files like filename_1.jsonl, filename_2.jsonl where *filename* is name of original file except extension.\n\n.. code-block:: bash\n\n    $ undatum split -c 10000 examples/ausgovdir.jsonl\n\n\nSplit dataset as number of files based of field *item.type\", generates files [filename]_[value1].jsonl, [filename]_[value2].jsonl and e.t.c.\nThere are *[filename]* - ausgovdir and *[value1]* - certain unique value from *item.type* field\n\n.. code-block:: bash\n\n    $ undatum split --fields item.type examples/ausgovdir.jsonl\n\n\n\nSelect command\n--------------\n\nSelect or re-order columns from file. Supports CSV, JSON lines, BSON\n\nReturns columns *item.title* and *item.type* from ausgovdir.jsonl\n\n.. code-block:: bash\n\n    $ undatum select --fields item.title,item.type examples/ausgovdir.jsonl\n\n\nReturns columns *item.title* and *item.type* from ausgovdir.jsonl and stores result as selected.jsonl\n\n.. code-block:: bash\n\n    $ undatum select --fields item.title,item.type -o selected.jsonl examples/ausgovdir.jsonl\n\nFlatten command\n---------------\n\nFlatten data records. Write them as one value per row\n\nReturns all columns as flattened key,value\n\n.. code-block:: bash\n\n    $ undatum flatten examples/ausgovdir.jsonl\n\n\nAdvanced\n========\n\nFiltering\n---------\n\nYou could filter values of any file record by using *filter* attr for any command where it's suported.\n\nReturns columns item.title and item.type filtered with *item.type* value as *role*. Note: keys should be surrounded by \"`\" and text values by \"'\".\n\n.. code-block:: bash\n\n    $ undatum select --fields item.title,item.type --filter \"`item.type` == 'role'\" examples/ausgovdir.jsonl\n\nData containers\n---------------\n\nSometimes, to keep keep memory usage as low as possible to process huge data files.\nThese files are inside compressed containers like .zip, .gz, .bz2 or .tar.gz files.\n*undatum* could process compressed files with little memory footprint, but it could slow down file processing.\n\nReturns headers from subs_dump_1.jsonl file inside subs_dump_1.zip file. Require parameter *-z* to be set and *--format-in* force input file type.\n\n.. code-block:: bash\n\n    $ undatum headers --format-in jsonl -z subs_dump_1.zip\n\n\nDate detection\n--------------\nJSON, JSON lines and CSV files do not support date and datetime data types.\nIf you manually prepare your data, than you could define datetime in JSON schema for example.B\nBut if data is external, you need to identify these fields.\n\nundatum supports date identification via `qddate <https://github.com/ivbeg/qddate>`_ python library with automatic date detection abilities.\n\n.. code-block:: bash\n\n    $ undatum stats --checkdates examples/ausgovdir.jsonl\n\n\nData types\n==========\n\nJSONl\n-----\n\nJSON lines is a replacement to CSV and JSON files, with JSON flexibility and ability to process data line by line, without loading everithing into memory.", "description_content_type": "", "docs_url": null, "download_url": "https://github.com/datacoon/undatum/", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/datacoon/undatum/", "keywords": "json jsonl csv bson cli dataset", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "undatum", "package_url": "https://pypi.org/project/undatum/", "platform": "", "project_url": "https://pypi.org/project/undatum/", "project_urls": {"Download": "https://github.com/datacoon/undatum/", "Homepage": "https://github.com/datacoon/undatum/"}, "release_url": "https://pypi.org/project/undatum/1.0.5/", "requires_dist": null, "requires_python": "", "summary": "undatum: a command-line tool for data processing. Brings CSV simplicity to JSON lines and BSON", "version": "1.0.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>undatum (pronounced <em>un-da-tum</em>) is a command line data processing tool.\nIts goal is to make CLI interaction with huge datasets so easy as possible.\nIt provides a simple <tt>undatum</tt> command that allows to convert, split, calculate frequency, statistics and to validate\ndata in CSV, JSON lines, BSON files.</p>\n<div id=\"contents\">\n<p>Contents</p>\n<ul>\n<li><a href=\"#main-features\" id=\"id2\" rel=\"nofollow\">1\u00a0\u00a0\u00a0Main features</a></li>\n<li><a href=\"#installation\" id=\"id3\" rel=\"nofollow\">2\u00a0\u00a0\u00a0Installation</a><ul>\n<li><a href=\"#macos\" id=\"id4\" rel=\"nofollow\">2.1\u00a0\u00a0\u00a0macOS</a></li>\n<li><a href=\"#linux\" id=\"id5\" rel=\"nofollow\">2.2\u00a0\u00a0\u00a0Linux</a></li>\n<li><a href=\"#windows-etc\" id=\"id6\" rel=\"nofollow\">2.3\u00a0\u00a0\u00a0Windows, etc.</a></li>\n<li><a href=\"#python-version\" id=\"id7\" rel=\"nofollow\">2.4\u00a0\u00a0\u00a0Python version</a></li>\n</ul>\n</li>\n<li><a href=\"#usage\" id=\"id8\" rel=\"nofollow\">3\u00a0\u00a0\u00a0Usage</a><ul>\n<li><a href=\"#examples\" id=\"id9\" rel=\"nofollow\">3.1\u00a0\u00a0\u00a0Examples</a></li>\n</ul>\n</li>\n<li><a href=\"#commands\" id=\"id10\" rel=\"nofollow\">4\u00a0\u00a0\u00a0Commands</a><ul>\n<li><a href=\"#frequency-command\" id=\"id11\" rel=\"nofollow\">4.1\u00a0\u00a0\u00a0Frequency command</a></li>\n<li><a href=\"#uniq-command\" id=\"id12\" rel=\"nofollow\">4.2\u00a0\u00a0\u00a0Uniq command</a></li>\n<li><a href=\"#convert-command\" id=\"id13\" rel=\"nofollow\">4.3\u00a0\u00a0\u00a0Convert command</a></li>\n<li><a href=\"#validate-command\" id=\"id14\" rel=\"nofollow\">4.4\u00a0\u00a0\u00a0Validate command</a></li>\n<li><a href=\"#headers-command\" id=\"id15\" rel=\"nofollow\">4.5\u00a0\u00a0\u00a0Headers command</a></li>\n<li><a href=\"#stats-command\" id=\"id16\" rel=\"nofollow\">4.6\u00a0\u00a0\u00a0Stats command</a></li>\n<li><a href=\"#split-command\" id=\"id17\" rel=\"nofollow\">4.7\u00a0\u00a0\u00a0Split command</a></li>\n<li><a href=\"#select-command\" id=\"id18\" rel=\"nofollow\">4.8\u00a0\u00a0\u00a0Select command</a></li>\n<li><a href=\"#flatten-command\" id=\"id19\" rel=\"nofollow\">4.9\u00a0\u00a0\u00a0Flatten command</a></li>\n</ul>\n</li>\n<li><a href=\"#advanced\" id=\"id20\" rel=\"nofollow\">5\u00a0\u00a0\u00a0Advanced</a><ul>\n<li><a href=\"#filtering\" id=\"id21\" rel=\"nofollow\">5.1\u00a0\u00a0\u00a0Filtering</a></li>\n<li><a href=\"#data-containers\" id=\"id22\" rel=\"nofollow\">5.2\u00a0\u00a0\u00a0Data containers</a></li>\n<li><a href=\"#date-detection\" id=\"id23\" rel=\"nofollow\">5.3\u00a0\u00a0\u00a0Date detection</a></li>\n</ul>\n</li>\n<li><a href=\"#data-types\" id=\"id24\" rel=\"nofollow\">6\u00a0\u00a0\u00a0Data types</a></li>\n</ul>\n</div>\n<div id=\"main-features\">\n<h2><a href=\"#id2\" rel=\"nofollow\">1\u00a0\u00a0\u00a0Main features</a></h2>\n<ul>\n<li>Common data operations against CSV, JSON lines and BSON files</li>\n<li>Built-in data filtering</li>\n<li>Conversion between CSV, JSONl, BSON, XML, XLS, XLSX file types</li>\n<li>Low memory footprint</li>\n<li>Support for compressed datasets</li>\n<li>Advanced statistics calculations</li>\n<li>Date/datetime fields automatic recognition</li>\n<li>Data validation</li>\n<li>Documentation</li>\n<li>Test coverage</li>\n</ul>\n</div>\n<div id=\"installation\">\n<h2><a href=\"#id3\" rel=\"nofollow\">2\u00a0\u00a0\u00a0Installation</a></h2>\n<div id=\"macos\">\n<h3><a href=\"#id4\" rel=\"nofollow\">2.1\u00a0\u00a0\u00a0macOS</a></h3>\n<p>On macOS, undatum can be installed via <a href=\"https://brew.sh/\" rel=\"nofollow\">Homebrew</a>\n(recommended):</p>\n<pre>$ brew install undatum\n</pre>\n<p>A MacPorts <em>port</em> is also available:</p>\n<pre>$ port install undatum\n</pre>\n</div>\n<div id=\"linux\">\n<h3><a href=\"#id5\" rel=\"nofollow\">2.2\u00a0\u00a0\u00a0Linux</a></h3>\n<p>Most Linux distributions provide a package that can be installed using the\nsystem package manager, for example:</p>\n<pre><span class=\"c1\"># Debian, Ubuntu, etc.\n</span>$ apt install undatum\n</pre>\n<pre><span class=\"c1\"># Fedora\n</span>$ dnf install undatum\n</pre>\n<pre><span class=\"c1\"># CentOS, RHEL, ...\n</span>$ yum install undatum\n</pre>\n<pre><span class=\"c1\"># Arch Linux\n</span>$ pacman -S undatum\n</pre>\n</div>\n<div id=\"windows-etc\">\n<h3><a href=\"#id6\" rel=\"nofollow\">2.3\u00a0\u00a0\u00a0Windows, etc.</a></h3>\n<p>A universal installation method (that works on Windows, Mac OS X, Linux, \u2026,\nand always provides the latest version) is to use pip:</p>\n<pre><span class=\"c1\"># Make sure we have an up-to-date version of pip and setuptools:\n</span>$ pip install --upgrade pip setuptools\n\n$ pip install --upgrade undatum\n</pre>\n<p>(If <tt>pip</tt> installation fails for some reason, you can try\n<tt>easy_install undatum</tt> as a fallback.)</p>\n</div>\n<div id=\"python-version\">\n<h3><a href=\"#id7\" rel=\"nofollow\">2.4\u00a0\u00a0\u00a0Python version</a></h3>\n<p>Python version 3.6 or greater is required.</p>\n</div>\n</div>\n<div id=\"usage\">\n<h2><a href=\"#id8\" rel=\"nofollow\">3\u00a0\u00a0\u00a0Usage</a></h2>\n<p>Synopsis:</p>\n<pre>$ undatum <span class=\"o\">[</span>flags<span class=\"o\">]</span> <span class=\"o\">[</span>command<span class=\"o\">]</span> inputfile\n</pre>\n<p>See also <tt>undatum <span class=\"pre\">--help</span></tt>.</p>\n<div id=\"examples\">\n<h3><a href=\"#id9\" rel=\"nofollow\">3.1\u00a0\u00a0\u00a0Examples</a></h3>\n<p>Get headers from file as <a href=\"#headers-command\" rel=\"nofollow\">headers command</a>,  <a href=\"#jsonl\" rel=\"nofollow\">JSONl</a> data:</p>\n<pre>$ undatum headers examples/ausgovdir.jsonl\n</pre>\n<p>Analyze file and generate statistics <a href=\"#stats-command\" rel=\"nofollow\">stats command</a>:</p>\n<pre>$ undatum stats examples/ausgovdir.jsonl\n</pre>\n<p>Get <a href=\"#frequency-command\" rel=\"nofollow\">frequency command</a> of values for field GovSystem in the list of Russian federal government domains from  <a href=\"https://github.com/infoculture/govdomains/tree/master/refined\" rel=\"nofollow\">govdomains repository</a></p>\n<pre>$ undatum frequency examples/feddomains.csv --fields GovSystem\n</pre>\n<p>Get all unique values using <a href=\"#uniq-command\" rel=\"nofollow\">uniq command</a> of the <em>item.type</em> field</p>\n<pre>$ undatum uniq --fields item.type examples/ausgovdir.jsonl\n</pre>\n<p><a href=\"#convert-command\" rel=\"nofollow\">convert command</a> from XML to JSON lines file on tag <em>item</em>:</p>\n<pre>$ undatum convert --tagname item examples/ausgovdir.xml examples/ausgovdir.jsonl\n</pre>\n<p>Validate data with <a href=\"#validate-command\" rel=\"nofollow\">validate command</a> against validation rule <em>ru.org.inn</em> and field <em>VendorINN</em> in  data file. Output is statistcs only :</p>\n<pre>$ undatum validate -r ru.org.inn --mode stats --fields VendorINN examples/roszdravvendors_final.jsonl &gt; inn_stats.json\n</pre>\n<p>Validate data with <a href=\"#validate-command\" rel=\"nofollow\">validate command</a> against validation rule <em>ru.org.inn</em> and field <em>VendorINN</em> in  data file. Output all invalid records :</p>\n<pre>$ undatum validate -r ru.org.inn --mode invalid --fields VendorINN examples/roszdravvendors_final.jsonl &gt; inn_invalid.json\n</pre>\n</div>\n</div>\n<div id=\"commands\">\n<h2><a href=\"#id10\" rel=\"nofollow\">4\u00a0\u00a0\u00a0Commands</a></h2>\n<div id=\"frequency-command\">\n<h3><a href=\"#id11\" rel=\"nofollow\">4.1\u00a0\u00a0\u00a0Frequency command</a></h3>\n<p>Field value frequency calculator. Returns frequency table for certain field</p>\n<p>Get frequencies of values for field <em>GovSystem</em> in the list of Russian federal government domains from  <a href=\"https://github.com/infoculture/govdomains/tree/master/refined\" rel=\"nofollow\">govdomains repository</a></p>\n<pre>$ undatum frequency examples/feddomains.csv --fields GovSystem\n</pre>\n</div>\n<div id=\"uniq-command\">\n<h3><a href=\"#id12\" rel=\"nofollow\">4.2\u00a0\u00a0\u00a0Uniq command</a></h3>\n<p>Returns all unique files of certain field(s). Accepts parameter <em>fields</em> with comma separated fields to gets it unique values.\nProvide single field name to get unique values of this field or provide list of fields to get combined unique values.</p>\n<p>Returns all unique values of field <em>regions</em> in selected JSONl file</p>\n<pre>$ undatum uniq --fields region examples/reestrgp_final.jsonl\n</pre>\n<p>Returns all unique combinations of fields <em>status</em> and <em>regions</em> in selected JSONl file</p>\n<pre>$ undatum uniq --fields status,region examples/reestrgp_final.jsonl\n</pre>\n</div>\n<div id=\"convert-command\">\n<h3><a href=\"#id13\" rel=\"nofollow\">4.3\u00a0\u00a0\u00a0Convert command</a></h3>\n<p>Converts data from one format to another.\nSupports conversions:</p>\n<ul>\n<li>XML to JSON lines</li>\n<li>CSV to JSON lines</li>\n<li>XLS to JSON lines</li>\n<li>XLSX to JSON lines</li>\n<li>XLS to CSV</li>\n<li>CSV to BSON</li>\n<li>XLS to BSON</li>\n</ul>\n<p>Conversion between XML and JSON lines require flag <em>tagname</em> with name of tag which should be converted into single JSON record.</p>\n<p>Converts XML ausgovdir.xml with tag named <em>item</em> to ausgovdir.jsonl</p>\n<pre>$ undatum convert --tagname item examples/ausgovdir.xml examples/ausgovdir.jsonl\n</pre>\n</div>\n<div id=\"validate-command\">\n<h3><a href=\"#id14\" rel=\"nofollow\">4.4\u00a0\u00a0\u00a0Validate command</a></h3>\n<p><em>Validate</em> command used to check every value of of field against validation rules like rule to validate email or url.</p>\n<p>Current supported rules:</p>\n<ul>\n<li><em>common.email</em> - checks if value is email</li>\n<li><em>common.url</em> - checks if value is url</li>\n<li><em>ru.org.inn</em> - checks if value is russian organization INN identifier</li>\n<li><em>ru.org.ogrn</em> - checks if value if russian organization OGRN identifier</li>\n</ul>\n<p>Validate data with <a href=\"#validate-command\" rel=\"nofollow\">validate command</a> against validation rule <em>ru.org.inn</em> and field <em>VendorINN</em> in  data file. Output all invalid records :</p>\n<pre>$ undatum validate -r ru.org.inn --mode invalid --fields VendorINN examples/roszdravvendors_final.jsonl &gt; inn_invalid.json\n</pre>\n</div>\n<div id=\"headers-command\">\n<h3><a href=\"#id15\" rel=\"nofollow\">4.5\u00a0\u00a0\u00a0Headers command</a></h3>\n<p>Returns fieldnames of the file. Supports CSV, JSON, BSON file types.\nFor CSV file it takes first line of the file and for JSON lines and BSON files it processes number of records provided as <em>limit</em> parameter with default value 10000.</p>\n<p>Returns headers of JSON lines file with top 10 000 records (default value)</p>\n<pre>$ undatum headers examples/ausgovdir.jsonl\n</pre>\n<p>Returns headers of JSON lines file using top 50 000 records</p>\n<pre>$ undatum headers --limit <span class=\"m\">50000</span> examples/ausgovdir.jsonl\n</pre>\n</div>\n<div id=\"stats-command\">\n<h3><a href=\"#id16\" rel=\"nofollow\">4.6\u00a0\u00a0\u00a0Stats command</a></h3>\n<p>Collects statistics about data in dataset. Right now supports only JSON lines files</p>\n<p>Returns table with following data:</p>\n<ul>\n<li><em>key</em> - name of the key</li>\n<li><em>ftype</em> - data type of the values with this key</li>\n<li><em>is_dictkey</em> - if True, than this key is identified as dictionary value</li>\n<li><em>is_uniq</em> - if True, identified as unique field</li>\n<li><em>n_uniq</em> - number of unique values</li>\n<li><em>share_uniq</em> - share of unique values among all values</li>\n<li><em>minlen</em> - minimal length of the field</li>\n<li><em>maxlen</em> - maximum length of the field</li>\n<li><em>avglen</em> - average length of the field</li>\n</ul>\n<p>Returns stats for JSON lines file</p>\n<pre>$ undatum stats examples/ausgovdir.jsonl\n</pre>\n<p>Analysis of JSON lines file and verifies each field that it\u2019s date field, detects date format:</p>\n<pre>$ undatum stats --checkdates examples/ausgovdir.jsonl\n</pre>\n</div>\n<div id=\"split-command\">\n<h3><a href=\"#id17\" rel=\"nofollow\">4.7\u00a0\u00a0\u00a0Split command</a></h3>\n<p>Splits dataset into number of datasets based on number of records or field value.\nChunksize parameter <em>-c</em> used to set size of chunk if dataset should be splitted by chunk size rule.\nIf dataset should be splitted by field value than <em>\u2013fields</em> parameter used.</p>\n<p>Split dataset as 10000 records chunks, procuces files like filename_1.jsonl, filename_2.jsonl where <em>filename</em> is name of original file except extension.</p>\n<pre>$ undatum split -c <span class=\"m\">10000</span> examples/ausgovdir.jsonl\n</pre>\n<p>Split dataset as number of files based of field <em>item.type\u201d, generates files [filename]_[value1].jsonl, [filename]_[value2].jsonl and e.t.c.\nThere are *[filename]</em> - ausgovdir and <em>[value1]</em> - certain unique value from <em>item.type</em> field</p>\n<pre>$ undatum split --fields item.type examples/ausgovdir.jsonl\n</pre>\n</div>\n<div id=\"select-command\">\n<h3><a href=\"#id18\" rel=\"nofollow\">4.8\u00a0\u00a0\u00a0Select command</a></h3>\n<p>Select or re-order columns from file. Supports CSV, JSON lines, BSON</p>\n<p>Returns columns <em>item.title</em> and <em>item.type</em> from ausgovdir.jsonl</p>\n<pre>$ undatum <span class=\"k\">select</span> --fields item.title,item.type examples/ausgovdir.jsonl\n</pre>\n<p>Returns columns <em>item.title</em> and <em>item.type</em> from ausgovdir.jsonl and stores result as selected.jsonl</p>\n<pre>$ undatum <span class=\"k\">select</span> --fields item.title,item.type -o selected.jsonl examples/ausgovdir.jsonl\n</pre>\n</div>\n<div id=\"flatten-command\">\n<h3><a href=\"#id19\" rel=\"nofollow\">4.9\u00a0\u00a0\u00a0Flatten command</a></h3>\n<p>Flatten data records. Write them as one value per row</p>\n<p>Returns all columns as flattened key,value</p>\n<pre>$ undatum flatten examples/ausgovdir.jsonl\n</pre>\n</div>\n</div>\n<div id=\"advanced\">\n<h2><a href=\"#id20\" rel=\"nofollow\">5\u00a0\u00a0\u00a0Advanced</a></h2>\n<div id=\"filtering\">\n<h3><a href=\"#id21\" rel=\"nofollow\">5.1\u00a0\u00a0\u00a0Filtering</a></h3>\n<p>You could filter values of any file record by using <em>filter</em> attr for any command where it\u2019s suported.</p>\n<p>Returns columns item.title and item.type filtered with <em>item.type</em> value as <em>role</em>. Note: keys should be surrounded by \u201c`\u201d and text values by \u201c\u2019\u201d.</p>\n<pre>$ undatum <span class=\"k\">select</span> --fields item.title,item.type --filter <span class=\"s2\">\"`item.type` == 'role'\"</span> examples/ausgovdir.jsonl\n</pre>\n</div>\n<div id=\"data-containers\">\n<h3><a href=\"#id22\" rel=\"nofollow\">5.2\u00a0\u00a0\u00a0Data containers</a></h3>\n<p>Sometimes, to keep keep memory usage as low as possible to process huge data files.\nThese files are inside compressed containers like .zip, .gz, .bz2 or .tar.gz files.\n<em>undatum</em> could process compressed files with little memory footprint, but it could slow down file processing.</p>\n<p>Returns headers from subs_dump_1.jsonl file inside subs_dump_1.zip file. Require parameter <em>-z</em> to be set and <em>\u2013format-in</em> force input file type.</p>\n<pre>$ undatum headers --format-in jsonl -z subs_dump_1.zip\n</pre>\n</div>\n<div id=\"date-detection\">\n<h3><a href=\"#id23\" rel=\"nofollow\">5.3\u00a0\u00a0\u00a0Date detection</a></h3>\n<p>JSON, JSON lines and CSV files do not support date and datetime data types.\nIf you manually prepare your data, than you could define datetime in JSON schema for example.B\nBut if data is external, you need to identify these fields.</p>\n<p>undatum supports date identification via <a href=\"https://github.com/ivbeg/qddate\" rel=\"nofollow\">qddate</a> python library with automatic date detection abilities.</p>\n<pre>$ undatum stats --checkdates examples/ausgovdir.jsonl\n</pre>\n</div>\n</div>\n<div id=\"data-types\">\n<h2><a href=\"#id24\" rel=\"nofollow\">6\u00a0\u00a0\u00a0Data types</a></h2>\n<h2 id=\"jsonl\"><span class=\"section-subtitle\">JSONl</span></h2>\n<p>JSON lines is a replacement to CSV and JSON files, with JSON flexibility and ability to process data line by line, without loading everithing into memory.</p>\n</div>\n\n          </div>"}, "last_serial": 7039062, "releases": {"1.0.1": [{"comment_text": "", "digests": {"md5": "71c13f0e2f8b7af4a89a84ef2295b2d4", "sha256": "69e728b14d05be1f1df0535add7a9eceb33d65a2b4c3dce3a40ec9d99895ff15"}, "downloads": -1, "filename": "undatum-1.0.1.tar.gz", "has_sig": false, "md5_digest": "71c13f0e2f8b7af4a89a84ef2295b2d4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22373, "upload_time": "2020-04-16T12:15:45", "upload_time_iso_8601": "2020-04-16T12:15:45.235234Z", "url": "https://files.pythonhosted.org/packages/67/31/e4d1f7ec30044361cc08d12ff96bec4520a3502ffb4745af512aeddad4bd/undatum-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "226aee7864333a4117f692cc1fe03156", "sha256": "e6b5d3cac599a915fdebfea85fc28c927c11c89f365160a96a49c7aab2ad963b"}, "downloads": -1, "filename": "undatum-1.0.2.tar.gz", "has_sig": false, "md5_digest": "226aee7864333a4117f692cc1fe03156", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22366, "upload_time": "2020-04-16T12:20:21", "upload_time_iso_8601": "2020-04-16T12:20:21.550605Z", "url": "https://files.pythonhosted.org/packages/57/06/1cbc9bab3f9bc446609f78992e3639662994ce654b2f38f977454a664a12/undatum-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "5f4af3b0113ec49193e95773359cd50e", "sha256": "41e8bbb524a2c4832cbd582d49955e5284925075c6ba62f62c5117b6f7637347"}, "downloads": -1, "filename": "undatum-1.0.3.tar.gz", "has_sig": false, "md5_digest": "5f4af3b0113ec49193e95773359cd50e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22374, "upload_time": "2020-04-16T13:35:19", "upload_time_iso_8601": "2020-04-16T13:35:19.365756Z", "url": "https://files.pythonhosted.org/packages/aa/ac/37144a848631121337849a5337f8f15d22995abc5dcdfa189a6da57dae46/undatum-1.0.3.tar.gz", "yanked": false}], "1.0.4": [{"comment_text": "", "digests": {"md5": "9f65855c75760ba03f3dee567ff60c95", "sha256": "bc483cca98571df2750b22130909c9198780306ccbd6335e9ae3e91fbfdc06ef"}, "downloads": -1, "filename": "undatum-1.0.4.tar.gz", "has_sig": false, "md5_digest": "9f65855c75760ba03f3dee567ff60c95", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22323, "upload_time": "2020-04-16T13:38:47", "upload_time_iso_8601": "2020-04-16T13:38:47.989342Z", "url": "https://files.pythonhosted.org/packages/9a/cd/7e46eb3681949f590d487be7fcfea33c590861dc7a42c312568bf2ee5357/undatum-1.0.4.tar.gz", "yanked": false}], "1.0.5": [{"comment_text": "", "digests": {"md5": "23cc66b6ae6fefc04841a5a39982d1b4", "sha256": "770b6aeb14f0b4f5e5675ad6b9a5cacdf20058e832b8eec5187d4dc2b38b1a01"}, "downloads": -1, "filename": "undatum-1.0.5.tar.gz", "has_sig": false, "md5_digest": "23cc66b6ae6fefc04841a5a39982d1b4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22600, "upload_time": "2020-04-17T09:47:57", "upload_time_iso_8601": "2020-04-17T09:47:57.696002Z", "url": "https://files.pythonhosted.org/packages/c4/bb/1b76a36388653fca5c35b731b0bfcde9158d58bc97e296601c3d19656dbb/undatum-1.0.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "23cc66b6ae6fefc04841a5a39982d1b4", "sha256": "770b6aeb14f0b4f5e5675ad6b9a5cacdf20058e832b8eec5187d4dc2b38b1a01"}, "downloads": -1, "filename": "undatum-1.0.5.tar.gz", "has_sig": false, "md5_digest": "23cc66b6ae6fefc04841a5a39982d1b4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22600, "upload_time": "2020-04-17T09:47:57", "upload_time_iso_8601": "2020-04-17T09:47:57.696002Z", "url": "https://files.pythonhosted.org/packages/c4/bb/1b76a36388653fca5c35b731b0bfcde9158d58bc97e296601c3d19656dbb/undatum-1.0.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:40:54 2020"}