{"info": {"author": "Philippe Remy", "author_email": "", "bugtrack_url": null, "classifiers": [], "description": "# Keras Attention Mechanism\n[![license](https://img.shields.io/badge/License-Apache_2.0-brightgreen.svg)](https://github.com/philipperemy/keras-attention-mechanism/blob/master/LICENSE) [![dep1](https://img.shields.io/badge/Tensorflow-2.0+-brightgreen.svg)](https://www.tensorflow.org/) [![dep2](https://img.shields.io/badge/Keras-2.0+-brightgreen.svg)](https://keras.io/) \n\n```\npip install attention\n```\n\nMany-to-one attention mechanism for Keras.\n\n<p align=\"center\">\n  <img src=\"examples/equations.png\">\n</p>\n\n## Examples\n\n### IMDB Dataset\n\nIn this experiment, we demonstrate that using attention yields a higher accuracy on the IMDB dataset. We consider two\nLSTM networks: one with this attention layer and the other one with a fully connected layer. Both have the same number\nof parameters for a fair comparison (250K).\n\nHere are the results on 10 runs. For every run, we record the max accuracy on the test set for 10 epochs. It does not have to be the final accuracy at the end of the training.\n\n\n| Measure  | No Attention (250K params) | Attention (250K params) |\n| ------------- | ------------- | ------------- |\n| MAX Accuracy | 88.22 | 88.76 |\n| AVG Accuracy | 87.02 | 87.62 |\n| STDDEV Accuracy | 0.18 | 0.14 |\n\nAs expected, there is a boost in accuracy for the model with attention. It also reduces the variability between the runs, which is something nice to have.\n\n\n### Adding two numbers\n\nLet's consider the task of adding two numbers that come right after some delimiters (0 in this case):\n\n`x = [1, 2, 3, 0, 4, 5, 6, 0, 7, 8]`. Result is `y = 4 + 7 = 11`.\n\nThe attention is expected to be the highest after the delimiters. An overview of the training is shown below, where the\ntop represents the attention map and the bottom the ground truth. As the training  progresses, the model learns the \ntask and the attention map converges to the ground truth.\n\n<p align=\"center\">\n  <img src=\"examples/attention.gif\" width=\"320\">\n</p>\n\n## References\n\n- https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf\n- https://arxiv.org/abs/1508.04025\n- https://arxiv.org/abs/1409.0473\n- https://github.com/philipperemy/keras-attention-mechanism/issues/14\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "attention", "package_url": "https://pypi.org/project/attention/", "platform": "", "project_url": "https://pypi.org/project/attention/", "project_urls": null, "release_url": "https://pypi.org/project/attention/2.2/", "requires_dist": ["numpy (>=1.18.1)", "keras (>=2.3.1)", "gast (>=0.2.2)"], "requires_python": "", "summary": "Keras Attention Many to One", "version": "2.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Keras Attention Mechanism</h1>\n<p><a href=\"https://github.com/philipperemy/keras-attention-mechanism/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"license\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2cd90bda2fbf6970b2b6a4695a3da0d2f1d825cb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368655f322e302d627269676874677265656e2e737667\"></a> <a href=\"https://www.tensorflow.org/\" rel=\"nofollow\"><img alt=\"dep1\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/47a40a2b9faeb1c2d84fd75c8e20278bb3f2bb56/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f54656e736f72666c6f772d322e302b2d627269676874677265656e2e737667\"></a> <a href=\"https://keras.io/\" rel=\"nofollow\"><img alt=\"dep2\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/802b1b0d645ce23b195879053c68b70aa27252f5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4b657261732d322e302b2d627269676874677265656e2e737667\"></a></p>\n<pre><code>pip install attention\n</code></pre>\n<p>Many-to-one attention mechanism for Keras.</p>\n<p align=\"center\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/09a4d446602f0da6dcd869faed8b4594e212452d/6578616d706c65732f6571756174696f6e732e706e67\">\n</p>\n<h2>Examples</h2>\n<h3>IMDB Dataset</h3>\n<p>In this experiment, we demonstrate that using attention yields a higher accuracy on the IMDB dataset. We consider two\nLSTM networks: one with this attention layer and the other one with a fully connected layer. Both have the same number\nof parameters for a fair comparison (250K).</p>\n<p>Here are the results on 10 runs. For every run, we record the max accuracy on the test set for 10 epochs. It does not have to be the final accuracy at the end of the training.</p>\n<table>\n<thead>\n<tr>\n<th>Measure</th>\n<th>No Attention (250K params)</th>\n<th>Attention (250K params)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MAX Accuracy</td>\n<td>88.22</td>\n<td>88.76</td>\n</tr>\n<tr>\n<td>AVG Accuracy</td>\n<td>87.02</td>\n<td>87.62</td>\n</tr>\n<tr>\n<td>STDDEV Accuracy</td>\n<td>0.18</td>\n<td>0.14</td>\n</tr></tbody></table>\n<p>As expected, there is a boost in accuracy for the model with attention. It also reduces the variability between the runs, which is something nice to have.</p>\n<h3>Adding two numbers</h3>\n<p>Let's consider the task of adding two numbers that come right after some delimiters (0 in this case):</p>\n<p><code>x = [1, 2, 3, 0, 4, 5, 6, 0, 7, 8]</code>. Result is <code>y = 4 + 7 = 11</code>.</p>\n<p>The attention is expected to be the highest after the delimiters. An overview of the training is shown below, where the\ntop represents the attention map and the bottom the ground truth. As the training  progresses, the model learns the\ntask and the attention map converges to the ground truth.</p>\n<p align=\"center\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6c8c1fae56a85611f33375b709733a0feda6dfb4/6578616d706c65732f617474656e74696f6e2e676966\" width=\"320\">\n</p>\n<h2>References</h2>\n<ul>\n<li><a href=\"https://www.cs.cmu.edu/%7E./hovy/papers/16HLT-hierarchical-attention-networks.pdf\" rel=\"nofollow\">https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf</a></li>\n<li><a href=\"https://arxiv.org/abs/1508.04025\" rel=\"nofollow\">https://arxiv.org/abs/1508.04025</a></li>\n<li><a href=\"https://arxiv.org/abs/1409.0473\" rel=\"nofollow\">https://arxiv.org/abs/1409.0473</a></li>\n<li><a href=\"https://github.com/philipperemy/keras-attention-mechanism/issues/14\" rel=\"nofollow\">https://github.com/philipperemy/keras-attention-mechanism/issues/14</a></li>\n</ul>\n\n          </div>"}, "last_serial": 6848793, "releases": {"2.1": [{"comment_text": "", "digests": {"md5": "9e855b0d580786284d9564eb6706d0aa", "sha256": "1467a4882dc5e443eced34fd9dcc5691e28720c8dd821dd6c4c5006fe021db19"}, "downloads": -1, "filename": "attention-2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "9e855b0d580786284d9564eb6706d0aa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 6436, "upload_time": "2020-03-20T07:08:31", "upload_time_iso_8601": "2020-03-20T07:08:31.890838Z", "url": "https://files.pythonhosted.org/packages/48/ee/93770438865dafef64ee9512c771d9f75f091b808ea1e1648d31dda52974/attention-2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c8e9df3a6872a0ff15dd0874629b4749", "sha256": "fdfd54dc57f2fb661785afef6c017f651650b4d1d13afc7c7a2737d12ddb56f9"}, "downloads": -1, "filename": "attention-2.1.tar.gz", "has_sig": false, "md5_digest": "c8e9df3a6872a0ff15dd0874629b4749", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1771, "upload_time": "2020-03-20T07:08:34", "upload_time_iso_8601": "2020-03-20T07:08:34.111268Z", "url": "https://files.pythonhosted.org/packages/65/b5/a022cb78b01d945becc76be51c1f76b086d4d4d35c213cd49aaa203c5695/attention-2.1.tar.gz", "yanked": false}], "2.2": [{"comment_text": "", "digests": {"md5": "fa0871f387f5c291fc725a3e68bc7c08", "sha256": "5e98e7163ae961925eeb9fd1174069b56aa9766722b5da6c14c96d3174c4f863"}, "downloads": -1, "filename": "attention-2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "fa0871f387f5c291fc725a3e68bc7c08", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7187, "upload_time": "2020-03-20T10:06:44", "upload_time_iso_8601": "2020-03-20T10:06:44.014775Z", "url": "https://files.pythonhosted.org/packages/9c/54/f045b9eb9000684fefc8c47ef2fdb5d3497123709191b66000b87591d69d/attention-2.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8cd9c68c8abfc2561f3f6160b0511614", "sha256": "078bd053e57b37179958cc8ee7e19d9bd9501a6d8d1563a0621f3c248495cafc"}, "downloads": -1, "filename": "attention-2.2.tar.gz", "has_sig": false, "md5_digest": "8cd9c68c8abfc2561f3f6160b0511614", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2701, "upload_time": "2020-03-20T10:06:45", "upload_time_iso_8601": "2020-03-20T10:06:45.306780Z", "url": "https://files.pythonhosted.org/packages/52/b2/a14d6d2e5ecc3a8e0cf45714de5bc26095102b36f3e3a96c1c75d476348a/attention-2.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "fa0871f387f5c291fc725a3e68bc7c08", "sha256": "5e98e7163ae961925eeb9fd1174069b56aa9766722b5da6c14c96d3174c4f863"}, "downloads": -1, "filename": "attention-2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "fa0871f387f5c291fc725a3e68bc7c08", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7187, "upload_time": "2020-03-20T10:06:44", "upload_time_iso_8601": "2020-03-20T10:06:44.014775Z", "url": "https://files.pythonhosted.org/packages/9c/54/f045b9eb9000684fefc8c47ef2fdb5d3497123709191b66000b87591d69d/attention-2.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8cd9c68c8abfc2561f3f6160b0511614", "sha256": "078bd053e57b37179958cc8ee7e19d9bd9501a6d8d1563a0621f3c248495cafc"}, "downloads": -1, "filename": "attention-2.2.tar.gz", "has_sig": false, "md5_digest": "8cd9c68c8abfc2561f3f6160b0511614", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2701, "upload_time": "2020-03-20T10:06:45", "upload_time_iso_8601": "2020-03-20T10:06:45.306780Z", "url": "https://files.pythonhosted.org/packages/52/b2/a14d6d2e5ecc3a8e0cf45714de5bc26095102b36f3e3a96c1c75d476348a/attention-2.2.tar.gz", "yanked": false}], "timestamp": "Thu May  7 18:16:30 2020"}