{"info": {"author": "Robin H.", "author_email": "twizzle@gmx.net", "bugtrack_url": null, "classifiers": [], "description": "# Twizzl - A Multi-Purpose Benchmarking Tool\n\nTwizzl was originally developed to offer an easy to use and flexible benchmarking framework for perceptual image hashing algorithms having different approaches of generating, saving and comparing hashes. But Twizzle is more than that. You can use it to evaluate every algorithm used for the task of content identification like facial recognition, video hashing and many more.\n\n## Basic Idea\n\nThe underlying idea of Twizzle is the usecase of content identification. You have original objects like images, videos, audio files etc. and you compare them to manipulated versions of them. What you wanna know is how good a specific algorithm with different settings or many different algorithms perform at this task.\n\n### Example: Perceptual Image Hashing for Print-Scan usage\n\nThink about the following task: You try to find a perceptual image hashing algorithm that works best for matching images to its printed and scanned selfs. In a later setup you would like to generate a hash for every image that should be identified and save the hash together with metadata like the name or contextual information in a database. Given a printed and scanned image you would generate a hash using the same algorithm and settings and search the database for hashes being close to this one. For binary hashes one would normally use the normalized hamming distance and call it a match if some threshold falls below a certain limit.\n\n<img src=\"_img/content_identification.png\" height=\"300\">\n\nFacing that every algorithm has its own way of hash representation we realized that we have to abstract the task to the following Question: Are two objects (in this case images) the same or not?\n\n<img src=\"_img/content_identification2.png\" height=\"300\">\n \n#### Challenges and Tests\nTwizzl distinguishes between **challenges** and **tests**.\n\nA **challenge** is a set of original objects, a set of comparative objects (both images in our case) and a set of correct decisions that a algorithm under test should make if it works correctly. Additionally it can be enriched with arbitrary metadata like in our example the printer used to create the images or the printer settings used.\n\n<table>\n  <tr>\n    <th>Original</th>\n    <th>Comparative</th>\n    <th>Target Decision</th>\n    <th>Metadata</th>\n  </tr>\n  <tr>\n    <td><img src=\"_img/p1.png\" height=\"100\"></td>\n    <td><img src=\"_img/p1_S.png\" height=\"100\"></td>\n    <td width=\"20%\">True</td>\n    <td width=\"30%\" rowspan=\"3\">\n{\n    \"printer\": \"DC785\",\n    \"toner_settings\": \"toner saving mode\",\n    \"printer_dpi\": 300\n}\n</td>\n  </tr>\n  <tr>\n    <td><img src=\"_img/p2.png\" height=\"100\"></td>\n    <td><img src=\"_img/p2_D.png\" height=\"100\"></td>\n    <td>False</td>\n  </tr>\n  <tr>\n    <td><img src=\"_img/p3.png\" height=\"100\"></td>\n    <td><img src=\"_img/p3_S.png\" height=\"100\"></td>\n    <td>True</td>\n  </tr>\n</table>\n\n**Tests** are runs of a challenge done by one algorithm running with specific settings. A run gets a list of original objects and a list of comparative objects and decides based on the algorithm whether a original-comparative pair is believed to be the same or not.\n\n## Installation\n\n```\npip install twizzle\n```\n\n## Create challenges\n\nTwizzle offeres an easy way to add challenges. Just initiate a new instance of Twizzle. Then create a list of strings describing paths to original objects and one describing pathes to ist comparative objects. Create a third list of booleans coding whether the objects are the same or not. See the basic example in `example_challenge_creator.py`.\n\n```python\nfrom twizzle import Twizzle\n\nsDBPath = \"test.db\"\ntw = Twizzle(sDBPath)\n\nsChallengeName = \"image_hashing_challenge_print_scan_1\"\n\naOriginals = [\"c1.png\", \"c2.png\", \"c3.png\"]\naComparatives = [\"c1.png\", \"c5.png\", \"c6.png\"]\naTargetDecisions = [True, False, False]\n\ndicMetadata = {\n    \"printer\": \"DC783\",\n    \"paper\": \"recycled paper\",\n    \"print_dpi\": 300\n}\n\ntw.add_challenge(sChallengeName, aOriginals,\n                    aComparatives, aTargetDecisions, dicMetadata)\n```\n\n## Run tests\n\nThe **tests** of Twizzle are like a blind test for the algorithms. A test gives a set of original objects and corresponding comparative objects to a user defined algorithm. This algorithms compares every single original and comparative object pair and decides whether they are the same for it or not. With all decisions for all object pairs of a challenge returned to the Twizzle framework it can compare the decisions with the target decisions for the challenge and calculate the error rate (and accuracy, recall, precision, F1 score, FAR, FRR). Based on the error rate you can compare your algorithm or different configurations of your algorithm with others.\n\nRunning tests and evaluating the performance of your algorithms can take a lot of time. Make sure you used relative paths while defining challenges in order to run the testing part on a server having a lot of computational power.\n\nThe Twizzle Framework offers the user a TestRunner component. Have a look at `example_tests.py` in order to get an idea how to use it. There you can see an example for the image hashing algorithm `dHash`. First of all you have to write a wrapper function for your algorithm that handles to get a list of original and comparative objects (images in this case) as first two arguments. Remember that the two list are list of strings. You can also use the strings to save paths where the objects are saved or to encode the objects directly. Moreover you can set an arbitrary amount of additional named arguments.\n\nThe first step your wrapper function has to do is to load the objects link to in the two list. Then it has to evaluate whether the two objects are the same or not. In this example the `dhash` wrapper has to compare every original to its comparative image and decide whether they are the same or not based on the normalized hamming distance of the two hashes and a threshold. The wrapper function has to return a list of decisions having the same dimension like the list of original and comparative images. Additionally it can return a dictionary filled with arbitrary metadata. This metadata will be saved along with the results of the test.\n\n```python\ndef test_dhash(aOriginalImages, aComparativeImages, lThreshold=0.15, lHashSize=16):\n    from twizzle.hashalgos_preset import dhash\n\n    # create dictionary of metadata\n    dicMetadata = {\"algorithm\": \"dhash\",\n                   \"hash_size\": lHashSize, \"threshold\": lThreshold}\n\n    # compare every image\n    aDecisions = []\n    for i, aOriginalImagePath in enumerate(aOriginalImages):\n        aComparativeImagePath = aComparativeImages[i]\n\n        # get images from path\n        aOriginalImage = load_image(aOriginalImagePath)\n        aComparativeImage = load_image(aComparativeImagePath)\n\n        # calculate hashes\n        aHashOriginal = dhash(aOriginalImage, hash_size=lHashSize)\n        aHashComparative = dhash(aComparativeImage, hash_size=lHashSize)\n\n        # calculate deviation\n        dDeviation = hamming_distance(aHashComparative, aHashOriginal)\n\n        # make decision\n        bDecision = False\n        if(dDeviation <= lThreshold):\n            # images are considered to be the same\n            bDecision = True\n\n        # push decision to array of decisions\n        aDecisions.append(bDecision)\n\n    # return decision and dictionary of metadata\n    return aDecisions, dicMetadata\n```\n\nHaving this wrapper around `dHash` we now can test the performance of different configurations of the algorithm. First of all create a instance of the `TestRunner` and define the number of threads it should use:\n\n```python\n    oRunner = TestRunner(lNrOfThreads=NR_OF_THREADS)\n```\n\nNow we can create some useful values for the two variable arguments and add single test for all of them to the `TestRunner` instance. The first parameter of `run_test_async` is the name of the challenge. The second is a pointer to the wrapper function defined before and the last is a dictionary define all further named arguments for the wrapper function.\n\n```python\n # iterate over thresholds\n    for lThreshold in np.arange(0.05, 0.5, 0.05):\n        # iterate over hash sizes\n        for lHashSize in [8, 16, 32]:\n            # add test to testrunner\n            oRunner.run_test_async(\"image_hashing_challenge_print_scan_1\",  test_dhash, {\n                                   \"lThreshold\": lThreshold, \"lHashSize\": lHashSize})\n```\n\nThe test will be executed as fast as a CPU is available to execute the thread. To ensure your script does not exit before all tests are done call the `wait_till_tests_finished()` function to wait for all threads being finished.\n\n```python\n    oRunner.wait_till_tests_finished()\n```\n\n## Analyze data\n\nAfter all your test are done you can get the database from the server and analyze the data. Twizzle supplies you with an `AnalysisDataGenerator` component. It will collect and merge all tests and the corresponding challenges and give you a [pandas](https://pandas.pydata.org/) dataframe. Have a look at `example_analyser.py` to get an idea how to use the component.\n\n`AnalysisDataGenerator` provides you with the `get_pandas_dataframe()` function to get all data as pandas dataframe. Additionally you can save all data to a `csv` file by calling `save_pandas_dataframe_to_file(sPathToFile)`.\n\n## MISC:\n\nTwizzl offers many utils and predefined manipulation functions for the test of perceptual image hashing. Read the corresponding documentation of the [Challenge Creator script](CC_PIH.md)", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/twizzle-code/twizzle", "keywords": "", "license": "GNU GPLv3", "maintainer": "", "maintainer_email": "", "name": "twizzle", "package_url": "https://pypi.org/project/twizzle/", "platform": "Linux", "project_url": "https://pypi.org/project/twizzle/", "project_urls": {"Homepage": "https://github.com/twizzle-code/twizzle"}, "release_url": "https://pypi.org/project/twizzle/0.1.1/", "requires_dist": null, "requires_python": "", "summary": "A multi-purpose benchmarking framework", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Twizzl - A Multi-Purpose Benchmarking Tool</h1>\n<p>Twizzl was originally developed to offer an easy to use and flexible benchmarking framework for perceptual image hashing algorithms having different approaches of generating, saving and comparing hashes. But Twizzle is more than that. You can use it to evaluate every algorithm used for the task of content identification like facial recognition, video hashing and many more.</p>\n<h2>Basic Idea</h2>\n<p>The underlying idea of Twizzle is the usecase of content identification. You have original objects like images, videos, audio files etc. and you compare them to manipulated versions of them. What you wanna know is how good a specific algorithm with different settings or many different algorithms perform at this task.</p>\n<h3>Example: Perceptual Image Hashing for Print-Scan usage</h3>\n<p>Think about the following task: You try to find a perceptual image hashing algorithm that works best for matching images to its printed and scanned selfs. In a later setup you would like to generate a hash for every image that should be identified and save the hash together with metadata like the name or contextual information in a database. Given a printed and scanned image you would generate a hash using the same algorithm and settings and search the database for hashes being close to this one. For binary hashes one would normally use the normalized hamming distance and call it a match if some threshold falls below a certain limit.</p>\n<img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/18fe690689f3b8038f8d8fb0c9c40c27e34e50b5/5f696d672f636f6e74656e745f6964656e74696669636174696f6e2e706e67\">\n<p>Facing that every algorithm has its own way of hash representation we realized that we have to abstract the task to the following Question: Are two objects (in this case images) the same or not?</p>\n<img height=\"300\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/633d868266db6570cc71a2faba45e21afe02ee9e/5f696d672f636f6e74656e745f6964656e74696669636174696f6e322e706e67\">\n<h4>Challenges and Tests</h4>\n<p>Twizzl distinguishes between <strong>challenges</strong> and <strong>tests</strong>.</p>\n<p>A <strong>challenge</strong> is a set of original objects, a set of comparative objects (both images in our case) and a set of correct decisions that a algorithm under test should make if it works correctly. Additionally it can be enriched with arbitrary metadata like in our example the printer used to create the images or the printer settings used.</p>\n<table>\n  <tr>\n    <th>Original</th>\n    <th>Comparative</th>\n    <th>Target Decision</th>\n    <th>Metadata</th>\n  </tr>\n  <tr>\n    <td><img height=\"100\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f971694d94218655e96bc4600c0ab6f62bc84c1c/5f696d672f70312e706e67\"></td>\n    <td><img height=\"100\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ec36c77995f918c693bc0e92795598fc46b83043/5f696d672f70315f532e706e67\"></td>\n    <td>True</td>\n    <td>\n{\n    \"printer\": \"DC785\",\n    \"toner_settings\": \"toner saving mode\",\n    \"printer_dpi\": 300\n}\n</td>\n  </tr>\n  <tr>\n    <td><img height=\"100\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4c123ac9919a0fe5cfd075bb314c13e751830995/5f696d672f70322e706e67\"></td>\n    <td><img height=\"100\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/274469e2b1f715be17f2a643a5dabbd08088658f/5f696d672f70325f442e706e67\"></td>\n    <td>False</td>\n  </tr>\n  <tr>\n    <td><img height=\"100\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/89e4cb1bf2788f4575fad73f72a63126372dc5ae/5f696d672f70332e706e67\"></td>\n    <td><img height=\"100\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0b9f2424ef5dd384d37e6dff416209bd914dc989/5f696d672f70335f532e706e67\"></td>\n    <td>True</td>\n  </tr>\n</table>\n<p><strong>Tests</strong> are runs of a challenge done by one algorithm running with specific settings. A run gets a list of original objects and a list of comparative objects and decides based on the algorithm whether a original-comparative pair is believed to be the same or not.</p>\n<h2>Installation</h2>\n<pre><code>pip install twizzle\n</code></pre>\n<h2>Create challenges</h2>\n<p>Twizzle offeres an easy way to add challenges. Just initiate a new instance of Twizzle. Then create a list of strings describing paths to original objects and one describing pathes to ist comparative objects. Create a third list of booleans coding whether the objects are the same or not. See the basic example in <code>example_challenge_creator.py</code>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">twizzle</span> <span class=\"kn\">import</span> <span class=\"n\">Twizzle</span>\n\n<span class=\"n\">sDBPath</span> <span class=\"o\">=</span> <span class=\"s2\">\"test.db\"</span>\n<span class=\"n\">tw</span> <span class=\"o\">=</span> <span class=\"n\">Twizzle</span><span class=\"p\">(</span><span class=\"n\">sDBPath</span><span class=\"p\">)</span>\n\n<span class=\"n\">sChallengeName</span> <span class=\"o\">=</span> <span class=\"s2\">\"image_hashing_challenge_print_scan_1\"</span>\n\n<span class=\"n\">aOriginals</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"c1.png\"</span><span class=\"p\">,</span> <span class=\"s2\">\"c2.png\"</span><span class=\"p\">,</span> <span class=\"s2\">\"c3.png\"</span><span class=\"p\">]</span>\n<span class=\"n\">aComparatives</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"c1.png\"</span><span class=\"p\">,</span> <span class=\"s2\">\"c5.png\"</span><span class=\"p\">,</span> <span class=\"s2\">\"c6.png\"</span><span class=\"p\">]</span>\n<span class=\"n\">aTargetDecisions</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"kc\">False</span><span class=\"p\">]</span>\n\n<span class=\"n\">dicMetadata</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">\"printer\"</span><span class=\"p\">:</span> <span class=\"s2\">\"DC783\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"paper\"</span><span class=\"p\">:</span> <span class=\"s2\">\"recycled paper\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"print_dpi\"</span><span class=\"p\">:</span> <span class=\"mi\">300</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">tw</span><span class=\"o\">.</span><span class=\"n\">add_challenge</span><span class=\"p\">(</span><span class=\"n\">sChallengeName</span><span class=\"p\">,</span> <span class=\"n\">aOriginals</span><span class=\"p\">,</span>\n                    <span class=\"n\">aComparatives</span><span class=\"p\">,</span> <span class=\"n\">aTargetDecisions</span><span class=\"p\">,</span> <span class=\"n\">dicMetadata</span><span class=\"p\">)</span>\n</pre>\n<h2>Run tests</h2>\n<p>The <strong>tests</strong> of Twizzle are like a blind test for the algorithms. A test gives a set of original objects and corresponding comparative objects to a user defined algorithm. This algorithms compares every single original and comparative object pair and decides whether they are the same for it or not. With all decisions for all object pairs of a challenge returned to the Twizzle framework it can compare the decisions with the target decisions for the challenge and calculate the error rate (and accuracy, recall, precision, F1 score, FAR, FRR). Based on the error rate you can compare your algorithm or different configurations of your algorithm with others.</p>\n<p>Running tests and evaluating the performance of your algorithms can take a lot of time. Make sure you used relative paths while defining challenges in order to run the testing part on a server having a lot of computational power.</p>\n<p>The Twizzle Framework offers the user a TestRunner component. Have a look at <code>example_tests.py</code> in order to get an idea how to use it. There you can see an example for the image hashing algorithm <code>dHash</code>. First of all you have to write a wrapper function for your algorithm that handles to get a list of original and comparative objects (images in this case) as first two arguments. Remember that the two list are list of strings. You can also use the strings to save paths where the objects are saved or to encode the objects directly. Moreover you can set an arbitrary amount of additional named arguments.</p>\n<p>The first step your wrapper function has to do is to load the objects link to in the two list. Then it has to evaluate whether the two objects are the same or not. In this example the <code>dhash</code> wrapper has to compare every original to its comparative image and decide whether they are the same or not based on the normalized hamming distance of the two hashes and a threshold. The wrapper function has to return a list of decisions having the same dimension like the list of original and comparative images. Additionally it can return a dictionary filled with arbitrary metadata. This metadata will be saved along with the results of the test.</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">test_dhash</span><span class=\"p\">(</span><span class=\"n\">aOriginalImages</span><span class=\"p\">,</span> <span class=\"n\">aComparativeImages</span><span class=\"p\">,</span> <span class=\"n\">lThreshold</span><span class=\"o\">=</span><span class=\"mf\">0.15</span><span class=\"p\">,</span> <span class=\"n\">lHashSize</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">):</span>\n    <span class=\"kn\">from</span> <span class=\"nn\">twizzle.hashalgos_preset</span> <span class=\"kn\">import</span> <span class=\"n\">dhash</span>\n\n    <span class=\"c1\"># create dictionary of metadata</span>\n    <span class=\"n\">dicMetadata</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"algorithm\"</span><span class=\"p\">:</span> <span class=\"s2\">\"dhash\"</span><span class=\"p\">,</span>\n                   <span class=\"s2\">\"hash_size\"</span><span class=\"p\">:</span> <span class=\"n\">lHashSize</span><span class=\"p\">,</span> <span class=\"s2\">\"threshold\"</span><span class=\"p\">:</span> <span class=\"n\">lThreshold</span><span class=\"p\">}</span>\n\n    <span class=\"c1\"># compare every image</span>\n    <span class=\"n\">aDecisions</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">aOriginalImagePath</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">aOriginalImages</span><span class=\"p\">):</span>\n        <span class=\"n\">aComparativeImagePath</span> <span class=\"o\">=</span> <span class=\"n\">aComparativeImages</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n\n        <span class=\"c1\"># get images from path</span>\n        <span class=\"n\">aOriginalImage</span> <span class=\"o\">=</span> <span class=\"n\">load_image</span><span class=\"p\">(</span><span class=\"n\">aOriginalImagePath</span><span class=\"p\">)</span>\n        <span class=\"n\">aComparativeImage</span> <span class=\"o\">=</span> <span class=\"n\">load_image</span><span class=\"p\">(</span><span class=\"n\">aComparativeImagePath</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># calculate hashes</span>\n        <span class=\"n\">aHashOriginal</span> <span class=\"o\">=</span> <span class=\"n\">dhash</span><span class=\"p\">(</span><span class=\"n\">aOriginalImage</span><span class=\"p\">,</span> <span class=\"n\">hash_size</span><span class=\"o\">=</span><span class=\"n\">lHashSize</span><span class=\"p\">)</span>\n        <span class=\"n\">aHashComparative</span> <span class=\"o\">=</span> <span class=\"n\">dhash</span><span class=\"p\">(</span><span class=\"n\">aComparativeImage</span><span class=\"p\">,</span> <span class=\"n\">hash_size</span><span class=\"o\">=</span><span class=\"n\">lHashSize</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># calculate deviation</span>\n        <span class=\"n\">dDeviation</span> <span class=\"o\">=</span> <span class=\"n\">hamming_distance</span><span class=\"p\">(</span><span class=\"n\">aHashComparative</span><span class=\"p\">,</span> <span class=\"n\">aHashOriginal</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># make decision</span>\n        <span class=\"n\">bDecision</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n        <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"n\">dDeviation</span> <span class=\"o\">&lt;=</span> <span class=\"n\">lThreshold</span><span class=\"p\">):</span>\n            <span class=\"c1\"># images are considered to be the same</span>\n            <span class=\"n\">bDecision</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n\n        <span class=\"c1\"># push decision to array of decisions</span>\n        <span class=\"n\">aDecisions</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">bDecision</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># return decision and dictionary of metadata</span>\n    <span class=\"k\">return</span> <span class=\"n\">aDecisions</span><span class=\"p\">,</span> <span class=\"n\">dicMetadata</span>\n</pre>\n<p>Having this wrapper around <code>dHash</code> we now can test the performance of different configurations of the algorithm. First of all create a instance of the <code>TestRunner</code> and define the number of threads it should use:</p>\n<pre>    <span class=\"n\">oRunner</span> <span class=\"o\">=</span> <span class=\"n\">TestRunner</span><span class=\"p\">(</span><span class=\"n\">lNrOfThreads</span><span class=\"o\">=</span><span class=\"n\">NR_OF_THREADS</span><span class=\"p\">)</span>\n</pre>\n<p>Now we can create some useful values for the two variable arguments and add single test for all of them to the <code>TestRunner</code> instance. The first parameter of <code>run_test_async</code> is the name of the challenge. The second is a pointer to the wrapper function defined before and the last is a dictionary define all further named arguments for the wrapper function.</p>\n<pre> <span class=\"c1\"># iterate over thresholds</span>\n    <span class=\"k\">for</span> <span class=\"n\">lThreshold</span> <span class=\"ow\">in</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mf\">0.05</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.05</span><span class=\"p\">):</span>\n        <span class=\"c1\"># iterate over hash sizes</span>\n        <span class=\"k\">for</span> <span class=\"n\">lHashSize</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">]:</span>\n            <span class=\"c1\"># add test to testrunner</span>\n            <span class=\"n\">oRunner</span><span class=\"o\">.</span><span class=\"n\">run_test_async</span><span class=\"p\">(</span><span class=\"s2\">\"image_hashing_challenge_print_scan_1\"</span><span class=\"p\">,</span>  <span class=\"n\">test_dhash</span><span class=\"p\">,</span> <span class=\"p\">{</span>\n                                   <span class=\"s2\">\"lThreshold\"</span><span class=\"p\">:</span> <span class=\"n\">lThreshold</span><span class=\"p\">,</span> <span class=\"s2\">\"lHashSize\"</span><span class=\"p\">:</span> <span class=\"n\">lHashSize</span><span class=\"p\">})</span>\n</pre>\n<p>The test will be executed as fast as a CPU is available to execute the thread. To ensure your script does not exit before all tests are done call the <code>wait_till_tests_finished()</code> function to wait for all threads being finished.</p>\n<pre>    <span class=\"n\">oRunner</span><span class=\"o\">.</span><span class=\"n\">wait_till_tests_finished</span><span class=\"p\">()</span>\n</pre>\n<h2>Analyze data</h2>\n<p>After all your test are done you can get the database from the server and analyze the data. Twizzle supplies you with an <code>AnalysisDataGenerator</code> component. It will collect and merge all tests and the corresponding challenges and give you a <a href=\"https://pandas.pydata.org/\" rel=\"nofollow\">pandas</a> dataframe. Have a look at <code>example_analyser.py</code> to get an idea how to use the component.</p>\n<p><code>AnalysisDataGenerator</code> provides you with the <code>get_pandas_dataframe()</code> function to get all data as pandas dataframe. Additionally you can save all data to a <code>csv</code> file by calling <code>save_pandas_dataframe_to_file(sPathToFile)</code>.</p>\n<h2>MISC:</h2>\n<p>Twizzl offers many utils and predefined manipulation functions for the test of perceptual image hashing. Read the corresponding documentation of the <a href=\"CC_PIH.md\" rel=\"nofollow\">Challenge Creator script</a></p>\n\n          </div>"}, "last_serial": 5443294, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "a66c23c9d713e9921a166f1c8c6d221d", "sha256": "9660c69247aaaaa3ddaf6636aebee8fe445b9f1b3fcf461d413cfe1d6ee2b95c"}, "downloads": -1, "filename": "twizzle-0.1.1.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "a66c23c9d713e9921a166f1c8c6d221d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26758, "upload_time": "2019-06-25T00:55:47", "upload_time_iso_8601": "2019-06-25T00:55:47.135772Z", "url": "https://files.pythonhosted.org/packages/88/6c/dae36f258519fbd8b70261131358ac94f553cfa1d2564698b9402def2b2c/twizzle-0.1.1.linux-x86_64.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a66c23c9d713e9921a166f1c8c6d221d", "sha256": "9660c69247aaaaa3ddaf6636aebee8fe445b9f1b3fcf461d413cfe1d6ee2b95c"}, "downloads": -1, "filename": "twizzle-0.1.1.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "a66c23c9d713e9921a166f1c8c6d221d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26758, "upload_time": "2019-06-25T00:55:47", "upload_time_iso_8601": "2019-06-25T00:55:47.135772Z", "url": "https://files.pythonhosted.org/packages/88/6c/dae36f258519fbd8b70261131358ac94f553cfa1d2564698b9402def2b2c/twizzle-0.1.1.linux-x86_64.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:43:31 2020"}