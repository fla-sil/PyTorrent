{"info": {"author": "Nikolai Tschacher", "author_email": "nikolai@tschacher.ch", "bugtrack_url": null, "classifiers": [], "description": "# GoogleScraper - Scraping search engines professionally\n\n## New News - August 2018\n\nFor questions you can [contact me on my wegpage](https://incolumitas.com/) and write me an email there.\n\nThis project is back to live after two years of abandonment. In the coming weeks, I will take some time to update all functionality to the most recent developments. This encompasses updating all Regexes and changes in search engine behavior. After a couple of weeks, you can expect this project to work again as documented here.\n\n#### Goals\n\n+ Remove PhantomJS support **[Done]**\n+ Maybe remove Firefox support **[Done: Decision was to keep Firefox]**\n+ Think about using https://github.com/GoogleChrome/puppeteer instead of selenium for Chrome.\n+ Test all supported search engines [Partly Done]\n\n### Table of Contents\n\n1. [Installation](#install)\n2. [Quick Start](#quick)\n3. [Asynchronous mode](#async)\n4. [Testing](#testing)\n5. [About](#about)\n6. [Command line usage](#cli-usage)\n7. [Contact](#contact)\n\n\n<a name=\"install\" />\n\n## Installation\n\nGoogleScraper is written in Python 3. You should install at least Python 3.6. The last major development was all done with Python 3.7. So when using Ubuntu 16.04 and Python 3.7 for instance, please install Python 3 from the official packages. I use the [Anaconda Python distribution](https://anaconda.org/anaconda/python), which does work very well for me.\n\nFurthermore, you need to install the Chrome Browser and also the ChromeDriver for Selenium mode. Alternatively install the Firefox Browser and the geckodriver for Selenium Mode. See instructions below.\n\nYou can also install GoogleScraper comfortably with pip:\n\n```\nvirtualenv --python python3 env\nsource env/bin/activate\npip install GoogleScraper\n```\n\nRight now (September 2018) this is discouraged. Please install from latest Github sources.\n\n### Alternatively install directly from Github\n\nSometimes the newest and most awesome stuff is not available in the cheeseshop (That's how they call\nhttps://pypi.python.org/pypi/pip). Therefore you maybe want to install GoogleScraper from the latest source that resides in this Github repository. You can do so like this:\n\n```\nvirtualenv --python python3 env\nsource env/bin/activate\npip install git+git://github.com/NikolaiT/GoogleScraper/\n```\n\nPlease note that some features and examples might not work as expected. I also don't guarantee that\nthe app even runs. I only guarantee (to a certain degree at least) that installing from pip will yield a\nusable version.\n\n\n### Chromedriver\n\nDownload the latest chromedriver from here: https://sites.google.com/a/chromium.org/chromedriver/downloads\n\nUnzip the driver and save it somewhere and then update the `chromedriver_path` in the GoogleScraper configuration file `scrape_config.py` to the path where you saved the driver `chromedriver_path = 'Drivers/chromedriver'`\n\n\n### Geckodriver\n\nDownload the latest geckodriver from here: https://github.com/mozilla/geckodriver/releases\n\nUnzip the driver and save it somewhere and then update the `geckodriver_path` in the GoogleScraper configuration file `scrape_config.py` to the path where you saved the driver `geckodriver_path = 'Drivers/geckodriver'`\n\n### Update the settings for selenium and firefox/chrome\n\nUpdate the following settings in the GoogleScraper configuration file `scrape_config.py` to your values.\n\n```\n# chrome driver executable path\n# get chrome drivers here: https://chromedriver.storage.googleapis.com/index.html?path=2.41/\nchromedriver_path = 'Drivers/chromedriver'\n\n# geckodriver executable path\n# get gecko drivers here: https://github.com/mozilla/geckodriver/releases\ngeckodriver_path = 'Drivers/geckodriver'\n\n# path to firefox binary\nfirefox_binary_path = '/home/nikolai/firefox/firefox'\n\n# path to chromium browser binary\nchrome_binary_path = '/usr/bin/chromium-browser'\n```\n\n\n<a name=\"quick\" />\n\n## Quick Start\n\nInstall as described above. Make sure that you have the selenium drivers for chrome/firefox if you want to use GoogleScraper in selenium mode.\n\nSee all options\n```\nGoogleScraper -h\n```\n\nScrape the single keyword \"apple\" with http mode:\n```\nGoogleScraper -m http --keyword \"apple\" -v info\n```\n\nScrape all keywords that are in the file `SearchData/5words` in selenium mode using chrome in headless mode:\n```\nGoogleScraper -m selenium --sel-browser chrome --browser-mode headless --keyword-file SearchData/5words -v info\n```\n\nScrape all keywords that are in\n+ keywords.txt\n+ with http mode\n+ using 5 threads\n+ scrape in the search engines bing and yahoo\n+ store the output in a JSON file\n+ increase verbosity to the debug level\n```\nGoogleScraper -m http --keyword-file SearchData/some_words.txt --num-workers 5 --search-engines \"bing,yahoo\" --output-filename threaded-results.json -v debug\n```\n\nDo an image search for the keyword \"K2 mountain\" on google:\n\n```\nGoogleScraper -s \"google\" -q \"K2 mountain\" -t image -v info\n```\n\n<a name=\"async\" />\n\n## Asynchronous mode\n\nThis is probably the most awesome feature of GoogleScraper. You can scrape with thousands of requests per second if either\n\n+ The search engine doesn't block you (Bing didn't block me when requesting **100 keywords / second**)\n+ You have enough proxies\n\nExample for Asynchronous mode:\n\nSearch the keywords in the keyword file [SearchData/marketing-models-brands.txt](SearchData/marketing-models-brands.txt) on bing and yahoo. By default asynchronous mode spawns 100 requests at the same time. This means around 100 requests per second (depends on the actual connection...).\n\n```\nGoogleScraper -s \"bing,yahoo\" --keyword-file SearchData/marketing-models-brands.txt -m http-async -v info -o marketing.json\n```\n\nThe results (partial results, because there were too many keywords for one IP address) can be inspected in the file [Outputs/marketing.json](Outputs/marketing.json).\n\n\n<a name=\"testing\" />\n\n## Testing GoogleScraper\n\nGoogleScraper is hugely complex. Because GoogleScraper supports many search engines and the HTML and Javascript of those Search Providers changes frequently, it is often the case that GoogleScraper ceases to function for some search engine. To spot this, you can run **functional tests**.\n\nFor example the test below runs a scraping session for Google and Bing and tests that the gathered data looks more or less okay.\n\n```\npython -m pytest Tests/functional_tests.py::GoogleScraperMinimalFunctionalTestCase\n```\n\n\n<a name=\"about\" />\n\n## What does GoogleScraper.py?\n\nGoogleScraper parses Google search engine results (and many other search engines *_*) easily and in a fast way. It allows you to extract all found\nlinks and their titles and descriptions programmatically which enables you to process scraped data further.\n\nThere are unlimited *usage scenarios*:\n\n+ Quickly harvest masses of [google dorks][1].\n+ Use it as a SEO tool.\n+ Discover trends.\n+ Compile lists of sites to feed your own database.\n+ Many more use cases...\n+ Quite easily extendable since the code is well documented\n\nFirst of all you need to understand that GoogleScraper uses **two completely different scraping approaches**:\n+ Scraping with low level http libraries such as `urllib.request` or `requests` modules. This simulates the http packets sent by real browsers.\n+ Scrape by controlling a real browser with the selenium framework\n\nWhereas the former approach was implemented first, the later approach looks much more promising in comparison, because\nsearch engines have no easy way detecting it.\n\nGoogleScraper is implemented with the following techniques/software:\n\n+ Written in Python 3.7\n+ Uses multithreading/asynchronous IO.\n+ Supports parallel scraping with multiple IP addresses.\n+ Provides proxy support using [socksipy][2] and built in browser proxies:\n  * Socks5\n  * Socks4\n  * HttpProxy\n+ Support for alternative search modes like news/image/video search.\n\n### What search engines are suppported ?\nCurrently the following search engines are supported:\n+ Google\n+ Bing\n+ Yahoo\n+ Yandex\n+ Baidu\n+ Duckduckgo\n\n### How does GoogleScraper maximize the amount of extracted information per IP address?\n\nScraping is a critical and highly complex subject. Google and other search engine giants have a strong inclination\nto make the scrapers life as hard as possible. There are several ways for the search engine providers to detect that a robot is using\ntheir search engine:\n\n+ The User-Agent is not one of a browser.\n+ The search params are not identical to the ones that browser used by a human sets:\n  * Javascript generates challenges dynamically on the client side. This might include heuristics that try to detect human behaviour. Example: Only humans move their mouses and hover over the interesting search results.\n+ Robots have a strict requests pattern (very fast requests, without a random time between the sent packets).\n+ Dorks are heavily used\n+ No pictures/ads/css/javascript are loaded (like a browser does normally) which in turn won't trigger certain javascript events\n\nSo the biggest hurdle to tackle is the javascript detection algorithms. I don't know what Google does in their javascript, but I will soon investigate it further and then decide if it's not better to change strategies and\nswitch to a **approach that scrapes by simulating browsers in a browserlike environment** that can execute javascript. The networking of each of these virtual browsers is proxified and manipulated such that it behaves like\na real physical user agent. I am pretty sure that it must be possible to handle 20 such browser sessions in a parallel way without stressing resources too much. The real problem is as always the lack of good proxies...\n\n### How to overcome difficulties of low level (http) scraping?\n\nAs mentioned above, there are several drawbacks when scraping with `urllib.request` or `requests` modules and doing the networking on my own:\n\nBrowsers are ENORMOUSLY complex software systems. Chrome has around 8 millions line of code and firefox even 10 LOC. Huge companies invest a lot of money to push technology forward (HTML5, CSS3, new standards) and each browser\nhas a unique behaviour. Therefore it's almost impossible to simulate such a browser manually with HTTP requests.  This means Google has numerous ways to detect anomalies and inconsistencies in the browsing usage. Alone the\ndynamic nature of Javascript makes it impossible to scrape undetected.\n\nThis cries for an alternative approach, that automates a **real** browser with Python. Best would be to control the Chrome browser since Google has the least incentives to restrict capabilities for their own native browser.\nHence I need a way to automate Chrome with Python and controlling several independent instances with different proxies set. Then the output of result grows linearly with the number of used proxies...\n\nSome interesting technologies/software to do so:\n+ [Selenium](https://pypi.python.org/pypi/selenium)\n+ [Mechanize](http://wwwsearch.sourceforge.net/mechanize/)\n\n\n<a name=\"cli-usage\" />\n\n## More detailed Explanation\n\nProbably the best way to use GoogleScraper is to use it from the command line and fire a command such as\nthe following:\n```\nGoogleScraper --keyword-file /tmp/keywords --search-engine bing --num-pages-for-keyword 3 --scrape-method selenium\n```\n\nHere *sel* marks the scraping mode as 'selenium'. This means GoogleScraper.py scrapes with real browsers. This is pretty powerful, since\nyou can scrape long and a lot of sites (Google has a hard time blocking real browsers). The argument of the flag `--keyword-file` must be a file with keywords separated by\nnewlines. So: For every google query one line. Easy, isnt' it?\n\nFurthermore, the option `--num-pages-for-keyword` means that GoogleScraper will fetch 3 consecutive pages for each keyword.\n\nExample keyword-file:\n```\nkeyword number one\nhow to become a good rapper\ninurl:\"index.php?sl=43\"\nfiletype:.cfg\nallintext:\"You have a Mysql Error in your\"\nintitle:\"admin config\"\nBest brothels in atlanta\n```\n\nAfter the scraping you'll automatically have a new sqlite3 database in the named `google_scraper.db` in the same directory. You can open and inspect the database with the command:\n```\nGoogleScraper --shell\n```\n\nIt shouldn't be a problem to scrape **_10'000 keywords in 2 hours_**. If you are really crazy, set the maximal browsers in the config a little\nbit higher (in the top of the script file).\n\nIf you want, you can specify the flag `--proxy-file`. As argument you need to pass a file with proxies in it and with the following format:\n\n```\nprotocol proxyhost:proxyport username:password\n(...)\n```\nExample:\n```\nsocks5 127.0.0.1:1080 blabla:12345\nsocks4 77.66.55.44:9999 elite:js@fkVA3(Va3)\n```\n\nIn case you want to use GoogleScraper.py in *http* mode (which means that raw http headers are sent), use it as follows:\n\n```\nGoogleScraper -m http -p 1 -n 25 -q \"white light\"\n```\n\n<a name=\"contact\" />\n\n## Contact\n\nIf you feel like contacting me, do so and send me a mail. You can find my contact information on my [blog][3].\n\n[1]: http://www.webvivant.com/google-hacking.html \"Google Dorks\"\n[2]: https://code.google.com/p/socksipy-branch/ \"Socksipy Branch\"\n[3]: http://incolumitas.com/about/contact/ \"Contact with author\"\n[4]: http://incolumitas.com/2013/01/06/googlesearch-a-rapid-python-class-to-get-search-results/\n[5]: http://incolumitas.com/2014/11/12/scraping-and-extracting-links-from-any-major-search-engine-like-google-yandex-baidu-bing-and-duckduckgo/", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://incolumitas.com", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "GoogleScraper", "package_url": "https://pypi.org/project/GoogleScraper/", "platform": "", "project_url": "https://pypi.org/project/GoogleScraper/", "project_urls": {"Homepage": "http://incolumitas.com"}, "release_url": "https://pypi.org/project/GoogleScraper/0.2.4/", "requires_dist": null, "requires_python": "", "summary": "A module to scrape and extract links, titles and descriptions from various search engines. Supports google,bing,yandex and many more.", "version": "0.2.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>GoogleScraper - Scraping search engines professionally</h1>\n<h2>New News - August 2018</h2>\n<p>For questions you can <a href=\"https://incolumitas.com/\" rel=\"nofollow\">contact me on my wegpage</a> and write me an email there.</p>\n<p>This project is back to live after two years of abandonment. In the coming weeks, I will take some time to update all functionality to the most recent developments. This encompasses updating all Regexes and changes in search engine behavior. After a couple of weeks, you can expect this project to work again as documented here.</p>\n<h4>Goals</h4>\n<ul>\n<li>Remove PhantomJS support <strong>[Done]</strong></li>\n<li>Maybe remove Firefox support <strong>[Done: Decision was to keep Firefox]</strong></li>\n<li>Think about using <a href=\"https://github.com/GoogleChrome/puppeteer\" rel=\"nofollow\">https://github.com/GoogleChrome/puppeteer</a> instead of selenium for Chrome.</li>\n<li>Test all supported search engines [Partly Done]</li>\n</ul>\n<h3>Table of Contents</h3>\n<ol>\n<li><a href=\"#install\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#quick\" rel=\"nofollow\">Quick Start</a></li>\n<li><a href=\"#async\" rel=\"nofollow\">Asynchronous mode</a></li>\n<li><a href=\"#testing\" rel=\"nofollow\">Testing</a></li>\n<li><a href=\"#about\" rel=\"nofollow\">About</a></li>\n<li><a href=\"#cli-usage\" rel=\"nofollow\">Command line usage</a></li>\n<li><a href=\"#contact\" rel=\"nofollow\">Contact</a></li>\n</ol>\n<a>\n<h2>Installation</h2>\n</a><p><a>GoogleScraper is written in Python 3. You should install at least Python 3.6. The last major development was all done with Python 3.7. So when using Ubuntu 16.04 and Python 3.7 for instance, please install Python 3 from the official packages. I use the </a><a href=\"https://anaconda.org/anaconda/python\" rel=\"nofollow\">Anaconda Python distribution</a>, which does work very well for me.</p>\n<p>Furthermore, you need to install the Chrome Browser and also the ChromeDriver for Selenium mode. Alternatively install the Firefox Browser and the geckodriver for Selenium Mode. See instructions below.</p>\n<p>You can also install GoogleScraper comfortably with pip:</p>\n<pre><code>virtualenv --python python3 env\nsource env/bin/activate\npip install GoogleScraper\n</code></pre>\n<p>Right now (September 2018) this is discouraged. Please install from latest Github sources.</p>\n<h3>Alternatively install directly from Github</h3>\n<p>Sometimes the newest and most awesome stuff is not available in the cheeseshop (That's how they call\n<a href=\"https://pypi.python.org/pypi/pip\" rel=\"nofollow\">https://pypi.python.org/pypi/pip</a>). Therefore you maybe want to install GoogleScraper from the latest source that resides in this Github repository. You can do so like this:</p>\n<pre><code>virtualenv --python python3 env\nsource env/bin/activate\npip install git+git://github.com/NikolaiT/GoogleScraper/\n</code></pre>\n<p>Please note that some features and examples might not work as expected. I also don't guarantee that\nthe app even runs. I only guarantee (to a certain degree at least) that installing from pip will yield a\nusable version.</p>\n<h3>Chromedriver</h3>\n<p>Download the latest chromedriver from here: <a href=\"https://sites.google.com/a/chromium.org/chromedriver/downloads\" rel=\"nofollow\">https://sites.google.com/a/chromium.org/chromedriver/downloads</a></p>\n<p>Unzip the driver and save it somewhere and then update the <code>chromedriver_path</code> in the GoogleScraper configuration file <code>scrape_config.py</code> to the path where you saved the driver <code>chromedriver_path = 'Drivers/chromedriver'</code></p>\n<h3>Geckodriver</h3>\n<p>Download the latest geckodriver from here: <a href=\"https://github.com/mozilla/geckodriver/releases\" rel=\"nofollow\">https://github.com/mozilla/geckodriver/releases</a></p>\n<p>Unzip the driver and save it somewhere and then update the <code>geckodriver_path</code> in the GoogleScraper configuration file <code>scrape_config.py</code> to the path where you saved the driver <code>geckodriver_path = 'Drivers/geckodriver'</code></p>\n<h3>Update the settings for selenium and firefox/chrome</h3>\n<p>Update the following settings in the GoogleScraper configuration file <code>scrape_config.py</code> to your values.</p>\n<pre><code># chrome driver executable path\n# get chrome drivers here: https://chromedriver.storage.googleapis.com/index.html?path=2.41/\nchromedriver_path = 'Drivers/chromedriver'\n\n# geckodriver executable path\n# get gecko drivers here: https://github.com/mozilla/geckodriver/releases\ngeckodriver_path = 'Drivers/geckodriver'\n\n# path to firefox binary\nfirefox_binary_path = '/home/nikolai/firefox/firefox'\n\n# path to chromium browser binary\nchrome_binary_path = '/usr/bin/chromium-browser'\n</code></pre>\n<a>\n<h2>Quick Start</h2>\n<p>Install as described above. Make sure that you have the selenium drivers for chrome/firefox if you want to use GoogleScraper in selenium mode.</p>\n<p>See all options</p>\n<pre><code>GoogleScraper -h\n</code></pre>\n<p>Scrape the single keyword \"apple\" with http mode:</p>\n<pre><code>GoogleScraper -m http --keyword \"apple\" -v info\n</code></pre>\n<p>Scrape all keywords that are in the file <code>SearchData/5words</code> in selenium mode using chrome in headless mode:</p>\n<pre><code>GoogleScraper -m selenium --sel-browser chrome --browser-mode headless --keyword-file SearchData/5words -v info\n</code></pre>\n<p>Scrape all keywords that are in</p>\n<ul>\n<li>keywords.txt</li>\n<li>with http mode</li>\n<li>using 5 threads</li>\n<li>scrape in the search engines bing and yahoo</li>\n<li>store the output in a JSON file</li>\n<li>increase verbosity to the debug level</li>\n</ul>\n<pre><code>GoogleScraper -m http --keyword-file SearchData/some_words.txt --num-workers 5 --search-engines \"bing,yahoo\" --output-filename threaded-results.json -v debug\n</code></pre>\n<p>Do an image search for the keyword \"K2 mountain\" on google:</p>\n<pre><code>GoogleScraper -s \"google\" -q \"K2 mountain\" -t image -v info\n</code></pre>\n</a><a>\n<h2>Asynchronous mode</h2>\n<p>This is probably the most awesome feature of GoogleScraper. You can scrape with thousands of requests per second if either</p>\n<ul>\n<li>The search engine doesn't block you (Bing didn't block me when requesting <strong>100 keywords / second</strong>)</li>\n<li>You have enough proxies</li>\n</ul>\n<p>Example for Asynchronous mode:</p>\n</a><p><a>Search the keywords in the keyword file </a><a href=\"SearchData/marketing-models-brands.txt\" rel=\"nofollow\">SearchData/marketing-models-brands.txt</a> on bing and yahoo. By default asynchronous mode spawns 100 requests at the same time. This means around 100 requests per second (depends on the actual connection...).</p>\n<pre><code>GoogleScraper -s \"bing,yahoo\" --keyword-file SearchData/marketing-models-brands.txt -m http-async -v info -o marketing.json\n</code></pre>\n<p>The results (partial results, because there were too many keywords for one IP address) can be inspected in the file <a href=\"Outputs/marketing.json\" rel=\"nofollow\">Outputs/marketing.json</a>.</p>\n<a>\n<h2>Testing GoogleScraper</h2>\n<p>GoogleScraper is hugely complex. Because GoogleScraper supports many search engines and the HTML and Javascript of those Search Providers changes frequently, it is often the case that GoogleScraper ceases to function for some search engine. To spot this, you can run <strong>functional tests</strong>.</p>\n<p>For example the test below runs a scraping session for Google and Bing and tests that the gathered data looks more or less okay.</p>\n<pre><code>python -m pytest Tests/functional_tests.py::GoogleScraperMinimalFunctionalTestCase\n</code></pre>\n</a><a>\n<h2>What does GoogleScraper.py?</h2>\n<p>GoogleScraper parses Google search engine results (and many other search engines <em>_</em>) easily and in a fast way. It allows you to extract all found\nlinks and their titles and descriptions programmatically which enables you to process scraped data further.</p>\n<p>There are unlimited <em>usage scenarios</em>:</p>\n</a><ul><a>\n</a><li><a>Quickly harvest masses of </a><a href=\"http://www.webvivant.com/google-hacking.html\" rel=\"nofollow\" title=\"Google Dorks\">google dorks</a>.</li>\n<li>Use it as a SEO tool.</li>\n<li>Discover trends.</li>\n<li>Compile lists of sites to feed your own database.</li>\n<li>Many more use cases...</li>\n<li>Quite easily extendable since the code is well documented</li>\n</ul>\n<p>First of all you need to understand that GoogleScraper uses <strong>two completely different scraping approaches</strong>:</p>\n<ul>\n<li>Scraping with low level http libraries such as <code>urllib.request</code> or <code>requests</code> modules. This simulates the http packets sent by real browsers.</li>\n<li>Scrape by controlling a real browser with the selenium framework</li>\n</ul>\n<p>Whereas the former approach was implemented first, the later approach looks much more promising in comparison, because\nsearch engines have no easy way detecting it.</p>\n<p>GoogleScraper is implemented with the following techniques/software:</p>\n<ul>\n<li>Written in Python 3.7</li>\n<li>Uses multithreading/asynchronous IO.</li>\n<li>Supports parallel scraping with multiple IP addresses.</li>\n<li>Provides proxy support using <a href=\"https://code.google.com/p/socksipy-branch/\" rel=\"nofollow\" title=\"Socksipy Branch\">socksipy</a> and built in browser proxies:\n<ul>\n<li>Socks5</li>\n<li>Socks4</li>\n<li>HttpProxy</li>\n</ul>\n</li>\n<li>Support for alternative search modes like news/image/video search.</li>\n</ul>\n<h3>What search engines are suppported ?</h3>\n<p>Currently the following search engines are supported:</p>\n<ul>\n<li>Google</li>\n<li>Bing</li>\n<li>Yahoo</li>\n<li>Yandex</li>\n<li>Baidu</li>\n<li>Duckduckgo</li>\n</ul>\n<h3>How does GoogleScraper maximize the amount of extracted information per IP address?</h3>\n<p>Scraping is a critical and highly complex subject. Google and other search engine giants have a strong inclination\nto make the scrapers life as hard as possible. There are several ways for the search engine providers to detect that a robot is using\ntheir search engine:</p>\n<ul>\n<li>The User-Agent is not one of a browser.</li>\n<li>The search params are not identical to the ones that browser used by a human sets:\n<ul>\n<li>Javascript generates challenges dynamically on the client side. This might include heuristics that try to detect human behaviour. Example: Only humans move their mouses and hover over the interesting search results.</li>\n</ul>\n</li>\n<li>Robots have a strict requests pattern (very fast requests, without a random time between the sent packets).</li>\n<li>Dorks are heavily used</li>\n<li>No pictures/ads/css/javascript are loaded (like a browser does normally) which in turn won't trigger certain javascript events</li>\n</ul>\n<p>So the biggest hurdle to tackle is the javascript detection algorithms. I don't know what Google does in their javascript, but I will soon investigate it further and then decide if it's not better to change strategies and\nswitch to a <strong>approach that scrapes by simulating browsers in a browserlike environment</strong> that can execute javascript. The networking of each of these virtual browsers is proxified and manipulated such that it behaves like\na real physical user agent. I am pretty sure that it must be possible to handle 20 such browser sessions in a parallel way without stressing resources too much. The real problem is as always the lack of good proxies...</p>\n<h3>How to overcome difficulties of low level (http) scraping?</h3>\n<p>As mentioned above, there are several drawbacks when scraping with <code>urllib.request</code> or <code>requests</code> modules and doing the networking on my own:</p>\n<p>Browsers are ENORMOUSLY complex software systems. Chrome has around 8 millions line of code and firefox even 10 LOC. Huge companies invest a lot of money to push technology forward (HTML5, CSS3, new standards) and each browser\nhas a unique behaviour. Therefore it's almost impossible to simulate such a browser manually with HTTP requests.  This means Google has numerous ways to detect anomalies and inconsistencies in the browsing usage. Alone the\ndynamic nature of Javascript makes it impossible to scrape undetected.</p>\n<p>This cries for an alternative approach, that automates a <strong>real</strong> browser with Python. Best would be to control the Chrome browser since Google has the least incentives to restrict capabilities for their own native browser.\nHence I need a way to automate Chrome with Python and controlling several independent instances with different proxies set. Then the output of result grows linearly with the number of used proxies...</p>\n<p>Some interesting technologies/software to do so:</p>\n<ul>\n<li><a href=\"https://pypi.python.org/pypi/selenium\" rel=\"nofollow\">Selenium</a></li>\n<li><a href=\"http://wwwsearch.sourceforge.net/mechanize/\" rel=\"nofollow\">Mechanize</a></li>\n</ul>\n<a>\n<h2>More detailed Explanation</h2>\n<p>Probably the best way to use GoogleScraper is to use it from the command line and fire a command such as\nthe following:</p>\n<pre><code>GoogleScraper --keyword-file /tmp/keywords --search-engine bing --num-pages-for-keyword 3 --scrape-method selenium\n</code></pre>\n<p>Here <em>sel</em> marks the scraping mode as 'selenium'. This means GoogleScraper.py scrapes with real browsers. This is pretty powerful, since\nyou can scrape long and a lot of sites (Google has a hard time blocking real browsers). The argument of the flag <code>--keyword-file</code> must be a file with keywords separated by\nnewlines. So: For every google query one line. Easy, isnt' it?</p>\n<p>Furthermore, the option <code>--num-pages-for-keyword</code> means that GoogleScraper will fetch 3 consecutive pages for each keyword.</p>\n<p>Example keyword-file:</p>\n<pre><code>keyword number one\nhow to become a good rapper\ninurl:\"index.php?sl=43\"\nfiletype:.cfg\nallintext:\"You have a Mysql Error in your\"\nintitle:\"admin config\"\nBest brothels in atlanta\n</code></pre>\n<p>After the scraping you'll automatically have a new sqlite3 database in the named <code>google_scraper.db</code> in the same directory. You can open and inspect the database with the command:</p>\n<pre><code>GoogleScraper --shell\n</code></pre>\n<p>It shouldn't be a problem to scrape <strong><em>10'000 keywords in 2 hours</em></strong>. If you are really crazy, set the maximal browsers in the config a little\nbit higher (in the top of the script file).</p>\n<p>If you want, you can specify the flag <code>--proxy-file</code>. As argument you need to pass a file with proxies in it and with the following format:</p>\n<pre><code>protocol proxyhost:proxyport username:password\n(...)\n</code></pre>\n<p>Example:</p>\n<pre><code>socks5 127.0.0.1:1080 blabla:12345\nsocks4 77.66.55.44:9999 elite:js@fkVA3(Va3)\n</code></pre>\n<p>In case you want to use GoogleScraper.py in <em>http</em> mode (which means that raw http headers are sent), use it as follows:</p>\n<pre><code>GoogleScraper -m http -p 1 -n 25 -q \"white light\"\n</code></pre>\n</a><a>\n<h2>Contact</h2>\n</a><p><a>If you feel like contacting me, do so and send me a mail. You can find my contact information on my </a><a href=\"http://incolumitas.com/about/contact/\" rel=\"nofollow\" title=\"Contact with author\">blog</a>.</p>\n\n          </div>"}, "last_serial": 4330740, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "7bc7bbd7e1783a712d0c22cb4d153846", "sha256": "565e4de01ecf8a8d52a8b4d748c11b8c4aeef69b1214372a963845e0322143b2"}, "downloads": -1, "filename": "GoogleScraper-0.0.1.tar.gz", "has_sig": false, "md5_digest": "7bc7bbd7e1783a712d0c22cb4d153846", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23864, "upload_time": "2014-06-17T14:58:49", "upload_time_iso_8601": "2014-06-17T14:58:49.238375Z", "url": "https://files.pythonhosted.org/packages/94/59/acf52335f539904b87bcf7dc85ad9e651a346118103ffeb7e619fa46f820/GoogleScraper-0.0.1.tar.gz", "yanked": false}], "0.0.2.dev1": [{"comment_text": "", "digests": {"md5": "33acb854b456fa827add100425ed0f12", "sha256": "131ae6f0d4dfaa67a9cf853a2b8bbe3e9479aa733cdd6c996af1ae15f99a0027"}, "downloads": -1, "filename": "GoogleScraper-0.0.2.dev1.tar.gz", "has_sig": false, "md5_digest": "33acb854b456fa827add100425ed0f12", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 46805, "upload_time": "2014-08-12T21:29:40", "upload_time_iso_8601": "2014-08-12T21:29:40.384676Z", "url": "https://files.pythonhosted.org/packages/d1/fc/871d075362c70c00a7ffe9f450ae0ac7d5bfd567b09a7150c6b4c1fa493f/GoogleScraper-0.0.2.dev1.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "66e9fab34a16b6802d8cc2deb7e84de6", "sha256": "3a26dc1ea9a24d9176d862255d5304d44f1efb375c83079e023465ea98e1096e"}, "downloads": -1, "filename": "GoogleScraper-0.1.0.tar.gz", "has_sig": false, "md5_digest": "66e9fab34a16b6802d8cc2deb7e84de6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 48165, "upload_time": "2014-09-06T19:20:51", "upload_time_iso_8601": "2014-09-06T19:20:51.493756Z", "url": "https://files.pythonhosted.org/packages/5c/6f/3013cbc53fc79910e9aea4389b29dd2ca9e4a434255c05002c4bdf88fde1/GoogleScraper-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "dbe118615ef222d52018f210a1f2242a", "sha256": "15145b0a8296fb8b451d14f9690214114c4481e95744df8033504ff5dff56209"}, "downloads": -1, "filename": "GoogleScraper-0.1.1.tar.gz", "has_sig": false, "md5_digest": "dbe118615ef222d52018f210a1f2242a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 56608, "upload_time": "2014-11-03T18:56:35", "upload_time_iso_8601": "2014-11-03T18:56:35.895031Z", "url": "https://files.pythonhosted.org/packages/a2/d5/42ced7456f289a63d6751e01954d2c907103ba83e017814b35180993b6df/GoogleScraper-0.1.1.tar.gz", "yanked": false}], "0.1.10": [{"comment_text": "", "digests": {"md5": "deeefbf728354b3461728699ac9107ac", "sha256": "c15f53ea1798a6da6efd2c81e7972708cd55d97d92c80ae3035966159cd3558a"}, "downloads": -1, "filename": "GoogleScraper-0.1.10.tar.gz", "has_sig": false, "md5_digest": "deeefbf728354b3461728699ac9107ac", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 60439, "upload_time": "2014-12-22T22:28:54", "upload_time_iso_8601": "2014-12-22T22:28:54.279459Z", "url": "https://files.pythonhosted.org/packages/8c/55/23a1208e1520352f1ead0654e99a07e7790e5cb066e4ee34b38337e3e56a/GoogleScraper-0.1.10.tar.gz", "yanked": false}], "0.1.11": [{"comment_text": "", "digests": {"md5": "e99ee13faf11d26d919248f7a88d60c0", "sha256": "b03cdbbcf29b439e946269dd75b70c775488a044667fe4854f0ce6f75679bc26"}, "downloads": -1, "filename": "GoogleScraper-0.1.11.tar.gz", "has_sig": false, "md5_digest": "e99ee13faf11d26d919248f7a88d60c0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 60466, "upload_time": "2014-12-22T22:31:34", "upload_time_iso_8601": "2014-12-22T22:31:34.978868Z", "url": "https://files.pythonhosted.org/packages/3e/f6/674f6011069371e46ddd53892a1b9b46f42ce736754a60d16f698bb7ea22/GoogleScraper-0.1.11.tar.gz", "yanked": false}], "0.1.12": [{"comment_text": "", "digests": {"md5": "2129bcc20765d227eff2849e41a633f0", "sha256": "a5e0889e2c3004ce3c156c7da73e186080927a180426dbed1b090d56c82689a8"}, "downloads": -1, "filename": "GoogleScraper-0.1.12.tar.gz", "has_sig": false, "md5_digest": "2129bcc20765d227eff2849e41a633f0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 60718, "upload_time": "2014-12-22T23:10:44", "upload_time_iso_8601": "2014-12-22T23:10:44.602497Z", "url": "https://files.pythonhosted.org/packages/da/46/7ae8584f093231b8ca9e39d256c4e1baa8c79fa9f5d28cecf4020b4a104e/GoogleScraper-0.1.12.tar.gz", "yanked": false}], "0.1.13": [{"comment_text": "", "digests": {"md5": "b23d9a7a4609c6b4f85af07c915cb9b8", "sha256": "95af676c2f2df6afb69cbcecc8b6480950c5f40d309ab843076b894c0a6a1eac"}, "downloads": -1, "filename": "GoogleScraper-0.1.13.tar.gz", "has_sig": false, "md5_digest": "b23d9a7a4609c6b4f85af07c915cb9b8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 61127, "upload_time": "2014-12-23T13:04:20", "upload_time_iso_8601": "2014-12-23T13:04:20.078733Z", "url": "https://files.pythonhosted.org/packages/8f/fd/97a8ae522ca88c9c4b9dd20ff53c61b38b1a99427eb6c7cecca8f3e459e0/GoogleScraper-0.1.13.tar.gz", "yanked": false}], "0.1.15": [{"comment_text": "", "digests": {"md5": "66956d415e7bff01112971e210dc5ba6", "sha256": "43397509a12f3b1c65eb94ad9e3f2899c42b0eae0877b163fb5a244e13d8ef85"}, "downloads": -1, "filename": "GoogleScraper-0.1.15.tar.gz", "has_sig": false, "md5_digest": "66956d415e7bff01112971e210dc5ba6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 65367, "upload_time": "2015-01-10T14:49:06", "upload_time_iso_8601": "2015-01-10T14:49:06.200830Z", "url": "https://files.pythonhosted.org/packages/63/b7/40983b5104d8a2deedcca1811b1a2883d4239110866dec285375bf52aedf/GoogleScraper-0.1.15.tar.gz", "yanked": false}], "0.1.16": [{"comment_text": "", "digests": {"md5": "5c2a18423fa9fd728710def5b131ef70", "sha256": "fa44b70fc7404f02f36a79c167599dc54a0e0b0af522f8a206411efc6c6b34ca"}, "downloads": -1, "filename": "GoogleScraper-0.1.16.tar.gz", "has_sig": false, "md5_digest": "5c2a18423fa9fd728710def5b131ef70", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 66467, "upload_time": "2015-01-11T00:15:41", "upload_time_iso_8601": "2015-01-11T00:15:41.748931Z", "url": "https://files.pythonhosted.org/packages/dd/60/ca9733ee3038b7c3ba1bc5bc9698417e90ffcb51ea611094f8bc513fe272/GoogleScraper-0.1.16.tar.gz", "yanked": false}], "0.1.17": [{"comment_text": "", "digests": {"md5": "4bc19877f8ef1ed569ac67c4361041d8", "sha256": "f67c50a74f70cc2ab158f725b82660c6ce9af3da269d483a8fbbf0c227481c07"}, "downloads": -1, "filename": "GoogleScraper-0.1.17.tar.gz", "has_sig": false, "md5_digest": "4bc19877f8ef1ed569ac67c4361041d8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 66770, "upload_time": "2015-01-13T23:07:29", "upload_time_iso_8601": "2015-01-13T23:07:29.613522Z", "url": "https://files.pythonhosted.org/packages/04/42/d3e8d5777ac6447d9601d84d921c2b6aaff9d65846f51fe011456de1ac5b/GoogleScraper-0.1.17.tar.gz", "yanked": false}], "0.1.18": [{"comment_text": "", "digests": {"md5": "3e3148974abbcb1afd8ace4a0ad2878d", "sha256": "a92a457be24dc1e3b90096140477b2a9a526b6b3ba1df1bb47bdb7980834a5fb"}, "downloads": -1, "filename": "GoogleScraper-0.1.18.tar.gz", "has_sig": false, "md5_digest": "3e3148974abbcb1afd8ace4a0ad2878d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 67326, "upload_time": "2015-01-15T00:17:24", "upload_time_iso_8601": "2015-01-15T00:17:24.186953Z", "url": "https://files.pythonhosted.org/packages/60/e2/481ffddb0ebbe35cf3623f92ceaa10304a7cfd3c1183591b67a337b638df/GoogleScraper-0.1.18.tar.gz", "yanked": false}], "0.1.19": [{"comment_text": "", "digests": {"md5": "f067480f50640e536c2f082f08375688", "sha256": "4713181c6735ba54d76e9f0239d38abfd64b48ca45f4ada4bc0dad4fe3456ef8"}, "downloads": -1, "filename": "GoogleScraper-0.1.19.tar.gz", "has_sig": false, "md5_digest": "f067480f50640e536c2f082f08375688", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 73230, "upload_time": "2015-01-23T19:33:33", "upload_time_iso_8601": "2015-01-23T19:33:33.804864Z", "url": "https://files.pythonhosted.org/packages/9d/e5/8c7cfb350a877ac9cf1d9d9c23789080a39cf07ee76c28423656bd2c8786/GoogleScraper-0.1.19.tar.gz", "yanked": false}], "0.1.1dev": [{"comment_text": "", "digests": {"md5": "c1aef7f797c9ed54615df1b39e15b05a", "sha256": "173a699e2d7ada46d06da676e878dc5fb43459625922cca4b891165d258986d4"}, "downloads": -1, "filename": "GoogleScraper-0.1.1dev.tar.gz", "has_sig": false, "md5_digest": "c1aef7f797c9ed54615df1b39e15b05a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 56618, "upload_time": "2014-11-03T15:44:56", "upload_time_iso_8601": "2014-11-03T15:44:56.849007Z", "url": "https://files.pythonhosted.org/packages/22/26/e2bc049a80db14ecd0a840904b4b38bc199c26b0967ae48f23d98b60143d/GoogleScraper-0.1.1dev.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "49a3878f9edf429d7ef95cbc34d9b612", "sha256": "02fb391e6fe097e695efb95c04bfd28eb9eab26e00a62e4061bb56d955e93233"}, "downloads": -1, "filename": "GoogleScraper-0.1.2.tar.gz", "has_sig": false, "md5_digest": "49a3878f9edf429d7ef95cbc34d9b612", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 57951, "upload_time": "2014-11-04T22:05:27", "upload_time_iso_8601": "2014-11-04T22:05:27.906616Z", "url": "https://files.pythonhosted.org/packages/37/24/cbe042bb462edd9f856b0ba8524488892652940cde3b88aaaea367853d04/GoogleScraper-0.1.2.tar.gz", "yanked": false}], "0.1.20": [{"comment_text": "", "digests": {"md5": "11211f32ec27b3e26ae5c6c4a68b0c7a", "sha256": "277a39ef62ab7adfd3b80e743487f7d046727533782f584ee57325df146cbc1d"}, "downloads": -1, "filename": "GoogleScraper-0.1.20.tar.gz", "has_sig": false, "md5_digest": "11211f32ec27b3e26ae5c6c4a68b0c7a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74184, "upload_time": "2015-01-23T21:29:07", "upload_time_iso_8601": "2015-01-23T21:29:07.274124Z", "url": "https://files.pythonhosted.org/packages/fe/a3/2eb679f6cd4ab0ac3cafd9223dde6913e71251a60b64ef25ed814ca70441/GoogleScraper-0.1.20.tar.gz", "yanked": false}], "0.1.22": [{"comment_text": "", "digests": {"md5": "2f801b2df322a61a41ab875da9e9d2ac", "sha256": "17ad2c85dae5034023c75841e360647e1e814913ac2dc0f6077c37f014245b40"}, "downloads": -1, "filename": "GoogleScraper-0.1.22.tar.gz", "has_sig": false, "md5_digest": "2f801b2df322a61a41ab875da9e9d2ac", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74317, "upload_time": "2015-01-29T22:39:56", "upload_time_iso_8601": "2015-01-29T22:39:56.639788Z", "url": "https://files.pythonhosted.org/packages/c1/4f/52550b6fe6b6bc2d26cf77a42f400e131ef1f3945575db40af1978e62810/GoogleScraper-0.1.22.tar.gz", "yanked": false}], "0.1.23": [{"comment_text": "", "digests": {"md5": "6cb789f80ec0ab663e09a0c240195f6c", "sha256": "dee235118fd72838f210df7d09d212323074c2c9a19aeb8923f8cb3ff4eb519c"}, "downloads": -1, "filename": "GoogleScraper-0.1.23.tar.gz", "has_sig": false, "md5_digest": "6cb789f80ec0ab663e09a0c240195f6c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74288, "upload_time": "2015-01-31T01:25:22", "upload_time_iso_8601": "2015-01-31T01:25:22.483874Z", "url": "https://files.pythonhosted.org/packages/08/f0/3a643e25c54bf03a1c11e6b9dd7f5f01fa15bd5c6f00dd6352479d277391/GoogleScraper-0.1.23.tar.gz", "yanked": false}], "0.1.25": [{"comment_text": "", "digests": {"md5": "f11bb5f77ccaf4f487760f00f3059852", "sha256": "374e1bea58f745e857f277dee24c000db25f90037cadec9ac5bf5e26580be8b2"}, "downloads": -1, "filename": "GoogleScraper-0.1.25.tar.gz", "has_sig": false, "md5_digest": "f11bb5f77ccaf4f487760f00f3059852", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74310, "upload_time": "2015-01-31T15:33:16", "upload_time_iso_8601": "2015-01-31T15:33:16.537094Z", "url": "https://files.pythonhosted.org/packages/42/bc/d178be221e7c1d67c051e3a68f2fa6f63b063103bd14cc8db06cea856c70/GoogleScraper-0.1.25.tar.gz", "yanked": false}], "0.1.26": [{"comment_text": "", "digests": {"md5": "62a373ed6a57bb8b77c717d2db8edf12", "sha256": "641c6bf5f91fc5e2aff52238af767a5f1901148dc27f8f40f092e92ff4951033"}, "downloads": -1, "filename": "GoogleScraper-0.1.26.tar.gz", "has_sig": false, "md5_digest": "62a373ed6a57bb8b77c717d2db8edf12", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74655, "upload_time": "2015-02-02T17:05:43", "upload_time_iso_8601": "2015-02-02T17:05:43.006013Z", "url": "https://files.pythonhosted.org/packages/e6/f8/88de790693447df670f7f488bd108d04736fb824b27bfd106f9e41cfa5bf/GoogleScraper-0.1.26.tar.gz", "yanked": false}], "0.1.27": [{"comment_text": "", "digests": {"md5": "3a7c7c0b80fc25c2b9de4fb6dd021693", "sha256": "6ae56894c9d71bfc68d7e0f62d4d77e9f6595b9ab46a6be9163bd0b71577b5b5"}, "downloads": -1, "filename": "GoogleScraper-0.1.27.tar.gz", "has_sig": false, "md5_digest": "3a7c7c0b80fc25c2b9de4fb6dd021693", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74390, "upload_time": "2015-02-05T00:59:29", "upload_time_iso_8601": "2015-02-05T00:59:29.695937Z", "url": "https://files.pythonhosted.org/packages/f0/66/a2191a3eba83334c99501d0a6d1d87c5ecb3ca36b848e247ea3140937a7f/GoogleScraper-0.1.27.tar.gz", "yanked": false}], "0.1.28": [{"comment_text": "", "digests": {"md5": "1b14d7b94140f54db55ea51101b2cd37", "sha256": "0d4ef6d699aca8fb1855771688766f64e1b3f3828f4141df7c395c0588e8b14d"}, "downloads": -1, "filename": "GoogleScraper-0.1.28.tar.gz", "has_sig": false, "md5_digest": "1b14d7b94140f54db55ea51101b2cd37", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74360, "upload_time": "2015-02-05T01:14:10", "upload_time_iso_8601": "2015-02-05T01:14:10.878705Z", "url": "https://files.pythonhosted.org/packages/df/c8/2e3dddf00755a420dbaa55465182a6d4a028ad26a3b6cee89b064d2721fd/GoogleScraper-0.1.28.tar.gz", "yanked": false}], "0.1.29": [{"comment_text": "", "digests": {"md5": "8c0fb3441f0b0c168c3c500a5c09685c", "sha256": "bcf8ab874a5079de89968a2978744421631311895a6a101cbdf64449c4ea5e9e"}, "downloads": -1, "filename": "GoogleScraper-0.1.29.tar.gz", "has_sig": false, "md5_digest": "8c0fb3441f0b0c168c3c500a5c09685c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74413, "upload_time": "2015-02-05T19:10:50", "upload_time_iso_8601": "2015-02-05T19:10:50.942802Z", "url": "https://files.pythonhosted.org/packages/c2/96/ee22302b871c41961d88f0591cb921d0637cec38528ff9b9191d172dff41/GoogleScraper-0.1.29.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "bf4f100b181ba15f97a21cd94396d170", "sha256": "d8a420f40fab58d463d053e896252a77230b49f472e51773bd6859057a5daea9"}, "downloads": -1, "filename": "GoogleScraper-0.1.3.tar.gz", "has_sig": false, "md5_digest": "bf4f100b181ba15f97a21cd94396d170", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 58114, "upload_time": "2014-11-06T11:41:30", "upload_time_iso_8601": "2014-11-06T11:41:30.709214Z", "url": "https://files.pythonhosted.org/packages/8d/a0/54d47956cebe40fafe9304fb6cf19917416556c9830f4ae13ce7e1ea2b63/GoogleScraper-0.1.3.tar.gz", "yanked": false}], "0.1.30": [{"comment_text": "", "digests": {"md5": "f9f7fc14c022174f938997d5fdd55358", "sha256": "7ab37983d0efa1a3f874bae20032e08e53be5e332bb9af3436c76321f2ff268a"}, "downloads": -1, "filename": "GoogleScraper-0.1.30.tar.gz", "has_sig": false, "md5_digest": "f9f7fc14c022174f938997d5fdd55358", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74427, "upload_time": "2015-02-05T19:29:32", "upload_time_iso_8601": "2015-02-05T19:29:32.653211Z", "url": "https://files.pythonhosted.org/packages/60/4d/7b8327217c151f69629df175cac0407ff44bb2b59fdf9b3fdee89bd7f7a2/GoogleScraper-0.1.30.tar.gz", "yanked": false}], "0.1.31": [{"comment_text": "", "digests": {"md5": "8924bca8a6629237f23ebc7f54f635d8", "sha256": "19e25635f0a10239135a0ec7643b579c3c12099a42651e6a4832f45d690efafb"}, "downloads": -1, "filename": "GoogleScraper-0.1.31.tar.gz", "has_sig": false, "md5_digest": "8924bca8a6629237f23ebc7f54f635d8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74442, "upload_time": "2015-02-05T19:33:36", "upload_time_iso_8601": "2015-02-05T19:33:36.079501Z", "url": "https://files.pythonhosted.org/packages/cf/7e/03bfe89e20ff7e0fb276530c2d121081e3197cc27d0c80df6a20d8829f0e/GoogleScraper-0.1.31.tar.gz", "yanked": false}], "0.1.32": [{"comment_text": "", "digests": {"md5": "9d3f5f4ee1a14d942ac64c10a5d61a9e", "sha256": "66a8aa8992b2efb1e999a5cb3e0e78a57a0440e55ece3a654a0c7c619ab960d7"}, "downloads": -1, "filename": "GoogleScraper-0.1.32.tar.gz", "has_sig": false, "md5_digest": "9d3f5f4ee1a14d942ac64c10a5d61a9e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74448, "upload_time": "2015-02-05T19:40:03", "upload_time_iso_8601": "2015-02-05T19:40:03.931567Z", "url": "https://files.pythonhosted.org/packages/7b/c3/7b448ee0337e7a90c49676052ef09ad35c92fb4727eae6dac9967c667d30/GoogleScraper-0.1.32.tar.gz", "yanked": false}], "0.1.33": [{"comment_text": "", "digests": {"md5": "dfe802e70e1ace48ed59e5a21b4acdfb", "sha256": "0afb6bce3f42569e13297bb1787607d1a476698c28da45cb289f879454ee32cd"}, "downloads": -1, "filename": "GoogleScraper-0.1.33.tar.gz", "has_sig": false, "md5_digest": "dfe802e70e1ace48ed59e5a21b4acdfb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74455, "upload_time": "2015-02-05T19:44:54", "upload_time_iso_8601": "2015-02-05T19:44:54.476158Z", "url": "https://files.pythonhosted.org/packages/01/90/7378de7cbfcf70a690f9936ffd099a78d8f46af8619bed27faa378af3205/GoogleScraper-0.1.33.tar.gz", "yanked": false}], "0.1.35": [{"comment_text": "", "digests": {"md5": "5458e544f995e3381dd28988af865d7b", "sha256": "9ed8584930f52ccaca47444a19045fd6c65823fdbdf4f028ef6374c369c9840b"}, "downloads": -1, "filename": "GoogleScraper-0.1.35.tar.gz", "has_sig": false, "md5_digest": "5458e544f995e3381dd28988af865d7b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74446, "upload_time": "2015-02-05T19:51:49", "upload_time_iso_8601": "2015-02-05T19:51:49.759495Z", "url": "https://files.pythonhosted.org/packages/37/37/ffad411b42392e992a2ca52c8a55eb754b4ae9b00255011fe5e1f4a0ed84/GoogleScraper-0.1.35.tar.gz", "yanked": false}], "0.1.36": [{"comment_text": "", "digests": {"md5": "5a48b02dd0b3610d2412c997a71047b5", "sha256": "1860695ee1261f92e8f8acb5aaf34b2bc683476246796c6ec74cd10300df0b01"}, "downloads": -1, "filename": "GoogleScraper-0.1.36.tar.gz", "has_sig": false, "md5_digest": "5a48b02dd0b3610d2412c997a71047b5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 74553, "upload_time": "2015-02-13T12:05:20", "upload_time_iso_8601": "2015-02-13T12:05:20.343078Z", "url": "https://files.pythonhosted.org/packages/56/71/1595ecebd12a5e0b1a90486e9bf636b212999574533e4f3b0d378b0ebafe/GoogleScraper-0.1.36.tar.gz", "yanked": false}], "0.1.37": [{"comment_text": "", "digests": {"md5": "aaae565a89b25391044782f21333f004", "sha256": "e0d9f036e4ea32ba640fa56bdc01174fae99bc2509dda78d074767e32387c89e"}, "downloads": -1, "filename": "GoogleScraper-0.1.37.tar.gz", "has_sig": false, "md5_digest": "aaae565a89b25391044782f21333f004", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 75727, "upload_time": "2015-03-22T15:30:07", "upload_time_iso_8601": "2015-03-22T15:30:07.328376Z", "url": "https://files.pythonhosted.org/packages/79/5a/60448d0f2a78a1c8aa3c138d683964cd492eee9c5d8cc109bbbc0f58467c/GoogleScraper-0.1.37.tar.gz", "yanked": false}], "0.1.38": [{"comment_text": "", "digests": {"md5": "3d62a8bda8f422a00b836b75ccb2e71a", "sha256": "d7b347fb7b93642f9729c6dfd096dace6bbdade15b8465447528dd584018618a"}, "downloads": -1, "filename": "GoogleScraper-0.1.38-py3.5.egg", "has_sig": false, "md5_digest": "3d62a8bda8f422a00b836b75ccb2e71a", "packagetype": "bdist_egg", "python_version": "3.5", "requires_python": null, "size": 158162, "upload_time": "2018-10-01T23:47:33", "upload_time_iso_8601": "2018-10-01T23:47:33.853414Z", "url": "https://files.pythonhosted.org/packages/e1/66/f720bd28b23cf2e83a774ca0b74f2c3db570bb0e62df6d6198164ee92ad2/GoogleScraper-0.1.38-py3.5.egg", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "5fef8da24eccfe283d5943cf3c608332", "sha256": "00136be5ce0cdb7ae86d3785bc55f4d30a47d4e87d4930631a8a5c1bd9414089"}, "downloads": -1, "filename": "GoogleScraper-0.1.5.tar.gz", "has_sig": false, "md5_digest": "5fef8da24eccfe283d5943cf3c608332", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 52577, "upload_time": "2014-11-17T17:03:28", "upload_time_iso_8601": "2014-11-17T17:03:28.714530Z", "url": "https://files.pythonhosted.org/packages/8a/17/0fd458555f0fa5f56b6a26227ca74ae7c3a2be591ef7fb425fe1dae4a176/GoogleScraper-0.1.5.tar.gz", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "b6d0448964f36c5f059e3789fbacea57", "sha256": "25e805703200628da50bb99b61eaf3e7847079de78945e9dcd819ef850477368"}, "downloads": -1, "filename": "GoogleScraper-0.1.6.tar.gz", "has_sig": false, "md5_digest": "b6d0448964f36c5f059e3789fbacea57", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 52753, "upload_time": "2014-11-18T11:27:25", "upload_time_iso_8601": "2014-11-18T11:27:25.903476Z", "url": "https://files.pythonhosted.org/packages/ec/5f/3460dca36a8076487a7ffb95d84e458cccb3b59ab8a6443d53f2633aff6f/GoogleScraper-0.1.6.tar.gz", "yanked": false}], "0.1.7": [{"comment_text": "", "digests": {"md5": "fadefc336d6c37039cdbfaee3276a746", "sha256": "21ed3f5d444594b6bd71ad5c3a08f3e2fe2e4d02c8bcea8d6360122963df1dc3"}, "downloads": -1, "filename": "GoogleScraper-0.1.7.tar.gz", "has_sig": false, "md5_digest": "fadefc336d6c37039cdbfaee3276a746", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 53230, "upload_time": "2014-11-20T20:49:28", "upload_time_iso_8601": "2014-11-20T20:49:28.630047Z", "url": "https://files.pythonhosted.org/packages/26/1a/0956fc1963d9aa504e94b24f64c60e9c16b6da539580424db039cc059eef/GoogleScraper-0.1.7.tar.gz", "yanked": false}], "0.1.8": [{"comment_text": "", "digests": {"md5": "6e275b3fd24cf5af46cc45d520f0afff", "sha256": "4208fc612a4109f517e7613a0a1e0712d9ee8909dce66ceab0f2c8cc0eb7b82f"}, "downloads": -1, "filename": "GoogleScraper-0.1.8.tar.gz", "has_sig": false, "md5_digest": "6e275b3fd24cf5af46cc45d520f0afff", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 54435, "upload_time": "2014-11-25T11:40:07", "upload_time_iso_8601": "2014-11-25T11:40:07.658373Z", "url": "https://files.pythonhosted.org/packages/38/7b/564714f41497776a4cf834b6a851aa89cfb71ddd92d8b13018030fbfb500/GoogleScraper-0.1.8.tar.gz", "yanked": false}], "0.1.9": [{"comment_text": "", "digests": {"md5": "61b2afbc51314f2970715cbf145c63a8", "sha256": "590bea0cc4fd4265c106ca00eb467f6422288b095892d16976408b48fd5de7d9"}, "downloads": -1, "filename": "GoogleScraper-0.1.9.tar.gz", "has_sig": false, "md5_digest": "61b2afbc51314f2970715cbf145c63a8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 56132, "upload_time": "2014-12-11T17:31:00", "upload_time_iso_8601": "2014-12-11T17:31:00.929532Z", "url": "https://files.pythonhosted.org/packages/13/8b/b004c6763edd21ee7dd6b9b56406b986fcfcbd8ff70d876863f1cc5835e5/GoogleScraper-0.1.9.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "7a38f409ac9d4d98bb48ea11bd6fa5dd", "sha256": "031835c106658554ffa731d448920db1e5834cf2a6468dee91727855d8169c07"}, "downloads": -1, "filename": "GoogleScraper-0.2.0-py3.5.egg", "has_sig": false, "md5_digest": "7a38f409ac9d4d98bb48ea11bd6fa5dd", "packagetype": "bdist_egg", "python_version": "3.5", "requires_python": null, "size": 158476, "upload_time": "2018-10-01T23:47:35", "upload_time_iso_8601": "2018-10-01T23:47:35.777248Z", "url": "https://files.pythonhosted.org/packages/c5/5b/d3f648aff1ae51101503407acb1ef50eb9764748825f356f0d1a59c02829/GoogleScraper-0.2.0-py3.5.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "b6047454504559175d47b7d770532a4b", "sha256": "f5c0c181cd292ab3760e1c0fc264e143100a51c64cb0943c00ab70be56278272"}, "downloads": -1, "filename": "GoogleScraper-0.2.0.tar.gz", "has_sig": false, "md5_digest": "b6047454504559175d47b7d770532a4b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 76756, "upload_time": "2015-12-03T17:11:43", "upload_time_iso_8601": "2015-12-03T17:11:43.222666Z", "url": "https://files.pythonhosted.org/packages/e7/20/4ce6bd6be4c41326cdf5666c87ab35a6e86820e6ff9a86c2c6368210cdce/GoogleScraper-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "35895e7c74bfe1c3dc40efd75fb818de", "sha256": "cf261310f0652e0fd06be19bd85b0cb94b490001faf1d99cad46f4b8bcb808ab"}, "downloads": -1, "filename": "GoogleScraper-0.2.1.tar.gz", "has_sig": false, "md5_digest": "35895e7c74bfe1c3dc40efd75fb818de", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 77162, "upload_time": "2015-12-04T14:54:31", "upload_time_iso_8601": "2015-12-04T14:54:31.533202Z", "url": "https://files.pythonhosted.org/packages/51/e5/c60b0f6cb72b7faeb38da5171331450ab84d90e87224b442fe77817c69b8/GoogleScraper-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "d11bd9248b2aa5a753b2ee37fa4ec57b", "sha256": "f83c08a7ac8605fe9105b87edddd8fae59d8283835771ef8b99d9951503a7a25"}, "downloads": -1, "filename": "GoogleScraper-0.2.2.tar.gz", "has_sig": false, "md5_digest": "d11bd9248b2aa5a753b2ee37fa4ec57b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 82547, "upload_time": "2018-10-01T23:27:39", "upload_time_iso_8601": "2018-10-01T23:27:39.027179Z", "url": "https://files.pythonhosted.org/packages/96/8c/002d695ef8ed24e48504fad0f207fd8a0487b8599a6e13832694070b7011/GoogleScraper-0.2.2.tar.gz", "yanked": false}], "0.2.4": [{"comment_text": "", "digests": {"md5": "2a63aad2b2cbec7584e618ced7eb4762", "sha256": "c6fc6a334553a64f646eef9668899f5b0cca8da8d9041388277e900a614b39e4"}, "downloads": -1, "filename": "GoogleScraper-0.2.4.tar.gz", "has_sig": false, "md5_digest": "2a63aad2b2cbec7584e618ced7eb4762", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 82604, "upload_time": "2018-10-01T23:48:13", "upload_time_iso_8601": "2018-10-01T23:48:13.981366Z", "url": "https://files.pythonhosted.org/packages/65/e4/b6f3014181d04a0c0232f63d3af2eac38493d5efbbb2a9e6ccc26d3af045/GoogleScraper-0.2.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2a63aad2b2cbec7584e618ced7eb4762", "sha256": "c6fc6a334553a64f646eef9668899f5b0cca8da8d9041388277e900a614b39e4"}, "downloads": -1, "filename": "GoogleScraper-0.2.4.tar.gz", "has_sig": false, "md5_digest": "2a63aad2b2cbec7584e618ced7eb4762", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 82604, "upload_time": "2018-10-01T23:48:13", "upload_time_iso_8601": "2018-10-01T23:48:13.981366Z", "url": "https://files.pythonhosted.org/packages/65/e4/b6f3014181d04a0c0232f63d3af2eac38493d5efbbb2a9e6ccc26d3af045/GoogleScraper-0.2.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:55:33 2020"}