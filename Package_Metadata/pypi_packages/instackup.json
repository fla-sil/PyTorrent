{"info": {"author": "Daniel Lavedonio de Lima", "author_email": "daniel.lavedonio@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Instackup\nThis Python library is an open source way to standardize and simplify connections with cloud-based tools and databases and commonly used tools in data manipulation and analysis.\n\n# Index\n\n- Current release\n- Prerequisites\n- Installation\n- Documentation\n  - bigquery_tools\n  - gcloudstorage_tools\n  - general_tools\n  - redshift_tools\n  - s3_tools\n- Version log\n\n# Current release\n## Version 0.0.4 (alpha)\nFourth alpha release.\n\n#### New functionalities:\n- bigquery_tools\n  - BigQueryTool\n    - query_and_save_results\n\n#### Modified functionalities:\n- general_tools\n  - fetch_credentials\n\n#### Functionalities still in development:\n- gcloudstorage_tools\n  - GCloudStorageTool\n    - rename_file\n    - rename_subfolder\n    - upload_subfolder\n    - download_subfolder\n    - delete_file\n    - delete_subfolder\n- s3_tools\n  - S3Tool\n    - upload_subfolder\n    - download_subfolder\n\n# Prerequisites\n1. Have a [Python 3.6 version or superior](https://www.python.org/downloads/) installed;\n2. Create a YAML (or JSON) file with credentials information;\n3. [Optional but recommended] Configure an Environment Variable that points where the Credentials file is.\n\n### 1. Have a Python 3.6 version or superior installed\nGot to this [link](https://www.python.org/downloads/) e download the most current version that is compatible with this package.\n\n### 2. Create a YAML (or JSON) file with credentials information\n\nUse the files [secret_template.yml](https://github.com/Lavedonio/instackup/blob/master/credentials/secret_template.yml) or [secret_blank.yml](https://github.com/Lavedonio/instackup/blob/master/credentials/secret_blank.yml) as a base or copy and paste the code bellow and modify its values to the ones in your credentials/projects:\n\n```\n#################################################################\n#                                                               #\n#        ACCOUNTS CREDENTIALS. DO NOT SHARE THIS FILE.          #\n#                                                               #\n# Specifications:                                               #\n# - For the credentials you don't have, leave it blank.         #\n# - Keep Google's secret file in the same folder as this file.  #\n# - BigQuery project_ids must be strings, i.e., inside quotes.  #\n#                                                               #\n# Recommendations:                                              #\n# - YAML specification: https://yaml.org/spec/1.2/spec.html     #\n# - Keep this file in a static path like a folder within the    #\n# Desktop. Ex.: C:\\Users\\USER\\Desktop\\Credentials\\secret.yml    #\n#                                                               #\n#################################################################\n\n\nGoogle:\n  secret_filename: file.json\n\nBigQuery:\n  project_id:\n    project_name: \"000000000000\"\n\nAWS:\n  access_key: AWSAWSAWSAWSAWSAWSAWS\n  secret_key: CcasldUYkfsadcSDadskfDSDAsdUYalf\n\nRedShift:\n  cluster_credentials:\n    dbname: db\n    user: masteruser\n    host: blablabla.random.us-east-2.redshift.amazonaws.com\n    cluster_id: cluster\n    port: 5439\n  master_password:\n    dbname: db\n    user: masteruser\n    host: blablabla.random.us-east-2.redshift.amazonaws.com\n    password: masterpassword\n    port: 5439\n```\nSave this file with `.yml` extension in a folder where you know the path won't be modified, like the Desktop folder (Example: `C:\\Users\\USER\\Desktop\\Credentials\\secret.yml`).\n\nIf you prefer, you can follow this step using a JSON file instead. Follow the same instructions but using `.json` instead of `.yml`.\n\n### 3. [Optional but recommended] Configure an Environment Variable that points where the Credentials file is.\n\nTo configure the Environment Variable, follow the instructions bellow, based on your Operating System.\n\n#### Windows\n1. Place the YAML (or JSON) file in a folder you won't change its name or path later;\n2. In Windows Search, type `Environment Variables` and click in the Control Panel result;\n3. Click on the button `Environment Variables...`;\n4. In **Environment Variables**, click on the button `New`;\n5. In **Variable name** type `CREDENTIALS_HOME` and in **Variable value** paste the full path to the recently created YAML (or JSON) file;\n6. Click **Ok** in the 3 open windows.\n\n#### Linux/MacOS\n1. Place the YAML (or JSON) file in a folder you won't change its name or path later;\n2. Open the file `.bashrc`. If it doesn't exists, create one in the `HOME` directory. If you don't know how to get there, open the Terminal, type `cd` and then **ENTER**;\n3. Inside the file, in a new line, type the command: `export CREDENTIALS_HOME=\"/path/to/file\"`, replacing the content inside quotes by the full path to the recently created YAML (or JSON) file;\n4. Save the file and restart all open Terminal windows.\n\n> **Note:** If you don't follow this last prerequisite, you need to set the environment variable manually inside the code. To do that, inside your python code, after the imports, type the command (replacing the content inside quotes by the full path to the recently created YAML (or JSON) file):\n\n```\nos.environ[\"CREDENTIALS_HOME\"] = \"/path/to/file\"\n```\n\n# Installation\nGo to the Terminal and type:\n\n    pip install instackup\n\n# Documentation\n## bigquery_tools\n### BigQueryTool\nThis class handle most of the interaction needed with BigQuery, so the base code becomes more readable and straightforward.\n\n#### \\_\\_init\\_\\_(self)\nInitialization takes no parameter and has no return value. It sets the bigquery client.\n\nUsage example:\n```\nfrom instackup.bigquery_tools import BigQueryTool\n\n\nbq = BigQueryTool()\n```\n\n#### query(self, sql_query)\nRun a SQL query and return the results as a Pandas Dataframe.\n\nUsage example:\n```\nimport pandas as pd\nfrom instackup.bigquery_tools import BigQueryTool\n\n\nbq = BigQueryTool()\n\nsql_query = \"\"\"SELECT * FROM `project_name.dataset.table`\"\"\"\ndf = bq.query(sql_query)\n```\n\n#### query_and_save_results(self, sql_query, dest_dataset, dest_table, writing_mode=\"TRUNCATE\", create_table_if_needed=False)\nExecutes a query and saves the result in a table.\n\nwriting_mode parameter determines how the data is going to be written in BigQuery.\nDoes not apply if table doesn't exist. Can be one of 3 types (defaults to 'TRUNCATE'):\n- APPEND: If the table already exists, BigQuery appends the data to the table.\n- EMPTY: If the table already exists and contains data, a 'duplicate' error\n         is returned in the job result.\n- TRUNCATE: If the table already exists, BigQuery overwrites the table data.\n\nIf create_table_if_needed is set to False and the table doesn't exist, it'll raise an error.\nDafaults to False.\n\nUsage example:\n```\nfrom instackup.bigquery_tools import BigQueryTool\n\n\n# Enter valid values here\ndest_dataset = \"dataset\"\ndest_table = \"some_other_table\"\nsql_query = \"\"\"SELECT * FROM `project_name.dataset.table`\"\"\"\n\nbq = BigQueryTool()\n\nbq.query_and_save_results(self, sql_query, dest_dataset, dest_table, create_table_if_needed=True)\n```\n\n#### list_datasets(self)\nReturns a list with all dataset names inside the project.\n\nUsage example:\n```\nfrom instackup.bigquery_tools import BigQueryTool\n\n\nbq = BigQueryTool()\n\ndatasets = bq.list_datasets()\n\nprint(\"There are {num} datasets, which are listed bellow:\\n\".format(num=len(datasets)))\nfor ds in datasets:\n    print(ds)\n```\n\n#### list_tables_in_dataset(self, dataset, get=None, return_type=\"dict\")\nLists all tables inside a dataset. Will fail if dataset doesn't exist.\n\nget parameter can be a string or list of strings. If only a string is passed,\nwill return a list of values of that attribute of all tables\n(this case overrides return_type parameter).\n\nValid get parameters are:\n[\"clustering_fields\", \"created\", \"dataset_id\", \"expires\", \"friendly_name\",\n\"full_table_id\", \"labels\", \"partition_expiration\", \"partitioning_type\", \"project\",\n\"reference\", \"table_id\", \"table_type\", \"time_partitioning\", \"view_use_legacy_sql\"]\n\nreturn_type parameter can be 1 out of 3 types and sets how the result will be returned:\n- dict: dictionary of lists, i.e., each key has a list of all tables values for that attribute.\n        The same index for different attibutes refer to the same table;\n- list: list of dictionaries, i.e., each item in the list is a dictionary with all the attributes\n        of the respective table;\n- dataframe: Pandas DataFrame.\n\nUsage example:\n```\nfrom instackup.bigquery_tools import BigQueryTool\n\n\nbq = BigQueryTool()\n\ndataset = \"dataset\"  # Enter a valid dataset name\n\ntables = bq.list_tables_in_dataset(dataset, get=\"table_id\")  # Getting only table name\n\nprint(\"There are {num} tables in {ds}, which are listed bellow:\\n\".format(num=len(tables), ds=dataset))\nfor tb in tables:\n    print(tb)\n\n# Getting all table info\ndf = bq.list_tables_in_dataset(dataset, return_type=\"dataframe\")\nprint(df)\n```\n\n#### get_table_schema(self, dataset, table)\nGets schema information and returns a properly formatted dictionary.\n\nUsage example:\n```\nimport json\nfrom instackup.bigquery_tools import BigQueryTool\n\n\nbq = BigQueryTool()\n\ndataset = \"dataset\"  # Enter a valid dataset name\ntable = \"table\"      # Enter a valid table name\n\nschema = bq.get_table_schema(dataset, table)\n\nwith open('data.json', 'w') as fp:\n    json.dump(schema, fp, sort_keys=True, indent=4)\n```\n\n#### convert_dataframe_to_numeric(dataframe, exclude_columns=[], \\*\\*kwargs)\nTransform all string type columns into floats, except those in exclude_columns list.\n\n\\*\\*kwargs are passed directly to pandas.to_numeric method.\nThe complete documentation of this method can be found here:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html\n\nUsage example:\n```\nimport pandas as pd\nfrom instackup.bigquery_tools import BigQueryTool\n\n\n# You can often find these kind of data when reading from a file\ndf = pd.DataFrame({\"col.1\": [\"1\", \"2\"], \"col.2\": [\"3\", \"junk\"], \"col.3\": [\"string1\", \"string2\"]})\n\nbq = BigQueryTool()\ndf = bq.convert_dataframe_to_numeric(df, exclude_columns=[\"col.3\"], errors=\"coerce\")\nprint(df)\n\n# output:\n#\n#    col.1  col.2    col.3\n# 0      1    3.0  string1\n# 1      2    NaN  string2\n```\n\n#### clean_dataframe_column_names(dataframe, allowed_chars=\"abcdefghijklmnopqrstuvwxyz0123456789\", special_treatment={})\nReplace dataframe columns to only contain chars allowed in BigQuery tables column name.\n\nspecial_treatment dictionary substitutes the terms in the keys by its value pair.\n\nUsage example:\n```\nimport pandas as pd\nfrom instackup.bigquery_tools import BigQueryTool\n\n\n# You can often find these kind of data when reading from a file\ndf = pd.DataFrame({\"col.1\": [\"1\", \"2\"], \"col.2\": [\"3\", \"junk\"], \"col.3!\": [\"string1\", \"string2\"]})\n\nbq = BigQueryTool()\ndf = bq.clean_dataframe_column_names(df, special_treatment={\"!\": \"_factorial\"})\nprint(df)\n\n# output:\n#\n#   col_1 col_2 col_3_factorial\n# 0     1     3         string1\n# 1     2  junk         string2\n```\n\n#### upload(self, dataframe, dataset, table, \\*\\*kwargs)\nPrepare dataframe columns and executes an insert SQL command into BigQuery.\n\n\\*\\*kwargs are passed directly to pandas.to_gbq method.\nThe complete documentation of this method can be found here:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_gbq.html\n\nUsage example:\n```\nimport pandas as pd\nfrom instackup.bigquery_tools import BigQueryTool\n\n\nfixed_data = {\n  'col1': [1, 2],\n  'col2': [0.5, 0.75]\n}\n\ndf = pd.DataFrame(fixed_data)\n\ndataset = \"some_dataset_name\"\ntable = \"some_table_name\"\n\nbq = BigQueryTool()\nbq.upload(df, dataset, table)\n```\n\n#### upload_from_gcs(self, dataset, table, gs_path, file_format=\"CSV\", header_rows=1, delimiter=\",\", encoding=\"UTF-8\", writing_mode=\"APPEND\", create_table_if_needed=False, schema=None)\nUploads data from Google Cloud Storage directly to BigQuery.\n\ndataset and table parameters determines the destination of the upload.\ngs_path parameter is the file location in Google Cloud Storage.\nAll 3 of them are required string parameters.\n\nfile_format can be either 'AVRO', 'CSV', 'JSON', 'ORC' or 'PARQUET'. Defaults to 'CSV'.\nheader_rows, delimiter and encoding are only used when file_format is 'CSV'.\n\nheader_rows parameter determine the length in rows of the 'CSV' file given.\nShould be 0 if there are no headers in the file. Defaults to 1.\n\ndelimiter determines the string character used to delimite the data. Defaults to ','.\n\nencoding tells the file encoding. Can be either 'UTF-8' or 'ISO-8859-1' (latin-1).\nDefaults to 'UTF-8'.\n\nwriting_mode parameter determines how the data is going to be written in BigQuery.\nDoes not apply if table doesn't exist. Can be one of 3 types (defaults in 'APPEND'):\n- APPEND: If the table already exists, BigQuery appends the data to the table.\n- EMPTY: If the table already exists and contains data, a 'duplicate' error\n         is returned in the job result.\n- TRUNCATE: If the table already exists, BigQuery overwrites the table data.\n\nIf create_table_if_needed is set to False and the table doesn't exist, it'll raise an error.\nDafaults to False.\n\nschema is either a list of dictionaries containing the schema information or\na dictionary encapsulating the previous list with a key of 'fields'.\nThis latter format can be found when directly importing the schema info from a JSON generated file.\nIf the file_format is either 'CSV' or 'JSON' or the table already exists, it can be ommited.\n\nUsage example:\n```\nimport json\nfrom instackup.bigquery_tools import BigQueryTool\n\n\n# Enter valid values here\ndataset = \"sandbox\"\ntable = \"test\"\ngs_path = \"gs://some-bucket/some-subfolder/test.json\"\n\n# schema must be in the same format of the output of get_table_schema method.\nwith open('data.json', 'r') as fp:\n    schema = json.load(fp)\n\nbq.upload_from_gcs(dataset, table, gs_path, file_format=\"JSON\", create_table_if_needed=True, schema=schema)\n```\n\n#### upload_from_file(self, dataset, table, file_location, file_format=\"CSV\", header_rows=1, delimiter=\",\", encoding=\"UTF-8\", writing_mode=\"APPEND\", create_table_if_needed=False, schema=None)\nUploads data from a local file to BigQuery.\n\ndataset and table parameters determines the destination of the upload.\nfile_location parameter is either the file full or relative path in the local computer.\nAll 3 of them are required string parameters.\n\nfile_format can be either 'AVRO', 'CSV', 'JSON', 'ORC' or 'PARQUET'. Defaults to 'CSV'.\nheader_rows, delimiter and encoding are only used when file_format is 'CSV'.\n\nheader_rows parameter determine the length in rows of the 'CSV' file given.\nShould be 0 if there are no headers in the file. Defaults to 1.\n\ndelimiter determines the string character used to delimite the data. Defaults to ','.\n\nencoding tells the file encoding. Can be either 'UTF-8' or 'ISO-8859-1' (latin-1).\nDefaults to 'UTF-8'.\n\nwriting_mode parameter determines how the data is going to be written in BigQuery.\nDoes not apply if table doesn't exist. Can be one of 3 types (defaults in 'APPEND'):\n- APPEND: If the table already exists, BigQuery appends the data to the table.\n- EMPTY: If the table already exists and contains data, a 'duplicate' error\n         is returned in the job result.\n- TRUNCATE: If the table already exists, BigQuery overwrites the table data.\n\nIf create_table_if_needed is set to False and the table doesn't exist, it'll raise an error.\nDafaults to False.\n\nschema is either a list of dictionaries containing the schema information or\na dictionary encapsulating the previous list with a key of 'fields'.\nThis latter format can be found when directly importing the schema info from a JSON generated file.\nIf the file_format is either 'CSV' or 'JSON' or the table already exists, it can be ommited.\n\nUsage example:\n```\nimport json\nfrom instackup.bigquery_tools import BigQueryTool\n\n\n# Enter valid values here\ndataset = \"sandbox\"\ntable = \"test\"\nfile_location = \"test.csv\"\n\n# schema must be in the same format of the output of get_table_schema method.\nwith open('data.json', 'r') as fp:\n    schema = json.load(fp)\n\nbq.upload_from_file(dataset, table, file_location, create_table_if_needed=True, schema=schema)\n```\n\n#### start_transfer(self, project_path=None, project_name=None, transfer_name=None)\nTakes a project path or both project name and transfer name to trigger a transfer to start executing in BigQuery Transfer. Returns a status indicating if the request was processed (if it does, the response should be 'PENDING').\nAPI documentation: https://googleapis.dev/python/bigquerydatatransfer/latest/gapic/v1/api.html\n\nUsage example:\n```\nfrom instackup.bigquery_tools import BigQueryTool\n\n\ntransfer_config = \"projects/000000000000/transferConfigs/00000000-0000-0000-0000-000000000000\"\n\nuse_project_path = True\nprint(\"Starting transfer...\")\n\n# Both options do the same thing\nif use_project_path:\n    state_response = bq.start_transfer(project_path=transfer_config)\nelse:\n    state_response = bq.start_transfer(project_name=\"project_name\", transfer_name=\"transfer_name\")\n\nprint(f\"Transfer status: {state_response}\")\n```\n\n## gcloudstorage_tools\n### GCloudStorageTool\nThis class handle most of the interaction needed with Google Cloud Storage,\nso the base code becomes more readable and straightforward.\n\n#### \\_\\_init\\_\\_(self, bucket=None, subfolder=\"\", gs_path=None)\nTakes a either gs_path or both bucket name and subfolder name as parameters to set the current working directory. It also opens a connection with Google Cloud Storage.\n\nThe paradigm of this class is that all the operations are done in the current working directory, so it is important to set the right path (you can reset it later, but still).\n\nUsage example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\n# or\n\ngs = GCloudStorageTool(bucket=\"some_other_bucket\", subfolder=\"some_subfolder/subpath/\")\n```\n\n#### bucket(self) @property\nReturns the bucket object from the client based on the bucket name given in \\_\\_init\\_\\_ or set_bucket\n\n#### set_bucket(self, bucket)\nTakes a string as a parameter to reset the bucket name and bucket object. It has no return value.\n\nUsage Example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\ngs.set_bucket(\"some_other_bucket\")\n\n# Check new path structure\nprint(gs.get_gs_path())\n```\n\n#### set_subfolder(self, subfolder)\nTakes a string as a parameter to reset the subfolder name. It has no return value.\n\nUsage Example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\ngs.set_subfolder(\"some/more_complex/subfolder/structure/\")\n\n# Check new path structure\nprint(gs.get_gs_path())\n```\n\n#### set_by_path(self, s3_path)\nTakes a string as a parameter to reset the bucket name and subfolder name by its GS path. It has no return value.\n\nUsage Example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\ngs.set_by_path(\"gs://some_other_bucket/some/more_complex/subfolder/structure/\")\n\n# Check new path structure\nprint(gs.get_gs_path())\n```\n\n#### get_gs_path(self)\nReturns a string containing the GS path for the currently set bucket and subfolder. It takes no parameter.\n\nUsage Example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\nprint(gs.get_gs_path())\n```\n\n#### list_all_buckets(self)\nReturns a list of all Buckets in Google Cloud Storage. It takes no parameter.\n\nUsage Example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\n# Setting or not a subfolder doesn't change the output of this function\ngs = GCloudStorageTool(bucket=\"some_bucket\")\n\nall_buckets = gs.list_all_buckets()\n\n# some code here\n```\n\n#### get_bucket_info(self, bucket=None)\nReturns a dictionary with the information of Name, Datetime Created, Datetime Updated and Owner ID\nof the currently selected bucket (or the one passed in the parameters).\n\nUsage Example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(bucket=\"some_bucket\")\n\nbucket_info = gs.get_bucket_info()\nprint(bucket_info)\n```\n\n#### list_bucket_attributes(self)\nA list of all curently supported bucket attributes that comes in get_bucket_info method return dictionary.\n\nUsage Example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(bucket=\"some_bucket\")\n\nbucket_info_attributes = gs.list_bucket_attributes()\nprint(bucket_info_attributes)\n```\n\n#### get_blob_info(self)\nConverts a google.cloud.storage.Blob (which represents a storage object) to context format (GCS.BucketObject).\n\nUsage Example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(bucket=\"some_bucket\", subfolder=\"some_subfolder\")\ngs.set_blob(\"some_subfolder/file.csv\")\n\nblob_info_attributes = gs.get_blob_info()\nprint(blob_info_attributes)\n```\n\n#### list_blob_attributes(self)\nA list of all curently supported bucket attributes that comes in get_blob_info method return dictionary.\n\nUsage Example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(bucket=\"some_bucket\")\ngs.set_blob(\"some_subfolder/file.csv\")\n\nblob_info_attributes = gs.list_blob_attributes()\nprint(blob_info_attributes)\n```\n\n#### list_contents(self, yield_results=False):\nLists all files that correspond with bucket and subfolder set at the initialization.\n\nIt can either return a list or yield a generator. Lists can be more familiar to use, but when dealing with large amounts of data, yielding the results may be a better option in terms of efficiency.\n\nFor more information on how to use generators and yield, check this video:\nhttps://www.youtube.com/watch?v=bD05uGo_sVI\n\nUsage Example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\npath_contents = gs.list_contents()\n\nif len(path_contents) == 0:\n    s3.set_subfolder(\"logs/subfolder/\")\n\n    # When a specific bucket/ bucket + subfolder contains a lot of data,\n    # that's when yielding the results may be more efficient.\n    for file in gs.list_contents(yield_results=True):\n        # Do something\n\n# some code here\n```\n\n#### rename_file(self, new_filename, old_filename)\nNot implemented.\n\n#### rename_subfolder(self, new_subfolder)\nNot implemented.\n\n#### upload_file(self, filename, remote_path=None)\nUploads file to remote path in Google Cloud Storage (GS).\n\nremote_path can take either a full GS path or a subfolder only one.\n\nIf the remote_path parameter is not set, it will default to whatever subfolder\nis set in instance of the class plus the file name that is being uploaded.\n\nUsage Example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\nfile_location = \"C:\\\\Users\\\\USER\\\\Desktop\\\\file.csv\"\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\n# upload_file method accepts all 3 options\ngs.upload_file(file_location)\ngs.upload_file(file_location, \"gs://some_bucket/other_subfolder/\")\ngs.upload_file(file_location, \"another_subfolder/\")  # Just subfolder\n```\n\n#### upload_subfolder(self, folder_path)\nNot implemented.\n\n#### download_file(self, fullfilename=None, replace=False)\nDownloads remote gs file to local path.\n\nIf the fullfilename parameter is not set, it will default to the currently set blob.\n\nIf replace is set to True and there is already a file downloaded with the same filename and path,\nit will replace the file. Otherwise it will create a new file with a number attached to the end.\n\nUsage Example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\nfile_location = \"gs://some_bucket/other_subfolder/\"\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\n# download_file method accepts both options\ngs.download_file(file_location)\ngs.download_file(file_location, \"C:\\\\Users\\\\USER\\\\Desktop\\\\file.csv\")\n```\n\n#### download_subfolder(self)\nNot implemented.\n\n#### download_on_dataframe(self, \\*\\*kwargs)\nUse blob information to download file and use it directly on a Pandas DataFrame\nwithout having to save the file.\n\n\\*\\*kwargs are passed directly to pandas.read_csv method.\nThe complete documentation of this method can be found here:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n\nUsage Example:\n```\nfrom instackup.gcloudstorage_tools import GCloudStorageTool\n\n\nfile_location = \"gs://some_bucket/other_subfolder/\"\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\n# For a well behaved file, you may just use the method directly\ngs.set_blob(\"subfolder/file.csv\")\ndf = gs.download_on_dataframe()\n\n# For a file with a weird layout, you may want to use some parameters to save some time in data treatment\ngs.set_blob(\"subfolder/weird_file.csv\")\ndf = gs.download_on_dataframe(sep=\";\", encoding=\"ISO-8859-1\", decimal=\",\")\n```\n\n#### delete_file(self)\nNot implemented.\n\n#### delete_subfolder(self)\nNot implemented.\n\n## general_tools\n### fetch_credentials(service_name, \\*\\*kwargs)\nGets the credentials from the secret file set in `CREDENTIALS_HOME` variable and returns the credentials of the selected service in a dictionary. If service is \"credentials_path\", a path is returned instead.\n\nIt's meant to be used basically by the other modules, not actually by the user of the library.\n\nUsage example:\n```\nfrom instackup.general_tools import fetch_credentials\n\nprint(fetch_credentials(service_name=\"Google\"))\nprint(fetch_credentials(\"AWS\"))\nprint(fetch_credentials(\"RedShift\", connection_type=\"cluster_credentials\"))\nprint(fetch_credentials(\"credentials_path\"))\n```\n\n### unicode_to_ascii(unicode_string)\nReplaces all non-ascii chars in string by the closest possible match.\n\nThis solution was inpired by this answer:\nhttps://stackoverflow.com/a/517974/11981524\n\n### parse_remote_uri(uri, service)\nParses a Google Cloud Storage (GS) or an Amazon S3 path into bucket and subfolder(s).\nRaises an error if path is with wrong format.\n\nservice parameter can be either \"gs\" or \"s3\"\n\nUsage example:\n```\nfrom instackup.general_tools import parse_remote_uri\n\n\n### S3\ns3_path = \"s3://some_bucket/subfolder/\"\nbucket_name, subfolder = parse_remote_uri(s3_path, \"s3\")\n\nprint(f\"Bucket name: {bucket_name}\")  # output: >>> some_bucket\nprint(f\"Subfolder: {subfolder}\")      # output: >>> subfolder\n\n\n### Storage\ngs_path = \"gs://some_bucket/subfolder/\"\nbucket_name, subfolder = parse_remote_uri(gs_path, \"gs\")\n\nprint(f\"Bucket name: {bucket_name}\")  # output: >>> some_bucket\nprint(f\"Subfolder: {subfolder}\")      # output: >>> subfolder\n```\n\n## redshift_tools\n### RedShiftTool\nThis class handle most of the interaction needed with RedShift, so the base code becomes more readable and straightforward.\n\nThis class implements the with statement, so there are 2 ways of using it.\n\n**1st way:**\n\n```\nfrom instackup.redshift_tools import RedShiftTool\n\nwith RedShiftTool() as rs:\n    # use rs object to interact with RedShift database\n```\n\n**2nd way:**\n\n```\nfrom instackup.redshift_tools import RedShiftTool\n\nrs = RedShiftTool()\nrs.connect()\n\ntry:\n    # use rs object to interact with RedShift database\nexcept Exception as e:\n    rs.rollback()\n    raise e\nelse:\n    rs.commit()\nfinally:\n    rs.close_connection()\n```\n\nEasy to see that it is recommended (and easier) to use the first syntax.\n\n#### \\_\\_init\\_\\_(self, connect_by_cluster=True)\nInitialization takes connect_by_cluster parameter that sets connection type and has no return value.\n\nThe \\_\\_init\\_\\_ method doesn't actually opens the connection, but sets all values required by the connect method.\n\nUsage example:\n```\nfrom instackup.bigquery import RedShiftTool\n\nrs = RedShiftTool()\n```\n\n#### connect(self, fail_silently=False)\nCreate the connection using the \\_\\_init\\_\\_ attributes and returns its own object for with statement.\n\nIf fail_silently parameter is set to True, any errors will be surpressed and not stop the code execution.\n\nUsage example:\n```\nfrom instackup.bigquery import RedShiftTool\n\nrs = RedShiftTool()\nrs.connect()\n# remember to close the connection later\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n```\n\n#### commit(self)\nCommits any pending transaction to the database. It has no extra parameter or return value.\n\nUsage example:\n```\nfrom instackup.bigquery import RedShiftTool\n\nrs = RedShiftTool()\nrs.connect()\n# Do stuff\nrs.commit()\n# remember to close the connection later\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n    # Do stuff\n\n    # No need to explictly commit as it will do when leaving this context, but nonetheless:\n    rs.commit()\n```\n\n#### rollback(self)\nRoll back to the start of any pending transaction. It has no extra parameter or return value.\n\nUsage example:\n```\nfrom instackup.bigquery import RedShiftTool\n\nrs = RedShiftTool()\nrs.connect()\n\ntry:\n    # Do stuff\nexcept Exception as e:\n    rs.rollback()\n    raise e\nelse:\n    rs.commit()\nfinally:\n    # remember to close the connection later\n    rs.close_connection()\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n    # Do stuff\n\n    # No need to explictly commit or rollback as it will do when leaving this context, but nonetheless:\n    if meet_condition:\n        rs.commit()\n    else:\n        rs.rollback()\n```\n\n#### execute_sql(self, command, fail_silently=False)\nExecute a SQL command (CREATE, UPDATE and DROP). It has no return value.\n\nIf fail_silently parameter is set to True, any errors will be surpressed and not stop the code execution.\n\nUsage example:\n```\nfrom instackup.bigquery import RedShiftTool\n\n\nsql_cmd = \"\"\"CREATE TABLE test (\n    id          integer NOT NULL CONSTRAINT firstkey PRIMARY KEY,\n    username    varchar(40) UNIQUE NOT NULL,\n    fullname    varchar(64) NOT NULL,\n    created_at  TIMESTAMP NOT NULL,\n    last_login  TIMESTAMP\n);\n\"\"\"\n\n\nrs = RedShiftTool()\nrs.connect()\n\ntry:\n    # Execute the command\n    rs.execute_sql(sql_cmd)\n\nexcept Exception as e:\n    rs.rollback()\n    raise e\nelse:\n    rs.commit()\nfinally:\n    # remember to close the connection later\n    rs.close_connection()\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n    # This command would throw an error (since the table already was created before),\n    # but since fail_silently parameter is set to True, it'll catch the exception\n    # and let the code continue past this point.\n    rs.execute_sql(sql_cmd, fail_silently=True)\n\n    # other code\n```\n\n#### query(self, sql_query, fetch_through_pandas=True, fail_silently=False)\nRun a query and return the results.\n\nfetch_through_pandas parameter tells if the query should be parsed by psycopg2 cursor or pandas.\n\nIf fail_silently parameter is set to True, any errors will be surpressed and not stop the code execution.\n\nUsage example:\n```\nfrom instackup.bigquery import RedShiftTool\n\n\nsql_query = \"\"\"SELECT * FROM table LIMIT 100\"\"\"\n\n\nrs = RedShiftTool()\nrs.connect()\n\ntry:\n    # Returns a list of tuples containing the rows of the response\n    table = rs.query(sql_cmd, fetch_through_pandas=False, fail_silently=True)\n\n    # Do something with table variable\n\nexcept Exception as e:\n    rs.rollback()\n    raise e\nelse:\n    rs.commit()\nfinally:\n    # remember to close the connection later\n    rs.close_connection()\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n    # Returns a Pandas dataframe\n    df = rs.query(sql_cmd)\n\n    # To do operations with dataframe, you'll need to import pandas library\n\n    # other code\n```\n\n#### unload_to_S3(self, redshift_query, s3_path, filename, unload_options=\"MANIFEST GZIP ALLOWOVERWRITE REGION 'us-east-2'\")\nExecutes an unload command in RedShift database to copy data to S3.\n\nTakes the parameters redshift_query to grab the data, s3_path to set the location of copied data, filename as the custom prefix of the file and unload options.\n\nUnload options can be better understood in this link: https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\n\nUsage example:\n```\nfrom instackup.bigquery import RedShiftTool\n\n\n# Maybe you'll get this timestamp from other source\ntimestamp = '2019-11-29 19:31:42.766000+00:00'\nextraction_query = \"\"\"SELECT * FROM schema.table WHERE tstamp = '{timestamp}'\"\"\".format(timestamp=timestamp)\n\ns3_path = \"s3://redshift-data/unload/\"\nfilename = \"file_\"\nunload_options = \"DELIMITER '|' ESCAPE ADDQUOTES\"\n\n\nrs = RedShiftTool()\nrs.connect()\n\ntry:\n    # Unload data with custom options\n    rs.unload_to_S3(extraction_query, s3_path, filename, unload_options)\n\nexcept Exception as e:\n    rs.rollback()\n    raise e\nelse:\n    rs.commit()\nfinally:\n    # remember to close the connection later\n    rs.close_connection()\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n    # Unload data without custom options (will overwrite)\n    rs.unload_to_S3(extraction_query, s3_path, filename)\n\n    # other code\n```\n\n#### close_connection(self)\nCloses Connection with RedShift database. It has no extra parameter or return value.\n\nUsage example:\n```\nfrom instackup.bigquery import RedShiftTool\n\nrs = RedShiftTool()\nrs.connect()\n\ntry:\n    # Do stuff\nexcept Exception as e:\n    rs.rollback()\n    raise e\nelse:\n    rs.commit()\nfinally:\n    rs.close_connection()\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n    # Do stuff\n\n    # Will close the connection automatically when existing this scope\n```\n\n## s3_tools\n### S3Tool\nThis class handle most of the interaction needed with S3,\nso the base code becomes more readable and straightforward.\n\nTo understand the S3 structure, you need to know it is not a hierarchical filesystem,\nit is only a key-value store, though the key is often used like a file path for organising data,\nprefix + filename. More information about this can be read in this StackOverFlow thread:\nhttps://stackoverflow.com/questions/52443839/s3-what-exactly-is-a-prefix-and-what-ratelimits-apply\n\nAll that means is that while you may see a path as:\n```\ns3://bucket-1/folder1/subfolder1/some_file.csv\nroot| folder | sub.1 |  sub.2   |    file    |\n```\n\nIt is actually:\n```\ns3://bucket-1/folder1/sub1/file.csv\nroot| bucket |         key        |\n```\n\nA great (not directly related) thread that can help that sink in (and help understand some methods here)\nis this one: https://stackoverflow.com/questions/35803027/retrieving-subfolders-names-in-s3-bucket-from-boto3\n\nIn this class, all keys and keys prefix are being treated as a folder tree structure,\nsince the reason for this to exists is to make the programmers interactions with S3\neasier to write and the code easier to read.\n\n#### \\_\\_init\\_\\_(self, bucket=None, subfolder=\"\", s3_path=None)\nTakes a either s3_path or both bucket name and subfolder name as parameters to set the current working directory. It also opens a connection with AWS S3.\n\nThe paradigm of this class is that all the operations are done in the current working directory, so it is important to set the right path (you can reset it later, but still).\n\nUsage example:\n```\nfrom instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\n# or\n\ns3 = S3Tool(bucket=\"some_other_bucket\", subfolder=\"some_subfolder/subpath/\")\n```\n\n#### bucket(self) @property\nReturns the bucket object from the client based on the bucket name given in \\_\\_init\\_\\_ or set_bucket\n\n#### set_bucket(self, bucket)\nTakes a string as a parameter to reset the bucket name and bucket object. It has no return value.\n\nUsage Example:\n```\nfrom instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\ns3.set_bucket(\"some_other_bucket\")\n\n# Check new path structure\nprint(s3.get_s3_path())\n```\n\n#### set_subfolder(self, subfolder)\nTakes a string as a parameter to reset the subfolder name. It has no return value.\n\nUsage Example:\n```\nfrom instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\ns3.set_subfolder(\"some/more_complex/subfolder/structure/\")\n\n# Check new path structure\nprint(s3.get_s3_path())\n```\n\n#### set_by_path(self, s3_path)\nTakes a string as a parameter to reset the bucket name and subfolder name by its S3 path. It has no return value.\n\nUsage Example:\n```\nfrom instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\ns3.set_by_path(\"s3://some_other_bucket/some/more_complex/subfolder/structure/\")\n\n# Check new path structure\nprint(s3.get_s3_path())\n```\n\n#### get_s3_path(self)\nReturns a string containing the S3 path for the currently set bucket and subfolder. It takes no parameter.\n\nUsage Example:\n```\nfrom instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(bucket=\"some_bucket\", subfolder=\"subfolder/\")\n\nprint(s3.get_s3_path())\n```\n\n#### rename_file(self, new_filename, old_filename)\nTakes 2 strings containing file names and rename only the filename from path key, so the final result is similar to rename a file. It has no return value.\n\nUsage Example:\n```\nfrom instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(bucket=\"some_bucket\", subfolder=\"subfolder/\")\n\ns3.rename_file(\"new_name\", \"old_name\")\n```\n\n#### rename_subfolder(self, new_subfolder)\nTakes a string containing the new subfolder name and renames all keys in the currently set path, so the final result is similar to rename a subfolder. It has no return value.\n\nUsage Example:\n```\nfrom instackup.s3_tools import S3Tool\n\n\nold_subfolder = \"some/more_complex/subfolder/structure/\"\nnew_subfolder = \"some/new/subfolder/structure/\"\n\ns3 = S3Tool(bucket=\"some_bucket\", subfolder=old_subfolder)\n\n# The final result is similar to just rename the \"more_complex\" folder to \"new\"\ns3.rename_subfolder(new_subfolder)\n```\n\n#### list_all_buckets(self)\nReturns a list of all Buckets in S3. It takes no parameter.\n\nUsage Example:\n```\nfrom instackup.s3_tools import S3Tool\n\n\n# Setting or not a subfolder doesn't change the output of this function\ns3 = S3Tool(bucket=\"some_bucket\")\n\nall_buckets = s3.list_all_buckets()\n\n# some code here\n```\n\n#### list_contents(self, yield_results=False):\nLists all files that correspond with bucket and subfolder set at the initialization.\n\nIt can either return a list or yield a generator. Lists can be more familiar to use, but when dealing with large amounts of data, yielding the results may be a better option in terms of efficiency.\n\nFor more information on how to use generators and yield, check this video:\nhttps://www.youtube.com/watch?v=bD05uGo_sVI\n\nUsage Example:\n```\nfrom instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\npath_contents = s3.list_contents()\n\nif len(path_contents) == 0:\n    s3.set_subfolder(\"logs/subfolder/\")\n\n    # When a specific bucket/ bucket + subfolder contains a lot of data,\n    # that's when yielding the results may be more efficient.\n    for file in s3.list_contents(yield_results=True):\n        # Do something\n\n# some code here\n```\n\n#### upload_file(self, filename, remote_path=None)\nUploads file to remote path in S3.\n\nremote_path can take either a full S3 path or a subfolder only one. It has no return value.\n\nIf the remote_path parameter is not set, it will default to whatever subfolder\nis set in instance of the class plus the file name that is being uploaded.\n\nUsage Example:\n```\nfrom instackup.s3_tools import S3Tool\n\n\nfile_location = \"C:\\\\Users\\\\USER\\\\Desktop\\\\file.csv\"\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\n# upload_file method accepts all 3 options\ns3.upload_file(file_location)\ns3.upload_file(file_location, \"s3://some_bucket/other_subfolder/\")\ns3.upload_file(file_location, \"another_subfolder/\")  # Just subfolder\n```\n\n#### upload_subfolder(self, folder_path)\nNot implemented.\n\n#### download_file(self, remote_path, filename=None)\nDownloads remote S3 file to local path.\n\nremote_path can take either a full S3 path or a subfolder only one. It has no return value.\n\nIf the filename parameter is not set, it will default to whatever subfolder\nis set in instance of the class plus the file name that is being downloaded.\n\nUsage Example:\n```\nfrom instackup.s3_tools import S3Tool\n\n\nfile_desired_location = \"C:\\\\Users\\\\USER\\\\Desktop\\\\file.csv\"\nremote_location = \"s3://some_bucket/other_subfolder/file.csv\"\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\n# download_file method accepts both options\ns3.download_file(remote_location)\ns3.download_file(remote_location, file_desired_location)\n```\n\n#### download_subfolder(self)\nNot implemented.\n\n#### delete_file(self, filename, fail_silently=False)\nDeletes file from currently set path. It has no return value.\n\nRaises an error if file doesn't exist and fail_silently parameter is set to False.\n\nUsage Example:\n```\nfrom instackup.s3_tools import S3Tool\n\n\nfilename = \"file.csv\"\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\ns3.delete_file(file_location)\n\n# Will fail to delete the same file it was deleted before,\n# but won't raise any error due to fail_silently being set to True\ns3.delete_file(file_location, fail_silently=True)\n```\n\n#### delete_subfolder(self)\nDeletes all files with subfolder prefix, so the final result is similar to deleting a subfolder. It has no return value.\n\nRaises an error if file doesn't exist and fail_silently parameter is set to False.\n\nOnce the subfolder is deleted, it resets to no extra path (empty subfolder name).\n\nUsage Example:\n```\nfrom instackup.s3_tools import S3Tool\n\n\nfilename = \"file.csv\"\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\ns3.delete_folder()\n\n# Check new path structure\nprint(s3.get_s3_path())\n```\n\n# Version log\nSee what changed in every version.\n\n### Version 0.0.3 (alpha)\nThird alpha release.\n\n#### New functionalities:\n- bigquery_tools\n  - BigQueryTool\n    - list_datasets\n    - list_tables_in_dataset\n    - get_table_schema\n    - \\_\\_job_preparation_file_upload (private method)\n    - upload_from_gcs\n    - upload_from_file\n\n#### Functionalities still in development:\n- gcloudstorage_tools\n  - GCloudStorageTool\n    - rename_file\n    - rename_subfolder\n    - upload_subfolder\n    - download_subfolder\n    - delete_file\n    - delete_subfolder\n- s3_tools\n  - S3Tool\n    - upload_subfolder\n    - download_subfolder\n\n### Version 0.0.2 (alpha)\nSecond alpha release.\n\n#### Added modules:\n- gcloudstorage_tools\n\nInside this module, these classes and functions/methods were added:\n- GCloudStorageTool\n  - \\_\\_init\\_\\_\n  - bucket @property\n  - set_bucket\n  - set_subfolder\n  - set_blob\n  - set_by_path\n  - get_gs_path\n  - list_all_buckets\n  - get_bucket_info\n  - list_bucket_attributes\n  - get_blob_info\n  - list_blob_attributes\n  - list_contents\n  - upload_file\n  - download_file\n  - download_on_dataframe\n\n#### New functionalities:\n- bigquery_tools\n  - BigQueryTool\n    - convert_dataframe_to_numeric\n    - clean_dataframe_column_names\n- general_tools\n  - unicode_to_ascii\n  - parse_remote_uri\n\n#### Modified functionalities:\n- bigquery_tools\n  - BigQueryTool\n    - upload\n\n#### Deleted functionalities:\n- gcloudstorage_tools\n  - parse_gs_path\n- s3_tools\n  - parse_s3_path\n\n#### Functionalities still in development:\n- gcloudstorage_tools\n  - GCloudStorageTool\n    - rename_file\n    - rename_subfolder\n    - upload_subfolder\n    - download_subfolder\n    - delete_file\n    - delete_subfolder\n- s3_tools\n  - S3Tool\n    - upload_subfolder\n    - download_subfolder\n\n### Version 0.0.1 (alpha)\nFirst alpha release:\n\nAdded modules:\n- bigquery_tools\n- general_tools\n- redshift_tools\n- s3_tools\n\nInside those modules, these classes and functions/methods were added:\n- BigQueryTool\n  - \\_\\_init\\_\\_\n  - query\n  - upload\n  - start_transfer\n- fetch_credentials\n- RedShiftTool\n  - \\_\\_init\\_\\_\n  - connect\n  - commit\n  - rollback\n  - execute_sql\n  - query\n  - unload_to_S3\n  - close_connection\n  - \\_\\_enter\\_\\_\n  - \\_\\_exit\\_\\_\n- parse_s3_path\n- S3Tool\n  - \\_\\_init\\_\\_\n  - bucket @property\n  - set_bucket\n  - set_subfolder\n  - set_by_path\n  - get_s3_path\n  - rename_file\n  - rename_subfolder\n  - list_all_buckets\n  - list_contents\n  - upload_file\n  - download_file\n  - delete_file\n  - delete_subfolder\n\nModules still in development:\n- gcloudstorage_tools\n\nInside this module, these classes and functions/methods are in development:\n- parse_gs_path\n- GCloudStorageTool\n  - \\_\\_init\\_\\_\n  - bucket @property\n  - set_bucket\n  - set_subfolder\n  - set_by_path\n  - get_gs_path\n  - list_all_buckets\n  - list_bucket_contents\n  - upload_file\n  - download_file\n- S3Tool\n  - upload_subfolder\n  - download_subfolder\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Lavedonio/instackup", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "instackup", "package_url": "https://pypi.org/project/instackup/", "platform": "any", "project_url": "https://pypi.org/project/instackup/", "project_urls": {"Homepage": "https://github.com/Lavedonio/instackup"}, "release_url": "https://pypi.org/project/instackup/0.0.4/", "requires_dist": ["pyyaml", "boto3", "google-cloud-bigquery", "google-cloud-bigquery-datatransfer", "google-cloud-storage (>=1.18.0)", "gcsfs", "pandas", "psycopg2"], "requires_python": ">=3.6", "summary": "A package to ease interaction with cloud services, DB connections and commonly used functionalities in data analytics.", "version": "0.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Instackup</h1>\n<p>This Python library is an open source way to standardize and simplify connections with cloud-based tools and databases and commonly used tools in data manipulation and analysis.</p>\n<h1>Index</h1>\n<ul>\n<li>Current release</li>\n<li>Prerequisites</li>\n<li>Installation</li>\n<li>Documentation\n<ul>\n<li>bigquery_tools</li>\n<li>gcloudstorage_tools</li>\n<li>general_tools</li>\n<li>redshift_tools</li>\n<li>s3_tools</li>\n</ul>\n</li>\n<li>Version log</li>\n</ul>\n<h1>Current release</h1>\n<h2>Version 0.0.4 (alpha)</h2>\n<p>Fourth alpha release.</p>\n<h4>New functionalities:</h4>\n<ul>\n<li>bigquery_tools\n<ul>\n<li>BigQueryTool\n<ul>\n<li>query_and_save_results</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4>Modified functionalities:</h4>\n<ul>\n<li>general_tools\n<ul>\n<li>fetch_credentials</li>\n</ul>\n</li>\n</ul>\n<h4>Functionalities still in development:</h4>\n<ul>\n<li>gcloudstorage_tools\n<ul>\n<li>GCloudStorageTool\n<ul>\n<li>rename_file</li>\n<li>rename_subfolder</li>\n<li>upload_subfolder</li>\n<li>download_subfolder</li>\n<li>delete_file</li>\n<li>delete_subfolder</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>s3_tools\n<ul>\n<li>S3Tool\n<ul>\n<li>upload_subfolder</li>\n<li>download_subfolder</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h1>Prerequisites</h1>\n<ol>\n<li>Have a <a href=\"https://www.python.org/downloads/\" rel=\"nofollow\">Python 3.6 version or superior</a> installed;</li>\n<li>Create a YAML (or JSON) file with credentials information;</li>\n<li>[Optional but recommended] Configure an Environment Variable that points where the Credentials file is.</li>\n</ol>\n<h3>1. Have a Python 3.6 version or superior installed</h3>\n<p>Got to this <a href=\"https://www.python.org/downloads/\" rel=\"nofollow\">link</a> e download the most current version that is compatible with this package.</p>\n<h3>2. Create a YAML (or JSON) file with credentials information</h3>\n<p>Use the files <a href=\"https://github.com/Lavedonio/instackup/blob/master/credentials/secret_template.yml\" rel=\"nofollow\">secret_template.yml</a> or <a href=\"https://github.com/Lavedonio/instackup/blob/master/credentials/secret_blank.yml\" rel=\"nofollow\">secret_blank.yml</a> as a base or copy and paste the code bellow and modify its values to the ones in your credentials/projects:</p>\n<pre><code>#################################################################\n#                                                               #\n#        ACCOUNTS CREDENTIALS. DO NOT SHARE THIS FILE.          #\n#                                                               #\n# Specifications:                                               #\n# - For the credentials you don't have, leave it blank.         #\n# - Keep Google's secret file in the same folder as this file.  #\n# - BigQuery project_ids must be strings, i.e., inside quotes.  #\n#                                                               #\n# Recommendations:                                              #\n# - YAML specification: https://yaml.org/spec/1.2/spec.html     #\n# - Keep this file in a static path like a folder within the    #\n# Desktop. Ex.: C:\\Users\\USER\\Desktop\\Credentials\\secret.yml    #\n#                                                               #\n#################################################################\n\n\nGoogle:\n  secret_filename: file.json\n\nBigQuery:\n  project_id:\n    project_name: \"000000000000\"\n\nAWS:\n  access_key: AWSAWSAWSAWSAWSAWSAWS\n  secret_key: CcasldUYkfsadcSDadskfDSDAsdUYalf\n\nRedShift:\n  cluster_credentials:\n    dbname: db\n    user: masteruser\n    host: blablabla.random.us-east-2.redshift.amazonaws.com\n    cluster_id: cluster\n    port: 5439\n  master_password:\n    dbname: db\n    user: masteruser\n    host: blablabla.random.us-east-2.redshift.amazonaws.com\n    password: masterpassword\n    port: 5439\n</code></pre>\n<p>Save this file with <code>.yml</code> extension in a folder where you know the path won't be modified, like the Desktop folder (Example: <code>C:\\Users\\USER\\Desktop\\Credentials\\secret.yml</code>).</p>\n<p>If you prefer, you can follow this step using a JSON file instead. Follow the same instructions but using <code>.json</code> instead of <code>.yml</code>.</p>\n<h3>3. [Optional but recommended] Configure an Environment Variable that points where the Credentials file is.</h3>\n<p>To configure the Environment Variable, follow the instructions bellow, based on your Operating System.</p>\n<h4>Windows</h4>\n<ol>\n<li>Place the YAML (or JSON) file in a folder you won't change its name or path later;</li>\n<li>In Windows Search, type <code>Environment Variables</code> and click in the Control Panel result;</li>\n<li>Click on the button <code>Environment Variables...</code>;</li>\n<li>In <strong>Environment Variables</strong>, click on the button <code>New</code>;</li>\n<li>In <strong>Variable name</strong> type <code>CREDENTIALS_HOME</code> and in <strong>Variable value</strong> paste the full path to the recently created YAML (or JSON) file;</li>\n<li>Click <strong>Ok</strong> in the 3 open windows.</li>\n</ol>\n<h4>Linux/MacOS</h4>\n<ol>\n<li>Place the YAML (or JSON) file in a folder you won't change its name or path later;</li>\n<li>Open the file <code>.bashrc</code>. If it doesn't exists, create one in the <code>HOME</code> directory. If you don't know how to get there, open the Terminal, type <code>cd</code> and then <strong>ENTER</strong>;</li>\n<li>Inside the file, in a new line, type the command: <code>export CREDENTIALS_HOME=\"/path/to/file\"</code>, replacing the content inside quotes by the full path to the recently created YAML (or JSON) file;</li>\n<li>Save the file and restart all open Terminal windows.</li>\n</ol>\n<blockquote>\n<p><strong>Note:</strong> If you don't follow this last prerequisite, you need to set the environment variable manually inside the code. To do that, inside your python code, after the imports, type the command (replacing the content inside quotes by the full path to the recently created YAML (or JSON) file):</p>\n</blockquote>\n<pre><code>os.environ[\"CREDENTIALS_HOME\"] = \"/path/to/file\"\n</code></pre>\n<h1>Installation</h1>\n<p>Go to the Terminal and type:</p>\n<pre><code>pip install instackup\n</code></pre>\n<h1>Documentation</h1>\n<h2>bigquery_tools</h2>\n<h3>BigQueryTool</h3>\n<p>This class handle most of the interaction needed with BigQuery, so the base code becomes more readable and straightforward.</p>\n<h4>__init__(self)</h4>\n<p>Initialization takes no parameter and has no return value. It sets the bigquery client.</p>\n<p>Usage example:</p>\n<pre><code>from instackup.bigquery_tools import BigQueryTool\n\n\nbq = BigQueryTool()\n</code></pre>\n<h4>query(self, sql_query)</h4>\n<p>Run a SQL query and return the results as a Pandas Dataframe.</p>\n<p>Usage example:</p>\n<pre><code>import pandas as pd\nfrom instackup.bigquery_tools import BigQueryTool\n\n\nbq = BigQueryTool()\n\nsql_query = \"\"\"SELECT * FROM `project_name.dataset.table`\"\"\"\ndf = bq.query(sql_query)\n</code></pre>\n<h4>query_and_save_results(self, sql_query, dest_dataset, dest_table, writing_mode=\"TRUNCATE\", create_table_if_needed=False)</h4>\n<p>Executes a query and saves the result in a table.</p>\n<p>writing_mode parameter determines how the data is going to be written in BigQuery.\nDoes not apply if table doesn't exist. Can be one of 3 types (defaults to 'TRUNCATE'):</p>\n<ul>\n<li>APPEND: If the table already exists, BigQuery appends the data to the table.</li>\n<li>EMPTY: If the table already exists and contains data, a 'duplicate' error\nis returned in the job result.</li>\n<li>TRUNCATE: If the table already exists, BigQuery overwrites the table data.</li>\n</ul>\n<p>If create_table_if_needed is set to False and the table doesn't exist, it'll raise an error.\nDafaults to False.</p>\n<p>Usage example:</p>\n<pre><code>from instackup.bigquery_tools import BigQueryTool\n\n\n# Enter valid values here\ndest_dataset = \"dataset\"\ndest_table = \"some_other_table\"\nsql_query = \"\"\"SELECT * FROM `project_name.dataset.table`\"\"\"\n\nbq = BigQueryTool()\n\nbq.query_and_save_results(self, sql_query, dest_dataset, dest_table, create_table_if_needed=True)\n</code></pre>\n<h4>list_datasets(self)</h4>\n<p>Returns a list with all dataset names inside the project.</p>\n<p>Usage example:</p>\n<pre><code>from instackup.bigquery_tools import BigQueryTool\n\n\nbq = BigQueryTool()\n\ndatasets = bq.list_datasets()\n\nprint(\"There are {num} datasets, which are listed bellow:\\n\".format(num=len(datasets)))\nfor ds in datasets:\n    print(ds)\n</code></pre>\n<h4>list_tables_in_dataset(self, dataset, get=None, return_type=\"dict\")</h4>\n<p>Lists all tables inside a dataset. Will fail if dataset doesn't exist.</p>\n<p>get parameter can be a string or list of strings. If only a string is passed,\nwill return a list of values of that attribute of all tables\n(this case overrides return_type parameter).</p>\n<p>Valid get parameters are:\n[\"clustering_fields\", \"created\", \"dataset_id\", \"expires\", \"friendly_name\",\n\"full_table_id\", \"labels\", \"partition_expiration\", \"partitioning_type\", \"project\",\n\"reference\", \"table_id\", \"table_type\", \"time_partitioning\", \"view_use_legacy_sql\"]</p>\n<p>return_type parameter can be 1 out of 3 types and sets how the result will be returned:</p>\n<ul>\n<li>dict: dictionary of lists, i.e., each key has a list of all tables values for that attribute.\nThe same index for different attibutes refer to the same table;</li>\n<li>list: list of dictionaries, i.e., each item in the list is a dictionary with all the attributes\nof the respective table;</li>\n<li>dataframe: Pandas DataFrame.</li>\n</ul>\n<p>Usage example:</p>\n<pre><code>from instackup.bigquery_tools import BigQueryTool\n\n\nbq = BigQueryTool()\n\ndataset = \"dataset\"  # Enter a valid dataset name\n\ntables = bq.list_tables_in_dataset(dataset, get=\"table_id\")  # Getting only table name\n\nprint(\"There are {num} tables in {ds}, which are listed bellow:\\n\".format(num=len(tables), ds=dataset))\nfor tb in tables:\n    print(tb)\n\n# Getting all table info\ndf = bq.list_tables_in_dataset(dataset, return_type=\"dataframe\")\nprint(df)\n</code></pre>\n<h4>get_table_schema(self, dataset, table)</h4>\n<p>Gets schema information and returns a properly formatted dictionary.</p>\n<p>Usage example:</p>\n<pre><code>import json\nfrom instackup.bigquery_tools import BigQueryTool\n\n\nbq = BigQueryTool()\n\ndataset = \"dataset\"  # Enter a valid dataset name\ntable = \"table\"      # Enter a valid table name\n\nschema = bq.get_table_schema(dataset, table)\n\nwith open('data.json', 'w') as fp:\n    json.dump(schema, fp, sort_keys=True, indent=4)\n</code></pre>\n<h4>convert_dataframe_to_numeric(dataframe, exclude_columns=[], **kwargs)</h4>\n<p>Transform all string type columns into floats, except those in exclude_columns list.</p>\n<p>**kwargs are passed directly to pandas.to_numeric method.\nThe complete documentation of this method can be found here:\n<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html\" rel=\"nofollow\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html</a></p>\n<p>Usage example:</p>\n<pre><code>import pandas as pd\nfrom instackup.bigquery_tools import BigQueryTool\n\n\n# You can often find these kind of data when reading from a file\ndf = pd.DataFrame({\"col.1\": [\"1\", \"2\"], \"col.2\": [\"3\", \"junk\"], \"col.3\": [\"string1\", \"string2\"]})\n\nbq = BigQueryTool()\ndf = bq.convert_dataframe_to_numeric(df, exclude_columns=[\"col.3\"], errors=\"coerce\")\nprint(df)\n\n# output:\n#\n#    col.1  col.2    col.3\n# 0      1    3.0  string1\n# 1      2    NaN  string2\n</code></pre>\n<h4>clean_dataframe_column_names(dataframe, allowed_chars=\"abcdefghijklmnopqrstuvwxyz0123456789\", special_treatment={})</h4>\n<p>Replace dataframe columns to only contain chars allowed in BigQuery tables column name.</p>\n<p>special_treatment dictionary substitutes the terms in the keys by its value pair.</p>\n<p>Usage example:</p>\n<pre><code>import pandas as pd\nfrom instackup.bigquery_tools import BigQueryTool\n\n\n# You can often find these kind of data when reading from a file\ndf = pd.DataFrame({\"col.1\": [\"1\", \"2\"], \"col.2\": [\"3\", \"junk\"], \"col.3!\": [\"string1\", \"string2\"]})\n\nbq = BigQueryTool()\ndf = bq.clean_dataframe_column_names(df, special_treatment={\"!\": \"_factorial\"})\nprint(df)\n\n# output:\n#\n#   col_1 col_2 col_3_factorial\n# 0     1     3         string1\n# 1     2  junk         string2\n</code></pre>\n<h4>upload(self, dataframe, dataset, table, **kwargs)</h4>\n<p>Prepare dataframe columns and executes an insert SQL command into BigQuery.</p>\n<p>**kwargs are passed directly to pandas.to_gbq method.\nThe complete documentation of this method can be found here:\n<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_gbq.html\" rel=\"nofollow\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_gbq.html</a></p>\n<p>Usage example:</p>\n<pre><code>import pandas as pd\nfrom instackup.bigquery_tools import BigQueryTool\n\n\nfixed_data = {\n  'col1': [1, 2],\n  'col2': [0.5, 0.75]\n}\n\ndf = pd.DataFrame(fixed_data)\n\ndataset = \"some_dataset_name\"\ntable = \"some_table_name\"\n\nbq = BigQueryTool()\nbq.upload(df, dataset, table)\n</code></pre>\n<h4>upload_from_gcs(self, dataset, table, gs_path, file_format=\"CSV\", header_rows=1, delimiter=\",\", encoding=\"UTF-8\", writing_mode=\"APPEND\", create_table_if_needed=False, schema=None)</h4>\n<p>Uploads data from Google Cloud Storage directly to BigQuery.</p>\n<p>dataset and table parameters determines the destination of the upload.\ngs_path parameter is the file location in Google Cloud Storage.\nAll 3 of them are required string parameters.</p>\n<p>file_format can be either 'AVRO', 'CSV', 'JSON', 'ORC' or 'PARQUET'. Defaults to 'CSV'.\nheader_rows, delimiter and encoding are only used when file_format is 'CSV'.</p>\n<p>header_rows parameter determine the length in rows of the 'CSV' file given.\nShould be 0 if there are no headers in the file. Defaults to 1.</p>\n<p>delimiter determines the string character used to delimite the data. Defaults to ','.</p>\n<p>encoding tells the file encoding. Can be either 'UTF-8' or 'ISO-8859-1' (latin-1).\nDefaults to 'UTF-8'.</p>\n<p>writing_mode parameter determines how the data is going to be written in BigQuery.\nDoes not apply if table doesn't exist. Can be one of 3 types (defaults in 'APPEND'):</p>\n<ul>\n<li>APPEND: If the table already exists, BigQuery appends the data to the table.</li>\n<li>EMPTY: If the table already exists and contains data, a 'duplicate' error\nis returned in the job result.</li>\n<li>TRUNCATE: If the table already exists, BigQuery overwrites the table data.</li>\n</ul>\n<p>If create_table_if_needed is set to False and the table doesn't exist, it'll raise an error.\nDafaults to False.</p>\n<p>schema is either a list of dictionaries containing the schema information or\na dictionary encapsulating the previous list with a key of 'fields'.\nThis latter format can be found when directly importing the schema info from a JSON generated file.\nIf the file_format is either 'CSV' or 'JSON' or the table already exists, it can be ommited.</p>\n<p>Usage example:</p>\n<pre><code>import json\nfrom instackup.bigquery_tools import BigQueryTool\n\n\n# Enter valid values here\ndataset = \"sandbox\"\ntable = \"test\"\ngs_path = \"gs://some-bucket/some-subfolder/test.json\"\n\n# schema must be in the same format of the output of get_table_schema method.\nwith open('data.json', 'r') as fp:\n    schema = json.load(fp)\n\nbq.upload_from_gcs(dataset, table, gs_path, file_format=\"JSON\", create_table_if_needed=True, schema=schema)\n</code></pre>\n<h4>upload_from_file(self, dataset, table, file_location, file_format=\"CSV\", header_rows=1, delimiter=\",\", encoding=\"UTF-8\", writing_mode=\"APPEND\", create_table_if_needed=False, schema=None)</h4>\n<p>Uploads data from a local file to BigQuery.</p>\n<p>dataset and table parameters determines the destination of the upload.\nfile_location parameter is either the file full or relative path in the local computer.\nAll 3 of them are required string parameters.</p>\n<p>file_format can be either 'AVRO', 'CSV', 'JSON', 'ORC' or 'PARQUET'. Defaults to 'CSV'.\nheader_rows, delimiter and encoding are only used when file_format is 'CSV'.</p>\n<p>header_rows parameter determine the length in rows of the 'CSV' file given.\nShould be 0 if there are no headers in the file. Defaults to 1.</p>\n<p>delimiter determines the string character used to delimite the data. Defaults to ','.</p>\n<p>encoding tells the file encoding. Can be either 'UTF-8' or 'ISO-8859-1' (latin-1).\nDefaults to 'UTF-8'.</p>\n<p>writing_mode parameter determines how the data is going to be written in BigQuery.\nDoes not apply if table doesn't exist. Can be one of 3 types (defaults in 'APPEND'):</p>\n<ul>\n<li>APPEND: If the table already exists, BigQuery appends the data to the table.</li>\n<li>EMPTY: If the table already exists and contains data, a 'duplicate' error\nis returned in the job result.</li>\n<li>TRUNCATE: If the table already exists, BigQuery overwrites the table data.</li>\n</ul>\n<p>If create_table_if_needed is set to False and the table doesn't exist, it'll raise an error.\nDafaults to False.</p>\n<p>schema is either a list of dictionaries containing the schema information or\na dictionary encapsulating the previous list with a key of 'fields'.\nThis latter format can be found when directly importing the schema info from a JSON generated file.\nIf the file_format is either 'CSV' or 'JSON' or the table already exists, it can be ommited.</p>\n<p>Usage example:</p>\n<pre><code>import json\nfrom instackup.bigquery_tools import BigQueryTool\n\n\n# Enter valid values here\ndataset = \"sandbox\"\ntable = \"test\"\nfile_location = \"test.csv\"\n\n# schema must be in the same format of the output of get_table_schema method.\nwith open('data.json', 'r') as fp:\n    schema = json.load(fp)\n\nbq.upload_from_file(dataset, table, file_location, create_table_if_needed=True, schema=schema)\n</code></pre>\n<h4>start_transfer(self, project_path=None, project_name=None, transfer_name=None)</h4>\n<p>Takes a project path or both project name and transfer name to trigger a transfer to start executing in BigQuery Transfer. Returns a status indicating if the request was processed (if it does, the response should be 'PENDING').\nAPI documentation: <a href=\"https://googleapis.dev/python/bigquerydatatransfer/latest/gapic/v1/api.html\" rel=\"nofollow\">https://googleapis.dev/python/bigquerydatatransfer/latest/gapic/v1/api.html</a></p>\n<p>Usage example:</p>\n<pre><code>from instackup.bigquery_tools import BigQueryTool\n\n\ntransfer_config = \"projects/000000000000/transferConfigs/00000000-0000-0000-0000-000000000000\"\n\nuse_project_path = True\nprint(\"Starting transfer...\")\n\n# Both options do the same thing\nif use_project_path:\n    state_response = bq.start_transfer(project_path=transfer_config)\nelse:\n    state_response = bq.start_transfer(project_name=\"project_name\", transfer_name=\"transfer_name\")\n\nprint(f\"Transfer status: {state_response}\")\n</code></pre>\n<h2>gcloudstorage_tools</h2>\n<h3>GCloudStorageTool</h3>\n<p>This class handle most of the interaction needed with Google Cloud Storage,\nso the base code becomes more readable and straightforward.</p>\n<h4>__init__(self, bucket=None, subfolder=\"\", gs_path=None)</h4>\n<p>Takes a either gs_path or both bucket name and subfolder name as parameters to set the current working directory. It also opens a connection with Google Cloud Storage.</p>\n<p>The paradigm of this class is that all the operations are done in the current working directory, so it is important to set the right path (you can reset it later, but still).</p>\n<p>Usage example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\n# or\n\ngs = GCloudStorageTool(bucket=\"some_other_bucket\", subfolder=\"some_subfolder/subpath/\")\n</code></pre>\n<h4>bucket(self) @property</h4>\n<p>Returns the bucket object from the client based on the bucket name given in __init__ or set_bucket</p>\n<h4>set_bucket(self, bucket)</h4>\n<p>Takes a string as a parameter to reset the bucket name and bucket object. It has no return value.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\ngs.set_bucket(\"some_other_bucket\")\n\n# Check new path structure\nprint(gs.get_gs_path())\n</code></pre>\n<h4>set_subfolder(self, subfolder)</h4>\n<p>Takes a string as a parameter to reset the subfolder name. It has no return value.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\ngs.set_subfolder(\"some/more_complex/subfolder/structure/\")\n\n# Check new path structure\nprint(gs.get_gs_path())\n</code></pre>\n<h4>set_by_path(self, s3_path)</h4>\n<p>Takes a string as a parameter to reset the bucket name and subfolder name by its GS path. It has no return value.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\ngs.set_by_path(\"gs://some_other_bucket/some/more_complex/subfolder/structure/\")\n\n# Check new path structure\nprint(gs.get_gs_path())\n</code></pre>\n<h4>get_gs_path(self)</h4>\n<p>Returns a string containing the GS path for the currently set bucket and subfolder. It takes no parameter.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\nprint(gs.get_gs_path())\n</code></pre>\n<h4>list_all_buckets(self)</h4>\n<p>Returns a list of all Buckets in Google Cloud Storage. It takes no parameter.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\n# Setting or not a subfolder doesn't change the output of this function\ngs = GCloudStorageTool(bucket=\"some_bucket\")\n\nall_buckets = gs.list_all_buckets()\n\n# some code here\n</code></pre>\n<h4>get_bucket_info(self, bucket=None)</h4>\n<p>Returns a dictionary with the information of Name, Datetime Created, Datetime Updated and Owner ID\nof the currently selected bucket (or the one passed in the parameters).</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(bucket=\"some_bucket\")\n\nbucket_info = gs.get_bucket_info()\nprint(bucket_info)\n</code></pre>\n<h4>list_bucket_attributes(self)</h4>\n<p>A list of all curently supported bucket attributes that comes in get_bucket_info method return dictionary.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(bucket=\"some_bucket\")\n\nbucket_info_attributes = gs.list_bucket_attributes()\nprint(bucket_info_attributes)\n</code></pre>\n<h4>get_blob_info(self)</h4>\n<p>Converts a google.cloud.storage.Blob (which represents a storage object) to context format (GCS.BucketObject).</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(bucket=\"some_bucket\", subfolder=\"some_subfolder\")\ngs.set_blob(\"some_subfolder/file.csv\")\n\nblob_info_attributes = gs.get_blob_info()\nprint(blob_info_attributes)\n</code></pre>\n<h4>list_blob_attributes(self)</h4>\n<p>A list of all curently supported bucket attributes that comes in get_blob_info method return dictionary.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(bucket=\"some_bucket\")\ngs.set_blob(\"some_subfolder/file.csv\")\n\nblob_info_attributes = gs.list_blob_attributes()\nprint(blob_info_attributes)\n</code></pre>\n<h4>list_contents(self, yield_results=False):</h4>\n<p>Lists all files that correspond with bucket and subfolder set at the initialization.</p>\n<p>It can either return a list or yield a generator. Lists can be more familiar to use, but when dealing with large amounts of data, yielding the results may be a better option in terms of efficiency.</p>\n<p>For more information on how to use generators and yield, check this video:\n<a href=\"https://www.youtube.com/watch?v=bD05uGo_sVI\" rel=\"nofollow\">https://www.youtube.com/watch?v=bD05uGo_sVI</a></p>\n<p>Usage Example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\npath_contents = gs.list_contents()\n\nif len(path_contents) == 0:\n    s3.set_subfolder(\"logs/subfolder/\")\n\n    # When a specific bucket/ bucket + subfolder contains a lot of data,\n    # that's when yielding the results may be more efficient.\n    for file in gs.list_contents(yield_results=True):\n        # Do something\n\n# some code here\n</code></pre>\n<h4>rename_file(self, new_filename, old_filename)</h4>\n<p>Not implemented.</p>\n<h4>rename_subfolder(self, new_subfolder)</h4>\n<p>Not implemented.</p>\n<h4>upload_file(self, filename, remote_path=None)</h4>\n<p>Uploads file to remote path in Google Cloud Storage (GS).</p>\n<p>remote_path can take either a full GS path or a subfolder only one.</p>\n<p>If the remote_path parameter is not set, it will default to whatever subfolder\nis set in instance of the class plus the file name that is being uploaded.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\nfile_location = \"C:\\\\Users\\\\USER\\\\Desktop\\\\file.csv\"\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\n# upload_file method accepts all 3 options\ngs.upload_file(file_location)\ngs.upload_file(file_location, \"gs://some_bucket/other_subfolder/\")\ngs.upload_file(file_location, \"another_subfolder/\")  # Just subfolder\n</code></pre>\n<h4>upload_subfolder(self, folder_path)</h4>\n<p>Not implemented.</p>\n<h4>download_file(self, fullfilename=None, replace=False)</h4>\n<p>Downloads remote gs file to local path.</p>\n<p>If the fullfilename parameter is not set, it will default to the currently set blob.</p>\n<p>If replace is set to True and there is already a file downloaded with the same filename and path,\nit will replace the file. Otherwise it will create a new file with a number attached to the end.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\nfile_location = \"gs://some_bucket/other_subfolder/\"\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\n# download_file method accepts both options\ngs.download_file(file_location)\ngs.download_file(file_location, \"C:\\\\Users\\\\USER\\\\Desktop\\\\file.csv\")\n</code></pre>\n<h4>download_subfolder(self)</h4>\n<p>Not implemented.</p>\n<h4>download_on_dataframe(self, **kwargs)</h4>\n<p>Use blob information to download file and use it directly on a Pandas DataFrame\nwithout having to save the file.</p>\n<p>**kwargs are passed directly to pandas.read_csv method.\nThe complete documentation of this method can be found here:\n<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\" rel=\"nofollow\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html</a></p>\n<p>Usage Example:</p>\n<pre><code>from instackup.gcloudstorage_tools import GCloudStorageTool\n\n\nfile_location = \"gs://some_bucket/other_subfolder/\"\n\ngs = GCloudStorageTool(gs_path=\"gs://some_bucket/subfolder/\")\n\n# For a well behaved file, you may just use the method directly\ngs.set_blob(\"subfolder/file.csv\")\ndf = gs.download_on_dataframe()\n\n# For a file with a weird layout, you may want to use some parameters to save some time in data treatment\ngs.set_blob(\"subfolder/weird_file.csv\")\ndf = gs.download_on_dataframe(sep=\";\", encoding=\"ISO-8859-1\", decimal=\",\")\n</code></pre>\n<h4>delete_file(self)</h4>\n<p>Not implemented.</p>\n<h4>delete_subfolder(self)</h4>\n<p>Not implemented.</p>\n<h2>general_tools</h2>\n<h3>fetch_credentials(service_name, **kwargs)</h3>\n<p>Gets the credentials from the secret file set in <code>CREDENTIALS_HOME</code> variable and returns the credentials of the selected service in a dictionary. If service is \"credentials_path\", a path is returned instead.</p>\n<p>It's meant to be used basically by the other modules, not actually by the user of the library.</p>\n<p>Usage example:</p>\n<pre><code>from instackup.general_tools import fetch_credentials\n\nprint(fetch_credentials(service_name=\"Google\"))\nprint(fetch_credentials(\"AWS\"))\nprint(fetch_credentials(\"RedShift\", connection_type=\"cluster_credentials\"))\nprint(fetch_credentials(\"credentials_path\"))\n</code></pre>\n<h3>unicode_to_ascii(unicode_string)</h3>\n<p>Replaces all non-ascii chars in string by the closest possible match.</p>\n<p>This solution was inpired by this answer:\n<a href=\"https://stackoverflow.com/a/517974/11981524\" rel=\"nofollow\">https://stackoverflow.com/a/517974/11981524</a></p>\n<h3>parse_remote_uri(uri, service)</h3>\n<p>Parses a Google Cloud Storage (GS) or an Amazon S3 path into bucket and subfolder(s).\nRaises an error if path is with wrong format.</p>\n<p>service parameter can be either \"gs\" or \"s3\"</p>\n<p>Usage example:</p>\n<pre><code>from instackup.general_tools import parse_remote_uri\n\n\n### S3\ns3_path = \"s3://some_bucket/subfolder/\"\nbucket_name, subfolder = parse_remote_uri(s3_path, \"s3\")\n\nprint(f\"Bucket name: {bucket_name}\")  # output: &gt;&gt;&gt; some_bucket\nprint(f\"Subfolder: {subfolder}\")      # output: &gt;&gt;&gt; subfolder\n\n\n### Storage\ngs_path = \"gs://some_bucket/subfolder/\"\nbucket_name, subfolder = parse_remote_uri(gs_path, \"gs\")\n\nprint(f\"Bucket name: {bucket_name}\")  # output: &gt;&gt;&gt; some_bucket\nprint(f\"Subfolder: {subfolder}\")      # output: &gt;&gt;&gt; subfolder\n</code></pre>\n<h2>redshift_tools</h2>\n<h3>RedShiftTool</h3>\n<p>This class handle most of the interaction needed with RedShift, so the base code becomes more readable and straightforward.</p>\n<p>This class implements the with statement, so there are 2 ways of using it.</p>\n<p><strong>1st way:</strong></p>\n<pre><code>from instackup.redshift_tools import RedShiftTool\n\nwith RedShiftTool() as rs:\n    # use rs object to interact with RedShift database\n</code></pre>\n<p><strong>2nd way:</strong></p>\n<pre><code>from instackup.redshift_tools import RedShiftTool\n\nrs = RedShiftTool()\nrs.connect()\n\ntry:\n    # use rs object to interact with RedShift database\nexcept Exception as e:\n    rs.rollback()\n    raise e\nelse:\n    rs.commit()\nfinally:\n    rs.close_connection()\n</code></pre>\n<p>Easy to see that it is recommended (and easier) to use the first syntax.</p>\n<h4>__init__(self, connect_by_cluster=True)</h4>\n<p>Initialization takes connect_by_cluster parameter that sets connection type and has no return value.</p>\n<p>The __init__ method doesn't actually opens the connection, but sets all values required by the connect method.</p>\n<p>Usage example:</p>\n<pre><code>from instackup.bigquery import RedShiftTool\n\nrs = RedShiftTool()\n</code></pre>\n<h4>connect(self, fail_silently=False)</h4>\n<p>Create the connection using the __init__ attributes and returns its own object for with statement.</p>\n<p>If fail_silently parameter is set to True, any errors will be surpressed and not stop the code execution.</p>\n<p>Usage example:</p>\n<pre><code>from instackup.bigquery import RedShiftTool\n\nrs = RedShiftTool()\nrs.connect()\n# remember to close the connection later\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n</code></pre>\n<h4>commit(self)</h4>\n<p>Commits any pending transaction to the database. It has no extra parameter or return value.</p>\n<p>Usage example:</p>\n<pre><code>from instackup.bigquery import RedShiftTool\n\nrs = RedShiftTool()\nrs.connect()\n# Do stuff\nrs.commit()\n# remember to close the connection later\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n    # Do stuff\n\n    # No need to explictly commit as it will do when leaving this context, but nonetheless:\n    rs.commit()\n</code></pre>\n<h4>rollback(self)</h4>\n<p>Roll back to the start of any pending transaction. It has no extra parameter or return value.</p>\n<p>Usage example:</p>\n<pre><code>from instackup.bigquery import RedShiftTool\n\nrs = RedShiftTool()\nrs.connect()\n\ntry:\n    # Do stuff\nexcept Exception as e:\n    rs.rollback()\n    raise e\nelse:\n    rs.commit()\nfinally:\n    # remember to close the connection later\n    rs.close_connection()\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n    # Do stuff\n\n    # No need to explictly commit or rollback as it will do when leaving this context, but nonetheless:\n    if meet_condition:\n        rs.commit()\n    else:\n        rs.rollback()\n</code></pre>\n<h4>execute_sql(self, command, fail_silently=False)</h4>\n<p>Execute a SQL command (CREATE, UPDATE and DROP). It has no return value.</p>\n<p>If fail_silently parameter is set to True, any errors will be surpressed and not stop the code execution.</p>\n<p>Usage example:</p>\n<pre><code>from instackup.bigquery import RedShiftTool\n\n\nsql_cmd = \"\"\"CREATE TABLE test (\n    id          integer NOT NULL CONSTRAINT firstkey PRIMARY KEY,\n    username    varchar(40) UNIQUE NOT NULL,\n    fullname    varchar(64) NOT NULL,\n    created_at  TIMESTAMP NOT NULL,\n    last_login  TIMESTAMP\n);\n\"\"\"\n\n\nrs = RedShiftTool()\nrs.connect()\n\ntry:\n    # Execute the command\n    rs.execute_sql(sql_cmd)\n\nexcept Exception as e:\n    rs.rollback()\n    raise e\nelse:\n    rs.commit()\nfinally:\n    # remember to close the connection later\n    rs.close_connection()\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n    # This command would throw an error (since the table already was created before),\n    # but since fail_silently parameter is set to True, it'll catch the exception\n    # and let the code continue past this point.\n    rs.execute_sql(sql_cmd, fail_silently=True)\n\n    # other code\n</code></pre>\n<h4>query(self, sql_query, fetch_through_pandas=True, fail_silently=False)</h4>\n<p>Run a query and return the results.</p>\n<p>fetch_through_pandas parameter tells if the query should be parsed by psycopg2 cursor or pandas.</p>\n<p>If fail_silently parameter is set to True, any errors will be surpressed and not stop the code execution.</p>\n<p>Usage example:</p>\n<pre><code>from instackup.bigquery import RedShiftTool\n\n\nsql_query = \"\"\"SELECT * FROM table LIMIT 100\"\"\"\n\n\nrs = RedShiftTool()\nrs.connect()\n\ntry:\n    # Returns a list of tuples containing the rows of the response\n    table = rs.query(sql_cmd, fetch_through_pandas=False, fail_silently=True)\n\n    # Do something with table variable\n\nexcept Exception as e:\n    rs.rollback()\n    raise e\nelse:\n    rs.commit()\nfinally:\n    # remember to close the connection later\n    rs.close_connection()\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n    # Returns a Pandas dataframe\n    df = rs.query(sql_cmd)\n\n    # To do operations with dataframe, you'll need to import pandas library\n\n    # other code\n</code></pre>\n<h4>unload_to_S3(self, redshift_query, s3_path, filename, unload_options=\"MANIFEST GZIP ALLOWOVERWRITE REGION 'us-east-2'\")</h4>\n<p>Executes an unload command in RedShift database to copy data to S3.</p>\n<p>Takes the parameters redshift_query to grab the data, s3_path to set the location of copied data, filename as the custom prefix of the file and unload options.</p>\n<p>Unload options can be better understood in this link: <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\" rel=\"nofollow\">https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html</a></p>\n<p>Usage example:</p>\n<pre><code>from instackup.bigquery import RedShiftTool\n\n\n# Maybe you'll get this timestamp from other source\ntimestamp = '2019-11-29 19:31:42.766000+00:00'\nextraction_query = \"\"\"SELECT * FROM schema.table WHERE tstamp = '{timestamp}'\"\"\".format(timestamp=timestamp)\n\ns3_path = \"s3://redshift-data/unload/\"\nfilename = \"file_\"\nunload_options = \"DELIMITER '|' ESCAPE ADDQUOTES\"\n\n\nrs = RedShiftTool()\nrs.connect()\n\ntry:\n    # Unload data with custom options\n    rs.unload_to_S3(extraction_query, s3_path, filename, unload_options)\n\nexcept Exception as e:\n    rs.rollback()\n    raise e\nelse:\n    rs.commit()\nfinally:\n    # remember to close the connection later\n    rs.close_connection()\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n    # Unload data without custom options (will overwrite)\n    rs.unload_to_S3(extraction_query, s3_path, filename)\n\n    # other code\n</code></pre>\n<h4>close_connection(self)</h4>\n<p>Closes Connection with RedShift database. It has no extra parameter or return value.</p>\n<p>Usage example:</p>\n<pre><code>from instackup.bigquery import RedShiftTool\n\nrs = RedShiftTool()\nrs.connect()\n\ntry:\n    # Do stuff\nexcept Exception as e:\n    rs.rollback()\n    raise e\nelse:\n    rs.commit()\nfinally:\n    rs.close_connection()\n\n# or\n\nwith RedShiftTool() as rs:\n    # Already connected, use rs object in this context\n\n    # Do stuff\n\n    # Will close the connection automatically when existing this scope\n</code></pre>\n<h2>s3_tools</h2>\n<h3>S3Tool</h3>\n<p>This class handle most of the interaction needed with S3,\nso the base code becomes more readable and straightforward.</p>\n<p>To understand the S3 structure, you need to know it is not a hierarchical filesystem,\nit is only a key-value store, though the key is often used like a file path for organising data,\nprefix + filename. More information about this can be read in this StackOverFlow thread:\n<a href=\"https://stackoverflow.com/questions/52443839/s3-what-exactly-is-a-prefix-and-what-ratelimits-apply\" rel=\"nofollow\">https://stackoverflow.com/questions/52443839/s3-what-exactly-is-a-prefix-and-what-ratelimits-apply</a></p>\n<p>All that means is that while you may see a path as:</p>\n<pre><code>s3://bucket-1/folder1/subfolder1/some_file.csv\nroot| folder | sub.1 |  sub.2   |    file    |\n</code></pre>\n<p>It is actually:</p>\n<pre><code>s3://bucket-1/folder1/sub1/file.csv\nroot| bucket |         key        |\n</code></pre>\n<p>A great (not directly related) thread that can help that sink in (and help understand some methods here)\nis this one: <a href=\"https://stackoverflow.com/questions/35803027/retrieving-subfolders-names-in-s3-bucket-from-boto3\" rel=\"nofollow\">https://stackoverflow.com/questions/35803027/retrieving-subfolders-names-in-s3-bucket-from-boto3</a></p>\n<p>In this class, all keys and keys prefix are being treated as a folder tree structure,\nsince the reason for this to exists is to make the programmers interactions with S3\neasier to write and the code easier to read.</p>\n<h4>__init__(self, bucket=None, subfolder=\"\", s3_path=None)</h4>\n<p>Takes a either s3_path or both bucket name and subfolder name as parameters to set the current working directory. It also opens a connection with AWS S3.</p>\n<p>The paradigm of this class is that all the operations are done in the current working directory, so it is important to set the right path (you can reset it later, but still).</p>\n<p>Usage example:</p>\n<pre><code>from instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\n# or\n\ns3 = S3Tool(bucket=\"some_other_bucket\", subfolder=\"some_subfolder/subpath/\")\n</code></pre>\n<h4>bucket(self) @property</h4>\n<p>Returns the bucket object from the client based on the bucket name given in __init__ or set_bucket</p>\n<h4>set_bucket(self, bucket)</h4>\n<p>Takes a string as a parameter to reset the bucket name and bucket object. It has no return value.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\ns3.set_bucket(\"some_other_bucket\")\n\n# Check new path structure\nprint(s3.get_s3_path())\n</code></pre>\n<h4>set_subfolder(self, subfolder)</h4>\n<p>Takes a string as a parameter to reset the subfolder name. It has no return value.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\ns3.set_subfolder(\"some/more_complex/subfolder/structure/\")\n\n# Check new path structure\nprint(s3.get_s3_path())\n</code></pre>\n<h4>set_by_path(self, s3_path)</h4>\n<p>Takes a string as a parameter to reset the bucket name and subfolder name by its S3 path. It has no return value.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\ns3.set_by_path(\"s3://some_other_bucket/some/more_complex/subfolder/structure/\")\n\n# Check new path structure\nprint(s3.get_s3_path())\n</code></pre>\n<h4>get_s3_path(self)</h4>\n<p>Returns a string containing the S3 path for the currently set bucket and subfolder. It takes no parameter.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(bucket=\"some_bucket\", subfolder=\"subfolder/\")\n\nprint(s3.get_s3_path())\n</code></pre>\n<h4>rename_file(self, new_filename, old_filename)</h4>\n<p>Takes 2 strings containing file names and rename only the filename from path key, so the final result is similar to rename a file. It has no return value.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(bucket=\"some_bucket\", subfolder=\"subfolder/\")\n\ns3.rename_file(\"new_name\", \"old_name\")\n</code></pre>\n<h4>rename_subfolder(self, new_subfolder)</h4>\n<p>Takes a string containing the new subfolder name and renames all keys in the currently set path, so the final result is similar to rename a subfolder. It has no return value.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.s3_tools import S3Tool\n\n\nold_subfolder = \"some/more_complex/subfolder/structure/\"\nnew_subfolder = \"some/new/subfolder/structure/\"\n\ns3 = S3Tool(bucket=\"some_bucket\", subfolder=old_subfolder)\n\n# The final result is similar to just rename the \"more_complex\" folder to \"new\"\ns3.rename_subfolder(new_subfolder)\n</code></pre>\n<h4>list_all_buckets(self)</h4>\n<p>Returns a list of all Buckets in S3. It takes no parameter.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.s3_tools import S3Tool\n\n\n# Setting or not a subfolder doesn't change the output of this function\ns3 = S3Tool(bucket=\"some_bucket\")\n\nall_buckets = s3.list_all_buckets()\n\n# some code here\n</code></pre>\n<h4>list_contents(self, yield_results=False):</h4>\n<p>Lists all files that correspond with bucket and subfolder set at the initialization.</p>\n<p>It can either return a list or yield a generator. Lists can be more familiar to use, but when dealing with large amounts of data, yielding the results may be a better option in terms of efficiency.</p>\n<p>For more information on how to use generators and yield, check this video:\n<a href=\"https://www.youtube.com/watch?v=bD05uGo_sVI\" rel=\"nofollow\">https://www.youtube.com/watch?v=bD05uGo_sVI</a></p>\n<p>Usage Example:</p>\n<pre><code>from instackup.s3_tools import S3Tool\n\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\npath_contents = s3.list_contents()\n\nif len(path_contents) == 0:\n    s3.set_subfolder(\"logs/subfolder/\")\n\n    # When a specific bucket/ bucket + subfolder contains a lot of data,\n    # that's when yielding the results may be more efficient.\n    for file in s3.list_contents(yield_results=True):\n        # Do something\n\n# some code here\n</code></pre>\n<h4>upload_file(self, filename, remote_path=None)</h4>\n<p>Uploads file to remote path in S3.</p>\n<p>remote_path can take either a full S3 path or a subfolder only one. It has no return value.</p>\n<p>If the remote_path parameter is not set, it will default to whatever subfolder\nis set in instance of the class plus the file name that is being uploaded.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.s3_tools import S3Tool\n\n\nfile_location = \"C:\\\\Users\\\\USER\\\\Desktop\\\\file.csv\"\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\n# upload_file method accepts all 3 options\ns3.upload_file(file_location)\ns3.upload_file(file_location, \"s3://some_bucket/other_subfolder/\")\ns3.upload_file(file_location, \"another_subfolder/\")  # Just subfolder\n</code></pre>\n<h4>upload_subfolder(self, folder_path)</h4>\n<p>Not implemented.</p>\n<h4>download_file(self, remote_path, filename=None)</h4>\n<p>Downloads remote S3 file to local path.</p>\n<p>remote_path can take either a full S3 path or a subfolder only one. It has no return value.</p>\n<p>If the filename parameter is not set, it will default to whatever subfolder\nis set in instance of the class plus the file name that is being downloaded.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.s3_tools import S3Tool\n\n\nfile_desired_location = \"C:\\\\Users\\\\USER\\\\Desktop\\\\file.csv\"\nremote_location = \"s3://some_bucket/other_subfolder/file.csv\"\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\n# download_file method accepts both options\ns3.download_file(remote_location)\ns3.download_file(remote_location, file_desired_location)\n</code></pre>\n<h4>download_subfolder(self)</h4>\n<p>Not implemented.</p>\n<h4>delete_file(self, filename, fail_silently=False)</h4>\n<p>Deletes file from currently set path. It has no return value.</p>\n<p>Raises an error if file doesn't exist and fail_silently parameter is set to False.</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.s3_tools import S3Tool\n\n\nfilename = \"file.csv\"\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\ns3.delete_file(file_location)\n\n# Will fail to delete the same file it was deleted before,\n# but won't raise any error due to fail_silently being set to True\ns3.delete_file(file_location, fail_silently=True)\n</code></pre>\n<h4>delete_subfolder(self)</h4>\n<p>Deletes all files with subfolder prefix, so the final result is similar to deleting a subfolder. It has no return value.</p>\n<p>Raises an error if file doesn't exist and fail_silently parameter is set to False.</p>\n<p>Once the subfolder is deleted, it resets to no extra path (empty subfolder name).</p>\n<p>Usage Example:</p>\n<pre><code>from instackup.s3_tools import S3Tool\n\n\nfilename = \"file.csv\"\n\ns3 = S3Tool(s3_path=\"s3://some_bucket/subfolder/\")\n\ns3.delete_folder()\n\n# Check new path structure\nprint(s3.get_s3_path())\n</code></pre>\n<h1>Version log</h1>\n<p>See what changed in every version.</p>\n<h3>Version 0.0.3 (alpha)</h3>\n<p>Third alpha release.</p>\n<h4>New functionalities:</h4>\n<ul>\n<li>bigquery_tools\n<ul>\n<li>BigQueryTool\n<ul>\n<li>list_datasets</li>\n<li>list_tables_in_dataset</li>\n<li>get_table_schema</li>\n<li>__job_preparation_file_upload (private method)</li>\n<li>upload_from_gcs</li>\n<li>upload_from_file</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4>Functionalities still in development:</h4>\n<ul>\n<li>gcloudstorage_tools\n<ul>\n<li>GCloudStorageTool\n<ul>\n<li>rename_file</li>\n<li>rename_subfolder</li>\n<li>upload_subfolder</li>\n<li>download_subfolder</li>\n<li>delete_file</li>\n<li>delete_subfolder</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>s3_tools\n<ul>\n<li>S3Tool\n<ul>\n<li>upload_subfolder</li>\n<li>download_subfolder</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3>Version 0.0.2 (alpha)</h3>\n<p>Second alpha release.</p>\n<h4>Added modules:</h4>\n<ul>\n<li>gcloudstorage_tools</li>\n</ul>\n<p>Inside this module, these classes and functions/methods were added:</p>\n<ul>\n<li>GCloudStorageTool\n<ul>\n<li>__init__</li>\n<li>bucket @property</li>\n<li>set_bucket</li>\n<li>set_subfolder</li>\n<li>set_blob</li>\n<li>set_by_path</li>\n<li>get_gs_path</li>\n<li>list_all_buckets</li>\n<li>get_bucket_info</li>\n<li>list_bucket_attributes</li>\n<li>get_blob_info</li>\n<li>list_blob_attributes</li>\n<li>list_contents</li>\n<li>upload_file</li>\n<li>download_file</li>\n<li>download_on_dataframe</li>\n</ul>\n</li>\n</ul>\n<h4>New functionalities:</h4>\n<ul>\n<li>bigquery_tools\n<ul>\n<li>BigQueryTool\n<ul>\n<li>convert_dataframe_to_numeric</li>\n<li>clean_dataframe_column_names</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>general_tools\n<ul>\n<li>unicode_to_ascii</li>\n<li>parse_remote_uri</li>\n</ul>\n</li>\n</ul>\n<h4>Modified functionalities:</h4>\n<ul>\n<li>bigquery_tools\n<ul>\n<li>BigQueryTool\n<ul>\n<li>upload</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4>Deleted functionalities:</h4>\n<ul>\n<li>gcloudstorage_tools\n<ul>\n<li>parse_gs_path</li>\n</ul>\n</li>\n<li>s3_tools\n<ul>\n<li>parse_s3_path</li>\n</ul>\n</li>\n</ul>\n<h4>Functionalities still in development:</h4>\n<ul>\n<li>gcloudstorage_tools\n<ul>\n<li>GCloudStorageTool\n<ul>\n<li>rename_file</li>\n<li>rename_subfolder</li>\n<li>upload_subfolder</li>\n<li>download_subfolder</li>\n<li>delete_file</li>\n<li>delete_subfolder</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>s3_tools\n<ul>\n<li>S3Tool\n<ul>\n<li>upload_subfolder</li>\n<li>download_subfolder</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3>Version 0.0.1 (alpha)</h3>\n<p>First alpha release:</p>\n<p>Added modules:</p>\n<ul>\n<li>bigquery_tools</li>\n<li>general_tools</li>\n<li>redshift_tools</li>\n<li>s3_tools</li>\n</ul>\n<p>Inside those modules, these classes and functions/methods were added:</p>\n<ul>\n<li>BigQueryTool\n<ul>\n<li>__init__</li>\n<li>query</li>\n<li>upload</li>\n<li>start_transfer</li>\n</ul>\n</li>\n<li>fetch_credentials</li>\n<li>RedShiftTool\n<ul>\n<li>__init__</li>\n<li>connect</li>\n<li>commit</li>\n<li>rollback</li>\n<li>execute_sql</li>\n<li>query</li>\n<li>unload_to_S3</li>\n<li>close_connection</li>\n<li>__enter__</li>\n<li>__exit__</li>\n</ul>\n</li>\n<li>parse_s3_path</li>\n<li>S3Tool\n<ul>\n<li>__init__</li>\n<li>bucket @property</li>\n<li>set_bucket</li>\n<li>set_subfolder</li>\n<li>set_by_path</li>\n<li>get_s3_path</li>\n<li>rename_file</li>\n<li>rename_subfolder</li>\n<li>list_all_buckets</li>\n<li>list_contents</li>\n<li>upload_file</li>\n<li>download_file</li>\n<li>delete_file</li>\n<li>delete_subfolder</li>\n</ul>\n</li>\n</ul>\n<p>Modules still in development:</p>\n<ul>\n<li>gcloudstorage_tools</li>\n</ul>\n<p>Inside this module, these classes and functions/methods are in development:</p>\n<ul>\n<li>parse_gs_path</li>\n<li>GCloudStorageTool\n<ul>\n<li>__init__</li>\n<li>bucket @property</li>\n<li>set_bucket</li>\n<li>set_subfolder</li>\n<li>set_by_path</li>\n<li>get_gs_path</li>\n<li>list_all_buckets</li>\n<li>list_bucket_contents</li>\n<li>upload_file</li>\n<li>download_file</li>\n</ul>\n</li>\n<li>S3Tool\n<ul>\n<li>upload_subfolder</li>\n<li>download_subfolder</li>\n</ul>\n</li>\n</ul>\n\n          </div>"}, "last_serial": 7057274, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "af77770e00f37b7598f5590d2810e4b9", "sha256": "af8f58f90d51f395734938bfd864e138a2b4c3d1cf91d06b6e75015eb70bd62f"}, "downloads": -1, "filename": "instackup-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "af77770e00f37b7598f5590d2810e4b9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 21425, "upload_time": "2020-01-28T02:43:42", "upload_time_iso_8601": "2020-01-28T02:43:42.707849Z", "url": "https://files.pythonhosted.org/packages/a5/55/0a4e4e07348b1c7f07598ebe7e8cc9437be01d721517acfd37eee62cc82c/instackup-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "13ccf43eeb55c7d9b9ae783b1d97ac72", "sha256": "c868359d3965e2d9e96d9f26b8d4d38269d21f011bd618af4ad7c07fd55d4b7b"}, "downloads": -1, "filename": "instackup-0.0.1.tar.gz", "has_sig": false, "md5_digest": "13ccf43eeb55c7d9b9ae783b1d97ac72", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 28630, "upload_time": "2020-01-28T02:43:44", "upload_time_iso_8601": "2020-01-28T02:43:44.800209Z", "url": "https://files.pythonhosted.org/packages/f0/ab/c67efe8028777a97841242a773a993b6f4005f9575d6138d78e7ace3abe8/instackup-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "88df3ee7320ac5bea097d9fcfb3c2fcb", "sha256": "965d4fb7d155e16bf9a861910cf003dfe812df3332d7a3f7b1865cf49fadb9b6"}, "downloads": -1, "filename": "instackup-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "88df3ee7320ac5bea097d9fcfb3c2fcb", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 25542, "upload_time": "2020-04-08T02:51:44", "upload_time_iso_8601": "2020-04-08T02:51:44.674108Z", "url": "https://files.pythonhosted.org/packages/09/02/9755a9f7b61387f4be55a36d8e7ecfe19da66ca171981d4b92f397292d07/instackup-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "042e27c18149899f3518e49c3838cc40", "sha256": "8fdd852fd727ec82cd6843775e8a6d467f41fc1fb5c45098ddcdf4e0d684829b"}, "downloads": -1, "filename": "instackup-0.0.2.tar.gz", "has_sig": false, "md5_digest": "042e27c18149899f3518e49c3838cc40", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 37556, "upload_time": "2020-04-08T02:51:46", "upload_time_iso_8601": "2020-04-08T02:51:46.328964Z", "url": "https://files.pythonhosted.org/packages/5e/6f/474885505853676f6fc1a60591fe0fb2e96463a65921a49e5c621caceaf9/instackup-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "8d9aff096819a1b08cd8f629c67e0a01", "sha256": "b105bb533bb4eafceac67dc09bfc4280ad55d495e130a79be9bc3dadab4d2cbf"}, "downloads": -1, "filename": "instackup-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "8d9aff096819a1b08cd8f629c67e0a01", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 30488, "upload_time": "2020-04-19T20:14:48", "upload_time_iso_8601": "2020-04-19T20:14:48.408753Z", "url": "https://files.pythonhosted.org/packages/61/c1/8b156d60b305fa501c9dbf8fe2bb9663105de4641191a2acea00412ba0af/instackup-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4344df9ba682d5c188e1cf6b7630ba11", "sha256": "f42dd82456015be5fcf7811489b7f5e25e8e9c5f1629880d015ea09297d871d9"}, "downloads": -1, "filename": "instackup-0.0.3.tar.gz", "has_sig": false, "md5_digest": "4344df9ba682d5c188e1cf6b7630ba11", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 46525, "upload_time": "2020-04-19T20:14:51", "upload_time_iso_8601": "2020-04-19T20:14:51.367568Z", "url": "https://files.pythonhosted.org/packages/84/49/1b954db8bf5904190946c4f121f3399c03b9957cd5a565de41450956df63/instackup-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "969b1440c0a46609b14422d39b3bf8a2", "sha256": "bf49af9e5c0dc56816b8ba4e22c255bc603280345fc2730452e38525c0b8c4c2"}, "downloads": -1, "filename": "instackup-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "969b1440c0a46609b14422d39b3bf8a2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 30747, "upload_time": "2020-04-20T03:26:08", "upload_time_iso_8601": "2020-04-20T03:26:08.010977Z", "url": "https://files.pythonhosted.org/packages/5f/74/c12095fb37d233be450e3549b872785a14a8d3e12311ff2584c725f2812e/instackup-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "67a7793ee212f5e56d067eeba0f945d7", "sha256": "518d0b5638b0e428cb196b4ba3bb78ff0645f316f5f39fea75def1d206aa050a"}, "downloads": -1, "filename": "instackup-0.0.4.tar.gz", "has_sig": false, "md5_digest": "67a7793ee212f5e56d067eeba0f945d7", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 47411, "upload_time": "2020-04-20T03:26:09", "upload_time_iso_8601": "2020-04-20T03:26:09.369991Z", "url": "https://files.pythonhosted.org/packages/57/1f/8abe6d4e05cb064f0fb265daa1dfa6f3146ce654e2a18b5356e29731e2f9/instackup-0.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "969b1440c0a46609b14422d39b3bf8a2", "sha256": "bf49af9e5c0dc56816b8ba4e22c255bc603280345fc2730452e38525c0b8c4c2"}, "downloads": -1, "filename": "instackup-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "969b1440c0a46609b14422d39b3bf8a2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 30747, "upload_time": "2020-04-20T03:26:08", "upload_time_iso_8601": "2020-04-20T03:26:08.010977Z", "url": "https://files.pythonhosted.org/packages/5f/74/c12095fb37d233be450e3549b872785a14a8d3e12311ff2584c725f2812e/instackup-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "67a7793ee212f5e56d067eeba0f945d7", "sha256": "518d0b5638b0e428cb196b4ba3bb78ff0645f316f5f39fea75def1d206aa050a"}, "downloads": -1, "filename": "instackup-0.0.4.tar.gz", "has_sig": false, "md5_digest": "67a7793ee212f5e56d067eeba0f945d7", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 47411, "upload_time": "2020-04-20T03:26:09", "upload_time_iso_8601": "2020-04-20T03:26:09.369991Z", "url": "https://files.pythonhosted.org/packages/57/1f/8abe6d4e05cb064f0fb265daa1dfa6f3146ce654e2a18b5356e29731e2f9/instackup-0.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:55:43 2020"}