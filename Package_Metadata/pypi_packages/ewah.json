{"info": {"author": "Bijan Soltani", "author_email": "bijan.soltani+ewah@gemmaanalytics.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3"], "description": "# ewah\nEwah: ELT With Airflow Helper - Classes and functions to make apache airflow life easier.\n\nPre-Alpha. Used by myself for specific usecases at the moment.\n\nGoal: Have functions to create all DAGs required for ELT using only a simple config file. Use this as a basis to build a GUI on top of it.\n\n## DWHs Implemented\n- Snowflake\n- PostgreSQL\n\n## DWHs Planned\n- Bigquery\n- Redshift\n\n## Operators Implemented\n- PostgreSQL\n- MySQL\n- OracleSQL\n- Google Analytics (incremental only)\n- S3 (for JSON files stored in an S3 bucket, e.g. from Kinesis Firehose)\n- FX Rates (from Yahoo Finance)\n- Facebook (partially, so far: ads insights; incremental only)\n- Google Sheets\n\n## Philosophy\n\nThis package strictly follows an ELT Philosophy:\n- Business value is created by infusing business logic into the data and making great analyses and usable data available to stakeholders, not by building data pipelines\n- Airflow solely orchestrates loading raw data into a central DWH\n- Data is either loaded as full refresh (all data at every load) or incrementally, exploiting airflow's catchup and execution logic\n- The only additional DAGs are dbt DAGs and utility DAGs\n- Within that DWH, each data source lives in its own schema (e.g. `raw_salesforce`)\n- Irrespective of full refresh or incremental loading, DAGs always load into a separate schema (e.g. `raw_salesforce_next`) and at the end replace the schema with the old data with the schema with the new data, to avoid data corruption due to errors in DAG execution\n- Any data transformation is defined using SQL, ideally using [dbt](https://github.com/fishtown-analytics/dbt)\n- Seriously, dbt is awesome, give it a shot!\n- *(Non-SQL) Code contains no transformations*\n\n## Usage\n\nIn your airflow Dags folder, define the DAGs by invoking either the incremental loading or full refresh DAG factory. The incremental loading DAG factory returns three DAGs in a tuple, make sure to call it like so: `dag1, dag2, dag3 = dag_factory_incremental_loading()` or add the dag IDs to your namespace like so:\n```python\ndags = dag_factory_incremental_loading()\nfor dag in dags:\n  globals()[dag._dag_id] = dag\n```\nOtherwise, airflow will not recognize the DAGs. Most arguments should be self-explanatory. The two noteworthy arguments are `el_operator` and `operator_config`.\nThe former must be a child object of `ewah.operators.base_operator.EWAHBaseOperator`. Ideally, The required operator is already available for your use. Please feel free to fork and commit your own operators to this project! The latter is a dictionary containing the entire configuration of the operator. This is where you define what tables to load, how to load them, if loading specific columns only, and any other detail related to your EL job.\n\n### Full refresh factory\n\nA `filename.py` file in your airflow/dags folder may look something like this:\n```python\nfrom ewah.ewah_utils.dag_factory_full_refresh import dag_factory_drop_and_replace\nfrom ewah.constants import EWAHConstants as EC\nfrom ewah.operators.postgres_operator import EWAHPostgresOperator\n\nfrom datetime import datetime, timedelta\n\ndag = dag_factory_drop_and_replace(\n    dag_name='EL_production_postgres_database', # Name of the DAG\n    dwh_engine=EC.DWH_ENGINE_POSTGRES, # Implemented DWH Engine\n    dwh_conn_id='dwh', # Airflow connection ID with connection details to the DWH\n    el_operator=EWAHPostgresOperator, # Ewah Operator (or custom child class of EWAHBaseOperator)\n    target_schema_name='raw_production', # Name of the raw schema where data will end up in the DWH\n    target_schema_suffix='_next', # suffix of the schema containing the data before replacing the production data schema with the temporary loading schema\n    # target_database_name='raw', # Only Snowflake\n    start_date=datetime(2019, 10, 23), # As per airflow standard\n    schedule_interval=timedelta(hours=1), # Only timedelta is allowed!\n    default_args={ # Default args for DAG as per airflow standard\n        'owner': 'Data Engineering',\n        'retries': 1,\n        'retry_delay': timedelta(minutes=5),\n        'email_on_retry': False,\n        'email_on_failure': True,\n        'email': ['email@address.com'],\n    },\n    operator_config={\n        'general_config': {\n            'source_conn_id': 'production_postgres',\n            'source_schema_name': 'public',\n        },\n        'tables': {\n            'table_name':{},\n            # ...\n            # Additional optional kwargs at the table level:\n            #   columns_definition  \n            #   update_on_columns\n            #   primary_key_column_name\n            #   + any operator specific arguments\n        },\n    },\n)\n```\n\nFor all kwargs of the operator config, the general config can be overwritten by supplying specific kwargs at the table level.\n\n### Configure all DAGs in a single YAML file\n\nStandard data loading DAGs should be just a configuration. Thus, you can\nconfigure the DAGs using a simple YAML file. Your `dags.py` file in your\n`$AIRFLOW_HOME/dags` folder may then look like that, and nothing more:\n```python\nimport os\nfrom airflow import DAG # This module must be imported for airflow to see DAGs\nfrom airflow.configuration import conf\n\nfrom ewah.dag_factories import dags_from_yml_file\n\nfolder = os.environ.get('AIRFLOW__CORE__DAGS_FOLDER', None)\nfolder = folder or conf.get(\"core\", \"dags_folder\")\ndags = dags_from_yml_file(folder + os.sep + 'dags.yml', True, True)\nfor dag in dags: # Must add the individual DAGs to the global namespace\n    globals()[dag._dag_id] = dag\n\n```\nAnd the YAML file may look like this:\n```YAML\n---\n\nbase_config: # applied to all DAGs unless overwritten\n  dwh_engine: postgres\n  dwh_conn_id: dwh\n  airflow_conn_id: airflow\n  start_date: 2019-10-23 00:00:00+00:00\n  schedule_interval: !!python/object/apply:datetime.timedelta\n    - 0 # days\n    - 3600 # seconds\n  schedule_interval_backfill: !!python/object/apply:datetime.timedelta\n    - 7\n  schedule_interval_future: !!python/object/apply:datetime.timedelta\n    - 0\n    - 3600\n  additional_task_args:\n    retries: 1\n    retry_delay: !!python/object/apply:datetime.timedelta\n      - 0\n      - 300\n    email_on_retry: False\n    email_on_failure: True\n    email: ['me+airflowerror@mail.com']\nel_dags:\n  EL_Production: # equals the name of the DAG\n    incremental: False\n    el_operator: postgres\n    target_schema_name: raw_production\n    operator_config:\n      general_config:\n        source_conn_id: production_postgres\n        source_schema_name: public\n      tables:\n        users:\n          source_table_name: Users\n        transactions:\n          source_table_name: UserTransactions\n          source_schema_name: transaction_schema # Overwrite general_config args as needed\n  EL_Facebook:\n    incremental: True\n    el_operator: fb\n    start_date: 2019-07-01 00:00:00+00:00\n    target_schema_name: raw_facebook\n    operator_config:\n      general_config:\n        source_conn_id: facebook\n        account_ids:\n          - 123\n          - 987\n        data_from: '{{ execution_date }}' # Some fields allow airflow templating, depending on the operator\n        data_until: '{{ next_execution_date }}'\n        level: ad\n      tables:\n        ads_data_age_gender:\n          insight_fields:\n            - adset_id\n            - adset_name\n            - campaign_name\n            - campaign_id\n            - spend\n          breackdowns:\n            - age\n            - gender\n...\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://gemmaanalytics.com/", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "ewah", "package_url": "https://pypi.org/project/ewah/", "platform": "", "project_url": "https://pypi.org/project/ewah/", "project_urls": {"Homepage": "https://gemmaanalytics.com/"}, "release_url": "https://pypi.org/project/ewah/0.1.8/", "requires_dist": ["pyyaml", "psycopg2", "gspread", "pytz", "yahoofinancials", "google-api-python-client", "oauth2client", "cx-Oracle", "facebook-business", "mysql-connector-python", "snowflake-connector-python"], "requires_python": ">=3.6", "summary": "An ELT with airflow helper module: Ewah", "version": "0.1.8", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>ewah</h1>\n<p>Ewah: ELT With Airflow Helper - Classes and functions to make apache airflow life easier.</p>\n<p>Pre-Alpha. Used by myself for specific usecases at the moment.</p>\n<p>Goal: Have functions to create all DAGs required for ELT using only a simple config file. Use this as a basis to build a GUI on top of it.</p>\n<h2>DWHs Implemented</h2>\n<ul>\n<li>Snowflake</li>\n<li>PostgreSQL</li>\n</ul>\n<h2>DWHs Planned</h2>\n<ul>\n<li>Bigquery</li>\n<li>Redshift</li>\n</ul>\n<h2>Operators Implemented</h2>\n<ul>\n<li>PostgreSQL</li>\n<li>MySQL</li>\n<li>OracleSQL</li>\n<li>Google Analytics (incremental only)</li>\n<li>S3 (for JSON files stored in an S3 bucket, e.g. from Kinesis Firehose)</li>\n<li>FX Rates (from Yahoo Finance)</li>\n<li>Facebook (partially, so far: ads insights; incremental only)</li>\n<li>Google Sheets</li>\n</ul>\n<h2>Philosophy</h2>\n<p>This package strictly follows an ELT Philosophy:</p>\n<ul>\n<li>Business value is created by infusing business logic into the data and making great analyses and usable data available to stakeholders, not by building data pipelines</li>\n<li>Airflow solely orchestrates loading raw data into a central DWH</li>\n<li>Data is either loaded as full refresh (all data at every load) or incrementally, exploiting airflow's catchup and execution logic</li>\n<li>The only additional DAGs are dbt DAGs and utility DAGs</li>\n<li>Within that DWH, each data source lives in its own schema (e.g. <code>raw_salesforce</code>)</li>\n<li>Irrespective of full refresh or incremental loading, DAGs always load into a separate schema (e.g. <code>raw_salesforce_next</code>) and at the end replace the schema with the old data with the schema with the new data, to avoid data corruption due to errors in DAG execution</li>\n<li>Any data transformation is defined using SQL, ideally using <a href=\"https://github.com/fishtown-analytics/dbt\" rel=\"nofollow\">dbt</a></li>\n<li>Seriously, dbt is awesome, give it a shot!</li>\n<li><em>(Non-SQL) Code contains no transformations</em></li>\n</ul>\n<h2>Usage</h2>\n<p>In your airflow Dags folder, define the DAGs by invoking either the incremental loading or full refresh DAG factory. The incremental loading DAG factory returns three DAGs in a tuple, make sure to call it like so: <code>dag1, dag2, dag3 = dag_factory_incremental_loading()</code> or add the dag IDs to your namespace like so:</p>\n<pre><span class=\"n\">dags</span> <span class=\"o\">=</span> <span class=\"n\">dag_factory_incremental_loading</span><span class=\"p\">()</span>\n<span class=\"k\">for</span> <span class=\"n\">dag</span> <span class=\"ow\">in</span> <span class=\"n\">dags</span><span class=\"p\">:</span>\n  <span class=\"nb\">globals</span><span class=\"p\">()[</span><span class=\"n\">dag</span><span class=\"o\">.</span><span class=\"n\">_dag_id</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">dag</span>\n</pre>\n<p>Otherwise, airflow will not recognize the DAGs. Most arguments should be self-explanatory. The two noteworthy arguments are <code>el_operator</code> and <code>operator_config</code>.\nThe former must be a child object of <code>ewah.operators.base_operator.EWAHBaseOperator</code>. Ideally, The required operator is already available for your use. Please feel free to fork and commit your own operators to this project! The latter is a dictionary containing the entire configuration of the operator. This is where you define what tables to load, how to load them, if loading specific columns only, and any other detail related to your EL job.</p>\n<h3>Full refresh factory</h3>\n<p>A <code>filename.py</code> file in your airflow/dags folder may look something like this:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">ewah.ewah_utils.dag_factory_full_refresh</span> <span class=\"kn\">import</span> <span class=\"n\">dag_factory_drop_and_replace</span>\n<span class=\"kn\">from</span> <span class=\"nn\">ewah.constants</span> <span class=\"kn\">import</span> <span class=\"n\">EWAHConstants</span> <span class=\"k\">as</span> <span class=\"n\">EC</span>\n<span class=\"kn\">from</span> <span class=\"nn\">ewah.operators.postgres_operator</span> <span class=\"kn\">import</span> <span class=\"n\">EWAHPostgresOperator</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">datetime</span> <span class=\"kn\">import</span> <span class=\"n\">datetime</span><span class=\"p\">,</span> <span class=\"n\">timedelta</span>\n\n<span class=\"n\">dag</span> <span class=\"o\">=</span> <span class=\"n\">dag_factory_drop_and_replace</span><span class=\"p\">(</span>\n    <span class=\"n\">dag_name</span><span class=\"o\">=</span><span class=\"s1\">'EL_production_postgres_database'</span><span class=\"p\">,</span> <span class=\"c1\"># Name of the DAG</span>\n    <span class=\"n\">dwh_engine</span><span class=\"o\">=</span><span class=\"n\">EC</span><span class=\"o\">.</span><span class=\"n\">DWH_ENGINE_POSTGRES</span><span class=\"p\">,</span> <span class=\"c1\"># Implemented DWH Engine</span>\n    <span class=\"n\">dwh_conn_id</span><span class=\"o\">=</span><span class=\"s1\">'dwh'</span><span class=\"p\">,</span> <span class=\"c1\"># Airflow connection ID with connection details to the DWH</span>\n    <span class=\"n\">el_operator</span><span class=\"o\">=</span><span class=\"n\">EWAHPostgresOperator</span><span class=\"p\">,</span> <span class=\"c1\"># Ewah Operator (or custom child class of EWAHBaseOperator)</span>\n    <span class=\"n\">target_schema_name</span><span class=\"o\">=</span><span class=\"s1\">'raw_production'</span><span class=\"p\">,</span> <span class=\"c1\"># Name of the raw schema where data will end up in the DWH</span>\n    <span class=\"n\">target_schema_suffix</span><span class=\"o\">=</span><span class=\"s1\">'_next'</span><span class=\"p\">,</span> <span class=\"c1\"># suffix of the schema containing the data before replacing the production data schema with the temporary loading schema</span>\n    <span class=\"c1\"># target_database_name='raw', # Only Snowflake</span>\n    <span class=\"n\">start_date</span><span class=\"o\">=</span><span class=\"n\">datetime</span><span class=\"p\">(</span><span class=\"mi\">2019</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">23</span><span class=\"p\">),</span> <span class=\"c1\"># As per airflow standard</span>\n    <span class=\"n\">schedule_interval</span><span class=\"o\">=</span><span class=\"n\">timedelta</span><span class=\"p\">(</span><span class=\"n\">hours</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"c1\"># Only timedelta is allowed!</span>\n    <span class=\"n\">default_args</span><span class=\"o\">=</span><span class=\"p\">{</span> <span class=\"c1\"># Default args for DAG as per airflow standard</span>\n        <span class=\"s1\">'owner'</span><span class=\"p\">:</span> <span class=\"s1\">'Data Engineering'</span><span class=\"p\">,</span>\n        <span class=\"s1\">'retries'</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n        <span class=\"s1\">'retry_delay'</span><span class=\"p\">:</span> <span class=\"n\">timedelta</span><span class=\"p\">(</span><span class=\"n\">minutes</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">),</span>\n        <span class=\"s1\">'email_on_retry'</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n        <span class=\"s1\">'email_on_failure'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"s1\">'email'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">'email@address.com'</span><span class=\"p\">],</span>\n    <span class=\"p\">},</span>\n    <span class=\"n\">operator_config</span><span class=\"o\">=</span><span class=\"p\">{</span>\n        <span class=\"s1\">'general_config'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s1\">'source_conn_id'</span><span class=\"p\">:</span> <span class=\"s1\">'production_postgres'</span><span class=\"p\">,</span>\n            <span class=\"s1\">'source_schema_name'</span><span class=\"p\">:</span> <span class=\"s1\">'public'</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n        <span class=\"s1\">'tables'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s1\">'table_name'</span><span class=\"p\">:{},</span>\n            <span class=\"c1\"># ...</span>\n            <span class=\"c1\"># Additional optional kwargs at the table level:</span>\n            <span class=\"c1\">#   columns_definition  </span>\n            <span class=\"c1\">#   update_on_columns</span>\n            <span class=\"c1\">#   primary_key_column_name</span>\n            <span class=\"c1\">#   + any operator specific arguments</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">)</span>\n</pre>\n<p>For all kwargs of the operator config, the general config can be overwritten by supplying specific kwargs at the table level.</p>\n<h3>Configure all DAGs in a single YAML file</h3>\n<p>Standard data loading DAGs should be just a configuration. Thus, you can\nconfigure the DAGs using a simple YAML file. Your <code>dags.py</code> file in your\n<code>$AIRFLOW_HOME/dags</code> folder may then look like that, and nothing more:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"kn\">from</span> <span class=\"nn\">airflow</span> <span class=\"kn\">import</span> <span class=\"n\">DAG</span> <span class=\"c1\"># This module must be imported for airflow to see DAGs</span>\n<span class=\"kn\">from</span> <span class=\"nn\">airflow.configuration</span> <span class=\"kn\">import</span> <span class=\"n\">conf</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">ewah.dag_factories</span> <span class=\"kn\">import</span> <span class=\"n\">dags_from_yml_file</span>\n\n<span class=\"n\">folder</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'AIRFLOW__CORE__DAGS_FOLDER'</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>\n<span class=\"n\">folder</span> <span class=\"o\">=</span> <span class=\"n\">folder</span> <span class=\"ow\">or</span> <span class=\"n\">conf</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">\"core\"</span><span class=\"p\">,</span> <span class=\"s2\">\"dags_folder\"</span><span class=\"p\">)</span>\n<span class=\"n\">dags</span> <span class=\"o\">=</span> <span class=\"n\">dags_from_yml_file</span><span class=\"p\">(</span><span class=\"n\">folder</span> <span class=\"o\">+</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">sep</span> <span class=\"o\">+</span> <span class=\"s1\">'dags.yml'</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">dag</span> <span class=\"ow\">in</span> <span class=\"n\">dags</span><span class=\"p\">:</span> <span class=\"c1\"># Must add the individual DAGs to the global namespace</span>\n    <span class=\"nb\">globals</span><span class=\"p\">()[</span><span class=\"n\">dag</span><span class=\"o\">.</span><span class=\"n\">_dag_id</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">dag</span>\n</pre>\n<p>And the YAML file may look like this:</p>\n<pre><span class=\"nn\">---</span>\n\n<span class=\"nt\">base_config</span><span class=\"p\">:</span> <span class=\"c1\"># applied to all DAGs unless overwritten</span>\n  <span class=\"nt\">dwh_engine</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">postgres</span>\n  <span class=\"nt\">dwh_conn_id</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">dwh</span>\n  <span class=\"nt\">airflow_conn_id</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">airflow</span>\n  <span class=\"nt\">start_date</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">2019-10-23 00:00:00+00:00</span>\n  <span class=\"nt\">schedule_interval</span><span class=\"p\">:</span> <span class=\"kt\">!!python/object/apply:datetime.timedelta</span>\n    <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">0</span> <span class=\"c1\"># days</span>\n    <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">3600</span> <span class=\"c1\"># seconds</span>\n  <span class=\"nt\">schedule_interval_backfill</span><span class=\"p\">:</span> <span class=\"kt\">!!python/object/apply:datetime.timedelta</span>\n    <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">7</span>\n  <span class=\"nt\">schedule_interval_future</span><span class=\"p\">:</span> <span class=\"kt\">!!python/object/apply:datetime.timedelta</span>\n    <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">0</span>\n    <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">3600</span>\n  <span class=\"nt\">additional_task_args</span><span class=\"p\">:</span>\n    <span class=\"nt\">retries</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n    <span class=\"nt\">retry_delay</span><span class=\"p\">:</span> <span class=\"kt\">!!python/object/apply:datetime.timedelta</span>\n      <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">0</span>\n      <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">300</span>\n    <span class=\"nt\">email_on_retry</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">False</span>\n    <span class=\"nt\">email_on_failure</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">True</span>\n    <span class=\"nt\">email</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">[</span><span class=\"s\">'me+airflowerror@mail.com'</span><span class=\"p p-Indicator\">]</span>\n<span class=\"nt\">el_dags</span><span class=\"p\">:</span>\n  <span class=\"nt\">EL_Production</span><span class=\"p\">:</span> <span class=\"c1\"># equals the name of the DAG</span>\n    <span class=\"nt\">incremental</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">False</span>\n    <span class=\"nt\">el_operator</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">postgres</span>\n    <span class=\"nt\">target_schema_name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">raw_production</span>\n    <span class=\"nt\">operator_config</span><span class=\"p\">:</span>\n      <span class=\"nt\">general_config</span><span class=\"p\">:</span>\n        <span class=\"nt\">source_conn_id</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">production_postgres</span>\n        <span class=\"nt\">source_schema_name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">public</span>\n      <span class=\"nt\">tables</span><span class=\"p\">:</span>\n        <span class=\"nt\">users</span><span class=\"p\">:</span>\n          <span class=\"nt\">source_table_name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">Users</span>\n        <span class=\"nt\">transactions</span><span class=\"p\">:</span>\n          <span class=\"nt\">source_table_name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">UserTransactions</span>\n          <span class=\"nt\">source_schema_name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">transaction_schema</span> <span class=\"c1\"># Overwrite general_config args as needed</span>\n  <span class=\"nt\">EL_Facebook</span><span class=\"p\">:</span>\n    <span class=\"nt\">incremental</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">True</span>\n    <span class=\"nt\">el_operator</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">fb</span>\n    <span class=\"nt\">start_date</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">2019-07-01 00:00:00+00:00</span>\n    <span class=\"nt\">target_schema_name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">raw_facebook</span>\n    <span class=\"nt\">operator_config</span><span class=\"p\">:</span>\n      <span class=\"nt\">general_config</span><span class=\"p\">:</span>\n        <span class=\"nt\">source_conn_id</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">facebook</span>\n        <span class=\"nt\">account_ids</span><span class=\"p\">:</span>\n          <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">123</span>\n          <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">987</span>\n        <span class=\"nt\">data_from</span><span class=\"p\">:</span> <span class=\"s\">'{{</span><span class=\"nv\"> </span><span class=\"s\">execution_date</span><span class=\"nv\"> </span><span class=\"s\">}}'</span> <span class=\"c1\"># Some fields allow airflow templating, depending on the operator</span>\n        <span class=\"nt\">data_until</span><span class=\"p\">:</span> <span class=\"s\">'{{</span><span class=\"nv\"> </span><span class=\"s\">next_execution_date</span><span class=\"nv\"> </span><span class=\"s\">}}'</span>\n        <span class=\"nt\">level</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">ad</span>\n      <span class=\"nt\">tables</span><span class=\"p\">:</span>\n        <span class=\"nt\">ads_data_age_gender</span><span class=\"p\">:</span>\n          <span class=\"nt\">insight_fields</span><span class=\"p\">:</span>\n            <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">adset_id</span>\n            <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">adset_name</span>\n            <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">campaign_name</span>\n            <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">campaign_id</span>\n            <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">spend</span>\n          <span class=\"nt\">breackdowns</span><span class=\"p\">:</span>\n            <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">age</span>\n            <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">gender</span>\n<span class=\"nn\">...</span>\n</pre>\n\n          </div>"}, "last_serial": 7139887, "releases": {"0.1.7": [{"comment_text": "", "digests": {"md5": "12753087c0d0ffda5559fcf52791bfc8", "sha256": "4d4a7437b138c5db1aaef734f70b283d6e960cb94dc7f4a6a26005602036ac03"}, "downloads": -1, "filename": "ewah-0.1.7-py3-none-any.whl", "has_sig": false, "md5_digest": "12753087c0d0ffda5559fcf52791bfc8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 49719, "upload_time": "2020-04-30T19:26:13", "upload_time_iso_8601": "2020-04-30T19:26:13.098088Z", "url": "https://files.pythonhosted.org/packages/71/c9/86825f432ed32114236a867dca32069b52c89bfd4215d31709b38f3ee60a/ewah-0.1.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d98d5b7003b78e1954ebe2b10fe92ffd", "sha256": "4c48363f6d53b7f33151d8e9866b563b686831e0aff85d004f3776d3aaa23338"}, "downloads": -1, "filename": "ewah-0.1.7.tar.gz", "has_sig": false, "md5_digest": "d98d5b7003b78e1954ebe2b10fe92ffd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 38553, "upload_time": "2020-04-30T19:26:14", "upload_time_iso_8601": "2020-04-30T19:26:14.200500Z", "url": "https://files.pythonhosted.org/packages/2f/bd/6b4a127357ea243bb766d62ee6ad5ffdfba5de0bd36e82d25cef813a7865/ewah-0.1.7.tar.gz", "yanked": false}], "0.1.8": [{"comment_text": "", "digests": {"md5": "13b99f5448f0b56c7b906d2ef698ffc5", "sha256": "7f6e10b4bca04d1dca2eae28d504b5e3db81dd027456a534939cedd6bcc1c7b6"}, "downloads": -1, "filename": "ewah-0.1.8-py3-none-any.whl", "has_sig": false, "md5_digest": "13b99f5448f0b56c7b906d2ef698ffc5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 49817, "upload_time": "2020-04-30T19:39:19", "upload_time_iso_8601": "2020-04-30T19:39:19.354276Z", "url": "https://files.pythonhosted.org/packages/4f/99/d9d0529016425842a134e072816c2c51370225a1763dd202f9f1648972fe/ewah-0.1.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "da561f1f042321cb702c8d52aef33e88", "sha256": "2821edc3c184564f1ce002ffcbfee534ac225a7b0c5522c7b44c3447da8a95ab"}, "downloads": -1, "filename": "ewah-0.1.8.tar.gz", "has_sig": false, "md5_digest": "da561f1f042321cb702c8d52aef33e88", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 38727, "upload_time": "2020-04-30T19:39:21", "upload_time_iso_8601": "2020-04-30T19:39:21.450640Z", "url": "https://files.pythonhosted.org/packages/8a/0b/5455c440bece40918105ffb16b814bb70e142e1ec407d549aed882ed1e25/ewah-0.1.8.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "13b99f5448f0b56c7b906d2ef698ffc5", "sha256": "7f6e10b4bca04d1dca2eae28d504b5e3db81dd027456a534939cedd6bcc1c7b6"}, "downloads": -1, "filename": "ewah-0.1.8-py3-none-any.whl", "has_sig": false, "md5_digest": "13b99f5448f0b56c7b906d2ef698ffc5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 49817, "upload_time": "2020-04-30T19:39:19", "upload_time_iso_8601": "2020-04-30T19:39:19.354276Z", "url": "https://files.pythonhosted.org/packages/4f/99/d9d0529016425842a134e072816c2c51370225a1763dd202f9f1648972fe/ewah-0.1.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "da561f1f042321cb702c8d52aef33e88", "sha256": "2821edc3c184564f1ce002ffcbfee534ac225a7b0c5522c7b44c3447da8a95ab"}, "downloads": -1, "filename": "ewah-0.1.8.tar.gz", "has_sig": false, "md5_digest": "da561f1f042321cb702c8d52aef33e88", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 38727, "upload_time": "2020-04-30T19:39:21", "upload_time_iso_8601": "2020-04-30T19:39:21.450640Z", "url": "https://files.pythonhosted.org/packages/8a/0b/5455c440bece40918105ffb16b814bb70e142e1ec407d549aed882ed1e25/ewah-0.1.8.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:45:04 2020"}