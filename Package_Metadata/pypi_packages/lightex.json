{"info": {"author": "Nishant Sinha", "author_email": "nishant@offnote.co", "bugtrack_url": null, "classifiers": ["Environment :: Console", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development"], "description": "![experimental](https://img.shields.io/badge/stability-experimental-orange.svg)\n\n\n## LightEx\n\n`LightEx` is a lightweight experiment framework to create, monitor and record your machine learning experiments. Targeted towards individual data scientists, researchers, small teams and, in general, resource-constrained experimentation. Compatible with all machine-learning frameworks.\n\n**Project Status:** Alpha\n\nUnlike most experiment frameworks, `LightEx`sports a modular, and highly configurable design:\n\n* **dispatcher**: run experiments using `process`,`docker` containers or `kubernetes` pods. Switch between modes seamlessly by minor changes to config.\n* **mulogger**: log metrics and artifacts to multiple logger backends, using an *unified* API. Supports `mlflow`, `tensorboard` and `trains` \u2014 add new loggers easily as *plugins*.\n* **namedconf**:  python `dataclass` based flexible and *unified* configuration specification for jobs, parameters and model architectures. Config instances are *named* and can be *locally modified*.\n* **qviz**: query, compare and visualize your experiment results.\n\nThe run environment and parameters for your experiments are specified using a config file `lxconfig.py` in your project directory. Modify, inherit, and create new *named* config instances, on-the-fly, as you perform a series of experiments. \n\nLearn more about the **anatomy** of a ML experimentation framework [here](docs/anatomy.md). Also, why [yet another experiment framework] (#yet-another-experiment-framework) ?\n\n#### Benefits\n\n Start with a basic `train` or `eval` project. In a few minutes,\n\n* introduce systematic logging (multiple loggers) and visualization to your project\n* go from running a single experiment to multiple parameterized experiments, e.g.,\n  * multiple training runs over a set of hyper-parameters.\n  * multiple `efficient-net` or `bert` train or eval runs.\n  * a neural architecture search over multiple architectures in parallel.\n\n\n### Installation\n\n>  pip install -U lightex\n\n### Quick Start\n\nImagine we have an existing ML project, with the following model train command:\n\n> `train.py --data-dir ./data \u2014-lr 0.1 -\u2014hidden_dim 512` \n\nNow, let us bring in `lightex` to organize and scale experiments for the project. In the main project directory, initialize `lightex` :\n\n> `lx init`                               \n\nThis creates files `lxconfig.py` and `run_expts.py`. The file `lxconfig.py` contains pre-defined `dataclass`es for specifying *named* experiment configs.\n\n* The main class `Config`, contains three fields: `er` , `hp` and `run` (see the example below), which store configurations for logging and storage **resources**, **hyper-parameters**, and **runtime**, respectively. \n* By configuring these three fields (depending on the project environment, parameters), we can *seamlessly* generate multiple experiments via different dispatchers and log the results to one or more logger backends.\n* The config classes `Resources`, `HP` and `Run` for these fields are pre-defined  in `lxconfig.py`. See [config.md](docs/config.md) for full description of the defined dataclasses.\n* `Config` also includes a `get_experiments` function, which generates a list of experiment configs to be executed by the dispatcher. .\n\n```python\n\n@dataclass\nclass Config:\n    er: Resources \t\t\t\t\t#(Logging, Storage resources)\n    hp: HP \t\t\t\t\t\t#(Hyper-parameters of model, training)\n    run: Run \t\t\t\t\t\t#(Run-time config)\n\n    def get_experiments(self): #required: generate a list of experiments to run\n        expts = [Experiment(er=self.er, hp=self.hp, run=self.run)]\n        return expts\n```\n\nTo create a `Config` instance for our existing project, we instantiate \n\n* class `HP` with parameter values (`lr` and `hidden_dim`), and \n* class `Run` 's `cmd` field with the train command **template**. The template refers to `Experiment` objects returned by `get_experiments` function (of this `Config` instance).\n  * Need not create the template manually. Use [`argparse_to_command`](lightex/namedconf/config_utils.py) in your existing code to generate it.\n* class `Resources` with default values (stores experiments in `/tmp/ltex`).\n\n```python\ncmd=\"python train.py --data-dir {{run.data_dir}} --lr {{hp.lr}} --hidden_dim {{hp.hidden_dim}}\" #template placeholders refer to fields of Experiment instance\nRu1 = Run(cmd=cmd, experiment_name=\"find_hp\")\nH2 = HP(lr=1e-2, hidden_dim=512)\nR1 = Resources()\n\nC1 = Config(er=R1, hp=H1, run=Ru1)\n```\n\nOnce the config instance `C1` is defined, run your experiments as follows:\n\n> python run_expts.py -c C1\n\n\n**That's it!** Now, your experiments, logs, metrics and models are executed and recorded systematically.\n\n------\n\n\n\n### Modify Experiment Parameters, Experiment Groups\n\nModify configs from previous experiments quickly using `replace` and run new experiments. \n\n**Example**: Create a new `HP` instance and replace it in `C1` to create a new `Config`. *Recursive* replace also supported.\n\n```python\nH2 = HP(lr=1e-3, hidden_dim=1024)\nC2 = replace(C1, hp=H2) #inherit er=R1 and run=Ru1\n```\n\n> python run_expts.py -c C2\n\n##### Experiment Groups\n\nWe can create multiple similar experiments programmatically and run them in parallel. To specify such **experiment groups**, specify a set of `HP`s in a `HPGroup` and update the `get_experiments` function (see [scripts/lxconfig.py](scripts/lxconfig.py) for an example). In the usual case, these experiments have different hyper-parameters but share `Resources` and `Run` instances.\n\n**Note**: Although LightEx pre-defines the dataclass hierarchy, it allows the developer plenty of flexibility in defining the individual fields of classes, in particular, the fields of the `HP` class. \n\n### Adding Logging to your Code\n\nUse the unified `MultiLogger` [API](lightex/mulogger) to log metrics and artifacts to multiple logger backends.\n\nSupported Loggers: `mlflow`, `tensorboard`,`trains`, `wandb`.\n\n```python\nfrom lightex.mulogger import MLFlowLogger, MultiLogger, PytorchTBLogger\nlogger = MultiLogger(['mlflow', 'trains'])\nlogger.start_run()\n# log to trains only\nlogger.log('trains', ltype='hpdict', value={'alpha': alpha, 'l1_ratio': l1_ratio})\n# log to mlflow only\nlogger.log('mlflow', ltype='scalardict', value={'mae': mae, 'rmse': rmse, 'r2': r2}, step=1)\n# log to all\nlogger.log('*', ltype='scalardict', value={'mae': mae, 'rmse': rmse, 'r2': r2}, step=3)\n# log scalars and tensors, if supported by the logger backend\nlogger.log('trains', ltype='1d', name='W1', value=Tensor(..), histogram=True, step=4)\nlogger.end_run()\n```\n\nOr, use one of the existing loggers' API directly.\n\n```python\nlogger = MLFlowLogger(); mlflow = logger.mlflow  # call mlflow API\nlogger = PytorchTBLogger() ; writer = logger.writer #call tensorboard's API\n# Similarly, use TrainsLogger for trains and WBLogger for wandb\n```\n\n**Note**: Except for changes in logging, no changes are required to your existing code! Setup the logger backend using scripts [here](backend/).\n\n### Switch to Docker\n\nSetting up the `lxconfig` instances pays off here! \n\nNow, add a `Dockerfile` to your project which builds the runtime environment with all the project dependencies. Update the `Build` instance inside `Resources` config. See [examples/sklearn](examples/sklearn), for example.\n\n> python run_expts.py -c C2 -e docker\n\nBoth your code and data are mounted on the container (no copying involved) \u2014 minimal disruption in your dev cycle.\n\n## Advanced Features\n\nMore advanced features are in development stage.\n\n**Running Experiments on multiple nodes / servers**\n\nIf you've setup a docker `swarm` or `kubernetes` cluster, few changes to the existing config instance allow changing the underlying experiment dispatcher.\n\nWe need to virtualize code (by adding to Dockerfile) and storage.\n\nCreate a shared NFS on your nodes. Switch storage config to the NFS partition. Setup scripts will be added.\n\n**Better QViz module, Logger Plugins**\n\nImprovements to qviz module and a better plugin system for loggers being developed.\n\n**Setup Summary**\n\nIn summary, `LightEx` involves the following **one-time setup**:\n\n- config values in `lxconfig.py`\n- Setup backend logger servers (only the ones required). Instructions [here](backend/). (Optional)\n- Update logging calls in your code to call `mulogger` API. (Optional)\n- Dockerfile for your project, if you want to use containers for dispatch. (Optional)\n\nWhile `LightEx` is quick to start with, it is advisable to spend some time understanding the [config schema](llightex/config_base.py).\n\n\n\n### Dependencies\n\nPython > 3.6 (require `dataclasses`, included during install). Other dependencies defined in [setup.py](setup.py).\n\n\n\n### Design Challenges\n\n- The configuration system allows you to abstract away the locations of the **code** (project directory), **data** directory, and the **output** directory, via the `cmd` field of `Run`. This enables running your code as-is across all dispatcher environments (process, docker, kubernetes). \n- A significant part of experiment manager design is about setting up and propagating a giant web of configuration variables. \n  - No optimal choice here: `json`,`yaml`,`jsonnet`\u2014 all formats have issues. \n  - Using `dataclass`es, we can write complex config specs, with built-in inheritance and ability to do *local updates*. Tiny bit of a learning curve here, bound to python, but we gain a lot of flexibility.\n- A unified `mulogger` API to abstract away the API of multiple logging backends.\n- Designing multiple dispatchers with similar API, enabling containers and varying storage options.\n- Read more on challenges [here](docs/anatomy.md).\n\n### References\n\n- ML Experiment Frameworks: [kubeflow](https://github.com/kubeflow/kubeflow), [mlflow](https://www.mlflow.org/docs/latest/index.html), [polyaxon](https://polyaxon.com/), ...\n- Loggers: [sacred](https://sacred.readthedocs.io/en/latest/index.html), [trains](https://github.com/allegroai/trains), [wandb](https://www.wandb.com/),  [Trixi](https://github.com/MIC-DKFZ/trixi), [ml_logger](https://github.com/episodeyang/ml_logger)\n- Motivating Dataclasses [intro](https://blog.jetbrains.com/pycharm/2018/04/python-37-introducing-data-class/), [how-different](https://stackoverflow.com/questions/47955263/what-are-data-classes-and-how-are-they-different-from-common-classes)\n- Flexible configuration\n  - in modeling: allennlp, gin, jiant.\n  - in orchestration: [ksonnet](https://github.com/ksonnet), kubernetes-operator \n- On the pains of ML experimentation\n  - an article from [wandb](https://www.wandb.com/articles/iteratively-fine-tuning-neural-networks-with-weights-biases) \n\nMost current (2019 Q3) tools focus on the *logger* component and provide selective `qviz` components. `kubeflow` and `polyaxon` are tied to the (k8s) *dispatcher*. Every tool has its own version of config management \u2014 mostly *yaml* based, where config types are absent or have a non-nested config class. Config-specific languages have been also proposed (ksonnet, sonnet, gin).\n\n\n\n### Yet Another Experiment Framework\n\nSystematic experimentation tools are essential for a data scientist. Unfortunately, many existing tools (`kubeflow`, `mlflow`, `polyaxon`) are too monolithic, kubernetes-first, cloud-first, target very diverse audiences and hence spread too thin, and yet lack important dev-friendly features. `sacred` 's design is' tightly coupled and requires several `sacred`-specific changes to your main code. Other tools cater only to a specific task , e.g., `tensorboard` only handles log recording and visualization. Also, contrasting different experiment frameworks is hard: there is no standardized expt-management architecture for machine learning and most open-source frameworks are undergoing a process of adhoc requirements discovery. \n\n### Author\n\nNishant Sinha, [OffNote Labs](http://offnote.co) (nishant@offnote.co, @[medium](https://medium.com/@ekshakhs), @[twitter](https://twitter.com/ekshakhs))\n\n### Contributors\n\nAkarsh E S, [github](https://github.com/AkarshES)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ofnote/lightex", "keywords": "", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "lightex", "package_url": "https://pypi.org/project/lightex/", "platform": "POSIX", "project_url": "https://pypi.org/project/lightex/", "project_urls": {"Homepage": "https://github.com/ofnote/lightex"}, "release_url": "https://pypi.org/project/lightex/0.0.5.2/", "requires_dist": ["easydict", "dacite", "kubernetes", "docker", "dataclasses ; python_version < \"3.7\""], "requires_python": ">3.6.0", "summary": "LightEx: A Light Experiment Manager", "version": "0.0.5.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><img alt=\"experimental\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6dde6f958579c153d24bcb2f66de81207d4d9217/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73746162696c6974792d6578706572696d656e74616c2d6f72616e67652e737667\"></p>\n<h2>LightEx</h2>\n<p><code>LightEx</code> is a lightweight experiment framework to create, monitor and record your machine learning experiments. Targeted towards individual data scientists, researchers, small teams and, in general, resource-constrained experimentation. Compatible with all machine-learning frameworks.</p>\n<p><strong>Project Status:</strong> Alpha</p>\n<p>Unlike most experiment frameworks, <code>LightEx</code>sports a modular, and highly configurable design:</p>\n<ul>\n<li><strong>dispatcher</strong>: run experiments using <code>process</code>,<code>docker</code> containers or <code>kubernetes</code> pods. Switch between modes seamlessly by minor changes to config.</li>\n<li><strong>mulogger</strong>: log metrics and artifacts to multiple logger backends, using an <em>unified</em> API. Supports <code>mlflow</code>, <code>tensorboard</code> and <code>trains</code> \u2014 add new loggers easily as <em>plugins</em>.</li>\n<li><strong>namedconf</strong>:  python <code>dataclass</code> based flexible and <em>unified</em> configuration specification for jobs, parameters and model architectures. Config instances are <em>named</em> and can be <em>locally modified</em>.</li>\n<li><strong>qviz</strong>: query, compare and visualize your experiment results.</li>\n</ul>\n<p>The run environment and parameters for your experiments are specified using a config file <code>lxconfig.py</code> in your project directory. Modify, inherit, and create new <em>named</em> config instances, on-the-fly, as you perform a series of experiments.</p>\n<p>Learn more about the <strong>anatomy</strong> of a ML experimentation framework <a href=\"docs/anatomy.md\" rel=\"nofollow\">here</a>. Also, why [yet another experiment framework] (#yet-another-experiment-framework) ?</p>\n<h4>Benefits</h4>\n<p>Start with a basic <code>train</code> or <code>eval</code> project. In a few minutes,</p>\n<ul>\n<li>introduce systematic logging (multiple loggers) and visualization to your project</li>\n<li>go from running a single experiment to multiple parameterized experiments, e.g.,\n<ul>\n<li>multiple training runs over a set of hyper-parameters.</li>\n<li>multiple <code>efficient-net</code> or <code>bert</code> train or eval runs.</li>\n<li>a neural architecture search over multiple architectures in parallel.</li>\n</ul>\n</li>\n</ul>\n<h3>Installation</h3>\n<blockquote>\n<p>pip install -U lightex</p>\n</blockquote>\n<h3>Quick Start</h3>\n<p>Imagine we have an existing ML project, with the following model train command:</p>\n<blockquote>\n<p><code>train.py --data-dir ./data \u2014-lr 0.1 -\u2014hidden_dim 512</code></p>\n</blockquote>\n<p>Now, let us bring in <code>lightex</code> to organize and scale experiments for the project. In the main project directory, initialize <code>lightex</code> :</p>\n<blockquote>\n<p><code>lx init</code></p>\n</blockquote>\n<p>This creates files <code>lxconfig.py</code> and <code>run_expts.py</code>. The file <code>lxconfig.py</code> contains pre-defined <code>dataclass</code>es for specifying <em>named</em> experiment configs.</p>\n<ul>\n<li>The main class <code>Config</code>, contains three fields: <code>er</code> , <code>hp</code> and <code>run</code> (see the example below), which store configurations for logging and storage <strong>resources</strong>, <strong>hyper-parameters</strong>, and <strong>runtime</strong>, respectively.</li>\n<li>By configuring these three fields (depending on the project environment, parameters), we can <em>seamlessly</em> generate multiple experiments via different dispatchers and log the results to one or more logger backends.</li>\n<li>The config classes <code>Resources</code>, <code>HP</code> and <code>Run</code> for these fields are pre-defined  in <code>lxconfig.py</code>. See <a href=\"docs/config.md\" rel=\"nofollow\">config.md</a> for full description of the defined dataclasses.</li>\n<li><code>Config</code> also includes a <code>get_experiments</code> function, which generates a list of experiment configs to be executed by the dispatcher. .</li>\n</ul>\n<pre><span class=\"nd\">@dataclass</span>\n<span class=\"k\">class</span> <span class=\"nc\">Config</span><span class=\"p\">:</span>\n    <span class=\"n\">er</span><span class=\"p\">:</span> <span class=\"n\">Resources</span> \t\t\t\t\t<span class=\"c1\">#(Logging, Storage resources)</span>\n    <span class=\"n\">hp</span><span class=\"p\">:</span> <span class=\"n\">HP</span> \t\t\t\t\t\t<span class=\"c1\">#(Hyper-parameters of model, training)</span>\n    <span class=\"n\">run</span><span class=\"p\">:</span> <span class=\"n\">Run</span> \t\t\t\t\t\t<span class=\"c1\">#(Run-time config)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">get_experiments</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span> <span class=\"c1\">#required: generate a list of experiments to run</span>\n        <span class=\"n\">expts</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">Experiment</span><span class=\"p\">(</span><span class=\"n\">er</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">er</span><span class=\"p\">,</span> <span class=\"n\">hp</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">hp</span><span class=\"p\">,</span> <span class=\"n\">run</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">)]</span>\n        <span class=\"k\">return</span> <span class=\"n\">expts</span>\n</pre>\n<p>To create a <code>Config</code> instance for our existing project, we instantiate</p>\n<ul>\n<li>class <code>HP</code> with parameter values (<code>lr</code> and <code>hidden_dim</code>), and</li>\n<li>class <code>Run</code> 's <code>cmd</code> field with the train command <strong>template</strong>. The template refers to <code>Experiment</code> objects returned by <code>get_experiments</code> function (of this <code>Config</code> instance).\n<ul>\n<li>Need not create the template manually. Use <a href=\"lightex/namedconf/config_utils.py\" rel=\"nofollow\"><code>argparse_to_command</code></a> in your existing code to generate it.</li>\n</ul>\n</li>\n<li>class <code>Resources</code> with default values (stores experiments in <code>/tmp/ltex</code>).</li>\n</ul>\n<pre><span class=\"n\">cmd</span><span class=\"o\">=</span><span class=\"s2\">\"python train.py --data-dir {{run.data_dir}} --lr {{hp.lr}} --hidden_dim {{hp.hidden_dim}}\"</span> <span class=\"c1\">#template placeholders refer to fields of Experiment instance</span>\n<span class=\"n\">Ru1</span> <span class=\"o\">=</span> <span class=\"n\">Run</span><span class=\"p\">(</span><span class=\"n\">cmd</span><span class=\"o\">=</span><span class=\"n\">cmd</span><span class=\"p\">,</span> <span class=\"n\">experiment_name</span><span class=\"o\">=</span><span class=\"s2\">\"find_hp\"</span><span class=\"p\">)</span>\n<span class=\"n\">H2</span> <span class=\"o\">=</span> <span class=\"n\">HP</span><span class=\"p\">(</span><span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">hidden_dim</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">)</span>\n<span class=\"n\">R1</span> <span class=\"o\">=</span> <span class=\"n\">Resources</span><span class=\"p\">()</span>\n\n<span class=\"n\">C1</span> <span class=\"o\">=</span> <span class=\"n\">Config</span><span class=\"p\">(</span><span class=\"n\">er</span><span class=\"o\">=</span><span class=\"n\">R1</span><span class=\"p\">,</span> <span class=\"n\">hp</span><span class=\"o\">=</span><span class=\"n\">H1</span><span class=\"p\">,</span> <span class=\"n\">run</span><span class=\"o\">=</span><span class=\"n\">Ru1</span><span class=\"p\">)</span>\n</pre>\n<p>Once the config instance <code>C1</code> is defined, run your experiments as follows:</p>\n<blockquote>\n<p>python run_expts.py -c C1</p>\n</blockquote>\n<p><strong>That's it!</strong> Now, your experiments, logs, metrics and models are executed and recorded systematically.</p>\n<hr>\n<h3>Modify Experiment Parameters, Experiment Groups</h3>\n<p>Modify configs from previous experiments quickly using <code>replace</code> and run new experiments.</p>\n<p><strong>Example</strong>: Create a new <code>HP</code> instance and replace it in <code>C1</code> to create a new <code>Config</code>. <em>Recursive</em> replace also supported.</p>\n<pre><span class=\"n\">H2</span> <span class=\"o\">=</span> <span class=\"n\">HP</span><span class=\"p\">(</span><span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span> <span class=\"n\">hidden_dim</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">)</span>\n<span class=\"n\">C2</span> <span class=\"o\">=</span> <span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"n\">C1</span><span class=\"p\">,</span> <span class=\"n\">hp</span><span class=\"o\">=</span><span class=\"n\">H2</span><span class=\"p\">)</span> <span class=\"c1\">#inherit er=R1 and run=Ru1</span>\n</pre>\n<blockquote>\n<p>python run_expts.py -c C2</p>\n</blockquote>\n<h5>Experiment Groups</h5>\n<p>We can create multiple similar experiments programmatically and run them in parallel. To specify such <strong>experiment groups</strong>, specify a set of <code>HP</code>s in a <code>HPGroup</code> and update the <code>get_experiments</code> function (see <a href=\"scripts/lxconfig.py\" rel=\"nofollow\">scripts/lxconfig.py</a> for an example). In the usual case, these experiments have different hyper-parameters but share <code>Resources</code> and <code>Run</code> instances.</p>\n<p><strong>Note</strong>: Although LightEx pre-defines the dataclass hierarchy, it allows the developer plenty of flexibility in defining the individual fields of classes, in particular, the fields of the <code>HP</code> class.</p>\n<h3>Adding Logging to your Code</h3>\n<p>Use the unified <code>MultiLogger</code> <a href=\"lightex/mulogger\" rel=\"nofollow\">API</a> to log metrics and artifacts to multiple logger backends.</p>\n<p>Supported Loggers: <code>mlflow</code>, <code>tensorboard</code>,<code>trains</code>, <code>wandb</code>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">lightex.mulogger</span> <span class=\"kn\">import</span> <span class=\"n\">MLFlowLogger</span><span class=\"p\">,</span> <span class=\"n\">MultiLogger</span><span class=\"p\">,</span> <span class=\"n\">PytorchTBLogger</span>\n<span class=\"n\">logger</span> <span class=\"o\">=</span> <span class=\"n\">MultiLogger</span><span class=\"p\">([</span><span class=\"s1\">'mlflow'</span><span class=\"p\">,</span> <span class=\"s1\">'trains'</span><span class=\"p\">])</span>\n<span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">start_run</span><span class=\"p\">()</span>\n<span class=\"c1\"># log to trains only</span>\n<span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"s1\">'trains'</span><span class=\"p\">,</span> <span class=\"n\">ltype</span><span class=\"o\">=</span><span class=\"s1\">'hpdict'</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'alpha'</span><span class=\"p\">:</span> <span class=\"n\">alpha</span><span class=\"p\">,</span> <span class=\"s1\">'l1_ratio'</span><span class=\"p\">:</span> <span class=\"n\">l1_ratio</span><span class=\"p\">})</span>\n<span class=\"c1\"># log to mlflow only</span>\n<span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"s1\">'mlflow'</span><span class=\"p\">,</span> <span class=\"n\">ltype</span><span class=\"o\">=</span><span class=\"s1\">'scalardict'</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'mae'</span><span class=\"p\">:</span> <span class=\"n\">mae</span><span class=\"p\">,</span> <span class=\"s1\">'rmse'</span><span class=\"p\">:</span> <span class=\"n\">rmse</span><span class=\"p\">,</span> <span class=\"s1\">'r2'</span><span class=\"p\">:</span> <span class=\"n\">r2</span><span class=\"p\">},</span> <span class=\"n\">step</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"c1\"># log to all</span>\n<span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"s1\">'*'</span><span class=\"p\">,</span> <span class=\"n\">ltype</span><span class=\"o\">=</span><span class=\"s1\">'scalardict'</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'mae'</span><span class=\"p\">:</span> <span class=\"n\">mae</span><span class=\"p\">,</span> <span class=\"s1\">'rmse'</span><span class=\"p\">:</span> <span class=\"n\">rmse</span><span class=\"p\">,</span> <span class=\"s1\">'r2'</span><span class=\"p\">:</span> <span class=\"n\">r2</span><span class=\"p\">},</span> <span class=\"n\">step</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n<span class=\"c1\"># log scalars and tensors, if supported by the logger backend</span>\n<span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"s1\">'trains'</span><span class=\"p\">,</span> <span class=\"n\">ltype</span><span class=\"o\">=</span><span class=\"s1\">'1d'</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'W1'</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"o\">=</span><span class=\"n\">Tensor</span><span class=\"p\">(</span><span class=\"o\">..</span><span class=\"p\">),</span> <span class=\"n\">histogram</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">step</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">end_run</span><span class=\"p\">()</span>\n</pre>\n<p>Or, use one of the existing loggers' API directly.</p>\n<pre><span class=\"n\">logger</span> <span class=\"o\">=</span> <span class=\"n\">MLFlowLogger</span><span class=\"p\">();</span> <span class=\"n\">mlflow</span> <span class=\"o\">=</span> <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">mlflow</span>  <span class=\"c1\"># call mlflow API</span>\n<span class=\"n\">logger</span> <span class=\"o\">=</span> <span class=\"n\">PytorchTBLogger</span><span class=\"p\">()</span> <span class=\"p\">;</span> <span class=\"n\">writer</span> <span class=\"o\">=</span> <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">writer</span> <span class=\"c1\">#call tensorboard's API</span>\n<span class=\"c1\"># Similarly, use TrainsLogger for trains and WBLogger for wandb</span>\n</pre>\n<p><strong>Note</strong>: Except for changes in logging, no changes are required to your existing code! Setup the logger backend using scripts <a href=\"backend/\" rel=\"nofollow\">here</a>.</p>\n<h3>Switch to Docker</h3>\n<p>Setting up the <code>lxconfig</code> instances pays off here!</p>\n<p>Now, add a <code>Dockerfile</code> to your project which builds the runtime environment with all the project dependencies. Update the <code>Build</code> instance inside <code>Resources</code> config. See <a href=\"examples/sklearn\" rel=\"nofollow\">examples/sklearn</a>, for example.</p>\n<blockquote>\n<p>python run_expts.py -c C2 -e docker</p>\n</blockquote>\n<p>Both your code and data are mounted on the container (no copying involved) \u2014 minimal disruption in your dev cycle.</p>\n<h2>Advanced Features</h2>\n<p>More advanced features are in development stage.</p>\n<p><strong>Running Experiments on multiple nodes / servers</strong></p>\n<p>If you've setup a docker <code>swarm</code> or <code>kubernetes</code> cluster, few changes to the existing config instance allow changing the underlying experiment dispatcher.</p>\n<p>We need to virtualize code (by adding to Dockerfile) and storage.</p>\n<p>Create a shared NFS on your nodes. Switch storage config to the NFS partition. Setup scripts will be added.</p>\n<p><strong>Better QViz module, Logger Plugins</strong></p>\n<p>Improvements to qviz module and a better plugin system for loggers being developed.</p>\n<p><strong>Setup Summary</strong></p>\n<p>In summary, <code>LightEx</code> involves the following <strong>one-time setup</strong>:</p>\n<ul>\n<li>config values in <code>lxconfig.py</code></li>\n<li>Setup backend logger servers (only the ones required). Instructions <a href=\"backend/\" rel=\"nofollow\">here</a>. (Optional)</li>\n<li>Update logging calls in your code to call <code>mulogger</code> API. (Optional)</li>\n<li>Dockerfile for your project, if you want to use containers for dispatch. (Optional)</li>\n</ul>\n<p>While <code>LightEx</code> is quick to start with, it is advisable to spend some time understanding the <a href=\"llightex/config_base.py\" rel=\"nofollow\">config schema</a>.</p>\n<h3>Dependencies</h3>\n<p>Python &gt; 3.6 (require <code>dataclasses</code>, included during install). Other dependencies defined in <a href=\"setup.py\" rel=\"nofollow\">setup.py</a>.</p>\n<h3>Design Challenges</h3>\n<ul>\n<li>The configuration system allows you to abstract away the locations of the <strong>code</strong> (project directory), <strong>data</strong> directory, and the <strong>output</strong> directory, via the <code>cmd</code> field of <code>Run</code>. This enables running your code as-is across all dispatcher environments (process, docker, kubernetes).</li>\n<li>A significant part of experiment manager design is about setting up and propagating a giant web of configuration variables.\n<ul>\n<li>No optimal choice here: <code>json</code>,<code>yaml</code>,<code>jsonnet</code>\u2014 all formats have issues.</li>\n<li>Using <code>dataclass</code>es, we can write complex config specs, with built-in inheritance and ability to do <em>local updates</em>. Tiny bit of a learning curve here, bound to python, but we gain a lot of flexibility.</li>\n</ul>\n</li>\n<li>A unified <code>mulogger</code> API to abstract away the API of multiple logging backends.</li>\n<li>Designing multiple dispatchers with similar API, enabling containers and varying storage options.</li>\n<li>Read more on challenges <a href=\"docs/anatomy.md\" rel=\"nofollow\">here</a>.</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li>ML Experiment Frameworks: <a href=\"https://github.com/kubeflow/kubeflow\" rel=\"nofollow\">kubeflow</a>, <a href=\"https://www.mlflow.org/docs/latest/index.html\" rel=\"nofollow\">mlflow</a>, <a href=\"https://polyaxon.com/\" rel=\"nofollow\">polyaxon</a>, ...</li>\n<li>Loggers: <a href=\"https://sacred.readthedocs.io/en/latest/index.html\" rel=\"nofollow\">sacred</a>, <a href=\"https://github.com/allegroai/trains\" rel=\"nofollow\">trains</a>, <a href=\"https://www.wandb.com/\" rel=\"nofollow\">wandb</a>,  <a href=\"https://github.com/MIC-DKFZ/trixi\" rel=\"nofollow\">Trixi</a>, <a href=\"https://github.com/episodeyang/ml_logger\" rel=\"nofollow\">ml_logger</a></li>\n<li>Motivating Dataclasses <a href=\"https://blog.jetbrains.com/pycharm/2018/04/python-37-introducing-data-class/\" rel=\"nofollow\">intro</a>, <a href=\"https://stackoverflow.com/questions/47955263/what-are-data-classes-and-how-are-they-different-from-common-classes\" rel=\"nofollow\">how-different</a></li>\n<li>Flexible configuration\n<ul>\n<li>in modeling: allennlp, gin, jiant.</li>\n<li>in orchestration: <a href=\"https://github.com/ksonnet\" rel=\"nofollow\">ksonnet</a>, kubernetes-operator</li>\n</ul>\n</li>\n<li>On the pains of ML experimentation\n<ul>\n<li>an article from <a href=\"https://www.wandb.com/articles/iteratively-fine-tuning-neural-networks-with-weights-biases\" rel=\"nofollow\">wandb</a></li>\n</ul>\n</li>\n</ul>\n<p>Most current (2019 Q3) tools focus on the <em>logger</em> component and provide selective <code>qviz</code> components. <code>kubeflow</code> and <code>polyaxon</code> are tied to the (k8s) <em>dispatcher</em>. Every tool has its own version of config management \u2014 mostly <em>yaml</em> based, where config types are absent or have a non-nested config class. Config-specific languages have been also proposed (ksonnet, sonnet, gin).</p>\n<h3>Yet Another Experiment Framework</h3>\n<p>Systematic experimentation tools are essential for a data scientist. Unfortunately, many existing tools (<code>kubeflow</code>, <code>mlflow</code>, <code>polyaxon</code>) are too monolithic, kubernetes-first, cloud-first, target very diverse audiences and hence spread too thin, and yet lack important dev-friendly features. <code>sacred</code> 's design is' tightly coupled and requires several <code>sacred</code>-specific changes to your main code. Other tools cater only to a specific task , e.g., <code>tensorboard</code> only handles log recording and visualization. Also, contrasting different experiment frameworks is hard: there is no standardized expt-management architecture for machine learning and most open-source frameworks are undergoing a process of adhoc requirements discovery.</p>\n<h3>Author</h3>\n<p>Nishant Sinha, <a href=\"http://offnote.co\" rel=\"nofollow\">OffNote Labs</a> (<a href=\"mailto:nishant@offnote.co\">nishant@offnote.co</a>, @<a href=\"https://medium.com/@ekshakhs\" rel=\"nofollow\">medium</a>, @<a href=\"https://twitter.com/ekshakhs\" rel=\"nofollow\">twitter</a>)</p>\n<h3>Contributors</h3>\n<p>Akarsh E S, <a href=\"https://github.com/AkarshES\" rel=\"nofollow\">github</a></p>\n\n          </div>"}, "last_serial": 6499987, "releases": {"0.0.2": [{"comment_text": "", "digests": {"md5": "30d14c6023ce4bb6dd9a9ab72af5c699", "sha256": "2ce9ecd261d688bf3e279844e92b5d3e0e386f01b2ecdeb7532c6009b54356f1"}, "downloads": -1, "filename": "lightex-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "30d14c6023ce4bb6dd9a9ab72af5c699", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 13379, "upload_time": "2019-06-21T20:07:23", "upload_time_iso_8601": "2019-06-21T20:07:23.556281Z", "url": "https://files.pythonhosted.org/packages/d8/45/595c95af5e6acef5d02da1471cfd3bbb5947ad68ee2eb4cb076c71601657/lightex-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "472f7e941bd62e96f1fdc49729c760f3", "sha256": "28208a0b208b8ef1d1e87298b85f47b839850728f5ff948a1e6111b48014c300"}, "downloads": -1, "filename": "lightex-0.0.2.tar.gz", "has_sig": false, "md5_digest": "472f7e941bd62e96f1fdc49729c760f3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7837, "upload_time": "2019-06-21T20:07:26", "upload_time_iso_8601": "2019-06-21T20:07:26.070634Z", "url": "https://files.pythonhosted.org/packages/a3/35/cdf54aeed9ffb6798fb1db715c001ccd31dabd49f532993ecd5cff895b15/lightex-0.0.2.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "c4eb847cafd05fb24bcb29cf43619b5f", "sha256": "8ac6c33a9ba6d8f5efc1fcd9b3c932106467092f69a25cff250e65c0d04f244f"}, "downloads": -1, "filename": "lightex-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "c4eb847cafd05fb24bcb29cf43619b5f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">3.6.0", "size": 28652, "upload_time": "2019-08-01T06:50:52", "upload_time_iso_8601": "2019-08-01T06:50:52.657637Z", "url": "https://files.pythonhosted.org/packages/6d/76/18cf72c6265ea7aef0915a99caaaf7497bbd66135f4eb2f9ae391a2a00a8/lightex-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d916583a6c64c75ec6712b67a70feb3a", "sha256": "d429b6e2d3e38c5f1eb917ba476fdd5bda1c4998bfdd94d518f1d930fee02c2a"}, "downloads": -1, "filename": "lightex-0.0.4.tar.gz", "has_sig": false, "md5_digest": "d916583a6c64c75ec6712b67a70feb3a", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.6.0", "size": 23544, "upload_time": "2019-08-01T06:50:54", "upload_time_iso_8601": "2019-08-01T06:50:54.711018Z", "url": "https://files.pythonhosted.org/packages/ca/37/70486a3e97a9a121e2b4d15cbda13d2679e7d240510f45570c5dcd2b8f86/lightex-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "d6d5486383a30436754a790dc14ab4c4", "sha256": "61229e14f0c7c493c4f804e07a670a174bd127bc5206c9177f62c4dc30c00b4d"}, "downloads": -1, "filename": "lightex-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "d6d5486383a30436754a790dc14ab4c4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">3.6.0", "size": 30716, "upload_time": "2020-01-22T08:57:08", "upload_time_iso_8601": "2020-01-22T08:57:08.843278Z", "url": "https://files.pythonhosted.org/packages/ae/1f/fd8c31727f817e0ab7e68a14c9e34055b0b8ac8fcf4d1554307cd1d1eadc/lightex-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "896a38c406d366a9378cfb3243dbcddc", "sha256": "c65631fcec603311ba53a6a29c3365a4735c4804949d12d3959048700d434219"}, "downloads": -1, "filename": "lightex-0.0.5.tar.gz", "has_sig": false, "md5_digest": "896a38c406d366a9378cfb3243dbcddc", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.6.0", "size": 25853, "upload_time": "2020-01-22T08:57:11", "upload_time_iso_8601": "2020-01-22T08:57:11.078590Z", "url": "https://files.pythonhosted.org/packages/0c/98/fa31641aafc998be42cf9553519f1e2420073562e04a801c0c2a7ec0aec1/lightex-0.0.5.tar.gz", "yanked": false}], "0.0.5.1": [{"comment_text": "", "digests": {"md5": "cb3df9c3c6f4c78d09555f9a22cc316f", "sha256": "3fc5b09eb443df7fbc5370904711690585e27328f920cd0c9a157e763d54ce74"}, "downloads": -1, "filename": "lightex-0.0.5.1-py3-none-any.whl", "has_sig": false, "md5_digest": "cb3df9c3c6f4c78d09555f9a22cc316f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">3.6.0", "size": 30833, "upload_time": "2020-01-22T12:20:25", "upload_time_iso_8601": "2020-01-22T12:20:25.172260Z", "url": "https://files.pythonhosted.org/packages/08/25/67b3c8a0277369073f738caeebd0c5f87f013b1104282906483c7d4835ce/lightex-0.0.5.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0844a8bd140ba08ec9e7fb14cd56fa50", "sha256": "8ea4a86c695510d34956fec9eabcbccc8ead1b1c1c65aca57c637de88e60940f"}, "downloads": -1, "filename": "lightex-0.0.5.1.tar.gz", "has_sig": false, "md5_digest": "0844a8bd140ba08ec9e7fb14cd56fa50", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.6.0", "size": 25935, "upload_time": "2020-01-22T12:20:27", "upload_time_iso_8601": "2020-01-22T12:20:27.191492Z", "url": "https://files.pythonhosted.org/packages/8e/c9/cf0e365d54a71ab29d292d084e945c6146b75b01ecea49e4ca364e7db980/lightex-0.0.5.1.tar.gz", "yanked": false}], "0.0.5.2": [{"comment_text": "", "digests": {"md5": "7a46a95717437181e9ea4a78f05814a1", "sha256": "ce1f6b553ec87e3126ac101d55c33275bd502779d9bbeef5083acac7f4226b73"}, "downloads": -1, "filename": "lightex-0.0.5.2-py3-none-any.whl", "has_sig": false, "md5_digest": "7a46a95717437181e9ea4a78f05814a1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">3.6.0", "size": 32758, "upload_time": "2020-01-22T12:25:18", "upload_time_iso_8601": "2020-01-22T12:25:18.491282Z", "url": "https://files.pythonhosted.org/packages/d1/82/72d03486798cbd8d704d8668c4654b94b5880df8b4916d8519e39c99e04b/lightex-0.0.5.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d863b6af41b8187fcb92da02c4dec85a", "sha256": "66ce973ededdcb78064d293d0a5cd329f65d5ef27e7e5a4599f5ccb16cee78c5"}, "downloads": -1, "filename": "lightex-0.0.5.2.tar.gz", "has_sig": false, "md5_digest": "d863b6af41b8187fcb92da02c4dec85a", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.6.0", "size": 27064, "upload_time": "2020-01-22T12:25:20", "upload_time_iso_8601": "2020-01-22T12:25:20.356453Z", "url": "https://files.pythonhosted.org/packages/8e/73/f0c533ff8400d2a9ef2439f197f8ec486546b74290bc9df8d53b86c3264a/lightex-0.0.5.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "7a46a95717437181e9ea4a78f05814a1", "sha256": "ce1f6b553ec87e3126ac101d55c33275bd502779d9bbeef5083acac7f4226b73"}, "downloads": -1, "filename": "lightex-0.0.5.2-py3-none-any.whl", "has_sig": false, "md5_digest": "7a46a95717437181e9ea4a78f05814a1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">3.6.0", "size": 32758, "upload_time": "2020-01-22T12:25:18", "upload_time_iso_8601": "2020-01-22T12:25:18.491282Z", "url": "https://files.pythonhosted.org/packages/d1/82/72d03486798cbd8d704d8668c4654b94b5880df8b4916d8519e39c99e04b/lightex-0.0.5.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d863b6af41b8187fcb92da02c4dec85a", "sha256": "66ce973ededdcb78064d293d0a5cd329f65d5ef27e7e5a4599f5ccb16cee78c5"}, "downloads": -1, "filename": "lightex-0.0.5.2.tar.gz", "has_sig": false, "md5_digest": "d863b6af41b8187fcb92da02c4dec85a", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.6.0", "size": 27064, "upload_time": "2020-01-22T12:25:20", "upload_time_iso_8601": "2020-01-22T12:25:20.356453Z", "url": "https://files.pythonhosted.org/packages/8e/73/f0c533ff8400d2a9ef2439f197f8ec486546b74290bc9df8d53b86c3264a/lightex-0.0.5.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:46:04 2020"}