{"info": {"author": "cleocn", "author_email": "cleocn@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Framework :: Scrapy", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python"], "description": "Scrapy Random User-Agent\n========================\n\nDoes your scrapy spider get identified and blocked by servers because\nyou use the default user-agent or a generic one?\n\nUse this ``random_useragent`` module and set a random user-agent for\nevery request. \n\nInstalling\n----------\n\nInstalling it is pretty simple.\n\n.. code-block:: python\n\n    pip install git+https://github.com/cleocn/scrapy-random-useragent.git\n\nUsage\n-----\n\nIn your ``settings.py`` file, update the ``DOWNLOADER_MIDDLEWARES``\nvariable like this.\n\n.. code-block:: python\n\n    DOWNLOADER_MIDDLEWARES = {\n        'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,\n        'random_useragent.RandomUserAgentMiddleware': 400\n    }\n\nThis disables the default ``UserAgentMiddleware`` and enables the\n``RandomUserAgentMiddleware``.\n\nNow all the requests from your crawler will have a random user-agent.\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/cleocn/scrapy-random-useragent", "keywords": "scrapy random user-agent", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "scrapy-random-ua", "package_url": "https://pypi.org/project/scrapy-random-ua/", "platform": "Any", "project_url": "https://pypi.org/project/scrapy-random-ua/", "project_urls": {"Homepage": "https://github.com/cleocn/scrapy-random-useragent"}, "release_url": "https://pypi.org/project/scrapy-random-ua/0.3/", "requires_dist": null, "requires_python": "", "summary": "Scrapy Middleware to set a random User-Agent for every Request.", "version": "0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Does your scrapy spider get identified and blocked by servers because\nyou use the default user-agent or a generic one?</p>\n<p>Use this <tt>random_useragent</tt> module and set a random user-agent for\nevery request.</p>\n<div id=\"installing\">\n<h2>Installing</h2>\n<p>Installing it is pretty simple.</p>\n<pre><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">git</span><span class=\"o\">+</span><span class=\"n\">https</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">github</span><span class=\"o\">.</span><span class=\"n\">com</span><span class=\"o\">/</span><span class=\"n\">cleocn</span><span class=\"o\">/</span><span class=\"n\">scrapy</span><span class=\"o\">-</span><span class=\"n\">random</span><span class=\"o\">-</span><span class=\"n\">useragent</span><span class=\"o\">.</span><span class=\"n\">git</span>\n</pre>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>In your <tt>settings.py</tt> file, update the <tt>DOWNLOADER_MIDDLEWARES</tt>\nvariable like this.</p>\n<pre><span class=\"n\">DOWNLOADER_MIDDLEWARES</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s1\">'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware'</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"s1\">'random_useragent.RandomUserAgentMiddleware'</span><span class=\"p\">:</span> <span class=\"mi\">400</span>\n<span class=\"p\">}</span>\n</pre>\n<p>This disables the default <tt>UserAgentMiddleware</tt> and enables the\n<tt>RandomUserAgentMiddleware</tt>.</p>\n<p>Now all the requests from your crawler will have a random user-agent.</p>\n</div>\n\n          </div>"}, "last_serial": 3493384, "releases": {"0.3": [{"comment_text": "", "digests": {"md5": "2ac2a82c06b11252e0ce1b724416aaab", "sha256": "0b21843e0eb86eb7351c0762c1851343b865d4a0b750146250f0c86581e2b972"}, "downloads": -1, "filename": "scrapy-random-ua-0.3.tar.gz", "has_sig": false, "md5_digest": "2ac2a82c06b11252e0ce1b724416aaab", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3169, "upload_time": "2018-01-16T09:40:16", "upload_time_iso_8601": "2018-01-16T09:40:16.797005Z", "url": "https://files.pythonhosted.org/packages/0b/6f/d48d5c0f651e2ede25637ca91584550f793dc967f68f56e56541a154e216/scrapy-random-ua-0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2ac2a82c06b11252e0ce1b724416aaab", "sha256": "0b21843e0eb86eb7351c0762c1851343b865d4a0b750146250f0c86581e2b972"}, "downloads": -1, "filename": "scrapy-random-ua-0.3.tar.gz", "has_sig": false, "md5_digest": "2ac2a82c06b11252e0ce1b724416aaab", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3169, "upload_time": "2018-01-16T09:40:16", "upload_time_iso_8601": "2018-01-16T09:40:16.797005Z", "url": "https://files.pythonhosted.org/packages/0b/6f/d48d5c0f651e2ede25637ca91584550f793dc967f68f56e56541a154e216/scrapy-random-ua-0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:43 2020"}