{"info": {"author": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Google AI Language Team Authors, Open AI team Authors", "author_email": "thomas@huggingface.co", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# \ud83d\udc7e PyTorch-Transformers\n\n[![CircleCI](https://circleci.com/gh/huggingface/pytorch-transformers.svg?style=svg)](https://circleci.com/gh/huggingface/pytorch-transformers)\n\nPyTorch-Transformers (formerly known as `pytorch-pretrained-bert`) is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).\n\nThe library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the following models:\n\n1. **[BERT](https://github.com/google-research/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n2. **[GPT](https://github.com/openai/finetune-transformer-lm)** (from OpenAI) released with the paper [Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/) by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.\n3. **[GPT-2](https://blog.openai.com/better-language-models/)** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/) by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.\n4. **[Transformer-XL](https://github.com/kimiyoung/transformer-xl)** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n5. **[XLNet](https://github.com/zihangdai/xlnet/)** (from Google/CMU) released with the paper [\u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n6. **[XLM](https://github.com/facebookresearch/XLM/)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.\n7. **[RoBERTa](https://github.com/pytorch/fairseq/tree/master/examples/roberta)** (from Facebook), released together with the paper a [Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n8. **[DistilBERT](https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation)** (from HuggingFace), released together with the blogpost [Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of\u00a0BERT](https://medium.com/huggingface/distilbert-8cf3380435b5\n) by Victor Sanh, Lysandre Debut and Thomas Wolf.\n\nThese implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the [documentation](https://huggingface.co/pytorch-transformers/examples.html).\n\n| Section | Description |\n|-|-|\n| [Installation](#installation) | How to install the package |\n| [Quick tour: Usage](#quick-tour) | Tokenizers & models usage: Bert and GPT-2 |\n| [Quick tour: Fine-tuning/usage scripts](#quick-tour-of-the-fine-tuningusage-scripts) | Using provided scripts: GLUE, SQuAD and Text generation |\n| [Migrating from pytorch-pretrained-bert to pytorch-transformers](#Migrating-from-pytorch-pretrained-bert-to-pytorch-transformers) | Migrating your code from pytorch-pretrained-bert to pytorch-transformers |\n| [Documentation](https://huggingface.co/pytorch-transformers/) | Full API documentation and more |\n\n## Installation\n\nThis repo is tested on Python 2.7 and 3.5+ (examples are tested only on python 3.5+) and PyTorch 1.0.0+\n\n### With pip\n\nPyTorch-Transformers can be installed by pip as follows:\n\n```bash\npip install pytorch-transformers\n```\n\n### From source\n\nClone the repository and run:\n\n```bash\npip install [--editable] .\n```\n\n### Tests\n\nA series of tests is included for the library and the example scripts. Library tests can be found in the [tests folder](https://github.com/huggingface/pytorch-transformers/tree/master/pytorch_transformers/tests) and examples tests in the [examples folder](https://github.com/huggingface/pytorch-transformers/tree/master/examples).\n\nThese tests can be run using `pytest` (install pytest if needed with `pip install pytest`).\n\nYou can run the tests from the root of the cloned repository with the commands:\n\n```bash\npython -m pytest -sv ./pytorch_transformers/tests/\npython -m pytest -sv ./examples/\n```\n\n### Do you want to run a Transformer model on a mobile device?\n\nYou should check out our [`swift-coreml-transformers`](https://github.com/huggingface/swift-coreml-transformers) repo.\n\nIt contains an example of a conversion script from a Pytorch trained Transformer model (here, `GPT-2`) to a CoreML model that runs on iOS devices.\n\nAt some point in the future, you'll be able to seamlessly move from pre-training or fine-tuning models in PyTorch to productizing them in CoreML,\nor prototype a model or an app in CoreML then research its hyperparameters or architecture from PyTorch. Super exciting!\n\n\n## Quick tour\n\nLet's do a very quick overview of PyTorch-Transformers. Detailed examples for each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the [full documentation](https://huggingface.co/pytorch-transformers/).\n\n```python\nimport torch\nfrom pytorch_transformers import *\n\n# PyTorch-Transformers has a unified API\n# for 7 transformer architectures and 30 pretrained weights.\n#          Model          | Tokenizer          | Pretrained weights shortcut\nMODELS = [(BertModel,       BertTokenizer,      'bert-base-uncased'),\n          (OpenAIGPTModel,  OpenAIGPTTokenizer, 'openai-gpt'),\n          (GPT2Model,       GPT2Tokenizer,      'gpt2'),\n          (TransfoXLModel,  TransfoXLTokenizer, 'transfo-xl-wt103'),\n          (XLNetModel,      XLNetTokenizer,     'xlnet-base-cased'),\n          (XLMModel,        XLMTokenizer,       'xlm-mlm-enfr-1024'),\n          (RobertaModel,    RobertaTokenizer,   'roberta-base')]\n\n# Let's encode some text in a sequence of hidden-states using each model:\nfor model_class, tokenizer_class, pretrained_weights in MODELS:\n    # Load pretrained model/tokenizer\n    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n    model = model_class.from_pretrained(pretrained_weights)\n\n    # Encode text\n    input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n    with torch.no_grad():\n        last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples\n\n# Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.\nBERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n                      BertForSequenceClassification, BertForMultipleChoice, BertForTokenClassification,\n                      BertForQuestionAnswering]\n\n# All the classes for an architecture can be initiated from pretrained weights for this architecture\n# Note that additional weights added for fine-tuning are only initialized\n# and need to be trained on the down-stream task\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nfor model_class in BERT_MODEL_CLASSES:\n    # Load pretrained model/tokenizer\n    model = model_class.from_pretrained('bert-base-uncased')\n\n# Models can return full list of hidden-states & attentions weights at each layer\nmodel = model_class.from_pretrained(pretrained_weights,\n                                    output_hidden_states=True,\n                                    output_attentions=True)\ninput_ids = torch.tensor([tokenizer.encode(\"Let's see all hidden-states and attentions on this text\")])\nall_hidden_states, all_attentions = model(input_ids)[-2:]\n\n# Models are compatible with Torchscript\nmodel = model_class.from_pretrained(pretrained_weights, torchscript=True)\ntraced_model = torch.jit.trace(model, (input_ids,))\n\n# Simple serialization for models and tokenizers\nmodel.save_pretrained('./directory/to/save/')  # save\nmodel = model_class.from_pretrained('./directory/to/save/')  # re-load\ntokenizer.save_pretrained('./directory/to/save/')  # save\ntokenizer = tokenizer_class.from_pretrained('./directory/to/save/')  # re-load\n\n# SOTA examples for GLUE, SQUAD, text generation...\n```\n\n## Quick tour of the fine-tuning/usage scripts\n\nThe library comprises several example scripts with SOTA performances for NLU and NLG tasks:\n\n- `run_glue.py`: an example fine-tuning Bert, XLNet and XLM on nine different GLUE tasks (*sequence-level classification*)\n- `run_squad.py`: an example fine-tuning Bert, XLNet and XLM on the question answering dataset SQuAD 2.0 (*token-level classification*)\n- `run_generation.py`: an example using GPT, GPT-2, Transformer-XL and XLNet for conditional language generation\n- other model-specific examples (see the documentation).\n\nHere are three quick usage examples for these scripts:\n\n### `run_glue.py`: Fine-tuning on GLUE tasks for sequence classification\n\nThe [General Language Understanding Evaluation (GLUE) benchmark](https://gluebenchmark.com/) is a collection of nine sentence- or sentence-pair language understanding tasks for evaluating and analyzing natural language understanding systems.\n\nBefore running anyone of these GLUE tasks you should download the\n[GLUE data](https://gluebenchmark.com/tasks) by running\n[this script](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e)\nand unpack it to some directory `$GLUE_DIR`.\n\nYou should also install the additional packages required by the examples:\n\n```shell\npip install -r ./examples/requirements.txt\n```\n\n```shell\nexport GLUE_DIR=/path/to/glue\nexport TASK_NAME=MRPC\n\npython ./examples/run_glue.py \\\n    --model_type bert \\\n    --model_name_or_path bert-base-uncased \\\n    --task_name $TASK_NAME \\\n    --do_train \\\n    --do_eval \\\n    --do_lower_case \\\n    --data_dir $GLUE_DIR/$TASK_NAME \\\n    --max_seq_length 128 \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 3.0 \\\n    --output_dir /tmp/$TASK_NAME/\n```\n\nwhere task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.\n\nThe dev set results will be present within the text file 'eval_results.txt' in the specified output_dir. In case of MNLI, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called '/tmp/MNLI-MM/' in addition to '/tmp/MNLI/'.\n\n#### Fine-tuning XLNet model on the STS-B regression task\n\nThis example code fine-tunes XLNet on the STS-B corpus using parallel training on a server with 4 V100 GPUs.\nParallel training is a simple way to use several GPUs (but is slower and less flexible than distributed training, see below).\n\n```shell\nexport GLUE_DIR=/path/to/glue\n\npython ./examples/run_glue.py \\\n    --model_type xlnet \\\n    --model_name_or_path xlnet-large-cased \\\n    --do_train  \\\n    --do_eval   \\\n    --task_name=sts-b     \\\n    --data_dir=${GLUE_DIR}/STS-B  \\\n    --output_dir=./proc_data/sts-b-110   \\\n    --max_seq_length=128   \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --gradient_accumulation_steps=1 \\\n    --max_steps=1200  \\\n    --model_name=xlnet-large-cased   \\\n    --overwrite_output_dir   \\\n    --overwrite_cache \\\n    --warmup_steps=120\n```\n\nOn this machine we thus have a batch size of 32, please increase `gradient_accumulation_steps` to reach the same batch size if you have a smaller machine. These hyper-parameters should result in a Pearson correlation coefficient of `+0.917` on the development set.\n\n#### Fine-tuning Bert model on the MRPC classification task\n\nThis example code fine-tunes the Bert Whole Word Masking model on the Microsoft Research Paraphrase Corpus (MRPC) corpus using distributed training on 8 V100 GPUs to reach a F1 > 92.\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 8 ./examples/run_glue.py   \\\n    --model_type bert \\\n    --model_name_or_path bert-large-uncased-whole-word-masking \\\n    --task_name MRPC \\\n    --do_train   \\\n    --do_eval   \\\n    --do_lower_case   \\\n    --data_dir $GLUE_DIR/MRPC/   \\\n    --max_seq_length 128   \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --learning_rate 2e-5   \\\n    --num_train_epochs 3.0  \\\n    --output_dir /tmp/mrpc_output/ \\\n    --overwrite_output_dir   \\\n    --overwrite_cache \\\n```\n\nTraining with these hyper-parameters gave us the following results:\n\n```bash\n  acc = 0.8823529411764706\n  acc_and_f1 = 0.901702786377709\n  eval_loss = 0.3418912578906332\n  f1 = 0.9210526315789473\n  global_step = 174\n  loss = 0.07231863956341798\n```\n\n### `run_squad.py`: Fine-tuning on SQuAD for question-answering\n\nThis example code fine-tunes BERT on the SQuAD dataset using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 > 93 on SQuAD:\n\n```bash\npython -m torch.distributed.launch --nproc_per_node=8 ./examples/run_squad.py \\\n    --model_type bert \\\n    --model_name_or_path bert-large-uncased-whole-word-masking \\\n    --do_train \\\n    --do_eval \\\n    --do_lower_case \\\n    --train_file $SQUAD_DIR/train-v1.1.json \\\n    --predict_file $SQUAD_DIR/dev-v1.1.json \\\n    --learning_rate 3e-5 \\\n    --num_train_epochs 2 \\\n    --max_seq_length 384 \\\n    --doc_stride 128 \\\n    --output_dir ../models/wwm_uncased_finetuned_squad/ \\\n    --per_gpu_eval_batch_size=3   \\\n    --per_gpu_train_batch_size=3   \\\n```\n\nTraining with these hyper-parameters gave us the following results:\n\n```bash\npython $SQUAD_DIR/evaluate-v1.1.py $SQUAD_DIR/dev-v1.1.json ../models/wwm_uncased_finetuned_squad/predictions.json\n{\"exact_match\": 86.91579943235573, \"f1\": 93.1532499015869}\n```\n\nThis is the model provided as `bert-large-uncased-whole-word-masking-finetuned-squad`.\n\n### `run_generation.py`: Text generation with GPT, GPT-2, Transformer-XL and XLNet\n\nA conditional generation script is also included to generate text from a prompt.\nThe generation script includes the [tricks](https://github.com/rusiaaman/XLNet-gen#methodology) proposed by by Aman Rusia to get high quality generation with memory models like Transformer-XL and XLNet (include a predefined text to make short inputs longer).\n\nHere is how to run the script with the small version of OpenAI GPT-2 model:\n\n```shell\npython ./examples/run_generation.py \\\n    --model_type=gpt2 \\\n    --length=20 \\\n    --model_name_or_path=gpt2 \\\n```\n\n## Migrating from pytorch-pretrained-bert to pytorch-transformers\n\nHere is a quick summary of what you should take care of when migrating from `pytorch-pretrained-bert` to `pytorch-transformers`\n\n### Models always output `tuples`\n\nThe main breaking change when migrating from `pytorch-pretrained-bert` to `pytorch-transformers` is that the models forward method always outputs a `tuple` with various elements depending on the model and the configuration parameters.\n\nThe exact content of the tuples for each model are detailed in the models' docstrings and the [documentation](https://huggingface.co/pytorch-transformers/).\n\nIn pretty much every case, you will be fine by taking the first element of the output as the output you previously used in `pytorch-pretrained-bert`.\n\nHere is a `pytorch-pretrained-bert` to `pytorch-transformers` conversion example for a `BertForSequenceClassification` classification model:\n\n```python\n# Let's load our model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\n# If you used to have this line in pytorch-pretrained-bert:\nloss = model(input_ids, labels=labels)\n\n# Now just use this line in pytorch-transformers to extract the loss from the output tuple:\noutputs = model(input_ids, labels=labels)\nloss = outputs[0]\n\n# In pytorch-transformers you can also have access to the logits:\nloss, logits = outputs[:2]\n\n# And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', output_attentions=True)\noutputs = model(input_ids, labels=labels)\nloss, logits, attentions = outputs\n```\n\n### Serialization\n\nBreaking change in the `from_pretrained()`method:\n\n1. Models are now set in evaluation mode by default when instantiated with the `from_pretrained()` method. To train them don't forget to set them back in training mode (`model.train()`) to activate the dropout modules.\n\n2. The additional `*input` and `**kwargs` arguments supplied to the `from_pretrained()` method used to be directly passed to the underlying model's class `__init__()` method. They are now used to update the model configuration attribute instead which can break derived model classes build based on the previous `BertForSequenceClassification` examples. We are working on a way to mitigate this breaking change in [#866](https://github.com/huggingface/pytorch-transformers/pull/866) by forwarding the the model `__init__()` method (i) the provided positional arguments and (ii) the keyword arguments which do not match any configuration class attributes.\n\nAlso, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method `save_pretrained(save_directory)` if you were using any other serialization method before.\n\nHere is an example:\n\n```python\n### Let's load a model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n### Do some stuff to our model and tokenizer\n# Ex: add new tokens to the vocabulary and embeddings of our model\ntokenizer.add_tokens(['[SPECIAL_TOKEN_1]', '[SPECIAL_TOKEN_2]'])\nmodel.resize_token_embeddings(len(tokenizer))\n# Train our model\ntrain(model)\n\n### Now let's save our model and tokenizer to a directory\nmodel.save_pretrained('./my_saved_model_directory/')\ntokenizer.save_pretrained('./my_saved_model_directory/')\n\n### Reload the model and the tokenizer\nmodel = BertForSequenceClassification.from_pretrained('./my_saved_model_directory/')\ntokenizer = BertTokenizer.from_pretrained('./my_saved_model_directory/')\n```\n\n### Optimizers: BertAdam & OpenAIAdam are now AdamW, schedules are standard PyTorch schedules\n\nThe two optimizers previously included, `BertAdam` and `OpenAIAdam`, have been replaced by a single `AdamW` optimizer which has a few differences:\n\n- it only implements weights decay correction,\n- schedules are now externals (see below),\n- gradient clipping is now also external (see below).\n\nThe new optimizer `AdamW` matches PyTorch `Adam` optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping.\n\nThe schedules are now standard [PyTorch learning rate schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) and not part of the optimizer anymore.\n\nHere is a conversion examples from `BertAdam` with a linear warmup and decay schedule to `AdamW` and the same schedule:\n\n```python\n# Parameters:\nlr = 1e-3\nmax_grad_norm = 1.0\nnum_total_steps = 1000\nnum_warmup_steps = 100\nwarmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n\n### Previously BertAdam optimizer was instantiated like this:\noptimizer = BertAdam(model.parameters(), lr=lr, schedule='warmup_linear', warmup=warmup_proportion, t_total=num_total_steps)\n### and used like this:\nfor batch in train_data:\n    loss = model(batch)\n    loss.backward()\n    optimizer.step()\n\n### In PyTorch-Transformers, optimizer and schedules are splitted and instantiated like this:\noptimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\nscheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_total_steps)  # PyTorch scheduler\n### and used like this:\nfor batch in train_data:\n    loss = model(batch)\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n```\n\n## Citation\n\nAt the moment, there is no paper associated to PyTorch-Transformers but we are working on preparing one. In the meantime, please include a mention of the library and a link to the present repository if you use this work in a published or open-source project.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/huggingface/pytorch-transformers", "keywords": "NLP deep learning transformer pytorch BERT GPT GPT-2 google openai CMU", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "pytorch-transformers", "package_url": "https://pypi.org/project/pytorch-transformers/", "platform": "", "project_url": "https://pypi.org/project/pytorch-transformers/", "project_urls": {"Homepage": "https://github.com/huggingface/pytorch-transformers"}, "release_url": "https://pypi.org/project/pytorch-transformers/1.2.0/", "requires_dist": ["torch (>=1.0.0)", "numpy", "boto3", "requests", "tqdm", "regex", "sentencepiece", "sacremoses"], "requires_python": "", "summary": "Repository of pre-trained NLP Transformer models: BERT & RoBERTa, GPT & GPT-2, Transformer-XL, XLNet and XLM", "version": "1.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>\ud83d\udc7e PyTorch-Transformers</h1>\n<p><a href=\"https://circleci.com/gh/huggingface/pytorch-transformers\" rel=\"nofollow\"><img alt=\"CircleCI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/734018d847f89ffa66a9183ff7cfd8e30cb4fcfa/68747470733a2f2f636972636c6563692e636f6d2f67682f68756767696e67666163652f7079746f7263682d7472616e73666f726d6572732e7376673f7374796c653d737667\"></a></p>\n<p>PyTorch-Transformers (formerly known as <code>pytorch-pretrained-bert</code>) is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).</p>\n<p>The library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the following models:</p>\n<ol>\n<li><strong><a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">BERT</a></strong> (from Google) released with the paper <a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.</li>\n<li><strong><a href=\"https://github.com/openai/finetune-transformer-lm\" rel=\"nofollow\">GPT</a></strong> (from OpenAI) released with the paper <a href=\"https://blog.openai.com/language-unsupervised/\" rel=\"nofollow\">Improving Language Understanding by Generative Pre-Training</a> by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.</li>\n<li><strong><a href=\"https://blog.openai.com/better-language-models/\" rel=\"nofollow\">GPT-2</a></strong> (from OpenAI) released with the paper <a href=\"https://blog.openai.com/better-language-models/\" rel=\"nofollow\">Language Models are Unsupervised Multitask Learners</a> by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.</li>\n<li><strong><a href=\"https://github.com/kimiyoung/transformer-xl\" rel=\"nofollow\">Transformer-XL</a></strong> (from Google/CMU) released with the paper <a href=\"https://arxiv.org/abs/1901.02860\" rel=\"nofollow\">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.</li>\n<li><strong><a href=\"https://github.com/zihangdai/xlnet/\" rel=\"nofollow\">XLNet</a></strong> (from Google/CMU) released with the paper <a href=\"https://arxiv.org/abs/1906.08237\" rel=\"nofollow\">\u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding</a> by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.</li>\n<li><strong><a href=\"https://github.com/facebookresearch/XLM/\" rel=\"nofollow\">XLM</a></strong> (from Facebook) released together with the paper <a href=\"https://arxiv.org/abs/1901.07291\" rel=\"nofollow\">Cross-lingual Language Model Pretraining</a> by Guillaume Lample and Alexis Conneau.</li>\n<li><strong><a href=\"https://github.com/pytorch/fairseq/tree/master/examples/roberta\" rel=\"nofollow\">RoBERTa</a></strong> (from Facebook), released together with the paper a <a href=\"https://arxiv.org/abs/1907.11692\" rel=\"nofollow\">Robustly Optimized BERT Pretraining Approach</a> by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.</li>\n<li><strong><a href=\"https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation\" rel=\"nofollow\">DistilBERT</a></strong> (from HuggingFace), released together with the blogpost <a href=\"https://medium.com/huggingface/distilbert-8cf3380435b5\" rel=\"nofollow\">Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of\u00a0BERT</a> by Victor Sanh, Lysandre Debut and Thomas Wolf.</li>\n</ol>\n<p>These implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the <a href=\"https://huggingface.co/pytorch-transformers/examples.html\" rel=\"nofollow\">documentation</a>.</p>\n<table>\n<thead>\n<tr>\n<th>Section</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"#installation\" rel=\"nofollow\">Installation</a></td>\n<td>How to install the package</td>\n</tr>\n<tr>\n<td><a href=\"#quick-tour\" rel=\"nofollow\">Quick tour: Usage</a></td>\n<td>Tokenizers &amp; models usage: Bert and GPT-2</td>\n</tr>\n<tr>\n<td><a href=\"#quick-tour-of-the-fine-tuningusage-scripts\" rel=\"nofollow\">Quick tour: Fine-tuning/usage scripts</a></td>\n<td>Using provided scripts: GLUE, SQuAD and Text generation</td>\n</tr>\n<tr>\n<td><a href=\"#Migrating-from-pytorch-pretrained-bert-to-pytorch-transformers\" rel=\"nofollow\">Migrating from pytorch-pretrained-bert to pytorch-transformers</a></td>\n<td>Migrating your code from pytorch-pretrained-bert to pytorch-transformers</td>\n</tr>\n<tr>\n<td><a href=\"https://huggingface.co/pytorch-transformers/\" rel=\"nofollow\">Documentation</a></td>\n<td>Full API documentation and more</td>\n</tr></tbody></table>\n<h2>Installation</h2>\n<p>This repo is tested on Python 2.7 and 3.5+ (examples are tested only on python 3.5+) and PyTorch 1.0.0+</p>\n<h3>With pip</h3>\n<p>PyTorch-Transformers can be installed by pip as follows:</p>\n<pre>pip install pytorch-transformers\n</pre>\n<h3>From source</h3>\n<p>Clone the repository and run:</p>\n<pre>pip install <span class=\"o\">[</span>--editable<span class=\"o\">]</span> .\n</pre>\n<h3>Tests</h3>\n<p>A series of tests is included for the library and the example scripts. Library tests can be found in the <a href=\"https://github.com/huggingface/pytorch-transformers/tree/master/pytorch_transformers/tests\" rel=\"nofollow\">tests folder</a> and examples tests in the <a href=\"https://github.com/huggingface/pytorch-transformers/tree/master/examples\" rel=\"nofollow\">examples folder</a>.</p>\n<p>These tests can be run using <code>pytest</code> (install pytest if needed with <code>pip install pytest</code>).</p>\n<p>You can run the tests from the root of the cloned repository with the commands:</p>\n<pre>python -m pytest -sv ./pytorch_transformers/tests/\npython -m pytest -sv ./examples/\n</pre>\n<h3>Do you want to run a Transformer model on a mobile device?</h3>\n<p>You should check out our <a href=\"https://github.com/huggingface/swift-coreml-transformers\" rel=\"nofollow\"><code>swift-coreml-transformers</code></a> repo.</p>\n<p>It contains an example of a conversion script from a Pytorch trained Transformer model (here, <code>GPT-2</code>) to a CoreML model that runs on iOS devices.</p>\n<p>At some point in the future, you'll be able to seamlessly move from pre-training or fine-tuning models in PyTorch to productizing them in CoreML,\nor prototype a model or an app in CoreML then research its hyperparameters or architecture from PyTorch. Super exciting!</p>\n<h2>Quick tour</h2>\n<p>Let's do a very quick overview of PyTorch-Transformers. Detailed examples for each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the <a href=\"https://huggingface.co/pytorch-transformers/\" rel=\"nofollow\">full documentation</a>.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pytorch_transformers</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n\n<span class=\"c1\"># PyTorch-Transformers has a unified API</span>\n<span class=\"c1\"># for 7 transformer architectures and 30 pretrained weights.</span>\n<span class=\"c1\">#          Model          | Tokenizer          | Pretrained weights shortcut</span>\n<span class=\"n\">MODELS</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"n\">BertModel</span><span class=\"p\">,</span>       <span class=\"n\">BertTokenizer</span><span class=\"p\">,</span>      <span class=\"s1\">'bert-base-uncased'</span><span class=\"p\">),</span>\n          <span class=\"p\">(</span><span class=\"n\">OpenAIGPTModel</span><span class=\"p\">,</span>  <span class=\"n\">OpenAIGPTTokenizer</span><span class=\"p\">,</span> <span class=\"s1\">'openai-gpt'</span><span class=\"p\">),</span>\n          <span class=\"p\">(</span><span class=\"n\">GPT2Model</span><span class=\"p\">,</span>       <span class=\"n\">GPT2Tokenizer</span><span class=\"p\">,</span>      <span class=\"s1\">'gpt2'</span><span class=\"p\">),</span>\n          <span class=\"p\">(</span><span class=\"n\">TransfoXLModel</span><span class=\"p\">,</span>  <span class=\"n\">TransfoXLTokenizer</span><span class=\"p\">,</span> <span class=\"s1\">'transfo-xl-wt103'</span><span class=\"p\">),</span>\n          <span class=\"p\">(</span><span class=\"n\">XLNetModel</span><span class=\"p\">,</span>      <span class=\"n\">XLNetTokenizer</span><span class=\"p\">,</span>     <span class=\"s1\">'xlnet-base-cased'</span><span class=\"p\">),</span>\n          <span class=\"p\">(</span><span class=\"n\">XLMModel</span><span class=\"p\">,</span>        <span class=\"n\">XLMTokenizer</span><span class=\"p\">,</span>       <span class=\"s1\">'xlm-mlm-enfr-1024'</span><span class=\"p\">),</span>\n          <span class=\"p\">(</span><span class=\"n\">RobertaModel</span><span class=\"p\">,</span>    <span class=\"n\">RobertaTokenizer</span><span class=\"p\">,</span>   <span class=\"s1\">'roberta-base'</span><span class=\"p\">)]</span>\n\n<span class=\"c1\"># Let's encode some text in a sequence of hidden-states using each model:</span>\n<span class=\"k\">for</span> <span class=\"n\">model_class</span><span class=\"p\">,</span> <span class=\"n\">tokenizer_class</span><span class=\"p\">,</span> <span class=\"n\">pretrained_weights</span> <span class=\"ow\">in</span> <span class=\"n\">MODELS</span><span class=\"p\">:</span>\n    <span class=\"c1\"># Load pretrained model/tokenizer</span>\n    <span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer_class</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">pretrained_weights</span><span class=\"p\">)</span>\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">model_class</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">pretrained_weights</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Encode text</span>\n    <span class=\"n\">input_ids</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"s2\">\"Here is some text to encode\"</span><span class=\"p\">,</span> <span class=\"n\">add_special_tokens</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)])</span>  <span class=\"c1\"># Add special tokens takes care of adding [CLS], [SEP], &lt;s&gt;... tokens in the right way for each model.</span>\n    <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n        <span class=\"n\">last_hidden_states</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>  <span class=\"c1\"># Models outputs are now tuples</span>\n\n<span class=\"c1\"># Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.</span>\n<span class=\"n\">BERT_MODEL_CLASSES</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">BertModel</span><span class=\"p\">,</span> <span class=\"n\">BertForPreTraining</span><span class=\"p\">,</span> <span class=\"n\">BertForMaskedLM</span><span class=\"p\">,</span> <span class=\"n\">BertForNextSentencePrediction</span><span class=\"p\">,</span>\n                      <span class=\"n\">BertForSequenceClassification</span><span class=\"p\">,</span> <span class=\"n\">BertForMultipleChoice</span><span class=\"p\">,</span> <span class=\"n\">BertForTokenClassification</span><span class=\"p\">,</span>\n                      <span class=\"n\">BertForQuestionAnswering</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># All the classes for an architecture can be initiated from pretrained weights for this architecture</span>\n<span class=\"c1\"># Note that additional weights added for fine-tuning are only initialized</span>\n<span class=\"c1\"># and need to be trained on the down-stream task</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">BertTokenizer</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'bert-base-uncased'</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">model_class</span> <span class=\"ow\">in</span> <span class=\"n\">BERT_MODEL_CLASSES</span><span class=\"p\">:</span>\n    <span class=\"c1\"># Load pretrained model/tokenizer</span>\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">model_class</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'bert-base-uncased'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Models can return full list of hidden-states &amp; attentions weights at each layer</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">model_class</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">pretrained_weights</span><span class=\"p\">,</span>\n                                    <span class=\"n\">output_hidden_states</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n                                    <span class=\"n\">output_attentions</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">input_ids</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"s2\">\"Let's see all hidden-states and attentions on this text\"</span><span class=\"p\">)])</span>\n<span class=\"n\">all_hidden_states</span><span class=\"p\">,</span> <span class=\"n\">all_attentions</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">)[</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">:]</span>\n\n<span class=\"c1\"># Models are compatible with Torchscript</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">model_class</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">pretrained_weights</span><span class=\"p\">,</span> <span class=\"n\">torchscript</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">traced_model</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">jit</span><span class=\"o\">.</span><span class=\"n\">trace</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">,))</span>\n\n<span class=\"c1\"># Simple serialization for models and tokenizers</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">save_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'./directory/to/save/'</span><span class=\"p\">)</span>  <span class=\"c1\"># save</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">model_class</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'./directory/to/save/'</span><span class=\"p\">)</span>  <span class=\"c1\"># re-load</span>\n<span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">save_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'./directory/to/save/'</span><span class=\"p\">)</span>  <span class=\"c1\"># save</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer_class</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'./directory/to/save/'</span><span class=\"p\">)</span>  <span class=\"c1\"># re-load</span>\n\n<span class=\"c1\"># SOTA examples for GLUE, SQUAD, text generation...</span>\n</pre>\n<h2>Quick tour of the fine-tuning/usage scripts</h2>\n<p>The library comprises several example scripts with SOTA performances for NLU and NLG tasks:</p>\n<ul>\n<li><code>run_glue.py</code>: an example fine-tuning Bert, XLNet and XLM on nine different GLUE tasks (<em>sequence-level classification</em>)</li>\n<li><code>run_squad.py</code>: an example fine-tuning Bert, XLNet and XLM on the question answering dataset SQuAD 2.0 (<em>token-level classification</em>)</li>\n<li><code>run_generation.py</code>: an example using GPT, GPT-2, Transformer-XL and XLNet for conditional language generation</li>\n<li>other model-specific examples (see the documentation).</li>\n</ul>\n<p>Here are three quick usage examples for these scripts:</p>\n<h3><code>run_glue.py</code>: Fine-tuning on GLUE tasks for sequence classification</h3>\n<p>The <a href=\"https://gluebenchmark.com/\" rel=\"nofollow\">General Language Understanding Evaluation (GLUE) benchmark</a> is a collection of nine sentence- or sentence-pair language understanding tasks for evaluating and analyzing natural language understanding systems.</p>\n<p>Before running anyone of these GLUE tasks you should download the\n<a href=\"https://gluebenchmark.com/tasks\" rel=\"nofollow\">GLUE data</a> by running\n<a href=\"https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e\" rel=\"nofollow\">this script</a>\nand unpack it to some directory <code>$GLUE_DIR</code>.</p>\n<p>You should also install the additional packages required by the examples:</p>\n<pre>pip install -r ./examples/requirements.txt\n</pre>\n<pre><span class=\"nb\">export</span> <span class=\"nv\">GLUE_DIR</span><span class=\"o\">=</span>/path/to/glue\n<span class=\"nb\">export</span> <span class=\"nv\">TASK_NAME</span><span class=\"o\">=</span>MRPC\n\npython ./examples/run_glue.py <span class=\"se\">\\</span>\n    --model_type bert <span class=\"se\">\\</span>\n    --model_name_or_path bert-base-uncased <span class=\"se\">\\</span>\n    --task_name <span class=\"nv\">$TASK_NAME</span> <span class=\"se\">\\</span>\n    --do_train <span class=\"se\">\\</span>\n    --do_eval <span class=\"se\">\\</span>\n    --do_lower_case <span class=\"se\">\\</span>\n    --data_dir <span class=\"nv\">$GLUE_DIR</span>/<span class=\"nv\">$TASK_NAME</span> <span class=\"se\">\\</span>\n    --max_seq_length <span class=\"m\">128</span> <span class=\"se\">\\</span>\n    --per_gpu_eval_batch_size<span class=\"o\">=</span><span class=\"m\">8</span>   <span class=\"se\">\\</span>\n    --per_gpu_train_batch_size<span class=\"o\">=</span><span class=\"m\">8</span>   <span class=\"se\">\\</span>\n    --learning_rate 2e-5 <span class=\"se\">\\</span>\n    --num_train_epochs <span class=\"m\">3</span>.0 <span class=\"se\">\\</span>\n    --output_dir /tmp/<span class=\"nv\">$TASK_NAME</span>/\n</pre>\n<p>where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.</p>\n<p>The dev set results will be present within the text file 'eval_results.txt' in the specified output_dir. In case of MNLI, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called '/tmp/MNLI-MM/' in addition to '/tmp/MNLI/'.</p>\n<h4>Fine-tuning XLNet model on the STS-B regression task</h4>\n<p>This example code fine-tunes XLNet on the STS-B corpus using parallel training on a server with 4 V100 GPUs.\nParallel training is a simple way to use several GPUs (but is slower and less flexible than distributed training, see below).</p>\n<pre><span class=\"nb\">export</span> <span class=\"nv\">GLUE_DIR</span><span class=\"o\">=</span>/path/to/glue\n\npython ./examples/run_glue.py <span class=\"se\">\\</span>\n    --model_type xlnet <span class=\"se\">\\</span>\n    --model_name_or_path xlnet-large-cased <span class=\"se\">\\</span>\n    --do_train  <span class=\"se\">\\</span>\n    --do_eval   <span class=\"se\">\\</span>\n    --task_name<span class=\"o\">=</span>sts-b     <span class=\"se\">\\</span>\n    --data_dir<span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">GLUE_DIR</span><span class=\"si\">}</span>/STS-B  <span class=\"se\">\\</span>\n    --output_dir<span class=\"o\">=</span>./proc_data/sts-b-110   <span class=\"se\">\\</span>\n    --max_seq_length<span class=\"o\">=</span><span class=\"m\">128</span>   <span class=\"se\">\\</span>\n    --per_gpu_eval_batch_size<span class=\"o\">=</span><span class=\"m\">8</span>   <span class=\"se\">\\</span>\n    --per_gpu_train_batch_size<span class=\"o\">=</span><span class=\"m\">8</span>   <span class=\"se\">\\</span>\n    --gradient_accumulation_steps<span class=\"o\">=</span><span class=\"m\">1</span> <span class=\"se\">\\</span>\n    --max_steps<span class=\"o\">=</span><span class=\"m\">1200</span>  <span class=\"se\">\\</span>\n    --model_name<span class=\"o\">=</span>xlnet-large-cased   <span class=\"se\">\\</span>\n    --overwrite_output_dir   <span class=\"se\">\\</span>\n    --overwrite_cache <span class=\"se\">\\</span>\n    --warmup_steps<span class=\"o\">=</span><span class=\"m\">120</span>\n</pre>\n<p>On this machine we thus have a batch size of 32, please increase <code>gradient_accumulation_steps</code> to reach the same batch size if you have a smaller machine. These hyper-parameters should result in a Pearson correlation coefficient of <code>+0.917</code> on the development set.</p>\n<h4>Fine-tuning Bert model on the MRPC classification task</h4>\n<p>This example code fine-tunes the Bert Whole Word Masking model on the Microsoft Research Paraphrase Corpus (MRPC) corpus using distributed training on 8 V100 GPUs to reach a F1 &gt; 92.</p>\n<pre>python -m torch.distributed.launch --nproc_per_node <span class=\"m\">8</span> ./examples/run_glue.py   <span class=\"se\">\\</span>\n    --model_type bert <span class=\"se\">\\</span>\n    --model_name_or_path bert-large-uncased-whole-word-masking <span class=\"se\">\\</span>\n    --task_name MRPC <span class=\"se\">\\</span>\n    --do_train   <span class=\"se\">\\</span>\n    --do_eval   <span class=\"se\">\\</span>\n    --do_lower_case   <span class=\"se\">\\</span>\n    --data_dir <span class=\"nv\">$GLUE_DIR</span>/MRPC/   <span class=\"se\">\\</span>\n    --max_seq_length <span class=\"m\">128</span>   <span class=\"se\">\\</span>\n    --per_gpu_eval_batch_size<span class=\"o\">=</span><span class=\"m\">8</span>   <span class=\"se\">\\</span>\n    --per_gpu_train_batch_size<span class=\"o\">=</span><span class=\"m\">8</span>   <span class=\"se\">\\</span>\n    --learning_rate 2e-5   <span class=\"se\">\\</span>\n    --num_train_epochs <span class=\"m\">3</span>.0  <span class=\"se\">\\</span>\n    --output_dir /tmp/mrpc_output/ <span class=\"se\">\\</span>\n    --overwrite_output_dir   <span class=\"se\">\\</span>\n    --overwrite_cache <span class=\"se\">\\</span>\n</pre>\n<p>Training with these hyper-parameters gave us the following results:</p>\n<pre>  <span class=\"nv\">acc</span> <span class=\"o\">=</span> <span class=\"m\">0</span>.8823529411764706\n  <span class=\"nv\">acc_and_f1</span> <span class=\"o\">=</span> <span class=\"m\">0</span>.901702786377709\n  <span class=\"nv\">eval_loss</span> <span class=\"o\">=</span> <span class=\"m\">0</span>.3418912578906332\n  <span class=\"nv\">f1</span> <span class=\"o\">=</span> <span class=\"m\">0</span>.9210526315789473\n  <span class=\"nv\">global_step</span> <span class=\"o\">=</span> <span class=\"m\">174</span>\n  <span class=\"nv\">loss</span> <span class=\"o\">=</span> <span class=\"m\">0</span>.07231863956341798\n</pre>\n<h3><code>run_squad.py</code>: Fine-tuning on SQuAD for question-answering</h3>\n<p>This example code fine-tunes BERT on the SQuAD dataset using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 &gt; 93 on SQuAD:</p>\n<pre>python -m torch.distributed.launch --nproc_per_node<span class=\"o\">=</span><span class=\"m\">8</span> ./examples/run_squad.py <span class=\"se\">\\</span>\n    --model_type bert <span class=\"se\">\\</span>\n    --model_name_or_path bert-large-uncased-whole-word-masking <span class=\"se\">\\</span>\n    --do_train <span class=\"se\">\\</span>\n    --do_eval <span class=\"se\">\\</span>\n    --do_lower_case <span class=\"se\">\\</span>\n    --train_file <span class=\"nv\">$SQUAD_DIR</span>/train-v1.1.json <span class=\"se\">\\</span>\n    --predict_file <span class=\"nv\">$SQUAD_DIR</span>/dev-v1.1.json <span class=\"se\">\\</span>\n    --learning_rate 3e-5 <span class=\"se\">\\</span>\n    --num_train_epochs <span class=\"m\">2</span> <span class=\"se\">\\</span>\n    --max_seq_length <span class=\"m\">384</span> <span class=\"se\">\\</span>\n    --doc_stride <span class=\"m\">128</span> <span class=\"se\">\\</span>\n    --output_dir ../models/wwm_uncased_finetuned_squad/ <span class=\"se\">\\</span>\n    --per_gpu_eval_batch_size<span class=\"o\">=</span><span class=\"m\">3</span>   <span class=\"se\">\\</span>\n    --per_gpu_train_batch_size<span class=\"o\">=</span><span class=\"m\">3</span>   <span class=\"se\">\\</span>\n</pre>\n<p>Training with these hyper-parameters gave us the following results:</p>\n<pre>python <span class=\"nv\">$SQUAD_DIR</span>/evaluate-v1.1.py <span class=\"nv\">$SQUAD_DIR</span>/dev-v1.1.json ../models/wwm_uncased_finetuned_squad/predictions.json\n<span class=\"o\">{</span><span class=\"s2\">\"exact_match\"</span>: <span class=\"m\">86</span>.91579943235573, <span class=\"s2\">\"f1\"</span>: <span class=\"m\">93</span>.1532499015869<span class=\"o\">}</span>\n</pre>\n<p>This is the model provided as <code>bert-large-uncased-whole-word-masking-finetuned-squad</code>.</p>\n<h3><code>run_generation.py</code>: Text generation with GPT, GPT-2, Transformer-XL and XLNet</h3>\n<p>A conditional generation script is also included to generate text from a prompt.\nThe generation script includes the <a href=\"https://github.com/rusiaaman/XLNet-gen#methodology\" rel=\"nofollow\">tricks</a> proposed by by Aman Rusia to get high quality generation with memory models like Transformer-XL and XLNet (include a predefined text to make short inputs longer).</p>\n<p>Here is how to run the script with the small version of OpenAI GPT-2 model:</p>\n<pre>python ./examples/run_generation.py <span class=\"se\">\\</span>\n    --model_type<span class=\"o\">=</span>gpt2 <span class=\"se\">\\</span>\n    --length<span class=\"o\">=</span><span class=\"m\">20</span> <span class=\"se\">\\</span>\n    --model_name_or_path<span class=\"o\">=</span>gpt2 <span class=\"se\">\\</span>\n</pre>\n<h2>Migrating from pytorch-pretrained-bert to pytorch-transformers</h2>\n<p>Here is a quick summary of what you should take care of when migrating from <code>pytorch-pretrained-bert</code> to <code>pytorch-transformers</code></p>\n<h3>Models always output <code>tuples</code></h3>\n<p>The main breaking change when migrating from <code>pytorch-pretrained-bert</code> to <code>pytorch-transformers</code> is that the models forward method always outputs a <code>tuple</code> with various elements depending on the model and the configuration parameters.</p>\n<p>The exact content of the tuples for each model are detailed in the models' docstrings and the <a href=\"https://huggingface.co/pytorch-transformers/\" rel=\"nofollow\">documentation</a>.</p>\n<p>In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in <code>pytorch-pretrained-bert</code>.</p>\n<p>Here is a <code>pytorch-pretrained-bert</code> to <code>pytorch-transformers</code> conversion example for a <code>BertForSequenceClassification</code> classification model:</p>\n<pre><span class=\"c1\"># Let's load our model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">BertForSequenceClassification</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'bert-base-uncased'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># If you used to have this line in pytorch-pretrained-bert:</span>\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">labels</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Now just use this line in pytorch-transformers to extract the loss from the output tuple:</span>\n<span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">labels</span><span class=\"p\">)</span>\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># In pytorch-transformers you can also have access to the logits:</span>\n<span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"p\">[:</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">BertForSequenceClassification</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'bert-base-uncased'</span><span class=\"p\">,</span> <span class=\"n\">output_attentions</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">labels</span><span class=\"p\">)</span>\n<span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">attentions</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span>\n</pre>\n<h3>Serialization</h3>\n<p>Breaking change in the <code>from_pretrained()</code>method:</p>\n<ol>\n<li>\n<p>Models are now set in evaluation mode by default when instantiated with the <code>from_pretrained()</code> method. To train them don't forget to set them back in training mode (<code>model.train()</code>) to activate the dropout modules.</p>\n</li>\n<li>\n<p>The additional <code>*input</code> and <code>**kwargs</code> arguments supplied to the <code>from_pretrained()</code> method used to be directly passed to the underlying model's class <code>__init__()</code> method. They are now used to update the model configuration attribute instead which can break derived model classes build based on the previous <code>BertForSequenceClassification</code> examples. We are working on a way to mitigate this breaking change in <a href=\"https://github.com/huggingface/pytorch-transformers/pull/866\" rel=\"nofollow\">#866</a> by forwarding the the model <code>__init__()</code> method (i) the provided positional arguments and (ii) the keyword arguments which do not match any configuration class attributes.</p>\n</li>\n</ol>\n<p>Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method <code>save_pretrained(save_directory)</code> if you were using any other serialization method before.</p>\n<p>Here is an example:</p>\n<pre><span class=\"c1\">### Let's load a model and tokenizer</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">BertForSequenceClassification</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'bert-base-uncased'</span><span class=\"p\">)</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">BertTokenizer</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'bert-base-uncased'</span><span class=\"p\">)</span>\n\n<span class=\"c1\">### Do some stuff to our model and tokenizer</span>\n<span class=\"c1\"># Ex: add new tokens to the vocabulary and embeddings of our model</span>\n<span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">add_tokens</span><span class=\"p\">([</span><span class=\"s1\">'[SPECIAL_TOKEN_1]'</span><span class=\"p\">,</span> <span class=\"s1\">'[SPECIAL_TOKEN_2]'</span><span class=\"p\">])</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">resize_token_embeddings</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">tokenizer</span><span class=\"p\">))</span>\n<span class=\"c1\"># Train our model</span>\n<span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n\n<span class=\"c1\">### Now let's save our model and tokenizer to a directory</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">save_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'./my_saved_model_directory/'</span><span class=\"p\">)</span>\n<span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">save_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'./my_saved_model_directory/'</span><span class=\"p\">)</span>\n\n<span class=\"c1\">### Reload the model and the tokenizer</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">BertForSequenceClassification</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'./my_saved_model_directory/'</span><span class=\"p\">)</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">BertTokenizer</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'./my_saved_model_directory/'</span><span class=\"p\">)</span>\n</pre>\n<h3>Optimizers: BertAdam &amp; OpenAIAdam are now AdamW, schedules are standard PyTorch schedules</h3>\n<p>The two optimizers previously included, <code>BertAdam</code> and <code>OpenAIAdam</code>, have been replaced by a single <code>AdamW</code> optimizer which has a few differences:</p>\n<ul>\n<li>it only implements weights decay correction,</li>\n<li>schedules are now externals (see below),</li>\n<li>gradient clipping is now also external (see below).</li>\n</ul>\n<p>The new optimizer <code>AdamW</code> matches PyTorch <code>Adam</code> optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping.</p>\n<p>The schedules are now standard <a href=\"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\" rel=\"nofollow\">PyTorch learning rate schedulers</a> and not part of the optimizer anymore.</p>\n<p>Here is a conversion examples from <code>BertAdam</code> with a linear warmup and decay schedule to <code>AdamW</code> and the same schedule:</p>\n<pre><span class=\"c1\"># Parameters:</span>\n<span class=\"n\">lr</span> <span class=\"o\">=</span> <span class=\"mf\">1e-3</span>\n<span class=\"n\">max_grad_norm</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>\n<span class=\"n\">num_total_steps</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span>\n<span class=\"n\">num_warmup_steps</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n<span class=\"n\">warmup_proportion</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">num_warmup_steps</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">num_total_steps</span><span class=\"p\">)</span>  <span class=\"c1\"># 0.1</span>\n\n<span class=\"c1\">### Previously BertAdam optimizer was instantiated like this:</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">BertAdam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">schedule</span><span class=\"o\">=</span><span class=\"s1\">'warmup_linear'</span><span class=\"p\">,</span> <span class=\"n\">warmup</span><span class=\"o\">=</span><span class=\"n\">warmup_proportion</span><span class=\"p\">,</span> <span class=\"n\">t_total</span><span class=\"o\">=</span><span class=\"n\">num_total_steps</span><span class=\"p\">)</span>\n<span class=\"c1\">### and used like this:</span>\n<span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">train_data</span><span class=\"p\">:</span>\n    <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n    <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n\n<span class=\"c1\">### In PyTorch-Transformers, optimizer and schedules are splitted and instantiated like this:</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">AdamW</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">correct_bias</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>  <span class=\"c1\"># To reproduce BertAdam specific behavior set correct_bias=False</span>\n<span class=\"n\">scheduler</span> <span class=\"o\">=</span> <span class=\"n\">WarmupLinearSchedule</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">warmup_steps</span><span class=\"o\">=</span><span class=\"n\">num_warmup_steps</span><span class=\"p\">,</span> <span class=\"n\">t_total</span><span class=\"o\">=</span><span class=\"n\">num_total_steps</span><span class=\"p\">)</span>  <span class=\"c1\"># PyTorch scheduler</span>\n<span class=\"c1\">### and used like this:</span>\n<span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">train_data</span><span class=\"p\">:</span>\n    <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n    <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">clip_grad_norm_</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">max_grad_norm</span><span class=\"p\">)</span>  <span class=\"c1\"># Gradient clipping is not in AdamW anymore (so you can use amp without issue)</span>\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n    <span class=\"n\">scheduler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n</pre>\n<h2>Citation</h2>\n<p>At the moment, there is no paper associated to PyTorch-Transformers but we are working on preparing one. In the meantime, please include a mention of the library and a link to the present repository if you use this work in a published or open-source project.</p>\n\n          </div>"}, "last_serial": 5780726, "releases": {"0.7.0": [{"comment_text": "", "digests": {"md5": "96d666aeae9d7e1d530174c4e159a746", "sha256": "f21184abaf9704b9fb7a1cf361cf3e6d4cfad33f8c1324180314744ed951f922"}, "downloads": -1, "filename": "pytorch_transformers-0.7.0-py3-none-any.whl", "has_sig": false, "md5_digest": "96d666aeae9d7e1d530174c4e159a746", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 150690, "upload_time": "2019-07-05T10:23:15", "upload_time_iso_8601": "2019-07-05T10:23:15.024429Z", "url": "https://files.pythonhosted.org/packages/2d/b4/2592044f6cefe77e6bf9bb418e6b15cbff7385b434c79e499b5e6e72812f/pytorch_transformers-0.7.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c5e3ef8b995fa33d1c14650e27d15311", "sha256": "164dd082f4186e91822d9d44074cceb226b1fe634813a1cb1f02519c5f3376d5"}, "downloads": -1, "filename": "pytorch_transformers-0.7.0.tar.gz", "has_sig": false, "md5_digest": "c5e3ef8b995fa33d1c14650e27d15311", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 168148, "upload_time": "2019-07-05T10:23:18", "upload_time_iso_8601": "2019-07-05T10:23:18.321417Z", "url": "https://files.pythonhosted.org/packages/c1/5f/143e82a823339ffc503d013c47878890aab383a9101582220a335e6bc877/pytorch_transformers-0.7.0.tar.gz", "yanked": false}], "1.0.0": [{"comment_text": "", "digests": {"md5": "8898512f9728bfc3586c791f767da89c", "sha256": "9e36f64a497d3e43aed5effd826d9d9ba0e0b335c279359a56562c8238966c78"}, "downloads": -1, "filename": "pytorch_transformers-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "8898512f9728bfc3586c791f767da89c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 137195, "upload_time": "2019-07-16T14:13:32", "upload_time_iso_8601": "2019-07-16T14:13:32.313366Z", "url": "https://files.pythonhosted.org/packages/40/b5/2d78e74001af0152ee61d5ad4e290aec9a1e43925b21df2dc74ec100f1ab/pytorch_transformers-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d9d7886f07ecfba433873168fa044964", "sha256": "5ee90ec47b1862f9c94304abe60359d06ab641e7235d9346d142559740ee7854"}, "downloads": -1, "filename": "pytorch_transformers-1.0.0.tar.gz", "has_sig": false, "md5_digest": "d9d7886f07ecfba433873168fa044964", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 118954, "upload_time": "2019-07-16T14:13:38", "upload_time_iso_8601": "2019-07-16T14:13:38.110015Z", "url": "https://files.pythonhosted.org/packages/aa/89/5f24ad07fe768fb3c6e2f210541d578e87a78f2587b18f7a37868b8bbedb/pytorch_transformers-1.0.0.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "6f888189cbe38f77c987bc8ba8714738", "sha256": "cd670284de1b1b0bfe2f4a27531e873b5a76cd163960661971a340f33c3f4290"}, "downloads": -1, "filename": "pytorch_transformers-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "6f888189cbe38f77c987bc8ba8714738", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 158066, "upload_time": "2019-08-15T21:14:28", "upload_time_iso_8601": "2019-08-15T21:14:28.126686Z", "url": "https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8727e7ba90e3be94807a607f218c141e", "sha256": "f1eadbd394564c0dbe0e12e3ebb5b1470169af275c2a71e20c2480ee5a93e94c"}, "downloads": -1, "filename": "pytorch_transformers-1.1.0.tar.gz", "has_sig": false, "md5_digest": "8727e7ba90e3be94807a607f218c141e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 133084, "upload_time": "2019-08-15T21:14:31", "upload_time_iso_8601": "2019-08-15T21:14:31.741408Z", "url": "https://files.pythonhosted.org/packages/72/a7/68f2d8fd6d927e374181af34f827a9f4a469487d99d0c017177c5db8726e/pytorch_transformers-1.1.0.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "885d028952907e3b9425567892e89d6f", "sha256": "15f12a04424c0f6d3a7c7b57d6c79628dc9c117a204fb7db8c1ea330c77a6898"}, "downloads": -1, "filename": "pytorch_transformers-1.2.0-py2-none-any.whl", "has_sig": false, "md5_digest": "885d028952907e3b9425567892e89d6f", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 176428, "upload_time": "2019-09-04T11:36:34", "upload_time_iso_8601": "2019-09-04T11:36:34.558075Z", "url": "https://files.pythonhosted.org/packages/a9/be/587dd871b8abcdb34bbfbfac976b403bb22bc5c41d3d18cff329bf669fd3/pytorch_transformers-1.2.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2dba40cda929caa2d2c8c843bf327336", "sha256": "bdb606fe1f2d27586710ed03cfa49dbbd80215c38bf965862daada0c137fd7ce"}, "downloads": -1, "filename": "pytorch_transformers-1.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "2dba40cda929caa2d2c8c843bf327336", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 176430, "upload_time": "2019-09-04T11:36:36", "upload_time_iso_8601": "2019-09-04T11:36:36.704743Z", "url": "https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "48e994040a609c050cf559764e430cb6", "sha256": "293e4a864ae9d9401f9fba13f16b8696e4a1cb38bcd0b56562d03af5489daeb9"}, "downloads": -1, "filename": "pytorch_transformers-1.2.0.tar.gz", "has_sig": false, "md5_digest": "48e994040a609c050cf559764e430cb6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 152283, "upload_time": "2019-09-04T11:36:39", "upload_time_iso_8601": "2019-09-04T11:36:39.462509Z", "url": "https://files.pythonhosted.org/packages/39/46/60ade12cd10f3e3dc7cb109361a73ebd8a8530c35bed71f681d2588aa277/pytorch_transformers-1.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "885d028952907e3b9425567892e89d6f", "sha256": "15f12a04424c0f6d3a7c7b57d6c79628dc9c117a204fb7db8c1ea330c77a6898"}, "downloads": -1, "filename": "pytorch_transformers-1.2.0-py2-none-any.whl", "has_sig": false, "md5_digest": "885d028952907e3b9425567892e89d6f", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 176428, "upload_time": "2019-09-04T11:36:34", "upload_time_iso_8601": "2019-09-04T11:36:34.558075Z", "url": "https://files.pythonhosted.org/packages/a9/be/587dd871b8abcdb34bbfbfac976b403bb22bc5c41d3d18cff329bf669fd3/pytorch_transformers-1.2.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2dba40cda929caa2d2c8c843bf327336", "sha256": "bdb606fe1f2d27586710ed03cfa49dbbd80215c38bf965862daada0c137fd7ce"}, "downloads": -1, "filename": "pytorch_transformers-1.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "2dba40cda929caa2d2c8c843bf327336", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 176430, "upload_time": "2019-09-04T11:36:36", "upload_time_iso_8601": "2019-09-04T11:36:36.704743Z", "url": "https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "48e994040a609c050cf559764e430cb6", "sha256": "293e4a864ae9d9401f9fba13f16b8696e4a1cb38bcd0b56562d03af5489daeb9"}, "downloads": -1, "filename": "pytorch_transformers-1.2.0.tar.gz", "has_sig": false, "md5_digest": "48e994040a609c050cf559764e430cb6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 152283, "upload_time": "2019-09-04T11:36:39", "upload_time_iso_8601": "2019-09-04T11:36:39.462509Z", "url": "https://files.pythonhosted.org/packages/39/46/60ade12cd10f3e3dc7cb109361a73ebd8a8530c35bed71f681d2588aa277/pytorch_transformers-1.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:13:42 2020"}