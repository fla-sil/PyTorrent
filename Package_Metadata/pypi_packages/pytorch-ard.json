{"info": {"author": "Artem Ryzhikov", "author_email": "", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "Programming Language :: Python :: 3"], "description": "# Variational Dropout Sparsifies NN (Pytorch)\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](LICENSE)\n[![PyPI version](https://badge.fury.io/py/pytorch-ard.svg)](https://badge.fury.io/py/pytorch-ard)\n\n\nMake your neural network 300 times faster!\n\nPytorch implementation of Variational Dropout Sparsifies Deep Neural Networks ([arxiv:1701.05369](https://arxiv.org/abs/1701.05369)).\n\n## Description\nThe discovered approach helps to train both convolutional and dense deep sparsified models without significant loss of quality. Additive Noise Reparameterization\nand the Local Reparameterization Trick discovered in the paper helps to eliminate weights prior's restrictions (<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\alpha\\leq&space;1\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\alpha\\leq&space;1\" title=\"\\alpha\\leq 1\" /></a>) and achieve Automatic Relevance Determination (ARD) effect on (typically most) network's parameters. According to the original paper, authors reduced the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy. Experiments with Boston dataset in this repository proves that: 99% of simple dense model were dropped using paper's ARD-prior without any significant loss of MSE. Moreover, this technique helps to significantly reduce overfitting and helps to not worry about model's complexity - all redundant parameters will be dropped automatically. Moreover, you can achieve any degree of regularization variating regularization factor tradeoff (see ***reg_factor*** variable in [boston_ard.py](examples/boston/boston_ard.py) and [cifar_ard.py](examples/cifar/cifar_ard.py) scripts)\n\n## Usage\n\n```python\nimport torch_ard as nn_ard\nfrom torch import nn\nimport torch.nn.functional as F\n\ninput_size, hidden_size, output_size = 60, 150, 1\n\nmodel = nn.Sequential(\n    nn_ard.LinearARD(input_size, hidden_size),\n    nn.ReLU(),\n    nn_ard.LinearARD(hidden_size, output_size)\n)\n\nreg_factor = 1.0\ncriterion = lambda input, target: F.binary_cross_entropy(input, target) + reg_factor*nn_ard.get_ard_reg(model)\nprint('Sparsification ratio: %.3f%%' % (100.*nn_ard.get_dropped_params_ratio(model)))\n```\n\n## Installation\n\n```\npip install pytorch-ard\n```\n\n## Experiments\n\nAll experiments are placed at [examples](examples/) folder and contains baseline and implemented models comparison.\n\n### Boston dataset\n\nTwo scripts were used in the experiment: [boston_baseline.py](examples/boston/boston_baseline.py) and [boston_ard.py](examples/boston/boston_ard.py). Training procedure for each experiment was **100000 epoches, Adam(lr=1e-3)**. Baseline model was dense neural network with single hidden layer with hidden size 150.\n\n|                | Baseline (nn.Linear) | LinearARD, no reg | LinearARD, reg=0.0001 | LinearARD, reg=0.001 | LinearARD, reg=0.1 | LinearARD, reg=1 |\n|----------------|----------|-------------|-----------------|----------------|--------------|------------|\n| MSE (train)    | 1.751    | 1.626       | <span style=\"color:green\"><b>1.587</b></span>           | 1.962          | 17.167       | 33.682     |\n| MSE (test)     | <span style=\"color:red\"><b>22.580</b></span>   | 16.229      | <span style=\"color:green\"><b>15.957</b></span>          | <span style=\"color:green\"><b>8.416</b></span>          | 25.695       | 30.231     |\n| Compression, % | <span style=\"color:red\"><b>0</b></span>        | 0.38        | <span style=\"color:green\"><b>52.95</b></span>           | <span style=\"color:green\"><b>64.19</b></span>          | <span style=\"color:green\"><b>97.29</b></span>        | <span style=\"color:green\"><b>99.29</b></span>      |\n\nYou can see on the table above that variating regularization factor any degree of compression can be achieved (for example, ~99.29% of connections can be dropped if reg_factor=1 will be used). Moreover, you can see that training with LinearARD layers with some regularization parameters (like reg=0.001 in the table above) not only significantly reduces number of model parameters (>64% of parameters can be dropped after training), but also significantly increases quality on test, reducing overfitting.\n\n## Tips\n\n1. Despite the high performance of implemented layers in \"end-to-end\" mode, authors recommends to use in fine-tuning pretrained models without ARD prior. In this case the best performance could be achieved. Moreover, it will be faster - despite of comparable convergence speed of this layers optimization, each training epoch takes more time (approx. twice longer - ~2 times more parameters in \\*ARD implementations).  This fact well describable - using ARD prior in earlier stages can drop useful connections with unobvious dependencies.\n2. Model's sparsification takes almost no any speed-up effects until You convert it to the sparse one! (*TODO*)\n\n\n## Requirements\n* **PyTorch** >= 0.4.0\n* **SkLearn** >= 0.19.1\n* **Pandas** >= 0.23.3\n* **Numpy** >= 1.14.5\n\n## TODO\n- [X] LinearARD layer implementation\n- [X] Conv2dARD layer implementation\n- [ ] Learnable bias for Conv2dARD\n- [ ] Implement *to_sparse(model)* utility\n\n## Authors\n\n```\n@article{molchanov2017variational,\n  title={Variational Dropout Sparsifies Deep Neural Networks},\n  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},\n  journal={arXiv preprint arXiv:1701.05369},\n  year={2017}\n}\n```\n[Original implementation](https://github.com/ars-ashuha/variational-dropout-sparsifies-dnn) (Theano/Lasagne)\n\n## Citation\n\n```\n@misc{pytorch_ard,\n  author = {Artem Ryzhikov},\n  title = {HolyBayes/pytorch_ard},\n  url = {https://github.com/HolyBayes/pytorch_ard},\n  year = {2018}\n}\n```\n\n## Contacts\n\nArtem Ryzhikov, LAMBDA laboratory, Higher School of Economics, Yandex School of Data Analysis\n\n**E-mail:** artemryzhikoff@yandex.ru\n\n**Linkedin:** https://www.linkedin.com/in/artem-ryzhikov-2b6308103/\n\n**Link:** https://www.hse.ru/org/persons/190912317", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/HolyBayes/pytorch_ard", "keywords": "pytorch,bayesian neural networks,ard,deep learning,neural networks,machine learning", "license": "", "maintainer": "", "maintainer_email": "", "name": "pytorch-ard", "package_url": "https://pypi.org/project/pytorch-ard/", "platform": "", "project_url": "https://pypi.org/project/pytorch-ard/", "project_urls": {"Homepage": "https://github.com/HolyBayes/pytorch_ard"}, "release_url": "https://pypi.org/project/pytorch-ard/0.2.0/", "requires_dist": null, "requires_python": "", "summary": "Make your PyTorch faster", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Variational Dropout Sparsifies NN (Pytorch)</h1>\n<p><a href=\"LICENSE\" rel=\"nofollow\"><img alt=\"license\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ae646c9f8bcc796bfc74debf7ac2c93ee374c6ef/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6173686170652f6170697374617475732e7376673f6d61784167653d32353932303030\"></a>\n<a href=\"https://badge.fury.io/py/pytorch-ard\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a363121c99aca52171c5b8af0224d8681d6010e5/68747470733a2f2f62616467652e667572792e696f2f70792f7079746f7263682d6172642e737667\"></a></p>\n<p>Make your neural network 300 times faster!</p>\n<p>Pytorch implementation of Variational Dropout Sparsifies Deep Neural Networks (<a href=\"https://arxiv.org/abs/1701.05369\" rel=\"nofollow\">arxiv:1701.05369</a>).</p>\n<h2>Description</h2>\n<p>The discovered approach helps to train both convolutional and dense deep sparsified models without significant loss of quality. Additive Noise Reparameterization\nand the Local Reparameterization Trick discovered in the paper helps to eliminate weights prior's restrictions (<a href=\"https://www.codecogs.com/eqnedit.php?latex=%5Calpha%5Cleq&amp;space;1\" rel=\"nofollow\"><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0ed3ed52f1aa52705724122fadcbaab3db46a05c/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c616c7068615c6c65712673706163653b31\"></a>) and achieve Automatic Relevance Determination (ARD) effect on (typically most) network's parameters. According to the original paper, authors reduced the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy. Experiments with Boston dataset in this repository proves that: 99% of simple dense model were dropped using paper's ARD-prior without any significant loss of MSE. Moreover, this technique helps to significantly reduce overfitting and helps to not worry about model's complexity - all redundant parameters will be dropped automatically. Moreover, you can achieve any degree of regularization variating regularization factor tradeoff (see <em><strong>reg_factor</strong></em> variable in <a href=\"examples/boston/boston_ard.py\" rel=\"nofollow\">boston_ard.py</a> and <a href=\"examples/cifar/cifar_ard.py\" rel=\"nofollow\">cifar_ard.py</a> scripts)</p>\n<h2>Usage</h2>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_ard</span> <span class=\"k\">as</span> <span class=\"nn\">nn_ard</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch</span> <span class=\"kn\">import</span> <span class=\"n\">nn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span> <span class=\"k\">as</span> <span class=\"nn\">F</span>\n\n<span class=\"n\">input_size</span><span class=\"p\">,</span> <span class=\"n\">hidden_size</span><span class=\"p\">,</span> <span class=\"n\">output_size</span> <span class=\"o\">=</span> <span class=\"mi\">60</span><span class=\"p\">,</span> <span class=\"mi\">150</span><span class=\"p\">,</span> <span class=\"mi\">1</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n    <span class=\"n\">nn_ard</span><span class=\"o\">.</span><span class=\"n\">LinearARD</span><span class=\"p\">(</span><span class=\"n\">input_size</span><span class=\"p\">,</span> <span class=\"n\">hidden_size</span><span class=\"p\">),</span>\n    <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n    <span class=\"n\">nn_ard</span><span class=\"o\">.</span><span class=\"n\">LinearARD</span><span class=\"p\">(</span><span class=\"n\">hidden_size</span><span class=\"p\">,</span> <span class=\"n\">output_size</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">reg_factor</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>\n<span class=\"n\">criterion</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"nb\">input</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">binary_cross_entropy</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">reg_factor</span><span class=\"o\">*</span><span class=\"n\">nn_ard</span><span class=\"o\">.</span><span class=\"n\">get_ard_reg</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'Sparsification ratio: </span><span class=\"si\">%.3f%%</span><span class=\"s1\">'</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"mf\">100.</span><span class=\"o\">*</span><span class=\"n\">nn_ard</span><span class=\"o\">.</span><span class=\"n\">get_dropped_params_ratio</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)))</span>\n</pre>\n<h2>Installation</h2>\n<pre><code>pip install pytorch-ard\n</code></pre>\n<h2>Experiments</h2>\n<p>All experiments are placed at <a href=\"examples/\" rel=\"nofollow\">examples</a> folder and contains baseline and implemented models comparison.</p>\n<h3>Boston dataset</h3>\n<p>Two scripts were used in the experiment: <a href=\"examples/boston/boston_baseline.py\" rel=\"nofollow\">boston_baseline.py</a> and <a href=\"examples/boston/boston_ard.py\" rel=\"nofollow\">boston_ard.py</a>. Training procedure for each experiment was <strong>100000 epoches, Adam(lr=1e-3)</strong>. Baseline model was dense neural network with single hidden layer with hidden size 150.</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Baseline (nn.Linear)</th>\n<th>LinearARD, no reg</th>\n<th>LinearARD, reg=0.0001</th>\n<th>LinearARD, reg=0.001</th>\n<th>LinearARD, reg=0.1</th>\n<th>LinearARD, reg=1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MSE (train)</td>\n<td>1.751</td>\n<td>1.626</td>\n<td><span><b>1.587</b></span></td>\n<td>1.962</td>\n<td>17.167</td>\n<td>33.682</td>\n</tr>\n<tr>\n<td>MSE (test)</td>\n<td><span><b>22.580</b></span></td>\n<td>16.229</td>\n<td><span><b>15.957</b></span></td>\n<td><span><b>8.416</b></span></td>\n<td>25.695</td>\n<td>30.231</td>\n</tr>\n<tr>\n<td>Compression, %</td>\n<td><span><b>0</b></span></td>\n<td>0.38</td>\n<td><span><b>52.95</b></span></td>\n<td><span><b>64.19</b></span></td>\n<td><span><b>97.29</b></span></td>\n<td><span><b>99.29</b></span></td>\n</tr></tbody></table>\n<p>You can see on the table above that variating regularization factor any degree of compression can be achieved (for example, ~99.29% of connections can be dropped if reg_factor=1 will be used). Moreover, you can see that training with LinearARD layers with some regularization parameters (like reg=0.001 in the table above) not only significantly reduces number of model parameters (&gt;64% of parameters can be dropped after training), but also significantly increases quality on test, reducing overfitting.</p>\n<h2>Tips</h2>\n<ol>\n<li>Despite the high performance of implemented layers in \"end-to-end\" mode, authors recommends to use in fine-tuning pretrained models without ARD prior. In this case the best performance could be achieved. Moreover, it will be faster - despite of comparable convergence speed of this layers optimization, each training epoch takes more time (approx. twice longer - ~2 times more parameters in *ARD implementations).  This fact well describable - using ARD prior in earlier stages can drop useful connections with unobvious dependencies.</li>\n<li>Model's sparsification takes almost no any speed-up effects until You convert it to the sparse one! (<em>TODO</em>)</li>\n</ol>\n<h2>Requirements</h2>\n<ul>\n<li><strong>PyTorch</strong> &gt;= 0.4.0</li>\n<li><strong>SkLearn</strong> &gt;= 0.19.1</li>\n<li><strong>Pandas</strong> &gt;= 0.23.3</li>\n<li><strong>Numpy</strong> &gt;= 1.14.5</li>\n</ul>\n<h2>TODO</h2>\n<ul>\n<li>[X] LinearARD layer implementation</li>\n<li>[X] Conv2dARD layer implementation</li>\n<li>[ ] Learnable bias for Conv2dARD</li>\n<li>[ ] Implement <em>to_sparse(model)</em> utility</li>\n</ul>\n<h2>Authors</h2>\n<pre><code>@article{molchanov2017variational,\n  title={Variational Dropout Sparsifies Deep Neural Networks},\n  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},\n  journal={arXiv preprint arXiv:1701.05369},\n  year={2017}\n}\n</code></pre>\n<p><a href=\"https://github.com/ars-ashuha/variational-dropout-sparsifies-dnn\" rel=\"nofollow\">Original implementation</a> (Theano/Lasagne)</p>\n<h2>Citation</h2>\n<pre><code>@misc{pytorch_ard,\n  author = {Artem Ryzhikov},\n  title = {HolyBayes/pytorch_ard},\n  url = {https://github.com/HolyBayes/pytorch_ard},\n  year = {2018}\n}\n</code></pre>\n<h2>Contacts</h2>\n<p>Artem Ryzhikov, LAMBDA laboratory, Higher School of Economics, Yandex School of Data Analysis</p>\n<p><strong>E-mail:</strong> <a href=\"mailto:artemryzhikoff@yandex.ru\">artemryzhikoff@yandex.ru</a></p>\n<p><strong>Linkedin:</strong> <a href=\"https://www.linkedin.com/in/artem-ryzhikov-2b6308103/\" rel=\"nofollow\">https://www.linkedin.com/in/artem-ryzhikov-2b6308103/</a></p>\n<p><strong>Link:</strong> <a href=\"https://www.hse.ru/org/persons/190912317\" rel=\"nofollow\">https://www.hse.ru/org/persons/190912317</a></p>\n\n          </div>"}, "last_serial": 5155735, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "588f045a3b9af8f105a2b16baa710cc4", "sha256": "b2020bf62c3c901dfecfa0150a09a59472ffbfd70a8a6b33baf94a4b7b08bd31"}, "downloads": -1, "filename": "pytorch_ard-0.1.0.tar.gz", "has_sig": false, "md5_digest": "588f045a3b9af8f105a2b16baa710cc4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5840, "upload_time": "2019-02-19T08:39:01", "upload_time_iso_8601": "2019-02-19T08:39:01.059139Z", "url": "https://files.pythonhosted.org/packages/3d/f7/1408ee4bd5ada3e53a042797bc0314912d9d1b063ec9486db8a33b0f687d/pytorch_ard-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "64538bf633e21d37dd55ffec26367bea", "sha256": "8ddf9ac488fe7d7ceb65aad3e3e0abb32f5a0eedcbbe31ac6ba103b40a5a70e0"}, "downloads": -1, "filename": "pytorch_ard-0.1.1.tar.gz", "has_sig": false, "md5_digest": "64538bf633e21d37dd55ffec26367bea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5903, "upload_time": "2019-02-19T08:47:56", "upload_time_iso_8601": "2019-02-19T08:47:56.997659Z", "url": "https://files.pythonhosted.org/packages/67/ed/0378d584e024cf570c0a58de6c9cda055a66dd4c6c0910651cff20180a42/pytorch_ard-0.1.1.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "43fe14fc3116d486c909a2385b258f2c", "sha256": "534484c71a89c7df6658363ebf3569b3fe6c93f0cfe2002542031a8b8bc0afbc"}, "downloads": -1, "filename": "pytorch_ard-0.2.0.tar.gz", "has_sig": false, "md5_digest": "43fe14fc3116d486c909a2385b258f2c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5942, "upload_time": "2019-04-17T15:24:13", "upload_time_iso_8601": "2019-04-17T15:24:13.026041Z", "url": "https://files.pythonhosted.org/packages/e8/17/0d8ff81a28beae2a573d0ea4dc92fb73125adc70fd864c86ef73567f44e3/pytorch_ard-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "43fe14fc3116d486c909a2385b258f2c", "sha256": "534484c71a89c7df6658363ebf3569b3fe6c93f0cfe2002542031a8b8bc0afbc"}, "downloads": -1, "filename": "pytorch_ard-0.2.0.tar.gz", "has_sig": false, "md5_digest": "43fe14fc3116d486c909a2385b258f2c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5942, "upload_time": "2019-04-17T15:24:13", "upload_time_iso_8601": "2019-04-17T15:24:13.026041Z", "url": "https://files.pythonhosted.org/packages/e8/17/0d8ff81a28beae2a573d0ea4dc92fb73125adc70fd864c86ef73567f44e3/pytorch_ard-0.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:14:01 2020"}