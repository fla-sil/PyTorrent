{"info": {"author": "Richard Keene", "author_email": "rmkeene@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# TWIT - Tensor Weighted Interpolative Transfer\n\n(Release 1.0.  Updated the README.md)\n\n(Update 12/2/2019 Multithread C code update.)\n(This is changeing quite a bit due to the port to C so python can call multithreaded C functions for TWIT.  Stay tuned....  10/19/2019 RK)\n(Updated 11/12/2019 - C code works well. 200x faster than pure python code.)\n\nRichard Keene December 2, 2019\npython PyPI package https://pypi.org/project/twit-rkeene/\nPython 3 only.\n\n# Concept of TWIT\nThe purpose of TWIT is to allow transfer of values from a source tensor to a destination tensor\nwith correct interpolation of values.  The source and destination do not have to have the same number of dimensions, \ne.g. len(src.shape) != len(dst.shape).\nAlso the range of indices do not have to match in count.  For example given two one dimensional tensors (vectors of values)\none could say copy from source range (2,7) to destination range (0,2) and use source to destination multipliers of (0.5, 0.9).  This will\ncopy source values at indices 2,3,4,5,6,7 to destination indices 0,1,2 (and 'scale' the data) and multiply \nsource[2] by 0.5 to go to destination[0] and linear interpolate the multiplier (weight) up to 0.9 for subsequent indices.\n\n### Energy Conservation\nIn the above example of scaling down the naive approach would just sum in the source values multiplied by the interpolate\nweights would result in a destination that is 6/3 or 2x 'brighter' than the source.\nWhat TWIT does is maintain constant energy or brightness while scaling values down by the source to destination ratio if the destination is shorter. If the destination is equal or longer then values are simply interpolated.  This maintains 'brightness' for both\nup and down scale.\n\nAn intuitive example is a 3D tensor which happens to be a color image, going to a destination tensor \nthat is a 2D grey scale image.  We want to maintain brightness and the dimensions do not match.\n\nThis image scale operation might be a source image that is 300x400x3 to a destination that is 400x600.  Nothing matches.\nLets take the example of input to TWIT of \n*Note: One can use python square brackets or parenthisis for clarity, twit does not care.*\n\n# Typical twit call.\nmake_and_apply_twit(1, np.array([0, 4, 0, 3], dtype=np.int64),  np.array([1.0, 1.0], dtype=np.float64), t1, t2, 1)\n\nThis takes a tensor t1 that is 1D and uses 0 to 4 inclusive as the source, and t2 of 1D indicies 0 to 3 inclusive,\nand copies the values from t1 to t2. The interpolation along the 1D dimension is 1.0 to 1.0, so no interpolation.\nThe final parameter 1 indicates preclear the destination to 0.0 before doing the transfer.\n\nA more complex call might be\nmake_and_apply_twit(2, np.array([0, 4, 0, 3, 0, 2, 0, 4], dtype=np.int64),  np.array([1.0, 0.0, 1.0, 1.0], dtype=np.float64), t1, t2, 1)\n\nThe first array is o to 4 in the source outer dimension, to 0 to 3 in the destination outer dimension, and \n0 to 2 in the source second innermost dimension, and 0 to 4 in the destination second dimension. Interpolation of the values ranges linearly \nfrom 0 to 1 along the first dimension, and 1 to 1 on the second dimension.\n\nIf the source and destination tensors are of different number of dimensions the lesser dimension tensor gets 1's filled in for the missing dimensions as a prepend to the shape of the smaller tensor.\n\n# Reuse of twit results\nIf you are going to use the twit result repeatedly you should make the iterator, generate all the axis weights and indicies\nonce and then reuse the cached data. This is done with compute_twit_multi_dimension.  Then apply_twit can be called repeatedly.\n\n# Why TWIT?\nThe motivation for TWIT is to support the development of the SRS cognition algorithm and Cognate.  The system has\nlots of concept maps that are N dimensional tensors represent some concept space.  They are neurons and they are\nconnections between the concept maps.  Every system tick we do a twit transfer (using cached twit iterator values)\nto transform data.  The fact this library can als be used for image scaling in tensors is a side effect.\n\n# Python and C code\nAll of twit is written in python, and then the core is written again in C and multi-threaded.\ntwitc is the C interface to the DLL.\nIf you do not call twitc.set_thread_count then it will be single threaded C code.\nYou can call set_thread_count(N) where N is a power of 2, one of 2,4,8,16,32,64,128,256.\nOne can only call set_thread_count once.\n\n# Speed issues.\ntwit in pure python will scale the test image shape (200,300,3) at about 3 seconds per apply_twit.\nThe single thread  code is about 1.8 milliseconds per apply_twit, and on an 8 core machine \nthe multithread apply_twit is 0.5 milliseconds per apply_twit.\n\n# Range strings - **Not reworked yet to new style of twit parameters!!!!!**\nThere is also a range definition string format used for Neural net concept map editing and viewing.\nInput s is like \"{5,17} <2,3> [-0.1, 0.9]\" (see doc string for the module)\nFormat is ((srcstart, srcend), (dststart, dstend), (weightstart, weightend)).\nsrc, dst, etc are to determine the ranges. The ..._shape_idx are which dimension of the array to use\nfor defaults.\nBoth indices and weights can be revered to indicate a reversed interpolation along that axis.\nIndexes are inclusive \"start value to end value\", so {5, 9} means 5,6,7,8,9. \nThis is in support of editors for people to easily enter ranges and weights for neural nets.\n(See Cognate and NuTank)\n\n# Helper Functions\nTWIT includes some static functions to help do things.\n\nmake_and_apply_twit combines both making the twit cached data, and then applying it.\n\napply_twit(twt, t1, t2, preclear) will iterate the twit twt and do the copy and multiplies from t1 to t2.  If t1 and t2 are not the\nsame number of dimensions it will create the appropriate view and then do the work.  There is a clear destination flag to \nzero out the destination before iteration.  t2 MUST be a array style tensor since it gets written to in-place. You can pass\nin an optional twit cache.\n\nA little helper function if you need it, match_tensor_shape_lengths, will make the views and return them given a t1 and t2.\n\n# Tests\nThere is a twit_test.py file with all the unit tests, and twitc_test.py.\n\n# Examples\nThere is a scale_image_sample.py test file for an example.  It is not very generic. (Not yet done.)\n\n# Source GitHub\nAt https://github.com/RMKeene/twit is the project.  Any improvements or bug fixes will be appreciated. Also available from PyPI.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/RMKeene/twit", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "twit", "package_url": "https://pypi.org/project/twit/", "platform": "", "project_url": "https://pypi.org/project/twit/", "project_urls": {"Homepage": "https://github.com/RMKeene/twit"}, "release_url": "https://pypi.org/project/twit/0.1.1/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Tensor Weighted Interpolative Transfer", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>TWIT - Tensor Weighted Interpolative Transfer</h1>\n<p>(Release 1.0.  Updated the README.md)</p>\n<p>(Update 12/2/2019 Multithread C code update.)\n(This is changeing quite a bit due to the port to C so python can call multithreaded C functions for TWIT.  Stay tuned....  10/19/2019 RK)\n(Updated 11/12/2019 - C code works well. 200x faster than pure python code.)</p>\n<p>Richard Keene December 2, 2019\npython PyPI package <a href=\"https://pypi.org/project/twit-rkeene/\" rel=\"nofollow\">https://pypi.org/project/twit-rkeene/</a>\nPython 3 only.</p>\n<h1>Concept of TWIT</h1>\n<p>The purpose of TWIT is to allow transfer of values from a source tensor to a destination tensor\nwith correct interpolation of values.  The source and destination do not have to have the same number of dimensions,\ne.g. len(src.shape) != len(dst.shape).\nAlso the range of indices do not have to match in count.  For example given two one dimensional tensors (vectors of values)\none could say copy from source range (2,7) to destination range (0,2) and use source to destination multipliers of (0.5, 0.9).  This will\ncopy source values at indices 2,3,4,5,6,7 to destination indices 0,1,2 (and 'scale' the data) and multiply\nsource[2] by 0.5 to go to destination[0] and linear interpolate the multiplier (weight) up to 0.9 for subsequent indices.</p>\n<h3>Energy Conservation</h3>\n<p>In the above example of scaling down the naive approach would just sum in the source values multiplied by the interpolate\nweights would result in a destination that is 6/3 or 2x 'brighter' than the source.\nWhat TWIT does is maintain constant energy or brightness while scaling values down by the source to destination ratio if the destination is shorter. If the destination is equal or longer then values are simply interpolated.  This maintains 'brightness' for both\nup and down scale.</p>\n<p>An intuitive example is a 3D tensor which happens to be a color image, going to a destination tensor\nthat is a 2D grey scale image.  We want to maintain brightness and the dimensions do not match.</p>\n<p>This image scale operation might be a source image that is 300x400x3 to a destination that is 400x600.  Nothing matches.\nLets take the example of input to TWIT of\n<em>Note: One can use python square brackets or parenthisis for clarity, twit does not care.</em></p>\n<h1>Typical twit call.</h1>\n<p>make_and_apply_twit(1, np.array([0, 4, 0, 3], dtype=np.int64),  np.array([1.0, 1.0], dtype=np.float64), t1, t2, 1)</p>\n<p>This takes a tensor t1 that is 1D and uses 0 to 4 inclusive as the source, and t2 of 1D indicies 0 to 3 inclusive,\nand copies the values from t1 to t2. The interpolation along the 1D dimension is 1.0 to 1.0, so no interpolation.\nThe final parameter 1 indicates preclear the destination to 0.0 before doing the transfer.</p>\n<p>A more complex call might be\nmake_and_apply_twit(2, np.array([0, 4, 0, 3, 0, 2, 0, 4], dtype=np.int64),  np.array([1.0, 0.0, 1.0, 1.0], dtype=np.float64), t1, t2, 1)</p>\n<p>The first array is o to 4 in the source outer dimension, to 0 to 3 in the destination outer dimension, and\n0 to 2 in the source second innermost dimension, and 0 to 4 in the destination second dimension. Interpolation of the values ranges linearly\nfrom 0 to 1 along the first dimension, and 1 to 1 on the second dimension.</p>\n<p>If the source and destination tensors are of different number of dimensions the lesser dimension tensor gets 1's filled in for the missing dimensions as a prepend to the shape of the smaller tensor.</p>\n<h1>Reuse of twit results</h1>\n<p>If you are going to use the twit result repeatedly you should make the iterator, generate all the axis weights and indicies\nonce and then reuse the cached data. This is done with compute_twit_multi_dimension.  Then apply_twit can be called repeatedly.</p>\n<h1>Why TWIT?</h1>\n<p>The motivation for TWIT is to support the development of the SRS cognition algorithm and Cognate.  The system has\nlots of concept maps that are N dimensional tensors represent some concept space.  They are neurons and they are\nconnections between the concept maps.  Every system tick we do a twit transfer (using cached twit iterator values)\nto transform data.  The fact this library can als be used for image scaling in tensors is a side effect.</p>\n<h1>Python and C code</h1>\n<p>All of twit is written in python, and then the core is written again in C and multi-threaded.\ntwitc is the C interface to the DLL.\nIf you do not call twitc.set_thread_count then it will be single threaded C code.\nYou can call set_thread_count(N) where N is a power of 2, one of 2,4,8,16,32,64,128,256.\nOne can only call set_thread_count once.</p>\n<h1>Speed issues.</h1>\n<p>twit in pure python will scale the test image shape (200,300,3) at about 3 seconds per apply_twit.\nThe single thread  code is about 1.8 milliseconds per apply_twit, and on an 8 core machine\nthe multithread apply_twit is 0.5 milliseconds per apply_twit.</p>\n<h1>Range strings - <strong>Not reworked yet to new style of twit parameters!!!!!</strong></h1>\n<p>There is also a range definition string format used for Neural net concept map editing and viewing.\nInput s is like \"{5,17} &lt;2,3&gt; [-0.1, 0.9]\" (see doc string for the module)\nFormat is ((srcstart, srcend), (dststart, dstend), (weightstart, weightend)).\nsrc, dst, etc are to determine the ranges. The ..._shape_idx are which dimension of the array to use\nfor defaults.\nBoth indices and weights can be revered to indicate a reversed interpolation along that axis.\nIndexes are inclusive \"start value to end value\", so {5, 9} means 5,6,7,8,9.\nThis is in support of editors for people to easily enter ranges and weights for neural nets.\n(See Cognate and NuTank)</p>\n<h1>Helper Functions</h1>\n<p>TWIT includes some static functions to help do things.</p>\n<p>make_and_apply_twit combines both making the twit cached data, and then applying it.</p>\n<p>apply_twit(twt, t1, t2, preclear) will iterate the twit twt and do the copy and multiplies from t1 to t2.  If t1 and t2 are not the\nsame number of dimensions it will create the appropriate view and then do the work.  There is a clear destination flag to\nzero out the destination before iteration.  t2 MUST be a array style tensor since it gets written to in-place. You can pass\nin an optional twit cache.</p>\n<p>A little helper function if you need it, match_tensor_shape_lengths, will make the views and return them given a t1 and t2.</p>\n<h1>Tests</h1>\n<p>There is a twit_test.py file with all the unit tests, and twitc_test.py.</p>\n<h1>Examples</h1>\n<p>There is a scale_image_sample.py test file for an example.  It is not very generic. (Not yet done.)</p>\n<h1>Source GitHub</h1>\n<p>At <a href=\"https://github.com/RMKeene/twit\" rel=\"nofollow\">https://github.com/RMKeene/twit</a> is the project.  Any improvements or bug fixes will be appreciated. Also available from PyPI.</p>\n\n          </div>"}, "last_serial": 6304915, "releases": {"0.0.11": [{"comment_text": "", "digests": {"md5": "54d7e3290ab3cee08629eb42b474d763", "sha256": "6d78261f24748202c45525cf81666e884b45688f1ed21bfd4d0e0474ca7fca1b"}, "downloads": -1, "filename": "twit-0.0.11-py3-none-any.whl", "has_sig": false, "md5_digest": "54d7e3290ab3cee08629eb42b474d763", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 4968, "upload_time": "2019-12-03T03:32:12", "upload_time_iso_8601": "2019-12-03T03:32:12.923438Z", "url": "https://files.pythonhosted.org/packages/56/71/ae3528cbe1643cd4856b6eccf6522d0dfe43c8abeeb9d0eedabc99699bb9/twit-0.0.11-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "15609231ab46874e4aa2b1e7fabbd9f2", "sha256": "eca37a8ebe266efcffb867d25de3ad566c68bc39f1d2f6af56e3f472544d70e9"}, "downloads": -1, "filename": "twit-0.0.11.tar.gz", "has_sig": false, "md5_digest": "15609231ab46874e4aa2b1e7fabbd9f2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 4540, "upload_time": "2019-12-03T03:32:14", "upload_time_iso_8601": "2019-12-03T03:32:14.314491Z", "url": "https://files.pythonhosted.org/packages/13/a4/af75b53c488e31fdd0dcd86443128279d643c2aa96a9b60b0dbc94213b71/twit-0.0.11.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "0553e57f006fbee009be12a6598390b8", "sha256": "8945d58640599ccaab492e3b1b2a4c0466396a4fd1ed9536780c8e3116bcd015"}, "downloads": -1, "filename": "twit-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0553e57f006fbee009be12a6598390b8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 4760, "upload_time": "2019-12-03T04:04:32", "upload_time_iso_8601": "2019-12-03T04:04:32.091685Z", "url": "https://files.pythonhosted.org/packages/6a/53/f17139611acb748ee37a96867cfdda17c88d1aa0f497af6043b93a32a10d/twit-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3d7965adb085198d72e9f8d069b3e6c9", "sha256": "90eabbf56e4af329a0708c36f23bae1c8a1f45665585e4a0e69d4dbbddc42e93"}, "downloads": -1, "filename": "twit-0.1.0.tar.gz", "has_sig": false, "md5_digest": "3d7965adb085198d72e9f8d069b3e6c9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 4285, "upload_time": "2019-12-03T04:04:33", "upload_time_iso_8601": "2019-12-03T04:04:33.583960Z", "url": "https://files.pythonhosted.org/packages/f0/91/5f338e07d107aeb366d80e2c60ee865cf18bef0204ee72c35e5bedd23894/twit-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "3fd302f3bd24cdfaeb3f780e311b6fa8", "sha256": "8c8ef32a722ec6c3e3c893df7b72f5fd17501985bff3e56adb094ead2f13a192"}, "downloads": -1, "filename": "twit-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "3fd302f3bd24cdfaeb3f780e311b6fa8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 6127, "upload_time": "2019-12-14T23:08:18", "upload_time_iso_8601": "2019-12-14T23:08:18.929984Z", "url": "https://files.pythonhosted.org/packages/f5/d9/69ca49db71c9e193ca6f9b62245cb7c0d5a0a035c16a7215deb9ae4b1b86/twit-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "335d8d3a1fa52aa832e22d01e5805886", "sha256": "a98447b5f2ff0577943ab9435823ab12275b91bbc7d56c351dde0c2832e53ef3"}, "downloads": -1, "filename": "twit-0.1.1.tar.gz", "has_sig": false, "md5_digest": "335d8d3a1fa52aa832e22d01e5805886", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 5050, "upload_time": "2019-12-14T23:08:19", "upload_time_iso_8601": "2019-12-14T23:08:19.837168Z", "url": "https://files.pythonhosted.org/packages/55/8b/3e3493270ef29553b9d10e769256c788d0f343ac599c7723473bdb2e1d67/twit-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3fd302f3bd24cdfaeb3f780e311b6fa8", "sha256": "8c8ef32a722ec6c3e3c893df7b72f5fd17501985bff3e56adb094ead2f13a192"}, "downloads": -1, "filename": "twit-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "3fd302f3bd24cdfaeb3f780e311b6fa8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 6127, "upload_time": "2019-12-14T23:08:18", "upload_time_iso_8601": "2019-12-14T23:08:18.929984Z", "url": "https://files.pythonhosted.org/packages/f5/d9/69ca49db71c9e193ca6f9b62245cb7c0d5a0a035c16a7215deb9ae4b1b86/twit-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "335d8d3a1fa52aa832e22d01e5805886", "sha256": "a98447b5f2ff0577943ab9435823ab12275b91bbc7d56c351dde0c2832e53ef3"}, "downloads": -1, "filename": "twit-0.1.1.tar.gz", "has_sig": false, "md5_digest": "335d8d3a1fa52aa832e22d01e5805886", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 5050, "upload_time": "2019-12-14T23:08:19", "upload_time_iso_8601": "2019-12-14T23:08:19.837168Z", "url": "https://files.pythonhosted.org/packages/55/8b/3e3493270ef29553b9d10e769256c788d0f343ac599c7723473bdb2e1d67/twit-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:43:48 2020"}