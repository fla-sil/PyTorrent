{"info": {"author": "Somyajit Chakraborty(Sam)", "author_email": "somyajitchppr@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Software Development :: Build Tools"], "description": "[![Gitter](https://badges.gitter.im/s-atmech/community.svg)](https://gitter.im/s-atmech/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge) [![CircleCI](https://circleci.com/gh/Samsomyajit/s-atmech/tree/master.svg?style=svg)](https://circleci.com/gh/Samsomyajit/s-atmech/tree/master) [![PyPI version](https://badge.fury.io/py/s-atmech.svg)](https://badge.fury.io/py/s-atmech)\n <p align=\"center\">\n  <img  src=\"https://github.com/Samsomyajit/s-atmech/blob/master/Misc/logo.png\">\n</p>\n\n# s-atmech\ns-atmech is an independent Open Source, Deep Learning python library which implements attention mechanism as a RNN(Recurrent Neural Network) Layer as Encoder-Decoder system. The papers:[click_here1](https://arxiv.org/pdf/1508.04025.pdf)|[click_here2](https://arxiv.org/pdf/1409.0473.pdf) aimed to improve the sequence-to-sequence model in machine translation by aligning the decoder with the relevant input sentences and implementing Attention. The Flow of calculating Attention weights in Bahdanau Attention is shown below:<br>\n<br>\n![Attention Weights](https://github.com/Samsomyajit/s-atmech/blob/master/Misc/Slide50.jpg)<br>\nAfter obtaining all of our encoder outputs, we can start using the decoder to produce outputs. At each time step of the decoder, we have to calculate the alignment score of each encoder output with respect to the decoder input and hidden state at that time step. The alignment score is the essence of the Attention mechanism, as it quantifies the amount of \u201cAttention\u201d the decoder will place on each of the encoder outputs when producing the next output.\n\nThe alignment scores for Bahdanau Attention are calculated using the hidden state produced by the decoder in the previous time step and the encoder outputs with the following equation:<br>\n![Score Equation](https://github.com/Samsomyajit/s-atmech/blob/master/Misc/score.JPG)<br>\nThe decoder hidden state and encoder outputs will be passed through their individual Linear layer and have their own individual trainable weights. Thereafter, they will be added together before being passed through a <i>tanh</i>activation function. The decoder hidden state is added to each encoder output in this case. Lastly, the resultant vector from the previous few steps will undergo matrix multiplication with a trainable vector, obtaining a final alignment score vector which holds a score for each encoder output. After generating the alignment scores vector in the previous step, we can then apply a softmax on this vector to obtain the attention weights. The softmax function will cause the values in the vector to sum up to 1 and each individual value will lie between 0 and 1, therefore representing the weightage each input holds at that time step. After computing the attention weights in the previous step, we can now generate the context vector by doing an element-wise multiplication of the attention weights with the encoder outputs.due to the softmax function in the previous step, if the score of a specific input element is closer to 1 its effect and influence on the decoder output is amplified, whereas if the score is close to 0, its influence is drowned out and nullified. The context vector we produced will then be concatenated with the previous decoder output. It is then fed into the decoder RNN cell to produce a new hidden state and the process repeats itself from step 2. The final output for the time step is obtained by passing the new hidden state through a Linear layer, which acts as a classifier to give the probability scores of the next predicted word. Steps 2 to 4 are repeated until the decoder generates an End Of Sentence token or the output length exceeds a specified maximum length. This above is the entire process how Bhadanau Attention works.\n\n# Next in Line:\nThis is still in alpha stage so we are planning to add a Luong Attention implementation which will be added by 2020. We are also developing a new attention algorithm for our library.\n\n# Naming:\ns-atmech actually is symbolic name for Sam's Attention Mechanism simple yet catchy! \\_(^_^)_/\n\n# Installation and Implementation:\nTo install s-atmech follow the steps:<br>\nInstall pip:\n```\n$ python get-pip.py\n```\nInstall s-atmech via pip:\n```\n$ pip install s-atmech\n```\nFor upgrade:\n```\n$ pip install --upgrade s-atmech\n```\nImplementation:\n```python\n>>> from s-atmech.AttentionLayer import AttentionLayer as atl\n>>> from s-atmech import luong_EncoderRNN\n>>> from s-atmech import luong_Attn\n>>> from s-atmech import luong_DecoderRNN\n```\n  \n# Developer Info:\nAuthor: Somyajit Chakraborty<br>\nAuthor-email: somyajitchppr@gmail.com<br>\nTeam: Bread and Code\n\n# Required Libraries:\n  'numpy',\n          'pandas',\n          'tensorflow',\n          'matplotlib',\n          'scikit-learn',\n          'jupyter',\n          'pillow',\n          'nltk',\n          'pyYAML',\n\n\n# LISCENSE:\n```\nMIT License\n\nCopyright (c) 2019 Somyajit Chakraborty Sam\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/Samsomyajit/s-atmech/archive/ver1.0.1.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Samsomyajit/s-atmech", "keywords": "Attention Mechanism,Natural Language Processing,Copy Mechanism,Text Summarization,Text Processing,Sentiment Analysis", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "s-atmech", "package_url": "https://pypi.org/project/s-atmech/", "platform": "", "project_url": "https://pypi.org/project/s-atmech/", "project_urls": {"Download": "https://github.com/Samsomyajit/s-atmech/archive/ver1.0.1.tar.gz", "Homepage": "https://github.com/Samsomyajit/s-atmech"}, "release_url": "https://pypi.org/project/s-atmech/2.0/", "requires_dist": null, "requires_python": "", "summary": "s-atmech is an independent Open Source, Deep Learning python library which implements attention mechanism as a RNN(Recurrent Neural Network) Layer as Encoder-Decoder system. (only supports Bahdanau Attention right now).", "version": "2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://gitter.im/s-atmech/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge\" rel=\"nofollow\"><img alt=\"Gitter\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b56480a8ac4b9c7577d5b442ca43529c845da167/68747470733a2f2f6261646765732e6769747465722e696d2f732d61746d6563682f636f6d6d756e6974792e737667\"></a> <a href=\"https://circleci.com/gh/Samsomyajit/s-atmech/tree/master\" rel=\"nofollow\"><img alt=\"CircleCI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/081e52d35abfa7ec73ae5f98c22caf9da8bec202/68747470733a2f2f636972636c6563692e636f6d2f67682f53616d736f6d79616a69742f732d61746d6563682f747265652f6d61737465722e7376673f7374796c653d737667\"></a> <a href=\"https://badge.fury.io/py/s-atmech\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fe9fec0b4259862c25dc3be79d6282d4a0a6d7b0/68747470733a2f2f62616467652e667572792e696f2f70792f732d61746d6563682e737667\"></a></p>\n <p align=\"center\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8c3ed483ad86886c2549ba2d1526d5080e4f987c/68747470733a2f2f6769746875622e636f6d2f53616d736f6d79616a69742f732d61746d6563682f626c6f622f6d61737465722f4d6973632f6c6f676f2e706e67\">\n</p>\n<h1>s-atmech</h1>\n<p>s-atmech is an independent Open Source, Deep Learning python library which implements attention mechanism as a RNN(Recurrent Neural Network) Layer as Encoder-Decoder system. The papers:<a href=\"https://arxiv.org/pdf/1508.04025.pdf\" rel=\"nofollow\">click_here1</a>|<a href=\"https://arxiv.org/pdf/1409.0473.pdf\" rel=\"nofollow\">click_here2</a> aimed to improve the sequence-to-sequence model in machine translation by aligning the decoder with the relevant input sentences and implementing Attention. The Flow of calculating Attention weights in Bahdanau Attention is shown below:<br>\n<br>\n<img alt=\"Attention Weights\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8e2c74a20f21c7d99d797c6d24649a7c829d0c69/68747470733a2f2f6769746875622e636f6d2f53616d736f6d79616a69742f732d61746d6563682f626c6f622f6d61737465722f4d6973632f536c69646535302e6a7067\"><br>\nAfter obtaining all of our encoder outputs, we can start using the decoder to produce outputs. At each time step of the decoder, we have to calculate the alignment score of each encoder output with respect to the decoder input and hidden state at that time step. The alignment score is the essence of the Attention mechanism, as it quantifies the amount of \u201cAttention\u201d the decoder will place on each of the encoder outputs when producing the next output.</p>\n<p>The alignment scores for Bahdanau Attention are calculated using the hidden state produced by the decoder in the previous time step and the encoder outputs with the following equation:<br>\n<img alt=\"Score Equation\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5b76164af288ab351649ed5afc2697eb0924e69f/68747470733a2f2f6769746875622e636f6d2f53616d736f6d79616a69742f732d61746d6563682f626c6f622f6d61737465722f4d6973632f73636f72652e4a5047\"><br>\nThe decoder hidden state and encoder outputs will be passed through their individual Linear layer and have their own individual trainable weights. Thereafter, they will be added together before being passed through a <i>tanh</i>activation function. The decoder hidden state is added to each encoder output in this case. Lastly, the resultant vector from the previous few steps will undergo matrix multiplication with a trainable vector, obtaining a final alignment score vector which holds a score for each encoder output. After generating the alignment scores vector in the previous step, we can then apply a softmax on this vector to obtain the attention weights. The softmax function will cause the values in the vector to sum up to 1 and each individual value will lie between 0 and 1, therefore representing the weightage each input holds at that time step. After computing the attention weights in the previous step, we can now generate the context vector by doing an element-wise multiplication of the attention weights with the encoder outputs.due to the softmax function in the previous step, if the score of a specific input element is closer to 1 its effect and influence on the decoder output is amplified, whereas if the score is close to 0, its influence is drowned out and nullified. The context vector we produced will then be concatenated with the previous decoder output. It is then fed into the decoder RNN cell to produce a new hidden state and the process repeats itself from step 2. The final output for the time step is obtained by passing the new hidden state through a Linear layer, which acts as a classifier to give the probability scores of the next predicted word. Steps 2 to 4 are repeated until the decoder generates an End Of Sentence token or the output length exceeds a specified maximum length. This above is the entire process how Bhadanau Attention works.</p>\n<h1>Next in Line:</h1>\n<p>This is still in alpha stage so we are planning to add a Luong Attention implementation which will be added by 2020. We are also developing a new attention algorithm for our library.</p>\n<h1>Naming:</h1>\n<p>s-atmech actually is symbolic name for Sam's Attention Mechanism simple yet catchy! _(^<em>^)</em>/</p>\n<h1>Installation and Implementation:</h1>\n<p>To install s-atmech follow the steps:<br>\nInstall pip:</p>\n<pre><code>$ python get-pip.py\n</code></pre>\n<p>Install s-atmech via pip:</p>\n<pre><code>$ pip install s-atmech\n</code></pre>\n<p>For upgrade:</p>\n<pre><code>$ pip install --upgrade s-atmech\n</code></pre>\n<p>Implementation:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">s</span><span class=\"o\">-</span><span class=\"n\">atmech</span><span class=\"o\">.</span><span class=\"n\">AttentionLayer</span> <span class=\"kn\">import</span> <span class=\"nn\">AttentionLayer</span> <span class=\"k\">as</span> <span class=\"nn\">atl</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">s</span><span class=\"o\">-</span><span class=\"n\">atmech</span> <span class=\"kn\">import</span> <span class=\"nn\">luong_EncoderRNN</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">s</span><span class=\"o\">-</span><span class=\"n\">atmech</span> <span class=\"kn\">import</span> <span class=\"nn\">luong_Attn</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">s</span><span class=\"o\">-</span><span class=\"n\">atmech</span> <span class=\"kn\">import</span> <span class=\"nn\">luong_DecoderRNN</span>\n</pre>\n<h1>Developer Info:</h1>\n<p>Author: Somyajit Chakraborty<br>\nAuthor-email: <a href=\"mailto:somyajitchppr@gmail.com\">somyajitchppr@gmail.com</a><br>\nTeam: Bread and Code</p>\n<h1>Required Libraries:</h1>\n<p>'numpy',\n'pandas',\n'tensorflow',\n'matplotlib',\n'scikit-learn',\n'jupyter',\n'pillow',\n'nltk',\n'pyYAML',</p>\n<h1>LISCENSE:</h1>\n<pre><code>MIT License\n\nCopyright (c) 2019 Somyajit Chakraborty Sam\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>\n\n          </div>"}, "last_serial": 6492587, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "d9d4b6faf1b3496f1501220a3287c8f1", "sha256": "a3668d6f9f6b29e8a1e19789c6e1453abd7548e140154c1c3febc2c351a10d30"}, "downloads": -1, "filename": "s-atmech-1.0.1.tar.gz", "has_sig": false, "md5_digest": "d9d4b6faf1b3496f1501220a3287c8f1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2677, "upload_time": "2019-12-28T14:23:42", "upload_time_iso_8601": "2019-12-28T14:23:42.595276Z", "url": "https://files.pythonhosted.org/packages/8e/c7/da966ea28869d3f7b898f0a8146dc9659201d42fa69694222b2c96814beb/s-atmech-1.0.1.tar.gz", "yanked": false}], "1.1.1": [{"comment_text": "", "digests": {"md5": "398a53b6b7b7db7f49f6b851198fc046", "sha256": "71826164eee97526fa5144addfde35668b664f158ea64fff01a79bf08783f0f0"}, "downloads": -1, "filename": "s-atmech-1.1.1.tar.gz", "has_sig": false, "md5_digest": "398a53b6b7b7db7f49f6b851198fc046", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4700, "upload_time": "2019-12-30T14:59:20", "upload_time_iso_8601": "2019-12-30T14:59:20.938609Z", "url": "https://files.pythonhosted.org/packages/2a/f2/9764de6e8129944f649e4f39bf4a9a3a206cdaa9d80d75b3ef6cb14cd269/s-atmech-1.1.1.tar.gz", "yanked": false}], "2.0": [{"comment_text": "", "digests": {"md5": "1daf4bf2b9eb95c4777f67e28dbda3a3", "sha256": "ba36ff24b0cdf4c22ff3dc35adbd2d887a1acc7988f78d48d8dd6f1e96665e5f"}, "downloads": -1, "filename": "s-atmech-2.0.tar.gz", "has_sig": false, "md5_digest": "1daf4bf2b9eb95c4777f67e28dbda3a3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8454, "upload_time": "2020-01-21T07:51:05", "upload_time_iso_8601": "2020-01-21T07:51:05.151512Z", "url": "https://files.pythonhosted.org/packages/8b/dd/a61bb2ca68798f82397c2dd8e3e20b8f6b2ac90d6e2d8738bf40c28f1037/s-atmech-2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1daf4bf2b9eb95c4777f67e28dbda3a3", "sha256": "ba36ff24b0cdf4c22ff3dc35adbd2d887a1acc7988f78d48d8dd6f1e96665e5f"}, "downloads": -1, "filename": "s-atmech-2.0.tar.gz", "has_sig": false, "md5_digest": "1daf4bf2b9eb95c4777f67e28dbda3a3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8454, "upload_time": "2020-01-21T07:51:05", "upload_time_iso_8601": "2020-01-21T07:51:05.151512Z", "url": "https://files.pythonhosted.org/packages/8b/dd/a61bb2ca68798f82397c2dd8e3e20b8f6b2ac90d6e2d8738bf40c28f1037/s-atmech-2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:58:29 2020"}