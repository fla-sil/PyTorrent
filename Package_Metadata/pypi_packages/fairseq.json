{"info": {"author": "", "author_email": "", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# <img src=\"fairseq_logo.png\" width=\"30\"> Introduction\n\nFairseq(-py) is a sequence modeling toolkit that allows researchers and\ndevelopers to train custom models for translation, summarization, language\nmodeling and other text generation tasks.\n\n### What's New:\n\n- November 2019: [CamemBERT model and code released](examples/camembert/README.md)\n- November 2019: [BART model and code released](examples/bart/README.md)\n- November 2019: [XLM-R models and code released](examples/xlmr/README.md)\n- September 2019: [Nonautoregressive translation code released](examples/nonautoregressive_translation/README.md)\n- August 2019: [WMT'19 models released](examples/wmt19/README.md)\n- July 2019: fairseq relicensed under MIT license\n- July 2019: [RoBERTa models and code released](examples/roberta/README.md)\n- June 2019: [wav2vec models and code released](examples/wav2vec/README.md)\n\n### Features:\n\nFairseq provides reference implementations of various sequence-to-sequence models, including:\n- **Convolutional Neural Networks (CNN)**\n  - [Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)](examples/language_model/conv_lm/README.md)\n  - [Convolutional Sequence to Sequence Learning (Gehring et al., 2017)](examples/conv_seq2seq/README.md)\n  - [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel)\n  - [Hierarchical Neural Story Generation (Fan et al., 2018)](examples/stories/README.md)\n  - [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)](examples/wav2vec/README.md)\n- **LightConv and DynamicConv models**\n  - [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)](examples/pay_less_attention_paper/README.md)\n- **Long Short-Term Memory (LSTM) networks**\n  - Effective Approaches to Attention-based Neural Machine Translation (Luong et al., 2015)\n- **Transformer (self-attention) networks**\n  - Attention Is All You Need (Vaswani et al., 2017)\n  - [Scaling Neural Machine Translation (Ott et al., 2018)](examples/scaling_nmt/README.md)\n  - [Understanding Back-Translation at Scale (Edunov et al., 2018)](examples/backtranslation/README.md)\n  - [Adaptive Input Representations for Neural Language Modeling (Baevski and Auli, 2018)](examples/language_model/transformer_lm/README.md)\n  - [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)](examples/translation_moe/README.md)\n  - [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)](examples/roberta/README.md)\n  - [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)](examples/wmt19/README.md)\n  - [Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)](examples/joint_alignment_translation/README.md )\n- **Non-autoregressive Transformers**\n  - Non-Autoregressive Neural Machine Translation (Gu et al., 2017)\n  - Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement (Lee et al. 2018)\n  - Insertion Transformer: Flexible Sequence Generation via Insertion Operations (Stern et al. 2019)\n  - Mask-Predict: Parallel Decoding of Conditional Masked Language Models (Ghazvininejad et al., 2019)\n  - [Levenshtein Transformer (Gu et al., 2019)](examples/nonautoregressive_translation/README.md)\n\n\n**Additionally:**\n- multi-GPU (distributed) training on one machine or across multiple machines\n- fast generation on both CPU and GPU with multiple search algorithms implemented:\n  - beam search\n  - Diverse Beam Search ([Vijayakumar et al., 2016](https://arxiv.org/abs/1610.02424))\n  - sampling (unconstrained, top-k and top-p/nucleus)\n- large mini-batch training even on a single GPU via delayed updates\n- mixed precision training (trains faster with less GPU memory on [NVIDIA tensor cores](https://developer.nvidia.com/tensor-cores))\n- extensible: easily register new models, criterions, tasks, optimizers and learning rate schedulers\n\nWe also provide [pre-trained models for translation and language modeling](#pre-trained-models-and-examples)\nwith a convenient `torch.hub` interface:\n```python\nen2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model')\nen2de.translate('Hello world', beam=5)\n# 'Hallo Welt'\n```\nSee the PyTorch Hub tutorials for [translation](https://pytorch.org/hub/pytorch_fairseq_translation/)\nand [RoBERTa](https://pytorch.org/hub/pytorch_fairseq_roberta/) for more examples.\n\n![Model](fairseq.gif)\n\n# Requirements and Installation\n\n* [PyTorch](http://pytorch.org/) version >= 1.2.0\n* Python version >= 3.5\n* For training new models, you'll also need an NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl)\n* **For faster training** install NVIDIA's [apex](https://github.com/NVIDIA/apex) library with the `--cuda_ext` option\n\nTo install fairseq:\n```bash\npip install fairseq\n```\n\nOn MacOS:\n```bash\nCFLAGS=\"-stdlib=libc++\" pip install fairseq\n```\n\nIf you use Docker make sure to increase the shared memory size either with\n`--ipc=host` or `--shm-size` as command line options to `nvidia-docker run`.\n\n**Installing from source**\n\nTo install fairseq from source and develop locally:\n```bash\ngit clone https://github.com/pytorch/fairseq\ncd fairseq\npip install --editable .\n```\n\n# Getting Started\n\nThe [full documentation](https://fairseq.readthedocs.io/) contains instructions\nfor getting started, training new models and extending fairseq with new model\ntypes and tasks.\n\n# Pre-trained models and examples\n\nWe provide pre-trained models and pre-processed, binarized test sets for several tasks listed below,\nas well as example training and evaluation commands.\n\n- [Translation](examples/translation/README.md): convolutional and transformer models are available\n- [Language Modeling](examples/language_model/README.md): convolutional and transformer models are available\n- [wav2vec](examples/wav2vec/README.md): wav2vec large model is available\n\nWe also have more detailed READMEs to reproduce results from specific papers:\n- [Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)](examples/joint_alignment_translation/README.md )\n- [Levenshtein Transformer (Gu et al., 2019)](examples/nonautoregressive_translation/README.md)\n- [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)](examples/wmt19/README.md)\n- [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)](examples/roberta/README.md)\n- [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)](examples/wav2vec/README.md)\n- [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)](examples/translation_moe/README.md)\n- [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)](examples/pay_less_attention_paper/README.md)\n- [Understanding Back-Translation at Scale (Edunov et al., 2018)](examples/backtranslation/README.md)\n- [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel)\n- [Hierarchical Neural Story Generation (Fan et al., 2018)](examples/stories/README.md)\n- [Scaling Neural Machine Translation (Ott et al., 2018)](examples/scaling_nmt/README.md)\n- [Convolutional Sequence to Sequence Learning (Gehring et al., 2017)](examples/conv_seq2seq/README.md)\n- [Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)](examples/language_model/conv_lm/README.md)\n\n# Join the fairseq community\n\n* Facebook page: https://www.facebook.com/groups/fairseq.users\n* Google group: https://groups.google.com/forum/#!forum/fairseq-users\n\n# License\nfairseq(-py) is MIT-licensed.\nThe license applies to the pre-trained models as well.\n\n# Citation\n\nPlease cite as:\n\n```bibtex\n@inproceedings{ott2019fairseq,\n  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},\n  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},\n  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},\n  year = {2019},\n}\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/pytorch/fairseq", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "fairseq", "package_url": "https://pypi.org/project/fairseq/", "platform": "", "project_url": "https://pypi.org/project/fairseq/", "project_urls": {"Homepage": "https://github.com/pytorch/fairseq"}, "release_url": "https://pypi.org/project/fairseq/0.9.0/", "requires_dist": null, "requires_python": "", "summary": "Facebook AI Research Sequence-to-Sequence Toolkit", "version": "0.9.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c48b3cf65be3c923e98df7fd5acb5408cebce3e4/666169727365715f6c6f676f2e706e67\" width=\"30\"> Introduction</h1>\n<p>Fairseq(-py) is a sequence modeling toolkit that allows researchers and\ndevelopers to train custom models for translation, summarization, language\nmodeling and other text generation tasks.</p>\n<h3>What's New:</h3>\n<ul>\n<li>November 2019: <a href=\"examples/camembert/README.md\" rel=\"nofollow\">CamemBERT model and code released</a></li>\n<li>November 2019: <a href=\"examples/bart/README.md\" rel=\"nofollow\">BART model and code released</a></li>\n<li>November 2019: <a href=\"examples/xlmr/README.md\" rel=\"nofollow\">XLM-R models and code released</a></li>\n<li>September 2019: <a href=\"examples/nonautoregressive_translation/README.md\" rel=\"nofollow\">Nonautoregressive translation code released</a></li>\n<li>August 2019: <a href=\"examples/wmt19/README.md\" rel=\"nofollow\">WMT'19 models released</a></li>\n<li>July 2019: fairseq relicensed under MIT license</li>\n<li>July 2019: <a href=\"examples/roberta/README.md\" rel=\"nofollow\">RoBERTa models and code released</a></li>\n<li>June 2019: <a href=\"examples/wav2vec/README.md\" rel=\"nofollow\">wav2vec models and code released</a></li>\n</ul>\n<h3>Features:</h3>\n<p>Fairseq provides reference implementations of various sequence-to-sequence models, including:</p>\n<ul>\n<li><strong>Convolutional Neural Networks (CNN)</strong>\n<ul>\n<li><a href=\"examples/language_model/conv_lm/README.md\" rel=\"nofollow\">Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)</a></li>\n<li><a href=\"examples/conv_seq2seq/README.md\" rel=\"nofollow\">Convolutional Sequence to Sequence Learning (Gehring et al., 2017)</a></li>\n<li><a href=\"https://github.com/pytorch/fairseq/tree/classic_seqlevel\" rel=\"nofollow\">Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)</a></li>\n<li><a href=\"examples/stories/README.md\" rel=\"nofollow\">Hierarchical Neural Story Generation (Fan et al., 2018)</a></li>\n<li><a href=\"examples/wav2vec/README.md\" rel=\"nofollow\">wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)</a></li>\n</ul>\n</li>\n<li><strong>LightConv and DynamicConv models</strong>\n<ul>\n<li><a href=\"examples/pay_less_attention_paper/README.md\" rel=\"nofollow\">Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)</a></li>\n</ul>\n</li>\n<li><strong>Long Short-Term Memory (LSTM) networks</strong>\n<ul>\n<li>Effective Approaches to Attention-based Neural Machine Translation (Luong et al., 2015)</li>\n</ul>\n</li>\n<li><strong>Transformer (self-attention) networks</strong>\n<ul>\n<li>Attention Is All You Need (Vaswani et al., 2017)</li>\n<li><a href=\"examples/scaling_nmt/README.md\" rel=\"nofollow\">Scaling Neural Machine Translation (Ott et al., 2018)</a></li>\n<li><a href=\"examples/backtranslation/README.md\" rel=\"nofollow\">Understanding Back-Translation at Scale (Edunov et al., 2018)</a></li>\n<li><a href=\"examples/language_model/transformer_lm/README.md\" rel=\"nofollow\">Adaptive Input Representations for Neural Language Modeling (Baevski and Auli, 2018)</a></li>\n<li><a href=\"examples/translation_moe/README.md\" rel=\"nofollow\">Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)</a></li>\n<li><a href=\"examples/roberta/README.md\" rel=\"nofollow\">RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)</a></li>\n<li><a href=\"examples/wmt19/README.md\" rel=\"nofollow\">Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)</a></li>\n<li><a href=\"examples/joint_alignment_translation/README.md\" rel=\"nofollow\">Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)</a></li>\n</ul>\n</li>\n<li><strong>Non-autoregressive Transformers</strong>\n<ul>\n<li>Non-Autoregressive Neural Machine Translation (Gu et al., 2017)</li>\n<li>Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement (Lee et al. 2018)</li>\n<li>Insertion Transformer: Flexible Sequence Generation via Insertion Operations (Stern et al. 2019)</li>\n<li>Mask-Predict: Parallel Decoding of Conditional Masked Language Models (Ghazvininejad et al., 2019)</li>\n<li><a href=\"examples/nonautoregressive_translation/README.md\" rel=\"nofollow\">Levenshtein Transformer (Gu et al., 2019)</a></li>\n</ul>\n</li>\n</ul>\n<p><strong>Additionally:</strong></p>\n<ul>\n<li>multi-GPU (distributed) training on one machine or across multiple machines</li>\n<li>fast generation on both CPU and GPU with multiple search algorithms implemented:\n<ul>\n<li>beam search</li>\n<li>Diverse Beam Search (<a href=\"https://arxiv.org/abs/1610.02424\" rel=\"nofollow\">Vijayakumar et al., 2016</a>)</li>\n<li>sampling (unconstrained, top-k and top-p/nucleus)</li>\n</ul>\n</li>\n<li>large mini-batch training even on a single GPU via delayed updates</li>\n<li>mixed precision training (trains faster with less GPU memory on <a href=\"https://developer.nvidia.com/tensor-cores\" rel=\"nofollow\">NVIDIA tensor cores</a>)</li>\n<li>extensible: easily register new models, criterions, tasks, optimizers and learning rate schedulers</li>\n</ul>\n<p>We also provide <a href=\"#pre-trained-models-and-examples\" rel=\"nofollow\">pre-trained models for translation and language modeling</a>\nwith a convenient <code>torch.hub</code> interface:</p>\n<pre><span class=\"n\">en2de</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">hub</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">'pytorch/fairseq'</span><span class=\"p\">,</span> <span class=\"s1\">'transformer.wmt19.en-de.single_model'</span><span class=\"p\">)</span>\n<span class=\"n\">en2de</span><span class=\"o\">.</span><span class=\"n\">translate</span><span class=\"p\">(</span><span class=\"s1\">'Hello world'</span><span class=\"p\">,</span> <span class=\"n\">beam</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"c1\"># 'Hallo Welt'</span>\n</pre>\n<p>See the PyTorch Hub tutorials for <a href=\"https://pytorch.org/hub/pytorch_fairseq_translation/\" rel=\"nofollow\">translation</a>\nand <a href=\"https://pytorch.org/hub/pytorch_fairseq_roberta/\" rel=\"nofollow\">RoBERTa</a> for more examples.</p>\n<p><img alt=\"Model\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c604c22158d8f9ef9dc77487c2f313deb551a47b/666169727365712e676966\"></p>\n<h1>Requirements and Installation</h1>\n<ul>\n<li><a href=\"http://pytorch.org/\" rel=\"nofollow\">PyTorch</a> version &gt;= 1.2.0</li>\n<li>Python version &gt;= 3.5</li>\n<li>For training new models, you'll also need an NVIDIA GPU and <a href=\"https://github.com/NVIDIA/nccl\" rel=\"nofollow\">NCCL</a></li>\n<li><strong>For faster training</strong> install NVIDIA's <a href=\"https://github.com/NVIDIA/apex\" rel=\"nofollow\">apex</a> library with the <code>--cuda_ext</code> option</li>\n</ul>\n<p>To install fairseq:</p>\n<pre>pip install fairseq\n</pre>\n<p>On MacOS:</p>\n<pre><span class=\"nv\">CFLAGS</span><span class=\"o\">=</span><span class=\"s2\">\"-stdlib=libc++\"</span> pip install fairseq\n</pre>\n<p>If you use Docker make sure to increase the shared memory size either with\n<code>--ipc=host</code> or <code>--shm-size</code> as command line options to <code>nvidia-docker run</code>.</p>\n<p><strong>Installing from source</strong></p>\n<p>To install fairseq from source and develop locally:</p>\n<pre>git clone https://github.com/pytorch/fairseq\n<span class=\"nb\">cd</span> fairseq\npip install --editable .\n</pre>\n<h1>Getting Started</h1>\n<p>The <a href=\"https://fairseq.readthedocs.io/\" rel=\"nofollow\">full documentation</a> contains instructions\nfor getting started, training new models and extending fairseq with new model\ntypes and tasks.</p>\n<h1>Pre-trained models and examples</h1>\n<p>We provide pre-trained models and pre-processed, binarized test sets for several tasks listed below,\nas well as example training and evaluation commands.</p>\n<ul>\n<li><a href=\"examples/translation/README.md\" rel=\"nofollow\">Translation</a>: convolutional and transformer models are available</li>\n<li><a href=\"examples/language_model/README.md\" rel=\"nofollow\">Language Modeling</a>: convolutional and transformer models are available</li>\n<li><a href=\"examples/wav2vec/README.md\" rel=\"nofollow\">wav2vec</a>: wav2vec large model is available</li>\n</ul>\n<p>We also have more detailed READMEs to reproduce results from specific papers:</p>\n<ul>\n<li><a href=\"examples/joint_alignment_translation/README.md\" rel=\"nofollow\">Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)</a></li>\n<li><a href=\"examples/nonautoregressive_translation/README.md\" rel=\"nofollow\">Levenshtein Transformer (Gu et al., 2019)</a></li>\n<li><a href=\"examples/wmt19/README.md\" rel=\"nofollow\">Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)</a></li>\n<li><a href=\"examples/roberta/README.md\" rel=\"nofollow\">RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)</a></li>\n<li><a href=\"examples/wav2vec/README.md\" rel=\"nofollow\">wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)</a></li>\n<li><a href=\"examples/translation_moe/README.md\" rel=\"nofollow\">Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)</a></li>\n<li><a href=\"examples/pay_less_attention_paper/README.md\" rel=\"nofollow\">Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)</a></li>\n<li><a href=\"examples/backtranslation/README.md\" rel=\"nofollow\">Understanding Back-Translation at Scale (Edunov et al., 2018)</a></li>\n<li><a href=\"https://github.com/pytorch/fairseq/tree/classic_seqlevel\" rel=\"nofollow\">Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)</a></li>\n<li><a href=\"examples/stories/README.md\" rel=\"nofollow\">Hierarchical Neural Story Generation (Fan et al., 2018)</a></li>\n<li><a href=\"examples/scaling_nmt/README.md\" rel=\"nofollow\">Scaling Neural Machine Translation (Ott et al., 2018)</a></li>\n<li><a href=\"examples/conv_seq2seq/README.md\" rel=\"nofollow\">Convolutional Sequence to Sequence Learning (Gehring et al., 2017)</a></li>\n<li><a href=\"examples/language_model/conv_lm/README.md\" rel=\"nofollow\">Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)</a></li>\n</ul>\n<h1>Join the fairseq community</h1>\n<ul>\n<li>Facebook page: <a href=\"https://www.facebook.com/groups/fairseq.users\" rel=\"nofollow\">https://www.facebook.com/groups/fairseq.users</a></li>\n<li>Google group: <a href=\"https://groups.google.com/forum/#!forum/fairseq-users\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/fairseq-users</a></li>\n</ul>\n<h1>License</h1>\n<p>fairseq(-py) is MIT-licensed.\nThe license applies to the pre-trained models as well.</p>\n<h1>Citation</h1>\n<p>Please cite as:</p>\n<pre><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">ott2019fairseq</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{fairseq: A Fast, Extensible Toolkit for Sequence Modeling}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{Proceedings of NAACL-HLT 2019: Demonstrations}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2019}</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n</pre>\n\n          </div>"}, "last_serial": 6241956, "releases": {"0.6.1": [{"comment_text": "", "digests": {"md5": "42fa1c58ff11a36f9ff500e0f08909ce", "sha256": "7133d1b163805535e13e91068c03c4b9cddec327a8748df27d90560215f3f5a6"}, "downloads": -1, "filename": "fairseq-0.6.1.tar.gz", "has_sig": false, "md5_digest": "42fa1c58ff11a36f9ff500e0f08909ce", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 132864, "upload_time": "2019-02-09T05:46:51", "upload_time_iso_8601": "2019-02-09T05:46:51.246071Z", "url": "https://files.pythonhosted.org/packages/cb/56/2032410e608c310418fbe6be6a89cd23d1b0e392872063d5464611b63267/fairseq-0.6.1.tar.gz", "yanked": false}], "0.6.2": [{"comment_text": "", "digests": {"md5": "a52ca46bd35d83e81a8e2e7bed0045d8", "sha256": "4ef01eb5f4e1731e16ac3ea06122ca1526f768ab46a218a20fd972687f46cbd7"}, "downloads": -1, "filename": "fairseq-0.6.2.tar.gz", "has_sig": false, "md5_digest": "a52ca46bd35d83e81a8e2e7bed0045d8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 139260, "upload_time": "2019-03-15T17:36:05", "upload_time_iso_8601": "2019-03-15T17:36:05.684525Z", "url": "https://files.pythonhosted.org/packages/d7/b1/bb10d50b5fb0a030ddbeb049c8df66137f3a137e42d6d2474e73aa878a51/fairseq-0.6.2.tar.gz", "yanked": false}], "0.7.1": [{"comment_text": "", "digests": {"md5": "84f1fb0a46258991bfd0cf2cbeac0bf1", "sha256": "559116342b3c11f948ea29eea0d35a82668351c83c058d77800bb88aa6151842"}, "downloads": -1, "filename": "fairseq-0.7.1.tar.gz", "has_sig": false, "md5_digest": "84f1fb0a46258991bfd0cf2cbeac0bf1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 186342, "upload_time": "2019-06-20T15:21:16", "upload_time_iso_8601": "2019-06-20T15:21:16.345713Z", "url": "https://files.pythonhosted.org/packages/97/b4/b37d9ef01891ed1884f7d969d6b2ba4ac3e1adcada34dda3e39c4caac9b9/fairseq-0.7.1.tar.gz", "yanked": false}], "0.7.2": [{"comment_text": "", "digests": {"md5": "4c3624eeb4a45027975d06b9a3bc59ee", "sha256": "66675264017ed345da5d085325189705f091a3da83896049d4f37508c6d33426"}, "downloads": -1, "filename": "fairseq-0.7.2.tar.gz", "has_sig": false, "md5_digest": "4c3624eeb4a45027975d06b9a3bc59ee", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 190759, "upload_time": "2019-07-19T13:43:26", "upload_time_iso_8601": "2019-07-19T13:43:26.118164Z", "url": "https://files.pythonhosted.org/packages/1c/13/41fb03306f9e50581210d2fb24f2f056f700f9ffdddb0f7734c7bda5d715/fairseq-0.7.2.tar.gz", "yanked": false}], "0.8.0": [{"comment_text": "", "digests": {"md5": "b3fcd8aeb7a7420636040b54fd2c8d47", "sha256": "55dfedb630ba76ac6c25bf443fd624ce78e097b4820fdc3b1c6d2b153e9f212e"}, "downloads": -1, "filename": "fairseq-0.8.0.tar.gz", "has_sig": false, "md5_digest": "b3fcd8aeb7a7420636040b54fd2c8d47", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 216335, "upload_time": "2019-08-14T12:35:17", "upload_time_iso_8601": "2019-08-14T12:35:17.563948Z", "url": "https://files.pythonhosted.org/packages/62/19/a71af3ea3bdf7c2fd66c8076a3be090147f700e3e513b0b3b11d80d97fe3/fairseq-0.8.0.tar.gz", "yanked": false}], "0.9.0": [{"comment_text": "", "digests": {"md5": "174a84c432d209995e8f0bcfff93cf78", "sha256": "61206358b79f325ea0b46cfd8c95cdb81bfbcfb43cf12b47d1d5124ce7321d3b"}, "downloads": -1, "filename": "fairseq-0.9.0.tar.gz", "has_sig": false, "md5_digest": "174a84c432d209995e8f0bcfff93cf78", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 306103, "upload_time": "2019-12-04T14:33:03", "upload_time_iso_8601": "2019-12-04T14:33:03.217666Z", "url": "https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "174a84c432d209995e8f0bcfff93cf78", "sha256": "61206358b79f325ea0b46cfd8c95cdb81bfbcfb43cf12b47d1d5124ce7321d3b"}, "downloads": -1, "filename": "fairseq-0.9.0.tar.gz", "has_sig": false, "md5_digest": "174a84c432d209995e8f0bcfff93cf78", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 306103, "upload_time": "2019-12-04T14:33:03", "upload_time_iso_8601": "2019-12-04T14:33:03.217666Z", "url": "https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:58 2020"}