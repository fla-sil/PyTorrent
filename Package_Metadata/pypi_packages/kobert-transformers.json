{"info": {"author": "Jangwon Park", "author_email": "adieujw@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# DistilKoBERT\n\nDistillation of KoBERT (`SKTBrain KoBERT` \uacbd\ub7c9\ud654)\n\n**January 27th, 2020 - Update**: 10GB\uc758 Corpus\ub97c \uac00\uc9c0\uace0 \uc0c8\ub85c \ud559\uc2b5\ud558\uc600\uc2b5\ub2c8\ub2e4. Subtask\uc5d0\uc11c \uc131\ub2a5\uc774 \uc18c\ud3ed \uc0c1\uc2b9\ud588\uc2b5\ub2c8\ub2e4.\n\n## Pretraining DistilKoBERT\n\n- \uae30\uc874\uc758 12 layer\ub97c **3 layer**\ub85c \uc904\uc600\uc73c\uba70, \uae30\ud0c0 configuration\uc740 kobert\ub97c \uadf8\ub300\ub85c \ub530\ub790\uc2b5\ub2c8\ub2e4.\n  - [\uc6d0 \ub17c\ubb38](https://arxiv.org/abs/1910.01108)\uc740 6 layer\ub97c \ucc44\ud0dd\ud558\uc600\uc2b5\ub2c8\ub2e4.\n- Layer \ucd08\uae30\ud654\uc758 \uacbd\uc6b0 \uae30\uc874 KoBERT\uc758 1, 5, 9\ubc88\uc9f8 layer \uac12\uc744 \uadf8\ub300\ub85c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.\n- Pretraining Corpus\ub294 \ud55c\uad6d\uc5b4 \uc704\ud0a4, \ub098\ubb34\uc704\ud0a4, \ub274\uc2a4 \ub4f1 \uc57d 10GB\uc758 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud588\uc73c\uba70, 3 epoch \ud559\uc2b5\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n## KoBERT / DistilKoBERT for transformers library\n\n- \uae30\uc874\uc758 KoBERT\ub97c transformers \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c \uace7\ubc14\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ub9de\ucdc4\uc2b5\ub2c8\ub2e4.\n  - transformers v2.2.2\ubd80\ud130 \uac1c\uc778\uc774 \ub9cc\ub4e0 \ubaa8\ub378\uc744 transformers\ub97c \ud1b5\ud574 \uc9c1\uc811 \uc5c5\ub85c\ub4dc/\ub2e4\uc6b4\ub85c\ub4dc\ud558\uc5ec \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4\n  - DistilKoBERT \uc5ed\uc2dc transformers \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c \uace7\ubc14\ub85c \ub2e4\uc6b4 \ubc1b\uc544\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n### Dependencies\n\n- torch>=1.1.0\n- transformers>=2.2.2\n- sentencepiece>=0.1.82\n\n### How to Use\n\n```python\n>>> from transformers import BertModel, DistilBertModel\n>>> bert_model = BertModel.from_pretrained('monologg/kobert')\n>>> distilbert_model = DistilBertModel.from_pretrained('monologg/distilkobert')\n```\n\n- Tokenizer\ub97c \uc0ac\uc6a9\ud558\ub824\uba74, \ub8e8\ud2b8 \ub514\ub809\ud1a0\ub9ac\uc758 `tokenization_kobert.py` \ud30c\uc77c\uc744 \ubcf5\uc0ac\ud55c \ud6c4, `KoBertTokenizer`\ub97c \uc784\ud3ec\ud2b8\ud558\uba74 \ub429\ub2c8\ub2e4.\n  - KoBERT\uc640 DistilKoBERT \ubaa8\ub450 \ub3d9\uc77c\ud55c \ud1a0\ud06c\ub098\uc774\uc800\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n  - **\uae30\uc874 KoBERT\uc758 \uacbd\uc6b0 Special Token\uc774 \uc81c\ub300\ub85c \ubd84\ub9ac\ub418\uc9c0 \uc54a\ub294 \uc774\uc288\uac00 \uc788\uc5b4\uc11c \ud574\ub2f9 \ubd80\ubd84\uc744 \uc218\uc815\ud558\uc5ec \ubc18\uc601\ud558\uc600\uc2b5\ub2c8\ub2e4.** ([Issue link](https://github.com/SKTBrain/KoBERT/issues/11))\n\n```python\n>>> from tokenization_kobert import KoBertTokenizer\n>>> tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') # monologg/distilkobert\ub3c4 \ub3d9\uc77c\n>>> tokenizer.tokenize(\"[CLS] \ud55c\uad6d\uc5b4 \ubaa8\ub378\uc744 \uacf5\uc720\ud569\ub2c8\ub2e4. [SEP]\")\n['[CLS]', '\u2581\ud55c\uad6d', '\uc5b4', '\u2581\ubaa8\ub378', '\uc744', '\u2581\uacf5\uc720', '\ud569\ub2c8\ub2e4', '.', '[SEP]']\n>>> tokenizer.convert_tokens_to_ids(['[CLS]', '\u2581\ud55c\uad6d', '\uc5b4', '\u2581\ubaa8\ub378', '\uc744', '\u2581\uacf5\uc720', '\ud569\ub2c8\ub2e4', '.', '[SEP]'])\n[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]\n```\n\n## What is different between BERT and DistilBERT\n\n- DistilBert\ub294 \uae30\uc874\uc758 Bert\uc640 \ub2ec\ub9ac token-type embedding\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n\n  - Transformers \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 DistilBertModel\uc744 \uc0ac\uc6a9\ud560 \ub54c \uae30\uc874 BertModel \uacfc \ub2ec\ub9ac `token_type_ids`\ub97c \ub123\uc744 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.\n\n- \ub610\ud55c DistilBert\ub294 pooler\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n\n  - \uace0\ub85c \uae30\uc874 BertModel\uc758 \uacbd\uc6b0 forward\uc758 return \uac12\uc73c\ub85c `sequence_output, pooled_output, (hidden_states), (attentions)`\uc744 \ubf51\uc544\ub0b4\uc9c0\ub9cc, DistilBertModel\uc758 \uacbd\uc6b0 `sequence_output, (hidden_states), (attentions)`\ub97c \ubf51\uc544\ub0c5\ub2c8\ub2e4.\n  - DistilBert\uc5d0\uc11c `[CLS]` \ud1a0\ud070\uc744 \ubf51\uc544\ub0b4\ub824\uba74 `sequence_output[0][:, 0]`\ub97c \uc801\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n## Kobert-Transformers python library\n\n[![Release](https://img.shields.io/badge/release-v0.2.1-green)](https://pypi.org/project/kobert-transformers/)\n[![Downloads](https://pepy.tech/badge/kobert-transformers)](https://pepy.tech/project/kobert-transformers)\n[![license](https://img.shields.io/badge/license-Apache%202.0-red)](https://github.com/monologg/DistilKoBERT/blob/master/LICENSE)\n\n- tokenization_kobert.py\ub97c \ub7a9\ud551\ud55c \ud30c\uc774\uc36c \ub77c\uc774\ube0c\ub7ec\ub9ac\n- KoBERT, DistilKoBERT\ub97c Huggingface Transformers \ub77c\uc774\ube0c\ub7ec\ub9ac \ud615\ud0dc\ub85c \uc784\ud3ec\ud2b8\n\n### Install Kobert-Transformers\n\n```bash\n$ pip3 install kobert-transformers\n```\n\n### How to Use\n\n```python\n>>> import torch\n>>> from kobert_transformers import get_distilkobert_model, get_kobert_model\n\n>>> model = get_distilkobert_model()\n>>> input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n>>> attention_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n>>> last_layer_hidden_state, _ = model(input_ids, attention_mask)\n>>> last_layer_hidden_state\ntensor([[[-0.4294,  0.1849,  0.2622,  ..., -0.8856, -0.0617, -0.0664],\n         [ 0.0580,  0.2065,  0.1131,  ..., -0.9954, -1.2588, -0.1635],\n         [-0.3945,  0.0641, -0.2223,  ..., -0.9819, -0.9723,  0.0929]],\n\n        [[ 0.1698, -0.2389, -0.0153,  ..., -0.0329, -0.0892, -0.0428],\n         [ 0.1348, -0.5269, -0.2861,  ..., -0.6471, -0.6776, -0.2948],\n         [ 0.0655, -0.4104, -0.0467,  ..., -0.5906, -0.6362, -0.0361]]],\n       grad_fn=<AddcmulBackward>)\n```\n\n```python\n>>> from kobert_transformers import get_tokenizer\n>>> tokenizer = get_tokenizer()\n>>> tokenizer.tokenize(\"[CLS] \ud55c\uad6d\uc5b4 \ubaa8\ub378\uc744 \uacf5\uc720\ud569\ub2c8\ub2e4. [SEP]\")\n['[CLS]', '\u2581\ud55c\uad6d', '\uc5b4', '\u2581\ubaa8\ub378', '\uc744', '\u2581\uacf5\uc720', '\ud569\ub2c8\ub2e4', '.', '[SEP]']\n>>> tokenizer.convert_tokens_to_ids(['[CLS]', '\u2581\ud55c\uad6d', '\uc5b4', '\u2581\ubaa8\ub378', '\uc744', '\u2581\uacf5\uc720', '\ud569\ub2c8\ub2e4', '.', '[SEP]'])\n[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]\n```\n\n## Result on Sub-task\n\n|                     | KoBERT | DistilKoBERT | Bert-multilingual |\n| ------------------- | ------ | ------------ | ----------------- |\n| Model Size (MB)     | 351    | 108          | 681               |\n| **NSMC** (acc)      | 89.63  | 88.41        | 87.07             |\n| **Naver NER** (F1)  | 84.23  | 82.14        | 81.78             |\n| **KorQuAD** (EM/F1) | TBD    | TBD          | 77.04/87.85       |\n\n- NSMC (Naver Sentiment Movie Corpus) ([Implementation of KoBERT-nsmc](https://github.com/monologg/KoBERT-nsmc))\n- Naver NER (NER task on Naver NLP Challenge 2018) ([Implementation of KoBERT-NER](https://github.com/monologg/KoBERT-NER))\n\n## Reference\n\n- [KoBERT](https://github.com/SKTBrain/KoBERT)\n- [Huggingface Transformers](https://github.com/huggingface/transformers)\n- [DistilBERT Github](https://github.com/huggingface/transformers/blob/master/examples/distillation/README.md)\n- [DistilBERT Paper](https://arxiv.org/abs/1910.01108)\n- [\ub525\ub7ec\ub2dd\uc73c\ub85c \ub3d9\ub124\uc0dd\ud65c \uac8c\uc2dc\uae00 \ud544\ud130\ub9c1\ud558\uae30](https://medium.com/daangn/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9C%BC%EB%A1%9C-%EB%8F%99%EB%84%A4%EC%83%9D%ED%99%9C-%EA%B2%8C%EC%8B%9C%EA%B8%80-%ED%95%84%ED%84%B0%EB%A7%81%ED%95%98%EA%B8%B0-263cfe4bc58d)\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/monologg/DistilKoBERT", "keywords": "distilkobert kobert bert pytorch transformers lightweight", "license": "Apache-2.0", "maintainer": "", "maintainer_email": "", "name": "kobert-transformers", "package_url": "https://pypi.org/project/kobert-transformers/", "platform": "", "project_url": "https://pypi.org/project/kobert-transformers/", "project_urls": {"Homepage": "https://github.com/monologg/DistilKoBERT"}, "release_url": "https://pypi.org/project/kobert-transformers/0.3.0/", "requires_dist": ["torch (>=1.1.0)", "transformers (>=2.2.2)"], "requires_python": ">=3", "summary": "Transformers library for KoBERT, DistilKoBERT", "version": "0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>DistilKoBERT</h1>\n<p>Distillation of KoBERT (<code>SKTBrain KoBERT</code> \uacbd\ub7c9\ud654)</p>\n<p><strong>January 27th, 2020 - Update</strong>: 10GB\uc758 Corpus\ub97c \uac00\uc9c0\uace0 \uc0c8\ub85c \ud559\uc2b5\ud558\uc600\uc2b5\ub2c8\ub2e4. Subtask\uc5d0\uc11c \uc131\ub2a5\uc774 \uc18c\ud3ed \uc0c1\uc2b9\ud588\uc2b5\ub2c8\ub2e4.</p>\n<h2>Pretraining DistilKoBERT</h2>\n<ul>\n<li>\uae30\uc874\uc758 12 layer\ub97c <strong>3 layer</strong>\ub85c \uc904\uc600\uc73c\uba70, \uae30\ud0c0 configuration\uc740 kobert\ub97c \uadf8\ub300\ub85c \ub530\ub790\uc2b5\ub2c8\ub2e4.\n<ul>\n<li><a href=\"https://arxiv.org/abs/1910.01108\" rel=\"nofollow\">\uc6d0 \ub17c\ubb38</a>\uc740 6 layer\ub97c \ucc44\ud0dd\ud558\uc600\uc2b5\ub2c8\ub2e4.</li>\n</ul>\n</li>\n<li>Layer \ucd08\uae30\ud654\uc758 \uacbd\uc6b0 \uae30\uc874 KoBERT\uc758 1, 5, 9\ubc88\uc9f8 layer \uac12\uc744 \uadf8\ub300\ub85c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.</li>\n<li>Pretraining Corpus\ub294 \ud55c\uad6d\uc5b4 \uc704\ud0a4, \ub098\ubb34\uc704\ud0a4, \ub274\uc2a4 \ub4f1 \uc57d 10GB\uc758 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud588\uc73c\uba70, 3 epoch \ud559\uc2b5\ud558\uc600\uc2b5\ub2c8\ub2e4.</li>\n</ul>\n<h2>KoBERT / DistilKoBERT for transformers library</h2>\n<ul>\n<li>\uae30\uc874\uc758 KoBERT\ub97c transformers \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c \uace7\ubc14\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ub9de\ucdc4\uc2b5\ub2c8\ub2e4.\n<ul>\n<li>transformers v2.2.2\ubd80\ud130 \uac1c\uc778\uc774 \ub9cc\ub4e0 \ubaa8\ub378\uc744 transformers\ub97c \ud1b5\ud574 \uc9c1\uc811 \uc5c5\ub85c\ub4dc/\ub2e4\uc6b4\ub85c\ub4dc\ud558\uc5ec \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4</li>\n<li>DistilKoBERT \uc5ed\uc2dc transformers \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c \uace7\ubc14\ub85c \ub2e4\uc6b4 \ubc1b\uc544\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li>\n</ul>\n</li>\n</ul>\n<h3>Dependencies</h3>\n<ul>\n<li>torch&gt;=1.1.0</li>\n<li>transformers&gt;=2.2.2</li>\n<li>sentencepiece&gt;=0.1.82</li>\n</ul>\n<h3>How to Use</h3>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">transformers</span> <span class=\"kn\">import</span> <span class=\"n\">BertModel</span><span class=\"p\">,</span> <span class=\"n\">DistilBertModel</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">bert_model</span> <span class=\"o\">=</span> <span class=\"n\">BertModel</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'monologg/kobert'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">distilbert_model</span> <span class=\"o\">=</span> <span class=\"n\">DistilBertModel</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'monologg/distilkobert'</span><span class=\"p\">)</span>\n</pre>\n<ul>\n<li>Tokenizer\ub97c \uc0ac\uc6a9\ud558\ub824\uba74, \ub8e8\ud2b8 \ub514\ub809\ud1a0\ub9ac\uc758 <code>tokenization_kobert.py</code> \ud30c\uc77c\uc744 \ubcf5\uc0ac\ud55c \ud6c4, <code>KoBertTokenizer</code>\ub97c \uc784\ud3ec\ud2b8\ud558\uba74 \ub429\ub2c8\ub2e4.\n<ul>\n<li>KoBERT\uc640 DistilKoBERT \ubaa8\ub450 \ub3d9\uc77c\ud55c \ud1a0\ud06c\ub098\uc774\uc800\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.</li>\n<li><strong>\uae30\uc874 KoBERT\uc758 \uacbd\uc6b0 Special Token\uc774 \uc81c\ub300\ub85c \ubd84\ub9ac\ub418\uc9c0 \uc54a\ub294 \uc774\uc288\uac00 \uc788\uc5b4\uc11c \ud574\ub2f9 \ubd80\ubd84\uc744 \uc218\uc815\ud558\uc5ec \ubc18\uc601\ud558\uc600\uc2b5\ub2c8\ub2e4.</strong> (<a href=\"https://github.com/SKTBrain/KoBERT/issues/11\" rel=\"nofollow\">Issue link</a>)</li>\n</ul>\n</li>\n</ul>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">tokenization_kobert</span> <span class=\"kn\">import</span> <span class=\"n\">KoBertTokenizer</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">KoBertTokenizer</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'monologg/kobert'</span><span class=\"p\">)</span> <span class=\"c1\"># monologg/distilkobert\ub3c4 \ub3d9\uc77c</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"s2\">\"[CLS] \ud55c\uad6d\uc5b4 \ubaa8\ub378\uc744 \uacf5\uc720\ud569\ub2c8\ub2e4. [SEP]\"</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"s1\">'[CLS]'</span><span class=\"p\">,</span> <span class=\"s1\">'\u2581\ud55c\uad6d'</span><span class=\"p\">,</span> <span class=\"s1\">'\uc5b4'</span><span class=\"p\">,</span> <span class=\"s1\">'\u2581\ubaa8\ub378'</span><span class=\"p\">,</span> <span class=\"s1\">'\uc744'</span><span class=\"p\">,</span> <span class=\"s1\">'\u2581\uacf5\uc720'</span><span class=\"p\">,</span> <span class=\"s1\">'\ud569\ub2c8\ub2e4'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">,</span> <span class=\"s1\">'[SEP]'</span><span class=\"p\">]</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">convert_tokens_to_ids</span><span class=\"p\">([</span><span class=\"s1\">'[CLS]'</span><span class=\"p\">,</span> <span class=\"s1\">'\u2581\ud55c\uad6d'</span><span class=\"p\">,</span> <span class=\"s1\">'\uc5b4'</span><span class=\"p\">,</span> <span class=\"s1\">'\u2581\ubaa8\ub378'</span><span class=\"p\">,</span> <span class=\"s1\">'\uc744'</span><span class=\"p\">,</span> <span class=\"s1\">'\u2581\uacf5\uc720'</span><span class=\"p\">,</span> <span class=\"s1\">'\ud569\ub2c8\ub2e4'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">,</span> <span class=\"s1\">'[SEP]'</span><span class=\"p\">])</span>\n<span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">4958</span><span class=\"p\">,</span> <span class=\"mi\">6855</span><span class=\"p\">,</span> <span class=\"mi\">2046</span><span class=\"p\">,</span> <span class=\"mi\">7088</span><span class=\"p\">,</span> <span class=\"mi\">1050</span><span class=\"p\">,</span> <span class=\"mi\">7843</span><span class=\"p\">,</span> <span class=\"mi\">54</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">]</span>\n</pre>\n<h2>What is different between BERT and DistilBERT</h2>\n<ul>\n<li>\n<p>DistilBert\ub294 \uae30\uc874\uc758 Bert\uc640 \ub2ec\ub9ac token-type embedding\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.</p>\n<ul>\n<li>Transformers \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 DistilBertModel\uc744 \uc0ac\uc6a9\ud560 \ub54c \uae30\uc874 BertModel \uacfc \ub2ec\ub9ac <code>token_type_ids</code>\ub97c \ub123\uc744 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.</li>\n</ul>\n</li>\n<li>\n<p>\ub610\ud55c DistilBert\ub294 pooler\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.</p>\n<ul>\n<li>\uace0\ub85c \uae30\uc874 BertModel\uc758 \uacbd\uc6b0 forward\uc758 return \uac12\uc73c\ub85c <code>sequence_output, pooled_output, (hidden_states), (attentions)</code>\uc744 \ubf51\uc544\ub0b4\uc9c0\ub9cc, DistilBertModel\uc758 \uacbd\uc6b0 <code>sequence_output, (hidden_states), (attentions)</code>\ub97c \ubf51\uc544\ub0c5\ub2c8\ub2e4.</li>\n<li>DistilBert\uc5d0\uc11c <code>[CLS]</code> \ud1a0\ud070\uc744 \ubf51\uc544\ub0b4\ub824\uba74 <code>sequence_output[0][:, 0]</code>\ub97c \uc801\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4.</li>\n</ul>\n</li>\n</ul>\n<h2>Kobert-Transformers python library</h2>\n<p><a href=\"https://pypi.org/project/kobert-transformers/\" rel=\"nofollow\"><img alt=\"Release\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a50b3745e9f759eb46a3241460fbfda75a7836e5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72656c656173652d76302e322e312d677265656e\"></a>\n<a href=\"https://pepy.tech/project/kobert-transformers\" rel=\"nofollow\"><img alt=\"Downloads\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/289879216301bc3ab29821bd89103300d87881a7/68747470733a2f2f706570792e746563682f62616467652f6b6f626572742d7472616e73666f726d657273\"></a>\n<a href=\"https://github.com/monologg/DistilKoBERT/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"license\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f60dbae5f8c05fef9aa4a38e32013bb9495803df/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322e302d726564\"></a></p>\n<ul>\n<li>tokenization_kobert.py\ub97c \ub7a9\ud551\ud55c \ud30c\uc774\uc36c \ub77c\uc774\ube0c\ub7ec\ub9ac</li>\n<li>KoBERT, DistilKoBERT\ub97c Huggingface Transformers \ub77c\uc774\ube0c\ub7ec\ub9ac \ud615\ud0dc\ub85c \uc784\ud3ec\ud2b8</li>\n</ul>\n<h3>Install Kobert-Transformers</h3>\n<pre>$ pip3 install kobert-transformers\n</pre>\n<h3>How to Use</h3>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">kobert_transformers</span> <span class=\"kn\">import</span> <span class=\"n\">get_distilkobert_model</span><span class=\"p\">,</span> <span class=\"n\">get_kobert_model</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">get_distilkobert_model</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">input_ids</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">LongTensor</span><span class=\"p\">([[</span><span class=\"mi\">31</span><span class=\"p\">,</span> <span class=\"mi\">51</span><span class=\"p\">,</span> <span class=\"mi\">99</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]])</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">attention_mask</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">LongTensor</span><span class=\"p\">([[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]])</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">last_layer_hidden_state</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_ids</span><span class=\"p\">,</span> <span class=\"n\">attention_mask</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">last_layer_hidden_state</span>\n<span class=\"n\">tensor</span><span class=\"p\">([[[</span><span class=\"o\">-</span><span class=\"mf\">0.4294</span><span class=\"p\">,</span>  <span class=\"mf\">0.1849</span><span class=\"p\">,</span>  <span class=\"mf\">0.2622</span><span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.8856</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.0617</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.0664</span><span class=\"p\">],</span>\n         <span class=\"p\">[</span> <span class=\"mf\">0.0580</span><span class=\"p\">,</span>  <span class=\"mf\">0.2065</span><span class=\"p\">,</span>  <span class=\"mf\">0.1131</span><span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.9954</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">1.2588</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.1635</span><span class=\"p\">],</span>\n         <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">0.3945</span><span class=\"p\">,</span>  <span class=\"mf\">0.0641</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.2223</span><span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.9819</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.9723</span><span class=\"p\">,</span>  <span class=\"mf\">0.0929</span><span class=\"p\">]],</span>\n\n        <span class=\"p\">[[</span> <span class=\"mf\">0.1698</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.2389</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.0153</span><span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.0329</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.0892</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.0428</span><span class=\"p\">],</span>\n         <span class=\"p\">[</span> <span class=\"mf\">0.1348</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.5269</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.2861</span><span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.6471</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.6776</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.2948</span><span class=\"p\">],</span>\n         <span class=\"p\">[</span> <span class=\"mf\">0.0655</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.4104</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.0467</span><span class=\"p\">,</span>  <span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.5906</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.6362</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">0.0361</span><span class=\"p\">]]],</span>\n       <span class=\"n\">grad_fn</span><span class=\"o\">=&lt;</span><span class=\"n\">AddcmulBackward</span><span class=\"o\">&gt;</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">kobert_transformers</span> <span class=\"kn\">import</span> <span class=\"n\">get_tokenizer</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">get_tokenizer</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"s2\">\"[CLS] \ud55c\uad6d\uc5b4 \ubaa8\ub378\uc744 \uacf5\uc720\ud569\ub2c8\ub2e4. [SEP]\"</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"s1\">'[CLS]'</span><span class=\"p\">,</span> <span class=\"s1\">'\u2581\ud55c\uad6d'</span><span class=\"p\">,</span> <span class=\"s1\">'\uc5b4'</span><span class=\"p\">,</span> <span class=\"s1\">'\u2581\ubaa8\ub378'</span><span class=\"p\">,</span> <span class=\"s1\">'\uc744'</span><span class=\"p\">,</span> <span class=\"s1\">'\u2581\uacf5\uc720'</span><span class=\"p\">,</span> <span class=\"s1\">'\ud569\ub2c8\ub2e4'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">,</span> <span class=\"s1\">'[SEP]'</span><span class=\"p\">]</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">convert_tokens_to_ids</span><span class=\"p\">([</span><span class=\"s1\">'[CLS]'</span><span class=\"p\">,</span> <span class=\"s1\">'\u2581\ud55c\uad6d'</span><span class=\"p\">,</span> <span class=\"s1\">'\uc5b4'</span><span class=\"p\">,</span> <span class=\"s1\">'\u2581\ubaa8\ub378'</span><span class=\"p\">,</span> <span class=\"s1\">'\uc744'</span><span class=\"p\">,</span> <span class=\"s1\">'\u2581\uacf5\uc720'</span><span class=\"p\">,</span> <span class=\"s1\">'\ud569\ub2c8\ub2e4'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">,</span> <span class=\"s1\">'[SEP]'</span><span class=\"p\">])</span>\n<span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">4958</span><span class=\"p\">,</span> <span class=\"mi\">6855</span><span class=\"p\">,</span> <span class=\"mi\">2046</span><span class=\"p\">,</span> <span class=\"mi\">7088</span><span class=\"p\">,</span> <span class=\"mi\">1050</span><span class=\"p\">,</span> <span class=\"mi\">7843</span><span class=\"p\">,</span> <span class=\"mi\">54</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">]</span>\n</pre>\n<h2>Result on Sub-task</h2>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>KoBERT</th>\n<th>DistilKoBERT</th>\n<th>Bert-multilingual</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Model Size (MB)</td>\n<td>351</td>\n<td>108</td>\n<td>681</td>\n</tr>\n<tr>\n<td><strong>NSMC</strong> (acc)</td>\n<td>89.63</td>\n<td>88.41</td>\n<td>87.07</td>\n</tr>\n<tr>\n<td><strong>Naver NER</strong> (F1)</td>\n<td>84.23</td>\n<td>82.14</td>\n<td>81.78</td>\n</tr>\n<tr>\n<td><strong>KorQuAD</strong> (EM/F1)</td>\n<td>TBD</td>\n<td>TBD</td>\n<td>77.04/87.85</td>\n</tr></tbody></table>\n<ul>\n<li>NSMC (Naver Sentiment Movie Corpus) (<a href=\"https://github.com/monologg/KoBERT-nsmc\" rel=\"nofollow\">Implementation of KoBERT-nsmc</a>)</li>\n<li>Naver NER (NER task on Naver NLP Challenge 2018) (<a href=\"https://github.com/monologg/KoBERT-NER\" rel=\"nofollow\">Implementation of KoBERT-NER</a>)</li>\n</ul>\n<h2>Reference</h2>\n<ul>\n<li><a href=\"https://github.com/SKTBrain/KoBERT\" rel=\"nofollow\">KoBERT</a></li>\n<li><a href=\"https://github.com/huggingface/transformers\" rel=\"nofollow\">Huggingface Transformers</a></li>\n<li><a href=\"https://github.com/huggingface/transformers/blob/master/examples/distillation/README.md\" rel=\"nofollow\">DistilBERT Github</a></li>\n<li><a href=\"https://arxiv.org/abs/1910.01108\" rel=\"nofollow\">DistilBERT Paper</a></li>\n<li><a href=\"https://medium.com/daangn/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9C%BC%EB%A1%9C-%EB%8F%99%EB%84%A4%EC%83%9D%ED%99%9C-%EA%B2%8C%EC%8B%9C%EA%B8%80-%ED%95%84%ED%84%B0%EB%A7%81%ED%95%98%EA%B8%B0-263cfe4bc58d\" rel=\"nofollow\">\ub525\ub7ec\ub2dd\uc73c\ub85c \ub3d9\ub124\uc0dd\ud65c \uac8c\uc2dc\uae00 \ud544\ud130\ub9c1\ud558\uae30</a></li>\n</ul>\n\n          </div>"}, "last_serial": 7008764, "releases": {"0.3.0": [{"comment_text": "", "digests": {"md5": "23ba06e19f4ef512ece6de6f78797331", "sha256": "ff462640acb7b4a3637d2f2afb8bfd842c59bbc3053a4ca040bc3ef12f697d33"}, "downloads": -1, "filename": "kobert_transformers-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "23ba06e19f4ef512ece6de6f78797331", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 8480, "upload_time": "2020-02-12T17:24:48", "upload_time_iso_8601": "2020-02-12T17:24:48.157686Z", "url": "https://files.pythonhosted.org/packages/d1/e4/2090062beab003460f7109af611aa330359e60ad676ae350486e55c13806/kobert_transformers-0.3.0-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "23ba06e19f4ef512ece6de6f78797331", "sha256": "ff462640acb7b4a3637d2f2afb8bfd842c59bbc3053a4ca040bc3ef12f697d33"}, "downloads": -1, "filename": "kobert_transformers-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "23ba06e19f4ef512ece6de6f78797331", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 8480, "upload_time": "2020-02-12T17:24:48", "upload_time_iso_8601": "2020-02-12T17:24:48.157686Z", "url": "https://files.pythonhosted.org/packages/d1/e4/2090062beab003460f7109af611aa330359e60ad676ae350486e55c13806/kobert_transformers-0.3.0-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 00:49:09 2020"}