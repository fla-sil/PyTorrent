{"info": {"author": "shobrook", "author_email": "shobrookj@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# sequitur\n\n`sequitur` is a Recurrent Autoencoder (RAE) for sequence data that works out-of-the-box. It's easy to configure and only takes one line of code to use.\n\n```python\nfrom sequitur import QuickEncode\n\nsequences = [[1,2,3,4], [5,6,7,8], [9,10,11,12]]\nencoder, decoder, embeddings, f_loss  = QuickEncode(sequences, embedding_dim=2)\n\nencoder([13,14,15,16]) # => [0.19, 0.84]\n```\n\n<!--An autoencoder will learn how to-->`sequitur` will learn how to represent sequences of any length as lower-dimensional, fixed-size vectors. This can be useful for finding patterns among sequences, clustering, converting sequences into inputs for a machine learning algorithm, and dimensionality reduction.\n\n## Installation\n\n> Requires Python 3.X and PyTorch 1.2.X\n\nYou can download a compiled binary [here](https://github.com/shobrook/sequitur/) or install `sequitur` with pip:\n\n`$ pip install sequitur`\n\n## API\n\n#### `sequitur.QuickEncode(sequences, embedding_dim, logging=False, lr=1e-3, epochs=100)`\n\nLets you train an autoencoder with just one line of code. This wraps a PyTorch implementation of an Encoder-Decoder architecture with an LSTM, making this optimal for sequences with long-term dependencies (e.g. time series data).\n\n**Parameters**\n\n- `sequences`: A list (or tensor) of shape `[num_seqs, seq_len, num_features]` representing your training set of sequences.\n  - Each sequence should have the same length, `seq_len`, and contain a sequence of vectors of size `num_features`.\n  - If `num_features=1`, then you can input a list of shape `[num_seqs, seq_len]` instead.\n- `embedding_dim`: Size of the vector encodings you want to create.\n- `logging`: Boolean for whether you want logging statements to be printed during training.\n- `lr`: Learning rate for the autoencoder.\n- `epochs`: Number of epochs to train for.\n\n**Returns**\n\n- `encoder`: The trained encoder as a PyTorch module.\n  - Takes as input a tensor of shape `[seq_len, num_features]` representing a sequence where each element is a vector of size `num_features`.\n- `decoder`: The trained decoder as a PyTorch module.\n  - Takes as input a tensor of shape `[embedding_dim]` representing an encoded sequence.\n- `embeddings`: A tensor of shape `[num_seqs, embedding_dim]` which holds the learned vector encodings of each sequence in the training set.\n- `f_loss`: The final mean squared error of the autoencoder on the training set.\n\n#### `sequitur.autoencoders.RAE(hyperparams)`\n\nTo-Do.\n\n#### `sequitur.autoencoders.SAE(hyperparams)`\n\nTo-Do.\n\n### `sequitur.autoencoders.VAE(hyperparams)`\n\nTo-Do.\n\n## Contributing\n\n`QuickEncode` is useful for rapid prototyping but doesn't give you much control over the model and training process. For that, you can import the RAE implementation itself from `sequitur.autoencoders`.\n\n`sequitur` not only implements an RAE but also a Stacked Autoencoder (SAE) and a WIP Variational Autoencoder (VAE). If you've implemented a sequence autoencoder, or know of an implementation, please feel free to add it to the codebase and open a pull request. With enough autoencoders, I can turn `sequitur` into a small PyTorch extension library.\n\n<!--Provide proof that it's generally effective-->\n\n<!-- https://github.com/szagoruyko/pytorchviz\nhttps://github.com/RobRomijnders/AE_ts\nhttps://github.com/erickrf/autoencoder\nhttps://miro.medium.com/max/1400/1*sWc8g2yiQrOzntbVeGzbEQ.png\nhttps://arxiv.org/pdf/1502.04681.pdf -->", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/shobrook/sequitur", "keywords": "sequitur,autoencoder,lstm,sequence,sequence-data,seq2seq", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "sequitur", "package_url": "https://pypi.org/project/sequitur/", "platform": "", "project_url": "https://pypi.org/project/sequitur/", "project_urls": {"Homepage": "https://github.com/shobrook/sequitur"}, "release_url": "https://pypi.org/project/sequitur/1.0.2/", "requires_dist": null, "requires_python": ">=3", "summary": "Recurrent Autoencoder for sequence data that works out-of-the-box", "version": "1.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>sequitur</h1>\n<p><code>sequitur</code> is a Recurrent Autoencoder (RAE) for sequence data that works out-of-the-box. It's easy to configure and only takes one line of code to use.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sequitur</span> <span class=\"kn\">import</span> <span class=\"n\">QuickEncode</span>\n\n<span class=\"n\">sequences</span> <span class=\"o\">=</span> <span class=\"p\">[[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"mi\">6</span><span class=\"p\">,</span><span class=\"mi\">7</span><span class=\"p\">,</span><span class=\"mi\">8</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">11</span><span class=\"p\">,</span><span class=\"mi\">12</span><span class=\"p\">]]</span>\n<span class=\"n\">encoder</span><span class=\"p\">,</span> <span class=\"n\">decoder</span><span class=\"p\">,</span> <span class=\"n\">embeddings</span><span class=\"p\">,</span> <span class=\"n\">f_loss</span>  <span class=\"o\">=</span> <span class=\"n\">QuickEncode</span><span class=\"p\">(</span><span class=\"n\">sequences</span><span class=\"p\">,</span> <span class=\"n\">embedding_dim</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"n\">encoder</span><span class=\"p\">([</span><span class=\"mi\">13</span><span class=\"p\">,</span><span class=\"mi\">14</span><span class=\"p\">,</span><span class=\"mi\">15</span><span class=\"p\">,</span><span class=\"mi\">16</span><span class=\"p\">])</span> <span class=\"c1\"># =&gt; [0.19, 0.84]</span>\n</pre>\n`sequitur` will learn how to represent sequences of any length as lower-dimensional, fixed-size vectors. This can be useful for finding patterns among sequences, clustering, converting sequences into inputs for a machine learning algorithm, and dimensionality reduction.\n<h2>Installation</h2>\n<blockquote>\n<p>Requires Python 3.X and PyTorch 1.2.X</p>\n</blockquote>\n<p>You can download a compiled binary <a href=\"https://github.com/shobrook/sequitur/\" rel=\"nofollow\">here</a> or install <code>sequitur</code> with pip:</p>\n<p><code>$ pip install sequitur</code></p>\n<h2>API</h2>\n<h4><code>sequitur.QuickEncode(sequences, embedding_dim, logging=False, lr=1e-3, epochs=100)</code></h4>\n<p>Lets you train an autoencoder with just one line of code. This wraps a PyTorch implementation of an Encoder-Decoder architecture with an LSTM, making this optimal for sequences with long-term dependencies (e.g. time series data).</p>\n<p><strong>Parameters</strong></p>\n<ul>\n<li><code>sequences</code>: A list (or tensor) of shape <code>[num_seqs, seq_len, num_features]</code> representing your training set of sequences.\n<ul>\n<li>Each sequence should have the same length, <code>seq_len</code>, and contain a sequence of vectors of size <code>num_features</code>.</li>\n<li>If <code>num_features=1</code>, then you can input a list of shape <code>[num_seqs, seq_len]</code> instead.</li>\n</ul>\n</li>\n<li><code>embedding_dim</code>: Size of the vector encodings you want to create.</li>\n<li><code>logging</code>: Boolean for whether you want logging statements to be printed during training.</li>\n<li><code>lr</code>: Learning rate for the autoencoder.</li>\n<li><code>epochs</code>: Number of epochs to train for.</li>\n</ul>\n<p><strong>Returns</strong></p>\n<ul>\n<li><code>encoder</code>: The trained encoder as a PyTorch module.\n<ul>\n<li>Takes as input a tensor of shape <code>[seq_len, num_features]</code> representing a sequence where each element is a vector of size <code>num_features</code>.</li>\n</ul>\n</li>\n<li><code>decoder</code>: The trained decoder as a PyTorch module.\n<ul>\n<li>Takes as input a tensor of shape <code>[embedding_dim]</code> representing an encoded sequence.</li>\n</ul>\n</li>\n<li><code>embeddings</code>: A tensor of shape <code>[num_seqs, embedding_dim]</code> which holds the learned vector encodings of each sequence in the training set.</li>\n<li><code>f_loss</code>: The final mean squared error of the autoencoder on the training set.</li>\n</ul>\n<h4><code>sequitur.autoencoders.RAE(hyperparams)</code></h4>\n<p>To-Do.</p>\n<h4><code>sequitur.autoencoders.SAE(hyperparams)</code></h4>\n<p>To-Do.</p>\n<h3><code>sequitur.autoencoders.VAE(hyperparams)</code></h3>\n<p>To-Do.</p>\n<h2>Contributing</h2>\n<p><code>QuickEncode</code> is useful for rapid prototyping but doesn't give you much control over the model and training process. For that, you can import the RAE implementation itself from <code>sequitur.autoencoders</code>.</p>\n<p><code>sequitur</code> not only implements an RAE but also a Stacked Autoencoder (SAE) and a WIP Variational Autoencoder (VAE). If you've implemented a sequence autoencoder, or know of an implementation, please feel free to add it to the codebase and open a pull request. With enough autoencoders, I can turn <code>sequitur</code> into a small PyTorch extension library.</p>\n\n\n\n          </div>"}, "last_serial": 5901331, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "10397eb4cd53f7d1a297bae33f4dd3cd", "sha256": "e9a1da1853efa731c85e8e341d312b0c366fd569195d7c2c9bec188cd76ad3e9"}, "downloads": -1, "filename": "sequitur-1.0.0.tar.gz", "has_sig": false, "md5_digest": "10397eb4cd53f7d1a297bae33f4dd3cd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 3436, "upload_time": "2019-09-29T03:01:44", "upload_time_iso_8601": "2019-09-29T03:01:44.175878Z", "url": "https://files.pythonhosted.org/packages/89/a1/d2c01bab2321c43d378fcf07930badf119d0bf7569355f6f3548639a589d/sequitur-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "4917126caa71ce9621577bd543a9964d", "sha256": "36c5103df8bab6561d21c64308c0bbae4e1911ba45e636a3e3f51c88adc84eab"}, "downloads": -1, "filename": "sequitur-1.0.1.tar.gz", "has_sig": false, "md5_digest": "4917126caa71ce9621577bd543a9964d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 3438, "upload_time": "2019-09-29T03:04:32", "upload_time_iso_8601": "2019-09-29T03:04:32.667354Z", "url": "https://files.pythonhosted.org/packages/26/c5/c4249dc500d8ed37a0d6c1ab87c81be83b2f9be71af3b6fb35a58bdc0a81/sequitur-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "16d899abfd714263857d6c47969b8843", "sha256": "dab6b8c551ce4b12e8fdd912e54a10a1de406fd49fd791904179293fb71a9f83"}, "downloads": -1, "filename": "sequitur-1.0.2.tar.gz", "has_sig": false, "md5_digest": "16d899abfd714263857d6c47969b8843", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 3852, "upload_time": "2019-09-29T03:08:06", "upload_time_iso_8601": "2019-09-29T03:08:06.504011Z", "url": "https://files.pythonhosted.org/packages/68/de/6f730755810d3e115b196f345d14b59e014a672602a522975032c06969bf/sequitur-1.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "16d899abfd714263857d6c47969b8843", "sha256": "dab6b8c551ce4b12e8fdd912e54a10a1de406fd49fd791904179293fb71a9f83"}, "downloads": -1, "filename": "sequitur-1.0.2.tar.gz", "has_sig": false, "md5_digest": "16d899abfd714263857d6c47969b8843", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 3852, "upload_time": "2019-09-29T03:08:06", "upload_time_iso_8601": "2019-09-29T03:08:06.504011Z", "url": "https://files.pythonhosted.org/packages/68/de/6f730755810d3e115b196f345d14b59e014a672602a522975032c06969bf/sequitur-1.0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:54:47 2020"}