{"info": {"author": "Matthias Kestenholz", "author_email": "mk@feinheit.ch", "bugtrack_url": null, "classifiers": ["Environment :: Web Environment", "Framework :: Django", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Topic :: Internet :: WWW/HTTP :: Dynamic Content", "Topic :: Software Development", "Topic :: Software Development :: Libraries :: Application Frameworks"], "description": "==========================================\ndjango-spark - Event sourcing and handling\n==========================================\n\n.. image:: https://travis-ci.org/matthiask/django-spark.png?branch=master\n   :target: https://travis-ci.org/matthiask/django-spark\n\nVersion |release|\n\nThis is not supposed to be real documentation; it's more a reminder for\nmyself.\n\nThe idea is that there are event sources and event handlers. Event\nsources may create a stream of ``spark.api.Event`` instances, where each\nevent must have a ``group`` and a ``key``. Additional data may be added\nto the ``Event`` as well. Keys are globally unique -- events with the\nsame key are still only processed exactly once. Groups are used to\ndetermine which handlers handle a certain event.\n\nEvent handlers are functions which are called once per\n``spark.api.Event`` instance if the event's group matches the event\nhandler's regex.\n\n\nSome usage example code\n=======================\n\nGiven a challenge, create events for the challenge (the specifics do not\nmatter):\n\n.. code-block:: python\n\n    from datetime import date\n    from spark import api\n\n    def events_from_challenge(challenge):\n        if not challenge.is_active:\n            return\n\n        yield {\n            \"group\": 'challenge_created',\n            \"key\": 'challenge_created_%s' % challenge.pk,\n            \"context\": {\"challenge\": challenge},\n        }\n\n        if (date.today() - challenge.start_date).days > 2:\n            if challenge.donations.count() < 2:\n                yield {\n                    \"group\": 'challenge_inactivity_2d',\n                    \"key\": 'challenge_inactivity_2d_%s' % challenge.pk,\n                    \"context\": {\"challenge\": challenge},\n                }\n\n        if (challenge.end_date - date.today()).days <= 2:\n            yield {\n                \"group\": 'challenge_ends_2d',\n                \"key\": 'challenge_ends_2d_%s' % challenge.pk,\n                \"context\": {\"challenge\": challenge},\n            }\n\n        if challenge.end_date < date.today():\n            yield {\n                \"group\": 'challenge_ended',\n                \"key\": 'challenge_ended_%s' % challenge.pk,\n                \"context\": {\"challenge\": challenge},\n            }\n\n\nSend mails related to challenges (uses django-authlib's\n``render_to_mail``):\n\n.. code-block:: python\n\n    from authlib.email import render_to_mail\n\n    def send_challenge_mails(event):\n        challenge = event[\"context\"][\"challenge\"]\n        render_to_mail(\n            # Different mail text per event group:\n            \"challenges/mails/%s\" % event[\"group\"],\n            {\n                \"challenge\": challenge,\n            },\n            to=[challenge.user.email],\n        ).send(fail_silently=True)\n\n\nRegister the handlers:\n\n.. code-block:: python\n\n    class ChallengesConfig(AppConfig):\n        def ready(self):\n            # Prevent circular imports:\n            from spark import api\n\n            api.register_group_handler(\n                handler=send_challenge_mails,\n                group=r'^challenge',\n            )\n\n            Challenge = self.get_model('Challenge')\n\n            # All this does right now is register a post_save signal\n            # handler which runs the challenge instance through\n            # events_from_challenge:\n            api.register_model_event_source(\n                sender=Challenge,\n                source=events_from_challenge,\n            )\n\n\nNow, events are generated and handled directly in process.\nAlternatively, you might want to handle events outside the\nrequest-response cycle. This can be achieved by only registering the\nmodel event source e.g. in a management command, and then sending all\nmodel instances through all event sources, and directly processing those\nevents, for example like this:\n\n.. code-block:: python\n\n    from spark import api\n\n    api.register_model_event_source(...)\n\n    # Copied from the process_spark_sources management command inside\n    # this repository\n    for model, sources in api.MODEL_SOURCES.items():\n        for instance in model.objects.all():\n            for source in sources:\n                api.process_events(api.only_new_events(source(instance)))\n\n\n- `Documentation <https://django-spark.readthedocs.io>`_\n- `Github <https://github.com/matthiask/django-spark/>`_\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/matthiask/django-spark/", "keywords": "", "license": "BSD License", "maintainer": "", "maintainer_email": "", "name": "django-spark", "package_url": "https://pypi.org/project/django-spark/", "platform": "OS Independent", "project_url": "https://pypi.org/project/django-spark/", "project_urls": {"Homepage": "http://github.com/matthiask/django-spark/"}, "release_url": "https://pypi.org/project/django-spark/0.3.0/", "requires_dist": null, "requires_python": "", "summary": "Event sourcing and handling", "version": "0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            ==========================================<br>django-spark - Event sourcing and handling<br>==========================================<br><br>.. image:: https://travis-ci.org/matthiask/django-spark.png?branch=master<br>   :target: https://travis-ci.org/matthiask/django-spark<br><br>Version |release|<br><br>This is not supposed to be real documentation; it's more a reminder for<br>myself.<br><br>The idea is that there are event sources and event handlers. Event<br>sources may create a stream of ``spark.api.Event`` instances, where each<br>event must have a ``group`` and a ``key``. Additional data may be added<br>to the ``Event`` as well. Keys are globally unique -- events with the<br>same key are still only processed exactly once. Groups are used to<br>determine which handlers handle a certain event.<br><br>Event handlers are functions which are called once per<br>``spark.api.Event`` instance if the event's group matches the event<br>handler's regex.<br><br><br>Some usage example code<br>=======================<br><br>Given a challenge, create events for the challenge (the specifics do not<br>matter):<br><br>.. code-block:: python<br><br>    from datetime import date<br>    from spark import api<br><br>    def events_from_challenge(challenge):<br>        if not challenge.is_active:<br>            return<br><br>        yield {<br>            \"group\": 'challenge_created',<br>            \"key\": 'challenge_created_%s' % challenge.pk,<br>            \"context\": {\"challenge\": challenge},<br>        }<br><br>        if (date.today() - challenge.start_date).days &gt; 2:<br>            if challenge.donations.count() &lt; 2:<br>                yield {<br>                    \"group\": 'challenge_inactivity_2d',<br>                    \"key\": 'challenge_inactivity_2d_%s' % challenge.pk,<br>                    \"context\": {\"challenge\": challenge},<br>                }<br><br>        if (challenge.end_date - date.today()).days &lt;= 2:<br>            yield {<br>                \"group\": 'challenge_ends_2d',<br>                \"key\": 'challenge_ends_2d_%s' % challenge.pk,<br>                \"context\": {\"challenge\": challenge},<br>            }<br><br>        if challenge.end_date &lt; date.today():<br>            yield {<br>                \"group\": 'challenge_ended',<br>                \"key\": 'challenge_ended_%s' % challenge.pk,<br>                \"context\": {\"challenge\": challenge},<br>            }<br><br><br>Send mails related to challenges (uses django-authlib's<br>``render_to_mail``):<br><br>.. code-block:: python<br><br>    from authlib.email import render_to_mail<br><br>    def send_challenge_mails(event):<br>        challenge = event[\"context\"][\"challenge\"]<br>        render_to_mail(<br>            # Different mail text per event group:<br>            \"challenges/mails/%s\" % event[\"group\"],<br>            {<br>                \"challenge\": challenge,<br>            },<br>            to=[challenge.user.email],<br>        ).send(fail_silently=True)<br><br><br>Register the handlers:<br><br>.. code-block:: python<br><br>    class ChallengesConfig(AppConfig):<br>        def ready(self):<br>            # Prevent circular imports:<br>            from spark import api<br><br>            api.register_group_handler(<br>                handler=send_challenge_mails,<br>                group=r'^challenge',<br>            )<br><br>            Challenge = self.get_model('Challenge')<br><br>            # All this does right now is register a post_save signal<br>            # handler which runs the challenge instance through<br>            # events_from_challenge:<br>            api.register_model_event_source(<br>                sender=Challenge,<br>                source=events_from_challenge,<br>            )<br><br><br>Now, events are generated and handled directly in process.<br>Alternatively, you might want to handle events outside the<br>request-response cycle. This can be achieved by only registering the<br>model event source e.g. in a management command, and then sending all<br>model instances through all event sources, and directly processing those<br>events, for example like this:<br><br>.. code-block:: python<br><br>    from spark import api<br><br>    api.register_model_event_source(...)<br><br>    # Copied from the process_spark_sources management command inside<br>    # this repository<br>    for model, sources in api.MODEL_SOURCES.items():<br>        for instance in model.objects.all():<br>            for source in sources:<br>                api.process_events(api.only_new_events(source(instance)))<br><br><br>- `Documentation &lt;https://django-spark.readthedocs.io&gt;`_<br>- `Github &lt;https://github.com/matthiask/django-spark/&gt;`_<br><br><br>\n          </div>"}, "last_serial": 5877932, "releases": {"0.0.0": [], "0.1.0": [{"comment_text": "", "digests": {"md5": "698b003b88e4a5569ef62b86ed656d9b", "sha256": "0572dcef372df9367a5b7e79daa621340a572ef8c2a3cdacff367efdb7fab87a"}, "downloads": -1, "filename": "django_spark-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "698b003b88e4a5569ef62b86ed656d9b", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 6397, "upload_time": "2017-12-19T15:16:46", "upload_time_iso_8601": "2017-12-19T15:16:46.332556Z", "url": "https://files.pythonhosted.org/packages/67/a1/4d1c961ff8164671bf0e40e39cbf33719a9980bc5088e5a908999c62c2be/django_spark-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cda43c4f2356052280ad190efb5695ea", "sha256": "4127ef64f3d4cbcc9a266871126ff230f00261034ca63df8029b1aac65cfa880"}, "downloads": -1, "filename": "django-spark-0.1.0.tar.gz", "has_sig": false, "md5_digest": "cda43c4f2356052280ad190efb5695ea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4454, "upload_time": "2017-12-19T15:16:47", "upload_time_iso_8601": "2017-12-19T15:16:47.521668Z", "url": "https://files.pythonhosted.org/packages/47/ba/6c7dba64896de1b18c3216d6652fcce3734a7d163ae1d7e709bf8de3c95b/django-spark-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "3a2b5f4d44183f7ad81a790cfbc68ac0", "sha256": "d86c4d93d7918caa35a244178348c0fde24511a0257082fb2b6322fdb73051c8"}, "downloads": -1, "filename": "django_spark-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "3a2b5f4d44183f7ad81a790cfbc68ac0", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 9063, "upload_time": "2018-10-16T13:44:31", "upload_time_iso_8601": "2018-10-16T13:44:31.886530Z", "url": "https://files.pythonhosted.org/packages/b7/c3/129662f18e2ae035353052b2c4d11b362f5f313fa373e46912c192ea4b50/django_spark-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "624fd8aaf2c3e0e1733b3fd0e842235d", "sha256": "7a24cbd504137f9acc166c2c869a710bea8959d53fff1c8487483fec3fb19bb2"}, "downloads": -1, "filename": "django-spark-0.2.0.tar.gz", "has_sig": false, "md5_digest": "624fd8aaf2c3e0e1733b3fd0e842235d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5984, "upload_time": "2018-10-16T13:44:33", "upload_time_iso_8601": "2018-10-16T13:44:33.338479Z", "url": "https://files.pythonhosted.org/packages/62/d7/d9a1f0d689a8533b65da5ef75f9f30ecf91d5efe41d9aeffc4f5d6b6a8fa/django-spark-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "438003f2fd086891942f0ccdf740cc9d", "sha256": "bf3bad5b96b6c95383c10af14ed7632209c56e687b9b7436f0124ce804e2fb3c"}, "downloads": -1, "filename": "django_spark-0.2.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "438003f2fd086891942f0ccdf740cc9d", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 13055, "upload_time": "2018-10-26T07:13:45", "upload_time_iso_8601": "2018-10-26T07:13:45.405478Z", "url": "https://files.pythonhosted.org/packages/2e/05/56cf2ec55a71cc805b7a221d9a1324d11e8ddd637ba485d93f10180e7ea5/django_spark-0.2.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "93118c11abcff55dcedce3ecd761991a", "sha256": "5765f36f0cc97b0cb0c191ecd5113d4e62784f7b65298329ae2204c5d3eec2d4"}, "downloads": -1, "filename": "django-spark-0.2.1.tar.gz", "has_sig": false, "md5_digest": "93118c11abcff55dcedce3ecd761991a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10197, "upload_time": "2018-10-26T07:13:47", "upload_time_iso_8601": "2018-10-26T07:13:47.726038Z", "url": "https://files.pythonhosted.org/packages/4e/07/485cc602483f9bfda9a3c2477fe7a01dca025f736946bce3ed6e293c171d/django-spark-0.2.1.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "dc41b9b1ab5ccb6e355dfe4d7beb6586", "sha256": "02033ad7c16c0d94e3e82bfec8e0653c3eb1e69b554a91c8d606ddfbbc0ffff6"}, "downloads": -1, "filename": "django_spark-0.3.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "dc41b9b1ab5ccb6e355dfe4d7beb6586", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 16953, "upload_time": "2018-10-29T09:24:12", "upload_time_iso_8601": "2018-10-29T09:24:12.496140Z", "url": "https://files.pythonhosted.org/packages/bc/87/caccafc969a7c47730df98afc1ede4abaf5cf495844a11725b94679c1ec9/django_spark-0.3.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c678d4b8488ef6d600fb80a7ffff8328", "sha256": "3628469898775786116f7363228dcd4d7c7f6403306a070500972af71f6b4af3"}, "downloads": -1, "filename": "django-spark-0.3.0.tar.gz", "has_sig": false, "md5_digest": "c678d4b8488ef6d600fb80a7ffff8328", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12147, "upload_time": "2018-10-29T09:24:14", "upload_time_iso_8601": "2018-10-29T09:24:14.183590Z", "url": "https://files.pythonhosted.org/packages/a7/7c/45d2b29f270286867896f00c5f4748c41008c1a006859689b9928671ad90/django-spark-0.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "dc41b9b1ab5ccb6e355dfe4d7beb6586", "sha256": "02033ad7c16c0d94e3e82bfec8e0653c3eb1e69b554a91c8d606ddfbbc0ffff6"}, "downloads": -1, "filename": "django_spark-0.3.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "dc41b9b1ab5ccb6e355dfe4d7beb6586", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 16953, "upload_time": "2018-10-29T09:24:12", "upload_time_iso_8601": "2018-10-29T09:24:12.496140Z", "url": "https://files.pythonhosted.org/packages/bc/87/caccafc969a7c47730df98afc1ede4abaf5cf495844a11725b94679c1ec9/django_spark-0.3.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c678d4b8488ef6d600fb80a7ffff8328", "sha256": "3628469898775786116f7363228dcd4d7c7f6403306a070500972af71f6b4af3"}, "downloads": -1, "filename": "django-spark-0.3.0.tar.gz", "has_sig": false, "md5_digest": "c678d4b8488ef6d600fb80a7ffff8328", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12147, "upload_time": "2018-10-29T09:24:14", "upload_time_iso_8601": "2018-10-29T09:24:14.183590Z", "url": "https://files.pythonhosted.org/packages/a7/7c/45d2b29f270286867896f00c5f4748c41008c1a006859689b9928671ad90/django-spark-0.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:53:03 2020"}