{"info": {"author": "junqueira", "author_email": "lcjneto@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# py-pack  \n\nGenerate package egg, that can be attached to Spark clusters in production or included in a PySpark console.\n\n$ python=3.6.1\n\n#test\n\npython setup.py bdist_wheel sdist\ntwine upload dist\\*\n\n### Install dependences\n\tpip install -r requirements.txt\n\n### Create module pypack\n\tpython setup.py install\n\n### Test solutions \n\tpython tests/test_mission.py \n\n### Command line to Generate package egg, your code in a dist/pypack-0.0.1-py3.6.egg \n\tpython setup.py bdist_egg\n\n### Start the PySpark console and attach the egg file.\n\tpyspark --py-files dist/pypack-0.0.1-py3.6.egg\n\n\n### From the PySpark REPL, you can import the pypack code and execute the application code.\n\n+ from pypack.spark import *\n+ from pypack.mission import with_life_goal\n+ source_data = [ (\"jose\", 1), (\"pedro\", 2) ]\n+ source_df = spark.createDataFrame( source_data, [\"name\", \"age\"])\n+ actual_df = with_life_goal(source_df)\n+ actual_df.show()\n\n\n\n### The pypack library can be attached to spark-submit commands for launching applications in a similar manner.\n\n\n\nDATAFLOW\nProcessamento simplificado de dados de stream e em lote, com a mesma confiabilidade e expressividade\nAVALIA\u00c7\u00c3O GRATUITA\nDesenvolvimento mais r\u00e1pido e gerenciamento mais simples\nO Cloud Dataflow \u00e9 um servi\u00e7o totalmente gerenciado para transformar e aprimorar dados nos modos de stream (tempo real) e em lote (do hist\u00f3rico) com a mesma confiabilidade e expressividade. Voc\u00ea n\u00e3o precisa mais encontrar solu\u00e7\u00f5es alternativas complexas. E, com a abordagem sem servidor para o provisionamento e gerenciamento de recursos, voc\u00ea tem acesso a uma capacidade praticamente ilimitada para solucionar seus maiores desafios de processamento de dados, ao mesmo tempo em que paga apenas por aquilo que usa.\n\nO Cloud Dataflow habilita casos de uso transformacionais em v\u00e1rios setores, incluindo:\n\ncheck an\u00e1lise de sequ\u00eancia de cliques, pontos de venda e segmenta\u00e7\u00e3o no varejo\ncheck detec\u00e7\u00e3o de fraude em servi\u00e7os financeiros\ncheck experi\u00eancia do usu\u00e1rio personalizada em jogos\ncheck an\u00e1lises da IoT na ind\u00fastria, em servi\u00e7os de sa\u00fade e em log\u00edstica\nfaster-development-easier-management\nDesenvolvimento acelerado para dados em lote e de stream\nO Cloud Dataflow possibilita o desenvolvimento r\u00e1pido e simplificado de canais por meio das APIs expressivas de Java e Python no SDK do Apache Beam. Ele oferece um conjunto avan\u00e7ado de primitivos de an\u00e1lise de sess\u00e3o e janelas, assim como um ecossistema de conectores de coletor e origem. Al\u00e9m disso, com o modelo de desenvolvimento exclusivo e unificado do Beam, \u00e9 poss\u00edvel reutilizar mais c\u00f3digos nos canais de stream e em lote.\n\naccelerate-development-with-no-compromises\nSimplifique opera\u00e7\u00f5es e gerenciamento\nA abordagem sem servidor do GCP remove a sobrecarga operacional com o processamento autom\u00e1tico de desempenho, escalonabilidade, disponibilidade, seguran\u00e7a e conformidade. Desta forma, os usu\u00e1rios podem se concentrar na programa\u00e7\u00e3o em vez de precisar gerenciar clusters de servidores. A integra\u00e7\u00e3o com o Stackdriver, a solu\u00e7\u00e3o de monitoramento e gera\u00e7\u00e3o de registros unificada do GCP, permite que voc\u00ea fa\u00e7a o monitoramento e resolva problemas nos seus canais enquanto eles est\u00e3o em execu\u00e7\u00e3o. A visualiza\u00e7\u00e3o avan\u00e7ada, a gera\u00e7\u00e3o de registros e o sistema de alertas avan\u00e7ado ajudam voc\u00ea a identificar e a tomar medidas em rela\u00e7\u00e3o a poss\u00edveis problemas.\n\nsimplify-operations-and-management\nDesenvolva a base para o machine learning\nUse o Cloud Dataflow como um elemento de integra\u00e7\u00e3o pr\u00e1tico para incluir a an\u00e1lise preditiva na detec\u00e7\u00e3o de fraude, na personaliza\u00e7\u00e3o em tempo real e em casos de uso semelhantes. Para isso, adicione modelos do Cloud Machine Learning com base no TensorFlow e APIs nos seus canais de processamento de dados.\n\nbuild-on-a-foundation-for-machine-learning\nUse ferramentas que voc\u00ea conhece e prefere\nO Cloud Dataflow se integra perfeitamente aos servi\u00e7os do GCP para o processamento de eventos de streaming (Cloud Pub/Sub), armazenamento de dados (BigQuery), machine learning (Cloud Machine Learning) e muito mais. Com o SDK com base em Beam, os desenvolvedores tamb\u00e9m podem criar extens\u00f5es personalizadas e at\u00e9 mesmo escolher mecanismos de execu\u00e7\u00e3o alternativos, como o Apache Spark, por meio do Cloud Dataproc ou no local. Para os usu\u00e1rios do Apache Kafka, um conector do Cloud Dataflow facilita a integra\u00e7\u00e3o com o GCP.\n\nuse-your-favorite-and-familiar-tools\nTransforma\u00e7\u00e3o de dados com o Cloud Dataflow\ndiagram-dataflow\n\nRECURSOS DO CLOUD DATAFLOW\nGerenciamento de recursos automatizado\nO Cloud Dataflow automatiza o provisionamento e o gerenciamento de recursos em processamento para reduzir a lat\u00eancia e maximizar a utiliza\u00e7\u00e3o. Voc\u00ea n\u00e3o precisa mais executar inst\u00e2ncias manualmente ou reserv\u00e1-las.\nReequil\u00edbrio din\u00e2mico de trabalho\nO particionamento automatizado e otimizado do trabalho reequilibra as atividades atrasadas de maneira din\u00e2mica. N\u00e3o \u00e9 preciso procurar teclas de atalho ou fazer o pr\u00e9-processamento dos seus dados de entrada.\nProcessamento \u00fanico, confi\u00e1vel e consistente\nFornece suporte integrado para a execu\u00e7\u00e3o tolerante a falhas consistente e correta, independentemente do tamanho dos dados, do tamanho do cluster, do padr\u00e3o de processamento e da complexidade do canal.\nEscalonamento autom\u00e1tico horizontal\nEscalonamento autom\u00e1tico horizontal do n\u00famero de workers para alcan\u00e7ar os melhores resultados de capacidade com o melhor custo-benef\u00edcio.\nModelo de programa\u00e7\u00e3o unificado\nO SDK do Apache Beam oferece opera\u00e7\u00f5es avan\u00e7adas, similares ao MapReduce e de igual pot\u00eancia, al\u00e9m de sistema de gest\u00e3o de janelas avan\u00e7ado e controle de corre\u00e7\u00e3o detalhado tanto para os dados de stream quanto em lote.\nInova\u00e7\u00f5es voltadas para a comunidade\nOs desenvolvedores que quiserem estender o modelo de programa\u00e7\u00e3o do Cloud Dataflow podem fazer bifurca\u00e7\u00f5es e/ou contribuir para o Apache Beam.\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/junqueira/py-dataflow", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "py-dataflow", "package_url": "https://pypi.org/project/py-dataflow/", "platform": "", "project_url": "https://pypi.org/project/py-dataflow/", "project_urls": {"Homepage": "http://github.com/junqueira/py-dataflow"}, "release_url": "https://pypi.org/project/py-dataflow/0.0.5/", "requires_dist": ["flask-restful"], "requires_python": "", "summary": "PySpark application", "version": "0.0.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p># py-pack</p>\n<p>Generate package egg, that can be attached to Spark clusters in production or included in a PySpark console.</p>\n<p>$ python=3.6.1</p>\n<p>#test</p>\n<p>python setup.py bdist_wheel sdist\ntwine upload dist*</p>\n<dl>\n<dt>### Install dependences</dt>\n<dd>pip install -r requirements.txt</dd>\n<dt>### Create module pypack</dt>\n<dd>python setup.py install</dd>\n<dt>### Test solutions</dt>\n<dd>python tests/test_mission.py</dd>\n<dt>### Command line to Generate package egg, your code in a dist/pypack-0.0.1-py3.6.egg</dt>\n<dd>python setup.py bdist_egg</dd>\n<dt>### Start the PySpark console and attach the egg file.</dt>\n<dd>pyspark \u2013py-files dist/pypack-0.0.1-py3.6.egg</dd>\n</dl>\n<p>### From the PySpark REPL, you can import the pypack code and execute the application code.</p>\n<ul>\n<li>from pypack.spark import *</li>\n<li>from pypack.mission import with_life_goal</li>\n<li>source_data = [ (\u201cjose\u201d, 1), (\u201cpedro\u201d, 2) ]</li>\n<li>source_df = spark.createDataFrame( source_data, [\u201cname\u201d, \u201cage\u201d])</li>\n<li>actual_df = with_life_goal(source_df)</li>\n<li>actual_df.show()</li>\n</ul>\n<p>### The pypack library can be attached to spark-submit commands for launching applications in a similar manner.</p>\n<p>DATAFLOW\nProcessamento simplificado de dados de stream e em lote, com a mesma confiabilidade e expressividade\nAVALIA\u00c7\u00c3O GRATUITA\nDesenvolvimento mais r\u00e1pido e gerenciamento mais simples\nO Cloud Dataflow \u00e9 um servi\u00e7o totalmente gerenciado para transformar e aprimorar dados nos modos de stream (tempo real) e em lote (do hist\u00f3rico) com a mesma confiabilidade e expressividade. Voc\u00ea n\u00e3o precisa mais encontrar solu\u00e7\u00f5es alternativas complexas. E, com a abordagem sem servidor para o provisionamento e gerenciamento de recursos, voc\u00ea tem acesso a uma capacidade praticamente ilimitada para solucionar seus maiores desafios de processamento de dados, ao mesmo tempo em que paga apenas por aquilo que usa.</p>\n<p>O Cloud Dataflow habilita casos de uso transformacionais em v\u00e1rios setores, incluindo:</p>\n<p>check an\u00e1lise de sequ\u00eancia de cliques, pontos de venda e segmenta\u00e7\u00e3o no varejo\ncheck detec\u00e7\u00e3o de fraude em servi\u00e7os financeiros\ncheck experi\u00eancia do usu\u00e1rio personalizada em jogos\ncheck an\u00e1lises da IoT na ind\u00fastria, em servi\u00e7os de sa\u00fade e em log\u00edstica\nfaster-development-easier-management\nDesenvolvimento acelerado para dados em lote e de stream\nO Cloud Dataflow possibilita o desenvolvimento r\u00e1pido e simplificado de canais por meio das APIs expressivas de Java e Python no SDK do Apache Beam. Ele oferece um conjunto avan\u00e7ado de primitivos de an\u00e1lise de sess\u00e3o e janelas, assim como um ecossistema de conectores de coletor e origem. Al\u00e9m disso, com o modelo de desenvolvimento exclusivo e unificado do Beam, \u00e9 poss\u00edvel reutilizar mais c\u00f3digos nos canais de stream e em lote.</p>\n<p>accelerate-development-with-no-compromises\nSimplifique opera\u00e7\u00f5es e gerenciamento\nA abordagem sem servidor do GCP remove a sobrecarga operacional com o processamento autom\u00e1tico de desempenho, escalonabilidade, disponibilidade, seguran\u00e7a e conformidade. Desta forma, os usu\u00e1rios podem se concentrar na programa\u00e7\u00e3o em vez de precisar gerenciar clusters de servidores. A integra\u00e7\u00e3o com o Stackdriver, a solu\u00e7\u00e3o de monitoramento e gera\u00e7\u00e3o de registros unificada do GCP, permite que voc\u00ea fa\u00e7a o monitoramento e resolva problemas nos seus canais enquanto eles est\u00e3o em execu\u00e7\u00e3o. A visualiza\u00e7\u00e3o avan\u00e7ada, a gera\u00e7\u00e3o de registros e o sistema de alertas avan\u00e7ado ajudam voc\u00ea a identificar e a tomar medidas em rela\u00e7\u00e3o a poss\u00edveis problemas.</p>\n<p>simplify-operations-and-management\nDesenvolva a base para o machine learning\nUse o Cloud Dataflow como um elemento de integra\u00e7\u00e3o pr\u00e1tico para incluir a an\u00e1lise preditiva na detec\u00e7\u00e3o de fraude, na personaliza\u00e7\u00e3o em tempo real e em casos de uso semelhantes. Para isso, adicione modelos do Cloud Machine Learning com base no TensorFlow e APIs nos seus canais de processamento de dados.</p>\n<p>build-on-a-foundation-for-machine-learning\nUse ferramentas que voc\u00ea conhece e prefere\nO Cloud Dataflow se integra perfeitamente aos servi\u00e7os do GCP para o processamento de eventos de streaming (Cloud Pub/Sub), armazenamento de dados (BigQuery), machine learning (Cloud Machine Learning) e muito mais. Com o SDK com base em Beam, os desenvolvedores tamb\u00e9m podem criar extens\u00f5es personalizadas e at\u00e9 mesmo escolher mecanismos de execu\u00e7\u00e3o alternativos, como o Apache Spark, por meio do Cloud Dataproc ou no local. Para os usu\u00e1rios do Apache Kafka, um conector do Cloud Dataflow facilita a integra\u00e7\u00e3o com o GCP.</p>\n<p>use-your-favorite-and-familiar-tools\nTransforma\u00e7\u00e3o de dados com o Cloud Dataflow\ndiagram-dataflow</p>\n<p>RECURSOS DO CLOUD DATAFLOW\nGerenciamento de recursos automatizado\nO Cloud Dataflow automatiza o provisionamento e o gerenciamento de recursos em processamento para reduzir a lat\u00eancia e maximizar a utiliza\u00e7\u00e3o. Voc\u00ea n\u00e3o precisa mais executar inst\u00e2ncias manualmente ou reserv\u00e1-las.\nReequil\u00edbrio din\u00e2mico de trabalho\nO particionamento automatizado e otimizado do trabalho reequilibra as atividades atrasadas de maneira din\u00e2mica. N\u00e3o \u00e9 preciso procurar teclas de atalho ou fazer o pr\u00e9-processamento dos seus dados de entrada.\nProcessamento \u00fanico, confi\u00e1vel e consistente\nFornece suporte integrado para a execu\u00e7\u00e3o tolerante a falhas consistente e correta, independentemente do tamanho dos dados, do tamanho do cluster, do padr\u00e3o de processamento e da complexidade do canal.\nEscalonamento autom\u00e1tico horizontal\nEscalonamento autom\u00e1tico horizontal do n\u00famero de workers para alcan\u00e7ar os melhores resultados de capacidade com o melhor custo-benef\u00edcio.\nModelo de programa\u00e7\u00e3o unificado\nO SDK do Apache Beam oferece opera\u00e7\u00f5es avan\u00e7adas, similares ao MapReduce e de igual pot\u00eancia, al\u00e9m de sistema de gest\u00e3o de janelas avan\u00e7ado e controle de corre\u00e7\u00e3o detalhado tanto para os dados de stream quanto em lote.\nInova\u00e7\u00f5es voltadas para a comunidade\nOs desenvolvedores que quiserem estender o modelo de programa\u00e7\u00e3o do Cloud Dataflow podem fazer bifurca\u00e7\u00f5es e/ou contribuir para o Apache Beam.</p>\n\n          </div>"}, "last_serial": 6480291, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "5588de3ee61b2219a7b41a30e31a0a36", "sha256": "8a3b863d2578a5c51bd17c4f644d7e756d3406dc413a8bf865a38e854b78bfee"}, "downloads": -1, "filename": "py_dataflow-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "5588de3ee61b2219a7b41a30e31a0a36", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 2563, "upload_time": "2019-01-27T20:20:26", "upload_time_iso_8601": "2019-01-27T20:20:26.340706Z", "url": "https://files.pythonhosted.org/packages/76/8c/9f3126314b638e1c3bf700a525705c60c83b775b6c1a91d461d677f76366/py_dataflow-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "447f04d5b9449909b2c560addc3face9", "sha256": "511db0c14f6394a41eb3fba48d88ad89644ff26af3d2530a0feb0adadce19ece"}, "downloads": -1, "filename": "py-dataflow-0.0.1.tar.gz", "has_sig": false, "md5_digest": "447f04d5b9449909b2c560addc3face9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1564, "upload_time": "2019-01-27T20:20:28", "upload_time_iso_8601": "2019-01-27T20:20:28.358913Z", "url": "https://files.pythonhosted.org/packages/f4/d0/3ee99caadf35d3e55fa32a0317e0e3b92637870cf00e69c20c03973dc0cc/py-dataflow-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "9079e45e4567c45f3a24830c84aa3fce", "sha256": "1e7d271cf4ffaedddddc84d2ffc97cc6883bf031d17398dbfd5cc16ffe9075e6"}, "downloads": -1, "filename": "py_dataflow-0.0.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "9079e45e4567c45f3a24830c84aa3fce", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 48964, "upload_time": "2020-01-18T08:19:24", "upload_time_iso_8601": "2020-01-18T08:19:24.291394Z", "url": "https://files.pythonhosted.org/packages/57/09/6318015d232f0c460a77d7d1435349b8ee836740971e4a9d2c2bd46945f2/py_dataflow-0.0.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d108f48da6e60cbc95d5d3923d85e6fa", "sha256": "4be458ba37e44a942f9f2a55c0891b96227db601ae8f8a31822dfd36a22a66d0"}, "downloads": -1, "filename": "py_dataflow-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "d108f48da6e60cbc95d5d3923d85e6fa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 2559, "upload_time": "2019-01-27T20:36:20", "upload_time_iso_8601": "2019-01-27T20:36:20.787533Z", "url": "https://files.pythonhosted.org/packages/0d/41/6e94224e92fdc6828a2a7c6bb67b186b24017ee40c022c8ef85d244e6611/py_dataflow-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "74d7be314651ccab26a6233e6d8595c7", "sha256": "ca2eec5eafbdcc587efba67174d8d9c3f4f6024036c28a9c4044140507ac951b"}, "downloads": -1, "filename": "py-dataflow-0.0.2.tar.gz", "has_sig": false, "md5_digest": "74d7be314651ccab26a6233e6d8595c7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3691, "upload_time": "2019-01-27T20:36:21", "upload_time_iso_8601": "2019-01-27T20:36:21.924567Z", "url": "https://files.pythonhosted.org/packages/24/c0/d4c31edef894d5e33777806038c61db961d775cc5bfdea710bebb41ff40a/py-dataflow-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "be5d040b6fd71192d4e478224826b33f", "sha256": "7129453f51d9ce455c311abc4a93379ad9373264c5e8c53b16d947440c17ed3c"}, "downloads": -1, "filename": "py_dataflow-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "be5d040b6fd71192d4e478224826b33f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 48960, "upload_time": "2020-01-18T08:25:04", "upload_time_iso_8601": "2020-01-18T08:25:04.424311Z", "url": "https://files.pythonhosted.org/packages/29/51/8ac38969189efae0f69873d0b05427b222a5e24ac8debda196050f0c5183/py_dataflow-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f8fe8d52be95e57875b5e298a6764663", "sha256": "0846a086678e1122a3fa1585cbfd37bf0f5860863e579772408a8ad789d43cb7"}, "downloads": -1, "filename": "py-dataflow-0.0.3.tar.gz", "has_sig": false, "md5_digest": "f8fe8d52be95e57875b5e298a6764663", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24123, "upload_time": "2020-01-18T08:25:05", "upload_time_iso_8601": "2020-01-18T08:25:05.952770Z", "url": "https://files.pythonhosted.org/packages/1e/c7/8b350bba55161525c9b33950884a9f3b9b8bb7ad88d5f01504b8eaf954ae/py-dataflow-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "318ca1374ebdc0ca2ad622694e66b2b6", "sha256": "29e38e68114d25c51c0b982c6f2f02eb1fa42db618a97c3ec76026ba218f3f7a"}, "downloads": -1, "filename": "py_dataflow-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "318ca1374ebdc0ca2ad622694e66b2b6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 51493, "upload_time": "2020-01-18T08:48:54", "upload_time_iso_8601": "2020-01-18T08:48:54.209897Z", "url": "https://files.pythonhosted.org/packages/b9/d8/00a2dad4e52f9002a2572f75ff5569d58f941f5937b976e3220c40088f21/py_dataflow-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6e573555f91edfeef8b588da16afbc28", "sha256": "82276c4354d63b38119e17f27c9d759ea9b7ff3ecba9b1a992a5666854200dff"}, "downloads": -1, "filename": "py-dataflow-0.0.4.tar.gz", "has_sig": false, "md5_digest": "6e573555f91edfeef8b588da16afbc28", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5289, "upload_time": "2020-01-18T08:48:55", "upload_time_iso_8601": "2020-01-18T08:48:55.806514Z", "url": "https://files.pythonhosted.org/packages/25/c7/05fb8610336d3ac93284e034ab001ffdb69b458338b0d8e1f75453416626/py-dataflow-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "5c49d3d4947679c281f6e7b6eb487d1e", "sha256": "ea8a68e765f19afcc066da379a52ed01fde0fc8eb95d639a5a36fc963338b0f9"}, "downloads": -1, "filename": "py_dataflow-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "5c49d3d4947679c281f6e7b6eb487d1e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 52186, "upload_time": "2020-01-19T01:55:51", "upload_time_iso_8601": "2020-01-19T01:55:51.096630Z", "url": "https://files.pythonhosted.org/packages/f9/82/931b208354e0c40ccb1f8d1c000f74eb2112a3ed9f4375488d31a8d02e2d/py_dataflow-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f0f78fc1f0c7d0423c08cde97db1462e", "sha256": "39ff5e36894ea0657ce9afabd6b3ba24c43b5ceb1e2bba6c63c0ce40eaa60fe0"}, "downloads": -1, "filename": "py-dataflow-0.0.5.tar.gz", "has_sig": false, "md5_digest": "f0f78fc1f0c7d0423c08cde97db1462e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13159, "upload_time": "2020-01-19T01:55:52", "upload_time_iso_8601": "2020-01-19T01:55:52.845895Z", "url": "https://files.pythonhosted.org/packages/6a/28/5681d1e789d7732f8d725f379f0c5dce380cf06d663c0e0633fef09e7f25/py-dataflow-0.0.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "5c49d3d4947679c281f6e7b6eb487d1e", "sha256": "ea8a68e765f19afcc066da379a52ed01fde0fc8eb95d639a5a36fc963338b0f9"}, "downloads": -1, "filename": "py_dataflow-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "5c49d3d4947679c281f6e7b6eb487d1e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 52186, "upload_time": "2020-01-19T01:55:51", "upload_time_iso_8601": "2020-01-19T01:55:51.096630Z", "url": "https://files.pythonhosted.org/packages/f9/82/931b208354e0c40ccb1f8d1c000f74eb2112a3ed9f4375488d31a8d02e2d/py_dataflow-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f0f78fc1f0c7d0423c08cde97db1462e", "sha256": "39ff5e36894ea0657ce9afabd6b3ba24c43b5ceb1e2bba6c63c0ce40eaa60fe0"}, "downloads": -1, "filename": "py-dataflow-0.0.5.tar.gz", "has_sig": false, "md5_digest": "f0f78fc1f0c7d0423c08cde97db1462e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13159, "upload_time": "2020-01-19T01:55:52", "upload_time_iso_8601": "2020-01-19T01:55:52.845895Z", "url": "https://files.pythonhosted.org/packages/6a/28/5681d1e789d7732f8d725f379f0c5dce380cf06d663c0e0633fef09e7f25/py-dataflow-0.0.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:07:23 2020"}