{"info": {"author": "Tianshu Huang", "author_email": "thetianshuhuang@gmail.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Information Analysis", "Topic :: Scientific/Engineering :: Mathematics"], "description": "# Bayesian Markov Chain Monte Carlo Clustering\n\nImplementation of Markov Chain Monte Carlo Bayesian Clustering techniques, including DPM (Dirichlet Process Mixture Models [1]) and MFM (Mixture of Finite Mixtures [2]) mixture models, with an abstract Mixture Model and Component Model API.\n\nHyperparameter updates for DPM are implemented using an Empirical Bayes update procedure [3].\n\nFinal configuration selection is implemented using Least Squares clustering [4].\n\n\n## Usage\n\n### Installation and Setup\nPython:\nFirst, install with ```pip install bmcc``` (or ```pip3```, depending on your version). Then, simply ```import bmcc```.\n**NOTE**: Only Python 3 is officially supported.\n\nR:\nFirst, make sure Python 3 is installed, and install bmcc with ```pip install bmcc```. Then, install the R package with\n```R\nlibrary(devtools)\ninstall_github(\"https://github.com/thetianshuhuang/bmcc_r\")\n```\n\nTo use, load the package ```bmcc```. You will also need to load ```reticulate``` in order to deal with type conversions.\n```R\nlibrary(bmcc)\nlibrary(reticulate)\n```\n\n### Expected Types\n\nThe dataset should be an array with two dimensions, where each row is a data point. Arrays should be numpy arrays with data type float64, in contiguous C order.\n\n- Python: ```data = np.array(<source>, dtype=np.float64)```\n- R/reticulate: ```data = np_array(<source>, dtype=\"float64\", order=\"C\")```\n\nAssignment vectors should be arrays. The value at each index indicates the cluster assignment. Since clusters are unordered, the value itself has no meaning, and should be ignored other than to determine uniqueness. Assignment vectors have type uint16.\n\n- Python: ```assignments = np.array(<source>, dtype=np.uint16)```\n- R/reticulate: ```assignments = np_array(<source>, dtype=\"uint16\")```\n\nNote that when using reticulate, R types default to 'numeric' (double). When calling functions that require integer arguments (i.e. indices, number of dimensions), integers must be explicitly specified:\n\n```R\nx <- 25\nx <- as.integer(x)\n# or\nx <- 25L\n```\n\n### Creating the Model\n\nThe model has two parts: the mixture model, and the component model. Currently, the component model has one option, and the mixture model has two options.\n\n#### Normal Wishart\nThe normal wishart model assumes each component is drawn from a wishart distribution with degrees of freedom specified in the initializer, and scale matrix proportional to C^-1/df, where C is the covariance matrix of the observed points.\n\n- Python:\n```\ncomponent_model = bmcc.NormalWishart(df=3)\n```\n\n- R/reticulate:\n```\ncomponent_model = NormalWishart(df = 3)\n```\n\n#### MFM (Mixture of Finite Mixtures)\nSee [2]. Takes two arguments: the dirichlet mixing parameter gamma, and a log prior function on the number of clusters. Gamma defaults to 1, and the prior defaults to poisson(mu=4).\n**NOTE**: Make sure that the prior is given in log form!\n\n- Python:\n```python\nmixture_model = bmcc.MFM(gamma=1, prior=lambda k: poisson.logpmf(k, 4))\n```\n\n- R/reticulate:\n```R\nprior <- function(k) { dpois(k, 4, log = TRUE) }\nmixture_model = MFM(gamma=1, prior=py_func(prior))\n```\n\n### Running the Gibbs Sampler\n\nCurrently, only collapsed gibbs samplers are implemented. Is is possible to extend this to a general gibbs sampler using the API (documentation todo), but for now, the core library only implements collapsed gibbs.\n\nThe GibbsMixtureModel gibbs sampler takes 5 arguments: the dataset, the models created previously, an initial assignment vector (usually assigning all points to the same cluster), and a thinning factor. If the thinning factor is 1, all samples are kept; otherwise, only one sample out of every ```thinning``` samples are kept, with the rest being immediately discarded.\n\n- Python:\n```python\nsampler = bmcc.GibbsMixtureModel(\n    data=data,\n    component_model=component_model,\n    mixture_model=mixture_model,\n    assignments=np.zeros(1000).astype(np.uint16),\n    thinning=1)\n```\n\n- R:\n```R\nsampler = GibbsMixtureModel(\n    data=data,\n    component_model=component_model,\n    mixture_model=mixture_model,\n    assignments=np_array(rep(0, 1000), dtype=\"uint16\"),\n    thinning=1L)\n```\n\nFinally, simply call the ```iter``` method once for every iteration:\n\n- Python:\n```python\nfor i in range(1000):\n    sampler.iter()\n```\n\n- R:\n```R\nfor(i in 1:1000):\n    sampler$iter()\n```\n\nYou can also call ```iter``` with an argument (i.e. ```sampler.iter(10)```) to run multiple iterations at once. I suggest running the loop with a progress bar of some sort:\n\n- Python:\n```python\nimport tqdm\n# ...\nfor i in tqdm(range(1000)):\n    sampler.iter()\n```\n\n- R:\n```R\npb <- txtProgressBar(min = 0, max = 5000, style = 3)\nfor(i in 1:5000) {\n    model$iter()\n    setTxtProgressBar(pb, i)\n}\nclose(pb)\n```\n\n### Selecting a Result\n\nCurrently, only least squares configuration selection is implemented. Run by calling the ```select_lstsq``` method of the ```GibbsMixtureModel``` object with the burn in duration.\n\n- Python:\n```python\nres = sampler.select_lstsq(burn_in=100)\n```\n\n- R:\n```R\nres <- sampler$select_lstsq(burn_in=100L)\n```\n\nIf ground truths are available, call the ```evaluate``` method of the resulting ```LstsqResult``` object to run evaluation statistics. If oracle information is available, the ```oracle``` (oracle assignments) and ```oracle_matrix``` (oracle pairwise probability) arguments can optionally be passed to allow comparison.\n\n- Python:\n```python\nres.evaluate(\n    dataset.assignments,\n    oracle=dataset.oracle,\n    oracle_matrix=dataset.oracle_matrix)\n# Plot traces\nres.trace(plot=True)\n# Plot pairwise membership and probability matrices\nres.matrices(plot=True)\n# Plot clustering\nres.clustering(plot=True)\n```\n\n- R:\n```R\nres$evaluate(\n    dataset$assignments,\n    oracle=dataset$oracle,\n    oracle_matrix=dataset$oracle_matrix)\n# Plot traces\nres$trace(plot=TRUE)\n# Plot pairwise membership and probability matrices\nres$matrices(plot=TRUE)\n# Plot clustering\nres$clustering(plot=TRUE)\n```\n\n## References\n\n[1] Radford M. Neal (2000), \"Markov Chain Sampling Methods for Dirichlet Process Mixture Models\". Journal of Computational and Graphical Statistics, Vol. 9, No. 2.\n\n[2] Jeffrey W. Miller, Matthew T. Harrison (2018), \"Mixture Models with a Prior on the Number of Components\". Journal of the American Statistical Association, Vol. 113, Issue 521.\n\n[3] Jon D. McAuliffe, David M. Blei, Michael I. Jordan (2006), \"Nonparametric empirical Bayes for the Dirichlet process mixture model\". Statistics and Computing, Vol. 16, Issue 1.\n\n[4] David B. Dahl (2006), \"Model-Based Clustering for Expression Data via a Dirichlet Process Mixture Model\". Bayesian Inference for Gene Expression and Proteomics.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/thetianshuhuang/bmcc", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "bmcc", "package_url": "https://pypi.org/project/bmcc/", "platform": "", "project_url": "https://pypi.org/project/bmcc/", "project_urls": {"Homepage": "https://github.com/thetianshuhuang/bmcc"}, "release_url": "https://pypi.org/project/bmcc/1.0.3/", "requires_dist": null, "requires_python": ">=3, <4", "summary": "Implementation of Markov Chain Bayesian Clustering techniques, including DPM and MFM, with an abstract Mixture Model and Component Model API.", "version": "1.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Bayesian Markov Chain Monte Carlo Clustering</h1>\n<p>Implementation of Markov Chain Monte Carlo Bayesian Clustering techniques, including DPM (Dirichlet Process Mixture Models [1]) and MFM (Mixture of Finite Mixtures [2]) mixture models, with an abstract Mixture Model and Component Model API.</p>\n<p>Hyperparameter updates for DPM are implemented using an Empirical Bayes update procedure [3].</p>\n<p>Final configuration selection is implemented using Least Squares clustering [4].</p>\n<h2>Usage</h2>\n<h3>Installation and Setup</h3>\n<p>Python:\nFirst, install with <code>pip install bmcc</code> (or <code>pip3</code>, depending on your version). Then, simply <code>import bmcc</code>.\n<strong>NOTE</strong>: Only Python 3 is officially supported.</p>\n<p>R:\nFirst, make sure Python 3 is installed, and install bmcc with <code>pip install bmcc</code>. Then, install the R package with</p>\n<pre><span class=\"nf\">library</span><span class=\"p\">(</span><span class=\"n\">devtools</span><span class=\"p\">)</span>\n<span class=\"nf\">install_github</span><span class=\"p\">(</span><span class=\"s\">\"https://github.com/thetianshuhuang/bmcc_r\"</span><span class=\"p\">)</span>\n</pre>\n<p>To use, load the package <code>bmcc</code>. You will also need to load <code>reticulate</code> in order to deal with type conversions.</p>\n<pre><span class=\"nf\">library</span><span class=\"p\">(</span><span class=\"n\">bmcc</span><span class=\"p\">)</span>\n<span class=\"nf\">library</span><span class=\"p\">(</span><span class=\"n\">reticulate</span><span class=\"p\">)</span>\n</pre>\n<h3>Expected Types</h3>\n<p>The dataset should be an array with two dimensions, where each row is a data point. Arrays should be numpy arrays with data type float64, in contiguous C order.</p>\n<ul>\n<li>Python: <code>data = np.array(&lt;source&gt;, dtype=np.float64)</code></li>\n<li>R/reticulate: <code>data = np_array(&lt;source&gt;, dtype=\"float64\", order=\"C\")</code></li>\n</ul>\n<p>Assignment vectors should be arrays. The value at each index indicates the cluster assignment. Since clusters are unordered, the value itself has no meaning, and should be ignored other than to determine uniqueness. Assignment vectors have type uint16.</p>\n<ul>\n<li>Python: <code>assignments = np.array(&lt;source&gt;, dtype=np.uint16)</code></li>\n<li>R/reticulate: <code>assignments = np_array(&lt;source&gt;, dtype=\"uint16\")</code></li>\n</ul>\n<p>Note that when using reticulate, R types default to 'numeric' (double). When calling functions that require integer arguments (i.e. indices, number of dimensions), integers must be explicitly specified:</p>\n<pre><span class=\"n\">x</span> <span class=\"o\">&lt;-</span> <span class=\"m\">25</span>\n<span class=\"n\">x</span> <span class=\"o\">&lt;-</span> <span class=\"nf\">as.integer</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"c1\"># or</span>\n<span class=\"n\">x</span> <span class=\"o\">&lt;-</span> <span class=\"m\">25L</span>\n</pre>\n<h3>Creating the Model</h3>\n<p>The model has two parts: the mixture model, and the component model. Currently, the component model has one option, and the mixture model has two options.</p>\n<h4>Normal Wishart</h4>\n<p>The normal wishart model assumes each component is drawn from a wishart distribution with degrees of freedom specified in the initializer, and scale matrix proportional to C^-1/df, where C is the covariance matrix of the observed points.</p>\n<ul>\n<li>Python:</li>\n</ul>\n<pre><code>component_model = bmcc.NormalWishart(df=3)\n</code></pre>\n<ul>\n<li>R/reticulate:</li>\n</ul>\n<pre><code>component_model = NormalWishart(df = 3)\n</code></pre>\n<h4>MFM (Mixture of Finite Mixtures)</h4>\n<p>See [2]. Takes two arguments: the dirichlet mixing parameter gamma, and a log prior function on the number of clusters. Gamma defaults to 1, and the prior defaults to poisson(mu=4).\n<strong>NOTE</strong>: Make sure that the prior is given in log form!</p>\n<ul>\n<li>Python:</li>\n</ul>\n<pre><span class=\"n\">mixture_model</span> <span class=\"o\">=</span> <span class=\"n\">bmcc</span><span class=\"o\">.</span><span class=\"n\">MFM</span><span class=\"p\">(</span><span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">prior</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">k</span><span class=\"p\">:</span> <span class=\"n\">poisson</span><span class=\"o\">.</span><span class=\"n\">logpmf</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">))</span>\n</pre>\n<ul>\n<li>R/reticulate:</li>\n</ul>\n<pre><span class=\"n\">prior</span> <span class=\"o\">&lt;-</span> <span class=\"nf\">function</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">)</span> <span class=\"p\">{</span> <span class=\"nf\">dpois</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"m\">4</span><span class=\"p\">,</span> <span class=\"n\">log</span> <span class=\"o\">=</span> <span class=\"kc\">TRUE</span><span class=\"p\">)</span> <span class=\"p\">}</span>\n<span class=\"n\">mixture_model</span> <span class=\"o\">=</span> <span class=\"nf\">MFM</span><span class=\"p\">(</span><span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"m\">1</span><span class=\"p\">,</span> <span class=\"n\">prior</span><span class=\"o\">=</span><span class=\"nf\">py_func</span><span class=\"p\">(</span><span class=\"n\">prior</span><span class=\"p\">))</span>\n</pre>\n<h3>Running the Gibbs Sampler</h3>\n<p>Currently, only collapsed gibbs samplers are implemented. Is is possible to extend this to a general gibbs sampler using the API (documentation todo), but for now, the core library only implements collapsed gibbs.</p>\n<p>The GibbsMixtureModel gibbs sampler takes 5 arguments: the dataset, the models created previously, an initial assignment vector (usually assigning all points to the same cluster), and a thinning factor. If the thinning factor is 1, all samples are kept; otherwise, only one sample out of every <code>thinning</code> samples are kept, with the rest being immediately discarded.</p>\n<ul>\n<li>Python:</li>\n</ul>\n<pre><span class=\"n\">sampler</span> <span class=\"o\">=</span> <span class=\"n\">bmcc</span><span class=\"o\">.</span><span class=\"n\">GibbsMixtureModel</span><span class=\"p\">(</span>\n    <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"p\">,</span>\n    <span class=\"n\">component_model</span><span class=\"o\">=</span><span class=\"n\">component_model</span><span class=\"p\">,</span>\n    <span class=\"n\">mixture_model</span><span class=\"o\">=</span><span class=\"n\">mixture_model</span><span class=\"p\">,</span>\n    <span class=\"n\">assignments</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint16</span><span class=\"p\">),</span>\n    <span class=\"n\">thinning</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n</pre>\n<ul>\n<li>R:</li>\n</ul>\n<pre><span class=\"n\">sampler</span> <span class=\"o\">=</span> <span class=\"nf\">GibbsMixtureModel</span><span class=\"p\">(</span>\n    <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"p\">,</span>\n    <span class=\"n\">component_model</span><span class=\"o\">=</span><span class=\"n\">component_model</span><span class=\"p\">,</span>\n    <span class=\"n\">mixture_model</span><span class=\"o\">=</span><span class=\"n\">mixture_model</span><span class=\"p\">,</span>\n    <span class=\"n\">assignments</span><span class=\"o\">=</span><span class=\"nf\">np_array</span><span class=\"p\">(</span><span class=\"nf\">rep</span><span class=\"p\">(</span><span class=\"m\">0</span><span class=\"p\">,</span> <span class=\"m\">1000</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s\">\"uint16\"</span><span class=\"p\">),</span>\n    <span class=\"n\">thinning</span><span class=\"o\">=</span><span class=\"m\">1L</span><span class=\"p\">)</span>\n</pre>\n<p>Finally, simply call the <code>iter</code> method once for every iteration:</p>\n<ul>\n<li>Python:</li>\n</ul>\n<pre><span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">):</span>\n    <span class=\"n\">sampler</span><span class=\"o\">.</span><span class=\"n\">iter</span><span class=\"p\">()</span>\n</pre>\n<ul>\n<li>R:</li>\n</ul>\n<pre><span class=\"nf\">for</span><span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"n\">in</span> <span class=\"m\">1</span><span class=\"o\">:</span><span class=\"m\">1000</span><span class=\"p\">)</span><span class=\"o\">:</span>\n    <span class=\"n\">sampler</span><span class=\"o\">$</span><span class=\"nf\">iter</span><span class=\"p\">()</span>\n</pre>\n<p>You can also call <code>iter</code> with an argument (i.e. <code>sampler.iter(10)</code>) to run multiple iterations at once. I suggest running the loop with a progress bar of some sort:</p>\n<ul>\n<li>Python:</li>\n</ul>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">tqdm</span>\n<span class=\"c1\"># ...</span>\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">tqdm</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">)):</span>\n    <span class=\"n\">sampler</span><span class=\"o\">.</span><span class=\"n\">iter</span><span class=\"p\">()</span>\n</pre>\n<ul>\n<li>R:</li>\n</ul>\n<pre><span class=\"n\">pb</span> <span class=\"o\">&lt;-</span> <span class=\"nf\">txtProgressBar</span><span class=\"p\">(</span><span class=\"n\">min</span> <span class=\"o\">=</span> <span class=\"m\">0</span><span class=\"p\">,</span> <span class=\"n\">max</span> <span class=\"o\">=</span> <span class=\"m\">5000</span><span class=\"p\">,</span> <span class=\"n\">style</span> <span class=\"o\">=</span> <span class=\"m\">3</span><span class=\"p\">)</span>\n<span class=\"nf\">for</span><span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"n\">in</span> <span class=\"m\">1</span><span class=\"o\">:</span><span class=\"m\">5000</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"n\">model</span><span class=\"o\">$</span><span class=\"nf\">iter</span><span class=\"p\">()</span>\n    <span class=\"nf\">setTxtProgressBar</span><span class=\"p\">(</span><span class=\"n\">pb</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n<span class=\"nf\">close</span><span class=\"p\">(</span><span class=\"n\">pb</span><span class=\"p\">)</span>\n</pre>\n<h3>Selecting a Result</h3>\n<p>Currently, only least squares configuration selection is implemented. Run by calling the <code>select_lstsq</code> method of the <code>GibbsMixtureModel</code> object with the burn in duration.</p>\n<ul>\n<li>Python:</li>\n</ul>\n<pre><span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"n\">sampler</span><span class=\"o\">.</span><span class=\"n\">select_lstsq</span><span class=\"p\">(</span><span class=\"n\">burn_in</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n</pre>\n<ul>\n<li>R:</li>\n</ul>\n<pre><span class=\"n\">res</span> <span class=\"o\">&lt;-</span> <span class=\"n\">sampler</span><span class=\"o\">$</span><span class=\"nf\">select_lstsq</span><span class=\"p\">(</span><span class=\"n\">burn_in</span><span class=\"o\">=</span><span class=\"m\">100L</span><span class=\"p\">)</span>\n</pre>\n<p>If ground truths are available, call the <code>evaluate</code> method of the resulting <code>LstsqResult</code> object to run evaluation statistics. If oracle information is available, the <code>oracle</code> (oracle assignments) and <code>oracle_matrix</code> (oracle pairwise probability) arguments can optionally be passed to allow comparison.</p>\n<ul>\n<li>Python:</li>\n</ul>\n<pre><span class=\"n\">res</span><span class=\"o\">.</span><span class=\"n\">evaluate</span><span class=\"p\">(</span>\n    <span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">assignments</span><span class=\"p\">,</span>\n    <span class=\"n\">oracle</span><span class=\"o\">=</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">oracle</span><span class=\"p\">,</span>\n    <span class=\"n\">oracle_matrix</span><span class=\"o\">=</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">oracle_matrix</span><span class=\"p\">)</span>\n<span class=\"c1\"># Plot traces</span>\n<span class=\"n\">res</span><span class=\"o\">.</span><span class=\"n\">trace</span><span class=\"p\">(</span><span class=\"n\">plot</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"c1\"># Plot pairwise membership and probability matrices</span>\n<span class=\"n\">res</span><span class=\"o\">.</span><span class=\"n\">matrices</span><span class=\"p\">(</span><span class=\"n\">plot</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"c1\"># Plot clustering</span>\n<span class=\"n\">res</span><span class=\"o\">.</span><span class=\"n\">clustering</span><span class=\"p\">(</span><span class=\"n\">plot</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<ul>\n<li>R:</li>\n</ul>\n<pre><span class=\"n\">res</span><span class=\"o\">$</span><span class=\"nf\">evaluate</span><span class=\"p\">(</span>\n    <span class=\"n\">dataset</span><span class=\"o\">$</span><span class=\"n\">assignments</span><span class=\"p\">,</span>\n    <span class=\"n\">oracle</span><span class=\"o\">=</span><span class=\"n\">dataset</span><span class=\"o\">$</span><span class=\"n\">oracle</span><span class=\"p\">,</span>\n    <span class=\"n\">oracle_matrix</span><span class=\"o\">=</span><span class=\"n\">dataset</span><span class=\"o\">$</span><span class=\"n\">oracle_matrix</span><span class=\"p\">)</span>\n<span class=\"c1\"># Plot traces</span>\n<span class=\"n\">res</span><span class=\"o\">$</span><span class=\"nf\">trace</span><span class=\"p\">(</span><span class=\"n\">plot</span><span class=\"o\">=</span><span class=\"kc\">TRUE</span><span class=\"p\">)</span>\n<span class=\"c1\"># Plot pairwise membership and probability matrices</span>\n<span class=\"n\">res</span><span class=\"o\">$</span><span class=\"nf\">matrices</span><span class=\"p\">(</span><span class=\"n\">plot</span><span class=\"o\">=</span><span class=\"kc\">TRUE</span><span class=\"p\">)</span>\n<span class=\"c1\"># Plot clustering</span>\n<span class=\"n\">res</span><span class=\"o\">$</span><span class=\"nf\">clustering</span><span class=\"p\">(</span><span class=\"n\">plot</span><span class=\"o\">=</span><span class=\"kc\">TRUE</span><span class=\"p\">)</span>\n</pre>\n<h2>References</h2>\n<p>[1] Radford M. Neal (2000), \"Markov Chain Sampling Methods for Dirichlet Process Mixture Models\". Journal of Computational and Graphical Statistics, Vol. 9, No. 2.</p>\n<p>[2] Jeffrey W. Miller, Matthew T. Harrison (2018), \"Mixture Models with a Prior on the Number of Components\". Journal of the American Statistical Association, Vol. 113, Issue 521.</p>\n<p>[3] Jon D. McAuliffe, David M. Blei, Michael I. Jordan (2006), \"Nonparametric empirical Bayes for the Dirichlet process mixture model\". Statistics and Computing, Vol. 16, Issue 1.</p>\n<p>[4] David B. Dahl (2006), \"Model-Based Clustering for Expression Data via a Dirichlet Process Mixture Model\". Bayesian Inference for Gene Expression and Proteomics.</p>\n\n          </div>"}, "last_serial": 5532213, "releases": {"0.2.0": [{"comment_text": "", "digests": {"md5": "ce9cbe1992751250c4a466b299e6db7c", "sha256": "2077eff97d3733f50b72c187fd9b9af040b2630fed0402b9ca6c85d9bde256ad"}, "downloads": -1, "filename": "bmcc-0.2.0.tar.gz", "has_sig": false, "md5_digest": "ce9cbe1992751250c4a466b299e6db7c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20551, "upload_time": "2019-07-11T16:42:07", "upload_time_iso_8601": "2019-07-11T16:42:07.496239Z", "url": "https://files.pythonhosted.org/packages/0e/73/298e67fcb7ef6298cf331ea03e4b18d9632df76bd734be0e1d6a52c273a8/bmcc-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "e52ff81853251d1a19c11b2561b52183", "sha256": "ada69a9bf7f230da63e885124e15b00bfebc05f9d72be0cc3837535b6e412d4b"}, "downloads": -1, "filename": "bmcc-0.2.1.tar.gz", "has_sig": false, "md5_digest": "e52ff81853251d1a19c11b2561b52183", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20634, "upload_time": "2019-07-11T16:44:55", "upload_time_iso_8601": "2019-07-11T16:44:55.014802Z", "url": "https://files.pythonhosted.org/packages/78/48/07d0898d4c58770f45a92611c2e375e90ed8563386c931f4d28a3d8763ff/bmcc-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "0160163bf7be00799ab3e3333066f63e", "sha256": "14ab0828292fd459d1a76c1c4965dce3dbf0809172256f2812c9dd41993269d6"}, "downloads": -1, "filename": "bmcc-0.2.2.tar.gz", "has_sig": false, "md5_digest": "0160163bf7be00799ab3e3333066f63e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20664, "upload_time": "2019-07-11T16:55:25", "upload_time_iso_8601": "2019-07-11T16:55:25.897290Z", "url": "https://files.pythonhosted.org/packages/5c/bb/c1087427c44f5f1ca3605e25a85091dfa29049f7d3799e3783fe793a6d9d/bmcc-0.2.2.tar.gz", "yanked": false}], "0.2.3": [{"comment_text": "", "digests": {"md5": "01b85277f5dd0109a395426ef8117a3f", "sha256": "c7e5ade0e4f09bfb1d8391cbe56c0f875f968612ffc3aabd32b30784212b5a24"}, "downloads": -1, "filename": "bmcc-0.2.3.tar.gz", "has_sig": false, "md5_digest": "01b85277f5dd0109a395426ef8117a3f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22890, "upload_time": "2019-07-11T16:59:54", "upload_time_iso_8601": "2019-07-11T16:59:54.520562Z", "url": "https://files.pythonhosted.org/packages/8f/d5/d9e00e91ab859766e6ede145535f25d23ca2c1ed6229cdc7700090450961/bmcc-0.2.3.tar.gz", "yanked": false}], "0.2.4": [{"comment_text": "", "digests": {"md5": "19db441e808a6b44271fb983a204c35f", "sha256": "f62af89f47059801a52962fade788eabd3ef813e1ab23660a1ec8cbaa4814d0e"}, "downloads": -1, "filename": "bmcc-0.2.4-py3.6-linux-x86_64.egg", "has_sig": false, "md5_digest": "19db441e808a6b44271fb983a204c35f", "packagetype": "bdist_egg", "python_version": "3.6", "requires_python": ">=3, <4", "size": 100687, "upload_time": "2019-07-11T17:28:10", "upload_time_iso_8601": "2019-07-11T17:28:10.465184Z", "url": "https://files.pythonhosted.org/packages/7d/1f/661b85295b4e028d5a36dab41ec2947a57078122d5dcb7a0b227abb8312c/bmcc-0.2.4-py3.6-linux-x86_64.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "a539bba02750d48fb2b2dfce3a988054", "sha256": "4b6800b10a84cfa7efed1cf1635b4a69c5f3f6e80a5defdb047dfd13df96828a"}, "downloads": -1, "filename": "bmcc-0.2.4.tar.gz", "has_sig": false, "md5_digest": "a539bba02750d48fb2b2dfce3a988054", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, <4", "size": 23604, "upload_time": "2019-07-11T17:28:12", "upload_time_iso_8601": "2019-07-11T17:28:12.300991Z", "url": "https://files.pythonhosted.org/packages/60/1d/705dbf4e4b8b3d130563c6e6ac8a03dc86a12326746c928462ded7e539d2/bmcc-0.2.4.tar.gz", "yanked": false}], "0.2.5": [{"comment_text": "", "digests": {"md5": "4b2df2d647b8f2d010f6c76ded25aaa6", "sha256": "969448030507abb2931d2285ad3bd33528163fa6717578ca6a93b2d89fd4ef05"}, "downloads": -1, "filename": "bmcc-0.2.5.tar.gz", "has_sig": false, "md5_digest": "4b2df2d647b8f2d010f6c76ded25aaa6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, <4", "size": 23593, "upload_time": "2019-07-11T17:33:43", "upload_time_iso_8601": "2019-07-11T17:33:43.486859Z", "url": "https://files.pythonhosted.org/packages/41/ac/0f6b99a9adf7cc85a4eefe5a565825ea66221490efaf73e8d0b1cba5296c/bmcc-0.2.5.tar.gz", "yanked": false}], "0.2.6": [{"comment_text": "", "digests": {"md5": "c57f6d8a33db4ddf14d90e5acfb27384", "sha256": "11a9b74325a54663b06d356984978ff2fec498fc9c23bcb04237b51d59223418"}, "downloads": -1, "filename": "bmcc-0.2.6.tar.gz", "has_sig": false, "md5_digest": "c57f6d8a33db4ddf14d90e5acfb27384", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23000, "upload_time": "2019-07-11T17:55:24", "upload_time_iso_8601": "2019-07-11T17:55:24.793431Z", "url": "https://files.pythonhosted.org/packages/d0/c9/dbfcd54e15b5ffaef8d413a21f7365753297887f83089276de138bcea8d0/bmcc-0.2.6.tar.gz", "yanked": false}], "0.2.7": [{"comment_text": "", "digests": {"md5": "d024d920c30dfbfe3e7bdeb9ebaf1f23", "sha256": "78d85ca03598634b6c8d9a263b0a484eeb9372dda39c53a65de5aa21af0b80df"}, "downloads": -1, "filename": "bmcc-0.2.7.tar.gz", "has_sig": false, "md5_digest": "d024d920c30dfbfe3e7bdeb9ebaf1f23", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23350, "upload_time": "2019-07-11T18:43:06", "upload_time_iso_8601": "2019-07-11T18:43:06.792347Z", "url": "https://files.pythonhosted.org/packages/c8/22/6457f20e11d318935618339864d13ccf098bbcad18f3d6e86be45da4e4b0/bmcc-0.2.7.tar.gz", "yanked": false}], "1.0.0": [{"comment_text": "", "digests": {"md5": "2baa72241db9e2778df675701e64ea84", "sha256": "ffd2aee30e5cba2f1331b43c0ac7d03ea0fd2fa47bfd62d9178b281f445875fe"}, "downloads": -1, "filename": "bmcc-1.0.0.tar.gz", "has_sig": false, "md5_digest": "2baa72241db9e2778df675701e64ea84", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, <4", "size": 25225, "upload_time": "2019-07-12T20:07:17", "upload_time_iso_8601": "2019-07-12T20:07:17.632638Z", "url": "https://files.pythonhosted.org/packages/e6/1f/eb601fc4c476128422c476080ed1ca98767b587f93523b6ee950fdce77cd/bmcc-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "31f58b0c215c8c9952be0787dd7f2b91", "sha256": "7bcc42e5ae0554336a803a32ad71150e9a05cee7568483ef6677290af643d910"}, "downloads": -1, "filename": "bmcc-1.0.1.tar.gz", "has_sig": false, "md5_digest": "31f58b0c215c8c9952be0787dd7f2b91", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, <4", "size": 27915, "upload_time": "2019-07-14T20:40:34", "upload_time_iso_8601": "2019-07-14T20:40:34.335340Z", "url": "https://files.pythonhosted.org/packages/5b/a3/8f2eea1e7f3a72f59db49f3aee53a87aad4320c15e4c0edf89ab1e959229/bmcc-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "68713451f9828cefd3088aaa2f9e444c", "sha256": "4b18d4b844a696bc427b76a0e9d6c14dc771a33c33355bb554e6317e67c184b8"}, "downloads": -1, "filename": "bmcc-1.0.2.tar.gz", "has_sig": false, "md5_digest": "68713451f9828cefd3088aaa2f9e444c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, <4", "size": 27921, "upload_time": "2019-07-14T20:57:58", "upload_time_iso_8601": "2019-07-14T20:57:58.258618Z", "url": "https://files.pythonhosted.org/packages/df/03/3fde25718e59ac999c8c6afbe87a1ceb251603cd326bea88899916dce8f8/bmcc-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "55e59fd88fdfff5cdab9ba8094133bb6", "sha256": "7110a65eb9c8c0ec90c9565edc11139f5ace8f220c8544926ce6ea7cc3f3f193"}, "downloads": -1, "filename": "bmcc-1.0.3.tar.gz", "has_sig": false, "md5_digest": "55e59fd88fdfff5cdab9ba8094133bb6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, <4", "size": 28073, "upload_time": "2019-07-14T22:13:35", "upload_time_iso_8601": "2019-07-14T22:13:35.815196Z", "url": "https://files.pythonhosted.org/packages/67/bc/bf059e4474c06ca11e0fc81b9e51a836339bffcbb5e417eb073e05e6122c/bmcc-1.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "55e59fd88fdfff5cdab9ba8094133bb6", "sha256": "7110a65eb9c8c0ec90c9565edc11139f5ace8f220c8544926ce6ea7cc3f3f193"}, "downloads": -1, "filename": "bmcc-1.0.3.tar.gz", "has_sig": false, "md5_digest": "55e59fd88fdfff5cdab9ba8094133bb6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, <4", "size": 28073, "upload_time": "2019-07-14T22:13:35", "upload_time_iso_8601": "2019-07-14T22:13:35.815196Z", "url": "https://files.pythonhosted.org/packages/67/bc/bf059e4474c06ca11e0fc81b9e51a836339bffcbb5e417eb073e05e6122c/bmcc-1.0.3.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:36:56 2020"}