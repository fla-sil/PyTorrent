{"info": {"author": "Lukas Hedegaard", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Framework :: Pytest", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "<div align=\"center\">\n  <img src=\"docs/pics/logo.svg\"><br>\n</div>\n\n# Dataset Ops: Fluent dataset operations, compatible with your favorite libraries\n\n![Python package](https://github.com/LukasHedegaard/datasetops/workflows/Python%20package/badge.svg) [![Documentation Status](https://readthedocs.org/projects/datasetops/badge/?version=latest)](https://datasetops.readthedocs.io/en/latest/?badge=latest) [![codecov](https://codecov.io/gh/LukasHedegaard/datasetops/branch/master/graph/badge.svg)](https://codecov.io/gh/LukasHedegaard/datasetops) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\nDataset Ops provides a [fluent interface](https://martinfowler.com/bliki/FluentInterface.html) for _loading, filtering, transforming, splitting,_ and _combining_ datasets. \nDesigned specifically with data science and machine learning applications in mind, it integrates seamlessly with [Tensorflow](https://www.tensorflow.org) and [PyTorch](https://pytorch.org).\n\n## Appetizer\n```python\nimport datasetops as do\n\n# prepare your data\ntrain, val, test = (\n    do.from_folder_class_data('path/to/data/folder')\n    .named(\"data\", \"label\")\n    .image_resize((240, 240))\n    .one_hot(\"label\")\n    .shuffle(seed=42)\n    .split([0.6, 0.2, 0.2])\n)\n\n# use with your favorite framework\ntrain_tf = train.to_tensorflow() \ntrain_pt = train.to_pytorch() \n\n# or do your own thing\nfor img, label in train:\n    ...\n```\n\n## Installation \nBaniry installers available at the [Python package index](https://pypi.org/project/datasetops/)\n```bash\npip install datasetops\n```\n\n\n## Why? \nCollecting and preprocessing datasets is tiresome and often takes upwards of 50% of the effort spent in the data science and machine learning lifecycle.\nWhile [Tensorflow](https://www.tensorflow.org/datasets) and [PyTorch](https://www.tensorflow.org/datasets) have some useful datasets utilities available, they are designed specifically with the respective frameworks in mind.\nUnsuprisingly, this makes it hard to switch between them, and training-ready dataset definitions are bound to one or the other.\nMoreover, they do not aid you in standard scenarios where you want to:\n- Sample your dataset non-random ways (e.g with a fixed number of samples per class)\n- Center, standardize, normalise you data\n- Combine multiple datasets, e.g. for parallel input to a multi-stream network\n- Create non-standard data splits\n\n_Dataset Ops_ aims to make these processing steps easier, faster, and more intuitive to perform, while retaining full compatibility to and from the leading libraries. This also means you can grab a dataset from [torchvision datasets](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist) and use it directly with tensorflow:\n\n```python\nimport do\nimport torchvision\n\ntorch_usps = torchvision.datasets.USPS('../dataset/path', download=True)\ntensorflow_usps = do.from_pytorch(torch_usps).to_tensorflow()\n```\n\n\n## Development Status\nThe library is still under heavy development and the API may be subject to change.\n\nWhat follows here is a list of implemented and planned features.\n\n### Loaders\n- [x] `Loader` (utility class used to define a dataset)\n- [x] `from_pytorch` (load from a `torch.utils.data.Dataset`)\n- [ ] `from_tensorflow` (load from a `tf.data.Dataset`)\n- [x] `from_folder_data` (load flat folder with data)\n- [x] `from_folder_class_data` (load nested folder with a folder for each class)\n- [x] `from_folder_dataset_class_data` (load nested folder with multiple datasets, each with a nested class folder structure )\n- [ ] `from_mat` (load contents of a .mat file as a single dataaset)\n- [x] `from_mat_single_mult_data` (load contents of a .mat file as multiple dataasets)\n- [ ] `load` (load data from a path, automatically inferring type and structure)\n\n### Converters\n- [x] `to_tensorflow` (convert Dataset into tensorflow.data.Dataset)\n- [x] `to_pytorch` (convert Dataset into torchvision.Dataset)\n\n### Dataset information\n- [x] `shape` (get shape of a dataset item)\n- [x] `counts` (compute the counts of each unique item in the dataset by key)\n- [x] `unique` (get a list of unique items in the dataset by key)\n- [x] `named` (supply names for the item elements)\n- [x] `names` (get a list of names for the elements in an item)\n- [ ] `stats` (provide an overview of the dataset statistics)\n- [ ] `origin` (provide an description of how the dataset was made)\n\n### Sampling and splitting\n- [x] `shuffle` (shuffle the items   in a dataset randomly)\n- [x] `sample` (sample data at random a dataset)\n- [x] `filter` (filter the dataset using a predicate)\n- [x] `split` (split a dataset randomly based on fractions)\n- [x] `split_filter` (split a dataset into two based on a predicate)\n- [x] `allow_unique` (handy predicate used for balanced classwise filtering/sampling)\n- [x] `take` (take the first items in dataset)\n- [x] `repeat` (repeat the items in a dataset, either itemwise or as a whole)\n\n### Item manipulation\n- [x] `reorder` (reorder the elements of the dataset items (e.g. flip label and data order))\n- [x] `transform` (transform function which takes other functions and applies them to the dataset items.)\n- [x] `categorical` (transforms an element into a categorical integer encoded label)\n- [x] `one_hot` (transforms an element into a one-hot encoded label)\n- [x] `numpy` (transforms an element into a numpy.ndarray)\n- [x] `reshape` (reshapes numpy.ndarray elements)\n- [x] `image` (transforms a numpy array or path string into a PIL.Image.Image)\n- [x] `image_resize` (resizes PIL.Image.Image elements)\n- [ ] `image_crop` (crops PIL.Image.Image elements)\n- [ ] `image_rotate` (rotates PIL.Image.Image elements)\n- [ ] `image_transform` (transforms PIL.Image.Image elements)\n- [ ] `image_brightness` (modify brightness of PIL.Image.Image elements)\n- [ ] `image_contrast` (modify contrast of PIL.Image.Image elements)\n- [ ] `image_filter` (apply an image filter to PIL.Image.Image elements)\n- [ ] `noise` (adds noise to the data)\n- [ ] `center` (modify each item according to dataset statistics)\n- [ ] `normalize` (modify each item according to dataset statistics)\n- [ ] `standardize` (modify each item according to dataset statistics)\n- [ ] `whiten` (modify each item according to dataset statistics)\n- [ ] `randomly` (apply data transformations with some probability)\n\n### Dataset combinations \n- [x] `concat` (concatenate two datasets, placing the items of one after the other)\n- [x] `zip` (zip datasets itemwise, extending the size of each item)\n- [x] `cartesian_product` (create a dataset whose items are all combinations of items (zipped) of the originating datasets)\n\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/LukasHedegaard/datasetops", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "datasetops", "package_url": "https://pypi.org/project/datasetops/", "platform": "", "project_url": "https://pypi.org/project/datasetops/", "project_urls": {"Homepage": "https://github.com/LukasHedegaard/datasetops"}, "release_url": "https://pypi.org/project/datasetops/0.0.6/", "requires_dist": ["numpy", "pillow", "pandas", "scipy", "setuptools ; extra == 'build'", "wheel ; extra == 'build'", "twine ; extra == 'build'", "Sphinx ; extra == 'docs'", "recommonmark ; extra == 'docs'", "sphinx-rtd-theme ; extra == 'docs'", "sphinx-autoapi ; extra == 'docs'", "pytest ; extra == 'tests'", "pytest-cov ; extra == 'tests'", "flake8 ; extra == 'tests'", "tensorflow ; extra == 'tests'", "torch ; extra == 'tests'", "torchvision ; extra == 'tests'"], "requires_python": ">=3.6", "summary": "Fluent dataset operations, compatible with your favorite libraries", "version": "0.0.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div>\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d471c7b766f7ee85bd8596d3039955cd830c7b00/646f63732f706963732f6c6f676f2e737667\"><br>\n</div>\n<h1>Dataset Ops: Fluent dataset operations, compatible with your favorite libraries</h1>\n<p><img alt=\"Python package\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f18e7c7776e41b2b03267c98c8aff9da195e1a52/68747470733a2f2f6769746875622e636f6d2f4c756b61734865646567616172642f646174617365746f70732f776f726b666c6f77732f507974686f6e2532307061636b6167652f62616467652e737667\"> <a href=\"https://datasetops.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/deac723544be6809c666ca1e6a5818a9933d04b8/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f646174617365746f70732f62616467652f3f76657273696f6e3d6c6174657374\"></a> <a href=\"https://codecov.io/gh/LukasHedegaard/datasetops\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/22b117afc1768c6ddda8bd98e68e52e6dc426f4b/68747470733a2f2f636f6465636f762e696f2f67682f4c756b61734865646567616172642f646174617365746f70732f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a> <a href=\"https://github.com/psf/black\" rel=\"nofollow\"><img alt=\"Code style: black\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fbfdc7754183ecf079bc71ddeabaf88f6cbc5c00/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667\"></a></p>\n<p>Dataset Ops provides a <a href=\"https://martinfowler.com/bliki/FluentInterface.html\" rel=\"nofollow\">fluent interface</a> for <em>loading, filtering, transforming, splitting,</em> and <em>combining</em> datasets.\nDesigned specifically with data science and machine learning applications in mind, it integrates seamlessly with <a href=\"https://www.tensorflow.org\" rel=\"nofollow\">Tensorflow</a> and <a href=\"https://pytorch.org\" rel=\"nofollow\">PyTorch</a>.</p>\n<h2>Appetizer</h2>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">datasetops</span> <span class=\"k\">as</span> <span class=\"nn\">do</span>\n\n<span class=\"c1\"># prepare your data</span>\n<span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">val</span><span class=\"p\">,</span> <span class=\"n\">test</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"n\">do</span><span class=\"o\">.</span><span class=\"n\">from_folder_class_data</span><span class=\"p\">(</span><span class=\"s1\">'path/to/data/folder'</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">named</span><span class=\"p\">(</span><span class=\"s2\">\"data\"</span><span class=\"p\">,</span> <span class=\"s2\">\"label\"</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">image_resize</span><span class=\"p\">((</span><span class=\"mi\">240</span><span class=\"p\">,</span> <span class=\"mi\">240</span><span class=\"p\">))</span>\n    <span class=\"o\">.</span><span class=\"n\">one_hot</span><span class=\"p\">(</span><span class=\"s2\">\"label\"</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">([</span><span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">])</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># use with your favorite framework</span>\n<span class=\"n\">train_tf</span> <span class=\"o\">=</span> <span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">to_tensorflow</span><span class=\"p\">()</span> \n<span class=\"n\">train_pt</span> <span class=\"o\">=</span> <span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">to_pytorch</span><span class=\"p\">()</span> \n\n<span class=\"c1\"># or do your own thing</span>\n<span class=\"k\">for</span> <span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"ow\">in</span> <span class=\"n\">train</span><span class=\"p\">:</span>\n    <span class=\"o\">...</span>\n</pre>\n<h2>Installation</h2>\n<p>Baniry installers available at the <a href=\"https://pypi.org/project/datasetops/\" rel=\"nofollow\">Python package index</a></p>\n<pre>pip install datasetops\n</pre>\n<h2>Why?</h2>\n<p>Collecting and preprocessing datasets is tiresome and often takes upwards of 50% of the effort spent in the data science and machine learning lifecycle.\nWhile <a href=\"https://www.tensorflow.org/datasets\" rel=\"nofollow\">Tensorflow</a> and <a href=\"https://www.tensorflow.org/datasets\" rel=\"nofollow\">PyTorch</a> have some useful datasets utilities available, they are designed specifically with the respective frameworks in mind.\nUnsuprisingly, this makes it hard to switch between them, and training-ready dataset definitions are bound to one or the other.\nMoreover, they do not aid you in standard scenarios where you want to:</p>\n<ul>\n<li>Sample your dataset non-random ways (e.g with a fixed number of samples per class)</li>\n<li>Center, standardize, normalise you data</li>\n<li>Combine multiple datasets, e.g. for parallel input to a multi-stream network</li>\n<li>Create non-standard data splits</li>\n</ul>\n<p><em>Dataset Ops</em> aims to make these processing steps easier, faster, and more intuitive to perform, while retaining full compatibility to and from the leading libraries. This also means you can grab a dataset from <a href=\"https://pytorch.org/docs/stable/torchvision/datasets.html#mnist\" rel=\"nofollow\">torchvision datasets</a> and use it directly with tensorflow:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">do</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision</span>\n\n<span class=\"n\">torch_usps</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">USPS</span><span class=\"p\">(</span><span class=\"s1\">'../dataset/path'</span><span class=\"p\">,</span> <span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">tensorflow_usps</span> <span class=\"o\">=</span> <span class=\"n\">do</span><span class=\"o\">.</span><span class=\"n\">from_pytorch</span><span class=\"p\">(</span><span class=\"n\">torch_usps</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to_tensorflow</span><span class=\"p\">()</span>\n</pre>\n<h2>Development Status</h2>\n<p>The library is still under heavy development and the API may be subject to change.</p>\n<p>What follows here is a list of implemented and planned features.</p>\n<h3>Loaders</h3>\n<ul>\n<li>[x] <code>Loader</code> (utility class used to define a dataset)</li>\n<li>[x] <code>from_pytorch</code> (load from a <code>torch.utils.data.Dataset</code>)</li>\n<li>[ ] <code>from_tensorflow</code> (load from a <code>tf.data.Dataset</code>)</li>\n<li>[x] <code>from_folder_data</code> (load flat folder with data)</li>\n<li>[x] <code>from_folder_class_data</code> (load nested folder with a folder for each class)</li>\n<li>[x] <code>from_folder_dataset_class_data</code> (load nested folder with multiple datasets, each with a nested class folder structure )</li>\n<li>[ ] <code>from_mat</code> (load contents of a .mat file as a single dataaset)</li>\n<li>[x] <code>from_mat_single_mult_data</code> (load contents of a .mat file as multiple dataasets)</li>\n<li>[ ] <code>load</code> (load data from a path, automatically inferring type and structure)</li>\n</ul>\n<h3>Converters</h3>\n<ul>\n<li>[x] <code>to_tensorflow</code> (convert Dataset into tensorflow.data.Dataset)</li>\n<li>[x] <code>to_pytorch</code> (convert Dataset into torchvision.Dataset)</li>\n</ul>\n<h3>Dataset information</h3>\n<ul>\n<li>[x] <code>shape</code> (get shape of a dataset item)</li>\n<li>[x] <code>counts</code> (compute the counts of each unique item in the dataset by key)</li>\n<li>[x] <code>unique</code> (get a list of unique items in the dataset by key)</li>\n<li>[x] <code>named</code> (supply names for the item elements)</li>\n<li>[x] <code>names</code> (get a list of names for the elements in an item)</li>\n<li>[ ] <code>stats</code> (provide an overview of the dataset statistics)</li>\n<li>[ ] <code>origin</code> (provide an description of how the dataset was made)</li>\n</ul>\n<h3>Sampling and splitting</h3>\n<ul>\n<li>[x] <code>shuffle</code> (shuffle the items   in a dataset randomly)</li>\n<li>[x] <code>sample</code> (sample data at random a dataset)</li>\n<li>[x] <code>filter</code> (filter the dataset using a predicate)</li>\n<li>[x] <code>split</code> (split a dataset randomly based on fractions)</li>\n<li>[x] <code>split_filter</code> (split a dataset into two based on a predicate)</li>\n<li>[x] <code>allow_unique</code> (handy predicate used for balanced classwise filtering/sampling)</li>\n<li>[x] <code>take</code> (take the first items in dataset)</li>\n<li>[x] <code>repeat</code> (repeat the items in a dataset, either itemwise or as a whole)</li>\n</ul>\n<h3>Item manipulation</h3>\n<ul>\n<li>[x] <code>reorder</code> (reorder the elements of the dataset items (e.g. flip label and data order))</li>\n<li>[x] <code>transform</code> (transform function which takes other functions and applies them to the dataset items.)</li>\n<li>[x] <code>categorical</code> (transforms an element into a categorical integer encoded label)</li>\n<li>[x] <code>one_hot</code> (transforms an element into a one-hot encoded label)</li>\n<li>[x] <code>numpy</code> (transforms an element into a numpy.ndarray)</li>\n<li>[x] <code>reshape</code> (reshapes numpy.ndarray elements)</li>\n<li>[x] <code>image</code> (transforms a numpy array or path string into a PIL.Image.Image)</li>\n<li>[x] <code>image_resize</code> (resizes PIL.Image.Image elements)</li>\n<li>[ ] <code>image_crop</code> (crops PIL.Image.Image elements)</li>\n<li>[ ] <code>image_rotate</code> (rotates PIL.Image.Image elements)</li>\n<li>[ ] <code>image_transform</code> (transforms PIL.Image.Image elements)</li>\n<li>[ ] <code>image_brightness</code> (modify brightness of PIL.Image.Image elements)</li>\n<li>[ ] <code>image_contrast</code> (modify contrast of PIL.Image.Image elements)</li>\n<li>[ ] <code>image_filter</code> (apply an image filter to PIL.Image.Image elements)</li>\n<li>[ ] <code>noise</code> (adds noise to the data)</li>\n<li>[ ] <code>center</code> (modify each item according to dataset statistics)</li>\n<li>[ ] <code>normalize</code> (modify each item according to dataset statistics)</li>\n<li>[ ] <code>standardize</code> (modify each item according to dataset statistics)</li>\n<li>[ ] <code>whiten</code> (modify each item according to dataset statistics)</li>\n<li>[ ] <code>randomly</code> (apply data transformations with some probability)</li>\n</ul>\n<h3>Dataset combinations</h3>\n<ul>\n<li>[x] <code>concat</code> (concatenate two datasets, placing the items of one after the other)</li>\n<li>[x] <code>zip</code> (zip datasets itemwise, extending the size of each item)</li>\n<li>[x] <code>cartesian_product</code> (create a dataset whose items are all combinations of items (zipped) of the originating datasets)</li>\n</ul>\n\n          </div>"}, "last_serial": 6894208, "releases": {"0.0.3": [{"comment_text": "", "digests": {"md5": "fd0da81bfc485a858b67456f5739a80f", "sha256": "8b5acdb9cdab7953e6a3c6f6d897153e7905b5dc2544473e756d583a6a8cb44f"}, "downloads": -1, "filename": "datasetops-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "fd0da81bfc485a858b67456f5739a80f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18256, "upload_time": "2020-03-19T16:28:12", "upload_time_iso_8601": "2020-03-19T16:28:12.278783Z", "url": "https://files.pythonhosted.org/packages/30/12/d06c5f17015314f1e3c4c74cf552f8bf5e17331e1f16f45d615172aae3bf/datasetops-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "83b5855f5872f418704692279d625af0", "sha256": "d7cc4f9ee34fe34737544de4b580135d4b75084024925de2e65e807ddf19f30e"}, "downloads": -1, "filename": "datasetops-0.0.3.tar.gz", "has_sig": false, "md5_digest": "83b5855f5872f418704692279d625af0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17442, "upload_time": "2020-03-19T16:28:14", "upload_time_iso_8601": "2020-03-19T16:28:14.995235Z", "url": "https://files.pythonhosted.org/packages/7b/f5/8ce9463404e02cc67bfaaeb7f48f1242b57ff630bfdef88a34720a2b5c2e/datasetops-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "9f0350a7d22e6c11844932ae5e993a8c", "sha256": "e60c3abb88a6ac7ca3295ea3fda88fef23b54c3e2765df5a80dd8e657de44704"}, "downloads": -1, "filename": "datasetops-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "9f0350a7d22e6c11844932ae5e993a8c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18256, "upload_time": "2020-03-19T16:51:03", "upload_time_iso_8601": "2020-03-19T16:51:03.187987Z", "url": "https://files.pythonhosted.org/packages/7c/24/97470b30cf628995694676c17b7b2189e23c87d01dc4d3d672679023c1bc/datasetops-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "35ab61d59607698bd5546726632030b5", "sha256": "458b0b67741db2a73c1b819160f5095d50f323e663ec39ef25947eef9921b12d"}, "downloads": -1, "filename": "datasetops-0.0.4.tar.gz", "has_sig": false, "md5_digest": "35ab61d59607698bd5546726632030b5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17440, "upload_time": "2020-03-19T16:51:04", "upload_time_iso_8601": "2020-03-19T16:51:04.824934Z", "url": "https://files.pythonhosted.org/packages/16/02/816a55ce13459fff8b0cb61bec2f8e7557f25010524aac70f3d402ff0807/datasetops-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "064d41382b5fdb9e527889464e027cf7", "sha256": "81e3c99af29fa4dff48df21d59ffadf93a8781e1cb6ef5c522c5cb48bf45acea"}, "downloads": -1, "filename": "datasetops-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "064d41382b5fdb9e527889464e027cf7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18294, "upload_time": "2020-03-24T11:48:29", "upload_time_iso_8601": "2020-03-24T11:48:29.394558Z", "url": "https://files.pythonhosted.org/packages/7c/51/41220bf65f70c65f54a6ca555f0d5d8b76332199c68367986bd881a53d77/datasetops-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "29c7b348e8dace4a0153c598098bffcf", "sha256": "26b1473ba5e3115ce062dded4e31e33067c67bb167332ec020c2c818daad4f03"}, "downloads": -1, "filename": "datasetops-0.0.5.tar.gz", "has_sig": false, "md5_digest": "29c7b348e8dace4a0153c598098bffcf", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17789, "upload_time": "2020-03-24T11:48:31", "upload_time_iso_8601": "2020-03-24T11:48:31.170779Z", "url": "https://files.pythonhosted.org/packages/89/70/c8b8e801c2f356e730ef042d1d34a46e9e5e712685b3a9c9912d3141ab4a/datasetops-0.0.5.tar.gz", "yanked": false}], "0.0.6": [{"comment_text": "", "digests": {"md5": "3a2e31c8002adc1b3d9f697d92bb6206", "sha256": "231bde372d0749c56500cf27fc76672d254fb7fa9f2c2c4a0a0619a8901dc623"}, "downloads": -1, "filename": "datasetops-0.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "3a2e31c8002adc1b3d9f697d92bb6206", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 19043, "upload_time": "2020-03-27T07:25:05", "upload_time_iso_8601": "2020-03-27T07:25:05.474616Z", "url": "https://files.pythonhosted.org/packages/05/53/a7077a8f054e7c05dfcdc431e53d7335b6d348df88fccd674d0ff8c4ffc0/datasetops-0.0.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a4e0b3107a9b374de9ec201e739441a5", "sha256": "52b7342e91d4ac5377b66bc0ea4b6a57c238717fb968bd8c26791b6ac2da3d09"}, "downloads": -1, "filename": "datasetops-0.0.6.tar.gz", "has_sig": false, "md5_digest": "a4e0b3107a9b374de9ec201e739441a5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 19069, "upload_time": "2020-03-27T07:25:07", "upload_time_iso_8601": "2020-03-27T07:25:07.273444Z", "url": "https://files.pythonhosted.org/packages/bf/20/54843d13af52542887031ee2f6a31180b678028618a29cea17f0ef091275/datasetops-0.0.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3a2e31c8002adc1b3d9f697d92bb6206", "sha256": "231bde372d0749c56500cf27fc76672d254fb7fa9f2c2c4a0a0619a8901dc623"}, "downloads": -1, "filename": "datasetops-0.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "3a2e31c8002adc1b3d9f697d92bb6206", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 19043, "upload_time": "2020-03-27T07:25:05", "upload_time_iso_8601": "2020-03-27T07:25:05.474616Z", "url": "https://files.pythonhosted.org/packages/05/53/a7077a8f054e7c05dfcdc431e53d7335b6d348df88fccd674d0ff8c4ffc0/datasetops-0.0.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a4e0b3107a9b374de9ec201e739441a5", "sha256": "52b7342e91d4ac5377b66bc0ea4b6a57c238717fb968bd8c26791b6ac2da3d09"}, "downloads": -1, "filename": "datasetops-0.0.6.tar.gz", "has_sig": false, "md5_digest": "a4e0b3107a9b374de9ec201e739441a5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 19069, "upload_time": "2020-03-27T07:25:07", "upload_time_iso_8601": "2020-03-27T07:25:07.273444Z", "url": "https://files.pythonhosted.org/packages/bf/20/54843d13af52542887031ee2f6a31180b678028618a29cea17f0ef091275/datasetops-0.0.6.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:40:12 2020"}