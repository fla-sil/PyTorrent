{"info": {"author": "Chou-han Yang", "author_email": "", "bugtrack_url": null, "classifiers": [], "description": "# s4cmd\n### Super S3 command line tool\n[![Build Status](https://travis-ci.com/bloomreach/s4cmd.svg?branch=master)](https://travis-ci.com/bloomreach/s4cmd) [![Join the chat at https://gitter.im/bloomreach/     s4cmd](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/bloomreach/s4cmd?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n----\n\n**Author**: Chou-han Yang ([@chouhanyang](https://github.com/chouhanyang))\n\n**Current Maintainers**: Naveen Vardhi ([@rozuur](https://github.com/rozuur)) | Navin Pai ([@navinpai](https://github.com/navinpai))\n\n----\n\n## What's New in s4cmd 2.x\n\n- Fully migrated from old boto 2.x to new [boto3](http://boto3.readthedocs.io/en/latest/reference/services/s3.html)  library, which provides more reliable and up-to-date S3 backend.\n- Support S3 `--API-ServerSideEncryption` along with **36 new API pass-through options**. See API pass-through options section for complete list.\n- Support batch delete (with delete_objects API) to delete up to 1000 files with single call. **100+ times faster** than sequential deletion.\n- Support `S4CMD_OPTS` environment variable for commonly used options such as `--API-ServerSideEncryption` across all your s4cmd operations.\n- Support moving files **larger than 5GB** with multipart upload. **20+ times faster** then sequential move operation when moving large files.\n- Support timestamp filtering with `--last-modified-before` and `--last-modified-after` options for all operations. Human friendly timestamps are supported, e.g. `--last-modified-before='2 months ago'`\n- Faster upload with lazy evaluation of md5 hash.\n- Listing large number of files with S3 pagination, with memory is the limit.\n- New directory to directory `dsync` command is better and standalone implementation to replace old `sync` command, which is implemented based on top of get/put/mv commands. `--delete-removed` work for all cases including local to s3, s3 to local, and s3 to s3. `sync` command preserves the old behavior in this version for compatibility.\n- Tested on both python 2 and 3.\n- Special thanks to [onera.com](http://www.onera.com) for supporting s4cmd.\n\n\n## Motivation\n\nS4cmd is a command-line utility for accessing\n[Amazon S3](http://en.wikipedia.org/wiki/Amazon_S3), inspired by\n[s3cmd](http://s3tools.org/s3cmd).\n\nWe have used s3cmd heavily for a number of scripted, data-intensive\napplications. However as the need for a variety of small improvements arose, we\ncreated our own implementation, s4cmd. It is intended as an alternative to\ns3cmd for enhanced performance and for large files, and with a number of\nadditional features and fixes that we have found useful.\n\nIt strives to be compatible with the most common usage scenarios for s3cmd. It\ndoes not offer exact drop-in compatibility, due to a number of corner cases where\ndifferent behavior seems preferable, or for bugfixes.\n\n\n## Features\n\nS4cmd supports the regular commands you might expect for fetching and storing\nfiles in S3: `ls`, `put`, `get`, `cp`, `mv`, `sync`, `del`, `du`.\n\nThe main features that distinguish s4cmd are:\n\n- Simple (less than 1500 lines of code) and implemented in pure Python, based\n  on the widely used [Boto3](https://github.com/boto/boto3) library.\n- Multi-threaded/multi-connection implementation for enhanced performance on all\n  commands. As with many network-intensive applications (like web browsers),\n  accessing S3 in a single-threaded way is often significantly less efficient than\n  having multiple connections actively transferring data at once.  In general, we\n  get a 2X boost to upload/download speeds from this.\n- Path handling: S3 is not a traditional filesystem with built-in support for\n  directory structure: internally, there are only objects, not directories or\n  folders. However, most people use S3 in a hierarchical structure, with paths\n  separated by slashes, to emulate traditional filesystems. S4cmd follows\n  conventions to more closely replicate the behavior of traditional filesystems\n  in certain corner cases.  For example, \"ls\" and \"cp\" work much like in Unix\n  shells, to avoid odd surprises. (For examples see compatibility notes below.)\n- Wildcard support: Wildcards, including multiple levels of wildcards, like in\n  Unix shells, are handled. For example:\n  s3://my-bucket/my-folder/20120512/*/*chunk00?1?\n- Automatic retry: Failure tasks will be executed again after a delay.\n- Multi-part upload support for files larger than 5GB.\n- Handling of MD5s properly with respect to multi-part uploads (for the sordid\n  details of this, see below).\n- Miscellaneous enhancements and bugfixes:\n  - Partial file creation: Avoid creating empty target files if source does not\n    exist. Avoid creating partial output files when commands are interrupted.\n  - General thread safety: Tool can be interrupted or killed at any time without\n    being blocked by child threads or leaving incomplete or corrupt files in\n    place.\n  - Ensure exit code is nonzero on all failure scenarios (a very important\n    feature in scripts).\n  - Expected handling of symlinks (they are followed).\n  - Support both `s3://` and `s3n://` prefixes (the latter is common with\n    Amazon Elastic Mapreduce).\n\nLimitations:\n\n- No CloudFront or other feature support.\n- Currently, we simulate `sync` with `get` and `put` with `--recursive --force --sync-check`.\n\n\n## Installation and Setup\nYou can install `s4cmd` [PyPI](https://pypi.python.org/pypi/s4cmd).\n\n```\npip install s4cmd\n```\n\n- Copy or create a symbolic link so you can run `s4cmd.py` as `s4cmd`. (It is just\na single file!)\n- If you already have a `~/.s3cfg` file from configuring `s3cmd`, credentials\nfrom this file will be used.  Otherwise, set the `S3_ACCESS_KEY` and\n`S3_SECRET_KEY` environment variables to contain your S3 credentials.\n- If no keys are provided, but an IAM role is associated with the EC2 instance, it will\nbe used transparently.\n\n\n## s4cmd Commands\n\n#### `s4cmd ls [path]`\n\nList all contents of a directory.\n\n* -r/--recursive: recursively display all contents including subdirectories under the given path.\n* -d/--show-directory: show the directory entry instead of its content.\n\n\n#### `s4cmd put [source] [target]`\n\nUpload local files up to S3.\n\n*   -r/--recursive: also upload directories recursively.\n*   -s/--sync-check: check md5 hash to avoid uploading the same content.\n*   -f/--force: override existing file instead of showing error message.\n*   -n/--dry-run: emulate the operation without real upload.\n\n#### `s4cmd get [source] [target]`\n\nDownload files from S3 to local filesystem.\n\n*   -r/--recursive: also download directories recursively.\n*   -s/--sync-check: check md5 hash to avoid downloading the same content.\n*   -f/--force: override existing file instead of showing error message.\n*   -n/--dry-run: emulate the operation without real download.\n\n\n#### `s4cmd dsync [source dir] [target dir]`\n\nSynchronize the contents of two directories. The directory can either be local or remote, but currently, it doesn't support two local directories.\n\n*   -r/--recursive: also sync directories recursively.\n*   -s/--sync-check: check md5 hash to avoid syncing the same content.\n*   -f/--force: override existing file instead of showing error message.\n*   -n/--dry-run: emulate the operation without real sync.\n*   --delete-removed: delete files not in source directory.\n\n#### `s4cmd sync [source] [target]`\n\n(Obsolete, use `dsync` instead) Synchronize the contents of two directories. The directory can either be local or remote, but currently, it doesn't support two local directories. This command simply invoke get/put/mv commands.\n\n*   -r/--recursive: also sync directories recursively.\n*   -s/--sync-check: check md5 hash to avoid syncing the same content.\n*   -f/--force: override existing file instead of showing error message.\n*   -n/--dry-run: emulate the operation without real sync.\n*   --delete-removed: delete files not in source directory. Only works when syncing local directory to s3 directory.\n\n#### `s4cmd cp [source] [target]`\n\nCopy a file or a directory from a S3 location to another.\n\n*   -r/--recursive: also copy directories recursively.\n*   -s/--sync-check: check md5 hash to avoid copying the same content.\n*   -f/--force: override existing file instead of showing error message.\n*   -n/--dry-run: emulate the operation without real copy.\n\n#### `s4cmd mv [source] [target]`\n\nMove a file or a directory from a S3 location to another.\n\n*   -r/--recursive: also move directories recursively.\n*   -s/--sync-check: check md5 hash to avoid moving the same content.\n*   -f/--force: override existing file instead of showing error message.\n*   -n/--dry-run: emulate the operation without real move.\n\n#### `s4cmd del [path]`\n\nDelete files or directories on S3.\n\n*   -r/--recursive: also delete directories recursively.\n*   -n/--dry-run: emulate the operation without real delete.\n\n#### `s4cmd du [path]`\n\nGet the size of the given directory.\n\nAvailable parameters:\n\n*   -r/--recursive: also add sizes of sub-directories recursively.\n\n## s4cmd Control Options\n\n##### `-p S3CFG, --config=[filename]`\npath to s3cfg config file\n\n##### `-f, --force`\nforce overwrite files when download or upload\n\n##### `-r, --recursive`\nrecursively checking subdirectories\n\n##### `-s, --sync-check`\ncheck file md5 before download or upload\n\n##### `-n, --dry-run`\ntrial run without actual download or upload\n\n##### `-t RETRY, --retry=[integer]`\nnumber of retries before giving up\n\n##### `--retry-delay=[integer]`\nseconds to sleep between retries\n\n##### `-c NUM_THREADS, --num-threads=NUM_THREADS`\nnumber of concurrent threads\n\n##### `--endpoint-url`\nendpoint url used in boto3 client\n\n##### `-d, --show-directory`\nshow directory instead of its content\n\n##### `--ignore-empty-source`\nignore empty source from s3\n\n##### `--use-ssl`\n(obsolete) use SSL connection to S3\n\n##### `--verbose`\nverbose output\n\n##### `--debug`\ndebug output\n\n##### `--validate`\n(obsolete) validate lookup operation\n\n##### `-D, --delete-removed`\ndelete remote files that do not exist in source after sync\n\n##### `--multipart-split-size=[integer]`\nsize in bytes to split multipart transfers\n\n##### `--max-singlepart-download-size=[integer]`\nfiles with size (in bytes) greater than this will be\ndownloaded in multipart transfers\n\n##### `--max-singlepart-upload-size=[integer]`\nfiles with size (in bytes) greater than this will be\nuploaded in multipart transfers\n\n##### `--max-singlepart-copy-size=[integer]`\nfiles with size (in bytes) greater than this will be\ncopied in multipart transfers\n\n##### `--batch-delete-size=[integer]`\nNumber of files (&lt;1000) to be combined in batch delete.\n\n##### `--last-modified-before=[datetime]`\nCondition on files where their last modified dates are\nbefore given parameter.\n\n##### `--last-modified-after=[datetime]`\nCondition on files where their last modified dates are\nafter given parameter.\n\n\n## S3 API Pass-through Options\n\nThose options are directly translated to boto3 API commands. The options provided will be filtered by the APIs that are taking parameters. For example, `--API-ServerSideEncryption` is only needed for `put_object`, `create_multipart_upload` but not for `list_buckets` and `get_objects` for example. Therefore, providing `--API-ServerSideEncryption` for `s4cmd ls` has no effect.\n\nFor more information, please see boto3 s3 documentations http://boto3.readthedocs.io/en/latest/reference/services/s3.html\n\n##### `--API-ACL=[string]`\nThe canned ACL to apply to the object.\n\n##### `--API-CacheControl=[string]`\nSpecifies caching behavior along the request/reply chain.\n\n##### `--API-ContentDisposition=[string]`\nSpecifies presentational information for the object.\n\n##### `--API-ContentEncoding=[string]`\nSpecifies what content encodings have been applied to the object and thus what decoding mechanisms must be applied to obtain the media-type referenced by the Content-Type header field.\n\n##### `--API-ContentLanguage=[string]`\nThe language the content is in.\n\n##### `--API-ContentMD5=[string]`\nThe base64-encoded 128-bit MD5 digest of the part data.\n\n##### `--API-ContentType=[string]`\nA standard MIME type describing the format of the object data.\n\n##### `--API-CopySourceIfMatch=[string]`\nCopies the object if its entity tag (ETag) matches the specified tag.\n\n##### `--API-CopySourceIfModifiedSince=[datetime]`\nCopies the object if it has been modified since the specified time.\n\n##### `--API-CopySourceIfNoneMatch=[string]`\nCopies the object if its entity tag (ETag) is different than the specified ETag.\n\n##### `--API-CopySourceIfUnmodifiedSince=[datetime]`\nCopies the object if it hasn't been modified since the specified time.\n\n##### `--API-CopySourceRange=[string]`\nThe range of bytes to copy from the source object. The range value must use the form bytes=first-last, where the first and last are the zero-based byte offsets to copy. For example, bytes=0-9 indicates that you want to copy the first ten bytes of the source. You can copy a range only if the source object is greater than 5 GB.\n\n##### `--API-CopySourceSSECustomerAlgorithm=[string]`\nSpecifies the algorithm to use when decrypting the source object (e.g., AES256).\n\n##### `--API-CopySourceSSECustomerKeyMD5=[string]`\nSpecifies the 128-bit MD5 digest of the encryption key according to RFC 1321. Amazon S3 uses this header for a message integrity check to ensure the encryption key was transmitted without error. Please note that this parameter is automatically populated if it is not provided. Including this parameter is not required\n\n##### `--API-CopySourceSSECustomerKey=[string]`\nSpecifies the customer-provided encryption key for Amazon S3 to use to decrypt the source object. The encryption key provided in this header must be one that was used when the source object was created.\n\n##### `--API-ETag=[string]`\nEntity tag returned when the part was uploaded.\n\n##### `--API-Expires=[datetime]`\nThe date and time at which the object is no longer cacheable.\n\n##### `--API-GrantFullControl=[string]`\nGives the grantee READ, READ_ACP, and WRITE_ACP permissions on the object.\n\n##### `--API-GrantReadACP=[string]`\nAllows grantee to read the object ACL.\n\n##### `--API-GrantRead=[string]`\nAllows grantee to read the object data and its metadata.\n\n##### `--API-GrantWriteACP=[string]`\nAllows grantee to write the ACL for the applicable object.\n\n##### `--API-IfMatch=[string]`\nReturn the object only if its entity tag (ETag) is the same as the one specified, otherwise return a 412 (precondition failed).\n\n##### `--API-IfModifiedSince=[datetime]`\nReturn the object only if it has been modified since the specified time, otherwise return a 304 (not modified).\n\n##### `--API-IfNoneMatch=[string]`\nReturn the object only if its entity tag (ETag) is different from the one specified, otherwise return a 304 (not modified).\n\n##### `--API-IfUnmodifiedSince=[datetime]`\nReturn the object only if it has not been modified since the specified time, otherwise return a 412 (precondition failed).\n\n##### `--API-Metadata=[dict]`\nA map (in json string) of metadata to store with the object in S3\n\n##### `--API-MetadataDirective=[string]`\nSpecifies whether the metadata is copied from the source object or replaced with metadata provided in the request.\n\n##### `--API-MFA=[string]`\nThe concatenation of the authentication device's serial number, a space, and the value that is displayed on your authentication device.\n\n##### `--API-RequestPayer=[string]`\nConfirms that the requester knows that she or he will be charged for the request. Bucket owners need not specify this parameter in their requests. Documentation on downloading objects from requester pays buckets can be found at http://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectsinRequesterPaysBuckets.html\n\n##### `--API-ServerSideEncryption=[string]`\nThe Server-side encryption algorithm used when storing this object in S3 (e.g., AES256, aws:kms).\n\n##### `--API-SSECustomerAlgorithm=[string]`\nSpecifies the algorithm to use to when encrypting the object (e.g., AES256).\n\n##### `--API-SSECustomerKeyMD5=[string]`\nSpecifies the 128-bit MD5 digest of the encryption key according to RFC 1321. Amazon S3 uses this header for a message integrity check to ensure the encryption key was transmitted without error. Please note that this parameter is automatically populated if it is not provided. Including this parameter is not required\n\n##### `--API-SSECustomerKey=[string]`\nSpecifies the customer-provided encryption key for Amazon S3 to use in encrypting data. This value is used to store the object and then it is discarded; Amazon does not store the encryption key. The key must be appropriate for use with the algorithm specified in the x-amz-server-side-encryption-customer-algorithm header.\n\n##### `--API-SSEKMSKeyId=[string]`\nSpecifies the AWS KMS key ID to use for object encryption. All GET and PUT requests for an object protected by AWS KMS will fail if not made via SSL or using SigV4. Documentation on configuring any of the officially supported AWS SDKs and CLI can be found at http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#specify-signature-version\n\n##### `--API-StorageClass=[string]`\nThe type of storage to use for the object. Defaults to 'STANDARD'.\n\n##### `--API-VersionId=[string]`\nVersionId used to reference a specific version of the object.\n\n##### `--API-WebsiteRedirectLocation=[string]`\nIf the bucket is configured as a website, redirects requests for this object to another object in the same bucket or to an external URL. Amazon S3 stores the value of this header in the object metadata.\n\n\n## Debugging Tips\n\nSimply enable `--debug` option to see the full log of s4cmd. If you even need to check what APIs are invoked from s4cmd to boto3, you can run:\n\n```\ns4cmd --debug [op] .... 2>&1 >/dev/null | grep S3APICALL\n```\n\nTo see all the parameters sending to S3 API.\n\n\n## Compatibility between s3cmd and s4cmd\n\nPrefix matching: In s3cmd, unlike traditional filesystems, prefix names match listings:\n\n```\n>> s3cmd ls s3://my-bucket/ch\ns3://my-bucket/charlie/\ns3://my-bucket/chyang/\n```\n\nIn s4cmd, behavior is the same as with a Unix shell:\n\n```\n>>s4cmd ls s3://my-bucket/ch\n>(empty)\n```\n\nTo get prefix behavior, use explicit wildcards instead: s4cmd ls s3://my-bucket/ch*\n\nSimilarly, sync and cp commands emulate the Unix cp command, so directory to\ndirectory sync use different syntax:\n\n```\n>> s3cmd sync s3://bucket/path/dirA s3://bucket/path/dirB/\n```\nwill copy contents in dirA to dirB.\n```\n>> s4cmd sync s3://bucket/path/dirA s3://bucket/path/dirB/\n```\nwill copy dirA *into* dirB.\n\nTo achieve the s3cmd behavior, use wildcards:\n```\ns4cmd sync s3://bucket/path/dirA/* s3://bucket/path/dirB/\n```\n\nNote s4cmd doesn't support dirA without trailing slash indicating dirA/* as\nwhat rsync supported.\n\nNo automatic override for put command:\ns3cmd put fileA s3://bucket/path/fileB will return error if fileB exists.\nUse -f as well as get command.\n\nBugfixes for handling of non-existent paths: Often s3cmd creates empty files when specified paths do not exist:\ns3cmd get s3://my-bucket/no_such_file downloads an empty file.\ns4cmd get s3://my-bucket/no_such_file returns an error.\ns3cmd put no_such_file s3://my-bucket/ uploads an empty file.\ns4cmd put no_such_file s3://my-bucket/ returns an error.\n\n\n## Additional technical notes\n\nEtags, MD5s and multi-part uploads: Traditionally, the etag of an object in S3\nhas been its MD5.  However, this changed with the introduction of S3 multi-part\nuploads; in this case the etag is still a unique ID, but it is not the MD5 of\nthe file. Amazon has not revealed the definition of the etag in this case, so\nthere is no way we can calculate and compare MD5s based on the etag header in\ngeneral. The workaround we use is to upload the MD5 as a supplemental content\nheader (called \"md5\", instead of \"etag\"). This enables s4cmd to check the MD5\nhash before upload or download. The only limitation is that this only works for\nfiles uploaded via s4cmd. Programs that do not understand this header will\nstill have to download and verify the MD5 directly.\n\n\n## Unimplemented features\n\n- CloudFront or other feature support beyond basic S3 access.\n\n## Credits\n\n* Bloomreach http://www.bloomreach.com\n* Onera http://www.onera.com", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/bloomreach/s4cmd", "keywords": "", "license": "http://www.apache.org/licenses/LICENSE-2.0", "maintainer": "", "maintainer_email": "", "name": "s4cmd", "package_url": "https://pypi.org/project/s4cmd/", "platform": "", "project_url": "https://pypi.org/project/s4cmd/", "project_urls": {"Homepage": "https://github.com/bloomreach/s4cmd"}, "release_url": "https://pypi.org/project/s4cmd/2.1.0/", "requires_dist": null, "requires_python": "", "summary": "Super S3 command line tool", "version": "2.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>s4cmd</h1>\n<h3>Super S3 command line tool</h3>\n<p><a href=\"https://travis-ci.com/bloomreach/s4cmd\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bae7377f969405f0e39c3c2e8fa2970045d2557c/68747470733a2f2f7472617669732d63692e636f6d2f626c6f6f6d72656163682f7334636d642e7376673f6272616e63683d6d6173746572\"></a> <a href=\"https://gitter.im/bloomreach/s4cmd?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge\" rel=\"nofollow\"><img alt=\"Join the chat at https://gitter.im/bloomreach/     s4cmd\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/454be82554a06af0fd3393415ef17b59d8550498/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667\"></a></p>\n<hr>\n<p><strong>Author</strong>: Chou-han Yang (<a href=\"https://github.com/chouhanyang\" rel=\"nofollow\">@chouhanyang</a>)</p>\n<p><strong>Current Maintainers</strong>: Naveen Vardhi (<a href=\"https://github.com/rozuur\" rel=\"nofollow\">@rozuur</a>) | Navin Pai (<a href=\"https://github.com/navinpai\" rel=\"nofollow\">@navinpai</a>)</p>\n<hr>\n<h2>What's New in s4cmd 2.x</h2>\n<ul>\n<li>Fully migrated from old boto 2.x to new <a href=\"http://boto3.readthedocs.io/en/latest/reference/services/s3.html\" rel=\"nofollow\">boto3</a>  library, which provides more reliable and up-to-date S3 backend.</li>\n<li>Support S3 <code>--API-ServerSideEncryption</code> along with <strong>36 new API pass-through options</strong>. See API pass-through options section for complete list.</li>\n<li>Support batch delete (with delete_objects API) to delete up to 1000 files with single call. <strong>100+ times faster</strong> than sequential deletion.</li>\n<li>Support <code>S4CMD_OPTS</code> environment variable for commonly used options such as <code>--API-ServerSideEncryption</code> across all your s4cmd operations.</li>\n<li>Support moving files <strong>larger than 5GB</strong> with multipart upload. <strong>20+ times faster</strong> then sequential move operation when moving large files.</li>\n<li>Support timestamp filtering with <code>--last-modified-before</code> and <code>--last-modified-after</code> options for all operations. Human friendly timestamps are supported, e.g. <code>--last-modified-before='2 months ago'</code></li>\n<li>Faster upload with lazy evaluation of md5 hash.</li>\n<li>Listing large number of files with S3 pagination, with memory is the limit.</li>\n<li>New directory to directory <code>dsync</code> command is better and standalone implementation to replace old <code>sync</code> command, which is implemented based on top of get/put/mv commands. <code>--delete-removed</code> work for all cases including local to s3, s3 to local, and s3 to s3. <code>sync</code> command preserves the old behavior in this version for compatibility.</li>\n<li>Tested on both python 2 and 3.</li>\n<li>Special thanks to <a href=\"http://www.onera.com\" rel=\"nofollow\">onera.com</a> for supporting s4cmd.</li>\n</ul>\n<h2>Motivation</h2>\n<p>S4cmd is a command-line utility for accessing\n<a href=\"http://en.wikipedia.org/wiki/Amazon_S3\" rel=\"nofollow\">Amazon S3</a>, inspired by\n<a href=\"http://s3tools.org/s3cmd\" rel=\"nofollow\">s3cmd</a>.</p>\n<p>We have used s3cmd heavily for a number of scripted, data-intensive\napplications. However as the need for a variety of small improvements arose, we\ncreated our own implementation, s4cmd. It is intended as an alternative to\ns3cmd for enhanced performance and for large files, and with a number of\nadditional features and fixes that we have found useful.</p>\n<p>It strives to be compatible with the most common usage scenarios for s3cmd. It\ndoes not offer exact drop-in compatibility, due to a number of corner cases where\ndifferent behavior seems preferable, or for bugfixes.</p>\n<h2>Features</h2>\n<p>S4cmd supports the regular commands you might expect for fetching and storing\nfiles in S3: <code>ls</code>, <code>put</code>, <code>get</code>, <code>cp</code>, <code>mv</code>, <code>sync</code>, <code>del</code>, <code>du</code>.</p>\n<p>The main features that distinguish s4cmd are:</p>\n<ul>\n<li>Simple (less than 1500 lines of code) and implemented in pure Python, based\non the widely used <a href=\"https://github.com/boto/boto3\" rel=\"nofollow\">Boto3</a> library.</li>\n<li>Multi-threaded/multi-connection implementation for enhanced performance on all\ncommands. As with many network-intensive applications (like web browsers),\naccessing S3 in a single-threaded way is often significantly less efficient than\nhaving multiple connections actively transferring data at once.  In general, we\nget a 2X boost to upload/download speeds from this.</li>\n<li>Path handling: S3 is not a traditional filesystem with built-in support for\ndirectory structure: internally, there are only objects, not directories or\nfolders. However, most people use S3 in a hierarchical structure, with paths\nseparated by slashes, to emulate traditional filesystems. S4cmd follows\nconventions to more closely replicate the behavior of traditional filesystems\nin certain corner cases.  For example, \"ls\" and \"cp\" work much like in Unix\nshells, to avoid odd surprises. (For examples see compatibility notes below.)</li>\n<li>Wildcard support: Wildcards, including multiple levels of wildcards, like in\nUnix shells, are handled. For example:\ns3://my-bucket/my-folder/20120512/*/*chunk00?1?</li>\n<li>Automatic retry: Failure tasks will be executed again after a delay.</li>\n<li>Multi-part upload support for files larger than 5GB.</li>\n<li>Handling of MD5s properly with respect to multi-part uploads (for the sordid\ndetails of this, see below).</li>\n<li>Miscellaneous enhancements and bugfixes:\n<ul>\n<li>Partial file creation: Avoid creating empty target files if source does not\nexist. Avoid creating partial output files when commands are interrupted.</li>\n<li>General thread safety: Tool can be interrupted or killed at any time without\nbeing blocked by child threads or leaving incomplete or corrupt files in\nplace.</li>\n<li>Ensure exit code is nonzero on all failure scenarios (a very important\nfeature in scripts).</li>\n<li>Expected handling of symlinks (they are followed).</li>\n<li>Support both <code>s3://</code> and <code>s3n://</code> prefixes (the latter is common with\nAmazon Elastic Mapreduce).</li>\n</ul>\n</li>\n</ul>\n<p>Limitations:</p>\n<ul>\n<li>No CloudFront or other feature support.</li>\n<li>Currently, we simulate <code>sync</code> with <code>get</code> and <code>put</code> with <code>--recursive --force --sync-check</code>.</li>\n</ul>\n<h2>Installation and Setup</h2>\n<p>You can install <code>s4cmd</code> <a href=\"https://pypi.python.org/pypi/s4cmd\" rel=\"nofollow\">PyPI</a>.</p>\n<pre><code>pip install s4cmd\n</code></pre>\n<ul>\n<li>Copy or create a symbolic link so you can run <code>s4cmd.py</code> as <code>s4cmd</code>. (It is just\na single file!)</li>\n<li>If you already have a <code>~/.s3cfg</code> file from configuring <code>s3cmd</code>, credentials\nfrom this file will be used.  Otherwise, set the <code>S3_ACCESS_KEY</code> and\n<code>S3_SECRET_KEY</code> environment variables to contain your S3 credentials.</li>\n<li>If no keys are provided, but an IAM role is associated with the EC2 instance, it will\nbe used transparently.</li>\n</ul>\n<h2>s4cmd Commands</h2>\n<h4><code>s4cmd ls [path]</code></h4>\n<p>List all contents of a directory.</p>\n<ul>\n<li>-r/--recursive: recursively display all contents including subdirectories under the given path.</li>\n<li>-d/--show-directory: show the directory entry instead of its content.</li>\n</ul>\n<h4><code>s4cmd put [source] [target]</code></h4>\n<p>Upload local files up to S3.</p>\n<ul>\n<li>-r/--recursive: also upload directories recursively.</li>\n<li>-s/--sync-check: check md5 hash to avoid uploading the same content.</li>\n<li>-f/--force: override existing file instead of showing error message.</li>\n<li>-n/--dry-run: emulate the operation without real upload.</li>\n</ul>\n<h4><code>s4cmd get [source] [target]</code></h4>\n<p>Download files from S3 to local filesystem.</p>\n<ul>\n<li>-r/--recursive: also download directories recursively.</li>\n<li>-s/--sync-check: check md5 hash to avoid downloading the same content.</li>\n<li>-f/--force: override existing file instead of showing error message.</li>\n<li>-n/--dry-run: emulate the operation without real download.</li>\n</ul>\n<h4><code>s4cmd dsync [source dir] [target dir]</code></h4>\n<p>Synchronize the contents of two directories. The directory can either be local or remote, but currently, it doesn't support two local directories.</p>\n<ul>\n<li>-r/--recursive: also sync directories recursively.</li>\n<li>-s/--sync-check: check md5 hash to avoid syncing the same content.</li>\n<li>-f/--force: override existing file instead of showing error message.</li>\n<li>-n/--dry-run: emulate the operation without real sync.</li>\n<li>--delete-removed: delete files not in source directory.</li>\n</ul>\n<h4><code>s4cmd sync [source] [target]</code></h4>\n<p>(Obsolete, use <code>dsync</code> instead) Synchronize the contents of two directories. The directory can either be local or remote, but currently, it doesn't support two local directories. This command simply invoke get/put/mv commands.</p>\n<ul>\n<li>-r/--recursive: also sync directories recursively.</li>\n<li>-s/--sync-check: check md5 hash to avoid syncing the same content.</li>\n<li>-f/--force: override existing file instead of showing error message.</li>\n<li>-n/--dry-run: emulate the operation without real sync.</li>\n<li>--delete-removed: delete files not in source directory. Only works when syncing local directory to s3 directory.</li>\n</ul>\n<h4><code>s4cmd cp [source] [target]</code></h4>\n<p>Copy a file or a directory from a S3 location to another.</p>\n<ul>\n<li>-r/--recursive: also copy directories recursively.</li>\n<li>-s/--sync-check: check md5 hash to avoid copying the same content.</li>\n<li>-f/--force: override existing file instead of showing error message.</li>\n<li>-n/--dry-run: emulate the operation without real copy.</li>\n</ul>\n<h4><code>s4cmd mv [source] [target]</code></h4>\n<p>Move a file or a directory from a S3 location to another.</p>\n<ul>\n<li>-r/--recursive: also move directories recursively.</li>\n<li>-s/--sync-check: check md5 hash to avoid moving the same content.</li>\n<li>-f/--force: override existing file instead of showing error message.</li>\n<li>-n/--dry-run: emulate the operation without real move.</li>\n</ul>\n<h4><code>s4cmd del [path]</code></h4>\n<p>Delete files or directories on S3.</p>\n<ul>\n<li>-r/--recursive: also delete directories recursively.</li>\n<li>-n/--dry-run: emulate the operation without real delete.</li>\n</ul>\n<h4><code>s4cmd du [path]</code></h4>\n<p>Get the size of the given directory.</p>\n<p>Available parameters:</p>\n<ul>\n<li>-r/--recursive: also add sizes of sub-directories recursively.</li>\n</ul>\n<h2>s4cmd Control Options</h2>\n<h5><code>-p S3CFG, --config=[filename]</code></h5>\n<p>path to s3cfg config file</p>\n<h5><code>-f, --force</code></h5>\n<p>force overwrite files when download or upload</p>\n<h5><code>-r, --recursive</code></h5>\n<p>recursively checking subdirectories</p>\n<h5><code>-s, --sync-check</code></h5>\n<p>check file md5 before download or upload</p>\n<h5><code>-n, --dry-run</code></h5>\n<p>trial run without actual download or upload</p>\n<h5><code>-t RETRY, --retry=[integer]</code></h5>\n<p>number of retries before giving up</p>\n<h5><code>--retry-delay=[integer]</code></h5>\n<p>seconds to sleep between retries</p>\n<h5><code>-c NUM_THREADS, --num-threads=NUM_THREADS</code></h5>\n<p>number of concurrent threads</p>\n<h5><code>--endpoint-url</code></h5>\n<p>endpoint url used in boto3 client</p>\n<h5><code>-d, --show-directory</code></h5>\n<p>show directory instead of its content</p>\n<h5><code>--ignore-empty-source</code></h5>\n<p>ignore empty source from s3</p>\n<h5><code>--use-ssl</code></h5>\n<p>(obsolete) use SSL connection to S3</p>\n<h5><code>--verbose</code></h5>\n<p>verbose output</p>\n<h5><code>--debug</code></h5>\n<p>debug output</p>\n<h5><code>--validate</code></h5>\n<p>(obsolete) validate lookup operation</p>\n<h5><code>-D, --delete-removed</code></h5>\n<p>delete remote files that do not exist in source after sync</p>\n<h5><code>--multipart-split-size=[integer]</code></h5>\n<p>size in bytes to split multipart transfers</p>\n<h5><code>--max-singlepart-download-size=[integer]</code></h5>\n<p>files with size (in bytes) greater than this will be\ndownloaded in multipart transfers</p>\n<h5><code>--max-singlepart-upload-size=[integer]</code></h5>\n<p>files with size (in bytes) greater than this will be\nuploaded in multipart transfers</p>\n<h5><code>--max-singlepart-copy-size=[integer]</code></h5>\n<p>files with size (in bytes) greater than this will be\ncopied in multipart transfers</p>\n<h5><code>--batch-delete-size=[integer]</code></h5>\n<p>Number of files (&lt;1000) to be combined in batch delete.</p>\n<h5><code>--last-modified-before=[datetime]</code></h5>\n<p>Condition on files where their last modified dates are\nbefore given parameter.</p>\n<h5><code>--last-modified-after=[datetime]</code></h5>\n<p>Condition on files where their last modified dates are\nafter given parameter.</p>\n<h2>S3 API Pass-through Options</h2>\n<p>Those options are directly translated to boto3 API commands. The options provided will be filtered by the APIs that are taking parameters. For example, <code>--API-ServerSideEncryption</code> is only needed for <code>put_object</code>, <code>create_multipart_upload</code> but not for <code>list_buckets</code> and <code>get_objects</code> for example. Therefore, providing <code>--API-ServerSideEncryption</code> for <code>s4cmd ls</code> has no effect.</p>\n<p>For more information, please see boto3 s3 documentations <a href=\"http://boto3.readthedocs.io/en/latest/reference/services/s3.html\" rel=\"nofollow\">http://boto3.readthedocs.io/en/latest/reference/services/s3.html</a></p>\n<h5><code>--API-ACL=[string]</code></h5>\n<p>The canned ACL to apply to the object.</p>\n<h5><code>--API-CacheControl=[string]</code></h5>\n<p>Specifies caching behavior along the request/reply chain.</p>\n<h5><code>--API-ContentDisposition=[string]</code></h5>\n<p>Specifies presentational information for the object.</p>\n<h5><code>--API-ContentEncoding=[string]</code></h5>\n<p>Specifies what content encodings have been applied to the object and thus what decoding mechanisms must be applied to obtain the media-type referenced by the Content-Type header field.</p>\n<h5><code>--API-ContentLanguage=[string]</code></h5>\n<p>The language the content is in.</p>\n<h5><code>--API-ContentMD5=[string]</code></h5>\n<p>The base64-encoded 128-bit MD5 digest of the part data.</p>\n<h5><code>--API-ContentType=[string]</code></h5>\n<p>A standard MIME type describing the format of the object data.</p>\n<h5><code>--API-CopySourceIfMatch=[string]</code></h5>\n<p>Copies the object if its entity tag (ETag) matches the specified tag.</p>\n<h5><code>--API-CopySourceIfModifiedSince=[datetime]</code></h5>\n<p>Copies the object if it has been modified since the specified time.</p>\n<h5><code>--API-CopySourceIfNoneMatch=[string]</code></h5>\n<p>Copies the object if its entity tag (ETag) is different than the specified ETag.</p>\n<h5><code>--API-CopySourceIfUnmodifiedSince=[datetime]</code></h5>\n<p>Copies the object if it hasn't been modified since the specified time.</p>\n<h5><code>--API-CopySourceRange=[string]</code></h5>\n<p>The range of bytes to copy from the source object. The range value must use the form bytes=first-last, where the first and last are the zero-based byte offsets to copy. For example, bytes=0-9 indicates that you want to copy the first ten bytes of the source. You can copy a range only if the source object is greater than 5 GB.</p>\n<h5><code>--API-CopySourceSSECustomerAlgorithm=[string]</code></h5>\n<p>Specifies the algorithm to use when decrypting the source object (e.g., AES256).</p>\n<h5><code>--API-CopySourceSSECustomerKeyMD5=[string]</code></h5>\n<p>Specifies the 128-bit MD5 digest of the encryption key according to RFC 1321. Amazon S3 uses this header for a message integrity check to ensure the encryption key was transmitted without error. Please note that this parameter is automatically populated if it is not provided. Including this parameter is not required</p>\n<h5><code>--API-CopySourceSSECustomerKey=[string]</code></h5>\n<p>Specifies the customer-provided encryption key for Amazon S3 to use to decrypt the source object. The encryption key provided in this header must be one that was used when the source object was created.</p>\n<h5><code>--API-ETag=[string]</code></h5>\n<p>Entity tag returned when the part was uploaded.</p>\n<h5><code>--API-Expires=[datetime]</code></h5>\n<p>The date and time at which the object is no longer cacheable.</p>\n<h5><code>--API-GrantFullControl=[string]</code></h5>\n<p>Gives the grantee READ, READ_ACP, and WRITE_ACP permissions on the object.</p>\n<h5><code>--API-GrantReadACP=[string]</code></h5>\n<p>Allows grantee to read the object ACL.</p>\n<h5><code>--API-GrantRead=[string]</code></h5>\n<p>Allows grantee to read the object data and its metadata.</p>\n<h5><code>--API-GrantWriteACP=[string]</code></h5>\n<p>Allows grantee to write the ACL for the applicable object.</p>\n<h5><code>--API-IfMatch=[string]</code></h5>\n<p>Return the object only if its entity tag (ETag) is the same as the one specified, otherwise return a 412 (precondition failed).</p>\n<h5><code>--API-IfModifiedSince=[datetime]</code></h5>\n<p>Return the object only if it has been modified since the specified time, otherwise return a 304 (not modified).</p>\n<h5><code>--API-IfNoneMatch=[string]</code></h5>\n<p>Return the object only if its entity tag (ETag) is different from the one specified, otherwise return a 304 (not modified).</p>\n<h5><code>--API-IfUnmodifiedSince=[datetime]</code></h5>\n<p>Return the object only if it has not been modified since the specified time, otherwise return a 412 (precondition failed).</p>\n<h5><code>--API-Metadata=[dict]</code></h5>\n<p>A map (in json string) of metadata to store with the object in S3</p>\n<h5><code>--API-MetadataDirective=[string]</code></h5>\n<p>Specifies whether the metadata is copied from the source object or replaced with metadata provided in the request.</p>\n<h5><code>--API-MFA=[string]</code></h5>\n<p>The concatenation of the authentication device's serial number, a space, and the value that is displayed on your authentication device.</p>\n<h5><code>--API-RequestPayer=[string]</code></h5>\n<p>Confirms that the requester knows that she or he will be charged for the request. Bucket owners need not specify this parameter in their requests. Documentation on downloading objects from requester pays buckets can be found at <a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectsinRequesterPaysBuckets.html\" rel=\"nofollow\">http://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectsinRequesterPaysBuckets.html</a></p>\n<h5><code>--API-ServerSideEncryption=[string]</code></h5>\n<p>The Server-side encryption algorithm used when storing this object in S3 (e.g., AES256, aws:kms).</p>\n<h5><code>--API-SSECustomerAlgorithm=[string]</code></h5>\n<p>Specifies the algorithm to use to when encrypting the object (e.g., AES256).</p>\n<h5><code>--API-SSECustomerKeyMD5=[string]</code></h5>\n<p>Specifies the 128-bit MD5 digest of the encryption key according to RFC 1321. Amazon S3 uses this header for a message integrity check to ensure the encryption key was transmitted without error. Please note that this parameter is automatically populated if it is not provided. Including this parameter is not required</p>\n<h5><code>--API-SSECustomerKey=[string]</code></h5>\n<p>Specifies the customer-provided encryption key for Amazon S3 to use in encrypting data. This value is used to store the object and then it is discarded; Amazon does not store the encryption key. The key must be appropriate for use with the algorithm specified in the x-amz-server-side-encryption-customer-algorithm header.</p>\n<h5><code>--API-SSEKMSKeyId=[string]</code></h5>\n<p>Specifies the AWS KMS key ID to use for object encryption. All GET and PUT requests for an object protected by AWS KMS will fail if not made via SSL or using SigV4. Documentation on configuring any of the officially supported AWS SDKs and CLI can be found at <a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#specify-signature-version\" rel=\"nofollow\">http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#specify-signature-version</a></p>\n<h5><code>--API-StorageClass=[string]</code></h5>\n<p>The type of storage to use for the object. Defaults to 'STANDARD'.</p>\n<h5><code>--API-VersionId=[string]</code></h5>\n<p>VersionId used to reference a specific version of the object.</p>\n<h5><code>--API-WebsiteRedirectLocation=[string]</code></h5>\n<p>If the bucket is configured as a website, redirects requests for this object to another object in the same bucket or to an external URL. Amazon S3 stores the value of this header in the object metadata.</p>\n<h2>Debugging Tips</h2>\n<p>Simply enable <code>--debug</code> option to see the full log of s4cmd. If you even need to check what APIs are invoked from s4cmd to boto3, you can run:</p>\n<pre><code>s4cmd --debug [op] .... 2&gt;&amp;1 &gt;/dev/null | grep S3APICALL\n</code></pre>\n<p>To see all the parameters sending to S3 API.</p>\n<h2>Compatibility between s3cmd and s4cmd</h2>\n<p>Prefix matching: In s3cmd, unlike traditional filesystems, prefix names match listings:</p>\n<pre><code>&gt;&gt; s3cmd ls s3://my-bucket/ch\ns3://my-bucket/charlie/\ns3://my-bucket/chyang/\n</code></pre>\n<p>In s4cmd, behavior is the same as with a Unix shell:</p>\n<pre><code>&gt;&gt;s4cmd ls s3://my-bucket/ch\n&gt;(empty)\n</code></pre>\n<p>To get prefix behavior, use explicit wildcards instead: s4cmd ls s3://my-bucket/ch*</p>\n<p>Similarly, sync and cp commands emulate the Unix cp command, so directory to\ndirectory sync use different syntax:</p>\n<pre><code>&gt;&gt; s3cmd sync s3://bucket/path/dirA s3://bucket/path/dirB/\n</code></pre>\n<p>will copy contents in dirA to dirB.</p>\n<pre><code>&gt;&gt; s4cmd sync s3://bucket/path/dirA s3://bucket/path/dirB/\n</code></pre>\n<p>will copy dirA <em>into</em> dirB.</p>\n<p>To achieve the s3cmd behavior, use wildcards:</p>\n<pre><code>s4cmd sync s3://bucket/path/dirA/* s3://bucket/path/dirB/\n</code></pre>\n<p>Note s4cmd doesn't support dirA without trailing slash indicating dirA/* as\nwhat rsync supported.</p>\n<p>No automatic override for put command:\ns3cmd put fileA s3://bucket/path/fileB will return error if fileB exists.\nUse -f as well as get command.</p>\n<p>Bugfixes for handling of non-existent paths: Often s3cmd creates empty files when specified paths do not exist:\ns3cmd get s3://my-bucket/no_such_file downloads an empty file.\ns4cmd get s3://my-bucket/no_such_file returns an error.\ns3cmd put no_such_file s3://my-bucket/ uploads an empty file.\ns4cmd put no_such_file s3://my-bucket/ returns an error.</p>\n<h2>Additional technical notes</h2>\n<p>Etags, MD5s and multi-part uploads: Traditionally, the etag of an object in S3\nhas been its MD5.  However, this changed with the introduction of S3 multi-part\nuploads; in this case the etag is still a unique ID, but it is not the MD5 of\nthe file. Amazon has not revealed the definition of the etag in this case, so\nthere is no way we can calculate and compare MD5s based on the etag header in\ngeneral. The workaround we use is to upload the MD5 as a supplemental content\nheader (called \"md5\", instead of \"etag\"). This enables s4cmd to check the MD5\nhash before upload or download. The only limitation is that this only works for\nfiles uploaded via s4cmd. Programs that do not understand this header will\nstill have to download and verify the MD5 directly.</p>\n<h2>Unimplemented features</h2>\n<ul>\n<li>CloudFront or other feature support beyond basic S3 access.</li>\n</ul>\n<h2>Credits</h2>\n<ul>\n<li>Bloomreach <a href=\"http://www.bloomreach.com\" rel=\"nofollow\">http://www.bloomreach.com</a></li>\n<li>Onera <a href=\"http://www.onera.com\" rel=\"nofollow\">http://www.onera.com</a></li>\n</ul>\n\n          </div>"}, "last_serial": 4167185, "releases": {"1.5.19": [{"comment_text": "", "digests": {"md5": "ae7d4eed5aa2e830a6e20c111071c76e", "sha256": "a14e1d859b8ae680693e7a1c1d479ba1078adc05ff435295b5e30aaebed58dbf"}, "downloads": -1, "filename": "s4cmd-1.5.19.tar.gz", "has_sig": false, "md5_digest": "ae7d4eed5aa2e830a6e20c111071c76e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15333, "upload_time": "2014-09-04T22:35:12", "upload_time_iso_8601": "2014-09-04T22:35:12.257625Z", "url": "https://files.pythonhosted.org/packages/86/70/1e2d0cda5f207c45b5cfba0e307ec65d116b5094e9548dd7ae8bf71fa81d/s4cmd-1.5.19.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "efdea7ad19927ce629b943236355c4e9", "sha256": "3013bc78cadf6f50e20375af5d75edaf91b7d9f03a78d78b3c71289186767e75"}, "downloads": -1, "filename": "s4cmd-1.5.20.tar.gz", "has_sig": false, "md5_digest": "efdea7ad19927ce629b943236355c4e9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25527, "upload_time": "2015-02-18T22:12:40", "upload_time_iso_8601": "2015-02-18T22:12:40.863702Z", "url": "https://files.pythonhosted.org/packages/e7/3b/e4dce078286a00d3dd90a9ef59d2d17400196e7e704a00e62d8f39d6d66c/s4cmd-1.5.20.tar.gz", "yanked": false}], "1.5.20": [], "2.0.0": [{"comment_text": "", "digests": {"md5": "70c30eb59c2a77a1f3a9776f16a86d79", "sha256": "19b498b95c6e88e1713e4ebc95554634d6b59071a289b4edfea4ce00ea848cec"}, "downloads": -1, "filename": "s4cmd-2.0.0-py2-none-any.whl", "has_sig": false, "md5_digest": "70c30eb59c2a77a1f3a9776f16a86d79", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 147597, "upload_time": "2016-05-12T00:17:49", "upload_time_iso_8601": "2016-05-12T00:17:49.949767Z", "url": "https://files.pythonhosted.org/packages/84/d3/1833c8b2c5ccffc421617b180d83a444a6882552a5907706f13c8b72baa3/s4cmd-2.0.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bf503ec7e4ce4988ef73a3d2c744c4d3", "sha256": "2166d819d4ebf8ffd1d442f67efffc877f721e670de29d335b3065394d2c656d"}, "downloads": -1, "filename": "s4cmd-2.0.0.tar.gz", "has_sig": false, "md5_digest": "bf503ec7e4ce4988ef73a3d2c744c4d3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21212, "upload_time": "2016-05-12T00:17:12", "upload_time_iso_8601": "2016-05-12T00:17:12.122032Z", "url": "https://files.pythonhosted.org/packages/fd/19/67e763f6d75a9d88708ea16a8d9b05131a7c4918ff2c81c4311be48cb503/s4cmd-2.0.0.tar.gz", "yanked": false}], "2.0.1": [{"comment_text": "", "digests": {"md5": "68a566e0fae997e7d4bc6933b1c58deb", "sha256": "c95659da0d2bfcd6222e9628d059810c249bd667c391500804d2bcdeeb6b8a14"}, "downloads": -1, "filename": "s4cmd-2.0.1-py2-none-any.whl", "has_sig": false, "md5_digest": "68a566e0fae997e7d4bc6933b1c58deb", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 149427, "upload_time": "2016-05-16T10:06:50", "upload_time_iso_8601": "2016-05-16T10:06:50.555305Z", "url": "https://files.pythonhosted.org/packages/d2/cd/6a7be0d744aa13b3acdc9711f6464d5ad9d5bfe5619f550b6058d6a7e22d/s4cmd-2.0.1-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2e83074531dedb22748cb9602f5c3de8", "sha256": "e64bc965e240d56440e55f8be1514af679fd6f6e49b43ffac03aa84ea4bdee91"}, "downloads": -1, "filename": "s4cmd-2.0.1.tar.gz", "has_sig": false, "md5_digest": "2e83074531dedb22748cb9602f5c3de8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21412, "upload_time": "2016-05-16T10:06:26", "upload_time_iso_8601": "2016-05-16T10:06:26.014363Z", "url": "https://files.pythonhosted.org/packages/f4/ae/ae50df9008a2df2acb0a2c8b56a68f9da50d2129f48140af03564c61649a/s4cmd-2.0.1.tar.gz", "yanked": false}], "2.1.0": [{"comment_text": "", "digests": {"md5": "ae67774b01f02b2868aa4596d3d4b676", "sha256": "42566058a74d3e1e553351966efaaffa08e4b6ac28a19e72a51be21151ea9534"}, "downloads": -1, "filename": "s4cmd-2.1.0.tar.gz", "has_sig": false, "md5_digest": "ae67774b01f02b2868aa4596d3d4b676", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 27534, "upload_time": "2018-08-13T22:59:41", "upload_time_iso_8601": "2018-08-13T22:59:41.521893Z", "url": "https://files.pythonhosted.org/packages/42/b4/0061f4930958cd790098738659c1c39f8feaf688e698142435eedaa4ae34/s4cmd-2.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ae67774b01f02b2868aa4596d3d4b676", "sha256": "42566058a74d3e1e553351966efaaffa08e4b6ac28a19e72a51be21151ea9534"}, "downloads": -1, "filename": "s4cmd-2.1.0.tar.gz", "has_sig": false, "md5_digest": "ae67774b01f02b2868aa4596d3d4b676", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 27534, "upload_time": "2018-08-13T22:59:41", "upload_time_iso_8601": "2018-08-13T22:59:41.521893Z", "url": "https://files.pythonhosted.org/packages/42/b4/0061f4930958cd790098738659c1c39f8feaf688e698142435eedaa4ae34/s4cmd-2.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:59:31 2020"}