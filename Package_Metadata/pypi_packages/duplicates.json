{"info": {"author": "Stefan Sch\u00f6nberger", "author_email": "me@s5s9r.de", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: BSD License", "Operating System :: POSIX", "Programming Language :: Python :: 3"], "description": "# duplicates\n\n[![Build Status](https://travis-ci.org/sniner/duplicates.svg?branch=master)](https://travis-ci.org/sniner/duplicates)\n\nScan for identical files (duplicates) in subdirectories.\n\n## Requirements\n\n* Python >= 3.6\n* MS Windows is not supported\n\n## Description\n\nTo find files with identical content the given directories will be scanned and\nfor files of same size their SHA-256 fingerprints are calculated and compared.\nTwo files with identical fingerprints are considered to have the same content.\nThere is a tiny chance for two files with same fingerprint to have different\ncontent, but this chance is [very\nremote](https://stackoverflow.com/questions/4014090).\n\nSymbolic links and hidden entries are ignored by default, this behaviour can\nbe changed with CLI options `--follow`/`--hidden` and constructor options\n`ignore_hidden`/`ignore_symlinks`.\n\n## CLI examples\n\nThis one will give you a short command overview:\n\n```console\n$ duplicates --help\n```\n\nScan directories `dirA`, `dirB` and `dirC` for duplicates and report all found\nidentical files:\n\n```console\n$ duplicates dirA dirB dirC\n\ndirA/file01\n        dirA/file01.bak\n        dirB/file.bak\ndirA/file02\n        dirB/file02~\n```\n\nThe oldest file is printed without indent, all identical files are printed\nindented by a tab character. The oldest file is supposed to be the original.\n\nIf you are willing to take risks, you can delete all duplicates at once.\nI wouldn't dare, but you get the picture:\n\n```console\n$ duplicates --dups-only dirA dirB | while read dups ; do xargs -0 rm $dups ; done\n```\n\nWith `--dups-only` all duplicates for one original are output on one line,\nseparated by `\\0` (ASCII code zero).\n\nFor [fish shell](https://fishshell.com/) it looks almost identical:\n\n```console\n$ duplicates --dups-only dirA dirB | while read -la dups ; xargs -0 rm $dups ; end\n```\n\n## Python examples\n\n```python\nimport duplicates\n\ndf = duplicates.DupFinder(verbose=True)\nuniq, dups = df.scan(\".\")\n```\n\n`uniq` is a list of unique file objects. `dups` is a list of identical files,\nwhich in turn are lists of file objects, the first being the oldest element\nand thus the supposed original.\n\nA file object is a dict consisting of the following elements:\n\n* `path`: a pathlib.Path object\n* `age`: modification time in seconds ([Unix time](https://docs.python.org/3/library/os.html#os.stat_result))\n* `size`: file size in bytes\n* `hash`: the SHA-256 fingerprint (not calculated for unique files)\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/sniner/duplicates", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "duplicates", "package_url": "https://pypi.org/project/duplicates/", "platform": "any", "project_url": "https://pypi.org/project/duplicates/", "project_urls": {"Homepage": "https://github.com/sniner/duplicates"}, "release_url": "https://pypi.org/project/duplicates/0.1.0/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Find identical files in subdirectories", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>duplicates</h1>\n<p><a href=\"https://travis-ci.org/sniner/duplicates\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/015c7153a777e71832b711867f7a9c3c96575ad5/68747470733a2f2f7472617669732d63692e6f72672f736e696e65722f6475706c6963617465732e7376673f6272616e63683d6d6173746572\"></a></p>\n<p>Scan for identical files (duplicates) in subdirectories.</p>\n<h2>Requirements</h2>\n<ul>\n<li>Python &gt;= 3.6</li>\n<li>MS Windows is not supported</li>\n</ul>\n<h2>Description</h2>\n<p>To find files with identical content the given directories will be scanned and\nfor files of same size their SHA-256 fingerprints are calculated and compared.\nTwo files with identical fingerprints are considered to have the same content.\nThere is a tiny chance for two files with same fingerprint to have different\ncontent, but this chance is <a href=\"https://stackoverflow.com/questions/4014090\" rel=\"nofollow\">very\nremote</a>.</p>\n<p>Symbolic links and hidden entries are ignored by default, this behaviour can\nbe changed with CLI options <code>--follow</code>/<code>--hidden</code> and constructor options\n<code>ignore_hidden</code>/<code>ignore_symlinks</code>.</p>\n<h2>CLI examples</h2>\n<p>This one will give you a short command overview:</p>\n<pre><span class=\"gp\">$</span> duplicates --help\n</pre>\n<p>Scan directories <code>dirA</code>, <code>dirB</code> and <code>dirC</code> for duplicates and report all found\nidentical files:</p>\n<pre><span class=\"gp\">$</span> duplicates dirA dirB dirC\n\n<span class=\"go\">dirA/file01</span>\n<span class=\"go\">        dirA/file01.bak</span>\n<span class=\"go\">        dirB/file.bak</span>\n<span class=\"go\">dirA/file02</span>\n<span class=\"go\">        dirB/file02~</span>\n</pre>\n<p>The oldest file is printed without indent, all identical files are printed\nindented by a tab character. The oldest file is supposed to be the original.</p>\n<p>If you are willing to take risks, you can delete all duplicates at once.\nI wouldn't dare, but you get the picture:</p>\n<pre><span class=\"gp\">$</span> duplicates --dups-only dirA dirB <span class=\"p\">|</span> <span class=\"k\">while</span> <span class=\"nb\">read</span> dups <span class=\"p\">;</span> <span class=\"k\">do</span> xargs -0 rm <span class=\"nv\">$dups</span> <span class=\"p\">;</span> <span class=\"k\">done</span>\n</pre>\n<p>With <code>--dups-only</code> all duplicates for one original are output on one line,\nseparated by <code>\\0</code> (ASCII code zero).</p>\n<p>For <a href=\"https://fishshell.com/\" rel=\"nofollow\">fish shell</a> it looks almost identical:</p>\n<pre><span class=\"gp\">$</span> duplicates --dups-only dirA dirB <span class=\"p\">|</span> <span class=\"k\">while</span> <span class=\"nb\">read</span> -la dups <span class=\"p\">;</span> xargs -0 rm <span class=\"nv\">$dups</span> <span class=\"p\">;</span> end\n</pre>\n<h2>Python examples</h2>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">duplicates</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">duplicates</span><span class=\"o\">.</span><span class=\"n\">DupFinder</span><span class=\"p\">(</span><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">uniq</span><span class=\"p\">,</span> <span class=\"n\">dups</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">scan</span><span class=\"p\">(</span><span class=\"s2\">\".\"</span><span class=\"p\">)</span>\n</pre>\n<p><code>uniq</code> is a list of unique file objects. <code>dups</code> is a list of identical files,\nwhich in turn are lists of file objects, the first being the oldest element\nand thus the supposed original.</p>\n<p>A file object is a dict consisting of the following elements:</p>\n<ul>\n<li><code>path</code>: a pathlib.Path object</li>\n<li><code>age</code>: modification time in seconds (<a href=\"https://docs.python.org/3/library/os.html#os.stat_result\" rel=\"nofollow\">Unix time</a>)</li>\n<li><code>size</code>: file size in bytes</li>\n<li><code>hash</code>: the SHA-256 fingerprint (not calculated for unique files)</li>\n</ul>\n\n          </div>"}, "last_serial": 6101429, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "3b843a41e973f7fe01cca9347425429d", "sha256": "01f5388ae4f4981d5aca6d313654b0bd2da1a05f9bdd0d86ab2212abc873da8c"}, "downloads": -1, "filename": "duplicates-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3b843a41e973f7fe01cca9347425429d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 7751, "upload_time": "2019-11-08T20:23:47", "upload_time_iso_8601": "2019-11-08T20:23:47.602096Z", "url": "https://files.pythonhosted.org/packages/0b/d9/79fc2ef0c3a0aba49ee341ac7579c71a58eccf407cfd7b0b4a8f8b0b1215/duplicates-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "71c17b8206a79c76df59884985bd092c", "sha256": "775bd8d96d169ba87f406c867cb6d7c2304e6aecc0e7efca9e1d91fda0f23675"}, "downloads": -1, "filename": "duplicates-0.1.0.tar.gz", "has_sig": false, "md5_digest": "71c17b8206a79c76df59884985bd092c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6005, "upload_time": "2019-11-08T20:23:51", "upload_time_iso_8601": "2019-11-08T20:23:51.008949Z", "url": "https://files.pythonhosted.org/packages/53/44/99969aba6d2708b58bc02a0bfc02459db436e3322efe047a2a0efca8bf7e/duplicates-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3b843a41e973f7fe01cca9347425429d", "sha256": "01f5388ae4f4981d5aca6d313654b0bd2da1a05f9bdd0d86ab2212abc873da8c"}, "downloads": -1, "filename": "duplicates-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3b843a41e973f7fe01cca9347425429d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 7751, "upload_time": "2019-11-08T20:23:47", "upload_time_iso_8601": "2019-11-08T20:23:47.602096Z", "url": "https://files.pythonhosted.org/packages/0b/d9/79fc2ef0c3a0aba49ee341ac7579c71a58eccf407cfd7b0b4a8f8b0b1215/duplicates-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "71c17b8206a79c76df59884985bd092c", "sha256": "775bd8d96d169ba87f406c867cb6d7c2304e6aecc0e7efca9e1d91fda0f23675"}, "downloads": -1, "filename": "duplicates-0.1.0.tar.gz", "has_sig": false, "md5_digest": "71c17b8206a79c76df59884985bd092c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6005, "upload_time": "2019-11-08T20:23:51", "upload_time_iso_8601": "2019-11-08T20:23:51.008949Z", "url": "https://files.pythonhosted.org/packages/53/44/99969aba6d2708b58bc02a0bfc02459db436e3322efe047a2a0efca8bf7e/duplicates-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:49:00 2020"}