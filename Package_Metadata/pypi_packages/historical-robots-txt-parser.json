{"info": {"author": "Alex Litel", "author_email": "", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "[![PyPI version shields.io](https://img.shields.io/pypi/v/historical-robots-txt-parser.svg)](https://pypi.python.org/pypi/historical-robots-txt-parser/) [![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)\n\n\n\n# Historical Robots.txt Parser\n\nThis is a small Python package that parses the historical robots.txt files from the Internet Archive's Wayback Machine and coerces the data into a CSV file for tracking addition and removal of `Allow` and `Disallow` rules by timestamp of addition, path, user-agent, rule type (optional), . It's a fairly narrow use case but may be helpful to researchers or journalists.\n\nIt also includes a parser to coerce a robots.txt file into a dictionary.\n\n## Requirements\n* Python 3.7 or later\n\n## Installation\n#### Install with Python\n```\npip3 install historical-robots-txt-parser\n```\n\n#### Install with Git\nThis package was developed using [Poetry](https://github.com/python-poetry/poetry), which greatly simplifies the experience of dealing with dependencies and everything. Using Poetry is strongly recommended.\n\n```\ngit clone https://github.com/alexlitel/historical-robots-txt-parser\ncd historical-robots-txt-parser\npoetry install\n```\n\nThere is a `requirements.txt` file included here, so you can also use `pip3 install -r requirements.txt` if you don't want to use Poetry.\n\n## Usage\nThere are two functions included in the package: `parse_robots_txt` and `historical_scraper`. `historical_scraper` scrapes the historical files for a domain from the Wayback Machine and exports to a CSV. `parse_robots_txt` makes a request to a robots.txt file, parses and coerces it to a dictionary.\n\n\n### historical_scraper\n#### Usage\n```\nfrom historical_robots import historical_scraper\n\nhistorical_scraper('website.com', 'website.csv', <optional arguments>)\n```\n\n#### Parameters\n| parameter | type | required | default value | description |\n|----------------------|------------|----------|---------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n| domain | string | true |  | The domain to scrape records from. Only should be hostname without `www`. |\n| file_path | string | true |  | Path of CSV file to export to |\n| accept_allow | boolean | false | False | Whether to allow parser to parse `Allow` rules and include those in dataset. Adds a new column to CSV for `Rule` to note `Disallow` or `Allow` rule. By default, function only checks `Disallow` rules.\n| skip_scrape_interval | boolean | false | False | Whether to skip the default sleep interval between each historical robots.txt request.  `True` value may cause errors. |\n| sleep_interval | number | false | 0.05 | Number of seconds to sleep in between robots.txt requests.  Ignored if `skip_scrape_interval` is `True` |\n| params | dictionary | false | {} | Key value pairs representing [valid URL params for the Wayback CDX API](https://github.com/internetarchive/wayback/tree/master/wayback-cdx-server) |\n\n\n\n### parse_robots_txt\n#### Usage\n```\nfrom historical_robots import parse_robots_text\n\nparse_robots_txt('https://www.website.com/robots.txt', False)\n```\n\n#### Parameters\n| parameter | type | required | default value | description |\n|----------------------|------------|----------|---------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n| URL | string | true |  | The URL to request robots.txt file from. |\n| accept_allow | boolean | false | False | Whether to parse `Allow` rules. By default, function only checks `Disallow` rules.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/alexlitel/historical-robots-txt-parser", "keywords": "robots.txt,wayback machine,historical data,historical", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "historical-robots-txt-parser", "package_url": "https://pypi.org/project/historical-robots-txt-parser/", "platform": "", "project_url": "https://pypi.org/project/historical-robots-txt-parser/", "project_urls": {"Homepage": "https://github.com/alexlitel/historical-robots-txt-parser", "Repository": "https://github.com/alexlitel/historical-robots-txt-parser"}, "release_url": "https://pypi.org/project/historical-robots-txt-parser/0.1.0/", "requires_dist": ["requests (>=2.23.0,<3.0.0)"], "requires_python": ">=3.7,<4.0", "summary": "Parses historical robots.txt files from Wayback Machine", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://pypi.python.org/pypi/historical-robots-txt-parser/\" rel=\"nofollow\"><img alt=\"PyPI version shields.io\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4173ca75926c81a570ed82eaaa78a232e3a5a9b0/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f686973746f726963616c2d726f626f74732d7478742d7061727365722e737667\"></a> <a href=\"https://lbesson.mit-license.org/\" rel=\"nofollow\"><img alt=\"MIT license\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4150014b4dfdd7b565fa18de88e9bb1b8ccd7c08/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667\"></a></p>\n<h1>Historical Robots.txt Parser</h1>\n<p>This is a small Python package that parses the historical robots.txt files from the Internet Archive's Wayback Machine and coerces the data into a CSV file for tracking addition and removal of <code>Allow</code> and <code>Disallow</code> rules by timestamp of addition, path, user-agent, rule type (optional), . It's a fairly narrow use case but may be helpful to researchers or journalists.</p>\n<p>It also includes a parser to coerce a robots.txt file into a dictionary.</p>\n<h2>Requirements</h2>\n<ul>\n<li>Python 3.7 or later</li>\n</ul>\n<h2>Installation</h2>\n<h4>Install with Python</h4>\n<pre><code>pip3 install historical-robots-txt-parser\n</code></pre>\n<h4>Install with Git</h4>\n<p>This package was developed using <a href=\"https://github.com/python-poetry/poetry\" rel=\"nofollow\">Poetry</a>, which greatly simplifies the experience of dealing with dependencies and everything. Using Poetry is strongly recommended.</p>\n<pre><code>git clone https://github.com/alexlitel/historical-robots-txt-parser\ncd historical-robots-txt-parser\npoetry install\n</code></pre>\n<p>There is a <code>requirements.txt</code> file included here, so you can also use <code>pip3 install -r requirements.txt</code> if you don't want to use Poetry.</p>\n<h2>Usage</h2>\n<p>There are two functions included in the package: <code>parse_robots_txt</code> and <code>historical_scraper</code>. <code>historical_scraper</code> scrapes the historical files for a domain from the Wayback Machine and exports to a CSV. <code>parse_robots_txt</code> makes a request to a robots.txt file, parses and coerces it to a dictionary.</p>\n<h3>historical_scraper</h3>\n<h4>Usage</h4>\n<pre><code>from historical_robots import historical_scraper\n\nhistorical_scraper('website.com', 'website.csv', &lt;optional arguments&gt;)\n</code></pre>\n<h4>Parameters</h4>\n<table>\n<thead>\n<tr>\n<th>parameter</th>\n<th>type</th>\n<th>required</th>\n<th>default value</th>\n<th>description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>domain</td>\n<td>string</td>\n<td>true</td>\n<td></td>\n<td>The domain to scrape records from. Only should be hostname without <code>www</code>.</td>\n</tr>\n<tr>\n<td>file_path</td>\n<td>string</td>\n<td>true</td>\n<td></td>\n<td>Path of CSV file to export to</td>\n</tr>\n<tr>\n<td>accept_allow</td>\n<td>boolean</td>\n<td>false</td>\n<td>False</td>\n<td>Whether to allow parser to parse <code>Allow</code> rules and include those in dataset. Adds a new column to CSV for <code>Rule</code> to note <code>Disallow</code> or <code>Allow</code> rule. By default, function only checks <code>Disallow</code> rules.</td>\n</tr>\n<tr>\n<td>skip_scrape_interval</td>\n<td>boolean</td>\n<td>false</td>\n<td>False</td>\n<td>Whether to skip the default sleep interval between each historical robots.txt request.  <code>True</code> value may cause errors.</td>\n</tr>\n<tr>\n<td>sleep_interval</td>\n<td>number</td>\n<td>false</td>\n<td>0.05</td>\n<td>Number of seconds to sleep in between robots.txt requests.  Ignored if <code>skip_scrape_interval</code> is <code>True</code></td>\n</tr>\n<tr>\n<td>params</td>\n<td>dictionary</td>\n<td>false</td>\n<td>{}</td>\n<td>Key value pairs representing <a href=\"https://github.com/internetarchive/wayback/tree/master/wayback-cdx-server\" rel=\"nofollow\">valid URL params for the Wayback CDX API</a></td>\n</tr></tbody></table>\n<h3>parse_robots_txt</h3>\n<h4>Usage</h4>\n<pre><code>from historical_robots import parse_robots_text\n\nparse_robots_txt('https://www.website.com/robots.txt', False)\n</code></pre>\n<h4>Parameters</h4>\n<table>\n<thead>\n<tr>\n<th>parameter</th>\n<th>type</th>\n<th>required</th>\n<th>default value</th>\n<th>description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>URL</td>\n<td>string</td>\n<td>true</td>\n<td></td>\n<td>The URL to request robots.txt file from.</td>\n</tr>\n<tr>\n<td>accept_allow</td>\n<td>boolean</td>\n<td>false</td>\n<td>False</td>\n<td>Whether to parse <code>Allow</code> rules. By default, function only checks <code>Disallow</code> rules.</td>\n</tr></tbody></table>\n\n          </div>"}, "last_serial": 7161073, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "72a7a673fcd2728247f379f685c0205b", "sha256": "bc1db0e984ff440f91889bfe358abb492b9e614a2fbb179d27bb81293f5d55af"}, "downloads": -1, "filename": "historical_robots_txt_parser-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "72a7a673fcd2728247f379f685c0205b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7,<4.0", "size": 4800, "upload_time": "2020-05-04T02:59:06", "upload_time_iso_8601": "2020-05-04T02:59:06.756350Z", "url": "https://files.pythonhosted.org/packages/e7/22/0eb457422c2254a1dfe5ae83bd2a4c3945ccdf204a3b3067f24777c453de/historical_robots_txt_parser-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c5384ab61cd3977acbaa64af2e0eef0e", "sha256": "c5790dcff336a65dae53c737543d6f27883a1648044da2f58ca6efc3419efb58"}, "downloads": -1, "filename": "historical-robots-txt-parser-0.1.0.tar.gz", "has_sig": false, "md5_digest": "c5384ab61cd3977acbaa64af2e0eef0e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7,<4.0", "size": 4442, "upload_time": "2020-05-04T02:59:04", "upload_time_iso_8601": "2020-05-04T02:59:04.694510Z", "url": "https://files.pythonhosted.org/packages/4a/e7/368cf44f91a8ecdec04e82e1acd1a6deb4b8f6c1457de16e893f3bf0593c/historical-robots-txt-parser-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "72a7a673fcd2728247f379f685c0205b", "sha256": "bc1db0e984ff440f91889bfe358abb492b9e614a2fbb179d27bb81293f5d55af"}, "downloads": -1, "filename": "historical_robots_txt_parser-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "72a7a673fcd2728247f379f685c0205b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7,<4.0", "size": 4800, "upload_time": "2020-05-04T02:59:06", "upload_time_iso_8601": "2020-05-04T02:59:06.756350Z", "url": "https://files.pythonhosted.org/packages/e7/22/0eb457422c2254a1dfe5ae83bd2a4c3945ccdf204a3b3067f24777c453de/historical_robots_txt_parser-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c5384ab61cd3977acbaa64af2e0eef0e", "sha256": "c5790dcff336a65dae53c737543d6f27883a1648044da2f58ca6efc3419efb58"}, "downloads": -1, "filename": "historical-robots-txt-parser-0.1.0.tar.gz", "has_sig": false, "md5_digest": "c5384ab61cd3977acbaa64af2e0eef0e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7,<4.0", "size": 4442, "upload_time": "2020-05-04T02:59:04", "upload_time_iso_8601": "2020-05-04T02:59:04.694510Z", "url": "https://files.pythonhosted.org/packages/4a/e7/368cf44f91a8ecdec04e82e1acd1a6deb4b8f6c1457de16e893f3bf0593c/historical-robots-txt-parser-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:51:04 2020"}