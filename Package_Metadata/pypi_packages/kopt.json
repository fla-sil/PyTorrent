{"info": {"author": "\u017diga Avsec", "author_email": "avsec@in.tum.de", "bugtrack_url": null, "classifiers": [], "description": "# kopt - Hyper-parameter optimization for Keras\n\n[![Build Status](https://travis-ci.org/Avsecz/keras-hyperopt.svg?branch=master)](https://travis-ci.org/avsecz/keras-hyperopt)\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/avsecz/keras-hyperopt/blob/master/LICENSE)\n\nkopt is a hyper-parameter optimization library for Keras. It is based on [hyperopt](https://github.com/hyperopt/hyperopt).\n\n## Getting started\n\nHere is an example of hyper-parameter optimization for the Keras IMDB\nexample model.\n\n```python\nfrom keras.datasets import imdb\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nimport keras.layers as kl\nfrom keras.optimizers import Adam\n# kopt and hyoperot imports\nfrom kopt import CompileFN, KMongoTrials, test_fn\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\n\n# 1. define the data function returning training, (validation, test) data\ndef data(max_features=5000, maxlen=80):\n    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n    x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n    x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n    return (x_train[:100], y_train[:100], max_features), (x_test, y_test)\n\n\n# 2. Define the model function returning a compiled Keras model\ndef model(train_data, lr=0.001,\n          embedding_dims=128, rnn_units=64,\n          dropout=0.2):\n\t# extract data dimensions\n    max_features = train_data[2]\n\n    model = Sequential()\n    model.add(kl.Embedding(max_features, embedding_dims))\n    model.add(kl.LSTM(rnn_units, dropout=dropout, recurrent_dropout=dropout))\n    model.add(kl.Dense(1, activation='sigmoid'))\n\n    model.compile(loss='binary_crossentropy',\n                  optimizer=Adam(lr=lr),\n                  metrics=['accuracy'])\n    return model\n\n# Specify the optimization metrics\ndb_name=\"imdb\"\nexp_name=\"myexp1\"\nobjective = CompileFN(db_name, exp_name,\n                      data_fn=data,\n                      model_fn=model,\n                      loss_metric=\"acc\", # which metric to optimize for\n                      loss_metric_mode=\"max\",  # try to maximize the metric\n                      valid_split=.2, # use 20% of the training data for the validation set\n                      save_model='best', # checkpoint the best model\n                      save_results=True, # save the results as .json (in addition to mongoDB)\n                      save_dir=\"./saved_models/\")  # place to store the models\n\n# define the hyper-parameter ranges\n# see https://github.com/hyperopt/hyperopt/wiki/FMin for more info\nhyper_params = {\n\t\"data\": {\n\t    \"max_features\": 100,\n\t\t\"maxlen\": 80,\n\t},\n\t\"model\": {\n     \t\"lr\": hp.loguniform(\"m_lr\", np.log(1e-4), np.log(1e-2)), # 0.0001 - 0.01\n\t    \"embedding_dims\": hp.choice(\"m_emb\", (64, 128)),\n\t    \"rnn_units\": 64,\n\t\t\"dropout\": hp.uniform(\"m_do\", 0, 0.5),\n\t},\n\t\"fit\": {\n\t    \"epochs\": 20\n\t}\n}\n\n# test model training, on a small subset for one epoch\ntest_fn(objective, hyper_params)\n\n# run hyper-parameter optimization sequentially (without any database)\ntrials = Trials()\nbest = fmin(objective, hyper_params, trials=trials, algo=tpe.suggest, max_evals=2)\n\n# run hyper-parameter optimization in parallel (saving the results to MonogoDB)\n# Follow the hyperopt guide:\n# https://github.com/hyperopt/hyperopt/wiki/Parallelizing-Evaluations-During-Search-via-MongoDB\n# KMongoTrials extends hyperopt.MongoTrials with convenience methods\ntrials = KMongoTrials(db_name, exp_name,\n                      ip=\"localhost\",\n\t              port=22334)\nbest = fmin(objective, hyper_params, trials=trials, algo=tpe.suggest, max_evals=2)\n```\n\n## See also\n\n- [nbs/imdb_example.ipynb](nbs/imdb_example.ipynb)\n\nThe documentation of `concise.hyopt` (`kopt` was ported from `concise.hyopt`):\n\n- [Tutorial](https://i12g-gagneurweb.in.tum.de/public/docs/concise/tutorials/hyper-parameter_optimization/)\n- [API documentation](https://i12g-gagneurweb.in.tum.de/public/docs/concise/hyopt/)\n- [Jupyter notebook](https://github.com/gagneurlab/concise/blob/master/nbs/hyper-parameter_optimization.ipynb)\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/avsecz/keras-hyperopt", "keywords": "hyper-parameter tuning", "license": "MIT license", "maintainer": "", "maintainer_email": "", "name": "kopt", "package_url": "https://pypi.org/project/kopt/", "platform": "", "project_url": "https://pypi.org/project/kopt/", "project_urls": {"Homepage": "https://github.com/avsecz/keras-hyperopt"}, "release_url": "https://pypi.org/project/kopt/0.1.0/", "requires_dist": null, "requires_python": "", "summary": "Keras-hyperopt (kopt); Hyper-parameter tuning for Keras using hyperopt.", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            # kopt - Hyper-parameter optimization for Keras<br><br>[![Build Status](https://travis-ci.org/Avsecz/keras-hyperopt.svg?branch=master)](https://travis-ci.org/avsecz/keras-hyperopt)<br>[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/avsecz/keras-hyperopt/blob/master/LICENSE)<br><br>kopt is a hyper-parameter optimization library for Keras. It is based on [hyperopt](https://github.com/hyperopt/hyperopt).<br><br>## Getting started<br><br>Here is an example of hyper-parameter optimization for the Keras IMDB<br>example model.<br><br>```python<br>from keras.datasets import imdb<br>from keras.preprocessing import sequence<br>from keras.models import Sequential<br>import keras.layers as kl<br>from keras.optimizers import Adam<br># kopt and hyoperot imports<br>from kopt import CompileFN, KMongoTrials, test_fn<br>from hyperopt import fmin, tpe, hp, STATUS_OK, Trials<br><br><br># 1. define the data function returning training, (validation, test) data<br>def data(max_features=5000, maxlen=80):<br>    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)<br>    x_train = sequence.pad_sequences(x_train, maxlen=maxlen)<br>    x_test = sequence.pad_sequences(x_test, maxlen=maxlen)<br>    return (x_train[:100], y_train[:100], max_features), (x_test, y_test)<br><br><br># 2. Define the model function returning a compiled Keras model<br>def model(train_data, lr=0.001,<br>          embedding_dims=128, rnn_units=64,<br>          dropout=0.2):<br>\t# extract data dimensions<br>    max_features = train_data[2]<br><br>    model = Sequential()<br>    model.add(kl.Embedding(max_features, embedding_dims))<br>    model.add(kl.LSTM(rnn_units, dropout=dropout, recurrent_dropout=dropout))<br>    model.add(kl.Dense(1, activation='sigmoid'))<br><br>    model.compile(loss='binary_crossentropy',<br>                  optimizer=Adam(lr=lr),<br>                  metrics=['accuracy'])<br>    return model<br><br># Specify the optimization metrics<br>db_name=\"imdb\"<br>exp_name=\"myexp1\"<br>objective = CompileFN(db_name, exp_name,<br>                      data_fn=data,<br>                      model_fn=model,<br>                      loss_metric=\"acc\", # which metric to optimize for<br>                      loss_metric_mode=\"max\",  # try to maximize the metric<br>                      valid_split=.2, # use 20% of the training data for the validation set<br>                      save_model='best', # checkpoint the best model<br>                      save_results=True, # save the results as .json (in addition to mongoDB)<br>                      save_dir=\"./saved_models/\")  # place to store the models<br><br># define the hyper-parameter ranges<br># see https://github.com/hyperopt/hyperopt/wiki/FMin for more info<br>hyper_params = {<br>\t\"data\": {<br>\t    \"max_features\": 100,<br>\t\t\"maxlen\": 80,<br>\t},<br>\t\"model\": {<br>     \t\"lr\": hp.loguniform(\"m_lr\", np.log(1e-4), np.log(1e-2)), # 0.0001 - 0.01<br>\t    \"embedding_dims\": hp.choice(\"m_emb\", (64, 128)),<br>\t    \"rnn_units\": 64,<br>\t\t\"dropout\": hp.uniform(\"m_do\", 0, 0.5),<br>\t},<br>\t\"fit\": {<br>\t    \"epochs\": 20<br>\t}<br>}<br><br># test model training, on a small subset for one epoch<br>test_fn(objective, hyper_params)<br><br># run hyper-parameter optimization sequentially (without any database)<br>trials = Trials()<br>best = fmin(objective, hyper_params, trials=trials, algo=tpe.suggest, max_evals=2)<br><br># run hyper-parameter optimization in parallel (saving the results to MonogoDB)<br># Follow the hyperopt guide:<br># https://github.com/hyperopt/hyperopt/wiki/Parallelizing-Evaluations-During-Search-via-MongoDB<br># KMongoTrials extends hyperopt.MongoTrials with convenience methods<br>trials = KMongoTrials(db_name, exp_name,<br>                      ip=\"localhost\",<br>\t              port=22334)<br>best = fmin(objective, hyper_params, trials=trials, algo=tpe.suggest, max_evals=2)<br>```<br><br>## See also<br><br>- [nbs/imdb_example.ipynb](nbs/imdb_example.ipynb)<br><br>The documentation of `concise.hyopt` (`kopt` was ported from `concise.hyopt`):<br><br>- [Tutorial](https://i12g-gagneurweb.in.tum.de/public/docs/concise/tutorials/hyper-parameter_optimization/)<br>- [API documentation](https://i12g-gagneurweb.in.tum.de/public/docs/concise/hyopt/)<br>- [Jupyter notebook](https://github.com/gagneurlab/concise/blob/master/nbs/hyper-parameter_optimization.ipynb)<br>\n          </div>"}, "last_serial": 3648066, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "18b48e691e7fe5d40fde2210a427539f", "sha256": "d90fe5f63dbf89dff514d0903c5cc933c5147e7bb7b109c2ad1d57b6505c7e85"}, "downloads": -1, "filename": "kopt-0.1.0.tar.gz", "has_sig": false, "md5_digest": "18b48e691e7fe5d40fde2210a427539f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18510, "upload_time": "2018-03-07T15:05:15", "upload_time_iso_8601": "2018-03-07T15:05:15.612798Z", "url": "https://files.pythonhosted.org/packages/a7/e2/650da402870396bcf6660e98e55499f6ec7d05c8443f3bfb2c0e64dda172/kopt-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "18b48e691e7fe5d40fde2210a427539f", "sha256": "d90fe5f63dbf89dff514d0903c5cc933c5147e7bb7b109c2ad1d57b6505c7e85"}, "downloads": -1, "filename": "kopt-0.1.0.tar.gz", "has_sig": false, "md5_digest": "18b48e691e7fe5d40fde2210a427539f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18510, "upload_time": "2018-03-07T15:05:15", "upload_time_iso_8601": "2018-03-07T15:05:15.612798Z", "url": "https://files.pythonhosted.org/packages/a7/e2/650da402870396bcf6660e98e55499f6ec7d05c8443f3bfb2c0e64dda172/kopt-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:48:58 2020"}