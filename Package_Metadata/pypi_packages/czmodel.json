{"info": {"author": "Sebastian Soyer", "author_email": "sebastian.soyer@zeiss.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "This project provides simple-to-use conversion tools to generate a CZModel file from a \n[TensorFlow](https://www.tensorflow.org/) model that resides in memory or on disk to be usable in the \n[ZEN Intellesis](https://www.zeiss.com/microscopy/int/products/microscope-software/zen-intellesis-image-segmentation-by-deep-learning.html) module starting with ZEN blue >=3.2 and ZEN Core >3.0.  \n\nThis version of czmodel produces the following model version: 3.0.0  \n\nIf you encounter a version mismatch when importing a model into ZEN, please check for the correct version of this package.\n\n## System setup\nThe current version of this toolbox only requires a fresh Python 3.x installation. \nIt was tested with Python 3.7 on Windows.\n\n## Model conversion\nThe toolbox provides a `convert` module that features all supported conversion strategies. It currently supports \nconverting Keras models in memory or stored on disk with a corresponding metadata JSON file.\n\n### Keras models in memory\nThe toolbox also provides functionality that can be imported e.g. in the training script used to fit a Keras model. The function is accessible by running: \n```python\nfrom czmodel.convert import convert_from_model_spec\n```\nIt accepts a `tensorflow.keras.Model` that will be exported to [SavedModel](https://www.tensorflow.org/guide/saved_model) \nformat and at the same time wrapped into a CZModel file to be compatible with the Intellesis infrastructure.  \nTo provide the meta data, the toolbox provides a ModelSpec class that must be filled with the model and a ModelMetadata \ninstance containing the required information described in the specification (see _Model Metadata_ below) \nfile. \n\nA CZModel can be created from a Keras model with the following three steps.\n\n#### Creating a model meta data class\nTo export a CZModel file several meta information is needed that must be provided through a `ModelMetadata` instance.\n```python\nfrom czmodel.model_metadata import ModelMetadata\n\nmodel_metadata = ModelMetadata.from_params(name='DNNModelFromKeras', \n                         color_handling='ConvertToMonochrome',\n                         pixel_type='Gray16',\n                         classes=[\"Background\", \"Interesting Object\", \"Foreground\"],\n                         border_size=90,\n                         license_file=\"C:\\\\some\\\\path\\\\to\\\\a\\\\LICENSE.txt\")\n```\n\n#### Creating a model specification\nThe model and its corresponding metadata are now wrapped into a ModelSpec object.\n```python\nfrom czmodel.model_metadata import ModelSpec\n\nmodel_spec = ModelSpec(model=model, model_metadata=model_metadata)\n```\n\n#### Converting the model\nThe actual model conversion is finally performed with the ModelSpec object and the output path and name of the CZModel \nfile.\n```python\nfrom czmodel.convert import convert_from_model_spec\n\nconvert_from_model_spec(model_spec=model_spec, output_path='some/path', output_name='some_file_name')\n```\n\n### Exported TensorFlow models\nTo convert an exported TensorFlow model the model and the provided meta data need to comply with \n(see _ANN Model Specification_ below).\n\nThe actual conversion is triggered by either calling:\n```python\nfrom czmodel.convert import convert_from_json_spec\n\nconvert_from_json_spec('Path to JSON file', 'Output path', 'Model Name')\n```\nor by using the command line interface of the `savedmodel2czmodel` script:\n```console\nsavedmodel2czmodel path/to/model_spec.json output/path/ output_name\n```\n\n### Addding pre-processing layers\nBoth, `convert_from_json_spec` and `convert_from_model_spec` additionally allow specifying the following optional parameters:\n- `spatial_dims`: Set new spatial dimensions for the new input node of the model. This parameter is expected to contain the new height \nand width in that order. **Note:** The spatial input dimensions can only be changed in ANN architectures that are invariant to the \nspatial dimensions of the input, e.g. FCNs.\n- `preprocessing`: One or more pre-processing layers that will be prepended to the deployed model. A pre-processing \nlayer must be derived from the `tensorflow.keras.layers.Layer` class.\n\nWhile ANN models are often trained on images in RGB(A) space, the ZEN infrastructure requires models inside a CZModel to \nexpect inputs in BGR(A) color space. This toolbox offers pre-processing layers to convert the color space before \npassing the input to the model to be actually deployed. The followig code shows how to add a RGB to BGR conversion layer \nto a model and set its spatial input dimensions to 512x512.\n\n```python\nfrom czmodel.util.preprocessing import RgbToBgr\n\n# Define dimensions and pre-processing\nspatial_dims = 512, 512  # Optional: Target spatial dimensions of the model\npreprocessing = RgbToBgr()  # Optional: Pre-Processing layers to be prepended to the model. Can be a list of layers.\n\n# Perform conversion\nconvert_from_model_spec(model_spec=model_spec, output_path='some/path', output_name='some_file_name', spatial_dims=spatial_dims, preprocessing=preprocessing)\n```\n\n## ANN Model Specification\nThis section specifies the requirements for an artificial neural network (ANN) model and the additionally required metadata to enable execution of the model inside the ZEN Intellesis infrastructure starting with ZEN blue >=3.2 and ZEN Core >3.0.\n\n### Core network structure and file format\nTo be usable in the SegmentationService infrastructure a neural network model must comply with the specified rules below.\n\n- The model must be provided as a [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model).\n- All operations in the contained execution graph must be supported by TensorFlow 2.0.0.\n- The model currently must provide one input and one output node. Multiple inputs and outputs are not supported.\n- The shape of the input node must have 4 dimensions where the first dimension specifies the batch size, the second and third dimensions specify the width and height of the expected input image and the third dimension represents the number of color channels.\n- The batch dimension of the input node must be undefined or 1.\n- The spatial dimension of the input image implicitly defines the maximum tile size of the model. Our infrastructure will ensure that all input images exactly match the specified dimensions. The spatial dimensions of the input node must be such that the model can be evaluated on the minimum required hardware (currently 8GB GPU memory) without running out of memory.\n- The output node must have the same shape as the input node except for the last dimension that represents the class probabilities. The size of the last dimension of the output must be the number of classes. The values of the output tensor must represent the class probabilities for each pixel. I.e. values must lie in the [0...1] range and summing the output over the last dimension must produce an all-1 tensor (within numeric accuracy). Softmax activation can be used to turn logits into such probabilities.\n- All types of pre-processing and post-processing (except the currently supported Conditional Random Field post-processing) e.g. normalization, standardization, down-sampling etc. must be included in the provided TensorFlow model so that no further action by the inference engine is needed before or after inference to obtain the expected results.\n\n### Model Metadata\nExecuting an ANN model within the Intellesis infrastructure requires additional meta information that needs to be provided along with the serialized model specified by the (see _Core network structure and file format_ above).\nMeta information for the ANN model must be provided in a separate JSON file adhering to [RFC8259](https://tools.ietf.org/html/rfc8259) that must contain the following attributes:\n\n- **BorderSize (Type: int)**: For Intellesis models this attribute defines the size of the border that needs to be added to an input image such that there are no border effects visible in the required area of the generated segmentation mask. For deep architectures this value can be infeasibly large so that the border size must be defined in a way that the border effects are \"acceptable\" in the ANN model creator's opinion.\n- **ColorHandling (Type: string)**: Specifies how color (RGB and RGBA) pixel data are converted to one or more channels of scalar pixel data. Possible values are:\n  - ConvertToMonochrome (Converts color to gray scale)\n  - SplitRgb (Keeps the pixel representation in RGB space)\n- **PixelType (Type: string)**: The expected input type of the model. Possible values are:\n  - **Gray8**: 8 bit unsigned\n  - **Gray16**: 16 bit unsigned\n  - **Gray32Float**: 4 byte IEEE float\n  - **Bgr24**: 8 bit triples, representing the color channels Blue, Green and Red\n  - **Bgr48**: 16 bit triples, representing the color channels Blue, Green and Red\n  - **Bgr96Float**: Triple of 4 byte IEEE float, representing the color channels Blue, Green and Red\n  - **Bgra32**: 8 bit triples followed by an alpha (transparency) channel\n  - **Gray64ComplexFloat**: 2 x 4 byte IEEE float, representing real and imaginary part of a complex number\n  - **Bgr192ComplexFloat**: A triple of 2 x 4 byte IEEE float, representing real and imaginary part of a complex number, for the color channels Blue, Green and Red\n- **Classes (Type: array, Value type: string)**: A list of class names corresponding to the output dimensions of the predicted segmentation mask. If the last dimension of the prediction has shape n the provided list must be of length n.\n- **ModelPath (Type: string)**: The path to the exported neural network model. Can be absolute or relative to the JSON file.\n\nThe file may also contain the following optional attributes:\n\n- **TestImageFile (Type: string)**: The path to a test image in a format supported by ZEN. This image is used for basic validation of the converted model inside ZEN. Can be absolute or relative to the JSON file.\n- **LicenseFile (Type: string)**: The path to a license file that is added to the generated CZModel. Can be absolute or relative to the JSON file.\n\nJson files can contain escape sequences and \\\\-characters in paths must be escaped with \\\\\\\\.\n\nThe following code snippet shows an example for a valid metadata file:\n\n```json\n{\n  \"BorderSize\": 90,\n  \"ColorHandling\": \"ConvertToMonochrome\",\n  \"PixelType\": \"Gray16\",\n  \"Classes\": [\"Background\", \"Interesting Object\", \"Foreground\"],\n  \"ModelPath\": \"C:\\\\tf\\\\saved\\\\model\\\\folder\\\\\",\n  \"TestImageFile\": \"C:\\\\test-image.png\",\n  \"LicenseFile\": \"C:\\\\LICENSE.txt\"\n}\n```\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "czmodel", "package_url": "https://pypi.org/project/czmodel/", "platform": "", "project_url": "https://pypi.org/project/czmodel/", "project_urls": {"Demo": "https://notebooks.azure.com/sebastian-soyer/projects/czmodel", "ZEN Intellesis": "https://www.zeiss.com/microscopy/int/products/microscope-software/zen-intellesis-image-segmentation-by-deep-learning.html"}, "release_url": "https://pypi.org/project/czmodel/0.1.2/", "requires_dist": ["tensorflow (!=2.1,<3,>=2)"], "requires_python": ">=3.6,<4", "summary": "A conversion tool for TensorFlow ANNs to CZModel", "version": "0.1.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>This project provides simple-to-use conversion tools to generate a CZModel file from a\n<a href=\"https://www.tensorflow.org/\" rel=\"nofollow\">TensorFlow</a> model that resides in memory or on disk to be usable in the\n<a href=\"https://www.zeiss.com/microscopy/int/products/microscope-software/zen-intellesis-image-segmentation-by-deep-learning.html\" rel=\"nofollow\">ZEN Intellesis</a> module starting with ZEN blue &gt;=3.2 and ZEN Core &gt;3.0.</p>\n<p>This version of czmodel produces the following model version: 3.0.0</p>\n<p>If you encounter a version mismatch when importing a model into ZEN, please check for the correct version of this package.</p>\n<h2>System setup</h2>\n<p>The current version of this toolbox only requires a fresh Python 3.x installation.\nIt was tested with Python 3.7 on Windows.</p>\n<h2>Model conversion</h2>\n<p>The toolbox provides a <code>convert</code> module that features all supported conversion strategies. It currently supports\nconverting Keras models in memory or stored on disk with a corresponding metadata JSON file.</p>\n<h3>Keras models in memory</h3>\n<p>The toolbox also provides functionality that can be imported e.g. in the training script used to fit a Keras model. The function is accessible by running:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">czmodel.convert</span> <span class=\"kn\">import</span> <span class=\"n\">convert_from_model_spec</span>\n</pre>\n<p>It accepts a <code>tensorflow.keras.Model</code> that will be exported to <a href=\"https://www.tensorflow.org/guide/saved_model\" rel=\"nofollow\">SavedModel</a>\nformat and at the same time wrapped into a CZModel file to be compatible with the Intellesis infrastructure.<br>\nTo provide the meta data, the toolbox provides a ModelSpec class that must be filled with the model and a ModelMetadata\ninstance containing the required information described in the specification (see <em>Model Metadata</em> below)\nfile.</p>\n<p>A CZModel can be created from a Keras model with the following three steps.</p>\n<h4>Creating a model meta data class</h4>\n<p>To export a CZModel file several meta information is needed that must be provided through a <code>ModelMetadata</code> instance.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">czmodel.model_metadata</span> <span class=\"kn\">import</span> <span class=\"n\">ModelMetadata</span>\n\n<span class=\"n\">model_metadata</span> <span class=\"o\">=</span> <span class=\"n\">ModelMetadata</span><span class=\"o\">.</span><span class=\"n\">from_params</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'DNNModelFromKeras'</span><span class=\"p\">,</span> \n                         <span class=\"n\">color_handling</span><span class=\"o\">=</span><span class=\"s1\">'ConvertToMonochrome'</span><span class=\"p\">,</span>\n                         <span class=\"n\">pixel_type</span><span class=\"o\">=</span><span class=\"s1\">'Gray16'</span><span class=\"p\">,</span>\n                         <span class=\"n\">classes</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"Background\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Interesting Object\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Foreground\"</span><span class=\"p\">],</span>\n                         <span class=\"n\">border_size</span><span class=\"o\">=</span><span class=\"mi\">90</span><span class=\"p\">,</span>\n                         <span class=\"n\">license_file</span><span class=\"o\">=</span><span class=\"s2\">\"C:</span><span class=\"se\">\\\\</span><span class=\"s2\">some</span><span class=\"se\">\\\\</span><span class=\"s2\">path</span><span class=\"se\">\\\\</span><span class=\"s2\">to</span><span class=\"se\">\\\\</span><span class=\"s2\">a</span><span class=\"se\">\\\\</span><span class=\"s2\">LICENSE.txt\"</span><span class=\"p\">)</span>\n</pre>\n<h4>Creating a model specification</h4>\n<p>The model and its corresponding metadata are now wrapped into a ModelSpec object.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">czmodel.model_metadata</span> <span class=\"kn\">import</span> <span class=\"n\">ModelSpec</span>\n\n<span class=\"n\">model_spec</span> <span class=\"o\">=</span> <span class=\"n\">ModelSpec</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">model_metadata</span><span class=\"o\">=</span><span class=\"n\">model_metadata</span><span class=\"p\">)</span>\n</pre>\n<h4>Converting the model</h4>\n<p>The actual model conversion is finally performed with the ModelSpec object and the output path and name of the CZModel\nfile.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">czmodel.convert</span> <span class=\"kn\">import</span> <span class=\"n\">convert_from_model_spec</span>\n\n<span class=\"n\">convert_from_model_spec</span><span class=\"p\">(</span><span class=\"n\">model_spec</span><span class=\"o\">=</span><span class=\"n\">model_spec</span><span class=\"p\">,</span> <span class=\"n\">output_path</span><span class=\"o\">=</span><span class=\"s1\">'some/path'</span><span class=\"p\">,</span> <span class=\"n\">output_name</span><span class=\"o\">=</span><span class=\"s1\">'some_file_name'</span><span class=\"p\">)</span>\n</pre>\n<h3>Exported TensorFlow models</h3>\n<p>To convert an exported TensorFlow model the model and the provided meta data need to comply with\n(see <em>ANN Model Specification</em> below).</p>\n<p>The actual conversion is triggered by either calling:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">czmodel.convert</span> <span class=\"kn\">import</span> <span class=\"n\">convert_from_json_spec</span>\n\n<span class=\"n\">convert_from_json_spec</span><span class=\"p\">(</span><span class=\"s1\">'Path to JSON file'</span><span class=\"p\">,</span> <span class=\"s1\">'Output path'</span><span class=\"p\">,</span> <span class=\"s1\">'Model Name'</span><span class=\"p\">)</span>\n</pre>\n<p>or by using the command line interface of the <code>savedmodel2czmodel</code> script:</p>\n<pre><span class=\"go\">savedmodel2czmodel path/to/model_spec.json output/path/ output_name</span>\n</pre>\n<h3>Addding pre-processing layers</h3>\n<p>Both, <code>convert_from_json_spec</code> and <code>convert_from_model_spec</code> additionally allow specifying the following optional parameters:</p>\n<ul>\n<li><code>spatial_dims</code>: Set new spatial dimensions for the new input node of the model. This parameter is expected to contain the new height\nand width in that order. <strong>Note:</strong> The spatial input dimensions can only be changed in ANN architectures that are invariant to the\nspatial dimensions of the input, e.g. FCNs.</li>\n<li><code>preprocessing</code>: One or more pre-processing layers that will be prepended to the deployed model. A pre-processing\nlayer must be derived from the <code>tensorflow.keras.layers.Layer</code> class.</li>\n</ul>\n<p>While ANN models are often trained on images in RGB(A) space, the ZEN infrastructure requires models inside a CZModel to\nexpect inputs in BGR(A) color space. This toolbox offers pre-processing layers to convert the color space before\npassing the input to the model to be actually deployed. The followig code shows how to add a RGB to BGR conversion layer\nto a model and set its spatial input dimensions to 512x512.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">czmodel.util.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">RgbToBgr</span>\n\n<span class=\"c1\"># Define dimensions and pre-processing</span>\n<span class=\"n\">spatial_dims</span> <span class=\"o\">=</span> <span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span>  <span class=\"c1\"># Optional: Target spatial dimensions of the model</span>\n<span class=\"n\">preprocessing</span> <span class=\"o\">=</span> <span class=\"n\">RgbToBgr</span><span class=\"p\">()</span>  <span class=\"c1\"># Optional: Pre-Processing layers to be prepended to the model. Can be a list of layers.</span>\n\n<span class=\"c1\"># Perform conversion</span>\n<span class=\"n\">convert_from_model_spec</span><span class=\"p\">(</span><span class=\"n\">model_spec</span><span class=\"o\">=</span><span class=\"n\">model_spec</span><span class=\"p\">,</span> <span class=\"n\">output_path</span><span class=\"o\">=</span><span class=\"s1\">'some/path'</span><span class=\"p\">,</span> <span class=\"n\">output_name</span><span class=\"o\">=</span><span class=\"s1\">'some_file_name'</span><span class=\"p\">,</span> <span class=\"n\">spatial_dims</span><span class=\"o\">=</span><span class=\"n\">spatial_dims</span><span class=\"p\">,</span> <span class=\"n\">preprocessing</span><span class=\"o\">=</span><span class=\"n\">preprocessing</span><span class=\"p\">)</span>\n</pre>\n<h2>ANN Model Specification</h2>\n<p>This section specifies the requirements for an artificial neural network (ANN) model and the additionally required metadata to enable execution of the model inside the ZEN Intellesis infrastructure starting with ZEN blue &gt;=3.2 and ZEN Core &gt;3.0.</p>\n<h3>Core network structure and file format</h3>\n<p>To be usable in the SegmentationService infrastructure a neural network model must comply with the specified rules below.</p>\n<ul>\n<li>The model must be provided as a <a href=\"https://www.tensorflow.org/guide/saved_model\" rel=\"nofollow\">TensorFlow SavedModel</a>.</li>\n<li>All operations in the contained execution graph must be supported by TensorFlow 2.0.0.</li>\n<li>The model currently must provide one input and one output node. Multiple inputs and outputs are not supported.</li>\n<li>The shape of the input node must have 4 dimensions where the first dimension specifies the batch size, the second and third dimensions specify the width and height of the expected input image and the third dimension represents the number of color channels.</li>\n<li>The batch dimension of the input node must be undefined or 1.</li>\n<li>The spatial dimension of the input image implicitly defines the maximum tile size of the model. Our infrastructure will ensure that all input images exactly match the specified dimensions. The spatial dimensions of the input node must be such that the model can be evaluated on the minimum required hardware (currently 8GB GPU memory) without running out of memory.</li>\n<li>The output node must have the same shape as the input node except for the last dimension that represents the class probabilities. The size of the last dimension of the output must be the number of classes. The values of the output tensor must represent the class probabilities for each pixel. I.e. values must lie in the [0...1] range and summing the output over the last dimension must produce an all-1 tensor (within numeric accuracy). Softmax activation can be used to turn logits into such probabilities.</li>\n<li>All types of pre-processing and post-processing (except the currently supported Conditional Random Field post-processing) e.g. normalization, standardization, down-sampling etc. must be included in the provided TensorFlow model so that no further action by the inference engine is needed before or after inference to obtain the expected results.</li>\n</ul>\n<h3>Model Metadata</h3>\n<p>Executing an ANN model within the Intellesis infrastructure requires additional meta information that needs to be provided along with the serialized model specified by the (see <em>Core network structure and file format</em> above).\nMeta information for the ANN model must be provided in a separate JSON file adhering to <a href=\"https://tools.ietf.org/html/rfc8259\" rel=\"nofollow\">RFC8259</a> that must contain the following attributes:</p>\n<ul>\n<li><strong>BorderSize (Type: int)</strong>: For Intellesis models this attribute defines the size of the border that needs to be added to an input image such that there are no border effects visible in the required area of the generated segmentation mask. For deep architectures this value can be infeasibly large so that the border size must be defined in a way that the border effects are \"acceptable\" in the ANN model creator's opinion.</li>\n<li><strong>ColorHandling (Type: string)</strong>: Specifies how color (RGB and RGBA) pixel data are converted to one or more channels of scalar pixel data. Possible values are:\n<ul>\n<li>ConvertToMonochrome (Converts color to gray scale)</li>\n<li>SplitRgb (Keeps the pixel representation in RGB space)</li>\n</ul>\n</li>\n<li><strong>PixelType (Type: string)</strong>: The expected input type of the model. Possible values are:\n<ul>\n<li><strong>Gray8</strong>: 8 bit unsigned</li>\n<li><strong>Gray16</strong>: 16 bit unsigned</li>\n<li><strong>Gray32Float</strong>: 4 byte IEEE float</li>\n<li><strong>Bgr24</strong>: 8 bit triples, representing the color channels Blue, Green and Red</li>\n<li><strong>Bgr48</strong>: 16 bit triples, representing the color channels Blue, Green and Red</li>\n<li><strong>Bgr96Float</strong>: Triple of 4 byte IEEE float, representing the color channels Blue, Green and Red</li>\n<li><strong>Bgra32</strong>: 8 bit triples followed by an alpha (transparency) channel</li>\n<li><strong>Gray64ComplexFloat</strong>: 2 x 4 byte IEEE float, representing real and imaginary part of a complex number</li>\n<li><strong>Bgr192ComplexFloat</strong>: A triple of 2 x 4 byte IEEE float, representing real and imaginary part of a complex number, for the color channels Blue, Green and Red</li>\n</ul>\n</li>\n<li><strong>Classes (Type: array, Value type: string)</strong>: A list of class names corresponding to the output dimensions of the predicted segmentation mask. If the last dimension of the prediction has shape n the provided list must be of length n.</li>\n<li><strong>ModelPath (Type: string)</strong>: The path to the exported neural network model. Can be absolute or relative to the JSON file.</li>\n</ul>\n<p>The file may also contain the following optional attributes:</p>\n<ul>\n<li><strong>TestImageFile (Type: string)</strong>: The path to a test image in a format supported by ZEN. This image is used for basic validation of the converted model inside ZEN. Can be absolute or relative to the JSON file.</li>\n<li><strong>LicenseFile (Type: string)</strong>: The path to a license file that is added to the generated CZModel. Can be absolute or relative to the JSON file.</li>\n</ul>\n<p>Json files can contain escape sequences and \\-characters in paths must be escaped with \\\\.</p>\n<p>The following code snippet shows an example for a valid metadata file:</p>\n<pre><span class=\"p\">{</span>\n  <span class=\"nt\">\"BorderSize\"</span><span class=\"p\">:</span> <span class=\"mi\">90</span><span class=\"p\">,</span>\n  <span class=\"nt\">\"ColorHandling\"</span><span class=\"p\">:</span> <span class=\"s2\">\"ConvertToMonochrome\"</span><span class=\"p\">,</span>\n  <span class=\"nt\">\"PixelType\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Gray16\"</span><span class=\"p\">,</span>\n  <span class=\"nt\">\"Classes\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"Background\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Interesting Object\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Foreground\"</span><span class=\"p\">],</span>\n  <span class=\"nt\">\"ModelPath\"</span><span class=\"p\">:</span> <span class=\"s2\">\"C:\\\\tf\\\\saved\\\\model\\\\folder\\\\\"</span><span class=\"p\">,</span>\n  <span class=\"nt\">\"TestImageFile\"</span><span class=\"p\">:</span> <span class=\"s2\">\"C:\\\\test-image.png\"</span><span class=\"p\">,</span>\n  <span class=\"nt\">\"LicenseFile\"</span><span class=\"p\">:</span> <span class=\"s2\">\"C:\\\\LICENSE.txt\"</span>\n<span class=\"p\">}</span>\n</pre>\n\n          </div>"}, "last_serial": 6942326, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "ae34bcfd55a216df37d6866ec79ec28d", "sha256": "47110ae5c08e5e96dc091965e3a5085c3524a18e8f72e6598c894b6f2e67e255"}, "downloads": -1, "filename": "czmodel-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "ae34bcfd55a216df37d6866ec79ec28d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4", "size": 19657, "upload_time": "2020-03-09T18:19:26", "upload_time_iso_8601": "2020-03-09T18:19:26.610782Z", "url": "https://files.pythonhosted.org/packages/c9/9b/6c80c4fc1321ddd5a7b43d37ef8aded7086dd044131071cdd7b6a1d15f8c/czmodel-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c4e0c3a3bf5443e6396d0250f39f92ce", "sha256": "e02907e2c08cb1e66cf3712f42d4a2cc13b3bdd89f40ae8e0fbea055ddac832c"}, "downloads": -1, "filename": "czmodel-0.1.0.tar.gz", "has_sig": false, "md5_digest": "c4e0c3a3bf5443e6396d0250f39f92ce", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4", "size": 14412, "upload_time": "2020-03-09T18:19:28", "upload_time_iso_8601": "2020-03-09T18:19:28.771664Z", "url": "https://files.pythonhosted.org/packages/16/de/00ba01c6fa6c1b54174d6fde7f6b73a92f9f31be17a5d08fa85dba90b8df/czmodel-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "1f6e375ab28153cf087eef1d8cde6b7a", "sha256": "a6abb9f6daf85653643c45fba69ee947c6c39d65173f8caf1341405fef823173"}, "downloads": -1, "filename": "czmodel-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "1f6e375ab28153cf087eef1d8cde6b7a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4", "size": 21464, "upload_time": "2020-03-11T16:07:01", "upload_time_iso_8601": "2020-03-11T16:07:01.435249Z", "url": "https://files.pythonhosted.org/packages/f7/ee/43f7c028945e59e08a49ed0b5fef860618a6b52f94404c5147d431d98ac2/czmodel-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7df63afc984583d929a39173007499a2", "sha256": "a787df0c27359a3eb184695c7e22ccab5181d354aa35ce8d457f8b822cd276ce"}, "downloads": -1, "filename": "czmodel-0.1.1.tar.gz", "has_sig": false, "md5_digest": "7df63afc984583d929a39173007499a2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4", "size": 15537, "upload_time": "2020-03-11T16:07:02", "upload_time_iso_8601": "2020-03-11T16:07:02.623619Z", "url": "https://files.pythonhosted.org/packages/42/e4/9f82e275411e50f6d53a9ed0276bb95479f7bf20b35aaa1b7a2b0146aeb7/czmodel-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "0ead810a3543559fa717902991271844", "sha256": "d89903ac2387f3c42389706ea0ac7132e8e8373b4d11ec7adf68921eca7b9ee2"}, "downloads": -1, "filename": "czmodel-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "0ead810a3543559fa717902991271844", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4", "size": 21065, "upload_time": "2020-04-03T10:06:06", "upload_time_iso_8601": "2020-04-03T10:06:06.374768Z", "url": "https://files.pythonhosted.org/packages/2d/de/d42fa4d78b7b01a40c58b9f65b49051dea3fc2ec89aa628b580bff0211a7/czmodel-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "77dce16f4a0e2db9bf9b899f28c497ee", "sha256": "c5fc5b7558bacdeeb44984d725ce15f227e0df994eb49dc8c15ad344192e13a3"}, "downloads": -1, "filename": "czmodel-0.1.2.tar.gz", "has_sig": false, "md5_digest": "77dce16f4a0e2db9bf9b899f28c497ee", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4", "size": 17328, "upload_time": "2020-04-03T10:06:07", "upload_time_iso_8601": "2020-04-03T10:06:07.581263Z", "url": "https://files.pythonhosted.org/packages/53/f5/f186b3fea8be134d7369ffa67f2bbcddaf61728ad2598fff38df612edd99/czmodel-0.1.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "0ead810a3543559fa717902991271844", "sha256": "d89903ac2387f3c42389706ea0ac7132e8e8373b4d11ec7adf68921eca7b9ee2"}, "downloads": -1, "filename": "czmodel-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "0ead810a3543559fa717902991271844", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4", "size": 21065, "upload_time": "2020-04-03T10:06:06", "upload_time_iso_8601": "2020-04-03T10:06:06.374768Z", "url": "https://files.pythonhosted.org/packages/2d/de/d42fa4d78b7b01a40c58b9f65b49051dea3fc2ec89aa628b580bff0211a7/czmodel-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "77dce16f4a0e2db9bf9b899f28c497ee", "sha256": "c5fc5b7558bacdeeb44984d725ce15f227e0df994eb49dc8c15ad344192e13a3"}, "downloads": -1, "filename": "czmodel-0.1.2.tar.gz", "has_sig": false, "md5_digest": "77dce16f4a0e2db9bf9b899f28c497ee", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4", "size": 17328, "upload_time": "2020-04-03T10:06:07", "upload_time_iso_8601": "2020-04-03T10:06:07.581263Z", "url": "https://files.pythonhosted.org/packages/53/f5/f186b3fea8be134d7369ffa67f2bbcddaf61728ad2598fff38df612edd99/czmodel-0.1.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:41:00 2020"}