{"info": {"author": "Adrien Hadj-Salah", "author_email": "adrien.hadj.salah@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "[![Build status](https://travis-ci.com/Hadjubuntu/sweet-rl.svg?branch=master)](https://travis-ci.com/Hadjubuntu/sweet-rl)<br />\n![Sweet-RL](https://raw.githubusercontent.com/Hadjubuntu/sweet-rl/develop/misc/logo.png)\n\n## Why Sweet-RL\n \nIt exists dozens of Reinforcement Learning frameworks and algorithms implementations.\nYet, most of them suffer of poor modularity and ease of understanding. This is why, I started to implement my own: Sweet-RL.\nIt's so sweet that you can switch from Tensorflow 2.1 to PyTorch 1.4 with a single configuration line:  \n![Sweet-RL](https://raw.githubusercontent.com/Hadjubuntu/sweet-rl/develop/misc/ml-platform.png)\n\n\n## Getting started\n\n### Install sweet-rl  \n\nFirst, create a virtualenv:  \n```bash\npython3.x -m venv ~/.virtualenvs/sweet/ \n# or: virtualenv ~/.virtualenvs/sweet/ -p python3\nsource ~/.virtualenvs/sweet/bin/activate\n```\nAnd then, install project dependancies:  \n```bash\nmake install # or pip install -e .\n```\n\n### First execution  \n\nRun a DQN training:  \n```bash\npython -m sweet.run --env=CartPole-v0 --algo=dqn --ml=tf\n\n# Parameters:\n#   -h, --help            show this help message and exit\n#   --env ENV             Environment to play with\n#   --algo ALGO           RL agent\n#   --ml ML               ML platform (tf or torch)\n#   --model MODEL         Model (dense, pi_actor_critic)\n#   --timesteps TIMESTEPS\n#                         Number of training steps\n#   --output OUTPUT       Output directory (eg. './target/')\n```\n\n### Custom neural network\n\nIf you want to specify your own model instead of default ones, take a look to\n`sweet.agents.dqn.experiments.train_custom_model`\n\n## Features, algorithms implemented\n\n### Algorithms\n| Algorithm     |\u00a0Implementation status |\u00a0 ML platform  |\n| ------------- | -------------         | ------------- |\n| DQN | <g-emoji class=\"g-emoji\" alias=\"heavy_check_mark\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2714.png\">\u2714\ufe0f</g-emoji>  |  TF2, Torch |\n| A2C           | <g-emoji class=\"g-emoji\" alias=\"heavy_check_mark\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2714.png\">\u2714\ufe0f</g-emoji>  |  TF2, Torch   |\n| PPO           | Soon                  |               |\n\n\n### IO: Logs, model, tensorboard events\nOutputs are configurable in training function:\n```python\ntargets: dict = {\n        'output_dir': Path('./target/'), #\u00a0Main directory to store your outputs\n        'models_dir': 'models_checkpoints', # Saving models (depending on model_checkpoint_freq)\n        'logs_dir': 'logs', # Saving logs (info, debug, errors)\n        'tb_dir': 'tb_events' # Saving tensorboard events\n}\n```\nModels are saved depending on `model_checkpoint_freq` parameter set in train function.\n\n## Benchmark\n\nTo reproduce benchmark, execute:\n```bash\npython -m sweet.benchmark.benchmark_runner\n```\n\nHere is an example of benchmark between TF 2.0 and Torch 1.4 with CartPole-v0 environment:  \n![Benchmark RL](https://raw.githubusercontent.com/Hadjubuntu/sweet-rl/develop/misc/bench-example.png)\n\n\n\n## Troubleshootings\n\n* Tensorflow 2.x doesn't work with Python 3.8 so far, so only Python versions 3.6 and 3.7 are supported\n* GPU is not used. See https://www.tensorflow.org/install/gpu\n\n## History/Author\n\nI started this open-source RL framework in january 2020, at first to take benefit of tensorflow 2.x readability without sacrifying the performance.\nBesides coding open-source project, i work for both Airbus and IRT Saint-Exup\u00e9ry on Earth Observation satellites. Our team is focus on mission planning for satellites and Reinforcement Learning is an approach to solve it. Feel free to discuss with me: [Adrien HADJ-SALAH @linkedin](https://www.linkedin.com/in/adrien-hadj-salah-1b119462/)\n\n**You are welcome to participate to this project**\n\n## RL related topics\n\n* **What is Reinforcement Learning**\nIt is supposed that you have knowledge in RL, if it is not the case, take a look to the [spinningup from OpenAI](https://spinningup.openai.com/en/latest/)", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Hadjubuntu/sweet-rl", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "sweet-rl", "package_url": "https://pypi.org/project/sweet-rl/", "platform": "", "project_url": "https://pypi.org/project/sweet-rl/", "project_urls": {"Homepage": "https://github.com/Hadjubuntu/sweet-rl"}, "release_url": "https://pypi.org/project/sweet-rl/0.1/", "requires_dist": null, "requires_python": ">=3.6", "summary": "The sweetest Reinforcement Learning framework", "version": "0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://travis-ci.com/Hadjubuntu/sweet-rl\" rel=\"nofollow\"><img alt=\"Build status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/06a6a07c8904c798b7f45976d6fd6dc4424dd848/68747470733a2f2f7472617669732d63692e636f6d2f4861646a7562756e74752f73776565742d726c2e7376673f6272616e63683d6d6173746572\"></a><br>\n<img alt=\"Sweet-RL\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/12d51feb3c0370720c0660c9487e62429d4b09c6/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4861646a7562756e74752f73776565742d726c2f646576656c6f702f6d6973632f6c6f676f2e706e67\"></p>\n<h2>Why Sweet-RL</h2>\n<p>It exists dozens of Reinforcement Learning frameworks and algorithms implementations.\nYet, most of them suffer of poor modularity and ease of understanding. This is why, I started to implement my own: Sweet-RL.\nIt's so sweet that you can switch from Tensorflow 2.1 to PyTorch 1.4 with a single configuration line:<br>\n<img alt=\"Sweet-RL\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0b35c1e8b5aef53e25196080c2556ee67f066a52/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4861646a7562756e74752f73776565742d726c2f646576656c6f702f6d6973632f6d6c2d706c6174666f726d2e706e67\"></p>\n<h2>Getting started</h2>\n<h3>Install sweet-rl</h3>\n<p>First, create a virtualenv:</p>\n<pre>python3.x -m venv ~/.virtualenvs/sweet/ \n<span class=\"c1\"># or: virtualenv ~/.virtualenvs/sweet/ -p python3</span>\n<span class=\"nb\">source</span> ~/.virtualenvs/sweet/bin/activate\n</pre>\n<p>And then, install project dependancies:</p>\n<pre>make install <span class=\"c1\"># or pip install -e .</span>\n</pre>\n<h3>First execution</h3>\n<p>Run a DQN training:</p>\n<pre>python -m sweet.run --env<span class=\"o\">=</span>CartPole-v0 --algo<span class=\"o\">=</span>dqn --ml<span class=\"o\">=</span>tf\n\n<span class=\"c1\"># Parameters:</span>\n<span class=\"c1\">#   -h, --help            show this help message and exit</span>\n<span class=\"c1\">#   --env ENV             Environment to play with</span>\n<span class=\"c1\">#   --algo ALGO           RL agent</span>\n<span class=\"c1\">#   --ml ML               ML platform (tf or torch)</span>\n<span class=\"c1\">#   --model MODEL         Model (dense, pi_actor_critic)</span>\n<span class=\"c1\">#   --timesteps TIMESTEPS</span>\n<span class=\"c1\">#                         Number of training steps</span>\n<span class=\"c1\">#   --output OUTPUT       Output directory (eg. './target/')</span>\n</pre>\n<h3>Custom neural network</h3>\n<p>If you want to specify your own model instead of default ones, take a look to\n<code>sweet.agents.dqn.experiments.train_custom_model</code></p>\n<h2>Features, algorithms implemented</h2>\n<h3>Algorithms</h3>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>\u00a0Implementation status</th>\n<th>\u00a0 ML platform</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>DQN</td>\n<td>&lt;g-emoji class=\"g-emoji\" alias=\"heavy_check_mark\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2714.png\"&gt;\u2714\ufe0f&lt;/g-emoji&gt;</td>\n<td>TF2, Torch</td>\n</tr>\n<tr>\n<td>A2C</td>\n<td>&lt;g-emoji class=\"g-emoji\" alias=\"heavy_check_mark\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2714.png\"&gt;\u2714\ufe0f&lt;/g-emoji&gt;</td>\n<td>TF2, Torch</td>\n</tr>\n<tr>\n<td>PPO</td>\n<td>Soon</td>\n<td></td>\n</tr></tbody></table>\n<h3>IO: Logs, model, tensorboard events</h3>\n<p>Outputs are configurable in training function:</p>\n<pre><span class=\"n\">targets</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n        <span class=\"s1\">'output_dir'</span><span class=\"p\">:</span> <span class=\"n\">Path</span><span class=\"p\">(</span><span class=\"s1\">'./target/'</span><span class=\"p\">),</span> <span class=\"c1\">#\u00a0Main directory to store your outputs</span>\n        <span class=\"s1\">'models_dir'</span><span class=\"p\">:</span> <span class=\"s1\">'models_checkpoints'</span><span class=\"p\">,</span> <span class=\"c1\"># Saving models (depending on model_checkpoint_freq)</span>\n        <span class=\"s1\">'logs_dir'</span><span class=\"p\">:</span> <span class=\"s1\">'logs'</span><span class=\"p\">,</span> <span class=\"c1\"># Saving logs (info, debug, errors)</span>\n        <span class=\"s1\">'tb_dir'</span><span class=\"p\">:</span> <span class=\"s1\">'tb_events'</span> <span class=\"c1\"># Saving tensorboard events</span>\n<span class=\"p\">}</span>\n</pre>\n<p>Models are saved depending on <code>model_checkpoint_freq</code> parameter set in train function.</p>\n<h2>Benchmark</h2>\n<p>To reproduce benchmark, execute:</p>\n<pre>python -m sweet.benchmark.benchmark_runner\n</pre>\n<p>Here is an example of benchmark between TF 2.0 and Torch 1.4 with CartPole-v0 environment:<br>\n<img alt=\"Benchmark RL\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a5c78e56f0cede1d4cdb1d3fca7e1f3a490aca43/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4861646a7562756e74752f73776565742d726c2f646576656c6f702f6d6973632f62656e63682d6578616d706c652e706e67\"></p>\n<h2>Troubleshootings</h2>\n<ul>\n<li>Tensorflow 2.x doesn't work with Python 3.8 so far, so only Python versions 3.6 and 3.7 are supported</li>\n<li>GPU is not used. See <a href=\"https://www.tensorflow.org/install/gpu\" rel=\"nofollow\">https://www.tensorflow.org/install/gpu</a></li>\n</ul>\n<h2>History/Author</h2>\n<p>I started this open-source RL framework in january 2020, at first to take benefit of tensorflow 2.x readability without sacrifying the performance.\nBesides coding open-source project, i work for both Airbus and IRT Saint-Exup\u00e9ry on Earth Observation satellites. Our team is focus on mission planning for satellites and Reinforcement Learning is an approach to solve it. Feel free to discuss with me: <a href=\"https://www.linkedin.com/in/adrien-hadj-salah-1b119462/\" rel=\"nofollow\">Adrien HADJ-SALAH @linkedin</a></p>\n<p><strong>You are welcome to participate to this project</strong></p>\n<h2>RL related topics</h2>\n<ul>\n<li><strong>What is Reinforcement Learning</strong>\nIt is supposed that you have knowledge in RL, if it is not the case, take a look to the <a href=\"https://spinningup.openai.com/en/latest/\" rel=\"nofollow\">spinningup from OpenAI</a></li>\n</ul>\n\n          </div>"}, "last_serial": 6712338, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "d4bdc8058f276effbfc7f1528fdb3792", "sha256": "655abbed804f149218d7378ed103019f492d6a2d9ed9e72b18b8ad321dd23faf"}, "downloads": -1, "filename": "sweet-rl-0.0.1.tar.gz", "has_sig": false, "md5_digest": "d4bdc8058f276effbfc7f1528fdb3792", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 11841, "upload_time": "2020-01-31T19:10:32", "upload_time_iso_8601": "2020-01-31T19:10:32.526375Z", "url": "https://files.pythonhosted.org/packages/2e/cb/3d5d1c532aa7774111a9c65d13b59fc12c0cb13bb6b0336fd7eb0af76a64/sweet-rl-0.0.1.tar.gz", "yanked": false}], "0.0.1rc1": [{"comment_text": "", "digests": {"md5": "76d0c5c81e5631df7b6f032129384df4", "sha256": "563016c3b6ca176559ece708c891f4fdf14311a68ab95f13526f2cd9ae3a51f5"}, "downloads": -1, "filename": "sweet-rl-0.0.1rc1.tar.gz", "has_sig": false, "md5_digest": "76d0c5c81e5631df7b6f032129384df4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13376, "upload_time": "2020-02-02T22:28:16", "upload_time_iso_8601": "2020-02-02T22:28:16.506687Z", "url": "https://files.pythonhosted.org/packages/9d/22/55139c651b4614869620128b05de150ae31b93d36b27b8fdf8c6df869353/sweet-rl-0.0.1rc1.tar.gz", "yanked": false}], "0.1": [{"comment_text": "", "digests": {"md5": "a487645da0792f1db1ba4fbf9afcbb6f", "sha256": "2cd6885f95b1fd7169cf11f172c102a0e5dd1c86368fc2973eb4b8e15cba7519"}, "downloads": -1, "filename": "sweet-rl-0.1.tar.gz", "has_sig": false, "md5_digest": "a487645da0792f1db1ba4fbf9afcbb6f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 12340, "upload_time": "2020-02-27T17:35:32", "upload_time_iso_8601": "2020-02-27T17:35:32.373701Z", "url": "https://files.pythonhosted.org/packages/fe/69/2f1d5ced0f81fc0fb2cc6196f2033420036ccd10e8b9ee1e94f3df4cfee2/sweet-rl-0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a487645da0792f1db1ba4fbf9afcbb6f", "sha256": "2cd6885f95b1fd7169cf11f172c102a0e5dd1c86368fc2973eb4b8e15cba7519"}, "downloads": -1, "filename": "sweet-rl-0.1.tar.gz", "has_sig": false, "md5_digest": "a487645da0792f1db1ba4fbf9afcbb6f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 12340, "upload_time": "2020-02-27T17:35:32", "upload_time_iso_8601": "2020-02-27T17:35:32.373701Z", "url": "https://files.pythonhosted.org/packages/fe/69/2f1d5ced0f81fc0fb2cc6196f2033420036ccd10e8b9ee1e94f3df4cfee2/sweet-rl-0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:59:36 2020"}