{"info": {"author": "Priyansh Trivedi", "author_email": "mail@priyansh.page", "bugtrack_url": null, "classifiers": ["Development Status :: 1 - Planning", "Operating System :: Unix", "Programming Language :: Python :: 3"], "description": "# mytorch is your torch :fire:\nA transparent boilerplate + bag of tricks to ease my (yours?) (our?) PyTorch dev time.\n\nSome parts here are inspired/copied from [fast.ai](https://github.com/fastai/fastai).\nHowever, I've tried to keep is such that the control of model (model architecture), vocabulary, preprocessing is always maintained outside of this library.\nThe [training loop](./mytorch/loops.py), [data samplers](./mytorch/dataiters.py) etc can be used independent of anything else in here, but ofcourse work better together.\n\nI'll be adding proper documentation, examples here, gradually.\n\n# Installation\n\n`pip install my-torch`\n\n(Added hyphen because someone beat me to the [mytorch](https://pypi.org/project/mytorch/) package name.)\n\n# Idea\n\nUse/Ignore most parts of the library. Will not hide code from you, and you retain control over your models. \n    If you need just one thing, no fluff, feel free to copy-paste snippets of the code from this repo to yours.\n    I'd be delighted if you drop me a line, if you found this stuff helpful.\n\n# Features\n\n1. **Customizable Training Loop**\n    - Callbacks @ epoch start and end\n    - Weight Decay (see [this blog post](https://www.fast.ai/2018/07/02/adam-weight-decay/) )\n    - :scissors: Gradient Clipping\n    - :floppy_disk: Model Saving \n    - :bell: Mobile push notifications @ the end of training :ghost: ( [See Usage](#notifications)) )\n\n2. **Sortist Sampling** \n\n3. **Custom Learning Rate Schedules** \n\n4. Customisability & Flat Hierarchy\n\n# Usage\n\n\n## Simplest Use Case\n```\nimport torch, torch.nn as nn, numpy as np\n\n# Assuming that you have a torch model with a predict and a forward function.\n# model = MyModel()\nassert type(model) is nn.Module\n\n# X, Y are input and output labels for a text classification task with four classes. 200 examples.\nX_trn = np.random.randint(0, 100, (200, 4))\nY_trn = np.random.randint(0, 4, (200, 1))\nX_val = np.random.randint(0, 100, (100, 4))\nY_val = np.random.randint(0, 4, (100, 1))\n\n# Preparing data\ndata = {\"train\":{\"x\":X_trn, \"y\":Y_trn}, \"valid\":{\"x\":X_val, \"y\":Y_val} }\n\n# Specifying other hyperparameters\nepochs = 10\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\nloss_function = nn.functional.cross_entropy\ntrain_function = model      # or model.forward\npredict_function = model.predict\n\ntrain_acc, valid_acc, train_loss = loops.simplest_loop(epochs=epochs, data=data, opt=optimizer,\n                                                        loss_fn=loss_function, \n                                                        train_fn=train_function,\n                                                        predict_fn=predict_function)\n```\n\n## Slightly more complex examples\n\n@TODO: They exist! Just need to add examples :sweat_smile:\n1. Custom eval\n2. Custom data sampler\n3. Custom learning rate annealing schedules\n\n## Saving the model\n@TODO\n\n\n## Notifications\nThe training loop can send notifications to your phone informing you that your model's done training and report metrics alongwith.\nWe use [push.techulus.com](https://push.techulus.com/) to do so and you'll need the app on your phone.\n*If you're not bothered, this part of the code will stay out of your way.* \nBut If you'd like this completely unnecessary gimmick, follow along:\n\n1. Get the app. [Play Store](https://play.google.com/store/apps/details?id=com.techulus.push) |  [AppStore](https://itunes.apple.com/us/app/push-by-techulus/id1444391917?ls=1&mt=8)\n2. Sign In/Up and get yout **api key**\n3. Making the key available. Options:\n    1. in a file, named `./push-techulus-key`, in plaintext at the root dir of this folder. You could just `echo 'your-api-key' >> ./push-techulus-ley`.\n    2. through arguments to the training loop as a string\n4. Pass flag to loop, to enable notifications\n5. Done :balloon: You'll be notified when your model's done training.\n\n# Changelog\n#### v0.0.1\n1. Added some tests.\n1. Wrapping spaCy tokenizers, with some vocab management. \n1. Packaging :confetti:\n\n# Upcoming\n1. Models\n    1. Classifiers \n    1. Encoders\n    1. ~~Transformers~~ (USE [pytorch-transformers by :huggingface:](https://github.com/huggingface/pytorch-transformers))\n3. Using FastProgress for progress + live plotting\n1. [W&B](https://wandb.ai) integration\n4. ?? (tell me [here](https://github.com/geraltofrivia/mytorch/issues))  \n\n# Contributions\nI'm eager to implement more tricks/features in the library, while maintaining the flat structure (and ensuring backward compatibility). \nOpen to suggestions and contributions. Thanks! \n\nPS: Always appreciate more tests.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/geraltofrivia/mytorch/", "keywords": "deep learning,pytorch,boilerplate,machine learning,neural network,preprocessing", "license": "", "maintainer": "", "maintainer_email": "", "name": "my-torch", "package_url": "https://pypi.org/project/my-torch/", "platform": "", "project_url": "https://pypi.org/project/my-torch/", "project_urls": {"Homepage": "https://github.com/geraltofrivia/mytorch/"}, "release_url": "https://pypi.org/project/my-torch/0.0.1/", "requires_dist": null, "requires_python": "", "summary": "A transparent boilerplate + bag of tricks to ease my (yours?) (our?) PyTorch dev time.", "version": "0.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>mytorch is your torch :fire:</h1>\n<p>A transparent boilerplate + bag of tricks to ease my (yours?) (our?) PyTorch dev time.</p>\n<p>Some parts here are inspired/copied from <a href=\"https://github.com/fastai/fastai\" rel=\"nofollow\">fast.ai</a>.\nHowever, I've tried to keep is such that the control of model (model architecture), vocabulary, preprocessing is always maintained outside of this library.\nThe <a href=\"./mytorch/loops.py\" rel=\"nofollow\">training loop</a>, <a href=\"./mytorch/dataiters.py\" rel=\"nofollow\">data samplers</a> etc can be used independent of anything else in here, but ofcourse work better together.</p>\n<p>I'll be adding proper documentation, examples here, gradually.</p>\n<h1>Installation</h1>\n<p><code>pip install my-torch</code></p>\n<p>(Added hyphen because someone beat me to the <a href=\"https://pypi.org/project/mytorch/\" rel=\"nofollow\">mytorch</a> package name.)</p>\n<h1>Idea</h1>\n<p>Use/Ignore most parts of the library. Will not hide code from you, and you retain control over your models.\nIf you need just one thing, no fluff, feel free to copy-paste snippets of the code from this repo to yours.\nI'd be delighted if you drop me a line, if you found this stuff helpful.</p>\n<h1>Features</h1>\n<ol>\n<li>\n<p><strong>Customizable Training Loop</strong></p>\n<ul>\n<li>Callbacks @ epoch start and end</li>\n<li>Weight Decay (see <a href=\"https://www.fast.ai/2018/07/02/adam-weight-decay/\" rel=\"nofollow\">this blog post</a> )</li>\n<li>:scissors: Gradient Clipping</li>\n<li>:floppy_disk: Model Saving</li>\n<li>:bell: Mobile push notifications @ the end of training :ghost: ( <a href=\"#notifications\" rel=\"nofollow\">See Usage</a>) )</li>\n</ul>\n</li>\n<li>\n<p><strong>Sortist Sampling</strong></p>\n</li>\n<li>\n<p><strong>Custom Learning Rate Schedules</strong></p>\n</li>\n<li>\n<p>Customisability &amp; Flat Hierarchy</p>\n</li>\n</ol>\n<h1>Usage</h1>\n<h2>Simplest Use Case</h2>\n<pre><code>import torch, torch.nn as nn, numpy as np\n\n# Assuming that you have a torch model with a predict and a forward function.\n# model = MyModel()\nassert type(model) is nn.Module\n\n# X, Y are input and output labels for a text classification task with four classes. 200 examples.\nX_trn = np.random.randint(0, 100, (200, 4))\nY_trn = np.random.randint(0, 4, (200, 1))\nX_val = np.random.randint(0, 100, (100, 4))\nY_val = np.random.randint(0, 4, (100, 1))\n\n# Preparing data\ndata = {\"train\":{\"x\":X_trn, \"y\":Y_trn}, \"valid\":{\"x\":X_val, \"y\":Y_val} }\n\n# Specifying other hyperparameters\nepochs = 10\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\nloss_function = nn.functional.cross_entropy\ntrain_function = model      # or model.forward\npredict_function = model.predict\n\ntrain_acc, valid_acc, train_loss = loops.simplest_loop(epochs=epochs, data=data, opt=optimizer,\n                                                        loss_fn=loss_function, \n                                                        train_fn=train_function,\n                                                        predict_fn=predict_function)\n</code></pre>\n<h2>Slightly more complex examples</h2>\n<p>@TODO: They exist! Just need to add examples :sweat_smile:</p>\n<ol>\n<li>Custom eval</li>\n<li>Custom data sampler</li>\n<li>Custom learning rate annealing schedules</li>\n</ol>\n<h2>Saving the model</h2>\n<p>@TODO</p>\n<h2>Notifications</h2>\n<p>The training loop can send notifications to your phone informing you that your model's done training and report metrics alongwith.\nWe use <a href=\"https://push.techulus.com/\" rel=\"nofollow\">push.techulus.com</a> to do so and you'll need the app on your phone.\n<em>If you're not bothered, this part of the code will stay out of your way.</em>\nBut If you'd like this completely unnecessary gimmick, follow along:</p>\n<ol>\n<li>Get the app. <a href=\"https://play.google.com/store/apps/details?id=com.techulus.push\" rel=\"nofollow\">Play Store</a> |  <a href=\"https://itunes.apple.com/us/app/push-by-techulus/id1444391917?ls=1&amp;mt=8\" rel=\"nofollow\">AppStore</a></li>\n<li>Sign In/Up and get yout <strong>api key</strong></li>\n<li>Making the key available. Options:\n<ol>\n<li>in a file, named <code>./push-techulus-key</code>, in plaintext at the root dir of this folder. You could just <code>echo 'your-api-key' &gt;&gt; ./push-techulus-ley</code>.</li>\n<li>through arguments to the training loop as a string</li>\n</ol>\n</li>\n<li>Pass flag to loop, to enable notifications</li>\n<li>Done :balloon: You'll be notified when your model's done training.</li>\n</ol>\n<h1>Changelog</h1>\n<h4>v0.0.1</h4>\n<ol>\n<li>Added some tests.</li>\n<li>Wrapping spaCy tokenizers, with some vocab management.</li>\n<li>Packaging :confetti:</li>\n</ol>\n<h1>Upcoming</h1>\n<ol>\n<li>Models\n<ol>\n<li>Classifiers</li>\n<li>Encoders</li>\n<li><del>Transformers</del> (USE <a href=\"https://github.com/huggingface/pytorch-transformers\" rel=\"nofollow\">pytorch-transformers by :huggingface:</a>)</li>\n</ol>\n</li>\n<li>Using FastProgress for progress + live plotting</li>\n<li><a href=\"https://wandb.ai\" rel=\"nofollow\">W&amp;B</a> integration</li>\n<li>?? (tell me <a href=\"https://github.com/geraltofrivia/mytorch/issues\" rel=\"nofollow\">here</a>)</li>\n</ol>\n<h1>Contributions</h1>\n<p>I'm eager to implement more tricks/features in the library, while maintaining the flat structure (and ensuring backward compatibility).\nOpen to suggestions and contributions. Thanks!</p>\n<p>PS: Always appreciate more tests.</p>\n\n          </div>"}, "last_serial": 5832676, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "20c9fa2d27ec7f95e972d615a75f7146", "sha256": "4a4d4a86705bb7eb3e7e6b1797279eb91f0fe7b37c18cf6c7e4aa8cac6ce193c"}, "downloads": -1, "filename": "my_torch-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "20c9fa2d27ec7f95e972d615a75f7146", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18515, "upload_time": "2019-09-15T17:54:42", "upload_time_iso_8601": "2019-09-15T17:54:42.187286Z", "url": "https://files.pythonhosted.org/packages/52/a9/1c15f77f1cb13f44c638188102907bd17641862d20d8b130b136091059ff/my_torch-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "eb94b096c1b923b59058c2c37fbb18ff", "sha256": "5466344cf22d1bfcdb4818109dd1e533678b53d1e4b0485b152bd55029b892ff"}, "downloads": -1, "filename": "my-torch-0.0.1.tar.gz", "has_sig": false, "md5_digest": "eb94b096c1b923b59058c2c37fbb18ff", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15657, "upload_time": "2019-09-15T17:54:44", "upload_time_iso_8601": "2019-09-15T17:54:44.729477Z", "url": "https://files.pythonhosted.org/packages/76/35/a3ddba47266240b8d2d87077267a9560bb18c6c900853d0e6fd58492b4d4/my-torch-0.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "20c9fa2d27ec7f95e972d615a75f7146", "sha256": "4a4d4a86705bb7eb3e7e6b1797279eb91f0fe7b37c18cf6c7e4aa8cac6ce193c"}, "downloads": -1, "filename": "my_torch-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "20c9fa2d27ec7f95e972d615a75f7146", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18515, "upload_time": "2019-09-15T17:54:42", "upload_time_iso_8601": "2019-09-15T17:54:42.187286Z", "url": "https://files.pythonhosted.org/packages/52/a9/1c15f77f1cb13f44c638188102907bd17641862d20d8b130b136091059ff/my_torch-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "eb94b096c1b923b59058c2c37fbb18ff", "sha256": "5466344cf22d1bfcdb4818109dd1e533678b53d1e4b0485b152bd55029b892ff"}, "downloads": -1, "filename": "my-torch-0.0.1.tar.gz", "has_sig": false, "md5_digest": "eb94b096c1b923b59058c2c37fbb18ff", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15657, "upload_time": "2019-09-15T17:54:44", "upload_time_iso_8601": "2019-09-15T17:54:44.729477Z", "url": "https://files.pythonhosted.org/packages/76/35/a3ddba47266240b8d2d87077267a9560bb18c6c900853d0e6fd58492b4d4/my-torch-0.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:47:57 2020"}