{"info": {"author": "Liu Changyu", "author_email": "liuchangyu1111@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: Apache Software License", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "\n# VGGNet-PyTorch\n\n### Update (Feb 14, 2020)\n\nThe update is for ease of use and deployment.\n\n * [Example: Export to ONNX](#example-export-to-onnx)\n * [Example: Extract features](#example-feature-extraction)\n * [Example: Visual](#example-visual)\n\nIt is also now incredibly simple to load a pretrained model with a new number of classes for transfer learning:\n\n```python\nfrom vgg_pytorch import VGG \nmodel = VGG.from_pretrained('vgg11', num_classes=10)\n```\n\n### Update (January 15, 2020)\n\nThis update allows you to use NVIDIA's Apex tool for accelerated training. By default choice `hybrid training precision` + `dynamic loss amplified` version, if you need to learn more and details about `apex` tools, please visit https://github.com/NVIDIA/apex.\n\n### Update (January 9, 2020)\n\nThis update adds a visual interface for testing, which is developed by pyqt5. At present, it has realized basic functions, and other functions will be gradually improved in the future.\n\n### Update (January 6, 2020)\n\nThis update adds a modular neural network, making it more flexible in use. It can be deployed to many common dataset classification tasks. Of course, it can also be used in your products.\n\n### Overview\nThis repository contains an op-for-op PyTorch reimplementation of [VGGNet](https://arxiv.org/pdf/1409.1556.pdf).\n\nThe goal of this implementation is to be simple, highly extensible, and easy to integrate into your own projects. This implementation is a work in progress -- new features are currently being implemented.  \n\nAt the moment, you can easily:  \n * Load pretrained VGGNet models \n * Use VGGNet models for classification or feature extraction \n\n_Upcoming features_: In the next few days, you will be able to:\n * Quickly finetune an VGGNet on your own dataset\n * Export VGGNet models for production\n\n### Table of contents\n1. [About VGG](#about-vgg)\n2. [Installation](#installation)\n3. [Usage](#usage)\n    * [Load pretrained models](#loading-pretrained-models)\n    * [Example: Classify](#example-classification)\n    * [Example: Extract features](#example-feature-extraction)\n    * [Example: Export to ONNX](#example-export-to-onnx)\n    * [Example: Visual](#example-visual)\n4. [Contributing](#contributing) \n\n### About VGG\n\nIf you're new to VGGNets, here is an explanation straight from the official PyTorch implementation: \n\nIn this work we investigate the effect of the convolutional network depth on its\naccuracy in the large-scale image recognition setting. Our main contribution is\na thorough evaluation of networks of increasing depth using an architecture with\nvery small (3 \u00d7 3) convolution filters, which shows that a significant improvement\non the prior-art configurations can be achieved by pushing the depth to 16\u201319\nweight layers. These findings were the basis of our ImageNet Challenge 2014\nsubmission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations\ngeneralise well to other datasets, where they achieve state-of-the-art results. We\nhave made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n\n### Installation\n\nInstall from pypi:\n```bash\n$ pip3 install vgg_pytorch\n```\n\nInstall from source:\n```bash\n$ git clone https://github.com/Lornatang/VGGNet-PyTorch.git\n$ cd VGGNet-PyTorch\n$ pip3 install -e .\n```\n\n### Usage\n\n#### Loading pretrained models\n\nLoad an vgg11 network:\n```python\nfrom vgg_pytorch import VGG\nmodel = VGG.from_name(\"vgg11\")\n```\n\nLoad a pretrained vgg11: \n```python\nfrom vgg_pytorch import VGG\nmodel = VGG.from_pretrained(\"vgg11\")\n```\n\nTheir 1-crop error rates on imagenet dataset with pretrained models are listed below.\n\n| Model structure | Top-1 error | Top-5 error |\n| --------------- | ----------- | ----------- |\n|  vgg11          | 30.98       | 11.37       |\n|  vgg11_bn       | 29.70       | 10.19       |\n|  vgg13          | 30.07       | 10.75       |\n|  vgg13_bn       | 28.45       | 9.63        |\n|  vgg16          | 28.41       | 9.62        |\n|  vgg16_bn       | 26.63       | 8.50        |\n|  vgg19          | 27.62       | 9.12        |\n|  vgg19_bn       | 25.76       | 8.15        |\n\nDetails about the models are below (for CIFAR10 dataset): \n\n|      *Name*       |*# Params*|*Top-1 Acc.*|*Pretrained?*|\n|:-----------------:|:--------:|:----------:|:-----------:|\n|     `vgg11`       |  132.9M  |    91.1    |      \u221a      |\n|     `vgg13`       |   133M   |    92.8    |      \u221a      |\n|     `vgg16`       |  138.4M  |    92.6    |      \u221a      |\n|     `vgg19`       |  143.7M  |    92.3    |      \u221a      |\n|-------------------|----------|------------|-------------|\n|     `vgg11_bn`    |  132.9M  |    92.2    |      \u221a      |\n|     `vgg13_bn`    |   133M   |    94.2    |      \u221a      |\n|     `vgg16_bn`    |  138.4M  |    93.9    |      \u221a      |\n|     `vgg19_bn`    |  143.7M  |    93.7    |      \u221a      |\n\n\n#### Example: Classification\n\nWe assume that in your current directory, there is a `img.jpg` file and a `labels_map.txt` file (ImageNet class names). These are both included in `examples/simple`. \n\nAll pre-trained models expect input images normalized in the same way,\ni.e. mini-batches of 3-channel RGB images of shape `(3 x H x W)`, where `H` and `W` are expected to be at least `224`.\nThe images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\nand `std = [0.229, 0.224, 0.225]`.\n\nHere's a sample execution.\n\n```python\nimport json\n\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nfrom vgg_pytorch import VGG \n\n# Open image\ninput_image = Image.open(\"img.jpg\")\n\n# Preprocess image\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n\n# Load class names\nlabels_map = json.load(open(\"labels_map.txt\"))\nlabels_map = [labels_map[str(i)] for i in range(1000)]\n\n# Classify with VGG11\nmodel = VGG.from_pretrained(\"vgg11\")\nmodel.eval()\n\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n    input_batch = input_batch.to(\"cuda\")\n    model.to(\"cuda\")\n\nwith torch.no_grad():\n    logits = model(input_batch)\npreds = torch.topk(logits, k=5).indices.squeeze(0).tolist()\n\nprint(\"-----\")\nfor idx in preds:\n    label = labels_map[idx]\n    prob = torch.softmax(logits, dim=1)[0, idx].item()\n    print(f\"{label:<75} ({prob * 100:.2f}%)\")\n```\n\n#### Example: Feature Extraction \n\nYou can easily extract features with `model.extract_features`:\n```python\nimport torch\nfrom vgg_pytorch import VGG \nmodel = VGG.from_pretrained('vgg11')\n\n# ... image preprocessing as in the classification example ...\ninputs = torch.randn(1, 3, 224, 224)\nprint(inputs.shape) # torch.Size([1, 3, 224, 224])\n\nfeatures = model.extract_features(inputs)\nprint(features.shape) # torch.Size([1, 512, 7, 7])\n```\n\n#### Example: Export to ONNX  \n\nExporting to ONNX for deploying to production is now simple: \n```python\nimport torch \nfrom vgg_pytorch import VGG \n\nmodel = VGG.from_pretrained('vgg11')\ndummy_input = torch.randn(16, 3, 224, 224)\n\ntorch.onnx.export(model, dummy_input, \"demo.onnx\", verbose=True)\n```\n\n#### Example: Visual\n\n```text\ncd $REPO$/framework\nsh start.sh\n```\n\nThen open the browser and type in the browser address [http://127.0.0.1:8000/](http://127.0.0.1:8000/).\n\nEnjoy it.\n\n#### ImageNet\n\nSee `examples/imagenet` for details about evaluating on ImageNet.\n\nFor more datasets result. Please see `research/README.md`.\n\n### Contributing\n\nIf you find a bug, create a GitHub issue, or even better, submit a pull request. Similarly, if you have questions, simply post them as GitHub issues.   \n\nI look forward to seeing what the community does with these models! \n\n### Credit\n\n#### Very Deep Convolutional Networks for Large-Scale Image Recognition\n\n*Karen Simonyan, Andrew Zisserman*\n\n##### Abstract\n\nIn this work we investigate the effect of the convolutional network depth on its accuracy in the \nlarge-scale image recognition setting. Our main contribution is a thorough evaluation of networks \nof increasing depth using an architecture with very small (3x3) convolution filters, which shows \nthat a significant improvement on the prior-art configurations can be achieved by pushing the depth \nto 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, \nwhere our team secured the first and the second places in the localisation and classification tracks \nrespectively. We also show that our representations generalise well to other datasets, where they \nachieve state-of-the-art results. We have made our two best-performing ConvNet models publicly \navailable to facilitate further research on the use of deep visual representations in computer vision.\n\n[paper](https://arxiv.org/abs/1409.1556)\n\n```text\n@article{VGG,\ntitle:{Very Deep Convolutional Networks for Large-Scale Image Recognition},\nauthor:{Karen Simonyan, Andrew Zisserman},\njournal={iclr},\nyear={2015}\n}\n```\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Lornatang/VGGNet-PyTorch", "keywords": "", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "vgg-pytorch", "package_url": "https://pypi.org/project/vgg-pytorch/", "platform": "", "project_url": "https://pypi.org/project/vgg-pytorch/", "project_urls": {"Homepage": "https://github.com/Lornatang/VGGNet-PyTorch"}, "release_url": "https://pypi.org/project/vgg-pytorch/0.3.0/", "requires_dist": ["torch"], "requires_python": ">=3.6.0", "summary": "An VGGNet implements of PyTorch.", "version": "0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>VGGNet-PyTorch</h1>\n<h3>Update (Feb 14, 2020)</h3>\n<p>The update is for ease of use and deployment.</p>\n<ul>\n<li><a href=\"#example-export-to-onnx\" rel=\"nofollow\">Example: Export to ONNX</a></li>\n<li><a href=\"#example-feature-extraction\" rel=\"nofollow\">Example: Extract features</a></li>\n<li><a href=\"#example-visual\" rel=\"nofollow\">Example: Visual</a></li>\n</ul>\n<p>It is also now incredibly simple to load a pretrained model with a new number of classes for transfer learning:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">vgg_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">VGG</span> \n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">VGG</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'vgg11'</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n</pre>\n<h3>Update (January 15, 2020)</h3>\n<p>This update allows you to use NVIDIA's Apex tool for accelerated training. By default choice <code>hybrid training precision</code> + <code>dynamic loss amplified</code> version, if you need to learn more and details about <code>apex</code> tools, please visit <a href=\"https://github.com/NVIDIA/apex\" rel=\"nofollow\">https://github.com/NVIDIA/apex</a>.</p>\n<h3>Update (January 9, 2020)</h3>\n<p>This update adds a visual interface for testing, which is developed by pyqt5. At present, it has realized basic functions, and other functions will be gradually improved in the future.</p>\n<h3>Update (January 6, 2020)</h3>\n<p>This update adds a modular neural network, making it more flexible in use. It can be deployed to many common dataset classification tasks. Of course, it can also be used in your products.</p>\n<h3>Overview</h3>\n<p>This repository contains an op-for-op PyTorch reimplementation of <a href=\"https://arxiv.org/pdf/1409.1556.pdf\" rel=\"nofollow\">VGGNet</a>.</p>\n<p>The goal of this implementation is to be simple, highly extensible, and easy to integrate into your own projects. This implementation is a work in progress -- new features are currently being implemented.</p>\n<p>At the moment, you can easily:</p>\n<ul>\n<li>Load pretrained VGGNet models</li>\n<li>Use VGGNet models for classification or feature extraction</li>\n</ul>\n<p><em>Upcoming features</em>: In the next few days, you will be able to:</p>\n<ul>\n<li>Quickly finetune an VGGNet on your own dataset</li>\n<li>Export VGGNet models for production</li>\n</ul>\n<h3>Table of contents</h3>\n<ol>\n<li><a href=\"#about-vgg\" rel=\"nofollow\">About VGG</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#usage\" rel=\"nofollow\">Usage</a>\n<ul>\n<li><a href=\"#loading-pretrained-models\" rel=\"nofollow\">Load pretrained models</a></li>\n<li><a href=\"#example-classification\" rel=\"nofollow\">Example: Classify</a></li>\n<li><a href=\"#example-feature-extraction\" rel=\"nofollow\">Example: Extract features</a></li>\n<li><a href=\"#example-export-to-onnx\" rel=\"nofollow\">Example: Export to ONNX</a></li>\n<li><a href=\"#example-visual\" rel=\"nofollow\">Example: Visual</a></li>\n</ul>\n</li>\n<li><a href=\"#contributing\" rel=\"nofollow\">Contributing</a></li>\n</ol>\n<h3>About VGG</h3>\n<p>If you're new to VGGNets, here is an explanation straight from the official PyTorch implementation:</p>\n<p>In this work we investigate the effect of the convolutional network depth on its\naccuracy in the large-scale image recognition setting. Our main contribution is\na thorough evaluation of networks of increasing depth using an architecture with\nvery small (3 \u00d7 3) convolution filters, which shows that a significant improvement\non the prior-art configurations can be achieved by pushing the depth to 16\u201319\nweight layers. These findings were the basis of our ImageNet Challenge 2014\nsubmission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations\ngeneralise well to other datasets, where they achieve state-of-the-art results. We\nhave made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.</p>\n<h3>Installation</h3>\n<p>Install from pypi:</p>\n<pre>$ pip3 install vgg_pytorch\n</pre>\n<p>Install from source:</p>\n<pre>$ git clone https://github.com/Lornatang/VGGNet-PyTorch.git\n$ <span class=\"nb\">cd</span> VGGNet-PyTorch\n$ pip3 install -e .\n</pre>\n<h3>Usage</h3>\n<h4>Loading pretrained models</h4>\n<p>Load an vgg11 network:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">vgg_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">VGG</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">VGG</span><span class=\"o\">.</span><span class=\"n\">from_name</span><span class=\"p\">(</span><span class=\"s2\">\"vgg11\"</span><span class=\"p\">)</span>\n</pre>\n<p>Load a pretrained vgg11:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">vgg_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">VGG</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">VGG</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s2\">\"vgg11\"</span><span class=\"p\">)</span>\n</pre>\n<p>Their 1-crop error rates on imagenet dataset with pretrained models are listed below.</p>\n<table>\n<thead>\n<tr>\n<th>Model structure</th>\n<th>Top-1 error</th>\n<th>Top-5 error</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>vgg11</td>\n<td>30.98</td>\n<td>11.37</td>\n</tr>\n<tr>\n<td>vgg11_bn</td>\n<td>29.70</td>\n<td>10.19</td>\n</tr>\n<tr>\n<td>vgg13</td>\n<td>30.07</td>\n<td>10.75</td>\n</tr>\n<tr>\n<td>vgg13_bn</td>\n<td>28.45</td>\n<td>9.63</td>\n</tr>\n<tr>\n<td>vgg16</td>\n<td>28.41</td>\n<td>9.62</td>\n</tr>\n<tr>\n<td>vgg16_bn</td>\n<td>26.63</td>\n<td>8.50</td>\n</tr>\n<tr>\n<td>vgg19</td>\n<td>27.62</td>\n<td>9.12</td>\n</tr>\n<tr>\n<td>vgg19_bn</td>\n<td>25.76</td>\n<td>8.15</td>\n</tr></tbody></table>\n<p>Details about the models are below (for CIFAR10 dataset):</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"><em>Name</em></th>\n<th align=\"center\"><em># Params</em></th>\n<th align=\"center\"><em>Top-1 Acc.</em></th>\n<th align=\"center\"><em>Pretrained?</em></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\"><code>vgg11</code></td>\n<td align=\"center\">132.9M</td>\n<td align=\"center\">91.1</td>\n<td align=\"center\">\u221a</td>\n</tr>\n<tr>\n<td align=\"center\"><code>vgg13</code></td>\n<td align=\"center\">133M</td>\n<td align=\"center\">92.8</td>\n<td align=\"center\">\u221a</td>\n</tr>\n<tr>\n<td align=\"center\"><code>vgg16</code></td>\n<td align=\"center\">138.4M</td>\n<td align=\"center\">92.6</td>\n<td align=\"center\">\u221a</td>\n</tr>\n<tr>\n<td align=\"center\"><code>vgg19</code></td>\n<td align=\"center\">143.7M</td>\n<td align=\"center\">92.3</td>\n<td align=\"center\">\u221a</td>\n</tr>\n<tr>\n<td align=\"center\">-------------------</td>\n<td align=\"center\">----------</td>\n<td align=\"center\">------------</td>\n<td align=\"center\">-------------</td>\n</tr>\n<tr>\n<td align=\"center\"><code>vgg11_bn</code></td>\n<td align=\"center\">132.9M</td>\n<td align=\"center\">92.2</td>\n<td align=\"center\">\u221a</td>\n</tr>\n<tr>\n<td align=\"center\"><code>vgg13_bn</code></td>\n<td align=\"center\">133M</td>\n<td align=\"center\">94.2</td>\n<td align=\"center\">\u221a</td>\n</tr>\n<tr>\n<td align=\"center\"><code>vgg16_bn</code></td>\n<td align=\"center\">138.4M</td>\n<td align=\"center\">93.9</td>\n<td align=\"center\">\u221a</td>\n</tr>\n<tr>\n<td align=\"center\"><code>vgg19_bn</code></td>\n<td align=\"center\">143.7M</td>\n<td align=\"center\">93.7</td>\n<td align=\"center\">\u221a</td>\n</tr></tbody></table>\n<h4>Example: Classification</h4>\n<p>We assume that in your current directory, there is a <code>img.jpg</code> file and a <code>labels_map.txt</code> file (ImageNet class names). These are both included in <code>examples/simple</code>.</p>\n<p>All pre-trained models expect input images normalized in the same way,\ni.e. mini-batches of 3-channel RGB images of shape <code>(3 x H x W)</code>, where <code>H</code> and <code>W</code> are expected to be at least <code>224</code>.\nThe images have to be loaded in to a range of <code>[0, 1]</code> and then normalized using <code>mean = [0.485, 0.456, 0.406]</code>\nand <code>std = [0.229, 0.224, 0.225]</code>.</p>\n<p>Here's a sample execution.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">json</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision.transforms</span> <span class=\"k\">as</span> <span class=\"nn\">transforms</span>\n<span class=\"kn\">from</span> <span class=\"nn\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">vgg_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">VGG</span> \n\n<span class=\"c1\"># Open image</span>\n<span class=\"n\">input_image</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"s2\">\"img.jpg\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Preprocess image</span>\n<span class=\"n\">preprocess</span> <span class=\"o\">=</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">([</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Resize</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">CenterCrop</span><span class=\"p\">(</span><span class=\"mi\">224</span><span class=\"p\">),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">(),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Normalize</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.485</span><span class=\"p\">,</span> <span class=\"mf\">0.456</span><span class=\"p\">,</span> <span class=\"mf\">0.406</span><span class=\"p\">],</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.229</span><span class=\"p\">,</span> <span class=\"mf\">0.224</span><span class=\"p\">,</span> <span class=\"mf\">0.225</span><span class=\"p\">]),</span>\n<span class=\"p\">])</span>\n<span class=\"n\">input_tensor</span> <span class=\"o\">=</span> <span class=\"n\">preprocess</span><span class=\"p\">(</span><span class=\"n\">input_image</span><span class=\"p\">)</span>\n<span class=\"n\">input_batch</span> <span class=\"o\">=</span> <span class=\"n\">input_tensor</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>  <span class=\"c1\"># create a mini-batch as expected by the model</span>\n\n<span class=\"c1\"># Load class names</span>\n<span class=\"n\">labels_map</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">\"labels_map.txt\"</span><span class=\"p\">))</span>\n<span class=\"n\">labels_map</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">labels_map</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)]</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">)]</span>\n\n<span class=\"c1\"># Classify with VGG11</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">VGG</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s2\">\"vgg11\"</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># move the input and model to GPU for speed if available</span>\n<span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">is_available</span><span class=\"p\">():</span>\n    <span class=\"n\">input_batch</span> <span class=\"o\">=</span> <span class=\"n\">input_batch</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"s2\">\"cuda\"</span><span class=\"p\">)</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"s2\">\"cuda\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n    <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_batch</span><span class=\"p\">)</span>\n<span class=\"n\">preds</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">topk</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">indices</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"-----\"</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">idx</span> <span class=\"ow\">in</span> <span class=\"n\">preds</span><span class=\"p\">:</span>\n    <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"n\">labels_map</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">]</span>\n    <span class=\"n\">prob</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">idx</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"</span><span class=\"si\">{</span><span class=\"n\">label</span><span class=\"si\">:</span><span class=\"s2\">&lt;75</span><span class=\"si\">}</span><span class=\"s2\"> (</span><span class=\"si\">{</span><span class=\"n\">prob</span> <span class=\"o\">*</span> <span class=\"mi\">100</span><span class=\"si\">:</span><span class=\"s2\">.2f</span><span class=\"si\">}</span><span class=\"s2\">%)\"</span><span class=\"p\">)</span>\n</pre>\n<h4>Example: Feature Extraction</h4>\n<p>You can easily extract features with <code>model.extract_features</code>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">vgg_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">VGG</span> \n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">VGG</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'vgg11'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># ... image preprocessing as in the classification example ...</span>\n<span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># torch.Size([1, 3, 224, 224])</span>\n\n<span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">extract_features</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># torch.Size([1, 512, 7, 7])</span>\n</pre>\n<h4>Example: Export to ONNX</h4>\n<p>Exporting to ONNX for deploying to production is now simple:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span> \n<span class=\"kn\">from</span> <span class=\"nn\">vgg_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">VGG</span> \n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">VGG</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'vgg11'</span><span class=\"p\">)</span>\n<span class=\"n\">dummy_input</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">)</span>\n\n<span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">onnx</span><span class=\"o\">.</span><span class=\"n\">export</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">dummy_input</span><span class=\"p\">,</span> <span class=\"s2\">\"demo.onnx\"</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<h4>Example: Visual</h4>\n<pre>cd $REPO$/framework\nsh start.sh\n</pre>\n<p>Then open the browser and type in the browser address <a href=\"http://127.0.0.1:8000/\" rel=\"nofollow\">http://127.0.0.1:8000/</a>.</p>\n<p>Enjoy it.</p>\n<h4>ImageNet</h4>\n<p>See <code>examples/imagenet</code> for details about evaluating on ImageNet.</p>\n<p>For more datasets result. Please see <code>research/README.md</code>.</p>\n<h3>Contributing</h3>\n<p>If you find a bug, create a GitHub issue, or even better, submit a pull request. Similarly, if you have questions, simply post them as GitHub issues.</p>\n<p>I look forward to seeing what the community does with these models!</p>\n<h3>Credit</h3>\n<h4>Very Deep Convolutional Networks for Large-Scale Image Recognition</h4>\n<p><em>Karen Simonyan, Andrew Zisserman</em></p>\n<h5>Abstract</h5>\n<p>In this work we investigate the effect of the convolutional network depth on its accuracy in the\nlarge-scale image recognition setting. Our main contribution is a thorough evaluation of networks\nof increasing depth using an architecture with very small (3x3) convolution filters, which shows\nthat a significant improvement on the prior-art configurations can be achieved by pushing the depth\nto 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission,\nwhere our team secured the first and the second places in the localisation and classification tracks\nrespectively. We also show that our representations generalise well to other datasets, where they\nachieve state-of-the-art results. We have made our two best-performing ConvNet models publicly\navailable to facilitate further research on the use of deep visual representations in computer vision.</p>\n<p><a href=\"https://arxiv.org/abs/1409.1556\" rel=\"nofollow\">paper</a></p>\n<pre>@article{VGG,\ntitle:{Very Deep Convolutional Networks for Large-Scale Image Recognition},\nauthor:{Karen Simonyan, Andrew Zisserman},\njournal={iclr},\nyear={2015}\n}\n</pre>\n\n          </div>"}, "last_serial": 6818804, "releases": {"0.1.2": [{"comment_text": "", "digests": {"md5": "52ffe181e2a3187a364f1a3f9b2c7762", "sha256": "a3b890b80f31054e939d9a337709d113be5ff93b43e1f3b6506a855737a40548"}, "downloads": -1, "filename": "vgg_pytorch-0.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "52ffe181e2a3187a364f1a3f9b2c7762", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5.0", "size": 12842, "upload_time": "2020-02-17T05:57:31", "upload_time_iso_8601": "2020-02-17T05:57:31.868373Z", "url": "https://files.pythonhosted.org/packages/6f/4b/6169f5a5d6c8d9b17e04fa061e71cfc58f8bed31030d2c6b3263519d284a/vgg_pytorch-0.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d5c0c07c4d9ac3116b2c8f83492ccd0f", "sha256": "e5ef9a322e76a24e7618179db50f9ae47db146700803a212e6bdd09b55167922"}, "downloads": -1, "filename": "vgg_pytorch-0.1.2.tar.gz", "has_sig": false, "md5_digest": "d5c0c07c4d9ac3116b2c8f83492ccd0f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 9847, "upload_time": "2020-02-17T05:57:35", "upload_time_iso_8601": "2020-02-17T05:57:35.132885Z", "url": "https://files.pythonhosted.org/packages/0a/bb/ea1b9b958a633185ce600a75538677e465a56b6432a5b6fb60170caa8337/vgg_pytorch-0.1.2.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "69a6824929f5bd715acc5503fb52559c", "sha256": "58722670b055133ae7f247966465c84609f3e3e19c819a61ce66dbbe7a9b8e72"}, "downloads": -1, "filename": "vgg_pytorch-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "69a6824929f5bd715acc5503fb52559c", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 12704, "upload_time": "2020-03-16T05:45:12", "upload_time_iso_8601": "2020-03-16T05:45:12.855253Z", "url": "https://files.pythonhosted.org/packages/a1/b8/72c018f23120e21236bebce4998c51d4adc1601dcf2a0f4cdef7a392cebd/vgg_pytorch-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "88a39a2d63d4a24f0a4e7d178cce3f16", "sha256": "bc5b211f98b8bafeaa8eef681e16602955cae0337991df86549483786800e6ce"}, "downloads": -1, "filename": "vgg_pytorch-0.2.0.tar.gz", "has_sig": false, "md5_digest": "88a39a2d63d4a24f0a4e7d178cce3f16", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 8842, "upload_time": "2020-03-16T05:45:14", "upload_time_iso_8601": "2020-03-16T05:45:14.857194Z", "url": "https://files.pythonhosted.org/packages/67/1d/0a641a04a53639c35972ce9625b32383255d37d36003c4fd4f3d81f0f311/vgg_pytorch-0.2.0.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "e595ad44b2b2ab7d3bdce2f397ec2d63", "sha256": "9e6324cf8e40da47d5140686e73a92ebb109b91056751a2d8a812623265ece8d"}, "downloads": -1, "filename": "vgg_pytorch-0.3.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "e595ad44b2b2ab7d3bdce2f397ec2d63", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 12929, "upload_time": "2020-03-16T05:51:54", "upload_time_iso_8601": "2020-03-16T05:51:54.894069Z", "url": "https://files.pythonhosted.org/packages/e7/f2/451cfbe59d5817a1144ff850bc506ac37cbc7f6f25380c91839403b645e6/vgg_pytorch-0.3.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b0db42db20d336b3e72920bbae60f8ec", "sha256": "0b1bb873da4e2d5eb142638dd71b4c54ef0354a71034c57a4e7d0154943b2dda"}, "downloads": -1, "filename": "vgg_pytorch-0.3.0.tar.gz", "has_sig": false, "md5_digest": "b0db42db20d336b3e72920bbae60f8ec", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 9144, "upload_time": "2020-03-16T05:51:56", "upload_time_iso_8601": "2020-03-16T05:51:56.189798Z", "url": "https://files.pythonhosted.org/packages/93/c0/99d68f3ec789aef406df7d1ab4974cd9f271b12ca574ad1b8a14f4f98791/vgg_pytorch-0.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e595ad44b2b2ab7d3bdce2f397ec2d63", "sha256": "9e6324cf8e40da47d5140686e73a92ebb109b91056751a2d8a812623265ece8d"}, "downloads": -1, "filename": "vgg_pytorch-0.3.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "e595ad44b2b2ab7d3bdce2f397ec2d63", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 12929, "upload_time": "2020-03-16T05:51:54", "upload_time_iso_8601": "2020-03-16T05:51:54.894069Z", "url": "https://files.pythonhosted.org/packages/e7/f2/451cfbe59d5817a1144ff850bc506ac37cbc7f6f25380c91839403b645e6/vgg_pytorch-0.3.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b0db42db20d336b3e72920bbae60f8ec", "sha256": "0b1bb873da4e2d5eb142638dd71b4c54ef0354a71034c57a4e7d0154943b2dda"}, "downloads": -1, "filename": "vgg_pytorch-0.3.0.tar.gz", "has_sig": false, "md5_digest": "b0db42db20d336b3e72920bbae60f8ec", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 9144, "upload_time": "2020-03-16T05:51:56", "upload_time_iso_8601": "2020-03-16T05:51:56.189798Z", "url": "https://files.pythonhosted.org/packages/93/c0/99d68f3ec789aef406df7d1ab4974cd9f271b12ca574ad1b8a14f4f98791/vgg_pytorch-0.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:36:24 2020"}