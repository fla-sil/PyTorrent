{"info": {"author": "R. Leonhardt, R. Bailey, M. Miklavec, J. Fee, H. Schovanec", "author_email": "roman.leonhardt@zamg.ac.at", "bugtrack_url": null, "classifiers": [], "description": "MagPy\n=====\n\n**MagPy (or GeomagPy) is a Python package for analysing and displaying\ngeomagnetic data.**\n\nVersion Info: (please note: this package is still in a development state\nwith frequent modifcations) please check the release notes.\n\nMagPy provides tools for geomagnetic data analysis with special focus on\ntypical data processing routines in observatories. MagPy provides\nmethods for data format conversion, plotting and mathematical procedures\nwith specifically geomagnetic analysis routines such as basevalue and\nbaseline calculation and database handling. Among the supported data\nformats are *ImagCDF, IAGA-02, WDC, IMF, IAF, BLV*, and many more. Full\ninstallation also provides a graphical user interface, *xmagpy*. You\nwill find a complete manual for *xmagpy* in the docs.\n\nTypical usage of the basic MagPy package for reading and visualising\ndata looks like this:\n\n::\n\n        #!/usr/bin/env python\n\n        from magpy.stream import read\n        import magpy.mpplot as mp\n        stream = read('filename_or_url')\n        mp.plot(stream)\n\nBelow you will find a quick guide to usage of the basic MagPy package.\nFor instructions on *xmagpy* please refer to the document \"`An\nintroduction to\nXMagPy <https://github.com/geomagpy/magpy/blob/master/magpy/doc/xmagpy-manual.pdf>`__\"\nin the docs. You can also subscribe to our information channel at\n`Telegram <https://t.me/geomagpy>`__ for further information on updates\nand current issues.\n\n1. INSTALLATION\n---------------\n\nPleas note that with the publication of MagPy 1.0 the recommended python\nenironment is >= 3.6. The following installation instructions will\nassume such an environment. Particularly if you are using Python2.7\nplease go to the end of this sections for help.\n\nThis section is currently updated and will be ready with the publication\nof MagPy 1.0.\n\n1.1 Linux installation (Ubuntu,Debian)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n1.1.1 Complete Install\n^^^^^^^^^^^^^^^^^^^^^^\n\nTested for Ubuntu 18.04 and Debian Stretch (full installation with all\noptional packages). Please note that installation requires python 3.x.\n\n::\n\n        $ sudo pip3 install geomagpy    #Will install MagPy and all dependencies\n        $ sudo pip3 install wxpython    #Will install WX graphics system for XMagPy\n\nYou can now run XMagPy by using the following command\n\n::\n\n        $ xmagpy\n\n1.1.2 Updates\n^^^^^^^^^^^^^\n\nTo upgrade to the most recent version:\n\n::\n\n        $ sudo pip3 install -U geomagpy\n\n1.1.3 Creating a desktop link\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn order to create a desktop link on linux systems please refer to\ninstruction too be found your distribution. For Ubunutu and other Debian\nsystems such links are created as follows:\n\nFirstly create a file \"xmagpy.desktop\" which contains:\n\n::\n\n        [Desktop Entry]\n        Type=Application\n        Name=XMagPy\n        GenericName=GeoMagPy User Interface\n        Exec=xmagpy\n        Icon=/usr/local/lib/python3.7/dist-packages/magpy/gui/magpy128.xpm\n        Terminal=false\n        Categories=Application;Development;\n\nThen copy this file to the systems application folder:\n\n::\n\n        sudo cp xmagpy.desktop /usr/share/applications/\n\n1.2 MacOs installation\n~~~~~~~~~~~~~~~~~~~~~~\n\n1.2.1 Install a python3 interpreter\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n-  we recommend\n   `Miniconda <https://docs.conda.io/en/latest/miniconda.html>`__ or\n   `Anaconda <https://www.continuum.io/downloads>`__\n-  see e.g. https://docs.continuum.io/anaconda/install for more details\n-  before continuiung, test whether python is working. Open a terminal\n   and run python\n\n1.2.2 Install MagPy\n^^^^^^^^^^^^^^^^^^^\n\nOpen a terminal and use the following commands:\n\n::\n\n        $ pip install geomagpy    #Will install MagPy and all dependencies\n        $ pip install wxpython    #Will install WX graphics system for XMagPy\n\nYou can now run XMagPy from the terminal by using the following command\n\n::\n\n        $ xmagpyw\n\n1.2.3 Creating a desktop link\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nOpen Finder and search for xmagpyw. Copy it to the desktop. To change\nthe icon, click on the xmagpyw link, open information and replace the\nimage on the upper left with e.g. magpy128.jpg (also to be found using\nfinder).\n\n1.3 Windows installation - WinPython Package\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n1.3.1 Install MagPy for Windows\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n-  get the `MagPy Windows\n   installer <https://cobs.zamg.ac.at/data/index.php/en/downloads/category/1-magnetism>`__\n   here (under Downloads): https://cobs.zamg.ac.at\n-  download and execute magpy-x.x.x.exe\n-  all required packages are included in the installer\n\n1.3.2 Post-installation information\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n-  MagPy will have a sub-folder in the Start menu. Here you will find\n   three items:\n\n   ::\n\n       * command -> opens a DOS shell within the Python environment e.g. for updates \n       * python  -> opens a python shell ready for MagPy\n       * xmagpy  -> opens the MagPy graphical user interface\n\n1.3.3 Update an existing MagPy installation on Windows\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n-  right-click on subfolder \"command\" in the start menu\n-  select \"run as administrator\"\n-  issue the following command \"pip install -U geomagpy\" (you can also\n   specify the version e.g. pip install geomagpy==0.x.x)\n\n1.3.4 Installation with user priviledges only\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n-  Download a most recent version of WinPython3.x\n-  Unpack in your home directory\n-  Go to the WinPython Folder and run WinPython command prompt\n-  issue the same commands as for MacOS installation\n-  to run XMagPy: use xmagpy from the WinPython command promt.\n\n1.4 Installation instructions for Python 2.7\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe current version of magpy is still supporting python 2.7, although it\nis highly recommended to switch to python >= 3.6. Installation on python\n2.7 is more complex, as some packages for graphical user interface and\nCDF support not as well supported. Please note: None of the addtional\nsteps is necessary for python 3.x.\n\n1.4.1 Pre-installation work\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nGet a recent version of NasaCDF for your platform, enables CDF support\nfor formats like ImagCDF. Package details and files can be found at\nhttp://cdf.gsfc.nasa.gov/\n\nOn Linux such installation will look like\n(http://cdf.gsfc.nasa.gov/html/sw\\_and\\_docs.html)\n\n::\n\n        $ tar -zxvf cdf37_0-dist-all.tar.gz\n        $ cd cdf37...\n        $ make OS=linux ENV=gnu CURSES=yes FORTRAN=no UCOPTIONS=-O2 SHARED=yes all\n        $ sudo make INSTALLDIR=/usr/local/cdf install\n\nInstall the following additional compilers before continuing (required\nfor spacepy): Linux: install gcc MacOs: install gcc and gfortran\n\nInstall coordinate system transformation support:\n\n::\n\n        $ sudo apt-get install libproj-dev proj-data proj-bin\n\n1.4.2 Install MagPy and dependencies\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nOn Linux this will look like:\n\n::\n\n        $ sudo apt-get install python-matplotlib python-scipy python-h5py cython python-pip  \n        $ sudo apt-get install python-wxgtk3.0 # or python-wxgtk2.8 (Debian Stretch)  \n        $ sudo apt-get install python-twisted  \n        $ sudo pip install ffnet\n        $ sudo pip install pyproj==1.9.5\n        $ sudo pip install pyserial\n        $ sudo pip install service_identity\n        $ sudo pip install ownet\n        $ sudo pip install spacepy\n        $ sudo pip install geomagpy  \n\nOn Mac and Windows you need to download a python interpreter like\n`Anaconda <https://www.continuum.io/downloads>`__ or [WinPython] and\nthen install similar packages, particluarly the old wxpython 3.x.\n\n1.5 Platform independent container - Docker\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n1.5.1 Install `Docker <https://www.docker.com/>`__ (toolbox) on your operating system\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n     - https://docs.docker.com/engine/installation/\n\n1.5.2 Get the MagPy Image\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n     - open a docker shell\n\n            >>> docker pull geomagpy/magpy:latest\n            >>> docker run -d --name magpy -p 8000:8000 geomagpy/magpy:latest\n\n1.5.3 Open a browser\n^^^^^^^^^^^^^^^^^^^^\n\n::\n\n     - open address http://localhost:8000 (or http://\"IP of your VM\":8000)\n     - NEW: first time access might require a token or passwd\n\n            >>> docker logs magpy\n\n          will show the token \n     - run python shell (not conda) \n     - in python shell\n\n            >>> %matplotlib inline\n            >>> from magpy.stream import read\n            >>> ...\n\n1.6 Install from source\n~~~~~~~~~~~~~~~~~~~~~~~\n\nRequirements: - Python 2.7, 3.x (recommended is >=3.6)\n\nRecommended: - Python packages: \\* wxpython (for python2.7 it needs to\nbe 3.x or older) \\* NasaCDF (python 2.7 only) \\* SpacePy (python 2.7\nonly)\n\n-  Other useful Software:\n\n   -  pyproj (for geographic coordinate systems)\n   -  MySQL (database features)\n   -  Webserver (e.g. Apache2, PHP)\n\n      git clone git://github.com/GeomagPy/MagPy.git cd magpy\\* sudo\n      python setup.py install\n\n2. A quick guide to MagPy\n-------------------------\n\nwritten by R. Leonhardt, R. Bailey (April 2017)\n\nMagPy's functionality can be accessed basically in three different ways:\n1) Directly import and use the magpy package into a python environment\n2) Run the graphical user interface xmagpy (xmagpyw for Mac) 3) Use\npredefined applications \"Scripts\"\n\nThe following section will primarily deal with way 1. For 2 - xmagpy -\nwe refer to the video tutorials whcih can be found here: Section 3\ncontains examples for predefined applications/scripts\n\n2.1 Getting started with the python package\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nStart python. Import all stream methods and classes using:\n\n::\n\n    from magpy.stream import *\n\nPlease note that this import will shadow any already existing ``read``\nmethod.\n\n2.2 Reading and writing data\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nMagPy supports the following data formats and thus conversions between\nthem: - WDC: World Data Centre format - JSON: JavaScript Object Notation\n- IMF: Intermagnet Format - IAF: Intermagnet Archive Format - NEIC: WGET\ndata from USGS - NEIC - IAGA: IAGA 2002 text format - IMAGCDF:\nIntermagnet CDF Format - GFZKP: GeoForschungsZentrum KP-Index format -\nGSM19/GSM90: Output formats from GSM magnetometers - POS1: POS-1 binary\noutput - BLV: Baseline format Intermagnet - IYFV: Yearly mean format\nIntermagnet\n\n... and many others. To get a full list, use:\n\n::\n\n        from magpy.stream import *\n        print(PYMAG_SUPPORTED_FORMATS)\n\nYou will find several example files provided with MagPy. The ``cdf``\nfile is stored along with meta information in NASA's common data format\n(cdf). Reading this file requires a working installation of Spacepy cdf.\n\nIf you do not have any geomagnetic data file you can access example data\nby using the following command (after ``import *``):\n\n::\n\n        data = read(example1)\n        \n\nThe data from ``example1`` has been read into a MagPy *DataStream* (or\n*stream*) object. Most data processing routines in MagPy are applied to\ndata streams.\n\nSeveral example data sets are provided within the MagPy package:\n\n-  ``example1``: `IAGA <http://www.iaga-aiga.org/>`__ ZIP (IAGA2002, zip\n   compressed) file with 1 second HEZ data\n-  ``example2``: `MagPy <#magpy>`__ Archive (CDF) file with 1 sec F data\n-  ``example3``: `MagPy <#magpy>`__ Basevalue (TXT) ascii file with DI\n   and baseline data\n-  ``example4``: `INTERMAGNET <http://www.intermagnet.org>`__ ImagCDF\n   (CDF) file with one week of 1 second data\n-  ``example5``: `MagPy <#magpy>`__ Archive (CDF) raw data file with xyz\n   and supporting data\n-  ``example6a``: `MagPy <#magpy>`__ DI (txt) raw data file with DI\n   measurement\n-  ``example6b``: `MagPy <#magpy>`__ like 6a to be used with example4\n\n-  ``flagging_example``: `MagPy <#magpy>`__ FlagDictionary (JSON)\n   flagging info to be used with example1\n-  ``recipe1_flags``: `MagPy <#magpy>`__ FlagDictionary (JSON) to be\n   used with cookbook recipe 1\n\n2.2.1 Reading\n^^^^^^^^^^^^^\n\nFor a file in the same directory:\n\n::\n\n        data = read(r'myfile.min') \n\n... or for specific paths in Linux:\n\n::\n\n        data = read(r'/path/to/file/myfile.min') \n\n... or for specific paths in Windows:\n\n::\n\n        data = read(r'c:\\path\\to\\file\\myfile.min')\n\nPathnames are related to your operating system. In this guide we will\nassume a Linux system. Files that are read in are uploaded to the memory\nand each data column (or piece of header information) is assigned to an\ninternal variable (key). To get a quick overview of the assigned keys in\nany given stream (``data``) you can use the following method:\n\n::\n\n        print(data._get_key_headers() )\n\n2.2.2 Writing\n^^^^^^^^^^^^^\n\nAfter loading data from a file, we can save the data in the standard\nIAGA02 and IMAGCDF formats with the following commands.\n\nTo create an IAGA-02 format file, use:\n\n::\n\n        data.write(r'/path/to/diretory/',format_type='IAGA')\n\nTo create an `INTERMAGNET <http://www.intermagnet.org>`__ CDF (ImagCDF)\nfile:\n\n::\n\n        data.write(r'/path/to/diretory/',format_type='IMAGCDF')\n\nThe filename will be created automatically according to the defined\nformat. By default, daily files are created and the date is added to the\nfilename in-between the optional parameters ``filenamebegins`` and\n``filenameends``. If ``filenameends`` is missing, ``.txt`` is used as\ndefault.\n\n2.2.3 Other possibilities for reading files\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTo read all local files ending with .min within a directory (creates a\nsingle stream of all data):\n\n::\n\n        data = read(r'/path/to/file/*.min')\n\nGetting magnetic data directly from an online source such as the WDC:\n\n::\n\n        data = read(r'ftp://thewellknownaddress/single_year/2011/fur2011.wdc')\n\nGetting *kp* data from the GFZ Potsdam:\n\n::\n\n        data = read(r'http://www-app3.gfz-potsdam.de/kp_index/qlyymm.tab')\n\n(Please note: data access and usage is subjected to the terms and\nconditions of the individual data provider. Please make sure to read\nthem before accessing any of these products.)\n\nNo format specifications are required for reading. If MagPy can handle\nthe format, it will be automatically recognized.\n\nGetting data for a specific time window for local files:\n\n::\n\n        data = read(r'/path/to/files/*.min',starttime=\"2014-01-01\", endtime=\"2014-05-01\")\n\n... and remote files:\n\n::\n\n        data = read(r'ftp://address/fur2013.wdc',starttime=\"2013-01-01\", endtime=\"2013-02-01\")\n\nReading data from the INTERMAGNET Webservice (starting soon):\n\n::\n\n        data = read('http://www.intermagnet.org/test/ws/?id=WIC')\n\n2.2.4 Selecting timerange\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe stream can be trimmed to a specific time interval after reading by\napplying the trim method, e.g. for a specific month:\n\n::\n\n        data = data.trim(starttime=\"2013-01-01\", endtime=\"2013-02-01\")\n\n2.3 Getting help on options and usage\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n2.3.1 Python's help function\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nInformation on individual methods and options can be obtained as\nfollows:\n\nFor basic functions:\n\n::\n\n        help(read)\n\nFor specific methods related to e.g. a stream object \"data\":\n\n::\n\n        help(data.fit)\n\nNote that this requires the existence of a \"data\" object, which is\nobtained e.g. by data = read(...). The help text can also be shown by\ndirectly calling the *DataStream* object method using:\n\n::\n\n        help(DataStream.fit)\n\n2.3.2 MagPy's logging system\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nMagPy automatically logs many function options and runtime information,\nwhich can be useful for debugging purposes. This log is saved by default\nin the temporary file directory of your operating system, e.g. for Linux\nthis would be ``/tmp/magpy.log``. The log is formatted as follows with\nthe date, module and function in use and the message leve\n(INFO/WARNING/ERROR):\n\n::\n\n        2017-04-22 09:50:11,308 INFO - magpy.stream - Initiating MagPy...\n\nMessages on the WARNING and ERROR level will automatically be printed to\nshell. Messages for more detailed debugging are written at the DEBUG\nlevel and will not be printed to the log unless an additional handler\nfor printing DEBUG is added.\n\nCustom loggers can be defined by creating a logger object after\nimporting MagPy and adding handlers (with formatting):\n\n::\n\n        from magpy.stream import *\n        import logging\n        \n        logger = logging.getLogger()\n        hdlr = logging.FileHandler('testlog.log')\n        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n        hdlr.setFormatter(formatter)\n        logger.addHandler(hdlr)\n        \n\nThe logger can also be configured to print to shell (stdout, without\nformatting):\n\n::\n\n        import sys\n        logger = logging.getLogger()\n        stdoutlog = logging.StreamHandler(sys.stdout)\n        logger.addHandler(stdoutlog)\n\n2.4 Plotting\n~~~~~~~~~~~~\n\nYou will find some example plots at the `Conrad\nObservatory <http://www.conrad-observatory.at>`__.\n\n2.4.1 Quick (and not dirty)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n        import magpy.mpplot as mp\n        mp.plot(data)\n\n2.4.2 Some options\n^^^^^^^^^^^^^^^^^^\n\nSelect specific keys to plot:\n\n::\n\n        mp.plot(data,variables=['x','y','z'])\n        \n\nDefining a plot title and specific colors (see ``help(mp.plot)`` for\nlist and all options):\n\n::\n\n        mp.plot(data,variables=['x','y'],plottitle=\"Test plot\",\n                colorlist=['g', 'c'])\n\n2.4.3 Data from multiple streams\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nVarious datasets from multiple data streams will be plotted above one\nanother. Provide a list of streams and an array of keys:\n\n::\n\n        mp.plotStreams([data1,data2],[['x','y','z'],['f']])\n\n2.5 Flagging data\n~~~~~~~~~~~~~~~~~\n\nThe flagging procedure allows the observer to mark specific data points\nor ranges. Falgs are useful for labelling data spikes, storm onsets,\npulsations, disturbances, lightning strikes, etc. Each flag is asociated\nwith a comment and a type number. The flagtype number ranges between 0\nand 4:\n\n-  0: normal data with comment (e.g. \"Hello World\")\n-  1: data marked by automated analysis (e.g. spike)\n-  2: data marked by observer as valid geomagnetic signature (e.g. storm\n   onset, pulsation). Such data cannot be marked invalid by automated\n   procedures\n-  3: data marked by observer as invalid (e.g. lightning, magnetic\n   disturbance)\n-  4: merged data (e.g. data inserted from another source/instrument as\n   defined in the comment)\n\nFlags can be stored along with the data set (requires CDF format output)\nor separately in a binary archive. These flags can then be applied to\nthe raw data again, ascertaining perfect reproducibility.\n\n2.5.1 Mark data spikes\n^^^^^^^^^^^^^^^^^^^^^^\n\nLoad a data record with data spikes:\n\n::\n\n        datawithspikes = read(example1)\n\nMark all spikes using the automated function ``flag_outlier`` with\ndefault options:\n\n::\n\n        flaggeddata = datawithspikes.flag_outlier(timerange=timedelta(minutes=1),threshold=3)\n\nShow flagged data in a plot:\n\n::\n\n        mp.plot(flaggeddata,['f'],annotate=True)\n\n2.5.2 Flag time range\n^^^^^^^^^^^^^^^^^^^^^\n\nFlag a certain time range:\n\n::\n\n        flaglist = flaggeddata.flag_range(keys=['f'], starttime='2012-08-02T04:33:40', \n                                          endtime='2012-08-02T04:44:10', \n                                          flagnum=3, text=\"iron metal near sensor\")\n\nApply these flags to the data:\n\n::\n\n        flaggeddata = flaggeddata.flag(flaglist)\n\nShow flagged data in a plot:\n\n::\n\n        mp.plot(flaggeddata,['f'],annotate=True)\n\n2.5.3 Save flagged data\n^^^^^^^^^^^^^^^^^^^^^^^\n\nTo save the data together with the list of flags to a CDF file:\n\n::\n\n        flaggeddata.write('/tmp/',filenamebegins='MyFlaggedExample_', format_type='PYCDF')\n\nTo check for correct save procedure, read and plot the new file:\n\n::\n\n        newdata = read(\"/tmp/MyFlaggedExample_*\")\n        mp.plot(newdata,annotate=True, plottitle='Reloaded flagged CDF data')\n\n2.5.4 Save flags separately\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTo save the list of flags seperately from the data in a pickled binary\nfile:\n\n::\n\n        fullflaglist = flaggeddata.extractflags()\n        saveflags(fullflaglist,\"/tmp/MyFlagList.pkl\"))\n\nThese flags can be loaded in and then reapplied to the data set:\n\n::\n\n        data = read(example1)\n        flaglist = loadflags(\"/tmp/MyFlagList.pkl\")\n        data = data.flag(flaglist)\n        mp.plot(data,annotate=True, plottitle='Raw data with flags from file')\n\n2.5.5 Drop flagged data\n^^^^^^^^^^^^^^^^^^^^^^^\n\nFor some analyses it is necessary to use \"clean\" data, which can be\nproduced by dropping data flagged as invalid (e.g. spikes). By default,\nthe following method removes all data marked with flagtype numbers 1 and\n3.\n\n::\n\n        cleandata = flaggeddata.remove_flagged()\n        mp.plot(cleandata, ['f'], plottitle='Flagged data dropped')\n\n2.6 Basic methods\n~~~~~~~~~~~~~~~~~\n\n2.6.1 Filtering\n^^^^^^^^^^^^^^^\n\nMagPy's ``filter`` uses the settings recommended by\n`IAGA <http://www.iaga-aiga.org/>`__/`INTERMAGNET <http://www.intermagnet.org>`__.\nCkeck ``help(data.filter)`` for further options and definitions of\nfilter types and pass bands.\n\nFirst, get the sampling rate before filtering in seconds:\n\n::\n\n        print(\"Sampling rate before [sec]:\", cleandata.samplingrate())\n\nFilter the data set with default parameters (``filter`` automatically\nchooses the correct settings depending on the provided sanmpling rate):\n\n::\n\n        filtereddata = cleandata.filter()\n\nGet sampling rate and filtered data after filtering (please note that\nall filter information is added to the data's meta information\ndictionary (data.header):\n\n::\n\n        print(\"Sampling rate after [sec]:\", filtereddata.samplingrate())\n        print(\"Filter and pass band:\", filtereddata.header.get('DataSamplingFilter',''))\n\n2.6.2 Coordinate transformation\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAssuming vector data in columns [x,y,z] you can freely convert between\nxyz, hdz, and idf coordinates:\n\n::\n\n        cleandata = cleandata.xyz2hdz()\n\n2.6.3 Calculate delta F\n^^^^^^^^^^^^^^^^^^^^^^^\n\nIf the data file contains xyz (hdz, idf) data and an independently\nmeasured f value, you can calculate delta F between the two instruments\nusing the following:\n\n::\n\n        cleandata = cleandata.delta_f()\n        mp.plot(cleandata,plottitle='delta F')\n\n2.6.4 Calculate Means\n^^^^^^^^^^^^^^^^^^^^^\n\nMean values for certain data columns can be obtained using the ``mean``\nmethod. The mean will only be calculated for data with the percentage of\nvalid data (in contrast to missing data) points not falling below the\nvalue given by the percentage option (default 95). If too much data is\nmissing, then no mean is calulated and the function returns NaN.\n\n::\n\n        print(cleandata.mean('df', percentage=80))\n        \n\nThe median can be calculated by defining the ``meanfunction`` option:\n\n::\n\n        print(cleandata.mean('df', meanfunction='median'))\n\n2.6.5 Applying offsets\n^^^^^^^^^^^^^^^^^^^^^^\n\nConstant offsets can be added to individual columns using the ``offset``\nmethod with a dictionary defining the MagPy stream column keys and the\noffset to be applied (datetime.timedelta object for time column, float\nfor all others):\n\n::\n\n        offsetdata = cleandata.offset({'time':timedelta(seconds=0.19),'f':1.24})\n\n2.6.6 Scaling data\n^^^^^^^^^^^^^^^^^^\n\nIndividual columns can also be multiplied by values provided in a\ndictionary:\n\n::\n\n        multdata = cleandata.multiply({'x':-1})\n\n2.6.7 Fit functions\n^^^^^^^^^^^^^^^^^^^\n\nMagPy offers the possibility to fit functions to data using either\npolynomial functions or cubic splines (default):\n\n::\n\n        func = cleandata.fit(keys=['x','y','z'],knotstep=0.1)\n        mp.plot(cleandata,variables=['x','y','z'],function=func)\n\n2.6.8 Derivatives\n^^^^^^^^^^^^^^^^^\n\nTime derivatives, which are useful to identify outliers and sharp\nchanges, are calculated as follows:\n\n::\n\n        diffdata = cleandata.differentiate(keys=['x','y','z'],put2keys = ['dx','dy','dz'])\n        mp.plot(diffdata,variables=['dx','dy','dz'])\n\n2.6.9 All methods at a glance\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nFor a summary of all supported methods, see the section **List of all\nMagPy methods** below.\n\n2.7 Geomagnetic analysis\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n2.7.1 Determination of K indices\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nMagPy supports the FMI method for determination of K indices. Please\nconsult the MagPy publication for details on this method and\napplication.\n\nA month of one minute data is provided in ``example2``, which\ncorresponds to an `INTERMAGNET <http://www.intermagnet.org>`__ IAF\narchive file. Reading a file in this format will load one minute data by\ndefault. Accessing hourly data and other information is described below.\n\n::\n\n        data2 = read(example2)\n        kvals = data2.k_fmi()\n\nThe determination of K values will take some time as the filtering\nwindow is dynamically adjusted. In order to plot the original data (H\ncomponent) and K values together, we now use the multiple stream\nplotting method ``plotStreams``. Here you need to provide a list of\nstreams and an array containing variables for each stream. The\nadditional options determine the appearance of the plot (limits, bar\nchart):\n\n::\n\n        mp.plotStreams([data2,kvals],[['x'],['var1']],\n                       specialdict = [{},{'var1':[0,9]}],\n                       symbollist=['-','z'],\n                       bartrange=0.06)\n        \n\n``'z'`` in ``symbollist`` refers to the second subplot (K), which should\nbe plotted as bars rather than the standard line (``'-'``).\n\n2.7.2 Automated geomagnetic storm detection\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nGeomagnetic storm detection is supported by MagPy using two procedures\nbased on wavelets and the Akaike Information Criterion (AIC) as outlined\nin detail in Bailey and Leonhardt (2016). A basic example of usage to\nfind an SSC using a Discrete Wavelet Transform (DWT) is shown below:\n\n::\n\n        from magpy.stream import read\n        from magpy.opt.stormdet import seekStorm\n        stormdata = read(\"LEMI025_2015-03-17.cdf\")      # 1s variometer data\n        stormdata = stormdata.xyz2hdz()\n        stormdata = stormdata.smooth('x', window_len=25)\n        detection, ssc_list = seekStorm(stormdata, method=\"MODWT\")\n        print(\"Possible SSCs detected:\", ssc_list)\n        \n\nThe method ``seekStorm`` will return two variables: ``detection`` is\nTrue if any detection was made, while ``ssc_list`` is a list of\ndictionaries containing data on each detection. Note that this method\nalone can return a long list of possible SSCs (most incorrectly\ndetected), particularly during active storm times. It is most useful\nwhen additional restrictions based on satellite solar wind data apply\n(currently only optimised for ACE data, e.g. from the NOAA website):\n\n::\n\n        satdata_ace_1m = read('20150317_ace_swepam_1m.txt')\n        satdata_ace_5m = read('20150317_ace_epam_5m.txt')\n        detection, ssc_list, sat_cme_list = seekStorm(stormdata,\n                    satdata_1m=satdata_ace_1m, satdata_5m=satdata_ace_5m,\n                    method='MODWT', returnsat=True)\n        print(\"Possible CMEs detected:\", sat_cme_list)\n        print(\"Possible SSCs detected:\", ssc_list)\n\n2.7.3 Sq analysis\n^^^^^^^^^^^^^^^^^\n\nMethods are currently in preparation.\n\n2.7.4 Validity check of data\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nA common and important application used in the geomagnetism community is\na general validity check of geomagnetic data to be submitted to the\nofficial data repositories `IAGA <http://www.iaga-aiga.org/>`__, WDC, or\n`INTERMAGNET <http://www.intermagnet.org>`__. Please note: this is\ncurrently under development and will be extended in the near future. A\n'one-click' test method will be included in xmagpy in the future,\nchecking:\n\nA) Validity of data formats, e.g.:\n\n   ::\n\n       data = read('myiaffile.bin', debug=True) \n\nB) Completeness of meta-information\n\nC) Conformity of applied techniques to respective rules\n\nD) Internal consistency of data\n\nE) Optional: regional consistency\n\n2.7.5 Spectral Analysis and Noise\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nFor analysis of the spectral content of data, MagPy provides two basic\nplotting methods. ``plotPS`` will calculate and display a power spectrum\nof the selected component. ``plotSpectrogram`` will plot a spectrogram\nof the time series. As usual, there are many options for plot window and\nprocessing parameters that can be accessed using the help method.\n\n::\n\n        data = read(example1)\n        mp.plotPS(data,key='f')\n        mp.plotSpectrogram(data,['f'])\n\n2.8 Handling multiple streams\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n2.8.1 Merging streams\n^^^^^^^^^^^^^^^^^^^^^\n\nMerging data comprises combining two streams into one new stream. This\nincludes adding a new column from another stream, filling gaps with data\nfrom another stream or replacing data from one column with data from\nanother stream. The following example sketches the typical usage:\n\n::\n\n        print(\"Data columns in data2:\", data2._get_key_headers())\n        newstream = mergeStreams(data2,kvals,keys=['var1'])\n        print(\"Data columns after merging:\", data2._get_key_headers())\n        mp.plot(newstream, ['x','y','z','var1'],symbollist=['-','-','-','z'])\n\nIf column ``var1`` does not existing in data2 (as above), then this\ncolumn is added. If column ``var1`` had already existed, then missing\ndata would be inserted from stream ``kvals``. In order to replace any\nexisting data, use option ``mode='replace'``.\n\n2.8.2 Differences between streams\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSometimes it is necessary to examine the differences between two data\nstreams e.g. differences between the F values of two instruments running\nin parallel at an observatory. The method ``subtractStreams`` is\nprovided for this analysis:\n\n::\n\n        diff = subtractStreams(data1,data2,keys=['f'])\n\n2.9 The art of meta-information\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nEach data set is accompanied by a dictionary containing meta-information\nfor this data. This dictionary is completely dynamic and can be filled\nfreely, but there are a number of predefined fields that help the user\nprovide essential meta-information as requested by\n`IAGA <http://www.iaga-aiga.org/>`__,\n`INTERMAGNET <http://www.intermagnet.org>`__ and other data providers.\nAll meta information is saved only to MagPy-specific archive formats\nPYCDF and PYSTR. All other export formats save only specific information\nas required by the projected format.\n\nThe current content of this dictionary can be accessed by:\n\n::\n\n        data = read(example1)\n        print(data.header)\n\nInformation is added/changed by using:\n\n::\n\n        data.header['SensorName'] = 'FGE'\n\nIndividual information is obtained from the dictionary using standard\nkey input:\n\n::\n\n        print(data.header.get('SensorName'))\n\nIf you want to have a more readable list of the header information, do:\n\n::\n\n        for key in data.header:\n            print (\"Key: {} \\t Content: {}\".format(key,data.header.get(key)))\n\n2.9.1 Conversion to ImagCDF - Adding meta-information\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTo convert data from `IAGA <http://www.iaga-aiga.org/>`__ or IAF formats\nto the new `INTERMAGNET <http://www.intermagnet.org>`__ CDF format, you\nwill usually need to add additional meta-information required for the\nnew format. MagPy can assist you here, firstly by extracting and\ncorrectly adding already existing meta-information into newly defined\nfields, and secondly by informing you of which information needs to be\nadded for producing the correct output format.\n\nExample of IAGA02 to ImagCDF:\n\n::\n\n        mydata = read('IAGA02-file.min')\n        mydata.write('/tmp',format_type='IMAGCDF')\n\nThe console output of the write command (see below) will tell you which\ninformation needs to be added (and how) in order to obtain correct\nImagCDF files. Please note, MagPy will store the data in any case and\nwill be able to read it again even if information is missing. Before\nsubmitting to a GIN, you need to make sure that the appropriate\ninformation is contained. Attributes that relate to publication of the\ndata will not be checked at this point, and might be included later.\n\n::\n\n        >>>Writing IMAGCDF Format /tmp/wic_20150828_0000_PT1M_4.cdf\n        >>>writeIMAGCDF: StandardLevel not defined - please specify by yourdata.header['DataStandardLevel'] = ['None','Partial','Full']\n        >>>writeIMAGCDF: Found F column\n        >>>writeIMAGCDF: given components are XYZF. Checking F column...\n        >>>writeIMAGCDF: analyzed F column - values are apparently independend from vector components - using column name 'S'\n\nNow add the missing information. Selecting 'Partial' will require\nadditional information. You will get a 'reminder' if you forget this.\nPlease check IMAGCDF instructions on specific codes:\n\n::\n\n        mydata.header['DataStandardLevel'] = 'Partial'\n        mydata.header['DataPartialStandDesc'] = 'IMOS-01,IMOS-02,IMOS-03,IMOS-04,IMOS-05,IMOS-06,IMOS-11,IMOS-12,IMOS-13,IMOS-14,IMOS-15,IMOS-21,IMOS-22,IMOS-31,IMOS-41'\n\nSimilar reminders to fill out complete header information will be shown\nfor other conversions like:\n\n::\n\n        mydata.write('/tmp',format_type='IAGA')\n        mydata.write('/tmp',format_type='IMF')\n        mydata.write('/tmp',format_type='IAF',coverage='month')\n        mydata.write('/tmp',format_type='WDC')\n\n2.9.2 Providing location data\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nProviding location data usually requires information on the reference\nsystem (ellipsoid,...). By default MagPy assumes that these values are\nprovided in WGS84/WGS84 reference system. In order to facilitate most\neasy referencing and conversions, MagPy supports\n`EPSG <https://www.epsg-registry.org/>`__ codes for coordinates. If you\nprovide the geodetic references as follows, and provided that the\n`proj4 <https://github.com/OSGeo/proj.4>`__ Python package is available,\nMagPy will automatically convert location data to the requested output\nformat (currently WGS84).\n\n::\n\n        mydata.header['DataAcquisitionLongitude'] = -34949.9\n        mydata.header['DataAcquisitionLatitude'] = 310087.0\n        mydata.header['DataLocationReference'] = 'GK M34, EPSG: 31253'\n\n        >>>...\n        >>>writeIMAGCDF: converting coordinates to epsg 4326\n        >>>...\n\n2.9.3 Special meta-information fields\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe meta-information fields can hold much more information than required\nby most output formats. This includes basevalue and baseline parameters,\nflagging details, detailed sensor information, serial numbers and much\nmore. MagPy makes use of these possibilities. In order to save this\nmeta-information along with your data set you can use MagPy internal\narchiving format, ``PYCDF``, which can later be converted to any of the\naforementioned output formats. You can even reconstruct a full data\nbase. Any upcoming meta-information or output request can be easily\nadded/modified without disrupting already existing data sets and the\nability to read and analyse old data. This data format is also based on\nNasa CDF. ASCII outputs are also supported by MagPy, of which the\n``PYSTR`` format also contains all meta information and ``PYASCII`` is\nthe most compact. Please consider that ASCII formats require a lot of\nmemory, especially for one second and higher resolution data.\n\n::\n\n        mydata.write('/tmp',format_type='PYCDF',coverage='year')\n\n2.10 Data transfer\n~~~~~~~~~~~~~~~~~~\n\nMagPy contains a number of methods to simplify data transfer for\nobservatory applications. Methods within the basic Python functionality\ncan also be very useful. Using the implemented methods requires:\n\n::\n\n        from magpy import transfer as mt\n\n2.10.1 Downloads\n^^^^^^^^^^^^^^^^\n\nUse the ``read`` method as outlined above. No additional imports are\nrequired.\n\n2.10.2 FTP upload\n^^^^^^^^^^^^^^^^^\n\nFiles can also be uploaded to an FTP server:\n\n::\n\n        mt.ftpdatatransfer(localfile='/path/to/data.cdf',ftppath='/remote/directory/',myproxy='ftpaddress or address of proxy',port=21,login='user',passwd='passwd',logfile='/path/mylog.log')\n        \n\nThe upload methods using FTP, SCP and GIN support logging. If the data\nfile failed to upload correctly, the path is added to a log file and,\nwhen called again, upload of the file is retried. This option is useful\nfor remote locations with unstable network connections.\n\n2.10.3 Secure communication protocol (SCP)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTo transfer via SCP:\n\n::\n\n        mt.scptransfer('user@address:/remote/directory/','/path/to/data.cdf',passwd,timeout=60)\n\n2.10.4 Upload data to GIN\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nUse the following command:\n\n::\n\n        mt.ginupload('/path/to/data.cdf', ginuser, ginpasswd, ginaddress, faillog=True, stdout=True)\n\n2.10.5 Avoiding real-text passwords in scripts\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn order to avoid using real-text password in scripts, MagPy comes along\nwith a simple encryption routine.\n\n::\n\n        from magpy.opt import cred as mpcred\n\nCredentials will be saved to a hidden file with encrypted passwords. To\nadd information for data transfer to a machine called 'MyRemoteFTP' with\nan IP of 192.168.0.99:\n\n::\n\n        mpcred.cc('transfer', 'MyRemoteFTP', user='user', passwd='secure', address='192.168.0.99', port=21)\n\nExtracting passwd information within your data transfer scripts:\n\n::\n\n        user = mpcred.lc('MyRemoteFTP', 'user')\n        password = mpcred.lc('MyRemoteFTP','passwd')\n\n2.11 DI measurements, basevalues and baselines\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThese procedures require an additional import:\n\n::\n\n        from magpy import absolutes as di\n\n2.11.1 Data structure of DI measurements\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nPlease check ``example3``, which is an example DI file. You can create\nthese DI files by using the input sheet from xmagpy or the online input\nsheet provided by the Conrad Observatory. If you want to use this\nservice, please contact the Observatory staff. Also supported are\nDI-files from the AUTODIF.\n\n2.11.2 Reading DI data\n^^^^^^^^^^^^^^^^^^^^^^\n\nReading and analyzing DI data requires valid DI file(s). For correct\nanalysis, variometer data and scalar field information needs to be\nprovided as well. Checkout ``help(di.absoluteAnalysis)`` for all\noptions. The analytical procedures are outlined in detail in the MagPy\narticle (citation). A typical analysis looks like:\n\n::\n\n        diresult = di.absoluteAnalysis('/path/to/DI/','path/to/vario/','path/to/scalar/')\n\nPath to DI can either point to a single file, a directory or even use\nwildcards to select data from a specific observatory/pillar. Using the\nexamples provided along with MagPy, the analysis line looks like\n\n::\n\n        diresult = di.absoluteAnalysis(example3,example2,example2)\n\nCalling this method will provide terminal output as follows and a stream\nobject ``diresult`` which can be used for further analyses.\n\n::\n\n        >>>...\n        >>>Analyzing manual measurement from 2015-03-25\n        >>>Vector at: 2015-03-25 08:18:00+00:00\n        >>>Declination: 3:53:46, Inclination: 64:17:17, H: 21027.2, Z: 43667.9, F: 48466.7\n        >>>Collimation and Offset:\n        >>>Declination:    S0: -3.081, delta H: -6.492, epsilon Z: -61.730\n        >>>Inclination:    S0: -1.531, epsilon Z: -60.307\n        >>>Scalevalue: 1.009 deg/unit\n        >>>Fext with delta F of 0.0 nT\n        >>>Delta D: 0.0, delta I: 0.0\n\nFext indicates that F values have been used from a separate file and not\nprovided along with DI data. Delta values for F, D, and I have not been\nprovided either. ``diresult`` is a stream object containing average D, I\nand F values, the collimation angles, scale factors and the base values\nfor the selected variometer, beside some additional meta information\nprovided in the data input form.\n\n2.11.3 Reading BLV files\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nBasevalues:\n\n::\n\n        blvdata = read('/path/myfile.blv')\n        mp.plot(blvdata, symbollist=['o','o','o'])\n\nAdopted baseline:\n\n::\n\n        bldata = read('/path/myfile.blv',mode='adopted')\n        mp.plot(bldata)\n\n2.11.4 Basevalues and baselines\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBasevalues as obtained in (2.11.2) or (2.11.3) are stored in a normal\ndata stream object, therefore all analysis methods outlined above can be\napplied to this data. The ``diresult`` object contains D, I, and F\nvalues for each measurement in columns x,y,z. Basevalues for H, D and Z\nrelated to the selected variometer are stored in columns dx,dy,dz. In\n``example4``, you will find some more DI analysis results. To plot these\nbasevalues we can use the following plot command, where we specify the\ncolumns, filled circles as plotsymbols and also define a minimum spread\nof each y-axis of +/- 5 nT for H and Z, +/- 0.05 deg for D.\n\n::\n\n        basevalues = read(example4)\n        mp.plot(basevalues, variables=['dx','dy','dz'], symbollist=['o','o','o'], padding=[5,0.05,5])\n\nFitting a baseline can be easily accomplished with the ``fit`` method.\nFirst we test a linear fit to the data by fitting a polynomial function\nwith degree 1.\n\n::\n\n        func = basevalues.fit(['dx','dy','dz'],fitfunc='poly', fitdegree=1)\n        mp.plot(basevalues, variables=['dx','dy','dz'], symbollist=['o','o','o'], padding=[5,0.05,5], function=func)\n\nWe then fit a spline function using 3 knotsteps over the timerange (the\nknotstep option is always related to the given timerange).\n\n::\n\n        func = basevalues.fit(['dx','dy','dz'],fitfunc='spline', knotstep=0.33)\n        mp.plot(basevalues, variables=['dx','dy','dz'], symbollist=['o','o','o'], padding=[5,0.05,5], function=func)\n\nHint: a good estimate on the necessary fit complexity can be obtained by\nlooking at delta F values. If delta F is mostly constant, then the\nbaseline should also not be very complex.\n\n2.11.5 Applying baselines\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe baseline method provides a number of options to assist the observer\nin determining baseline corrections and realted issues. The basic\nbuilding block of the baseline method is the fit function as discussed\nabove. Lets first load raw vectorial geomagnetic data, the absevalues of\nwhich are contained in above example:\n\n::\n\n        rawdata = read(example5)\n\nNow we can apply the basevalue information and the spline function as\ntested above:\n\n::\n\n        func = rawdata.baseline(basevalues, extradays=0, fitfunc='spline',\n                                knotstep=0.33,startabs='2015-09-01',endabs='2016-01-22')\n\nThe ``baseline`` method will determine and return a fit function between\nthe two given timeranges based on the provided basevalue data\n``blvdata``. The option ``extradays`` allows for adding days before and\nafter start/endtime for which the baseline function will be\nextrapolated. This option is useful for providing quasi-definitive data.\nWhen applying this method, a number of new meta-information attributes\nwill be added, containing basevalues and all functional parameters to\ndescribe the baseline. Thus, the stream object still contains\nuncorrected raw data, but all baseline correction information is now\ncontained within its meta data. To apply baseline correction you can use\nthe ``bc`` method:\n\n::\n\n        corrdata = rawdata.bc()\n\nIf baseline jumps/breaks are necessary due to missing data, you can call\nthe baseline function for each independent segment and combine the\nresulting baseline functions to a list:\n\n::\n\n        stream = read(mydata,starttime='2016-01-01',endtime='2016-03-01')\n        basevalues = read(mybasevalues)\n        adoptedbasefunc = []\n        adoptedbasefunc.append(stream.baseline(basevalues, extradays=0, fitfunc='poly', fitdegree=1,startabs='2016-01-01',endabs='2016-02-01')\n        adoptedbasefunc.append(stream.baseline(basevalues, extradays=0, fitfunc='spline', knotstep=0.33,startabs='2016-01-02',endabs='2016-01-03')\n\n        corr = stream.bc()\n\nThe combined baseline can be plotted accordingly. Extend the function\nparameters with each additional segment.\n\n::\n\n        mp.plot(basevalues, variables=['dx','dy','dz'], symbollist=['o','o','o'], padding=[5,0.05,5], function=adoptedbasefunc)\n\nAdding a baseline for scalar data, which is determined from the delta F\nvalues provided within the basevalue data stream:\n\n::\n\n        scalarbasefunc = []\n        scalarbasefunc.append(basevalues.baseline(basevalues, keys=['df'], extradays=0, fitfunc='poly', fitdegree=1,startabs='2016-01-01',endabs='2016-03-01'))\n        plotfunc = adoptedbasefunc\n        plotfunc.extend(scalarbasefunc)\n        mp.plot(basevalues, variables=['dx','dy','dz','df'], symbollist=['o','o','o','o'], padding=[5,0.05,5,5], function=plotfunc)\n\nGetting dailymeans and correction for scalar baseline can be acomplished\nby:\n\n::\n\n        meanstream = stream.dailymeans()\n        meanstream = meanstream.func2stream(scalarbasefunc,mode='sub',keys=['f'],fkeys=['df'])\n        meanstream = meanstream.delta_f()\n\nPlease note that here the function originally determined from the deltaF\n(df) values of the basevalue data needs to be applied to the F column\n(f) from the data stream. Before saving we will also extract the\nbaseline parameters from the meta information, which is automatically\ngenerated by the ``baseline`` method.\n\n::\n\n        absinfo = stream.header.get('DataAbsInfo','')\n        fabsinfo = basevalues.header.get('DataAbsInfo','')\n\n2.11.6 Saving basevalue and baseline information\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe following will create a BLV file:\n\n::\n\n        basevalues.write('/my/path', coverage='all', format_type='BLV', diff=meanstream, year='2016', absinfo=absinfo, deltaF=fabsinfo)\n\nInformation on the adopted baselines will be extracted from option\n``absinfo``. If several functions are provided, baseline jumps will be\nautomatically inserted into the BLV data file. The output of adopted\nscalar baselines is configured by option ``deltaF``. If a number is\nprovided, this value is assumed to represent the adopted scalar\nbaseline. If either 'mean' or 'median' are given (e.g.\n``deltaF='mean'``), then the mean/median value of all delta F values in\nthe ``basevalues`` stream is used, requiring that such data is\ncontained. Providing functional parameters as stored in a\n``DataAbsInfo`` meta information field, as shown above, will calculate\nand use the scalar baseline function. The ``meanstream`` stream contains\ndaily averages of delta F values between variometer and F measurements\nand the baseline adoption data in the meta-information. You can,\nhowever, provide all this information manually as well. The typical way\nto obtain such a ``meanstream`` is sketched above.\n\n2.12 Database support\n~~~~~~~~~~~~~~~~~~~~~\n\nMagPy supports database access and many methods for optimizing data\ntreatment in connection with databases. Among many other benefits, using\na database simplifies many typical procedures related to\nmeta-information. Currently, MagPy supports\n`MySQL <https://www.mysql.com/>`__ databases. To use these features, you\nneed to have MySQL installed on your system. In the following we provide\na brief outline of how to set up and use this optional addition. Please\nnote that a proper usage of the database requires sensor-specific\ninformation. In geomagnetism, it is common to combine data from\ndifferent sensors into one file structure. In this case, such data needs\nto remain separate for database usage and is only combined when\nproducing\n`IAGA <http://www.iaga-aiga.org/>`__/`INTERMAGNET <http://www.intermagnet.org>`__\ndefinitive data. Furthermore, unique sensor information such as type and\nserial number is required.\n\n::\n\n        import magpy import database as mdb\n\n2.12.1 Setting up a MagPy database (using MySQL)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nOpen mysql (e.g. Linux: ``mysql -u root -p mysql``) and create a new\ndatabase. Replace ``#DB-NAME`` with your database name (e.g. ``MyDB``).\nAfter creation, you will need to grant priviledges to this database to a\nuser of your choice. Please refer to official MySQL documentations for\ndetails and further commands.\n\n::\n\n         mysql> CREATE DATABASE #DB-NAME; \n         mysql> GRANT ALL PRIVILEGES ON #DB-NAME.* TO '#USERNAME'@'%' IDENTIFIED BY '#PASSWORD';\n\n2.12.2 Initializing a MagPy database\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nConnecting to a database using MagPy is done using following command:\n\n::\n\n        db = mdb.mysql.connect(host=\"localhost\",user=\"#USERNAME\",passwd=\"#PASSWORD\",db=\"#DB-NAME\")\n        mdb.dbinit(db)\n\n2.12.3 Adding data to the database\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nExamples of useful meta-information:\n\n::\n\n        iagacode = 'WIC'\n        data = read(example1)\n        gsm = data.selectkeys(['f'])\n        fge = data.selectkeys(['x','y','z'])\n        gsm.header['SensorID'] = 'GSM90_12345_0002'\n        gsm.header['StationID'] = iagacode\n        fge.header['SensorID'] = 'FGE_22222_0001'\n        fge.header['StationID'] = iagacode\n        mdb.writeDB(db,gsm)\n        mdb.writeDB(db,fge)\n\nAll available meta-information will be added automatically to the\nrelevant database tables. The SensorID scheme consists of three parts:\ninstrument (GSM90), serial number (12345), and a revision number (0002)\nwhich might change in dependency of maintenance, calibration, etc. As\nyou can see in the example above, we separate data from different\ninstruments, which we recommend particularly for high resolution data,\nas frequency and noise characteristics of sensor types will differ.\n\n2.12.4 Reading data\n^^^^^^^^^^^^^^^^^^^\n\nTo read data from an established database:\n\n::\n\n        data = mdb.readDB(db,'GSM90_12345_0002') \n\nOptions e.g. starttime='' and endtime='' are similar as for normal\n``read``.\n\n2.12.5 Meta data\n^^^^^^^^^^^^^^^^\n\nAn often used application of database connectivity with MagPy will be to\napply meta-information stored in the database to data files before\nsubmission. The following command demostrates how to extract all missing\nmeta-information from the database for the selected sensor and add it to\nthe header dictionary of the data object.\n\n::\n\n        rawdata = read('/path/to/rawdata.bin')\n        rawdata.header = mdb.dbfields2dict(db,'FGE_22222_0001')\n        rawdata.write(..., format_type='IMAGCDF')\n\n2.13 Monitoring scheduled scripts\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAutomated analysis can e easily accomplished by adding a series of MagPy\ncommands into a script. A typical script could be:\n\n::\n\n        # read some data and get means\n        data = read(example1)\n        mean_f = data.mean('f')\n\n        # import monitor method\n        from magpy.opt import Analysismonitor\n        analysisdict = Analysismonitor(logfile='/var/log/anamon.log')\n        analysisdict = analysisdict.load()\n        # check some arbitray threshold\n        analysisdict.check({'data_threshold_f_GSM90': [mean_f,'>',20000]})\n\nIf provided criteria are invalid, then the logfile is changed\naccordingly. This method can assist you particularly in checking data\nactuality, data contents, data validity, upload success, etc. In\ncombination with an independent monitoring tool like\n`Nagios <https://www.nagios.org/>`__, you can easily create mail/SMS\nnotfications of such changes, in addition to monitoring processes, live\ntimes, disks etc. `MARCOS <https://github.com/geomagpy/MARCOS>`__ comes\nalong with some instructions on how to use Nagios/MagPy for data\nacquisition monitoring.\n\n2.14 Data acquisition support\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nMagPy contains a couple of packages which can be used for data\nacquisition, collection and organization. These methods are primarily\ncontained in two applications:\n`MARTAS <https://github.com/geomagpy/MARTAS>`__ and\n`MARCOS <https://github.com/geomagpy/MARCOS>`__. MARTAS (Magpy Automated\nRealtime Acquisition System) supports communication with many common\ninstruments (e.g. GSM, LEMI, POS1, FGE, and many non-magnetic\ninstruments) and transfers serial port signals to\n`WAMP <http://wamp-proto.org/>`__ (Web Application Messaging Protocol),\nwhich allows for real-time data access using e.g. WebSocket\ncommunication through the internet. MARCOS (Magpy's Automated Realtime\nCollection and Organistaion System) can access such real-time streams\nand also data from many other sources and supports the observer by\nstoring, analyzing, archiving data, as well as monitoring all processes.\nDetails on these two applications can be found elsewhere.\n\n2.15 Graphical user interface\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nMany of the above mentioned methods are also available within the\ngraphical user interface of MagPy. To use this check the installation\ninstructions for your operating system. You will find Video Tutorials\nonline (to be added) describing its usage for specific analyses.\n\n2.16 Current developments\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n2.16.1 Exchange data objects with `ObsPy <https://github.com/obspy/obspy>`__\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nMagPy supports the exchange of data with ObsPy, the seismological\ntoolbox. Data objects of both python packages are very similar. Note:\nObsPy assumes regular spaced time intervals. Please be careful if this\nis not the case with your data. The example below shows a simple import\nroutine, on how to read a seed file and plot a spectrogram (which you\ncan identically obtain from ObsPy as well). Conversions to MagPy allow\nfor vectorial analyses, and geomagnetic applications. Conversions to\nObsPy are useful for effective high frequency analysis, requiring evenly\nspaced time intervals, and for exporting to seismological data formats.\n\n::\n\n        from obspy import read as obsread\n        seeddata = obsread('/path/to/seedfile')\n        magpydata = obspy2magpy(seeddata,keydict={'ObsPyColName': 'x'})\n        mp.plotSpectrogram(magpydata,['x'])\n\n2.16.2 Flagging in ImagCDF\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n        datawithspikes = read(example1)\n        flaggeddata = datawithspikes.flag_outlier(keys=['f'],timerange=timedelta(minutes=1),threshold=3)\n        mp.plot(flaggeddata,['f'],annotate=True)\n        flaggeddata.write(tmpdir,format_type='IMAGCDF',addflags=True)\n\nThe ``addflags`` option denotes that flagging information will be added\nto the ImagCDF format. Please note that this is still under development\nand thus content and format specifications may change. So please use it\nonly for test purposes and not for archiving. To read and view flagged\nImagCDF data, just use the normal read command, and activate annotation\nfor plotting.\n\n::\n\n        new = read('/tmp/cnb_20120802_000000_PT1S_1.cdf')\n        mp.plot(new,['f'],annotate=True)\n\n3. Predefined scripts\n---------------------\n\nMagPy comes with a steadily increasing number of applications for\nvarious purposes. These applications can be run from some command prompt\nand allow to simplify/automize some commonly used applications of MagPy.\nAll applications have the same syntax, consisting of the name of\napplication and options. The option -h is available for all applications\nand provides an overview about purpose and options of the application:\n\n::\n\n        $> application -h\n\n3.1 Running applications in Linux/MacOs\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nOn Linux Systems all applications are added the bin directory and can be\nrun directly from any command interface/terminal after installation of\nMagPy:\n\n::\n\n        $> application -h\n\n3.2 Running applications in Windows\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAfter installing MagPy/GeomagPy on Windows, three executables are found\nin the MagPy program folder. For running applications you have to start\nthe MagPy \"command prompt\". In this terminal you will have to go to the\nScripts directory:\n\n::\n\n        .../> cd Scripts\n\nAnd here you now can run the application of your choice using the python\nenvironment:\n\n::\n\n        .../Scripts>python application -h\n\n3.3 Applications\n~~~~~~~~~~~~~~~~\n\nThe available applications are briefly intruduced in the following.\nPlease refer to \"application -h\" for all available options for each\napplication.\n\n3.3.1 mpconvert\n^^^^^^^^^^^^^^^\n\nmpconvert converts bewteen data formats based on MagPy. Typical\napplications are the conversion of binary data formats to readable ASCII\ndata sets or the conversion.\n\nTypical applications include\n\na) Convert IAGA seconds to IMAGCDF and include obligatory meta\n   information:\n\n   ::\n\n       mpconvert -r \"/iagaseconds/wic201701*\" -f IMAGCDF -c month -w \"/tmp\"\n                    -m \"DataStandardLevel:Full,IAGACode:WIC,DataReferences:myref\"\n\nb) Convert IMAGCDF seconds to IAF minute (using IAGA/IM filtering\n   procedures):\n\n   ::\n\n       mpconvert -r \"/imagcdf/wic_201701_000000_PT1S_4.cdf\" -f IAF -i -w \"/tmp\"\n\nmpconvert -r\n\"/srv/products/data/magnetism/definitive/wic2017/ImagCDF/wic\\_201708\\_000000\\_PT1S\\_4.cdf\"\n-f IAF -i -w \"/tmp\"\n\n3.3.2 addcred\n^^^^^^^^^^^^^\n\nUsed to store encrypted credential information for automatic data\ntransfer. So that sensitive information has not to be written in plain\ntext in scripts or cron jobs.\n\na) Add information for ftp data transfer. This information is encrypted\n   and can be accessed by referring to the shortcut \"zamg\".\n\n   ::\n\n       addcred -t transfer -c zamg -u max -p geheim \n                 -a \"ftp://ftp.remote.ac.at\" -l 21", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://pypi.python.org/pypi/geomagpy/", "keywords": "", "license": "LICENSE.txt", "maintainer": "", "maintainer_email": "", "name": "geomagpy", "package_url": "https://pypi.org/project/geomagpy/", "platform": "", "project_url": "https://pypi.org/project/geomagpy/", "project_urls": {"Homepage": "http://pypi.python.org/pypi/geomagpy/"}, "release_url": "https://pypi.org/project/geomagpy/0.9.3/", "requires_dist": null, "requires_python": "", "summary": "Geomagnetic analysis tools.", "version": "0.9.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><strong>MagPy (or GeomagPy) is a Python package for analysing and displaying\ngeomagnetic data.</strong></p>\n<p>Version Info: (please note: this package is still in a development state\nwith frequent modifcations) please check the release notes.</p>\n<p>MagPy provides tools for geomagnetic data analysis with special focus on\ntypical data processing routines in observatories. MagPy provides\nmethods for data format conversion, plotting and mathematical procedures\nwith specifically geomagnetic analysis routines such as basevalue and\nbaseline calculation and database handling. Among the supported data\nformats are <em>ImagCDF, IAGA-02, WDC, IMF, IAF, BLV</em>, and many more. Full\ninstallation also provides a graphical user interface, <em>xmagpy</em>. You\nwill find a complete manual for <em>xmagpy</em> in the docs.</p>\n<p>Typical usage of the basic MagPy package for reading and visualising\ndata looks like this:</p>\n<pre>#!/usr/bin/env python\n\nfrom magpy.stream import read\nimport magpy.mpplot as mp\nstream = read('filename_or_url')\nmp.plot(stream)\n</pre>\n<p>Below you will find a quick guide to usage of the basic MagPy package.\nFor instructions on <em>xmagpy</em> please refer to the document \u201c<a href=\"https://github.com/geomagpy/magpy/blob/master/magpy/doc/xmagpy-manual.pdf\" rel=\"nofollow\">An\nintroduction to\nXMagPy</a>\u201d\nin the docs. You can also subscribe to our information channel at\n<a href=\"https://t.me/geomagpy\" rel=\"nofollow\">Telegram</a> for further information on updates\nand current issues.</p>\n<div id=\"installation\">\n<h2>1. INSTALLATION</h2>\n<p>Pleas note that with the publication of MagPy 1.0 the recommended python\nenironment is &gt;= 3.6. The following installation instructions will\nassume such an environment. Particularly if you are using Python2.7\nplease go to the end of this sections for help.</p>\n<p>This section is currently updated and will be ready with the publication\nof MagPy 1.0.</p>\n<div id=\"linux-installation-ubuntu-debian\">\n<h3>1.1 Linux installation (Ubuntu,Debian)</h3>\n<div id=\"complete-install\">\n<h4>1.1.1 Complete Install</h4>\n<p>Tested for Ubuntu 18.04 and Debian Stretch (full installation with all\noptional packages). Please note that installation requires python 3.x.</p>\n<pre>$ sudo pip3 install geomagpy    #Will install MagPy and all dependencies\n$ sudo pip3 install wxpython    #Will install WX graphics system for XMagPy\n</pre>\n<p>You can now run XMagPy by using the following command</p>\n<pre>$ xmagpy\n</pre>\n</div>\n<div id=\"updates\">\n<h4>1.1.2 Updates</h4>\n<p>To upgrade to the most recent version:</p>\n<pre>$ sudo pip3 install -U geomagpy\n</pre>\n</div>\n<div id=\"creating-a-desktop-link\">\n<h4>1.1.3 Creating a desktop link</h4>\n<p>In order to create a desktop link on linux systems please refer to\ninstruction too be found your distribution. For Ubunutu and other Debian\nsystems such links are created as follows:</p>\n<p>Firstly create a file \u201cxmagpy.desktop\u201d which contains:</p>\n<pre>[Desktop Entry]\nType=Application\nName=XMagPy\nGenericName=GeoMagPy User Interface\nExec=xmagpy\nIcon=/usr/local/lib/python3.7/dist-packages/magpy/gui/magpy128.xpm\nTerminal=false\nCategories=Application;Development;\n</pre>\n<p>Then copy this file to the systems application folder:</p>\n<pre>sudo cp xmagpy.desktop /usr/share/applications/\n</pre>\n</div>\n</div>\n<div id=\"macos-installation\">\n<h3>1.2 MacOs installation</h3>\n<div id=\"install-a-python3-interpreter\">\n<h4>1.2.1 Install a python3 interpreter</h4>\n<ul>\n<li>we recommend\n<a href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\">Miniconda</a> or\n<a href=\"https://www.continuum.io/downloads\" rel=\"nofollow\">Anaconda</a></li>\n<li>see e.g. <a href=\"https://docs.continuum.io/anaconda/install\" rel=\"nofollow\">https://docs.continuum.io/anaconda/install</a> for more details</li>\n<li>before continuiung, test whether python is working. Open a terminal\nand run python</li>\n</ul>\n</div>\n<div id=\"install-magpy\">\n<h4>1.2.2 Install MagPy</h4>\n<p>Open a terminal and use the following commands:</p>\n<pre>$ pip install geomagpy    #Will install MagPy and all dependencies\n$ pip install wxpython    #Will install WX graphics system for XMagPy\n</pre>\n<p>You can now run XMagPy from the terminal by using the following command</p>\n<pre>$ xmagpyw\n</pre>\n</div>\n<div id=\"id1\">\n<h4>1.2.3 Creating a desktop link</h4>\n<p>Open Finder and search for xmagpyw. Copy it to the desktop. To change\nthe icon, click on the xmagpyw link, open information and replace the\nimage on the upper left with e.g. magpy128.jpg (also to be found using\nfinder).</p>\n</div>\n</div>\n<div id=\"windows-installation-winpython-package\">\n<h3>1.3 Windows installation - WinPython Package</h3>\n<div id=\"install-magpy-for-windows\">\n<h4>1.3.1 Install MagPy for Windows</h4>\n<ul>\n<li>get the <a href=\"https://cobs.zamg.ac.at/data/index.php/en/downloads/category/1-magnetism\" rel=\"nofollow\">MagPy Windows\ninstaller</a>\nhere (under Downloads): <a href=\"https://cobs.zamg.ac.at\" rel=\"nofollow\">https://cobs.zamg.ac.at</a></li>\n<li>download and execute magpy-x.x.x.exe</li>\n<li>all required packages are included in the installer</li>\n</ul>\n</div>\n<div id=\"post-installation-information\">\n<h4>1.3.2 Post-installation information</h4>\n<ul>\n<li><p>MagPy will have a sub-folder in the Start menu. Here you will find\nthree items:</p>\n<pre>* command -&gt; opens a DOS shell within the Python environment e.g. for updates\n* python  -&gt; opens a python shell ready for MagPy\n* xmagpy  -&gt; opens the MagPy graphical user interface\n</pre>\n</li>\n</ul>\n</div>\n<div id=\"update-an-existing-magpy-installation-on-windows\">\n<h4>1.3.3 Update an existing MagPy installation on Windows</h4>\n<ul>\n<li>right-click on subfolder \u201ccommand\u201d in the start menu</li>\n<li>select \u201crun as administrator\u201d</li>\n<li>issue the following command \u201cpip install -U geomagpy\u201d (you can also\nspecify the version e.g. pip install geomagpy==0.x.x)</li>\n</ul>\n</div>\n<div id=\"installation-with-user-priviledges-only\">\n<h4>1.3.4 Installation with user priviledges only</h4>\n<ul>\n<li>Download a most recent version of WinPython3.x</li>\n<li>Unpack in your home directory</li>\n<li>Go to the WinPython Folder and run WinPython command prompt</li>\n<li>issue the same commands as for MacOS installation</li>\n<li>to run XMagPy: use xmagpy from the WinPython command promt.</li>\n</ul>\n</div>\n</div>\n<div id=\"installation-instructions-for-python-2-7\">\n<h3>1.4 Installation instructions for Python 2.7</h3>\n<p>The current version of magpy is still supporting python 2.7, although it\nis highly recommended to switch to python &gt;= 3.6. Installation on python\n2.7 is more complex, as some packages for graphical user interface and\nCDF support not as well supported. Please note: None of the addtional\nsteps is necessary for python 3.x.</p>\n<div id=\"pre-installation-work\">\n<h4>1.4.1 Pre-installation work</h4>\n<p>Get a recent version of NasaCDF for your platform, enables CDF support\nfor formats like ImagCDF. Package details and files can be found at\n<a href=\"http://cdf.gsfc.nasa.gov/\" rel=\"nofollow\">http://cdf.gsfc.nasa.gov/</a></p>\n<p>On Linux such installation will look like\n(<a href=\"http://cdf.gsfc.nasa.gov/html/sw_and_docs.html\" rel=\"nofollow\">http://cdf.gsfc.nasa.gov/html/sw_and_docs.html</a>)</p>\n<pre>$ tar -zxvf cdf37_0-dist-all.tar.gz\n$ cd cdf37...\n$ make OS=linux ENV=gnu CURSES=yes FORTRAN=no UCOPTIONS=-O2 SHARED=yes all\n$ sudo make INSTALLDIR=/usr/local/cdf install\n</pre>\n<p>Install the following additional compilers before continuing (required\nfor spacepy): Linux: install gcc MacOs: install gcc and gfortran</p>\n<p>Install coordinate system transformation support:</p>\n<pre>$ sudo apt-get install libproj-dev proj-data proj-bin\n</pre>\n</div>\n<div id=\"install-magpy-and-dependencies\">\n<h4>1.4.2 Install MagPy and dependencies</h4>\n<p>On Linux this will look like:</p>\n<pre>$ sudo apt-get install python-matplotlib python-scipy python-h5py cython python-pip\n$ sudo apt-get install python-wxgtk3.0 # or python-wxgtk2.8 (Debian Stretch)\n$ sudo apt-get install python-twisted\n$ sudo pip install ffnet\n$ sudo pip install pyproj==1.9.5\n$ sudo pip install pyserial\n$ sudo pip install service_identity\n$ sudo pip install ownet\n$ sudo pip install spacepy\n$ sudo pip install geomagpy\n</pre>\n<p>On Mac and Windows you need to download a python interpreter like\n<a href=\"https://www.continuum.io/downloads\" rel=\"nofollow\">Anaconda</a> or [WinPython] and\nthen install similar packages, particluarly the old wxpython 3.x.</p>\n</div>\n</div>\n<div id=\"platform-independent-container-docker\">\n<h3>1.5 Platform independent container - Docker</h3>\n<div id=\"install-docker-toolbox-on-your-operating-system\">\n<h4>1.5.1 Install <a href=\"https://www.docker.com/\" rel=\"nofollow\">Docker</a> (toolbox) on your operating system</h4>\n<pre>- https://docs.docker.com/engine/installation/\n</pre>\n</div>\n<div id=\"get-the-magpy-image\">\n<h4>1.5.2 Get the MagPy Image</h4>\n<pre>- open a docker shell\n\n       &gt;&gt;&gt; docker pull geomagpy/magpy:latest\n       &gt;&gt;&gt; docker run -d --name magpy -p 8000:8000 geomagpy/magpy:latest\n</pre>\n</div>\n<div id=\"open-a-browser\">\n<h4>1.5.3 Open a browser</h4>\n<pre>- open address http://localhost:8000 (or http://\"IP of your VM\":8000)\n- NEW: first time access might require a token or passwd\n\n       &gt;&gt;&gt; docker logs magpy\n\n     will show the token\n- run python shell (not conda)\n- in python shell\n\n       &gt;&gt;&gt; %matplotlib inline\n       &gt;&gt;&gt; from magpy.stream import read\n       &gt;&gt;&gt; ...\n</pre>\n</div>\n</div>\n<div id=\"install-from-source\">\n<h3>1.6 Install from source</h3>\n<p>Requirements: - Python 2.7, 3.x (recommended is &gt;=3.6)</p>\n<p>Recommended: - Python packages: * wxpython (for python2.7 it needs to\nbe 3.x or older) * NasaCDF (python 2.7 only) * SpacePy (python 2.7\nonly)</p>\n<ul>\n<li><p>Other useful Software:</p>\n<ul>\n<li><p>pyproj (for geographic coordinate systems)</p>\n</li>\n<li><p>MySQL (database features)</p>\n</li>\n<li><p>Webserver (e.g. Apache2, PHP)</p>\n<p>git clone git://github.com/GeomagPy/MagPy.git cd magpy* sudo\npython setup.py install</p>\n</li>\n</ul>\n</li>\n</ul>\n</div>\n</div>\n<div id=\"a-quick-guide-to-magpy\">\n<h2>2. A quick guide to MagPy</h2>\n<p>written by R. Leonhardt, R. Bailey (April 2017)</p>\n<p>MagPy\u2019s functionality can be accessed basically in three different ways:\n1) Directly import and use the magpy package into a python environment\n2) Run the graphical user interface xmagpy (xmagpyw for Mac) 3) Use\npredefined applications \u201cScripts\u201d</p>\n<p>The following section will primarily deal with way 1. For 2 - xmagpy -\nwe refer to the video tutorials whcih can be found here: Section 3\ncontains examples for predefined applications/scripts</p>\n<div id=\"getting-started-with-the-python-package\">\n<h3>2.1 Getting started with the python package</h3>\n<p>Start python. Import all stream methods and classes using:</p>\n<pre>from magpy.stream import *\n</pre>\n<p>Please note that this import will shadow any already existing <tt>read</tt>\nmethod.</p>\n</div>\n<div id=\"reading-and-writing-data\">\n<h3>2.2 Reading and writing data</h3>\n<p>MagPy supports the following data formats and thus conversions between\nthem: - WDC: World Data Centre format - JSON: JavaScript Object Notation\n- IMF: Intermagnet Format - IAF: Intermagnet Archive Format - NEIC: WGET\ndata from USGS - NEIC - IAGA: IAGA 2002 text format - IMAGCDF:\nIntermagnet CDF Format - GFZKP: GeoForschungsZentrum KP-Index format -\nGSM19/GSM90: Output formats from GSM magnetometers - POS1: POS-1 binary\noutput - BLV: Baseline format Intermagnet - IYFV: Yearly mean format\nIntermagnet</p>\n<p>\u2026 and many others. To get a full list, use:</p>\n<pre>from magpy.stream import *\nprint(PYMAG_SUPPORTED_FORMATS)\n</pre>\n<p>You will find several example files provided with MagPy. The <tt>cdf</tt>\nfile is stored along with meta information in NASA\u2019s common data format\n(cdf). Reading this file requires a working installation of Spacepy cdf.</p>\n<p>If you do not have any geomagnetic data file you can access example data\nby using the following command (after <tt>import *</tt>):</p>\n<pre>data = read(example1)\n</pre>\n<p>The data from <tt>example1</tt> has been read into a MagPy <em>DataStream</em> (or\n<em>stream</em>) object. Most data processing routines in MagPy are applied to\ndata streams.</p>\n<p>Several example data sets are provided within the MagPy package:</p>\n<ul>\n<li><tt>example1</tt>: <a href=\"http://www.iaga-aiga.org/\" rel=\"nofollow\">IAGA</a> ZIP (IAGA2002, zip\ncompressed) file with 1 second HEZ data</li>\n<li><tt>example2</tt>: <a href=\"#magpy\" rel=\"nofollow\">MagPy</a> Archive (CDF) file with 1 sec F data</li>\n<li><tt>example3</tt>: <a href=\"#magpy\" rel=\"nofollow\">MagPy</a> Basevalue (TXT) ascii file with DI\nand baseline data</li>\n<li><tt>example4</tt>: <a href=\"http://www.intermagnet.org\" rel=\"nofollow\">INTERMAGNET</a> ImagCDF\n(CDF) file with one week of 1 second data</li>\n<li><tt>example5</tt>: <a href=\"#magpy\" rel=\"nofollow\">MagPy</a> Archive (CDF) raw data file with xyz\nand supporting data</li>\n<li><tt>example6a</tt>: <a href=\"#magpy\" rel=\"nofollow\">MagPy</a> DI (txt) raw data file with DI\nmeasurement</li>\n<li><tt>example6b</tt>: <a href=\"#magpy\" rel=\"nofollow\">MagPy</a> like 6a to be used with example4</li>\n<li><tt>flagging_example</tt>: <a href=\"#magpy\" rel=\"nofollow\">MagPy</a> FlagDictionary (JSON)\nflagging info to be used with example1</li>\n<li><tt>recipe1_flags</tt>: <a href=\"#magpy\" rel=\"nofollow\">MagPy</a> FlagDictionary (JSON) to be\nused with cookbook recipe 1</li>\n</ul>\n<div id=\"reading\">\n<h4>2.2.1 Reading</h4>\n<p>For a file in the same directory:</p>\n<pre>data = read(r'myfile.min')\n</pre>\n<p>\u2026 or for specific paths in Linux:</p>\n<pre>data = read(r'/path/to/file/myfile.min')\n</pre>\n<p>\u2026 or for specific paths in Windows:</p>\n<pre>data = read(r'c:\\path\\to\\file\\myfile.min')\n</pre>\n<p>Pathnames are related to your operating system. In this guide we will\nassume a Linux system. Files that are read in are uploaded to the memory\nand each data column (or piece of header information) is assigned to an\ninternal variable (key). To get a quick overview of the assigned keys in\nany given stream (<tt>data</tt>) you can use the following method:</p>\n<pre>print(data._get_key_headers() )\n</pre>\n</div>\n<div id=\"writing\">\n<h4>2.2.2 Writing</h4>\n<p>After loading data from a file, we can save the data in the standard\nIAGA02 and IMAGCDF formats with the following commands.</p>\n<p>To create an IAGA-02 format file, use:</p>\n<pre>data.write(r'/path/to/diretory/',format_type='IAGA')\n</pre>\n<p>To create an <a href=\"http://www.intermagnet.org\" rel=\"nofollow\">INTERMAGNET</a> CDF (ImagCDF)\nfile:</p>\n<pre>data.write(r'/path/to/diretory/',format_type='IMAGCDF')\n</pre>\n<p>The filename will be created automatically according to the defined\nformat. By default, daily files are created and the date is added to the\nfilename in-between the optional parameters <tt>filenamebegins</tt> and\n<tt>filenameends</tt>. If <tt>filenameends</tt> is missing, <tt>.txt</tt> is used as\ndefault.</p>\n</div>\n<div id=\"other-possibilities-for-reading-files\">\n<h4>2.2.3 Other possibilities for reading files</h4>\n<p>To read all local files ending with .min within a directory (creates a\nsingle stream of all data):</p>\n<pre>data = read(r'/path/to/file/*.min')\n</pre>\n<p>Getting magnetic data directly from an online source such as the WDC:</p>\n<pre>data = read(r'ftp://thewellknownaddress/single_year/2011/fur2011.wdc')\n</pre>\n<p>Getting <em>kp</em> data from the GFZ Potsdam:</p>\n<pre>data = read(r'http://www-app3.gfz-potsdam.de/kp_index/qlyymm.tab')\n</pre>\n<p>(Please note: data access and usage is subjected to the terms and\nconditions of the individual data provider. Please make sure to read\nthem before accessing any of these products.)</p>\n<p>No format specifications are required for reading. If MagPy can handle\nthe format, it will be automatically recognized.</p>\n<p>Getting data for a specific time window for local files:</p>\n<pre>data = read(r'/path/to/files/*.min',starttime=\"2014-01-01\", endtime=\"2014-05-01\")\n</pre>\n<p>\u2026 and remote files:</p>\n<pre>data = read(r'ftp://address/fur2013.wdc',starttime=\"2013-01-01\", endtime=\"2013-02-01\")\n</pre>\n<p>Reading data from the INTERMAGNET Webservice (starting soon):</p>\n<pre>data = read('http://www.intermagnet.org/test/ws/?id=WIC')\n</pre>\n</div>\n<div id=\"selecting-timerange\">\n<h4>2.2.4 Selecting timerange</h4>\n<p>The stream can be trimmed to a specific time interval after reading by\napplying the trim method, e.g. for a specific month:</p>\n<pre>data = data.trim(starttime=\"2013-01-01\", endtime=\"2013-02-01\")\n</pre>\n</div>\n</div>\n<div id=\"getting-help-on-options-and-usage\">\n<h3>2.3 Getting help on options and usage</h3>\n<div id=\"python-s-help-function\">\n<h4>2.3.1 Python\u2019s help function</h4>\n<p>Information on individual methods and options can be obtained as\nfollows:</p>\n<p>For basic functions:</p>\n<pre>help(read)\n</pre>\n<p>For specific methods related to e.g. a stream object \u201cdata\u201d:</p>\n<pre>help(data.fit)\n</pre>\n<p>Note that this requires the existence of a \u201cdata\u201d object, which is\nobtained e.g. by data = read(\u2026). The help text can also be shown by\ndirectly calling the <em>DataStream</em> object method using:</p>\n<pre>help(DataStream.fit)\n</pre>\n</div>\n<div id=\"magpy-s-logging-system\">\n<h4>2.3.2 MagPy\u2019s logging system</h4>\n<p>MagPy automatically logs many function options and runtime information,\nwhich can be useful for debugging purposes. This log is saved by default\nin the temporary file directory of your operating system, e.g. for Linux\nthis would be <tt>/tmp/magpy.log</tt>. The log is formatted as follows with\nthe date, module and function in use and the message leve\n(INFO/WARNING/ERROR):</p>\n<pre>2017-04-22 09:50:11,308 INFO - magpy.stream - Initiating MagPy...\n</pre>\n<p>Messages on the WARNING and ERROR level will automatically be printed to\nshell. Messages for more detailed debugging are written at the DEBUG\nlevel and will not be printed to the log unless an additional handler\nfor printing DEBUG is added.</p>\n<p>Custom loggers can be defined by creating a logger object after\nimporting MagPy and adding handlers (with formatting):</p>\n<pre>from magpy.stream import *\nimport logging\n\nlogger = logging.getLogger()\nhdlr = logging.FileHandler('testlog.log')\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nhdlr.setFormatter(formatter)\nlogger.addHandler(hdlr)\n</pre>\n<p>The logger can also be configured to print to shell (stdout, without\nformatting):</p>\n<pre>import sys\nlogger = logging.getLogger()\nstdoutlog = logging.StreamHandler(sys.stdout)\nlogger.addHandler(stdoutlog)\n</pre>\n</div>\n</div>\n<div id=\"plotting\">\n<h3>2.4 Plotting</h3>\n<p>You will find some example plots at the <a href=\"http://www.conrad-observatory.at\" rel=\"nofollow\">Conrad\nObservatory</a>.</p>\n<div id=\"quick-and-not-dirty\">\n<h4>2.4.1 Quick (and not dirty)</h4>\n<pre>import magpy.mpplot as mp\nmp.plot(data)\n</pre>\n</div>\n<div id=\"some-options\">\n<h4>2.4.2 Some options</h4>\n<p>Select specific keys to plot:</p>\n<pre>mp.plot(data,variables=['x','y','z'])\n</pre>\n<p>Defining a plot title and specific colors (see <tt>help(mp.plot)</tt> for\nlist and all options):</p>\n<pre>mp.plot(data,variables=['x','y'],plottitle=\"Test plot\",\n        colorlist=['g', 'c'])\n</pre>\n</div>\n<div id=\"data-from-multiple-streams\">\n<h4>2.4.3 Data from multiple streams</h4>\n<p>Various datasets from multiple data streams will be plotted above one\nanother. Provide a list of streams and an array of keys:</p>\n<pre>mp.plotStreams([data1,data2],[['x','y','z'],['f']])\n</pre>\n</div>\n</div>\n<div id=\"flagging-data\">\n<h3>2.5 Flagging data</h3>\n<p>The flagging procedure allows the observer to mark specific data points\nor ranges. Falgs are useful for labelling data spikes, storm onsets,\npulsations, disturbances, lightning strikes, etc. Each flag is asociated\nwith a comment and a type number. The flagtype number ranges between 0\nand 4:</p>\n<ul>\n<li>0: normal data with comment (e.g. \u201cHello World\u201d)</li>\n<li>1: data marked by automated analysis (e.g. spike)</li>\n<li>2: data marked by observer as valid geomagnetic signature (e.g. storm\nonset, pulsation). Such data cannot be marked invalid by automated\nprocedures</li>\n<li>3: data marked by observer as invalid (e.g. lightning, magnetic\ndisturbance)</li>\n<li>4: merged data (e.g. data inserted from another source/instrument as\ndefined in the comment)</li>\n</ul>\n<p>Flags can be stored along with the data set (requires CDF format output)\nor separately in a binary archive. These flags can then be applied to\nthe raw data again, ascertaining perfect reproducibility.</p>\n<div id=\"mark-data-spikes\">\n<h4>2.5.1 Mark data spikes</h4>\n<p>Load a data record with data spikes:</p>\n<pre>datawithspikes = read(example1)\n</pre>\n<p>Mark all spikes using the automated function <tt>flag_outlier</tt> with\ndefault options:</p>\n<pre>flaggeddata = datawithspikes.flag_outlier(timerange=timedelta(minutes=1),threshold=3)\n</pre>\n<p>Show flagged data in a plot:</p>\n<pre>mp.plot(flaggeddata,['f'],annotate=True)\n</pre>\n</div>\n<div id=\"flag-time-range\">\n<h4>2.5.2 Flag time range</h4>\n<p>Flag a certain time range:</p>\n<pre>flaglist = flaggeddata.flag_range(keys=['f'], starttime='2012-08-02T04:33:40',\n                                  endtime='2012-08-02T04:44:10',\n                                  flagnum=3, text=\"iron metal near sensor\")\n</pre>\n<p>Apply these flags to the data:</p>\n<pre>flaggeddata = flaggeddata.flag(flaglist)\n</pre>\n<p>Show flagged data in a plot:</p>\n<pre>mp.plot(flaggeddata,['f'],annotate=True)\n</pre>\n</div>\n<div id=\"save-flagged-data\">\n<h4>2.5.3 Save flagged data</h4>\n<p>To save the data together with the list of flags to a CDF file:</p>\n<pre>flaggeddata.write('/tmp/',filenamebegins='MyFlaggedExample_', format_type='PYCDF')\n</pre>\n<p>To check for correct save procedure, read and plot the new file:</p>\n<pre>newdata = read(\"/tmp/MyFlaggedExample_*\")\nmp.plot(newdata,annotate=True, plottitle='Reloaded flagged CDF data')\n</pre>\n</div>\n<div id=\"save-flags-separately\">\n<h4>2.5.4 Save flags separately</h4>\n<p>To save the list of flags seperately from the data in a pickled binary\nfile:</p>\n<pre>fullflaglist = flaggeddata.extractflags()\nsaveflags(fullflaglist,\"/tmp/MyFlagList.pkl\"))\n</pre>\n<p>These flags can be loaded in and then reapplied to the data set:</p>\n<pre>data = read(example1)\nflaglist = loadflags(\"/tmp/MyFlagList.pkl\")\ndata = data.flag(flaglist)\nmp.plot(data,annotate=True, plottitle='Raw data with flags from file')\n</pre>\n</div>\n<div id=\"drop-flagged-data\">\n<h4>2.5.5 Drop flagged data</h4>\n<p>For some analyses it is necessary to use \u201cclean\u201d data, which can be\nproduced by dropping data flagged as invalid (e.g. spikes). By default,\nthe following method removes all data marked with flagtype numbers 1 and\n3.</p>\n<pre>cleandata = flaggeddata.remove_flagged()\nmp.plot(cleandata, ['f'], plottitle='Flagged data dropped')\n</pre>\n</div>\n</div>\n<div id=\"basic-methods\">\n<h3>2.6 Basic methods</h3>\n<div id=\"filtering\">\n<h4>2.6.1 Filtering</h4>\n<p>MagPy\u2019s <tt>filter</tt> uses the settings recommended by\n<a href=\"http://www.iaga-aiga.org/\" rel=\"nofollow\">IAGA</a>/<a href=\"http://www.intermagnet.org\" rel=\"nofollow\">INTERMAGNET</a>.\nCkeck <tt>help(data.filter)</tt> for further options and definitions of\nfilter types and pass bands.</p>\n<p>First, get the sampling rate before filtering in seconds:</p>\n<pre>print(\"Sampling rate before [sec]:\", cleandata.samplingrate())\n</pre>\n<p>Filter the data set with default parameters (<tt>filter</tt> automatically\nchooses the correct settings depending on the provided sanmpling rate):</p>\n<pre>filtereddata = cleandata.filter()\n</pre>\n<p>Get sampling rate and filtered data after filtering (please note that\nall filter information is added to the data\u2019s meta information\ndictionary (data.header):</p>\n<pre>print(\"Sampling rate after [sec]:\", filtereddata.samplingrate())\nprint(\"Filter and pass band:\", filtereddata.header.get('DataSamplingFilter',''))\n</pre>\n</div>\n<div id=\"coordinate-transformation\">\n<h4>2.6.2 Coordinate transformation</h4>\n<p>Assuming vector data in columns [x,y,z] you can freely convert between\nxyz, hdz, and idf coordinates:</p>\n<pre>cleandata = cleandata.xyz2hdz()\n</pre>\n</div>\n<div id=\"calculate-delta-f\">\n<h4>2.6.3 Calculate delta F</h4>\n<p>If the data file contains xyz (hdz, idf) data and an independently\nmeasured f value, you can calculate delta F between the two instruments\nusing the following:</p>\n<pre>cleandata = cleandata.delta_f()\nmp.plot(cleandata,plottitle='delta F')\n</pre>\n</div>\n<div id=\"calculate-means\">\n<h4>2.6.4 Calculate Means</h4>\n<p>Mean values for certain data columns can be obtained using the <tt>mean</tt>\nmethod. The mean will only be calculated for data with the percentage of\nvalid data (in contrast to missing data) points not falling below the\nvalue given by the percentage option (default 95). If too much data is\nmissing, then no mean is calulated and the function returns NaN.</p>\n<pre>print(cleandata.mean('df', percentage=80))\n</pre>\n<p>The median can be calculated by defining the <tt>meanfunction</tt> option:</p>\n<pre>print(cleandata.mean('df', meanfunction='median'))\n</pre>\n</div>\n<div id=\"applying-offsets\">\n<h4>2.6.5 Applying offsets</h4>\n<p>Constant offsets can be added to individual columns using the <tt>offset</tt>\nmethod with a dictionary defining the MagPy stream column keys and the\noffset to be applied (datetime.timedelta object for time column, float\nfor all others):</p>\n<pre>offsetdata = cleandata.offset({'time':timedelta(seconds=0.19),'f':1.24})\n</pre>\n</div>\n<div id=\"scaling-data\">\n<h4>2.6.6 Scaling data</h4>\n<p>Individual columns can also be multiplied by values provided in a\ndictionary:</p>\n<pre>multdata = cleandata.multiply({'x':-1})\n</pre>\n</div>\n<div id=\"fit-functions\">\n<h4>2.6.7 Fit functions</h4>\n<p>MagPy offers the possibility to fit functions to data using either\npolynomial functions or cubic splines (default):</p>\n<pre>func = cleandata.fit(keys=['x','y','z'],knotstep=0.1)\nmp.plot(cleandata,variables=['x','y','z'],function=func)\n</pre>\n</div>\n<div id=\"derivatives\">\n<h4>2.6.8 Derivatives</h4>\n<p>Time derivatives, which are useful to identify outliers and sharp\nchanges, are calculated as follows:</p>\n<pre>diffdata = cleandata.differentiate(keys=['x','y','z'],put2keys = ['dx','dy','dz'])\nmp.plot(diffdata,variables=['dx','dy','dz'])\n</pre>\n</div>\n<div id=\"all-methods-at-a-glance\">\n<h4>2.6.9 All methods at a glance</h4>\n<p>For a summary of all supported methods, see the section <strong>List of all\nMagPy methods</strong> below.</p>\n</div>\n</div>\n<div id=\"geomagnetic-analysis\">\n<h3>2.7 Geomagnetic analysis</h3>\n<div id=\"determination-of-k-indices\">\n<h4>2.7.1 Determination of K indices</h4>\n<p>MagPy supports the FMI method for determination of K indices. Please\nconsult the MagPy publication for details on this method and\napplication.</p>\n<p>A month of one minute data is provided in <tt>example2</tt>, which\ncorresponds to an <a href=\"http://www.intermagnet.org\" rel=\"nofollow\">INTERMAGNET</a> IAF\narchive file. Reading a file in this format will load one minute data by\ndefault. Accessing hourly data and other information is described below.</p>\n<pre>data2 = read(example2)\nkvals = data2.k_fmi()\n</pre>\n<p>The determination of K values will take some time as the filtering\nwindow is dynamically adjusted. In order to plot the original data (H\ncomponent) and K values together, we now use the multiple stream\nplotting method <tt>plotStreams</tt>. Here you need to provide a list of\nstreams and an array containing variables for each stream. The\nadditional options determine the appearance of the plot (limits, bar\nchart):</p>\n<pre>mp.plotStreams([data2,kvals],[['x'],['var1']],\n               specialdict = [{},{'var1':[0,9]}],\n               symbollist=['-','z'],\n               bartrange=0.06)\n</pre>\n<p><tt>'z'</tt> in <tt>symbollist</tt> refers to the second subplot (K), which should\nbe plotted as bars rather than the standard line (<tt><span class=\"pre\">'-'</span></tt>).</p>\n</div>\n<div id=\"automated-geomagnetic-storm-detection\">\n<h4>2.7.2 Automated geomagnetic storm detection</h4>\n<p>Geomagnetic storm detection is supported by MagPy using two procedures\nbased on wavelets and the Akaike Information Criterion (AIC) as outlined\nin detail in Bailey and Leonhardt (2016). A basic example of usage to\nfind an SSC using a Discrete Wavelet Transform (DWT) is shown below:</p>\n<pre>from magpy.stream import read\nfrom magpy.opt.stormdet import seekStorm\nstormdata = read(\"LEMI025_2015-03-17.cdf\")      # 1s variometer data\nstormdata = stormdata.xyz2hdz()\nstormdata = stormdata.smooth('x', window_len=25)\ndetection, ssc_list = seekStorm(stormdata, method=\"MODWT\")\nprint(\"Possible SSCs detected:\", ssc_list)\n</pre>\n<p>The method <tt>seekStorm</tt> will return two variables: <tt>detection</tt> is\nTrue if any detection was made, while <tt>ssc_list</tt> is a list of\ndictionaries containing data on each detection. Note that this method\nalone can return a long list of possible SSCs (most incorrectly\ndetected), particularly during active storm times. It is most useful\nwhen additional restrictions based on satellite solar wind data apply\n(currently only optimised for ACE data, e.g. from the NOAA website):</p>\n<pre>satdata_ace_1m = read('20150317_ace_swepam_1m.txt')\nsatdata_ace_5m = read('20150317_ace_epam_5m.txt')\ndetection, ssc_list, sat_cme_list = seekStorm(stormdata,\n            satdata_1m=satdata_ace_1m, satdata_5m=satdata_ace_5m,\n            method='MODWT', returnsat=True)\nprint(\"Possible CMEs detected:\", sat_cme_list)\nprint(\"Possible SSCs detected:\", ssc_list)\n</pre>\n</div>\n<div id=\"sq-analysis\">\n<h4>2.7.3 Sq analysis</h4>\n<p>Methods are currently in preparation.</p>\n</div>\n<div id=\"validity-check-of-data\">\n<h4>2.7.4 Validity check of data</h4>\n<p>A common and important application used in the geomagnetism community is\na general validity check of geomagnetic data to be submitted to the\nofficial data repositories <a href=\"http://www.iaga-aiga.org/\" rel=\"nofollow\">IAGA</a>, WDC, or\n<a href=\"http://www.intermagnet.org\" rel=\"nofollow\">INTERMAGNET</a>. Please note: this is\ncurrently under development and will be extended in the near future. A\n\u2018one-click\u2019 test method will be included in xmagpy in the future,\nchecking:</p>\n<ol>\n<li><p>Validity of data formats, e.g.:</p>\n<pre>data = read('myiaffile.bin', debug=True)\n</pre>\n</li>\n<li><p>Completeness of meta-information</p>\n</li>\n<li><p>Conformity of applied techniques to respective rules</p>\n</li>\n<li><p>Internal consistency of data</p>\n</li>\n<li><p>Optional: regional consistency</p>\n</li>\n</ol>\n</div>\n<div id=\"spectral-analysis-and-noise\">\n<h4>2.7.5 Spectral Analysis and Noise</h4>\n<p>For analysis of the spectral content of data, MagPy provides two basic\nplotting methods. <tt>plotPS</tt> will calculate and display a power spectrum\nof the selected component. <tt>plotSpectrogram</tt> will plot a spectrogram\nof the time series. As usual, there are many options for plot window and\nprocessing parameters that can be accessed using the help method.</p>\n<pre>data = read(example1)\nmp.plotPS(data,key='f')\nmp.plotSpectrogram(data,['f'])\n</pre>\n</div>\n</div>\n<div id=\"handling-multiple-streams\">\n<h3>2.8 Handling multiple streams</h3>\n<div id=\"merging-streams\">\n<h4>2.8.1 Merging streams</h4>\n<p>Merging data comprises combining two streams into one new stream. This\nincludes adding a new column from another stream, filling gaps with data\nfrom another stream or replacing data from one column with data from\nanother stream. The following example sketches the typical usage:</p>\n<pre>print(\"Data columns in data2:\", data2._get_key_headers())\nnewstream = mergeStreams(data2,kvals,keys=['var1'])\nprint(\"Data columns after merging:\", data2._get_key_headers())\nmp.plot(newstream, ['x','y','z','var1'],symbollist=['-','-','-','z'])\n</pre>\n<p>If column <tt>var1</tt> does not existing in data2 (as above), then this\ncolumn is added. If column <tt>var1</tt> had already existed, then missing\ndata would be inserted from stream <tt>kvals</tt>. In order to replace any\nexisting data, use option <tt><span class=\"pre\">mode='replace'</span></tt>.</p>\n</div>\n<div id=\"differences-between-streams\">\n<h4>2.8.2 Differences between streams</h4>\n<p>Sometimes it is necessary to examine the differences between two data\nstreams e.g. differences between the F values of two instruments running\nin parallel at an observatory. The method <tt>subtractStreams</tt> is\nprovided for this analysis:</p>\n<pre>diff = subtractStreams(data1,data2,keys=['f'])\n</pre>\n</div>\n</div>\n<div id=\"the-art-of-meta-information\">\n<h3>2.9 The art of meta-information</h3>\n<p>Each data set is accompanied by a dictionary containing meta-information\nfor this data. This dictionary is completely dynamic and can be filled\nfreely, but there are a number of predefined fields that help the user\nprovide essential meta-information as requested by\n<a href=\"http://www.iaga-aiga.org/\" rel=\"nofollow\">IAGA</a>,\n<a href=\"http://www.intermagnet.org\" rel=\"nofollow\">INTERMAGNET</a> and other data providers.\nAll meta information is saved only to MagPy-specific archive formats\nPYCDF and PYSTR. All other export formats save only specific information\nas required by the projected format.</p>\n<p>The current content of this dictionary can be accessed by:</p>\n<pre>data = read(example1)\nprint(data.header)\n</pre>\n<p>Information is added/changed by using:</p>\n<pre>data.header['SensorName'] = 'FGE'\n</pre>\n<p>Individual information is obtained from the dictionary using standard\nkey input:</p>\n<pre>print(data.header.get('SensorName'))\n</pre>\n<p>If you want to have a more readable list of the header information, do:</p>\n<pre>for key in data.header:\n    print (\"Key: {} \\t Content: {}\".format(key,data.header.get(key)))\n</pre>\n<div id=\"conversion-to-imagcdf-adding-meta-information\">\n<h4>2.9.1 Conversion to ImagCDF - Adding meta-information</h4>\n<p>To convert data from <a href=\"http://www.iaga-aiga.org/\" rel=\"nofollow\">IAGA</a> or IAF formats\nto the new <a href=\"http://www.intermagnet.org\" rel=\"nofollow\">INTERMAGNET</a> CDF format, you\nwill usually need to add additional meta-information required for the\nnew format. MagPy can assist you here, firstly by extracting and\ncorrectly adding already existing meta-information into newly defined\nfields, and secondly by informing you of which information needs to be\nadded for producing the correct output format.</p>\n<p>Example of IAGA02 to ImagCDF:</p>\n<pre>mydata = read('IAGA02-file.min')\nmydata.write('/tmp',format_type='IMAGCDF')\n</pre>\n<p>The console output of the write command (see below) will tell you which\ninformation needs to be added (and how) in order to obtain correct\nImagCDF files. Please note, MagPy will store the data in any case and\nwill be able to read it again even if information is missing. Before\nsubmitting to a GIN, you need to make sure that the appropriate\ninformation is contained. Attributes that relate to publication of the\ndata will not be checked at this point, and might be included later.</p>\n<pre>&gt;&gt;&gt;Writing IMAGCDF Format /tmp/wic_20150828_0000_PT1M_4.cdf\n&gt;&gt;&gt;writeIMAGCDF: StandardLevel not defined - please specify by yourdata.header['DataStandardLevel'] = ['None','Partial','Full']\n&gt;&gt;&gt;writeIMAGCDF: Found F column\n&gt;&gt;&gt;writeIMAGCDF: given components are XYZF. Checking F column...\n&gt;&gt;&gt;writeIMAGCDF: analyzed F column - values are apparently independend from vector components - using column name 'S'\n</pre>\n<p>Now add the missing information. Selecting \u2018Partial\u2019 will require\nadditional information. You will get a \u2018reminder\u2019 if you forget this.\nPlease check IMAGCDF instructions on specific codes:</p>\n<pre>mydata.header['DataStandardLevel'] = 'Partial'\nmydata.header['DataPartialStandDesc'] = 'IMOS-01,IMOS-02,IMOS-03,IMOS-04,IMOS-05,IMOS-06,IMOS-11,IMOS-12,IMOS-13,IMOS-14,IMOS-15,IMOS-21,IMOS-22,IMOS-31,IMOS-41'\n</pre>\n<p>Similar reminders to fill out complete header information will be shown\nfor other conversions like:</p>\n<pre>mydata.write('/tmp',format_type='IAGA')\nmydata.write('/tmp',format_type='IMF')\nmydata.write('/tmp',format_type='IAF',coverage='month')\nmydata.write('/tmp',format_type='WDC')\n</pre>\n</div>\n<div id=\"providing-location-data\">\n<h4>2.9.2 Providing location data</h4>\n<p>Providing location data usually requires information on the reference\nsystem (ellipsoid,\u2026). By default MagPy assumes that these values are\nprovided in WGS84/WGS84 reference system. In order to facilitate most\neasy referencing and conversions, MagPy supports\n<a href=\"https://www.epsg-registry.org/\" rel=\"nofollow\">EPSG</a> codes for coordinates. If you\nprovide the geodetic references as follows, and provided that the\n<a href=\"https://github.com/OSGeo/proj.4\" rel=\"nofollow\">proj4</a> Python package is available,\nMagPy will automatically convert location data to the requested output\nformat (currently WGS84).</p>\n<pre>mydata.header['DataAcquisitionLongitude'] = -34949.9\nmydata.header['DataAcquisitionLatitude'] = 310087.0\nmydata.header['DataLocationReference'] = 'GK M34, EPSG: 31253'\n\n&gt;&gt;&gt;...\n&gt;&gt;&gt;writeIMAGCDF: converting coordinates to epsg 4326\n&gt;&gt;&gt;...\n</pre>\n</div>\n<div id=\"special-meta-information-fields\">\n<h4>2.9.3 Special meta-information fields</h4>\n<p>The meta-information fields can hold much more information than required\nby most output formats. This includes basevalue and baseline parameters,\nflagging details, detailed sensor information, serial numbers and much\nmore. MagPy makes use of these possibilities. In order to save this\nmeta-information along with your data set you can use MagPy internal\narchiving format, <tt>PYCDF</tt>, which can later be converted to any of the\naforementioned output formats. You can even reconstruct a full data\nbase. Any upcoming meta-information or output request can be easily\nadded/modified without disrupting already existing data sets and the\nability to read and analyse old data. This data format is also based on\nNasa CDF. ASCII outputs are also supported by MagPy, of which the\n<tt>PYSTR</tt> format also contains all meta information and <tt>PYASCII</tt> is\nthe most compact. Please consider that ASCII formats require a lot of\nmemory, especially for one second and higher resolution data.</p>\n<pre>mydata.write('/tmp',format_type='PYCDF',coverage='year')\n</pre>\n</div>\n</div>\n<div id=\"data-transfer\">\n<h3>2.10 Data transfer</h3>\n<p>MagPy contains a number of methods to simplify data transfer for\nobservatory applications. Methods within the basic Python functionality\ncan also be very useful. Using the implemented methods requires:</p>\n<pre>from magpy import transfer as mt\n</pre>\n<div id=\"downloads\">\n<h4>2.10.1 Downloads</h4>\n<p>Use the <tt>read</tt> method as outlined above. No additional imports are\nrequired.</p>\n</div>\n<div id=\"ftp-upload\">\n<h4>2.10.2 FTP upload</h4>\n<p>Files can also be uploaded to an FTP server:</p>\n<pre>mt.ftpdatatransfer(localfile='/path/to/data.cdf',ftppath='/remote/directory/',myproxy='ftpaddress or address of proxy',port=21,login='user',passwd='passwd',logfile='/path/mylog.log')\n</pre>\n<p>The upload methods using FTP, SCP and GIN support logging. If the data\nfile failed to upload correctly, the path is added to a log file and,\nwhen called again, upload of the file is retried. This option is useful\nfor remote locations with unstable network connections.</p>\n</div>\n<div id=\"secure-communication-protocol-scp\">\n<h4>2.10.3 Secure communication protocol (SCP)</h4>\n<p>To transfer via SCP:</p>\n<pre>mt.scptransfer('user@address:/remote/directory/','/path/to/data.cdf',passwd,timeout=60)\n</pre>\n</div>\n<div id=\"upload-data-to-gin\">\n<h4>2.10.4 Upload data to GIN</h4>\n<p>Use the following command:</p>\n<pre>mt.ginupload('/path/to/data.cdf', ginuser, ginpasswd, ginaddress, faillog=True, stdout=True)\n</pre>\n</div>\n<div id=\"avoiding-real-text-passwords-in-scripts\">\n<h4>2.10.5 Avoiding real-text passwords in scripts</h4>\n<p>In order to avoid using real-text password in scripts, MagPy comes along\nwith a simple encryption routine.</p>\n<pre>from magpy.opt import cred as mpcred\n</pre>\n<p>Credentials will be saved to a hidden file with encrypted passwords. To\nadd information for data transfer to a machine called \u2018MyRemoteFTP\u2019 with\nan IP of 192.168.0.99:</p>\n<pre>mpcred.cc('transfer', 'MyRemoteFTP', user='user', passwd='secure', address='192.168.0.99', port=21)\n</pre>\n<p>Extracting passwd information within your data transfer scripts:</p>\n<pre>user = mpcred.lc('MyRemoteFTP', 'user')\npassword = mpcred.lc('MyRemoteFTP','passwd')\n</pre>\n</div>\n</div>\n<div id=\"di-measurements-basevalues-and-baselines\">\n<h3>2.11 DI measurements, basevalues and baselines</h3>\n<p>These procedures require an additional import:</p>\n<pre>from magpy import absolutes as di\n</pre>\n<div id=\"data-structure-of-di-measurements\">\n<h4>2.11.1 Data structure of DI measurements</h4>\n<p>Please check <tt>example3</tt>, which is an example DI file. You can create\nthese DI files by using the input sheet from xmagpy or the online input\nsheet provided by the Conrad Observatory. If you want to use this\nservice, please contact the Observatory staff. Also supported are\nDI-files from the AUTODIF.</p>\n</div>\n<div id=\"reading-di-data\">\n<h4>2.11.2 Reading DI data</h4>\n<p>Reading and analyzing DI data requires valid DI file(s). For correct\nanalysis, variometer data and scalar field information needs to be\nprovided as well. Checkout <tt>help(di.absoluteAnalysis)</tt> for all\noptions. The analytical procedures are outlined in detail in the MagPy\narticle (citation). A typical analysis looks like:</p>\n<pre>diresult = di.absoluteAnalysis('/path/to/DI/','path/to/vario/','path/to/scalar/')\n</pre>\n<p>Path to DI can either point to a single file, a directory or even use\nwildcards to select data from a specific observatory/pillar. Using the\nexamples provided along with MagPy, the analysis line looks like</p>\n<pre>diresult = di.absoluteAnalysis(example3,example2,example2)\n</pre>\n<p>Calling this method will provide terminal output as follows and a stream\nobject <tt>diresult</tt> which can be used for further analyses.</p>\n<pre>&gt;&gt;&gt;...\n&gt;&gt;&gt;Analyzing manual measurement from 2015-03-25\n&gt;&gt;&gt;Vector at: 2015-03-25 08:18:00+00:00\n&gt;&gt;&gt;Declination: 3:53:46, Inclination: 64:17:17, H: 21027.2, Z: 43667.9, F: 48466.7\n&gt;&gt;&gt;Collimation and Offset:\n&gt;&gt;&gt;Declination:    S0: -3.081, delta H: -6.492, epsilon Z: -61.730\n&gt;&gt;&gt;Inclination:    S0: -1.531, epsilon Z: -60.307\n&gt;&gt;&gt;Scalevalue: 1.009 deg/unit\n&gt;&gt;&gt;Fext with delta F of 0.0 nT\n&gt;&gt;&gt;Delta D: 0.0, delta I: 0.0\n</pre>\n<p>Fext indicates that F values have been used from a separate file and not\nprovided along with DI data. Delta values for F, D, and I have not been\nprovided either. <tt>diresult</tt> is a stream object containing average D, I\nand F values, the collimation angles, scale factors and the base values\nfor the selected variometer, beside some additional meta information\nprovided in the data input form.</p>\n</div>\n<div id=\"reading-blv-files\">\n<h4>2.11.3 Reading BLV files</h4>\n<p>Basevalues:</p>\n<pre>blvdata = read('/path/myfile.blv')\nmp.plot(blvdata, symbollist=['o','o','o'])\n</pre>\n<p>Adopted baseline:</p>\n<pre>bldata = read('/path/myfile.blv',mode='adopted')\nmp.plot(bldata)\n</pre>\n</div>\n<div id=\"basevalues-and-baselines\">\n<h4>2.11.4 Basevalues and baselines</h4>\n<p>Basevalues as obtained in (2.11.2) or (2.11.3) are stored in a normal\ndata stream object, therefore all analysis methods outlined above can be\napplied to this data. The <tt>diresult</tt> object contains D, I, and F\nvalues for each measurement in columns x,y,z. Basevalues for H, D and Z\nrelated to the selected variometer are stored in columns dx,dy,dz. In\n<tt>example4</tt>, you will find some more DI analysis results. To plot these\nbasevalues we can use the following plot command, where we specify the\ncolumns, filled circles as plotsymbols and also define a minimum spread\nof each y-axis of +/- 5 nT for H and Z, +/- 0.05 deg for D.</p>\n<pre>basevalues = read(example4)\nmp.plot(basevalues, variables=['dx','dy','dz'], symbollist=['o','o','o'], padding=[5,0.05,5])\n</pre>\n<p>Fitting a baseline can be easily accomplished with the <tt>fit</tt> method.\nFirst we test a linear fit to the data by fitting a polynomial function\nwith degree 1.</p>\n<pre>func = basevalues.fit(['dx','dy','dz'],fitfunc='poly', fitdegree=1)\nmp.plot(basevalues, variables=['dx','dy','dz'], symbollist=['o','o','o'], padding=[5,0.05,5], function=func)\n</pre>\n<p>We then fit a spline function using 3 knotsteps over the timerange (the\nknotstep option is always related to the given timerange).</p>\n<pre>func = basevalues.fit(['dx','dy','dz'],fitfunc='spline', knotstep=0.33)\nmp.plot(basevalues, variables=['dx','dy','dz'], symbollist=['o','o','o'], padding=[5,0.05,5], function=func)\n</pre>\n<p>Hint: a good estimate on the necessary fit complexity can be obtained by\nlooking at delta F values. If delta F is mostly constant, then the\nbaseline should also not be very complex.</p>\n</div>\n<div id=\"applying-baselines\">\n<h4>2.11.5 Applying baselines</h4>\n<p>The baseline method provides a number of options to assist the observer\nin determining baseline corrections and realted issues. The basic\nbuilding block of the baseline method is the fit function as discussed\nabove. Lets first load raw vectorial geomagnetic data, the absevalues of\nwhich are contained in above example:</p>\n<pre>rawdata = read(example5)\n</pre>\n<p>Now we can apply the basevalue information and the spline function as\ntested above:</p>\n<pre>func = rawdata.baseline(basevalues, extradays=0, fitfunc='spline',\n                        knotstep=0.33,startabs='2015-09-01',endabs='2016-01-22')\n</pre>\n<p>The <tt>baseline</tt> method will determine and return a fit function between\nthe two given timeranges based on the provided basevalue data\n<tt>blvdata</tt>. The option <tt>extradays</tt> allows for adding days before and\nafter start/endtime for which the baseline function will be\nextrapolated. This option is useful for providing quasi-definitive data.\nWhen applying this method, a number of new meta-information attributes\nwill be added, containing basevalues and all functional parameters to\ndescribe the baseline. Thus, the stream object still contains\nuncorrected raw data, but all baseline correction information is now\ncontained within its meta data. To apply baseline correction you can use\nthe <tt>bc</tt> method:</p>\n<pre>corrdata = rawdata.bc()\n</pre>\n<p>If baseline jumps/breaks are necessary due to missing data, you can call\nthe baseline function for each independent segment and combine the\nresulting baseline functions to a list:</p>\n<pre>stream = read(mydata,starttime='2016-01-01',endtime='2016-03-01')\nbasevalues = read(mybasevalues)\nadoptedbasefunc = []\nadoptedbasefunc.append(stream.baseline(basevalues, extradays=0, fitfunc='poly', fitdegree=1,startabs='2016-01-01',endabs='2016-02-01')\nadoptedbasefunc.append(stream.baseline(basevalues, extradays=0, fitfunc='spline', knotstep=0.33,startabs='2016-01-02',endabs='2016-01-03')\n\ncorr = stream.bc()\n</pre>\n<p>The combined baseline can be plotted accordingly. Extend the function\nparameters with each additional segment.</p>\n<pre>mp.plot(basevalues, variables=['dx','dy','dz'], symbollist=['o','o','o'], padding=[5,0.05,5], function=adoptedbasefunc)\n</pre>\n<p>Adding a baseline for scalar data, which is determined from the delta F\nvalues provided within the basevalue data stream:</p>\n<pre>scalarbasefunc = []\nscalarbasefunc.append(basevalues.baseline(basevalues, keys=['df'], extradays=0, fitfunc='poly', fitdegree=1,startabs='2016-01-01',endabs='2016-03-01'))\nplotfunc = adoptedbasefunc\nplotfunc.extend(scalarbasefunc)\nmp.plot(basevalues, variables=['dx','dy','dz','df'], symbollist=['o','o','o','o'], padding=[5,0.05,5,5], function=plotfunc)\n</pre>\n<p>Getting dailymeans and correction for scalar baseline can be acomplished\nby:</p>\n<pre>meanstream = stream.dailymeans()\nmeanstream = meanstream.func2stream(scalarbasefunc,mode='sub',keys=['f'],fkeys=['df'])\nmeanstream = meanstream.delta_f()\n</pre>\n<p>Please note that here the function originally determined from the deltaF\n(df) values of the basevalue data needs to be applied to the F column\n(f) from the data stream. Before saving we will also extract the\nbaseline parameters from the meta information, which is automatically\ngenerated by the <tt>baseline</tt> method.</p>\n<pre>absinfo = stream.header.get('DataAbsInfo','')\nfabsinfo = basevalues.header.get('DataAbsInfo','')\n</pre>\n</div>\n<div id=\"saving-basevalue-and-baseline-information\">\n<h4>2.11.6 Saving basevalue and baseline information</h4>\n<p>The following will create a BLV file:</p>\n<pre>basevalues.write('/my/path', coverage='all', format_type='BLV', diff=meanstream, year='2016', absinfo=absinfo, deltaF=fabsinfo)\n</pre>\n<p>Information on the adopted baselines will be extracted from option\n<tt>absinfo</tt>. If several functions are provided, baseline jumps will be\nautomatically inserted into the BLV data file. The output of adopted\nscalar baselines is configured by option <tt>deltaF</tt>. If a number is\nprovided, this value is assumed to represent the adopted scalar\nbaseline. If either \u2018mean\u2019 or \u2018median\u2019 are given (e.g.\n<tt><span class=\"pre\">deltaF='mean'</span></tt>), then the mean/median value of all delta F values in\nthe <tt>basevalues</tt> stream is used, requiring that such data is\ncontained. Providing functional parameters as stored in a\n<tt>DataAbsInfo</tt> meta information field, as shown above, will calculate\nand use the scalar baseline function. The <tt>meanstream</tt> stream contains\ndaily averages of delta F values between variometer and F measurements\nand the baseline adoption data in the meta-information. You can,\nhowever, provide all this information manually as well. The typical way\nto obtain such a <tt>meanstream</tt> is sketched above.</p>\n</div>\n</div>\n<div id=\"database-support\">\n<h3>2.12 Database support</h3>\n<p>MagPy supports database access and many methods for optimizing data\ntreatment in connection with databases. Among many other benefits, using\na database simplifies many typical procedures related to\nmeta-information. Currently, MagPy supports\n<a href=\"https://www.mysql.com/\" rel=\"nofollow\">MySQL</a> databases. To use these features, you\nneed to have MySQL installed on your system. In the following we provide\na brief outline of how to set up and use this optional addition. Please\nnote that a proper usage of the database requires sensor-specific\ninformation. In geomagnetism, it is common to combine data from\ndifferent sensors into one file structure. In this case, such data needs\nto remain separate for database usage and is only combined when\nproducing\n<a href=\"http://www.iaga-aiga.org/\" rel=\"nofollow\">IAGA</a>/<a href=\"http://www.intermagnet.org\" rel=\"nofollow\">INTERMAGNET</a>\ndefinitive data. Furthermore, unique sensor information such as type and\nserial number is required.</p>\n<pre>import magpy import database as mdb\n</pre>\n<div id=\"setting-up-a-magpy-database-using-mysql\">\n<h4>2.12.1 Setting up a MagPy database (using MySQL)</h4>\n<p>Open mysql (e.g. Linux: <tt>mysql <span class=\"pre\">-u</span> root <span class=\"pre\">-p</span> mysql</tt>) and create a new\ndatabase. Replace <tt><span class=\"pre\">#DB-NAME</span></tt> with your database name (e.g. <tt>MyDB</tt>).\nAfter creation, you will need to grant priviledges to this database to a\nuser of your choice. Please refer to official MySQL documentations for\ndetails and further commands.</p>\n<pre>mysql&gt; CREATE DATABASE #DB-NAME;\nmysql&gt; GRANT ALL PRIVILEGES ON #DB-NAME.* TO '#USERNAME'@'%' IDENTIFIED BY '#PASSWORD';\n</pre>\n</div>\n<div id=\"initializing-a-magpy-database\">\n<h4>2.12.2 Initializing a MagPy database</h4>\n<p>Connecting to a database using MagPy is done using following command:</p>\n<pre>db = mdb.mysql.connect(host=\"localhost\",user=\"#USERNAME\",passwd=\"#PASSWORD\",db=\"#DB-NAME\")\nmdb.dbinit(db)\n</pre>\n</div>\n<div id=\"adding-data-to-the-database\">\n<h4>2.12.3 Adding data to the database</h4>\n<p>Examples of useful meta-information:</p>\n<pre>iagacode = 'WIC'\ndata = read(example1)\ngsm = data.selectkeys(['f'])\nfge = data.selectkeys(['x','y','z'])\ngsm.header['SensorID'] = 'GSM90_12345_0002'\ngsm.header['StationID'] = iagacode\nfge.header['SensorID'] = 'FGE_22222_0001'\nfge.header['StationID'] = iagacode\nmdb.writeDB(db,gsm)\nmdb.writeDB(db,fge)\n</pre>\n<p>All available meta-information will be added automatically to the\nrelevant database tables. The SensorID scheme consists of three parts:\ninstrument (GSM90), serial number (12345), and a revision number (0002)\nwhich might change in dependency of maintenance, calibration, etc. As\nyou can see in the example above, we separate data from different\ninstruments, which we recommend particularly for high resolution data,\nas frequency and noise characteristics of sensor types will differ.</p>\n</div>\n<div id=\"reading-data\">\n<h4>2.12.4 Reading data</h4>\n<p>To read data from an established database:</p>\n<pre>data = mdb.readDB(db,'GSM90_12345_0002')\n</pre>\n<p>Options e.g. starttime=\u2019\u2019 and endtime=\u2019\u2019 are similar as for normal\n<tt>read</tt>.</p>\n</div>\n<div id=\"meta-data\">\n<h4>2.12.5 Meta data</h4>\n<p>An often used application of database connectivity with MagPy will be to\napply meta-information stored in the database to data files before\nsubmission. The following command demostrates how to extract all missing\nmeta-information from the database for the selected sensor and add it to\nthe header dictionary of the data object.</p>\n<pre>rawdata = read('/path/to/rawdata.bin')\nrawdata.header = mdb.dbfields2dict(db,'FGE_22222_0001')\nrawdata.write(..., format_type='IMAGCDF')\n</pre>\n</div>\n</div>\n<div id=\"monitoring-scheduled-scripts\">\n<h3>2.13 Monitoring scheduled scripts</h3>\n<p>Automated analysis can e easily accomplished by adding a series of MagPy\ncommands into a script. A typical script could be:</p>\n<pre># read some data and get means\ndata = read(example1)\nmean_f = data.mean('f')\n\n# import monitor method\nfrom magpy.opt import Analysismonitor\nanalysisdict = Analysismonitor(logfile='/var/log/anamon.log')\nanalysisdict = analysisdict.load()\n# check some arbitray threshold\nanalysisdict.check({'data_threshold_f_GSM90': [mean_f,'&gt;',20000]})\n</pre>\n<p>If provided criteria are invalid, then the logfile is changed\naccordingly. This method can assist you particularly in checking data\nactuality, data contents, data validity, upload success, etc. In\ncombination with an independent monitoring tool like\n<a href=\"https://www.nagios.org/\" rel=\"nofollow\">Nagios</a>, you can easily create mail/SMS\nnotfications of such changes, in addition to monitoring processes, live\ntimes, disks etc. <a href=\"https://github.com/geomagpy/MARCOS\" rel=\"nofollow\">MARCOS</a> comes\nalong with some instructions on how to use Nagios/MagPy for data\nacquisition monitoring.</p>\n</div>\n<div id=\"data-acquisition-support\">\n<h3>2.14 Data acquisition support</h3>\n<p>MagPy contains a couple of packages which can be used for data\nacquisition, collection and organization. These methods are primarily\ncontained in two applications:\n<a href=\"https://github.com/geomagpy/MARTAS\" rel=\"nofollow\">MARTAS</a> and\n<a href=\"https://github.com/geomagpy/MARCOS\" rel=\"nofollow\">MARCOS</a>. MARTAS (Magpy Automated\nRealtime Acquisition System) supports communication with many common\ninstruments (e.g. GSM, LEMI, POS1, FGE, and many non-magnetic\ninstruments) and transfers serial port signals to\n<a href=\"http://wamp-proto.org/\" rel=\"nofollow\">WAMP</a> (Web Application Messaging Protocol),\nwhich allows for real-time data access using e.g. WebSocket\ncommunication through the internet. MARCOS (Magpy\u2019s Automated Realtime\nCollection and Organistaion System) can access such real-time streams\nand also data from many other sources and supports the observer by\nstoring, analyzing, archiving data, as well as monitoring all processes.\nDetails on these two applications can be found elsewhere.</p>\n</div>\n<div id=\"graphical-user-interface\">\n<h3>2.15 Graphical user interface</h3>\n<p>Many of the above mentioned methods are also available within the\ngraphical user interface of MagPy. To use this check the installation\ninstructions for your operating system. You will find Video Tutorials\nonline (to be added) describing its usage for specific analyses.</p>\n</div>\n<div id=\"current-developments\">\n<h3>2.16 Current developments</h3>\n<div id=\"exchange-data-objects-with-obspy\">\n<h4>2.16.1 Exchange data objects with <a href=\"https://github.com/obspy/obspy\" rel=\"nofollow\">ObsPy</a></h4>\n<p>MagPy supports the exchange of data with ObsPy, the seismological\ntoolbox. Data objects of both python packages are very similar. Note:\nObsPy assumes regular spaced time intervals. Please be careful if this\nis not the case with your data. The example below shows a simple import\nroutine, on how to read a seed file and plot a spectrogram (which you\ncan identically obtain from ObsPy as well). Conversions to MagPy allow\nfor vectorial analyses, and geomagnetic applications. Conversions to\nObsPy are useful for effective high frequency analysis, requiring evenly\nspaced time intervals, and for exporting to seismological data formats.</p>\n<pre>from obspy import read as obsread\nseeddata = obsread('/path/to/seedfile')\nmagpydata = obspy2magpy(seeddata,keydict={'ObsPyColName': 'x'})\nmp.plotSpectrogram(magpydata,['x'])\n</pre>\n</div>\n<div id=\"flagging-in-imagcdf\">\n<h4>2.16.2 Flagging in ImagCDF</h4>\n<pre>datawithspikes = read(example1)\nflaggeddata = datawithspikes.flag_outlier(keys=['f'],timerange=timedelta(minutes=1),threshold=3)\nmp.plot(flaggeddata,['f'],annotate=True)\nflaggeddata.write(tmpdir,format_type='IMAGCDF',addflags=True)\n</pre>\n<p>The <tt>addflags</tt> option denotes that flagging information will be added\nto the ImagCDF format. Please note that this is still under development\nand thus content and format specifications may change. So please use it\nonly for test purposes and not for archiving. To read and view flagged\nImagCDF data, just use the normal read command, and activate annotation\nfor plotting.</p>\n<pre>new = read('/tmp/cnb_20120802_000000_PT1S_1.cdf')\nmp.plot(new,['f'],annotate=True)\n</pre>\n</div>\n</div>\n</div>\n<div id=\"predefined-scripts\">\n<h2>3. Predefined scripts</h2>\n<p>MagPy comes with a steadily increasing number of applications for\nvarious purposes. These applications can be run from some command prompt\nand allow to simplify/automize some commonly used applications of MagPy.\nAll applications have the same syntax, consisting of the name of\napplication and options. The option -h is available for all applications\nand provides an overview about purpose and options of the application:</p>\n<pre>$&gt; application -h\n</pre>\n<div id=\"running-applications-in-linux-macos\">\n<h3>3.1 Running applications in Linux/MacOs</h3>\n<p>On Linux Systems all applications are added the bin directory and can be\nrun directly from any command interface/terminal after installation of\nMagPy:</p>\n<pre>$&gt; application -h\n</pre>\n</div>\n<div id=\"running-applications-in-windows\">\n<h3>3.2 Running applications in Windows</h3>\n<p>After installing MagPy/GeomagPy on Windows, three executables are found\nin the MagPy program folder. For running applications you have to start\nthe MagPy \u201ccommand prompt\u201d. In this terminal you will have to go to the\nScripts directory:</p>\n<pre>.../&gt; cd Scripts\n</pre>\n<p>And here you now can run the application of your choice using the python\nenvironment:</p>\n<pre>.../Scripts&gt;python application -h\n</pre>\n</div>\n<div id=\"applications\">\n<h3>3.3 Applications</h3>\n<p>The available applications are briefly intruduced in the following.\nPlease refer to \u201capplication -h\u201d for all available options for each\napplication.</p>\n<div id=\"mpconvert\">\n<h4>3.3.1 mpconvert</h4>\n<p>mpconvert converts bewteen data formats based on MagPy. Typical\napplications are the conversion of binary data formats to readable ASCII\ndata sets or the conversion.</p>\n<p>Typical applications include</p>\n<ol>\n<li><p>Convert IAGA seconds to IMAGCDF and include obligatory meta\ninformation:</p>\n<pre>mpconvert -r \"/iagaseconds/wic201701*\" -f IMAGCDF -c month -w \"/tmp\"\n             -m \"DataStandardLevel:Full,IAGACode:WIC,DataReferences:myref\"\n</pre>\n</li>\n<li><p>Convert IMAGCDF seconds to IAF minute (using IAGA/IM filtering\nprocedures):</p>\n<pre>mpconvert -r \"/imagcdf/wic_201701_000000_PT1S_4.cdf\" -f IAF -i -w \"/tmp\"\n</pre>\n</li>\n</ol>\n<p>mpconvert -r\n\u201c/srv/products/data/magnetism/definitive/wic2017/ImagCDF/wic_201708_000000_PT1S_4.cdf\u201d\n-f IAF -i -w \u201c/tmp\u201d</p>\n</div>\n<div id=\"addcred\">\n<h4>3.3.2 addcred</h4>\n<p>Used to store encrypted credential information for automatic data\ntransfer. So that sensitive information has not to be written in plain\ntext in scripts or cron jobs.</p>\n<ol>\n<li><p>Add information for ftp data transfer. This information is encrypted\nand can be accessed by referring to the shortcut \u201czamg\u201d.</p>\n<pre>addcred -t transfer -c zamg -u max -p geheim\n          -a \"ftp://ftp.remote.ac.at\" -l 21\n</pre>\n</li>\n</ol>\n</div>\n</div>\n</div>\n\n          </div>"}, "last_serial": 6569936, "releases": {"0.3.95": [{"comment_text": "", "digests": {"md5": "3442713cc5676c3c69c2d8df0f301b20", "sha256": "4e3cfccd7e9d61753559bf3bcda1197d83f4d841d28888756393f0364496a181"}, "downloads": -1, "filename": "GeomagPy-0.3.95.tar.gz", "has_sig": false, "md5_digest": "3442713cc5676c3c69c2d8df0f301b20", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2974136, "upload_time": "2017-07-28T14:41:13", "upload_time_iso_8601": "2017-07-28T14:41:13.264688Z", "url": "https://files.pythonhosted.org/packages/fe/17/8fc52c92739e4eddaa60c270fe3670b86ad9bb2039a8175240537765c09b/GeomagPy-0.3.95.tar.gz", "yanked": false}], "0.3.96": [{"comment_text": "", "digests": {"md5": "d828513bc906854790476185ee63f991", "sha256": "89abfe62e046913a05802ac58c979affaeaf62a94e84210cf632d49d61d00557"}, "downloads": -1, "filename": "GeomagPy-0.3.96.tar.gz", "has_sig": false, "md5_digest": "d828513bc906854790476185ee63f991", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2997844, "upload_time": "2017-09-02T13:25:40", "upload_time_iso_8601": "2017-09-02T13:25:40.031049Z", "url": "https://files.pythonhosted.org/packages/ad/1f/27d276754cede6608d9eff2adeb71836afae311e0301069c7dc0294dea5a/GeomagPy-0.3.96.tar.gz", "yanked": false}], "0.3.97": [{"comment_text": "", "digests": {"md5": "1c8f10f7b8248ee4d4142583948a9133", "sha256": "a00592d58cd86f561b93f8a58aeb319a23aba0734517761266aa54ebceb6dc5c"}, "downloads": -1, "filename": "GeomagPy-0.3.97.tar.gz", "has_sig": false, "md5_digest": "1c8f10f7b8248ee4d4142583948a9133", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3002018, "upload_time": "2017-09-29T21:00:03", "upload_time_iso_8601": "2017-09-29T21:00:03.272986Z", "url": "https://files.pythonhosted.org/packages/53/f3/74bf5875f0f9a8d381ab20bbe26e2049ac3439a614c730865ea55cb3f6f1/GeomagPy-0.3.97.tar.gz", "yanked": false}], "0.3.98": [{"comment_text": "", "digests": {"md5": "41968963a791baa3f10ea83cb6e85adf", "sha256": "43c4b9a2b4661d59c3ddf8b245db9bb29502558837abad9d36502c3820368129"}, "downloads": -1, "filename": "GeomagPy-0.3.98.tar.gz", "has_sig": false, "md5_digest": "41968963a791baa3f10ea83cb6e85adf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3006016, "upload_time": "2018-01-11T19:48:57", "upload_time_iso_8601": "2018-01-11T19:48:57.824406Z", "url": "https://files.pythonhosted.org/packages/5b/c5/09f8275d4c181a99c7b751cea3f61880295a09216c9714dcabdb7fe97e23/GeomagPy-0.3.98.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "6a8850204127536a8fa4aef998b9f987", "sha256": "cc9b2a4c3d15288b713584a0962dd76c7fdd69e7732b73f161229676cec22451"}, "downloads": -1, "filename": "geomagpy-0.4.0.tar.gz", "has_sig": false, "md5_digest": "6a8850204127536a8fa4aef998b9f987", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3041554, "upload_time": "2018-06-14T05:02:03", "upload_time_iso_8601": "2018-06-14T05:02:03.350356Z", "url": "https://files.pythonhosted.org/packages/c5/b6/c3968676592c2e5d0af1c16c354f4538a1b53d72e68ed07d88184a746bf4/geomagpy-0.4.0.tar.gz", "yanked": false}], "0.4.4": [{"comment_text": "", "digests": {"md5": "30fea8c17633cb7cdc253fcedf23c80b", "sha256": "0740ed18628162d2c8d8dcd961eb0f22bc5b2d870e1e975973a39a1f0f567b95"}, "downloads": -1, "filename": "geomagpy-0.4.4.tar.gz", "has_sig": false, "md5_digest": "30fea8c17633cb7cdc253fcedf23c80b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3047968, "upload_time": "2018-09-14T10:48:12", "upload_time_iso_8601": "2018-09-14T10:48:12.497132Z", "url": "https://files.pythonhosted.org/packages/95/b9/7101de93cf36af430af3a12604e4036d0b5dd74cacc9419234aae4c23729/geomagpy-0.4.4.tar.gz", "yanked": false}], "0.4.5": [{"comment_text": "", "digests": {"md5": "4b3a3a44a329ae9b5537cf1819098a00", "sha256": "7813b7638986a386fbc2651bdaf399613384c48642167b898633669e4e7d6ebb"}, "downloads": -1, "filename": "geomagpy-0.4.5.tar.gz", "has_sig": false, "md5_digest": "4b3a3a44a329ae9b5537cf1819098a00", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3048111, "upload_time": "2018-11-15T15:44:36", "upload_time_iso_8601": "2018-11-15T15:44:36.386224Z", "url": "https://files.pythonhosted.org/packages/2d/a4/6e7600c02649c4631e4b8db838193df3bfbf37b7355c7f57d8bec5641b73/geomagpy-0.4.5.tar.gz", "yanked": false}], "0.4.6": [{"comment_text": "", "digests": {"md5": "54c159161cacd8a7adadf389b4876d6e", "sha256": "fcc9220358fd4a24a69cb1099f42bc4372cd5a91f385f4abe1eb684d27f7bdc8"}, "downloads": -1, "filename": "geomagpy-0.4.6.tar.gz", "has_sig": false, "md5_digest": "54c159161cacd8a7adadf389b4876d6e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3050691, "upload_time": "2019-02-01T09:09:33", "upload_time_iso_8601": "2019-02-01T09:09:33.545457Z", "url": "https://files.pythonhosted.org/packages/19/3c/dae16520db2211dc2a2f1b77e06a2e56a0badf24dfac9bb390e293050513/geomagpy-0.4.6.tar.gz", "yanked": false}], "0.4.7": [{"comment_text": "", "digests": {"md5": "bbabb2aac739ecfc86ef15b7ee2be9d5", "sha256": "558bec0767215e934c90f06277fe1957a72e7c19e8547ac9a57b0355de355233"}, "downloads": -1, "filename": "geomagpy-0.4.7.tar.gz", "has_sig": false, "md5_digest": "bbabb2aac739ecfc86ef15b7ee2be9d5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3035299, "upload_time": "2019-05-16T12:22:20", "upload_time_iso_8601": "2019-05-16T12:22:20.951824Z", "url": "https://files.pythonhosted.org/packages/f3/b8/5175ae56ee50246bea50097f4d89f31033891c64364840f8b03468509eb8/geomagpy-0.4.7.tar.gz", "yanked": false}], "0.9.2": [{"comment_text": "", "digests": {"md5": "c973eecad5532fd916ab87231c89d0fa", "sha256": "0b2215d53574a5780bfe494de7510341e5cba3920d4515ce32fbdf8e20f00d06"}, "downloads": -1, "filename": "geomagpy-0.9.2.tar.gz", "has_sig": false, "md5_digest": "c973eecad5532fd916ab87231c89d0fa", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21561389, "upload_time": "2020-01-23T14:33:21", "upload_time_iso_8601": "2020-01-23T14:33:21.167982Z", "url": "https://files.pythonhosted.org/packages/54/18/8af3509996f2127e6c792b9664178b176839167ef6d8d8e5792030977410/geomagpy-0.9.2.tar.gz", "yanked": false}], "0.9.3": [{"comment_text": "", "digests": {"md5": "ff7dea6d6f4493720bd5eb3d4f489a47", "sha256": "a5cda3d36de8c65f7f05930e38c2229f3bcd3c8e7e6f4be7e12100fcf2fa1b97"}, "downloads": -1, "filename": "geomagpy-0.9.3.tar.gz", "has_sig": false, "md5_digest": "ff7dea6d6f4493720bd5eb3d4f489a47", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25434488, "upload_time": "2020-02-04T16:44:19", "upload_time_iso_8601": "2020-02-04T16:44:19.686742Z", "url": "https://files.pythonhosted.org/packages/ed/32/6dfd23f220a5d56b49924836341747e199feaf3067e3298709479a858ae4/geomagpy-0.9.3.tar.gz", "yanked": false}], "v0.3.1": [{"comment_text": "", "digests": {"md5": "373b4dca79a3a822fc8cf861196caa7f", "sha256": "f2fe75a7d1453ed4564a4e2210dce458d3b2928294da4b2a684091e1164c2fa7"}, "downloads": -1, "filename": "GeomagPy-v0.3.1.tar.gz", "has_sig": false, "md5_digest": "373b4dca79a3a822fc8cf861196caa7f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1535562, "upload_time": "2016-09-02T14:57:12", "upload_time_iso_8601": "2016-09-02T14:57:12.623535Z", "url": "https://files.pythonhosted.org/packages/df/ef/7b7fd9457aa87c5ace3d145c3a8aa33c6ebe4d33cc1a8c0ea21d3d3bc6c9/GeomagPy-v0.3.1.tar.gz", "yanked": false}], "v0.3.2": [{"comment_text": "", "digests": {"md5": "92b74a42c24dcfed458831cdd455cb22", "sha256": "0de12010964a615c74b39f79fd1f9a2b4fc484f52a26097e5d0b9a62fe153a24"}, "downloads": -1, "filename": "GeomagPy-v0.3.2.tar.gz", "has_sig": false, "md5_digest": "92b74a42c24dcfed458831cdd455cb22", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1535641, "upload_time": "2016-09-03T17:01:50", "upload_time_iso_8601": "2016-09-03T17:01:50.414955Z", "url": "https://files.pythonhosted.org/packages/8e/8e/0d89daa4925b6e676642f05c4c662e006b0849d2542a8b63d63af6dc3df4/GeomagPy-v0.3.2.tar.gz", "yanked": false}], "v0.3.3": [{"comment_text": "", "digests": {"md5": "0534cd06f7235c52cda3187ce36329eb", "sha256": "0f6d9345787f398cf539b7c0fc72568c17cf90c521dbe420a149c96221219197"}, "downloads": -1, "filename": "GeomagPy-v0.3.3.tar.gz", "has_sig": false, "md5_digest": "0534cd06f7235c52cda3187ce36329eb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1531510, "upload_time": "2016-09-17T09:20:49", "upload_time_iso_8601": "2016-09-17T09:20:49.684058Z", "url": "https://files.pythonhosted.org/packages/64/ae/ba10669933546519e870c771e00e16d185c466d5d2c330b46ccecc5ef7ea/GeomagPy-v0.3.3.tar.gz", "yanked": false}], "v0.3.4": [{"comment_text": "", "digests": {"md5": "d94bf09ea0bb25434914c6d11585ed48", "sha256": "fdd9ba897f7b62a7519a0d432240000973e475cebbd0deec394c7721668f0a29"}, "downloads": -1, "filename": "GeomagPy-v0.3.4.tar.gz", "has_sig": false, "md5_digest": "d94bf09ea0bb25434914c6d11585ed48", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1532426, "upload_time": "2016-09-17T15:31:30", "upload_time_iso_8601": "2016-09-17T15:31:30.908204Z", "url": "https://files.pythonhosted.org/packages/e9/76/3f429e0c6fb35976f0e19d8dd54152409c23b4afbb7bb72bb7807a1a53e2/GeomagPy-v0.3.4.tar.gz", "yanked": false}], "v0.3.6": [{"comment_text": "", "digests": {"md5": "b2908ce8422e20f563830cf91b7fe92c", "sha256": "2137d42edef6b7354757f0c1f255b56a27a03b1094fa70d46c584b134335864c"}, "downloads": -1, "filename": "GeomagPy-v0.3.6.tar.gz", "has_sig": false, "md5_digest": "b2908ce8422e20f563830cf91b7fe92c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1530208, "upload_time": "2016-12-16T14:56:19", "upload_time_iso_8601": "2016-12-16T14:56:19.736705Z", "url": "https://files.pythonhosted.org/packages/12/01/4a2f2e0635570ab00c865425d532fefeb18b1583fc8a066d802f8e99632c/GeomagPy-v0.3.6.tar.gz", "yanked": false}], "v0.3.7": [{"comment_text": "", "digests": {"md5": "81281669c30621fc8c2c845cf2d4ef2b", "sha256": "b6f71389d14036f0dc961486f7650a1742e87a99c8e65ababe40f0cbc3dc7b86"}, "downloads": -1, "filename": "GeomagPy-v0.3.7.tar.gz", "has_sig": false, "md5_digest": "81281669c30621fc8c2c845cf2d4ef2b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1536503, "upload_time": "2017-01-18T11:06:13", "upload_time_iso_8601": "2017-01-18T11:06:13.164330Z", "url": "https://files.pythonhosted.org/packages/0c/61/b0dffed8456ca1ecc497be889f09c7d4faddfabf5e6a250fd2557973bcba/GeomagPy-v0.3.7.tar.gz", "yanked": false}], "v0.3.8": [{"comment_text": "", "digests": {"md5": "3e97c3fbe574e90ea86f970db3fcbcb1", "sha256": "7626c63092ebd434f4a47ba0ed0b1cce516d6655993c63e8d97f40e2d41b3876"}, "downloads": -1, "filename": "GeomagPy-v0.3.8.tar.gz", "has_sig": false, "md5_digest": "3e97c3fbe574e90ea86f970db3fcbcb1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1537001, "upload_time": "2017-02-16T22:09:40", "upload_time_iso_8601": "2017-02-16T22:09:40.494268Z", "url": "https://files.pythonhosted.org/packages/a9/f2/f35a17edee464da0ee9c273e93ee33f23f7c97349f4c2afedd1dc649aea4/GeomagPy-v0.3.8.tar.gz", "yanked": false}], "v0.3.9": [{"comment_text": "", "digests": {"md5": "6ad4bd7587868cae21b479209d9ce492", "sha256": "16ffd7026df5b98f92380ca4b9a6b600b179af01a5e654fc4dfa508e159351c9"}, "downloads": -1, "filename": "GeomagPy-v0.3.9.tar.gz", "has_sig": false, "md5_digest": "6ad4bd7587868cae21b479209d9ce492", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1538479, "upload_time": "2017-03-21T17:51:01", "upload_time_iso_8601": "2017-03-21T17:51:01.879373Z", "url": "https://files.pythonhosted.org/packages/ab/5d/01f52139ff57980d930f0613136cc97b40efe3c6e44a95da5d476b9ad618/GeomagPy-v0.3.9.tar.gz", "yanked": false}], "v0.3.91": [{"comment_text": "", "digests": {"md5": "0fd922d692af07a836a0fc585d251d17", "sha256": "497ce4a440ee535602e1ab4992ab7ccc2e3910ec56d9da6638f77ca4bbdc265b"}, "downloads": -1, "filename": "GeomagPy-v0.3.91.tar.gz", "has_sig": false, "md5_digest": "0fd922d692af07a836a0fc585d251d17", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2691976, "upload_time": "2017-04-05T22:25:49", "upload_time_iso_8601": "2017-04-05T22:25:49.746093Z", "url": "https://files.pythonhosted.org/packages/02/5d/5b3d05b5faaecb490a4e548069a4c9928f873f02b4b73e6794e17e1a2b4e/GeomagPy-v0.3.91.tar.gz", "yanked": false}], "v0.3.92": [{"comment_text": "", "digests": {"md5": "4a670894706eb216df2a75d52db3afaf", "sha256": "89c22de934dab5117b903a345ab35306ab15a2957e6ca24ba65de5f9a44e05f8"}, "downloads": -1, "filename": "GeomagPy-v0.3.92.tar.gz", "has_sig": false, "md5_digest": "4a670894706eb216df2a75d52db3afaf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2751370, "upload_time": "2017-04-07T17:32:51", "upload_time_iso_8601": "2017-04-07T17:32:51.983453Z", "url": "https://files.pythonhosted.org/packages/77/d0/d8eb1aee93304a36703de212a15ad0fe7aa3b58d798e2f4a670d09daf331/GeomagPy-v0.3.92.tar.gz", "yanked": false}], "v0.3.93": [{"comment_text": "", "digests": {"md5": "07dbef6eff0f06451b688fb217ee9e50", "sha256": "16a7f142b7a37f23c98be5e0f081caf1b8e42bfba17a53e6f5b1f797fdea2910"}, "downloads": -1, "filename": "GeomagPy-v0.3.93.tar.gz", "has_sig": false, "md5_digest": "07dbef6eff0f06451b688fb217ee9e50", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2779834, "upload_time": "2017-06-22T12:12:56", "upload_time_iso_8601": "2017-06-22T12:12:56.903365Z", "url": "https://files.pythonhosted.org/packages/f6/5c/a671546f60311c207c752e82f60875d2fa5f0f5c0fd684f3a9bd3b015398/GeomagPy-v0.3.93.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ff7dea6d6f4493720bd5eb3d4f489a47", "sha256": "a5cda3d36de8c65f7f05930e38c2229f3bcd3c8e7e6f4be7e12100fcf2fa1b97"}, "downloads": -1, "filename": "geomagpy-0.9.3.tar.gz", "has_sig": false, "md5_digest": "ff7dea6d6f4493720bd5eb3d4f489a47", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25434488, "upload_time": "2020-02-04T16:44:19", "upload_time_iso_8601": "2020-02-04T16:44:19.686742Z", "url": "https://files.pythonhosted.org/packages/ed/32/6dfd23f220a5d56b49924836341747e199feaf3067e3298709479a858ae4/geomagpy-0.9.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:58:02 2020"}