{"info": {"author": "Xiaoquan Kong", "author_email": "u1mail2me@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "==========================\n\u4e2d\u6587\u5206\u8bcd\u5668\u96c6\u5408\n==========================\n\n\n.. image:: https://img.shields.io/pypi/v/chinese_tokenzier_iterator.svg\n        :target: https://pypi.python.org/pypi/tokenizers_collection\n\n.. image:: https://img.shields.io/travis/howl-anderson/chinese_tokenzier_iterator.svg\n        :target: https://travis-ci.org/howl-anderson/tokenizers_collection\n\n.. image:: https://readthedocs.org/projects/chinese-tokenzier-iterator/badge/?version=latest\n        :target: https://tokenizers-collection.readthedocs.io/en/latest/?badge=latest\n        :alt: Documentation Status\n\n\n\n\n\u4e00\u4e9b\u4e2d\u6587\u5206\u8bcd\u5668\u7684\u7b80\u5355\u5c01\u88c5\u548c\u96c6\u5408\n\n\n* Free software: MIT license\n* Documentation: https://chinese-tokenzier-iterator.readthedocs.io.\n\n\nFeatures\n--------\n\n* TODO\n\n\u4f7f\u7528\n----\n.. code-block:: python\n\n    from tokenizers_collection.config import tokenizer_registry\n    for name, tokenizer in tokenizer_registry:\n        print(\"Tokenizer: {}\".format(name))\n        tokenizer('input_file.txt', 'output_file.txt')\n\n\u5b89\u88c5\n----\n.. code-block:: bash\n\n    pip install tokenizers_collection\n\n\u66f4\u65b0\u8bb8\u53ef\u6587\u4ef6\u4e0e\u4e0b\u8f7d\u6a21\u578b\n=======================\n\u56e0\u4e3a\u5176\u4e2d\u6709\u4e9b\u6a21\u578b\u9700\u8981\u66f4\u65b0\u8bb8\u53ef\u6587\u4ef6\uff08\u6bd4\u5982\uff1apynlpir\uff09\u6216\u8005\u9700\u8981\u4e0b\u8f7d\u6a21\u578b\u6587\u4ef6\uff08\u6bd4\u5982\uff1apyltp\uff09\uff0c\u56e0\u6b64\u5b89\u88c5\u540e\u9700\u8981\u6267\u884c\u7279\u5b9a\u7684\u547d\u4ee4\u5b8c\u6210\u64cd\u4f5c\uff0c\u8fd9\u91cc\u5df2\u7ecf\u5c06\u6240\u6709\u7684\u64cd\u4f5c\u5c01\u88c5\u6210\u4e86\u4e00\u4e2a\u51fd\u6570\uff0c\u53ea\u9700\u8981\u6267\u884c\u7c7b\u4f3c\u5982\u4e0b\u7684\u6307\u4ee4\u5373\u53ef\n\n.. code-block:: bash\n\n    python -m tokenizers_collection.helper\n\n\nCredits\n-------\n\nThis package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template.\n\n.. _Cookiecutter: https://github.com/audreyr/cookiecutter\n.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage\n\n\n=======\nHistory\n=======\n\n0.1.0 (2018-08-28)\n------------------\n\n* First release on PyPI.\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/howl-anderson/tokenizers_collection", "keywords": "tokenizers_collection", "license": "MIT license", "maintainer": "", "maintainer_email": "", "name": "tokenizers-collection", "package_url": "https://pypi.org/project/tokenizers-collection/", "platform": "", "project_url": "https://pypi.org/project/tokenizers-collection/", "project_urls": {"Homepage": "https://github.com/howl-anderson/tokenizers_collection"}, "release_url": "https://pypi.org/project/tokenizers-collection/0.1.2/", "requires_dist": ["Click (>=6.0)", "requests", "jieba", "thulac", "pynlpir", "pyltp"], "requires_python": "", "summary": "A simple iterator for using a set of Chinese tokenizer", "version": "0.1.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"id1\">\n<h2>\u4e2d\u6587\u5206\u8bcd\u5668\u96c6\u5408</h2>\n<a href=\"https://pypi.python.org/pypi/tokenizers_collection\" rel=\"nofollow\"><img alt=\"https://img.shields.io/pypi/v/chinese_tokenzier_iterator.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ba5d8872a9a0a14a63f7e170045b7c754b5057ee/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6368696e6573655f746f6b656e7a6965725f6974657261746f722e737667\"></a>\n<a href=\"https://travis-ci.org/howl-anderson/tokenizers_collection\" rel=\"nofollow\"><img alt=\"https://img.shields.io/travis/howl-anderson/chinese_tokenzier_iterator.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/338a3520fb3911d73a2e2c2206676ab269c9d5bc/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f686f776c2d616e646572736f6e2f6368696e6573655f746f6b656e7a6965725f6974657261746f722e737667\"></a>\n<a href=\"https://tokenizers-collection.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/233627c04200723d638a17c4ab54de96adebb046/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6368696e6573652d746f6b656e7a6965722d6974657261746f722f62616467652f3f76657273696f6e3d6c6174657374\"></a>\n<p>\u4e00\u4e9b\u4e2d\u6587\u5206\u8bcd\u5668\u7684\u7b80\u5355\u5c01\u88c5\u548c\u96c6\u5408</p>\n<ul>\n<li>Free software: MIT license</li>\n<li>Documentation: <a href=\"https://chinese-tokenzier-iterator.readthedocs.io\" rel=\"nofollow\">https://chinese-tokenzier-iterator.readthedocs.io</a>.</li>\n</ul>\n<div id=\"features\">\n<h3>Features</h3>\n<ul>\n<li>TODO</li>\n</ul>\n</div>\n<div id=\"id2\">\n<h3>\u4f7f\u7528</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tokenizers_collection.config</span> <span class=\"kn\">import</span> <span class=\"n\">tokenizer_registry</span>\n<span class=\"k\">for</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span> <span class=\"ow\">in</span> <span class=\"n\">tokenizer_registry</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Tokenizer: </span><span class=\"si\">{}</span><span class=\"s2\">\"</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">))</span>\n    <span class=\"n\">tokenizer</span><span class=\"p\">(</span><span class=\"s1\">'input_file.txt'</span><span class=\"p\">,</span> <span class=\"s1\">'output_file.txt'</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"id3\">\n<h3>\u5b89\u88c5</h3>\n<pre>pip install tokenizers_collection\n</pre>\n<div id=\"id4\">\n<h4>\u66f4\u65b0\u8bb8\u53ef\u6587\u4ef6\u4e0e\u4e0b\u8f7d\u6a21\u578b</h4>\n<p>\u56e0\u4e3a\u5176\u4e2d\u6709\u4e9b\u6a21\u578b\u9700\u8981\u66f4\u65b0\u8bb8\u53ef\u6587\u4ef6\uff08\u6bd4\u5982\uff1apynlpir\uff09\u6216\u8005\u9700\u8981\u4e0b\u8f7d\u6a21\u578b\u6587\u4ef6\uff08\u6bd4\u5982\uff1apyltp\uff09\uff0c\u56e0\u6b64\u5b89\u88c5\u540e\u9700\u8981\u6267\u884c\u7279\u5b9a\u7684\u547d\u4ee4\u5b8c\u6210\u64cd\u4f5c\uff0c\u8fd9\u91cc\u5df2\u7ecf\u5c06\u6240\u6709\u7684\u64cd\u4f5c\u5c01\u88c5\u6210\u4e86\u4e00\u4e2a\u51fd\u6570\uff0c\u53ea\u9700\u8981\u6267\u884c\u7c7b\u4f3c\u5982\u4e0b\u7684\u6307\u4ee4\u5373\u53ef</p>\n<pre>python -m tokenizers_collection.helper\n</pre>\n</div>\n</div>\n<div id=\"credits\">\n<h3>Credits</h3>\n<p>This package was created with <a href=\"https://github.com/audreyr/cookiecutter\" rel=\"nofollow\">Cookiecutter</a> and the <a href=\"https://github.com/audreyr/cookiecutter-pypackage\" rel=\"nofollow\">audreyr/cookiecutter-pypackage</a> project template.</p>\n</div>\n</div>\n<div id=\"history\">\n<h2>History</h2>\n<h2 id=\"id5\"><span class=\"section-subtitle\">0.1.0 (2018-08-28)</span></h2>\n<ul>\n<li>First release on PyPI.</li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 4215849, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "5b052a1c97d9cb1cdabb1302afda23bd", "sha256": "5635078fa7464de8e8f2422d7e2d5c04b5f221f8c46d183a73ccf58c5a65bf88"}, "downloads": -1, "filename": "tokenizers_collection-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "5b052a1c97d9cb1cdabb1302afda23bd", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 7638, "upload_time": "2018-08-28T16:31:31", "upload_time_iso_8601": "2018-08-28T16:31:31.372375Z", "url": "https://files.pythonhosted.org/packages/58/2e/be637297cae232cb44ebeb1d6916586d563d7b0120c27cd2f597d6b7060d/tokenizers_collection-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "815ed18733ba0e273ebfe116510c6642", "sha256": "4064b30f879b9e2648e8328667dd67315e095a3dc459988c086d93c6ad30d7d2"}, "downloads": -1, "filename": "tokenizers_collection-0.1.0.tar.gz", "has_sig": false, "md5_digest": "815ed18733ba0e273ebfe116510c6642", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11150, "upload_time": "2018-08-28T16:31:33", "upload_time_iso_8601": "2018-08-28T16:31:33.336525Z", "url": "https://files.pythonhosted.org/packages/c6/67/9d1bcf7e48c2abfe9f458bd3a79eb86a0029b9b4f9e13dafe5d4c51fee8d/tokenizers_collection-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "f19dfd8c152d88955aac2e14fd784044", "sha256": "2e2ef7e7ad287275c56feef66fddd934c097f82a3ab0720c375a97389cb63c88"}, "downloads": -1, "filename": "tokenizers_collection-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "f19dfd8c152d88955aac2e14fd784044", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 7670, "upload_time": "2018-08-28T16:47:40", "upload_time_iso_8601": "2018-08-28T16:47:40.167360Z", "url": "https://files.pythonhosted.org/packages/60/e5/f0c957658cac4b0562feaea5ebdb905d297a45a91ada0dcb0fd5cc9494a3/tokenizers_collection-0.1.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "00cdfdfbe8714af968d635029211a15e", "sha256": "b5fed4237c62691f7b1b378d11b20b948904418adc1612df6cecb78971854f38"}, "downloads": -1, "filename": "tokenizers_collection-0.1.1.tar.gz", "has_sig": false, "md5_digest": "00cdfdfbe8714af968d635029211a15e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11192, "upload_time": "2018-08-28T16:47:41", "upload_time_iso_8601": "2018-08-28T16:47:41.765174Z", "url": "https://files.pythonhosted.org/packages/76/df/08f9fb8fe1768ba0c6bbfc75e1859c7807de413859905c4ab8f8a24717b9/tokenizers_collection-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "fab7fe2976415933da7132badf12a8a8", "sha256": "40bf32f5352f0542a8c92c7bab73451f21c873676568ed9709acdfb88601dc03"}, "downloads": -1, "filename": "tokenizers_collection-0.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "fab7fe2976415933da7132badf12a8a8", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 7671, "upload_time": "2018-08-28T16:51:07", "upload_time_iso_8601": "2018-08-28T16:51:07.587130Z", "url": "https://files.pythonhosted.org/packages/f2/0a/2058b20bbaf939b8cfad4c7205af0853fbd6d75ff057c6e194f25f591dda/tokenizers_collection-0.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3569cce32dca6ee44e544e6a31b5bc30", "sha256": "b43e162333bf43e2ae80d567bc9dff48ed9ecfdffbfbf2fafc62035c52b39f9e"}, "downloads": -1, "filename": "tokenizers_collection-0.1.2.tar.gz", "has_sig": false, "md5_digest": "3569cce32dca6ee44e544e6a31b5bc30", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11192, "upload_time": "2018-08-28T16:51:09", "upload_time_iso_8601": "2018-08-28T16:51:09.572738Z", "url": "https://files.pythonhosted.org/packages/5f/13/524a0fae90c6254b9ccc62ec416ef5236e892b7e8455a8043c3cfc94e961/tokenizers_collection-0.1.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "fab7fe2976415933da7132badf12a8a8", "sha256": "40bf32f5352f0542a8c92c7bab73451f21c873676568ed9709acdfb88601dc03"}, "downloads": -1, "filename": "tokenizers_collection-0.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "fab7fe2976415933da7132badf12a8a8", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 7671, "upload_time": "2018-08-28T16:51:07", "upload_time_iso_8601": "2018-08-28T16:51:07.587130Z", "url": "https://files.pythonhosted.org/packages/f2/0a/2058b20bbaf939b8cfad4c7205af0853fbd6d75ff057c6e194f25f591dda/tokenizers_collection-0.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3569cce32dca6ee44e544e6a31b5bc30", "sha256": "b43e162333bf43e2ae80d567bc9dff48ed9ecfdffbfbf2fafc62035c52b39f9e"}, "downloads": -1, "filename": "tokenizers_collection-0.1.2.tar.gz", "has_sig": false, "md5_digest": "3569cce32dca6ee44e544e6a31b5bc30", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11192, "upload_time": "2018-08-28T16:51:09", "upload_time_iso_8601": "2018-08-28T16:51:09.572738Z", "url": "https://files.pythonhosted.org/packages/5f/13/524a0fae90c6254b9ccc62ec416ef5236e892b7e8455a8043c3cfc94e961/tokenizers_collection-0.1.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:51:25 2020"}