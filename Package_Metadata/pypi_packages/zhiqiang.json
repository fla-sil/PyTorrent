{"info": {"author": "Ming-Fan Li", "author_email": "li_m_f@163.com", "bugtrack_url": null, "classifiers": [], "description": "# ZhiQiang, \u4e4b\u5f3a\n\nzhiqiang, \u4e4b\u5f3a, become strong. And similar to ziqiang, \u81ea\u5f3a, Self-strengthening.\n\nA platform for reinforcement learning. The framework does not depend on any specific deep learning platform. But the implemented concrete agents are written with PyTorch.\n\n\n## Examples\n\nLearning curriculum of different agents for the environment GridWorld:\n\n<img src=\"https://github.com/Li-Ming-Fan/zhiqiang/blob/master/aaa_store/learning_curriculum.png\" width=\"50%\" height=\"50%\" alt=\"learning_curriculum\">\n\n\nA replay of a trained EntropyACV agent for GridWorld:\n\n<img src=\"https://github.com/Li-Ming-Fan/zhiqiang/blob/master/aaa_store/a_replay_gif.gif\" width=\"30%\" height=\"30%\" alt=\"gridworld_replay_gif\">\n\n\n## Description\n\nAbstract classes that form the framework:\n```\nfrom zhiqiang.agents import AbstractPQNet\nfrom zhiqiang.agents import AbstractAgent\nfrom zhiqiang.envs import AbstractEnv\nfrom zhiqiang.replay_buffers import AbstractBuffer\nfrom zhiqiang.trainers import AbstractTrainer\n```\n\nPlease run commands such as\n```\nAbstractPQNet.print_info()\nAbstractAgent.print_info()\n```\nto see necessary functions for implementing concrete classes.\n\n\nImplemented Trainers and Buffers:\n```\nfrom zhiqiang.trainers.simple_trainer import SimpleTrainer as Trainer\nfrom zhiqiang.trainers.paral_trainer import ParalTrainer as Trainer\nfrom zhiqiang.replay_buffers.simple_buffer import SimpleBuffer as Buffer\nfrom zhiqiang.replay_buffers.priority_buffer import PriorityBuffer as Buffer\n```\n\nSome of the implemented agents:\n```\nfrom zhiqiang.agents.dqn_vanila import VanilaDQN as Agent\nfrom zhiqiang.agents.dqn_double import DoubleDQN as Agent\nfrom zhiqiang.agents.dqn_mstep import MStepDQN as Agent\nfrom zhiqiang.agents.dqn_priority import PriorityDQN as Agent\n```\n\nMore:\n```\n.\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 agents\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 acq_entropy.py\n\u2502   \u251c\u2500\u2500 acv_entropy.py\n\u2502   \u251c\u2500\u2500 dqn_double.py\n\u2502   \u251c\u2500\u2500 dqn_mstep.py\n\u2502   \u251c\u2500\u2500 dqn_priority.py\n\u2502   \u251c\u2500\u2500 dqn_vanila.py\n\u2502   \u2514\u2500\u2500 policy_mstep.py\n\u251c\u2500\u2500 envs\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 replay_buffers\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 priority_buffer.py\n\u2502   \u2514\u2500\u2500 simple_buffer.py\n\u251c\u2500\u2500 trainers\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 paral_trainer.py\n\u2502   \u2514\u2500\u2500 simple_trainer.py\n\u2514\u2500\u2500 utils\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 basic_settings.py\n    \u251c\u2500\u2500 data_parallelism.py\n    \u251c\u2500\u2500 log_parser.py\n    \u251c\u2500\u2500 torch_utils.py\n    \u2514\u2500\u2500 uct_simple.py\n```\n\n## Quick Trial\n\nFor a quick trial, please try codes in the file examples/GridWorld/script_train_simple.py:\n\n```\n# define an env\nfrom grid_world import GridWorld as Env\n\n# define a qnet, in PyTorch\nfrom gridworld_qnet import GridWorldQNet as QNet\n\n# pick an agent\nfrom zhiqiang.agents.dqn_vanila import VanilaDQN as Agent\n# from zhiqiang.agents.dqn_double import DoubleDQN as Agent\n# from zhiqiang.agents.dqn_mstep import MStepDQN as Agent\n# from zhiqiang.agents.dqn_priority import PriorityDQN as Agent\n\n# pick a buffer\nfrom zhiqiang.replay_buffers.simple_buffer import SimpleBuffer as Buffer\n# from zhiqiang.replay_buffers.priority_buffer import PriorityBuffer as Buffer\n\n\n# pick a trainer\nfrom zhiqiang.trainers.simple_trainer import SimpleTrainer as Trainer\n# from zhiqiang.trainers.paral_trainer import ParalTrainer as Trainer\n\n# settings file, make sure the path is right\nsettings_filepath = \"./data_root/settings/settings_gridworld.json\"\nagent_name = \"agentname\"\nenv_name = \"GridWorld\"\n\n##\n#\nfrom zhiqiang.utils.basic_settings import BasicSettings\n#\nsettings = BasicSettings(settings_filepath)\nsettings.env = env_name\nsettings.agent = agent_name\nsettings.check_settings()\nsettings.display()\n#\n# device\nimport torch\nsettings.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\\n    if settings.device_type is None else torch.device(settings.device_type)\n#\nprint(\"device: {}\".format(settings.device))\n#\n# trainer\ntrainer = Trainer(settings, Agent, {\"qnet\": QNet}, Env, Buffer)\n#\n# train\nlist_aver_rewards = trainer.do_train()\n#\n# draw\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(8, 5))\n#\neval_period = settings.trainer_settings[\"eval_period\"]\nlist_x = [idx * eval_period for idx in range(len(list_aver_rewards))]\n#\nprint(list_x)\nprint(list_aver_rewards)\n#\nplt.plot(list_x, list_aver_rewards, label=\"Averaged Rewards\", color=\"r\", linewidth=2)\nplt.xlabel(\"Number Boost\")\nplt.ylabel(\"Averaged Rewards\")    # plt.title(\"Boost Curriculum\")\n# plt.xticks(list_x)              # plt.legend()\nplt.grid()\nplt.show()\n```\n\nFor utilization of more agents, please see codes in the file examples/GridWorld/script_train_all.py.\n\n\n## Philosophy\n\nThis package does not aim to encompass all kinds of reinforcement learning algorithms, but just to provide a framework for RL solutions of tasks.\n\nAn RL solution always involves an environment, an agent (agents) and some neural networks (as agent modules). For training the agent (agents), a trainer and a replay buffer are further required. If interface functions among these parts are well defined, then the different parts can be easy to change as plug-and-play. This is what this package aims to do.\n\nIn this package, a set of inferface functions is defined, and some simple implementations of the different parts are conducted. We hope these will pave way for users to make their own customized definitions and implementations. \n\n\n## Installation\n\nFrom PyPI distribution system:\n\n```\npip install zhiqiang\n```\n\nThis package is tested with PyTorch 1.4.0.\n\n\n## Usage\n\nFor usage examples of this package, please see:\n\n1, examples/GridWorld\n\n2, examples/Atari\n\n\n## Citation\n\nIf you find ZhiQiang helpful, please cite it in your publications.\n\n```\n@software{zhiqiang,\n  author = {Ming-Fan Li},\n  title = {ZhiQiang, a platform for reinforcement learning},\n  year = {2020},\n  url = {https://github.com/Li-Ming-Fan/zhiqiang}\n}\n```\n\n\n</br>", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Li-Ming-Fan/zhiqiang", "keywords": "", "license": "", "maintainer": "Ming-Fan Li", "maintainer_email": "li_m_f@163.com", "name": "zhiqiang", "package_url": "https://pypi.org/project/zhiqiang/", "platform": "any", "project_url": "https://pypi.org/project/zhiqiang/", "project_urls": {"Homepage": "https://github.com/Li-Ming-Fan/zhiqiang"}, "release_url": "https://pypi.org/project/zhiqiang/0.1.1/", "requires_dist": null, "requires_python": "", "summary": "A package for reinforcement learning algorithms.", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>ZhiQiang, \u4e4b\u5f3a</h1>\n<p>zhiqiang, \u4e4b\u5f3a, become strong. And similar to ziqiang, \u81ea\u5f3a, Self-strengthening.</p>\n<p>A platform for reinforcement learning. The framework does not depend on any specific deep learning platform. But the implemented concrete agents are written with PyTorch.</p>\n<h2>Examples</h2>\n<p>Learning curriculum of different agents for the environment GridWorld:</p>\n<img alt=\"learning_curriculum\" height=\"50%\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7220986be9386f2d954b2c3ef229cb195a1beb8e/68747470733a2f2f6769746875622e636f6d2f4c692d4d696e672d46616e2f7a68697169616e672f626c6f622f6d61737465722f6161615f73746f72652f6c6561726e696e675f637572726963756c756d2e706e67\" width=\"50%\">\n<p>A replay of a trained EntropyACV agent for GridWorld:</p>\n<img alt=\"gridworld_replay_gif\" height=\"30%\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d9dba3d0472c60e246e773fb1a7790a3e484372d/68747470733a2f2f6769746875622e636f6d2f4c692d4d696e672d46616e2f7a68697169616e672f626c6f622f6d61737465722f6161615f73746f72652f615f7265706c61795f6769662e676966\" width=\"30%\">\n<h2>Description</h2>\n<p>Abstract classes that form the framework:</p>\n<pre><code>from zhiqiang.agents import AbstractPQNet\nfrom zhiqiang.agents import AbstractAgent\nfrom zhiqiang.envs import AbstractEnv\nfrom zhiqiang.replay_buffers import AbstractBuffer\nfrom zhiqiang.trainers import AbstractTrainer\n</code></pre>\n<p>Please run commands such as</p>\n<pre><code>AbstractPQNet.print_info()\nAbstractAgent.print_info()\n</code></pre>\n<p>to see necessary functions for implementing concrete classes.</p>\n<p>Implemented Trainers and Buffers:</p>\n<pre><code>from zhiqiang.trainers.simple_trainer import SimpleTrainer as Trainer\nfrom zhiqiang.trainers.paral_trainer import ParalTrainer as Trainer\nfrom zhiqiang.replay_buffers.simple_buffer import SimpleBuffer as Buffer\nfrom zhiqiang.replay_buffers.priority_buffer import PriorityBuffer as Buffer\n</code></pre>\n<p>Some of the implemented agents:</p>\n<pre><code>from zhiqiang.agents.dqn_vanila import VanilaDQN as Agent\nfrom zhiqiang.agents.dqn_double import DoubleDQN as Agent\nfrom zhiqiang.agents.dqn_mstep import MStepDQN as Agent\nfrom zhiqiang.agents.dqn_priority import PriorityDQN as Agent\n</code></pre>\n<p>More:</p>\n<pre><code>.\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 agents\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 acq_entropy.py\n\u2502   \u251c\u2500\u2500 acv_entropy.py\n\u2502   \u251c\u2500\u2500 dqn_double.py\n\u2502   \u251c\u2500\u2500 dqn_mstep.py\n\u2502   \u251c\u2500\u2500 dqn_priority.py\n\u2502   \u251c\u2500\u2500 dqn_vanila.py\n\u2502   \u2514\u2500\u2500 policy_mstep.py\n\u251c\u2500\u2500 envs\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 replay_buffers\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 priority_buffer.py\n\u2502   \u2514\u2500\u2500 simple_buffer.py\n\u251c\u2500\u2500 trainers\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 paral_trainer.py\n\u2502   \u2514\u2500\u2500 simple_trainer.py\n\u2514\u2500\u2500 utils\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 basic_settings.py\n    \u251c\u2500\u2500 data_parallelism.py\n    \u251c\u2500\u2500 log_parser.py\n    \u251c\u2500\u2500 torch_utils.py\n    \u2514\u2500\u2500 uct_simple.py\n</code></pre>\n<h2>Quick Trial</h2>\n<p>For a quick trial, please try codes in the file examples/GridWorld/script_train_simple.py:</p>\n<pre><code># define an env\nfrom grid_world import GridWorld as Env\n\n# define a qnet, in PyTorch\nfrom gridworld_qnet import GridWorldQNet as QNet\n\n# pick an agent\nfrom zhiqiang.agents.dqn_vanila import VanilaDQN as Agent\n# from zhiqiang.agents.dqn_double import DoubleDQN as Agent\n# from zhiqiang.agents.dqn_mstep import MStepDQN as Agent\n# from zhiqiang.agents.dqn_priority import PriorityDQN as Agent\n\n# pick a buffer\nfrom zhiqiang.replay_buffers.simple_buffer import SimpleBuffer as Buffer\n# from zhiqiang.replay_buffers.priority_buffer import PriorityBuffer as Buffer\n\n\n# pick a trainer\nfrom zhiqiang.trainers.simple_trainer import SimpleTrainer as Trainer\n# from zhiqiang.trainers.paral_trainer import ParalTrainer as Trainer\n\n# settings file, make sure the path is right\nsettings_filepath = \"./data_root/settings/settings_gridworld.json\"\nagent_name = \"agentname\"\nenv_name = \"GridWorld\"\n\n##\n#\nfrom zhiqiang.utils.basic_settings import BasicSettings\n#\nsettings = BasicSettings(settings_filepath)\nsettings.env = env_name\nsettings.agent = agent_name\nsettings.check_settings()\nsettings.display()\n#\n# device\nimport torch\nsettings.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\\n    if settings.device_type is None else torch.device(settings.device_type)\n#\nprint(\"device: {}\".format(settings.device))\n#\n# trainer\ntrainer = Trainer(settings, Agent, {\"qnet\": QNet}, Env, Buffer)\n#\n# train\nlist_aver_rewards = trainer.do_train()\n#\n# draw\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(8, 5))\n#\neval_period = settings.trainer_settings[\"eval_period\"]\nlist_x = [idx * eval_period for idx in range(len(list_aver_rewards))]\n#\nprint(list_x)\nprint(list_aver_rewards)\n#\nplt.plot(list_x, list_aver_rewards, label=\"Averaged Rewards\", color=\"r\", linewidth=2)\nplt.xlabel(\"Number Boost\")\nplt.ylabel(\"Averaged Rewards\")    # plt.title(\"Boost Curriculum\")\n# plt.xticks(list_x)              # plt.legend()\nplt.grid()\nplt.show()\n</code></pre>\n<p>For utilization of more agents, please see codes in the file examples/GridWorld/script_train_all.py.</p>\n<h2>Philosophy</h2>\n<p>This package does not aim to encompass all kinds of reinforcement learning algorithms, but just to provide a framework for RL solutions of tasks.</p>\n<p>An RL solution always involves an environment, an agent (agents) and some neural networks (as agent modules). For training the agent (agents), a trainer and a replay buffer are further required. If interface functions among these parts are well defined, then the different parts can be easy to change as plug-and-play. This is what this package aims to do.</p>\n<p>In this package, a set of inferface functions is defined, and some simple implementations of the different parts are conducted. We hope these will pave way for users to make their own customized definitions and implementations.</p>\n<h2>Installation</h2>\n<p>From PyPI distribution system:</p>\n<pre><code>pip install zhiqiang\n</code></pre>\n<p>This package is tested with PyTorch 1.4.0.</p>\n<h2>Usage</h2>\n<p>For usage examples of this package, please see:</p>\n<p>1, examples/GridWorld</p>\n<p>2, examples/Atari</p>\n<h2>Citation</h2>\n<p>If you find ZhiQiang helpful, please cite it in your publications.</p>\n<pre><code>@software{zhiqiang,\n  author = {Ming-Fan Li},\n  title = {ZhiQiang, a platform for reinforcement learning},\n  year = {2020},\n  url = {https://github.com/Li-Ming-Fan/zhiqiang}\n}\n</code></pre>\n<br>\n\n          </div>"}, "last_serial": 7009612, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "949f53f8d44ac35855f77bb3075f5f2b", "sha256": "b35ce22421c93bf37b5a91c4a93f93c4cd15f2bf8c7e313c0d8b7986d65759f6"}, "downloads": -1, "filename": "zhiqiang-0.0.1.tar.gz", "has_sig": false, "md5_digest": "949f53f8d44ac35855f77bb3075f5f2b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2276, "upload_time": "2020-03-29T07:31:53", "upload_time_iso_8601": "2020-03-29T07:31:53.616218Z", "url": "https://files.pythonhosted.org/packages/11/3a/07088efae72e1f71ef16697c2f92c0e5d9e118a55616bc049a1d18cc8d21/zhiqiang-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "93af58151fbc3a31036e7f22ad463c48", "sha256": "488ee501550bd6115403852004e28b00851c0d63fbfd9ab7e85af3248493a4f4"}, "downloads": -1, "filename": "zhiqiang-0.0.2.tar.gz", "has_sig": false, "md5_digest": "93af58151fbc3a31036e7f22ad463c48", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6764, "upload_time": "2020-03-29T15:38:45", "upload_time_iso_8601": "2020-03-29T15:38:45.844147Z", "url": "https://files.pythonhosted.org/packages/60/d7/bc0abb1f9915b53ba326e1482c0efbb37229018dc7102ae4b2d4acd444e4/zhiqiang-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "3c8db59a75112629b68d0d74ff58cada", "sha256": "3a86979ec82337e2c8228924d560ff4b4a34a143bf10ff0b5b64bf8788c6e45e"}, "downloads": -1, "filename": "zhiqiang-0.0.3.tar.gz", "has_sig": false, "md5_digest": "3c8db59a75112629b68d0d74ff58cada", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13112, "upload_time": "2020-04-04T10:34:44", "upload_time_iso_8601": "2020-04-04T10:34:44.873809Z", "url": "https://files.pythonhosted.org/packages/48/36/b15e6ddbff6175b73173375894a4fd0bad6f6fe82ea736aecb67f82c7925/zhiqiang-0.0.3.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "f59cfd4cdbebe4044a71724edc24acb4", "sha256": "c25d691fa813bebb5fd84fa8fc743bb023dbf3fc278ff481339ec7b2ab8c2575"}, "downloads": -1, "filename": "zhiqiang-0.1.0.tar.gz", "has_sig": false, "md5_digest": "f59cfd4cdbebe4044a71724edc24acb4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21779, "upload_time": "2020-04-11T04:26:53", "upload_time_iso_8601": "2020-04-11T04:26:53.628372Z", "url": "https://files.pythonhosted.org/packages/87/86/5af5257eee4cfb3c829c1bba1eef8c5f9d9c0771e6423f7b3e3e262d98fc/zhiqiang-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "c546b7ca2aa9880179f4a18815f08203", "sha256": "60c969b9ecdbb74d92e3c5018c534c1f60bae8aaf1d1978ecf9fca4f2df7cdb4"}, "downloads": -1, "filename": "zhiqiang-0.1.1.tar.gz", "has_sig": false, "md5_digest": "c546b7ca2aa9880179f4a18815f08203", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23019, "upload_time": "2020-04-13T12:27:40", "upload_time_iso_8601": "2020-04-13T12:27:40.075991Z", "url": "https://files.pythonhosted.org/packages/5d/04/08a5112ac7a7ce73dc68c2b3a84ca02095cecf682ebb857a77d5f8be1b81/zhiqiang-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c546b7ca2aa9880179f4a18815f08203", "sha256": "60c969b9ecdbb74d92e3c5018c534c1f60bae8aaf1d1978ecf9fca4f2df7cdb4"}, "downloads": -1, "filename": "zhiqiang-0.1.1.tar.gz", "has_sig": false, "md5_digest": "c546b7ca2aa9880179f4a18815f08203", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23019, "upload_time": "2020-04-13T12:27:40", "upload_time_iso_8601": "2020-04-13T12:27:40.075991Z", "url": "https://files.pythonhosted.org/packages/5d/04/08a5112ac7a7ce73dc68c2b3a84ca02095cecf682ebb857a77d5f8be1b81/zhiqiang-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:16:48 2020"}