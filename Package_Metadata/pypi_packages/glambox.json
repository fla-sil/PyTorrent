{"info": {"author": "", "author_email": "", "bugtrack_url": null, "classifiers": [], "description": "# GLAMbox\n\nGLAMbox is a Python toolbox for investigating the association between gaze allocation and decision behaviour, and applying the Gaze-weighted Linear Accumulator Model ([Thomas, Molter et al., 2019](https://www.nature.com/articles/s41562-019-0584-8)).\n\nSee the [BioRxiv preprint](https://www.biorxiv.org/content/10.1101/741678v1) for detailed background, model description and example applications.\n\nA full documentation page of the toolbox is available at [https://glambox.readthedocs.io](https://glambox.readthedocs.io).\n\n## Installation\n\nGLAMbox is written for Python 3.7 and requires a working Python environment running on your computer. We recommend to install the [Anaconda Distribution](https://www.anaconda.com/distribution/) (available for all major platforms).\nWith the Python environment fully set up, the GLAMbox module can be installed from the command line using pip:\n\n```bash\npip install glambox\n```\n\nThis command also installs all of GLAMbox's dependencies, which are listed in the `requirements.txt`.\n\n## Quickstart\n\nFitting the GLAM to a dataset can be done in just a few lines of code:\n\n```py\nimport glambox as gb\nimport pandas as pd\n\n# load dataset (format must be GLAMbox compatible, of course)\ndata = pd.read_csv('data.csv')\n\n# create the GLAM model object\nmodel = gb.GLAM(data)\n\n# build the PyMC3 model\nmodel.make_model(kind='individual')\n\n# perform MCMC sampling\nmodel.fit()\n\n# inspect parameter estimates\nprint(model.estimates)\n\n# predict data using MAP estimates, save predictions\nmodel.predict()\nmodel.prediction.to_csv('prediction.csv')\n```\n\nA more detailed overview of the available functions can be found in the [Basic usage section](#basic-usage) and the [API Reference](https://glambox.readthedocs.io/en/latest/glambox.html).\n\n## Application Examples\n\nThis repository includes Jupyter notebooks with full code for three usage examples outlined in the [BioRxiv preprint](https://www.biorxiv.org/content/10.1101/741678v1). The notebook files for these examples can be found in the `examples` folder of this repository. They can be downloaded and run interactively with [Jupyter](https://www.jupyter.org) (also included in the Anaconda Python distribution). Fully rendered (non-interactive) html versions can be found on the [documentation page](https://glambox.readthedocs.io).\n\n### Example 1: Individual gaze biases\n\nIn this example, we demonstrate individual model fitting, model comparisons between model variants, and out-of-sample prediction. ([Jupyter](https://github.com/glamlab/glambox/blob/master/Example_1_Individual_estimation.ipynb), [html](https://glambox.readthedocs.io/en/latest/examples/Example_1_Individual_estimation.html))\n\n### Example 2: Hierarchical parameter estimation\n\nIn the second example, we demonstrate how to setup a hierarchical model with multiple groups, and compare parameter estimates between groups. ([Jupyter](https://github.com/glamlab/glambox/blob/master/Example_2_Hierarchical_estimation.ipynb), [html](https://glambox.readthedocs.io/en/latest/examples/Example_2_Hierarchical_estimation.html))\n\n### Example 3: Parameter Recovery\n\nIn the last example, we demonstrate how to perform a basic parameter recovery analyis for a given dataset, using GLAMbox. ([Jupyter](https://github.com/glamlab/glambox/blob/master/Example_3_Parameter_recovery.ipynb), [html](https://glambox.readthedocs.io/en/latest/examples/Example_3_Parameter_recovery.html))\n\n## Basic usage\n\n### Data format, the `GLAM` class\n\nThe core functionality of the GLAMbox is implemented in the `GLAM` model class. To apply the GLAM to data, an instance of the model class needs to be instantiated and supplied with the experimental data, first:\n\n```python\nimport glambox as gb\nglam = gb.GLAM(data=data)\n````\n\nThe data must be a pandas (McKinney, 2010) DataFrame with one row per trial, containing the following variable entries:\n\n- `subject`: Subject index (`int`, starting with 0)\n- `trial`: Trial index (`int`, starting with 0)\n- `choice`: Chosen item (`int`, items should be 0, 1, ..., N)\n- `rt`: Response time (`float`, in seconds)\n- for each item *i* in the choice set:\n    - `item_value_i`: The item value (`float`)\n    - `gaze_i`: The fraction of total time in this trial that the participant spent looking at this item (`float`, between 0 and 1)\n- additional variables coding groups or conditions (`str` or `int`)\n\nFor reference, the first two rows of a pandas DataFrame ready to be used with GLAMbox could look like this:\n\n| subject | trial | choice | rt   | item_value_0 | item_value_1 | item_value_2 | gaze_0 | gaze_1 | gaze_2 | speed  |\n|---------|-------|--------|------|--------------|--------------|--------------|--------|--------|--------|--------|\n| 0       | 0     | 0      | 2.41 | 6            | 4            | 3            | 0.56   | 0.22   | 0.22   | 'fast' |\n| 0       | 1     | 1      | 3.65 | 5            | 5            | 3            | 0.25   | 0.34   | 0.41   | 'slow' |\n\nNext, the respective PyMC3 model, which will later be used to estimate the model's parameters, can be built using the `make_model` method. Here, the researcher specifies the kind of the model: `'individual'` if the parameters should be estimated for each subject individually, `'hierarchical'` for hierarchical parameter estimation, or `'pooled'` to estimate a single parameter set for all subjects. At this stage, the researcher can also specify experimental parameter dependencies: For example, a parameter could be expected to vary between groups or conditions. In line with existing modeling toolboxes (e.g., Voss & Voss, 2007; Wiecki, Sofer, Frank, 2013) dependencies are defined using the `depends_on` argument. `depends_on` expects a dictionary with parameters as keys and experimental factors as values (e.g., `depends_on=dict(v='speed')` for factor `'speed'` with conditions `'fast'` and `'slow'` in the data). The toolbox internally handles within- and between subject designs and assigns parameters accordingly. If multiple conditions are given for a factor, one parameter will be designated for each condition. Finally, the `make_model` method allows parameters to be fixed to a specific value using the `*_val` arguments (e.g., `gamma_val=1` for a model without gaze bias). If parameters should be fixed for individual subjects, a list of individual values needs to be passed.\n\n```python\nmodel.make_model(kind='individual',\n                 depends_on=dict(v='speed'),\n                 gamma_val=1)\n```\n\n### Inference\n\nOnce the PyMC3 model is built, parameters can be estimated using the `fit` method:\n\n```python\nmodel.fit(method='MCMC',\n          tune=5000,\n          draws=5000)\n```\n\nThe `fit` method defaults to Metropolis Hastings Markov-Chain-Monte-Carlo (MCMC) sampling, but also allows for Variational Inference.\n\n### Accessing parameter estimates\n\nAfter parameter estimation is completed, the resulting estimates can be accessed with the `estimates` attribute of the GLAM model instance. This returns a table with one row for each set of parameter estimates for each individual and condition in the data. For each parameter, a maximum a posteriori (MAP) estimate is given, in addition to the 95\\% Highest-Posterior Density Interval (HPD). If the parameters were estimated hierarchically, the table also contains estimates of the group-level parameters. \n\n\n\n### Comparing parameters between groups or conditions\n\nParameter estimates can be compared between different experimental groups or conditions (specified with the `depends_on` keyword when calling `make_model`) using the `compare_parameters` function from the `analysis` module. It takes as input the fitted GLAM instance, a list of parameters (`'v'`, `'s'`, `'gamma'`, `'tau'`), and a list of pairwise comparisons between groups or conditions. The comparison argument expects a list of tuples (e.g., `[('group1', 'group2'), ('group1', 'group3')]`). For example, given a fitted model instance (here `glam`) a comparison of the $\\gamma$ parameter between two groups (`group1` and `group2`) can be computed as: \n\n```py\nfrom gb.analysis import compare_parameters\ncomparison = compare_parameters(model=glam, \n                                parameters=['gamma'],\n                                comparisons=[('group1', 'group2')])\n```\n\nThe function then returns a table with one row per specified comparison, and columns containing the mean posterior difference, percentage of the posterior above zero, and corresponding 95\\% HPD interval. If supplied with a hierarchical model, the function computes differences between group-level parameters. If an individual type model is given, it returns comparison statistics for each individual.\n\nComparisons can  be visualized using the `compare_parameters` function from the `plots` module. It takes the same input as its analogue in the `analysis` module. It plots posterior distributions of parameters and the posterior distributions of any differences specified using the `comparisons` argument. For a usage example and plot see [usage example 2](https://glambox.readthedocs.io/en/latest/examples/Example_2_Hierarchical_estimation.html) in the full documentation.\n\n### Comparing model variants\n\nModel comparisons between multiple GLAM variants (e.g., full and restricted variants) can be performed using the `compare_models` function, which wraps the function of the same name from the PyMC3 library. The `compare_models` function takes as input a list of fitted model instances that are to be compared. Additional keyword arguments can be given and are passed on to the underlying PyMC3 `compare` function. This allows the user, for example, to specify the information criterion used for the comparison via the `ic` argument (`'WAIC'` or `'LOO'` for Leave-One-Out cross validation). It returns a table containing an estimate of the specified information criterion, standard errors, difference to the best-fitting model, standard error of the difference, and other output variables from PyMC3 for each inputted model (and subject, if individually estimated models were given). We refer the reader to [usage example 1](https://glambox.readthedocs.io/en/latest/examples/Example_1_Individual_estimation.html) in the full documentation for the full code and exemplary output from the `compare_models` function.\n\n### Predicting choices and response times\n\nChoices and RTs can be predicted with the GLAM by the use of the `predict`method:\n\n```python\nmodel.predict(n_repeats=50)\n```\n\nFor each trial of the dataset that is attached to the model instance, this method predicts a choice and RT using the previously determined MAP parameter estimates. To obtain a stable estimate of the GLAM's predictions, as well as the noise contained within them, it is recommended to repeat every trial multiple times during the prediction. The number of trial repeats can be specified with the `n_repeats` argument. After the prediction is completed, the predicted data can be accessed with the `prediction` attribute of the model.\n\n# References\n- McKinney, W. (2010, June). Data structures for statistical computing in python. In Proceedings of the 9th Python in Science Conference (Vol. 445, pp. 51-56).\n- Thomas, A. W., Molter, F., Krajbich, I., Heekeren, H. R., & Mohr, P. N. (2019). Gaze bias differences capture individual choice behaviour. Nature human behaviour, 3(6), 625.\n- Voss, A., & Voss, J. (2007). Fast-dm: A free program for efficient diffusion model analysis. Behavior Research Methods, 39(4), 767-775.\n- Wiecki, T. V., Sofer, I., & Frank, M. J. (2013). HDDM: Hierarchical Bayesian estimation of the drift-diffusion model in Python. Frontiers in neuroinformatics, 7, 14.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/glamlab/glambox/archive/0.0.1.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/glamlab/glambox", "keywords": "", "license": "MIT", "maintainer": "Felix Molter <felixmolter@gmail.com>, Armin W. Thomas <athms.research@gmail.com>", "maintainer_email": "glambox.berlin@gmail.com", "name": "glambox", "package_url": "https://pypi.org/project/glambox/", "platform": "", "project_url": "https://pypi.org/project/glambox/", "project_urls": {"Download": "https://github.com/glamlab/glambox/archive/0.0.1.tar.gz", "Homepage": "http://github.com/glamlab/glambox"}, "release_url": "https://pypi.org/project/glambox/0.0.1/", "requires_dist": ["matplotlib (==3.1.1)", "numpy (==1.17.3)", "pandas (==0.25.2)", "pymc3 (==3.7)", "scipy (==1.3.1)", "seaborn (==0.9.0)", "statsmodels (==0.10.1)", "Theano (==1.0.4)", "tqdm (==4.36.1)"], "requires_python": "", "summary": "GLAMbox: A toolbox to fit the Gaze-weighted Linear Accumulator Model", "version": "0.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>GLAMbox</h1>\n<p>GLAMbox is a Python toolbox for investigating the association between gaze allocation and decision behaviour, and applying the Gaze-weighted Linear Accumulator Model (<a href=\"https://www.nature.com/articles/s41562-019-0584-8\" rel=\"nofollow\">Thomas, Molter et al., 2019</a>).</p>\n<p>See the <a href=\"https://www.biorxiv.org/content/10.1101/741678v1\" rel=\"nofollow\">BioRxiv preprint</a> for detailed background, model description and example applications.</p>\n<p>A full documentation page of the toolbox is available at <a href=\"https://glambox.readthedocs.io\" rel=\"nofollow\">https://glambox.readthedocs.io</a>.</p>\n<h2>Installation</h2>\n<p>GLAMbox is written for Python 3.7 and requires a working Python environment running on your computer. We recommend to install the <a href=\"https://www.anaconda.com/distribution/\" rel=\"nofollow\">Anaconda Distribution</a> (available for all major platforms).\nWith the Python environment fully set up, the GLAMbox module can be installed from the command line using pip:</p>\n<pre>pip install glambox\n</pre>\n<p>This command also installs all of GLAMbox's dependencies, which are listed in the <code>requirements.txt</code>.</p>\n<h2>Quickstart</h2>\n<p>Fitting the GLAM to a dataset can be done in just a few lines of code:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">glambox</span> <span class=\"k\">as</span> <span class=\"nn\">gb</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n\n<span class=\"c1\"># load dataset (format must be GLAMbox compatible, of course)</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s1\">'data.csv'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># create the GLAM model object</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">gb</span><span class=\"o\">.</span><span class=\"n\">GLAM</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># build the PyMC3 model</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">make_model</span><span class=\"p\">(</span><span class=\"n\">kind</span><span class=\"o\">=</span><span class=\"s1\">'individual'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># perform MCMC sampling</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># inspect parameter estimates</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">estimates</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># predict data using MAP estimates, save predictions</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">()</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">prediction</span><span class=\"o\">.</span><span class=\"n\">to_csv</span><span class=\"p\">(</span><span class=\"s1\">'prediction.csv'</span><span class=\"p\">)</span>\n</pre>\n<p>A more detailed overview of the available functions can be found in the <a href=\"#basic-usage\" rel=\"nofollow\">Basic usage section</a> and the <a href=\"https://glambox.readthedocs.io/en/latest/glambox.html\" rel=\"nofollow\">API Reference</a>.</p>\n<h2>Application Examples</h2>\n<p>This repository includes Jupyter notebooks with full code for three usage examples outlined in the <a href=\"https://www.biorxiv.org/content/10.1101/741678v1\" rel=\"nofollow\">BioRxiv preprint</a>. The notebook files for these examples can be found in the <code>examples</code> folder of this repository. They can be downloaded and run interactively with <a href=\"https://www.jupyter.org\" rel=\"nofollow\">Jupyter</a> (also included in the Anaconda Python distribution). Fully rendered (non-interactive) html versions can be found on the <a href=\"https://glambox.readthedocs.io\" rel=\"nofollow\">documentation page</a>.</p>\n<h3>Example 1: Individual gaze biases</h3>\n<p>In this example, we demonstrate individual model fitting, model comparisons between model variants, and out-of-sample prediction. (<a href=\"https://github.com/glamlab/glambox/blob/master/Example_1_Individual_estimation.ipynb\" rel=\"nofollow\">Jupyter</a>, <a href=\"https://glambox.readthedocs.io/en/latest/examples/Example_1_Individual_estimation.html\" rel=\"nofollow\">html</a>)</p>\n<h3>Example 2: Hierarchical parameter estimation</h3>\n<p>In the second example, we demonstrate how to setup a hierarchical model with multiple groups, and compare parameter estimates between groups. (<a href=\"https://github.com/glamlab/glambox/blob/master/Example_2_Hierarchical_estimation.ipynb\" rel=\"nofollow\">Jupyter</a>, <a href=\"https://glambox.readthedocs.io/en/latest/examples/Example_2_Hierarchical_estimation.html\" rel=\"nofollow\">html</a>)</p>\n<h3>Example 3: Parameter Recovery</h3>\n<p>In the last example, we demonstrate how to perform a basic parameter recovery analyis for a given dataset, using GLAMbox. (<a href=\"https://github.com/glamlab/glambox/blob/master/Example_3_Parameter_recovery.ipynb\" rel=\"nofollow\">Jupyter</a>, <a href=\"https://glambox.readthedocs.io/en/latest/examples/Example_3_Parameter_recovery.html\" rel=\"nofollow\">html</a>)</p>\n<h2>Basic usage</h2>\n<h3>Data format, the <code>GLAM</code> class</h3>\n<p>The core functionality of the GLAMbox is implemented in the <code>GLAM</code> model class. To apply the GLAM to data, an instance of the model class needs to be instantiated and supplied with the experimental data, first:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">glambox</span> <span class=\"k\">as</span> <span class=\"nn\">gb</span>\n<span class=\"n\">glam</span> <span class=\"o\">=</span> <span class=\"n\">gb</span><span class=\"o\">.</span><span class=\"n\">GLAM</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"p\">)</span>\n</pre>\n<p>The data must be a pandas (McKinney, 2010) DataFrame with one row per trial, containing the following variable entries:</p>\n<ul>\n<li><code>subject</code>: Subject index (<code>int</code>, starting with 0)</li>\n<li><code>trial</code>: Trial index (<code>int</code>, starting with 0)</li>\n<li><code>choice</code>: Chosen item (<code>int</code>, items should be 0, 1, ..., N)</li>\n<li><code>rt</code>: Response time (<code>float</code>, in seconds)</li>\n<li>for each item <em>i</em> in the choice set:\n<ul>\n<li><code>item_value_i</code>: The item value (<code>float</code>)</li>\n<li><code>gaze_i</code>: The fraction of total time in this trial that the participant spent looking at this item (<code>float</code>, between 0 and 1)</li>\n</ul>\n</li>\n<li>additional variables coding groups or conditions (<code>str</code> or <code>int</code>)</li>\n</ul>\n<p>For reference, the first two rows of a pandas DataFrame ready to be used with GLAMbox could look like this:</p>\n<table>\n<thead>\n<tr>\n<th>subject</th>\n<th>trial</th>\n<th>choice</th>\n<th>rt</th>\n<th>item_value_0</th>\n<th>item_value_1</th>\n<th>item_value_2</th>\n<th>gaze_0</th>\n<th>gaze_1</th>\n<th>gaze_2</th>\n<th>speed</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>2.41</td>\n<td>6</td>\n<td>4</td>\n<td>3</td>\n<td>0.56</td>\n<td>0.22</td>\n<td>0.22</td>\n<td>'fast'</td>\n</tr>\n<tr>\n<td>0</td>\n<td>1</td>\n<td>1</td>\n<td>3.65</td>\n<td>5</td>\n<td>5</td>\n<td>3</td>\n<td>0.25</td>\n<td>0.34</td>\n<td>0.41</td>\n<td>'slow'</td>\n</tr></tbody></table>\n<p>Next, the respective PyMC3 model, which will later be used to estimate the model's parameters, can be built using the <code>make_model</code> method. Here, the researcher specifies the kind of the model: <code>'individual'</code> if the parameters should be estimated for each subject individually, <code>'hierarchical'</code> for hierarchical parameter estimation, or <code>'pooled'</code> to estimate a single parameter set for all subjects. At this stage, the researcher can also specify experimental parameter dependencies: For example, a parameter could be expected to vary between groups or conditions. In line with existing modeling toolboxes (e.g., Voss &amp; Voss, 2007; Wiecki, Sofer, Frank, 2013) dependencies are defined using the <code>depends_on</code> argument. <code>depends_on</code> expects a dictionary with parameters as keys and experimental factors as values (e.g., <code>depends_on=dict(v='speed')</code> for factor <code>'speed'</code> with conditions <code>'fast'</code> and <code>'slow'</code> in the data). The toolbox internally handles within- and between subject designs and assigns parameters accordingly. If multiple conditions are given for a factor, one parameter will be designated for each condition. Finally, the <code>make_model</code> method allows parameters to be fixed to a specific value using the <code>*_val</code> arguments (e.g., <code>gamma_val=1</code> for a model without gaze bias). If parameters should be fixed for individual subjects, a list of individual values needs to be passed.</p>\n<pre><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">make_model</span><span class=\"p\">(</span><span class=\"n\">kind</span><span class=\"o\">=</span><span class=\"s1\">'individual'</span><span class=\"p\">,</span>\n                 <span class=\"n\">depends_on</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"o\">=</span><span class=\"s1\">'speed'</span><span class=\"p\">),</span>\n                 <span class=\"n\">gamma_val</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n</pre>\n<h3>Inference</h3>\n<p>Once the PyMC3 model is built, parameters can be estimated using the <code>fit</code> method:</p>\n<pre><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">method</span><span class=\"o\">=</span><span class=\"s1\">'MCMC'</span><span class=\"p\">,</span>\n          <span class=\"n\">tune</span><span class=\"o\">=</span><span class=\"mi\">5000</span><span class=\"p\">,</span>\n          <span class=\"n\">draws</span><span class=\"o\">=</span><span class=\"mi\">5000</span><span class=\"p\">)</span>\n</pre>\n<p>The <code>fit</code> method defaults to Metropolis Hastings Markov-Chain-Monte-Carlo (MCMC) sampling, but also allows for Variational Inference.</p>\n<h3>Accessing parameter estimates</h3>\n<p>After parameter estimation is completed, the resulting estimates can be accessed with the <code>estimates</code> attribute of the GLAM model instance. This returns a table with one row for each set of parameter estimates for each individual and condition in the data. For each parameter, a maximum a posteriori (MAP) estimate is given, in addition to the 95% Highest-Posterior Density Interval (HPD). If the parameters were estimated hierarchically, the table also contains estimates of the group-level parameters.</p>\n<h3>Comparing parameters between groups or conditions</h3>\n<p>Parameter estimates can be compared between different experimental groups or conditions (specified with the <code>depends_on</code> keyword when calling <code>make_model</code>) using the <code>compare_parameters</code> function from the <code>analysis</code> module. It takes as input the fitted GLAM instance, a list of parameters (<code>'v'</code>, <code>'s'</code>, <code>'gamma'</code>, <code>'tau'</code>), and a list of pairwise comparisons between groups or conditions. The comparison argument expects a list of tuples (e.g., <code>[('group1', 'group2'), ('group1', 'group3')]</code>). For example, given a fitted model instance (here <code>glam</code>) a comparison of the $\\gamma$ parameter between two groups (<code>group1</code> and <code>group2</code>) can be computed as:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">gb.analysis</span> <span class=\"kn\">import</span> <span class=\"n\">compare_parameters</span>\n<span class=\"n\">comparison</span> <span class=\"o\">=</span> <span class=\"n\">compare_parameters</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">glam</span><span class=\"p\">,</span> \n                                <span class=\"n\">parameters</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'gamma'</span><span class=\"p\">],</span>\n                                <span class=\"n\">comparisons</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">'group1'</span><span class=\"p\">,</span> <span class=\"s1\">'group2'</span><span class=\"p\">)])</span>\n</pre>\n<p>The function then returns a table with one row per specified comparison, and columns containing the mean posterior difference, percentage of the posterior above zero, and corresponding 95% HPD interval. If supplied with a hierarchical model, the function computes differences between group-level parameters. If an individual type model is given, it returns comparison statistics for each individual.</p>\n<p>Comparisons can  be visualized using the <code>compare_parameters</code> function from the <code>plots</code> module. It takes the same input as its analogue in the <code>analysis</code> module. It plots posterior distributions of parameters and the posterior distributions of any differences specified using the <code>comparisons</code> argument. For a usage example and plot see <a href=\"https://glambox.readthedocs.io/en/latest/examples/Example_2_Hierarchical_estimation.html\" rel=\"nofollow\">usage example 2</a> in the full documentation.</p>\n<h3>Comparing model variants</h3>\n<p>Model comparisons between multiple GLAM variants (e.g., full and restricted variants) can be performed using the <code>compare_models</code> function, which wraps the function of the same name from the PyMC3 library. The <code>compare_models</code> function takes as input a list of fitted model instances that are to be compared. Additional keyword arguments can be given and are passed on to the underlying PyMC3 <code>compare</code> function. This allows the user, for example, to specify the information criterion used for the comparison via the <code>ic</code> argument (<code>'WAIC'</code> or <code>'LOO'</code> for Leave-One-Out cross validation). It returns a table containing an estimate of the specified information criterion, standard errors, difference to the best-fitting model, standard error of the difference, and other output variables from PyMC3 for each inputted model (and subject, if individually estimated models were given). We refer the reader to <a href=\"https://glambox.readthedocs.io/en/latest/examples/Example_1_Individual_estimation.html\" rel=\"nofollow\">usage example 1</a> in the full documentation for the full code and exemplary output from the <code>compare_models</code> function.</p>\n<h3>Predicting choices and response times</h3>\n<p>Choices and RTs can be predicted with the GLAM by the use of the <code>predict</code>method:</p>\n<pre><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">n_repeats</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">)</span>\n</pre>\n<p>For each trial of the dataset that is attached to the model instance, this method predicts a choice and RT using the previously determined MAP parameter estimates. To obtain a stable estimate of the GLAM's predictions, as well as the noise contained within them, it is recommended to repeat every trial multiple times during the prediction. The number of trial repeats can be specified with the <code>n_repeats</code> argument. After the prediction is completed, the predicted data can be accessed with the <code>prediction</code> attribute of the model.</p>\n<h1>References</h1>\n<ul>\n<li>McKinney, W. (2010, June). Data structures for statistical computing in python. In Proceedings of the 9th Python in Science Conference (Vol. 445, pp. 51-56).</li>\n<li>Thomas, A. W., Molter, F., Krajbich, I., Heekeren, H. R., &amp; Mohr, P. N. (2019). Gaze bias differences capture individual choice behaviour. Nature human behaviour, 3(6), 625.</li>\n<li>Voss, A., &amp; Voss, J. (2007). Fast-dm: A free program for efficient diffusion model analysis. Behavior Research Methods, 39(4), 767-775.</li>\n<li>Wiecki, T. V., Sofer, I., &amp; Frank, M. J. (2013). HDDM: Hierarchical Bayesian estimation of the drift-diffusion model in Python. Frontiers in neuroinformatics, 7, 14.</li>\n</ul>\n\n          </div>"}, "last_serial": 6092946, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "8aacde80e954a4a1bd511f9e6ac40523", "sha256": "f9900a6a8800d31ed1499116adecf0b0b46ca667d6e6dce789cc80f6043b9b4a"}, "downloads": -1, "filename": "glambox-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "8aacde80e954a4a1bd511f9e6ac40523", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 44172, "upload_time": "2019-11-07T12:07:32", "upload_time_iso_8601": "2019-11-07T12:07:32.090388Z", "url": "https://files.pythonhosted.org/packages/e4/56/32dc1af85dac8808cf2fe57a1151e4901067ad23e82dc4aad67e0352564f/glambox-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a59483f83b73bde3aa1cbc241191e298", "sha256": "37ffc8579b07e04bb5dda3977a57852430200bc5fae2bccfaeaeb144045fcc60"}, "downloads": -1, "filename": "glambox-0.0.1.tar.gz", "has_sig": false, "md5_digest": "a59483f83b73bde3aa1cbc241191e298", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 41524, "upload_time": "2019-11-07T12:07:34", "upload_time_iso_8601": "2019-11-07T12:07:34.270850Z", "url": "https://files.pythonhosted.org/packages/e0/1c/91bb05d0b8d62675c915cd5c7c861aa79e50d9bbed7f6cc99b2976007962/glambox-0.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "8aacde80e954a4a1bd511f9e6ac40523", "sha256": "f9900a6a8800d31ed1499116adecf0b0b46ca667d6e6dce789cc80f6043b9b4a"}, "downloads": -1, "filename": "glambox-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "8aacde80e954a4a1bd511f9e6ac40523", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 44172, "upload_time": "2019-11-07T12:07:32", "upload_time_iso_8601": "2019-11-07T12:07:32.090388Z", "url": "https://files.pythonhosted.org/packages/e4/56/32dc1af85dac8808cf2fe57a1151e4901067ad23e82dc4aad67e0352564f/glambox-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a59483f83b73bde3aa1cbc241191e298", "sha256": "37ffc8579b07e04bb5dda3977a57852430200bc5fae2bccfaeaeb144045fcc60"}, "downloads": -1, "filename": "glambox-0.0.1.tar.gz", "has_sig": false, "md5_digest": "a59483f83b73bde3aa1cbc241191e298", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 41524, "upload_time": "2019-11-07T12:07:34", "upload_time_iso_8601": "2019-11-07T12:07:34.270850Z", "url": "https://files.pythonhosted.org/packages/e0/1c/91bb05d0b8d62675c915cd5c7c861aa79e50d9bbed7f6cc99b2976007962/glambox-0.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:56:40 2020"}