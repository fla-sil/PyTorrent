{"info": {"author": "ZhangDun", "author_email": "dunnzhang0@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: MacOS", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX", "Operating System :: Unix", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "KnowledgeDistillation\n======================\n\nUpdate\n------------\n**March, 2020**\n\n- Now, users could define their own loss functions. The requirement of loss function can be found in API document.\n\n- Add more built-in loss functions (**mse_with_mask** and **attention_mse_with_mask**).\n\n- Unify hidden loss and predict loss, the key \"type\" is removed from distill_config.\n\n- Now, the device information is removed from loss value.\n\nIntroduction\n------------\n\nWhat is knowledge distillation?\n:::::::::::::::::::::::::::::::::::::::::\n**Knowledge Distillation** is model compression method in which a small model is trained \nto mimic a pre-trained, larger model (or ensemble of models). Recently, many models have achieved SOTA performance.\nHowever, their billions of parameters make it computationally expensive and inefficient considering both memory \nconsumption and high latency. Hence, it is necessary to get a small model from a large model by using knowledge \ndistillation.\n\nKnowledgeDistillation's training setting is sometimes referred to as \"teacher-student\", \nwhere the large model is the teacher and the small model is the student.\nThe method was first proposed by `Bucila <https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf>`_\nand generalized by `Hinton <https://arxiv.org/abs/1503.02531>`_.\n\nIntroduction of KnowledgeDistillation Package\n:::::::::::::::::::::::::::::::::::::::::::::::\n**KnowledgeDistillation**  is a general knowledge distillation framework. You can distill your own model\nby using this toolkit. Our framework is highly abstract and you can achieve many distillation methods by using this framework.\nBesides, we also provide a distillation of MultiLayerBasedModel considering many models are multi layers.\n\nUsage\n--------\n\nTo use the package, you should define these objects:\n\n* **Teacher Model** (large model, trained)\n* **Student Model** (small model, untrained)\n* **Data loader**, a generator or iterator to get training data or dev data. For example, torch.utils.data.DataLoader\n* **Train data adaptor**, a function that turn batch_data (from train_dataloader) to the inputs of teacher_model and student_model\n* **Distill config**, a list-object, each item indicates how to calculate loss. It also defines which output of which layer to calculate loss.\n* **Output adaptor**, a function that turn your model's output to dict-object output which meet distiller's requirements\n* **Evaluator**, a class with evaluate function, it define when and how to save your student model\n\n\nInstallation\n---------------\nRequirements\n::::::::::::::::::\n- Python >= 3.6\n- PyTorch >= 1.1.0\n- NumPy\n- Transformers >= 2.0 (optional, used by some examples)\n\nInstall from PyPI\n::::::::::::::::::\n\n**KnowledgeDistillation**  is currently available on the PyPi's repository and you can\ninstall it via pip::\n\n pip install -U KnowledgeDistillation\n\nInstall from the Github\n::::::::::::::::::::::::::::::\nIf you prefer, you can clone it and run the setup.py file. Use the following\ncommand to get a copy from GitHub::\n\n git clone https://github.com/DunZhang/KnowledgeDistillation.git\n\nTODO List\n-------------\n* Add multi teacher model distiller\n* Use input mask when computing loss\n* Support custom loss functions\n\nA simple example\n----------------\nA simple example::\n\n    import torch\n    import logging\n    import numpy as np\n    from transformers import BertModel, BertConfig\n    from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n\n    from knowledge_distillation import KnowledgeDistiller, MultiLayerBasedDistillationLoss\n    from knowledge_distillation import MultiLayerBasedDistillationEvaluator\n\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    # Some global variables\n    train_batch_size = 16\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    learning_rate = 2e-5\n    num_epoch = 10\n    # Teacher Model\n    bert_config = BertConfig(num_hidden_layers=12, output_hidden_states=True, output_attentions=True)\n    teacher_model = BertModel(bert_config)\n    # Student Model\n    bert_config = BertConfig(num_hidden_layers=3, output_hidden_states=True, output_attentions=True)\n    student_model = BertModel(bert_config)\n\n    ### Train data loader\n    input_ids = torch.LongTensor(np.random.randint(100, 1000, (100000, 64)))\n    attention_mask = torch.LongTensor(np.ones((100000, 64)))\n    token_type_ids = torch.LongTensor(np.zeros((100000, 64)))\n    train_data = TensorDataset(input_ids, attention_mask, token_type_ids)\n    train_sampler = RandomSampler(train_data)\n    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n\n\n    ### Train data adaptor\n    ### It is a function that turn batch_data (from train_dataloader) to the inputs of teacher_model and student_model\n    ### You can define your own train_data_adaptor. Remember the input must be device and batch_data.\n    ###  The output is either dict or tuple, but must consistent with you model's input\n    def train_data_adaptor(device, batch_data):\n        batch_data = tuple(t.to(device) for t in batch_data)\n        batch_data_dict = {\"input_ids\": batch_data[0],\n                           \"attention_mask\": batch_data[1],\n                           \"token_type_ids\": batch_data[2], }\n        # In this case, the teacher and student use the same input\n        return batch_data_dict, batch_data_dict\n\n\n    ### The loss model is the key for this generation.\n    ### We have already provided a general loss model for distilling multi bert layer\n    ### In most cases, you can directly use this model.\n    #### First, we should define a distill_config which indicates how to compute ths loss between teacher and student.\n    #### distill_config is a list-object, each item indicates how to calculate loss.\n    #### It also defines which output of which layer to calculate loss.\n    #### type \"ts_distill\" means that we compute loss between teacher and student\n    #### type \"hard_distill\" means that we compute loss between student output and ground truth\n    #### loss_function can be mse, cross_entropy or cos. Args is extra parameters in this loss_function\n    #### loss_function(x,y,**args)\n    distill_config = [\n        {\"type\": \"ts_distill\",\n         \"teacher_layer_name\": \"embedding_layer\", \"teacher_layer_output_name\": \"embedding\",\n         \"student_layer_name\": \"embedding_layer\", \"student_layer_output_name\": \"embedding\",\n         \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n         },\n        {\"type\": \"ts_distill\",\n         \"teacher_layer_name\": \"bert_layer4\", \"teacher_layer_output_name\": \"hidden_states\",\n         \"student_layer_name\": \"bert_layer1\", \"student_layer_output_name\": \"hidden_states\",\n         \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n         },\n        {\"type\": \"ts_distill\",\n         \"teacher_layer_name\": \"bert_layer4\", \"teacher_layer_output_name\": \"attention\",\n         \"student_layer_name\": \"bert_layer1\", \"student_layer_output_name\": \"attention\",\n         \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n         },\n        {\"type\": \"ts_distill\",\n         \"teacher_layer_name\": \"bert_layer8\", \"teacher_layer_output_name\": \"hidden_states\",\n         \"student_layer_name\": \"bert_layer2\", \"student_layer_output_name\": \"hidden_states\",\n         \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n         },\n        {\"type\": \"ts_distill\",\n         \"teacher_layer_name\": \"bert_layer8\", \"teacher_layer_output_name\": \"attention\",\n         \"student_layer_name\": \"bert_layer2\", \"student_layer_output_name\": \"attention\",\n         \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n         },\n        {\"type\": \"ts_distill\",\n         \"teacher_layer_name\": \"bert_layer12\", \"teacher_layer_output_name\": \"hidden_states\",\n         \"student_layer_name\": \"bert_layer3\", \"student_layer_output_name\": \"hidden_states\",\n         \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n         },\n        {\"type\": \"ts_distill\",\n         \"teacher_layer_name\": \"bert_layer12\", \"teacher_layer_output_name\": \"attention\",\n         \"student_layer_name\": \"bert_layer3\", \"student_layer_output_name\": \"attention\",\n         \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n         },\n    ]\n\n    ### teacher_output_adaptor and student_output_adaptor\n    ### In most cases, model's output is tuple-object, However, in our package, we need the output is dict-object,\n    ### like: { \"layer_name\":{\"output_name\":value} .... }\n    ### Hence, the output adaptor is to turn your model's output to dict-object output\n    ### In my case, teacher and student can use one adaptor\n    def output_adaptor(model_output):\n        last_hidden_state, pooler_output, hidden_states, attentions = model_output\n        output = {\"embedding_layer\": {\"embedding\": hidden_states[0]}}\n        for idx in range(len(attentions)):\n            output[\"bert_layer\" + str(idx + 1)] = {\"hidden_states\": hidden_states[idx + 1],\n                                                   \"attention\": attentions[idx]}\n        return output\n\n\n    # loss_model\n    loss_model = MultiLayerBasedDistillationLoss(distill_config=distill_config,\n                                                 teacher_output_adaptor=output_adaptor,\n                                                 student_output_adaptor=output_adaptor)\n    # optimizer\n    param_optimizer = list(student_model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = torch.optim.Adam(params=optimizer_grouped_parameters, lr=learning_rate)\n    # evaluator\n    evaluator = MultiLayerBasedDistillationEvaluator(save_dir=None, save_step=None, print_loss_step=20)\n    # Get a KnowledgeDistiller\n    distiller = KnowledgeDistiller(teacher_model=teacher_model, student_model=student_model,\n                                   train_dataloader=train_dataloader, dev_dataloader=None,\n                                   train_data_adaptor=train_data_adaptor, dev_data_adaptor=None,\n                                   device=device, loss_model=loss_model, optimizer=optimizer,\n                                   evaluator=evaluator, num_epoch=num_epoch)\n    # start distillate\n    distiller.distillate()\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/DunZhang/KnowledgeDistillation", "keywords": "Transformer Networks BERT XLNet PyTorch NLP deep learning", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "KnowledgeDistillation", "package_url": "https://pypi.org/project/KnowledgeDistillation/", "platform": "", "project_url": "https://pypi.org/project/KnowledgeDistillation/", "project_urls": {"Homepage": "https://github.com/DunZhang/KnowledgeDistillation"}, "release_url": "https://pypi.org/project/KnowledgeDistillation/1.0.2/", "requires_dist": null, "requires_python": "", "summary": "A general knowledge distillation framework", "version": "1.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"update\">\n<h2>Update</h2>\n<p><strong>March, 2020</strong></p>\n<ul>\n<li>Now, users could define their own loss functions. The requirement of loss function can be found in API document.</li>\n<li>Add more built-in loss functions (<strong>mse_with_mask</strong> and <strong>attention_mse_with_mask</strong>).</li>\n<li>Unify hidden loss and predict loss, the key \u201ctype\u201d is removed from distill_config.</li>\n<li>Now, the device information is removed from loss value.</li>\n</ul>\n</div>\n<div id=\"introduction\">\n<h2>Introduction</h2>\n<div id=\"what-is-knowledge-distillation\">\n<h3>What is knowledge distillation?</h3>\n<p><strong>Knowledge Distillation</strong> is model compression method in which a small model is trained\nto mimic a pre-trained, larger model (or ensemble of models). Recently, many models have achieved SOTA performance.\nHowever, their billions of parameters make it computationally expensive and inefficient considering both memory\nconsumption and high latency. Hence, it is necessary to get a small model from a large model by using knowledge\ndistillation.</p>\n<p>KnowledgeDistillation\u2019s training setting is sometimes referred to as \u201cteacher-student\u201d,\nwhere the large model is the teacher and the small model is the student.\nThe method was first proposed by <a href=\"https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf\" rel=\"nofollow\">Bucila</a>\nand generalized by <a href=\"https://arxiv.org/abs/1503.02531\" rel=\"nofollow\">Hinton</a>.</p>\n</div>\n<div id=\"introduction-of-knowledgedistillation-package\">\n<h3>Introduction of KnowledgeDistillation Package</h3>\n<p><strong>KnowledgeDistillation</strong>  is a general knowledge distillation framework. You can distill your own model\nby using this toolkit. Our framework is highly abstract and you can achieve many distillation methods by using this framework.\nBesides, we also provide a distillation of MultiLayerBasedModel considering many models are multi layers.</p>\n</div>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>To use the package, you should define these objects:</p>\n<ul>\n<li><strong>Teacher Model</strong> (large model, trained)</li>\n<li><strong>Student Model</strong> (small model, untrained)</li>\n<li><strong>Data loader</strong>, a generator or iterator to get training data or dev data. For example, torch.utils.data.DataLoader</li>\n<li><strong>Train data adaptor</strong>, a function that turn batch_data (from train_dataloader) to the inputs of teacher_model and student_model</li>\n<li><strong>Distill config</strong>, a list-object, each item indicates how to calculate loss. It also defines which output of which layer to calculate loss.</li>\n<li><strong>Output adaptor</strong>, a function that turn your model\u2019s output to dict-object output which meet distiller\u2019s requirements</li>\n<li><strong>Evaluator</strong>, a class with evaluate function, it define when and how to save your student model</li>\n</ul>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<div id=\"requirements\">\n<h3>Requirements</h3>\n<ul>\n<li>Python &gt;= 3.6</li>\n<li>PyTorch &gt;= 1.1.0</li>\n<li>NumPy</li>\n<li>Transformers &gt;= 2.0 (optional, used by some examples)</li>\n</ul>\n</div>\n<div id=\"install-from-pypi\">\n<h3>Install from PyPI</h3>\n<p><strong>KnowledgeDistillation</strong>  is currently available on the PyPi\u2019s repository and you can\ninstall it via pip:</p>\n<pre>pip install -U KnowledgeDistillation\n</pre>\n</div>\n<div id=\"install-from-the-github\">\n<h3>Install from the Github</h3>\n<p>If you prefer, you can clone it and run the setup.py file. Use the following\ncommand to get a copy from GitHub:</p>\n<pre>git clone https://github.com/DunZhang/KnowledgeDistillation.git\n</pre>\n</div>\n</div>\n<div id=\"todo-list\">\n<h2>TODO List</h2>\n<ul>\n<li>Add multi teacher model distiller</li>\n<li>Use input mask when computing loss</li>\n<li>Support custom loss functions</li>\n</ul>\n</div>\n<div id=\"a-simple-example\">\n<h2>A simple example</h2>\n<p>A simple example:</p>\n<pre>import torch\nimport logging\nimport numpy as np\nfrom transformers import BertModel, BertConfig\nfrom torch.utils.data import DataLoader, RandomSampler, TensorDataset\n\nfrom knowledge_distillation import KnowledgeDistiller, MultiLayerBasedDistillationLoss\nfrom knowledge_distillation import MultiLayerBasedDistillationEvaluator\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# Some global variables\ntrain_batch_size = 16\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlearning_rate = 2e-5\nnum_epoch = 10\n# Teacher Model\nbert_config = BertConfig(num_hidden_layers=12, output_hidden_states=True, output_attentions=True)\nteacher_model = BertModel(bert_config)\n# Student Model\nbert_config = BertConfig(num_hidden_layers=3, output_hidden_states=True, output_attentions=True)\nstudent_model = BertModel(bert_config)\n\n### Train data loader\ninput_ids = torch.LongTensor(np.random.randint(100, 1000, (100000, 64)))\nattention_mask = torch.LongTensor(np.ones((100000, 64)))\ntoken_type_ids = torch.LongTensor(np.zeros((100000, 64)))\ntrain_data = TensorDataset(input_ids, attention_mask, token_type_ids)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n\n\n### Train data adaptor\n### It is a function that turn batch_data (from train_dataloader) to the inputs of teacher_model and student_model\n### You can define your own train_data_adaptor. Remember the input must be device and batch_data.\n###  The output is either dict or tuple, but must consistent with you model's input\ndef train_data_adaptor(device, batch_data):\n    batch_data = tuple(t.to(device) for t in batch_data)\n    batch_data_dict = {\"input_ids\": batch_data[0],\n                       \"attention_mask\": batch_data[1],\n                       \"token_type_ids\": batch_data[2], }\n    # In this case, the teacher and student use the same input\n    return batch_data_dict, batch_data_dict\n\n\n### The loss model is the key for this generation.\n### We have already provided a general loss model for distilling multi bert layer\n### In most cases, you can directly use this model.\n#### First, we should define a distill_config which indicates how to compute ths loss between teacher and student.\n#### distill_config is a list-object, each item indicates how to calculate loss.\n#### It also defines which output of which layer to calculate loss.\n#### type \"ts_distill\" means that we compute loss between teacher and student\n#### type \"hard_distill\" means that we compute loss between student output and ground truth\n#### loss_function can be mse, cross_entropy or cos. Args is extra parameters in this loss_function\n#### loss_function(x,y,**args)\ndistill_config = [\n    {\"type\": \"ts_distill\",\n     \"teacher_layer_name\": \"embedding_layer\", \"teacher_layer_output_name\": \"embedding\",\n     \"student_layer_name\": \"embedding_layer\", \"student_layer_output_name\": \"embedding\",\n     \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n     },\n    {\"type\": \"ts_distill\",\n     \"teacher_layer_name\": \"bert_layer4\", \"teacher_layer_output_name\": \"hidden_states\",\n     \"student_layer_name\": \"bert_layer1\", \"student_layer_output_name\": \"hidden_states\",\n     \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n     },\n    {\"type\": \"ts_distill\",\n     \"teacher_layer_name\": \"bert_layer4\", \"teacher_layer_output_name\": \"attention\",\n     \"student_layer_name\": \"bert_layer1\", \"student_layer_output_name\": \"attention\",\n     \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n     },\n    {\"type\": \"ts_distill\",\n     \"teacher_layer_name\": \"bert_layer8\", \"teacher_layer_output_name\": \"hidden_states\",\n     \"student_layer_name\": \"bert_layer2\", \"student_layer_output_name\": \"hidden_states\",\n     \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n     },\n    {\"type\": \"ts_distill\",\n     \"teacher_layer_name\": \"bert_layer8\", \"teacher_layer_output_name\": \"attention\",\n     \"student_layer_name\": \"bert_layer2\", \"student_layer_output_name\": \"attention\",\n     \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n     },\n    {\"type\": \"ts_distill\",\n     \"teacher_layer_name\": \"bert_layer12\", \"teacher_layer_output_name\": \"hidden_states\",\n     \"student_layer_name\": \"bert_layer3\", \"student_layer_output_name\": \"hidden_states\",\n     \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n     },\n    {\"type\": \"ts_distill\",\n     \"teacher_layer_name\": \"bert_layer12\", \"teacher_layer_output_name\": \"attention\",\n     \"student_layer_name\": \"bert_layer3\", \"student_layer_output_name\": \"attention\",\n     \"loss\": {\"loss_function\": \"mse\", \"args\": {}}, \"weight\": 1.0\n     },\n]\n\n### teacher_output_adaptor and student_output_adaptor\n### In most cases, model's output is tuple-object, However, in our package, we need the output is dict-object,\n### like: { \"layer_name\":{\"output_name\":value} .... }\n### Hence, the output adaptor is to turn your model's output to dict-object output\n### In my case, teacher and student can use one adaptor\ndef output_adaptor(model_output):\n    last_hidden_state, pooler_output, hidden_states, attentions = model_output\n    output = {\"embedding_layer\": {\"embedding\": hidden_states[0]}}\n    for idx in range(len(attentions)):\n        output[\"bert_layer\" + str(idx + 1)] = {\"hidden_states\": hidden_states[idx + 1],\n                                               \"attention\": attentions[idx]}\n    return output\n\n\n# loss_model\nloss_model = MultiLayerBasedDistillationLoss(distill_config=distill_config,\n                                             teacher_output_adaptor=output_adaptor,\n                                             student_output_adaptor=output_adaptor)\n# optimizer\nparam_optimizer = list(student_model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\noptimizer = torch.optim.Adam(params=optimizer_grouped_parameters, lr=learning_rate)\n# evaluator\nevaluator = MultiLayerBasedDistillationEvaluator(save_dir=None, save_step=None, print_loss_step=20)\n# Get a KnowledgeDistiller\ndistiller = KnowledgeDistiller(teacher_model=teacher_model, student_model=student_model,\n                               train_dataloader=train_dataloader, dev_dataloader=None,\n                               train_data_adaptor=train_data_adaptor, dev_data_adaptor=None,\n                               device=device, loss_model=loss_model, optimizer=optimizer,\n                               evaluator=evaluator, num_epoch=num_epoch)\n# start distillate\ndistiller.distillate()\n</pre>\n</div>\n\n          </div>"}, "last_serial": 6785022, "releases": {"1.0.1": [{"comment_text": "", "digests": {"md5": "ad1e3e7a8fedd91c68f8bc74079ba498", "sha256": "6d6d4f76894fc8681dae4b1d873efa36ecffc89c38df7339f1a7e8746666c9f5"}, "downloads": -1, "filename": "KnowledgeDistillation-1.0.1.tar.gz", "has_sig": false, "md5_digest": "ad1e3e7a8fedd91c68f8bc74079ba498", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7433, "upload_time": "2020-03-08T07:37:27", "upload_time_iso_8601": "2020-03-08T07:37:27.787997Z", "url": "https://files.pythonhosted.org/packages/4e/3f/01c99670cb0c507344686a52164f7705cf0bea0438c70460ccbc655612be/KnowledgeDistillation-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "d7291d2e8dc3a83dfe485fc28d8fd880", "sha256": "f5a408fc40d977341d229ef5f86a9c4e26cc0d1b4f71536f2130eaa6d748479e"}, "downloads": -1, "filename": "KnowledgeDistillation-1.0.2.tar.gz", "has_sig": false, "md5_digest": "d7291d2e8dc3a83dfe485fc28d8fd880", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8002, "upload_time": "2020-03-10T13:29:37", "upload_time_iso_8601": "2020-03-10T13:29:37.265383Z", "url": "https://files.pythonhosted.org/packages/bc/90/d881040a14dc208482913870722627a884bb675f1b947e4451dbec7adf8e/KnowledgeDistillation-1.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d7291d2e8dc3a83dfe485fc28d8fd880", "sha256": "f5a408fc40d977341d229ef5f86a9c4e26cc0d1b4f71536f2130eaa6d748479e"}, "downloads": -1, "filename": "KnowledgeDistillation-1.0.2.tar.gz", "has_sig": false, "md5_digest": "d7291d2e8dc3a83dfe485fc28d8fd880", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8002, "upload_time": "2020-03-10T13:29:37", "upload_time_iso_8601": "2020-03-10T13:29:37.265383Z", "url": "https://files.pythonhosted.org/packages/bc/90/d881040a14dc208482913870722627a884bb675f1b947e4451dbec7adf8e/KnowledgeDistillation-1.0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:49:12 2020"}