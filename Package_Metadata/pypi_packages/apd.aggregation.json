{"info": {"author": "", "author_email": "", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 3", "Programming Language :: Python :: 3.7"], "description": "# APD Sensor aggregator \n\nA programme that queries apd.sensor endpoints and aggregates their results.\n\nGeneric single-database configuration.\n\n# Database setup\n\nTo generate the required database tables you must create an alembic.ini file, as follows:\n\n    [alembic]\n    script_location = apd.aggregation:alembic\n    sqlalchemy.url = postgresql+psycopg2://apd@localhost/apd\n\nand run `alembic upgrade head`. This should also be done after every upgrade of the software.\n\n# Defining endpoints\n\nEndpoints to collect from are managed with the `sensor_deployments` CLI tool. After installation\nthere will be no deployments defined \n\n    sensor_deployments add --db postgresql+psycopg2://apd@localhost/apd \n                           --api-key 97f6b3e5ceb64a6ba88968d7c3786b38\n                           --colour xkcd:red\n                           http://rpi4:8081\n                           Loft\n\nThe optional colour argument is the colour to use when plotting charts with the built-in charting\ntools. This uses matplotlib's colour specification system, documented at https://matplotlib.org/tutorials/colors/colors.html\n\nThe sensors can then be listed with `sensor_deployments list`:\n\n    Loft\n    ID 53998a5160de48aeb71a5c37cd1455f2\n    URI http://rpi4:8081\n    API key 97f6b3e5ceb64a6ba88968d7c3786b38\n    Colour xkcd:red\n\nThe ID is the deployment ID, as set by the endpoint. It is only possible to add endpoints if they can be\nconnected to at the time.\n\n# Collating data\n\nData can be collated from all defined endpoints with the `collect_sensor_data` command line tool.\nAlthough you can specify URLs and an API key to explicitly load data from a one-off endpoint, running\nwithout specifying these will use the configured endpoints from the database.\n\n    collect_sensor_data --db postgresql+psycopg2://apd@localhost/apd\n\n# Viewing data\n\nYou can write scripts to visualise the data from the database. I recommend using Jupyter for this, as it\nhas good support for drawing charts and interactivity.\n\nAll configured charts can be displayed with:\n\n    from apd.aggregation.analysis import plot_multiple_charts\n    display(await plot_multiple_charts())\n\nMore complex charting can be achieved by passing `configs=` to this function, consisting of configuration\nobjects as defined in `apd.aggregation.analysis`. Iteractivity can be achieved using the\n`interactable_plot_multiple_charts` function with Jupyter/IPyWidgets' existing interactivity support.\n\nMore control can be achieved using other functions from this module, such as getting all data points from\na given sensor with:\n\n    from apd.aggregation.query import with_database, get_data\n\n    with with_database(\"postgresql+psycopg2://apd@localhost/apd\") as session:\n        points = [(dp.collected_at, dp.data) async for dp in get_data() if dp.sensor_name==\"RelativeHumidity\"]\n\nThese can be called from any Python code, not just Jupyter notebooks\n\n# Analysis and triggers\n\nThe aggregator allows for a long-running process that processes records as they are inserted to the database\nand apply rules to them.\n\nThis is configured with a Python-based configuration file, such as the following to log any time the\nTemperature fluctuates above or below 18c:\n\n    import operator\n\n    from apd.aggregation.actions.action import OnlyOnChangeActionWrapper, LoggingAction\n    from apd.aggregation.actions.runner import DataProcessor\n    from apd.aggregation.actions.trigger import ValueThresholdTrigger\n\n\n    handlers = [\n        DataProcessor(\n            name=\"TemperatureBelow18\",\n            action=OnlyOnChangeActionWrapper(LoggingAction()),\n            trigger=ValueThresholdTrigger(\n                name=\"TemperatureBelow18\",\n                threshold=18,\n                comparator=operator.lt,\n                sensor_name=\"Temperature\",\n            ),\n        )\n    ]\n\nThis is run with:\n\n    run_apd_actions --db postgresql+psycopg2://apd@localhost/apd sample_actions.py\n\nThe optional `--historical` option causes the actions to be triggered for all events in the database.\nIf it's omitted then the default behaviour applies, which is to only analyse data that is added to the\ndatabase after the actions process has started.\n\nThe possible actions are:\n\n* `apd.aggregation.actions.action.LoggingAction()` - Log data points\n* `apd.aggregation.actions.action.SaveToDatabaseAction()` - Save data points to the db\n\nThese can be wrapped with `OnlyOnChangeActionWrapper(subaction)` to only trigger an action when\nthe underlying value changes and/or with `OnlyAfterDateActionWrapper(subaction, min_date)` to \nonly trigger if the date on the discovered objects is strictly after `min_date`.\n\nThe possible triggers are:\n\n* `apd.aggregation.actions.trigger.ValueThresholdTrigger(...)` - This compares the value of a sensor with threshold, using the specified comparator.\n    Any records that don't match the `sensor_name` and `deployment_id` parameters are excluded.\n\n\n# Tips\n\nThe `--db` argument to all command-line tools can be omitted and the `APD_DB_URI` environment variable\nset instead.\n## Changes\n\n### 1.0.0 (2020-01-27)\n\n* Added management of known sensor endpoints\n* Added CLI script to collate data\n* Added analysis tools for Jupyter\n* Added long-running data synthesis and actions system\n\nCopyright (c) 2019, Matthew Wilkes\n\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from this\n  software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\nIN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,\nINDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\nBUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\nOF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\nOR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED\nOF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "BSD", "maintainer": "", "maintainer_email": "", "name": "apd.aggregation", "package_url": "https://pypi.org/project/apd.aggregation/", "platform": "", "project_url": "https://pypi.org/project/apd.aggregation/", "project_urls": null, "release_url": "https://pypi.org/project/apd.aggregation/1.0.0/", "requires_dist": ["sqlalchemy", "aiohttp", "pint", "psycopg2", "alembic", "click", "ipywidgets ; extra == 'jupyter'", "yappi ; extra == 'yappi'"], "requires_python": "", "summary": "A programme that queries apd.sensor endpoints and aggregates their results.", "version": "1.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>APD Sensor aggregator</h1>\n<p>A programme that queries apd.sensor endpoints and aggregates their results.</p>\n<p>Generic single-database configuration.</p>\n<h1>Database setup</h1>\n<p>To generate the required database tables you must create an alembic.ini file, as follows:</p>\n<pre><code>[alembic]\nscript_location = apd.aggregation:alembic\nsqlalchemy.url = postgresql+psycopg2://apd@localhost/apd\n</code></pre>\n<p>and run <code>alembic upgrade head</code>. This should also be done after every upgrade of the software.</p>\n<h1>Defining endpoints</h1>\n<p>Endpoints to collect from are managed with the <code>sensor_deployments</code> CLI tool. After installation\nthere will be no deployments defined</p>\n<pre><code>sensor_deployments add --db postgresql+psycopg2://apd@localhost/apd \n                       --api-key 97f6b3e5ceb64a6ba88968d7c3786b38\n                       --colour xkcd:red\n                       http://rpi4:8081\n                       Loft\n</code></pre>\n<p>The optional colour argument is the colour to use when plotting charts with the built-in charting\ntools. This uses matplotlib's colour specification system, documented at <a href=\"https://matplotlib.org/tutorials/colors/colors.html\" rel=\"nofollow\">https://matplotlib.org/tutorials/colors/colors.html</a></p>\n<p>The sensors can then be listed with <code>sensor_deployments list</code>:</p>\n<pre><code>Loft\nID 53998a5160de48aeb71a5c37cd1455f2\nURI http://rpi4:8081\nAPI key 97f6b3e5ceb64a6ba88968d7c3786b38\nColour xkcd:red\n</code></pre>\n<p>The ID is the deployment ID, as set by the endpoint. It is only possible to add endpoints if they can be\nconnected to at the time.</p>\n<h1>Collating data</h1>\n<p>Data can be collated from all defined endpoints with the <code>collect_sensor_data</code> command line tool.\nAlthough you can specify URLs and an API key to explicitly load data from a one-off endpoint, running\nwithout specifying these will use the configured endpoints from the database.</p>\n<pre><code>collect_sensor_data --db postgresql+psycopg2://apd@localhost/apd\n</code></pre>\n<h1>Viewing data</h1>\n<p>You can write scripts to visualise the data from the database. I recommend using Jupyter for this, as it\nhas good support for drawing charts and interactivity.</p>\n<p>All configured charts can be displayed with:</p>\n<pre><code>from apd.aggregation.analysis import plot_multiple_charts\ndisplay(await plot_multiple_charts())\n</code></pre>\n<p>More complex charting can be achieved by passing <code>configs=</code> to this function, consisting of configuration\nobjects as defined in <code>apd.aggregation.analysis</code>. Iteractivity can be achieved using the\n<code>interactable_plot_multiple_charts</code> function with Jupyter/IPyWidgets' existing interactivity support.</p>\n<p>More control can be achieved using other functions from this module, such as getting all data points from\na given sensor with:</p>\n<pre><code>from apd.aggregation.query import with_database, get_data\n\nwith with_database(\"postgresql+psycopg2://apd@localhost/apd\") as session:\n    points = [(dp.collected_at, dp.data) async for dp in get_data() if dp.sensor_name==\"RelativeHumidity\"]\n</code></pre>\n<p>These can be called from any Python code, not just Jupyter notebooks</p>\n<h1>Analysis and triggers</h1>\n<p>The aggregator allows for a long-running process that processes records as they are inserted to the database\nand apply rules to them.</p>\n<p>This is configured with a Python-based configuration file, such as the following to log any time the\nTemperature fluctuates above or below 18c:</p>\n<pre><code>import operator\n\nfrom apd.aggregation.actions.action import OnlyOnChangeActionWrapper, LoggingAction\nfrom apd.aggregation.actions.runner import DataProcessor\nfrom apd.aggregation.actions.trigger import ValueThresholdTrigger\n\n\nhandlers = [\n    DataProcessor(\n        name=\"TemperatureBelow18\",\n        action=OnlyOnChangeActionWrapper(LoggingAction()),\n        trigger=ValueThresholdTrigger(\n            name=\"TemperatureBelow18\",\n            threshold=18,\n            comparator=operator.lt,\n            sensor_name=\"Temperature\",\n        ),\n    )\n]\n</code></pre>\n<p>This is run with:</p>\n<pre><code>run_apd_actions --db postgresql+psycopg2://apd@localhost/apd sample_actions.py\n</code></pre>\n<p>The optional <code>--historical</code> option causes the actions to be triggered for all events in the database.\nIf it's omitted then the default behaviour applies, which is to only analyse data that is added to the\ndatabase after the actions process has started.</p>\n<p>The possible actions are:</p>\n<ul>\n<li><code>apd.aggregation.actions.action.LoggingAction()</code> - Log data points</li>\n<li><code>apd.aggregation.actions.action.SaveToDatabaseAction()</code> - Save data points to the db</li>\n</ul>\n<p>These can be wrapped with <code>OnlyOnChangeActionWrapper(subaction)</code> to only trigger an action when\nthe underlying value changes and/or with <code>OnlyAfterDateActionWrapper(subaction, min_date)</code> to\nonly trigger if the date on the discovered objects is strictly after <code>min_date</code>.</p>\n<p>The possible triggers are:</p>\n<ul>\n<li><code>apd.aggregation.actions.trigger.ValueThresholdTrigger(...)</code> - This compares the value of a sensor with threshold, using the specified comparator.\nAny records that don't match the <code>sensor_name</code> and <code>deployment_id</code> parameters are excluded.</li>\n</ul>\n<h1>Tips</h1>\n<p>The <code>--db</code> argument to all command-line tools can be omitted and the <code>APD_DB_URI</code> environment variable\nset instead.</p>\n<h2>Changes</h2>\n<h3>1.0.0 (2020-01-27)</h3>\n<ul>\n<li>Added management of known sensor endpoints</li>\n<li>Added CLI script to collate data</li>\n<li>Added analysis tools for Jupyter</li>\n<li>Added long-running data synthesis and actions system</li>\n</ul>\n<p>Copyright (c) 2019, Matthew Wilkes</p>\n<p>All rights reserved.</p>\n<p>Redistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:</p>\n<ul>\n<li>\n<p>Redistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.</p>\n</li>\n<li>\n<p>Redistributions in binary form must reproduce the above copyright notice, this\nlist of conditions and the following disclaimer in the documentation and/or\nother materials provided with the distribution.</p>\n</li>\n<li>\n<p>Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived from this\nsoftware without specific prior written permission.</p>\n</li>\n</ul>\n<p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\nIN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,\nINDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\nBUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\nOF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\nOR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED\nOF THE POSSIBILITY OF SUCH DAMAGE.</p>\n\n          </div>"}, "last_serial": 6526459, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "9db23de3774712e71f1285a3a8b4c0dc", "sha256": "8c2bb4760a3b9b8512c00a1bdf1c75c1bf50baf09901c6e2c27d9837b61f7b0d"}, "downloads": -1, "filename": "apd.aggregation-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "9db23de3774712e71f1285a3a8b4c0dc", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 29022, "upload_time": "2020-01-27T13:40:13", "upload_time_iso_8601": "2020-01-27T13:40:13.919455Z", "url": "https://files.pythonhosted.org/packages/0b/7c/d52ba2b86cc42df70fdafb05513233d7bf020d7d622155a4fc48354b80ad/apd.aggregation-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e39b67039f67beb1978194ee342340bf", "sha256": "07720eecba0685eaf7047938da90d650abb231e9f12c44ba79614810ed04e76f"}, "downloads": -1, "filename": "apd.aggregation-1.0.0.tar.gz", "has_sig": false, "md5_digest": "e39b67039f67beb1978194ee342340bf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24668, "upload_time": "2020-01-27T13:40:16", "upload_time_iso_8601": "2020-01-27T13:40:16.438498Z", "url": "https://files.pythonhosted.org/packages/f3/9d/691037e32f665eecc56cac2d7da34d22cc1c58c107bbfb5c1b105b3cc37b/apd.aggregation-1.0.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "9db23de3774712e71f1285a3a8b4c0dc", "sha256": "8c2bb4760a3b9b8512c00a1bdf1c75c1bf50baf09901c6e2c27d9837b61f7b0d"}, "downloads": -1, "filename": "apd.aggregation-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "9db23de3774712e71f1285a3a8b4c0dc", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 29022, "upload_time": "2020-01-27T13:40:13", "upload_time_iso_8601": "2020-01-27T13:40:13.919455Z", "url": "https://files.pythonhosted.org/packages/0b/7c/d52ba2b86cc42df70fdafb05513233d7bf020d7d622155a4fc48354b80ad/apd.aggregation-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e39b67039f67beb1978194ee342340bf", "sha256": "07720eecba0685eaf7047938da90d650abb231e9f12c44ba79614810ed04e76f"}, "downloads": -1, "filename": "apd.aggregation-1.0.0.tar.gz", "has_sig": false, "md5_digest": "e39b67039f67beb1978194ee342340bf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24668, "upload_time": "2020-01-27T13:40:16", "upload_time_iso_8601": "2020-01-27T13:40:16.438498Z", "url": "https://files.pythonhosted.org/packages/f3/9d/691037e32f665eecc56cac2d7da34d22cc1c58c107bbfb5c1b105b3cc37b/apd.aggregation-1.0.0.tar.gz", "yanked": false}], "timestamp": "Thu May  7 18:17:52 2020"}