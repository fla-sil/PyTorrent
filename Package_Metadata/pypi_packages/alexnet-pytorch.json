{"info": {"author": "Liu Changyu", "author_email": "liuchangyu1111@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: Apache Software License", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "\n# AlexNet-PyTorch\n\n### Update (Feb 16, 2020)\n\nNow you can install this library directly using pip!\n\n```text\npip3 install --upgrade alexnet_pytorch\n```\n\n### Update (Feb 13, 2020)\n\nThe update is for ease of use and deployment.\n\n * [Example: Export to ONNX](#example-export-to-onnx)\n * [Example: Extract features](#example-feature-extraction)\n * [Example: Visual](#example-visual)\n\nIt is also now incredibly simple to load a pretrained model with a new number of classes for transfer learning:\n\n```python\nfrom alexnet_pytorch import AlexNet\nmodel = AlexNet.from_pretrained('alexnet', num_classes=10)\n```\n\n### Update (January 15, 2020)\n\nThis update allows you to use NVIDIA's Apex tool for accelerated training. By default choice `hybrid training precision` + `dynamic loss amplified` version, if you need to learn more and details about `apex` tools, please visit https://github.com/NVIDIA/apex.\n\n### Overview\nThis repository contains an op-for-op PyTorch reimplementation of [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).\n\nThe goal of this implementation is to be simple, highly extensible, and easy to integrate into your own projects. This implementation is a work in progress -- new features are currently being implemented.  \n\nAt the moment, you can easily:  \n * Load pretrained AlexNet models \n * Use AlexNet models for classification or feature extraction \n\n_Upcoming features_: In the next few days, you will be able to:\n * Quickly finetune an AlexNet on your own dataset\n * Export AlexNet models for production\n\n### Table of contents\n1. [About AlexNet](#about-alexnet)\n2. [Model Description](#model-description)\n3. [Installation](#installation)\n4. [Usage](#usage)\n    * [Load pretrained models](#loading-pretrained-models)\n    * [Example: Classify](#example-classification)\n    * [Example: Extract features](#example-feature-extraction)\n    * [Example: Export to ONNX](#example-export-to-onnx)\n    * [Example: Visual](#example-visual)\n5. [Contributing](#contributing) \n\n### About AlexNet\n\nIf you're new to AlexNets, here is an explanation straight from the official PyTorch implementation: \n\nCurrent approaches to object recognition make essential use of machine learning methods. To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting. Until recently, datasets of labeled images were relatively\nsmall \u2014 on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and\nCIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size,\nespecially if they are augmented with label-preserving transformations. For example, the currentbest error rate on the MNIST digit-recognition task (<0.3%) approaches human performance [4].\nBut objects in realistic settings exhibit considerable variability, so to learn to recognize them it is\nnecessary to use much larger training sets. And indeed, the shortcomings of small image datasets\nhave been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to collect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which\nconsists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of\nover 15 million labeled high-resolution images in over 22,000 categories. \n\n### Model Description\n\nAlexNet competed in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up. The original paper's primary result was that the depth of the model was essential for its high performance, which was computationally expensive, but made feasible due to the utilization of graphics processing units (GPUs) during training.\n\nThe 1-crop error rates on the imagenet dataset with the pretrained model are listed below.\n\n|Model structure|*Top-1 error*|*Top-5 error*|\n|:-------------:|:-----------:|:-----------:|\n|    alexnet    |    43.48    |    20.93    |\n\n### Installation\n\nInstall from pypi:\n```bash\npip install alexnet_pytorch\n```\n\nInstall from source:\n```bash\ngit clone https://github.com/lornatang/AlexNet-PyTorch.git\ncd AlexNet-PyTorch\npip install -e .\n```\n\n### Usage\n\n#### Loading pretrained models\n\nLoad an AlexNet:  \n```python\nfrom alexnet_pytorch import AlexNet\nmodel = AlexNet.from_name('alexnet')\n```\n\nLoad a pretrained AlexNet: \n```python\nfrom alexnet_pytorch import AlexNet\nmodel = AlexNet.from_pretrained('alexnet')\n```\n\n#### Example: Classification\n\nWe assume that in your current directory, there is a `img.jpg` file and a `labels_map.txt` file (ImageNet class names). These are both included in `examples/simple`. \n\nAll pre-trained models expect input images normalized in the same way,\ni.e. mini-batches of 3-channel RGB images of shape `(3 x H x W)`, where `H` and `W` are expected to be at least `224`.\nThe images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\nand `std = [0.229, 0.224, 0.225]`.\n\nHere's a sample execution.\n\n```python\nimport json\n\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nfrom alexnet_pytorch import AlexNet\n\n# Open image\ninput_image = Image.open(\"img.jpg\")\n\n# Preprocess image\npreprocess = transforms.Compose([\n  transforms.Resize(256),\n  transforms.CenterCrop(224),\n  transforms.ToTensor(),\n  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n\n# Load class names\nlabels_map = json.load(open(\"labels_map.txt\"))\nlabels_map = [labels_map[str(i)] for i in range(1000)]\n\n# Classify with AlexNet\nmodel = AlexNet.from_pretrained(\"alexnet\")\nmodel.eval()\n\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n  input_batch = input_batch.to(\"cuda\")\n  model.to(\"cuda\")\n\nwith torch.no_grad():\n  logits = model(input_batch)\npreds = torch.topk(logits, k=5).indices.squeeze(0).tolist()\n\nprint(\"-----\")\nfor idx in preds:\n  label = labels_map[idx]\n  prob = torch.softmax(logits, dim=1)[0, idx].item()\n  print(f\"{label:<75} ({prob * 100:.2f}%)\")\n```\n\n#### Example: Feature Extraction \n\nYou can easily extract features with `model.extract_features`:\n```python\nimport torch\nfrom alexnet_pytorch import AlexNet\nmodel = AlexNet.from_pretrained('alexnet')\n\n# ... image preprocessing as in the classification example ...\ninputs = torch.randn(1, 3, 224, 224)\nprint(inputs.shape) # torch.Size([1, 3, 224, 224])\n\nfeatures = model.extract_features(inputs)\nprint(features.shape) # torch.Size([1, 256, 6, 6])\n```\n\n#### Example: Export to ONNX  \n\nExporting to ONNX for deploying to production is now simple: \n```python\nimport torch \nfrom alexnet_pytorch import AlexNet\n\nmodel = AlexNet.from_pretrained('alexnet')\ndummy_input = torch.randn(16, 3, 224, 224)\n\ntorch.onnx.export(model, dummy_input, \"demo.onnx\", verbose=True)\n```\n\n#### Example: Visual\n\n```text\ncd $REPO$/framework\nsh start.sh\n```\n\nThen open the browser and type in the browser address [http://127.0.0.1:20000/](http://127.0.0.1:20000/).\n\nEnjoy it.\n\n#### ImageNet\n\nSee `examples/imagenet` for details about evaluating on ImageNet.\n\nFor more datasets result. Please see `research/README.md`.\n\n### Contributing\n\nIf you find a bug, create a GitHub issue, or even better, submit a pull request. Similarly, if you have questions, simply post them as GitHub issues.   \n\nI look forward to seeing what the community does with these models! \n\n\n### Credit\n\n#### ImageNet Classification with Deep Convolutional Neural Networks\n\n*Alex Krizhevsky,Ilya Sutskever,Geoffrey E. Hinton*\n\n##### Abstract\n\nWe trained a large, deep convolutional neural network to classify the 1.2 million\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\nand 17.0% which is considerably better than the previous state-of-the-art. The\nneural network, which has 60 million parameters and 650,000 neurons, consists\nof five convolutional layers, some of which are followed by max-pooling layers,\nand three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected\nlayers we employed a recently-developed regularization method called \u201cdropout\u201d\nthat proved to be very effective. We also entered a variant of this model in the\nILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%,\ncompared to 26.2% achieved by the second-best entry.\n\n[paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n\n```text\n@article{AlexNet,\ntitle:{ImageNet Classification with Deep Convolutional Neural Networks},\nauthor:{Alex Krizhevsky,Ilya Sutskever,Geoffrey E. Hinton},\njournal={nips},\nyear={2012}\n}\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Lornatang/AlexNet-PyTorch", "keywords": "", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "alexnet-pytorch", "package_url": "https://pypi.org/project/alexnet-pytorch/", "platform": "", "project_url": "https://pypi.org/project/alexnet-pytorch/", "project_urls": {"Homepage": "https://github.com/Lornatang/AlexNet-PyTorch"}, "release_url": "https://pypi.org/project/alexnet-pytorch/0.2.0/", "requires_dist": ["torch"], "requires_python": ">=3.6.0", "summary": "An improved version of the AlexNet model, adding parameter initialization from ResNet.", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>AlexNet-PyTorch</h1>\n<h3>Update (Feb 16, 2020)</h3>\n<p>Now you can install this library directly using pip!</p>\n<pre>pip3 install --upgrade alexnet_pytorch\n</pre>\n<h3>Update (Feb 13, 2020)</h3>\n<p>The update is for ease of use and deployment.</p>\n<ul>\n<li><a href=\"#example-export-to-onnx\" rel=\"nofollow\">Example: Export to ONNX</a></li>\n<li><a href=\"#example-feature-extraction\" rel=\"nofollow\">Example: Extract features</a></li>\n<li><a href=\"#example-visual\" rel=\"nofollow\">Example: Visual</a></li>\n</ul>\n<p>It is also now incredibly simple to load a pretrained model with a new number of classes for transfer learning:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">alexnet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">AlexNet</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AlexNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'alexnet'</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n</pre>\n<h3>Update (January 15, 2020)</h3>\n<p>This update allows you to use NVIDIA's Apex tool for accelerated training. By default choice <code>hybrid training precision</code> + <code>dynamic loss amplified</code> version, if you need to learn more and details about <code>apex</code> tools, please visit <a href=\"https://github.com/NVIDIA/apex\" rel=\"nofollow\">https://github.com/NVIDIA/apex</a>.</p>\n<h3>Overview</h3>\n<p>This repository contains an op-for-op PyTorch reimplementation of <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" rel=\"nofollow\">AlexNet</a>.</p>\n<p>The goal of this implementation is to be simple, highly extensible, and easy to integrate into your own projects. This implementation is a work in progress -- new features are currently being implemented.</p>\n<p>At the moment, you can easily:</p>\n<ul>\n<li>Load pretrained AlexNet models</li>\n<li>Use AlexNet models for classification or feature extraction</li>\n</ul>\n<p><em>Upcoming features</em>: In the next few days, you will be able to:</p>\n<ul>\n<li>Quickly finetune an AlexNet on your own dataset</li>\n<li>Export AlexNet models for production</li>\n</ul>\n<h3>Table of contents</h3>\n<ol>\n<li><a href=\"#about-alexnet\" rel=\"nofollow\">About AlexNet</a></li>\n<li><a href=\"#model-description\" rel=\"nofollow\">Model Description</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#usage\" rel=\"nofollow\">Usage</a>\n<ul>\n<li><a href=\"#loading-pretrained-models\" rel=\"nofollow\">Load pretrained models</a></li>\n<li><a href=\"#example-classification\" rel=\"nofollow\">Example: Classify</a></li>\n<li><a href=\"#example-feature-extraction\" rel=\"nofollow\">Example: Extract features</a></li>\n<li><a href=\"#example-export-to-onnx\" rel=\"nofollow\">Example: Export to ONNX</a></li>\n<li><a href=\"#example-visual\" rel=\"nofollow\">Example: Visual</a></li>\n</ul>\n</li>\n<li><a href=\"#contributing\" rel=\"nofollow\">Contributing</a></li>\n</ol>\n<h3>About AlexNet</h3>\n<p>If you're new to AlexNets, here is an explanation straight from the official PyTorch implementation:</p>\n<p>Current approaches to object recognition make essential use of machine learning methods. To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting. Until recently, datasets of labeled images were relatively\nsmall \u2014 on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and\nCIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size,\nespecially if they are augmented with label-preserving transformations. For example, the currentbest error rate on the MNIST digit-recognition task (&lt;0.3%) approaches human performance [4].\nBut objects in realistic settings exhibit considerable variability, so to learn to recognize them it is\nnecessary to use much larger training sets. And indeed, the shortcomings of small image datasets\nhave been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to collect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which\nconsists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of\nover 15 million labeled high-resolution images in over 22,000 categories.</p>\n<h3>Model Description</h3>\n<p>AlexNet competed in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up. The original paper's primary result was that the depth of the model was essential for its high performance, which was computationally expensive, but made feasible due to the utilization of graphics processing units (GPUs) during training.</p>\n<p>The 1-crop error rates on the imagenet dataset with the pretrained model are listed below.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Model structure</th>\n<th align=\"center\"><em>Top-1 error</em></th>\n<th align=\"center\"><em>Top-5 error</em></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">alexnet</td>\n<td align=\"center\">43.48</td>\n<td align=\"center\">20.93</td>\n</tr></tbody></table>\n<h3>Installation</h3>\n<p>Install from pypi:</p>\n<pre>pip install alexnet_pytorch\n</pre>\n<p>Install from source:</p>\n<pre>git clone https://github.com/lornatang/AlexNet-PyTorch.git\n<span class=\"nb\">cd</span> AlexNet-PyTorch\npip install -e .\n</pre>\n<h3>Usage</h3>\n<h4>Loading pretrained models</h4>\n<p>Load an AlexNet:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">alexnet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">AlexNet</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AlexNet</span><span class=\"o\">.</span><span class=\"n\">from_name</span><span class=\"p\">(</span><span class=\"s1\">'alexnet'</span><span class=\"p\">)</span>\n</pre>\n<p>Load a pretrained AlexNet:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">alexnet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">AlexNet</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AlexNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'alexnet'</span><span class=\"p\">)</span>\n</pre>\n<h4>Example: Classification</h4>\n<p>We assume that in your current directory, there is a <code>img.jpg</code> file and a <code>labels_map.txt</code> file (ImageNet class names). These are both included in <code>examples/simple</code>.</p>\n<p>All pre-trained models expect input images normalized in the same way,\ni.e. mini-batches of 3-channel RGB images of shape <code>(3 x H x W)</code>, where <code>H</code> and <code>W</code> are expected to be at least <code>224</code>.\nThe images have to be loaded in to a range of <code>[0, 1]</code> and then normalized using <code>mean = [0.485, 0.456, 0.406]</code>\nand <code>std = [0.229, 0.224, 0.225]</code>.</p>\n<p>Here's a sample execution.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">json</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision.transforms</span> <span class=\"k\">as</span> <span class=\"nn\">transforms</span>\n<span class=\"kn\">from</span> <span class=\"nn\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">alexnet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">AlexNet</span>\n\n<span class=\"c1\"># Open image</span>\n<span class=\"n\">input_image</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"s2\">\"img.jpg\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Preprocess image</span>\n<span class=\"n\">preprocess</span> <span class=\"o\">=</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">([</span>\n  <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Resize</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">),</span>\n  <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">CenterCrop</span><span class=\"p\">(</span><span class=\"mi\">224</span><span class=\"p\">),</span>\n  <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">(),</span>\n  <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Normalize</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.485</span><span class=\"p\">,</span> <span class=\"mf\">0.456</span><span class=\"p\">,</span> <span class=\"mf\">0.406</span><span class=\"p\">],</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.229</span><span class=\"p\">,</span> <span class=\"mf\">0.224</span><span class=\"p\">,</span> <span class=\"mf\">0.225</span><span class=\"p\">]),</span>\n<span class=\"p\">])</span>\n<span class=\"n\">input_tensor</span> <span class=\"o\">=</span> <span class=\"n\">preprocess</span><span class=\"p\">(</span><span class=\"n\">input_image</span><span class=\"p\">)</span>\n<span class=\"n\">input_batch</span> <span class=\"o\">=</span> <span class=\"n\">input_tensor</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>  <span class=\"c1\"># create a mini-batch as expected by the model</span>\n\n<span class=\"c1\"># Load class names</span>\n<span class=\"n\">labels_map</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">\"labels_map.txt\"</span><span class=\"p\">))</span>\n<span class=\"n\">labels_map</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">labels_map</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)]</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">)]</span>\n\n<span class=\"c1\"># Classify with AlexNet</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AlexNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s2\">\"alexnet\"</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># move the input and model to GPU for speed if available</span>\n<span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">is_available</span><span class=\"p\">():</span>\n  <span class=\"n\">input_batch</span> <span class=\"o\">=</span> <span class=\"n\">input_batch</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"s2\">\"cuda\"</span><span class=\"p\">)</span>\n  <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"s2\">\"cuda\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n  <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_batch</span><span class=\"p\">)</span>\n<span class=\"n\">preds</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">topk</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">indices</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"-----\"</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">idx</span> <span class=\"ow\">in</span> <span class=\"n\">preds</span><span class=\"p\">:</span>\n  <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"n\">labels_map</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">]</span>\n  <span class=\"n\">prob</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">idx</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n  <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"</span><span class=\"si\">{</span><span class=\"n\">label</span><span class=\"si\">:</span><span class=\"s2\">&lt;75</span><span class=\"si\">}</span><span class=\"s2\"> (</span><span class=\"si\">{</span><span class=\"n\">prob</span> <span class=\"o\">*</span> <span class=\"mi\">100</span><span class=\"si\">:</span><span class=\"s2\">.2f</span><span class=\"si\">}</span><span class=\"s2\">%)\"</span><span class=\"p\">)</span>\n</pre>\n<h4>Example: Feature Extraction</h4>\n<p>You can easily extract features with <code>model.extract_features</code>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">alexnet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">AlexNet</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AlexNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'alexnet'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># ... image preprocessing as in the classification example ...</span>\n<span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># torch.Size([1, 3, 224, 224])</span>\n\n<span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">extract_features</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># torch.Size([1, 256, 6, 6])</span>\n</pre>\n<h4>Example: Export to ONNX</h4>\n<p>Exporting to ONNX for deploying to production is now simple:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span> \n<span class=\"kn\">from</span> <span class=\"nn\">alexnet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">AlexNet</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">AlexNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'alexnet'</span><span class=\"p\">)</span>\n<span class=\"n\">dummy_input</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">)</span>\n\n<span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">onnx</span><span class=\"o\">.</span><span class=\"n\">export</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">dummy_input</span><span class=\"p\">,</span> <span class=\"s2\">\"demo.onnx\"</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<h4>Example: Visual</h4>\n<pre>cd $REPO$/framework\nsh start.sh\n</pre>\n<p>Then open the browser and type in the browser address <a href=\"http://127.0.0.1:20000/\" rel=\"nofollow\">http://127.0.0.1:20000/</a>.</p>\n<p>Enjoy it.</p>\n<h4>ImageNet</h4>\n<p>See <code>examples/imagenet</code> for details about evaluating on ImageNet.</p>\n<p>For more datasets result. Please see <code>research/README.md</code>.</p>\n<h3>Contributing</h3>\n<p>If you find a bug, create a GitHub issue, or even better, submit a pull request. Similarly, if you have questions, simply post them as GitHub issues.</p>\n<p>I look forward to seeing what the community does with these models!</p>\n<h3>Credit</h3>\n<h4>ImageNet Classification with Deep Convolutional Neural Networks</h4>\n<p><em>Alex Krizhevsky,Ilya Sutskever,Geoffrey E. Hinton</em></p>\n<h5>Abstract</h5>\n<p>We trained a large, deep convolutional neural network to classify the 1.2 million\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\nand 17.0% which is considerably better than the previous state-of-the-art. The\nneural network, which has 60 million parameters and 650,000 neurons, consists\nof five convolutional layers, some of which are followed by max-pooling layers,\nand three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected\nlayers we employed a recently-developed regularization method called \u201cdropout\u201d\nthat proved to be very effective. We also entered a variant of this model in the\nILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%,\ncompared to 26.2% achieved by the second-best entry.</p>\n<p><a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" rel=\"nofollow\">paper</a></p>\n<pre>@article{AlexNet,\ntitle:{ImageNet Classification with Deep Convolutional Neural Networks},\nauthor:{Alex Krizhevsky,Ilya Sutskever,Geoffrey E. Hinton},\njournal={nips},\nyear={2012}\n}\n</pre>\n\n          </div>"}, "last_serial": 6815305, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "6cf54c0f3a33e855ea8994d89e15a28a", "sha256": "4e26c5fdcdd2cc626d17550b102697e47da99c16a6c76d9b2245393c0ff789a8"}, "downloads": -1, "filename": "alexnet_pytorch-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "6cf54c0f3a33e855ea8994d89e15a28a", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5.0", "size": 12224, "upload_time": "2020-02-16T06:53:04", "upload_time_iso_8601": "2020-02-16T06:53:04.091848Z", "url": "https://files.pythonhosted.org/packages/32/4e/6a6efd6edb78c578ef5bf1421b7f7c8fbce2b3882ded988c2fb37939a382/alexnet_pytorch-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d0a2b814bc126fbb7a2e37b777dd4fff", "sha256": "19909d580225269f69afe2aa8f1b59067fd030b1faa4272481ea8814a39d30b1"}, "downloads": -1, "filename": "alexnet_pytorch-0.1.0.tar.gz", "has_sig": false, "md5_digest": "d0a2b814bc126fbb7a2e37b777dd4fff", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 8438, "upload_time": "2020-02-16T06:53:07", "upload_time_iso_8601": "2020-02-16T06:53:07.054074Z", "url": "https://files.pythonhosted.org/packages/4a/f1/90cdd09b167ce97df677f427f7bed6a0fe0229808663872dfe3be7199732/alexnet_pytorch-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "92ddd172a2e6d656807b31236ef8734c", "sha256": "1628f75f15005793410ee630046684dae1d37afc2b9133568920231cced9ac40"}, "downloads": -1, "filename": "alexnet_pytorch-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "92ddd172a2e6d656807b31236ef8734c", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5.0", "size": 12420, "upload_time": "2020-02-16T08:01:18", "upload_time_iso_8601": "2020-02-16T08:01:18.893744Z", "url": "https://files.pythonhosted.org/packages/b7/7d/038e821a63a9e8bc593a5d14cb5ce35c92582fb00c1cd10313709a07c8e8/alexnet_pytorch-0.1.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e9dbcf0cda655b0be3fbdb022734632e", "sha256": "3da701415c1bb4df5098287cb6e46f330453c214a2eafbc8286629ea6f0d1882"}, "downloads": -1, "filename": "alexnet_pytorch-0.1.1.tar.gz", "has_sig": false, "md5_digest": "e9dbcf0cda655b0be3fbdb022734632e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 8664, "upload_time": "2020-02-16T08:01:20", "upload_time_iso_8601": "2020-02-16T08:01:20.943535Z", "url": "https://files.pythonhosted.org/packages/fc/a5/42b6b0b2165b27224cab90c87216ad8ea648b1c61eedd1bcb11cf7ba30d3/alexnet_pytorch-0.1.1.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "d2a1362c09caf16836924cfdc17c8e94", "sha256": "55b021d564c9d15f6fe05b4aa7bbe8424f6a6cf3e83367eae3d733a4064d08fe"}, "downloads": -1, "filename": "alexnet_pytorch-0.1.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "d2a1362c09caf16836924cfdc17c8e94", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5.0", "size": 12427, "upload_time": "2020-02-17T05:03:01", "upload_time_iso_8601": "2020-02-17T05:03:01.195348Z", "url": "https://files.pythonhosted.org/packages/92/74/94f7b3f68f9279bde8e45dc911bb3fd4e57b5373f8e381b42b8cbcfd7be4/alexnet_pytorch-0.1.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "414a8059d52c0ff4bff195f1118e6d1c", "sha256": "8359e2a7b9d2a53c4fbe41ea6d60b683274c6634dccbef2bcbc4c3ee8b26b3bd"}, "downloads": -1, "filename": "alexnet_pytorch-0.1.3.tar.gz", "has_sig": false, "md5_digest": "414a8059d52c0ff4bff195f1118e6d1c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 8687, "upload_time": "2020-02-17T05:03:04", "upload_time_iso_8601": "2020-02-17T05:03:04.012369Z", "url": "https://files.pythonhosted.org/packages/20/7e/6558f0e4bb02f4e8d6d5ae39fd03edf2c8b3dbf21cce6d818902b70477e0/alexnet_pytorch-0.1.3.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "6864492aecb807e825171a62d38ec820", "sha256": "a5a63fd773b9d3e3535daafda8530751ac0cda5de1e9d85ea232286ef3330288"}, "downloads": -1, "filename": "alexnet_pytorch-0.1.5-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "6864492aecb807e825171a62d38ec820", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5.0", "size": 12435, "upload_time": "2020-02-19T12:13:08", "upload_time_iso_8601": "2020-02-19T12:13:08.492271Z", "url": "https://files.pythonhosted.org/packages/05/88/7f8dd42a963dd2c209844b82b8e56f39fe99516581ec94f5ee1dfd52a0c5/alexnet_pytorch-0.1.5-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1de73f6d7b96e7fe7f348b2bec8461e7", "sha256": "14be61d8b6253fc352c63822cc13d186397b1178d423ddd6e8019a979334b7f7"}, "downloads": -1, "filename": "alexnet_pytorch-0.1.5.tar.gz", "has_sig": false, "md5_digest": "1de73f6d7b96e7fe7f348b2bec8461e7", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 8714, "upload_time": "2020-02-19T12:13:11", "upload_time_iso_8601": "2020-02-19T12:13:11.184143Z", "url": "https://files.pythonhosted.org/packages/a8/22/d8ca2dd4cd21c222eb3be26e0e704c83bc2568249c9e185de135cd1ec0b6/alexnet_pytorch-0.1.5.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "96a73eb8c8d39c727def195c4ac1939f", "sha256": "72f634b8a8601360dfdbd100bc1675afab5baaa3fd9a0395a93a25402a259337"}, "downloads": -1, "filename": "alexnet_pytorch-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "96a73eb8c8d39c727def195c4ac1939f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 13043, "upload_time": "2020-03-15T12:04:29", "upload_time_iso_8601": "2020-03-15T12:04:29.476844Z", "url": "https://files.pythonhosted.org/packages/b3/2f/90df0f49aec1cf9aca611f391ff4dcb8d24123bdc71d0ecef164ef4fa6fd/alexnet_pytorch-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "07d11d1831e9839bf248df9a1c4d453c", "sha256": "eca9070090b4033f0593bd5237b6d3b2aeee28d776dbe72cdc756dbd13390eb2"}, "downloads": -1, "filename": "alexnet_pytorch-0.2.0.tar.gz", "has_sig": false, "md5_digest": "07d11d1831e9839bf248df9a1c4d453c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 9247, "upload_time": "2020-03-15T12:04:31", "upload_time_iso_8601": "2020-03-15T12:04:31.089766Z", "url": "https://files.pythonhosted.org/packages/d3/cb/89ca9d4a7928ca4c52c98ec789d2efa93b3bfd6cd991b0d0c71fd634683e/alexnet_pytorch-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "96a73eb8c8d39c727def195c4ac1939f", "sha256": "72f634b8a8601360dfdbd100bc1675afab5baaa3fd9a0395a93a25402a259337"}, "downloads": -1, "filename": "alexnet_pytorch-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "96a73eb8c8d39c727def195c4ac1939f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 13043, "upload_time": "2020-03-15T12:04:29", "upload_time_iso_8601": "2020-03-15T12:04:29.476844Z", "url": "https://files.pythonhosted.org/packages/b3/2f/90df0f49aec1cf9aca611f391ff4dcb8d24123bdc71d0ecef164ef4fa6fd/alexnet_pytorch-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "07d11d1831e9839bf248df9a1c4d453c", "sha256": "eca9070090b4033f0593bd5237b6d3b2aeee28d776dbe72cdc756dbd13390eb2"}, "downloads": -1, "filename": "alexnet_pytorch-0.2.0.tar.gz", "has_sig": false, "md5_digest": "07d11d1831e9839bf248df9a1c4d453c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 9247, "upload_time": "2020-03-15T12:04:31", "upload_time_iso_8601": "2020-03-15T12:04:31.089766Z", "url": "https://files.pythonhosted.org/packages/d3/cb/89ca9d4a7928ca4c52c98ec789d2efa93b3bfd6cd991b0d0c71fd634683e/alexnet_pytorch-0.2.0.tar.gz", "yanked": false}], "timestamp": "Thu May  7 16:19:54 2020"}