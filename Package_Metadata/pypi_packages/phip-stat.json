{"info": {"author": "Laserson Lab", "author_email": "", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3"], "description": "# `phip-stat`: tools for analyzing PhIP-seq data\n\nThe PhIP-seq assay was first described in [Larman et.\nal.](https://dx.doi.org/10.1038/nbt.1856). This repo contains code for\nprocessing raw PhIP-seq data into analysis-ready enrichment scores.\n\nThe overall flow of the pipeline is to split `.fastq` data into pieces and\nalign it against the associated phage library. The alignments are converted to\ncounts, which are then converted to scores based on a Generalized Poisson\nregression model.\n\nThe tools are implemented in Python and can dispatch jobs to an HPC job\nscheduler such as LSF or Grid Engine.\n\nPlease submit [issues](https://github.com/lasersonlab/phip-stat/issues) to\nreport any problems.  This code implements the statistical model as described\nin the original Larman et al paper.  There are multiple alternative models\ncurrently in development.\n\n\n## Installation\n\n`phip-stat` was developed with Python 3 but should also be compatible with\nPython 2. Please submit as issue if there are any problems.\n\n```bash\npip install phip-stat\n```\n\nor to install the latest development version from GitHub\n\n```bash\npip install git+https://github.com/lasersonlab/phip-stat.git\n```\n\nor download a release\n[directly from GitHub](https://github.com/lasersonlab/phip-stat/releases).\n\nDependencies for `phip` (which should get installed by `pip`):\n\n*   `click`\n*   `biopython`\n*   `numpy`\n*   `scipy`\n\nAdditionally, the pipeline makes use of `bowtie` for short read alignment, and\nexpects it to be available on the `PATH`.\n\nWhen running on a cluster, we assume that you can access a common filesystem\nfrom all nodes (e.g. NFS), which is common for academic HPC computing\nenvironments.  It is also important that the `phip-stat` package is installed\ninto the Python distribution that will be invoked across the cluster.  We\nrecommend `conda` for easily installing Python into a local user directory (see\nappendix).\n\n\n## Running the pipeline\n\nAll the pipeline tools are accessed through the executable `phip`.  (Your\n`PATH` may need to be modified to include the `bin/` directory of your Python\ninstallation.)  A list of commands can be obtained by passing `-h`.\n\n```\n$ phip -h\nUsage: phip [OPTIONS] COMMAND [ARGS]...\n\n  phip -- PhIP-seq analysis tools\n\nOptions:\n  -h, --help  Show this message and exit.\n\nCommands:\n  align-parts     align fastq files to peptide reference\n  compute-counts  compute counts from aligned bam file\n  compute-pvals   compute p-values from counts\n  groupby-sample  group alignments by sample\n  join-barcodes   annotate Illumina reads with barcodes Some...\n  merge-columns   merge tab-delim files\n  split-fastq     split fastq files into smaller chunks\n```\n\nOptions/usage for a specific command can also be obtained with `-f`, for\nexample:\n\n```\n$ phip split-fastq -h\nUsage: phip split-fastq [OPTIONS]\n\n  split fastq files into smaller chunks\n\nOptions:\n  -i, --input TEXT          input path (fastq file)\n  -o, --output TEXT         output path (directory)\n  -n, --chunk-size INTEGER  number of reads per chunk\n  -h, --help                Show this message and exit.\n```\n\n\n### Pipeline inputs\n\nThe PhIP-seq pipeline as implemented requires:\n\n1.  Read data from a PhIP-seq experiment (the raw data)\n\n2.  Reference file for PhIP-seq library or bowtie index for alignment\n\n3.  Input counts from sequencing the PhIP-seq library without any\n    immunoprecipitation\n\n\n### Generate per-sample alignment files\n\nIf you start with a single `.fastq` file that contains all the reads together,\nwe'll first split them into smaller chunks for alignment.\n\n```bash\nphip split-fastq -n 2000000 -i path/to/input.fastq -o path/to/workdir/parts\n```\n\nWe then align each read to the reference PhIP-seq library using `bowtie`\n(making sure to set the right queue):\n\n```bash\nphip align-parts \\\n    -i workdir/parts -o workdir/alns \\\n    -x path/to/index \\ # implying path/to/index.1.ebwt exists etc.\n    -b \"bsub -q short\"\n```\n\nNote: `align-parts` works by constructing a `bowtie` command and executing it\nby prefixing it with the command given in to the `-b` option.  Each invocation\nis executed and blocks to completion, which is instantaneous if submitting to a\nbatch scheduler such as LSF (as shown).  If omitted or given whitespace as a\nstring, each command will be executed serially.\n\nNext, we reorganize the resulting alignments by sample, assuming that the\nsample barcode is contained in the \"query id\" of each read.\n\n```bash\nphip groupby-sample -i workdir/alns -o workdir/alns_by_sample -m mapping.tsv\n```\n\nThe `mapping.tsv` file is a tab-delimited file whose first column is a list of\nbarcode sequences and second column is a corresponding sample identifier (which\nshould work nicely as a file name).\n\nIf you instead started this pipeline with pre-demultiplexed `.fastq` files, you\ncan start with `align-parts` immediately and skip the `groupby-sample`.  We\nassume the sample identifier is the base part of the filename (e.g.,\n`sample1.fastq`).\n\n\n### Generating p-values for enrichments\n\nOnce we have per-sample alignment files, we will generate count vectors for\neach sample followed by enrichment p-values (using the \"input\" vector for the\nparticular phage library).\n\n```bash\nphip compute-counts \\\n    -i workdir/alns_by_sample -o workdir/counts -r path/to/reference/counts.tsv\n```\n\nThe count-generation is performed single-threaded/locally, but the p-value\ncomputation is more CPU-intensive so it can be parallelized with a job\nscheduler.\n\n```bash\nphip compute-pvals -i workdir/counts -o workdir/pvals -b \"bsub -q short\"\n```\n\nIt schedules single-file p-value computations using the same command without\nthe `-b`.  To manually compute p-values on just a single count file, execute\n\n```bash\nphip compute-pvals -i workdir/counts/sample1.counts.tsv -o sample1.pvals.tsv\n```\n\nFinally, the p-values from all samples can be merged into a single tab-\ndelimited file:\n\n```bash\nphip merge-columns -i workdir/pvals -o pvals.tsv -p 1\n```\n\nThe `-p 1` tells the command to take the 2nd column from each file (0-indexed).\n\n\n## Appendix\n\n### Using `conda` for easily managing packages\n\nConda is a package manager that works on multiple operating systems and is\nclosely connected to the Anaconda and Miniconda distributions of Python. Using\nConda makes it incredibly easy to install a full Python distribution into your\nlocal directory, along with all the heavy-weight packages. It also manages many\nnon-Python packages, including bowtie, for exmaple.\n\n```bash\ncurl https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh > miniconda3.sh\nbash miniconda3.sh -b -p $HOME/miniconda3\n# ADD $HOME/miniconda3/bin to your PATH in $HOME/.bash_profile\nconda install -y numpy scipy biopython click\nconda install -y bowtie\n```\n\nThis will install Python 3 and bowtie into your home directory, completely\nisolated from the system Python installation.\n\n\n### Joining reads to barcodes/indexes on older Illumina data\n\nIf you use the Basespace \"generate fastq\" pipeline or the MiSeq local pipeline\nfor creating `.fastq` files on indexed runs, then reads that do not match an\nindex (i.e., all of the reads if you will manually demultiplex) will go into an\n\"Unidentified\" file that does NOT include the index sequence for that read.\nInstead, the indexes are available in a separate `.fastq` file.  To use these\ndata sets, the reads `.fastq` file must be rewritten to include the index\nsequence in the read header.  This can be accomplished with the `phip join-\nbarcodes` command.\n\n\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/lasersonlab/phip-stat", "keywords": "", "license": "Apache License, Version 2.0", "maintainer": "", "maintainer_email": "", "name": "phip-stat", "package_url": "https://pypi.org/project/phip-stat/", "platform": "", "project_url": "https://pypi.org/project/phip-stat/", "project_urls": {"Homepage": "https://github.com/lasersonlab/phip-stat"}, "release_url": "https://pypi.org/project/phip-stat/0.3.0/", "requires_dist": ["biopython", "click", "numpy", "scipy"], "requires_python": "", "summary": "PhIP-seq analysis tools", "version": "0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            # `phip-stat`: tools for analyzing PhIP-seq data<br><br>The PhIP-seq assay was first described in [Larman et.<br>al.](https://dx.doi.org/10.1038/nbt.1856). This repo contains code for<br>processing raw PhIP-seq data into analysis-ready enrichment scores.<br><br>The overall flow of the pipeline is to split `.fastq` data into pieces and<br>align it against the associated phage library. The alignments are converted to<br>counts, which are then converted to scores based on a Generalized Poisson<br>regression model.<br><br>The tools are implemented in Python and can dispatch jobs to an HPC job<br>scheduler such as LSF or Grid Engine.<br><br>Please submit [issues](https://github.com/lasersonlab/phip-stat/issues) to<br>report any problems.  This code implements the statistical model as described<br>in the original Larman et al paper.  There are multiple alternative models<br>currently in development.<br><br><br>## Installation<br><br>`phip-stat` was developed with Python 3 but should also be compatible with<br>Python 2. Please submit as issue if there are any problems.<br><br>```bash<br>pip install phip-stat<br>```<br><br>or to install the latest development version from GitHub<br><br>```bash<br>pip install git+https://github.com/lasersonlab/phip-stat.git<br>```<br><br>or download a release<br>[directly from GitHub](https://github.com/lasersonlab/phip-stat/releases).<br><br>Dependencies for `phip` (which should get installed by `pip`):<br><br>*   `click`<br>*   `biopython`<br>*   `numpy`<br>*   `scipy`<br><br>Additionally, the pipeline makes use of `bowtie` for short read alignment, and<br>expects it to be available on the `PATH`.<br><br>When running on a cluster, we assume that you can access a common filesystem<br>from all nodes (e.g. NFS), which is common for academic HPC computing<br>environments.  It is also important that the `phip-stat` package is installed<br>into the Python distribution that will be invoked across the cluster.  We<br>recommend `conda` for easily installing Python into a local user directory (see<br>appendix).<br><br><br>## Running the pipeline<br><br>All the pipeline tools are accessed through the executable `phip`.  (Your<br>`PATH` may need to be modified to include the `bin/` directory of your Python<br>installation.)  A list of commands can be obtained by passing `-h`.<br><br>```<br>$ phip -h<br>Usage: phip [OPTIONS] COMMAND [ARGS]...<br><br>  phip -- PhIP-seq analysis tools<br><br>Options:<br>  -h, --help  Show this message and exit.<br><br>Commands:<br>  align-parts     align fastq files to peptide reference<br>  compute-counts  compute counts from aligned bam file<br>  compute-pvals   compute p-values from counts<br>  groupby-sample  group alignments by sample<br>  join-barcodes   annotate Illumina reads with barcodes Some...<br>  merge-columns   merge tab-delim files<br>  split-fastq     split fastq files into smaller chunks<br>```<br><br>Options/usage for a specific command can also be obtained with `-f`, for<br>example:<br><br>```<br>$ phip split-fastq -h<br>Usage: phip split-fastq [OPTIONS]<br><br>  split fastq files into smaller chunks<br><br>Options:<br>  -i, --input TEXT          input path (fastq file)<br>  -o, --output TEXT         output path (directory)<br>  -n, --chunk-size INTEGER  number of reads per chunk<br>  -h, --help                Show this message and exit.<br>```<br><br><br>### Pipeline inputs<br><br>The PhIP-seq pipeline as implemented requires:<br><br>1.  Read data from a PhIP-seq experiment (the raw data)<br><br>2.  Reference file for PhIP-seq library or bowtie index for alignment<br><br>3.  Input counts from sequencing the PhIP-seq library without any<br>    immunoprecipitation<br><br><br>### Generate per-sample alignment files<br><br>If you start with a single `.fastq` file that contains all the reads together,<br>we'll first split them into smaller chunks for alignment.<br><br>```bash<br>phip split-fastq -n 2000000 -i path/to/input.fastq -o path/to/workdir/parts<br>```<br><br>We then align each read to the reference PhIP-seq library using `bowtie`<br>(making sure to set the right queue):<br><br>```bash<br>phip align-parts \\<br>    -i workdir/parts -o workdir/alns \\<br>    -x path/to/index \\ # implying path/to/index.1.ebwt exists etc.<br>    -b \"bsub -q short\"<br>```<br><br>Note: `align-parts` works by constructing a `bowtie` command and executing it<br>by prefixing it with the command given in to the `-b` option.  Each invocation<br>is executed and blocks to completion, which is instantaneous if submitting to a<br>batch scheduler such as LSF (as shown).  If omitted or given whitespace as a<br>string, each command will be executed serially.<br><br>Next, we reorganize the resulting alignments by sample, assuming that the<br>sample barcode is contained in the \"query id\" of each read.<br><br>```bash<br>phip groupby-sample -i workdir/alns -o workdir/alns_by_sample -m mapping.tsv<br>```<br><br>The `mapping.tsv` file is a tab-delimited file whose first column is a list of<br>barcode sequences and second column is a corresponding sample identifier (which<br>should work nicely as a file name).<br><br>If you instead started this pipeline with pre-demultiplexed `.fastq` files, you<br>can start with `align-parts` immediately and skip the `groupby-sample`.  We<br>assume the sample identifier is the base part of the filename (e.g.,<br>`sample1.fastq`).<br><br><br>### Generating p-values for enrichments<br><br>Once we have per-sample alignment files, we will generate count vectors for<br>each sample followed by enrichment p-values (using the \"input\" vector for the<br>particular phage library).<br><br>```bash<br>phip compute-counts \\<br>    -i workdir/alns_by_sample -o workdir/counts -r path/to/reference/counts.tsv<br>```<br><br>The count-generation is performed single-threaded/locally, but the p-value<br>computation is more CPU-intensive so it can be parallelized with a job<br>scheduler.<br><br>```bash<br>phip compute-pvals -i workdir/counts -o workdir/pvals -b \"bsub -q short\"<br>```<br><br>It schedules single-file p-value computations using the same command without<br>the `-b`.  To manually compute p-values on just a single count file, execute<br><br>```bash<br>phip compute-pvals -i workdir/counts/sample1.counts.tsv -o sample1.pvals.tsv<br>```<br><br>Finally, the p-values from all samples can be merged into a single tab-<br>delimited file:<br><br>```bash<br>phip merge-columns -i workdir/pvals -o pvals.tsv -p 1<br>```<br><br>The `-p 1` tells the command to take the 2nd column from each file (0-indexed).<br><br><br>## Appendix<br><br>### Using `conda` for easily managing packages<br><br>Conda is a package manager that works on multiple operating systems and is<br>closely connected to the Anaconda and Miniconda distributions of Python. Using<br>Conda makes it incredibly easy to install a full Python distribution into your<br>local directory, along with all the heavy-weight packages. It also manages many<br>non-Python packages, including bowtie, for exmaple.<br><br>```bash<br>curl https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh &gt; miniconda3.sh<br>bash miniconda3.sh -b -p $HOME/miniconda3<br># ADD $HOME/miniconda3/bin to your PATH in $HOME/.bash_profile<br>conda install -y numpy scipy biopython click<br>conda install -y bowtie<br>```<br><br>This will install Python 3 and bowtie into your home directory, completely<br>isolated from the system Python installation.<br><br><br>### Joining reads to barcodes/indexes on older Illumina data<br><br>If you use the Basespace \"generate fastq\" pipeline or the MiSeq local pipeline<br>for creating `.fastq` files on indexed runs, then reads that do not match an<br>index (i.e., all of the reads if you will manually demultiplex) will go into an<br>\"Unidentified\" file that does NOT include the index sequence for that read.<br>Instead, the indexes are available in a separate `.fastq` file.  To use these<br>data sets, the reads `.fastq` file must be rewritten to include the index<br>sequence in the read header.  This can be accomplished with the `phip join-<br>barcodes` command.<br><br><br>\n          </div>"}, "last_serial": 2650220, "releases": {"0.3.0": [{"comment_text": "", "digests": {"md5": "3da6d47e73ff9a7021e35f4c63453345", "sha256": "ae641cf9cda25fbfb2798a681e957b9b3bac9f2fd0b8b7c63078cbd5e13fb006"}, "downloads": -1, "filename": "phip_stat-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3da6d47e73ff9a7021e35f4c63453345", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 14670, "upload_time": "2017-02-17T18:32:33", "upload_time_iso_8601": "2017-02-17T18:32:33.004935Z", "url": "https://files.pythonhosted.org/packages/fe/fe/40366b6cb2f29774135c77ff0ac28e378dfe7a4feb14e8ee104779413447/phip_stat-0.3.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bb499353be92c6bb85211597774023eb", "sha256": "e40275fee05a7e2222340b55a96b514255ed6d7b22aebe8ef5995b7b8652a26b"}, "downloads": -1, "filename": "phip-stat-0.3.0.tar.gz", "has_sig": false, "md5_digest": "bb499353be92c6bb85211597774023eb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9078, "upload_time": "2017-02-17T18:32:34", "upload_time_iso_8601": "2017-02-17T18:32:34.376762Z", "url": "https://files.pythonhosted.org/packages/ab/88/630164f8fbae84fc40b86569c4048d4301235ff6f80bf04f17d07bcab81b/phip-stat-0.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3da6d47e73ff9a7021e35f4c63453345", "sha256": "ae641cf9cda25fbfb2798a681e957b9b3bac9f2fd0b8b7c63078cbd5e13fb006"}, "downloads": -1, "filename": "phip_stat-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3da6d47e73ff9a7021e35f4c63453345", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 14670, "upload_time": "2017-02-17T18:32:33", "upload_time_iso_8601": "2017-02-17T18:32:33.004935Z", "url": "https://files.pythonhosted.org/packages/fe/fe/40366b6cb2f29774135c77ff0ac28e378dfe7a4feb14e8ee104779413447/phip_stat-0.3.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bb499353be92c6bb85211597774023eb", "sha256": "e40275fee05a7e2222340b55a96b514255ed6d7b22aebe8ef5995b7b8652a26b"}, "downloads": -1, "filename": "phip-stat-0.3.0.tar.gz", "has_sig": false, "md5_digest": "bb499353be92c6bb85211597774023eb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9078, "upload_time": "2017-02-17T18:32:34", "upload_time_iso_8601": "2017-02-17T18:32:34.376762Z", "url": "https://files.pythonhosted.org/packages/ab/88/630164f8fbae84fc40b86569c4048d4301235ff6f80bf04f17d07bcab81b/phip-stat-0.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:55:30 2020"}