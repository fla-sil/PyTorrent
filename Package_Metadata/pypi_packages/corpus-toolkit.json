{"info": {"author": "Kristopher Kyle", "author_email": "kristopherkyle1@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Corpus-toolkit\nThe corpus-toolkit package grew out of courses in corpus linguistics and learner corpus research. The toolkit attempts to balance simplicity of use, broad application, and scalability. Common corpus analyses such as the calculation of word and n-gram frequency and range, keyness, and collocation are included. In addition, more advanced analyses such as the identification of dependency bigrams (e.g., verb-direct object combinations) and their frequency, range, and strength of association are also included.\n## Install corpus-toolkit\nThe package can be downloaded using pip\n```bash\npip install corpus-toolkit\n```\n### Dependencies\nThe corpus-toolkit package makes use of Spacy for tagging and parsing. However, the package also includes a tokenization and lemmatization function that does not require Spacy. If you want to tag or parse your files, you will need to [install Spacy](https://spacy.io/usage) (and an appropriate [Spacy language model](https://spacy.io/usage/models#quickstart)).\n```bash\npip install -U spacy\npython -m spacy download en_core_web_sm\n```\n## Quickstart guide\nThere are three corpus pre-processing options. The first is to use the **tokenize()** function, which does not rely on a part of speech tagger. The second is to use the **tag()** function, which uses [Spacy](https://spacy.io/) to tokenize and tag the corpus. The third option is to pre-process the corpus in any way you like before using the other functions of the corpus-toolkit package.\n\nThis tutorial presumes that you have downloaded and extracted the [brown_single.zip](https://github.com/kristopherkyle/corpus_toolkit/blob/master/corpus_toolkit/brown_single.zip?raw=true), which is a version of the [Brown corpus](http://clu.uni.no/icame/manuals/BROWN/INDEX.HTM). The folder \"brown_single\" should be in your working directory.\n\n### Load, tokenize, and generate a frequency list\n\n```python\nfrom corpus_toolkit import corpus_tools as ct\nbrown_corp = ct.ldcorpus(\"brown_single\") #load and read corpus\ntok_corp = ct.tokenize(brown_corp) #tokenize corpus - by default this lemmatizes as well\nbrown_freq = ct.frequency(tok_corp) #creates a frequency dictionary\n#note that range can be calculated instead of frequency using the argument calc = \"range\"\nct.head(brown_freq, hits = 10) #print top 10 items\n\n```\n```\nthe     69836\nbe      37689\nof      36365\na       30475\nand     28826\nto      26126\nin      21318\nhe      19417\nhave    11938\nit      10932\n```\nThe functions **ldcorpus()** and **tokenize()** are [Python generators](https://wiki.python.org/moin/Generators), which means that they must be re-declared each time they are used (iterated over). A slightly messier (but more appropriate) way to achieve the results above is to nest the commands.\n```python\nbrown_freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\")))\nct.head(brown_freq, hits = 10)\n```\n```\nthe     69836\nbe      37689\nof      36365\na       30475\nand     28826\nto      26126\nin      21318\nhe      19417\nhave    11938\nit      10932\n```\n### Create a tagged version of your corpus\n\nThe most efficient way to conduct multiple analyses with a tagged corpus is to write a tagged version of your corpus to file and then conduct subsequent analyses with the tagged files. If this is not possible for some reason, one can always run the tagger each time an analysis is conducted.\n\n```python\ntagged_brown = ct.tag(ct.ldcorpus(\"brown_single\"))\nct.write_corpus(\"tagged_brown_single\",tagged_brown) #the first argument is the folder where the tagged files will be written\n```\nThe function **tag()** is also a Python generator, so the preferred way to write a corpus is:\n```python\nct.write_corpus(\"tagged_brown_single\",ct.tag(ct.ldcorpus(\"brown_single\")))\n```\n\nNow, we can reload our tagged corpus using the **reload()** function and generate a part of speech sensitive frequency list.\n\n```python\ntagged_freq = ct.frequency(ct.reload(\"tagged_brown_single\"))\nct.head(tagged_freq, hits = 10)\n```\n```\nthe_DET 69861\nbe_VERB 37800\nof_ADP  36322\nand_CCONJ       28889\na_DET   23069\nin_ADP  20967\nto_PART 15409\nhave_VERB       11978\nto_ADP  10800\nhe_PRON 9801\n```\n## Collocation\n\nUse the **collocator()** function to find collocates for a particular word.\n\n```Python\ncollocates = ct.collocator(ct.tokenize(ct.ldcorpus(\"brown_single\")),\"go\",stat = \"MI\")\n#stat options include: \"MI\", \"T\", \"freq\", \"left\", and \"right\"\n\nct.head(collocates, hits = 10)\n```\n```\ndownstairs      7.875170389265524\nupstairs        6.915812373762869\nbedroom 6.627242875821938\nabroad  6.273134375185426\nre      6.21620730710059\nm       6.211322724303333\nforever 6.174730671124432\nstanley 6.174730671124432\nlet     5.938347287580174\nwrong   5.868744120106091\n```\n\n## Keyness\nKeyness is calculated using two frequency dictionaries (consisting of raw frequency values). Only effect sizes are reported (_p_ values are arguably not particularly useful for keyness analyses). Keyness calculation options include \"log-ratio\", \"%diff\", and \"odds-ratio\".\n\n```python\n#First, generate frequency lists for each corpus\ncorp1freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"corp1\")))\ncorp2freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"corp2\")))\n\n#then calculate Keyness\ncorp_key = ct.keyness(corp1freq,corp2freq, effect = \"log-ratio\")\nct.head(corp_key, hits = 10) #to display top hits\n```\n## N-grams\n\nN-grams are contiguous sequences of _n_ words. The **tokenize()** function can be used to create an n-gram version of a corpus by employing the **ngram** argument. By default, words in an n-gram are separated by two underscores \"\\_\\_\"\n\n```Python\ntrigramfreq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False, ngram = 3))\nct.head(trigramfreq, hits = 10)\n```\n```\none__of__the    404\nthe__united__states     339\nas__well__as    237\nsome__of__the   179\nout__of__the    172\nthe__fact__that 167\ni__do__nt       162\nthe__end__of    149\npart__of__the   144\nit__was__a      143\n```\n\n## Dependency bigrams\nDependency bigrams consist of two words that are syntactically connected via a head-dependent relationship. For example, in the clause \"The player **_kicked_** the **_ball_**\", the main verb **_kicked_** is connected to the noun **_ball_** via a direct object relationship, wherein **_kicked_** is the head and **_ball_** is the dependent.\n\nThe function **dep_bigram()** generates frequency dictionaries for the dependent, the head, and the dependency bigram. In addition, range is calculated along with a complete list of sentences in which the relationship occurs.\n\n```Python\nbg_dict = ct.dep_bigram(ct.ldcorpus(\"brown_single\"),\"dobj\")\nct.head(bg_dict[\"bi_freq\"], hits = 10)\n#other keys include \"dep_freq\", \"head_freq\", and \"range\"\n#also note that the key \"samples\" can be used to obtain a list of sample sentences\n#but, this is not compatible with the ct.head() function (see ct.dep_conc() instead)\n```\n```\n#all dependency bigrams are formatted as dependent_head\nwhat_do 247\nplace_take      84\nwhat_say        80\nhim_told        67\nit_do   63\nthat_do 51\ntime_have       49\nwhat_mean       46\nthis_do 46\nwhat_call       42\n```\n\n### Strength of association\n\nVarious measures of strength of association can calculated between dependents and heads. The **_soa()_** function takes a dictionary generated by the **_dep_bigram()_** function and calculates the strength of association for each dependency bigram.\n\n```Python\nsoa_mi = ct.soa(bg_dict,stat = \"MI\")\n#other stat options include: \"T\", \"faith_dep\", \"faith_head\",\"dp_dep\", and \"dp_head\"\nct.head(soa_mi, hits = 10)\n```\n```\nradiation_ionize        12.037110123486007\nB_paragraph     12.037110123486007\nsuicide_commit  10.648544835568353\nnose_scratch    10.39700606857239\ncalendar_adjust 9.972979786066292\nimagination_capture     9.774075717652213\nnose_blow       9.672113306706759\nEnglish_speak   9.496541742123304\nthroat_clear    9.367258725178337\nexpense_deduct  9.256227412789594\n```\n### Concordance lines for dependency bigrams\nA number of excellent cross-platform GUI- based concordancers such as [AntConc](https://www.laurenceanthony.net/software/antconc/) are freely available, and are likely the preferred method for most concordancing.\n\nHowever, it is difficult to get concordance lines for dependency bigrams without a more advanced program. The **_dep_conc()_** function takes the samples generated by the **_dep_bigram()_** function and creates a random sample of hits (50 hits by default) formatted as an html file.\n\nThe following example will write an html file named \"dobj_results.html\" to your working directory.\n\n```python\nct.dep_conc(bg_dict[\"samples\"],\"dobj_results\")\n```\nWhen opened, the resulting file will include the following:\n\n<html><head><style>dep {color:red;}\n dep_head {color:blue;}</style></head><p><word>A </word><word>fringe </word><word>of </word><word>housing </word><word>and </word><word>gardens </word><dep_head>bearded_dobj_head </dep_head><word>the </word><dep>top_dobj_dep </dep><word>of </word><word>the </word><word>heights </word><word>, </word><word>and </word><word>behind </word><word>it </word><word>were </word><word>sandy </word><word>roads </word><word>leading </word><word>past </word><word>farms </word><word>and </word><word>hayfields </word><word>. </word><word>\n </word><word>39 </word></p><p><word>A </word><word>man </word><word>with </word><word>insomnia </word><word>had </word><word>better </word><dep_head>avoid_dobj_head </dep_head><word>bad </word><dep>dreams_dobj_dep </dep><word>of </word><word>that </word><word>kind </word><word>if </word><word>he </word><word>knew </word><word>what </word><word>was </word><word>good </word><word>for </word><word>him </word><word>. </word><word>\n </word><word>241 </word></p><p><word>He </word><word>simply </word><word>would </word><word>not </word><dep_head>work_dobj_head </dep_head><word>his </word><word>arithmetic </word><dep>problems_dobj_dep </dep><word>when </word><word>the </word><word>teacher </word><word>held </word><word>his </word><word>class </word><word>. </word><word>\n </word><word>192 </word></p><p><word>You </word><word>may </word><word>be </word><word>sure </word><word>he </word><word>marries </word><word>her </word><word>in </word><word>the </word><word>end </word><word>and </word><dep_head>has_dobj_head </dep_head><word>a </word><word>fine </word><word>old </word><word>knockdown </word><dep>fight_dobj_dep </dep><word>with </word><word>the </word><word>brother </word><word>, </word><word>and </word><word>that </word><word>there </word><word>are </word><word>plenty </word><word>of </word><word>minor </word><word>scraps </word><word>along </word><word>the </word><word>way </word><word>to </word><word>ensure </word><word>that </word><word>you </word><word>understand </word><word>what </word><word>the </word><word>word </word><word>Donnybrook </word><word>means </word><word>. </word><word>\n </word><word>198 </word></p><p><word>Anyone </word><word>familiar </word><word>with </word><word>the </word><word>details </word><word>of </word><word>the </word><word>McClellan </word><word>hearings </word><word>must </word><word>at </word><word>once </word><word>realize </word><word>that </word><word>the </word><word>sweetheart </word><word>arrangements </word><dep_head>augmented_dobj_head </dep_head><word>employer </word><dep>profits_dobj_dep </dep><word>far </word><word>more </word><word>than </word><word>they </word><word>augmented </word><word>the </word><word>earnings </word><word>of </word><word>the </word><word>corruptible </word><word>labor </word><word>leaders </word><word>. </word><word>\n </word><word>407 </word></p><p><word>If </word><word>the </word><word>transferor </word><dep_head>has_dobj_head </dep_head><word>substantial </word><dep>assets_dobj_dep </dep><word>other </word><word>than </word><word>the </word><word>claim </word><word>, </word><word>it </word><word>seems </word><word>reasonable </word><word>to </word><word>assume </word><word>no </word><word>corporation </word><word>would </word><word>be </word><word>willing </word><word>to </word><word>acquire </word><word>all </word><word>of </word><word>its </word><word>properties </word><word>in </word><word>the </word><word>dim </word><word>hope </word><word>of </word><word>collecting </word><word>a </word><word>claim </word><word>for </word><word>refund </word><word>of </word><word>taxes </word><word>. </word><word>\n </word><word>433 </word></p><p><word>For </word><word>the </word><word>first </word><word>few </word><word>months </word><word>of </word><word>their </word><word>marriage </word><word>she </word><word>had </word><word>tried </word><word>to </word><word>be </word><word>nice </word><word>about </word><word>Gunny </word><word>, </word><word>going </word><word>out </word><word>with </word><word>him </word><word>to </word><dep_head>watch_dobj_head </dep_head><word>this </word><dep>pearl_dobj_dep </dep><word>without </word><word>price </word><word>stamp </word><word>imperiously </word><word>around </word><word>in </word><word>her </word><word>stall </word><word>. </word><word>\n </word><word>441 </word></p><p><word>If </word><word>the </word><word>site </word><word>is </word><word>on </word><word>a </word><word>reservoir </word><word>, </word><word>the </word><word>level </word><word>of </word><word>the </word><word>water </word><word>at </word><word>various </word><word>seasons </word><word>as </word><word>it </word><dep_head>affects_dobj_head </dep_head><dep>recreation_dobj_dep </dep><word>should </word><word>be </word><word>studied </word><word>. </word><word>\n </word><word>471 </word></p><p><word>She </word><word>thrust </word><word>forward </word><word>through </word><word>the </word><word>shadows </word><word>and </word><word>the </word><word>trees </word><word>that </word><dep_head>resisted_dobj_head </dep_head><dep>her_dobj_dep </dep><word>and </word><word>tried </word><word>to </word><word>fling </word><word>her </word><word>back </word><word>. </word><word>\n </word><word>226 </word></p><p><word>The </word><word>most </word><word>infamous </word><word>of </word><word>all </word><word>was </word><word>launched </word><word>by </word><word>the </word><word>explosion </word><word>of </word><word>the </word><word>island </word><word>of </word><word>Krakatoa </word><word>in </word><word>1883 </word><word>; </word><word>; </word><word>it </word><word>raced </word><word>across </word><word>the </word><word>Pacific </word><word>at </word><word>300 </word><word>miles </word><word>an </word><word>hour </word><word>, </word><dep_head>devastated_dobj_head </dep_head><word>the </word><dep>coasts_dobj_dep </dep><word>of </word><word>Java </word><word>and </word><word>Sumatra </word><word>with </word><word>waves </word><word>100 </word><word>to </word><word>130 </word><word>feet </word><word>high </word><word>, </word><word>and </word><word>pounded </word><word>the </word><word>shore </word><word>as </word><word>far </word><word>away </word><word>as </word><word>San </word><word>Francisco </word><word>. </word><word>\n </word><word>40 </word></p></html>\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://kristopherkyle.github.io/corpus_toolkit/", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "corpus-toolkit", "package_url": "https://pypi.org/project/corpus-toolkit/", "platform": "", "project_url": "https://pypi.org/project/corpus-toolkit/", "project_urls": {"Homepage": "https://kristopherkyle.github.io/corpus_toolkit/"}, "release_url": "https://pypi.org/project/corpus-toolkit/0.29/", "requires_dist": null, "requires_python": "", "summary": "A simple Python toolkit for corpus analyses", "version": "0.29", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Corpus-toolkit</h1>\n<p>The corpus-toolkit package grew out of courses in corpus linguistics and learner corpus research. The toolkit attempts to balance simplicity of use, broad application, and scalability. Common corpus analyses such as the calculation of word and n-gram frequency and range, keyness, and collocation are included. In addition, more advanced analyses such as the identification of dependency bigrams (e.g., verb-direct object combinations) and their frequency, range, and strength of association are also included.</p>\n<h2>Install corpus-toolkit</h2>\n<p>The package can be downloaded using pip</p>\n<pre>pip install corpus-toolkit\n</pre>\n<h3>Dependencies</h3>\n<p>The corpus-toolkit package makes use of Spacy for tagging and parsing. However, the package also includes a tokenization and lemmatization function that does not require Spacy. If you want to tag or parse your files, you will need to <a href=\"https://spacy.io/usage\" rel=\"nofollow\">install Spacy</a> (and an appropriate <a href=\"https://spacy.io/usage/models#quickstart\" rel=\"nofollow\">Spacy language model</a>).</p>\n<pre>pip install -U spacy\npython -m spacy download en_core_web_sm\n</pre>\n<h2>Quickstart guide</h2>\n<p>There are three corpus pre-processing options. The first is to use the <strong>tokenize()</strong> function, which does not rely on a part of speech tagger. The second is to use the <strong>tag()</strong> function, which uses <a href=\"https://spacy.io/\" rel=\"nofollow\">Spacy</a> to tokenize and tag the corpus. The third option is to pre-process the corpus in any way you like before using the other functions of the corpus-toolkit package.</p>\n<p>This tutorial presumes that you have downloaded and extracted the <a href=\"https://github.com/kristopherkyle/corpus_toolkit/blob/master/corpus_toolkit/brown_single.zip?raw=true\" rel=\"nofollow\">brown_single.zip</a>, which is a version of the <a href=\"http://clu.uni.no/icame/manuals/BROWN/INDEX.HTM\" rel=\"nofollow\">Brown corpus</a>. The folder \"brown_single\" should be in your working directory.</p>\n<h3>Load, tokenize, and generate a frequency list</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">corpus_toolkit</span> <span class=\"kn\">import</span> <span class=\"n\">corpus_tools</span> <span class=\"k\">as</span> <span class=\"n\">ct</span>\n<span class=\"n\">brown_corp</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">ldcorpus</span><span class=\"p\">(</span><span class=\"s2\">\"brown_single\"</span><span class=\"p\">)</span> <span class=\"c1\">#load and read corpus</span>\n<span class=\"n\">tok_corp</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"n\">brown_corp</span><span class=\"p\">)</span> <span class=\"c1\">#tokenize corpus - by default this lemmatizes as well</span>\n<span class=\"n\">brown_freq</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">frequency</span><span class=\"p\">(</span><span class=\"n\">tok_corp</span><span class=\"p\">)</span> <span class=\"c1\">#creates a frequency dictionary</span>\n<span class=\"c1\">#note that range can be calculated instead of frequency using the argument calc = \"range\"</span>\n<span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"n\">brown_freq</span><span class=\"p\">,</span> <span class=\"n\">hits</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">)</span> <span class=\"c1\">#print top 10 items</span>\n</pre>\n<pre><code>the     69836\nbe      37689\nof      36365\na       30475\nand     28826\nto      26126\nin      21318\nhe      19417\nhave    11938\nit      10932\n</code></pre>\n<p>The functions <strong>ldcorpus()</strong> and <strong>tokenize()</strong> are <a href=\"https://wiki.python.org/moin/Generators\" rel=\"nofollow\">Python generators</a>, which means that they must be re-declared each time they are used (iterated over). A slightly messier (but more appropriate) way to achieve the results above is to nest the commands.</p>\n<pre><span class=\"n\">brown_freq</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">frequency</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">ldcorpus</span><span class=\"p\">(</span><span class=\"s2\">\"brown_single\"</span><span class=\"p\">)))</span>\n<span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"n\">brown_freq</span><span class=\"p\">,</span> <span class=\"n\">hits</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n</pre>\n<pre><code>the     69836\nbe      37689\nof      36365\na       30475\nand     28826\nto      26126\nin      21318\nhe      19417\nhave    11938\nit      10932\n</code></pre>\n<h3>Create a tagged version of your corpus</h3>\n<p>The most efficient way to conduct multiple analyses with a tagged corpus is to write a tagged version of your corpus to file and then conduct subsequent analyses with the tagged files. If this is not possible for some reason, one can always run the tagger each time an analysis is conducted.</p>\n<pre><span class=\"n\">tagged_brown</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">tag</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">ldcorpus</span><span class=\"p\">(</span><span class=\"s2\">\"brown_single\"</span><span class=\"p\">))</span>\n<span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">write_corpus</span><span class=\"p\">(</span><span class=\"s2\">\"tagged_brown_single\"</span><span class=\"p\">,</span><span class=\"n\">tagged_brown</span><span class=\"p\">)</span> <span class=\"c1\">#the first argument is the folder where the tagged files will be written</span>\n</pre>\n<p>The function <strong>tag()</strong> is also a Python generator, so the preferred way to write a corpus is:</p>\n<pre><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">write_corpus</span><span class=\"p\">(</span><span class=\"s2\">\"tagged_brown_single\"</span><span class=\"p\">,</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">tag</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">ldcorpus</span><span class=\"p\">(</span><span class=\"s2\">\"brown_single\"</span><span class=\"p\">)))</span>\n</pre>\n<p>Now, we can reload our tagged corpus using the <strong>reload()</strong> function and generate a part of speech sensitive frequency list.</p>\n<pre><span class=\"n\">tagged_freq</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">frequency</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">reload</span><span class=\"p\">(</span><span class=\"s2\">\"tagged_brown_single\"</span><span class=\"p\">))</span>\n<span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"n\">tagged_freq</span><span class=\"p\">,</span> <span class=\"n\">hits</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n</pre>\n<pre><code>the_DET 69861\nbe_VERB 37800\nof_ADP  36322\nand_CCONJ       28889\na_DET   23069\nin_ADP  20967\nto_PART 15409\nhave_VERB       11978\nto_ADP  10800\nhe_PRON 9801\n</code></pre>\n<h2>Collocation</h2>\n<p>Use the <strong>collocator()</strong> function to find collocates for a particular word.</p>\n<pre><span class=\"n\">collocates</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">collocator</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">ldcorpus</span><span class=\"p\">(</span><span class=\"s2\">\"brown_single\"</span><span class=\"p\">)),</span><span class=\"s2\">\"go\"</span><span class=\"p\">,</span><span class=\"n\">stat</span> <span class=\"o\">=</span> <span class=\"s2\">\"MI\"</span><span class=\"p\">)</span>\n<span class=\"c1\">#stat options include: \"MI\", \"T\", \"freq\", \"left\", and \"right\"</span>\n\n<span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"n\">collocates</span><span class=\"p\">,</span> <span class=\"n\">hits</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n</pre>\n<pre><code>downstairs      7.875170389265524\nupstairs        6.915812373762869\nbedroom 6.627242875821938\nabroad  6.273134375185426\nre      6.21620730710059\nm       6.211322724303333\nforever 6.174730671124432\nstanley 6.174730671124432\nlet     5.938347287580174\nwrong   5.868744120106091\n</code></pre>\n<h2>Keyness</h2>\n<p>Keyness is calculated using two frequency dictionaries (consisting of raw frequency values). Only effect sizes are reported (<em>p</em> values are arguably not particularly useful for keyness analyses). Keyness calculation options include \"log-ratio\", \"%diff\", and \"odds-ratio\".</p>\n<pre><span class=\"c1\">#First, generate frequency lists for each corpus</span>\n<span class=\"n\">corp1freq</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">frequency</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">ldcorpus</span><span class=\"p\">(</span><span class=\"s2\">\"corp1\"</span><span class=\"p\">)))</span>\n<span class=\"n\">corp2freq</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">frequency</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">ldcorpus</span><span class=\"p\">(</span><span class=\"s2\">\"corp2\"</span><span class=\"p\">)))</span>\n\n<span class=\"c1\">#then calculate Keyness</span>\n<span class=\"n\">corp_key</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">keyness</span><span class=\"p\">(</span><span class=\"n\">corp1freq</span><span class=\"p\">,</span><span class=\"n\">corp2freq</span><span class=\"p\">,</span> <span class=\"n\">effect</span> <span class=\"o\">=</span> <span class=\"s2\">\"log-ratio\"</span><span class=\"p\">)</span>\n<span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"n\">corp_key</span><span class=\"p\">,</span> <span class=\"n\">hits</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">)</span> <span class=\"c1\">#to display top hits</span>\n</pre>\n<h2>N-grams</h2>\n<p>N-grams are contiguous sequences of <em>n</em> words. The <strong>tokenize()</strong> function can be used to create an n-gram version of a corpus by employing the <strong>ngram</strong> argument. By default, words in an n-gram are separated by two underscores \"__\"</p>\n<pre><span class=\"n\">trigramfreq</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">frequency</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">ldcorpus</span><span class=\"p\">(</span><span class=\"s2\">\"brown_single\"</span><span class=\"p\">),</span><span class=\"n\">lemma</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">ngram</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">))</span>\n<span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"n\">trigramfreq</span><span class=\"p\">,</span> <span class=\"n\">hits</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n</pre>\n<pre><code>one__of__the    404\nthe__united__states     339\nas__well__as    237\nsome__of__the   179\nout__of__the    172\nthe__fact__that 167\ni__do__nt       162\nthe__end__of    149\npart__of__the   144\nit__was__a      143\n</code></pre>\n<h2>Dependency bigrams</h2>\n<p>Dependency bigrams consist of two words that are syntactically connected via a head-dependent relationship. For example, in the clause \"The player <strong><em>kicked</em></strong> the <strong><em>ball</em></strong>\", the main verb <strong><em>kicked</em></strong> is connected to the noun <strong><em>ball</em></strong> via a direct object relationship, wherein <strong><em>kicked</em></strong> is the head and <strong><em>ball</em></strong> is the dependent.</p>\n<p>The function <strong>dep_bigram()</strong> generates frequency dictionaries for the dependent, the head, and the dependency bigram. In addition, range is calculated along with a complete list of sentences in which the relationship occurs.</p>\n<pre><span class=\"n\">bg_dict</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">dep_bigram</span><span class=\"p\">(</span><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">ldcorpus</span><span class=\"p\">(</span><span class=\"s2\">\"brown_single\"</span><span class=\"p\">),</span><span class=\"s2\">\"dobj\"</span><span class=\"p\">)</span>\n<span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"n\">bg_dict</span><span class=\"p\">[</span><span class=\"s2\">\"bi_freq\"</span><span class=\"p\">],</span> <span class=\"n\">hits</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"c1\">#other keys include \"dep_freq\", \"head_freq\", and \"range\"</span>\n<span class=\"c1\">#also note that the key \"samples\" can be used to obtain a list of sample sentences</span>\n<span class=\"c1\">#but, this is not compatible with the ct.head() function (see ct.dep_conc() instead)</span>\n</pre>\n<pre><code>#all dependency bigrams are formatted as dependent_head\nwhat_do 247\nplace_take      84\nwhat_say        80\nhim_told        67\nit_do   63\nthat_do 51\ntime_have       49\nwhat_mean       46\nthis_do 46\nwhat_call       42\n</code></pre>\n<h3>Strength of association</h3>\n<p>Various measures of strength of association can calculated between dependents and heads. The <strong><em>soa()</em></strong> function takes a dictionary generated by the <strong><em>dep_bigram()</em></strong> function and calculates the strength of association for each dependency bigram.</p>\n<pre><span class=\"n\">soa_mi</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">soa</span><span class=\"p\">(</span><span class=\"n\">bg_dict</span><span class=\"p\">,</span><span class=\"n\">stat</span> <span class=\"o\">=</span> <span class=\"s2\">\"MI\"</span><span class=\"p\">)</span>\n<span class=\"c1\">#other stat options include: \"T\", \"faith_dep\", \"faith_head\",\"dp_dep\", and \"dp_head\"</span>\n<span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"n\">soa_mi</span><span class=\"p\">,</span> <span class=\"n\">hits</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n</pre>\n<pre><code>radiation_ionize        12.037110123486007\nB_paragraph     12.037110123486007\nsuicide_commit  10.648544835568353\nnose_scratch    10.39700606857239\ncalendar_adjust 9.972979786066292\nimagination_capture     9.774075717652213\nnose_blow       9.672113306706759\nEnglish_speak   9.496541742123304\nthroat_clear    9.367258725178337\nexpense_deduct  9.256227412789594\n</code></pre>\n<h3>Concordance lines for dependency bigrams</h3>\n<p>A number of excellent cross-platform GUI- based concordancers such as <a href=\"https://www.laurenceanthony.net/software/antconc/\" rel=\"nofollow\">AntConc</a> are freely available, and are likely the preferred method for most concordancing.</p>\n<p>However, it is difficult to get concordance lines for dependency bigrams without a more advanced program. The <strong><em>dep_conc()</em></strong> function takes the samples generated by the <strong><em>dep_bigram()</em></strong> function and creates a random sample of hits (50 hits by default) formatted as an html file.</p>\n<p>The following example will write an html file named \"dobj_results.html\" to your working directory.</p>\n<pre><span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">dep_conc</span><span class=\"p\">(</span><span class=\"n\">bg_dict</span><span class=\"p\">[</span><span class=\"s2\">\"samples\"</span><span class=\"p\">],</span><span class=\"s2\">\"dobj_results\"</span><span class=\"p\">)</span>\n</pre>\n<p>When opened, the resulting file will include the following:</p>\n&lt;html&gt;&lt;head&gt;&lt;style&gt;dep {color:red;}\n dep_head {color:blue;}&lt;/style&gt;&lt;/head&gt;<p>&lt;word&gt;A &lt;/word&gt;&lt;word&gt;fringe &lt;/word&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;housing &lt;/word&gt;&lt;word&gt;and &lt;/word&gt;&lt;word&gt;gardens &lt;/word&gt;&lt;dep_head&gt;bearded_dobj_head &lt;/dep_head&gt;&lt;word&gt;the &lt;/word&gt;&lt;dep&gt;top_dobj_dep &lt;/dep&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;heights &lt;/word&gt;&lt;word&gt;, &lt;/word&gt;&lt;word&gt;and &lt;/word&gt;&lt;word&gt;behind &lt;/word&gt;&lt;word&gt;it &lt;/word&gt;&lt;word&gt;were &lt;/word&gt;&lt;word&gt;sandy &lt;/word&gt;&lt;word&gt;roads &lt;/word&gt;&lt;word&gt;leading &lt;/word&gt;&lt;word&gt;past &lt;/word&gt;&lt;word&gt;farms &lt;/word&gt;&lt;word&gt;and &lt;/word&gt;&lt;word&gt;hayfields &lt;/word&gt;&lt;word&gt;. &lt;/word&gt;&lt;word&gt;\n &lt;/word&gt;&lt;word&gt;39 &lt;/word&gt;</p><p>&lt;word&gt;A &lt;/word&gt;&lt;word&gt;man &lt;/word&gt;&lt;word&gt;with &lt;/word&gt;&lt;word&gt;insomnia &lt;/word&gt;&lt;word&gt;had &lt;/word&gt;&lt;word&gt;better &lt;/word&gt;&lt;dep_head&gt;avoid_dobj_head &lt;/dep_head&gt;&lt;word&gt;bad &lt;/word&gt;&lt;dep&gt;dreams_dobj_dep &lt;/dep&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;that &lt;/word&gt;&lt;word&gt;kind &lt;/word&gt;&lt;word&gt;if &lt;/word&gt;&lt;word&gt;he &lt;/word&gt;&lt;word&gt;knew &lt;/word&gt;&lt;word&gt;what &lt;/word&gt;&lt;word&gt;was &lt;/word&gt;&lt;word&gt;good &lt;/word&gt;&lt;word&gt;for &lt;/word&gt;&lt;word&gt;him &lt;/word&gt;&lt;word&gt;. &lt;/word&gt;&lt;word&gt;\n &lt;/word&gt;&lt;word&gt;241 &lt;/word&gt;</p><p>&lt;word&gt;He &lt;/word&gt;&lt;word&gt;simply &lt;/word&gt;&lt;word&gt;would &lt;/word&gt;&lt;word&gt;not &lt;/word&gt;&lt;dep_head&gt;work_dobj_head &lt;/dep_head&gt;&lt;word&gt;his &lt;/word&gt;&lt;word&gt;arithmetic &lt;/word&gt;&lt;dep&gt;problems_dobj_dep &lt;/dep&gt;&lt;word&gt;when &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;teacher &lt;/word&gt;&lt;word&gt;held &lt;/word&gt;&lt;word&gt;his &lt;/word&gt;&lt;word&gt;class &lt;/word&gt;&lt;word&gt;. &lt;/word&gt;&lt;word&gt;\n &lt;/word&gt;&lt;word&gt;192 &lt;/word&gt;</p><p>&lt;word&gt;You &lt;/word&gt;&lt;word&gt;may &lt;/word&gt;&lt;word&gt;be &lt;/word&gt;&lt;word&gt;sure &lt;/word&gt;&lt;word&gt;he &lt;/word&gt;&lt;word&gt;marries &lt;/word&gt;&lt;word&gt;her &lt;/word&gt;&lt;word&gt;in &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;end &lt;/word&gt;&lt;word&gt;and &lt;/word&gt;&lt;dep_head&gt;has_dobj_head &lt;/dep_head&gt;&lt;word&gt;a &lt;/word&gt;&lt;word&gt;fine &lt;/word&gt;&lt;word&gt;old &lt;/word&gt;&lt;word&gt;knockdown &lt;/word&gt;&lt;dep&gt;fight_dobj_dep &lt;/dep&gt;&lt;word&gt;with &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;brother &lt;/word&gt;&lt;word&gt;, &lt;/word&gt;&lt;word&gt;and &lt;/word&gt;&lt;word&gt;that &lt;/word&gt;&lt;word&gt;there &lt;/word&gt;&lt;word&gt;are &lt;/word&gt;&lt;word&gt;plenty &lt;/word&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;minor &lt;/word&gt;&lt;word&gt;scraps &lt;/word&gt;&lt;word&gt;along &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;way &lt;/word&gt;&lt;word&gt;to &lt;/word&gt;&lt;word&gt;ensure &lt;/word&gt;&lt;word&gt;that &lt;/word&gt;&lt;word&gt;you &lt;/word&gt;&lt;word&gt;understand &lt;/word&gt;&lt;word&gt;what &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;word &lt;/word&gt;&lt;word&gt;Donnybrook &lt;/word&gt;&lt;word&gt;means &lt;/word&gt;&lt;word&gt;. &lt;/word&gt;&lt;word&gt;\n &lt;/word&gt;&lt;word&gt;198 &lt;/word&gt;</p><p>&lt;word&gt;Anyone &lt;/word&gt;&lt;word&gt;familiar &lt;/word&gt;&lt;word&gt;with &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;details &lt;/word&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;McClellan &lt;/word&gt;&lt;word&gt;hearings &lt;/word&gt;&lt;word&gt;must &lt;/word&gt;&lt;word&gt;at &lt;/word&gt;&lt;word&gt;once &lt;/word&gt;&lt;word&gt;realize &lt;/word&gt;&lt;word&gt;that &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;sweetheart &lt;/word&gt;&lt;word&gt;arrangements &lt;/word&gt;&lt;dep_head&gt;augmented_dobj_head &lt;/dep_head&gt;&lt;word&gt;employer &lt;/word&gt;&lt;dep&gt;profits_dobj_dep &lt;/dep&gt;&lt;word&gt;far &lt;/word&gt;&lt;word&gt;more &lt;/word&gt;&lt;word&gt;than &lt;/word&gt;&lt;word&gt;they &lt;/word&gt;&lt;word&gt;augmented &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;earnings &lt;/word&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;corruptible &lt;/word&gt;&lt;word&gt;labor &lt;/word&gt;&lt;word&gt;leaders &lt;/word&gt;&lt;word&gt;. &lt;/word&gt;&lt;word&gt;\n &lt;/word&gt;&lt;word&gt;407 &lt;/word&gt;</p><p>&lt;word&gt;If &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;transferor &lt;/word&gt;&lt;dep_head&gt;has_dobj_head &lt;/dep_head&gt;&lt;word&gt;substantial &lt;/word&gt;&lt;dep&gt;assets_dobj_dep &lt;/dep&gt;&lt;word&gt;other &lt;/word&gt;&lt;word&gt;than &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;claim &lt;/word&gt;&lt;word&gt;, &lt;/word&gt;&lt;word&gt;it &lt;/word&gt;&lt;word&gt;seems &lt;/word&gt;&lt;word&gt;reasonable &lt;/word&gt;&lt;word&gt;to &lt;/word&gt;&lt;word&gt;assume &lt;/word&gt;&lt;word&gt;no &lt;/word&gt;&lt;word&gt;corporation &lt;/word&gt;&lt;word&gt;would &lt;/word&gt;&lt;word&gt;be &lt;/word&gt;&lt;word&gt;willing &lt;/word&gt;&lt;word&gt;to &lt;/word&gt;&lt;word&gt;acquire &lt;/word&gt;&lt;word&gt;all &lt;/word&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;its &lt;/word&gt;&lt;word&gt;properties &lt;/word&gt;&lt;word&gt;in &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;dim &lt;/word&gt;&lt;word&gt;hope &lt;/word&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;collecting &lt;/word&gt;&lt;word&gt;a &lt;/word&gt;&lt;word&gt;claim &lt;/word&gt;&lt;word&gt;for &lt;/word&gt;&lt;word&gt;refund &lt;/word&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;taxes &lt;/word&gt;&lt;word&gt;. &lt;/word&gt;&lt;word&gt;\n &lt;/word&gt;&lt;word&gt;433 &lt;/word&gt;</p><p>&lt;word&gt;For &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;first &lt;/word&gt;&lt;word&gt;few &lt;/word&gt;&lt;word&gt;months &lt;/word&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;their &lt;/word&gt;&lt;word&gt;marriage &lt;/word&gt;&lt;word&gt;she &lt;/word&gt;&lt;word&gt;had &lt;/word&gt;&lt;word&gt;tried &lt;/word&gt;&lt;word&gt;to &lt;/word&gt;&lt;word&gt;be &lt;/word&gt;&lt;word&gt;nice &lt;/word&gt;&lt;word&gt;about &lt;/word&gt;&lt;word&gt;Gunny &lt;/word&gt;&lt;word&gt;, &lt;/word&gt;&lt;word&gt;going &lt;/word&gt;&lt;word&gt;out &lt;/word&gt;&lt;word&gt;with &lt;/word&gt;&lt;word&gt;him &lt;/word&gt;&lt;word&gt;to &lt;/word&gt;&lt;dep_head&gt;watch_dobj_head &lt;/dep_head&gt;&lt;word&gt;this &lt;/word&gt;&lt;dep&gt;pearl_dobj_dep &lt;/dep&gt;&lt;word&gt;without &lt;/word&gt;&lt;word&gt;price &lt;/word&gt;&lt;word&gt;stamp &lt;/word&gt;&lt;word&gt;imperiously &lt;/word&gt;&lt;word&gt;around &lt;/word&gt;&lt;word&gt;in &lt;/word&gt;&lt;word&gt;her &lt;/word&gt;&lt;word&gt;stall &lt;/word&gt;&lt;word&gt;. &lt;/word&gt;&lt;word&gt;\n &lt;/word&gt;&lt;word&gt;441 &lt;/word&gt;</p><p>&lt;word&gt;If &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;site &lt;/word&gt;&lt;word&gt;is &lt;/word&gt;&lt;word&gt;on &lt;/word&gt;&lt;word&gt;a &lt;/word&gt;&lt;word&gt;reservoir &lt;/word&gt;&lt;word&gt;, &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;level &lt;/word&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;water &lt;/word&gt;&lt;word&gt;at &lt;/word&gt;&lt;word&gt;various &lt;/word&gt;&lt;word&gt;seasons &lt;/word&gt;&lt;word&gt;as &lt;/word&gt;&lt;word&gt;it &lt;/word&gt;&lt;dep_head&gt;affects_dobj_head &lt;/dep_head&gt;&lt;dep&gt;recreation_dobj_dep &lt;/dep&gt;&lt;word&gt;should &lt;/word&gt;&lt;word&gt;be &lt;/word&gt;&lt;word&gt;studied &lt;/word&gt;&lt;word&gt;. &lt;/word&gt;&lt;word&gt;\n &lt;/word&gt;&lt;word&gt;471 &lt;/word&gt;</p><p>&lt;word&gt;She &lt;/word&gt;&lt;word&gt;thrust &lt;/word&gt;&lt;word&gt;forward &lt;/word&gt;&lt;word&gt;through &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;shadows &lt;/word&gt;&lt;word&gt;and &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;trees &lt;/word&gt;&lt;word&gt;that &lt;/word&gt;&lt;dep_head&gt;resisted_dobj_head &lt;/dep_head&gt;&lt;dep&gt;her_dobj_dep &lt;/dep&gt;&lt;word&gt;and &lt;/word&gt;&lt;word&gt;tried &lt;/word&gt;&lt;word&gt;to &lt;/word&gt;&lt;word&gt;fling &lt;/word&gt;&lt;word&gt;her &lt;/word&gt;&lt;word&gt;back &lt;/word&gt;&lt;word&gt;. &lt;/word&gt;&lt;word&gt;\n &lt;/word&gt;&lt;word&gt;226 &lt;/word&gt;</p><p>&lt;word&gt;The &lt;/word&gt;&lt;word&gt;most &lt;/word&gt;&lt;word&gt;infamous &lt;/word&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;all &lt;/word&gt;&lt;word&gt;was &lt;/word&gt;&lt;word&gt;launched &lt;/word&gt;&lt;word&gt;by &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;explosion &lt;/word&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;island &lt;/word&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;Krakatoa &lt;/word&gt;&lt;word&gt;in &lt;/word&gt;&lt;word&gt;1883 &lt;/word&gt;&lt;word&gt;; &lt;/word&gt;&lt;word&gt;; &lt;/word&gt;&lt;word&gt;it &lt;/word&gt;&lt;word&gt;raced &lt;/word&gt;&lt;word&gt;across &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;Pacific &lt;/word&gt;&lt;word&gt;at &lt;/word&gt;&lt;word&gt;300 &lt;/word&gt;&lt;word&gt;miles &lt;/word&gt;&lt;word&gt;an &lt;/word&gt;&lt;word&gt;hour &lt;/word&gt;&lt;word&gt;, &lt;/word&gt;&lt;dep_head&gt;devastated_dobj_head &lt;/dep_head&gt;&lt;word&gt;the &lt;/word&gt;&lt;dep&gt;coasts_dobj_dep &lt;/dep&gt;&lt;word&gt;of &lt;/word&gt;&lt;word&gt;Java &lt;/word&gt;&lt;word&gt;and &lt;/word&gt;&lt;word&gt;Sumatra &lt;/word&gt;&lt;word&gt;with &lt;/word&gt;&lt;word&gt;waves &lt;/word&gt;&lt;word&gt;100 &lt;/word&gt;&lt;word&gt;to &lt;/word&gt;&lt;word&gt;130 &lt;/word&gt;&lt;word&gt;feet &lt;/word&gt;&lt;word&gt;high &lt;/word&gt;&lt;word&gt;, &lt;/word&gt;&lt;word&gt;and &lt;/word&gt;&lt;word&gt;pounded &lt;/word&gt;&lt;word&gt;the &lt;/word&gt;&lt;word&gt;shore &lt;/word&gt;&lt;word&gt;as &lt;/word&gt;&lt;word&gt;far &lt;/word&gt;&lt;word&gt;away &lt;/word&gt;&lt;word&gt;as &lt;/word&gt;&lt;word&gt;San &lt;/word&gt;&lt;word&gt;Francisco &lt;/word&gt;&lt;word&gt;. &lt;/word&gt;&lt;word&gt;\n &lt;/word&gt;&lt;word&gt;40 &lt;/word&gt;</p>&lt;/html&gt;\n\n          </div>"}, "last_serial": 6055180, "releases": {"0.10": [{"comment_text": "", "digests": {"md5": "15464274ba2baac3dccf28abce7d990f", "sha256": "998ac2f02dd24083b1f8842a420e3210cf89605f75b219ce65498e4bd58823fd"}, "downloads": -1, "filename": "corpus_toolkit-0.10.tar.gz", "has_sig": false, "md5_digest": "15464274ba2baac3dccf28abce7d990f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1691679, "upload_time": "2019-08-13T00:31:18", "upload_time_iso_8601": "2019-08-13T00:31:18.120727Z", "url": "https://files.pythonhosted.org/packages/d4/6a/4114e1301988b877187ad48117160653c9af48bd8443249d6a9058f16602/corpus_toolkit-0.10.tar.gz", "yanked": false}], "0.21": [{"comment_text": "", "digests": {"md5": "b3d18b68080e87ea980169985b1daae3", "sha256": "fc0093e7cf02e4115a630173dac7b0aa6b4bcc3935a34bbc215ba7a062061e33"}, "downloads": -1, "filename": "corpus_toolkit-0.21-py3-none-any.whl", "has_sig": false, "md5_digest": "b3d18b68080e87ea980169985b1daae3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 1699968, "upload_time": "2019-10-09T21:36:37", "upload_time_iso_8601": "2019-10-09T21:36:37.356086Z", "url": "https://files.pythonhosted.org/packages/3b/e0/c5392c64b486d6d8a162379369a028a08cd0193e539d5eb52179db48006c/corpus_toolkit-0.21-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "63d4fb8c528dee2c5542b591c5efa0b5", "sha256": "343733f0bf18f31a4087e77f71acb0f574b64191ed2c147406e9ce75ab932f62"}, "downloads": -1, "filename": "corpus_toolkit-0.21.tar.gz", "has_sig": false, "md5_digest": "63d4fb8c528dee2c5542b591c5efa0b5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1708651, "upload_time": "2019-10-09T21:36:40", "upload_time_iso_8601": "2019-10-09T21:36:40.558788Z", "url": "https://files.pythonhosted.org/packages/1b/f8/dfa2bd8d6ddc37149497c4cb6b1c6d92d1f4f4585d89e123b4b98cb41281/corpus_toolkit-0.21.tar.gz", "yanked": false}], "0.22": [{"comment_text": "", "digests": {"md5": "2075aae2f16eb0b12ba5584d19b8186c", "sha256": "1d5b76d030f7d28e8ac3c460f4cedd60107fad68323d28c545adc90f63fa6f5b"}, "downloads": -1, "filename": "corpus_toolkit-0.22-py3-none-any.whl", "has_sig": false, "md5_digest": "2075aae2f16eb0b12ba5584d19b8186c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 1700013, "upload_time": "2019-10-17T16:31:05", "upload_time_iso_8601": "2019-10-17T16:31:05.426787Z", "url": "https://files.pythonhosted.org/packages/86/c3/d657345776366815a331b76cff5272b5516fcbdbe252218f208762edbcf2/corpus_toolkit-0.22-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b43aecd55855acc9231efe7f654963de", "sha256": "dd77d18ea4a462200c6c9cfab4a4bb14aca50ff7a68e6561e532e473d0024e64"}, "downloads": -1, "filename": "corpus_toolkit-0.22.tar.gz", "has_sig": false, "md5_digest": "b43aecd55855acc9231efe7f654963de", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1708660, "upload_time": "2019-10-17T16:31:17", "upload_time_iso_8601": "2019-10-17T16:31:17.463835Z", "url": "https://files.pythonhosted.org/packages/f1/eb/f1242ba814bd7602904baf7134b39b981d4e0222568484a908abedd31769/corpus_toolkit-0.22.tar.gz", "yanked": false}], "0.23": [{"comment_text": "", "digests": {"md5": "b1409877aacb8bcd5f8bcb46d86a23aa", "sha256": "8a9502d2f3fe824c97b82179b8a45d65ae4fae472159fb65c69f8b96c312b05a"}, "downloads": -1, "filename": "corpus_toolkit-0.23-py3-none-any.whl", "has_sig": false, "md5_digest": "b1409877aacb8bcd5f8bcb46d86a23aa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 1700018, "upload_time": "2019-10-17T16:38:18", "upload_time_iso_8601": "2019-10-17T16:38:18.521453Z", "url": "https://files.pythonhosted.org/packages/1d/92/cea7f2991cdafa5d8ea809c9374f3ecb40a7c721ed5543b3e64e12b4d5e8/corpus_toolkit-0.23-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "489527cd8bec51c56b089b58361347e1", "sha256": "a5a579e48431f7288a4467164bbac147bb6eedafa540cf9b107cce45b5e38553"}, "downloads": -1, "filename": "corpus_toolkit-0.23.tar.gz", "has_sig": false, "md5_digest": "489527cd8bec51c56b089b58361347e1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1708668, "upload_time": "2019-10-17T16:38:35", "upload_time_iso_8601": "2019-10-17T16:38:35.626785Z", "url": "https://files.pythonhosted.org/packages/28/fa/8d0f604c3b0b75f241d160829c7c454cc3e3ac9a71653a4d34b263092abf/corpus_toolkit-0.23.tar.gz", "yanked": false}], "0.24": [{"comment_text": "", "digests": {"md5": "a987cb288e8388ec3438eb4598352344", "sha256": "cfab434e92701584e9863aa1d715e535dab117f30e9310ba6a6ed589ddc78219"}, "downloads": -1, "filename": "corpus_toolkit-0.24-py3-none-any.whl", "has_sig": false, "md5_digest": "a987cb288e8388ec3438eb4598352344", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 1708081, "upload_time": "2019-10-17T16:43:37", "upload_time_iso_8601": "2019-10-17T16:43:37.591315Z", "url": "https://files.pythonhosted.org/packages/28/76/845cf671cc4c7ed3db9531f85cd9f72796b8590bc5d99454ae4bb9231568/corpus_toolkit-0.24-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0bf5a62213a041a49d83e5904c8f7c41", "sha256": "b3d79c020e5aaf90f7f557d8847fba7bbdaf17272c7ecebef6ea868726808ed0"}, "downloads": -1, "filename": "corpus_toolkit-0.24.tar.gz", "has_sig": false, "md5_digest": "0bf5a62213a041a49d83e5904c8f7c41", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1708667, "upload_time": "2019-10-17T16:44:01", "upload_time_iso_8601": "2019-10-17T16:44:01.199858Z", "url": "https://files.pythonhosted.org/packages/3e/de/0b0db1dfb108bf91c7c38e4d7c43e015300f61fa08440276e7012fe511c1/corpus_toolkit-0.24.tar.gz", "yanked": false}], "0.25": [{"comment_text": "", "digests": {"md5": "f6771dbc60928e484f76974f3d20a9b9", "sha256": "349bd86b3690197ee7a9e35002faa90693759d38503f49320a73cef1a238ae2b"}, "downloads": -1, "filename": "corpus_toolkit-0.25-py3-none-any.whl", "has_sig": false, "md5_digest": "f6771dbc60928e484f76974f3d20a9b9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 1708077, "upload_time": "2019-10-17T16:52:51", "upload_time_iso_8601": "2019-10-17T16:52:51.938798Z", "url": "https://files.pythonhosted.org/packages/16/f2/8962b9e2b3998de9b346a248150df3202492ff4b9440ecb256bb8f3f0af9/corpus_toolkit-0.25-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8b409694d7d520f02a9f50c2bbc7d3eb", "sha256": "3ed02214e8d737da949086c2fd745a66dd3f423cd35a8d1b1cf7953e694d9741"}, "downloads": -1, "filename": "corpus_toolkit-0.25.tar.gz", "has_sig": false, "md5_digest": "8b409694d7d520f02a9f50c2bbc7d3eb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1708675, "upload_time": "2019-10-17T16:52:58", "upload_time_iso_8601": "2019-10-17T16:52:58.842780Z", "url": "https://files.pythonhosted.org/packages/4c/56/248ab1f89af1e81d510bdaab7293cc1da0e0e446002c58d1bbc860026206/corpus_toolkit-0.25.tar.gz", "yanked": false}], "0.26": [{"comment_text": "", "digests": {"md5": "e4fe46517b353d012cb129f892ddfad6", "sha256": "11a6a6d594f194cacf89f73c93cfe36df044f70df830d6d128d3f7f5d4e7e02b"}, "downloads": -1, "filename": "corpus_toolkit-0.26-py3-none-any.whl", "has_sig": false, "md5_digest": "e4fe46517b353d012cb129f892ddfad6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 1708077, "upload_time": "2019-10-17T17:01:20", "upload_time_iso_8601": "2019-10-17T17:01:20.826778Z", "url": "https://files.pythonhosted.org/packages/c1/af/e673cf62ea5e8da58c0a1198740b292f01c91e66f702bb85f7e9317a9492/corpus_toolkit-0.26-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cf5b3fa0968d3e3badc1767befac6bf5", "sha256": "72ec344650ce8991da72981ab967cb960b20834301d861160da7c9e5507728e9"}, "downloads": -1, "filename": "corpus_toolkit-0.26.tar.gz", "has_sig": false, "md5_digest": "cf5b3fa0968d3e3badc1767befac6bf5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1700872, "upload_time": "2019-10-17T17:01:29", "upload_time_iso_8601": "2019-10-17T17:01:29.535059Z", "url": "https://files.pythonhosted.org/packages/7f/ec/0e5d21e2e4e63bdf6d9075da515685989f8f0ef056de2729e743615fb6b5/corpus_toolkit-0.26.tar.gz", "yanked": false}], "0.27": [{"comment_text": "", "digests": {"md5": "e3d2fc715992e6d990f9ff75888cd3d6", "sha256": "05f74e34b23d64010967ffb8f4a2db1a9b700635e772471e95cc4e32943157ef"}, "downloads": -1, "filename": "corpus_toolkit-0.27-py3-none-any.whl", "has_sig": false, "md5_digest": "e3d2fc715992e6d990f9ff75888cd3d6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 1708059, "upload_time": "2019-10-17T17:47:13", "upload_time_iso_8601": "2019-10-17T17:47:13.614779Z", "url": "https://files.pythonhosted.org/packages/e9/32/47ff8b6223043588616a5168881502cb6b58258ae08496fc29c306c4dac0/corpus_toolkit-0.27-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7bece7b12b56bca0842c6df8d1a2242d", "sha256": "182a5d910f190ced81a824a3c05014466cc2b90aeab386561920ffd3d6fcfe5f"}, "downloads": -1, "filename": "corpus_toolkit-0.27.tar.gz", "has_sig": false, "md5_digest": "7bece7b12b56bca0842c6df8d1a2242d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1700860, "upload_time": "2019-10-17T17:47:36", "upload_time_iso_8601": "2019-10-17T17:47:36.169345Z", "url": "https://files.pythonhosted.org/packages/20/6e/1be48c11ead3b0bb4f82829a1d5b456589cabb7624bb73a1a4c24e03ddd8/corpus_toolkit-0.27.tar.gz", "yanked": false}], "0.28": [{"comment_text": "", "digests": {"md5": "a7d00c63bf04783e3b3b69c7cf90d74b", "sha256": "bf3fc081b595ea50cf9e33780b5e31110a7308f7758058b1b998f0fc3385868f"}, "downloads": -1, "filename": "corpus_toolkit-0.28-py3-none-any.whl", "has_sig": false, "md5_digest": "a7d00c63bf04783e3b3b69c7cf90d74b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 1716161, "upload_time": "2019-10-17T20:45:25", "upload_time_iso_8601": "2019-10-17T20:45:25.275630Z", "url": "https://files.pythonhosted.org/packages/5f/50/8e61d905dde93c0160b0f4b667dd7118c2ec6739c9f28f86b21eaf97d696/corpus_toolkit-0.28-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "75d0aa5daa7095c34270c9f47c4e5c1d", "sha256": "79070e05134484fb6558ed68cb8e8f0cc33c4d15f977c963af74675871d4bcc3"}, "downloads": -1, "filename": "corpus_toolkit-0.28.tar.gz", "has_sig": false, "md5_digest": "75d0aa5daa7095c34270c9f47c4e5c1d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1708640, "upload_time": "2019-10-17T20:45:45", "upload_time_iso_8601": "2019-10-17T20:45:45.451870Z", "url": "https://files.pythonhosted.org/packages/c5/4a/d3ef62d8d7858dbc49efebafd27e7c302b4a943729e72fa41574c0a60fb1/corpus_toolkit-0.28.tar.gz", "yanked": false}], "0.29": [{"comment_text": "", "digests": {"md5": "d447e95c0e85b9341e89b221bba6c114", "sha256": "19f6b60f2b383ca84c6becfec5a1ac35d9b9f99e2209fa004434e202ea150a20"}, "downloads": -1, "filename": "corpus_toolkit-0.29-py3-none-any.whl", "has_sig": false, "md5_digest": "d447e95c0e85b9341e89b221bba6c114", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 1716393, "upload_time": "2019-10-30T22:45:13", "upload_time_iso_8601": "2019-10-30T22:45:13.405162Z", "url": "https://files.pythonhosted.org/packages/91/fa/da418891abd303752a0759c56ec4c661f81fa3d56c7456184f3812670379/corpus_toolkit-0.29-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ba58940669e0bf928967a03b5fab6ba4", "sha256": "b1a698c4264c4ee8d7c24b0970408906ee61f2111e457dfb14edbe44e3ab0f65"}, "downloads": -1, "filename": "corpus_toolkit-0.29.tar.gz", "has_sig": false, "md5_digest": "ba58940669e0bf928967a03b5fab6ba4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1708989, "upload_time": "2019-10-30T22:45:20", "upload_time_iso_8601": "2019-10-30T22:45:20.267605Z", "url": "https://files.pythonhosted.org/packages/25/3a/9f2a78190973b82926e43b3b42f918dd4a19bb27bc5c9d61894e1211d5e8/corpus_toolkit-0.29.tar.gz", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "1a2c01a8a5ec29b6fe9dd882acbe82fc", "sha256": "56428479d18dde0c50b6c6bd03e39492359e336fd7301050245b30f932681de6"}, "downloads": -1, "filename": "corpus_toolkit-0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "1a2c01a8a5ec29b6fe9dd882acbe82fc", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10549, "upload_time": "2019-08-12T09:45:18", "upload_time_iso_8601": "2019-08-12T09:45:18.956742Z", "url": "https://files.pythonhosted.org/packages/1f/9c/6a5794c22d41960a34ea5d027c69e4be18eb5b6f59dabd2b39a2796bf40c/corpus_toolkit-0.4-py3-none-any.whl", "yanked": false}], "0.5": [{"comment_text": "", "digests": {"md5": "f753ded01d894bb8aa13542d999a1bc3", "sha256": "4d90a8ca9bae7b98759226ab7c32ee2cf78ff21cbea80cf0a649ff765bd6034b"}, "downloads": -1, "filename": "corpus_toolkit-0.5.tar.gz", "has_sig": false, "md5_digest": "f753ded01d894bb8aa13542d999a1bc3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9193, "upload_time": "2019-08-12T09:59:17", "upload_time_iso_8601": "2019-08-12T09:59:17.061080Z", "url": "https://files.pythonhosted.org/packages/a2/26/0295a119c149f410da9d98db2ad27f8fa76a638b03bf31baf7fae908fe6e/corpus_toolkit-0.5.tar.gz", "yanked": false}], "0.6": [{"comment_text": "", "digests": {"md5": "343daeb481b8ccf0bf0fbcfef6aafe60", "sha256": "150d8617d8775f34fbfe35667d075e5b0fd14ce4b2c3bbb096d135f7e0895107"}, "downloads": -1, "filename": "corpus_toolkit-0.6.tar.gz", "has_sig": false, "md5_digest": "343daeb481b8ccf0bf0fbcfef6aafe60", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9178, "upload_time": "2019-08-12T10:05:25", "upload_time_iso_8601": "2019-08-12T10:05:25.705119Z", "url": "https://files.pythonhosted.org/packages/f6/25/35b080fe88048005254ae7ff7c5ca53995a8c345bf9fd45c28ea72a24ccb/corpus_toolkit-0.6.tar.gz", "yanked": false}], "0.7": [{"comment_text": "", "digests": {"md5": "2c5dad26474f9c7cd728e2edd7e6201d", "sha256": "27e85a95aff084672014cd61fcabb4d05721247794c628a238372ebeeb601163"}, "downloads": -1, "filename": "corpus_toolkit-0.7.tar.gz", "has_sig": false, "md5_digest": "2c5dad26474f9c7cd728e2edd7e6201d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9188, "upload_time": "2019-08-12T10:15:09", "upload_time_iso_8601": "2019-08-12T10:15:09.560540Z", "url": "https://files.pythonhosted.org/packages/87/d5/dc800a8bc91f528fa11babb10f97890f3468f8c56090a9107009e5ee81f1/corpus_toolkit-0.7.tar.gz", "yanked": false}], "0.8": [{"comment_text": "", "digests": {"md5": "e777f32e8437f364571db8b79e597e5f", "sha256": "003ffe236a9b3f0f333244a014e5550448176447fa920e88032338c5ebf47b16"}, "downloads": -1, "filename": "corpus_toolkit-0.8.tar.gz", "has_sig": false, "md5_digest": "e777f32e8437f364571db8b79e597e5f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1690738, "upload_time": "2019-08-12T10:26:42", "upload_time_iso_8601": "2019-08-12T10:26:42.525642Z", "url": "https://files.pythonhosted.org/packages/52/f0/fa9ca1b24a5c3c919bbdad6d23809daeb66b5df5e25b60d3de2145f37f0b/corpus_toolkit-0.8.tar.gz", "yanked": false}], "0.9": [{"comment_text": "", "digests": {"md5": "ae324d90c53164fcd5d75accb565ff2c", "sha256": "c169e2470534404f623367e310e0e061905633d6accd7f8676a849ecfb11aa38"}, "downloads": -1, "filename": "corpus_toolkit-0.9.tar.gz", "has_sig": false, "md5_digest": "ae324d90c53164fcd5d75accb565ff2c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1691131, "upload_time": "2019-08-12T23:21:00", "upload_time_iso_8601": "2019-08-12T23:21:00.251635Z", "url": "https://files.pythonhosted.org/packages/54/60/6819aeb268fa3f566b82eab0b8ea9665d57ad6d192fcd4681466f6152a4b/corpus_toolkit-0.9.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d447e95c0e85b9341e89b221bba6c114", "sha256": "19f6b60f2b383ca84c6becfec5a1ac35d9b9f99e2209fa004434e202ea150a20"}, "downloads": -1, "filename": "corpus_toolkit-0.29-py3-none-any.whl", "has_sig": false, "md5_digest": "d447e95c0e85b9341e89b221bba6c114", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 1716393, "upload_time": "2019-10-30T22:45:13", "upload_time_iso_8601": "2019-10-30T22:45:13.405162Z", "url": "https://files.pythonhosted.org/packages/91/fa/da418891abd303752a0759c56ec4c661f81fa3d56c7456184f3812670379/corpus_toolkit-0.29-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ba58940669e0bf928967a03b5fab6ba4", "sha256": "b1a698c4264c4ee8d7c24b0970408906ee61f2111e457dfb14edbe44e3ab0f65"}, "downloads": -1, "filename": "corpus_toolkit-0.29.tar.gz", "has_sig": false, "md5_digest": "ba58940669e0bf928967a03b5fab6ba4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1708989, "upload_time": "2019-10-30T22:45:20", "upload_time_iso_8601": "2019-10-30T22:45:20.267605Z", "url": "https://files.pythonhosted.org/packages/25/3a/9f2a78190973b82926e43b3b42f918dd4a19bb27bc5c9d61894e1211d5e8/corpus_toolkit-0.29.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:06 2020"}