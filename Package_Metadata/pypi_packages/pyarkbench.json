{"info": {"author": "driazati", "author_email": "", "bugtrack_url": null, "classifiers": [], "description": "This library is intended to make it easy to write small benchmarks and view the results.\n\n- [Usage](#usage)\n- [API](#api)\n  * [`Benchmark`](#benchmark)\n    + [`benchmark`](#benchmark)\n    + [`run`](#run)\n    + [`print_results`](#print-results)\n    + [`print_stats`](#print-stats)\n    + [`save_results`](#save-results)\n  * [`cleanup`](#cleanup)\n  * [`default_args`](#default-args)\n    + [`bench`](#bench)\n    + [`stats`](#stats)\n    + [`save`](#save)\n  * [`Timer`](#timer)\n  * [`Commit`](#commit)\n- [Developer Notes](#developer-notes)\n\n# Usage\n\nSee [examples/basic.py](examples/basic.py) for a full working example.\n\n```python\nfrom pyarkbench import Benchmark, Timer, default_args\n\nclass Basic(Benchmark):\n    def benchmark(self):\n        with Timer() as m1:\n            # Do some stuff\n            pass\n\n        with Timer() as m2:\n            # Do some other stuff\n            pass\n\n        return {\n            \"Metric 1 (ms)\": m1.ms_duration,\n            \"Metric 2 (ms)\": m2.ms_duration,\n        }\n\nif __name__ == '__main__':\n    # Initialize the benchmark and use the default command line args\n    bench = Basic(*default_args.bench())\n\n    # Run the benchmark (will run your code in `benchmark` many times, some to warm up and then some where the timer results are save)\n    results = bench.run()\n\n    # View the raw results\n    bench.print_results(results)\n\n    # See aggregate statistics about the results\n    bench.print_stats(results, stats=default_args.stats())\n\n    # Save the results to a JSON file named based on the benchmark class\n    bench.save_results(results, out_dir=default_args.save())\n```\n\n# API\n\n## `Benchmark`\n```python\nBenchmark(self, num_runs: int = 10, warmup_runs: int = 1, quiet: bool = False, commit: pybench.benchmarking_utils.Commit = None)\n```\n\nBenchmarks should extend this class and implement the `benchmark` method.\n\n### `benchmark`\n```python\nBenchmark.benchmark(self) -> Dict[str, float]\n```\n\nThis method must be implemented in your subclass and returns a dictionary\nof metric name to the time captured for that metric.\n\n### `run`\n```python\nBenchmark.run(self) -> Dict[str, Any]\n```\n\nThis is the entry point into your benchmark. It will first run `benchmark()`\n`self.warmup_runs` times without using the resulting timings, then it will\nrun `benchmark()` `self.num_runs` times and return the resulting timings.\n\n### `print_results`\n```python\nBenchmark.print_results(self, results)\n```\n\nPretty print the raw results by JSON dumping them.\n\n### `print_stats`\n```python\nBenchmark.print_stats(self, results, stats=('mean', 'median', 'variance'))\n```\n\nCollects and prints statistics over the results.\n\n### `save_results`\n```python\nBenchmark.save_results(self, results, out_dir, filename=None)\n```\n\nSave the results gathered from benchmarking and metadata about the commit\nto a JSON file named after the type of `self`.\n\n## `cleanup`\n```python\ncleanup()\n```\n\nChurn through a bunch of data, run the garbage collector, and sleep for a\nsecond to \"reset\" the Python interpreter.\n\n## `default_args`\n```python\ndefault_args(self, /, *args, **kwargs)\n```\n\nAdds a bunch of default command line arguments to make orchestrating\nbenchmark runs more convenient. To see all the options, call\n`default_args.init()` and run the script with the `--help` option.\n\n### `bench`\n```python\ndefault_args.bench()\n```\n\nDefault arguments to be passed to a `Benchmark` object\n\n### `stats`\n```python\ndefault_args.stats()\n```\n\nDefault arguments to be passed to the `Benchmark.print_stats` method\n\n### `save`\n```python\ndefault_args.save()\n```\n\nDefault arguments to be passed to the `Benchmark.save_results` method\n\n## `Timer`\n```python\nTimer(self, /, *args, **kwargs)\n```\n\nContext manager object that will time the execution of the statements it\nmanages.\n    `self.start` - start time\n    `self.end` - end time\n    `self.ms_duration` - end - start / 1000 / 1000\n\n## `Commit`\n```python\nCommit(self, time, pr, hash)\n```\n\nWrapper around a git commit\n\n# Developer Notes\n\nTo build this package locally, check it out and run\n\n```bash\npython setup.py develop\n```\n\nTo rebuild these docs, run\n\n```bash\npip install pydoc-markdown\npydocmd simple pybench.Benchmark+ pybench.cleanup pybench.default_args+ pybench.Timer pybench.Commit\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/driazati/pyarkbench", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "pyarkbench", "package_url": "https://pypi.org/project/pyarkbench/", "platform": "", "project_url": "https://pypi.org/project/pyarkbench/", "project_urls": {"Homepage": "http://github.com/driazati/pyarkbench"}, "release_url": "https://pypi.org/project/pyarkbench/1.0.1/", "requires_dist": null, "requires_python": "", "summary": "Benchmarking utilities", "version": "1.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>This library is intended to make it easy to write small benchmarks and view the results.</p>\n<ul>\n<li><a href=\"#usage\" rel=\"nofollow\">Usage</a></li>\n<li><a href=\"#api\" rel=\"nofollow\">API</a>\n<ul>\n<li><a href=\"#benchmark\" rel=\"nofollow\"><code>Benchmark</code></a>\n<ul>\n<li><a href=\"#benchmark\" rel=\"nofollow\"><code>benchmark</code></a></li>\n<li><a href=\"#run\" rel=\"nofollow\"><code>run</code></a></li>\n<li><a href=\"#print-results\" rel=\"nofollow\"><code>print_results</code></a></li>\n<li><a href=\"#print-stats\" rel=\"nofollow\"><code>print_stats</code></a></li>\n<li><a href=\"#save-results\" rel=\"nofollow\"><code>save_results</code></a></li>\n</ul>\n</li>\n<li><a href=\"#cleanup\" rel=\"nofollow\"><code>cleanup</code></a></li>\n<li><a href=\"#default-args\" rel=\"nofollow\"><code>default_args</code></a>\n<ul>\n<li><a href=\"#bench\" rel=\"nofollow\"><code>bench</code></a></li>\n<li><a href=\"#stats\" rel=\"nofollow\"><code>stats</code></a></li>\n<li><a href=\"#save\" rel=\"nofollow\"><code>save</code></a></li>\n</ul>\n</li>\n<li><a href=\"#timer\" rel=\"nofollow\"><code>Timer</code></a></li>\n<li><a href=\"#commit\" rel=\"nofollow\"><code>Commit</code></a></li>\n</ul>\n</li>\n<li><a href=\"#developer-notes\" rel=\"nofollow\">Developer Notes</a></li>\n</ul>\n<h1>Usage</h1>\n<p>See <a href=\"examples/basic.py\" rel=\"nofollow\">examples/basic.py</a> for a full working example.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">pyarkbench</span> <span class=\"kn\">import</span> <span class=\"n\">Benchmark</span><span class=\"p\">,</span> <span class=\"n\">Timer</span><span class=\"p\">,</span> <span class=\"n\">default_args</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">Basic</span><span class=\"p\">(</span><span class=\"n\">Benchmark</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">benchmark</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">with</span> <span class=\"n\">Timer</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">m1</span><span class=\"p\">:</span>\n            <span class=\"c1\"># Do some stuff</span>\n            <span class=\"k\">pass</span>\n\n        <span class=\"k\">with</span> <span class=\"n\">Timer</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">m2</span><span class=\"p\">:</span>\n            <span class=\"c1\"># Do some other stuff</span>\n            <span class=\"k\">pass</span>\n\n        <span class=\"k\">return</span> <span class=\"p\">{</span>\n            <span class=\"s2\">\"Metric 1 (ms)\"</span><span class=\"p\">:</span> <span class=\"n\">m1</span><span class=\"o\">.</span><span class=\"n\">ms_duration</span><span class=\"p\">,</span>\n            <span class=\"s2\">\"Metric 2 (ms)\"</span><span class=\"p\">:</span> <span class=\"n\">m2</span><span class=\"o\">.</span><span class=\"n\">ms_duration</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">'__main__'</span><span class=\"p\">:</span>\n    <span class=\"c1\"># Initialize the benchmark and use the default command line args</span>\n    <span class=\"n\">bench</span> <span class=\"o\">=</span> <span class=\"n\">Basic</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">default_args</span><span class=\"o\">.</span><span class=\"n\">bench</span><span class=\"p\">())</span>\n\n    <span class=\"c1\"># Run the benchmark (will run your code in `benchmark` many times, some to warm up and then some where the timer results are save)</span>\n    <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">bench</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># View the raw results</span>\n    <span class=\"n\">bench</span><span class=\"o\">.</span><span class=\"n\">print_results</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># See aggregate statistics about the results</span>\n    <span class=\"n\">bench</span><span class=\"o\">.</span><span class=\"n\">print_stats</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">,</span> <span class=\"n\">stats</span><span class=\"o\">=</span><span class=\"n\">default_args</span><span class=\"o\">.</span><span class=\"n\">stats</span><span class=\"p\">())</span>\n\n    <span class=\"c1\"># Save the results to a JSON file named based on the benchmark class</span>\n    <span class=\"n\">bench</span><span class=\"o\">.</span><span class=\"n\">save_results</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">,</span> <span class=\"n\">out_dir</span><span class=\"o\">=</span><span class=\"n\">default_args</span><span class=\"o\">.</span><span class=\"n\">save</span><span class=\"p\">())</span>\n</pre>\n<h1>API</h1>\n<h2><code>Benchmark</code></h2>\n<pre><span class=\"n\">Benchmark</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">num_runs</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">warmup_runs</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">quiet</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">commit</span><span class=\"p\">:</span> <span class=\"n\">pybench</span><span class=\"o\">.</span><span class=\"n\">benchmarking_utils</span><span class=\"o\">.</span><span class=\"n\">Commit</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">)</span>\n</pre>\n<p>Benchmarks should extend this class and implement the <code>benchmark</code> method.</p>\n<h3><code>benchmark</code></h3>\n<pre><span class=\"n\">Benchmark</span><span class=\"o\">.</span><span class=\"n\">benchmark</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span>\n</pre>\n<p>This method must be implemented in your subclass and returns a dictionary\nof metric name to the time captured for that metric.</p>\n<h3><code>run</code></h3>\n<pre><span class=\"n\">Benchmark</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>\n</pre>\n<p>This is the entry point into your benchmark. It will first run <code>benchmark()</code>\n<code>self.warmup_runs</code> times without using the resulting timings, then it will\nrun <code>benchmark()</code> <code>self.num_runs</code> times and return the resulting timings.</p>\n<h3><code>print_results</code></h3>\n<pre><span class=\"n\">Benchmark</span><span class=\"o\">.</span><span class=\"n\">print_results</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">results</span><span class=\"p\">)</span>\n</pre>\n<p>Pretty print the raw results by JSON dumping them.</p>\n<h3><code>print_stats</code></h3>\n<pre><span class=\"n\">Benchmark</span><span class=\"o\">.</span><span class=\"n\">print_stats</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">results</span><span class=\"p\">,</span> <span class=\"n\">stats</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"s1\">'mean'</span><span class=\"p\">,</span> <span class=\"s1\">'median'</span><span class=\"p\">,</span> <span class=\"s1\">'variance'</span><span class=\"p\">))</span>\n</pre>\n<p>Collects and prints statistics over the results.</p>\n<h3><code>save_results</code></h3>\n<pre><span class=\"n\">Benchmark</span><span class=\"o\">.</span><span class=\"n\">save_results</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">results</span><span class=\"p\">,</span> <span class=\"n\">out_dir</span><span class=\"p\">,</span> <span class=\"n\">filename</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">)</span>\n</pre>\n<p>Save the results gathered from benchmarking and metadata about the commit\nto a JSON file named after the type of <code>self</code>.</p>\n<h2><code>cleanup</code></h2>\n<pre><span class=\"n\">cleanup</span><span class=\"p\">()</span>\n</pre>\n<p>Churn through a bunch of data, run the garbage collector, and sleep for a\nsecond to \"reset\" the Python interpreter.</p>\n<h2><code>default_args</code></h2>\n<pre><span class=\"n\">default_args</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"o\">/</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n</pre>\n<p>Adds a bunch of default command line arguments to make orchestrating\nbenchmark runs more convenient. To see all the options, call\n<code>default_args.init()</code> and run the script with the <code>--help</code> option.</p>\n<h3><code>bench</code></h3>\n<pre><span class=\"n\">default_args</span><span class=\"o\">.</span><span class=\"n\">bench</span><span class=\"p\">()</span>\n</pre>\n<p>Default arguments to be passed to a <code>Benchmark</code> object</p>\n<h3><code>stats</code></h3>\n<pre><span class=\"n\">default_args</span><span class=\"o\">.</span><span class=\"n\">stats</span><span class=\"p\">()</span>\n</pre>\n<p>Default arguments to be passed to the <code>Benchmark.print_stats</code> method</p>\n<h3><code>save</code></h3>\n<pre><span class=\"n\">default_args</span><span class=\"o\">.</span><span class=\"n\">save</span><span class=\"p\">()</span>\n</pre>\n<p>Default arguments to be passed to the <code>Benchmark.save_results</code> method</p>\n<h2><code>Timer</code></h2>\n<pre><span class=\"n\">Timer</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"o\">/</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n</pre>\n<p>Context manager object that will time the execution of the statements it\nmanages.\n<code>self.start</code> - start time\n<code>self.end</code> - end time\n<code>self.ms_duration</code> - end - start / 1000 / 1000</p>\n<h2><code>Commit</code></h2>\n<pre><span class=\"n\">Commit</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">time</span><span class=\"p\">,</span> <span class=\"n\">pr</span><span class=\"p\">,</span> <span class=\"nb\">hash</span><span class=\"p\">)</span>\n</pre>\n<p>Wrapper around a git commit</p>\n<h1>Developer Notes</h1>\n<p>To build this package locally, check it out and run</p>\n<pre>python setup.py develop\n</pre>\n<p>To rebuild these docs, run</p>\n<pre>pip install pydoc-markdown\npydocmd simple pybench.Benchmark+ pybench.cleanup pybench.default_args+ pybench.Timer pybench.Commit\n</pre>\n\n          </div>"}, "last_serial": 6540550, "releases": {"1.0": [{"comment_text": "", "digests": {"md5": "63da8c712c41336409e85bb6d58b9600", "sha256": "2199142ee41716accb96b5c03f326453a3816fb8499c65a4cdb316bd36863c11"}, "downloads": -1, "filename": "pyarkbench-1.0.tar.gz", "has_sig": false, "md5_digest": "63da8c712c41336409e85bb6d58b9600", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5662, "upload_time": "2020-01-29T22:59:44", "upload_time_iso_8601": "2020-01-29T22:59:44.109047Z", "url": "https://files.pythonhosted.org/packages/be/73/1c6bf971704825fcb51e8b1e214d24c27d14e090cf98c8a0f5a073cafe79/pyarkbench-1.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "a0e7c929f7642602acbb2021000ca37f", "sha256": "ea89fab273dbcdda4f6c807359c7d0d3cf7f7c666656e67636c19c254e89c22a"}, "downloads": -1, "filename": "pyarkbench-1.0.1.tar.gz", "has_sig": false, "md5_digest": "a0e7c929f7642602acbb2021000ca37f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6275, "upload_time": "2020-01-29T23:02:37", "upload_time_iso_8601": "2020-01-29T23:02:37.302729Z", "url": "https://files.pythonhosted.org/packages/12/19/370401ea4219ced3f4302e707d1a319a16b89a16e4fc574a1bcb22dbafd8/pyarkbench-1.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a0e7c929f7642602acbb2021000ca37f", "sha256": "ea89fab273dbcdda4f6c807359c7d0d3cf7f7c666656e67636c19c254e89c22a"}, "downloads": -1, "filename": "pyarkbench-1.0.1.tar.gz", "has_sig": false, "md5_digest": "a0e7c929f7642602acbb2021000ca37f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6275, "upload_time": "2020-01-29T23:02:37", "upload_time_iso_8601": "2020-01-29T23:02:37.302729Z", "url": "https://files.pythonhosted.org/packages/12/19/370401ea4219ced3f4302e707d1a319a16b89a16e4fc574a1bcb22dbafd8/pyarkbench-1.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:10:21 2020"}