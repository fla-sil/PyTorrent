{"info": {"author": "William Bruschi", "author_email": "william.bruschi@gmail.com", "bugtrack_url": null, "classifiers": ["Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "pghops is a command line PostgreSQL schema migration utility written\nin Python. It aims to be the simplest database migration utility for\nPostgreSQL.\n\n1. [Features](#features)\n2. [Demo](#demo)\n3. [Usage Overview](#usage-overview)\n4. [Installation](#installation)\n5. [Best Practices](#best-practices)\n6. [Options](#options)\n7. [Managing Indexes](#managing-indexes)\n8. [Unit Testing](#unit-testing)\n9. [FAQ](#faq)\n10. [Miscellaneous](#miscellaneous)\n11. [License](#license)\n\n## Features\n\n* **Simple version file syntax:** pghops version files are yaml files\n  with keys representing directories and values of one or more sql\n  file names.\n* **Executes scripts with psql:** pghops uses psql to execute all sql,\n  leveraging the extensive functionality of the PostgreSQL client. Use\n  any psql command in your scripts.\n* **Unit testing framework:** pghops comes equipped with its own unit\n  testing framework. No more excuses for skipping sql unit tests!\n* **All or nothing migrations:** Wrap your entire migration in a\n  single transaction or each migration script in its own transaction.\n* **All sql commands saved to version table** pghops saves all sql\n  executed during migrations to its version table. Make the auditors\n  happy!\n\n## Demo\n\nThe below terminal session shows how to create a database named `mydb`\nthat contains two tables: `account` and `account_email`. We will\ncreate a simple test to ensure you cannot insert null emails into the\n`account_email` table. Then we will create a database function for\ncreating accounts, along with another unit test.\n\n```\n[mycluster]$ # Create a directory named after the database, along with a script to create the database.\n[mycluster]$ mkdir mydb\n[mycluster]$ echo \"create database mydb;\" > mydb/create_database.sql\n[mycluster]$ # Create a directory to hold our table definitions.\n[mycluster]$ mkdir -p mydb/schemas/public/tables\n[mycluster]$ # Create SQL files containing our table definitions.\n[mycluster]$ cat - > mydb/schemas/public/tables/account.sql <<EOF\n> create table if not exists public.account (\n>   account_id bigserial primary key\n> );\n> EOF\n[mycluster]$ cat - > mydb/schemas/public/tables/account_email.sql <<EOF\n> create table if not exists public.account_email (\n>   account_email_id bigserial primary key\n>   , account_id bigint not null references account\n>   , email text not null\n> );\n> EOF\n[mycluster]$ # Create our first migration file\n[mycluster]$ mkdir mydb/versions\n[mycluster]$ cat - > mydb/versions/0001.0001.0001.init.yml <<EOF\n> public/tables:\n>   - account\n>   - account_email\n> EOF\n[mycluster]$ # Create our first unit test to ensure we cannot insert NULLs into account_email.email\n[mycluster]$ mkdir mydb/tests\n[mycluster]$ cat - > mydb/tests/01_account_email_test.sql <<EOF\n> insert into account values (default);\n> insert into account_email (account_id, email) values ((select max(account_id) from account), null);\n> EOF\n[mycluster]$ # Generated the 'expected' file and review it.\n[mycluster]$ pghops_test generate\n2019-02-23 14:18:42.452426: Looping through tests in /tmp/mycluster/mydb/tests\n2019-02-23 14:18:42.458661: Stopping Postgres pghops-postgresql.\n2019-02-23 14:18:42.476721: Starting Postgres pghops-postgresql postgres.\n2019-02-23 14:18:45.632209: Done starting postgres pghops-postgresql.\n2019-02-23 14:18:45.673046: Migrating cluster /tmp/mycluster.\n2019-02-23 14:18:45.673440: Migrating database mydb\n2019-02-23 14:18:45.674398: Database mydb does not exist. Creating it with /tmp/mycluster/mydb/create_database.sql.\ncreate database mydb;\nCREATE DATABASE\n\n...\n<output elided>\n...\n\n2019-02-23 14:18:46.106990: Done migrating database mydb\n2019-02-23 14:18:46.107123: Done all migrations.\n2019-02-23 14:18:46.132066: Generated 01_account_email_test.sql expected file.\n2019-02-23 14:18:46.132145: Stopping Postgres pghops-postgresql.\n2019-02-23 14:18:48.449079: Done generating expected files!\n[mycluster]$ # Review the expected file.\n[mycluster]$ cat mydb/tests/01_account_email_expected.txt\ninsert into account values (default);\nINSERT 0 1\ninsert into account_email (account_id, email) values ((select max(account_id) from account), null);\nERROR:  null value in column \"email\" violates not-null constraint\nDETAIL:  Failing row contains (1, 1, null).\n[mycluster]$ # Looks good! We received the error as expected. As a sanity check, run the tests and they should succeeded\n[mycluster]$ pghops_test run\n\n...\n<output elided>\n...\n\n2019-02-23 14:22:31.269604: All tests passed!\n[mycluster]$ # Lets run our first migration against a real db!\n[mycluster]$ pghops\n2019-02-23 14:23:47.225273: Migrating cluster /tmp/mycluster.\n2019-02-23 14:23:47.225539: Migrating database mydb\n2019-02-23 14:23:47.226114: Database mydb does not exist. Creating it with /tmp/mycluster/mydb/create_database.sql.\nCREATE DATABASE\n\n\nBEGIN\nCREATE SCHEMA\nCREATE TABLE\nCREATE TABLE\nCREATE INDEX\nINSERT 0 1\nCREATE TABLE\nCREATE TABLE\nINSERT 0 1\nCOMMIT\n\n\n2019-02-23 14:23:47.827395: Done migrating database mydb\n2019-02-23 14:23:47.827536: Done all migrations.\n[mycluster]$ # Check the version table if you wish\n[mycluster]$ psql --dbname=mydb --command=\"select major, minor, patch, label, file_name from pghops.version;\"\n major | minor | patch |    label    |            file_name\n-------+-------+-------+-------------+---------------------------------\n 0000  | 0000  | 0000  | pghops-init | 0000.0000.0000.pghops-init.yaml\n 0001  | 0001  | 0001  | init        | 0001.0001.0001.init.yml\n(2 rows)\n\n[mycluster]$ # Create a function that creates accounts.\n[mycluster]$ mkdir mydb/schemas/public/functions\n[mycluster]$ cat - > mydb/schemas/public/functions/create_account.sql <<EOF\ncreate or replace function public.create_account\n> (\n>   p_email text\n> )\n> returns bigint\n> language plpgsql\n> as \\$\\$\n> declare l_account_id bigint;\n> begin\n>\n>   insert into account (account_id) values (default) returning account_id into l_account_id;\n>\n>   insert into account_email (account_id, email) values (l_account_id, p_email);\n>\n>   return l_account_id;\n>\n> end\\$\\$;\n> EOF\n[mycluster]$ # Next create our second migration file.\n[mycluster]$ cat - > mydb/versions/0001.0002.0001.create_account.yml <<EOF\n> public/functions: create_account\n> EOF\n[mycluster]$ # Create our second test\n[mycluster]$ echo \"select create_account('x@example.com');\" > mydb/tests/02_create_account_test.sql\n[mycluster]$ pghops_test generate\n2019-02-23 14:35:44.742060: Looping through tests in /tmp/mycluster/mydb/tests\n2019-02-23 14:35:44.748353: Stopping Postgres pghops-postgresql.\n2019-02-23 14:35:44.764216: Starting Postgres pghops-postgresql postgres.\n2019-02-23 14:35:47.726734: Done starting postgres pghops-postgresql.\n2019-02-23 14:35:47.767687: Migrating cluster /tmp/mycluster.\n2019-02-23 14:35:47.768116: Migrating database mydb\n2019-02-23 14:35:47.769223: Database mydb does not exist. Creating it with /tmp/mycluster/mydb/create_database.sql.\n...\n<output elided>\n...\n\n2019-02-23 14:35:48.230893: Done migrating database mydb\n2019-02-23 14:35:48.230981: Done all migrations.\n2019-02-23 14:35:48.251236: Generated 01_account_email_test.sql expected file.\n2019-02-23 14:35:48.269521: Generated 02_create_account_test.sql expected file.\n2019-02-23 14:35:48.269596: Stopping Postgres pghops-postgresql.\n2019-02-23 14:35:50.672626: Done generating expected files!\n[mycluster]$ cat mydb/tests/02_create_account_expected.txt\nselect create_account('x@example.com');\n create_account\n----------------\n              2\n(1 row)\n[mycluster]$ # Our new db function works! Lets run a migartion to update our db\n[mycluster]$ pghops\n2019-02-23 14:37:13.523780: Migrating cluster /tmp/mycluster.\n2019-02-23 14:37:13.524050: Migrating database mydb\nBEGIN\nCREATE FUNCTION\nINSERT 0 1\nCOMMIT\n\n\n2019-02-23 14:37:13.572099: Done migrating database mydb\n2019-02-23 14:37:13.572224: Done all migrations.\n[mycluster]$ psql --dbname=mydb --command=\"select major, minor, patch, label, file_name from pghops.version;\"\n major | minor | patch |     label      |             file_name\n-------+-------+-------+----------------+-----------------------------------\n 0000  | 0000  | 0000  | pghops-init    | 0000.0000.0000.pghops-init.yaml\n 0001  | 0001  | 0001  | init           | 0001.0001.0001.init.yml\n 0001  | 0002  | 0001  | create_account | 0001.0002.0001.create_account.yml\n(3 rows)\n\n[mycluster]$\n\n\n```\n\n## Usage Overview\n\nWhen you install PostgreSQL you initialize a storage area on disk\ncalled a [database\ncluster](https://www.postgresql.org/docs/current/creating-cluster.html),\nwhich is a collection of databases managed by a single instance of\nPostgreSQL. pghops expects you to place all files associated to\nbuilding and defining your cluster in a single directory, referred to\nhenceforth as the `cluster_directory`. Each sub-directory in\n`cluster_directory` should be the name of a database within your\ncluster (if not, you can add a file named `databases` that contains\nthe list of database directories).\n\nFor example, say your `cluster_directory` is /tmp/pghops/main and you\nhave two databases - dba and dbb. Your directory structure would look\nlike:\n```\n\u2514\u2500\u2500 main\n    \u251c\u2500\u2500 dba\n    \u2514\u2500\u2500 dbb\n```\n\npghops requires each database directory to have a sub-directory named\n`versions` which contain, you guessed it, all of you database\nmigration files. Each migration file must follow the following\nversioning convention:\n\n`<major>.<minor>.<patch>.<label>.yml`\n\nThis allows you to follow [Semantic Versioning](https://semver.org/)\nif you choose. pghops parses these file names and saves them to the\n`pghops.version` table.\n\nIf pghops detects the database does not exist on the cluster, pghops\nwill create it if the database directory has a file named\n`create_database.sql` containing the database creation\ncommands. pghops records all migrations in a table named `version` in\nthe schema `pghops`. If this table does not exist, pghops will run the\nincluded `0000.0000.0000.pghops-init.yaml` script first to create it.\n\nEach version file must be in yaml format and have a yaml or yml\nsuffix. The file can only contain comments and key / value pairs, with\nkeys representing directories and values of either a single file or a\nlist of files to execute. Directories can be absolute or relative to\neither the database directory or a directory named `schemas` within\nthe database directory. We recommend laying out your directory\nstructure the same as pgAdmin's. For example, if your\n`cluster_directory` looks like:\n```\n\u251c\u2500\u2500 cluster_a\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 databases\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 db_a1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 create_database.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 public\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 functions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 tables\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 visits.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 views\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u251c\u2500\u2500 vistor_view.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 location_view.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 versions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 0000.0011.0001.change-a.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 0000.0021.0002.change-b.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 0000.0032.0000.change-c.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 db_a2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 create_database.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 init-data.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 public\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 functions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 tables\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 user.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 views\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u251c\u2500\u2500 user_view.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 accounts_view.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 versions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 0000.0001.0001.feature-a.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 0000.0001.0002.feature-b.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 0000.0002.0000.feature-c.yml\n```\n\nand you want to use pghops to create new views defined in visitor_view\nand location_view, create a new migration script in db_a1/versions\nsuch as `0000.0033.0000.new-views.yml` and add the lines:\n\n```\nschemas/public/views:\n  - visitor_view.sql\n  - location_view.sql\n```\n\nYou can optionally omit the sql suffix. Again, `schemas` is optional\nas well.\n\nRun pghops by cd'ing into cluster_directory and running\n\n```\npghops\n```\n\nSee below for command line parameters. You can also pass the path of\n`cluster_directory` as the first argument.\n\nWhen you run pghops, it will concatenate the contents of\nvisitor_view.sql and location_view.sql into a single file and then\nexecute it via psql in a single transaction. If successful, a new\nrecord is added to pghops.version and your migration is complete! For\nmore examples see the [test\nclusters](src/tests/test_clusters/cluster_a).\n\n## Installation\n\n`pghops` requires python 3.7 and the psql client. `pghops_test`\nrequires a docker compatible container runtime. Install `pghops` with\npip:\n\n```\npip3 install pghops\n```\n\nThis should add the executeables `pghops`, `pghops_test`, and\n`pghops_create_indexes` to your path.\n\n## Best Practices\n\n### Directory layout for your sql code\nI recommend following the same layout as pgAdmin. For example, if you\nhave a database named dba, one possibility is:\n```\n\u251c\u2500\u2500 dba\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 myschema\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 functions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tables\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 views\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 public\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 functions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 tables\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 views\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 versions\n```\nThe `data` directory can contain scripts to load data during your\nmigrations.\n\n### Versioning\npghops is liberal when determining which migration files to\nexecute. It ignores the major, minor, and patch fields in the\npghops.version table and only looks at file_names.\n\nAs such, you can use whichever versioning scheme you like. [Semantic\nVersioning](https://semver.org/) is definitely a solid option. Another\nscheme, which requires slightly more effort for tracking but works\nwell when dealing with multiple people working with many branches, is\nto use an auto-incrementing number for `major` that increases on every\nmerge into your master/production branch. For `minor`, use something\nthat refers to either a feature branch or something that links back to\na ticketing system. For `patch`, use an incrementing number for\neach migration file you create for the feature. Use `label` to\ndifferentiate between two people creating migration scripts for the\nsame feature at the same time. This also helps to prevent merge\nconflicts.\n\n### Idempotency\nEssentially this means if you execute the same sql twice all changes\nwill only take affect once. So use \"if not exists\" when writing DDL\nstatements and check for the presence of your records first when\nexecuting update statements (or use the `on conflict do nothing`\nclause).\n\n### Keep old migration files up to date\nThe pghops.version table and Git (or another VCS) should be all you\nneed for auditing and history purposes. If you make changes that would\nbreak older migration scripts when run on a new database, best to go\nback and update the older scripts. Then you can use pghops to create a\nnew database from scratch for failover, setting up new environments,\nor testing purposes.\n\n### Passwords and psql\n\nNormally you want to avoid having to enter your password for every\npsql call. A couple options:\n\n1. Setup the user that runs pghops with password-less authentication,\n   such as [trust or\n   peer](https://www.postgresql.org/docs/current/auth-pg-hba-conf.html). Best\n   then to run pghops on the same box as PostgreSQL.\n2. Use a [password\n   file](https://www.postgresql.org/docs/current/libpq-pgpass.html).\n\n## Options\n\npghops has many configuration options, which you can set via the\ncommand line, environment variables or various property files. Options\nare loaded in the following order, from highest to lowest priority:\n\n1. Command line arguments\n2. Properties in the file specified by the `--options-file` command line\n   argument.\n3. Environment variables.\n4. Properties in `<cluster-dir>/<db>/pghops.properties`\n5. Properties in `<cluster-dir>/pghops.properties`\n6. Properties in `pghops/conf/default.properties`\n\nProperty files should be in yaml format and contain key/value pairs.\n\npghops treats options in property files that only differ in case or\nusage of underscore versus hyphen the same. For example:\n\n```\nwrap-all-in-transaction\nwrap_all_in_transaction\nWrap_All_In_Transaction\n```\n\nall refer to the same option. Environment variables should use\nunderscores instead of hyphens, be in all caps, and have a prefix of\nPGHOPS_. For example, the environment varaible for the\nwrap-all-in-transaction property above is\nPGHOPS_WRAP_ALL_IN_TRANSACTION.\n\npsql's environment variables are also in effect.\n\npghops options are as follows:\n\n**cluster_directory** - The first argument to pghops. Defaults to the\ncurrent working directory. The base directory containing your database\nsql.\n\n**dbname** - By default pghops will migrate all dbs in the cluster\ndirectory. Use this option to only update the specified db.\n\n**cluster_map** - Path to a yaml file containing a map of cluster names to\ndirectories. The cluster name can then be supplied as the\ncluster_directory argument instead of a directory.\n\n**dry_run** - Do not execute the migration, only print the files that\nwould have executed.\n\n**verbosity** - Verbosity level. One of default, verbose, or terse.\n\"terse\" only prints errors. \"verbose\" echos all executed sql.\n\n**psql_base_args** - \"Base\" arguments to psql. Defaults to \"--set\nON_ERROR_STOP=1 --no-psqlrc\". Use this in combination with\npsql_arguments.\n\n**psql_arguments** - A list of arguments to provide to psql, such as\n--host, --port, etc.\n\n**db_conninfo** - Alternative way to specify the connection parameters to\npsql.\n\n**wrap_all_in_transaction** - When true, the default, pghops will wrap the\nentire migration in a single transaction.\n\n**wrap_each_version_in_transaction** - When true, each version script will\nrun in its own transaction, not the entire migration.\n\n**fail_if_unable_to_connect** - When true, the default, pghops will raise\nan error if it cannot connect to the database server.\n\n**fail_if_standby** - When true, the default, pghops will raise an error\nif it can connect to the database server but the database server is in\nstandby mode.\n\n**save_sql_to_version_table** - When true, the default, pghops will save\nall executed sql to the pghops.version table. Consider setting to\nfalse for large migrations or migrations that contain sensitive info.\n\n**save_indexes** - When true, the default, pghops scans you sql code for\ncreate index statements and saves them to the pghops.index table. See\nbelow for more details.\n\n**migration_file** - Use this option to only execute the supplied file\ninstead of all files in the versions directory.\n\n**script_suffixes** - A case-insensitive comma separated list of\nsuffixes that migration file names must match in order to be\nexecuted. Defaults to yml and yaml.\n\n**option_file** - When supplied, also load options contained within\nthis properties file.\n\n## Managing Indexes\n\nAs your schema evolves, you may find the need to create new indexes on\nlarge, existing tables. If creating indexes during the migration is\nunacceptable, you can have pghops manage indexes for you so you can\ncreate them asynchronously at a later time.\n\nBy setting the option `save-indexes` to true (the default), pghops\nwill scan your sql code for create index statements and save any to\n`pghops.index`. For pghops to track an index, ensure the following:\n\n1. The index statement resides in a file with a '.sql' suffix.\n2. The entire index statement resides on a single line.\n3. The index statement begins on the first column of the line. pghops\n   ignores any indexes statements preceded by white space. Useful if,\n   for example, you have a function that creates a temp table and\n   defines indexes on said temp table, you do not want pghops to\n   manage this index.\n4. Use fully qualified table names in you index definitions\n   (schema.table_name). The create indexes script first checks for the\n   existence of the table before executing the index statement, and\n   when pghops saves an index it does not analyze any preceding set\n   path statements. If you do not use a fully qualified table name\n   pghops will not save the index.\n5. The statement uses `if not exists` so it can be run multiple times\n   without causing an error.\n6. The scanning for indexes is not perfect. If you use unconventional\n   names for your index or table which requires quoting the name,\n   pghops cannot parse the statement correctly.\n\nBy scanning your code for indexes, you can define indexes in the same\nfiles as their table and pghops will add them to pghops.index\nautomatically during the next migration.\n\nFor every record in `pghops.index`, `pghops_create_indexes` will first\ncheck to see if the table_name is a valid table. It then checks the\n`enabled` flag and, if set, executes the sql in `definition`. The\nscript runs in parallel based on the number of cpu cores, although\nthis advantage is mitigated in more recent PostgreSQLs that can create\na single index in parallel automatically.\n\n## Unit Testing\n\nUsing the `pghops_test` command, you can create and run simple SQL\nunit tests. You will need a docker compatible container runtime\ninstalled as the tests are run in a PostgreSQL container. Here's how\nit works:\n\n1. Create a directory named `tests` within your database directory.\n2. In the `tests` directory, create sql files ending in\n   `_test.sql`. Usually you will want the file names to contain a\n   number for ordering purposes, such as `01_base_test.sql`.\n3. Run `pghops_test generate`. This will launch a PostgreSQL\n   container, run the migration, then generate companion files for\n   each sql test file. For example, for the test file\n   `01_base_test.sql` it will generate `01_base_expected.txt`.\n4. Review the generated expected file. Ensure there is no host or\n   environment specific output, such as host names or timestamps.\n5. As your schema evolves, you can run `pghops_test run` to run your\n   unit test. It will launch a new PostgreSQL container, run the\n   migration, execute the unit test sql files and compare the output\n   to the contents of the expected files.\n\nIf you create many tests, you can organize them into suites by create\nsub-directories within the `tests` directory. Each suite is run within\nits own container.\n\n### Test Options\n\nOptions for `pghops_test` are loaded in the same way as `pghops`\nexcept it looks for property files named `pghops-test.properties` in\nthe test and test-suite directories. Test specific properties only\napply when running the tests, not when running the initial migration\nin the container.\n\n**command** - The only required option. Either `run` or `generate`.\n\n**test** - When provided, only runs the specific test. You can specify\na suite name, specific file, or specific file within a suite such as\nmy-suite/my-file_test.sql.\n\n**container_name** - The name of the image to use. Defaults to\npostgresql.\n\n**container_tag** - Optional tag. If omitted, uses latest.\n\n**container_name** - The name of the container to create. Defaults to\npghops-postgresql.\n\n**container_runtime** - Defaults to docker. Also tested with podman.\n\n**skip_container_shutdown** - Do not kill the container after running\nthe tests.\n\n**ignore_whitespace** - Whether or not to ignore whitespace when\ncomparing output against the expected files.\n\n**psql_base_migrations_args** - Similar to psql_base_args but only\napplies when running the migration.\n\n`pghops_test` also accepts the following arguments that are identical\nto the `pghops` arguments:\n\n**cluster_directory**, **dbname**, **psql_base_args**,\n**psql_arguments**, **db_conninfo**, **verbosity**, **option_file**\n\n## FAQ\n\n### What does pghops stand for?\n\nEither PostGresql Highly OPinionated migrationS. Or maybe you can use\npghops to \"hop\" to your next database version. Take your pick.\n\n### Why make pghops PostgresSQL specific? Why not make it database agnosistic by using drivers for the Python Database API?\n\nBy using psql you can leverage all of its power - your sql can contain\nany psql meta command which is not possible to do with adapters such\nas Psycopg.\n\n### Is there support for rolling back migrations?\n\nNo built in support. In a perfect world each database migration script\nwould be accompanied by a rollback script. But if something goes wrong\non production and you need to roll back, do you really feel\ncomfortable executing the rollback script? Have you tested all\npossible state that the rollback script can encounter?\n\nIn my experience the need to roll back is infrequent and when it is\nnecessary, careful examination of the database must happen before any\nchanges can take place. However, if you insist on having rollback\nscripts, you can initially create rollback files in the same versions\ndirectory and name them with a non-yaml suffix, such as\n.rollback. Then when you need to rollback, run pghops with the\n`--migration-file` option to run the rollback script. If you wish to\nerase the records from the pghops.version table, you will have to do\nthat manually.\n\n### I have dependencies between my databases and I need pghops to execute migrations in a particular order.\n\nIn your `cluster_directory`, create a file named `databases` and list\nthe databases in the required order.\n\n### What happens if I need to execute sql that cannot be in a transaction?\n\nProbably best to include a `commit` statement immediately preceding\nthe sql that cannot run inside a transaction, followed by a `begin`\nstatement to start a new transaction. You could also omit transactions\nfor this pghops run by setting the options wrap-all-in-transaction and\nwrap-each-version-in-transaction to false.\n\n### When working on a unit test, I don't want to re-run the entire migration when checking my changes.\n\nSet skip_container_shutdown to True and supply a test name in your command. Example:\n\n`pghops_test --skip-container-shutdown t generate 01_base_test.sql`\n\nThen next time you re-run the above command it will immediately\nexecute 01_base_test.sql against the container without having to\nre-launch.\n\n## Miscellaneous\n\npghops was developed and tested on GNU/Linux. Feel free to report bugs\nand contribute patches.\n\n## License\n\nGPLv3. See [COPYING](COPYING).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/brewski82/pghops", "keywords": "PostgreSQL database migrations", "license": "GPLv3", "maintainer": "", "maintainer_email": "", "name": "pghops", "package_url": "https://pypi.org/project/pghops/", "platform": "", "project_url": "https://pypi.org/project/pghops/", "project_urls": {"Homepage": "https://github.com/brewski82/pghops"}, "release_url": "https://pypi.org/project/pghops/3.0.0/", "requires_dist": ["PyYAML"], "requires_python": ">=3.7", "summary": "A highly opionated Postgresql migration tool", "version": "3.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>pghops is a command line PostgreSQL schema migration utility written\nin Python. It aims to be the simplest database migration utility for\nPostgreSQL.</p>\n<ol>\n<li><a href=\"#features\" rel=\"nofollow\">Features</a></li>\n<li><a href=\"#demo\" rel=\"nofollow\">Demo</a></li>\n<li><a href=\"#usage-overview\" rel=\"nofollow\">Usage Overview</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#best-practices\" rel=\"nofollow\">Best Practices</a></li>\n<li><a href=\"#options\" rel=\"nofollow\">Options</a></li>\n<li><a href=\"#managing-indexes\" rel=\"nofollow\">Managing Indexes</a></li>\n<li><a href=\"#unit-testing\" rel=\"nofollow\">Unit Testing</a></li>\n<li><a href=\"#faq\" rel=\"nofollow\">FAQ</a></li>\n<li><a href=\"#miscellaneous\" rel=\"nofollow\">Miscellaneous</a></li>\n<li><a href=\"#license\" rel=\"nofollow\">License</a></li>\n</ol>\n<h2>Features</h2>\n<ul>\n<li><strong>Simple version file syntax:</strong> pghops version files are yaml files\nwith keys representing directories and values of one or more sql\nfile names.</li>\n<li><strong>Executes scripts with psql:</strong> pghops uses psql to execute all sql,\nleveraging the extensive functionality of the PostgreSQL client. Use\nany psql command in your scripts.</li>\n<li><strong>Unit testing framework:</strong> pghops comes equipped with its own unit\ntesting framework. No more excuses for skipping sql unit tests!</li>\n<li><strong>All or nothing migrations:</strong> Wrap your entire migration in a\nsingle transaction or each migration script in its own transaction.</li>\n<li><strong>All sql commands saved to version table</strong> pghops saves all sql\nexecuted during migrations to its version table. Make the auditors\nhappy!</li>\n</ul>\n<h2>Demo</h2>\n<p>The below terminal session shows how to create a database named <code>mydb</code>\nthat contains two tables: <code>account</code> and <code>account_email</code>. We will\ncreate a simple test to ensure you cannot insert null emails into the\n<code>account_email</code> table. Then we will create a database function for\ncreating accounts, along with another unit test.</p>\n<pre><code>[mycluster]$ # Create a directory named after the database, along with a script to create the database.\n[mycluster]$ mkdir mydb\n[mycluster]$ echo \"create database mydb;\" &gt; mydb/create_database.sql\n[mycluster]$ # Create a directory to hold our table definitions.\n[mycluster]$ mkdir -p mydb/schemas/public/tables\n[mycluster]$ # Create SQL files containing our table definitions.\n[mycluster]$ cat - &gt; mydb/schemas/public/tables/account.sql &lt;&lt;EOF\n&gt; create table if not exists public.account (\n&gt;   account_id bigserial primary key\n&gt; );\n&gt; EOF\n[mycluster]$ cat - &gt; mydb/schemas/public/tables/account_email.sql &lt;&lt;EOF\n&gt; create table if not exists public.account_email (\n&gt;   account_email_id bigserial primary key\n&gt;   , account_id bigint not null references account\n&gt;   , email text not null\n&gt; );\n&gt; EOF\n[mycluster]$ # Create our first migration file\n[mycluster]$ mkdir mydb/versions\n[mycluster]$ cat - &gt; mydb/versions/0001.0001.0001.init.yml &lt;&lt;EOF\n&gt; public/tables:\n&gt;   - account\n&gt;   - account_email\n&gt; EOF\n[mycluster]$ # Create our first unit test to ensure we cannot insert NULLs into account_email.email\n[mycluster]$ mkdir mydb/tests\n[mycluster]$ cat - &gt; mydb/tests/01_account_email_test.sql &lt;&lt;EOF\n&gt; insert into account values (default);\n&gt; insert into account_email (account_id, email) values ((select max(account_id) from account), null);\n&gt; EOF\n[mycluster]$ # Generated the 'expected' file and review it.\n[mycluster]$ pghops_test generate\n2019-02-23 14:18:42.452426: Looping through tests in /tmp/mycluster/mydb/tests\n2019-02-23 14:18:42.458661: Stopping Postgres pghops-postgresql.\n2019-02-23 14:18:42.476721: Starting Postgres pghops-postgresql postgres.\n2019-02-23 14:18:45.632209: Done starting postgres pghops-postgresql.\n2019-02-23 14:18:45.673046: Migrating cluster /tmp/mycluster.\n2019-02-23 14:18:45.673440: Migrating database mydb\n2019-02-23 14:18:45.674398: Database mydb does not exist. Creating it with /tmp/mycluster/mydb/create_database.sql.\ncreate database mydb;\nCREATE DATABASE\n\n...\n&lt;output elided&gt;\n...\n\n2019-02-23 14:18:46.106990: Done migrating database mydb\n2019-02-23 14:18:46.107123: Done all migrations.\n2019-02-23 14:18:46.132066: Generated 01_account_email_test.sql expected file.\n2019-02-23 14:18:46.132145: Stopping Postgres pghops-postgresql.\n2019-02-23 14:18:48.449079: Done generating expected files!\n[mycluster]$ # Review the expected file.\n[mycluster]$ cat mydb/tests/01_account_email_expected.txt\ninsert into account values (default);\nINSERT 0 1\ninsert into account_email (account_id, email) values ((select max(account_id) from account), null);\nERROR:  null value in column \"email\" violates not-null constraint\nDETAIL:  Failing row contains (1, 1, null).\n[mycluster]$ # Looks good! We received the error as expected. As a sanity check, run the tests and they should succeeded\n[mycluster]$ pghops_test run\n\n...\n&lt;output elided&gt;\n...\n\n2019-02-23 14:22:31.269604: All tests passed!\n[mycluster]$ # Lets run our first migration against a real db!\n[mycluster]$ pghops\n2019-02-23 14:23:47.225273: Migrating cluster /tmp/mycluster.\n2019-02-23 14:23:47.225539: Migrating database mydb\n2019-02-23 14:23:47.226114: Database mydb does not exist. Creating it with /tmp/mycluster/mydb/create_database.sql.\nCREATE DATABASE\n\n\nBEGIN\nCREATE SCHEMA\nCREATE TABLE\nCREATE TABLE\nCREATE INDEX\nINSERT 0 1\nCREATE TABLE\nCREATE TABLE\nINSERT 0 1\nCOMMIT\n\n\n2019-02-23 14:23:47.827395: Done migrating database mydb\n2019-02-23 14:23:47.827536: Done all migrations.\n[mycluster]$ # Check the version table if you wish\n[mycluster]$ psql --dbname=mydb --command=\"select major, minor, patch, label, file_name from pghops.version;\"\n major | minor | patch |    label    |            file_name\n-------+-------+-------+-------------+---------------------------------\n 0000  | 0000  | 0000  | pghops-init | 0000.0000.0000.pghops-init.yaml\n 0001  | 0001  | 0001  | init        | 0001.0001.0001.init.yml\n(2 rows)\n\n[mycluster]$ # Create a function that creates accounts.\n[mycluster]$ mkdir mydb/schemas/public/functions\n[mycluster]$ cat - &gt; mydb/schemas/public/functions/create_account.sql &lt;&lt;EOF\ncreate or replace function public.create_account\n&gt; (\n&gt;   p_email text\n&gt; )\n&gt; returns bigint\n&gt; language plpgsql\n&gt; as \\$\\$\n&gt; declare l_account_id bigint;\n&gt; begin\n&gt;\n&gt;   insert into account (account_id) values (default) returning account_id into l_account_id;\n&gt;\n&gt;   insert into account_email (account_id, email) values (l_account_id, p_email);\n&gt;\n&gt;   return l_account_id;\n&gt;\n&gt; end\\$\\$;\n&gt; EOF\n[mycluster]$ # Next create our second migration file.\n[mycluster]$ cat - &gt; mydb/versions/0001.0002.0001.create_account.yml &lt;&lt;EOF\n&gt; public/functions: create_account\n&gt; EOF\n[mycluster]$ # Create our second test\n[mycluster]$ echo \"select create_account('x@example.com');\" &gt; mydb/tests/02_create_account_test.sql\n[mycluster]$ pghops_test generate\n2019-02-23 14:35:44.742060: Looping through tests in /tmp/mycluster/mydb/tests\n2019-02-23 14:35:44.748353: Stopping Postgres pghops-postgresql.\n2019-02-23 14:35:44.764216: Starting Postgres pghops-postgresql postgres.\n2019-02-23 14:35:47.726734: Done starting postgres pghops-postgresql.\n2019-02-23 14:35:47.767687: Migrating cluster /tmp/mycluster.\n2019-02-23 14:35:47.768116: Migrating database mydb\n2019-02-23 14:35:47.769223: Database mydb does not exist. Creating it with /tmp/mycluster/mydb/create_database.sql.\n...\n&lt;output elided&gt;\n...\n\n2019-02-23 14:35:48.230893: Done migrating database mydb\n2019-02-23 14:35:48.230981: Done all migrations.\n2019-02-23 14:35:48.251236: Generated 01_account_email_test.sql expected file.\n2019-02-23 14:35:48.269521: Generated 02_create_account_test.sql expected file.\n2019-02-23 14:35:48.269596: Stopping Postgres pghops-postgresql.\n2019-02-23 14:35:50.672626: Done generating expected files!\n[mycluster]$ cat mydb/tests/02_create_account_expected.txt\nselect create_account('x@example.com');\n create_account\n----------------\n              2\n(1 row)\n[mycluster]$ # Our new db function works! Lets run a migartion to update our db\n[mycluster]$ pghops\n2019-02-23 14:37:13.523780: Migrating cluster /tmp/mycluster.\n2019-02-23 14:37:13.524050: Migrating database mydb\nBEGIN\nCREATE FUNCTION\nINSERT 0 1\nCOMMIT\n\n\n2019-02-23 14:37:13.572099: Done migrating database mydb\n2019-02-23 14:37:13.572224: Done all migrations.\n[mycluster]$ psql --dbname=mydb --command=\"select major, minor, patch, label, file_name from pghops.version;\"\n major | minor | patch |     label      |             file_name\n-------+-------+-------+----------------+-----------------------------------\n 0000  | 0000  | 0000  | pghops-init    | 0000.0000.0000.pghops-init.yaml\n 0001  | 0001  | 0001  | init           | 0001.0001.0001.init.yml\n 0001  | 0002  | 0001  | create_account | 0001.0002.0001.create_account.yml\n(3 rows)\n\n[mycluster]$\n\n\n</code></pre>\n<h2>Usage Overview</h2>\n<p>When you install PostgreSQL you initialize a storage area on disk\ncalled a <a href=\"https://www.postgresql.org/docs/current/creating-cluster.html\" rel=\"nofollow\">database\ncluster</a>,\nwhich is a collection of databases managed by a single instance of\nPostgreSQL. pghops expects you to place all files associated to\nbuilding and defining your cluster in a single directory, referred to\nhenceforth as the <code>cluster_directory</code>. Each sub-directory in\n<code>cluster_directory</code> should be the name of a database within your\ncluster (if not, you can add a file named <code>databases</code> that contains\nthe list of database directories).</p>\n<p>For example, say your <code>cluster_directory</code> is /tmp/pghops/main and you\nhave two databases - dba and dbb. Your directory structure would look\nlike:</p>\n<pre><code>\u2514\u2500\u2500 main\n    \u251c\u2500\u2500 dba\n    \u2514\u2500\u2500 dbb\n</code></pre>\n<p>pghops requires each database directory to have a sub-directory named\n<code>versions</code> which contain, you guessed it, all of you database\nmigration files. Each migration file must follow the following\nversioning convention:</p>\n<p><code>&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;.&lt;label&gt;.yml</code></p>\n<p>This allows you to follow <a href=\"https://semver.org/\" rel=\"nofollow\">Semantic Versioning</a>\nif you choose. pghops parses these file names and saves them to the\n<code>pghops.version</code> table.</p>\n<p>If pghops detects the database does not exist on the cluster, pghops\nwill create it if the database directory has a file named\n<code>create_database.sql</code> containing the database creation\ncommands. pghops records all migrations in a table named <code>version</code> in\nthe schema <code>pghops</code>. If this table does not exist, pghops will run the\nincluded <code>0000.0000.0000.pghops-init.yaml</code> script first to create it.</p>\n<p>Each version file must be in yaml format and have a yaml or yml\nsuffix. The file can only contain comments and key / value pairs, with\nkeys representing directories and values of either a single file or a\nlist of files to execute. Directories can be absolute or relative to\neither the database directory or a directory named <code>schemas</code> within\nthe database directory. We recommend laying out your directory\nstructure the same as pgAdmin's. For example, if your\n<code>cluster_directory</code> looks like:</p>\n<pre><code>\u251c\u2500\u2500 cluster_a\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 databases\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 db_a1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 create_database.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 public\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 functions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 tables\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 visits.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 views\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u251c\u2500\u2500 vistor_view.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 location_view.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 versions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 0000.0011.0001.change-a.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 0000.0021.0002.change-b.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 0000.0032.0000.change-c.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 db_a2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 create_database.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 init-data.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 public\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 functions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 tables\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 user.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 views\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u251c\u2500\u2500 user_view.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 accounts_view.sql\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 versions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 0000.0001.0001.feature-a.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 0000.0001.0002.feature-b.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 0000.0002.0000.feature-c.yml\n</code></pre>\n<p>and you want to use pghops to create new views defined in visitor_view\nand location_view, create a new migration script in db_a1/versions\nsuch as <code>0000.0033.0000.new-views.yml</code> and add the lines:</p>\n<pre><code>schemas/public/views:\n  - visitor_view.sql\n  - location_view.sql\n</code></pre>\n<p>You can optionally omit the sql suffix. Again, <code>schemas</code> is optional\nas well.</p>\n<p>Run pghops by cd'ing into cluster_directory and running</p>\n<pre><code>pghops\n</code></pre>\n<p>See below for command line parameters. You can also pass the path of\n<code>cluster_directory</code> as the first argument.</p>\n<p>When you run pghops, it will concatenate the contents of\nvisitor_view.sql and location_view.sql into a single file and then\nexecute it via psql in a single transaction. If successful, a new\nrecord is added to pghops.version and your migration is complete! For\nmore examples see the <a href=\"src/tests/test_clusters/cluster_a\" rel=\"nofollow\">test\nclusters</a>.</p>\n<h2>Installation</h2>\n<p><code>pghops</code> requires python 3.7 and the psql client. <code>pghops_test</code>\nrequires a docker compatible container runtime. Install <code>pghops</code> with\npip:</p>\n<pre><code>pip3 install pghops\n</code></pre>\n<p>This should add the executeables <code>pghops</code>, <code>pghops_test</code>, and\n<code>pghops_create_indexes</code> to your path.</p>\n<h2>Best Practices</h2>\n<h3>Directory layout for your sql code</h3>\n<p>I recommend following the same layout as pgAdmin. For example, if you\nhave a database named dba, one possibility is:</p>\n<pre><code>\u251c\u2500\u2500 dba\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 myschema\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 functions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tables\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 views\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 public\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 functions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 tables\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 views\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 versions\n</code></pre>\n<p>The <code>data</code> directory can contain scripts to load data during your\nmigrations.</p>\n<h3>Versioning</h3>\n<p>pghops is liberal when determining which migration files to\nexecute. It ignores the major, minor, and patch fields in the\npghops.version table and only looks at file_names.</p>\n<p>As such, you can use whichever versioning scheme you like. <a href=\"https://semver.org/\" rel=\"nofollow\">Semantic\nVersioning</a> is definitely a solid option. Another\nscheme, which requires slightly more effort for tracking but works\nwell when dealing with multiple people working with many branches, is\nto use an auto-incrementing number for <code>major</code> that increases on every\nmerge into your master/production branch. For <code>minor</code>, use something\nthat refers to either a feature branch or something that links back to\na ticketing system. For <code>patch</code>, use an incrementing number for\neach migration file you create for the feature. Use <code>label</code> to\ndifferentiate between two people creating migration scripts for the\nsame feature at the same time. This also helps to prevent merge\nconflicts.</p>\n<h3>Idempotency</h3>\n<p>Essentially this means if you execute the same sql twice all changes\nwill only take affect once. So use \"if not exists\" when writing DDL\nstatements and check for the presence of your records first when\nexecuting update statements (or use the <code>on conflict do nothing</code>\nclause).</p>\n<h3>Keep old migration files up to date</h3>\n<p>The pghops.version table and Git (or another VCS) should be all you\nneed for auditing and history purposes. If you make changes that would\nbreak older migration scripts when run on a new database, best to go\nback and update the older scripts. Then you can use pghops to create a\nnew database from scratch for failover, setting up new environments,\nor testing purposes.</p>\n<h3>Passwords and psql</h3>\n<p>Normally you want to avoid having to enter your password for every\npsql call. A couple options:</p>\n<ol>\n<li>Setup the user that runs pghops with password-less authentication,\nsuch as <a href=\"https://www.postgresql.org/docs/current/auth-pg-hba-conf.html\" rel=\"nofollow\">trust or\npeer</a>. Best\nthen to run pghops on the same box as PostgreSQL.</li>\n<li>Use a <a href=\"https://www.postgresql.org/docs/current/libpq-pgpass.html\" rel=\"nofollow\">password\nfile</a>.</li>\n</ol>\n<h2>Options</h2>\n<p>pghops has many configuration options, which you can set via the\ncommand line, environment variables or various property files. Options\nare loaded in the following order, from highest to lowest priority:</p>\n<ol>\n<li>Command line arguments</li>\n<li>Properties in the file specified by the <code>--options-file</code> command line\nargument.</li>\n<li>Environment variables.</li>\n<li>Properties in <code>&lt;cluster-dir&gt;/&lt;db&gt;/pghops.properties</code></li>\n<li>Properties in <code>&lt;cluster-dir&gt;/pghops.properties</code></li>\n<li>Properties in <code>pghops/conf/default.properties</code></li>\n</ol>\n<p>Property files should be in yaml format and contain key/value pairs.</p>\n<p>pghops treats options in property files that only differ in case or\nusage of underscore versus hyphen the same. For example:</p>\n<pre><code>wrap-all-in-transaction\nwrap_all_in_transaction\nWrap_All_In_Transaction\n</code></pre>\n<p>all refer to the same option. Environment variables should use\nunderscores instead of hyphens, be in all caps, and have a prefix of\nPGHOPS_. For example, the environment varaible for the\nwrap-all-in-transaction property above is\nPGHOPS_WRAP_ALL_IN_TRANSACTION.</p>\n<p>psql's environment variables are also in effect.</p>\n<p>pghops options are as follows:</p>\n<p><strong>cluster_directory</strong> - The first argument to pghops. Defaults to the\ncurrent working directory. The base directory containing your database\nsql.</p>\n<p><strong>dbname</strong> - By default pghops will migrate all dbs in the cluster\ndirectory. Use this option to only update the specified db.</p>\n<p><strong>cluster_map</strong> - Path to a yaml file containing a map of cluster names to\ndirectories. The cluster name can then be supplied as the\ncluster_directory argument instead of a directory.</p>\n<p><strong>dry_run</strong> - Do not execute the migration, only print the files that\nwould have executed.</p>\n<p><strong>verbosity</strong> - Verbosity level. One of default, verbose, or terse.\n\"terse\" only prints errors. \"verbose\" echos all executed sql.</p>\n<p><strong>psql_base_args</strong> - \"Base\" arguments to psql. Defaults to \"--set\nON_ERROR_STOP=1 --no-psqlrc\". Use this in combination with\npsql_arguments.</p>\n<p><strong>psql_arguments</strong> - A list of arguments to provide to psql, such as\n--host, --port, etc.</p>\n<p><strong>db_conninfo</strong> - Alternative way to specify the connection parameters to\npsql.</p>\n<p><strong>wrap_all_in_transaction</strong> - When true, the default, pghops will wrap the\nentire migration in a single transaction.</p>\n<p><strong>wrap_each_version_in_transaction</strong> - When true, each version script will\nrun in its own transaction, not the entire migration.</p>\n<p><strong>fail_if_unable_to_connect</strong> - When true, the default, pghops will raise\nan error if it cannot connect to the database server.</p>\n<p><strong>fail_if_standby</strong> - When true, the default, pghops will raise an error\nif it can connect to the database server but the database server is in\nstandby mode.</p>\n<p><strong>save_sql_to_version_table</strong> - When true, the default, pghops will save\nall executed sql to the pghops.version table. Consider setting to\nfalse for large migrations or migrations that contain sensitive info.</p>\n<p><strong>save_indexes</strong> - When true, the default, pghops scans you sql code for\ncreate index statements and saves them to the pghops.index table. See\nbelow for more details.</p>\n<p><strong>migration_file</strong> - Use this option to only execute the supplied file\ninstead of all files in the versions directory.</p>\n<p><strong>script_suffixes</strong> - A case-insensitive comma separated list of\nsuffixes that migration file names must match in order to be\nexecuted. Defaults to yml and yaml.</p>\n<p><strong>option_file</strong> - When supplied, also load options contained within\nthis properties file.</p>\n<h2>Managing Indexes</h2>\n<p>As your schema evolves, you may find the need to create new indexes on\nlarge, existing tables. If creating indexes during the migration is\nunacceptable, you can have pghops manage indexes for you so you can\ncreate them asynchronously at a later time.</p>\n<p>By setting the option <code>save-indexes</code> to true (the default), pghops\nwill scan your sql code for create index statements and save any to\n<code>pghops.index</code>. For pghops to track an index, ensure the following:</p>\n<ol>\n<li>The index statement resides in a file with a '.sql' suffix.</li>\n<li>The entire index statement resides on a single line.</li>\n<li>The index statement begins on the first column of the line. pghops\nignores any indexes statements preceded by white space. Useful if,\nfor example, you have a function that creates a temp table and\ndefines indexes on said temp table, you do not want pghops to\nmanage this index.</li>\n<li>Use fully qualified table names in you index definitions\n(schema.table_name). The create indexes script first checks for the\nexistence of the table before executing the index statement, and\nwhen pghops saves an index it does not analyze any preceding set\npath statements. If you do not use a fully qualified table name\npghops will not save the index.</li>\n<li>The statement uses <code>if not exists</code> so it can be run multiple times\nwithout causing an error.</li>\n<li>The scanning for indexes is not perfect. If you use unconventional\nnames for your index or table which requires quoting the name,\npghops cannot parse the statement correctly.</li>\n</ol>\n<p>By scanning your code for indexes, you can define indexes in the same\nfiles as their table and pghops will add them to pghops.index\nautomatically during the next migration.</p>\n<p>For every record in <code>pghops.index</code>, <code>pghops_create_indexes</code> will first\ncheck to see if the table_name is a valid table. It then checks the\n<code>enabled</code> flag and, if set, executes the sql in <code>definition</code>. The\nscript runs in parallel based on the number of cpu cores, although\nthis advantage is mitigated in more recent PostgreSQLs that can create\na single index in parallel automatically.</p>\n<h2>Unit Testing</h2>\n<p>Using the <code>pghops_test</code> command, you can create and run simple SQL\nunit tests. You will need a docker compatible container runtime\ninstalled as the tests are run in a PostgreSQL container. Here's how\nit works:</p>\n<ol>\n<li>Create a directory named <code>tests</code> within your database directory.</li>\n<li>In the <code>tests</code> directory, create sql files ending in\n<code>_test.sql</code>. Usually you will want the file names to contain a\nnumber for ordering purposes, such as <code>01_base_test.sql</code>.</li>\n<li>Run <code>pghops_test generate</code>. This will launch a PostgreSQL\ncontainer, run the migration, then generate companion files for\neach sql test file. For example, for the test file\n<code>01_base_test.sql</code> it will generate <code>01_base_expected.txt</code>.</li>\n<li>Review the generated expected file. Ensure there is no host or\nenvironment specific output, such as host names or timestamps.</li>\n<li>As your schema evolves, you can run <code>pghops_test run</code> to run your\nunit test. It will launch a new PostgreSQL container, run the\nmigration, execute the unit test sql files and compare the output\nto the contents of the expected files.</li>\n</ol>\n<p>If you create many tests, you can organize them into suites by create\nsub-directories within the <code>tests</code> directory. Each suite is run within\nits own container.</p>\n<h3>Test Options</h3>\n<p>Options for <code>pghops_test</code> are loaded in the same way as <code>pghops</code>\nexcept it looks for property files named <code>pghops-test.properties</code> in\nthe test and test-suite directories. Test specific properties only\napply when running the tests, not when running the initial migration\nin the container.</p>\n<p><strong>command</strong> - The only required option. Either <code>run</code> or <code>generate</code>.</p>\n<p><strong>test</strong> - When provided, only runs the specific test. You can specify\na suite name, specific file, or specific file within a suite such as\nmy-suite/my-file_test.sql.</p>\n<p><strong>container_name</strong> - The name of the image to use. Defaults to\npostgresql.</p>\n<p><strong>container_tag</strong> - Optional tag. If omitted, uses latest.</p>\n<p><strong>container_name</strong> - The name of the container to create. Defaults to\npghops-postgresql.</p>\n<p><strong>container_runtime</strong> - Defaults to docker. Also tested with podman.</p>\n<p><strong>skip_container_shutdown</strong> - Do not kill the container after running\nthe tests.</p>\n<p><strong>ignore_whitespace</strong> - Whether or not to ignore whitespace when\ncomparing output against the expected files.</p>\n<p><strong>psql_base_migrations_args</strong> - Similar to psql_base_args but only\napplies when running the migration.</p>\n<p><code>pghops_test</code> also accepts the following arguments that are identical\nto the <code>pghops</code> arguments:</p>\n<p><strong>cluster_directory</strong>, <strong>dbname</strong>, <strong>psql_base_args</strong>,\n<strong>psql_arguments</strong>, <strong>db_conninfo</strong>, <strong>verbosity</strong>, <strong>option_file</strong></p>\n<h2>FAQ</h2>\n<h3>What does pghops stand for?</h3>\n<p>Either PostGresql Highly OPinionated migrationS. Or maybe you can use\npghops to \"hop\" to your next database version. Take your pick.</p>\n<h3>Why make pghops PostgresSQL specific? Why not make it database agnosistic by using drivers for the Python Database API?</h3>\n<p>By using psql you can leverage all of its power - your sql can contain\nany psql meta command which is not possible to do with adapters such\nas Psycopg.</p>\n<h3>Is there support for rolling back migrations?</h3>\n<p>No built in support. In a perfect world each database migration script\nwould be accompanied by a rollback script. But if something goes wrong\non production and you need to roll back, do you really feel\ncomfortable executing the rollback script? Have you tested all\npossible state that the rollback script can encounter?</p>\n<p>In my experience the need to roll back is infrequent and when it is\nnecessary, careful examination of the database must happen before any\nchanges can take place. However, if you insist on having rollback\nscripts, you can initially create rollback files in the same versions\ndirectory and name them with a non-yaml suffix, such as\n.rollback. Then when you need to rollback, run pghops with the\n<code>--migration-file</code> option to run the rollback script. If you wish to\nerase the records from the pghops.version table, you will have to do\nthat manually.</p>\n<h3>I have dependencies between my databases and I need pghops to execute migrations in a particular order.</h3>\n<p>In your <code>cluster_directory</code>, create a file named <code>databases</code> and list\nthe databases in the required order.</p>\n<h3>What happens if I need to execute sql that cannot be in a transaction?</h3>\n<p>Probably best to include a <code>commit</code> statement immediately preceding\nthe sql that cannot run inside a transaction, followed by a <code>begin</code>\nstatement to start a new transaction. You could also omit transactions\nfor this pghops run by setting the options wrap-all-in-transaction and\nwrap-each-version-in-transaction to false.</p>\n<h3>When working on a unit test, I don't want to re-run the entire migration when checking my changes.</h3>\n<p>Set skip_container_shutdown to True and supply a test name in your command. Example:</p>\n<p><code>pghops_test --skip-container-shutdown t generate 01_base_test.sql</code></p>\n<p>Then next time you re-run the above command it will immediately\nexecute 01_base_test.sql against the container without having to\nre-launch.</p>\n<h2>Miscellaneous</h2>\n<p>pghops was developed and tested on GNU/Linux. Feel free to report bugs\nand contribute patches.</p>\n<h2>License</h2>\n<p>GPLv3. See <a href=\"COPYING\" rel=\"nofollow\">COPYING</a>.</p>\n\n          </div>"}, "last_serial": 6520552, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "5507c412f4cf3a3a60a24ee308a44917", "sha256": "a24fe9608746a65c204d47bd17bc431d5e51a83fe46247ecd42475c701391e6d"}, "downloads": -1, "filename": "pghops-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "5507c412f4cf3a3a60a24ee308a44917", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 43789, "upload_time": "2019-01-01T13:01:45", "upload_time_iso_8601": "2019-01-01T13:01:45.498579Z", "url": "https://files.pythonhosted.org/packages/a1/a8/023b82c7e703e3a6f46850018669a9d6703f750fccc84a39339d07d94d70/pghops-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0c22a505001b61f3f3643d7907eee23e", "sha256": "271cfdcc69518d7f22930f2d9a733fa7dea64602c2533384fe8458db3748be5d"}, "downloads": -1, "filename": "pghops-1.0.0.tar.gz", "has_sig": false, "md5_digest": "0c22a505001b61f3f3643d7907eee23e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 28585, "upload_time": "2019-01-01T13:01:47", "upload_time_iso_8601": "2019-01-01T13:01:47.527320Z", "url": "https://files.pythonhosted.org/packages/54/ca/dde9adac23cba62d14efdb0b61c6b2e8cebcbf27fae3fbbac4a68c72d828/pghops-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "9ec77e9a7604b0221bb8d2b59baf6860", "sha256": "34575a9d643d8a1175121f0eee10cc6f35d90a7b879f64339be8c7b933266a8f"}, "downloads": -1, "filename": "pghops-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "9ec77e9a7604b0221bb8d2b59baf6860", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 43757, "upload_time": "2019-01-01T18:40:14", "upload_time_iso_8601": "2019-01-01T18:40:14.610508Z", "url": "https://files.pythonhosted.org/packages/d4/b1/a0f28ed8701fd78faee6c184497c2d19412081718312f6f7b1b6fb288838/pghops-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cbb99b3d7d9c2d28de10b1677a317b83", "sha256": "b849baeb5e5ebda4b0267392ff55721a46b58f7cb60d0dbc381dcb1e0cb6b630"}, "downloads": -1, "filename": "pghops-1.0.1.tar.gz", "has_sig": false, "md5_digest": "cbb99b3d7d9c2d28de10b1677a317b83", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 28510, "upload_time": "2019-01-01T18:40:16", "upload_time_iso_8601": "2019-01-01T18:40:16.279898Z", "url": "https://files.pythonhosted.org/packages/dd/1c/d255222c70dee086d8889943534ff79dc79e0a83070a44a4ce907d9f1134/pghops-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "ff5c1ad41861059d0f8bd0b36317726a", "sha256": "bd9d1196ba4f76af422527e83a8ea8c61ef66d692203a87554175f8210a5a8df"}, "downloads": -1, "filename": "pghops-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "ff5c1ad41861059d0f8bd0b36317726a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 43778, "upload_time": "2019-01-03T15:14:19", "upload_time_iso_8601": "2019-01-03T15:14:19.474161Z", "url": "https://files.pythonhosted.org/packages/dc/24/7ab212d63c0c5fb903142e98760782b6d3d724bdc2a22eefc1766cc54d22/pghops-1.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f9ffbfab2e27fd75cc01ae5a0047b242", "sha256": "0777d3223bbb130b3a65d0febd861da6e302072b0e0e07d9f1246690ddd4ed54"}, "downloads": -1, "filename": "pghops-1.0.2.tar.gz", "has_sig": false, "md5_digest": "f9ffbfab2e27fd75cc01ae5a0047b242", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 28551, "upload_time": "2019-01-03T15:14:21", "upload_time_iso_8601": "2019-01-03T15:14:21.187163Z", "url": "https://files.pythonhosted.org/packages/f8/c6/7c25307ffa394de1b585b98c6cf543ac62ed8360620b228b74d366f79394/pghops-1.0.2.tar.gz", "yanked": false}], "2.0.0": [{"comment_text": "", "digests": {"md5": "3b65fc29f6dc85a98c9b39701745a55e", "sha256": "51ccf29e78dbfe0bc88cf384e18dd085c39ae7c8b777381fe5a49ee7d344f395"}, "downloads": -1, "filename": "pghops-2.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3b65fc29f6dc85a98c9b39701745a55e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 52526, "upload_time": "2019-02-24T03:15:08", "upload_time_iso_8601": "2019-02-24T03:15:08.538209Z", "url": "https://files.pythonhosted.org/packages/00/23/e48d6475e1e41eb3de14e7c61aa67dc47ea9fbb54e4d9a1d397ca5ee2374/pghops-2.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1b4b774e67dfbf7fa496dc6df99576d3", "sha256": "89c73145b9915615070c910e547662ba98a83fb6bdd14d145bf488a0636722d5"}, "downloads": -1, "filename": "pghops-2.0.0.tar.gz", "has_sig": false, "md5_digest": "1b4b774e67dfbf7fa496dc6df99576d3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 39210, "upload_time": "2019-02-24T03:15:10", "upload_time_iso_8601": "2019-02-24T03:15:10.577357Z", "url": "https://files.pythonhosted.org/packages/3f/66/fd042a3f34235fd0e8f3306ea6d07f58a026912ca94b3fcab90a66c01f2a/pghops-2.0.0.tar.gz", "yanked": false}], "2.0.1": [{"comment_text": "", "digests": {"md5": "8e712167c4500cbd9bb49cfc1cbcfdab", "sha256": "5dae751714e1a29c4fe1fb200ecde144ca57c8aea7c956d860bd80ae48eb4bf2"}, "downloads": -1, "filename": "pghops-2.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "8e712167c4500cbd9bb49cfc1cbcfdab", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 52524, "upload_time": "2019-04-29T01:50:32", "upload_time_iso_8601": "2019-04-29T01:50:32.038893Z", "url": "https://files.pythonhosted.org/packages/3e/7f/fee0d10af93a6ceed2f63deddafadc2c57aa6351004ecb1f9506c517267d/pghops-2.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ce76adc7915a26eee65165d3cb2af72e", "sha256": "aeb9a40eeeb8673ae8fda9d739a02a8f439f6f5f3c94f230369a04e7f302fa68"}, "downloads": -1, "filename": "pghops-2.0.1.tar.gz", "has_sig": false, "md5_digest": "ce76adc7915a26eee65165d3cb2af72e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 39221, "upload_time": "2019-04-29T01:50:35", "upload_time_iso_8601": "2019-04-29T01:50:35.454572Z", "url": "https://files.pythonhosted.org/packages/bd/cb/072ae26d8a0a47a6ed02af296225042f42fbe5c759379e19758227734889/pghops-2.0.1.tar.gz", "yanked": false}], "3.0.0": [{"comment_text": "", "digests": {"md5": "024b2ab6cef3ff71e940d2991b9f1d21", "sha256": "83b8e40f8efff07cdf77e46d0baf42ed101c222c9b9a98dfd6f62849cb3858a8"}, "downloads": -1, "filename": "pghops-3.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "024b2ab6cef3ff71e940d2991b9f1d21", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 52710, "upload_time": "2020-01-26T02:11:35", "upload_time_iso_8601": "2020-01-26T02:11:35.140717Z", "url": "https://files.pythonhosted.org/packages/b5/34/43071c68c8aec2fa624adc93b761f22fdb9ae4cf1ffc5fc5ccbbeb7a7633/pghops-3.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "23660d4931e05f9cf4e6da0e501801fa", "sha256": "f32e94580f822e478b0f879456ec3c4a6b666eb3206c6e01a52c2f469ce23431"}, "downloads": -1, "filename": "pghops-3.0.0.tar.gz", "has_sig": false, "md5_digest": "23660d4931e05f9cf4e6da0e501801fa", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 39922, "upload_time": "2020-01-26T02:11:37", "upload_time_iso_8601": "2020-01-26T02:11:37.075062Z", "url": "https://files.pythonhosted.org/packages/73/7e/6c54f952f97bf1f73b44051aa54362a25ba34ab951ed8287201e9b29ea23/pghops-3.0.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "024b2ab6cef3ff71e940d2991b9f1d21", "sha256": "83b8e40f8efff07cdf77e46d0baf42ed101c222c9b9a98dfd6f62849cb3858a8"}, "downloads": -1, "filename": "pghops-3.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "024b2ab6cef3ff71e940d2991b9f1d21", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 52710, "upload_time": "2020-01-26T02:11:35", "upload_time_iso_8601": "2020-01-26T02:11:35.140717Z", "url": "https://files.pythonhosted.org/packages/b5/34/43071c68c8aec2fa624adc93b761f22fdb9ae4cf1ffc5fc5ccbbeb7a7633/pghops-3.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "23660d4931e05f9cf4e6da0e501801fa", "sha256": "f32e94580f822e478b0f879456ec3c4a6b666eb3206c6e01a52c2f469ce23431"}, "downloads": -1, "filename": "pghops-3.0.0.tar.gz", "has_sig": false, "md5_digest": "23660d4931e05f9cf4e6da0e501801fa", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 39922, "upload_time": "2020-01-26T02:11:37", "upload_time_iso_8601": "2020-01-26T02:11:37.075062Z", "url": "https://files.pythonhosted.org/packages/73/7e/6c54f952f97bf1f73b44051aa54362a25ba34ab951ed8287201e9b29ea23/pghops-3.0.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:55:49 2020"}