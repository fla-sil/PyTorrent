{"info": {"author": "Ali Haidar", "author_email": "ali.hdrv@outlook.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "![image](LOGO.png)\n\n\nWelcome to pspso's documentation!\n=================================\n\nOverview and Installation\n=========================\n\nOverview\n--------\n\n**pspso** is a python library for selecting machine learning algorithms\nparameters. The first version supports two single algorithms:\nMulti-Layer Perceptron (MLP) and Support Vector Machine (SVM). It\nsupports two ensembles: Extreme Gradient Boosting (XGBoost) and Gradient\nBoosting Decision Trees (GBDT).\n\nTwo types of machine learning tasks are supported by pspso:\n\n-   Regression.\n-   Binary classification.\n\nThree scores are supported in the first version of pspso:\n\n-   **Regression** :\n\n    > -   Root Mean Square Error (RMSE)\n\n-   **Binary Classication** :\n\n    > -   Area under the Curve (AUC) of the Receiver Operating\n    >     Characteristic (ROC)\n    > -   Accuracy\n\nInstallation\n------------\n\nUse the package manager [pip](https://pip.pypa.io/en/stable/) to install\npspso.\n\n``` {.sourceCode .bash}\npip install pspso\n```\n\nUsage\n=====\n\nMLP Example (Binary Classification)\n-----------------------------------\n\n**pspso** is used to select the machine learning algorithms parameters.\nBelow is an example for using the pspso to select the parameters of the\nMLP. pspso handles the MLP random weights intialization issue that may\ncause losing the best solution in consecutive iterations.\n\nThe following example demonstrates the selection process of the MLP\nparameters. A variable named *params* was not given by the user. Hence,\nthe default search space of the MLP is loaded. This search space\ncontains five parameters:\n\n``` {.sourceCode .python}\nparams = {\"optimizer\": [\"RMSprop\", \"adam\", \"sgd\",'adamax','nadam','adadelta'] ,\n      \"learning_rate\":  [0.1,0.3,2],\n      \"neurons\": [1,40,0],\n      \"hiddenactivation\": ['relu','sigmoid','tanh'],\n      \"activation\":['relu','sigmoid','tanh']} \n```\n\nThe task and the score were defined as *binary classification* and *auc*\nrespectively. Then, the PSO was used to select the parameters of the\nMLP. Results are provided back to the user through the\n**print\\_results()** function.\n\n``` {.sourceCode .python}\nfrom sklearn.preprocessing import MinMaxScaler\nfrom pspso import pspso\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nbreastcancer = datasets.load_breast_cancer()\ndata=breastcancer.data#get the breast cancer dataset input features\ntarget=breastcancer.target# target\nX_train, X_test, Y_train, Y_test = train_test_split(data, target,test_size=0.1,random_state=42,stratify=target)\nnormalize = MinMaxScaler(feature_range=(0,1))#normalize input features \nX_train=normalize.fit_transform(X_train)\nX_test=normalize.transform(X_test)\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train,test_size=0.15,random_state=42,stratify=Y_train)\np=pspso(estimator='mlp',task='binary classification', score='auc')\npos,cost,duration,model,optimizer=p.fitpspso(X_train,Y_train,X_val,Y_val)\np.print_results()#print the results\ntestscore=pspso.predict(p.model,p.estimator,p.task,p.score, X_test, Y_test)\nprint(1-testscore)\n```\n\nIn this example, four parameters were examined: optimizer,\nlearning\\_rate, hiddenactivation, and activation. The number of neurons\nin the hidden layer was kept as default.\n\nOutput:\n\n``` {.sourceCode .python}\nEstimator: mlp\nTask: binary classification\nSelection type: PSO\nNumber of attempts:50\nTotal number of combinations: 45360\nParameters:\n{'optimizer': 'nadam', 'learning_rate': 0.29, 'neurons': 4, 'hiddenactivation': 'sigmoid', 'activation': 'sigmoid'}\nGlobal best position: [3.8997699  0.28725911 4.21218138 1.41200923 0.84643591]\nGlobal best cost: 0.0\nTime taken to find the set of parameters: 160.3374378681183\nNumber of particles: 5\nNumber of iterations: 10\n0.9867724867724867\n```\n\nXGBoost Example (Binary Classification)\n---------------------------------------\n\n``` {.sourceCode .python}\nfrom sklearn.preprocessing import MinMaxScaler\nfrom pspso import pspso\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nbreastcancer = datasets.load_breast_cancer()\ndata=breastcancer.data#get the breast cancer dataset input features\ntarget=breastcancer.target# target\nX_train, X_test, Y_train, Y_test = train_test_split(data, target,test_size=0.1,random_state=42,stratify=target)\nnormalize = MinMaxScaler(feature_range=(0,1))#normalize input features \nX_train=normalize.fit_transform(X_train)\nX_test=normalize.transform(X_test)\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train,test_size=0.15,random_state=42,stratify=Y_train)\n\nparams = {\n        \"learning_rate\":  [0.01,0.2,2],\n        \"max_depth\": [1,10,0],\n        \"n_estimators\": [2,200,0],\n        \"subsample\": [0.7,1,1]}\np=pspso(estimator='xgboost',params=params,task='binary classification', score='auc')\npos,cost,duration,model,optimizer=p.fitpspso(X_train,Y_train,X_val,Y_val)\np.print_results()#print the results\ntestscore=pspso.predict(p.model,p.estimator,p.task,p.score, X_test, Y_test)\nprint(1-testscore)\n```\n\nXGBoost Example (Regression)\n----------------------------\n\nThe XGBoost is an implementation of boosting decision trees. Five\nparameters were utilized for selection: objective, learning rate,\nmaximum depth, number of estimators, and subsample. Three categorical\nvalues were selected for the objective parameter. The learning rate\nparameter values range between *0.01* and *0.2* with *2* decimal point,\nmaximum depth ranges between *1* and *10* with *0* decimal points\n*(1,2,3,4,5,6,7,8,9,10)*, etc. The task and score are selected as\nregression and RMSE respectively. The number of particles and number of\niterations can be left as default values if needed. Then, a pspso\ninstance is created. By applying the fitpspso function, the selection\nprocess is applied. Finally, results are printed back to the user. The\nbest model, best parameters, score, time, and other details will be\nsaved in the created instance for the user to check.\n\n``` {.sourceCode .python}\nfrom sklearn.preprocessing import MinMaxScaler\nfrom pspso import pspso\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nboston_data = datasets.load_boston()\ndata=boston_data.data\ntarget=boston_data.target\n\nX_train, X_test, Y_train, Y_test = train_test_split(data, target,test_size=0.1,random_state=42)\nnormalize = MinMaxScaler(feature_range=(0,1))#normalize input features\nnormalizetarget = MinMaxScaler(feature_range=(0,1))#normalize target\n\nX_train=normalize.fit_transform(X_train)\nX_test=normalize.transform(X_test)\nY_train=normalizetarget.fit_transform(Y_train.reshape(-1,1))\nY_test=normalizetarget.transform(Y_test.reshape(-1,1))\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train,test_size=0.25,random_state=42)\nparams = {\n        \"objective\":['reg:tweedie',\"reg:linear\",\"reg:gamma\"],\n        \"learning_rate\":  [0.01,0.2,2],\n        \"max_depth\": [1,10,0],\n        \"n_estimators\": [2,200,0],\n        \"subsample\": [0.7,1,1]}\np=pspso(estimator='xgboost',params=params,task='regression', score='rmse')\npos,cost,duration,model,optimizer=p.fitpspso(X_train,Y_train,X_val,Y_val)\np.print_results()#print the results\ntestscore=pspso.predict(p.model,p.estimator,p.task,p.score, X_test, Y_test)\nprint(testscore)\n```\n\nUser Input\n----------\n\nThe user is required to select the type of the algorithm ('mlp', 'svm',\n'xgboost', 'gbdt'); the task type ('binary\nclassification','regression'), score ('rmse', 'acc', or 'auc'). The user\ncan keep the parameters variable empty, where a default set of\nparameters and ranges is loaded for each algorithm.\n\n``` {.sourceCode .python}\nfrom pspso import pspso\ntask='binary classification'\nscore='auc'\np=pspso.pspso('xgboost',None,task,score)\n```\n\nPspso allows the user to provide a range of parameters for exploration.\nThe parameters vary between each algorithm. Any parameter supported by\nthe Scikit-Learn API for GBDT and XGBoost can be added to the selection\nprocess. A set of parameters that contains five XGBoost parameters is\nshown below. The parameters are encoded in JSON object that consists of\n*key,value* pairs:\n\n``` {.sourceCode .python}\nparams = {\"objective\":['reg:tweedie',\"reg:linear\",\"reg:gamma\"],\n        \"learning_rate\":  [0.01,0.2,2],\n        \"max_depth\": [1,10,0],\n        \"n_estimators\": [2,200,0],\n        \"subsample\": [0.7,1,1]}\n```\n\nThe key can be any parameter belonging to to the algorithm under\ninvestigation. The value is a list. Pspso will check the type of the\nfirst element in the list, which will determine if the values of the\nparameter are categorical or numerical.\n\n**Categorical Parameters**\n\nIf the parameter values are *categorical*, string values are expected to\nbe found in the list, as shown in *objective* parameter. The values in\nthe list will be automatically mapped into a list of integers, where\neach integer represents a value in the original list. The order of the\nvalues inside the list affect the position of the value in the search\nspace.\n\n**Numerical Parameters**\n\nIf the parameter is numerical, a list of three elements [lb,ub, rv] is\nexpected to be found:\n\n-   **lb**: repesents the lowest value in the search space\n-   **ub**: represents the maximum value in the search space\n-   **rv**: represents the number of decimal points the parameter values\n    are rounded to before being added for training the algorithm\n\nFor e.g if you want pspso to select n\\_estimators, add the following\nlist *[2,200,0]*. By that, the lowest n\\_estimators will be *2*, the\nhighest to be examined is *200*, and each possible value is rounded to\nan integer value ( *0* decimal points).\n\n**Other parameters**\n\nThe user is given the chance to handle some of the default parameters\nsuch as the number of epochs in the MLP. Although this parameter can be\noptimized, but its not encouraged. The user can modify this by changing\na pspso class instance. For e.g., to change the number of epochs from\ndefault to 10 in MLP training:\n\n``` {.sourceCode .python}\nfrom pspso import pspso\ntask='binary classification'\nscore='auc'\np=pspso.pspso('mlp',None,task,score)# in case of empty set of params (None) default search space is loaded\np.defaultparams['epochs']=10\n```\n\nThe verbosity can be modified for any algorithm, which allows showing\ndetails of the training process:\n\n``` {.sourceCode .python}\nfrom pspso import pspso\ntask='binary classification'\nscore='auc'\np=pspso.pspso('mlp',None,task,score)\np.verbosity=1\n```\n\nEarly stopping rounds can alos be modified, the user can set a value\ndifferent to the default value:\n\n``` {.sourceCode .python}\nfrom pspso import pspso\ntask='binary classification'\nscore='auc'\np=pspso.pspso('xgboost',None,task,score)\np.early_stopping=10\n```\n\nOther parameters such that n\\_jobs in XGBoost can also be modified\nbefore the start of the selection process.\n\n\nContributing\n============\n\nPull requests are welcome. For major changes, please open an issue first\nto discuss what you would like to change.\n\nPlease make sure to update tests as appropriate.\n\nWe are working towards adding the cross validation support that will\ntake the training data and number of folds, then split the records and\ntrain each fold. Finally, the average performance is retuned to the\nuser.\n\nWe are also working on adding multi-class classification and data\noversampling techniques.\n\nLicense\n=======\n\nCopyright (c) [2020] [Ali Haidar]\n\nPermission is hereby granted, free of charge, to any person obtaining a\ncopy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be included\nin all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\nOR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ayhaidar/pspso", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "pspso", "package_url": "https://pypi.org/project/pspso/", "platform": "", "project_url": "https://pypi.org/project/pspso/", "project_urls": {"Homepage": "https://github.com/ayhaidar/pspso"}, "release_url": "https://pypi.org/project/pspso/0.1.0/", "requires_dist": ["numpy (==1.16.1)", "lightgbm", "xgboost", "scikit-learn (>=0.21.2)", "keras", "pyswarms (>=1.0.2)", "matplotlib (>=3.1.1)"], "requires_python": ">=3.5", "summary": "pspso is a python package for selecting machine learning algorithms parameters.", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><img alt=\"image\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3ac37a98aa75bf0de7720ce0b08827025e3c1467/4c4f474f2e706e67\"></p>\n<h1>Welcome to pspso's documentation!</h1>\n<h1>Overview and Installation</h1>\n<h2>Overview</h2>\n<p><strong>pspso</strong> is a python library for selecting machine learning algorithms\nparameters. The first version supports two single algorithms:\nMulti-Layer Perceptron (MLP) and Support Vector Machine (SVM). It\nsupports two ensembles: Extreme Gradient Boosting (XGBoost) and Gradient\nBoosting Decision Trees (GBDT).</p>\n<p>Two types of machine learning tasks are supported by pspso:</p>\n<ul>\n<li>Regression.</li>\n<li>Binary classification.</li>\n</ul>\n<p>Three scores are supported in the first version of pspso:</p>\n<ul>\n<li>\n<p><strong>Regression</strong> :</p>\n<blockquote>\n<ul>\n<li>Root Mean Square Error (RMSE)</li>\n</ul>\n</blockquote>\n</li>\n<li>\n<p><strong>Binary Classication</strong> :</p>\n<blockquote>\n<ul>\n<li>Area under the Curve (AUC) of the Receiver Operating\nCharacteristic (ROC)</li>\n<li>Accuracy</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<h2>Installation</h2>\n<p>Use the package manager <a href=\"https://pip.pypa.io/en/stable/\" rel=\"nofollow\">pip</a> to install\npspso.</p>\n<pre>pip install pspso\n</pre>\n<h1>Usage</h1>\n<h2>MLP Example (Binary Classification)</h2>\n<p><strong>pspso</strong> is used to select the machine learning algorithms parameters.\nBelow is an example for using the pspso to select the parameters of the\nMLP. pspso handles the MLP random weights intialization issue that may\ncause losing the best solution in consecutive iterations.</p>\n<p>The following example demonstrates the selection process of the MLP\nparameters. A variable named <em>params</em> was not given by the user. Hence,\nthe default search space of the MLP is loaded. This search space\ncontains five parameters:</p>\n<pre>params = {\"optimizer\": [\"RMSprop\", \"adam\", \"sgd\",'adamax','nadam','adadelta'] ,\n      \"learning_rate\":  [0.1,0.3,2],\n      \"neurons\": [1,40,0],\n      \"hiddenactivation\": ['relu','sigmoid','tanh'],\n      \"activation\":['relu','sigmoid','tanh']} \n</pre>\n<p>The task and the score were defined as <em>binary classification</em> and <em>auc</em>\nrespectively. Then, the PSO was used to select the parameters of the\nMLP. Results are provided back to the user through the\n<strong>print_results()</strong> function.</p>\n<pre>from sklearn.preprocessing import MinMaxScaler\nfrom pspso import pspso\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nbreastcancer = datasets.load_breast_cancer()\ndata=breastcancer.data#get the breast cancer dataset input features\ntarget=breastcancer.target# target\nX_train, X_test, Y_train, Y_test = train_test_split(data, target,test_size=0.1,random_state=42,stratify=target)\nnormalize = MinMaxScaler(feature_range=(0,1))#normalize input features \nX_train=normalize.fit_transform(X_train)\nX_test=normalize.transform(X_test)\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train,test_size=0.15,random_state=42,stratify=Y_train)\np=pspso(estimator='mlp',task='binary classification', score='auc')\npos,cost,duration,model,optimizer=p.fitpspso(X_train,Y_train,X_val,Y_val)\np.print_results()#print the results\ntestscore=pspso.predict(p.model,p.estimator,p.task,p.score, X_test, Y_test)\nprint(1-testscore)\n</pre>\n<p>In this example, four parameters were examined: optimizer,\nlearning_rate, hiddenactivation, and activation. The number of neurons\nin the hidden layer was kept as default.</p>\n<p>Output:</p>\n<pre>Estimator: mlp\nTask: binary classification\nSelection type: PSO\nNumber of attempts:50\nTotal number of combinations: 45360\nParameters:\n{'optimizer': 'nadam', 'learning_rate': 0.29, 'neurons': 4, 'hiddenactivation': 'sigmoid', 'activation': 'sigmoid'}\nGlobal best position: [3.8997699  0.28725911 4.21218138 1.41200923 0.84643591]\nGlobal best cost: 0.0\nTime taken to find the set of parameters: 160.3374378681183\nNumber of particles: 5\nNumber of iterations: 10\n0.9867724867724867\n</pre>\n<h2>XGBoost Example (Binary Classification)</h2>\n<pre>from sklearn.preprocessing import MinMaxScaler\nfrom pspso import pspso\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nbreastcancer = datasets.load_breast_cancer()\ndata=breastcancer.data#get the breast cancer dataset input features\ntarget=breastcancer.target# target\nX_train, X_test, Y_train, Y_test = train_test_split(data, target,test_size=0.1,random_state=42,stratify=target)\nnormalize = MinMaxScaler(feature_range=(0,1))#normalize input features \nX_train=normalize.fit_transform(X_train)\nX_test=normalize.transform(X_test)\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train,test_size=0.15,random_state=42,stratify=Y_train)\n\nparams = {\n        \"learning_rate\":  [0.01,0.2,2],\n        \"max_depth\": [1,10,0],\n        \"n_estimators\": [2,200,0],\n        \"subsample\": [0.7,1,1]}\np=pspso(estimator='xgboost',params=params,task='binary classification', score='auc')\npos,cost,duration,model,optimizer=p.fitpspso(X_train,Y_train,X_val,Y_val)\np.print_results()#print the results\ntestscore=pspso.predict(p.model,p.estimator,p.task,p.score, X_test, Y_test)\nprint(1-testscore)\n</pre>\n<h2>XGBoost Example (Regression)</h2>\n<p>The XGBoost is an implementation of boosting decision trees. Five\nparameters were utilized for selection: objective, learning rate,\nmaximum depth, number of estimators, and subsample. Three categorical\nvalues were selected for the objective parameter. The learning rate\nparameter values range between <em>0.01</em> and <em>0.2</em> with <em>2</em> decimal point,\nmaximum depth ranges between <em>1</em> and <em>10</em> with <em>0</em> decimal points\n<em>(1,2,3,4,5,6,7,8,9,10)</em>, etc. The task and score are selected as\nregression and RMSE respectively. The number of particles and number of\niterations can be left as default values if needed. Then, a pspso\ninstance is created. By applying the fitpspso function, the selection\nprocess is applied. Finally, results are printed back to the user. The\nbest model, best parameters, score, time, and other details will be\nsaved in the created instance for the user to check.</p>\n<pre>from sklearn.preprocessing import MinMaxScaler\nfrom pspso import pspso\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nboston_data = datasets.load_boston()\ndata=boston_data.data\ntarget=boston_data.target\n\nX_train, X_test, Y_train, Y_test = train_test_split(data, target,test_size=0.1,random_state=42)\nnormalize = MinMaxScaler(feature_range=(0,1))#normalize input features\nnormalizetarget = MinMaxScaler(feature_range=(0,1))#normalize target\n\nX_train=normalize.fit_transform(X_train)\nX_test=normalize.transform(X_test)\nY_train=normalizetarget.fit_transform(Y_train.reshape(-1,1))\nY_test=normalizetarget.transform(Y_test.reshape(-1,1))\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train,test_size=0.25,random_state=42)\nparams = {\n        \"objective\":['reg:tweedie',\"reg:linear\",\"reg:gamma\"],\n        \"learning_rate\":  [0.01,0.2,2],\n        \"max_depth\": [1,10,0],\n        \"n_estimators\": [2,200,0],\n        \"subsample\": [0.7,1,1]}\np=pspso(estimator='xgboost',params=params,task='regression', score='rmse')\npos,cost,duration,model,optimizer=p.fitpspso(X_train,Y_train,X_val,Y_val)\np.print_results()#print the results\ntestscore=pspso.predict(p.model,p.estimator,p.task,p.score, X_test, Y_test)\nprint(testscore)\n</pre>\n<h2>User Input</h2>\n<p>The user is required to select the type of the algorithm ('mlp', 'svm',\n'xgboost', 'gbdt'); the task type ('binary\nclassification','regression'), score ('rmse', 'acc', or 'auc'). The user\ncan keep the parameters variable empty, where a default set of\nparameters and ranges is loaded for each algorithm.</p>\n<pre>from pspso import pspso\ntask='binary classification'\nscore='auc'\np=pspso.pspso('xgboost',None,task,score)\n</pre>\n<p>Pspso allows the user to provide a range of parameters for exploration.\nThe parameters vary between each algorithm. Any parameter supported by\nthe Scikit-Learn API for GBDT and XGBoost can be added to the selection\nprocess. A set of parameters that contains five XGBoost parameters is\nshown below. The parameters are encoded in JSON object that consists of\n<em>key,value</em> pairs:</p>\n<pre>params = {\"objective\":['reg:tweedie',\"reg:linear\",\"reg:gamma\"],\n        \"learning_rate\":  [0.01,0.2,2],\n        \"max_depth\": [1,10,0],\n        \"n_estimators\": [2,200,0],\n        \"subsample\": [0.7,1,1]}\n</pre>\n<p>The key can be any parameter belonging to to the algorithm under\ninvestigation. The value is a list. Pspso will check the type of the\nfirst element in the list, which will determine if the values of the\nparameter are categorical or numerical.</p>\n<p><strong>Categorical Parameters</strong></p>\n<p>If the parameter values are <em>categorical</em>, string values are expected to\nbe found in the list, as shown in <em>objective</em> parameter. The values in\nthe list will be automatically mapped into a list of integers, where\neach integer represents a value in the original list. The order of the\nvalues inside the list affect the position of the value in the search\nspace.</p>\n<p><strong>Numerical Parameters</strong></p>\n<p>If the parameter is numerical, a list of three elements [lb,ub, rv] is\nexpected to be found:</p>\n<ul>\n<li><strong>lb</strong>: repesents the lowest value in the search space</li>\n<li><strong>ub</strong>: represents the maximum value in the search space</li>\n<li><strong>rv</strong>: represents the number of decimal points the parameter values\nare rounded to before being added for training the algorithm</li>\n</ul>\n<p>For e.g if you want pspso to select n_estimators, add the following\nlist <em>[2,200,0]</em>. By that, the lowest n_estimators will be <em>2</em>, the\nhighest to be examined is <em>200</em>, and each possible value is rounded to\nan integer value ( <em>0</em> decimal points).</p>\n<p><strong>Other parameters</strong></p>\n<p>The user is given the chance to handle some of the default parameters\nsuch as the number of epochs in the MLP. Although this parameter can be\noptimized, but its not encouraged. The user can modify this by changing\na pspso class instance. For e.g., to change the number of epochs from\ndefault to 10 in MLP training:</p>\n<pre>from pspso import pspso\ntask='binary classification'\nscore='auc'\np=pspso.pspso('mlp',None,task,score)# in case of empty set of params (None) default search space is loaded\np.defaultparams['epochs']=10\n</pre>\n<p>The verbosity can be modified for any algorithm, which allows showing\ndetails of the training process:</p>\n<pre>from pspso import pspso\ntask='binary classification'\nscore='auc'\np=pspso.pspso('mlp',None,task,score)\np.verbosity=1\n</pre>\n<p>Early stopping rounds can alos be modified, the user can set a value\ndifferent to the default value:</p>\n<pre>from pspso import pspso\ntask='binary classification'\nscore='auc'\np=pspso.pspso('xgboost',None,task,score)\np.early_stopping=10\n</pre>\n<p>Other parameters such that n_jobs in XGBoost can also be modified\nbefore the start of the selection process.</p>\n<h1>Contributing</h1>\n<p>Pull requests are welcome. For major changes, please open an issue first\nto discuss what you would like to change.</p>\n<p>Please make sure to update tests as appropriate.</p>\n<p>We are working towards adding the cross validation support that will\ntake the training data and number of folds, then split the records and\ntrain each fold. Finally, the average performance is retuned to the\nuser.</p>\n<p>We are also working on adding multi-class classification and data\noversampling techniques.</p>\n<h1>License</h1>\n<p>Copyright (c) [2020] [Ali Haidar]</p>\n<p>Permission is hereby granted, free of charge, to any person obtaining a\ncopy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:</p>\n<p>The above copyright notice and this permission notice shall be included\nin all copies or substantial portions of the Software.</p>\n<p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\nOR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>\n\n          </div>"}, "last_serial": 6992803, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "33f2b58c6d45ed48a956fef9dd3324aa", "sha256": "12391bdb3900171d268b8b42a1e540c81e8d43fc68cac6a1a0d94de3633a9787"}, "downloads": -1, "filename": "pspso-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "33f2b58c6d45ed48a956fef9dd3324aa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11549, "upload_time": "2020-03-16T00:26:40", "upload_time_iso_8601": "2020-03-16T00:26:40.800690Z", "url": "https://files.pythonhosted.org/packages/59/5a/9059b86ff4195cbd49ef86a876344ac568ac110dc1a5e58e17821ef31bfe/pspso-0.0.1-py3-none-any.whl", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "a6f6ba8e6c059b518899cc563dd8ce9c", "sha256": "e55332879275faa97b350bf80d1e9e37e061bfd50515c36e98157e5720ac4b36"}, "downloads": -1, "filename": "pspso-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "a6f6ba8e6c059b518899cc563dd8ce9c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11547, "upload_time": "2020-03-16T00:30:39", "upload_time_iso_8601": "2020-03-16T00:30:39.647637Z", "url": "https://files.pythonhosted.org/packages/fd/2c/78b5c21080eef1fbfa88e4863c5449d31f1a67471477f5e4081669fb0226/pspso-0.0.2-py3-none-any.whl", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "8411a71670001cd6b8c3fd40c1381085", "sha256": "a5b87328481582b08eb776ce38497b9d55da5d40ffc8f36bc843263a920ba549"}, "downloads": -1, "filename": "pspso-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "8411a71670001cd6b8c3fd40c1381085", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11526, "upload_time": "2020-03-16T00:35:09", "upload_time_iso_8601": "2020-03-16T00:35:09.969536Z", "url": "https://files.pythonhosted.org/packages/8f/ee/711540caf000ad732265f1c3d2e7b7ad5e804a1c9369e4cdf5963adb8f2a/pspso-0.0.3-py3-none-any.whl", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "74874a1597e79759358da63beb36671b", "sha256": "587ac6ed2dee4aa152e1c4bb8b161210822122f77fb51fb5b63bd3cb911ca83c"}, "downloads": -1, "filename": "pspso-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "74874a1597e79759358da63beb36671b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11547, "upload_time": "2020-03-16T00:40:00", "upload_time_iso_8601": "2020-03-16T00:40:00.303936Z", "url": "https://files.pythonhosted.org/packages/5c/98/78a566916fef82cfb8a744f8f1393194fcfe5ce147295dda24d564c87da3/pspso-0.0.4-py3-none-any.whl", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "e9f7c295c65faab21a540b23ec01def1", "sha256": "fe18ab002c3a8caf98c32667c0e5112053b4a02de1141602f1973c9338bfea9d"}, "downloads": -1, "filename": "pspso-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "e9f7c295c65faab21a540b23ec01def1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11545, "upload_time": "2020-03-16T00:43:45", "upload_time_iso_8601": "2020-03-16T00:43:45.381264Z", "url": "https://files.pythonhosted.org/packages/1c/de/65ca53ed8050ce7cbd2da147033f40586075f0b8317b87f3f62e1a413ea0/pspso-0.0.5-py3-none-any.whl", "yanked": false}], "0.0.6": [{"comment_text": "", "digests": {"md5": "c8059e1154e4ab6d2e6e0c6d85b2889c", "sha256": "8114268106dff0896d3fc7f7224b4cb34f23eed781ad97f4c7e57681ee5feae0"}, "downloads": -1, "filename": "pspso-0.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "c8059e1154e4ab6d2e6e0c6d85b2889c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11587, "upload_time": "2020-03-17T09:27:09", "upload_time_iso_8601": "2020-03-17T09:27:09.871883Z", "url": "https://files.pythonhosted.org/packages/75/d6/295dc54d45a06853bfed7e541639f725a82b8ffa236ca21ba47f4211c442/pspso-0.0.6-py3-none-any.whl", "yanked": false}], "0.0.7": [{"comment_text": "", "digests": {"md5": "d9c5067e21b4e454780d650bc7dc2f4d", "sha256": "cf6fc3293f13fcb9fda44ceebd4a6a06108cebca10a50a1225d9cd300ecf9c11"}, "downloads": -1, "filename": "pspso-0.0.7-py3-none-any.whl", "has_sig": false, "md5_digest": "d9c5067e21b4e454780d650bc7dc2f4d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 14391, "upload_time": "2020-03-18T23:43:04", "upload_time_iso_8601": "2020-03-18T23:43:04.824948Z", "url": "https://files.pythonhosted.org/packages/71/a2/84b83f12265ad6a7a7920f8e178e2d7d9d7c650fdc1fe6322b6427b1a72a/pspso-0.0.7-py3-none-any.whl", "yanked": false}], "0.0.8": [{"comment_text": "", "digests": {"md5": "f316dffd5cec26d983c4fe8f804c9312", "sha256": "811e12aa34a123194667f4b69b37ab6def00e2f037c5cc174ddb78d36ecbc80c"}, "downloads": -1, "filename": "pspso-0.0.8-py3-none-any.whl", "has_sig": false, "md5_digest": "f316dffd5cec26d983c4fe8f804c9312", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 15615, "upload_time": "2020-04-04T05:32:33", "upload_time_iso_8601": "2020-04-04T05:32:33.932108Z", "url": "https://files.pythonhosted.org/packages/cc/85/1bddb51ecb3ec8b6767b2a933ea3ae4199e6d2721b418abcaf16627df044/pspso-0.0.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d1a1989202d91cacbd4bb5a63f17edd3", "sha256": "15f21f5f56bfa15a755e8ada28d84807fc9a41bb64ed9802aee655a712c35fcd"}, "downloads": -1, "filename": "pspso-0.0.8.tar.gz", "has_sig": false, "md5_digest": "d1a1989202d91cacbd4bb5a63f17edd3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 19680, "upload_time": "2020-04-04T05:32:35", "upload_time_iso_8601": "2020-04-04T05:32:35.952575Z", "url": "https://files.pythonhosted.org/packages/54/ea/720e3f49fb764b46166c0b06c45df47e8114c767379ada12446cc706e485/pspso-0.0.8.tar.gz", "yanked": false}], "0.0.9": [{"comment_text": "", "digests": {"md5": "c90757a058e35ac7d00dfca30a147d5f", "sha256": "51f21e12b0b25a6ceb249cb79cf22c813f86a052bfdba0fb701c6d3df49c1d86"}, "downloads": -1, "filename": "pspso-0.0.9-py3-none-any.whl", "has_sig": false, "md5_digest": "c90757a058e35ac7d00dfca30a147d5f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 15545, "upload_time": "2020-04-10T11:54:06", "upload_time_iso_8601": "2020-04-10T11:54:06.575165Z", "url": "https://files.pythonhosted.org/packages/0c/84/750b22e53908f27f7300edcd034dccd1208d673530c60efb0c65557f6be2/pspso-0.0.9-py3-none-any.whl", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "b02b190a88af9b7d70b8c762af2f4456", "sha256": "2f98a42808aaa056c250edc2cf3efd73fece0cfbfc09da2ad62ffd00a4e5f170"}, "downloads": -1, "filename": "pspso-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "b02b190a88af9b7d70b8c762af2f4456", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 15300, "upload_time": "2020-04-10T12:04:32", "upload_time_iso_8601": "2020-04-10T12:04:32.623869Z", "url": "https://files.pythonhosted.org/packages/36/52/ea73bc092378e332c61e4cdb26afb55a07a910734225a2e67e4eab7e1493/pspso-0.1.0-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b02b190a88af9b7d70b8c762af2f4456", "sha256": "2f98a42808aaa056c250edc2cf3efd73fece0cfbfc09da2ad62ffd00a4e5f170"}, "downloads": -1, "filename": "pspso-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "b02b190a88af9b7d70b8c762af2f4456", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 15300, "upload_time": "2020-04-10T12:04:32", "upload_time_iso_8601": "2020-04-10T12:04:32.623869Z", "url": "https://files.pythonhosted.org/packages/36/52/ea73bc092378e332c61e4cdb26afb55a07a910734225a2e67e4eab7e1493/pspso-0.1.0-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 03:15:47 2020"}