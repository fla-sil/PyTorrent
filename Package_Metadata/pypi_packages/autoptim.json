{"info": {"author": "", "author_email": "", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: MacOS", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX", "Operating System :: Unix", "Programming Language :: Python", "Topic :: Scientific/Engineering", "Topic :: Software Development"], "description": "# autoptim: automatic differentiation + optimization\n\nDo you have a new machine learning model that you want to optimize, and do not want to bother computing the gradients? Autoptim is for you.\n\n## Short presentation\nAutoptim is a small Python package that blends `autograd` automatic differentiation in `scipy.optimize.minimize`.\n\nThe gradients are computed under the hood using automatic differentiation; the user only provides the objective function:\n\n```python\nimport numpy as np\nfrom autoptim import minimize\n\n\ndef rosenbrock(x):\n    return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n\n\nx0 = np.zeros(2)\n\nx_min, _ = minimize(rosenbrock, x0)\nprint(x_min)\n\n>>> [0.99999913 0.99999825]\n```\n\nIt comes with the following features:\n\n- **Natural interfacing with Numpy**: The objective function is written in standard Numpy. The input/ output of `autoptim.minimize` are Numpy arrays.\n\n- **Smart input processing**: `scipy.optimize.minimize` is only meant to deal with one-dimensional arrays as input. In `autoptim`, variables can be multi-dimensional arrays or lists of arrays.\n- **Preconditioning**: Preconditioning is a simple way to accelerate minimization, by doing a change of variables. `autoptim` makes preconditioning straightforward. \n\n\n### Disclaimer\n\nThis package is meant to be as easy to use as possible. As so, some compromises on the speed of minimization are made.\n## Installation\n  To install, use `pip`:\n  ```\n  pip install autoptim\n  ```\n## Dependencies\n- numpy>=1.12\n- scipy>=0.18.0\n- autograd >= 1.2\n\n\n## Examples\nSeveral examples can be found in `autoptim/tutorials`\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "https://github.com/pierreablin/autoptim.git", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "MIT", "maintainer": "Pierre Ablin", "maintainer_email": "pierre.ablin@inria.fr", "name": "autoptim", "package_url": "https://pypi.org/project/autoptim/", "platform": "any", "project_url": "https://pypi.org/project/autoptim/", "project_urls": {"Download": "https://github.com/pierreablin/autoptim.git"}, "release_url": "https://pypi.org/project/autoptim/0.3/", "requires_dist": null, "requires_python": "", "summary": "Optimization with autodiff", "version": "0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            # autoptim: automatic differentiation + optimization<br><br>Do you have a new machine learning model that you want to optimize, and do not want to bother computing the gradients? Autoptim is for you.<br><br>## Short presentation<br>Autoptim is a small Python package that blends `autograd` automatic differentiation in `scipy.optimize.minimize`.<br><br>The gradients are computed under the hood using automatic differentiation; the user only provides the objective function:<br><br>```python<br>import numpy as np<br>from autoptim import minimize<br><br><br>def rosenbrock(x):<br>    return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2<br><br><br>x0 = np.zeros(2)<br><br>x_min, _ = minimize(rosenbrock, x0)<br>print(x_min)<br><br>&gt;&gt;&gt; [0.99999913 0.99999825]<br>```<br><br>It comes with the following features:<br><br>- **Natural interfacing with Numpy**: The objective function is written in standard Numpy. The input/ output of `autoptim.minimize` are Numpy arrays.<br><br>- **Smart input processing**: `scipy.optimize.minimize` is only meant to deal with one-dimensional arrays as input. In `autoptim`, variables can be multi-dimensional arrays or lists of arrays.<br>- **Preconditioning**: Preconditioning is a simple way to accelerate minimization, by doing a change of variables. `autoptim` makes preconditioning straightforward. <br><br><br>### Disclaimer<br><br>This package is meant to be as easy to use as possible. As so, some compromises on the speed of minimization are made.<br>## Installation<br>  To install, use `pip`:<br>  ```<br>  pip install autoptim<br>  ```<br>## Dependencies<br>- numpy&gt;=1.12<br>- scipy&gt;=0.18.0<br>- autograd &gt;= 1.2<br><br><br>## Examples<br>Several examples can be found in `autoptim/tutorials`<br><br><br>\n          </div>"}, "last_serial": 4894890, "releases": {"0.0": [{"comment_text": "", "digests": {"md5": "aed7956ae7979f707101d72c5049d881", "sha256": "ddde2c2ae1fbb5f4a27bedce707b8f68b9c7b231f97d222986a540190f5dbd09"}, "downloads": -1, "filename": "autoptim-0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "aed7956ae7979f707101d72c5049d881", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5701, "upload_time": "2019-02-18T12:21:48", "upload_time_iso_8601": "2019-02-18T12:21:48.685771Z", "url": "https://files.pythonhosted.org/packages/48/74/2eb48f62972747224ef39943ba45f07730b68cdd47535197755c5eef7850/autoptim-0.0-py3-none-any.whl", "yanked": false}], "0.1": [{"comment_text": "", "digests": {"md5": "230c974f9f9063d3253fb2d6ed742a06", "sha256": "8510f7271bf27227b6440091d7bd948de3eb446fb7fd7b123f7f108eef6db562"}, "downloads": -1, "filename": "autoptim-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "230c974f9f9063d3253fb2d6ed742a06", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5806, "upload_time": "2019-02-18T12:30:49", "upload_time_iso_8601": "2019-02-18T12:30:49.059732Z", "url": "https://files.pythonhosted.org/packages/66/e9/85bb25655bfc9c775c93f790610c3a2aff2cc33ebbda73beab7f22106b0b/autoptim-0.1-py3-none-any.whl", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "2ad900633d124ae89693ac275630a17c", "sha256": "ea5d5055c60e18299b479abfba5357f1f7c181fbcb0ea5b13689e57f1178dfbc"}, "downloads": -1, "filename": "autoptim-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "2ad900633d124ae89693ac275630a17c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5785, "upload_time": "2019-02-18T12:38:32", "upload_time_iso_8601": "2019-02-18T12:38:32.705987Z", "url": "https://files.pythonhosted.org/packages/8b/4f/0c0aee5457bc3ebe8d93a74a6912998b2e629e46abdec539ae19b76da332/autoptim-0.2-py3-none-any.whl", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "612678dc2703af0cd00356bccdba6b75", "sha256": "f4492bd4666c62f1913b0eb5a68f235d2ac574d9cc98b0f428198184324af8a4"}, "downloads": -1, "filename": "autoptim-0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "612678dc2703af0cd00356bccdba6b75", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5720, "upload_time": "2019-03-04T15:16:18", "upload_time_iso_8601": "2019-03-04T15:16:18.305817Z", "url": "https://files.pythonhosted.org/packages/8a/2c/f616fc7988db7883eeb29818e13a8508b9c2567bf1cf902b3839dc573438/autoptim-0.3-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "612678dc2703af0cd00356bccdba6b75", "sha256": "f4492bd4666c62f1913b0eb5a68f235d2ac574d9cc98b0f428198184324af8a4"}, "downloads": -1, "filename": "autoptim-0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "612678dc2703af0cd00356bccdba6b75", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5720, "upload_time": "2019-03-04T15:16:18", "upload_time_iso_8601": "2019-03-04T15:16:18.305817Z", "url": "https://files.pythonhosted.org/packages/8a/2c/f616fc7988db7883eeb29818e13a8508b9c2567bf1cf902b3839dc573438/autoptim-0.3-py3-none-any.whl", "yanked": false}], "timestamp": "Thu May  7 18:16:11 2020"}