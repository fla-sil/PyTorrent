{"info": {"author": "mrava", "author_email": "mrava@equinor.com", "bugtrack_url": null, "classifiers": ["Development Status :: 1 - Planning", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: GNU Lesser General Public License v3 or later (LGPLv3+)", "Natural Language :: English", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Mathematics"], "description": "![PyLops-gpu](https://github.com/equinor/pylops-gpu/blob/master/docs/source/_static/g-pylops_b.png)\n\n[![Build Status](https://travis-ci.org/equinor/pylops-gpu.svg?branch=master)](https://travis-ci.org/equinor/pylops-gpu)\n[![AzureDevOps Status](https://dev.azure.com/MRAVA/PyLops/_apis/build/status/equinor.pylops-gpu?branchName=master)](https://dev.azure.com/MRAVA/PyLops/_build/latest?definitionId=2&branchName=master)\n[![Documentation Status](https://readthedocs.org/projects/pylops-gpu/badge/?version=latest)](https://pylops-gpu.readthedocs.io/en/latest/?badge=latest)\n[![OS-support](https://img.shields.io/badge/OS-linux,osx-850A8B.svg)](https://github.com/equinor/pylops-gpu)\n[![Slack Status](https://img.shields.io/badge/chat-slack-green.svg)](https://pylops.slack.com)\n\n:vertical_traffic_light: :vertical_traffic_light: This library is under early development.\nExpect things to constantly change until version v1.0.0. :vertical_traffic_light: :vertical_traffic_light:\n\n## Objective\nThis library is an extension of [PyLops](https://pylops.readthedocs.io/en/latest/)\nto run operators on GPUs.\n\nAs much as [numpy](http://www.numpy.org) and [scipy](http://www.scipy.org/scipylib/index.html) lie\nat the core of the parent project PyLops, PyLops-GPU heavily builds on top of\n[PyTorch](http://pytorch.org) and takes advantage of the same optimized\ntensor computations used in PyTorch for deep learning using GPUs and CPUs.\n\nDoing so, linear operators can be computed on GPUs.\n\nHere is a simple example showing how a diagonal operator can be created,\napplied and inverted using PyLops:\n```python\nimport numpy as np\nfrom pylops import Diagonal\n\nn = int(1e6)\nx = np.ones(n)\nd = np.arange(n) + 1.\n\nDop = Diagonal(d)\n\n# y = Dx\ny = Dop*x\n```\n\nand similarly using PyLops-gpu:\n```python\nimport numpy as np\nimport torch\nfrom pylops_gpu.utils.backend import device\nfrom pylops_gpu import Diagonal\n\ndev = device()\n\nn = int(1e6)\nx = torch.ones(n, dtype=torch.float64).to(dev)\nd = (torch.arange(0, n, dtype=torch.float64) + 1.).to(dev)\n\nDop = Diagonal(d, device=dev)\n\n# y = Dx\ny = Dop*x\n```\n\nRunning these two snippets of code in Google Colab with GPU enabled gives a 50+\nspeed up for the forward pass.\n\nAs a by-product of implementing PyLops linear operators in PyTorch, we can easily\nchain our operators with any nonlinear mathematical operation (e.g., log, sin, tan, pow, ...)\nas well as with operators from the ``torch.nn`` submodule and obtain *Automatic\nDifferentiation* (AD) for the entire chain. Since the gradient of a linear\noperator is simply its *adjoint*, we have implemented a single class,\n`pylops_gpu.TorchOperator`, which can wrap any linear operator\nfrom PyLops and PyLops-gpu libraries and return a `torch.autograd.Function` object.\n\n\n## Project structure\nThis repository is organized as follows:\n* **pylops_gpu**: python library containing various GPU-powered linear operators and auxiliary routines\n* **pytests**:    set of pytests\n* **testdata**:   sample datasets used in pytests and documentation\n* **docs**:       sphinx documentation\n* **examples**:   set of python script examples for each linear operator to be embedded in documentation using sphinx-gallery\n* **tutorials**:  set of python script tutorials to be embedded in documentation using sphinx-gallery\n\n## Getting started\n\nYou need **Python 3.5 or greater**.\n\n#### From PyPi\nComing soon...\n\n#### From Github\n\nYou can also directly install from the master node\n\n```\npip install git+https://git@github.com/equinor/pylops-gpu.git@master\n```\n\n## Contributing\n*Feel like contributing to the project? Adding new operators or tutorial?*\n\nFollow the instructions from [PyLops official documentation](https://pylops.readthedocs.io/en/latest/contributing.html).\n\n## Documentation\nThe official documentation of PyLops-gpu is available [here](https://pylops-gpu.readthedocs.io/).\n\nVisit this page to get started learning about different operators and their applications as well as how to\ncreate new operators yourself and make it to the ``Contributors`` list.\n\nMoreover, if you have installed PyLops using the *developer environment* you can also build the documentation locally by\ntyping the following command:\n```\nmake doc\n```\nOnce the documentation is created, you can make any change to the source code and rebuild the documentation by\nsimply typing\n```\nmake docupdate\n```\nNote that if a new example or tutorial is created (and if any change is made to a previously available example or tutorial)\nyou are required to rebuild the entire documentation before your changes will be visible.\n\n\n## History\nPyLops-GPU was initially written and it is currently maintained by [Equinor](https://www.equinor.com).\nIt is an extension of [PyLops](https://pylops.readthedocs.io/en/latest/) for large-scale optimization with\n*GPU-driven* linear operators on that can be tailored to our needs, and as contribution to the free software community.\n\n\n\n## Contributors\n* Matteo Ravasi, mrava87\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "algebra,inverse problems,large-scale optimization", "license": "", "maintainer": "", "maintainer_email": "", "name": "pylops-gpu", "package_url": "https://pypi.org/project/pylops-gpu/", "platform": "", "project_url": "https://pypi.org/project/pylops-gpu/", "project_urls": null, "release_url": "https://pypi.org/project/pylops-gpu/0.0.0/", "requires_dist": ["numpy (>=1.15.0)", "torch (>=1.2.0)", "pytorch-complex-tensor", "pylops"], "requires_python": "", "summary": " An extension to PyLops for linear operators on GPUs.", "version": "0.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><img alt=\"PyLops-gpu\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/46d9315d175e20b00d556c09a8eaaf9a4a3c5cfb/68747470733a2f2f6769746875622e636f6d2f657175696e6f722f70796c6f70732d6770752f626c6f622f6d61737465722f646f63732f736f757263652f5f7374617469632f672d70796c6f70735f622e706e67\"></p>\n<p><a href=\"https://travis-ci.org/equinor/pylops-gpu\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d001f57b6be6b7b2cc3ecbb52010fcd1066b6317/68747470733a2f2f7472617669732d63692e6f72672f657175696e6f722f70796c6f70732d6770752e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://dev.azure.com/MRAVA/PyLops/_build/latest?definitionId=2&amp;branchName=master\" rel=\"nofollow\"><img alt=\"AzureDevOps Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c3bb4f5e8cd134a261261c576bc4bebae9a9011d/68747470733a2f2f6465762e617a7572652e636f6d2f4d524156412f50794c6f70732f5f617069732f6275696c642f7374617475732f657175696e6f722e70796c6f70732d6770753f6272616e63684e616d653d6d6173746572\"></a>\n<a href=\"https://pylops-gpu.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1216ad70dbfce20e6b7b6ee84214301bd17a2ce2/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f70796c6f70732d6770752f62616467652f3f76657273696f6e3d6c6174657374\"></a>\n<a href=\"https://github.com/equinor/pylops-gpu\" rel=\"nofollow\"><img alt=\"OS-support\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/83560f3d08e6da9a8a142d295214fdf03d62a3f9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4f532d6c696e75782c6f73782d3835304138422e737667\"></a>\n<a href=\"https://pylops.slack.com\" rel=\"nofollow\"><img alt=\"Slack Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/80e660135a65146d7663257f6e7b891c67fa843a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d736c61636b2d677265656e2e737667\"></a></p>\n<p>:vertical_traffic_light: :vertical_traffic_light: This library is under early development.\nExpect things to constantly change until version v1.0.0. :vertical_traffic_light: :vertical_traffic_light:</p>\n<h2>Objective</h2>\n<p>This library is an extension of <a href=\"https://pylops.readthedocs.io/en/latest/\" rel=\"nofollow\">PyLops</a>\nto run operators on GPUs.</p>\n<p>As much as <a href=\"http://www.numpy.org\" rel=\"nofollow\">numpy</a> and <a href=\"http://www.scipy.org/scipylib/index.html\" rel=\"nofollow\">scipy</a> lie\nat the core of the parent project PyLops, PyLops-GPU heavily builds on top of\n<a href=\"http://pytorch.org\" rel=\"nofollow\">PyTorch</a> and takes advantage of the same optimized\ntensor computations used in PyTorch for deep learning using GPUs and CPUs.</p>\n<p>Doing so, linear operators can be computed on GPUs.</p>\n<p>Here is a simple example showing how a diagonal operator can be created,\napplied and inverted using PyLops:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pylops</span> <span class=\"kn\">import</span> <span class=\"n\">Diagonal</span>\n\n<span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mf\">1e6</span><span class=\"p\">)</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span>\n<span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mf\">1.</span>\n\n<span class=\"n\">Dop</span> <span class=\"o\">=</span> <span class=\"n\">Diagonal</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># y = Dx</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">Dop</span><span class=\"o\">*</span><span class=\"n\">x</span>\n</pre>\n<p>and similarly using PyLops-gpu:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pylops_gpu.utils.backend</span> <span class=\"kn\">import</span> <span class=\"n\">device</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pylops_gpu</span> <span class=\"kn\">import</span> <span class=\"n\">Diagonal</span>\n\n<span class=\"n\">dev</span> <span class=\"o\">=</span> <span class=\"n\">device</span><span class=\"p\">()</span>\n\n<span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mf\">1e6</span><span class=\"p\">)</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float64</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">dev</span><span class=\"p\">)</span>\n<span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float64</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mf\">1.</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">dev</span><span class=\"p\">)</span>\n\n<span class=\"n\">Dop</span> <span class=\"o\">=</span> <span class=\"n\">Diagonal</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">dev</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># y = Dx</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">Dop</span><span class=\"o\">*</span><span class=\"n\">x</span>\n</pre>\n<p>Running these two snippets of code in Google Colab with GPU enabled gives a 50+\nspeed up for the forward pass.</p>\n<p>As a by-product of implementing PyLops linear operators in PyTorch, we can easily\nchain our operators with any nonlinear mathematical operation (e.g., log, sin, tan, pow, ...)\nas well as with operators from the <code>torch.nn</code> submodule and obtain <em>Automatic\nDifferentiation</em> (AD) for the entire chain. Since the gradient of a linear\noperator is simply its <em>adjoint</em>, we have implemented a single class,\n<code>pylops_gpu.TorchOperator</code>, which can wrap any linear operator\nfrom PyLops and PyLops-gpu libraries and return a <code>torch.autograd.Function</code> object.</p>\n<h2>Project structure</h2>\n<p>This repository is organized as follows:</p>\n<ul>\n<li><strong>pylops_gpu</strong>: python library containing various GPU-powered linear operators and auxiliary routines</li>\n<li><strong>pytests</strong>:    set of pytests</li>\n<li><strong>testdata</strong>:   sample datasets used in pytests and documentation</li>\n<li><strong>docs</strong>:       sphinx documentation</li>\n<li><strong>examples</strong>:   set of python script examples for each linear operator to be embedded in documentation using sphinx-gallery</li>\n<li><strong>tutorials</strong>:  set of python script tutorials to be embedded in documentation using sphinx-gallery</li>\n</ul>\n<h2>Getting started</h2>\n<p>You need <strong>Python 3.5 or greater</strong>.</p>\n<h4>From PyPi</h4>\n<p>Coming soon...</p>\n<h4>From Github</h4>\n<p>You can also directly install from the master node</p>\n<pre><code>pip install git+https://git@github.com/equinor/pylops-gpu.git@master\n</code></pre>\n<h2>Contributing</h2>\n<p><em>Feel like contributing to the project? Adding new operators or tutorial?</em></p>\n<p>Follow the instructions from <a href=\"https://pylops.readthedocs.io/en/latest/contributing.html\" rel=\"nofollow\">PyLops official documentation</a>.</p>\n<h2>Documentation</h2>\n<p>The official documentation of PyLops-gpu is available <a href=\"https://pylops-gpu.readthedocs.io/\" rel=\"nofollow\">here</a>.</p>\n<p>Visit this page to get started learning about different operators and their applications as well as how to\ncreate new operators yourself and make it to the <code>Contributors</code> list.</p>\n<p>Moreover, if you have installed PyLops using the <em>developer environment</em> you can also build the documentation locally by\ntyping the following command:</p>\n<pre><code>make doc\n</code></pre>\n<p>Once the documentation is created, you can make any change to the source code and rebuild the documentation by\nsimply typing</p>\n<pre><code>make docupdate\n</code></pre>\n<p>Note that if a new example or tutorial is created (and if any change is made to a previously available example or tutorial)\nyou are required to rebuild the entire documentation before your changes will be visible.</p>\n<h2>History</h2>\n<p>PyLops-GPU was initially written and it is currently maintained by <a href=\"https://www.equinor.com\" rel=\"nofollow\">Equinor</a>.\nIt is an extension of <a href=\"https://pylops.readthedocs.io/en/latest/\" rel=\"nofollow\">PyLops</a> for large-scale optimization with\n<em>GPU-driven</em> linear operators on that can be tailored to our needs, and as contribution to the free software community.</p>\n<h2>Contributors</h2>\n<ul>\n<li>Matteo Ravasi, mrava87</li>\n</ul>\n\n          </div>"}, "last_serial": 6439940, "releases": {"0.0.0": [{"comment_text": "", "digests": {"md5": "2b25cbba513d535dae68a39ffabcbece", "sha256": "05e2e9d445965e7dff1260c8b35a3c4e91e3c5d3409c605c676a26a78fb269bb"}, "downloads": -1, "filename": "pylops_gpu-0.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "2b25cbba513d535dae68a39ffabcbece", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 35300, "upload_time": "2020-01-12T14:33:43", "upload_time_iso_8601": "2020-01-12T14:33:43.686177Z", "url": "https://files.pythonhosted.org/packages/49/c2/8bc2c9238c72ea3f7288571637ffa30e79581d897d5c9dfc988983d02cb4/pylops_gpu-0.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "feacceeed6f11bee3e6b99b2a9190380", "sha256": "b336f5d67f739f0a35137c487d9b2812849f961923523fa888166fbd07702612"}, "downloads": -1, "filename": "pylops_gpu-0.0.0.tar.gz", "has_sig": false, "md5_digest": "feacceeed6f11bee3e6b99b2a9190380", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25649, "upload_time": "2020-01-12T14:33:46", "upload_time_iso_8601": "2020-01-12T14:33:46.498782Z", "url": "https://files.pythonhosted.org/packages/8f/04/0d21fe1eb3d9e67bd47f8aae919df3b11319d3f881ea70d51f2802168792/pylops_gpu-0.0.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2b25cbba513d535dae68a39ffabcbece", "sha256": "05e2e9d445965e7dff1260c8b35a3c4e91e3c5d3409c605c676a26a78fb269bb"}, "downloads": -1, "filename": "pylops_gpu-0.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "2b25cbba513d535dae68a39ffabcbece", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 35300, "upload_time": "2020-01-12T14:33:43", "upload_time_iso_8601": "2020-01-12T14:33:43.686177Z", "url": "https://files.pythonhosted.org/packages/49/c2/8bc2c9238c72ea3f7288571637ffa30e79581d897d5c9dfc988983d02cb4/pylops_gpu-0.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "feacceeed6f11bee3e6b99b2a9190380", "sha256": "b336f5d67f739f0a35137c487d9b2812849f961923523fa888166fbd07702612"}, "downloads": -1, "filename": "pylops_gpu-0.0.0.tar.gz", "has_sig": false, "md5_digest": "feacceeed6f11bee3e6b99b2a9190380", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25649, "upload_time": "2020-01-12T14:33:46", "upload_time_iso_8601": "2020-01-12T14:33:46.498782Z", "url": "https://files.pythonhosted.org/packages/8f/04/0d21fe1eb3d9e67bd47f8aae919df3b11319d3f881ea70d51f2802168792/pylops_gpu-0.0.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:02:47 2020"}