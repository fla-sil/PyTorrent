{"info": {"author": "Wessel Bruinsma", "author_email": "wessel.p.bruinsma@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# [FDM: Finite Difference Methods](http://github.com/wesselb/fdm)\n\n[![Build](https://travis-ci.org/wesselb/fdm.svg?branch=master)](https://travis-ci.org/wesselb/fdm)\n[![Coverage Status](https://coveralls.io/repos/github/wesselb/fdm/badge.svg?branch=master&service=github)](https://coveralls.io/github/wesselb/fdm?branch=master)\n[![Latest Docs](https://img.shields.io/badge/docs-latest-blue.svg)](https://wesselb.github.io/fdm)\n\nFDM estimates derivatives with finite differences.\nSee also [FDM.jl](https://github.com/invenia/FDM.jl).\n\n*Note:* FDM requires Python 3.6 or higher.\n\n* [Installation](#installation)\n* [Multivariate Derivatives](#multivariate-derivatives)\n    - [Gradients](#gradients)\n    - [Jacobians](#jacobians)\n    - [Jacobian-Vector Products (Directional Derivatives)](#jacobian-vector-products-directional-derivatives)\n    - [Hessian-Vector Products](#hessian-vector-products)\n* [Scalar Derivatives](#scalar-derivatives)\n* [Testing Sensitivities in a Reverse-Mode Automatic Differentation Framework](#testing-sensitivities-in-a-reverse-mode-automatic-differentation-framework)\n\n## Installation\n```bash\npip install fdm\n```\n\n## Multivariate Derivatives\n\n```python\nfrom fdm import gradient, jacobian, jvp, hvp\n```\n\nFor the purpose of illustration, let us consider a quadratic function:\n\n```python\n>>> a = np.random.randn(3, 3); a = a @ a.T\n>>> a\narray([[ 3.57224794,  0.22646662, -1.80432262],\n       [ 0.22646662,  4.72596213,  3.46435663],\n       [-1.80432262,  3.46435663,  3.70938152]])\n       \n>>> def f(x):\n...     return 0.5 * x @ a @ x\n```\n\nConsider the following input value:\n\n```python\n>>> x = np.array([1.0, 2.0, 3.0])\n```\n\n### Gradients\n\n```python\n>>> grad = gradient(f)\n>>> grad(x)\narray([-1.38778668, 20.07146076, 16.25253519])\n\n>>> a @ x\narray([-1.38778668, 20.07146076, 16.25253519])\n```\n\n### Jacobians\n\n```python\n>>> jac = jacobian(f)\n>>> jac(x)\narray([[-1.38778668, 20.07146076, 16.25253519]])\n\n>>> a @ x\narray([-1.38778668, 20.07146076, 16.25253519])\n```\n\nBut `jacobian` also works for multi-valued functions.\n\n```python\n>>> def f2(x):\n...     return a @ x\n\n>>> jac2 = jacobian(f2)\n>>> jac2(x)\narray([[ 3.57224794,  0.22646662, -1.80432262],\n       [ 0.22646662,  4.72596213,  3.46435663],\n       [-1.80432262,  3.46435663,  3.70938152]])\n       \n>>> a\narray([[ 3.57224794,  0.22646662, -1.80432262],\n       [ 0.22646662,  4.72596213,  3.46435663],\n       [-1.80432262,  3.46435663,  3.70938152]])\n```\n\n### Jacobian-Vector Products (Directional Derivatives)\n\nIn the scalar case, `jvp` computes directional derivatives:\n\n```python\n>>> v = np.array([0.5, 0.6, 0.7])  # A direction\n\n>>> dir_deriv = jvp(f, v) \n>>> dir_deriv(x)\n22.725757753354657\n\n>>> np.sum(grad(x) * v)\n22.72575775335481\n```\n\nIn the multivariate case, `jvp` generalises to Jacobian-vector products:\n\n```python\n>>> prod = jvp(f2, v)\n>>> prod(x)\narray([0.65897811, 5.37386023, 3.77301973])\n\n>>> a @ v\narray([0.65897811, 5.37386023, 3.77301973])\n```\n\n### Hessian-Vector Products\n\n```python\n>>> prod = hvp(f, v)\n>>> prod(x)\narray([[0.6589781 , 5.37386023, 3.77301973]])\n\n>>> 0.5 * (a + a.T) @ v\narray([0.65897811, 5.37386023, 3.77301973])\n```\n\n## Scalar Derivatives\n```python\n>>> from fdm import central_fdm\n```\n\nLet's try to estimate the first derivative of `np.sin` at `1` with a \nsecond-order method, where we know that `np.sin` is well conditioned.\n\n```python\n>>> central_fdm(order=2, deriv=1, condition=1)(np.sin, 1) - np.cos(1)  \n4.307577627926662e-10\n```\n\nAnd let's try to estimate the second derivative of `np.sin` at `1` with a \nthird-order method.\n\n```python\n>>> central_fdm(order=3, deriv=2, condition=1)(np.sin, 1) + np.sin(1)  \n-1.263876664436836e-07\n```\n\nHm.\nLet's check the accuracy of this third-order method.\nThe step size and accuracy of the method are computed upon calling\n`FDM.estimate()`.\n\n```python\n>>> central_fdm(order=3, deriv=2, condition=1).estimate().acc\n8.733476581980376e-06\n```\n\nWe might want a little more accuracy. Let's check the accuracy of a \nfifth-order method.\n\n```python\n>>> central_fdm(order=5, deriv=2, condition=1).estimate().acc\n7.343652562575155e-10\n```\n\nAnd let's estimate the second derivative of `np.sin` at `1` with a \nfifth-order method.\n\n```python\n>>> central_fdm(order=5, deriv=2, condition=1)(np.sin, 1) + np.sin(1)   \n-9.145184609593571e-11\n```\n\nHooray!\n\nFinally, let us verify that increasing the order indeed reliably increases \nthe accuracy.\n\n```python\n>>> for i in range(3, 11):\n...      print(central_fdm(order=i, deriv=2, condition=1)(np.sin, 1) + np.sin(1))\n-1.263876664436836e-07\n6.341286606925678e-09\n-9.145184609593571e-11\n2.7335911312320604e-12\n6.588063428125679e-13\n2.142730437526552e-13\n2.057243264630415e-13\n8.570921750106208e-14\n```\n\n## Testing Sensitivities in a Reverse-Mode Automatic Differentation Framework\n\nConsider the function\n\n```python\ndef mul(a, b):\n    return a * b\n```\n\nand its sensitivity\n\n```python\ndef s_mul(s_y, y, a, b):\n    return s_y * b, a * s_y\n```\n\nThe sensitivity `s_mul` takes in the sensitivity `s_y` of the output `y`, \nthe output `y`, and  the arguments of the function `mul`; and returns a tuple \ncontaining the sensitivities with respect to `a` and `b`.\nThen function `check_sensitivity` can be used to assert that the \nimplementation of `s_mul` is correct:\n\n```python\n>>> from fdm import check_sensitivity\n\n>>> check_sensitivity(mul, s_mul, (2, 3))  # Test at arguments `2` and `3`.\n```\n\nSuppose that the implementation were wrong, for example\n\n```python\ndef s_mul_wrong(s_y, y, a, b):\n    return s_y * b, b * s_y  # Used `b` instead of `a` for the second sensitivity!\n```\n\nThen `check_sensitivity` should throw an `AssertionError`:\n\n```python\n>>> check_sensitivity(mul, s_mul, (2, 3)) \nAssertionError: Sensitivity of argument 2 of function \"mul\" did not match numerical estimate.\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/wesselb/fdm", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "fdm", "package_url": "https://pypi.org/project/fdm/", "platform": "", "project_url": "https://pypi.org/project/fdm/", "project_urls": {"Homepage": "https://github.com/wesselb/fdm"}, "release_url": "https://pypi.org/project/fdm/0.2.0/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Compute derivatives with finite-difference methods", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1><a href=\"http://github.com/wesselb/fdm\" rel=\"nofollow\">FDM: Finite Difference Methods</a></h1>\n<p><a href=\"https://travis-ci.org/wesselb/fdm\" rel=\"nofollow\"><img alt=\"Build\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/24163eff13565f2720368175b9e94258fd5843fc/68747470733a2f2f7472617669732d63692e6f72672f77657373656c622f66646d2e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://coveralls.io/github/wesselb/fdm?branch=master\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/70b47ff27f6a8b485973e13467df7f551f7b3a63/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f77657373656c622f66646d2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562\"></a>\n<a href=\"https://wesselb.github.io/fdm\" rel=\"nofollow\"><img alt=\"Latest Docs\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/35e0b9e630dbc347bd4718399aa51fb3bb2bb889/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667\"></a></p>\n<p>FDM estimates derivatives with finite differences.\nSee also <a href=\"https://github.com/invenia/FDM.jl\" rel=\"nofollow\">FDM.jl</a>.</p>\n<p><em>Note:</em> FDM requires Python 3.6 or higher.</p>\n<ul>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#multivariate-derivatives\" rel=\"nofollow\">Multivariate Derivatives</a>\n<ul>\n<li><a href=\"#gradients\" rel=\"nofollow\">Gradients</a></li>\n<li><a href=\"#jacobians\" rel=\"nofollow\">Jacobians</a></li>\n<li><a href=\"#jacobian-vector-products-directional-derivatives\" rel=\"nofollow\">Jacobian-Vector Products (Directional Derivatives)</a></li>\n<li><a href=\"#hessian-vector-products\" rel=\"nofollow\">Hessian-Vector Products</a></li>\n</ul>\n</li>\n<li><a href=\"#scalar-derivatives\" rel=\"nofollow\">Scalar Derivatives</a></li>\n<li><a href=\"#testing-sensitivities-in-a-reverse-mode-automatic-differentation-framework\" rel=\"nofollow\">Testing Sensitivities in a Reverse-Mode Automatic Differentation Framework</a></li>\n</ul>\n<h2>Installation</h2>\n<pre>pip install fdm\n</pre>\n<h2>Multivariate Derivatives</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">fdm</span> <span class=\"kn\">import</span> <span class=\"n\">gradient</span><span class=\"p\">,</span> <span class=\"n\">jacobian</span><span class=\"p\">,</span> <span class=\"n\">jvp</span><span class=\"p\">,</span> <span class=\"n\">hvp</span>\n</pre>\n<p>For the purpose of illustration, let us consider a quadratic function:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">);</span> <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">a</span> <span class=\"o\">@</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">T</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">a</span>\n<span class=\"n\">array</span><span class=\"p\">([[</span> <span class=\"mf\">3.57224794</span><span class=\"p\">,</span>  <span class=\"mf\">0.22646662</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">1.80432262</span><span class=\"p\">],</span>\n       <span class=\"p\">[</span> <span class=\"mf\">0.22646662</span><span class=\"p\">,</span>  <span class=\"mf\">4.72596213</span><span class=\"p\">,</span>  <span class=\"mf\">3.46435663</span><span class=\"p\">],</span>\n       <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">1.80432262</span><span class=\"p\">,</span>  <span class=\"mf\">3.46435663</span><span class=\"p\">,</span>  <span class=\"mf\">3.70938152</span><span class=\"p\">]])</span>\n       \n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">def</span> <span class=\"nf\">f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n<span class=\"o\">...</span>     <span class=\"k\">return</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"n\">x</span> <span class=\"o\">@</span> <span class=\"n\">a</span> <span class=\"o\">@</span> <span class=\"n\">x</span>\n</pre>\n<p>Consider the following input value:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"mf\">2.0</span><span class=\"p\">,</span> <span class=\"mf\">3.0</span><span class=\"p\">])</span>\n</pre>\n<h3>Gradients</h3>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">gradient</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">grad</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"n\">array</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mf\">1.38778668</span><span class=\"p\">,</span> <span class=\"mf\">20.07146076</span><span class=\"p\">,</span> <span class=\"mf\">16.25253519</span><span class=\"p\">])</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">a</span> <span class=\"o\">@</span> <span class=\"n\">x</span>\n<span class=\"n\">array</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mf\">1.38778668</span><span class=\"p\">,</span> <span class=\"mf\">20.07146076</span><span class=\"p\">,</span> <span class=\"mf\">16.25253519</span><span class=\"p\">])</span>\n</pre>\n<h3>Jacobians</h3>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">jac</span> <span class=\"o\">=</span> <span class=\"n\">jacobian</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">jac</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"o\">-</span><span class=\"mf\">1.38778668</span><span class=\"p\">,</span> <span class=\"mf\">20.07146076</span><span class=\"p\">,</span> <span class=\"mf\">16.25253519</span><span class=\"p\">]])</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">a</span> <span class=\"o\">@</span> <span class=\"n\">x</span>\n<span class=\"n\">array</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mf\">1.38778668</span><span class=\"p\">,</span> <span class=\"mf\">20.07146076</span><span class=\"p\">,</span> <span class=\"mf\">16.25253519</span><span class=\"p\">])</span>\n</pre>\n<p>But <code>jacobian</code> also works for multi-valued functions.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">def</span> <span class=\"nf\">f2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n<span class=\"o\">...</span>     <span class=\"k\">return</span> <span class=\"n\">a</span> <span class=\"o\">@</span> <span class=\"n\">x</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">jac2</span> <span class=\"o\">=</span> <span class=\"n\">jacobian</span><span class=\"p\">(</span><span class=\"n\">f2</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">jac2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"n\">array</span><span class=\"p\">([[</span> <span class=\"mf\">3.57224794</span><span class=\"p\">,</span>  <span class=\"mf\">0.22646662</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">1.80432262</span><span class=\"p\">],</span>\n       <span class=\"p\">[</span> <span class=\"mf\">0.22646662</span><span class=\"p\">,</span>  <span class=\"mf\">4.72596213</span><span class=\"p\">,</span>  <span class=\"mf\">3.46435663</span><span class=\"p\">],</span>\n       <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">1.80432262</span><span class=\"p\">,</span>  <span class=\"mf\">3.46435663</span><span class=\"p\">,</span>  <span class=\"mf\">3.70938152</span><span class=\"p\">]])</span>\n       \n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">a</span>\n<span class=\"n\">array</span><span class=\"p\">([[</span> <span class=\"mf\">3.57224794</span><span class=\"p\">,</span>  <span class=\"mf\">0.22646662</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mf\">1.80432262</span><span class=\"p\">],</span>\n       <span class=\"p\">[</span> <span class=\"mf\">0.22646662</span><span class=\"p\">,</span>  <span class=\"mf\">4.72596213</span><span class=\"p\">,</span>  <span class=\"mf\">3.46435663</span><span class=\"p\">],</span>\n       <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">1.80432262</span><span class=\"p\">,</span>  <span class=\"mf\">3.46435663</span><span class=\"p\">,</span>  <span class=\"mf\">3.70938152</span><span class=\"p\">]])</span>\n</pre>\n<h3>Jacobian-Vector Products (Directional Derivatives)</h3>\n<p>In the scalar case, <code>jvp</code> computes directional derivatives:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.7</span><span class=\"p\">])</span>  <span class=\"c1\"># A direction</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">dir_deriv</span> <span class=\"o\">=</span> <span class=\"n\">jvp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">)</span> \n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">dir_deriv</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"mf\">22.725757753354657</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">grad</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">)</span>\n<span class=\"mf\">22.72575775335481</span>\n</pre>\n<p>In the multivariate case, <code>jvp</code> generalises to Jacobian-vector products:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">prod</span> <span class=\"o\">=</span> <span class=\"n\">jvp</span><span class=\"p\">(</span><span class=\"n\">f2</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">prod</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.65897811</span><span class=\"p\">,</span> <span class=\"mf\">5.37386023</span><span class=\"p\">,</span> <span class=\"mf\">3.77301973</span><span class=\"p\">])</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">a</span> <span class=\"o\">@</span> <span class=\"n\">v</span>\n<span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.65897811</span><span class=\"p\">,</span> <span class=\"mf\">5.37386023</span><span class=\"p\">,</span> <span class=\"mf\">3.77301973</span><span class=\"p\">])</span>\n</pre>\n<h3>Hessian-Vector Products</h3>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">prod</span> <span class=\"o\">=</span> <span class=\"n\">hvp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">prod</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">0.6589781</span> <span class=\"p\">,</span> <span class=\"mf\">5.37386023</span><span class=\"p\">,</span> <span class=\"mf\">3.77301973</span><span class=\"p\">]])</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">a</span> <span class=\"o\">+</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">@</span> <span class=\"n\">v</span>\n<span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.65897811</span><span class=\"p\">,</span> <span class=\"mf\">5.37386023</span><span class=\"p\">,</span> <span class=\"mf\">3.77301973</span><span class=\"p\">])</span>\n</pre>\n<h2>Scalar Derivatives</h2>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">fdm</span> <span class=\"kn\">import</span> <span class=\"n\">central_fdm</span>\n</pre>\n<p>Let's try to estimate the first derivative of <code>np.sin</code> at <code>1</code> with a\nsecond-order method, where we know that <code>np.sin</code> is well conditioned.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">central_fdm</span><span class=\"p\">(</span><span class=\"n\">order</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">deriv</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">condition</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">cos</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>  \n<span class=\"mf\">4.307577627926662e-10</span>\n</pre>\n<p>And let's try to estimate the second derivative of <code>np.sin</code> at <code>1</code> with a\nthird-order method.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">central_fdm</span><span class=\"p\">(</span><span class=\"n\">order</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">deriv</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">condition</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>  \n<span class=\"o\">-</span><span class=\"mf\">1.263876664436836e-07</span>\n</pre>\n<p>Hm.\nLet's check the accuracy of this third-order method.\nThe step size and accuracy of the method are computed upon calling\n<code>FDM.estimate()</code>.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">central_fdm</span><span class=\"p\">(</span><span class=\"n\">order</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">deriv</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">condition</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">estimate</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">acc</span>\n<span class=\"mf\">8.733476581980376e-06</span>\n</pre>\n<p>We might want a little more accuracy. Let's check the accuracy of a\nfifth-order method.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">central_fdm</span><span class=\"p\">(</span><span class=\"n\">order</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">deriv</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">condition</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">estimate</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">acc</span>\n<span class=\"mf\">7.343652562575155e-10</span>\n</pre>\n<p>And let's estimate the second derivative of <code>np.sin</code> at <code>1</code> with a\nfifth-order method.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">central_fdm</span><span class=\"p\">(</span><span class=\"n\">order</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">deriv</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">condition</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>   \n<span class=\"o\">-</span><span class=\"mf\">9.145184609593571e-11</span>\n</pre>\n<p>Hooray!</p>\n<p>Finally, let us verify that increasing the order indeed reliably increases\nthe accuracy.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">11</span><span class=\"p\">):</span>\n<span class=\"o\">...</span>      <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">central_fdm</span><span class=\"p\">(</span><span class=\"n\">order</span><span class=\"o\">=</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">deriv</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">condition</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"o\">-</span><span class=\"mf\">1.263876664436836e-07</span>\n<span class=\"mf\">6.341286606925678e-09</span>\n<span class=\"o\">-</span><span class=\"mf\">9.145184609593571e-11</span>\n<span class=\"mf\">2.7335911312320604e-12</span>\n<span class=\"mf\">6.588063428125679e-13</span>\n<span class=\"mf\">2.142730437526552e-13</span>\n<span class=\"mf\">2.057243264630415e-13</span>\n<span class=\"mf\">8.570921750106208e-14</span>\n</pre>\n<h2>Testing Sensitivities in a Reverse-Mode Automatic Differentation Framework</h2>\n<p>Consider the function</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">mul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">a</span> <span class=\"o\">*</span> <span class=\"n\">b</span>\n</pre>\n<p>and its sensitivity</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">s_mul</span><span class=\"p\">(</span><span class=\"n\">s_y</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">s_y</span> <span class=\"o\">*</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">a</span> <span class=\"o\">*</span> <span class=\"n\">s_y</span>\n</pre>\n<p>The sensitivity <code>s_mul</code> takes in the sensitivity <code>s_y</code> of the output <code>y</code>,\nthe output <code>y</code>, and  the arguments of the function <code>mul</code>; and returns a tuple\ncontaining the sensitivities with respect to <code>a</code> and <code>b</code>.\nThen function <code>check_sensitivity</code> can be used to assert that the\nimplementation of <code>s_mul</code> is correct:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">fdm</span> <span class=\"kn\">import</span> <span class=\"n\">check_sensitivity</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">check_sensitivity</span><span class=\"p\">(</span><span class=\"n\">mul</span><span class=\"p\">,</span> <span class=\"n\">s_mul</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">))</span>  <span class=\"c1\"># Test at arguments `2` and `3`.</span>\n</pre>\n<p>Suppose that the implementation were wrong, for example</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">s_mul_wrong</span><span class=\"p\">(</span><span class=\"n\">s_y</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">s_y</span> <span class=\"o\">*</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"o\">*</span> <span class=\"n\">s_y</span>  <span class=\"c1\"># Used `b` instead of `a` for the second sensitivity!</span>\n</pre>\n<p>Then <code>check_sensitivity</code> should throw an <code>AssertionError</code>:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">check_sensitivity</span><span class=\"p\">(</span><span class=\"n\">mul</span><span class=\"p\">,</span> <span class=\"n\">s_mul</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">))</span> \n<span class=\"ne\">AssertionError</span><span class=\"p\">:</span> <span class=\"n\">Sensitivity</span> <span class=\"n\">of</span> <span class=\"n\">argument</span> <span class=\"mi\">2</span> <span class=\"n\">of</span> <span class=\"n\">function</span> <span class=\"s2\">\"mul\"</span> <span class=\"n\">did</span> <span class=\"ow\">not</span> <span class=\"n\">match</span> <span class=\"n\">numerical</span> <span class=\"n\">estimate</span><span class=\"o\">.</span>\n</pre>\n\n          </div>"}, "last_serial": 6083795, "releases": {"0.1.2": [{"comment_text": "", "digests": {"md5": "4381ceddd1e17793ec7a1c5e3cb5df80", "sha256": "3837a7ebb5f55a9a613a30851d1fabb90cca347f196464ce0dd4f7c5f7f24ed6"}, "downloads": -1, "filename": "fdm-0.1.2.tar.gz", "has_sig": false, "md5_digest": "4381ceddd1e17793ec7a1c5e3cb5df80", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10162, "upload_time": "2019-06-12T19:57:22", "upload_time_iso_8601": "2019-06-12T19:57:22.507977Z", "url": "https://files.pythonhosted.org/packages/bd/68/d6beaea69d99431bcd14760d4c35af5cb021bd102e50f2d1aecbbf34f215/fdm-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "e364dfdce61f13665d8c744f794cc651", "sha256": "3d0f10656aca471687b3b4e985576815b726624c2fbe32454e858e9042d2d1da"}, "downloads": -1, "filename": "fdm-0.1.3.tar.gz", "has_sig": false, "md5_digest": "e364dfdce61f13665d8c744f794cc651", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10319, "upload_time": "2019-06-13T17:58:03", "upload_time_iso_8601": "2019-06-13T17:58:03.731218Z", "url": "https://files.pythonhosted.org/packages/94/89/c267b0171ab9c17e3869fd2582909c8f28133ee8dfaff87ae8522167e014/fdm-0.1.3.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "48752ee8ad1eb4b9de16c367d9a61a18", "sha256": "5d72439980fef7b02e59d2929e7c4797f060eddc4c60ddc057712ff4675e31ed"}, "downloads": -1, "filename": "fdm-0.2.0.tar.gz", "has_sig": false, "md5_digest": "48752ee8ad1eb4b9de16c367d9a61a18", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 10326, "upload_time": "2019-11-06T00:02:59", "upload_time_iso_8601": "2019-11-06T00:02:59.854294Z", "url": "https://files.pythonhosted.org/packages/d5/c9/5395602992aa3c8c8717fe7588eb1bc7b581d32da5b54a9dbfe21f34c4b0/fdm-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "48752ee8ad1eb4b9de16c367d9a61a18", "sha256": "5d72439980fef7b02e59d2929e7c4797f060eddc4c60ddc057712ff4675e31ed"}, "downloads": -1, "filename": "fdm-0.2.0.tar.gz", "has_sig": false, "md5_digest": "48752ee8ad1eb4b9de16c367d9a61a18", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 10326, "upload_time": "2019-11-06T00:02:59", "upload_time_iso_8601": "2019-11-06T00:02:59.854294Z", "url": "https://files.pythonhosted.org/packages/d5/c9/5395602992aa3c8c8717fe7588eb1bc7b581d32da5b54a9dbfe21f34c4b0/fdm-0.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:09 2020"}