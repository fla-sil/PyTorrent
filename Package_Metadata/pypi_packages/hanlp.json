{"info": {"author": "hankcs", "author_email": "hankcshe@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 3 :: Only", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Text Processing :: Linguistic"], "description": "# HanLP: Han Language Processing\n\n[\u4e2d\u6587](https://github.com/hankcs/HanLP/tree/doc-zh) | [1.x](https://github.com/hankcs/HanLP/tree/1.x) | [forum](https://bbs.hankcs.com/)\n\nThe multilingual NLP library for researchers and companies, built on TensorFlow 2.0, for advancing state-of-the-art deep learning techniques in both academia and industry. HanLP was designed from day one to be efficient, user friendly and extendable. It comes with pretrained models for various human languages including English, Chinese and many others. Currently, HanLP 2.0 is in alpha stage with more killer features on the roadmap. Discussions are welcomed on our [forum](https://bbs.hankcs.com/), while bug reports and feature requests are reserved for GitHub issues. For Java users, please checkout the [1.x](https://github.com/hankcs/HanLP/tree/1.x) branch.\n\n ## Installation\n\n```bash\npip install hanlp\n```\n\nHanLP requires Python 3.6 or later. GPU/TPU is suggested but not mandatory.\n\n## Quick Start\n\n### Tokenization\n\nFor an end user, the basic workflow starts with loading some pretrained models from disk or Internet. Each model has an identifier, which could be one path on your computer or an URL to any public servers. To tokenize Chinese, let's load a tokenizer called `CTB6_CONVSEG` with 2 lines of code.\n\n```python\n>>> import hanlp\n>>> tokenizer = hanlp.load('CTB6_CONVSEG')\n```\n\nHanLP will automatically resolve the identifier `CTB6_CONVSEG` to an [URL](https://file.hankcs.com/hanlp/cws/ctb6-convseg-cws_20191230_184525.zip), then download it and unzip it. Due to the huge network traffic, it could fail temporally then you need to retry or manually download and unzip it to the path shown in your terminal . \n\nOnce the model is loaded, you can then tokenize one sentence through calling the tokenizer as a function:\n\n```python\n>>> tokenizer('\u5546\u54c1\u548c\u670d\u52a1')\n['\u5546\u54c1', '\u548c', '\u670d\u52a1']\n```\n\nIf you're processing English, a rule based function should be good enough.\n\n```python\n>>> tokenizer = hanlp.utils.rules.tokenize_english\n>>> tokenizer(\"Don't go gentle into that good night.\")\n['Do', \"n't\", 'go', 'gentle', 'into', 'that', 'good', 'night', '.']\n```\n\n#### Going Further\n\nHowever, you can predict much faster. In the era of deep learning, batched computation usually gives a linear scale-up factor of `batch_size`. So, you can predict multiple sentences at once, at the cost of GPU memory.\n\n```python\n>>> tokenizer(['\u8428\u54c8\u592b\u8bf4\uff0c\u4f0a\u62c9\u514b\u5c06\u540c\u8054\u5408\u56fd\u9500\u6bc1\u4f0a\u62c9\u514b\u5927\u89c4\u6a21\u6740\u4f24\u6027\u6b66\u5668\u7279\u522b\u59d4\u5458\u4f1a\u7ee7\u7eed\u4fdd\u6301\u5408\u4f5c\u3002',\n               '\u4e0a\u6d77\u534e\u5b89\u5de5\u4e1a\uff08\u96c6\u56e2\uff09\u516c\u53f8\u8463\u4e8b\u957f\u8c2d\u65ed\u5149\u548c\u79d8\u4e66\u5f20\u665a\u971e\u6765\u5230\u7f8e\u56fd\u7ebd\u7ea6\u73b0\u4ee3\u827a\u672f\u535a\u7269\u9986\u53c2\u89c2\u3002',\n               'HanLP\u652f\u63f4\u81fa\u7063\u6b63\u9ad4\u3001\u9999\u6e2f\u7e41\u9ad4\uff0c\u5177\u6709\u65b0\u8a5e\u8fa8\u8b58\u80fd\u529b\u7684\u4e2d\u6587\u65b7\u8a5e\u7cfb\u7d71'])\n[['\u8428\u54c8\u592b', '\u8bf4', '\uff0c', '\u4f0a\u62c9\u514b', '\u5c06', '\u540c', '\u8054\u5408\u56fd', '\u9500\u6bc1', '\u4f0a\u62c9\u514b', '\u5927', '\u89c4\u6a21', '\u6740\u4f24\u6027', '\u6b66\u5668', '\u7279\u522b', '\u59d4\u5458\u4f1a', '\u7ee7\u7eed', '\u4fdd\u6301', '\u5408\u4f5c', '\u3002'], \n ['\u4e0a\u6d77', '\u534e\u5b89', '\u5de5\u4e1a', '\uff08', '\u96c6\u56e2', '\uff09', '\u516c\u53f8', '\u8463\u4e8b\u957f', '\u8c2d\u65ed\u5149', '\u548c', '\u79d8\u4e66', '\u5f20\u665a\u971e', '\u6765\u5230', '\u7f8e\u56fd', '\u7ebd\u7ea6', '\u73b0\u4ee3', '\u827a\u672f', '\u535a\u7269\u9986', '\u53c2\u89c2', '\u3002'], \n ['HanLP', '\u652f\u63f4', '\u81fa\u7063', '\u6b63\u9ad4', '\u3001', '\u9999\u6e2f', '\u7e41\u9ad4', '\uff0c', '\u5177\u6709', '\u65b0\u8a5e', '\u8fa8\u8b58', '\u80fd\u529b', '\u7684', '\u4e2d\u6587', '\u65b7\u8a5e', '\u7cfb\u7d71']]\n```\n\nThat's it! You're now ready to employ the latest DL models from HanLP in your research and work. Here are some tips if you want to go further.\n\n- Print `hanlp.pretrained.ALL` to list all the pretrained models available in HanLP.\n\n- Use `hanlp.pretrained.*` to browse pretrained models by categories of NLP tasks. You can use the variables to identify them too.\n\n  ```python\n  >>> hanlp.pretrained.cws.CTB6_CONVSEG\n  'https://file.hankcs.com/hanlp/cws/ctb6-convseg-cws_20191230_184525.zip'\n  ```\n\n### Part-of-Speech Tagging\n\nTaggers take lists of tokens as input, then outputs one tag for each token.\n\n```python\n>>> tagger = hanlp.load(hanlp.pretrained.pos.PTB_POS_RNN_FASTTEXT_EN)\n>>> tagger([['I', 'banked', '2', 'dollars', 'in', 'a', 'bank', '.'],\n            ['Is', 'this', 'the', 'future', 'of', 'chamber', 'music', '?']])\n[['PRP', 'VBD', 'CD', 'NNS', 'IN', 'DT', 'NN', '.'], \n ['VBZ', 'DT', 'DT', 'NN', 'IN', 'NN', 'NN', '.']]\n```\n\nThe language solely depends on which model you load.\n\n```python\n>>> tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)\n>>> tagger(['\u6211', '\u7684', '\u5e0c\u671b', '\u662f', '\u5e0c\u671b', '\u548c\u5e73'])\n['PN', 'DEG', 'NN', 'VC', 'VV', 'NN']\n```\n\nDid you notice the different pos tags for the same word `\u5e0c\u671b` (\"hope\")? The first one means \"my dream\" as a noun while the later means \"want\" as a verb. This tagger uses fasttext[^fasttext] as its embedding layer, which is free from OOV.\n\n### Named Entity Recognition\n\nThe NER component requires tokenized tokens as input, then outputs the entities along with their types and spans.\n\n```python\n>>> recognizer = hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN)\n>>> recognizer([\"President\", \"Obama\", \"is\", \"speaking\", \"at\", \"the\", \"White\", \"House\"])\n[('Obama', 'PER', 1, 2), ('White House', 'LOC', 6, 8)]\n```\n\nRecognizers take lists of tokens as input, so don't forget to wrap your sentence with `list`. For the outputs, each tuple stands for `(entity, type, begin, end)`.\n\n```python\n>>> recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)\n>>> recognizer([list('\u4e0a\u6d77\u534e\u5b89\u5de5\u4e1a\uff08\u96c6\u56e2\uff09\u516c\u53f8\u8463\u4e8b\u957f\u8c2d\u65ed\u5149\u548c\u79d8\u4e66\u5f20\u665a\u971e\u6765\u5230\u7f8e\u56fd\u7ebd\u7ea6\u73b0\u4ee3\u827a\u672f\u535a\u7269\u9986\u53c2\u89c2\u3002'),\n                list('\u8428\u54c8\u592b\u8bf4\uff0c\u4f0a\u62c9\u514b\u5c06\u540c\u8054\u5408\u56fd\u9500\u6bc1\u4f0a\u62c9\u514b\u5927\u89c4\u6a21\u6740\u4f24\u6027\u6b66\u5668\u7279\u522b\u59d4\u5458\u4f1a\u7ee7\u7eed\u4fdd\u6301\u5408\u4f5c\u3002')])\n[[('\u4e0a\u6d77\u534e\u5b89\u5de5\u4e1a\uff08\u96c6\u56e2\uff09\u516c\u53f8', 'NT', 0, 12), ('\u8c2d\u65ed\u5149', 'NR', 15, 18), ('\u5f20\u665a\u971e', 'NR', 21, 24), ('\u7f8e\u56fd', 'NS', 26, 28), ('\u7ebd\u7ea6\u73b0\u4ee3\u827a\u672f\u535a\u7269\u9986', 'NS', 28, 37)], \n [('\u8428\u54c8\u592b', 'NR', 0, 3), ('\u4f0a\u62c9\u514b', 'NS', 5, 8), ('\u8054\u5408\u56fd\u9500\u6bc1\u4f0a\u62c9\u514b\u5927\u89c4\u6a21\u6740\u4f24\u6027\u6b66\u5668\u7279\u522b\u59d4\u5458\u4f1a', 'NT', 10, 31)]]\n```\n\nThis `MSRA_NER_BERT_BASE_ZH` is the state-of-the-art NER model based on BERT[^bert]. You can read its evaluation log through:\n\n```bash\n$ cat ~/.hanlp/ner/ner_bert_base_msra_20200104_185735/test.log \n20-01-04 18:55:02 INFO Evaluation results for test.tsv - loss: 1.4949 - f1: 0.9522 - speed: 113.37 sample/sec \nprocessed 177342 tokens with 5268 phrases; found: 5316 phrases; correct: 5039.\naccuracy:  99.37%; precision:  94.79%; recall:  95.65%; FB1:  95.22\n               NR: precision:  96.39%; recall:  97.83%; FB1:  97.10  1357\n               NS: precision:  96.70%; recall:  95.79%; FB1:  96.24  2610\n               NT: precision:  89.47%; recall:  93.13%; FB1:  91.27  1349\n```\n\n### Syntactic Dependency Parsing\n\nParsing lies in the core of NLP. Without parsing, one cannot claim to be a NLP researcher or engineer. But using HanLP, it takes no more than two lines of code.\n\n```python\n>>> syntactic_parser = hanlp.load(hanlp.pretrained.dep.PTB_BIAFFINE_DEP_EN)\n>>> print(syntactic_parser([('Is', 'VBZ'), ('this', 'DT'), ('the', 'DT'), ('future', 'NN'), ('of', 'IN'), ('chamber', 'NN'), ('music', 'NN'), ('?', '.')]))\n1\tIs\t_\tVBZ\t_\t_\t4\tcop\t_\t_\n2\tthis\t_\tDT\t_\t_\t4\tnsubj\t_\t_\n3\tthe\t_\tDT\t_\t_\t4\tdet\t_\t_\n4\tfuture\t_\tNN\t_\t_\t0\troot\t_\t_\n5\tof\t_\tIN\t_\t_\t4\tprep\t_\t_\n6\tchamber\t_\tNN\t_\t_\t7\tnn\t_\t_\n7\tmusic\t_\tNN\t_\t_\t5\tpobj\t_\t_\n8\t?\t_\t.\t_\t_\t4\tpunct\t_\t_\n```\n\nParsers take both tokens and part-of-speech tags as input. The output is a tree in CoNLL-X format[^conllx], which can be manipulated through the `CoNLLSentence` class. Similar codes for Chinese:\n\n```python\n>>> syntactic_parser = hanlp.load(hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH)\n>>> print(syntactic_parser([('\u8721\u70db', 'NN'), ('\u4e24', 'CD'), ('\u5934', 'NN'), ('\u70e7', 'VV')]))\n1\t\u8721\u70db\t_\tNN\t_\t_\t4\tnsubj\t_\t_\n2\t\u4e24\t_\tCD\t_\t_\t3\tnummod\t_\t_\n3\t\u5934\t_\tNN\t_\t_\t4\tdep\t_\t_\n4\t\u70e7\t_\tVV\t_\t_\t0\troot\t_\t_\n```\n\n### Semantic Dependency Parsing\n\nA graph is a generalized tree, which conveys more information about the semantic relations between tokens. \n\n```python\n>>> semantic_parser = hanlp.load(hanlp.pretrained.sdp.SEMEVAL15_PAS_BIAFFINE_EN)\n>>> print(semantic_parser([('Is', 'VBZ'), ('this', 'DT'), ('the', 'DT'), ('future', 'NN'), ('of', 'IN'), ('chamber', 'NN'), ('music', 'NN'), ('?', '.')]))\n1\tIs\t_\tVBZ\t_\t_\t0\tROOT\t_\t_\n2\tthis\t_\tDT\t_\t_\t1\tverb_ARG1\t_\t_\n3\tthe\t_\tDT\t_\t_\t0\tROOT\t_\t_\n4\tfuture\t_\tNN\t_\t_\t1\tverb_ARG2\t_\t_\n4\tfuture\t_\tNN\t_\t_\t3\tdet_ARG1\t_\t_\n4\tfuture\t_\tNN\t_\t_\t5\tprep_ARG1\t_\t_\n5\tof\t_\tIN\t_\t_\t0\tROOT\t_\t_\n6\tchamber\t_\tNN\t_\t_\t0\tROOT\t_\t_\n7\tmusic\t_\tNN\t_\t_\t5\tprep_ARG2\t_\t_\n7\tmusic\t_\tNN\t_\t_\t6\tnoun_ARG1\t_\t_\n8\t?\t_\t.\t_\t_\t0\tROOT\t_\t_\n```\n\nHanLP implements the biaffine[^biaffine] model which delivers the SOTA performance.\n\n```python\n>>> semantic_parser = hanlp.load(hanlp.pretrained.sdp.SEMEVAL16_NEWS_BIAFFINE_ZH)\n>>> print(semantic_parser([('\u8721\u70db', 'NN'), ('\u4e24', 'CD'), ('\u5934', 'NN'), ('\u70e7', 'VV')]))\n1\t\u8721\u70db\t_\tNN\t_\t_\t3\tPoss\t_\t_\n1\t\u8721\u70db\t_\tNN\t_\t_\t4\tPat\t_\t_\n2\t\u4e24\t_\tCD\t_\t_\t3\tQuan\t_\t_\n3\t\u5934\t_\tNN\t_\t_\t4\tLoc\t_\t_\n4\t\u70e7\t_\tVV\t_\t_\t0\tRoot\t_\t_\n```\n\nThe output is a `CoNLLSentence` too. However, it's not a tree but a graph in which one node can have multiple heads, e.g. `\u8721\u70db` has two heads (ID 3 and 4).\n\n### Pipelines\n\nSince parsers require part-of-speech tagging and tokenization, while taggers expects tokenization to be done beforehand, wouldn't it be nice if we have a pipeline to connect the inputs and outputs, like a computation graph?\n\n```python\npipeline = hanlp.pipeline() \\\n    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \\\n    .append(tokenizer, output_key='tokens') \\\n    .append(tagger, output_key='part_of_speech_tags') \\\n    .append(syntactic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='syntactic_dependencies') \\\n    .append(semantic_parser, input_key=('tokens', 'part_of_speech_tags'), output_key='semantic_dependencies')\n```\n\nNotice that the first pipe is an old-school Python function `split_sentence`, which splits the input text into a list of sentences. Then the later DL components can utilize the batch processing seamlessly. This results in a pipeline with one input (text) pipe, multiple flow pipes and one output (parsed document). You can print out the pipeline to check its structure.\n\n```python\n>>> pipeline\n[None->LambdaComponent->sentences, sentences->NgramConvTokenizer->tokens, tokens->RNNPartOfSpeechTagger->part_of_speech_tags, ('tokens', 'part_of_speech_tags')->BiaffineDependencyParser->syntactic_dependencies, ('tokens', 'part_of_speech_tags')->BiaffineSemanticDependencyParser->semantic_dependencies]\n```\n\nThis time, let's feed in a whole document `text`, which might be the scenario in your daily work.\n\n```python\n>>> print(pipeline(text))\n{\n  \"sentences\": [\n    \"Jobs and Wozniak co-founded Apple in 1976 to sell Wozniak's Apple I personal computer.\",\n    \"Together the duo gained fame and wealth a year later with the Apple II.\"\n  ],\n  \"tokens\": [\n    [\"Jobs\", \"and\", \"Wozniak\", \"co-founded\", \"Apple\", \"in\", \"1976\", \"to\", \"sell\", \"Wozniak\", \"'s\", \"\", \"Apple\", \"I\", \"personal\", \"computer\", \".\"],\n    [\"Together\", \"the\", \"duo\", \"gained\", \"fame\", \"and\", \"wealth\", \"a\", \"year\", \"later\", \"with\", \"the\", \"Apple\", \"II\", \".\"]\n  ],\n  \"part_of_speech_tags\": [\n    [\"NNS\", \"CC\", \"NNP\", \"VBD\", \"NNP\", \"IN\", \"CD\", \"TO\", \"VB\", \"NNP\", \"POS\", \"``\", \"NNP\", \"PRP\", \"JJ\", \"NN\", \".\"],\n    [\"IN\", \"DT\", \"NN\", \"VBD\", \"NN\", \"CC\", \"NN\", \"DT\", \"NN\", \"RB\", \"IN\", \"DT\", \"NNP\", \"NNP\", \".\"]\n  ],\n  \"syntactic_dependencies\": [\n    [[4, \"nsubj\"], [1, \"cc\"], [1, \"conj\"], [0, \"root\"], [4, \"dobj\"], [4, \"prep\"], [6, \"pobj\"], [9, \"aux\"], [4, \"xcomp\"], [16, \"poss\"], [10, \"possessive\"], [16, \"punct\"], [16, \"nn\"], [16, \"nn\"], [16, \"amod\"], [9, \"dobj\"], [4, \"punct\"]],\n    [[4, \"advmod\"], [3, \"det\"], [4, \"nsubj\"], [0, \"root\"], [4, \"dobj\"], [5, \"cc\"], [5, \"conj\"], [9, \"det\"], [10, \"npadvmod\"], [4, \"advmod\"], [4, \"prep\"], [14, \"det\"], [14, \"nn\"], [11, \"pobj\"], [4, \"punct\"]]\n  ],\n  \"semantic_dependencies\": [\n    [[[2], [\"coord_ARG1\"]], [[4, 9], [\"verb_ARG1\", \"verb_ARG1\"]], [[2], [\"coord_ARG2\"]], [[6, 8], [\"prep_ARG1\", \"comp_MOD\"]], [[4], [\"verb_ARG2\"]], [[0], [\"ROOT\"]], [[6], [\"prep_ARG2\"]], [[0], [\"ROOT\"]], [[8], [\"comp_ARG1\"]], [[11], [\"poss_ARG2\"]], [[0], [\"ROOT\"]], [[0], [\"ROOT\"]], [[0], [\"ROOT\"]], [[0], [\"ROOT\"]], [[0], [\"ROOT\"]], [[9, 11, 12, 14, 15], [\"verb_ARG3\", \"poss_ARG1\", \"punct_ARG1\", \"noun_ARG1\", \"adj_ARG1\"]], [[0], [\"ROOT\"]]],\n    [[[0], [\"ROOT\"]], [[0], [\"ROOT\"]], [[1, 2, 4], [\"adj_ARG1\", \"det_ARG1\", \"verb_ARG1\"]], [[1, 10], [\"adj_ARG1\", \"adj_ARG1\"]], [[6], [\"coord_ARG1\"]], [[4], [\"verb_ARG2\"]], [[6], [\"coord_ARG2\"]], [[0], [\"ROOT\"]], [[8], [\"det_ARG1\"]], [[9], [\"noun_ARG1\"]], [[0], [\"ROOT\"]], [[0], [\"ROOT\"]], [[0], [\"ROOT\"]], [[11, 12, 13], [\"prep_ARG2\", \"det_ARG1\", \"noun_ARG1\"]], [[0], [\"ROOT\"]]]\n  ]\n}\n```\n\nThe output for Chinese looks similar to the English one.\n\n```python\n>>> print(pipeline(text))\n{\n  \"sentences\": [\n    \"HanLP\u662f\u4e00\u7cfb\u5217\u6a21\u578b\u4e0e\u7b97\u6cd5\u7ec4\u6210\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u5305\uff0c\u76ee\u6807\u662f\u666e\u53ca\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\",\n    \"HanLP\u5177\u5907\u529f\u80fd\u5b8c\u5584\u3001\u6027\u80fd\u9ad8\u6548\u3001\u67b6\u6784\u6e05\u6670\u3001\u8bed\u6599\u65f6\u65b0\u3001\u53ef\u81ea\u5b9a\u4e49\u7684\u7279\u70b9\u3002\",\n    \"\u5185\u90e8\u7b97\u6cd5\u7ecf\u8fc7\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u8003\u9a8c\uff0c\u914d\u5957\u4e66\u7c4d\u300a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5165\u95e8\u300b\u5df2\u7ecf\u51fa\u7248\u3002\"\n  ],\n  \"tokens\": [\n    [\"HanLP\", \"\u662f\", \"\u4e00\", \"\u7cfb\u5217\", \"\u6a21\u578b\", \"\u4e0e\", \"\u7b97\u6cd5\", \"\u7ec4\u6210\", \"\u7684\", \"\u81ea\u7136\", \"\u8bed\u8a00\", \"\u5904\u7406\", \"\u5de5\u5177\u5305\", \"\uff0c\", \"\u76ee\u6807\", \"\u662f\", \"\u666e\u53ca\", \"\u81ea\u7136\", \"\u8bed\u8a00\", \"\u5904\u7406\", \"\u5728\", \"\u751f\u4ea7\", \"\u73af\u5883\", \"\u4e2d\", \"\u7684\", \"\u5e94\u7528\", \"\u3002\"],\n    [\"HanLP\", \"\u5177\u5907\", \"\u529f\u80fd\", \"\u5b8c\u5584\", \"\u3001\", \"\u6027\u80fd\", \"\u9ad8\u6548\", \"\u3001\", \"\u67b6\u6784\", \"\u6e05\u6670\", \"\u3001\", \"\u8bed\u6599\", \"\u65f6\", \"\u65b0\", \"\u3001\", \"\u53ef\", \"\u81ea\", \"\u5b9a\u4e49\", \"\u7684\", \"\u7279\u70b9\", \"\u3002\"],\n    [\"\u5185\u90e8\", \"\u7b97\u6cd5\", \"\u7ecf\u8fc7\", \"\u5de5\u4e1a\u754c\", \"\u548c\", \"\u5b66\u672f\u754c\", \"\u8003\u9a8c\", \"\uff0c\", \"\u914d\u5957\", \"\u4e66\u7c4d\", \"\u300a\", \"\u81ea\u7136\", \"\u8bed\u8a00\", \"\u5904\u7406\", \"\u5165\u95e8\", \"\u300b\", \"\u5df2\u7ecf\", \"\u51fa\u7248\", \"\u3002\"]\n  ],\n  \"part_of_speech_tags\": [\n    [\"NR\", \"VC\", \"CD\", \"M\", \"NN\", \"CC\", \"NN\", \"VV\", \"DEC\", \"NN\", \"NN\", \"VV\", \"NN\", \"PU\", \"NN\", \"VC\", \"VV\", \"NN\", \"NN\", \"VV\", \"P\", \"NN\", \"NN\", \"LC\", \"DEG\", \"NN\", \"PU\"],\n    [\"NR\", \"VV\", \"NN\", \"VA\", \"PU\", \"NN\", \"VA\", \"PU\", \"NN\", \"VA\", \"PU\", \"NN\", \"LC\", \"VA\", \"PU\", \"VV\", \"P\", \"VV\", \"DEC\", \"NN\", \"PU\"],\n    [\"NN\", \"NN\", \"P\", \"NN\", \"CC\", \"NN\", \"NN\", \"PU\", \"VV\", \"NN\", \"PU\", \"NN\", \"NN\", \"NN\", \"NN\", \"PU\", \"AD\", \"VV\", \"PU\"]\n  ],\n  \"syntactic_dependencies\": [\n    [[2, \"top\"], [0, \"root\"], [4, \"nummod\"], [11, \"clf\"], [7, \"conj\"], [7, \"cc\"], [8, \"nsubj\"], [11, \"rcmod\"], [8, \"cpm\"], [11, \"nn\"], [12, \"nsubj\"], [2, \"ccomp\"], [12, \"dobj\"], [2, \"punct\"], [16, \"top\"], [2, \"conj\"], [16, \"ccomp\"], [19, \"nn\"], [20, \"nsubj\"], [17, \"conj\"], [26, \"assmod\"], [23, \"nn\"], [24, \"lobj\"], [21, \"plmod\"], [21, \"assm\"], [20, \"dobj\"], [2, \"punct\"]],\n    [[2, \"nsubj\"], [0, \"root\"], [4, \"nsubj\"], [20, \"rcmod\"], [4, \"punct\"], [7, \"nsubj\"], [4, \"conj\"], [4, \"punct\"], [10, \"nsubj\"], [4, \"conj\"], [4, \"punct\"], [13, \"lobj\"], [14, \"loc\"], [4, \"conj\"], [4, \"punct\"], [18, \"mmod\"], [18, \"advmod\"], [4, \"conj\"], [4, \"cpm\"], [2, \"dobj\"], [2, \"punct\"]],\n    [[2, \"nn\"], [18, \"nsubj\"], [18, \"prep\"], [6, \"conj\"], [6, \"cc\"], [7, \"nn\"], [3, \"pobj\"], [18, \"punct\"], [10, \"rcmod\"], [15, \"nn\"], [15, \"punct\"], [15, \"nn\"], [15, \"nn\"], [15, \"nn\"], [18, \"nsubj\"], [15, \"punct\"], [18, \"advmod\"], [0, \"root\"], [18, \"punct\"]]\n  ],\n  \"semantic_dependencies\": [\n    [[[2], [\"Exp\"]], [[0], [\"Aft\"]], [[4], [\"Quan\"]], [[0], [\"Aft\"]], [[8], [\"Poss\"]], [[7], [\"mConj\"]], [[8], [\"Datv\"]], [[11], [\"rProd\"]], [[8], [\"mAux\"]], [[11], [\"Desc\"]], [[12], [\"Datv\"]], [[2], [\"dClas\"]], [[2, 12], [\"Clas\", \"Cont\"]], [[2, 12], [\"mPunc\", \"mPunc\"]], [[16], [\"Exp\"]], [[17], [\"mMod\"]], [[2], [\"eSucc\"]], [[19], [\"Desc\"]], [[20], [\"Pat\"]], [[26], [\"rProd\"]], [[23], [\"mPrep\"]], [[23], [\"Desc\"]], [[20], [\"Loc\"]], [[23], [\"mRang\"]], [[0], [\"Aft\"]], [[16], [\"Clas\"]], [[16], [\"mPunc\"]]],\n    [[[2], [\"Poss\"]], [[0], [\"Aft\"]], [[4], [\"Exp\"]], [[0], [\"Aft\"]], [[4], [\"mPunc\"]], [[0], [\"Aft\"]], [[4], [\"eCoo\"]], [[4, 7], [\"mPunc\", \"mPunc\"]], [[0], [\"Aft\"]], [[0], [\"Aft\"]], [[7, 10], [\"mPunc\", \"mPunc\"]], [[0], [\"Aft\"]], [[12], [\"mTime\"]], [[0], [\"Aft\"]], [[14], [\"mPunc\"]], [[0], [\"Aft\"]], [[0], [\"Aft\"]], [[20], [\"Desc\"]], [[18], [\"mAux\"]], [[0], [\"Aft\"]], [[0], [\"Aft\"]]],\n    [[[2], [\"Desc\"]], [[7, 9, 18], [\"Exp\", \"Agt\", \"Exp\"]], [[4], [\"mPrep\"]], [[0], [\"Aft\"]], [[6], [\"mPrep\"]], [[7], [\"Datv\"]], [[0], [\"Aft\"]], [[7], [\"mPunc\"]], [[7], [\"eCoo\"]], [[0], [\"Aft\"]], [[0], [\"Aft\"]], [[13], [\"Desc\"]], [[0], [\"Aft\"]], [[0], [\"Aft\"]], [[0], [\"Aft\"]], [[0], [\"Aft\"]], [[18], [\"mTime\"]], [[0], [\"Aft\"]], [[18], [\"mPunc\"]]]\n  ]\n}\n```\n\nThe output is a json `dict`, which most people are familiar with.\n\n- Feel free to add more pre/post-processing to the pipeline, including cleaning, custom dictionary etc.\n- Use `pipeline.save('zh.json')` to save your pipeline and deploy it to your production server.\n\n## Train Your Own Models\n\nTo write DL models is not hard, the real hard thing is to write a model able to reproduce the score in papers. The snippet below shows how to train a 97% F1 cws model on MSR corpus.\n\n```python\ntokenizer = NgramConvTokenizer()\nsave_dir = 'data/model/cws/convseg-msr-nocrf-noembed'\ntokenizer.fit(SIGHAN2005_MSR_TRAIN,\n              SIGHAN2005_MSR_VALID,\n              save_dir,\n              word_embed={'class_name': 'HanLP>Word2VecEmbedding',\n                          'config': {\n                              'trainable': True,\n                              'filepath': CONVSEG_W2V_NEWS_TENSITE_CHAR,\n                              'expand_vocab': False,\n                              'lowercase': False,\n                          }},\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n                                                 epsilon=1e-8, clipnorm=5),\n              epochs=100,\n              window_size=0,\n              metrics='f1',\n              weight_norm=True)\ntokenizer.evaluate(SIGHAN2005_MSR_TEST, save_dir=save_dir)\n```\n\nThe training and evaluation logs are as follows.\n\n```\nTrain for 783 steps, validate for 87 steps\nEpoch 1/100\n783/783 [==============================] - 177s 226ms/step - loss: 15.6354 - f1: 0.8506 - val_loss: 9.9109 - val_f1: 0.9081\nEpoch 2/100\n236/783 [========>.....................] - ETA: 1:41 - loss: 9.0359 - f1: 0.9126\n...\n19-12-28 20:55:59 INFO Trained 100 epochs in 3 h 55 m 42 s, each epoch takes 2 m 21 s\n19-12-28 20:56:06 INFO Evaluation results for msr_test_gold.utf8 - loss: 3.6579 - f1: 0.9715 - speed: 1173.80 sample/sec\n```\n\nSimilarly, you can train a sentiment classifier to classify the comments of hotels.\n\n```python\nsave_dir = 'data/model/classification/chnsenticorp_bert_base'\nclassifier = TransformerClassifier(TransformerTextTransform(y_column=0))\nclassifier.fit(CHNSENTICORP_ERNIE_TRAIN, CHNSENTICORP_ERNIE_VALID, save_dir,\n               transformer='chinese_L-12_H-768_A-12')\nclassifier.load(save_dir)\nprint(classifier('\u524d\u53f0\u5ba2\u623f\u670d\u52a1\u6001\u5ea6\u975e\u5e38\u597d\uff01\u65e9\u9910\u5f88\u4e30\u5bcc\uff0c\u623f\u4ef7\u5f88\u5e72\u51c0\u3002\u518d\u63a5\u518d\u5389\uff01'))\nclassifier.evaluate(CHNSENTICORP_ERNIE_TEST, save_dir=save_dir)\n```\n\nDue to the size of models, and the fact that corpora are domain specific, HanLP has limited plan to distribute pretrained text classification models.\n\nFor more training scripts, please refer to [`tests/train`](https://github.com/hankcs/HanLP/tree/master/tests/train). We are also working hard to release more examples in [`tests/demo`](https://github.com/hankcs/HanLP/tree/master/tests/demo). Serving, documentations and more pretrained models are on the way too.\n\n## Citing\n\nIf you use HanLP in your research, please cite this repository. \n\n```latex\n@software{hanlp2,\n  author = {Han He},\n  title = {{HanLP: Han Language Processing}},\n  year = {2020},\n  url = {https://github.com/hankcs/HanLP},\n}\n```\n\n## License\n\nHanLP is licensed under **Apache License 2.0**. You can use HanLP in your commercial products for free. We would appreciate it if you add a link to HanLP on your website.\n\n## References\n\n[^fasttext]:\tA. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, \u201cBag of Tricks for Efficient Text Classification,\u201d vol. cs.CL. 07-Jul-2016.\n\n[^bert]: J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\u201d arXiv.org, vol. cs.CL. 10-Oct-2018.bert\u00a0\n\n[^biaffine]: T. Dozat and C. D. Manning, \u201cDeep Biaffine Attention for Neural Dependency Parsing.,\u201d ICLR, 2017.\n\n[^conllx]: Buchholz, S., & Marsi, E. (2006, June). CoNLL-X shared task on multilingual dependency parsing. In *Proceedings of the tenth conference on computational natural language learning* (pp. 149-164). Association for Computational Linguistics.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/hankcs/HanLP", "keywords": "corpus,machine-learning,NLU,NLP", "license": "Apache License 2.0", "maintainer": "", "maintainer_email": "", "name": "hanlp", "package_url": "https://pypi.org/project/hanlp/", "platform": "", "project_url": "https://pypi.org/project/hanlp/", "project_urls": {"Homepage": "https://github.com/hankcs/HanLP"}, "release_url": "https://pypi.org/project/hanlp/2.0.0a42/", "requires_dist": null, "requires_python": ">=3.6", "summary": "HanLP: Han Language Processing", "version": "2.0.0a42", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>HanLP: Han Language Processing</h1>\n<p><a href=\"https://github.com/hankcs/HanLP/tree/doc-zh\" rel=\"nofollow\">\u4e2d\u6587</a> | <a href=\"https://github.com/hankcs/HanLP/tree/1.x\" rel=\"nofollow\">1.x</a> | <a href=\"https://bbs.hankcs.com/\" rel=\"nofollow\">forum</a></p>\n<p>The multilingual NLP library for researchers and companies, built on TensorFlow 2.0, for advancing state-of-the-art deep learning techniques in both academia and industry. HanLP was designed from day one to be efficient, user friendly and extendable. It comes with pretrained models for various human languages including English, Chinese and many others. Currently, HanLP 2.0 is in alpha stage with more killer features on the roadmap. Discussions are welcomed on our <a href=\"https://bbs.hankcs.com/\" rel=\"nofollow\">forum</a>, while bug reports and feature requests are reserved for GitHub issues. For Java users, please checkout the <a href=\"https://github.com/hankcs/HanLP/tree/1.x\" rel=\"nofollow\">1.x</a> branch.</p>\n<h2>Installation</h2>\n<pre>pip install hanlp\n</pre>\n<p>HanLP requires Python 3.6 or later. GPU/TPU is suggested but not mandatory.</p>\n<h2>Quick Start</h2>\n<h3>Tokenization</h3>\n<p>For an end user, the basic workflow starts with loading some pretrained models from disk or Internet. Each model has an identifier, which could be one path on your computer or an URL to any public servers. To tokenize Chinese, let's load a tokenizer called <code>CTB6_CONVSEG</code> with 2 lines of code.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">hanlp</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">'CTB6_CONVSEG'</span><span class=\"p\">)</span>\n</pre>\n<p>HanLP will automatically resolve the identifier <code>CTB6_CONVSEG</code> to an <a href=\"https://file.hankcs.com/hanlp/cws/ctb6-convseg-cws_20191230_184525.zip\" rel=\"nofollow\">URL</a>, then download it and unzip it. Due to the huge network traffic, it could fail temporally then you need to retry or manually download and unzip it to the path shown in your terminal .</p>\n<p>Once the model is loaded, you can then tokenize one sentence through calling the tokenizer as a function:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tokenizer</span><span class=\"p\">(</span><span class=\"s1\">'\u5546\u54c1\u548c\u670d\u52a1'</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"s1\">'\u5546\u54c1'</span><span class=\"p\">,</span> <span class=\"s1\">'\u548c'</span><span class=\"p\">,</span> <span class=\"s1\">'\u670d\u52a1'</span><span class=\"p\">]</span>\n</pre>\n<p>If you're processing English, a rule based function should be good enough.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">rules</span><span class=\"o\">.</span><span class=\"n\">tokenize_english</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tokenizer</span><span class=\"p\">(</span><span class=\"s2\">\"Don't go gentle into that good night.\"</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"s1\">'Do'</span><span class=\"p\">,</span> <span class=\"s2\">\"n't\"</span><span class=\"p\">,</span> <span class=\"s1\">'go'</span><span class=\"p\">,</span> <span class=\"s1\">'gentle'</span><span class=\"p\">,</span> <span class=\"s1\">'into'</span><span class=\"p\">,</span> <span class=\"s1\">'that'</span><span class=\"p\">,</span> <span class=\"s1\">'good'</span><span class=\"p\">,</span> <span class=\"s1\">'night'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">]</span>\n</pre>\n<h4>Going Further</h4>\n<p>However, you can predict much faster. In the era of deep learning, batched computation usually gives a linear scale-up factor of <code>batch_size</code>. So, you can predict multiple sentences at once, at the cost of GPU memory.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tokenizer</span><span class=\"p\">([</span><span class=\"s1\">'\u8428\u54c8\u592b\u8bf4\uff0c\u4f0a\u62c9\u514b\u5c06\u540c\u8054\u5408\u56fd\u9500\u6bc1\u4f0a\u62c9\u514b\u5927\u89c4\u6a21\u6740\u4f24\u6027\u6b66\u5668\u7279\u522b\u59d4\u5458\u4f1a\u7ee7\u7eed\u4fdd\u6301\u5408\u4f5c\u3002'</span><span class=\"p\">,</span>\n               <span class=\"s1\">'\u4e0a\u6d77\u534e\u5b89\u5de5\u4e1a\uff08\u96c6\u56e2\uff09\u516c\u53f8\u8463\u4e8b\u957f\u8c2d\u65ed\u5149\u548c\u79d8\u4e66\u5f20\u665a\u971e\u6765\u5230\u7f8e\u56fd\u7ebd\u7ea6\u73b0\u4ee3\u827a\u672f\u535a\u7269\u9986\u53c2\u89c2\u3002'</span><span class=\"p\">,</span>\n               <span class=\"s1\">'HanLP\u652f\u63f4\u81fa\u7063\u6b63\u9ad4\u3001\u9999\u6e2f\u7e41\u9ad4\uff0c\u5177\u6709\u65b0\u8a5e\u8fa8\u8b58\u80fd\u529b\u7684\u4e2d\u6587\u65b7\u8a5e\u7cfb\u7d71'</span><span class=\"p\">])</span>\n<span class=\"p\">[[</span><span class=\"s1\">'\u8428\u54c8\u592b'</span><span class=\"p\">,</span> <span class=\"s1\">'\u8bf4'</span><span class=\"p\">,</span> <span class=\"s1\">'\uff0c'</span><span class=\"p\">,</span> <span class=\"s1\">'\u4f0a\u62c9\u514b'</span><span class=\"p\">,</span> <span class=\"s1\">'\u5c06'</span><span class=\"p\">,</span> <span class=\"s1\">'\u540c'</span><span class=\"p\">,</span> <span class=\"s1\">'\u8054\u5408\u56fd'</span><span class=\"p\">,</span> <span class=\"s1\">'\u9500\u6bc1'</span><span class=\"p\">,</span> <span class=\"s1\">'\u4f0a\u62c9\u514b'</span><span class=\"p\">,</span> <span class=\"s1\">'\u5927'</span><span class=\"p\">,</span> <span class=\"s1\">'\u89c4\u6a21'</span><span class=\"p\">,</span> <span class=\"s1\">'\u6740\u4f24\u6027'</span><span class=\"p\">,</span> <span class=\"s1\">'\u6b66\u5668'</span><span class=\"p\">,</span> <span class=\"s1\">'\u7279\u522b'</span><span class=\"p\">,</span> <span class=\"s1\">'\u59d4\u5458\u4f1a'</span><span class=\"p\">,</span> <span class=\"s1\">'\u7ee7\u7eed'</span><span class=\"p\">,</span> <span class=\"s1\">'\u4fdd\u6301'</span><span class=\"p\">,</span> <span class=\"s1\">'\u5408\u4f5c'</span><span class=\"p\">,</span> <span class=\"s1\">'\u3002'</span><span class=\"p\">],</span> \n <span class=\"p\">[</span><span class=\"s1\">'\u4e0a\u6d77'</span><span class=\"p\">,</span> <span class=\"s1\">'\u534e\u5b89'</span><span class=\"p\">,</span> <span class=\"s1\">'\u5de5\u4e1a'</span><span class=\"p\">,</span> <span class=\"s1\">'\uff08'</span><span class=\"p\">,</span> <span class=\"s1\">'\u96c6\u56e2'</span><span class=\"p\">,</span> <span class=\"s1\">'\uff09'</span><span class=\"p\">,</span> <span class=\"s1\">'\u516c\u53f8'</span><span class=\"p\">,</span> <span class=\"s1\">'\u8463\u4e8b\u957f'</span><span class=\"p\">,</span> <span class=\"s1\">'\u8c2d\u65ed\u5149'</span><span class=\"p\">,</span> <span class=\"s1\">'\u548c'</span><span class=\"p\">,</span> <span class=\"s1\">'\u79d8\u4e66'</span><span class=\"p\">,</span> <span class=\"s1\">'\u5f20\u665a\u971e'</span><span class=\"p\">,</span> <span class=\"s1\">'\u6765\u5230'</span><span class=\"p\">,</span> <span class=\"s1\">'\u7f8e\u56fd'</span><span class=\"p\">,</span> <span class=\"s1\">'\u7ebd\u7ea6'</span><span class=\"p\">,</span> <span class=\"s1\">'\u73b0\u4ee3'</span><span class=\"p\">,</span> <span class=\"s1\">'\u827a\u672f'</span><span class=\"p\">,</span> <span class=\"s1\">'\u535a\u7269\u9986'</span><span class=\"p\">,</span> <span class=\"s1\">'\u53c2\u89c2'</span><span class=\"p\">,</span> <span class=\"s1\">'\u3002'</span><span class=\"p\">],</span> \n <span class=\"p\">[</span><span class=\"s1\">'HanLP'</span><span class=\"p\">,</span> <span class=\"s1\">'\u652f\u63f4'</span><span class=\"p\">,</span> <span class=\"s1\">'\u81fa\u7063'</span><span class=\"p\">,</span> <span class=\"s1\">'\u6b63\u9ad4'</span><span class=\"p\">,</span> <span class=\"s1\">'\u3001'</span><span class=\"p\">,</span> <span class=\"s1\">'\u9999\u6e2f'</span><span class=\"p\">,</span> <span class=\"s1\">'\u7e41\u9ad4'</span><span class=\"p\">,</span> <span class=\"s1\">'\uff0c'</span><span class=\"p\">,</span> <span class=\"s1\">'\u5177\u6709'</span><span class=\"p\">,</span> <span class=\"s1\">'\u65b0\u8a5e'</span><span class=\"p\">,</span> <span class=\"s1\">'\u8fa8\u8b58'</span><span class=\"p\">,</span> <span class=\"s1\">'\u80fd\u529b'</span><span class=\"p\">,</span> <span class=\"s1\">'\u7684'</span><span class=\"p\">,</span> <span class=\"s1\">'\u4e2d\u6587'</span><span class=\"p\">,</span> <span class=\"s1\">'\u65b7\u8a5e'</span><span class=\"p\">,</span> <span class=\"s1\">'\u7cfb\u7d71'</span><span class=\"p\">]]</span>\n</pre>\n<p>That's it! You're now ready to employ the latest DL models from HanLP in your research and work. Here are some tips if you want to go further.</p>\n<ul>\n<li>\n<p>Print <code>hanlp.pretrained.ALL</code> to list all the pretrained models available in HanLP.</p>\n</li>\n<li>\n<p>Use <code>hanlp.pretrained.*</code> to browse pretrained models by categories of NLP tasks. You can use the variables to identify them too.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">pretrained</span><span class=\"o\">.</span><span class=\"n\">cws</span><span class=\"o\">.</span><span class=\"n\">CTB6_CONVSEG</span>\n<span class=\"s1\">'https://file.hankcs.com/hanlp/cws/ctb6-convseg-cws_20191230_184525.zip'</span>\n</pre>\n</li>\n</ul>\n<h3>Part-of-Speech Tagging</h3>\n<p>Taggers take lists of tokens as input, then outputs one tag for each token.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tagger</span> <span class=\"o\">=</span> <span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">pretrained</span><span class=\"o\">.</span><span class=\"n\">pos</span><span class=\"o\">.</span><span class=\"n\">PTB_POS_RNN_FASTTEXT_EN</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tagger</span><span class=\"p\">([[</span><span class=\"s1\">'I'</span><span class=\"p\">,</span> <span class=\"s1\">'banked'</span><span class=\"p\">,</span> <span class=\"s1\">'2'</span><span class=\"p\">,</span> <span class=\"s1\">'dollars'</span><span class=\"p\">,</span> <span class=\"s1\">'in'</span><span class=\"p\">,</span> <span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'bank'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">],</span>\n            <span class=\"p\">[</span><span class=\"s1\">'Is'</span><span class=\"p\">,</span> <span class=\"s1\">'this'</span><span class=\"p\">,</span> <span class=\"s1\">'the'</span><span class=\"p\">,</span> <span class=\"s1\">'future'</span><span class=\"p\">,</span> <span class=\"s1\">'of'</span><span class=\"p\">,</span> <span class=\"s1\">'chamber'</span><span class=\"p\">,</span> <span class=\"s1\">'music'</span><span class=\"p\">,</span> <span class=\"s1\">'?'</span><span class=\"p\">]])</span>\n<span class=\"p\">[[</span><span class=\"s1\">'PRP'</span><span class=\"p\">,</span> <span class=\"s1\">'VBD'</span><span class=\"p\">,</span> <span class=\"s1\">'CD'</span><span class=\"p\">,</span> <span class=\"s1\">'NNS'</span><span class=\"p\">,</span> <span class=\"s1\">'IN'</span><span class=\"p\">,</span> <span class=\"s1\">'DT'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">],</span> \n <span class=\"p\">[</span><span class=\"s1\">'VBZ'</span><span class=\"p\">,</span> <span class=\"s1\">'DT'</span><span class=\"p\">,</span> <span class=\"s1\">'DT'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">,</span> <span class=\"s1\">'IN'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">]]</span>\n</pre>\n<p>The language solely depends on which model you load.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tagger</span> <span class=\"o\">=</span> <span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">pretrained</span><span class=\"o\">.</span><span class=\"n\">pos</span><span class=\"o\">.</span><span class=\"n\">CTB5_POS_RNN_FASTTEXT_ZH</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tagger</span><span class=\"p\">([</span><span class=\"s1\">'\u6211'</span><span class=\"p\">,</span> <span class=\"s1\">'\u7684'</span><span class=\"p\">,</span> <span class=\"s1\">'\u5e0c\u671b'</span><span class=\"p\">,</span> <span class=\"s1\">'\u662f'</span><span class=\"p\">,</span> <span class=\"s1\">'\u5e0c\u671b'</span><span class=\"p\">,</span> <span class=\"s1\">'\u548c\u5e73'</span><span class=\"p\">])</span>\n<span class=\"p\">[</span><span class=\"s1\">'PN'</span><span class=\"p\">,</span> <span class=\"s1\">'DEG'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">,</span> <span class=\"s1\">'VC'</span><span class=\"p\">,</span> <span class=\"s1\">'VV'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">]</span>\n</pre>\n<p>Did you notice the different pos tags for the same word <code>\u5e0c\u671b</code> (\"hope\")? The first one means \"my dream\" as a noun while the later means \"want\" as a verb. This tagger uses fasttext[^fasttext] as its embedding layer, which is free from OOV.</p>\n<h3>Named Entity Recognition</h3>\n<p>The NER component requires tokenized tokens as input, then outputs the entities along with their types and spans.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">recognizer</span> <span class=\"o\">=</span> <span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">pretrained</span><span class=\"o\">.</span><span class=\"n\">ner</span><span class=\"o\">.</span><span class=\"n\">CONLL03_NER_BERT_BASE_UNCASED_EN</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">recognizer</span><span class=\"p\">([</span><span class=\"s2\">\"President\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Obama\"</span><span class=\"p\">,</span> <span class=\"s2\">\"is\"</span><span class=\"p\">,</span> <span class=\"s2\">\"speaking\"</span><span class=\"p\">,</span> <span class=\"s2\">\"at\"</span><span class=\"p\">,</span> <span class=\"s2\">\"the\"</span><span class=\"p\">,</span> <span class=\"s2\">\"White\"</span><span class=\"p\">,</span> <span class=\"s2\">\"House\"</span><span class=\"p\">])</span>\n<span class=\"p\">[(</span><span class=\"s1\">'Obama'</span><span class=\"p\">,</span> <span class=\"s1\">'PER'</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'White House'</span><span class=\"p\">,</span> <span class=\"s1\">'LOC'</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">)]</span>\n</pre>\n<p>Recognizers take lists of tokens as input, so don't forget to wrap your sentence with <code>list</code>. For the outputs, each tuple stands for <code>(entity, type, begin, end)</code>.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">recognizer</span> <span class=\"o\">=</span> <span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">pretrained</span><span class=\"o\">.</span><span class=\"n\">ner</span><span class=\"o\">.</span><span class=\"n\">MSRA_NER_BERT_BASE_ZH</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">recognizer</span><span class=\"p\">([</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"s1\">'\u4e0a\u6d77\u534e\u5b89\u5de5\u4e1a\uff08\u96c6\u56e2\uff09\u516c\u53f8\u8463\u4e8b\u957f\u8c2d\u65ed\u5149\u548c\u79d8\u4e66\u5f20\u665a\u971e\u6765\u5230\u7f8e\u56fd\u7ebd\u7ea6\u73b0\u4ee3\u827a\u672f\u535a\u7269\u9986\u53c2\u89c2\u3002'</span><span class=\"p\">),</span>\n                <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"s1\">'\u8428\u54c8\u592b\u8bf4\uff0c\u4f0a\u62c9\u514b\u5c06\u540c\u8054\u5408\u56fd\u9500\u6bc1\u4f0a\u62c9\u514b\u5927\u89c4\u6a21\u6740\u4f24\u6027\u6b66\u5668\u7279\u522b\u59d4\u5458\u4f1a\u7ee7\u7eed\u4fdd\u6301\u5408\u4f5c\u3002'</span><span class=\"p\">)])</span>\n<span class=\"p\">[[(</span><span class=\"s1\">'\u4e0a\u6d77\u534e\u5b89\u5de5\u4e1a\uff08\u96c6\u56e2\uff09\u516c\u53f8'</span><span class=\"p\">,</span> <span class=\"s1\">'NT'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">12</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'\u8c2d\u65ed\u5149'</span><span class=\"p\">,</span> <span class=\"s1\">'NR'</span><span class=\"p\">,</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"mi\">18</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'\u5f20\u665a\u971e'</span><span class=\"p\">,</span> <span class=\"s1\">'NR'</span><span class=\"p\">,</span> <span class=\"mi\">21</span><span class=\"p\">,</span> <span class=\"mi\">24</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'\u7f8e\u56fd'</span><span class=\"p\">,</span> <span class=\"s1\">'NS'</span><span class=\"p\">,</span> <span class=\"mi\">26</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'\u7ebd\u7ea6\u73b0\u4ee3\u827a\u672f\u535a\u7269\u9986'</span><span class=\"p\">,</span> <span class=\"s1\">'NS'</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">37</span><span class=\"p\">)],</span> \n <span class=\"p\">[(</span><span class=\"s1\">'\u8428\u54c8\u592b'</span><span class=\"p\">,</span> <span class=\"s1\">'NR'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'\u4f0a\u62c9\u514b'</span><span class=\"p\">,</span> <span class=\"s1\">'NS'</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'\u8054\u5408\u56fd\u9500\u6bc1\u4f0a\u62c9\u514b\u5927\u89c4\u6a21\u6740\u4f24\u6027\u6b66\u5668\u7279\u522b\u59d4\u5458\u4f1a'</span><span class=\"p\">,</span> <span class=\"s1\">'NT'</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">31</span><span class=\"p\">)]]</span>\n</pre>\n<p>This <code>MSRA_NER_BERT_BASE_ZH</code> is the state-of-the-art NER model based on BERT[^bert]. You can read its evaluation log through:</p>\n<pre>$ cat ~/.hanlp/ner/ner_bert_base_msra_20200104_185735/test.log \n<span class=\"m\">20</span>-01-04 <span class=\"m\">18</span>:55:02 INFO Evaluation results <span class=\"k\">for</span> test.tsv - loss: <span class=\"m\">1</span>.4949 - f1: <span class=\"m\">0</span>.9522 - speed: <span class=\"m\">113</span>.37 sample/sec \nprocessed <span class=\"m\">177342</span> tokens with <span class=\"m\">5268</span> phrases<span class=\"p\">;</span> found: <span class=\"m\">5316</span> phrases<span class=\"p\">;</span> correct: <span class=\"m\">5039</span>.\naccuracy:  <span class=\"m\">99</span>.37%<span class=\"p\">;</span> precision:  <span class=\"m\">94</span>.79%<span class=\"p\">;</span> recall:  <span class=\"m\">95</span>.65%<span class=\"p\">;</span> FB1:  <span class=\"m\">95</span>.22\n               NR: precision:  <span class=\"m\">96</span>.39%<span class=\"p\">;</span> recall:  <span class=\"m\">97</span>.83%<span class=\"p\">;</span> FB1:  <span class=\"m\">97</span>.10  <span class=\"m\">1357</span>\n               NS: precision:  <span class=\"m\">96</span>.70%<span class=\"p\">;</span> recall:  <span class=\"m\">95</span>.79%<span class=\"p\">;</span> FB1:  <span class=\"m\">96</span>.24  <span class=\"m\">2610</span>\n               NT: precision:  <span class=\"m\">89</span>.47%<span class=\"p\">;</span> recall:  <span class=\"m\">93</span>.13%<span class=\"p\">;</span> FB1:  <span class=\"m\">91</span>.27  <span class=\"m\">1349</span>\n</pre>\n<h3>Syntactic Dependency Parsing</h3>\n<p>Parsing lies in the core of NLP. Without parsing, one cannot claim to be a NLP researcher or engineer. But using HanLP, it takes no more than two lines of code.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">syntactic_parser</span> <span class=\"o\">=</span> <span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">pretrained</span><span class=\"o\">.</span><span class=\"n\">dep</span><span class=\"o\">.</span><span class=\"n\">PTB_BIAFFINE_DEP_EN</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">syntactic_parser</span><span class=\"p\">([(</span><span class=\"s1\">'Is'</span><span class=\"p\">,</span> <span class=\"s1\">'VBZ'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'this'</span><span class=\"p\">,</span> <span class=\"s1\">'DT'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'the'</span><span class=\"p\">,</span> <span class=\"s1\">'DT'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'future'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'of'</span><span class=\"p\">,</span> <span class=\"s1\">'IN'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'chamber'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'music'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'?'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">)]))</span>\n<span class=\"mi\">1</span>\t<span class=\"n\">Is</span>\t<span class=\"n\">_</span>\t<span class=\"n\">VBZ</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">4</span>\t<span class=\"n\">cop</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">2</span>\t<span class=\"n\">this</span>\t<span class=\"n\">_</span>\t<span class=\"n\">DT</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">4</span>\t<span class=\"n\">nsubj</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">3</span>\t<span class=\"n\">the</span>\t<span class=\"n\">_</span>\t<span class=\"n\">DT</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">4</span>\t<span class=\"n\">det</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">4</span>\t<span class=\"n\">future</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">0</span>\t<span class=\"n\">root</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">5</span>\t<span class=\"n\">of</span>\t<span class=\"n\">_</span>\t<span class=\"n\">IN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">4</span>\t<span class=\"n\">prep</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">6</span>\t<span class=\"n\">chamber</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">7</span>\t<span class=\"n\">nn</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">7</span>\t<span class=\"n\">music</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">5</span>\t<span class=\"n\">pobj</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">8</span>\t<span class=\"err\">?</span>\t<span class=\"n\">_</span>\t<span class=\"o\">.</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">4</span>\t<span class=\"n\">punct</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n</pre>\n<p>Parsers take both tokens and part-of-speech tags as input. The output is a tree in CoNLL-X format[^conllx], which can be manipulated through the <code>CoNLLSentence</code> class. Similar codes for Chinese:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">syntactic_parser</span> <span class=\"o\">=</span> <span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">pretrained</span><span class=\"o\">.</span><span class=\"n\">dep</span><span class=\"o\">.</span><span class=\"n\">CTB7_BIAFFINE_DEP_ZH</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">syntactic_parser</span><span class=\"p\">([(</span><span class=\"s1\">'\u8721\u70db'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'\u4e24'</span><span class=\"p\">,</span> <span class=\"s1\">'CD'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'\u5934'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'\u70e7'</span><span class=\"p\">,</span> <span class=\"s1\">'VV'</span><span class=\"p\">)]))</span>\n<span class=\"mi\">1</span>\t<span class=\"n\">\u8721\u70db</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">4</span>\t<span class=\"n\">nsubj</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">2</span>\t<span class=\"n\">\u4e24</span>\t<span class=\"n\">_</span>\t<span class=\"n\">CD</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">3</span>\t<span class=\"n\">nummod</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">3</span>\t<span class=\"n\">\u5934</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">4</span>\t<span class=\"n\">dep</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">4</span>\t<span class=\"n\">\u70e7</span>\t<span class=\"n\">_</span>\t<span class=\"n\">VV</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">0</span>\t<span class=\"n\">root</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n</pre>\n<h3>Semantic Dependency Parsing</h3>\n<p>A graph is a generalized tree, which conveys more information about the semantic relations between tokens.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">semantic_parser</span> <span class=\"o\">=</span> <span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">pretrained</span><span class=\"o\">.</span><span class=\"n\">sdp</span><span class=\"o\">.</span><span class=\"n\">SEMEVAL15_PAS_BIAFFINE_EN</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">semantic_parser</span><span class=\"p\">([(</span><span class=\"s1\">'Is'</span><span class=\"p\">,</span> <span class=\"s1\">'VBZ'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'this'</span><span class=\"p\">,</span> <span class=\"s1\">'DT'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'the'</span><span class=\"p\">,</span> <span class=\"s1\">'DT'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'future'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'of'</span><span class=\"p\">,</span> <span class=\"s1\">'IN'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'chamber'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'music'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'?'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">)]))</span>\n<span class=\"mi\">1</span>\t<span class=\"n\">Is</span>\t<span class=\"n\">_</span>\t<span class=\"n\">VBZ</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">0</span>\t<span class=\"n\">ROOT</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">2</span>\t<span class=\"n\">this</span>\t<span class=\"n\">_</span>\t<span class=\"n\">DT</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">1</span>\t<span class=\"n\">verb_ARG1</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">3</span>\t<span class=\"n\">the</span>\t<span class=\"n\">_</span>\t<span class=\"n\">DT</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">0</span>\t<span class=\"n\">ROOT</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">4</span>\t<span class=\"n\">future</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">1</span>\t<span class=\"n\">verb_ARG2</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">4</span>\t<span class=\"n\">future</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">3</span>\t<span class=\"n\">det_ARG1</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">4</span>\t<span class=\"n\">future</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">5</span>\t<span class=\"n\">prep_ARG1</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">5</span>\t<span class=\"n\">of</span>\t<span class=\"n\">_</span>\t<span class=\"n\">IN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">0</span>\t<span class=\"n\">ROOT</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">6</span>\t<span class=\"n\">chamber</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">0</span>\t<span class=\"n\">ROOT</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">7</span>\t<span class=\"n\">music</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">5</span>\t<span class=\"n\">prep_ARG2</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">7</span>\t<span class=\"n\">music</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">6</span>\t<span class=\"n\">noun_ARG1</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">8</span>\t<span class=\"err\">?</span>\t<span class=\"n\">_</span>\t<span class=\"o\">.</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">0</span>\t<span class=\"n\">ROOT</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n</pre>\n<p>HanLP implements the biaffine[^biaffine] model which delivers the SOTA performance.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">semantic_parser</span> <span class=\"o\">=</span> <span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">pretrained</span><span class=\"o\">.</span><span class=\"n\">sdp</span><span class=\"o\">.</span><span class=\"n\">SEMEVAL16_NEWS_BIAFFINE_ZH</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">semantic_parser</span><span class=\"p\">([(</span><span class=\"s1\">'\u8721\u70db'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'\u4e24'</span><span class=\"p\">,</span> <span class=\"s1\">'CD'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'\u5934'</span><span class=\"p\">,</span> <span class=\"s1\">'NN'</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'\u70e7'</span><span class=\"p\">,</span> <span class=\"s1\">'VV'</span><span class=\"p\">)]))</span>\n<span class=\"mi\">1</span>\t<span class=\"n\">\u8721\u70db</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">3</span>\t<span class=\"n\">Poss</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">1</span>\t<span class=\"n\">\u8721\u70db</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">4</span>\t<span class=\"n\">Pat</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">2</span>\t<span class=\"n\">\u4e24</span>\t<span class=\"n\">_</span>\t<span class=\"n\">CD</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">3</span>\t<span class=\"n\">Quan</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">3</span>\t<span class=\"n\">\u5934</span>\t<span class=\"n\">_</span>\t<span class=\"n\">NN</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">4</span>\t<span class=\"n\">Loc</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n<span class=\"mi\">4</span>\t<span class=\"n\">\u70e7</span>\t<span class=\"n\">_</span>\t<span class=\"n\">VV</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\t<span class=\"mi\">0</span>\t<span class=\"n\">Root</span>\t<span class=\"n\">_</span>\t<span class=\"n\">_</span>\n</pre>\n<p>The output is a <code>CoNLLSentence</code> too. However, it's not a tree but a graph in which one node can have multiple heads, e.g. <code>\u8721\u70db</code> has two heads (ID 3 and 4).</p>\n<h3>Pipelines</h3>\n<p>Since parsers require part-of-speech tagging and tokenization, while taggers expects tokenization to be done beforehand, wouldn't it be nice if we have a pipeline to connect the inputs and outputs, like a computation graph?</p>\n<pre><span class=\"n\">pipeline</span> <span class=\"o\">=</span> <span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">pipeline</span><span class=\"p\">()</span> \\\n    <span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">hanlp</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">rules</span><span class=\"o\">.</span><span class=\"n\">split_sentence</span><span class=\"p\">,</span> <span class=\"n\">output_key</span><span class=\"o\">=</span><span class=\"s1\">'sentences'</span><span class=\"p\">)</span> \\\n    <span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"n\">output_key</span><span class=\"o\">=</span><span class=\"s1\">'tokens'</span><span class=\"p\">)</span> \\\n    <span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">tagger</span><span class=\"p\">,</span> <span class=\"n\">output_key</span><span class=\"o\">=</span><span class=\"s1\">'part_of_speech_tags'</span><span class=\"p\">)</span> \\\n    <span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">syntactic_parser</span><span class=\"p\">,</span> <span class=\"n\">input_key</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"s1\">'tokens'</span><span class=\"p\">,</span> <span class=\"s1\">'part_of_speech_tags'</span><span class=\"p\">),</span> <span class=\"n\">output_key</span><span class=\"o\">=</span><span class=\"s1\">'syntactic_dependencies'</span><span class=\"p\">)</span> \\\n    <span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">semantic_parser</span><span class=\"p\">,</span> <span class=\"n\">input_key</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"s1\">'tokens'</span><span class=\"p\">,</span> <span class=\"s1\">'part_of_speech_tags'</span><span class=\"p\">),</span> <span class=\"n\">output_key</span><span class=\"o\">=</span><span class=\"s1\">'semantic_dependencies'</span><span class=\"p\">)</span>\n</pre>\n<p>Notice that the first pipe is an old-school Python function <code>split_sentence</code>, which splits the input text into a list of sentences. Then the later DL components can utilize the batch processing seamlessly. This results in a pipeline with one input (text) pipe, multiple flow pipes and one output (parsed document). You can print out the pipeline to check its structure.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">pipeline</span>\n<span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"o\">-&gt;</span><span class=\"n\">LambdaComponent</span><span class=\"o\">-&gt;</span><span class=\"n\">sentences</span><span class=\"p\">,</span> <span class=\"n\">sentences</span><span class=\"o\">-&gt;</span><span class=\"n\">NgramConvTokenizer</span><span class=\"o\">-&gt;</span><span class=\"n\">tokens</span><span class=\"p\">,</span> <span class=\"n\">tokens</span><span class=\"o\">-&gt;</span><span class=\"n\">RNNPartOfSpeechTagger</span><span class=\"o\">-&gt;</span><span class=\"n\">part_of_speech_tags</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s1\">'tokens'</span><span class=\"p\">,</span> <span class=\"s1\">'part_of_speech_tags'</span><span class=\"p\">)</span><span class=\"o\">-&gt;</span><span class=\"n\">BiaffineDependencyParser</span><span class=\"o\">-&gt;</span><span class=\"n\">syntactic_dependencies</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s1\">'tokens'</span><span class=\"p\">,</span> <span class=\"s1\">'part_of_speech_tags'</span><span class=\"p\">)</span><span class=\"o\">-&gt;</span><span class=\"n\">BiaffineSemanticDependencyParser</span><span class=\"o\">-&gt;</span><span class=\"n\">semantic_dependencies</span><span class=\"p\">]</span>\n</pre>\n<p>This time, let's feed in a whole document <code>text</code>, which might be the scenario in your daily work.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">pipeline</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">))</span>\n<span class=\"p\">{</span>\n  <span class=\"s2\">\"sentences\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"s2\">\"Jobs and Wozniak co-founded Apple in 1976 to sell Wozniak's Apple I personal computer.\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"Together the duo gained fame and wealth a year later with the Apple II.\"</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">\"tokens\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"Jobs\"</span><span class=\"p\">,</span> <span class=\"s2\">\"and\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Wozniak\"</span><span class=\"p\">,</span> <span class=\"s2\">\"co-founded\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Apple\"</span><span class=\"p\">,</span> <span class=\"s2\">\"in\"</span><span class=\"p\">,</span> <span class=\"s2\">\"1976\"</span><span class=\"p\">,</span> <span class=\"s2\">\"to\"</span><span class=\"p\">,</span> <span class=\"s2\">\"sell\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Wozniak\"</span><span class=\"p\">,</span> <span class=\"s2\">\"'s\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Apple\"</span><span class=\"p\">,</span> <span class=\"s2\">\"I\"</span><span class=\"p\">,</span> <span class=\"s2\">\"personal\"</span><span class=\"p\">,</span> <span class=\"s2\">\"computer\"</span><span class=\"p\">,</span> <span class=\"s2\">\".\"</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"Together\"</span><span class=\"p\">,</span> <span class=\"s2\">\"the\"</span><span class=\"p\">,</span> <span class=\"s2\">\"duo\"</span><span class=\"p\">,</span> <span class=\"s2\">\"gained\"</span><span class=\"p\">,</span> <span class=\"s2\">\"fame\"</span><span class=\"p\">,</span> <span class=\"s2\">\"and\"</span><span class=\"p\">,</span> <span class=\"s2\">\"wealth\"</span><span class=\"p\">,</span> <span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"year\"</span><span class=\"p\">,</span> <span class=\"s2\">\"later\"</span><span class=\"p\">,</span> <span class=\"s2\">\"with\"</span><span class=\"p\">,</span> <span class=\"s2\">\"the\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Apple\"</span><span class=\"p\">,</span> <span class=\"s2\">\"II\"</span><span class=\"p\">,</span> <span class=\"s2\">\".\"</span><span class=\"p\">]</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">\"part_of_speech_tags\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"NNS\"</span><span class=\"p\">,</span> <span class=\"s2\">\"CC\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NNP\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VBD\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NNP\"</span><span class=\"p\">,</span> <span class=\"s2\">\"IN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"CD\"</span><span class=\"p\">,</span> <span class=\"s2\">\"TO\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VB\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NNP\"</span><span class=\"p\">,</span> <span class=\"s2\">\"POS\"</span><span class=\"p\">,</span> <span class=\"s2\">\"``\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NNP\"</span><span class=\"p\">,</span> <span class=\"s2\">\"PRP\"</span><span class=\"p\">,</span> <span class=\"s2\">\"JJ\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\".\"</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"IN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"DT\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VBD\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"CC\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"DT\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"RB\"</span><span class=\"p\">,</span> <span class=\"s2\">\"IN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"DT\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NNP\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NNP\"</span><span class=\"p\">,</span> <span class=\"s2\">\".\"</span><span class=\"p\">]</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">\"syntactic_dependencies\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">[[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"nsubj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s2\">\"cc\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s2\">\"conj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s2\">\"root\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"dobj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"prep\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"s2\">\"pobj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"s2\">\"aux\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"xcomp\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"s2\">\"poss\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"s2\">\"possessive\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"s2\">\"nn\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"s2\">\"nn\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"s2\">\"amod\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"s2\">\"dobj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">]],</span>\n    <span class=\"p\">[[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"advmod\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s2\">\"det\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"nsubj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s2\">\"root\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"dobj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"s2\">\"cc\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"s2\">\"conj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"s2\">\"det\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"s2\">\"npadvmod\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"advmod\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"prep\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">14</span><span class=\"p\">,</span> <span class=\"s2\">\"det\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">14</span><span class=\"p\">,</span> <span class=\"s2\">\"nn\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"s2\">\"pobj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">]]</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">\"semantic_dependencies\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">[[[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"coord_ARG1\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"verb_ARG1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"verb_ARG1\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"coord_ARG2\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"prep_ARG1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"comp_MOD\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"verb_ARG2\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">6</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"prep_ARG2\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">8</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"comp_ARG1\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">11</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"poss_ARG2\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">14</span><span class=\"p\">,</span> <span class=\"mi\">15</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"verb_ARG3\"</span><span class=\"p\">,</span> <span class=\"s2\">\"poss_ARG1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"punct_ARG1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"noun_ARG1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"adj_ARG1\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]]],</span>\n    <span class=\"p\">[[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"adj_ARG1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"det_ARG1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"verb_ARG1\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"adj_ARG1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"adj_ARG1\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">6</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"coord_ARG1\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"verb_ARG2\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">6</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"coord_ARG2\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">8</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"det_ARG1\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">9</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"noun_ARG1\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">13</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"prep_ARG2\"</span><span class=\"p\">,</span> <span class=\"s2\">\"det_ARG1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"noun_ARG1\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"ROOT\"</span><span class=\"p\">]]]</span>\n  <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</pre>\n<p>The output for Chinese looks similar to the English one.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">pipeline</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">))</span>\n<span class=\"p\">{</span>\n  <span class=\"s2\">\"sentences\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"s2\">\"HanLP\u662f\u4e00\u7cfb\u5217\u6a21\u578b\u4e0e\u7b97\u6cd5\u7ec4\u6210\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u5305\uff0c\u76ee\u6807\u662f\u666e\u53ca\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"HanLP\u5177\u5907\u529f\u80fd\u5b8c\u5584\u3001\u6027\u80fd\u9ad8\u6548\u3001\u67b6\u6784\u6e05\u6670\u3001\u8bed\u6599\u65f6\u65b0\u3001\u53ef\u81ea\u5b9a\u4e49\u7684\u7279\u70b9\u3002\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"\u5185\u90e8\u7b97\u6cd5\u7ecf\u8fc7\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u8003\u9a8c\uff0c\u914d\u5957\u4e66\u7c4d\u300a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5165\u95e8\u300b\u5df2\u7ecf\u51fa\u7248\u3002\"</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">\"tokens\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"HanLP\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u662f\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u4e00\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u7cfb\u5217\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u6a21\u578b\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u4e0e\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u7b97\u6cd5\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u7ec4\u6210\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u7684\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u81ea\u7136\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u8bed\u8a00\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u5904\u7406\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u5de5\u5177\u5305\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\uff0c\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u76ee\u6807\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u662f\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u666e\u53ca\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u81ea\u7136\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u8bed\u8a00\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u5904\u7406\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u5728\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u751f\u4ea7\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u73af\u5883\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u4e2d\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u7684\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u5e94\u7528\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u3002\"</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"HanLP\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u5177\u5907\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u529f\u80fd\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u5b8c\u5584\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u3001\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u6027\u80fd\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u9ad8\u6548\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u3001\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u67b6\u6784\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u6e05\u6670\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u3001\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u8bed\u6599\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u65f6\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u65b0\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u3001\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u53ef\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u81ea\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u5b9a\u4e49\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u7684\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u7279\u70b9\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u3002\"</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"\u5185\u90e8\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u7b97\u6cd5\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u7ecf\u8fc7\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u5de5\u4e1a\u754c\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u548c\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u5b66\u672f\u754c\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u8003\u9a8c\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\uff0c\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u914d\u5957\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u4e66\u7c4d\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u300a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u81ea\u7136\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u8bed\u8a00\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u5904\u7406\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u5165\u95e8\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u300b\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u5df2\u7ecf\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u51fa\u7248\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\u3002\"</span><span class=\"p\">]</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">\"part_of_speech_tags\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"NR\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VC\"</span><span class=\"p\">,</span> <span class=\"s2\">\"CD\"</span><span class=\"p\">,</span> <span class=\"s2\">\"M\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"CC\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VV\"</span><span class=\"p\">,</span> <span class=\"s2\">\"DEC\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VV\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"PU\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VC\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VV\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VV\"</span><span class=\"p\">,</span> <span class=\"s2\">\"P\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"LC\"</span><span class=\"p\">,</span> <span class=\"s2\">\"DEG\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"PU\"</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"NR\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VV\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VA\"</span><span class=\"p\">,</span> <span class=\"s2\">\"PU\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VA\"</span><span class=\"p\">,</span> <span class=\"s2\">\"PU\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VA\"</span><span class=\"p\">,</span> <span class=\"s2\">\"PU\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"LC\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VA\"</span><span class=\"p\">,</span> <span class=\"s2\">\"PU\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VV\"</span><span class=\"p\">,</span> <span class=\"s2\">\"P\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VV\"</span><span class=\"p\">,</span> <span class=\"s2\">\"DEC\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"PU\"</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"P\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"CC\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"PU\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VV\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"PU\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NN\"</span><span class=\"p\">,</span> <span class=\"s2\">\"PU\"</span><span class=\"p\">,</span> <span class=\"s2\">\"AD\"</span><span class=\"p\">,</span> <span class=\"s2\">\"VV\"</span><span class=\"p\">,</span> <span class=\"s2\">\"PU\"</span><span class=\"p\">]</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">\"syntactic_dependencies\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">[[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s2\">\"top\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s2\">\"root\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"nummod\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"s2\">\"clf\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"s2\">\"conj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"s2\">\"cc\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"s2\">\"nsubj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"s2\">\"rcmod\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"s2\">\"cpm\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"s2\">\"nn\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"s2\">\"nsubj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s2\">\"ccomp\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"s2\">\"dobj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"s2\">\"top\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s2\">\"conj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"s2\">\"ccomp\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">19</span><span class=\"p\">,</span> <span class=\"s2\">\"nn\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"s2\">\"nsubj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">17</span><span class=\"p\">,</span> <span class=\"s2\">\"conj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">26</span><span class=\"p\">,</span> <span class=\"s2\">\"assmod\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">23</span><span class=\"p\">,</span> <span class=\"s2\">\"nn\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">24</span><span class=\"p\">,</span> <span class=\"s2\">\"lobj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">21</span><span class=\"p\">,</span> <span class=\"s2\">\"plmod\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">21</span><span class=\"p\">,</span> <span class=\"s2\">\"assm\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"s2\">\"dobj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">]],</span>\n    <span class=\"p\">[[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s2\">\"nsubj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s2\">\"root\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"nsubj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"s2\">\"rcmod\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"s2\">\"nsubj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"conj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"s2\">\"nsubj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"conj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">13</span><span class=\"p\">,</span> <span class=\"s2\">\"lobj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">14</span><span class=\"p\">,</span> <span class=\"s2\">\"loc\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"conj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">18</span><span class=\"p\">,</span> <span class=\"s2\">\"mmod\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">18</span><span class=\"p\">,</span> <span class=\"s2\">\"advmod\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"conj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s2\">\"cpm\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s2\">\"dobj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">]],</span>\n    <span class=\"p\">[[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s2\">\"nn\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">18</span><span class=\"p\">,</span> <span class=\"s2\">\"nsubj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">18</span><span class=\"p\">,</span> <span class=\"s2\">\"prep\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"s2\">\"conj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"s2\">\"cc\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"s2\">\"nn\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s2\">\"pobj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">18</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"s2\">\"rcmod\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">\"nn\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">\"nn\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">\"nn\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">\"nn\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">18</span><span class=\"p\">,</span> <span class=\"s2\">\"nsubj\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">18</span><span class=\"p\">,</span> <span class=\"s2\">\"advmod\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s2\">\"root\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">18</span><span class=\"p\">,</span> <span class=\"s2\">\"punct\"</span><span class=\"p\">]]</span>\n  <span class=\"p\">],</span>\n  <span class=\"s2\">\"semantic_dependencies\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n    <span class=\"p\">[[[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Exp\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Quan\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">8</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Poss\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">7</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mConj\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">8</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Datv\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">11</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"rProd\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">8</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mAux\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">11</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Desc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">12</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Datv\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"dClas\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">12</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Clas\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Cont\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">12</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mPunc\"</span><span class=\"p\">,</span> <span class=\"s2\">\"mPunc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">16</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Exp\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">17</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mMod\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"eSucc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">19</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Desc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">20</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Pat\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">26</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"rProd\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">23</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mPrep\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">23</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Desc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">20</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Loc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">23</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mRang\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">16</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Clas\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">16</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mPunc\"</span><span class=\"p\">]]],</span>\n    <span class=\"p\">[[[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Poss\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Exp\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mPunc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"eCoo\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mPunc\"</span><span class=\"p\">,</span> <span class=\"s2\">\"mPunc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mPunc\"</span><span class=\"p\">,</span> <span class=\"s2\">\"mPunc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">12</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mTime\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">14</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mPunc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">20</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Desc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">18</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mAux\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]]],</span>\n    <span class=\"p\">[[[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Desc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"mi\">18</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Exp\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Agt\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Exp\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mPrep\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">6</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mPrep\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">7</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Datv\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">7</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mPunc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">7</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"eCoo\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">13</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Desc\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">18</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mTime\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Aft\"</span><span class=\"p\">]],</span> <span class=\"p\">[[</span><span class=\"mi\">18</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"mPunc\"</span><span class=\"p\">]]]</span>\n  <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</pre>\n<p>The output is a json <code>dict</code>, which most people are familiar with.</p>\n<ul>\n<li>Feel free to add more pre/post-processing to the pipeline, including cleaning, custom dictionary etc.</li>\n<li>Use <code>pipeline.save('zh.json')</code> to save your pipeline and deploy it to your production server.</li>\n</ul>\n<h2>Train Your Own Models</h2>\n<p>To write DL models is not hard, the real hard thing is to write a model able to reproduce the score in papers. The snippet below shows how to train a 97% F1 cws model on MSR corpus.</p>\n<pre><span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">NgramConvTokenizer</span><span class=\"p\">()</span>\n<span class=\"n\">save_dir</span> <span class=\"o\">=</span> <span class=\"s1\">'data/model/cws/convseg-msr-nocrf-noembed'</span>\n<span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">SIGHAN2005_MSR_TRAIN</span><span class=\"p\">,</span>\n              <span class=\"n\">SIGHAN2005_MSR_VALID</span><span class=\"p\">,</span>\n              <span class=\"n\">save_dir</span><span class=\"p\">,</span>\n              <span class=\"n\">word_embed</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'class_name'</span><span class=\"p\">:</span> <span class=\"s1\">'HanLP&gt;Word2VecEmbedding'</span><span class=\"p\">,</span>\n                          <span class=\"s1\">'config'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                              <span class=\"s1\">'trainable'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n                              <span class=\"s1\">'filepath'</span><span class=\"p\">:</span> <span class=\"n\">CONVSEG_W2V_NEWS_TENSITE_CHAR</span><span class=\"p\">,</span>\n                              <span class=\"s1\">'expand_vocab'</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                              <span class=\"s1\">'lowercase'</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                          <span class=\"p\">}},</span>\n              <span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">,</span>\n                                                 <span class=\"n\">epsilon</span><span class=\"o\">=</span><span class=\"mf\">1e-8</span><span class=\"p\">,</span> <span class=\"n\">clipnorm</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">),</span>\n              <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n              <span class=\"n\">window_size</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n              <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"s1\">'f1'</span><span class=\"p\">,</span>\n              <span class=\"n\">weight_norm</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">evaluate</span><span class=\"p\">(</span><span class=\"n\">SIGHAN2005_MSR_TEST</span><span class=\"p\">,</span> <span class=\"n\">save_dir</span><span class=\"o\">=</span><span class=\"n\">save_dir</span><span class=\"p\">)</span>\n</pre>\n<p>The training and evaluation logs are as follows.</p>\n<pre><code>Train for 783 steps, validate for 87 steps\nEpoch 1/100\n783/783 [==============================] - 177s 226ms/step - loss: 15.6354 - f1: 0.8506 - val_loss: 9.9109 - val_f1: 0.9081\nEpoch 2/100\n236/783 [========&gt;.....................] - ETA: 1:41 - loss: 9.0359 - f1: 0.9126\n...\n19-12-28 20:55:59 INFO Trained 100 epochs in 3 h 55 m 42 s, each epoch takes 2 m 21 s\n19-12-28 20:56:06 INFO Evaluation results for msr_test_gold.utf8 - loss: 3.6579 - f1: 0.9715 - speed: 1173.80 sample/sec\n</code></pre>\n<p>Similarly, you can train a sentiment classifier to classify the comments of hotels.</p>\n<pre><span class=\"n\">save_dir</span> <span class=\"o\">=</span> <span class=\"s1\">'data/model/classification/chnsenticorp_bert_base'</span>\n<span class=\"n\">classifier</span> <span class=\"o\">=</span> <span class=\"n\">TransformerClassifier</span><span class=\"p\">(</span><span class=\"n\">TransformerTextTransform</span><span class=\"p\">(</span><span class=\"n\">y_column</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n<span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">CHNSENTICORP_ERNIE_TRAIN</span><span class=\"p\">,</span> <span class=\"n\">CHNSENTICORP_ERNIE_VALID</span><span class=\"p\">,</span> <span class=\"n\">save_dir</span><span class=\"p\">,</span>\n               <span class=\"n\">transformer</span><span class=\"o\">=</span><span class=\"s1\">'chinese_L-12_H-768_A-12'</span><span class=\"p\">)</span>\n<span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">save_dir</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">classifier</span><span class=\"p\">(</span><span class=\"s1\">'\u524d\u53f0\u5ba2\u623f\u670d\u52a1\u6001\u5ea6\u975e\u5e38\u597d\uff01\u65e9\u9910\u5f88\u4e30\u5bcc\uff0c\u623f\u4ef7\u5f88\u5e72\u51c0\u3002\u518d\u63a5\u518d\u5389\uff01'</span><span class=\"p\">))</span>\n<span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">evaluate</span><span class=\"p\">(</span><span class=\"n\">CHNSENTICORP_ERNIE_TEST</span><span class=\"p\">,</span> <span class=\"n\">save_dir</span><span class=\"o\">=</span><span class=\"n\">save_dir</span><span class=\"p\">)</span>\n</pre>\n<p>Due to the size of models, and the fact that corpora are domain specific, HanLP has limited plan to distribute pretrained text classification models.</p>\n<p>For more training scripts, please refer to <a href=\"https://github.com/hankcs/HanLP/tree/master/tests/train\" rel=\"nofollow\"><code>tests/train</code></a>. We are also working hard to release more examples in <a href=\"https://github.com/hankcs/HanLP/tree/master/tests/demo\" rel=\"nofollow\"><code>tests/demo</code></a>. Serving, documentations and more pretrained models are on the way too.</p>\n<h2>Citing</h2>\n<p>If you use HanLP in your research, please cite this repository.</p>\n<pre>@software<span class=\"nb\">{</span>hanlp2,\n  author = <span class=\"nb\">{</span>Han He<span class=\"nb\">}</span>,\n  title = <span class=\"nb\">{{</span>HanLP: Han Language Processing<span class=\"nb\">}}</span>,\n  year = <span class=\"nb\">{</span>2020<span class=\"nb\">}</span>,\n  url = <span class=\"nb\">{</span>https://github.com/hankcs/HanLP<span class=\"nb\">}</span>,\n<span class=\"nb\">}</span>\n</pre>\n<h2>License</h2>\n<p>HanLP is licensed under <strong>Apache License 2.0</strong>. You can use HanLP in your commercial products for free. We would appreciate it if you add a link to HanLP on your website.</p>\n<h2>References</h2>\n<p>[^fasttext]:\tA. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, \u201cBag of Tricks for Efficient Text Classification,\u201d vol. cs.CL. 07-Jul-2016.</p>\n<p>[^bert]: J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\u201d arXiv.org, vol. cs.CL. 10-Oct-2018.bert\u00a0</p>\n<p>[^biaffine]: T. Dozat and C. D. Manning, \u201cDeep Biaffine Attention for Neural Dependency Parsing.,\u201d ICLR, 2017.</p>\n<p>[^conllx]: Buchholz, S., &amp; Marsi, E. (2006, June). CoNLL-X shared task on multilingual dependency parsing. In <em>Proceedings of the tenth conference on computational natural language learning</em> (pp. 149-164). Association for Computational Linguistics.</p>\n\n          </div>"}, "last_serial": 6974679, "releases": {"2.0.0a10": [{"comment_text": "", "digests": {"md5": "4aa0aad4f30286c068e360cd6a9a31e2", "sha256": "33966d5e69516235e9630bd990515f725548046e704a646fbb2254504b498148"}, "downloads": -1, "filename": "hanlp-2.0.0a10.tar.gz", "has_sig": false, "md5_digest": "4aa0aad4f30286c068e360cd6a9a31e2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 878515, "upload_time": "2020-01-07T02:01:25", "upload_time_iso_8601": "2020-01-07T02:01:25.851548Z", "url": "https://files.pythonhosted.org/packages/2e/47/75ac45e8c8d54ddb434701e48e88d4f761f5ecb260b7b09147cd1586ba4a/hanlp-2.0.0a10.tar.gz", "yanked": false}], "2.0.0a11": [{"comment_text": "", "digests": {"md5": "5d4443f85e756ff751d1e31165e46474", "sha256": "139ee779f72264be872841f04ef88320ce1fc56397851430bee1aee88428975d"}, "downloads": -1, "filename": "hanlp-2.0.0a11.tar.gz", "has_sig": false, "md5_digest": "5d4443f85e756ff751d1e31165e46474", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 880683, "upload_time": "2020-01-09T05:16:26", "upload_time_iso_8601": "2020-01-09T05:16:26.299431Z", "url": "https://files.pythonhosted.org/packages/d5/83/90101c2015eb4aaa6926afa286bfc3b3b0a5a0c0d5d525fa04201700bb08/hanlp-2.0.0a11.tar.gz", "yanked": false}], "2.0.0a12": [{"comment_text": "", "digests": {"md5": "32e647d6e5474ce9df6efff0c4aea43b", "sha256": "2c05da67a3a3fef05d307984b9ce6b777cf1e7124791331acd60f7eed30d7a06"}, "downloads": -1, "filename": "hanlp-2.0.0a12.tar.gz", "has_sig": false, "md5_digest": "32e647d6e5474ce9df6efff0c4aea43b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 880830, "upload_time": "2020-01-09T05:49:53", "upload_time_iso_8601": "2020-01-09T05:49:53.792359Z", "url": "https://files.pythonhosted.org/packages/d2/ac/76a5de9decb1276faa2783e36d73347fec5d6fd7d6003b32b3e940a27610/hanlp-2.0.0a12.tar.gz", "yanked": false}], "2.0.0a13": [{"comment_text": "", "digests": {"md5": "495fb930ec45808ab5279d2a999e7142", "sha256": "06f3b0e4d8a5e84ff2d7211cbb287dccfb9a33c3524560de6fb3f6ad0572f3bd"}, "downloads": -1, "filename": "hanlp-2.0.0a13.tar.gz", "has_sig": false, "md5_digest": "495fb930ec45808ab5279d2a999e7142", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 880870, "upload_time": "2020-01-09T07:30:46", "upload_time_iso_8601": "2020-01-09T07:30:46.995400Z", "url": "https://files.pythonhosted.org/packages/71/40/f3d9d0fc1b881d21987e3e2bc0cd7f64dbbc9cacc6f6a72930f2922c07e6/hanlp-2.0.0a13.tar.gz", "yanked": false}], "2.0.0a14": [{"comment_text": "", "digests": {"md5": "5fe1b008fe5ca57ce0d4afc03def1c62", "sha256": "232df5eca1d24461305cff17fae668f45aabef591cc393e5da846074ea9af6ea"}, "downloads": -1, "filename": "hanlp-2.0.0a14.tar.gz", "has_sig": false, "md5_digest": "5fe1b008fe5ca57ce0d4afc03def1c62", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 126806, "upload_time": "2020-01-10T05:33:03", "upload_time_iso_8601": "2020-01-10T05:33:03.800013Z", "url": "https://files.pythonhosted.org/packages/87/16/031c5f446ab301a02a8ad61ecb3f150c959177e3c53be32cb68fc2cae80d/hanlp-2.0.0a14.tar.gz", "yanked": false}], "2.0.0a15": [{"comment_text": "", "digests": {"md5": "760e0a1226ce4c3a761290e75cd84cf4", "sha256": "f7efc600db68520cf007e263c637b374352bc93139f5d0d9882c5714e9d8316c"}, "downloads": -1, "filename": "hanlp-2.0.0a15.tar.gz", "has_sig": false, "md5_digest": "760e0a1226ce4c3a761290e75cd84cf4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 126876, "upload_time": "2020-01-10T06:32:35", "upload_time_iso_8601": "2020-01-10T06:32:35.367496Z", "url": "https://files.pythonhosted.org/packages/88/17/6c506b51ef4ad7b4fa92dd4860a5124c6699a5d939d3e19e18ed01eb4894/hanlp-2.0.0a15.tar.gz", "yanked": false}], "2.0.0a16": [{"comment_text": "", "digests": {"md5": "7cbae86129e482ae1fb94503123db44a", "sha256": "26afe0ea08b94363960728e55f0a69c4a5bcf48734f966133b0f6049468f4671"}, "downloads": -1, "filename": "hanlp-2.0.0a16.tar.gz", "has_sig": false, "md5_digest": "7cbae86129e482ae1fb94503123db44a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 126877, "upload_time": "2020-01-10T06:33:05", "upload_time_iso_8601": "2020-01-10T06:33:05.960613Z", "url": "https://files.pythonhosted.org/packages/3f/d1/f77b13e61d49bc8f7da6af5c134e243daa2deedd6feab7378572621a14f5/hanlp-2.0.0a16.tar.gz", "yanked": false}], "2.0.0a17": [{"comment_text": "", "digests": {"md5": "39a6075bc9fbb8461ed1d9b7e7c99bdd", "sha256": "f80b520fb2ee24e6c9df65a84c54184b91eea0989370a460b3d80299007f4faa"}, "downloads": -1, "filename": "hanlp-2.0.0a17.tar.gz", "has_sig": false, "md5_digest": "39a6075bc9fbb8461ed1d9b7e7c99bdd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 126395, "upload_time": "2020-01-10T06:58:27", "upload_time_iso_8601": "2020-01-10T06:58:27.290414Z", "url": "https://files.pythonhosted.org/packages/7d/a9/86d16d3a6542ca25efea0c6304d418669c925b69fc887f198b4b9ddb8e06/hanlp-2.0.0a17.tar.gz", "yanked": false}], "2.0.0a18": [{"comment_text": "", "digests": {"md5": "e235bfa00835d0434bf832cae44b9c8f", "sha256": "0e2d12d9e19a58dbdc5c863fd4b88af43bca9e876dd8f215eb874dbed18f705e"}, "downloads": -1, "filename": "hanlp-2.0.0a18.tar.gz", "has_sig": false, "md5_digest": "e235bfa00835d0434bf832cae44b9c8f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 126305, "upload_time": "2020-01-10T07:11:21", "upload_time_iso_8601": "2020-01-10T07:11:21.956408Z", "url": "https://files.pythonhosted.org/packages/72/25/23bafe932474046f8db4e15aae5de1c642eb7053664db54047a3071d259a/hanlp-2.0.0a18.tar.gz", "yanked": false}], "2.0.0a19": [{"comment_text": "", "digests": {"md5": "e7c270aa338c7cc033ff12d2f5434764", "sha256": "3285e000fcfd4bbbab1c0c5b9ccbe709d0e962b59d5ab0987968b17b1dde3cc1"}, "downloads": -1, "filename": "hanlp-2.0.0a19.tar.gz", "has_sig": false, "md5_digest": "e7c270aa338c7cc033ff12d2f5434764", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 126692, "upload_time": "2020-01-10T20:41:58", "upload_time_iso_8601": "2020-01-10T20:41:58.716179Z", "url": "https://files.pythonhosted.org/packages/b2/3c/1d27bc1d4a4efcd94521f3a6e907acc3a3897b88397b1e525b0e2ecfee62/hanlp-2.0.0a19.tar.gz", "yanked": false}], "2.0.0a20": [{"comment_text": "", "digests": {"md5": "4992fc47c263eb200e95b18d9b237c56", "sha256": "a15fc9dfcc39ff63330f41942caa3535065809541744633afc045cdfb1bbfeaa"}, "downloads": -1, "filename": "hanlp-2.0.0a20.tar.gz", "has_sig": false, "md5_digest": "4992fc47c263eb200e95b18d9b237c56", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 127402, "upload_time": "2020-01-11T05:20:17", "upload_time_iso_8601": "2020-01-11T05:20:17.664470Z", "url": "https://files.pythonhosted.org/packages/fb/f9/0941de7f6c89eef9929fd690fa1d724469f96b4708bb29c5f89968f5c539/hanlp-2.0.0a20.tar.gz", "yanked": false}], "2.0.0a21": [{"comment_text": "", "digests": {"md5": "37d05ceae74c2d820b53ebe748f4b7eb", "sha256": "e3e7e89a79909cb3b9f8bbd7c51109fbc530e396fbbb97c848014a0990fedb9c"}, "downloads": -1, "filename": "hanlp-2.0.0a21.tar.gz", "has_sig": false, "md5_digest": "37d05ceae74c2d820b53ebe748f4b7eb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 127410, "upload_time": "2020-01-11T22:08:07", "upload_time_iso_8601": "2020-01-11T22:08:07.761608Z", "url": "https://files.pythonhosted.org/packages/bc/60/2aeb71bb52ffc4e6d97f7c430d3a2926f1453264ba55a4f05528904a81e6/hanlp-2.0.0a21.tar.gz", "yanked": false}], "2.0.0a22": [{"comment_text": "", "digests": {"md5": "59308608a37b686c1145b6b2dfcf8dca", "sha256": "3a1100fca45fabb49c5775b9dcec124223e22e77a652a1368c6188e604d26269"}, "downloads": -1, "filename": "hanlp-2.0.0a22.tar.gz", "has_sig": false, "md5_digest": "59308608a37b686c1145b6b2dfcf8dca", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 127421, "upload_time": "2020-01-11T22:26:12", "upload_time_iso_8601": "2020-01-11T22:26:12.373546Z", "url": "https://files.pythonhosted.org/packages/38/18/ce4b2096d35aa451c6aa3e5d4e3226a946492f9cac1037543361ec092743/hanlp-2.0.0a22.tar.gz", "yanked": false}], "2.0.0a24": [{"comment_text": "", "digests": {"md5": "a559fc1539776dbec21491ad4b2a2089", "sha256": "1797b2697e56920d76c893776af3df31bf29b90d9ec4710dba08daeed7d92117"}, "downloads": -1, "filename": "hanlp-2.0.0a24.tar.gz", "has_sig": false, "md5_digest": "a559fc1539776dbec21491ad4b2a2089", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 129273, "upload_time": "2020-01-12T12:22:57", "upload_time_iso_8601": "2020-01-12T12:22:57.957056Z", "url": "https://files.pythonhosted.org/packages/5e/c7/877054d3f8c2756c0b4dad7fb4c3196c7153e31616437f301d6da4c28d0e/hanlp-2.0.0a24.tar.gz", "yanked": false}], "2.0.0a25": [{"comment_text": "", "digests": {"md5": "d12203c42abd398371af5f7fa8df45bf", "sha256": "d49f6f45feccfb16512ba6fa4972497ab8998b8b5c082101eb26102dbba4592b"}, "downloads": -1, "filename": "hanlp-2.0.0a25.tar.gz", "has_sig": false, "md5_digest": "d12203c42abd398371af5f7fa8df45bf", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 129329, "upload_time": "2020-01-13T15:57:58", "upload_time_iso_8601": "2020-01-13T15:57:58.942832Z", "url": "https://files.pythonhosted.org/packages/90/8d/bb17e60052945da8d75ff746297f811f08dc8a7e2018bf474d3e44d1620c/hanlp-2.0.0a25.tar.gz", "yanked": false}], "2.0.0a26": [{"comment_text": "", "digests": {"md5": "6cea21728dd9dad1be59aeb6bf78d68f", "sha256": "7faead4ef9d47b729175b4194d9267d20c78bda3a144377fe80ead52b56bab4b"}, "downloads": -1, "filename": "hanlp-2.0.0a26.tar.gz", "has_sig": false, "md5_digest": "6cea21728dd9dad1be59aeb6bf78d68f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 129360, "upload_time": "2020-01-14T05:03:15", "upload_time_iso_8601": "2020-01-14T05:03:15.093435Z", "url": "https://files.pythonhosted.org/packages/be/09/4e96684fb3b4ecbd2f7e4d19c66b6492b43f58e71a835426adc8d8eee07e/hanlp-2.0.0a26.tar.gz", "yanked": false}], "2.0.0a27": [{"comment_text": "", "digests": {"md5": "166a548fc8396d4cace17f07e8af2029", "sha256": "b57d3ada956973e184d9cebbc9e8c3b704f6e024e0cb7a456b8d9e25cdca7f58"}, "downloads": -1, "filename": "hanlp-2.0.0a27.tar.gz", "has_sig": false, "md5_digest": "166a548fc8396d4cace17f07e8af2029", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 129454, "upload_time": "2020-01-21T01:29:10", "upload_time_iso_8601": "2020-01-21T01:29:10.516939Z", "url": "https://files.pythonhosted.org/packages/59/b7/0096e30a5c8894bf25535f1ee2608840cb3267c236ace584204eabc5db2c/hanlp-2.0.0a27.tar.gz", "yanked": false}], "2.0.0a28": [{"comment_text": "", "digests": {"md5": "1ac65f9cb4b25de2deb8304868b6f6af", "sha256": "e73baa291607cf1ae354da2904b7ce6927a03ed2614ef8dc4bdcac89ac6b16e3"}, "downloads": -1, "filename": "hanlp-2.0.0a28.tar.gz", "has_sig": false, "md5_digest": "1ac65f9cb4b25de2deb8304868b6f6af", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 130890, "upload_time": "2020-01-23T02:35:12", "upload_time_iso_8601": "2020-01-23T02:35:12.897171Z", "url": "https://files.pythonhosted.org/packages/78/2f/a40cfab552ddf40b9c6dca2ccc4a8e8acc4daa6978f21cd069153646541a/hanlp-2.0.0a28.tar.gz", "yanked": false}], "2.0.0a29": [{"comment_text": "", "digests": {"md5": "a6986a2886ec787b5de937755b9f437d", "sha256": "3ab02e05f6b8fe8b5b2e3c8772f6e500a5b96d49340dbdba215ad58a9b2e73ee"}, "downloads": -1, "filename": "hanlp-2.0.0a29.tar.gz", "has_sig": false, "md5_digest": "a6986a2886ec787b5de937755b9f437d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 130916, "upload_time": "2020-01-23T19:36:38", "upload_time_iso_8601": "2020-01-23T19:36:38.594822Z", "url": "https://files.pythonhosted.org/packages/51/b1/194b3d4c6186446393aed6f0595f18257d7ff8f8748617d6fc298e77d1cd/hanlp-2.0.0a29.tar.gz", "yanked": false}], "2.0.0a3": [{"comment_text": "", "digests": {"md5": "e709957e17624a6e54eda1e2f2def4bb", "sha256": "5d3122b93e283555ee55d0b6d7fba2727a2b89ad5b76db4ff77213cfc341ed08"}, "downloads": -1, "filename": "hanlp-2.0.0a3.tar.gz", "has_sig": false, "md5_digest": "e709957e17624a6e54eda1e2f2def4bb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 867090, "upload_time": "2020-01-02T21:24:56", "upload_time_iso_8601": "2020-01-02T21:24:56.504198Z", "url": "https://files.pythonhosted.org/packages/8a/9a/d3a4624957ce49d7f3e0219fd30095c661464eda463afe6851c031d6a01b/hanlp-2.0.0a3.tar.gz", "yanked": false}], "2.0.0a30": [{"comment_text": "", "digests": {"md5": "87abfd02bf22de177164120bfd39133c", "sha256": "0ab0127e8805e3f2d4e8dcc50065be98794078ac32ac80da0c6328cbacd139cc"}, "downloads": -1, "filename": "hanlp-2.0.0a30.tar.gz", "has_sig": false, "md5_digest": "87abfd02bf22de177164120bfd39133c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 130928, "upload_time": "2020-01-24T16:02:44", "upload_time_iso_8601": "2020-01-24T16:02:44.798425Z", "url": "https://files.pythonhosted.org/packages/61/da/69cbde2fa3656fe1c283b51487d299662758781edb93c90f4913965a30b6/hanlp-2.0.0a30.tar.gz", "yanked": false}], "2.0.0a31": [{"comment_text": "", "digests": {"md5": "fccd10414fc5fd91a2422fbd333c6c91", "sha256": "43b6e3caf4a0311075608ad0d67aaac7dca6364ffcb87bbc80d62e3710eddd7a"}, "downloads": -1, "filename": "hanlp-2.0.0a31.tar.gz", "has_sig": false, "md5_digest": "fccd10414fc5fd91a2422fbd333c6c91", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 131002, "upload_time": "2020-01-25T17:31:03", "upload_time_iso_8601": "2020-01-25T17:31:03.718031Z", "url": "https://files.pythonhosted.org/packages/38/a2/964032fa25ebf44e79c2958a798b04db0a8ba78bd6df6029312c779c771a/hanlp-2.0.0a31.tar.gz", "yanked": false}], "2.0.0a32": [{"comment_text": "", "digests": {"md5": "ead73acd3235d508b9eab628cc237bdb", "sha256": "3d4eac41594fea5cf4b268139ee35c92628b10c5eddef1ef384f92fb5292d1e1"}, "downloads": -1, "filename": "hanlp-2.0.0a32.tar.gz", "has_sig": false, "md5_digest": "ead73acd3235d508b9eab628cc237bdb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 131015, "upload_time": "2020-01-26T05:48:43", "upload_time_iso_8601": "2020-01-26T05:48:43.315799Z", "url": "https://files.pythonhosted.org/packages/06/ee/1f5539a9a7b351c41e81f0567db691b4c0ddb8ba3b06a72f6a4a334219ec/hanlp-2.0.0a32.tar.gz", "yanked": false}], "2.0.0a33": [{"comment_text": "", "digests": {"md5": "370d555185bcbadf7b7eddf3c23b3ca0", "sha256": "46efe15d12e55f8e2352fe3d971fb7f1487a01aa13c5c60b6082a3f0e04036d8"}, "downloads": -1, "filename": "hanlp-2.0.0a33.tar.gz", "has_sig": false, "md5_digest": "370d555185bcbadf7b7eddf3c23b3ca0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 130914, "upload_time": "2020-01-29T23:05:23", "upload_time_iso_8601": "2020-01-29T23:05:23.416153Z", "url": "https://files.pythonhosted.org/packages/2c/6d/3eeb523d0c9297971e9eae60fa66c4915fa98cb1e07ea3b5845416690f0b/hanlp-2.0.0a33.tar.gz", "yanked": false}], "2.0.0a34": [{"comment_text": "", "digests": {"md5": "aa57f465e640ffb2439d67ad3b1f809c", "sha256": "d66f14a5169ced43521c26615a1b529d37ddcf7183c79db984888f06c72aa652"}, "downloads": -1, "filename": "hanlp-2.0.0a34.tar.gz", "has_sig": false, "md5_digest": "aa57f465e640ffb2439d67ad3b1f809c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 130943, "upload_time": "2020-01-30T15:20:18", "upload_time_iso_8601": "2020-01-30T15:20:18.609193Z", "url": "https://files.pythonhosted.org/packages/36/16/e7803f5e89f5b215cdfe8d335e10b936986dcff8b6b0d9a3c7e2592c9c1c/hanlp-2.0.0a34.tar.gz", "yanked": false}], "2.0.0a35": [{"comment_text": "", "digests": {"md5": "e1f5ccf5f46ad3f63c11a2ac30ec5051", "sha256": "75215c3eb57ea739935e168cbc877f110f14a718418a5b73d54e59a5983b703f"}, "downloads": -1, "filename": "hanlp-2.0.0a35.tar.gz", "has_sig": false, "md5_digest": "e1f5ccf5f46ad3f63c11a2ac30ec5051", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 131374, "upload_time": "2020-02-03T05:48:16", "upload_time_iso_8601": "2020-02-03T05:48:16.626088Z", "url": "https://files.pythonhosted.org/packages/f6/eb/50533ee0c60dc556c57c33d7fae05db601c1ca42b682e20c82cf38f92de1/hanlp-2.0.0a35.tar.gz", "yanked": false}], "2.0.0a36": [{"comment_text": "", "digests": {"md5": "93b7c86ef23ebc9de3f7fac140bf68bb", "sha256": "7a1070b16f9a44cb811c122c5111910d4e3757502014b71dffcdee436b63ff90"}, "downloads": -1, "filename": "hanlp-2.0.0a36.tar.gz", "has_sig": false, "md5_digest": "93b7c86ef23ebc9de3f7fac140bf68bb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 131442, "upload_time": "2020-02-07T15:48:27", "upload_time_iso_8601": "2020-02-07T15:48:27.150659Z", "url": "https://files.pythonhosted.org/packages/0d/76/8e5fc94454664bac5e6abc19a4f0875cd02eaa9705f89592394ba2a6850b/hanlp-2.0.0a36.tar.gz", "yanked": false}], "2.0.0a37": [{"comment_text": "", "digests": {"md5": "0d348f40bbb85099b8c376a1db43982e", "sha256": "b694d952ae1cc947cc8e2b2caf882b3f867325d0ce966842feadb2fef225027b"}, "downloads": -1, "filename": "hanlp-2.0.0a37.tar.gz", "has_sig": false, "md5_digest": "0d348f40bbb85099b8c376a1db43982e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 131431, "upload_time": "2020-02-10T14:11:04", "upload_time_iso_8601": "2020-02-10T14:11:04.644152Z", "url": "https://files.pythonhosted.org/packages/c7/05/db37211f5ed0762d482b41574311e09514d5e4d150f87c0e83a747966820/hanlp-2.0.0a37.tar.gz", "yanked": false}], "2.0.0a38": [{"comment_text": "", "digests": {"md5": "d64e081f3d63d54ad8d08a66cd9143b0", "sha256": "0f1f7f57ef1e309c8db175aaaaea3be5196df31a6ffce838350f9701cef3c1af"}, "downloads": -1, "filename": "hanlp-2.0.0a38.tar.gz", "has_sig": false, "md5_digest": "d64e081f3d63d54ad8d08a66cd9143b0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 131518, "upload_time": "2020-02-11T14:28:06", "upload_time_iso_8601": "2020-02-11T14:28:06.837110Z", "url": "https://files.pythonhosted.org/packages/2f/15/29cadcd85786411033360721873a7849bb401f567d5093ec304f21009f98/hanlp-2.0.0a38.tar.gz", "yanked": false}], "2.0.0a39": [{"comment_text": "", "digests": {"md5": "203ed8948cf815c323162985c345e4aa", "sha256": "729d8c3f22aed1962b1614087bea82ac2358cc2a53cdcf614af28a7a5aaa7d91"}, "downloads": -1, "filename": "hanlp-2.0.0a39.tar.gz", "has_sig": false, "md5_digest": "203ed8948cf815c323162985c345e4aa", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 131685, "upload_time": "2020-03-06T14:58:57", "upload_time_iso_8601": "2020-03-06T14:58:57.833442Z", "url": "https://files.pythonhosted.org/packages/9c/b5/55385c02f62fa026fdfb71e828d47859d253258d08ade0b13417d7ade7df/hanlp-2.0.0a39.tar.gz", "yanked": false}], "2.0.0a4": [{"comment_text": "", "digests": {"md5": "802d4a40c4f127ccd46f3573750df89e", "sha256": "f8d94ab0258976ccfdbf1b74d5e999bffb2c20ff1c933416c99753c62d80051e"}, "downloads": -1, "filename": "hanlp-2.0.0a4.tar.gz", "has_sig": false, "md5_digest": "802d4a40c4f127ccd46f3573750df89e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 872101, "upload_time": "2020-01-03T02:02:42", "upload_time_iso_8601": "2020-01-03T02:02:42.796850Z", "url": "https://files.pythonhosted.org/packages/a2/e6/c91a67096b51fc6a400f3f55ff1e1df69ebe9bcf05eb944164647bb74c08/hanlp-2.0.0a4.tar.gz", "yanked": false}], "2.0.0a40": [{"comment_text": "", "digests": {"md5": "bf9e07728fbe29f54bdabeb831b6e753", "sha256": "83eb1cef3542e8a0ce337722b72a2fab907a37611f191d52c1c068e535ac141c"}, "downloads": -1, "filename": "hanlp-2.0.0a40.tar.gz", "has_sig": false, "md5_digest": "bf9e07728fbe29f54bdabeb831b6e753", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 132618, "upload_time": "2020-04-04T01:00:10", "upload_time_iso_8601": "2020-04-04T01:00:10.160158Z", "url": "https://files.pythonhosted.org/packages/f2/86/493d5e2bf42ad004d4e831e4403caefec0012d403bfbdd8c0231de3b5eb4/hanlp-2.0.0a40.tar.gz", "yanked": false}], "2.0.0a41": [{"comment_text": "", "digests": {"md5": "859714f52835168435c5bfa2da8f4fa9", "sha256": "edfc93fa287d4df6ad911cf95da0bbf43c2f910cbcc94d27cb27b0368397104d"}, "downloads": -1, "filename": "hanlp-2.0.0a41.tar.gz", "has_sig": false, "md5_digest": "859714f52835168435c5bfa2da8f4fa9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 132578, "upload_time": "2020-04-07T03:38:51", "upload_time_iso_8601": "2020-04-07T03:38:51.301331Z", "url": "https://files.pythonhosted.org/packages/7a/2a/10dc5f9a276b58107289a3b9fd4afe5e1d20386a57d50f80c9b912beca47/hanlp-2.0.0a41.tar.gz", "yanked": false}], "2.0.0a42": [{"comment_text": "", "digests": {"md5": "8e78fc8c37f67db4a7fa12117ce7a434", "sha256": "a2de00673a164bd526fc1871ae5f9fb52655317dfa18e9520f029d721965c527"}, "downloads": -1, "filename": "hanlp-2.0.0a42.tar.gz", "has_sig": false, "md5_digest": "8e78fc8c37f67db4a7fa12117ce7a434", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 132623, "upload_time": "2020-04-08T02:44:05", "upload_time_iso_8601": "2020-04-08T02:44:05.137566Z", "url": "https://files.pythonhosted.org/packages/23/30/53abd6f86b5cfa36713a2c1713b70936005389258584c0f07b499168d721/hanlp-2.0.0a42.tar.gz", "yanked": false}], "2.0.0a5": [{"comment_text": "", "digests": {"md5": "2341c88f5cb8605f1c1e6307597eef78", "sha256": "6de0b45bae4ee71127f0cdd28ef6a7fdc6a8d5434c20ea211f44d65a4e53fb34"}, "downloads": -1, "filename": "hanlp-2.0.0a5.tar.gz", "has_sig": false, "md5_digest": "2341c88f5cb8605f1c1e6307597eef78", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 873029, "upload_time": "2020-01-04T04:53:23", "upload_time_iso_8601": "2020-01-04T04:53:23.953490Z", "url": "https://files.pythonhosted.org/packages/61/02/96a8881c1239d15f784007fd20fa5a626503b5e0af2303510d8a7def42af/hanlp-2.0.0a5.tar.gz", "yanked": false}], "2.0.0a6": [{"comment_text": "", "digests": {"md5": "6bb827dc9f3474a7047698256d223c04", "sha256": "5ff987bb9135a6d513ece770ca759a1e967f46d510f50477135227e786680180"}, "downloads": -1, "filename": "hanlp-2.0.0a6.tar.gz", "has_sig": false, "md5_digest": "6bb827dc9f3474a7047698256d223c04", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 876890, "upload_time": "2020-01-05T02:33:16", "upload_time_iso_8601": "2020-01-05T02:33:16.316391Z", "url": "https://files.pythonhosted.org/packages/77/24/61a07e16e5374704dee808ab51d07534375242868229f7e3a67148450360/hanlp-2.0.0a6.tar.gz", "yanked": false}], "2.0.0a8": [{"comment_text": "", "digests": {"md5": "431b982a453394f597430e19aa6819ea", "sha256": "23e347b7b1a6bad971497ab658f8f708afda3515119b650db0bc2cf518379196"}, "downloads": -1, "filename": "hanlp-2.0.0a8.tar.gz", "has_sig": false, "md5_digest": "431b982a453394f597430e19aa6819ea", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 876936, "upload_time": "2020-01-05T09:49:25", "upload_time_iso_8601": "2020-01-05T09:49:25.460144Z", "url": "https://files.pythonhosted.org/packages/ba/97/41114dbba86513433f131b578f80aee02d92412cff097e48824713ba9482/hanlp-2.0.0a8.tar.gz", "yanked": false}], "2.0.0a9": [{"comment_text": "", "digests": {"md5": "79f1cd811fc49f9024aa38f3821bd9ed", "sha256": "eed13d6281c87d2b35502975d1ea0adc511b38b3ab41b8ffea7450d908d235b0"}, "downloads": -1, "filename": "hanlp-2.0.0a9.tar.gz", "has_sig": false, "md5_digest": "79f1cd811fc49f9024aa38f3821bd9ed", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 878418, "upload_time": "2020-01-06T06:11:20", "upload_time_iso_8601": "2020-01-06T06:11:20.538182Z", "url": "https://files.pythonhosted.org/packages/2c/aa/ab734ade7fcc9777b08665fe52b6e16a3a17dece8f7d7f86c2efc84495cb/hanlp-2.0.0a9.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "8e78fc8c37f67db4a7fa12117ce7a434", "sha256": "a2de00673a164bd526fc1871ae5f9fb52655317dfa18e9520f029d721965c527"}, "downloads": -1, "filename": "hanlp-2.0.0a42.tar.gz", "has_sig": false, "md5_digest": "8e78fc8c37f67db4a7fa12117ce7a434", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 132623, "upload_time": "2020-04-08T02:44:05", "upload_time_iso_8601": "2020-04-08T02:44:05.137566Z", "url": "https://files.pythonhosted.org/packages/23/30/53abd6f86b5cfa36713a2c1713b70936005389258584c0f07b499168d721/hanlp-2.0.0a42.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:52:34 2020"}