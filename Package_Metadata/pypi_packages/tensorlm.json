{"info": {"author": "Kilian Batzner", "author_email": "tensorlm@kilians.net", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3 :: Only", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "tensorlm\n========\n\nGenerate Shakespeare poems with 4 lines of code.\n\nInstallation\n------------\n\n``tensorlm`` is written in / for Python 3.4+ and TensorFlow 1.1+\n\n::\n\n    pip3 install tensorlm\n\nBasic Usage\n-----------\n\nUse the ``CharLM`` or ``WordLM`` class:\n\n.. code:: python\n\n    import tensorflow as tf\n    from tensorlm import CharLM\n        \n    with tf.Session() as session:\n        \n        # Create a new model. You can also use WordLM\n        model = CharLM(session, \"datasets/sherlock/tinytrain.txt\", max_vocab_size=96,\n                       neurons_per_layer=100, num_layers=3, num_timesteps=15)\n        \n        # Train it \n        model.train(session, max_epochs=5, max_steps=500)\n        \n        # Let it generate a text\n        generated = model.sample(session, \"The \", num_steps=100)\n        print(\"The \" + generated)\n\nThis should output something like:\n\n::\n\n    The  ee e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e \n\nCommand Line Usage\n------------------\n\n**Train:**\n``python3 -m tensorlm.cli --train=True --level=char --train_text_path=datasets/sherlock/tinytrain.txt --max_vocab_size=96 --neurons_per_layer=100 --num_layers=2 --batch_size=10 --num_timesteps=15 --save_dir=out/model --max_epochs=300 --save_interval_hours=0.5``\n\n**Sample:**\n``python3 -m tensorlm.cli --sample=True --level=char --neurons_per_layer=400 --num_layers=3 --num_timesteps=160 --save_dir=out/model``\n\n**Evaluate:**\n``python3 -m tensorlm.cli --evaluate=True --level=char --evaluate_text_path=datasets/sherlock/tinyvalid.txt --neurons_per_layer=400 --num_layers=3 --batch_size=10 --num_timesteps=160 --save_dir=out/model``\n\nSee ``python3 -m tensorlm.cli --help`` for all options.\n\nAdvanced Usage\n--------------\n\nCustom Input Data\n~~~~~~~~~~~~~~~~~\n\nThe inputs and targets don\u2019t have to be text. ``GeneratingLSTM`` only\nexpects token ids, so you can use any data type for the sequences, as\nlong as you can encode the data to integer ids.\n\n.. code:: python\n\n    # We use integer ids from 0 to 19, so the vocab size is 20. The range of ids must always start\n    # at zero.\n    batch_inputs = np.array([[1, 2, 3, 4], [15, 16, 17, 18]])  # 2 batches, 4 time steps each\n    batch_targets = np.array([[2, 3, 4, 5], [16, 17, 18, 19]])\n\n    # Create the model in a TensorFlow graph\n    model = GeneratingLSTM(vocab_size=20, neurons_per_layer=10, num_layers=2, max_batch_size=2)\n\n    # Initialize all defined TF Variables\n    session.run(tf.global_variables_initializer())\n\n    for _ in range(5000):\n        model.train_step(session, batch_inputs, batch_targets)\n\n    sampled = model.sample_ids(session, [15], num_steps=3)\n    print(\"Sampled: \" + str(sampled))\n\nThis should output something like:\n\n::\n\n    Sampled: [16, 18, 19]\n\nCustom Training, Dropout etc.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nUse the ``GeneratingLSTM`` class directly. This class is agnostic to the\ndataset type. It expects integer ids and returns integer ids.\n\n.. code:: python\n\n    import tensorflow as tf\n    from tensorlm import Vocabulary, Dataset, GeneratingLSTM\n\n    BATCH_SIZE = 20\n    NUM_TIMESTEPS = 15\n\n    with tf.Session() as session:\n        # Generate a token -> id vocabulary based on the text\n        vocab = Vocabulary.create_from_text(\"datasets/sherlock/tinytrain.txt\", max_vocab_size=96,\n                                            level=\"char\")\n\n        # Obtain input and target batches from the text file\n        dataset = Dataset(\"datasets/sherlock/tinytrain.txt\", vocab, BATCH_SIZE, NUM_TIMESTEPS)\n\n        # Create the model in a TensorFlow graph\n        model = GeneratingLSTM(vocab_size=vocab.get_size(), neurons_per_layer=100, num_layers=2,\n                               max_batch_size=BATCH_SIZE, output_keep_prob=0.5)\n\n        # Initialize all defined TF Variables\n        session.run(tf.global_variables_initializer())\n\n        # Do the training\n        epoch = 1\n        step = 1\n        for epoch in range(20):\n            for inputs, targets in dataset:\n                loss = model.train_step(session, inputs, targets)\n\n                if step % 100 == 0:\n                    # Evaluate from time to time\n                    dev_dataset = Dataset(\"datasets/sherlock/tinyvalid.txt\", vocab,\n                                          batch_size=BATCH_SIZE, num_timesteps=NUM_TIMESTEPS)\n                    dev_loss = model.evaluate(session, dev_dataset)\n                    print(\"Epoch: %d, Step: %d, Train Loss: %f, Dev Loss: %f\" % (\n                        epoch, step, loss, dev_loss))\n\n                    # Sample from the model from time to time\n                    print(\"Sampled: \\\"The \" + model.sample_text(session, vocab, \"The \") + \"\\\"\")\n\n                step += 1\n\nThis should output something like:\n\n::\n\n    Epoch: 3, Step: 100, Train Loss: 3.824941, Dev Loss: 3.778008\n    Sampled: \"The                                                                                                     \"\n    Epoch: 7, Step: 200, Train Loss: 2.832825, Dev Loss: 2.896187\n    Sampled: \"The                                                                                                     \"\n    Epoch: 11, Step: 300, Train Loss: 2.778579, Dev Loss: 2.830176\n    Sampled: \"The         eee                                                                                         \"\n    Epoch: 15, Step: 400, Train Loss: 2.655153, Dev Loss: 2.684828\n    Sampled: \"The        ee    e  e   e  e  e  e  e  e  e   e  e  e   e  e  e   e  e  e   e  e  e   e  e  e   e  e  e \"\n    Epoch: 19, Step: 500, Train Loss: 2.444502, Dev Loss: 2.479753\n    Sampled: \"The    an  an  an  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  o\"\n", "description_content_type": "", "docs_url": null, "download_url": "https://github.com/batzner/tensorlm/archive/v0.4.2.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/batzner/tensorlm", "keywords": "tensorflow", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "tensorlm", "package_url": "https://pypi.org/project/tensorlm/", "platform": "", "project_url": "https://pypi.org/project/tensorlm/", "project_urls": {"Download": "https://github.com/batzner/tensorlm/archive/v0.4.2.tar.gz", "Homepage": "https://github.com/batzner/tensorlm"}, "release_url": "https://pypi.org/project/tensorlm/0.4.2/", "requires_dist": null, "requires_python": "", "summary": "TensorFlow wrapper for deep neural text generation on character or word level with RNNs / LSTMs", "version": "0.4.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Generate Shakespeare poems with 4 lines of code.</p>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p><tt>tensorlm</tt> is written in / for Python 3.4+ and TensorFlow 1.1+</p>\n<pre>pip3 install tensorlm\n</pre>\n</div>\n<div id=\"basic-usage\">\n<h2>Basic Usage</h2>\n<p>Use the <tt>CharLM</tt> or <tt>WordLM</tt> class:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"nn\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorlm</span> <span class=\"kn\">import</span> <span class=\"n\">CharLM</span>\n\n<span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">session</span><span class=\"p\">:</span>\n\n    <span class=\"c1\"># Create a new model. You can also use WordLM</span>\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">CharLM</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"s2\">\"datasets/sherlock/tinytrain.txt\"</span><span class=\"p\">,</span> <span class=\"n\">max_vocab_size</span><span class=\"o\">=</span><span class=\"mi\">96</span><span class=\"p\">,</span>\n                   <span class=\"n\">neurons_per_layer</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">num_layers</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">num_timesteps</span><span class=\"o\">=</span><span class=\"mi\">15</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Train it</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"n\">max_epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">max_steps</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Let it generate a text</span>\n    <span class=\"n\">generated</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"s2\">\"The \"</span><span class=\"p\">,</span> <span class=\"n\">num_steps</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"The \"</span> <span class=\"o\">+</span> <span class=\"n\">generated</span><span class=\"p\">)</span>\n</pre>\n<p>This should output something like:</p>\n<pre>The  ee e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e\n</pre>\n</div>\n<div id=\"command-line-usage\">\n<h2>Command Line Usage</h2>\n<p><strong>Train:</strong>\n<tt>python3 <span class=\"pre\">-m</span> tensorlm.cli <span class=\"pre\">--train=True</span> <span class=\"pre\">--level=char</span> <span class=\"pre\">--train_text_path=datasets/sherlock/tinytrain.txt</span> <span class=\"pre\">--max_vocab_size=96</span> <span class=\"pre\">--neurons_per_layer=100</span> <span class=\"pre\">--num_layers=2</span> <span class=\"pre\">--batch_size=10</span> <span class=\"pre\">--num_timesteps=15</span> <span class=\"pre\">--save_dir=out/model</span> <span class=\"pre\">--max_epochs=300</span> <span class=\"pre\">--save_interval_hours=0.5</span></tt></p>\n<p><strong>Sample:</strong>\n<tt>python3 <span class=\"pre\">-m</span> tensorlm.cli <span class=\"pre\">--sample=True</span> <span class=\"pre\">--level=char</span> <span class=\"pre\">--neurons_per_layer=400</span> <span class=\"pre\">--num_layers=3</span> <span class=\"pre\">--num_timesteps=160</span> <span class=\"pre\">--save_dir=out/model</span></tt></p>\n<p><strong>Evaluate:</strong>\n<tt>python3 <span class=\"pre\">-m</span> tensorlm.cli <span class=\"pre\">--evaluate=True</span> <span class=\"pre\">--level=char</span> <span class=\"pre\">--evaluate_text_path=datasets/sherlock/tinyvalid.txt</span> <span class=\"pre\">--neurons_per_layer=400</span> <span class=\"pre\">--num_layers=3</span> <span class=\"pre\">--batch_size=10</span> <span class=\"pre\">--num_timesteps=160</span> <span class=\"pre\">--save_dir=out/model</span></tt></p>\n<p>See <tt>python3 <span class=\"pre\">-m</span> tensorlm.cli <span class=\"pre\">--help</span></tt> for all options.</p>\n</div>\n<div id=\"advanced-usage\">\n<h2>Advanced Usage</h2>\n<div id=\"custom-input-data\">\n<h3>Custom Input Data</h3>\n<p>The inputs and targets don\u2019t have to be text. <tt>GeneratingLSTM</tt> only\nexpects token ids, so you can use any data type for the sequences, as\nlong as you can encode the data to integer ids.</p>\n<pre><span class=\"c1\"># We use integer ids from 0 to 19, so the vocab size is 20. The range of ids must always start</span>\n<span class=\"c1\"># at zero.</span>\n<span class=\"n\">batch_inputs</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">17</span><span class=\"p\">,</span> <span class=\"mi\">18</span><span class=\"p\">]])</span>  <span class=\"c1\"># 2 batches, 4 time steps each</span>\n<span class=\"n\">batch_targets</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">17</span><span class=\"p\">,</span> <span class=\"mi\">18</span><span class=\"p\">,</span> <span class=\"mi\">19</span><span class=\"p\">]])</span>\n\n<span class=\"c1\"># Create the model in a TensorFlow graph</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">GeneratingLSTM</span><span class=\"p\">(</span><span class=\"n\">vocab_size</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">neurons_per_layer</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">num_layers</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">max_batch_size</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Initialize all defined TF Variables</span>\n<span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">global_variables_initializer</span><span class=\"p\">())</span>\n\n<span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">5000</span><span class=\"p\">):</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_step</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"n\">batch_inputs</span><span class=\"p\">,</span> <span class=\"n\">batch_targets</span><span class=\"p\">)</span>\n\n<span class=\"n\">sampled</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">sample_ids</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">15</span><span class=\"p\">],</span> <span class=\"n\">num_steps</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Sampled: \"</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">sampled</span><span class=\"p\">))</span>\n</pre>\n<p>This should output something like:</p>\n<pre>Sampled: [16, 18, 19]\n</pre>\n</div>\n<div id=\"custom-training-dropout-etc\">\n<h3>Custom Training, Dropout etc.</h3>\n<p>Use the <tt>GeneratingLSTM</tt> class directly. This class is agnostic to the\ndataset type. It expects integer ids and returns integer ids.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"nn\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorlm</span> <span class=\"kn\">import</span> <span class=\"n\">Vocabulary</span><span class=\"p\">,</span> <span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">GeneratingLSTM</span>\n\n<span class=\"n\">BATCH_SIZE</span> <span class=\"o\">=</span> <span class=\"mi\">20</span>\n<span class=\"n\">NUM_TIMESTEPS</span> <span class=\"o\">=</span> <span class=\"mi\">15</span>\n\n<span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">session</span><span class=\"p\">:</span>\n    <span class=\"c1\"># Generate a token -&gt; id vocabulary based on the text</span>\n    <span class=\"n\">vocab</span> <span class=\"o\">=</span> <span class=\"n\">Vocabulary</span><span class=\"o\">.</span><span class=\"n\">create_from_text</span><span class=\"p\">(</span><span class=\"s2\">\"datasets/sherlock/tinytrain.txt\"</span><span class=\"p\">,</span> <span class=\"n\">max_vocab_size</span><span class=\"o\">=</span><span class=\"mi\">96</span><span class=\"p\">,</span>\n                                        <span class=\"n\">level</span><span class=\"o\">=</span><span class=\"s2\">\"char\"</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Obtain input and target batches from the text file</span>\n    <span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">Dataset</span><span class=\"p\">(</span><span class=\"s2\">\"datasets/sherlock/tinytrain.txt\"</span><span class=\"p\">,</span> <span class=\"n\">vocab</span><span class=\"p\">,</span> <span class=\"n\">BATCH_SIZE</span><span class=\"p\">,</span> <span class=\"n\">NUM_TIMESTEPS</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Create the model in a TensorFlow graph</span>\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">GeneratingLSTM</span><span class=\"p\">(</span><span class=\"n\">vocab_size</span><span class=\"o\">=</span><span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">get_size</span><span class=\"p\">(),</span> <span class=\"n\">neurons_per_layer</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">num_layers</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n                           <span class=\"n\">max_batch_size</span><span class=\"o\">=</span><span class=\"n\">BATCH_SIZE</span><span class=\"p\">,</span> <span class=\"n\">output_keep_prob</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Initialize all defined TF Variables</span>\n    <span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">global_variables_initializer</span><span class=\"p\">())</span>\n\n    <span class=\"c1\"># Do the training</span>\n    <span class=\"n\">epoch</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n    <span class=\"n\">step</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n    <span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">):</span>\n        <span class=\"k\">for</span> <span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">targets</span> <span class=\"ow\">in</span> <span class=\"n\">dataset</span><span class=\"p\">:</span>\n            <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_step</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">targets</span><span class=\"p\">)</span>\n\n            <span class=\"k\">if</span> <span class=\"n\">step</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"c1\"># Evaluate from time to time</span>\n                <span class=\"n\">dev_dataset</span> <span class=\"o\">=</span> <span class=\"n\">Dataset</span><span class=\"p\">(</span><span class=\"s2\">\"datasets/sherlock/tinyvalid.txt\"</span><span class=\"p\">,</span> <span class=\"n\">vocab</span><span class=\"p\">,</span>\n                                      <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">BATCH_SIZE</span><span class=\"p\">,</span> <span class=\"n\">num_timesteps</span><span class=\"o\">=</span><span class=\"n\">NUM_TIMESTEPS</span><span class=\"p\">)</span>\n                <span class=\"n\">dev_loss</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">evaluate</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"n\">dev_dataset</span><span class=\"p\">)</span>\n                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Epoch: </span><span class=\"si\">%d</span><span class=\"s2\">, Step: </span><span class=\"si\">%d</span><span class=\"s2\">, Train Loss: </span><span class=\"si\">%f</span><span class=\"s2\">, Dev Loss: </span><span class=\"si\">%f</span><span class=\"s2\">\"</span> <span class=\"o\">%</span> <span class=\"p\">(</span>\n                    <span class=\"n\">epoch</span><span class=\"p\">,</span> <span class=\"n\">step</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">dev_loss</span><span class=\"p\">))</span>\n\n                <span class=\"c1\"># Sample from the model from time to time</span>\n                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Sampled: </span><span class=\"se\">\\\"</span><span class=\"s2\">The \"</span> <span class=\"o\">+</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">sample_text</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"n\">vocab</span><span class=\"p\">,</span> <span class=\"s2\">\"The \"</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"s2\">\"</span><span class=\"se\">\\\"</span><span class=\"s2\">\"</span><span class=\"p\">)</span>\n\n            <span class=\"n\">step</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n</pre>\n<p>This should output something like:</p>\n<pre>Epoch: 3, Step: 100, Train Loss: 3.824941, Dev Loss: 3.778008\nSampled: \"The                                                                                                     \"\nEpoch: 7, Step: 200, Train Loss: 2.832825, Dev Loss: 2.896187\nSampled: \"The                                                                                                     \"\nEpoch: 11, Step: 300, Train Loss: 2.778579, Dev Loss: 2.830176\nSampled: \"The         eee                                                                                         \"\nEpoch: 15, Step: 400, Train Loss: 2.655153, Dev Loss: 2.684828\nSampled: \"The        ee    e  e   e  e  e  e  e  e  e   e  e  e   e  e  e   e  e  e   e  e  e   e  e  e   e  e  e \"\nEpoch: 19, Step: 500, Train Loss: 2.444502, Dev Loss: 2.479753\nSampled: \"The    an  an  an  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  on  o\"\n</pre>\n</div>\n</div>\n\n          </div>"}, "last_serial": 3747859, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "b9110589493680534c72ad0214312c5d", "sha256": "92efa334883fb4e7f25db52af6a144b4c331a485d9666ba6b9bda94827b5656a"}, "downloads": -1, "filename": "tensorlm-0.1.tar.gz", "has_sig": false, "md5_digest": "b9110589493680534c72ad0214312c5d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22711, "upload_time": "2017-08-30T11:55:09", "upload_time_iso_8601": "2017-08-30T11:55:09.255037Z", "url": "https://files.pythonhosted.org/packages/6e/31/9bbffe12bd459eea77d2f1886d84e31083dc48ce24429084d07dcad8ef65/tensorlm-0.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "287fd30c9547f362a0e12ad21f7b23e2", "sha256": "5c9bff74ccc69330d2dca34ef1df92e549436ac191f5aba1328f7d10d85b90e2"}, "downloads": -1, "filename": "tensorlm-0.2.tar.gz", "has_sig": false, "md5_digest": "287fd30c9547f362a0e12ad21f7b23e2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21703, "upload_time": "2017-08-30T12:06:48", "upload_time_iso_8601": "2017-08-30T12:06:48.228959Z", "url": "https://files.pythonhosted.org/packages/1d/5a/7b97bf9a5e825db537ffd7a87a37ac12523c580dce604dc32fe1e526643f/tensorlm-0.2.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "bf4e68f29e761fcec95c85cfa4c00d1e", "sha256": "fe5ba4d28f3021a7223a3d5c22ebf4687ef84e7c820895ac5373212ff9366d17"}, "downloads": -1, "filename": "tensorlm-0.3.tar.gz", "has_sig": false, "md5_digest": "bf4e68f29e761fcec95c85cfa4c00d1e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21808, "upload_time": "2017-08-30T12:10:55", "upload_time_iso_8601": "2017-08-30T12:10:55.989618Z", "url": "https://files.pythonhosted.org/packages/40/10/29ad0fa9f0fde4a287aaf01f6defc6755ccfffa319ab20c33a04bd9c6ab8/tensorlm-0.3.tar.gz", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "38d9e5ddcbb93cc6f8c2c684e3206eae", "sha256": "7d9156a60e7299c807916cafbf3ff8d4aaccc51729a4b8439da4c89d40a66cc1"}, "downloads": -1, "filename": "tensorlm-0.4.tar.gz", "has_sig": false, "md5_digest": "38d9e5ddcbb93cc6f8c2c684e3206eae", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23327, "upload_time": "2018-04-09T08:09:20", "upload_time_iso_8601": "2018-04-09T08:09:20.890382Z", "url": "https://files.pythonhosted.org/packages/ac/79/20e12938b9cebd3f0ac173526d1ff1528b3d1dfeb395e9713c878489f318/tensorlm-0.4.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "8287e1647c5ba306559c06b74cba92b5", "sha256": "49c5d436537978b08a8b93412dfd7280d833dab5f663f123fe2b715f1899f5a3"}, "downloads": -1, "filename": "tensorlm-0.4.1.tar.gz", "has_sig": false, "md5_digest": "8287e1647c5ba306559c06b74cba92b5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23380, "upload_time": "2018-04-09T08:31:38", "upload_time_iso_8601": "2018-04-09T08:31:38.992697Z", "url": "https://files.pythonhosted.org/packages/be/7d/e773a4dda8ddf8b42538cc62bf5b212aa8caf6170f02881c1edac0eafe2f/tensorlm-0.4.1.tar.gz", "yanked": false}], "0.4.2": [{"comment_text": "", "digests": {"md5": "13fc437a0d9fe7fae1a2567a352e9831", "sha256": "323286c7231d56b18c68896fad114333342e6389b8ffb190127d753038c865cf"}, "downloads": -1, "filename": "tensorlm-0.4.2.tar.gz", "has_sig": false, "md5_digest": "13fc437a0d9fe7fae1a2567a352e9831", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23202, "upload_time": "2018-04-09T08:36:33", "upload_time_iso_8601": "2018-04-09T08:36:33.194639Z", "url": "https://files.pythonhosted.org/packages/74/23/844fab370dcf69182fb7088e1738f30871a34bc1b2fe5ee592c15f2bb541/tensorlm-0.4.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "13fc437a0d9fe7fae1a2567a352e9831", "sha256": "323286c7231d56b18c68896fad114333342e6389b8ffb190127d753038c865cf"}, "downloads": -1, "filename": "tensorlm-0.4.2.tar.gz", "has_sig": false, "md5_digest": "13fc437a0d9fe7fae1a2567a352e9831", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23202, "upload_time": "2018-04-09T08:36:33", "upload_time_iso_8601": "2018-04-09T08:36:33.194639Z", "url": "https://files.pythonhosted.org/packages/74/23/844fab370dcf69182fb7088e1738f30871a34bc1b2fe5ee592c15f2bb541/tensorlm-0.4.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:03 2020"}