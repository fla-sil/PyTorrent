{"info": {"author": "Michael Fuerst", "author_email": "mail@michaelfuerst.de", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Programming Language :: Python"], "description": "# starttf - Simplified Deeplearning for Tensorflow [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)\n\nThis repo aims to contain everything required to quickly develop a deep neural network with tensorflow.\nThe idea is that if you use write a compatible SimpleSequence for data loading and networks based on the StartTFModel, you will automatically obey best practices and have super fast training speeds.\n\n## Install\n\nProperly install `tensorflow` or `tensorflow-gpu` please follow the [official instructions](https://www.tensorflow.org/install/) carefully.\n\nThen, simply pip install from the github repo.\n\n```bash\npip install starttf\n```\n\n## Datasets\n\nExtensions SimpleSequences from [opendatalake.simple_sequence.SimpleSequence](https://github.com/penguinmenac3/opendatalake/blob/master/opendatalake/simple_sequence.py) are supported.\nThey work like keras.Sequence however with an augmentation and a preprocessing function.\n\nFor details checkout the [readme of opendatalake](https://github.com/penguinmenac3/opendatalake/blob/master/README.md).\n\n## Models\n\nEvery model returns a dictionary containing output tensors and a dictionary containing debug tensors\n\n1. [Model Base Classes](starttf/models/models.py)\n2. [Common Encoders](starttf/models/encoders.py)\n3. [Untrained Backbones](starttf/models/backbones)\n\n## Simple to use tensorflow\n\n### Simple Training (No Boilerplate)\n\nThere are pre-implemented models which can be glued together and trained with just a few lines.\nHowever, before training you will have to create tf-records as shown in the section *Simple TF Record Creation*.\nThis is actually a full main file.\n\n```python\n# Import helpers\nfrom starttf.estimators.tf_estimator import easy_train_and_evaluate\nfrom starttf.utils.hyperparams import load_params\n\n# Import a/your model (here one for mnist)\nfrom mymodel import MyStartTFModel\n\n# Import your loss (here an example)\nfrom myloss import create_loss\n\n# Load params (here for mnist)\nhyperparams = load_params(\"hyperparams/experiment1.json\")\n\n# Train model\neasy_train_and_evaluate(hyperparams, MyStartTFModel, create_loss, continue_training=False)\n```\n\n### Quick Model Definition\n\nSimply implement a create_model function.\nThis model is only a feed forward model.\n\nThe model function returns a dictionary containing all layers that should be accessible from outside and a dictionary containing debug values that should be availible for loss or plotting in tensorboard.\n\n```python\nimport tensorflow as tf\n\nfrom starttf.models.model import StartTFModel\nfrom starttf.models.encoders import Encoder\n\nConv2D = tf.keras.layers.Conv2D\n\n\nclass ExampleModel(StartTFModel):\n    def __init__(self, hyperparams):\n        super(ExampleModel, self).__init__(hyperparams)\n        num_classes = hyperparams.problem.number_of_categories\n\n        # Create the vgg encoder\n        self.encoder = Encoder(hyperparams)\n\n        #Use the generated model \n        self.conv6 = Conv2D(filters=1024, kernel_size=(1, 1), padding=\"same\", activation=\"relu\")\n        self.conv7 = Conv2D(filters=1024, kernel_size=(1, 1), padding=\"same\", activation=\"relu\")\n        self.conv8 = Conv2D(filters=num_classes, kernel_size=(1, 1), padding=\"same\", activation=None, name=\"probs\")\n\n    def call(self, input_tensor, training=False):\n        \"\"\"\n        Run the model.\n        \"\"\"\n        encoder, debug = self.encoder(input_tensor, training)\n        result = self.conv6(encoder[\"features\"])\n        result = self.conv7(result)\n        logits = self.conv8(result)\n        probs = tf.nn.softmax(logits)\n        return {\"logits\": logits, \"probs\": probs}, debug\n```\n\n### Quick Loss Definition\n\n```python\ndef create_loss(model, labels, mode, hyper_params):\n    metrics = {}\n    losses = {}\n\n    # Add loss\n    labels = tf.reshape(labels[\"probs\"], [-1, hyper_params.problem.number_of_categories])\n    ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=model[\"logits\"], labels=labels)\n    loss_op = tf.reduce_mean(ce)\n\n    # Add losses to dict. \"loss\" is the primary loss that is optimized.\n    losses[\"loss\"] = loss_op\n    metrics['accuracy'] = tf.metrics.accuracy(labels=labels,\n                                              predictions=model[\"probs\"],\n                                              name='acc_op')\n\n    return losses, metrics\n```\n\n### Simple TF Record Creation\n\nFast training speed can be achieved by using tf records.\nHowever, usually tf records are a hastle to use the write_data method makes it simple.\n\n```python\nfrom starttf.utils.hyperparams import load_params\nfrom starttf.data.autorecords import write_data\n\nfrom my_data import MySimpleSequence\n\n# Load the hyper parameters.\nhyperparams = load_params(\"hyperparams/experiment1.json\")\n\n# Get a generator and its parameters\ntraining_data = MySimpleSequence(hyperparams)\nvalidation_data = MySimpleSequence(hyperparams)\n\n# Write the data\nwrite_data(hyperparams, PHASE_TRAIN, training_data, 4)\nwrite_data(hyperparams, PHASE_VALIDATION, validation_data, 2)\n```\n\n### Tensorboard Integration\n\nTensorboard integration is simple.\n\nEvery loss in the losses dict is automatically added to tensorboard.\nIf you also want debug images, you can add a tf.summary.image() in your create_loss method.\n\n### TF Estimator + Cluster Support\n\nIf you use the easy_train_and_evaluate method, a correctly configured TF Estimator is created.\nThe estimator is then trained in a way that supports cluster training if you have a cluster.\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "https://github.com/penguinmenac3/starttf/tarball/1.0.3", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/penguinmenac3/starttf", "keywords": "", "license": "BSD", "maintainer": "", "maintainer_email": "", "name": "starttf", "package_url": "https://pypi.org/project/starttf/", "platform": "", "project_url": "https://pypi.org/project/starttf/", "project_urls": {"Download": "https://github.com/penguinmenac3/starttf/tarball/1.0.3", "Homepage": "https://github.com/penguinmenac3/starttf"}, "release_url": "https://pypi.org/project/starttf/1.0.3/", "requires_dist": ["numpy", "scikit-image", "scikit-learn", "scipy", "matplotlib", "h5py", "pandas", "opencv-python", "tensorflow-hub", "hyperparams", "opendatalake", "GPUtil", "keras"], "requires_python": "", "summary": "A tensorflow batteries included kit to write tensorflow networks from scratch or use existing ones.", "version": "1.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            # starttf - Simplified Deeplearning for Tensorflow [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)<br><br>This repo aims to contain everything required to quickly develop a deep neural network with tensorflow.<br>The idea is that if you use write a compatible SimpleSequence for data loading and networks based on the StartTFModel, you will automatically obey best practices and have super fast training speeds.<br><br>## Install<br><br>Properly install `tensorflow` or `tensorflow-gpu` please follow the [official instructions](https://www.tensorflow.org/install/) carefully.<br><br>Then, simply pip install from the github repo.<br><br>```bash<br>pip install starttf<br>```<br><br>## Datasets<br><br>Extensions SimpleSequences from [opendatalake.simple_sequence.SimpleSequence](https://github.com/penguinmenac3/opendatalake/blob/master/opendatalake/simple_sequence.py) are supported.<br>They work like keras.Sequence however with an augmentation and a preprocessing function.<br><br>For details checkout the [readme of opendatalake](https://github.com/penguinmenac3/opendatalake/blob/master/README.md).<br><br>## Models<br><br>Every model returns a dictionary containing output tensors and a dictionary containing debug tensors<br><br>1. [Model Base Classes](starttf/models/models.py)<br>2. [Common Encoders](starttf/models/encoders.py)<br>3. [Untrained Backbones](starttf/models/backbones)<br><br>## Simple to use tensorflow<br><br>### Simple Training (No Boilerplate)<br><br>There are pre-implemented models which can be glued together and trained with just a few lines.<br>However, before training you will have to create tf-records as shown in the section *Simple TF Record Creation*.<br>This is actually a full main file.<br><br>```python<br># Import helpers<br>from starttf.estimators.tf_estimator import easy_train_and_evaluate<br>from starttf.utils.hyperparams import load_params<br><br># Import a/your model (here one for mnist)<br>from mymodel import MyStartTFModel<br><br># Import your loss (here an example)<br>from myloss import create_loss<br><br># Load params (here for mnist)<br>hyperparams = load_params(\"hyperparams/experiment1.json\")<br><br># Train model<br>easy_train_and_evaluate(hyperparams, MyStartTFModel, create_loss, continue_training=False)<br>```<br><br>### Quick Model Definition<br><br>Simply implement a create_model function.<br>This model is only a feed forward model.<br><br>The model function returns a dictionary containing all layers that should be accessible from outside and a dictionary containing debug values that should be availible for loss or plotting in tensorboard.<br><br>```python<br>import tensorflow as tf<br><br>from starttf.models.model import StartTFModel<br>from starttf.models.encoders import Encoder<br><br>Conv2D = tf.keras.layers.Conv2D<br><br><br>class ExampleModel(StartTFModel):<br>    def __init__(self, hyperparams):<br>        super(ExampleModel, self).__init__(hyperparams)<br>        num_classes = hyperparams.problem.number_of_categories<br><br>        # Create the vgg encoder<br>        self.encoder = Encoder(hyperparams)<br><br>        #Use the generated model <br>        self.conv6 = Conv2D(filters=1024, kernel_size=(1, 1), padding=\"same\", activation=\"relu\")<br>        self.conv7 = Conv2D(filters=1024, kernel_size=(1, 1), padding=\"same\", activation=\"relu\")<br>        self.conv8 = Conv2D(filters=num_classes, kernel_size=(1, 1), padding=\"same\", activation=None, name=\"probs\")<br><br>    def call(self, input_tensor, training=False):<br>        \"\"\"<br>        Run the model.<br>        \"\"\"<br>        encoder, debug = self.encoder(input_tensor, training)<br>        result = self.conv6(encoder[\"features\"])<br>        result = self.conv7(result)<br>        logits = self.conv8(result)<br>        probs = tf.nn.softmax(logits)<br>        return {\"logits\": logits, \"probs\": probs}, debug<br>```<br><br>### Quick Loss Definition<br><br>```python<br>def create_loss(model, labels, mode, hyper_params):<br>    metrics = {}<br>    losses = {}<br><br>    # Add loss<br>    labels = tf.reshape(labels[\"probs\"], [-1, hyper_params.problem.number_of_categories])<br>    ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=model[\"logits\"], labels=labels)<br>    loss_op = tf.reduce_mean(ce)<br><br>    # Add losses to dict. \"loss\" is the primary loss that is optimized.<br>    losses[\"loss\"] = loss_op<br>    metrics['accuracy'] = tf.metrics.accuracy(labels=labels,<br>                                              predictions=model[\"probs\"],<br>                                              name='acc_op')<br><br>    return losses, metrics<br>```<br><br>### Simple TF Record Creation<br><br>Fast training speed can be achieved by using tf records.<br>However, usually tf records are a hastle to use the write_data method makes it simple.<br><br>```python<br>from starttf.utils.hyperparams import load_params<br>from starttf.data.autorecords import write_data<br><br>from my_data import MySimpleSequence<br><br># Load the hyper parameters.<br>hyperparams = load_params(\"hyperparams/experiment1.json\")<br><br># Get a generator and its parameters<br>training_data = MySimpleSequence(hyperparams)<br>validation_data = MySimpleSequence(hyperparams)<br><br># Write the data<br>write_data(hyperparams, PHASE_TRAIN, training_data, 4)<br>write_data(hyperparams, PHASE_VALIDATION, validation_data, 2)<br>```<br><br>### Tensorboard Integration<br><br>Tensorboard integration is simple.<br><br>Every loss in the losses dict is automatically added to tensorboard.<br>If you also want debug images, you can add a tf.summary.image() in your create_loss method.<br><br>### TF Estimator + Cluster Support<br><br>If you use the easy_train_and_evaluate method, a correctly configured TF Estimator is created.<br>The estimator is then trained in a way that supports cluster training if you have a cluster.<br><br><br>\n          </div>"}, "last_serial": 4481866, "releases": {"0.0.2": [{"comment_text": "", "digests": {"md5": "b50683d6fb7e84cfe0bd208cdf85f20f", "sha256": "8fbf2fd7b48f72b54c7a0f9f01d127bc5d711528c25f4bed996f63c5b9b690d0"}, "downloads": -1, "filename": "starttf-0.0.2.tar.gz", "has_sig": false, "md5_digest": "b50683d6fb7e84cfe0bd208cdf85f20f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 40353, "upload_time": "2018-05-12T17:55:35", "upload_time_iso_8601": "2018-05-12T17:55:35.889074Z", "url": "https://files.pythonhosted.org/packages/96/be/5d00f2aae8fcddf53e4bcd34c09a38469dec3d49e11eabaf0e122f03fd2a/starttf-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "78ff1f5b309d49b24068381cccff52e6", "sha256": "8c01517cf54a66a832e8585fbca09f803e5759f4c4b4d1770c9a0de8c6a0ab8e"}, "downloads": -1, "filename": "starttf-0.0.3.tar.gz", "has_sig": false, "md5_digest": "78ff1f5b309d49b24068381cccff52e6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 40447, "upload_time": "2018-05-23T13:06:34", "upload_time_iso_8601": "2018-05-23T13:06:34.229949Z", "url": "https://files.pythonhosted.org/packages/46/90/35582fde84554348341fa91948dbadb6f02a3b4443a68c83d84b048d383e/starttf-0.0.3.tar.gz", "yanked": false}], "1.0.0": [{"comment_text": "", "digests": {"md5": "b3b04fb70c5bb5cc1a7bb4ef18711520", "sha256": "de132f40e0fa1e7516f9c0c5155a1cae6d0ac5bc7372b654fce0687b0bfddeb0"}, "downloads": -1, "filename": "starttf-1.0.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "b3b04fb70c5bb5cc1a7bb4ef18711520", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 64355, "upload_time": "2018-10-26T18:00:32", "upload_time_iso_8601": "2018-10-26T18:00:32.140771Z", "url": "https://files.pythonhosted.org/packages/11/7a/546c19740a1797b0a72c9098d1ee1464decf32ad28eaa0c842bf625b67b9/starttf-1.0.0-py2.py3-none-any.whl", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "e71344826313c5f1f40782c22077b5b1", "sha256": "2e94467fbb2883bf52577f05a5fefd861700c6cc1732cdf493d79c9c372090c7"}, "downloads": -1, "filename": "starttf-1.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "e71344826313c5f1f40782c22077b5b1", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 64487, "upload_time": "2018-10-30T10:50:34", "upload_time_iso_8601": "2018-10-30T10:50:34.467635Z", "url": "https://files.pythonhosted.org/packages/12/20/f75c70dc1706e24303db3265fb03eb175478f16c86b966b2771964e9d124/starttf-1.0.1-py2.py3-none-any.whl", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "1c58017c49e0c6417bf27b3f8f86f51c", "sha256": "c5b8bc66b0ea9d2e84b2826bfe9eb675aeeeeb8c94a1d4ce98ff2da2fe25e0e2"}, "downloads": -1, "filename": "starttf-1.0.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "1c58017c49e0c6417bf27b3f8f86f51c", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 64695, "upload_time": "2018-11-13T14:05:25", "upload_time_iso_8601": "2018-11-13T14:05:25.089004Z", "url": "https://files.pythonhosted.org/packages/62/67/d2b73956ef790b9b42ab88776de99e8e046bceec6e1294fec94ea3c57f61/starttf-1.0.3-py2.py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1c58017c49e0c6417bf27b3f8f86f51c", "sha256": "c5b8bc66b0ea9d2e84b2826bfe9eb675aeeeeb8c94a1d4ce98ff2da2fe25e0e2"}, "downloads": -1, "filename": "starttf-1.0.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "1c58017c49e0c6417bf27b3f8f86f51c", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 64695, "upload_time": "2018-11-13T14:05:25", "upload_time_iso_8601": "2018-11-13T14:05:25.089004Z", "url": "https://files.pythonhosted.org/packages/62/67/d2b73956ef790b9b42ab88776de99e8e046bceec6e1294fec94ea3c57f61/starttf-1.0.3-py2.py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 03:02:32 2020"}