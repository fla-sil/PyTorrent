{"info": {"author": "Max Frenzel", "author_email": "maxfrenzel+cvae@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# CompressionVAE\n\nData embedding API based on the Variational Autoencoder (VAE), originally proposed by Kingma and Welling https://arxiv.org/abs/1312.6114.\n\nThis tool, implemented in TensorFlow 1.x, is designed to work similar to familiar dimensionality reduction methods such as scikit-learn's t-SNE or UMAP, but also go beyond their capabilities in some notable ways, making full use of the VAE as a generative model.\n\nWhile I decided to call the tool itself CompressionVAE, or CVAE for short, I mainly chose this to give it a unique name.\nIn practice, it is based on a standard VAE, with the (optional) addition of Inverse Autoregressive Flow (IAF) layers to allow for more flexible posterior distributions.\nFor details on the IAF layers, I refer you to the original paper: https://arxiv.org/pdf/1606.04934.pdf.\n\nCompressionVAE has **several unique advantages** over the common manifold learning methods like t-SNE and UMAP:\n* Rather than just a transformation of the training data, it provides a **reversible and deterministic function**, mapping from data space to embedding space.\n* Due to the reversibility of the mapping, the model can be used to **generate new data from arbitrary latent variables**. It also makes them highly suitable as **intermediary representations for downstream tasks**.\n* Once a model is trained, it can be reused to transform new data, making it **suitable for use in live settings**.\n* Like UMAP, CVAE is **fast and scales much better to large datasets, and high dimensional input and latent spaces**.\n* The neural network architecture and training parameters are **highly customisable** through the simple API, allowing more advanced users to tailor the system to their needs.\n* VAEs have a **very strong theoretical foundation**, and the learned latent spaces have many desirable properties. There is also extensive literature on different variants, and CVAE can easily be extended to keep up with new research advances.\n\n## Installing CompressionVAE\n\nCompressionVAE is distributed through PyPI under the name `cvae` (https://pypi.org/project/cvae/). To install the latest version, simply run\n```\npip install cvae\n```\nAlternatively, to locally install CompressionVAE, clone this repository and run the following command from the CompressionVAE root directory.\n```\npip install -e .\n```\n\n## Basic Use Case\n\nTo use CVAE to learn an embedding function, we first need to import the cvae library.\n```\nfrom cvae import cvae\n```\n\nWhen creating a CompressionVAE object for a new model, it needs to be provided a training dataset. \nFor small datasets that fit in memory we can directly follow the sklean convention. Let's look at this case first and take MNIST as an example.\n\nFirst, load the MNIST data. (Note: this example requires scikit-learn which is not installed with CVAE. You might have to install it first by running `pip install sklearn`.)\n```\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1, cache=True)\nX = mnist.data\n```\n\n### Initializing CVAE\nNow we can create a CompressionVAE object/model based on this data. The minimal code to do this is\n```\nembedder = cvae.CompressionVAE(X)\n```\nBy default, this creates a model with two-dimensional latent space, splits the data X randomly into 90% train and 10% validation data, applies feature normalization, and tries to match the model architecture to the input and latent feature dimensions.\nIt also saves the model in a temporary directory which gets overwritten the next time you create a new CVAE object there.\n\nWe will look at customising all this later, but for now let's move on to training.\n\n### Training CVAE\nOnce a CVAE object is initialised and associated with data, we can train the embedder using its `train` method. This works similar to t-SNE or UMAP's `fit` method.\nIn the simplest case, we just run \n```\nembedder.train()\n```\nThis will train the model, applying automatic learning rate scheduling based on the validation data loss, and stop either when the model converges or after 50k training steps.\nWe can also stop the training process early through a KeyboardInterrupt (ctrl-c or 'interrupt kernel' in Jupyter notebook). The model will be saved at this point.\n\nIt is also possible to stop training and then re-start with different parameters (see more details below).\n\nOne note/warning: At the moment, the model can be quite sensitive to initialization (in some rare cases even giving NAN losses). Re-initializing/training the model can improve the results if a training run did not give satisfactory results.\n\n### Embedding data\nOnce we have a trained model (well, technically even before training, but the results would be random), we can use CVAE to compress data, embedding it into the latent space.\nTo do this, we use CVAE's `embed` method.\n\nTo embed the entire MNIST data:\n```\nz = embedder.embed(X)\n```\nBut note that other than t-SNE or UMAP, this data does not have to be the same as the training data. It can be new and previously unseen data.\n\n### Visualising the embedding\nFor two-dimensional latent spaces, CVAE comes with a built-in visualization method, `visualize`. It provides a two-dimensional plot of the embeddings, including class information if available.\n\nTo visualize the MNIST embeddings and color them by their respective class, we can run\n```\nembedder.visualize(z, labels=[int(label) for label in mnist.target])\n```\nWe could also passed the string labels `mnist.target` directly to `labels`, but in that case they would not necessarily be ordered from 0 to 9. \nOptionally, if we pass `labels` as a list of integers like above, we can also pass the `categories` parameter, a list of strings associating names with the labels. In the case of MNIST this is irrelevant since the label and class names are the same.\nBy default the `visualize` simply displays the plot. By setting the `filename` parameter we can alternatively save the plot to a file.\n\n### Generating data\nFinally, we can use CVAE as a generative model, generating data by decoding arbitrary latent vectors using the `decode` method.\nIf we simply want to 'undo' our MNIST embedding and try to re-create the input data, we can run our embeddings `z` through the `decode` method.\n```\nX_reconstructed = embedder.decode(z)\n```\nAs a more interesting example, we can use this for data interpolation. Let's say we want to create the data that's halfway between the first and the second MNIST datapoint (a '5' and a '0' respectively).\nWe can achieve this with the following code\n```\nimport numpy as np\n# Combine the two examples and add batch dimension\nz_interp = np.expand_dims(0.5*z[0] + 0.5*z[1], axis=0)\n# Decode the new latent vector.\nX_interp = embedder.decode(z_interp)\n```\n\n#### Visualizing the latent space\nIn the case of image data, such as MNIST, CVAE also has a method that allows us to quickly visualize the latent space as seen through the decoder.\nTo plot a 20 by 20 grid of reconstructed images, spanning the latent space region [-4, 4] in both x and y, we can run\n```\nembedder.visualize_latent_grid(xy_range=(-4.0, 4.0),\n                               grid_size=20,\n                               shape=(28, 28))\n```\n\n## Advanced Use Cases\nThe example above shows the simplest usage of CVAE. However, if desired a user can take much more control over the system and customize the model and training processes.\n\n### Customizing the model\nIn the previous example we initialised a CompressionVAE with default parameters. If we want more control, we might want to initialise it the following way:\n```\nembedder = cvae.CompressionVAE(X,\n                               train_valid_split=0.99,\n                               dim_latent=16,\n                               iaf_flow_length=10,\n                               cells_encoder=[512, 256, 128],\n                               initializer='lecun_normal',\n                               batch_size=32,\n                               batch_size_test=128,\n                               logdir='~/mnist_16d',\n                               feature_normalization=False,\n                               tb_logging=True)\n```\n`train_valid_split` controls the random splitting into train and test data. Here 99% of X is used for training, and only 1% is reserved for validation.\n\nAlternatively, to get more control over the data the user can also provide `X_valid` as an input. In this case `train_valid_split` is ignored and the model uses `X` for training and `X_valid` for validation.\n\n`dim_latent` specifies the dimensionality of the latent space.\n\n`iaf_flow_length` controls how many IAF flow layers the model has.\n\n`cells_encoder` determines the number, as well as size of the encoders fully connected layers. In the case above, we have three layers with 512, 256, and 128 units respectively. The decoder uses the mirrored version of this.\nIf this parameter is not set, CVAE creates a two layer network with sizes adjusted to the input dimension and latent dimension. The logic behind this is very handwavy and arbitrary for now, and I generally recommend setting this manually.\n\n`initializer` controls how the model weights are initialized, with options being `orthogonal` (default), `truncated_normal`, and `lecun_normal`.\n\n`batch_size` and `batch_size_test` determine the batch sizes used for training and testing respectively.\n\n`logdir` specifies the path to the model, and also acts as the model name. The default, `'temp'`, gets overwritten every time it is used, but other model names can be used to save and restore models for later use or even to continue training.\n\n`feature_normalization` tells CVAE whether it should internally apply feature normalization (zero mean, unit variance, based on the training data) or not. If True, the normalisation factors are stored with the model and get applied to any future data.\n\n`tb_logging` determines whether the model writes summaries for TensorBoard during the training process.\n\n### Customizing the training process\nIn the simple example we called the `train` method without any parameter. A more advanced call might look like\n```\nembedder.train(learning_rate=1e-4,\n               num_steps=2000,\n               dropout_keep_prob=0.6,\n               test_every=50,\n               lr_scheduling=False)\n```\n`learning_rate` sets the initial learning rate of training.\n\n`num_steps` controls the maximum number of training steps before stopping.\n\n`dropout_keep_prob` determines the keep probability for dropout in the fully connected layers.\n\n`test_every` sets the frequency of test steps.\n\n`lr_scheduling` controls whether learning rate scheduling is applied. If `False`, training continues at `learning_rate` until `num_steps` is reached.\n\nFor more arguments/details, for example controlling the details of the learning rate scheduler and the convergence criteria, check the method definition. \n\n### Using large datasets\n\nAlternatively to providing the input data `X` as a single numpy array, as done with t-SNE and UMAP, CVAE also allows for much larger datasets that do not fit into a single array.\n\nTo prepare such a dataset, create a new directory, e.g. `'~/my_dataset'`, and save the training data as individual npy files per example in this directory. \n\n(Note: the data can also be saved in nested sub-directories, for example one directory per category. CVAE will look through the entire directory tree for npy files.)\n\nWhen initialising a model based on this kind of data pass the root directory of the dataset as `X`. E.g.\n```\nembedder = cvae.CompressionVAE(X='~/my_dataset')\n```  \nInitialising will take slightly longer than if `X` is passed as an array, even for the same number of data points. But this method scales in principle to arbitrarily large datasets, and only loads one batch at a time during training.\n\n### Restarting an existing model\n\nIf a CompressionVAE object is initialized with `logdir='temp'` it always starts from a new untrained model, possible overwriting any previous temp model.\nHowever, if a different `logdir` is chosen, the model persists and can be reloaded.\n\nIf CompressionVAE is initialized with a `logdir` that already exists and contains parameter and checkpoint files of a previous model, it attempts to restore that model's checkpoints.\nIn this case, any model specific input parameter (e.g. `dim_latent` and `cells_encoder`) is ignored in favor of the original models parameters.\n\nA restored model can be use straight away to embed or generate data, but it is also possible to continue the training process, picking up from the most recent checkpoint.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/maxfrenzel/CompressionVAE", "keywords": "vae variational autoencoder manifold dimensionality reduction compression tensorflow", "license": "", "maintainer": "", "maintainer_email": "", "name": "cvae", "package_url": "https://pypi.org/project/cvae/", "platform": "", "project_url": "https://pypi.org/project/cvae/", "project_urls": {"Homepage": "https://github.com/maxfrenzel/CompressionVAE"}, "release_url": "https://pypi.org/project/cvae/0.0.3/", "requires_dist": ["numpy", "matplotlib", "joblib", "tqdm", "tensorflow (<2,>=1)"], "requires_python": ">=3.6", "summary": "CompressionVAE: General purpose dimensionality reduction and manifold learning tool based on Variational Autoencoder.", "version": "0.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>CompressionVAE</h1>\n<p>Data embedding API based on the Variational Autoencoder (VAE), originally proposed by Kingma and Welling <a href=\"https://arxiv.org/abs/1312.6114\" rel=\"nofollow\">https://arxiv.org/abs/1312.6114</a>.</p>\n<p>This tool, implemented in TensorFlow 1.x, is designed to work similar to familiar dimensionality reduction methods such as scikit-learn's t-SNE or UMAP, but also go beyond their capabilities in some notable ways, making full use of the VAE as a generative model.</p>\n<p>While I decided to call the tool itself CompressionVAE, or CVAE for short, I mainly chose this to give it a unique name.\nIn practice, it is based on a standard VAE, with the (optional) addition of Inverse Autoregressive Flow (IAF) layers to allow for more flexible posterior distributions.\nFor details on the IAF layers, I refer you to the original paper: <a href=\"https://arxiv.org/pdf/1606.04934.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1606.04934.pdf</a>.</p>\n<p>CompressionVAE has <strong>several unique advantages</strong> over the common manifold learning methods like t-SNE and UMAP:</p>\n<ul>\n<li>Rather than just a transformation of the training data, it provides a <strong>reversible and deterministic function</strong>, mapping from data space to embedding space.</li>\n<li>Due to the reversibility of the mapping, the model can be used to <strong>generate new data from arbitrary latent variables</strong>. It also makes them highly suitable as <strong>intermediary representations for downstream tasks</strong>.</li>\n<li>Once a model is trained, it can be reused to transform new data, making it <strong>suitable for use in live settings</strong>.</li>\n<li>Like UMAP, CVAE is <strong>fast and scales much better to large datasets, and high dimensional input and latent spaces</strong>.</li>\n<li>The neural network architecture and training parameters are <strong>highly customisable</strong> through the simple API, allowing more advanced users to tailor the system to their needs.</li>\n<li>VAEs have a <strong>very strong theoretical foundation</strong>, and the learned latent spaces have many desirable properties. There is also extensive literature on different variants, and CVAE can easily be extended to keep up with new research advances.</li>\n</ul>\n<h2>Installing CompressionVAE</h2>\n<p>CompressionVAE is distributed through PyPI under the name <code>cvae</code> (<a href=\"https://pypi.org/project/cvae/\" rel=\"nofollow\">https://pypi.org/project/cvae/</a>). To install the latest version, simply run</p>\n<pre><code>pip install cvae\n</code></pre>\n<p>Alternatively, to locally install CompressionVAE, clone this repository and run the following command from the CompressionVAE root directory.</p>\n<pre><code>pip install -e .\n</code></pre>\n<h2>Basic Use Case</h2>\n<p>To use CVAE to learn an embedding function, we first need to import the cvae library.</p>\n<pre><code>from cvae import cvae\n</code></pre>\n<p>When creating a CompressionVAE object for a new model, it needs to be provided a training dataset.\nFor small datasets that fit in memory we can directly follow the sklean convention. Let's look at this case first and take MNIST as an example.</p>\n<p>First, load the MNIST data. (Note: this example requires scikit-learn which is not installed with CVAE. You might have to install it first by running <code>pip install sklearn</code>.)</p>\n<pre><code>from sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1, cache=True)\nX = mnist.data\n</code></pre>\n<h3>Initializing CVAE</h3>\n<p>Now we can create a CompressionVAE object/model based on this data. The minimal code to do this is</p>\n<pre><code>embedder = cvae.CompressionVAE(X)\n</code></pre>\n<p>By default, this creates a model with two-dimensional latent space, splits the data X randomly into 90% train and 10% validation data, applies feature normalization, and tries to match the model architecture to the input and latent feature dimensions.\nIt also saves the model in a temporary directory which gets overwritten the next time you create a new CVAE object there.</p>\n<p>We will look at customising all this later, but for now let's move on to training.</p>\n<h3>Training CVAE</h3>\n<p>Once a CVAE object is initialised and associated with data, we can train the embedder using its <code>train</code> method. This works similar to t-SNE or UMAP's <code>fit</code> method.\nIn the simplest case, we just run</p>\n<pre><code>embedder.train()\n</code></pre>\n<p>This will train the model, applying automatic learning rate scheduling based on the validation data loss, and stop either when the model converges or after 50k training steps.\nWe can also stop the training process early through a KeyboardInterrupt (ctrl-c or 'interrupt kernel' in Jupyter notebook). The model will be saved at this point.</p>\n<p>It is also possible to stop training and then re-start with different parameters (see more details below).</p>\n<p>One note/warning: At the moment, the model can be quite sensitive to initialization (in some rare cases even giving NAN losses). Re-initializing/training the model can improve the results if a training run did not give satisfactory results.</p>\n<h3>Embedding data</h3>\n<p>Once we have a trained model (well, technically even before training, but the results would be random), we can use CVAE to compress data, embedding it into the latent space.\nTo do this, we use CVAE's <code>embed</code> method.</p>\n<p>To embed the entire MNIST data:</p>\n<pre><code>z = embedder.embed(X)\n</code></pre>\n<p>But note that other than t-SNE or UMAP, this data does not have to be the same as the training data. It can be new and previously unseen data.</p>\n<h3>Visualising the embedding</h3>\n<p>For two-dimensional latent spaces, CVAE comes with a built-in visualization method, <code>visualize</code>. It provides a two-dimensional plot of the embeddings, including class information if available.</p>\n<p>To visualize the MNIST embeddings and color them by their respective class, we can run</p>\n<pre><code>embedder.visualize(z, labels=[int(label) for label in mnist.target])\n</code></pre>\n<p>We could also passed the string labels <code>mnist.target</code> directly to <code>labels</code>, but in that case they would not necessarily be ordered from 0 to 9.\nOptionally, if we pass <code>labels</code> as a list of integers like above, we can also pass the <code>categories</code> parameter, a list of strings associating names with the labels. In the case of MNIST this is irrelevant since the label and class names are the same.\nBy default the <code>visualize</code> simply displays the plot. By setting the <code>filename</code> parameter we can alternatively save the plot to a file.</p>\n<h3>Generating data</h3>\n<p>Finally, we can use CVAE as a generative model, generating data by decoding arbitrary latent vectors using the <code>decode</code> method.\nIf we simply want to 'undo' our MNIST embedding and try to re-create the input data, we can run our embeddings <code>z</code> through the <code>decode</code> method.</p>\n<pre><code>X_reconstructed = embedder.decode(z)\n</code></pre>\n<p>As a more interesting example, we can use this for data interpolation. Let's say we want to create the data that's halfway between the first and the second MNIST datapoint (a '5' and a '0' respectively).\nWe can achieve this with the following code</p>\n<pre><code>import numpy as np\n# Combine the two examples and add batch dimension\nz_interp = np.expand_dims(0.5*z[0] + 0.5*z[1], axis=0)\n# Decode the new latent vector.\nX_interp = embedder.decode(z_interp)\n</code></pre>\n<h4>Visualizing the latent space</h4>\n<p>In the case of image data, such as MNIST, CVAE also has a method that allows us to quickly visualize the latent space as seen through the decoder.\nTo plot a 20 by 20 grid of reconstructed images, spanning the latent space region [-4, 4] in both x and y, we can run</p>\n<pre><code>embedder.visualize_latent_grid(xy_range=(-4.0, 4.0),\n                               grid_size=20,\n                               shape=(28, 28))\n</code></pre>\n<h2>Advanced Use Cases</h2>\n<p>The example above shows the simplest usage of CVAE. However, if desired a user can take much more control over the system and customize the model and training processes.</p>\n<h3>Customizing the model</h3>\n<p>In the previous example we initialised a CompressionVAE with default parameters. If we want more control, we might want to initialise it the following way:</p>\n<pre><code>embedder = cvae.CompressionVAE(X,\n                               train_valid_split=0.99,\n                               dim_latent=16,\n                               iaf_flow_length=10,\n                               cells_encoder=[512, 256, 128],\n                               initializer='lecun_normal',\n                               batch_size=32,\n                               batch_size_test=128,\n                               logdir='~/mnist_16d',\n                               feature_normalization=False,\n                               tb_logging=True)\n</code></pre>\n<p><code>train_valid_split</code> controls the random splitting into train and test data. Here 99% of X is used for training, and only 1% is reserved for validation.</p>\n<p>Alternatively, to get more control over the data the user can also provide <code>X_valid</code> as an input. In this case <code>train_valid_split</code> is ignored and the model uses <code>X</code> for training and <code>X_valid</code> for validation.</p>\n<p><code>dim_latent</code> specifies the dimensionality of the latent space.</p>\n<p><code>iaf_flow_length</code> controls how many IAF flow layers the model has.</p>\n<p><code>cells_encoder</code> determines the number, as well as size of the encoders fully connected layers. In the case above, we have three layers with 512, 256, and 128 units respectively. The decoder uses the mirrored version of this.\nIf this parameter is not set, CVAE creates a two layer network with sizes adjusted to the input dimension and latent dimension. The logic behind this is very handwavy and arbitrary for now, and I generally recommend setting this manually.</p>\n<p><code>initializer</code> controls how the model weights are initialized, with options being <code>orthogonal</code> (default), <code>truncated_normal</code>, and <code>lecun_normal</code>.</p>\n<p><code>batch_size</code> and <code>batch_size_test</code> determine the batch sizes used for training and testing respectively.</p>\n<p><code>logdir</code> specifies the path to the model, and also acts as the model name. The default, <code>'temp'</code>, gets overwritten every time it is used, but other model names can be used to save and restore models for later use or even to continue training.</p>\n<p><code>feature_normalization</code> tells CVAE whether it should internally apply feature normalization (zero mean, unit variance, based on the training data) or not. If True, the normalisation factors are stored with the model and get applied to any future data.</p>\n<p><code>tb_logging</code> determines whether the model writes summaries for TensorBoard during the training process.</p>\n<h3>Customizing the training process</h3>\n<p>In the simple example we called the <code>train</code> method without any parameter. A more advanced call might look like</p>\n<pre><code>embedder.train(learning_rate=1e-4,\n               num_steps=2000,\n               dropout_keep_prob=0.6,\n               test_every=50,\n               lr_scheduling=False)\n</code></pre>\n<p><code>learning_rate</code> sets the initial learning rate of training.</p>\n<p><code>num_steps</code> controls the maximum number of training steps before stopping.</p>\n<p><code>dropout_keep_prob</code> determines the keep probability for dropout in the fully connected layers.</p>\n<p><code>test_every</code> sets the frequency of test steps.</p>\n<p><code>lr_scheduling</code> controls whether learning rate scheduling is applied. If <code>False</code>, training continues at <code>learning_rate</code> until <code>num_steps</code> is reached.</p>\n<p>For more arguments/details, for example controlling the details of the learning rate scheduler and the convergence criteria, check the method definition.</p>\n<h3>Using large datasets</h3>\n<p>Alternatively to providing the input data <code>X</code> as a single numpy array, as done with t-SNE and UMAP, CVAE also allows for much larger datasets that do not fit into a single array.</p>\n<p>To prepare such a dataset, create a new directory, e.g. <code>'~/my_dataset'</code>, and save the training data as individual npy files per example in this directory.</p>\n<p>(Note: the data can also be saved in nested sub-directories, for example one directory per category. CVAE will look through the entire directory tree for npy files.)</p>\n<p>When initialising a model based on this kind of data pass the root directory of the dataset as <code>X</code>. E.g.</p>\n<pre><code>embedder = cvae.CompressionVAE(X='~/my_dataset')\n</code></pre>\n<p>Initialising will take slightly longer than if <code>X</code> is passed as an array, even for the same number of data points. But this method scales in principle to arbitrarily large datasets, and only loads one batch at a time during training.</p>\n<h3>Restarting an existing model</h3>\n<p>If a CompressionVAE object is initialized with <code>logdir='temp'</code> it always starts from a new untrained model, possible overwriting any previous temp model.\nHowever, if a different <code>logdir</code> is chosen, the model persists and can be reloaded.</p>\n<p>If CompressionVAE is initialized with a <code>logdir</code> that already exists and contains parameter and checkpoint files of a previous model, it attempts to restore that model's checkpoints.\nIn this case, any model specific input parameter (e.g. <code>dim_latent</code> and <code>cells_encoder</code>) is ignored in favor of the original models parameters.</p>\n<p>A restored model can be use straight away to embed or generate data, but it is also possible to continue the training process, picking up from the most recent checkpoint.</p>\n\n          </div>"}, "last_serial": 6658855, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "51c504c6754591714f6bbd5a0c81a3ab", "sha256": "1610b0a1da68b7ed5c3eaca95ded395d292233ff8926f1c7ff1afe5c95acb109"}, "downloads": -1, "filename": "cvae-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "51c504c6754591714f6bbd5a0c81a3ab", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 23598, "upload_time": "2020-02-13T07:36:57", "upload_time_iso_8601": "2020-02-13T07:36:57.238059Z", "url": "https://files.pythonhosted.org/packages/21/5b/34b0376940506f71e46907638e332e2b5f3edef9f8f714b5a65bcfef5b7e/cvae-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d6743968552919117bc0a29af7b4c181", "sha256": "166f1ebd70d7a1b2caad39de0d1ef7b186d08732fa17dc8977c955304771e276"}, "downloads": -1, "filename": "cvae-0.0.1.tar.gz", "has_sig": false, "md5_digest": "d6743968552919117bc0a29af7b4c181", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 24393, "upload_time": "2020-02-13T07:37:00", "upload_time_iso_8601": "2020-02-13T07:37:00.118973Z", "url": "https://files.pythonhosted.org/packages/f9/09/9a4ddfac5192e96670400e58491462632a51994d74340cd578a3f9df929c/cvae-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "5e5a8f64678427fae1b1cc6b3ed6ab0d", "sha256": "17b472e10b2b3c66a21bfd4fac5e5621c4d61d1508886ada2bfc969d147272d5"}, "downloads": -1, "filename": "cvae-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "5e5a8f64678427fae1b1cc6b3ed6ab0d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 23769, "upload_time": "2020-02-13T07:52:41", "upload_time_iso_8601": "2020-02-13T07:52:41.103243Z", "url": "https://files.pythonhosted.org/packages/80/a2/653cf41c3c7df3a9c5fdf9891b35831c78e89425bdfebb97ab4fb7cbe12c/cvae-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "20476e902ff4d6b58fffffb127a3260b", "sha256": "910d0fe3f88d4d5f15259bedce96d828605da42258b1c0f39f8f2748f9876dd3"}, "downloads": -1, "filename": "cvae-0.0.2.tar.gz", "has_sig": false, "md5_digest": "20476e902ff4d6b58fffffb127a3260b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 24805, "upload_time": "2020-02-13T07:52:43", "upload_time_iso_8601": "2020-02-13T07:52:43.006851Z", "url": "https://files.pythonhosted.org/packages/b9/c3/d98695d35a054da1d2f149f3607cc41e66c292d4b1ca9ce45d1fac95ba7c/cvae-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "ffce90e699701290e30ab70e1ebddd78", "sha256": "af2effc9b8b35f3bee4ee3d1aee37068835f1656ecc6a752e16e8a9016f26ad5"}, "downloads": -1, "filename": "cvae-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "ffce90e699701290e30ab70e1ebddd78", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 24579, "upload_time": "2020-02-19T07:30:17", "upload_time_iso_8601": "2020-02-19T07:30:17.015928Z", "url": "https://files.pythonhosted.org/packages/99/7e/2a00db1b6f00f8c7ccfba5c5c148e650618df91a0ea02708fb220d928437/cvae-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "59bf481cd9b04d169a0d5b2d4551d7b9", "sha256": "6813a7f0c63dd4691753adfcb6b7a84c5d8eeb12dd2d90bbde962df6a94259b0"}, "downloads": -1, "filename": "cvae-0.0.3.tar.gz", "has_sig": false, "md5_digest": "59bf481cd9b04d169a0d5b2d4551d7b9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 25854, "upload_time": "2020-02-19T07:30:18", "upload_time_iso_8601": "2020-02-19T07:30:18.744498Z", "url": "https://files.pythonhosted.org/packages/af/6f/ca86b9b8f9538b9323d3c8c2e22e4fd8581e7e1ab8dcf80176c16cc0a154/cvae-0.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ffce90e699701290e30ab70e1ebddd78", "sha256": "af2effc9b8b35f3bee4ee3d1aee37068835f1656ecc6a752e16e8a9016f26ad5"}, "downloads": -1, "filename": "cvae-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "ffce90e699701290e30ab70e1ebddd78", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 24579, "upload_time": "2020-02-19T07:30:17", "upload_time_iso_8601": "2020-02-19T07:30:17.015928Z", "url": "https://files.pythonhosted.org/packages/99/7e/2a00db1b6f00f8c7ccfba5c5c148e650618df91a0ea02708fb220d928437/cvae-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "59bf481cd9b04d169a0d5b2d4551d7b9", "sha256": "6813a7f0c63dd4691753adfcb6b7a84c5d8eeb12dd2d90bbde962df6a94259b0"}, "downloads": -1, "filename": "cvae-0.0.3.tar.gz", "has_sig": false, "md5_digest": "59bf481cd9b04d169a0d5b2d4551d7b9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 25854, "upload_time": "2020-02-19T07:30:18", "upload_time_iso_8601": "2020-02-19T07:30:18.744498Z", "url": "https://files.pythonhosted.org/packages/af/6f/ca86b9b8f9538b9323d3c8c2e22e4fd8581e7e1ab8dcf80176c16cc0a154/cvae-0.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:41:15 2020"}