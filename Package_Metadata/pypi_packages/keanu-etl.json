{"info": {"author": "Romain Thouvenin, Marcin Koziej", "author_email": "romain@wemove.eu, marcin@cahoots.pl", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Project Keanu\n\nProject Keanu constist of:\n- Handbook for analytic terms, metrics and their definitions\n- Common data model for campaign evaluation\n- Set of queries to visualize metrics and key indicators\n- ETL tool to transform data in CRM (Currently CiviCRM, Identity in the future)\n- Metabase setup tool (mainly import/export of questions, dashboards) \n\n\n# Keanu CLI\n\nKeanu command lives in `cli` directory. It can (incrementally) load data from CiviCRM to keanu schema and delete it. With this functionality you can develop ETL SQL scripts with ease of rewinding and forwarding the state of keanu DB. \n\n![](doc/img/keanu-load.png)\n\n## Setup\n\n1. Install Pipenv command in your system: `sudo apt install pipenv`\n2. Install dependencies in `cli` directory: `pipenv install`\n\n## Configuration\n\n1. Copy the `.env.sample` file to `.env` and change it to taste. \n\n- `DATABASE_URL` - must use schema understood by SQLAlchemy library, required only for SQL related commands \n- `SOURCE` - is a name of CiviCRM database (also called schema), required only for SQL related commands\n- `METABASE_ENDPOINT`, `METABASE_AUTH_EMAIL`, `METABASE_AUTH_PASSWORD` - Metabase credentials, required only metabase commands\n\n## Usage\n\nKeanu commands has a set of subcommands, similar to git or heroku.\nYou need to enable the Pipenv, by running `pipenv shell`.\n\n## Package building\n\nTo build the package use `python setup.py bdist_wheel`.\n\nTo upload it to PyPI:\n\n1. Create a `~/.pypirc` file:\n```\n[distutils] \nindex-servers=pypi\n[pypi] \nrepository = https://upload.pypi.org/legacy/ \nusername = pypi-username\n```\n\nTo install the package locally:\n\n```pip install --upgrade dist/keanu-0.1-py3-none-any.whl```\n\n2. Use `python -m twine upload dist/*`.\n\n### SQL commands\n\nThese are the commands related to creating and filling in the Keanu schema (from CiviCRM DB).\n\n#### schema: manipulating database schema\n\n- `keanu.py schema -L FILENAME` - loads SQL schema file.\n- `keanu.py schema -D` - drops all tables from the database.\n\n#### load: loading data\n\n`keanu.py load SQLDIR` loads data from CiviCRM to keanu db in ordered steps. Every SQL file in `SQLDIR` directory has `-- ORDER: 12` comment which specifies an order. The default order is 100.\n\nOptions:\n\n- `-o 10:` - _order_, start from SQL file with order 10 (also see order spec)\n- `-n` - _dry run_, do not execute the SQL. Usefull to see the order of scripts\n- `-i` - _incremental_, also run the incremental parts of the SQL, normally deleted (they are marked by `-- BEGIN INCREMENTAL` and `-- END INCREMENTAL` comments)\n- `-d` - _display_ a whole SQL statement instead of abbreviation of first line\n\n#### delete: deleting data\n\n`keanu.py delete SQLDIR` will go through the SQL scripts in _reverse order_ and execute the commented `DELETE` and `TRUNCATE` statements in the scripts. \n\nOptions:\n\n- `-n` - _dry run_, do not execute the SQL. Usefull to see the order of scripts\n- `-o 10:` - delete data until the SQL file _order_ (inclusive) and stop\n- `-d` - _display_ a whole SQL statement instead of abbreviation of first line\n\n#### Order spec\n\nYou can provide just a number to order (`-o`) option, but you can also provide:\n- a list of numbers such as `10,11,50` and scripts with respective order will be run\n- a range in python format such as `10:16` equal to `10,11,12,13,14,15` (excluding 16)\n- a mix of them: `10:13,31,50:52`\n\n\n#### SQL file reference:\n\nWe add metadata to SQL files in SQL comments. This is usefull because you can still work with the SQL files using tools like MySQL Workbench, or mysql command - the commands will be ignored.\n\nSupported commands are:\n\n`-- ORDER: arg` - arg should be a 3 digit number. It specifies the order this script will be run.\n`-- DELETE FROM .. ` - a DELETE statement will be saved to revert this script, so to remove all the data it has added. Must start with all caps DELETE.\n`-- TRUNCATE` - similar to above. Please note: TRUNCATE will fail if any other table has reference to this table, even when it's empty.\n`-- IGNORE` - the rest of the file will be ignored.\n\nBlocks are also supported. They are marked by a pair of lines: \n\n```sql\n-- BEGIN keyword\n .. some SQL code here..\n-- END keyword\n```\n\nThey should be properly nested.\n\nSupported blocks are:\n- `BEGIN INCREMENTAL` / `END INCREMENTAL` - marks part of the SQL that will normally be removed, and will be only used if `-i` (incremental flag) is used while loading. You can have many INCREMENTAL blocks. Example use:\n\n```sql\n-- this scripts inserts into cookie jar\n-- ORDER: 10\n-- DELETE FROM jar\n\nINSERT INTO jar (colour, size, external_id)\nSELECT\n  cd.name, \n  c.size,\n  c.id\nFROM cookie c\nJOIN colour_dictionary cd ON c.colour_id = cd.id\n\n-- when we do incremental, we exclude external_id's we already have:\n-- BEGIN INCREMENTAL\nWHERE c.id NOT IN (SELECT external_id FROM jar)\n-- END INCREMENTAL\n\n```\n\n- `BEGIN INITIAL` / `END INITIAL` - marks part of the SQL that will be removed in incremental `-i` mode.\n\nEnviroment variable interpolation. Similar to shell, simple variable interpolation is supported. `${FOO}` will be interpolated into value of FOO environment variable. This is currently used to insert the source schema name with `${SOURCE}`.\n\n### Metabase commands\n\nThese are the commands to export / import Metabase questions and dashboards that are related to the Keanu schema\n\n#### Export\n\n`keanu.py metabase export` generates a JSON dump of all the data required to import the collections / dashboards / questions given in options.\n\nThe generated JSON is printed on standard output. It contains a property `items` with the list of objects that got exported, as well as a property `mappings` with information to translate ids that make sense only in the context of a Metabase instance (table ids, card ids, etc).\n\nOptions:\n - `-c \"Some collection\"` - Exports all the questions and dashboards contained in the named collection and its child collections. You should ensure that the dashboards refer only to questions contained within that collection, unless you plan to import the data within the same Metabase instance. The collection must exist.\n\n#### Import\n\n`keanu.py metabase import` reads a JSON dump generated by the export command and import the object into a Metabase instance.\n\nOptions:\n - `-c \"Some collection\"` - The name of the collection into which the objects should be imported. The collection must exist and be empty (no support for overwrite / merge at the moment).\n - `-j /path/to/file.json` - The JSON file to import.\n\n## Product vision\n\nRight now during initial development, we run keanu command from the source\ndirectory `cli`, and the SQL scripts are kept in `sql`. The vision for the\nfuture product is the following:\n\nKeanu will be a command installed gloablly into the system, with `pip install\nkeanu`. It will work like git or heroku, a multi-tool to do everything\nconntected to common DB model, ETL, running AI logic, installing dashboards in\nMetabase (this last function fits this command least, but let's see).\n\nIt will download a repository of SQL files, properly divided between general\nuse, CiviCRM users, Identity users, etc, and be able to configure itself with\n`keanu setup` - saving information about source and target DBs. It will be able\nalso to upload SQL files that were changed and verion manage them somehow, to\nhelp in collaboration (this could be done using git).\n\nSimilarly, it will enable import and export of Metabase questions (called\n_cards_) and dashboards. Not only their SQL, but also their JSON configuration\n(using [python Metabase API client\nlibrary](https://github.com/stunitas/metabase-py)).\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://gitlab.wemove.eu/internal/keanu", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "keanu-etl", "package_url": "https://pypi.org/project/keanu-etl/", "platform": "", "project_url": "https://pypi.org/project/keanu-etl/", "project_urls": {"Homepage": "https://gitlab.wemove.eu/internal/keanu"}, "release_url": "https://pypi.org/project/keanu-etl/0.5.2/", "requires_dist": ["click", "jaeger-client", "metabase", "pandas", "petl", "pyaml", "pygments", "pymysql", "python-dotenv", "requests", "sqlalchemy"], "requires_python": "", "summary": "Analytics ETL and collaboration tool for progressive campaigning", "version": "0.5.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Project Keanu</h1>\n<p>Project Keanu constist of:</p>\n<ul>\n<li>Handbook for analytic terms, metrics and their definitions</li>\n<li>Common data model for campaign evaluation</li>\n<li>Set of queries to visualize metrics and key indicators</li>\n<li>ETL tool to transform data in CRM (Currently CiviCRM, Identity in the future)</li>\n<li>Metabase setup tool (mainly import/export of questions, dashboards)</li>\n</ul>\n<h1>Keanu CLI</h1>\n<p>Keanu command lives in <code>cli</code> directory. It can (incrementally) load data from CiviCRM to keanu schema and delete it. With this functionality you can develop ETL SQL scripts with ease of rewinding and forwarding the state of keanu DB.</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/68e26c78b162d409e9c06b9d691c607ed4da9b9e/646f632f696d672f6b65616e752d6c6f61642e706e67\"></p>\n<h2>Setup</h2>\n<ol>\n<li>Install Pipenv command in your system: <code>sudo apt install pipenv</code></li>\n<li>Install dependencies in <code>cli</code> directory: <code>pipenv install</code></li>\n</ol>\n<h2>Configuration</h2>\n<ol>\n<li>Copy the <code>.env.sample</code> file to <code>.env</code> and change it to taste.</li>\n</ol>\n<ul>\n<li><code>DATABASE_URL</code> - must use schema understood by SQLAlchemy library, required only for SQL related commands</li>\n<li><code>SOURCE</code> - is a name of CiviCRM database (also called schema), required only for SQL related commands</li>\n<li><code>METABASE_ENDPOINT</code>, <code>METABASE_AUTH_EMAIL</code>, <code>METABASE_AUTH_PASSWORD</code> - Metabase credentials, required only metabase commands</li>\n</ul>\n<h2>Usage</h2>\n<p>Keanu commands has a set of subcommands, similar to git or heroku.\nYou need to enable the Pipenv, by running <code>pipenv shell</code>.</p>\n<h2>Package building</h2>\n<p>To build the package use <code>python setup.py bdist_wheel</code>.</p>\n<p>To upload it to PyPI:</p>\n<ol>\n<li>Create a <code>~/.pypirc</code> file:</li>\n</ol>\n<pre><code>[distutils] \nindex-servers=pypi\n[pypi] \nrepository = https://upload.pypi.org/legacy/ \nusername = pypi-username\n</code></pre>\n<p>To install the package locally:</p>\n<p><code>pip install --upgrade dist/keanu-0.1-py3-none-any.whl</code></p>\n<ol>\n<li>Use <code>python -m twine upload dist/*</code>.</li>\n</ol>\n<h3>SQL commands</h3>\n<p>These are the commands related to creating and filling in the Keanu schema (from CiviCRM DB).</p>\n<h4>schema: manipulating database schema</h4>\n<ul>\n<li><code>keanu.py schema -L FILENAME</code> - loads SQL schema file.</li>\n<li><code>keanu.py schema -D</code> - drops all tables from the database.</li>\n</ul>\n<h4>load: loading data</h4>\n<p><code>keanu.py load SQLDIR</code> loads data from CiviCRM to keanu db in ordered steps. Every SQL file in <code>SQLDIR</code> directory has <code>-- ORDER: 12</code> comment which specifies an order. The default order is 100.</p>\n<p>Options:</p>\n<ul>\n<li><code>-o 10:</code> - <em>order</em>, start from SQL file with order 10 (also see order spec)</li>\n<li><code>-n</code> - <em>dry run</em>, do not execute the SQL. Usefull to see the order of scripts</li>\n<li><code>-i</code> - <em>incremental</em>, also run the incremental parts of the SQL, normally deleted (they are marked by <code>-- BEGIN INCREMENTAL</code> and <code>-- END INCREMENTAL</code> comments)</li>\n<li><code>-d</code> - <em>display</em> a whole SQL statement instead of abbreviation of first line</li>\n</ul>\n<h4>delete: deleting data</h4>\n<p><code>keanu.py delete SQLDIR</code> will go through the SQL scripts in <em>reverse order</em> and execute the commented <code>DELETE</code> and <code>TRUNCATE</code> statements in the scripts.</p>\n<p>Options:</p>\n<ul>\n<li><code>-n</code> - <em>dry run</em>, do not execute the SQL. Usefull to see the order of scripts</li>\n<li><code>-o 10:</code> - delete data until the SQL file <em>order</em> (inclusive) and stop</li>\n<li><code>-d</code> - <em>display</em> a whole SQL statement instead of abbreviation of first line</li>\n</ul>\n<h4>Order spec</h4>\n<p>You can provide just a number to order (<code>-o</code>) option, but you can also provide:</p>\n<ul>\n<li>a list of numbers such as <code>10,11,50</code> and scripts with respective order will be run</li>\n<li>a range in python format such as <code>10:16</code> equal to <code>10,11,12,13,14,15</code> (excluding 16)</li>\n<li>a mix of them: <code>10:13,31,50:52</code></li>\n</ul>\n<h4>SQL file reference:</h4>\n<p>We add metadata to SQL files in SQL comments. This is usefull because you can still work with the SQL files using tools like MySQL Workbench, or mysql command - the commands will be ignored.</p>\n<p>Supported commands are:</p>\n<p><code>-- ORDER: arg</code> - arg should be a 3 digit number. It specifies the order this script will be run.\n<code>-- DELETE FROM ..</code> - a DELETE statement will be saved to revert this script, so to remove all the data it has added. Must start with all caps DELETE.\n<code>-- TRUNCATE</code> - similar to above. Please note: TRUNCATE will fail if any other table has reference to this table, even when it's empty.\n<code>-- IGNORE</code> - the rest of the file will be ignored.</p>\n<p>Blocks are also supported. They are marked by a pair of lines:</p>\n<pre><span class=\"c1\">-- BEGIN keyword</span>\n <span class=\"p\">..</span> <span class=\"k\">some</span> <span class=\"k\">SQL</span> <span class=\"n\">code</span> <span class=\"n\">here</span><span class=\"p\">..</span>\n<span class=\"c1\">-- END keyword</span>\n</pre>\n<p>They should be properly nested.</p>\n<p>Supported blocks are:</p>\n<ul>\n<li><code>BEGIN INCREMENTAL</code> / <code>END INCREMENTAL</code> - marks part of the SQL that will normally be removed, and will be only used if <code>-i</code> (incremental flag) is used while loading. You can have many INCREMENTAL blocks. Example use:</li>\n</ul>\n<pre><span class=\"c1\">-- this scripts inserts into cookie jar</span>\n<span class=\"c1\">-- ORDER: 10</span>\n<span class=\"c1\">-- DELETE FROM jar</span>\n\n<span class=\"k\">INSERT</span> <span class=\"k\">INTO</span> <span class=\"n\">jar</span> <span class=\"p\">(</span><span class=\"n\">colour</span><span class=\"p\">,</span> <span class=\"k\">size</span><span class=\"p\">,</span> <span class=\"n\">external_id</span><span class=\"p\">)</span>\n<span class=\"k\">SELECT</span>\n  <span class=\"n\">cd</span><span class=\"p\">.</span><span class=\"n\">name</span><span class=\"p\">,</span> \n  <span class=\"k\">c</span><span class=\"p\">.</span><span class=\"k\">size</span><span class=\"p\">,</span>\n  <span class=\"k\">c</span><span class=\"p\">.</span><span class=\"n\">id</span>\n<span class=\"k\">FROM</span> <span class=\"n\">cookie</span> <span class=\"k\">c</span>\n<span class=\"k\">JOIN</span> <span class=\"n\">colour_dictionary</span> <span class=\"n\">cd</span> <span class=\"k\">ON</span> <span class=\"k\">c</span><span class=\"p\">.</span><span class=\"n\">colour_id</span> <span class=\"o\">=</span> <span class=\"n\">cd</span><span class=\"p\">.</span><span class=\"n\">id</span>\n\n<span class=\"c1\">-- when we do incremental, we exclude external_id's we already have:</span>\n<span class=\"c1\">-- BEGIN INCREMENTAL</span>\n<span class=\"k\">WHERE</span> <span class=\"k\">c</span><span class=\"p\">.</span><span class=\"n\">id</span> <span class=\"k\">NOT</span> <span class=\"k\">IN</span> <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"n\">external_id</span> <span class=\"k\">FROM</span> <span class=\"n\">jar</span><span class=\"p\">)</span>\n<span class=\"c1\">-- END INCREMENTAL</span>\n</pre>\n<ul>\n<li><code>BEGIN INITIAL</code> / <code>END INITIAL</code> - marks part of the SQL that will be removed in incremental <code>-i</code> mode.</li>\n</ul>\n<p>Enviroment variable interpolation. Similar to shell, simple variable interpolation is supported. <code>${FOO}</code> will be interpolated into value of FOO environment variable. This is currently used to insert the source schema name with <code>${SOURCE}</code>.</p>\n<h3>Metabase commands</h3>\n<p>These are the commands to export / import Metabase questions and dashboards that are related to the Keanu schema</p>\n<h4>Export</h4>\n<p><code>keanu.py metabase export</code> generates a JSON dump of all the data required to import the collections / dashboards / questions given in options.</p>\n<p>The generated JSON is printed on standard output. It contains a property <code>items</code> with the list of objects that got exported, as well as a property <code>mappings</code> with information to translate ids that make sense only in the context of a Metabase instance (table ids, card ids, etc).</p>\n<p>Options:</p>\n<ul>\n<li><code>-c \"Some collection\"</code> - Exports all the questions and dashboards contained in the named collection and its child collections. You should ensure that the dashboards refer only to questions contained within that collection, unless you plan to import the data within the same Metabase instance. The collection must exist.</li>\n</ul>\n<h4>Import</h4>\n<p><code>keanu.py metabase import</code> reads a JSON dump generated by the export command and import the object into a Metabase instance.</p>\n<p>Options:</p>\n<ul>\n<li><code>-c \"Some collection\"</code> - The name of the collection into which the objects should be imported. The collection must exist and be empty (no support for overwrite / merge at the moment).</li>\n<li><code>-j /path/to/file.json</code> - The JSON file to import.</li>\n</ul>\n<h2>Product vision</h2>\n<p>Right now during initial development, we run keanu command from the source\ndirectory <code>cli</code>, and the SQL scripts are kept in <code>sql</code>. The vision for the\nfuture product is the following:</p>\n<p>Keanu will be a command installed gloablly into the system, with <code>pip install keanu</code>. It will work like git or heroku, a multi-tool to do everything\nconntected to common DB model, ETL, running AI logic, installing dashboards in\nMetabase (this last function fits this command least, but let's see).</p>\n<p>It will download a repository of SQL files, properly divided between general\nuse, CiviCRM users, Identity users, etc, and be able to configure itself with\n<code>keanu setup</code> - saving information about source and target DBs. It will be able\nalso to upload SQL files that were changed and verion manage them somehow, to\nhelp in collaboration (this could be done using git).</p>\n<p>Similarly, it will enable import and export of Metabase questions (called\n<em>cards</em>) and dashboards. Not only their SQL, but also their JSON configuration\n(using <a href=\"https://github.com/stunitas/metabase-py\" rel=\"nofollow\">python Metabase API client\nlibrary</a>).</p>\n\n          </div>"}, "last_serial": 6929851, "releases": {"0.2": [{"comment_text": "", "digests": {"md5": "6c9c79389aff5789ed9bad9a0ff20b42", "sha256": "941f2bf71e5995d3b4f4cf920443b3553f1c196ce3dd40c134175e630d7f40ce"}, "downloads": -1, "filename": "keanu_etl-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "6c9c79389aff5789ed9bad9a0ff20b42", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 19722, "upload_time": "2020-01-15T14:18:33", "upload_time_iso_8601": "2020-01-15T14:18:33.039418Z", "url": "https://files.pythonhosted.org/packages/c7/56/f6861e4ab06b203f27d88458b714ca884e454f86b7c9246648c647d9a85f/keanu_etl-0.2-py3-none-any.whl", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "6607116c93a5225d91b5965614dfe495", "sha256": "8da830b4b6980a8a5b9beff9157d550f7ada179846f144eef9e57f9bc55197b6"}, "downloads": -1, "filename": "keanu_etl-0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "6607116c93a5225d91b5965614dfe495", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 20495, "upload_time": "2020-01-27T11:14:20", "upload_time_iso_8601": "2020-01-27T11:14:20.388110Z", "url": "https://files.pythonhosted.org/packages/ef/89/36128371c06543f695177a3125bc1fa2d7cb431c7eea7f443388ebf7892d/keanu_etl-0.3-py3-none-any.whl", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "2e0bb10eca5478565aed97305b9ad861", "sha256": "d4658afb529db948bb743599e9caf68665a61e17f0c2541acab4bae960621ff9"}, "downloads": -1, "filename": "keanu_etl-0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "2e0bb10eca5478565aed97305b9ad861", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 25591, "upload_time": "2020-03-12T09:54:25", "upload_time_iso_8601": "2020-03-12T09:54:25.214124Z", "url": "https://files.pythonhosted.org/packages/5d/20/0932c3b95fdfcb8510f604f8c4fcc366d8fb95c8c0191e651b7a0a96fc46/keanu_etl-0.4-py3-none-any.whl", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "27847492160f5a561ce65fbafed2149c", "sha256": "8a0fd0762de64c4cd33a6c4e1a2bd4f0b9b5558e74c201c8db332604ac3081fa"}, "downloads": -1, "filename": "keanu_etl-0.4.1-py3-none-any.whl", "has_sig": false, "md5_digest": "27847492160f5a561ce65fbafed2149c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 25889, "upload_time": "2020-03-12T11:08:54", "upload_time_iso_8601": "2020-03-12T11:08:54.210361Z", "url": "https://files.pythonhosted.org/packages/b5/2f/e536640dcbbeed9ed2fae93b1a16a4e0c291e5b96421f4513ab919bb0879/keanu_etl-0.4.1-py3-none-any.whl", "yanked": false}], "0.4.2": [{"comment_text": "", "digests": {"md5": "fabae720481b1f4ddc643671b7d996d4", "sha256": "0b0fad01b064980bec2b11a85ddb5fb5798ebd7ccc5f9a7a360999bc89349057"}, "downloads": -1, "filename": "keanu_etl-0.4.2-py3-none-any.whl", "has_sig": false, "md5_digest": "fabae720481b1f4ddc643671b7d996d4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 25887, "upload_time": "2020-03-12T11:11:16", "upload_time_iso_8601": "2020-03-12T11:11:16.820254Z", "url": "https://files.pythonhosted.org/packages/84/c9/ae90a266708b82bcf6b9269eb8278ce5fff1a616b144933f0d5b77f9a4fa/keanu_etl-0.4.2-py3-none-any.whl", "yanked": false}], "0.5.0": [{"comment_text": "", "digests": {"md5": "b58bc2deaee1e6699bf7bf122991feef", "sha256": "078cc372ace108658744625089ee8967d2d4d4de3c0602347855c08968595fa1"}, "downloads": -1, "filename": "keanu_etl-0.5.0-py3-none-any.whl", "has_sig": false, "md5_digest": "b58bc2deaee1e6699bf7bf122991feef", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 27184, "upload_time": "2020-03-24T20:29:14", "upload_time_iso_8601": "2020-03-24T20:29:14.135439Z", "url": "https://files.pythonhosted.org/packages/0f/b8/dd915387e997d556eae185c18557179487a0c6eedeec50010255d766a3c8/keanu_etl-0.5.0-py3-none-any.whl", "yanked": false}], "0.5.1": [{"comment_text": "", "digests": {"md5": "5a2f9e12c0afdd25d88cf87dbc8a5475", "sha256": "d015ea01c70c71a525d0d3010b1f3590103b8b6e8878a604b7e93f8d9a33403c"}, "downloads": -1, "filename": "keanu_etl-0.5.1-py3-none-any.whl", "has_sig": false, "md5_digest": "5a2f9e12c0afdd25d88cf87dbc8a5475", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 27883, "upload_time": "2020-03-31T12:09:43", "upload_time_iso_8601": "2020-03-31T12:09:43.304593Z", "url": "https://files.pythonhosted.org/packages/2b/65/8f4147351ceffc4a8eefd6de77e31e27bcc0329b8abfff71258a7012fe74/keanu_etl-0.5.1-py3-none-any.whl", "yanked": false}], "0.5.2": [{"comment_text": "", "digests": {"md5": "b6542d8ea5225be43871e404a72b1174", "sha256": "41411a5c02bf526ed09c1b4ce738ec2a8ee56f381521c4a43cfc5b1325efa1ac"}, "downloads": -1, "filename": "keanu_etl-0.5.2-py3-none-any.whl", "has_sig": false, "md5_digest": "b6542d8ea5225be43871e404a72b1174", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 27882, "upload_time": "2020-04-01T18:42:51", "upload_time_iso_8601": "2020-04-01T18:42:51.202538Z", "url": "https://files.pythonhosted.org/packages/ee/06/b8139bf8a114998cbc93f28e60dab162874ec6f6236075f77c2ccc134739/keanu_etl-0.5.2-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b6542d8ea5225be43871e404a72b1174", "sha256": "41411a5c02bf526ed09c1b4ce738ec2a8ee56f381521c4a43cfc5b1325efa1ac"}, "downloads": -1, "filename": "keanu_etl-0.5.2-py3-none-any.whl", "has_sig": false, "md5_digest": "b6542d8ea5225be43871e404a72b1174", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 27882, "upload_time": "2020-04-01T18:42:51", "upload_time_iso_8601": "2020-04-01T18:42:51.202538Z", "url": "https://files.pythonhosted.org/packages/ee/06/b8139bf8a114998cbc93f28e60dab162874ec6f6236075f77c2ccc134739/keanu_etl-0.5.2-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 00:50:25 2020"}