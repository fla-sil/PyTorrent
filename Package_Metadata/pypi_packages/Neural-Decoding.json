{"info": {"author": "", "author_email": "", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved", "Operating System :: MacOS", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX", "Operating System :: Unix", "Programming Language :: Python", "Topic :: Scientific/Engineering", "Topic :: Software Development"], "description": "# Neural_Decoding:\n\n### A python package that includes many methods for decoding neural activity\n\nThe package contains a mixture of classic decoding methods (Wiener Filter, Wiener Cascade, Kalman Filter, Naive Bayes, Support Vector Regression) and modern machine learning methods (XGBoost, Dense Neural Network, Recurrent Neural Net, GRU, LSTM).\n\nThe decoders are currently designed to predict continuously valued output. In the future, we will modify the functions to also allow classification. \n\n## Our manuscript and datasets\nThis package accompanies a [manuscript](https://arxiv.org/abs/1708.00909) that compares the performance of these methods on several datasets. We would appreciate if you cite that manuscript if you use our code or data for your research.\n\nCode used for the paper is in the \"Paper_code\" folder. It is described further at the bottom of this read-me.\n\nAll 3 datasets (motor cortex, somatosensory cortex, and hippocampus) used in the paper can be downloaded [here](https://www.dropbox.com/sh/n4924ipcfjqc0t6/AACPWjxDKPEzQiXKUUFriFkJa?dl=0). They are in both matlab and python formats, and can be used in the example files described below.\n\n## Installation\n\nThis package can be installed via `pip` at the command line by typing\n```buildoutcfg\npip install Neural-Decoding\n```\nor manually via\n```buildoutcfg\ngit clone https://github.com/KordingLab/Neural_Decoding.git\ncd Neural_Decoding\npython setup.py install\n```\nYou'll have to install each dependency yourself if you install manually. We've designed the code so that not all machine learning packages\nneed to be installed for the others to work.\n\n## Dependencies\nAll packages will be installed automatically when installing from `pip` (because of the `requirements.txt` file).\n\nIf installing manually via `python setup.py install`:\nIn order to run all the decoders based on neural networks, you need to install [Keras](https://keras.io/#installation) <br>\nIn order to run the XGBoost Decoder, you need to install [XGBoost](https://pypi.python.org/pypi/xgboost/) <br>\nIn order to run the Wiener Filter, Wiener Cascade, or Support Vector Regression you will need [scikit-learn](http://scikit-learn.org/stable/install.html). <br>\nIn order to do hyperparameter optimization, you need to install [BayesianOptimization](https://github.com/fmfn/BayesianOptimization)\n\n## Getting started\nWe have included jupyter notebooks that provide detailed examples of how to use the decoders.\n - The file \"Examples_kf_decoder\" is for the Kalman filter decoder and the file \"Examples_all_decoders\" is for all other decoders. These examples work well with the somatosensory and motor cortex datasets.\n - There are minor differences in the hippocampus dataset, so we have included a folder, \"Examples_hippocampus\", with analogous example files. This folder also includes an example file for using the Naive Bayes decoder (since it works much better on our hippocampus dataset).\n - We have also included a notebook, \"Example_hyperparam_opt\", that demonstrates how to do hyperparameter optimization for the decoders.\n\nHere we provide a basic example where we are using a LSTM decoder. <br>\nFor this example we assume we have already loaded matrices:\n - \"neural_data\": a matrix of size \"total number of time bins\" x \"number of neurons,\" where each entry is the firing rate of a given neuron in a given time bin.\n - \"y\": the output variable that you are decoding (e.g. velocity), and is a matrix of size \"total number of time bins\" x \"number of features you are decoding.\"  <br>\n\nWe have provided a jupyter notebook, \"Example_format_data\" with an example of how to get Matlab data into this format.\n<br>\n\nFirst we will import the necessary functions\n```python\nfrom Neural_Decoding.decoders import LSTMDecoder #Import LSTM decoder\nfrom Neural_Decoding.preprocessing_funcs import get_spikes_with_history #Import function to get the covariate matrix that includes spike history from previous bins\n```\nNext, we will define the time period we are using spikes from (relative to the output we are decoding)\n```python\nbins_before=13 #How many bins of neural data prior to the output are used for decoding\nbins_current=1 #Whether to use concurrent time bin of neural data\nbins_after=0 #How many bins of neural data after the output are used for decoding\n```\n\nNext, we will compute the covariate matrix that includes the spike history from previous bins\n```python\n# Function to get the covariate matrix that includes spike history from previous bins\nX=get_spikes_with_history(neural_data,bins_before,bins_after,bins_current)\n```\nIn this basic example, we will ignore some additional preprocessing we do in the example notebooks. Let's assume we have now divided the data into a training set (X_train, y_train) and a testing set (X_test,y_test).\n\nWe will now finally train and test the decoder:\n```python\n#Declare model and set parameters of the model\nmodel_lstm=LSTMDecoder(units=400,num_epochs=5)\n\n#Fit model\nmodel_lstm.fit(X_train,y_train)\n\n#Get predictions\ny_test_predicted_lstm=model_lstm.predict(X_test)\n```\n\n## What's Included\nThere are 3 files with functions. An overview of the functions are below. More details can be found in the comments within the files.\n\n### decoders.py:\nThis file provides all of the decoders. Each decoder is a class with functions \"fit\" and \"predict\".\n\nFirst, we will describe the format of data that is necessary for the decoders\n- For all the decoders, you will need to decide the time period of spikes (relative to the output) that you are using for decoding.\n- For all the decoders other than the Kalman filter, you can set \"bins_before\" (the number of bins of spikes preceding the output), \"bins_current\" (whether to use the bin of spikes concurrent with the output), and \"bins_after\" (the number of bins of spikes after the output). Let \"surrounding_bins\" = bins_before+bins_current+bins_after. This allows us to get a 3d covariate matrix \"X\" that has size \"total number of time bins\" x \"surrounding_bins\" x \"number of neurons.\" We use this input format for the recurrent neural networks (SimpleRNN, GRU, LSTM). We can also flatten the matrix, so that there is a vector of features for every time bin, to get \"X_flat\" which is a 2d matrix of size \"total number of time bins\" x \"surrounding_bins x number of neurons.\" This input format is used for the Wiener Filter, Wiener Cascade, Support Vector Regression, XGBoost, and Dense Neural Net.\n- For the Kalman filter, you can set the \"lag\" - what time bin of the neural data (relative to the output) is used to predict the output. The input format for the Kalman filter is simply the 2d matrix of size \"total number of time bins\" x \"number of neurons,\" where each entry is the firing rate of a given neuron in a given time bin.\n- The output, \"y\" is a 2d matrix of size \"total number of time bins\" x \"number of output features.\"\n\n<br> Here are all the decoders within \"decoders.py\":\n1. **WienerFilterDecoder**\n - The Wiener Filter is simply multiple linear regression using X_flat as an input.\n - It has no input parameters\n2. **WienerCascadeDecoder**\n - The Wiener Cascade (also known as a linear nonlinear model) fits a linear regression (the Wiener filter) followed by fitting a static nonlearity.\n - It has parameter *degree* (the degree of the polynomial used for the nonlinearity)\n3. **KalmanFilterDecoder**\n - We used a Kalman filter similar to that implemented in [Wu et al. 2003](https://papers.nips.cc/paper/2178-neural-decoding-of-cursor-motion-using-a-kalman-filter.pdf). In the Kalman filter, the measurement was the neural spike trains, and the hidden state was the kinematics.\n - We have one parameter *C* (which is not in the previous implementation). This parameter scales the noise matrix associated with the transition in kinematic states. It effectively allows changing the weight of the new neural evidence in the current update.\n4. **NaiveBayesDecoder**\n - We used a Naive Bayes decoder similar to that implemented in [Zhang et al. 1998](https://www.physiology.org/doi/abs/10.1152/jn.1998.79.2.1017) (see manuscript for details).\n - It has parameters *encoding_model* (for either a linear or quadratic encoding model) and *res* (to set the resolution of predicted values)\n5. **SVRDecoder**\n - This decoder uses support vector regression using X_flat as an input.\n - It has parameters *C* (the penalty of the error term) and *max_iter* (the maximum number of iterations).\n - It works best when the output (\"y\") has been normalized\n6. **XGBoostDecoder**\n - We used the Extreme Gradient Boosting [XGBoost](http://xgboost.readthedocs.io/en/latest/model.html) algorithm to relate X_flat to the outputs. XGBoost is based on the idea of boosted trees.\n - It has parameters *max_depth* (the maximum depth of the trees), *num_round* (the number of trees that are fit), *eta* (the learning rate), and *gpu* (if you have the [gpu version](https://github.com/dmlc/xgboost/tree/master/plugin/updater_gpu) of XGBoost installed, you can select which gpu to use)\n7. **DenseNNDecoder**\n - Using the Keras library, we created a dense feedforward neural network that uses X_flat to predict the outputs. It can have any number of hidden layers.\n - It has parameters *units* (the number of units in each layer), *dropout* (the proportion of units that get dropped out), *num_epochs* (the number of epochs used for training), and *verbose* (whether to display progress of the fit after each epoch)\n8. **SimpleRNNDecoder**\n - Using the Keras library, we created a neural network architecture where the spiking input (from matrix X) was fed into a standard recurrent neural network (RNN) with a relu activation. The units from this recurrent layer were fully connected to the output layer.\n - It has parameters *units*, *dropout*, *num_epochs*, and *verbose*\n9. **GRUDecoder**\n - Using the Keras library, we created a neural network architecture where the spiking input (from matrix X) was fed into a network of gated recurrent units (GRUs; a more sophisticated RNN). The units from this recurrent layer were fully connected to the output layer.\n - It has parameters *units*, *dropout*, *num_epochs*, and *verbose*\n10. **LSTMDecoder**\n - All methods were the same as for the GRUDecoder, except  Long Short Term Memory networks (LSTMs; another more sophisticated RNN) were used rather than GRUs.\n - It has parameters *units*, *dropout*, *num_epochs*, and *verbose*\n\nWhen designing the XGBoost and neural network decoders, there were many additional parameters that could have been utilized (e.g. regularization). To simplify ease of use, we only included parameters that were sufficient for producing good fits.\n\n### metrics.py:\nThe file has functions for metrics to evaluate model fit. It currently has functions to calculate:\n - ![equation](https://latex.codecogs.com/gif.latex?%24R%5E2%3D1-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%7D%5Cfrac%7B%5Cleft%28y_i-%5Cwidehat%7By_i%7D%20%5Cright%20%29%5E2%7D%7B%5Cleft%28y_i-%5Cbar%7By_i%7D%20%5Cright%20%29%5E2%7D)\n - ![equation](https://latex.codecogs.com/gif.latex?%24%5Crho%24) : The pearson correlation coefficient\n\n### preprocessing_funcs.py\nThe file contains functions for preprocessing data that may be useful for putting the neural activity and outputs in the correct format for our decoding functions\n - **bin_spikes**: converts spike times to the number of spikes within time bins\n - **bin_output**: converts a continuous stream of outputs to the average output within time bins\n - **get_spikes_with_history**: using binned spikes as input, this function creates a covariate matrix of neural data that incorporates spike history\n\n## Paper code\nIn the folder \"Paper_code\", we include code used for the manuscript.\n - Files starting with \"ManyDecoders\" use all decoders except the Kalman Filter and Naive Bayes\n - Files starting with \"KF\" use the Kalman filter\n - Files starting with \"BayesDecoder\" use the Naive Bayes decoder\n - Files starting with \"Plot\" create the figures in the paper\n - Files ending with \"FullData\" are for figures 3/4\n - Files ending with \"DataAmt\" are for figures 5/6\n - Files ending with \"FewNeurons\" are for figure 7\n - Files ending with \"BinSize\" are for figure 8\n - Files mentioning \"Hyperparams\" are for figure 9\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/KordingLab/Neural_Decoding.git", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "BSD 3-Clause License", "maintainer": "Josh Glaser", "maintainer_email": "joshglaser88@gmail.com", "name": "Neural-Decoding", "package_url": "https://pypi.org/project/Neural-Decoding/", "platform": "any", "project_url": "https://pypi.org/project/Neural-Decoding/", "project_urls": {"Download": "https://github.com/KordingLab/Neural_Decoding.git"}, "release_url": "https://pypi.org/project/Neural-Decoding/0.1.4/", "requires_dist": ["xgboost (>=0.9.0)", "statsmodels (>=0.9.0)", "scipy (>=1.2.1)", "numpy (>=1.16.3)", "keras (>=2.2.4)", "scikit-learn (>=0.21.2)", "xgboost (>=0.90)"], "requires_python": "", "summary": "A python package that includes many methods for decoding neural activity.", "version": "0.1.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Neural_Decoding:</h1>\n<h3>A python package that includes many methods for decoding neural activity</h3>\n<p>The package contains a mixture of classic decoding methods (Wiener Filter, Wiener Cascade, Kalman Filter, Naive Bayes, Support Vector Regression) and modern machine learning methods (XGBoost, Dense Neural Network, Recurrent Neural Net, GRU, LSTM).</p>\n<p>The decoders are currently designed to predict continuously valued output. In the future, we will modify the functions to also allow classification.</p>\n<h2>Our manuscript and datasets</h2>\n<p>This package accompanies a <a href=\"https://arxiv.org/abs/1708.00909\" rel=\"nofollow\">manuscript</a> that compares the performance of these methods on several datasets. We would appreciate if you cite that manuscript if you use our code or data for your research.</p>\n<p>Code used for the paper is in the \"Paper_code\" folder. It is described further at the bottom of this read-me.</p>\n<p>All 3 datasets (motor cortex, somatosensory cortex, and hippocampus) used in the paper can be downloaded <a href=\"https://www.dropbox.com/sh/n4924ipcfjqc0t6/AACPWjxDKPEzQiXKUUFriFkJa?dl=0\" rel=\"nofollow\">here</a>. They are in both matlab and python formats, and can be used in the example files described below.</p>\n<h2>Installation</h2>\n<p>This package can be installed via <code>pip</code> at the command line by typing</p>\n<pre>pip install Neural-Decoding\n</pre>\n<p>or manually via</p>\n<pre>git clone https://github.com/KordingLab/Neural_Decoding.git\ncd Neural_Decoding\npython setup.py install\n</pre>\n<p>You'll have to install each dependency yourself if you install manually. We've designed the code so that not all machine learning packages\nneed to be installed for the others to work.</p>\n<h2>Dependencies</h2>\n<p>All packages will be installed automatically when installing from <code>pip</code> (because of the <code>requirements.txt</code> file).</p>\n<p>If installing manually via <code>python setup.py install</code>:\nIn order to run all the decoders based on neural networks, you need to install <a href=\"https://keras.io/#installation\" rel=\"nofollow\">Keras</a> <br>\nIn order to run the XGBoost Decoder, you need to install <a href=\"https://pypi.python.org/pypi/xgboost/\" rel=\"nofollow\">XGBoost</a> <br>\nIn order to run the Wiener Filter, Wiener Cascade, or Support Vector Regression you will need <a href=\"http://scikit-learn.org/stable/install.html\" rel=\"nofollow\">scikit-learn</a>. <br>\nIn order to do hyperparameter optimization, you need to install <a href=\"https://github.com/fmfn/BayesianOptimization\" rel=\"nofollow\">BayesianOptimization</a></p>\n<h2>Getting started</h2>\n<p>We have included jupyter notebooks that provide detailed examples of how to use the decoders.</p>\n<ul>\n<li>The file \"Examples_kf_decoder\" is for the Kalman filter decoder and the file \"Examples_all_decoders\" is for all other decoders. These examples work well with the somatosensory and motor cortex datasets.</li>\n<li>There are minor differences in the hippocampus dataset, so we have included a folder, \"Examples_hippocampus\", with analogous example files. This folder also includes an example file for using the Naive Bayes decoder (since it works much better on our hippocampus dataset).</li>\n<li>We have also included a notebook, \"Example_hyperparam_opt\", that demonstrates how to do hyperparameter optimization for the decoders.</li>\n</ul>\n<p>Here we provide a basic example where we are using a LSTM decoder. <br>\nFor this example we assume we have already loaded matrices:</p>\n<ul>\n<li>\"neural_data\": a matrix of size \"total number of time bins\" x \"number of neurons,\" where each entry is the firing rate of a given neuron in a given time bin.</li>\n<li>\"y\": the output variable that you are decoding (e.g. velocity), and is a matrix of size \"total number of time bins\" x \"number of features you are decoding.\"  <br></li>\n</ul>\n<p>We have provided a jupyter notebook, \"Example_format_data\" with an example of how to get Matlab data into this format.\n<br></p>\n<p>First we will import the necessary functions</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">Neural_Decoding.decoders</span> <span class=\"kn\">import</span> <span class=\"n\">LSTMDecoder</span> <span class=\"c1\">#Import LSTM decoder</span>\n<span class=\"kn\">from</span> <span class=\"nn\">Neural_Decoding.preprocessing_funcs</span> <span class=\"kn\">import</span> <span class=\"n\">get_spikes_with_history</span> <span class=\"c1\">#Import function to get the covariate matrix that includes spike history from previous bins</span>\n</pre>\n<p>Next, we will define the time period we are using spikes from (relative to the output we are decoding)</p>\n<pre><span class=\"n\">bins_before</span><span class=\"o\">=</span><span class=\"mi\">13</span> <span class=\"c1\">#How many bins of neural data prior to the output are used for decoding</span>\n<span class=\"n\">bins_current</span><span class=\"o\">=</span><span class=\"mi\">1</span> <span class=\"c1\">#Whether to use concurrent time bin of neural data</span>\n<span class=\"n\">bins_after</span><span class=\"o\">=</span><span class=\"mi\">0</span> <span class=\"c1\">#How many bins of neural data after the output are used for decoding</span>\n</pre>\n<p>Next, we will compute the covariate matrix that includes the spike history from previous bins</p>\n<pre><span class=\"c1\"># Function to get the covariate matrix that includes spike history from previous bins</span>\n<span class=\"n\">X</span><span class=\"o\">=</span><span class=\"n\">get_spikes_with_history</span><span class=\"p\">(</span><span class=\"n\">neural_data</span><span class=\"p\">,</span><span class=\"n\">bins_before</span><span class=\"p\">,</span><span class=\"n\">bins_after</span><span class=\"p\">,</span><span class=\"n\">bins_current</span><span class=\"p\">)</span>\n</pre>\n<p>In this basic example, we will ignore some additional preprocessing we do in the example notebooks. Let's assume we have now divided the data into a training set (X_train, y_train) and a testing set (X_test,y_test).</p>\n<p>We will now finally train and test the decoder:</p>\n<pre><span class=\"c1\">#Declare model and set parameters of the model</span>\n<span class=\"n\">model_lstm</span><span class=\"o\">=</span><span class=\"n\">LSTMDecoder</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"o\">=</span><span class=\"mi\">400</span><span class=\"p\">,</span><span class=\"n\">num_epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#Fit model</span>\n<span class=\"n\">model_lstm</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span><span class=\"n\">y_train</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#Get predictions</span>\n<span class=\"n\">y_test_predicted_lstm</span><span class=\"o\">=</span><span class=\"n\">model_lstm</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">)</span>\n</pre>\n<h2>What's Included</h2>\n<p>There are 3 files with functions. An overview of the functions are below. More details can be found in the comments within the files.</p>\n<h3>decoders.py:</h3>\n<p>This file provides all of the decoders. Each decoder is a class with functions \"fit\" and \"predict\".</p>\n<p>First, we will describe the format of data that is necessary for the decoders</p>\n<ul>\n<li>For all the decoders, you will need to decide the time period of spikes (relative to the output) that you are using for decoding.</li>\n<li>For all the decoders other than the Kalman filter, you can set \"bins_before\" (the number of bins of spikes preceding the output), \"bins_current\" (whether to use the bin of spikes concurrent with the output), and \"bins_after\" (the number of bins of spikes after the output). Let \"surrounding_bins\" = bins_before+bins_current+bins_after. This allows us to get a 3d covariate matrix \"X\" that has size \"total number of time bins\" x \"surrounding_bins\" x \"number of neurons.\" We use this input format for the recurrent neural networks (SimpleRNN, GRU, LSTM). We can also flatten the matrix, so that there is a vector of features for every time bin, to get \"X_flat\" which is a 2d matrix of size \"total number of time bins\" x \"surrounding_bins x number of neurons.\" This input format is used for the Wiener Filter, Wiener Cascade, Support Vector Regression, XGBoost, and Dense Neural Net.</li>\n<li>For the Kalman filter, you can set the \"lag\" - what time bin of the neural data (relative to the output) is used to predict the output. The input format for the Kalman filter is simply the 2d matrix of size \"total number of time bins\" x \"number of neurons,\" where each entry is the firing rate of a given neuron in a given time bin.</li>\n<li>The output, \"y\" is a 2d matrix of size \"total number of time bins\" x \"number of output features.\"</li>\n</ul>\n<p><br> Here are all the decoders within \"decoders.py\":</p>\n<ol>\n<li><strong>WienerFilterDecoder</strong></li>\n</ol>\n<ul>\n<li>The Wiener Filter is simply multiple linear regression using X_flat as an input.</li>\n<li>It has no input parameters</li>\n</ul>\n<ol>\n<li><strong>WienerCascadeDecoder</strong></li>\n</ol>\n<ul>\n<li>The Wiener Cascade (also known as a linear nonlinear model) fits a linear regression (the Wiener filter) followed by fitting a static nonlearity.</li>\n<li>It has parameter <em>degree</em> (the degree of the polynomial used for the nonlinearity)</li>\n</ul>\n<ol>\n<li><strong>KalmanFilterDecoder</strong></li>\n</ol>\n<ul>\n<li>We used a Kalman filter similar to that implemented in <a href=\"https://papers.nips.cc/paper/2178-neural-decoding-of-cursor-motion-using-a-kalman-filter.pdf\" rel=\"nofollow\">Wu et al. 2003</a>. In the Kalman filter, the measurement was the neural spike trains, and the hidden state was the kinematics.</li>\n<li>We have one parameter <em>C</em> (which is not in the previous implementation). This parameter scales the noise matrix associated with the transition in kinematic states. It effectively allows changing the weight of the new neural evidence in the current update.</li>\n</ul>\n<ol>\n<li><strong>NaiveBayesDecoder</strong></li>\n</ol>\n<ul>\n<li>We used a Naive Bayes decoder similar to that implemented in <a href=\"https://www.physiology.org/doi/abs/10.1152/jn.1998.79.2.1017\" rel=\"nofollow\">Zhang et al. 1998</a> (see manuscript for details).</li>\n<li>It has parameters <em>encoding_model</em> (for either a linear or quadratic encoding model) and <em>res</em> (to set the resolution of predicted values)</li>\n</ul>\n<ol>\n<li><strong>SVRDecoder</strong></li>\n</ol>\n<ul>\n<li>This decoder uses support vector regression using X_flat as an input.</li>\n<li>It has parameters <em>C</em> (the penalty of the error term) and <em>max_iter</em> (the maximum number of iterations).</li>\n<li>It works best when the output (\"y\") has been normalized</li>\n</ul>\n<ol>\n<li><strong>XGBoostDecoder</strong></li>\n</ol>\n<ul>\n<li>We used the Extreme Gradient Boosting <a href=\"http://xgboost.readthedocs.io/en/latest/model.html\" rel=\"nofollow\">XGBoost</a> algorithm to relate X_flat to the outputs. XGBoost is based on the idea of boosted trees.</li>\n<li>It has parameters <em>max_depth</em> (the maximum depth of the trees), <em>num_round</em> (the number of trees that are fit), <em>eta</em> (the learning rate), and <em>gpu</em> (if you have the <a href=\"https://github.com/dmlc/xgboost/tree/master/plugin/updater_gpu\" rel=\"nofollow\">gpu version</a> of XGBoost installed, you can select which gpu to use)</li>\n</ul>\n<ol>\n<li><strong>DenseNNDecoder</strong></li>\n</ol>\n<ul>\n<li>Using the Keras library, we created a dense feedforward neural network that uses X_flat to predict the outputs. It can have any number of hidden layers.</li>\n<li>It has parameters <em>units</em> (the number of units in each layer), <em>dropout</em> (the proportion of units that get dropped out), <em>num_epochs</em> (the number of epochs used for training), and <em>verbose</em> (whether to display progress of the fit after each epoch)</li>\n</ul>\n<ol>\n<li><strong>SimpleRNNDecoder</strong></li>\n</ol>\n<ul>\n<li>Using the Keras library, we created a neural network architecture where the spiking input (from matrix X) was fed into a standard recurrent neural network (RNN) with a relu activation. The units from this recurrent layer were fully connected to the output layer.</li>\n<li>It has parameters <em>units</em>, <em>dropout</em>, <em>num_epochs</em>, and <em>verbose</em></li>\n</ul>\n<ol>\n<li><strong>GRUDecoder</strong></li>\n</ol>\n<ul>\n<li>Using the Keras library, we created a neural network architecture where the spiking input (from matrix X) was fed into a network of gated recurrent units (GRUs; a more sophisticated RNN). The units from this recurrent layer were fully connected to the output layer.</li>\n<li>It has parameters <em>units</em>, <em>dropout</em>, <em>num_epochs</em>, and <em>verbose</em></li>\n</ul>\n<ol>\n<li><strong>LSTMDecoder</strong></li>\n</ol>\n<ul>\n<li>All methods were the same as for the GRUDecoder, except  Long Short Term Memory networks (LSTMs; another more sophisticated RNN) were used rather than GRUs.</li>\n<li>It has parameters <em>units</em>, <em>dropout</em>, <em>num_epochs</em>, and <em>verbose</em></li>\n</ul>\n<p>When designing the XGBoost and neural network decoders, there were many additional parameters that could have been utilized (e.g. regularization). To simplify ease of use, we only included parameters that were sufficient for producing good fits.</p>\n<h3>metrics.py:</h3>\n<p>The file has functions for metrics to evaluate model fit. It currently has functions to calculate:</p>\n<ul>\n<li><img alt=\"equation\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/980a0259ce8ed2ddaec0f4bba50d156009957f57/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f2532345225354532253344312d25354373756d5f25374269253344312537442535452537426e253744253742253744253543667261632537422535436c656674253238795f692d25354377696465686174253742795f692537442532302535437269676874253230253239253545322537442537422535436c656674253238795f692d253543626172253742795f69253744253230253543726967687425323025323925354532253744\"></li>\n<li><img alt=\"equation\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a7f4398f4029c15ee61e541cbeff07c8dbb0f80b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25323425354372686f253234\"> : The pearson correlation coefficient</li>\n</ul>\n<h3>preprocessing_funcs.py</h3>\n<p>The file contains functions for preprocessing data that may be useful for putting the neural activity and outputs in the correct format for our decoding functions</p>\n<ul>\n<li><strong>bin_spikes</strong>: converts spike times to the number of spikes within time bins</li>\n<li><strong>bin_output</strong>: converts a continuous stream of outputs to the average output within time bins</li>\n<li><strong>get_spikes_with_history</strong>: using binned spikes as input, this function creates a covariate matrix of neural data that incorporates spike history</li>\n</ul>\n<h2>Paper code</h2>\n<p>In the folder \"Paper_code\", we include code used for the manuscript.</p>\n<ul>\n<li>Files starting with \"ManyDecoders\" use all decoders except the Kalman Filter and Naive Bayes</li>\n<li>Files starting with \"KF\" use the Kalman filter</li>\n<li>Files starting with \"BayesDecoder\" use the Naive Bayes decoder</li>\n<li>Files starting with \"Plot\" create the figures in the paper</li>\n<li>Files ending with \"FullData\" are for figures 3/4</li>\n<li>Files ending with \"DataAmt\" are for figures 5/6</li>\n<li>Files ending with \"FewNeurons\" are for figure 7</li>\n<li>Files ending with \"BinSize\" are for figure 8</li>\n<li>Files mentioning \"Hyperparams\" are for figure 9</li>\n</ul>\n\n          </div>"}, "last_serial": 5528074, "releases": {"0.1.2.dev0": [{"comment_text": "", "digests": {"md5": "09840bd57736289104bc30c91c17eaff", "sha256": "2d700ec44631512d27060656c25c2732e62f7c38133e4abf4a4b391027acb1f2"}, "downloads": -1, "filename": "Neural_Decoding-0.1.2.dev0-py3-none-any.whl", "has_sig": false, "md5_digest": "09840bd57736289104bc30c91c17eaff", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 17200, "upload_time": "2019-07-11T14:26:43", "upload_time_iso_8601": "2019-07-11T14:26:43.327118Z", "url": "https://files.pythonhosted.org/packages/1e/2b/9c943ec877aa78c0ccde85a68d047c01241ac79440b25ee86984f6850a9e/Neural_Decoding-0.1.2.dev0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bf340a7297de9820fb78fa24ccb6d62b", "sha256": "d052c96efe02d4e2f4f4cd9a77fdf34ca804df8cfe34b880df26919df6874fa1"}, "downloads": -1, "filename": "Neural Decoding-0.1.2.dev0.tar.gz", "has_sig": false, "md5_digest": "bf340a7297de9820fb78fa24ccb6d62b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15379, "upload_time": "2019-07-11T14:26:44", "upload_time_iso_8601": "2019-07-11T14:26:44.869021Z", "url": "https://files.pythonhosted.org/packages/41/22/8034bd456038ae9808a87774e70e0599d0277f7066ad91b2082cf43c1538/Neural%20Decoding-0.1.2.dev0.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "e57fb3cd5ba65f7cdaf581b21da611a9", "sha256": "bb9fbfdd8660e65bc1d26c1491fb3c6763290a97e50551e209d0c628da594746"}, "downloads": -1, "filename": "Neural_Decoding-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "e57fb3cd5ba65f7cdaf581b21da611a9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18220, "upload_time": "2019-07-13T16:47:14", "upload_time_iso_8601": "2019-07-13T16:47:14.340035Z", "url": "https://files.pythonhosted.org/packages/f3/c3/e10905b9eaf671852347377fd8b1ca6746fd4ce8e547211060c78c09aa3d/Neural_Decoding-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "630b4c8f25b8d2c38211330dbacc96a7", "sha256": "222421e11b45e9ff5e2592510ec32747a3a19cd85f756e1433cc5c24a3e3d3a6"}, "downloads": -1, "filename": "Neural Decoding-0.1.3.tar.gz", "has_sig": false, "md5_digest": "630b4c8f25b8d2c38211330dbacc96a7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16370, "upload_time": "2019-07-13T16:47:15", "upload_time_iso_8601": "2019-07-13T16:47:15.648713Z", "url": "https://files.pythonhosted.org/packages/d8/80/d57639dfeefa3b49f443f798ba0ed63c0dada24535f441458818230b15e3/Neural%20Decoding-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "68eb74d5cffbce8d2d73ee64db7640d0", "sha256": "5fe5d4af3ea17dfd744f47c21ac06b4540dd5875d57243d57f9d15e9fb6f6529"}, "downloads": -1, "filename": "Neural_Decoding-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "68eb74d5cffbce8d2d73ee64db7640d0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18436, "upload_time": "2019-07-13T22:17:01", "upload_time_iso_8601": "2019-07-13T22:17:01.838606Z", "url": "https://files.pythonhosted.org/packages/4e/2a/cedfcd392b72c30dcd6646dbb34661adbad289462a9a49afa53803b68993/Neural_Decoding-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3dd7b0db8f6d8472d613fa4152f57860", "sha256": "f49439a55b9c3040d1d5902be8103ea88dbddd92ff6cb11852d6dfb46cbc847b"}, "downloads": -1, "filename": "Neural Decoding-0.1.4.tar.gz", "has_sig": false, "md5_digest": "3dd7b0db8f6d8472d613fa4152f57860", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16594, "upload_time": "2019-07-13T22:17:04", "upload_time_iso_8601": "2019-07-13T22:17:04.410604Z", "url": "https://files.pythonhosted.org/packages/de/0b/53dcd98904ce77471f24800e0cc4b7d64aa5b1f3aeef1b066ede12670e72/Neural%20Decoding-0.1.4.tar.gz", "yanked": false}], "0.1.dev0": [{"comment_text": "", "digests": {"md5": "0fa02fdcbcd6e0aab562642984e442ab", "sha256": "e6503e4f9288d3afe537f8e2d080bca9afd79492c0eccd10b37debde11d918cd"}, "downloads": -1, "filename": "Neural_Decoding-0.1.dev0-py3-none-any.whl", "has_sig": false, "md5_digest": "0fa02fdcbcd6e0aab562642984e442ab", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 16794, "upload_time": "2019-07-03T20:55:03", "upload_time_iso_8601": "2019-07-03T20:55:03.023677Z", "url": "https://files.pythonhosted.org/packages/bb/8a/a39110da87e617a078aa7280025a6f8e437dfb70c13742a6bacc167a2d76/Neural_Decoding-0.1.dev0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4da501c45b0c1b1daf967239482444f7", "sha256": "e5ff19f769b9e7905bed02e944712feaaaa8baafc0ee6fab9ceea2e0460d4992"}, "downloads": -1, "filename": "Neural Decoding-0.1.dev0.tar.gz", "has_sig": false, "md5_digest": "4da501c45b0c1b1daf967239482444f7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14984, "upload_time": "2019-07-03T20:55:05", "upload_time_iso_8601": "2019-07-03T20:55:05.579794Z", "url": "https://files.pythonhosted.org/packages/75/e4/9a76fdd8397cd47003e61db563cddbed79169a65576cb1e6520053734656/Neural%20Decoding-0.1.dev0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "68eb74d5cffbce8d2d73ee64db7640d0", "sha256": "5fe5d4af3ea17dfd744f47c21ac06b4540dd5875d57243d57f9d15e9fb6f6529"}, "downloads": -1, "filename": "Neural_Decoding-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "68eb74d5cffbce8d2d73ee64db7640d0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18436, "upload_time": "2019-07-13T22:17:01", "upload_time_iso_8601": "2019-07-13T22:17:01.838606Z", "url": "https://files.pythonhosted.org/packages/4e/2a/cedfcd392b72c30dcd6646dbb34661adbad289462a9a49afa53803b68993/Neural_Decoding-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3dd7b0db8f6d8472d613fa4152f57860", "sha256": "f49439a55b9c3040d1d5902be8103ea88dbddd92ff6cb11852d6dfb46cbc847b"}, "downloads": -1, "filename": "Neural Decoding-0.1.4.tar.gz", "has_sig": false, "md5_digest": "3dd7b0db8f6d8472d613fa4152f57860", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16594, "upload_time": "2019-07-13T22:17:04", "upload_time_iso_8601": "2019-07-13T22:17:04.410604Z", "url": "https://files.pythonhosted.org/packages/de/0b/53dcd98904ce77471f24800e0cc4b7d64aa5b1f3aeef1b066ede12670e72/Neural%20Decoding-0.1.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:45:49 2020"}