{"info": {"author": "Ben Acland", "author_email": "benacland@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Science/Research", "License :: OSI Approved :: BSD License", "Topic :: Scientific/Engineering :: Information Analysis"], "description": "Installation\n============\n\nQuick Note\n----------\n\nFor brevity's sake, all instructions assume that you are using a BASH\nshell. If you have made an informed decision to do otherwise, I assume\nyou know enough to translate everything into your own environment. If\nyou have been forced to do otherwise by some powers that be, poor soul,\nask your systems administrator (or local friendly nerd) for help, or\ntake this as a learning opportunity.\n\nThese instructions also assume that your python environment is already\nset up. If that's not the case, you may find it helpful to consult our\n`cluster setup\ninstructions <https://github.com/beOn/hcpre/wiki/Setup-for-the-WUSTL-HPC-Cluster>`__.\nWhile the instructions there are specific to the High Performance\nComputing cluster at Washington University in St Louis, most of them\nwill apply to your environment, as well.\n\nThe HCP Pipeline\n----------------\n\nThis code has been tested against HCP Pipeline v3.0RC3 (commit\n058c132fc, Tue, Jan 14 2014). You'll have to make sure that the this or\na later compatible version of the HCP code is installed on all of the\nmachines on which you want to run the workflow. Installation is fairly\neasy - as long as you already have all of the HCP Pipeline's\ndependencies installed. Check the HCP Pipeline readme.txt for more\ninformation on how to get that done, paying special attention to FSL and\nFreeSurfer versions, and installing all of the dependencies of\ngradient\\_unwarp.py. If you have multiple versions of\ngradient\\_unwarp.py on your machine - be careful! Make sure that the\nversion you call from the command line is the first version found on\nyour python path, otherwise you might see some crashes.\n\nOne final note: because the HCP Pipelines include some pretty large\nfiles, your systems admin would probably appriciate it if there weren't\na new installation for every user. Check around with anyone else who\nuses the systems you're planning to use who might also use the HCP\nPipelines. If they're already installed, it'll save you some pain.\n\nhcprep (this project)\n---------------------\n\nThis project was developed for python versions > 2.7 and < 3.0, Nipype >\n0.9.2.\n\nTo get the latest release, install nipype, then hcpre using pip. The\nnipype installation is a little obnoxious right now. You can either\ncheck their install documentation, or go ahead and call\n``pip install hcpre`` then check the errors to see what dependencies\nyou're missing. The first time you run it, for example, you may get an\nerror like:\n\n::\n\n    Need nisext package from nibabel installation - please install nibabel first\n\nOkay. So call ``pip install nibabel``, then call ``pip install hcpre``\nagain. That'll get you the next error. Perhaps it'll be one of these:\n\n.. code:: bash\n\n    RuntimeError: Cannot import package \"networkx\" - is it installed?\n    # or\n    RuntimeError: Cannot import package \"scipy\" - is it installed?\n\nSo, once again, call ``pip install X`` where X is ``networkx``, or\n``scipy``, or whatever it tells you is missing. I know this stinks, and\nbelieve me I have tried to make this dependency install cleanly, but had\nto throw in the towel (for now). Numpy and scipy can take a while to\nbuild, but hopefully this process won't take too long, and eventually\nthis command will work:\n\n.. code:: bash\n\n    pip install hcpre\n\nOnce that works, you should get yourself a beer. I sure did.\n\nTo install the development version, clone this repository to your\nmachine, and update your PATH and PYTHONPATH variables, or run setup.py\nmanually\n\n.. code:: bash\n\n    export PATH=$PATH:/path/to/hcpre/hcpre\n    export PYTHONPATH=$PYTHONPATH:/path/to/hcpre\n\n    # or...\n\n    cd /path/to/hcpre\n    python setup.py install\n\nYou can also try using the requirements.txt file to install dependencies\nusing pip. Again, you may have to pip install some stuff one-by-one.\n\n.. code:: bash\n\n    pip install -r requirements.txt\n\nIf you're working on a community machine, talk to your systems\nadministrator about the contents of requirements.txt, whether or not\nthese dependencies are already installed, and any modifications that you\nmay need to make to your environment to make sure that they're on your\nPYTHONPATH.\n\nYou'll also need to install mricron, and make sure that its dcm2nii\nDICOM conversion application is on your PATH.\n\nEnvironment Variables\n---------------------\n\nThe HCP Pipelines make heavy use of environment variables - most of that\nis taken care of by the nipype workflow. But there are still a couple of\nvariables that it's important to set correctly: ``$FREESURFER_HOME`` and\n``$FSLDIR``. What's more, it's important that you call the FSL and\nFreeSurfer setup scripts from your .bashrc or .bash\\_profile file. Check\nthe HCP Pipeline readme for information regarding which particular\nversions of FreeSurfer and FSL their code currently targets.\n\nRunning the Pipeline\n====================\n\nThe file that contains the nipype workflow, hcpipe.py, can be called as\na command line script. See the section on configuration, then when\nyou're ready to run pass the -r argument, along with any others you\nchoose to use (see below).\n\nFor help, call:\n\n.. code:: bash\n\n    hcpipe.py --help\n\nConfiguring the Pipeline\n========================\n\nWe currently use `configobj <https://pypi.python.org/pypi/configobj/>`__\nto write and read files. hcpipe.py includes some tools to help you build\nand update config files pretty quickly, but since they're plain text you\ncan always open them up with a text editor and change them by hand (more\non this below).\n\nTo build a new config file, call hcpipe.py with the -i or --init\nargument. You'll be walked throught the creation of a new config file.\nYou'll want to have already downloaded your data, and you should be sure\nthat you have run the freesurfer and fsl setup scripts. You'll also need\nto know the path to the HCP Pipeline code. Let's quickly walk through\nthe config steps as they are now. I'll take some time to discuss each of\nthe questions, and how to figure out the appropriate answers.\n\nConfiguration Walkthrough\n-------------------------\n\nWe start by initializing a new config file.\n\n::\n\n    hcpipe.py --init\n    New config file name [hcp.conf]:\n\nDecisions decisions - what shall we call the new config file? The square\nbrackets mean that whatever they contain is the default value. So if we\njust press return, we'll choose the name \"hcp.conf\". Sounds good to me,\nso let's just press return. If the file already exists, the script will\nexit.\n\n::\n\n    The subjects directory should contain all raw data for all subjects.\n    Subjects Directory [./]: /data/nil-external/ccp/MOOD_RISK/DICOMs\n\nThe subjects directory is the lowest directory below which we can find\nall of your experiment's dicoms. So, if you had dicoms sorted into\n/some/dir/sub\\_a, /some/dir/sub\\_b, etc., then the subjects directory\nwould be /some/dir. Here, I've chosen something appropriate for my\ncurrent experiment.\n\n::\n\n    The DICOM template should be a format string for a glob which, when combined\n    with an individual subject ID, will get us all of the subject's DICOM files.\n    DICOM template [data/raw_dicom/%s/*.dcm]: DR%s/SCANS/*/DICOM/*.dcm\n\nThis is perhaps the trickiest one to answer, so I'm going to walk you\nthrough it step by step. The goal here is to tell the workflow how to\nfind all of the dicoms for a given subject. We do that by giving it what\nis called a format string (google FMI), which allows us to substitute in\neach subject number in place of what is called a string format\nspecifier, in this case '%s'. If that all seems like jargon, just follow\nalong and hopefully things will start to make sense.\n\nThe first step towards finding my format string is understanding how my\ndata is organized. Let's take a look. We know my subject folder is\n/data/nil- external/ccp/MOOD\\_RISK/DICOMs, so let's make a quick\nexploration of that directory's organization:\n\n::\n\n    $> ls /data/nil-external/ccp/MOOD_RISK/DICOMs\n    DR060  DR061  DR064\n    $> ls /data/nil-external/ccp/MOOD_RISK/DICOMs/DR060/\n    SCANS\n    $> ls /data/nil-external/ccp/MOOD_RISK/DICOMs/DR060/SCANS/\n    1  10  11  12  13  14  15  16  17  18  19  2  20  21  22  23  24  25  26  27\n    28  29  3  30  31  32  33  34  35  36  37  38  39  4  40  5  6  7  8  9\n    $> ls /data/nil-external/ccp/MOOD_RISK/DICOMs/DR060/SCANS/1/\n    DICOM/     SNAPSHOTS/\n    $> ls /data/nil-external/ccp/MOOD_RISK/DICOMs/DR060/SCANS/1/DICOM/\n    DR060.MR.Barch_MoodRisk.1.1.20131205.173445.s02tx.dcm    DR060.MR.Barch_MoodRisk.1.3.20131205.173445.19qm3q2.dcm\n    DR060.MR.Barch_MoodRisk.1.2.20131205.173445.10iky0u.dcm  scan_1_catalog.xml\n\nI might do the same looking into other subjects to verify that the\norganization is consistent. If that's not the case, you'll need to do\nsome cleanup to make it so, then come back to this point. Assuming that\nwe're satisfied on this point for now, we can see that a valid path to a\nspecific dicom might be:\n\n::\n\n    /data/nil-external/ccp/MOOD_RISK/DICOMs/DR060/SCANS/1/DICOM/DR060.MR.Barch_MoodRisk.1.1.20131205.173445.s02tx.dcm\n\nSo how to we get from here to a list of *all* of the experiments dicoms?\nFirst, we make use of the wildcard, ``*``. Because we use this string as\nwhat is known as a 'glob,' the character \\* will be expanded to match\nany number of characters, with a few exceptions (like '/'). So using\nthis, we can begin to shrink our string:\n\n::\n\n    /data/nil-external/ccp/MOOD_RISK/DICOMs/DR060/SCANS/*/DICOM/*.dcm\n\nSo here we've used two globs, one to replace the specific file name (we\nwant everything that ends in .dcm), and another to replace the scan\nnumber. So now what about the subject number? This case is a little\nspecial. Since we want the script to be able to substitute in specific\nsubject numbers, we use a string format specifier here instead of a\nwildcard. In the case of this data, the pattern seems to be\n``.../DICOMs/DR<SUBJECT_NUMBER>/SCANS...``, so let's put a string format\nspecifier in place of the subject number:\n\n::\n\n    /data/nil-external/ccp/MOOD_RISK/DICOMs/DR%s/SCANS/*/DICOM/*.dcm\n\nNote that you must use ``%s`` here - not ``%d``, even if all your\nsubject numbers are, in fact, numbers.\n\nOne last change. We don't need to include the subject directory\nprepended to the DICOM template (in fact, it is important that we do\nnot). So let's get that out of there, leaving us with:\n\n::\n\n    DR%s/SCANS/*/DICOM/*.dcm\n\nWhich is what we hand to the script! Moving right along:\n\n::\n\n    Subjects should be a comma separated list of subject ids.\n    Subject list ['']: 060, 061, 064\n\nFeel free to provide nothing here. If you want to store a particular\nlist of users to whom this config script should be applied, you can\nsupply them here or later by hand. You can also specify them on the\ncommand line when you call hcpipe.py using the -s parameter.\n\nAfter pressing enter, the script will look through all of the DICOM\nfiles that it can find. If you need to speed this up, I'll leave it as\nan exercise for the reader to figure out how to make it so the script\nonly finds one or two subjects worth of data.\n\n::\n\n    Checking series names (this may take some time) (41 chunks remaining)...\n\nAfter it gets to 0, we can start in on the fun stuff. The script will\nprint a numbered list of all the Series Descriptions it was able to\nfind. Our job now is to tell it how to use that information to feed our\ndata through the HCP Pipeline. Let's take a look at what it found in my\ncase:\n\n::\n\n    Found 25 unique series descriptions.\n    -------\n    Series:\n    -------\n    0:  AAHScout\n    1:  AAHScout_MPR_cor\n    2:  AAHScout_MPR_sag\n    3:  AAHScout_MPR_tra\n    4:  BIAS_32CH\n    5:  BIAS_BC\n    6:  BOLD_FACE1\n    7:  BOLD_FACE1_SBRef\n    8:  BOLD_FACE2\n    9:  BOLD_FACE2_SBRef\n    10: BOLD_REWARD1\n    11: BOLD_REWARD1_SBRef\n    12: BOLD_REWARD2\n    13: BOLD_REWARD2_SBRef\n    14: BOLD_REWARD3\n    15: BOLD_REWARD3_SBRef\n    16: BOLD_TEST\n    17: BOLD_TEST_SBRef\n    18: FieldMap\n    19: Localizer\n    20: Localizer_aligned\n    21: SpinEchoFieldMap_AP\n    22: SpinEchoFieldMap_PA\n    23: T1w_MPR_08mm\n    24: T2w_SPC_08mm\n\nAlright - the first couple of questions are pretty easy. Your SBRef\nimages might be called Scout or something else instead:\n\n::\n\n    Which series do you use for 'bold'?\n    [None] or comma separated values 0-24: 6,8,10,12,14\n\n    Which series do you use for 'bold_sbref'?\n    [None] or comma separated values 0-24: 7,9,11,13,15\n\nFor the next two, you'll often provide the same answer twice:\n\n::\n\n    Which series do you use for 'fieldmap_magnitude'?\n    [None] or comma separated values 0-24: 18\n\n    Which series do you use for 'fieldmap_phase'?\n    [None] or comma separated values 0-24: 18\n\nIf you collected Spin Echo Fieldmaps, they're probably either PA/AP, or\nRL/LR. Just leave blank responses for those you didn't collect:\n\n::\n\n    Which series do you use for 'fieldmap_ap'?\n    [None] or comma separated values 0-24: 21\n\n    Which series do you use for 'fieldmap_lr'?\n    [None] or comma separated values 0-24:\n\n    Which series do you use for 'fieldmap_pa'?\n    [None] or comma separated values 0-24: 22\n\n    Which series do you use for 'fieldmap_rl'?\n    [None] or comma separated values 0-24:\n\nT1/T2 are pretty easy:\n\n::\n\n    Which series do you use for 't1'?\n    [None] or comma separated values 0-24: 23\n\n    Which series do you use for 't2'?\n    [None] or comma separated values 0-24: 24\n\nThe next one, ``polarity_swapped``, requires some explanation. In some\nexperiments, you might acquire both RL and LR (or AP and PA) BOLD\nimages. We're always trying to improve the list of values that the\nworkflow derives at runtime, but for various reasons detecting this\nswitch is difficult to do reliably. So we need the user's help. If you\nacquire images with opposing polarities, choose one of them, say ``LR``\nif you're RL/LR, or ``PA`` if you're AP/PA, to call \"swapped.\" We don't\nsee that in this experiment, so we'll leave this blank. But if I did\nhave two of each bold image, one with suffix ``_AP`` and one with suffix\n``_PA``, I would list here the numbers of all of the \"swapped\" series,\nie those that ended with ``_PA``.\n\n::\n\n    Which series do you use for 'polarity_swapped'?\n    [None] or comma separated values 0-24:\n\nIf the FreeSurfer and FSL environment variables are set correctly, the\nnext two questions should provide correct default options. This works\nfor me, so I don't supply an alternate value:\n\n::\n\n    Path for FREESURFER_HOME [/usr/local/freesurfer]:\n\n    Path for FSLDIR [/usr/share/fsl/5.0]:\n\nCool beans. For the next one, I need to know the location of the HCP\nPipeline code:\n\n::\n\n    Path to HCP Pipelines dir [ ]: /scratch_cl1/hcp_pipe/Pipelines\n\nPop quiz: do you know the resolution of your structural data?\n\n::\n\n    What is your structural image resolution (mm)?\n    [Skip] or one of (.7, .8, 1): .8\n\nNow it asks if I want to use the default template files for my t1\nresolution, and the default config files. Yes and yes.\n\n::\n\n    Use default template files for resolution 0.8 [y]/n?\n\n    Use default config files [y]/n?\n\nGetting close! The next step was added to handle cases in which you may\nacquire more than one Spin Echo Fieldmap. In these cases you have a\nchoice between two policies. Either we'll just choose the first set we\nfind in each session (AP/PA or RL/LR pair), or we'll do something a\nlittle subtler. If you want the simple option, just choose 'first.' If\nyou choose 'most\\_recent,' then for each bold we'll either use the SE\nFieldmap pair most recently acquired prior to that particular BOLD, or\nif none were acquired before the BOLD run, then the first one acquired\nthereafter. In our case, we want the more complex option:\n\n::\n\n    If you have more than one ep fieldmap set, you may either\n    want to use the first, or always use the most recent.\n    Which policy would you like to use:\n\n    0: first\n    1: most_recent\n\n    Select 0-1: 1\n\nThis next one is pretty self explanatory:\n\n::\n\n    If you collect multiple t1 or t2 images, and averaging them yields warped\n    results, try blocking structural image averaging.\n\n    Block averaging of structural images [y]/n? y\n\nThe last thing that the config setup script does is to make a guess at\nthe unwarp direction.\n\n::\n\n    Very weak guess that your primary unwarp direction is y.\n    Did I mention this is a GUESS?\n\n    When finished, please open your config file check the value for ep_unwarp_dir.\n\nAt this point, unfortunately, we do not have fully automated derivation\nof unwarpdir in place. So this really is just a guess. You should look\nover the final results carefully, taking care to check for any untoward\ndistortions (like swirls, or unlikely overall shapes). If you see any of\nthese things, try opening the config file and changing the unwarp\ndirecion from x to y, from -x to x, or from y to -y, or any other\ncombination. You might even try z in a pinch - but I wouldn't try it\nfirst!\n\nThat's it for the config script at this point. To re-run the later part\n(optionally skipping the series mapping), call hcpipe.py -u or hcpipe.py\n--update.\n\nConfig by Hand\n--------------\n\nIf you choose to edit the config file by hand, please note that we use\nconfigobj in unrepr mode, which means that it expects the values to be\nin python format. The means that strings \"need to be quoted,\" lists\n[\"must\",\"be\",\"in\",\"brackets\"], and other primitives like 2.3 (floats), 2\n(integets), True and False (boolean values) can be used just as you\nwould use them in a python script.\n\nCustomizing Node Configuration\n------------------------------\n\nIf you want to customize any of the hcp node settings, you can either\nwrite your own script that creates a HCPrepWorkflow instance, and set\nthem programmatically or you can extend the config file. Just before\nrunning, nifti\\_wrangler and all of the hcp script wrapper nodes check\nto see if any of their values have been set in the config file. To do\nthis, the workflow first checks to see if the config file contains\nsections with names matching any of these nodes, including:\nnifti\\_wrangler, pre\\_freesurfer, freesurfer, post\\_freesurfer,\nvolume\\_processing, and surface\\_processing. Then it check to see if any\nof the settings in that section have names that match any of the\nrespective node's input parameters. If so, I try to set the value. We\ncan see this at work in the default config script, where we supply\nseveral arguments to nifti\\_wrangler:\n\n::\n\n    [nifti_wrangler]\n    ep_fieldmap_selection = 'most_recent'\n    block_struct_averaging = True\n    ep_unwarp_dir = 'y'\n\nFor a full list of the inputs of each node, check out\ninterface\\_docs.txt in the docs directory. This file is generated by\nlooking at the current interface specifications, so it should be\naccurate.\n\nTake note that not all attributes are config-file-settable. Those that\nare not include those that are derived at runtime. This list is\nundocumented at the moment.\n\nCustomizing Pipeline Environment Variables\n------------------------------------------\n\nEach of the HCP Pipeline interfaces takes a dictionary of environment\nvariables to be set before calling the HCP script. Take a look at the\n``get_hcp_env_for_config`` method for a list of variables that get set.\nTo override any of these, or add additional environment variables, add\nan ``env`` section to your config file.\n\nYou might find this particularly handy if you're using a version of\ncaret (provided by the connectome workbench). In this case, you'll need\nto override the default value for CARET7DIR. For example, on a Mac, you\nmight do like so:\n\n::\n\n    [env]\n    CARET7DIR = '/Applications/workbench/bin_macosx64'\n\nKeeping the Analysis By-products\n--------------------------------\n\nBy default, the pipeline does not keep all of the files generated in the\ncourse of analysis. If you want to keep them all, open your config file\nand set the following to false:\n\n::\n\n    [output_select]\n    output_mni_only = True\n\nValidation\n----------\n\nSome time in the future, we'll put in some nice config file validation\nstuff. Maybe.\n\nProject Status\n==============\n\nAlpha release.\n\nThis software is not a replacement for knowing what you're doing, and\nyou should inspect all of the pipeline's output logs carefully. We make\nno promises whatsoever at this point that the output of this pipeline is\nworth using.\n\nReporting Bugs\n==============\n\nPlease use the githup isses tab for bug reports and feature requests.", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/beOn/hcpre", "keywords": "connectome preprocessing fmri bold", "license": "BSD", "maintainer": null, "maintainer_email": null, "name": "hcpre", "package_url": "https://pypi.org/project/hcpre/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/hcpre/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/beOn/hcpre"}, "release_url": "https://pypi.org/project/hcpre/0.5.5/", "requires_dist": null, "requires_python": null, "summary": "Generalized launcher for human connectome project BOLD preprocessing", "version": "0.5.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"installation\">\n<h2>Installation</h2>\n<div id=\"quick-note\">\n<h3>Quick Note</h3>\n<p>For brevity\u2019s sake, all instructions assume that you are using a BASH\nshell. If you have made an informed decision to do otherwise, I assume\nyou know enough to translate everything into your own environment. If\nyou have been forced to do otherwise by some powers that be, poor soul,\nask your systems administrator (or local friendly nerd) for help, or\ntake this as a learning opportunity.</p>\n<p>These instructions also assume that your python environment is already\nset up. If that\u2019s not the case, you may find it helpful to consult our\n<a href=\"https://github.com/beOn/hcpre/wiki/Setup-for-the-WUSTL-HPC-Cluster\" rel=\"nofollow\">cluster setup\ninstructions</a>.\nWhile the instructions there are specific to the High Performance\nComputing cluster at Washington University in St Louis, most of them\nwill apply to your environment, as well.</p>\n</div>\n<div id=\"the-hcp-pipeline\">\n<h3>The HCP Pipeline</h3>\n<p>This code has been tested against HCP Pipeline v3.0RC3 (commit\n058c132fc, Tue, Jan 14 2014). You\u2019ll have to make sure that the this or\na later compatible version of the HCP code is installed on all of the\nmachines on which you want to run the workflow. Installation is fairly\neasy - as long as you already have all of the HCP Pipeline\u2019s\ndependencies installed. Check the HCP Pipeline readme.txt for more\ninformation on how to get that done, paying special attention to FSL and\nFreeSurfer versions, and installing all of the dependencies of\ngradient_unwarp.py. If you have multiple versions of\ngradient_unwarp.py on your machine - be careful! Make sure that the\nversion you call from the command line is the first version found on\nyour python path, otherwise you might see some crashes.</p>\n<p>One final note: because the HCP Pipelines include some pretty large\nfiles, your systems admin would probably appriciate it if there weren\u2019t\na new installation for every user. Check around with anyone else who\nuses the systems you\u2019re planning to use who might also use the HCP\nPipelines. If they\u2019re already installed, it\u2019ll save you some pain.</p>\n</div>\n<div id=\"hcprep-this-project\">\n<h3>hcprep (this project)</h3>\n<p>This project was developed for python versions &gt; 2.7 and &lt; 3.0, Nipype &gt;\n0.9.2.</p>\n<p>To get the latest release, install nipype, then hcpre using pip. The\nnipype installation is a little obnoxious right now. You can either\ncheck their install documentation, or go ahead and call\n<tt>pip install hcpre</tt> then check the errors to see what dependencies\nyou\u2019re missing. The first time you run it, for example, you may get an\nerror like:</p>\n<pre>Need nisext package from nibabel installation - please install nibabel first\n</pre>\n<p>Okay. So call <tt>pip install nibabel</tt>, then call <tt>pip install hcpre</tt>\nagain. That\u2019ll get you the next error. Perhaps it\u2019ll be one of these:</p>\n<pre>RuntimeError: Cannot import package <span class=\"s2\">\"networkx\"</span> - is it installed?\n<span class=\"c1\"># or\n</span>RuntimeError: Cannot import package <span class=\"s2\">\"scipy\"</span> - is it installed?\n</pre>\n<p>So, once again, call <tt>pip install X</tt> where X is <tt>networkx</tt>, or\n<tt>scipy</tt>, or whatever it tells you is missing. I know this stinks, and\nbelieve me I have tried to make this dependency install cleanly, but had\nto throw in the towel (for now). Numpy and scipy can take a while to\nbuild, but hopefully this process won\u2019t take too long, and eventually\nthis command will work:</p>\n<pre>pip install hcpre\n</pre>\n<p>Once that works, you should get yourself a beer. I sure did.</p>\n<p>To install the development version, clone this repository to your\nmachine, and update your PATH and PYTHONPATH variables, or run setup.py\nmanually</p>\n<pre><span class=\"nb\">export</span> <span class=\"nv\">PATH</span><span class=\"o\">=</span><span class=\"nv\">$PATH</span>:/path/to/hcpre/hcpre\n<span class=\"nb\">export</span> <span class=\"nv\">PYTHONPATH</span><span class=\"o\">=</span><span class=\"nv\">$PYTHONPATH</span>:/path/to/hcpre\n\n<span class=\"c1\"># or...\n</span>\n<span class=\"nb\">cd</span> /path/to/hcpre\npython setup.py install\n</pre>\n<p>You can also try using the requirements.txt file to install dependencies\nusing pip. Again, you may have to pip install some stuff one-by-one.</p>\n<pre>pip install -r requirements.txt\n</pre>\n<p>If you\u2019re working on a community machine, talk to your systems\nadministrator about the contents of requirements.txt, whether or not\nthese dependencies are already installed, and any modifications that you\nmay need to make to your environment to make sure that they\u2019re on your\nPYTHONPATH.</p>\n<p>You\u2019ll also need to install mricron, and make sure that its dcm2nii\nDICOM conversion application is on your PATH.</p>\n</div>\n<div id=\"environment-variables\">\n<h3>Environment Variables</h3>\n<p>The HCP Pipelines make heavy use of environment variables - most of that\nis taken care of by the nipype workflow. But there are still a couple of\nvariables that it\u2019s important to set correctly: <tt>$FREESURFER_HOME</tt> and\n<tt>$FSLDIR</tt>. What\u2019s more, it\u2019s important that you call the FSL and\nFreeSurfer setup scripts from your .bashrc or .bash_profile file. Check\nthe HCP Pipeline readme for information regarding which particular\nversions of FreeSurfer and FSL their code currently targets.</p>\n</div>\n</div>\n<div id=\"running-the-pipeline\">\n<h2>Running the Pipeline</h2>\n<p>The file that contains the nipype workflow, hcpipe.py, can be called as\na command line script. See the section on configuration, then when\nyou\u2019re ready to run pass the -r argument, along with any others you\nchoose to use (see below).</p>\n<p>For help, call:</p>\n<pre>hcpipe.py --help\n</pre>\n</div>\n<div id=\"configuring-the-pipeline\">\n<h2>Configuring the Pipeline</h2>\n<p>We currently use <a href=\"https://pypi.python.org/pypi/configobj/\" rel=\"nofollow\">configobj</a>\nto write and read files. hcpipe.py includes some tools to help you build\nand update config files pretty quickly, but since they\u2019re plain text you\ncan always open them up with a text editor and change them by hand (more\non this below).</p>\n<p>To build a new config file, call hcpipe.py with the -i or \u2013init\nargument. You\u2019ll be walked throught the creation of a new config file.\nYou\u2019ll want to have already downloaded your data, and you should be sure\nthat you have run the freesurfer and fsl setup scripts. You\u2019ll also need\nto know the path to the HCP Pipeline code. Let\u2019s quickly walk through\nthe config steps as they are now. I\u2019ll take some time to discuss each of\nthe questions, and how to figure out the appropriate answers.</p>\n<div id=\"configuration-walkthrough\">\n<h3>Configuration Walkthrough</h3>\n<p>We start by initializing a new config file.</p>\n<pre>hcpipe.py --init\nNew config file name [hcp.conf]:\n</pre>\n<p>Decisions decisions - what shall we call the new config file? The square\nbrackets mean that whatever they contain is the default value. So if we\njust press return, we\u2019ll choose the name \u201chcp.conf\u201d. Sounds good to me,\nso let\u2019s just press return. If the file already exists, the script will\nexit.</p>\n<pre>The subjects directory should contain all raw data for all subjects.\nSubjects Directory [./]: /data/nil-external/ccp/MOOD_RISK/DICOMs\n</pre>\n<p>The subjects directory is the lowest directory below which we can find\nall of your experiment\u2019s dicoms. So, if you had dicoms sorted into\n/some/dir/sub_a, /some/dir/sub_b, etc., then the subjects directory\nwould be /some/dir. Here, I\u2019ve chosen something appropriate for my\ncurrent experiment.</p>\n<pre>The DICOM template should be a format string for a glob which, when combined\nwith an individual subject ID, will get us all of the subject's DICOM files.\nDICOM template [data/raw_dicom/%s/*.dcm]: DR%s/SCANS/*/DICOM/*.dcm\n</pre>\n<p>This is perhaps the trickiest one to answer, so I\u2019m going to walk you\nthrough it step by step. The goal here is to tell the workflow how to\nfind all of the dicoms for a given subject. We do that by giving it what\nis called a format string (google FMI), which allows us to substitute in\neach subject number in place of what is called a string format\nspecifier, in this case \u2018%s\u2019. If that all seems like jargon, just follow\nalong and hopefully things will start to make sense.</p>\n<p>The first step towards finding my format string is understanding how my\ndata is organized. Let\u2019s take a look. We know my subject folder is\n/data/nil- external/ccp/MOOD_RISK/DICOMs, so let\u2019s make a quick\nexploration of that directory\u2019s organization:</p>\n<pre>$&gt; ls /data/nil-external/ccp/MOOD_RISK/DICOMs\nDR060  DR061  DR064\n$&gt; ls /data/nil-external/ccp/MOOD_RISK/DICOMs/DR060/\nSCANS\n$&gt; ls /data/nil-external/ccp/MOOD_RISK/DICOMs/DR060/SCANS/\n1  10  11  12  13  14  15  16  17  18  19  2  20  21  22  23  24  25  26  27\n28  29  3  30  31  32  33  34  35  36  37  38  39  4  40  5  6  7  8  9\n$&gt; ls /data/nil-external/ccp/MOOD_RISK/DICOMs/DR060/SCANS/1/\nDICOM/     SNAPSHOTS/\n$&gt; ls /data/nil-external/ccp/MOOD_RISK/DICOMs/DR060/SCANS/1/DICOM/\nDR060.MR.Barch_MoodRisk.1.1.20131205.173445.s02tx.dcm    DR060.MR.Barch_MoodRisk.1.3.20131205.173445.19qm3q2.dcm\nDR060.MR.Barch_MoodRisk.1.2.20131205.173445.10iky0u.dcm  scan_1_catalog.xml\n</pre>\n<p>I might do the same looking into other subjects to verify that the\norganization is consistent. If that\u2019s not the case, you\u2019ll need to do\nsome cleanup to make it so, then come back to this point. Assuming that\nwe\u2019re satisfied on this point for now, we can see that a valid path to a\nspecific dicom might be:</p>\n<pre>/data/nil-external/ccp/MOOD_RISK/DICOMs/DR060/SCANS/1/DICOM/DR060.MR.Barch_MoodRisk.1.1.20131205.173445.s02tx.dcm\n</pre>\n<p>So how to we get from here to a list of <em>all</em> of the experiments dicoms?\nFirst, we make use of the wildcard, <tt>*</tt>. Because we use this string as\nwhat is known as a \u2018glob,\u2019 the character * will be expanded to match\nany number of characters, with a few exceptions (like \u2018/\u2019). So using\nthis, we can begin to shrink our string:</p>\n<pre>/data/nil-external/ccp/MOOD_RISK/DICOMs/DR060/SCANS/*/DICOM/*.dcm\n</pre>\n<p>So here we\u2019ve used two globs, one to replace the specific file name (we\nwant everything that ends in .dcm), and another to replace the scan\nnumber. So now what about the subject number? This case is a little\nspecial. Since we want the script to be able to substitute in specific\nsubject numbers, we use a string format specifier here instead of a\nwildcard. In the case of this data, the pattern seems to be\n<tt><span class=\"pre\">.../DICOMs/DR&lt;SUBJECT_NUMBER&gt;/SCANS...</span></tt>, so let\u2019s put a string format\nspecifier in place of the subject number:</p>\n<pre>/data/nil-external/ccp/MOOD_RISK/DICOMs/DR%s/SCANS/*/DICOM/*.dcm\n</pre>\n<p>Note that you must use <tt>%s</tt> here - not <tt>%d</tt>, even if all your\nsubject numbers are, in fact, numbers.</p>\n<p>One last change. We don\u2019t need to include the subject directory\nprepended to the DICOM template (in fact, it is important that we do\nnot). So let\u2019s get that out of there, leaving us with:</p>\n<pre>DR%s/SCANS/*/DICOM/*.dcm\n</pre>\n<p>Which is what we hand to the script! Moving right along:</p>\n<pre>Subjects should be a comma separated list of subject ids.\nSubject list ['']: 060, 061, 064\n</pre>\n<p>Feel free to provide nothing here. If you want to store a particular\nlist of users to whom this config script should be applied, you can\nsupply them here or later by hand. You can also specify them on the\ncommand line when you call hcpipe.py using the -s parameter.</p>\n<p>After pressing enter, the script will look through all of the DICOM\nfiles that it can find. If you need to speed this up, I\u2019ll leave it as\nan exercise for the reader to figure out how to make it so the script\nonly finds one or two subjects worth of data.</p>\n<pre>Checking series names (this may take some time) (41 chunks remaining)...\n</pre>\n<p>After it gets to 0, we can start in on the fun stuff. The script will\nprint a numbered list of all the Series Descriptions it was able to\nfind. Our job now is to tell it how to use that information to feed our\ndata through the HCP Pipeline. Let\u2019s take a look at what it found in my\ncase:</p>\n<pre>Found 25 unique series descriptions.\n-------\nSeries:\n-------\n0:  AAHScout\n1:  AAHScout_MPR_cor\n2:  AAHScout_MPR_sag\n3:  AAHScout_MPR_tra\n4:  BIAS_32CH\n5:  BIAS_BC\n6:  BOLD_FACE1\n7:  BOLD_FACE1_SBRef\n8:  BOLD_FACE2\n9:  BOLD_FACE2_SBRef\n10: BOLD_REWARD1\n11: BOLD_REWARD1_SBRef\n12: BOLD_REWARD2\n13: BOLD_REWARD2_SBRef\n14: BOLD_REWARD3\n15: BOLD_REWARD3_SBRef\n16: BOLD_TEST\n17: BOLD_TEST_SBRef\n18: FieldMap\n19: Localizer\n20: Localizer_aligned\n21: SpinEchoFieldMap_AP\n22: SpinEchoFieldMap_PA\n23: T1w_MPR_08mm\n24: T2w_SPC_08mm\n</pre>\n<p>Alright - the first couple of questions are pretty easy. Your SBRef\nimages might be called Scout or something else instead:</p>\n<pre>Which series do you use for 'bold'?\n[None] or comma separated values 0-24: 6,8,10,12,14\n\nWhich series do you use for 'bold_sbref'?\n[None] or comma separated values 0-24: 7,9,11,13,15\n</pre>\n<p>For the next two, you\u2019ll often provide the same answer twice:</p>\n<pre>Which series do you use for 'fieldmap_magnitude'?\n[None] or comma separated values 0-24: 18\n\nWhich series do you use for 'fieldmap_phase'?\n[None] or comma separated values 0-24: 18\n</pre>\n<p>If you collected Spin Echo Fieldmaps, they\u2019re probably either PA/AP, or\nRL/LR. Just leave blank responses for those you didn\u2019t collect:</p>\n<pre>Which series do you use for 'fieldmap_ap'?\n[None] or comma separated values 0-24: 21\n\nWhich series do you use for 'fieldmap_lr'?\n[None] or comma separated values 0-24:\n\nWhich series do you use for 'fieldmap_pa'?\n[None] or comma separated values 0-24: 22\n\nWhich series do you use for 'fieldmap_rl'?\n[None] or comma separated values 0-24:\n</pre>\n<p>T1/T2 are pretty easy:</p>\n<pre>Which series do you use for 't1'?\n[None] or comma separated values 0-24: 23\n\nWhich series do you use for 't2'?\n[None] or comma separated values 0-24: 24\n</pre>\n<p>The next one, <tt>polarity_swapped</tt>, requires some explanation. In some\nexperiments, you might acquire both RL and LR (or AP and PA) BOLD\nimages. We\u2019re always trying to improve the list of values that the\nworkflow derives at runtime, but for various reasons detecting this\nswitch is difficult to do reliably. So we need the user\u2019s help. If you\nacquire images with opposing polarities, choose one of them, say <tt>LR</tt>\nif you\u2019re RL/LR, or <tt>PA</tt> if you\u2019re AP/PA, to call \u201cswapped.\u201d We don\u2019t\nsee that in this experiment, so we\u2019ll leave this blank. But if I did\nhave two of each bold image, one with suffix <tt>_AP</tt> and one with suffix\n<tt>_PA</tt>, I would list here the numbers of all of the \u201cswapped\u201d series,\nie those that ended with <tt>_PA</tt>.</p>\n<pre>Which series do you use for 'polarity_swapped'?\n[None] or comma separated values 0-24:\n</pre>\n<p>If the FreeSurfer and FSL environment variables are set correctly, the\nnext two questions should provide correct default options. This works\nfor me, so I don\u2019t supply an alternate value:</p>\n<pre>Path for FREESURFER_HOME [/usr/local/freesurfer]:\n\nPath for FSLDIR [/usr/share/fsl/5.0]:\n</pre>\n<p>Cool beans. For the next one, I need to know the location of the HCP\nPipeline code:</p>\n<pre>Path to HCP Pipelines dir [ ]: /scratch_cl1/hcp_pipe/Pipelines\n</pre>\n<p>Pop quiz: do you know the resolution of your structural data?</p>\n<pre>What is your structural image resolution (mm)?\n[Skip] or one of (.7, .8, 1): .8\n</pre>\n<p>Now it asks if I want to use the default template files for my t1\nresolution, and the default config files. Yes and yes.</p>\n<pre>Use default template files for resolution 0.8 [y]/n?\n\nUse default config files [y]/n?\n</pre>\n<p>Getting close! The next step was added to handle cases in which you may\nacquire more than one Spin Echo Fieldmap. In these cases you have a\nchoice between two policies. Either we\u2019ll just choose the first set we\nfind in each session (AP/PA or RL/LR pair), or we\u2019ll do something a\nlittle subtler. If you want the simple option, just choose \u2018first.\u2019 If\nyou choose \u2018most_recent,\u2019 then for each bold we\u2019ll either use the SE\nFieldmap pair most recently acquired prior to that particular BOLD, or\nif none were acquired before the BOLD run, then the first one acquired\nthereafter. In our case, we want the more complex option:</p>\n<pre>If you have more than one ep fieldmap set, you may either\nwant to use the first, or always use the most recent.\nWhich policy would you like to use:\n\n0: first\n1: most_recent\n\nSelect 0-1: 1\n</pre>\n<p>This next one is pretty self explanatory:</p>\n<pre>If you collect multiple t1 or t2 images, and averaging them yields warped\nresults, try blocking structural image averaging.\n\nBlock averaging of structural images [y]/n? y\n</pre>\n<p>The last thing that the config setup script does is to make a guess at\nthe unwarp direction.</p>\n<pre>Very weak guess that your primary unwarp direction is y.\nDid I mention this is a GUESS?\n\nWhen finished, please open your config file check the value for ep_unwarp_dir.\n</pre>\n<p>At this point, unfortunately, we do not have fully automated derivation\nof unwarpdir in place. So this really is just a guess. You should look\nover the final results carefully, taking care to check for any untoward\ndistortions (like swirls, or unlikely overall shapes). If you see any of\nthese things, try opening the config file and changing the unwarp\ndirecion from x to y, from -x to x, or from y to -y, or any other\ncombination. You might even try z in a pinch - but I wouldn\u2019t try it\nfirst!</p>\n<p>That\u2019s it for the config script at this point. To re-run the later part\n(optionally skipping the series mapping), call hcpipe.py -u or hcpipe.py\n\u2013update.</p>\n</div>\n<div id=\"config-by-hand\">\n<h3>Config by Hand</h3>\n<p>If you choose to edit the config file by hand, please note that we use\nconfigobj in unrepr mode, which means that it expects the values to be\nin python format. The means that strings \u201cneed to be quoted,\u201d lists\n[\u201cmust\u201d,\u201dbe\u201d,\u201din\u201d,\u201dbrackets\u201d], and other primitives like 2.3 (floats), 2\n(integets), True and False (boolean values) can be used just as you\nwould use them in a python script.</p>\n</div>\n<div id=\"customizing-node-configuration\">\n<h3>Customizing Node Configuration</h3>\n<p>If you want to customize any of the hcp node settings, you can either\nwrite your own script that creates a HCPrepWorkflow instance, and set\nthem programmatically or you can extend the config file. Just before\nrunning, nifti_wrangler and all of the hcp script wrapper nodes check\nto see if any of their values have been set in the config file. To do\nthis, the workflow first checks to see if the config file contains\nsections with names matching any of these nodes, including:\nnifti_wrangler, pre_freesurfer, freesurfer, post_freesurfer,\nvolume_processing, and surface_processing. Then it check to see if any\nof the settings in that section have names that match any of the\nrespective node\u2019s input parameters. If so, I try to set the value. We\ncan see this at work in the default config script, where we supply\nseveral arguments to nifti_wrangler:</p>\n<pre>[nifti_wrangler]\nep_fieldmap_selection = 'most_recent'\nblock_struct_averaging = True\nep_unwarp_dir = 'y'\n</pre>\n<p>For a full list of the inputs of each node, check out\ninterface_docs.txt in the docs directory. This file is generated by\nlooking at the current interface specifications, so it should be\naccurate.</p>\n<p>Take note that not all attributes are config-file-settable. Those that\nare not include those that are derived at runtime. This list is\nundocumented at the moment.</p>\n</div>\n<div id=\"customizing-pipeline-environment-variables\">\n<h3>Customizing Pipeline Environment Variables</h3>\n<p>Each of the HCP Pipeline interfaces takes a dictionary of environment\nvariables to be set before calling the HCP script. Take a look at the\n<tt>get_hcp_env_for_config</tt> method for a list of variables that get set.\nTo override any of these, or add additional environment variables, add\nan <tt>env</tt> section to your config file.</p>\n<p>You might find this particularly handy if you\u2019re using a version of\ncaret (provided by the connectome workbench). In this case, you\u2019ll need\nto override the default value for CARET7DIR. For example, on a Mac, you\nmight do like so:</p>\n<pre>[env]\nCARET7DIR = '/Applications/workbench/bin_macosx64'\n</pre>\n</div>\n<div id=\"keeping-the-analysis-by-products\">\n<h3>Keeping the Analysis By-products</h3>\n<p>By default, the pipeline does not keep all of the files generated in the\ncourse of analysis. If you want to keep them all, open your config file\nand set the following to false:</p>\n<pre>[output_select]\noutput_mni_only = True\n</pre>\n</div>\n<div id=\"validation\">\n<h3>Validation</h3>\n<p>Some time in the future, we\u2019ll put in some nice config file validation\nstuff. Maybe.</p>\n</div>\n</div>\n<div id=\"project-status\">\n<h2>Project Status</h2>\n<p>Alpha release.</p>\n<p>This software is not a replacement for knowing what you\u2019re doing, and\nyou should inspect all of the pipeline\u2019s output logs carefully. We make\nno promises whatsoever at this point that the output of this pipeline is\nworth using.</p>\n</div>\n<div id=\"reporting-bugs\">\n<h2>Reporting Bugs</h2>\n<p>Please use the githup isses tab for bug reports and feature requests.</p>\n</div>\n\n          </div>"}, "last_serial": 1122104, "releases": {"0.5.0": [{"comment_text": "", "digests": {"md5": "43e197e82fea7068248d5b50df63ec6f", "sha256": "13a877cbb22248764687281f07300210ce2ea0dc94bc9b6b42dd82afaecb16ae"}, "downloads": -1, "filename": "hcpre-0.5.0-py2.7.egg", "has_sig": true, "md5_digest": "43e197e82fea7068248d5b50df63ec6f", "packagetype": "bdist_egg", "python_version": "2.7", "requires_python": null, "size": 51020, "upload_time": "2014-05-12T23:07:00", "upload_time_iso_8601": "2014-05-12T23:07:00.096652Z", "url": "https://files.pythonhosted.org/packages/39/61/4fbe7390faf97bdcbef57ce174c132ae6a3cbcac1cf6bf95fe4e1ebcc53f/hcpre-0.5.0-py2.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "3667a50675242f1384b12221aa878404", "sha256": "cf52cd5fd9e66fd62ffe65eeaeb964e20b37b7da850ade5381f144a553f3db0e"}, "downloads": -1, "filename": "hcpre-0.5.0-py2-none-any.whl", "has_sig": true, "md5_digest": "3667a50675242f1384b12221aa878404", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 59644, "upload_time": "2014-05-12T23:06:55", "upload_time_iso_8601": "2014-05-12T23:06:55.728002Z", "url": "https://files.pythonhosted.org/packages/1b/6b/68de67eb53e241159045754399ecd7ca849cdf22b192e4905e8b3543a984/hcpre-0.5.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b1a8bc07de898e752d4aa8a04ea65a98", "sha256": "3c6db2159ca7a96065489171ecc09e9366f20a72e3768c4342038603d19a9109"}, "downloads": -1, "filename": "hcpre-0.5.0.tar.gz", "has_sig": true, "md5_digest": "b1a8bc07de898e752d4aa8a04ea65a98", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 46041, "upload_time": "2014-05-12T23:06:52", "upload_time_iso_8601": "2014-05-12T23:06:52.048018Z", "url": "https://files.pythonhosted.org/packages/2a/93/75f7bf98f853c44a8d21abd94fbf39c78d2f1042a0a881c847500bcfaea1/hcpre-0.5.0.tar.gz", "yanked": false}], "0.5.1": [{"comment_text": "", "digests": {"md5": "8f79c7db76cf208314ee253887a809a2", "sha256": "7f6eb927e82b50c43b786b21284b920a822ebfde45ca4a28afc0d3eeca7b90b9"}, "downloads": -1, "filename": "hcpre-0.5.1-py2.7.egg", "has_sig": true, "md5_digest": "8f79c7db76cf208314ee253887a809a2", "packagetype": "bdist_egg", "python_version": "2.7", "requires_python": null, "size": 68762, "upload_time": "2014-05-13T20:11:08", "upload_time_iso_8601": "2014-05-13T20:11:08.349407Z", "url": "https://files.pythonhosted.org/packages/84/d7/b0c7b5344fc3d046ec1ef96bdaaa8796e164410020d594c4cead6cfcca10/hcpre-0.5.1-py2.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "a879401bf640b17d643f4004bbb60ed2", "sha256": "3827ee890b38da2250c972c3a5440162fc29d4f17f7f83f1d295de048d192072"}, "downloads": -1, "filename": "hcpre-0.5.1-py2-none-any.whl", "has_sig": true, "md5_digest": "a879401bf640b17d643f4004bbb60ed2", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 77556, "upload_time": "2014-05-13T20:11:04", "upload_time_iso_8601": "2014-05-13T20:11:04.681813Z", "url": "https://files.pythonhosted.org/packages/1d/7a/d68d80a9ea485d93961ea197fd18dc0a01c969e0c391a56f299bfdeba630/hcpre-0.5.1-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1272ee269caa36a7c24e1cfd5ba526a1", "sha256": "ddb628657b47f53f240e195ba81adcbbfad066d487a6ae2f815df1e490f7ccd4"}, "downloads": -1, "filename": "hcpre-0.5.1.tar.gz", "has_sig": true, "md5_digest": "1272ee269caa36a7c24e1cfd5ba526a1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59395, "upload_time": "2014-05-13T20:11:01", "upload_time_iso_8601": "2014-05-13T20:11:01.316223Z", "url": "https://files.pythonhosted.org/packages/ee/e1/020f026d4db162ab6876829a570eb06ec686b2a1a9f45ffc7366f4fa201d/hcpre-0.5.1.tar.gz", "yanked": false}], "0.5.2": [{"comment_text": "", "digests": {"md5": "17b502fbe3aabc004e071f97d5cd5ab6", "sha256": "5f5a59cc86813ff0ccf740fbe5f7471fa2546fd1b26818d894e6881ee97c7b8e"}, "downloads": -1, "filename": "hcpre-0.5.2-py2.7.egg", "has_sig": true, "md5_digest": "17b502fbe3aabc004e071f97d5cd5ab6", "packagetype": "bdist_egg", "python_version": "2.7", "requires_python": null, "size": 97817, "upload_time": "2014-05-15T13:59:57", "upload_time_iso_8601": "2014-05-15T13:59:57.673791Z", "url": "https://files.pythonhosted.org/packages/3c/6b/bd10828f20faed6eedbcc8d83a8f62c461ad35ab3b51191f7eeeaa088e5b/hcpre-0.5.2-py2.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "98638b7a0d8c98afc0e79c358e006de8", "sha256": "aedf4c3c4491fdbce9b83954baec6c7b2d9e3c3f5963a54d7caffec72d4cb71a"}, "downloads": -1, "filename": "hcpre-0.5.2-py2-none-any.whl", "has_sig": true, "md5_digest": "98638b7a0d8c98afc0e79c358e006de8", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 77592, "upload_time": "2014-05-15T14:00:01", "upload_time_iso_8601": "2014-05-15T14:00:01.119556Z", "url": "https://files.pythonhosted.org/packages/59/58/ed5b2acf16ef8dba0f985e54efb2a3396cb10cc020a28b9cec2da4c84491/hcpre-0.5.2-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c589928ab0163dce743a37e7c127d52e", "sha256": "4d288d970d1677b2be81ed9b48af76de0eddb7cd9285563d83d25dfca0b7ea27"}, "downloads": -1, "filename": "hcpre-0.5.2.tar.gz", "has_sig": true, "md5_digest": "c589928ab0163dce743a37e7c127d52e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59417, "upload_time": "2014-05-15T13:59:54", "upload_time_iso_8601": "2014-05-15T13:59:54.122645Z", "url": "https://files.pythonhosted.org/packages/a2/c1/81e415dddd50fee2f08500e5367e6cc854ace3773402fb9b6d98e2a176c0/hcpre-0.5.2.tar.gz", "yanked": false}], "0.5.3": [{"comment_text": "", "digests": {"md5": "4b1a96d062189b5cbf92ffafe50786e7", "sha256": "06a60d4b9c512a6f670471398801650a6974166b25c6fce433ac525d700733d8"}, "downloads": -1, "filename": "hcpre-0.5.3-py2.7.egg", "has_sig": true, "md5_digest": "4b1a96d062189b5cbf92ffafe50786e7", "packagetype": "bdist_egg", "python_version": "2.7", "requires_python": null, "size": 98061, "upload_time": "2014-05-15T18:01:29", "upload_time_iso_8601": "2014-05-15T18:01:29.661593Z", "url": "https://files.pythonhosted.org/packages/df/cb/797daccbac3cc3f9da95437e2c930fe96b24de0bf57fdef41ae1a0271a5f/hcpre-0.5.3-py2.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "6fe8ceaf9ce238ee392801725b5f17a1", "sha256": "e711cf13c9378724eae04d22650e8bb20d875c0de4b56f93ed9af76dd2fc06ff"}, "downloads": -1, "filename": "hcpre-0.5.3-py2-none-any.whl", "has_sig": true, "md5_digest": "6fe8ceaf9ce238ee392801725b5f17a1", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 77682, "upload_time": "2014-05-15T18:01:32", "upload_time_iso_8601": "2014-05-15T18:01:32.107070Z", "url": "https://files.pythonhosted.org/packages/2e/3f/b0dc2b276ed5e74a6e274f276375062e7c0de779438830146805eacdceea/hcpre-0.5.3-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "095f34cf5ec2cb05d71144e9e6f11dfe", "sha256": "1da0a9e0ed7cd458d7bf16e403631c119c2b322920cff058f8c3ce04ad1bd9d5"}, "downloads": -1, "filename": "hcpre-0.5.3.tar.gz", "has_sig": true, "md5_digest": "095f34cf5ec2cb05d71144e9e6f11dfe", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59467, "upload_time": "2014-05-15T18:01:26", "upload_time_iso_8601": "2014-05-15T18:01:26.730271Z", "url": "https://files.pythonhosted.org/packages/e5/ce/7e388ebae786878fd377919cfdfae44b805e84ab0239c4cf74af9da1718b/hcpre-0.5.3.tar.gz", "yanked": false}], "0.5.4": [{"comment_text": "", "digests": {"md5": "3d3f7782e3c71578bf8b68b895637a46", "sha256": "f924450e2dd586395c6f4e333d7fa0a56707ab16cbaa84f64c975a19c0dca378"}, "downloads": -1, "filename": "hcpre-0.5.4-py2.7.egg", "has_sig": true, "md5_digest": "3d3f7782e3c71578bf8b68b895637a46", "packagetype": "bdist_egg", "python_version": "2.7", "requires_python": null, "size": 69171, "upload_time": "2014-05-19T17:35:39", "upload_time_iso_8601": "2014-05-19T17:35:39.089301Z", "url": "https://files.pythonhosted.org/packages/71/76/1c6bbc7ca86ac53ff9df5ce9650065d3660fb0ca08f8f45dd058858ebedc/hcpre-0.5.4-py2.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "baf83a91eb2a0229ee40b2e0851d5853", "sha256": "20774a3a3ce1574948fa4e95786a3477adc2beed1ff1a84c2d8aa26788fab796"}, "downloads": -1, "filename": "hcpre-0.5.4-py2-none-any.whl", "has_sig": true, "md5_digest": "baf83a91eb2a0229ee40b2e0851d5853", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 78203, "upload_time": "2014-05-19T17:35:36", "upload_time_iso_8601": "2014-05-19T17:35:36.791072Z", "url": "https://files.pythonhosted.org/packages/12/fb/113b350b0fb0d06606e379609d24fd4482959c672baec3eeda588c8c8870/hcpre-0.5.4-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0f87f1900888b66110518cf46b47549a", "sha256": "85e87a0205413798538556ffddad1e733188fc7aefa7c99cfad3ab02515aaa22"}, "downloads": -1, "filename": "hcpre-0.5.4.tar.gz", "has_sig": true, "md5_digest": "0f87f1900888b66110518cf46b47549a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59758, "upload_time": "2014-05-19T17:35:34", "upload_time_iso_8601": "2014-05-19T17:35:34.747190Z", "url": "https://files.pythonhosted.org/packages/8c/8b/5494816ea6d8641e56cf4e27b5d3a7188ff39278fe200444363cb5622240/hcpre-0.5.4.tar.gz", "yanked": false}], "0.5.5": [{"comment_text": "", "digests": {"md5": "e800710249cbe06fc3f42c726a0e1182", "sha256": "64657ce8db70de56e8d85b117b10157ca2554abe1aebf50629e93cc1b41d9db9"}, "downloads": -1, "filename": "hcpre-0.5.5-py2.7.egg", "has_sig": true, "md5_digest": "e800710249cbe06fc3f42c726a0e1182", "packagetype": "bdist_egg", "python_version": "2.7", "requires_python": null, "size": 97745, "upload_time": "2014-06-11T20:24:32", "upload_time_iso_8601": "2014-06-11T20:24:32.376281Z", "url": "https://files.pythonhosted.org/packages/a4/6a/9e6f38113be7c817b771d09f4914e0d6a6e2552ff85e783af2eecb9f5792/hcpre-0.5.5-py2.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "0e803125e7c11ebfd880cc6fa095e7f1", "sha256": "50fae78fe2e8504485eed0d0f099c16709372f37be5d075ec832f6a1063d5435"}, "downloads": -1, "filename": "hcpre-0.5.5-py2-none-any.whl", "has_sig": true, "md5_digest": "0e803125e7c11ebfd880cc6fa095e7f1", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 78233, "upload_time": "2014-06-11T20:24:35", "upload_time_iso_8601": "2014-06-11T20:24:35.547797Z", "url": "https://files.pythonhosted.org/packages/b0/13/2b821471b9a570fb8636fbbf65db1715a1c144735fffc4433ea34f749970/hcpre-0.5.5-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "264cbc2769b8975ab2bb577c0f389e52", "sha256": "74a0aa2fc04fc1a4388a92ca928b9f8a2e290bd93a67d5545a9392b7c22adeed"}, "downloads": -1, "filename": "hcpre-0.5.5.tar.gz", "has_sig": true, "md5_digest": "264cbc2769b8975ab2bb577c0f389e52", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59649, "upload_time": "2014-06-11T20:24:29", "upload_time_iso_8601": "2014-06-11T20:24:29.602289Z", "url": "https://files.pythonhosted.org/packages/50/11/d5cb4411576e53d61f2bd2402b21bda7558f04b6cee7662ae7ed03cc2d09/hcpre-0.5.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e800710249cbe06fc3f42c726a0e1182", "sha256": "64657ce8db70de56e8d85b117b10157ca2554abe1aebf50629e93cc1b41d9db9"}, "downloads": -1, "filename": "hcpre-0.5.5-py2.7.egg", "has_sig": true, "md5_digest": "e800710249cbe06fc3f42c726a0e1182", "packagetype": "bdist_egg", "python_version": "2.7", "requires_python": null, "size": 97745, "upload_time": "2014-06-11T20:24:32", "upload_time_iso_8601": "2014-06-11T20:24:32.376281Z", "url": "https://files.pythonhosted.org/packages/a4/6a/9e6f38113be7c817b771d09f4914e0d6a6e2552ff85e783af2eecb9f5792/hcpre-0.5.5-py2.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "0e803125e7c11ebfd880cc6fa095e7f1", "sha256": "50fae78fe2e8504485eed0d0f099c16709372f37be5d075ec832f6a1063d5435"}, "downloads": -1, "filename": "hcpre-0.5.5-py2-none-any.whl", "has_sig": true, "md5_digest": "0e803125e7c11ebfd880cc6fa095e7f1", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 78233, "upload_time": "2014-06-11T20:24:35", "upload_time_iso_8601": "2014-06-11T20:24:35.547797Z", "url": "https://files.pythonhosted.org/packages/b0/13/2b821471b9a570fb8636fbbf65db1715a1c144735fffc4433ea34f749970/hcpre-0.5.5-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "264cbc2769b8975ab2bb577c0f389e52", "sha256": "74a0aa2fc04fc1a4388a92ca928b9f8a2e290bd93a67d5545a9392b7c22adeed"}, "downloads": -1, "filename": "hcpre-0.5.5.tar.gz", "has_sig": true, "md5_digest": "264cbc2769b8975ab2bb577c0f389e52", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59649, "upload_time": "2014-06-11T20:24:29", "upload_time_iso_8601": "2014-06-11T20:24:29.602289Z", "url": "https://files.pythonhosted.org/packages/50/11/d5cb4411576e53d61f2bd2402b21bda7558f04b6cee7662ae7ed03cc2d09/hcpre-0.5.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:52:07 2020"}