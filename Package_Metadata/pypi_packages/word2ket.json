{"info": {"author": "Ali Panahi", "author_email": "panaali@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "License :: OSI Approved :: BSD License", "Programming Language :: Python", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# word2ket\n**word2ket** is an space-efficient embedding layer that can reduce the space required to store the embeddings by up to 100,000x.\n\nThis is a PyTorch implementaion of the embedding layer that is proposed in the paper [word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement](https://arxiv.org/abs/1911.04975). \n\n- [word2ket](#word2ket)\n- [Installation](#installation)\n  - [Dependencies](#dependencies)\n  - [Install word2ket](#install-word2ket)\n- [Getting Started](#getting-started)\n- [Running examples](#running-examples)\n  - [Dependencies](#dependencies-1)\n  - [Download Datasets](#download-datasets)\n  - [Run text summarization](#run-text-summarization)\n  - [Run German-English machine translation](#run-german-english-machine-translation)\n- [Reference](#reference)\n  - [APA](#apa)\n  - [BibTex](#bibtex)\n- [License](#license)\n\n# Installation\n## Dependencies\n```bash\n# Install PyTorch\nconda install pytorch torchvision -c pytorch\n# Install GPyTorch\nconda install gpytorch -c gpytorch\n```\n## Install word2ket\n```bash\npip install word2ket\n```\n# Getting Started\nYou can directly use `EmbeddingKet` and `EmbeddingKetXS` layers in your model definition or you can use the `ketify` function to automatically replace all the `nn.Embedding` layers in your model. Below you can see a model that is using `7,680,000` parameters for its embedding could be `ketify` to just use `448` parameters.\n\n```python\nfrom word2ket import EmbeddingKet, EmbeddingKetXS, ketify, summary\nfrom torch import nn\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n\n# Word2Ket Embedding Layer\nw2v_embedding_layer = EmbeddingKetXS(num_embeddings=30000, embedding_dim=256, order=4, rank=1)\nsummary(w2v_embedding_layer)\n\"\"\"\nINFO:root:EmbeddingKetXS num_embeddings_leaf: 14\nINFO:root:EmbeddingKetXS embedding_dim_leaf: 4\nINFO:root:EmbeddingKetXS weight_leafs shape: torch.Size([4, 1, 14, 4])\nModule Name                                                                           Total Parameters  Trainable Parameters # Elements in Trainable Parametrs       \nEmbeddingKetXS(30000, 256)                                                            1                 1                    224                                     \nTotal number of trainable parameters elements 224\n\"\"\"\n\n# PyTorch Embedding Layer\nclass MyModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(MyModel, self).__init__()\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n\n    def forward(self, x):\n        self.embedding()\nmodel = MyModel(vocab_size=30000, embedding_dim=256)\nprint(\"embedding.weight.shape: \", model.embedding.weight.shape)\n\"\"\"\nembedding.weight.shape:  torch.Size([30000, 256])\n\"\"\"\nsummary(model)\n\"\"\"\nModule Name                                                                           Total Parameters  Trainable Parameters # Elements in Trainable Parametrs       \nEmbedding(30000, 256)                                                                 1                 1                    7,680,000                               \nTotal number of trainable parameters elements 7,680,000\n\"\"\"\n\n# Replace the nn.Embedding to EmbeddingKetXS automatically using the ketify function.\nketify(model, order=4, rank=2, use_EmbeddingKetXS=True)\nsummary(model)\n\"\"\"\nINFO:root:EmbeddingKetXS num_embeddings_leaf: 14\nINFO:root:EmbeddingKetXS embedding_dim_leaf: 4\nINFO:root:EmbeddingKetXS weight_leafs shape: torch.Size([4, 2, 14, 4])\nINFO:root:Replaced embedding in MyModel\nModule Name                                                                           Total Parameters  Trainable Parameters # Elements in Trainable Parametrs       \nEmbeddingKetXS(30000, 256)                                                            1                 1                    448                                     \nTotal number of trainable parameters elements 448\n\"\"\"\n```\n\n# Running examples\n## Dependencies\n```bash\n# Install Texar-PyTorch\npip install texar-pytorch\n# Install Rouge\npip install rouge\n```\n\n## Download Datasets\n```bash\ncd ./examples/texar/\npython ./prepare_data.py --data iwslt14\npython ./prepare_data.py --data giga\n```\n\n## Run text summarization\nUsing **GIGAWORD** dataset.\n```bash\n# Using Pytorch nn.Embedding Layer\npython seq2seq_attn.py --embedding_type nn.Embedding --gpu 4 --runName G_000 --config-model config_model --config-data config_giga\n\n# Using EmbeddingKet Layer\npython seq2seq_attn.py --embedding_type EmbeddingKet   --gpu 0 --runName V2K_G_000       --config-model config_model --config-data config_giga --order 4 --rank 1\n\n# Using EmbeddingKetXS Layer\npython seq2seq_attn.py --embedding_type EmbeddingKetXS --gpu 0 --runName V2K_XS_G_000 --config-model config_model --config-data config_giga --order 4 --rank 1\n```\n\n## Run German-English machine translation\nUsing **IWSLT2014** (DE-EN) dataset\n```bash\n# Using Pytorch nn.Embedding Layer\npython seq2seq_attn.py --embedding_type nn.Embedding --gpu 4 --runName I_000 --config-model config_model --config-data config_iwslt14\n\n# Using EmbeddingKet Layer\npython seq2seq_attn.py --embedding_type EmbeddingKet   --gpu 0 --runName V2K_I_000       --config-model config_model --config-data config_iwslt14 --order 4 --rank 1\n\n# Using EmbeddingKetXS Layer\npython seq2seq_attn.py --embedding_type EmbeddingKetXS --gpu 0 --runName V2K_XS_I_000 --config-model config_model --config-data config_iwslt14 --order 4 --rank 1\n\n```\n\n\n# Reference\nIf you use **word2ket**, please cite the tech report with the following BibTex entry:\n\n## APA\n```\nPanahi, A., Saeedi, S., & Arodz, T. (2020). word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement. In International Conference on Learning Representations. Retrieved from https://openreview.net/forum?id=HkxARkrFwB\n```\n\n## BibTex\n```\n@inproceedings{panahi2020wordket,\n  title={word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement},\n  author={Aliakbar Panahi and Seyran Saeedi and Tom Arodz},\n  booktitle={International Conference on Learning Representations},\n  year={2020},\n  url={https://openreview.net/forum?id=HkxARkrFwB}\n}\n```\n# License\n[BSD-3-Clause](./LICENSE)\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/panaali/word2ket", "keywords": "", "license": "BSD-3-Clause", "maintainer": "", "maintainer_email": "", "name": "word2ket", "package_url": "https://pypi.org/project/word2ket/", "platform": "", "project_url": "https://pypi.org/project/word2ket/", "project_urls": {"Homepage": "https://github.com/panaali/word2ket"}, "release_url": "https://pypi.org/project/word2ket/0.0.2/", "requires_dist": ["torch", "gpytorch"], "requires_python": ">=3.5", "summary": "word2ket is an effiecient embedding layer for PyTorch that is inspired by Quantum Entanglement.", "version": "0.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>word2ket</h1>\n<p><strong>word2ket</strong> is an space-efficient embedding layer that can reduce the space required to store the embeddings by up to 100,000x.</p>\n<p>This is a PyTorch implementaion of the embedding layer that is proposed in the paper <a href=\"https://arxiv.org/abs/1911.04975\" rel=\"nofollow\">word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement</a>.</p>\n<ul>\n<li><a href=\"#word2ket\" rel=\"nofollow\">word2ket</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a>\n<ul>\n<li><a href=\"#dependencies\" rel=\"nofollow\">Dependencies</a></li>\n<li><a href=\"#install-word2ket\" rel=\"nofollow\">Install word2ket</a></li>\n</ul>\n</li>\n<li><a href=\"#getting-started\" rel=\"nofollow\">Getting Started</a></li>\n<li><a href=\"#running-examples\" rel=\"nofollow\">Running examples</a>\n<ul>\n<li><a href=\"#dependencies-1\" rel=\"nofollow\">Dependencies</a></li>\n<li><a href=\"#download-datasets\" rel=\"nofollow\">Download Datasets</a></li>\n<li><a href=\"#run-text-summarization\" rel=\"nofollow\">Run text summarization</a></li>\n<li><a href=\"#run-german-english-machine-translation\" rel=\"nofollow\">Run German-English machine translation</a></li>\n</ul>\n</li>\n<li><a href=\"#reference\" rel=\"nofollow\">Reference</a>\n<ul>\n<li><a href=\"#apa\" rel=\"nofollow\">APA</a></li>\n<li><a href=\"#bibtex\" rel=\"nofollow\">BibTex</a></li>\n</ul>\n</li>\n<li><a href=\"#license\" rel=\"nofollow\">License</a></li>\n</ul>\n<h1>Installation</h1>\n<h2>Dependencies</h2>\n<pre><span class=\"c1\"># Install PyTorch</span>\nconda install pytorch torchvision -c pytorch\n<span class=\"c1\"># Install GPyTorch</span>\nconda install gpytorch -c gpytorch\n</pre>\n<h2>Install word2ket</h2>\n<pre>pip install word2ket\n</pre>\n<h1>Getting Started</h1>\n<p>You can directly use <code>EmbeddingKet</code> and <code>EmbeddingKetXS</code> layers in your model definition or you can use the <code>ketify</code> function to automatically replace all the <code>nn.Embedding</code> layers in your model. Below you can see a model that is using <code>7,680,000</code> parameters for its embedding could be <code>ketify</code> to just use <code>448</code> parameters.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">word2ket</span> <span class=\"kn\">import</span> <span class=\"n\">EmbeddingKet</span><span class=\"p\">,</span> <span class=\"n\">EmbeddingKetXS</span><span class=\"p\">,</span> <span class=\"n\">ketify</span><span class=\"p\">,</span> <span class=\"n\">summary</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch</span> <span class=\"kn\">import</span> <span class=\"n\">nn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n<span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n\n\n<span class=\"c1\"># Word2Ket Embedding Layer</span>\n<span class=\"n\">w2v_embedding_layer</span> <span class=\"o\">=</span> <span class=\"n\">EmbeddingKetXS</span><span class=\"p\">(</span><span class=\"n\">num_embeddings</span><span class=\"o\">=</span><span class=\"mi\">30000</span><span class=\"p\">,</span> <span class=\"n\">embedding_dim</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">rank</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">w2v_embedding_layer</span><span class=\"p\">)</span>\n<span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">INFO:root:EmbeddingKetXS num_embeddings_leaf: 14</span>\n<span class=\"sd\">INFO:root:EmbeddingKetXS embedding_dim_leaf: 4</span>\n<span class=\"sd\">INFO:root:EmbeddingKetXS weight_leafs shape: torch.Size([4, 1, 14, 4])</span>\n<span class=\"sd\">Module Name                                                                           Total Parameters  Trainable Parameters # Elements in Trainable Parametrs       </span>\n<span class=\"sd\">EmbeddingKetXS(30000, 256)                                                            1                 1                    224                                     </span>\n<span class=\"sd\">Total number of trainable parameters elements 224</span>\n<span class=\"sd\">\"\"\"</span>\n\n<span class=\"c1\"># PyTorch Embedding Layer</span>\n<span class=\"k\">class</span> <span class=\"nc\">MyModel</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">vocab_size</span><span class=\"p\">,</span> <span class=\"n\">embedding_dim</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">MyModel</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">embedding</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Embedding</span><span class=\"p\">(</span><span class=\"n\">num_embeddings</span><span class=\"o\">=</span><span class=\"n\">vocab_size</span><span class=\"p\">,</span> <span class=\"n\">embedding_dim</span><span class=\"o\">=</span><span class=\"n\">embedding_dim</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">embedding</span><span class=\"p\">()</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">MyModel</span><span class=\"p\">(</span><span class=\"n\">vocab_size</span><span class=\"o\">=</span><span class=\"mi\">30000</span><span class=\"p\">,</span> <span class=\"n\">embedding_dim</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"embedding.weight.shape: \"</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">embedding</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">embedding.weight.shape:  torch.Size([30000, 256])</span>\n<span class=\"sd\">\"\"\"</span>\n<span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n<span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">Module Name                                                                           Total Parameters  Trainable Parameters # Elements in Trainable Parametrs       </span>\n<span class=\"sd\">Embedding(30000, 256)                                                                 1                 1                    7,680,000                               </span>\n<span class=\"sd\">Total number of trainable parameters elements 7,680,000</span>\n<span class=\"sd\">\"\"\"</span>\n\n<span class=\"c1\"># Replace the nn.Embedding to EmbeddingKetXS automatically using the ketify function.</span>\n<span class=\"n\">ketify</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">rank</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">use_EmbeddingKetXS</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n<span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">INFO:root:EmbeddingKetXS num_embeddings_leaf: 14</span>\n<span class=\"sd\">INFO:root:EmbeddingKetXS embedding_dim_leaf: 4</span>\n<span class=\"sd\">INFO:root:EmbeddingKetXS weight_leafs shape: torch.Size([4, 2, 14, 4])</span>\n<span class=\"sd\">INFO:root:Replaced embedding in MyModel</span>\n<span class=\"sd\">Module Name                                                                           Total Parameters  Trainable Parameters # Elements in Trainable Parametrs       </span>\n<span class=\"sd\">EmbeddingKetXS(30000, 256)                                                            1                 1                    448                                     </span>\n<span class=\"sd\">Total number of trainable parameters elements 448</span>\n<span class=\"sd\">\"\"\"</span>\n</pre>\n<h1>Running examples</h1>\n<h2>Dependencies</h2>\n<pre><span class=\"c1\"># Install Texar-PyTorch</span>\npip install texar-pytorch\n<span class=\"c1\"># Install Rouge</span>\npip install rouge\n</pre>\n<h2>Download Datasets</h2>\n<pre><span class=\"nb\">cd</span> ./examples/texar/\npython ./prepare_data.py --data iwslt14\npython ./prepare_data.py --data giga\n</pre>\n<h2>Run text summarization</h2>\n<p>Using <strong>GIGAWORD</strong> dataset.</p>\n<pre><span class=\"c1\"># Using Pytorch nn.Embedding Layer</span>\npython seq2seq_attn.py --embedding_type nn.Embedding --gpu <span class=\"m\">4</span> --runName G_000 --config-model config_model --config-data config_giga\n\n<span class=\"c1\"># Using EmbeddingKet Layer</span>\npython seq2seq_attn.py --embedding_type EmbeddingKet   --gpu <span class=\"m\">0</span> --runName V2K_G_000       --config-model config_model --config-data config_giga --order <span class=\"m\">4</span> --rank <span class=\"m\">1</span>\n\n<span class=\"c1\"># Using EmbeddingKetXS Layer</span>\npython seq2seq_attn.py --embedding_type EmbeddingKetXS --gpu <span class=\"m\">0</span> --runName V2K_XS_G_000 --config-model config_model --config-data config_giga --order <span class=\"m\">4</span> --rank <span class=\"m\">1</span>\n</pre>\n<h2>Run German-English machine translation</h2>\n<p>Using <strong>IWSLT2014</strong> (DE-EN) dataset</p>\n<pre><span class=\"c1\"># Using Pytorch nn.Embedding Layer</span>\npython seq2seq_attn.py --embedding_type nn.Embedding --gpu <span class=\"m\">4</span> --runName I_000 --config-model config_model --config-data config_iwslt14\n\n<span class=\"c1\"># Using EmbeddingKet Layer</span>\npython seq2seq_attn.py --embedding_type EmbeddingKet   --gpu <span class=\"m\">0</span> --runName V2K_I_000       --config-model config_model --config-data config_iwslt14 --order <span class=\"m\">4</span> --rank <span class=\"m\">1</span>\n\n<span class=\"c1\"># Using EmbeddingKetXS Layer</span>\npython seq2seq_attn.py --embedding_type EmbeddingKetXS --gpu <span class=\"m\">0</span> --runName V2K_XS_I_000 --config-model config_model --config-data config_iwslt14 --order <span class=\"m\">4</span> --rank <span class=\"m\">1</span>\n</pre>\n<h1>Reference</h1>\n<p>If you use <strong>word2ket</strong>, please cite the tech report with the following BibTex entry:</p>\n<h2>APA</h2>\n<pre><code>Panahi, A., Saeedi, S., &amp; Arodz, T. (2020). word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement. In International Conference on Learning Representations. Retrieved from https://openreview.net/forum?id=HkxARkrFwB\n</code></pre>\n<h2>BibTex</h2>\n<pre><code>@inproceedings{panahi2020wordket,\n  title={word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement},\n  author={Aliakbar Panahi and Seyran Saeedi and Tom Arodz},\n  booktitle={International Conference on Learning Representations},\n  year={2020},\n  url={https://openreview.net/forum?id=HkxARkrFwB}\n}\n</code></pre>\n<h1>License</h1>\n<p><a href=\"./LICENSE\" rel=\"nofollow\">BSD-3-Clause</a></p>\n\n          </div>"}, "last_serial": 6600108, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "d3bfc72658d9bc3763ca040843478cfb", "sha256": "16373563568ba9b839ff741a62b3ab2057405ea9a6a0afc31f3a6677894e71c9"}, "downloads": -1, "filename": "word2ket-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "d3bfc72658d9bc3763ca040843478cfb", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 7780, "upload_time": "2020-02-10T02:29:30", "upload_time_iso_8601": "2020-02-10T02:29:30.822885Z", "url": "https://files.pythonhosted.org/packages/07/34/37747b69d8868165aa451ca378d59fd3cc1b9ad1f5ea153318df0080cb0e/word2ket-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "68cdd01ab7619e2b39ebe566d14005b1", "sha256": "6c44da8e235aefd4dde7aa6f0d607e40d0cbe2769d5f076fd17a4284d2139d6a"}, "downloads": -1, "filename": "word2ket-0.0.1.tar.gz", "has_sig": false, "md5_digest": "68cdd01ab7619e2b39ebe566d14005b1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 7196, "upload_time": "2020-02-10T02:29:33", "upload_time_iso_8601": "2020-02-10T02:29:33.168319Z", "url": "https://files.pythonhosted.org/packages/c4/e5/e6bee40e5862b016a08699385737ea304b236af3a92d2fdd74fc2379a5ea/word2ket-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "c0d2cc9644b555434c2de3f3cb65ce22", "sha256": "8e8b2bc017bc6c861b9fa897885f42a53591ccfe57b2dfe71a085746bc4190d9"}, "downloads": -1, "filename": "word2ket-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "c0d2cc9644b555434c2de3f3cb65ce22", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 7728, "upload_time": "2020-02-10T03:07:14", "upload_time_iso_8601": "2020-02-10T03:07:14.855520Z", "url": "https://files.pythonhosted.org/packages/78/09/6b115fd2d10caff1d7b25cd2e39da803eb4e954c27f62f07c39862ab13dc/word2ket-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c69463a4870cad97a1946ffb4192d86b", "sha256": "93a21fcf58ac5ad8845708e812af77d4542de1b99d55bae9e4e24af6ed57573a"}, "downloads": -1, "filename": "word2ket-0.0.2.tar.gz", "has_sig": false, "md5_digest": "c69463a4870cad97a1946ffb4192d86b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 7129, "upload_time": "2020-02-10T03:07:16", "upload_time_iso_8601": "2020-02-10T03:07:16.410045Z", "url": "https://files.pythonhosted.org/packages/48/e1/dbe5feac5f09512d3f690e5589ad4c993550e5fe3d4c42c26cea18ec6ce5/word2ket-0.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c0d2cc9644b555434c2de3f3cb65ce22", "sha256": "8e8b2bc017bc6c861b9fa897885f42a53591ccfe57b2dfe71a085746bc4190d9"}, "downloads": -1, "filename": "word2ket-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "c0d2cc9644b555434c2de3f3cb65ce22", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 7728, "upload_time": "2020-02-10T03:07:14", "upload_time_iso_8601": "2020-02-10T03:07:14.855520Z", "url": "https://files.pythonhosted.org/packages/78/09/6b115fd2d10caff1d7b25cd2e39da803eb4e954c27f62f07c39862ab13dc/word2ket-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c69463a4870cad97a1946ffb4192d86b", "sha256": "93a21fcf58ac5ad8845708e812af77d4542de1b99d55bae9e4e24af6ed57573a"}, "downloads": -1, "filename": "word2ket-0.0.2.tar.gz", "has_sig": false, "md5_digest": "c69463a4870cad97a1946ffb4192d86b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 7129, "upload_time": "2020-02-10T03:07:16", "upload_time_iso_8601": "2020-02-10T03:07:16.410045Z", "url": "https://files.pythonhosted.org/packages/48/e1/dbe5feac5f09512d3f690e5589ad4c993550e5fe3d4c42c26cea18ec6ce5/word2ket-0.0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:28:07 2020"}