{"info": {"author": "MadryLab", "author_email": "ailyas@mit.edu", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Software Development :: Build Tools"], "description": "robustness package\n==================\nInstall via ``pip``: ``pip install robustness``\n\nRead the docs: https://robustness.readthedocs.io/en/latest/index.html\n\n``robustness`` is a package we (students in the `MadryLab <http://madry-lab.ml>`_) created\nto make training, evaluating, and exploring neural networks flexible and easy.\nWe use it in almost all of our projects (whether they involve\nadversarial training or not!) and it will be a dependency in many of our\nupcoming code releases. A few projects using the library include:\n\n- `Code for \"Learning Perceptually-Aligned Representations via Adversarial Robustness\" <https://github.com/MadryLab/robust_representations>`_ (https://arxiv.org/abs/1906.00945) \n- `Code for\n  \"Image Synthesis with a Single (Robust) Classifier\" <https://github.com/MadryLab/robustness_applications>`_ (https://arxiv.org/abs/1906.09453)\n\nWe\ndemonstrate how to use the library in a set of walkthroughs and our API\nreference. Functionality provided by the library includes:\n\n- Training and evaluating standard and robust models for a variety of\n  datasets/architectures using a `CLI interface\n  <https://robustness.readthedocs.io/en/latest/example_usage/cli_usage.html>`_. The library also provides support for adding\n  `custom datasets <https://robustness.readthedocs.io/en/latest/example_usage/training_lib_part_2.html#training-on-custom-datasets>`_ and `model architectures <https://robustness.readthedocs.io/en/latest/example_usage/training_lib_part_2.html#training-with-custom-architectures>`_.\n\n.. code-block:: bash\n\n   python -m robustness.main --dataset cifar --data /path/to/cifar \\\n      --adv-train 0 --arch resnet18 --out-dir /logs/checkpoints/dir/\n\n- Performing `input manipulation\n  <https://robustness.readthedocs.io/en/latest/example_usage/input_space_manipulation.html>`_ using robust (or standard)\n  models---this includes making adversarial examples, inverting representations,\n  feature visualization, etc. The library offers a variety of optimization\n  options (e.g. choice between real/estimated gradients, Fourier/pixel basis,\n  custom loss functions etc.), and is easily extendable.\n\n.. code-block:: python\n\n   import torch as ch\n   from robustness.datasets import CIFAR\n   from robustness.model_utils import make_and_restore_model\n\n   ds = CIFAR('/path/to/cifar')\n   model, _ = make_and_restore_model(arch='resnet50', dataset=ds, \n                resume_path='/path/to/model', state_dict_path='model')\n   model.eval()\n   attack_kwargs = {\n      'constraint': 'inf', # L-inf PGD \n      'eps': 0.05, # Epsilon constraint (L-inf norm)\n      'step_size': 0.01, # Learning rate for PGD\n      'iterations': 100, # Number of PGD steps\n      'targeted': True # Targeted attack\n      'custom_loss': None # Use default cross-entropy loss\n   }\n\n   _, test_loader = ds.make_loaders(workers=0, batch_size=10)\n   im, label = next(iter(test_loader))\n   target_label = (label + ch.randint_like(label, high=9)) % 10\n   adv_out, adv_im = model(im, target_label, make_adv, **attack_kwargs)\n\n- Importing ``robustness`` as a package, which allows for easy training of\n  neural networks with support for custom loss functions, logging, data loading,\n  and more! A good introduction can be found in our two-part walkthrough\n  (`Part 1 <https://robustness.readthedocs.io/en/latest/example_usage/training_lib_part_1.html>`_, \n  `Part 2 <https://robustness.readthedocs.io/en/latest/example_usage/training_lib_part_2.html>`_).\n\n.. code-block:: python\n\n   from robustness import model_utils, datasets, train, defaults\n   from robustness.datasets import CIFAR\n\n   # We use cox (http://github.com/MadryLab/cox) to log, store and analyze \n   # results. Read more at https//cox.readthedocs.io.\n   from cox.utils import Parameters\n   import cox.store\n\n   # Hard-coded dataset, architecture, batch size, workers\n   ds = CIFAR('/path/to/cifar')\n   m, _ = model_utils.make_and_restore_model(arch='resnet50', dataset=ds)\n   train_loader, val_loader = ds.make_loaders(batch_size=128, workers=8)\n\n   # Create a cox store for logging\n   out_store = cox.store.Store(OUT_DIR)\n\n   # Hard-coded base parameters\n   train_kwargs = {\n       'out_dir': \"train_out\",\n       'adv_train': 1,\n       'constraint': '2',\n       'eps': 0.5,\n       'attack_lr': 1.5,\n       'attack_steps': 20\n   }\n   train_args = Parameters(train_kwargs)\n\n   # Fill whatever parameters are missing from the defaults\n   train_args = defaults.check_and_fill_args(train_args,\n                           defaults.TRAINING_ARGS, CIFAR)\n   train_args = defaults.check_and_fill_args(train_args,\n                           defaults.PGD_ARGS, CIFAR)\n\n   # Train a model\n   train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n\n**Note**: ``robustness`` requires PyTorch to be installed with CUDA support.\n\nPretrained models\n-----------------\n\nAlong with the training code, we release a number of pretrained models for\ndifferent datasets, norms and \u03b5-train values. This list will be updated as\nwe release more or improved models. *Please cite this library (see bibtex\nentry below) if you use these models in your research.* \n\nFor each (model, \u03b5-test) combination we evaluate 20-step and 100-step PGD with a\nstep size of `2.5 * \u03b5-test / num_steps`. Since these two accuracies are quite \nclose to each other, we do not consider more steps of PGD.\nFor each value of \u03b5-test, we highlight the best robust accuracy achieved over\ndifferent \u03b5-train in bold.\n\n**Note #1**: We did not perform any hyperparameter tuning and simply used the same\nhyperparameters as standard training. It is likely that exploring different \ntraining hyperparameters will increasse these robust accuracies by a few percent\npoints.\n\n**Note #2**: The pytorch checkpoint (``.pt``) files below were saved with the following versions of PyTorch and Dill:\n\n.. code-block::\n\n  torch==1.1.0\n  dill==0.2.9\n\n\nCIFAR10 L2-norm (ResNet50):\n\n- `\u03b5 = 0.0 <https://www.dropbox.com/s/yhpp4yws7sgi6lj/cifar_nat.pt?dl=0>`_ (standard training)\n- `\u03b5 = 0.25 <https://www.dropbox.com/s/2qsp7pt6t7uo71w/cifar_l2_0_25.pt?dl=0>`_\n- `\u03b5 = 0.5 <https://www.dropbox.com/s/1zazwjfzee7c8i4/cifar_l2_0_5.pt?dl=0>`_\n- `\u03b5 = 1.0 <https://www.dropbox.com/s/s2x7thisiqxz095/cifar_l2_1_0.pt?dl=0>`_\n\n+--------------+----------------+-----------------+---------------------+---------------------+\n| CIFAR10 L2-robust accuracy                                                                  |\n+--------------+----------------+-----------------+---------------------+---------------------+\n|              | \u03b5-train                                                                      |\n+--------------+----------------+-----------------+---------------------+---------------------+\n| \u03b5-test       | 0.0            | 0.25            | 0.5                 | 1.0                 |\n+==============+================+=================+=====================+=====================+\n| 0.0          | **95.25% / -** | 92.77%  / -     | 90.83% / -          | 81.62% / -          |\n+--------------+----------------+-----------------+---------------------+---------------------+\n| 0.25         |  8.66% / 7.34% | 81.21% / 81.19% | **82.34% / 82.31%** | 75.53% / 75.53%     |\n+--------------+----------------+-----------------+---------------------+---------------------+\n| 0.5          |  0.28% / 0.14% | 62.30% / 62.13% | **70.17% / 70.11%** | 68.63% / 68.61%     |\n+--------------+----------------+-----------------+---------------------+---------------------+\n| 1.0          |  0.00% / 0.00% | 21.18% / 20.66% | 40.47% / 40.22%     | **52.72% / 52.61%** |\n+--------------+----------------+-----------------+---------------------+---------------------+\n| 2.0          |  0.00% / 0.00% |  0.58% /  0.46% |  5.23% /  4.97%     | **18.59% / 18.05%** |\n+--------------+----------------+-----------------+---------------------+---------------------+\n\nCIFAR10 Linf-norm (ResNet50):\n\n- \u03b5 = 0.0 (PyTorch pre-trained)\n- `\u03b5 = 8/255 <https://www.dropbox.com/s/c9qlt1lbdnu9tlo/cifar_linf_8.pt?dl=0>`_\n\n+--------------+-----------------+---------------------+\n| CIFAR10 Linf-robust accuracy                         |\n+--------------+-----------------+---------------------+\n|              | \u03b5-train                               |\n+--------------+-----------------+---------------------+\n| \u03b5-test       | 0 / 255         | 8 / 255             |\n+==============+=================+=====================+\n|  0 / 255     | **95.25% / -**  | 87.03%  / -         |\n+--------------+-----------------+---------------------+\n|  8 / 255     |  0.00% /  0.00% | **53.49% / 53.29%** |\n+--------------+-----------------+---------------------+\n| 16 / 255     |  0.00% /  0.00% | **18.13% / 17.62%** |\n+--------------+-----------------+---------------------+\n\nImageNet L2-norm (ResNet50):\n\n- \u03b5 = 0.0 (PyTorch pre-trained)\n- `\u03b5 = 3.0 <https://www.dropbox.com/s/knf4uimlqsi1yz8/imagenet_l2_3_0.pt?dl=0>`_\n\n+--------------+-----------------+---------------------+\n| ImageNet L2-robust accuracy                          |\n+--------------+-----------------+---------------------+\n|              | \u03b5-train                               |\n+--------------+-----------------+---------------------+\n| \u03b5-test       | 0.0             | 3.0                 |\n+==============+=================+=====================+\n| 0.0          | **76.13% / -**  | 57.90%  / -         |\n+--------------+-----------------+---------------------+\n| 0.5          |  3.35% /  2.98% | **54.42% / 54.42%** |\n+--------------+-----------------+---------------------+\n| 1.0          |  0.44% /  0.37% | **50.67% / 50.67%** |\n+--------------+-----------------+---------------------+\n| 2.0          |  0.16% /  0.14% | **43.04% / 43.02%** |\n+--------------+-----------------+---------------------+\n| 3.0          |  0.13% /  0.12% | **35.16% / 35.09%** |\n+--------------+-----------------+---------------------+\n\nImageNet Linf-norm (ResNet50):\n\n- \u03b5 = 0.0 (PyTorch pre-trained)\n- `\u03b5 = 4 / 255 <https://www.dropbox.com/s/axfuary2w1cnyrg/imagenet_linf_4.pt?dl=0>`_\n- `\u03b5 = 8 / 255 <https://www.dropbox.com/s/yxn15a9zklz3s8q/imagenet_linf_8.pt?dl=0>`_\n\n+--------------+-----------------+---------------------+---------------------+\n| ImageNet Linf-robust accuracy                                              |\n+--------------+-----------------+---------------------+---------------------+\n|              | \u03b5-train                                                     |\n+--------------+-----------------+---------------------+---------------------+\n| \u03b5-test       | 0.0             | 4 / 255             | 8 / 255             |\n+==============+=================+=====================+=====================+\n|  0 / 255     | **76.13% / -**  | 62.42%  / -         | 47.91%  / -         |\n+--------------+-----------------+---------------------+---------------------+\n|  4 / 255     | 0.04% / 0.03%   | **33.58% / 33.38%** |   33.06% / 33.03%   |\n+--------------+-----------------+---------------------+---------------------+\n|  8 / 255     | 0.01% / 0.01%   |   13.13% / 12.73%   | **19.63% / 19.52%** |\n+--------------+-----------------+---------------------+---------------------+\n| 16 / 255     | 0.01% / 0.01%   |    1.53% /  1.37%   |  **5.00% /  4.82%** |\n+--------------+-----------------+---------------------+---------------------+\n\nCitation\n--------\nIf you use this library in your research, cite it as\nfollows:\n\n.. code-block:: bibtex\n\n   @misc{robustness,\n      title={Robustness (Python Library)},\n      author={Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras},\n      year={2019},\n      url={https://github.com/MadryLab/robustness}\n   }\n\n*(Have you used the package and found it useful? Let us know!)*.\n\nContributors\n-------------\n- `Andrew Ilyas <https://twitter.com/andrew_ilyas>`_\n- `Logan Engstrom <https://twitter.com/logan_engstrom>`_\n- `Shibani Santurkar <https://twitter.com/ShibaniSan>`_\n- `Dimitris Tsipras <https://twitter.com/tsiprasd>`_\n\n\n\n", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "logging tools madrylab", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "robustness", "package_url": "https://pypi.org/project/robustness/", "platform": "", "project_url": "https://pypi.org/project/robustness/", "project_urls": null, "release_url": "https://pypi.org/project/robustness/1.1.post2/", "requires_dist": ["tqdm", "grpcio", "psutil", "gitpython", "py3nvml", "cox", "scikit-learn", "seaborn", "torch", "torchvision", "pandas", "numpy", "scipy", "GPUtil", "dill", "tensorboardX", "tables", "matplotlib"], "requires_python": "", "summary": "Tools for Robustness", "version": "1.1.post2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Install via <tt>pip</tt>: <tt>pip install robustness</tt></p>\n<p>Read the docs: <a href=\"https://robustness.readthedocs.io/en/latest/index.html\" rel=\"nofollow\">https://robustness.readthedocs.io/en/latest/index.html</a></p>\n<p><tt>robustness</tt> is a package we (students in the <a href=\"http://madry-lab.ml\" rel=\"nofollow\">MadryLab</a>) created\nto make training, evaluating, and exploring neural networks flexible and easy.\nWe use it in almost all of our projects (whether they involve\nadversarial training or not!) and it will be a dependency in many of our\nupcoming code releases. A few projects using the library include:</p>\n<ul>\n<li><a href=\"https://github.com/MadryLab/robust_representations\" rel=\"nofollow\">Code for \u201cLearning Perceptually-Aligned Representations via Adversarial Robustness\u201d</a> (<a href=\"https://arxiv.org/abs/1906.00945\" rel=\"nofollow\">https://arxiv.org/abs/1906.00945</a>)</li>\n<li><a href=\"https://github.com/MadryLab/robustness_applications\" rel=\"nofollow\">Code for\n\u201cImage Synthesis with a Single (Robust) Classifier\u201d</a> (<a href=\"https://arxiv.org/abs/1906.09453\" rel=\"nofollow\">https://arxiv.org/abs/1906.09453</a>)</li>\n</ul>\n<p>We\ndemonstrate how to use the library in a set of walkthroughs and our API\nreference. Functionality provided by the library includes:</p>\n<ul>\n<li>Training and evaluating standard and robust models for a variety of\ndatasets/architectures using a <a href=\"https://robustness.readthedocs.io/en/latest/example_usage/cli_usage.html\" rel=\"nofollow\">CLI interface</a>. The library also provides support for adding\n<a href=\"https://robustness.readthedocs.io/en/latest/example_usage/training_lib_part_2.html#training-on-custom-datasets\" rel=\"nofollow\">custom datasets</a> and <a href=\"https://robustness.readthedocs.io/en/latest/example_usage/training_lib_part_2.html#training-with-custom-architectures\" rel=\"nofollow\">model architectures</a>.</li>\n</ul>\n<pre>python -m robustness.main --dataset cifar --data /path/to/cifar <span class=\"se\">\\\n</span>   --adv-train <span class=\"m\">0</span> --arch resnet18 --out-dir /logs/checkpoints/dir/\n</pre>\n<ul>\n<li>Performing <a href=\"https://robustness.readthedocs.io/en/latest/example_usage/input_space_manipulation.html\" rel=\"nofollow\">input manipulation</a> using robust (or standard)\nmodels\u2014this includes making adversarial examples, inverting representations,\nfeature visualization, etc. The library offers a variety of optimization\noptions (e.g. choice between real/estimated gradients, Fourier/pixel basis,\ncustom loss functions etc.), and is easily extendable.</li>\n</ul>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span> <span class=\"k\">as</span> <span class=\"nn\">ch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">robustness.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">CIFAR</span>\n<span class=\"kn\">from</span> <span class=\"nn\">robustness.model_utils</span> <span class=\"kn\">import</span> <span class=\"n\">make_and_restore_model</span>\n\n<span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"n\">CIFAR</span><span class=\"p\">(</span><span class=\"s1\">'/path/to/cifar'</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">make_and_restore_model</span><span class=\"p\">(</span><span class=\"n\">arch</span><span class=\"o\">=</span><span class=\"s1\">'resnet50'</span><span class=\"p\">,</span> <span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"n\">ds</span><span class=\"p\">,</span>\n             <span class=\"n\">resume_path</span><span class=\"o\">=</span><span class=\"s1\">'/path/to/model'</span><span class=\"p\">,</span> <span class=\"n\">state_dict_path</span><span class=\"o\">=</span><span class=\"s1\">'model'</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\n<span class=\"n\">attack_kwargs</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n   <span class=\"s1\">'constraint'</span><span class=\"p\">:</span> <span class=\"s1\">'inf'</span><span class=\"p\">,</span> <span class=\"c1\"># L-inf PGD</span>\n   <span class=\"s1\">'eps'</span><span class=\"p\">:</span> <span class=\"mf\">0.05</span><span class=\"p\">,</span> <span class=\"c1\"># Epsilon constraint (L-inf norm)</span>\n   <span class=\"s1\">'step_size'</span><span class=\"p\">:</span> <span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"c1\"># Learning rate for PGD</span>\n   <span class=\"s1\">'iterations'</span><span class=\"p\">:</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"c1\"># Number of PGD steps</span>\n   <span class=\"s1\">'targeted'</span><span class=\"p\">:</span> <span class=\"kc\">True</span> <span class=\"c1\"># Targeted attack</span>\n   <span class=\"s1\">'custom_loss'</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"c1\"># Use default cross-entropy loss</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">test_loader</span> <span class=\"o\">=</span> <span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">make_loaders</span><span class=\"p\">(</span><span class=\"n\">workers</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"n\">im</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"nb\">iter</span><span class=\"p\">(</span><span class=\"n\">test_loader</span><span class=\"p\">))</span>\n<span class=\"n\">target_label</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">label</span> <span class=\"o\">+</span> <span class=\"n\">ch</span><span class=\"o\">.</span><span class=\"n\">randint_like</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">,</span> <span class=\"n\">high</span><span class=\"o\">=</span><span class=\"mi\">9</span><span class=\"p\">))</span> <span class=\"o\">%</span> <span class=\"mi\">10</span>\n<span class=\"n\">adv_out</span><span class=\"p\">,</span> <span class=\"n\">adv_im</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">im</span><span class=\"p\">,</span> <span class=\"n\">target_label</span><span class=\"p\">,</span> <span class=\"n\">make_adv</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">attack_kwargs</span><span class=\"p\">)</span>\n</pre>\n<ul>\n<li>Importing <tt>robustness</tt> as a package, which allows for easy training of\nneural networks with support for custom loss functions, logging, data loading,\nand more! A good introduction can be found in our two-part walkthrough\n(<a href=\"https://robustness.readthedocs.io/en/latest/example_usage/training_lib_part_1.html\" rel=\"nofollow\">Part 1</a>,\n<a href=\"https://robustness.readthedocs.io/en/latest/example_usage/training_lib_part_2.html\" rel=\"nofollow\">Part 2</a>).</li>\n</ul>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">robustness</span> <span class=\"kn\">import</span> <span class=\"n\">model_utils</span><span class=\"p\">,</span> <span class=\"n\">datasets</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">defaults</span>\n<span class=\"kn\">from</span> <span class=\"nn\">robustness.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">CIFAR</span>\n\n<span class=\"c1\"># We use cox (http://github.com/MadryLab/cox) to log, store and analyze</span>\n<span class=\"c1\"># results. Read more at https//cox.readthedocs.io.</span>\n<span class=\"kn\">from</span> <span class=\"nn\">cox.utils</span> <span class=\"kn\">import</span> <span class=\"n\">Parameters</span>\n<span class=\"kn\">import</span> <span class=\"nn\">cox.store</span>\n\n<span class=\"c1\"># Hard-coded dataset, architecture, batch size, workers</span>\n<span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"n\">CIFAR</span><span class=\"p\">(</span><span class=\"s1\">'/path/to/cifar'</span><span class=\"p\">)</span>\n<span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">model_utils</span><span class=\"o\">.</span><span class=\"n\">make_and_restore_model</span><span class=\"p\">(</span><span class=\"n\">arch</span><span class=\"o\">=</span><span class=\"s1\">'resnet50'</span><span class=\"p\">,</span> <span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"n\">ds</span><span class=\"p\">)</span>\n<span class=\"n\">train_loader</span><span class=\"p\">,</span> <span class=\"n\">val_loader</span> <span class=\"o\">=</span> <span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">make_loaders</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">workers</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create a cox store for logging</span>\n<span class=\"n\">out_store</span> <span class=\"o\">=</span> <span class=\"n\">cox</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">Store</span><span class=\"p\">(</span><span class=\"n\">OUT_DIR</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Hard-coded base parameters</span>\n<span class=\"n\">train_kwargs</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s1\">'out_dir'</span><span class=\"p\">:</span> <span class=\"s2\">\"train_out\"</span><span class=\"p\">,</span>\n    <span class=\"s1\">'adv_train'</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"s1\">'constraint'</span><span class=\"p\">:</span> <span class=\"s1\">'2'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'eps'</span><span class=\"p\">:</span> <span class=\"mf\">0.5</span><span class=\"p\">,</span>\n    <span class=\"s1\">'attack_lr'</span><span class=\"p\">:</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span>\n    <span class=\"s1\">'attack_steps'</span><span class=\"p\">:</span> <span class=\"mi\">20</span>\n<span class=\"p\">}</span>\n<span class=\"n\">train_args</span> <span class=\"o\">=</span> <span class=\"n\">Parameters</span><span class=\"p\">(</span><span class=\"n\">train_kwargs</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Fill whatever parameters are missing from the defaults</span>\n<span class=\"n\">train_args</span> <span class=\"o\">=</span> <span class=\"n\">defaults</span><span class=\"o\">.</span><span class=\"n\">check_and_fill_args</span><span class=\"p\">(</span><span class=\"n\">train_args</span><span class=\"p\">,</span>\n                        <span class=\"n\">defaults</span><span class=\"o\">.</span><span class=\"n\">TRAINING_ARGS</span><span class=\"p\">,</span> <span class=\"n\">CIFAR</span><span class=\"p\">)</span>\n<span class=\"n\">train_args</span> <span class=\"o\">=</span> <span class=\"n\">defaults</span><span class=\"o\">.</span><span class=\"n\">check_and_fill_args</span><span class=\"p\">(</span><span class=\"n\">train_args</span><span class=\"p\">,</span>\n                        <span class=\"n\">defaults</span><span class=\"o\">.</span><span class=\"n\">PGD_ARGS</span><span class=\"p\">,</span> <span class=\"n\">CIFAR</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Train a model</span>\n<span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"n\">train_args</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">train_loader</span><span class=\"p\">,</span> <span class=\"n\">val_loader</span><span class=\"p\">),</span> <span class=\"n\">store</span><span class=\"o\">=</span><span class=\"n\">out_store</span><span class=\"p\">)</span>\n</pre>\n<p><strong>Note</strong>: <tt>robustness</tt> requires PyTorch to be installed with CUDA support.</p>\n<div id=\"pretrained-models\">\n<h2>Pretrained models</h2>\n<p>Along with the training code, we release a number of pretrained models for\ndifferent datasets, norms and \u03b5-train values. This list will be updated as\nwe release more or improved models. <em>Please cite this library (see bibtex\nentry below) if you use these models in your research.</em></p>\n<p>For each (model, \u03b5-test) combination we evaluate 20-step and 100-step PGD with a\nstep size of <cite>2.5 * \u03b5-test / num_steps</cite>. Since these two accuracies are quite\nclose to each other, we do not consider more steps of PGD.\nFor each value of \u03b5-test, we highlight the best robust accuracy achieved over\ndifferent \u03b5-train in bold.</p>\n<p><strong>Note #1</strong>: We did not perform any hyperparameter tuning and simply used the same\nhyperparameters as standard training. It is likely that exploring different\ntraining hyperparameters will increasse these robust accuracies by a few percent\npoints.</p>\n<p><strong>Note #2</strong>: The pytorch checkpoint (<tt>.pt</tt>) files below were saved with the following versions of PyTorch and Dill:</p>\n<pre>torch==1.1.0\ndill==0.2.9\n</pre>\n<p>CIFAR10 L2-norm (ResNet50):</p>\n<ul>\n<li><a href=\"https://www.dropbox.com/s/yhpp4yws7sgi6lj/cifar_nat.pt?dl=0\" rel=\"nofollow\">\u03b5 = 0.0</a> (standard training)</li>\n<li><a href=\"https://www.dropbox.com/s/2qsp7pt6t7uo71w/cifar_l2_0_25.pt?dl=0\" rel=\"nofollow\">\u03b5 = 0.25</a></li>\n<li><a href=\"https://www.dropbox.com/s/1zazwjfzee7c8i4/cifar_l2_0_5.pt?dl=0\" rel=\"nofollow\">\u03b5 = 0.5</a></li>\n<li><a href=\"https://www.dropbox.com/s/s2x7thisiqxz095/cifar_l2_1_0.pt?dl=0\" rel=\"nofollow\">\u03b5 = 1.0</a></li>\n</ul>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>CIFAR10 L2-robust accuracy</th>\n</tr>\n<tr><th>\u00a0</th>\n<th>\u03b5-train</th>\n</tr>\n<tr><th>\u03b5-test</th>\n<th>0.0</th>\n<th>0.25</th>\n<th>0.5</th>\n<th>1.0</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>0.0</td>\n<td><strong>95.25% / -</strong></td>\n<td>92.77%  / -</td>\n<td>90.83% / -</td>\n<td>81.62% / -</td>\n</tr>\n<tr><td>0.25</td>\n<td>8.66% / 7.34%</td>\n<td>81.21% / 81.19%</td>\n<td><strong>82.34% / 82.31%</strong></td>\n<td>75.53% / 75.53%</td>\n</tr>\n<tr><td>0.5</td>\n<td>0.28% / 0.14%</td>\n<td>62.30% / 62.13%</td>\n<td><strong>70.17% / 70.11%</strong></td>\n<td>68.63% / 68.61%</td>\n</tr>\n<tr><td>1.0</td>\n<td>0.00% / 0.00%</td>\n<td>21.18% / 20.66%</td>\n<td>40.47% / 40.22%</td>\n<td><strong>52.72% / 52.61%</strong></td>\n</tr>\n<tr><td>2.0</td>\n<td>0.00% / 0.00%</td>\n<td>0.58% /  0.46%</td>\n<td>5.23% /  4.97%</td>\n<td><strong>18.59% / 18.05%</strong></td>\n</tr>\n</tbody>\n</table>\n<p>CIFAR10 Linf-norm (ResNet50):</p>\n<ul>\n<li>\u03b5 = 0.0 (PyTorch pre-trained)</li>\n<li><a href=\"https://www.dropbox.com/s/c9qlt1lbdnu9tlo/cifar_linf_8.pt?dl=0\" rel=\"nofollow\">\u03b5 = 8/255</a></li>\n</ul>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>CIFAR10 Linf-robust accuracy</th>\n</tr>\n<tr><th>\u00a0</th>\n<th>\u03b5-train</th>\n</tr>\n<tr><th>\u03b5-test</th>\n<th>0 / 255</th>\n<th>8 / 255</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>0 / 255</td>\n<td><strong>95.25% / -</strong></td>\n<td>87.03%  / -</td>\n</tr>\n<tr><td>8 / 255</td>\n<td>0.00% /  0.00%</td>\n<td><strong>53.49% / 53.29%</strong></td>\n</tr>\n<tr><td>16 / 255</td>\n<td>0.00% /  0.00%</td>\n<td><strong>18.13% / 17.62%</strong></td>\n</tr>\n</tbody>\n</table>\n<p>ImageNet L2-norm (ResNet50):</p>\n<ul>\n<li>\u03b5 = 0.0 (PyTorch pre-trained)</li>\n<li><a href=\"https://www.dropbox.com/s/knf4uimlqsi1yz8/imagenet_l2_3_0.pt?dl=0\" rel=\"nofollow\">\u03b5 = 3.0</a></li>\n</ul>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>ImageNet L2-robust accuracy</th>\n</tr>\n<tr><th>\u00a0</th>\n<th>\u03b5-train</th>\n</tr>\n<tr><th>\u03b5-test</th>\n<th>0.0</th>\n<th>3.0</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>0.0</td>\n<td><strong>76.13% / -</strong></td>\n<td>57.90%  / -</td>\n</tr>\n<tr><td>0.5</td>\n<td>3.35% /  2.98%</td>\n<td><strong>54.42% / 54.42%</strong></td>\n</tr>\n<tr><td>1.0</td>\n<td>0.44% /  0.37%</td>\n<td><strong>50.67% / 50.67%</strong></td>\n</tr>\n<tr><td>2.0</td>\n<td>0.16% /  0.14%</td>\n<td><strong>43.04% / 43.02%</strong></td>\n</tr>\n<tr><td>3.0</td>\n<td>0.13% /  0.12%</td>\n<td><strong>35.16% / 35.09%</strong></td>\n</tr>\n</tbody>\n</table>\n<p>ImageNet Linf-norm (ResNet50):</p>\n<ul>\n<li>\u03b5 = 0.0 (PyTorch pre-trained)</li>\n<li><a href=\"https://www.dropbox.com/s/axfuary2w1cnyrg/imagenet_linf_4.pt?dl=0\" rel=\"nofollow\">\u03b5 = 4 / 255</a></li>\n<li><a href=\"https://www.dropbox.com/s/yxn15a9zklz3s8q/imagenet_linf_8.pt?dl=0\" rel=\"nofollow\">\u03b5 = 8 / 255</a></li>\n</ul>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>ImageNet Linf-robust accuracy</th>\n</tr>\n<tr><th>\u00a0</th>\n<th>\u03b5-train</th>\n</tr>\n<tr><th>\u03b5-test</th>\n<th>0.0</th>\n<th>4 / 255</th>\n<th>8 / 255</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>0 / 255</td>\n<td><strong>76.13% / -</strong></td>\n<td>62.42%  / -</td>\n<td>47.91%  / -</td>\n</tr>\n<tr><td>4 / 255</td>\n<td>0.04% / 0.03%</td>\n<td><strong>33.58% / 33.38%</strong></td>\n<td>33.06% / 33.03%</td>\n</tr>\n<tr><td>8 / 255</td>\n<td>0.01% / 0.01%</td>\n<td>13.13% / 12.73%</td>\n<td><strong>19.63% / 19.52%</strong></td>\n</tr>\n<tr><td>16 / 255</td>\n<td>0.01% / 0.01%</td>\n<td>1.53% /  1.37%</td>\n<td><strong>5.00% /  4.82%</strong></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div id=\"citation\">\n<h2>Citation</h2>\n<p>If you use this library in your research, cite it as\nfollows:</p>\n<pre><span class=\"nc\">@misc</span><span class=\"p\">{</span><span class=\"nl\">robustness</span><span class=\"p\">,</span>\n   <span class=\"na\">title</span><span class=\"p\">=</span><span class=\"s\">{Robustness (Python Library)}</span><span class=\"p\">,</span>\n   <span class=\"na\">author</span><span class=\"p\">=</span><span class=\"s\">{Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras}</span><span class=\"p\">,</span>\n   <span class=\"na\">year</span><span class=\"p\">=</span><span class=\"s\">{2019}</span><span class=\"p\">,</span>\n   <span class=\"na\">url</span><span class=\"p\">=</span><span class=\"s\">{https://github.com/MadryLab/robustness}</span>\n<span class=\"p\">}</span>\n</pre>\n<p><em>(Have you used the package and found it useful? Let us know!)</em>.</p>\n</div>\n<div id=\"contributors\">\n<h2>Contributors</h2>\n<ul>\n<li><a href=\"https://twitter.com/andrew_ilyas\" rel=\"nofollow\">Andrew Ilyas</a></li>\n<li><a href=\"https://twitter.com/logan_engstrom\" rel=\"nofollow\">Logan Engstrom</a></li>\n<li><a href=\"https://twitter.com/ShibaniSan\" rel=\"nofollow\">Shibani Santurkar</a></li>\n<li><a href=\"https://twitter.com/tsiprasd\" rel=\"nofollow\">Dimitris Tsipras</a></li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 6917079, "releases": {"1.0": [{"comment_text": "", "digests": {"md5": "3d5359f066a3c63a1ea963ae42d69b18", "sha256": "08706856574c8167a3520d481f1228cafd4848a9c0e469499f7573b0f7422969"}, "downloads": -1, "filename": "robustness-1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3d5359f066a3c63a1ea963ae42d69b18", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 67245, "upload_time": "2019-08-21T10:06:46", "upload_time_iso_8601": "2019-08-21T10:06:46.574451Z", "url": "https://files.pythonhosted.org/packages/68/ca/3d83745f8d7b797f5f9b1832a26ceb2b45701b68a24f6418d1e7b100d95a/robustness-1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "70c909a301b392ef7bca6a14318a47e8", "sha256": "ffc1cf68bfe3bed267eeb085a8b6142a983a8516c1191ece5cf7d3609de3f387"}, "downloads": -1, "filename": "robustness-1.0.tar.gz", "has_sig": false, "md5_digest": "70c909a301b392ef7bca6a14318a47e8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 57307, "upload_time": "2019-08-21T10:06:49", "upload_time_iso_8601": "2019-08-21T10:06:49.026646Z", "url": "https://files.pythonhosted.org/packages/e3/ca/9f1a8756161a47b2d8982a2dcf81fb4f83ebfa8e4de86e69f3edac3a28d7/robustness-1.0.tar.gz", "yanked": false}], "1.0.post1": [{"comment_text": "", "digests": {"md5": "461392605dd6c96d054549372d83ec9e", "sha256": "38c834b4d5b13e023df7262d590e073c6c83725af3d1e2fd30491fcdca3bd4d4"}, "downloads": -1, "filename": "robustness-1.0.post1-py3-none-any.whl", "has_sig": false, "md5_digest": "461392605dd6c96d054549372d83ec9e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 67541, "upload_time": "2019-08-21T18:25:25", "upload_time_iso_8601": "2019-08-21T18:25:25.627196Z", "url": "https://files.pythonhosted.org/packages/01/31/c9b45a4f6ac909c39e6b01dedbd648258693a3b4d54968838e64a6ca5366/robustness-1.0.post1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "21bf6865e82dfef9a3d3d8c4dead9347", "sha256": "b09d0b692fbc3850d4d49103ae9ccc7b9e8c03a421a798ea50de51f17e56f3c3"}, "downloads": -1, "filename": "robustness-1.0.post1.tar.gz", "has_sig": false, "md5_digest": "21bf6865e82dfef9a3d3d8c4dead9347", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 57566, "upload_time": "2019-08-21T18:25:27", "upload_time_iso_8601": "2019-08-21T18:25:27.577957Z", "url": "https://files.pythonhosted.org/packages/f5/a5/7565532da7baad8da02d370a0113509ce8bb88ae5916a0e5d7de64ae7279/robustness-1.0.post1.tar.gz", "yanked": false}], "1.1": [{"comment_text": "", "digests": {"md5": "d2884d38fd409ed9c575b8d929b39d27", "sha256": "261c1e00b89dd3e81a1b9602fc3d67498a0ba6a0b3d0ea5eb4ee0796be95001d"}, "downloads": -1, "filename": "robustness-1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "d2884d38fd409ed9c575b8d929b39d27", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 73200, "upload_time": "2019-11-01T06:17:36", "upload_time_iso_8601": "2019-11-01T06:17:36.904300Z", "url": "https://files.pythonhosted.org/packages/97/40/b5bfe4a47fc3ac83b396c890d7b993fe1fb2f0770fffc80be189bd25a62a/robustness-1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "480f0eb30daf904acef8a2436780ec13", "sha256": "80eef79e55f26e282b3130ae572b45a50664072f8a65ca4eea6b70f1e2ff669a"}, "downloads": -1, "filename": "robustness-1.1.tar.gz", "has_sig": false, "md5_digest": "480f0eb30daf904acef8a2436780ec13", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 61857, "upload_time": "2019-11-01T06:17:39", "upload_time_iso_8601": "2019-11-01T06:17:39.266388Z", "url": "https://files.pythonhosted.org/packages/31/ca/5ebef26400830e340ba92b6a6726c059054fac04c69f9f5b955d0d143088/robustness-1.1.tar.gz", "yanked": false}], "1.1.post1": [{"comment_text": "", "digests": {"md5": "6c9184857acc1f2cf37907650ea98fc1", "sha256": "14514965082cd06a0ccb3616bc55278818237d616f4dd530000743962b99a31c"}, "downloads": -1, "filename": "robustness-1.1.post1-py3-none-any.whl", "has_sig": false, "md5_digest": "6c9184857acc1f2cf37907650ea98fc1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 73349, "upload_time": "2019-11-01T15:20:31", "upload_time_iso_8601": "2019-11-01T15:20:31.932333Z", "url": "https://files.pythonhosted.org/packages/8c/6a/00c70eea7ccdca5ca498918a3591e5e985b3777491cd01596cbb4b8e451e/robustness-1.1.post1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5449b265e2d6de1a500fbe892d17fa11", "sha256": "748da4e84af04c9839174764fa5e91e3f7d3ca99eef07e611ff70483cfbe60c0"}, "downloads": -1, "filename": "robustness-1.1.post1.tar.gz", "has_sig": false, "md5_digest": "5449b265e2d6de1a500fbe892d17fa11", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 60548, "upload_time": "2019-11-01T15:20:33", "upload_time_iso_8601": "2019-11-01T15:20:33.435488Z", "url": "https://files.pythonhosted.org/packages/1f/4c/f874c0dc963a5d07d02ec045dad8bcafa047191deb20526fb4a797f0db0e/robustness-1.1.post1.tar.gz", "yanked": false}], "1.1.post2": [{"comment_text": "", "digests": {"md5": "dffcd5bc1f2eaf16700c297fabac2886", "sha256": "1188a129a106607eef7a3db9f63fa695f0c91fc88712bb0729f58e3e0712c1e9"}, "downloads": -1, "filename": "robustness-1.1.post2-py3-none-any.whl", "has_sig": false, "md5_digest": "dffcd5bc1f2eaf16700c297fabac2886", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 73396, "upload_time": "2020-03-31T00:51:06", "upload_time_iso_8601": "2020-03-31T00:51:06.488242Z", "url": "https://files.pythonhosted.org/packages/14/a7/08c95c2adaaac0e12db226f407cd2f1b87626be89ecf8fcf57af334920b4/robustness-1.1.post2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6bfb32e099ee9a5226d06c18b8b932d6", "sha256": "94b3a7715196144a16a74372b9292cc4eaa6f0018b79bd390e0e533eda8150c8"}, "downloads": -1, "filename": "robustness-1.1.post2.tar.gz", "has_sig": false, "md5_digest": "6bfb32e099ee9a5226d06c18b8b932d6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 60805, "upload_time": "2020-03-31T00:51:08", "upload_time_iso_8601": "2020-03-31T00:51:08.502173Z", "url": "https://files.pythonhosted.org/packages/1b/8b/91edda6b854f7da52329dcad62a216c13c04ad39fb703b5d037190d7003e/robustness-1.1.post2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "dffcd5bc1f2eaf16700c297fabac2886", "sha256": "1188a129a106607eef7a3db9f63fa695f0c91fc88712bb0729f58e3e0712c1e9"}, "downloads": -1, "filename": "robustness-1.1.post2-py3-none-any.whl", "has_sig": false, "md5_digest": "dffcd5bc1f2eaf16700c297fabac2886", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 73396, "upload_time": "2020-03-31T00:51:06", "upload_time_iso_8601": "2020-03-31T00:51:06.488242Z", "url": "https://files.pythonhosted.org/packages/14/a7/08c95c2adaaac0e12db226f407cd2f1b87626be89ecf8fcf57af334920b4/robustness-1.1.post2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6bfb32e099ee9a5226d06c18b8b932d6", "sha256": "94b3a7715196144a16a74372b9292cc4eaa6f0018b79bd390e0e533eda8150c8"}, "downloads": -1, "filename": "robustness-1.1.post2.tar.gz", "has_sig": false, "md5_digest": "6bfb32e099ee9a5226d06c18b8b932d6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 60805, "upload_time": "2020-03-31T00:51:08", "upload_time_iso_8601": "2020-03-31T00:51:08.502173Z", "url": "https://files.pythonhosted.org/packages/1b/8b/91edda6b854f7da52329dcad62a216c13c04ad39fb703b5d037190d7003e/robustness-1.1.post2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:01:37 2020"}