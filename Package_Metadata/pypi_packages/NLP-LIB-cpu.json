{"info": {"author": "Chulayuth Asawaroengchai", "author_email": "twilightdema@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "NLP_LIB\n=======\nThe python library for language modeling and fine tuning using Transformer based deep learning models with built-in Thai data set supported.\n\n### Features\n#### Lanugage Models Supported\n - Transformer Decoder-only model (Next token predicton objective function)\n - Transformer Encoder-only model (Masked tokens prediction objective function)\n#### Fine Tuning Models Supported\n - Sequence-to-Sequence Model\n - Multi Class Classification\n - Multi Label Classification\n#### Built-in Data Set Supported (All are Thai language)\n - **NECTEC BEST2010** for Language Model\n - **Thailand Wikipedia Dump** for Langauge Model\n - **NECTEC BEST2010** for Topic Classification\n - **Truevoice** for Intention Detection\n - **Wisesight** for Sentiment Analysis\n - **Wongnai** for Rating Prediction\n#### Build-in Input / Output Transformation\n - Full word dictionary\n - Bi-gram dictionary\n - Sentencepiece coding\n#### Other Features\n - Build in API server for quick deploying the model\n - Automatic multi-GPUs detection and training support (Data Parallel)\n - Automatic state saving and resume training\n - Automatic saving best model and last model during training\n - Automatic generate Tensorboard log\n - Sequence generation from language model using ARGMAX, BEAM Search\n - Support initialization from trained language model weights in fine tuning\n - Modularized and fully extensible\n\nInstallation\n============\nThe library requires python 3.6 or later. You can use pip3 to install the library as below:\n```\npip3 install NLP_LIB\n```\nOr if you want to use CPU version of the library (not recommended for model training):\n```\npip3 install NLP_LIB_cpu\n```\n\nBasic library usages\n====================\nFor Language Model Training\n```\npython3 -m NLP_LIB <language_model>:<training_data_file>\n```\nFor Fine Tuning\n```\npython3 -m NLP_LIB <language_model>:<training_data_file>:<finetune_model>:<finetune_data_file>\n```\nFor lanching API Server, just add additional option the the command\n```\nserve\n```\n\n\nExamples of normal use cases\n============================\n\nTrain 6 layers of transformer decoder-only model with sentencepiece dict model with data in data/lm_train.txt\n```\npython3 -m NLP_LIB tf6-dec-sp:data/lm_train.txt\n```\n\nFinetune the above model with data in data/sp_train.txt, which is single class classifier of 3 possible values\n```\npython3 -m NLP_LIB tf6-dec-sp:data/lm_train.txt:sa3:data/sp_train.txt\n```\n\nLaunch API Server for the above model\n```\npython3 -m NLP_LIB tf6-dec-sp:data/lm_train.txt:sa3:data/sp_train.txt serve\n```\n - The model API test page can be accessed at: ```http://localhost:5555```\n\n\nTraining data input file format\n===============================\nFor Lanugage Modeling (Minimum 1,000 sentences)\n```\nsentence 1\nsentence 2\n...\nsentence N\n```\nBelow is an example\n```\n\u0e19\u0e35\u0e48\u0e04\u0e37\u0e2d\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e41\u0e23\u0e01\n\u0e19\u0e35\u0e48\u0e04\u0e37\u0e2d\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e17\u0e35\u0e48\u0e2a\u0e2d\u0e07\n...\n\u0e19\u0e35\u0e48\u0e04\u0e37\u0e2d\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\n```\n\nFor Fine Tuning Classification Task (Minimum 320 sentences)\n```\nsentence 1[TAB]label\nsentence 2[TAB]label\n...\nsentence N[TAB]label\n```\nBelow is an example\n```\n\u0e1c\u0e25\u0e07\u0e32\u0e19\u0e14\u0e35\u0e21\u0e32\u0e01\u0e46          positive\n\u0e2a\u0e48\u0e07\u0e02\u0e2d\u0e07\u0e21\u0e32\u0e41\u0e15\u0e48\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49    negative\n...\n\u0e27\u0e31\u0e19\u0e19\u0e35\u0e49\u0e2d\u0e32\u0e01\u0e32\u0e28\u0e40\u0e22\u0e47\u0e19         neutral\n```\n\nMore advance library usages\n===========================\n```\npython3 -m NLP_LIB <model_name | model_json_path> <operation> <extra_params>\n```\n - **model_name**: Predefined model name shipped with the library (See appendix A. for list of predefined models)\n - **model_json_path**: JSON Configuration File path of the model (See appendix B. JSON file format)\n - **operation**: train | predict | generate - default is train (See example section for how to use \"generate\" mode)\n\n\nExamples of using built-in data set\n===================================\n\nTrain language model of 6 layers transformer decoder-only with default BEST2010 corpus\n```\npython3 -m NLP_LIB tf6-dec\n```\nFinetune 4 layers of transformer encoder-only with sentencepiece dict model on truevoice data\n```\npython3 -m NLP_LIB tf4-enc-sp+truevoice\n```\nRun prediction on input data file\n```\npython3 -m NLP_LIB tf4-enc-sp+truevoice predict file:input_data.txt\n```\nRun prediction on input string\n```\npython3 -m NLP_LIB tf4-dec-bigram+best2010 predict str:This,is,input,text\n```\nRun sequence generation for 20 tokens using BEAM search on 3 best prediction sequences\n```\npython3 -m NLP_LIB tf6-dec generate:20:beam3 str:This,is,seed,text\n```\n\nAPPENDIX A) List of predefined models\n=====================================\n\n#### For language model:\n```\ntf<N>-<Arch>-<Dict> : Transformer models\n```\n - **N** : Number of transformer layers, support 2, 4, 6 and 12. Default is 6.\n - **Arch**: Architecture of language model, support \"enc\" and \"dec\" for encoder-only and decoder-only.\n - **Dict**: Data transformation, support \"full\", \"bigram\" and \"sp\" for full word dict, bigram dict and sentencepiece dict. Default is \"full\"\n\n **Examples**:\n ```\n tf-dec\n tf6-dec\n tf4-enc-full\n tf12-dec-sp\n tf2-enc-bigram\n```\n#### For fine tuning model:\n```\ntf<N>-<Arch>-<Dict>+<Finetune Data> : Transformer models\n```\n - **N** : Number of transformer layers, support 2, 4, 6 and 12. Default is 6.\n - **Arch**: Architecture of language model, support \"enc\" and \"dec\" for encoder-only and decoder-only.\n - **Dict**: Data transformation, support \"full\", \"bigram\" and \"sp\" for full word dict, bigram dict and sentencepiece dict. Default is \"full\"\n - **Finetune Data**: Fine tuning data set, support \"best2010\", \"truevoice\", \"wongnai\" and \"wisesight\"\n\n **Examples**:\n ```\n tf-dec+best2010\n tf6-dec+truevoice\n tf4-enc-full+wongnai\n tf12-dec-sp+wisesight\n```\n\nAPPENDIX B) JSON Configuration File Format\n==========================================\n\nThis file defines how to run the model training.\nThe model training run is defined by 5 components below:\n - **Model** : Model architecture to be used\n - **Dataset** : Dataset to be used\n - **Input / Output Transformation** : How to encode / decode input and output data\n - **Callbacks** : List of additional flow need to be run in training loop\n - **Execution** : Training processes to be used, for example what optimizer, how many epoch\n\nThe JSON file needs to supply configuration of each component, the overall format is shown below:\n```\n{\n  \"model\": {\n    \"class\": <CLASS NAME OF MODEL>,\n    \"config\": {\n      <CONFIGURATIONS OF THE MODEL>\n    }\n  },\n  \"dataset\": {\n    \"class\": <CLASS NAME OF DATA SET>,\n    \"config\": {\n      <CONFIGURATIONS OF THE DATA SET>\n    }\n  },\n  \"input_transform\": {\n    \"class\": <CLASS NAME OF INPUT TRANSFORMATION>,\n    \"config\": {\n      <CONFIGURATIONS OF THE INPUT TRANFORMATION>\n    }\n  },\n  \"output_transform\": {\n    \"class\": <CLASS NAME OF OUTPUT TRANSFORMATION>,\n    \"config\": {\n      <CONFIGURATIONS OF THE OUTPUT TRANFORMATION>\n    }\n  },\n  \"callbacks\": [\n    .... [MULTIPLE CALLBACK HOOKS] ....\n    {\n      \"class\": <CLASS NAME OF CALLBACK HOOKS>,\n      \"config\": {\n        <CONFIGURATIONS OF THE CALLBACK>\n      }\n    }\n  ],\n  \"execution\": {\n    \"config\": {\n      <CONFIGURATIONS OF THE TRAINING PROCESS>\n    }\n  }\n}\n```\nOverall is that the configuration of each module requires class name of the module and also\nconfigurations for them. The required / optional configurations of each module are depended\non module class so you have to read document for each module class to find out how to config them.\nThe class name of each module is used to look up for implementation of the module in \nthe following directories:\n\n - **model** => ./models\n - **dataset** => ./datasets\n - **input / output transformations** => ./transforms\n - **callbacks** => ./callbacks\n\nYou can implement new module by putting module python class in above directories and the library\nwill be able to resolve for implementation when it finds class name in JSON configuration file.\n\nBelow is example of JSON configuration file for training 12 layers of transformer decoder-only model\nwith sentencepiece dictionary data transformation and dynamic learning rate on THWIKI data set:\n```\n{\n  \"model\": {\n    \"class\": \"TransformerDecoderOnlyWrapper\",\n    \"config\": {\n      \"len_limit\": 256,\n      \"d_model\": 512,\n      \"d_inner_hid\": 2048,\n      \"n_head\": 8,\n      \"d_k\": 512,\n      \"d_v\": 512,\n      \"layers\": 12,\n      \"dropout\": 0.1,\n      \"share_word_emb\": true,\n      \"max_input_length\": 256,\n      \"cached_data_dir\": \"_cache_\"\n    }\n  },\n  \"dataset\": {\n    \"class\": \"THWIKILMDatasetWrapper\",\n    \"config\": {\n      \"base_data_dir\": \"_tmp_\"\n    }\n  },\n  \"input_transform\": {\n    \"class\": \"SentencePieceDictionaryWrapper\",\n    \"config\": {\n      \"column_id\": 0,\n      \"max_dict_size\" : 15000,\n      \"mask_last_token\": false\n    }\n  },\n  \"output_transform\": {\n    \"class\": \"SentencePieceDictionaryWrapper\",\n    \"config\": {\n      \"column_id\": 1,\n      \"max_dict_size\" : 15000\n    }\n  },\n  \"callbacks\": [\n    {\n      \"class\": \"DynamicLearningRateWrapper\",\n      \"config\": {\n        \"d_model\": 512,\n        \"warmup\": 50000,\n        \"scale\": 0.5\n      }\n    }\n  ],\n  \"execution\": {\n    \"config\": {\n      \"optimizer\": \"adam\",\n      \"optimizer_params\": [0.1, 0.9, 0.997, 1e-9],\n      \"batch_size\": 32,\n      \"epochs\": 60,\n      \"watch_metric\": \"val_acc\",\n      \"output_dir\": \"_outputs_/thwikilm_tfbase12_dec_s2_sp\",\n      \"save_weight_history\": false,\n      \"resume_if_possible\": true,\n      \"multi_gpu\": false\n    }\n  }\n}\n```\nBelow is another example of using the trained model above to finetune on TRUEVOICE data set.\nNote that we use \"SequenceTransferLearningWrapper\" model class, which accept configuration of\nlanguage model to be used as an encoder and also the original data set configuration used to pre-train\nthe encoder model:\n```\n{\n  \"model\": {\n    \"class\": \"SequenceTransferLearningWrapper\",\n    \"config\": {\n      \"output_class_num\": 8,\n      \"encoder_checkpoint\": \"_outputs_/thwikilm_tfbase12_dec_s2_sp/checkpoint/best_weight.h5\",\n      \"train_encoder\": true,\n      \"max_input_length\": 256,\n      \"drop_out\": 0.4,\n      \"cached_data_dir\": \"_cache_\",\n\n      \"encoder_model\": {\n        \"class\": \"TransformerDecoderOnlyWrapper\",\n        \"config\": {\n          \"len_limit\": 256,\n          \"d_model\": 512,\n          \"d_inner_hid\": 2048,\n          \"n_head\": 8,\n          \"d_k\": 512,\n          \"d_v\": 512,\n          \"layers\": 12,\n          \"dropout\": 0.1,\n          \"share_word_emb\": true,\n          \"max_input_length\": 256,\n          \"cached_data_dir\": \"_cache_\"\n        }\n      },\n\n      \"encoder_dict_dataset\": {\n        \"class\": \"THWIKILMDatasetWrapper\",\n        \"config\": {\n          \"base_data_dir\": \"_tmp_\"\n        }\n      }\n\n    }\n  },\n  \"dataset\": {\n    \"class\": \"TruevoiceDatasetWrapper\",\n    \"config\": {\n      \"base_data_dir\": \"_tmp_\"\n    }\n  },\n  \"input_transform\": {\n    \"class\": \"SentencePieceDictionaryWrapper\",\n    \"config\": {\n      \"column_id\": 0,\n      \"max_dict_size\" : 15000,\n      \"clf_pos_offset\": -1,\n      \"clf_id\": 3,\n      \"mask_last_token\": false\n    }\n  },  \n  \"output_transform\": {\n    \"class\": \"SingleClassTransformWrapper\",\n    \"config\": {\n      \"column_id\": 1\n    }\n  },\n  \"callbacks\": [\n    {\n      \"class\": \"DynamicLearningRateWrapper\",\n      \"config\": {\n        \"d_model\": 512,\n        \"warmup\": 50000,\n        \"scale\" : 0.25\n      }\n    }\n  ],\n  \"execution\": {\n    \"config\": {\n      \"optimizer\": \"adam\",\n      \"optimizer_params\": [0.0001, 0.9, 0.98, 1e-9],\n      \"batch_size\": 32,\n      \"epochs\": 30,\n      \"watch_metric\": \"val_acc\",\n      \"output_dir\": \"_outputs_/finetune_thwikilm_tfbase12_dec_s2s_sp_truevoice\",\n      \"save_weight_history\": false,\n      \"resume_if_possible\": true\n    }\n  }\n}\n```\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/twilightdema/NLP_LIB.git", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "NLP-LIB-cpu", "package_url": "https://pypi.org/project/NLP-LIB-cpu/", "platform": "", "project_url": "https://pypi.org/project/NLP-LIB-cpu/", "project_urls": {"Homepage": "https://github.com/twilightdema/NLP_LIB.git"}, "release_url": "https://pypi.org/project/NLP-LIB-cpu/0.0.5/", "requires_dist": ["Flask (==1.1.1)", "Keras-Applications (==1.0.8)", "Keras-Preprocessing (==1.1.0)", "Keras (==2.3.1)", "Markdown (==3.1.1)", "PyYAML (==5.2)", "Werkzeug (==0.16.0)", "absl-py (==0.9.0)", "astor (==0.8.1)", "gast (==0.3.2)", "google-pasta (==0.1.8)", "grpcio (==1.26.0)", "h5py (==2.10.0)", "numpy (==1.18.0)", "protobuf (==3.11.2)", "scipy (==1.4.1)", "sentencepiece (==0.1.85)", "six (==1.13.0)", "tensorboard (==1.14.0)", "tensorflow-estimator (==1.14.0)", "tensorflow (==1.14.0)", "termcolor (==1.1.0)", "wrapt (==1.11.2)"], "requires_python": ">=3.6", "summary": "Python library for Language Model / Finetune using Transformer based models.", "version": "0.0.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>NLP_LIB</h1>\n<p>The python library for language modeling and fine tuning using Transformer based deep learning models with built-in Thai data set supported.</p>\n<h3>Features</h3>\n<h4>Lanugage Models Supported</h4>\n<ul>\n<li>Transformer Decoder-only model (Next token predicton objective function)</li>\n<li>Transformer Encoder-only model (Masked tokens prediction objective function)</li>\n</ul>\n<h4>Fine Tuning Models Supported</h4>\n<ul>\n<li>Sequence-to-Sequence Model</li>\n<li>Multi Class Classification</li>\n<li>Multi Label Classification</li>\n</ul>\n<h4>Built-in Data Set Supported (All are Thai language)</h4>\n<ul>\n<li><strong>NECTEC BEST2010</strong> for Language Model</li>\n<li><strong>Thailand Wikipedia Dump</strong> for Langauge Model</li>\n<li><strong>NECTEC BEST2010</strong> for Topic Classification</li>\n<li><strong>Truevoice</strong> for Intention Detection</li>\n<li><strong>Wisesight</strong> for Sentiment Analysis</li>\n<li><strong>Wongnai</strong> for Rating Prediction</li>\n</ul>\n<h4>Build-in Input / Output Transformation</h4>\n<ul>\n<li>Full word dictionary</li>\n<li>Bi-gram dictionary</li>\n<li>Sentencepiece coding</li>\n</ul>\n<h4>Other Features</h4>\n<ul>\n<li>Build in API server for quick deploying the model</li>\n<li>Automatic multi-GPUs detection and training support (Data Parallel)</li>\n<li>Automatic state saving and resume training</li>\n<li>Automatic saving best model and last model during training</li>\n<li>Automatic generate Tensorboard log</li>\n<li>Sequence generation from language model using ARGMAX, BEAM Search</li>\n<li>Support initialization from trained language model weights in fine tuning</li>\n<li>Modularized and fully extensible</li>\n</ul>\n<h1>Installation</h1>\n<p>The library requires python 3.6 or later. You can use pip3 to install the library as below:</p>\n<pre><code>pip3 install NLP_LIB\n</code></pre>\n<p>Or if you want to use CPU version of the library (not recommended for model training):</p>\n<pre><code>pip3 install NLP_LIB_cpu\n</code></pre>\n<h1>Basic library usages</h1>\n<p>For Language Model Training</p>\n<pre><code>python3 -m NLP_LIB &lt;language_model&gt;:&lt;training_data_file&gt;\n</code></pre>\n<p>For Fine Tuning</p>\n<pre><code>python3 -m NLP_LIB &lt;language_model&gt;:&lt;training_data_file&gt;:&lt;finetune_model&gt;:&lt;finetune_data_file&gt;\n</code></pre>\n<p>For lanching API Server, just add additional option the the command</p>\n<pre><code>serve\n</code></pre>\n<h1>Examples of normal use cases</h1>\n<p>Train 6 layers of transformer decoder-only model with sentencepiece dict model with data in data/lm_train.txt</p>\n<pre><code>python3 -m NLP_LIB tf6-dec-sp:data/lm_train.txt\n</code></pre>\n<p>Finetune the above model with data in data/sp_train.txt, which is single class classifier of 3 possible values</p>\n<pre><code>python3 -m NLP_LIB tf6-dec-sp:data/lm_train.txt:sa3:data/sp_train.txt\n</code></pre>\n<p>Launch API Server for the above model</p>\n<pre><code>python3 -m NLP_LIB tf6-dec-sp:data/lm_train.txt:sa3:data/sp_train.txt serve\n</code></pre>\n<ul>\n<li>The model API test page can be accessed at: <code>http://localhost:5555</code></li>\n</ul>\n<h1>Training data input file format</h1>\n<p>For Lanugage Modeling (Minimum 1,000 sentences)</p>\n<pre><code>sentence 1\nsentence 2\n...\nsentence N\n</code></pre>\n<p>Below is an example</p>\n<pre><code>\u0e19\u0e35\u0e48\u0e04\u0e37\u0e2d\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e41\u0e23\u0e01\n\u0e19\u0e35\u0e48\u0e04\u0e37\u0e2d\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e17\u0e35\u0e48\u0e2a\u0e2d\u0e07\n...\n\u0e19\u0e35\u0e48\u0e04\u0e37\u0e2d\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\n</code></pre>\n<p>For Fine Tuning Classification Task (Minimum 320 sentences)</p>\n<pre><code>sentence 1[TAB]label\nsentence 2[TAB]label\n...\nsentence N[TAB]label\n</code></pre>\n<p>Below is an example</p>\n<pre><code>\u0e1c\u0e25\u0e07\u0e32\u0e19\u0e14\u0e35\u0e21\u0e32\u0e01\u0e46          positive\n\u0e2a\u0e48\u0e07\u0e02\u0e2d\u0e07\u0e21\u0e32\u0e41\u0e15\u0e48\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49    negative\n...\n\u0e27\u0e31\u0e19\u0e19\u0e35\u0e49\u0e2d\u0e32\u0e01\u0e32\u0e28\u0e40\u0e22\u0e47\u0e19         neutral\n</code></pre>\n<h1>More advance library usages</h1>\n<pre><code>python3 -m NLP_LIB &lt;model_name | model_json_path&gt; &lt;operation&gt; &lt;extra_params&gt;\n</code></pre>\n<ul>\n<li><strong>model_name</strong>: Predefined model name shipped with the library (See appendix A. for list of predefined models)</li>\n<li><strong>model_json_path</strong>: JSON Configuration File path of the model (See appendix B. JSON file format)</li>\n<li><strong>operation</strong>: train | predict | generate - default is train (See example section for how to use \"generate\" mode)</li>\n</ul>\n<h1>Examples of using built-in data set</h1>\n<p>Train language model of 6 layers transformer decoder-only with default BEST2010 corpus</p>\n<pre><code>python3 -m NLP_LIB tf6-dec\n</code></pre>\n<p>Finetune 4 layers of transformer encoder-only with sentencepiece dict model on truevoice data</p>\n<pre><code>python3 -m NLP_LIB tf4-enc-sp+truevoice\n</code></pre>\n<p>Run prediction on input data file</p>\n<pre><code>python3 -m NLP_LIB tf4-enc-sp+truevoice predict file:input_data.txt\n</code></pre>\n<p>Run prediction on input string</p>\n<pre><code>python3 -m NLP_LIB tf4-dec-bigram+best2010 predict str:This,is,input,text\n</code></pre>\n<p>Run sequence generation for 20 tokens using BEAM search on 3 best prediction sequences</p>\n<pre><code>python3 -m NLP_LIB tf6-dec generate:20:beam3 str:This,is,seed,text\n</code></pre>\n<h1>APPENDIX A) List of predefined models</h1>\n<h4>For language model:</h4>\n<pre><code>tf&lt;N&gt;-&lt;Arch&gt;-&lt;Dict&gt; : Transformer models\n</code></pre>\n<ul>\n<li><strong>N</strong> : Number of transformer layers, support 2, 4, 6 and 12. Default is 6.</li>\n<li><strong>Arch</strong>: Architecture of language model, support \"enc\" and \"dec\" for encoder-only and decoder-only.</li>\n<li><strong>Dict</strong>: Data transformation, support \"full\", \"bigram\" and \"sp\" for full word dict, bigram dict and sentencepiece dict. Default is \"full\"</li>\n</ul>\n<p><strong>Examples</strong>:</p>\n<pre><code>tf-dec\ntf6-dec\ntf4-enc-full\ntf12-dec-sp\ntf2-enc-bigram\n</code></pre>\n<h4>For fine tuning model:</h4>\n<pre><code>tf&lt;N&gt;-&lt;Arch&gt;-&lt;Dict&gt;+&lt;Finetune Data&gt; : Transformer models\n</code></pre>\n<ul>\n<li><strong>N</strong> : Number of transformer layers, support 2, 4, 6 and 12. Default is 6.</li>\n<li><strong>Arch</strong>: Architecture of language model, support \"enc\" and \"dec\" for encoder-only and decoder-only.</li>\n<li><strong>Dict</strong>: Data transformation, support \"full\", \"bigram\" and \"sp\" for full word dict, bigram dict and sentencepiece dict. Default is \"full\"</li>\n<li><strong>Finetune Data</strong>: Fine tuning data set, support \"best2010\", \"truevoice\", \"wongnai\" and \"wisesight\"</li>\n</ul>\n<p><strong>Examples</strong>:</p>\n<pre><code>tf-dec+best2010\ntf6-dec+truevoice\ntf4-enc-full+wongnai\ntf12-dec-sp+wisesight\n</code></pre>\n<h1>APPENDIX B) JSON Configuration File Format</h1>\n<p>This file defines how to run the model training.\nThe model training run is defined by 5 components below:</p>\n<ul>\n<li><strong>Model</strong> : Model architecture to be used</li>\n<li><strong>Dataset</strong> : Dataset to be used</li>\n<li><strong>Input / Output Transformation</strong> : How to encode / decode input and output data</li>\n<li><strong>Callbacks</strong> : List of additional flow need to be run in training loop</li>\n<li><strong>Execution</strong> : Training processes to be used, for example what optimizer, how many epoch</li>\n</ul>\n<p>The JSON file needs to supply configuration of each component, the overall format is shown below:</p>\n<pre><code>{\n  \"model\": {\n    \"class\": &lt;CLASS NAME OF MODEL&gt;,\n    \"config\": {\n      &lt;CONFIGURATIONS OF THE MODEL&gt;\n    }\n  },\n  \"dataset\": {\n    \"class\": &lt;CLASS NAME OF DATA SET&gt;,\n    \"config\": {\n      &lt;CONFIGURATIONS OF THE DATA SET&gt;\n    }\n  },\n  \"input_transform\": {\n    \"class\": &lt;CLASS NAME OF INPUT TRANSFORMATION&gt;,\n    \"config\": {\n      &lt;CONFIGURATIONS OF THE INPUT TRANFORMATION&gt;\n    }\n  },\n  \"output_transform\": {\n    \"class\": &lt;CLASS NAME OF OUTPUT TRANSFORMATION&gt;,\n    \"config\": {\n      &lt;CONFIGURATIONS OF THE OUTPUT TRANFORMATION&gt;\n    }\n  },\n  \"callbacks\": [\n    .... [MULTIPLE CALLBACK HOOKS] ....\n    {\n      \"class\": &lt;CLASS NAME OF CALLBACK HOOKS&gt;,\n      \"config\": {\n        &lt;CONFIGURATIONS OF THE CALLBACK&gt;\n      }\n    }\n  ],\n  \"execution\": {\n    \"config\": {\n      &lt;CONFIGURATIONS OF THE TRAINING PROCESS&gt;\n    }\n  }\n}\n</code></pre>\n<p>Overall is that the configuration of each module requires class name of the module and also\nconfigurations for them. The required / optional configurations of each module are depended\non module class so you have to read document for each module class to find out how to config them.\nThe class name of each module is used to look up for implementation of the module in\nthe following directories:</p>\n<ul>\n<li><strong>model</strong> =&gt; ./models</li>\n<li><strong>dataset</strong> =&gt; ./datasets</li>\n<li><strong>input / output transformations</strong> =&gt; ./transforms</li>\n<li><strong>callbacks</strong> =&gt; ./callbacks</li>\n</ul>\n<p>You can implement new module by putting module python class in above directories and the library\nwill be able to resolve for implementation when it finds class name in JSON configuration file.</p>\n<p>Below is example of JSON configuration file for training 12 layers of transformer decoder-only model\nwith sentencepiece dictionary data transformation and dynamic learning rate on THWIKI data set:</p>\n<pre><code>{\n  \"model\": {\n    \"class\": \"TransformerDecoderOnlyWrapper\",\n    \"config\": {\n      \"len_limit\": 256,\n      \"d_model\": 512,\n      \"d_inner_hid\": 2048,\n      \"n_head\": 8,\n      \"d_k\": 512,\n      \"d_v\": 512,\n      \"layers\": 12,\n      \"dropout\": 0.1,\n      \"share_word_emb\": true,\n      \"max_input_length\": 256,\n      \"cached_data_dir\": \"_cache_\"\n    }\n  },\n  \"dataset\": {\n    \"class\": \"THWIKILMDatasetWrapper\",\n    \"config\": {\n      \"base_data_dir\": \"_tmp_\"\n    }\n  },\n  \"input_transform\": {\n    \"class\": \"SentencePieceDictionaryWrapper\",\n    \"config\": {\n      \"column_id\": 0,\n      \"max_dict_size\" : 15000,\n      \"mask_last_token\": false\n    }\n  },\n  \"output_transform\": {\n    \"class\": \"SentencePieceDictionaryWrapper\",\n    \"config\": {\n      \"column_id\": 1,\n      \"max_dict_size\" : 15000\n    }\n  },\n  \"callbacks\": [\n    {\n      \"class\": \"DynamicLearningRateWrapper\",\n      \"config\": {\n        \"d_model\": 512,\n        \"warmup\": 50000,\n        \"scale\": 0.5\n      }\n    }\n  ],\n  \"execution\": {\n    \"config\": {\n      \"optimizer\": \"adam\",\n      \"optimizer_params\": [0.1, 0.9, 0.997, 1e-9],\n      \"batch_size\": 32,\n      \"epochs\": 60,\n      \"watch_metric\": \"val_acc\",\n      \"output_dir\": \"_outputs_/thwikilm_tfbase12_dec_s2_sp\",\n      \"save_weight_history\": false,\n      \"resume_if_possible\": true,\n      \"multi_gpu\": false\n    }\n  }\n}\n</code></pre>\n<p>Below is another example of using the trained model above to finetune on TRUEVOICE data set.\nNote that we use \"SequenceTransferLearningWrapper\" model class, which accept configuration of\nlanguage model to be used as an encoder and also the original data set configuration used to pre-train\nthe encoder model:</p>\n<pre><code>{\n  \"model\": {\n    \"class\": \"SequenceTransferLearningWrapper\",\n    \"config\": {\n      \"output_class_num\": 8,\n      \"encoder_checkpoint\": \"_outputs_/thwikilm_tfbase12_dec_s2_sp/checkpoint/best_weight.h5\",\n      \"train_encoder\": true,\n      \"max_input_length\": 256,\n      \"drop_out\": 0.4,\n      \"cached_data_dir\": \"_cache_\",\n\n      \"encoder_model\": {\n        \"class\": \"TransformerDecoderOnlyWrapper\",\n        \"config\": {\n          \"len_limit\": 256,\n          \"d_model\": 512,\n          \"d_inner_hid\": 2048,\n          \"n_head\": 8,\n          \"d_k\": 512,\n          \"d_v\": 512,\n          \"layers\": 12,\n          \"dropout\": 0.1,\n          \"share_word_emb\": true,\n          \"max_input_length\": 256,\n          \"cached_data_dir\": \"_cache_\"\n        }\n      },\n\n      \"encoder_dict_dataset\": {\n        \"class\": \"THWIKILMDatasetWrapper\",\n        \"config\": {\n          \"base_data_dir\": \"_tmp_\"\n        }\n      }\n\n    }\n  },\n  \"dataset\": {\n    \"class\": \"TruevoiceDatasetWrapper\",\n    \"config\": {\n      \"base_data_dir\": \"_tmp_\"\n    }\n  },\n  \"input_transform\": {\n    \"class\": \"SentencePieceDictionaryWrapper\",\n    \"config\": {\n      \"column_id\": 0,\n      \"max_dict_size\" : 15000,\n      \"clf_pos_offset\": -1,\n      \"clf_id\": 3,\n      \"mask_last_token\": false\n    }\n  },  \n  \"output_transform\": {\n    \"class\": \"SingleClassTransformWrapper\",\n    \"config\": {\n      \"column_id\": 1\n    }\n  },\n  \"callbacks\": [\n    {\n      \"class\": \"DynamicLearningRateWrapper\",\n      \"config\": {\n        \"d_model\": 512,\n        \"warmup\": 50000,\n        \"scale\" : 0.25\n      }\n    }\n  ],\n  \"execution\": {\n    \"config\": {\n      \"optimizer\": \"adam\",\n      \"optimizer_params\": [0.0001, 0.9, 0.98, 1e-9],\n      \"batch_size\": 32,\n      \"epochs\": 30,\n      \"watch_metric\": \"val_acc\",\n      \"output_dir\": \"_outputs_/finetune_thwikilm_tfbase12_dec_s2s_sp_truevoice\",\n      \"save_weight_history\": false,\n      \"resume_if_possible\": true\n    }\n  }\n}\n</code></pre>\n\n          </div>"}, "last_serial": 6683206, "releases": {"0.0.4": [{"comment_text": "", "digests": {"md5": "74e56089249ef331dc412ec5d70e1415", "sha256": "debce08e43a21cecbd563bfcee7bee439dc79a5dc0b5aaa81862b0e94cd0364f"}, "downloads": -1, "filename": "NLP_LIB_cpu-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "74e56089249ef331dc412ec5d70e1415", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 159912, "upload_time": "2020-02-22T18:18:07", "upload_time_iso_8601": "2020-02-22T18:18:07.381836Z", "url": "https://files.pythonhosted.org/packages/c3/b6/cf79507a0e04ba4c03e2e0e58ac77f6b3b2bfab75d38fa653e021d910aa6/NLP_LIB_cpu-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9412882d6f23b03684adfaa4bf1d89a5", "sha256": "de11f52c0269a1e71fc497065e5640bc6fbd9f96c696bdc04f9b0faaa3e8471c"}, "downloads": -1, "filename": "NLP_LIB_cpu-0.0.4.tar.gz", "has_sig": false, "md5_digest": "9412882d6f23b03684adfaa4bf1d89a5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 51217, "upload_time": "2020-02-22T18:18:09", "upload_time_iso_8601": "2020-02-22T18:18:09.737943Z", "url": "https://files.pythonhosted.org/packages/a8/06/ed2228d8b95ab8b321c268399041fafe16256c6775da52320344a260c9de/NLP_LIB_cpu-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "1212c067c695d807d98d46d91fe9c8f6", "sha256": "815c20f351233d0cf674ac1626153fbbc3aab1390c44029dab94f3aff1abfec7"}, "downloads": -1, "filename": "NLP_LIB_cpu-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "1212c067c695d807d98d46d91fe9c8f6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 162697, "upload_time": "2020-02-23T06:45:58", "upload_time_iso_8601": "2020-02-23T06:45:58.844666Z", "url": "https://files.pythonhosted.org/packages/d8/e8/9afdd604e95e89958bb4d302afdbde2ce8b302d741e3fa398cc633a60170/NLP_LIB_cpu-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "45004b1387beb86433f84325aa24ac63", "sha256": "0b467e5050c7944ecd13c7abe5e4f21c59827a5c169e23993488de117efc1a22"}, "downloads": -1, "filename": "NLP_LIB_cpu-0.0.5.tar.gz", "has_sig": false, "md5_digest": "45004b1387beb86433f84325aa24ac63", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 54604, "upload_time": "2020-02-23T06:46:00", "upload_time_iso_8601": "2020-02-23T06:46:00.582381Z", "url": "https://files.pythonhosted.org/packages/2f/f6/04b35e0e843279f96a1b59d12d4335720ea481ab23e05d1cb809fec5f00b/NLP_LIB_cpu-0.0.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1212c067c695d807d98d46d91fe9c8f6", "sha256": "815c20f351233d0cf674ac1626153fbbc3aab1390c44029dab94f3aff1abfec7"}, "downloads": -1, "filename": "NLP_LIB_cpu-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "1212c067c695d807d98d46d91fe9c8f6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 162697, "upload_time": "2020-02-23T06:45:58", "upload_time_iso_8601": "2020-02-23T06:45:58.844666Z", "url": "https://files.pythonhosted.org/packages/d8/e8/9afdd604e95e89958bb4d302afdbde2ce8b302d741e3fa398cc633a60170/NLP_LIB_cpu-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "45004b1387beb86433f84325aa24ac63", "sha256": "0b467e5050c7944ecd13c7abe5e4f21c59827a5c169e23993488de117efc1a22"}, "downloads": -1, "filename": "NLP_LIB_cpu-0.0.5.tar.gz", "has_sig": false, "md5_digest": "45004b1387beb86433f84325aa24ac63", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 54604, "upload_time": "2020-02-23T06:46:00", "upload_time_iso_8601": "2020-02-23T06:46:00.582381Z", "url": "https://files.pythonhosted.org/packages/2f/f6/04b35e0e843279f96a1b59d12d4335720ea481ab23e05d1cb809fec5f00b/NLP_LIB_cpu-0.0.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:45:10 2020"}