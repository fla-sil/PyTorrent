{"info": {"author": "Emile van Krieken", "author_email": "e.van.krieken@vu.nl", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: GNU Affero General Public License v3", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "![](logo.png)\n\nStochastic Deep Learning for Pytorch\n\n\n## About\n**Storchastic** is a PyTorch library for stochastic gradient estimation in Deep Learning [1]. Many state of the art deep learning\nmodels use gradient estimation, in particular within the fields of Variational Inference and Reinforcement Learning.\nWhile PyTorch computes gradients of deterministic computation graphs automatically, it will not estimate\ngradients on **stochastic computation graphs** [2].\n\nWith Storchastic, you can easily define any stochastic deep learning model and let it estimate the gradients for you. \nStorchastic provides a large range of gradient estimation methods that you can plug and play, to figure out which one works\nbest for your problem. Storchastic provides automatic broadcasting of sampled batch dimensions, which increases code\nreadability and allows implementing complex models with ease.\n\nWhen dealing with continuous random variables and differentiable functions, the popular reparameterization method [3] is usually\nvery effective. However, this method is not applicable when dealing with discrete random variables or non-differentiable functions.\nThis is why Storchastic has a focus on gradient estimators for discrete random variables, non-differentiable functions and\nsequence models.\n\n\n[Documentation (under construction) on Read the Docs.](https://storchastic.readthedocs.io/en/latest/)\n\n[Example: Discrete Variational Auto-Encoder](TODO)\n\n## Installation\nPip is going to be set up soon. Requires [Pyro](http://pyro.ai) for now. Build on Python 3.7 and PyTorch 1.4.\n\n## Implemented Algorithms\nFeel free to create an issue if an estimator is missing here.\n- Reparameterization [1, 3]\n- Score Function (REINFORCE) with Moving Average baseline [1, 4]\n- Score Function with Batch Average Baseline [5, 6]\n- Expected value for enumerable distributions\n- (Straight through) Gumbel Softmax [7, 8]\n- LAX, RELAX [9] \n- REBAR [10]\n\n### In development\n- Memory Augmented Policy Optimization [11]\n- Rao-Blackwellized REINFORCE [12]\n- Unordered Set Estimator [13]\n### Planned\n- Measure valued derivatives [1, 14]\n- ARM [15]\n- ...\n\n## References\n- [1] [Monte Carlo Gradient Estimation in Machine Learning](https://arxiv.org/abs/1906.10652), Mohamed et al, 2019\n- [2] [Gradient Estimation Using Stochastic Computation Graphs](https://arxiv.org/abs/1506.05254), Schulman et al, NeurIPS 2015\n- [3] [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114), Kingma and Welling, ICLR 2014\n- [4] [Simple statistical gradient-following algorithms for connectionist reinforcement learning](https://link-springer-com.vu-nl.idm.oclc.org/article/10.1007/BF00992696), Williams, Machine Learning 1992\n- [5] [Variational inference for Monte Carlo objectives](https://arxiv.org/abs/1602.06725), Mnih and Rezende, ICML 2016\n- [6] [Buy 4 REINFORCE Samples, Get a Baseline for Free!](https://openreview.net/pdf?id=r1lgTGL5DE), Kool et al, ICLR Workshop dlStructPred 2019\n- [7] [Categorical Reparameterization with Gumbel-Softmax](https://arxiv.org/abs/1611.01144), Jang et al, ICLR 2017\n- [8] [The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables](https://arxiv.org/abs/1611.00712), Maddison et al, ICLR 2017\n- [9] [Backpropagation through the Void: Optimizing control variates for black-box gradient estimation](https://arxiv.org/abs/1711.00123), Grathwohl et al, ICLR 2018\n- [10] [REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models](https://arxiv.org/abs/1703.07370), Tucker et al, NeurIPS 2017\n- [11] [Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing](https://arxiv.org/abs/1807.02322), Liang et al, NeurIPS 2018\n- [12] [Rao-Blackwellized Stochastic Gradients for Discrete Distributions](https://arxiv.org/abs/1810.04777), Liu et al, ICML 2019\n- [13] [Estimating Gradients for Discrete Random Variables by Sampling without Replacement](https://openreview.net/forum?id=rklEj2EFvB), Kool et al, ICLR 2020\n- [14] [Measure-Valued Derivatives for Approximate Bayesian Inference](http://bayesiandeeplearning.org/2019/papers/76.pdf), Rosca et al, Workshop on Bayesian Deep Learning (NeurIPS 2019)\n- [15] [ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks](https://arxiv.org/abs/1807.11143), Yin and Zhou, ICLR 2019\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/HEmile/storchastic", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "storchastic", "package_url": "https://pypi.org/project/storchastic/", "platform": "", "project_url": "https://pypi.org/project/storchastic/", "project_urls": {"Homepage": "https://github.com/HEmile/storchastic"}, "release_url": "https://pypi.org/project/storchastic/0.0.1/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Deep Learning for PyTorch", "version": "0.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/388655114bc901a8aeb49270f41e805988057feb/6c6f676f2e706e67\"></p>\n<p>Stochastic Deep Learning for Pytorch</p>\n<h2>About</h2>\n<p><strong>Storchastic</strong> is a PyTorch library for stochastic gradient estimation in Deep Learning [1]. Many state of the art deep learning\nmodels use gradient estimation, in particular within the fields of Variational Inference and Reinforcement Learning.\nWhile PyTorch computes gradients of deterministic computation graphs automatically, it will not estimate\ngradients on <strong>stochastic computation graphs</strong> [2].</p>\n<p>With Storchastic, you can easily define any stochastic deep learning model and let it estimate the gradients for you.\nStorchastic provides a large range of gradient estimation methods that you can plug and play, to figure out which one works\nbest for your problem. Storchastic provides automatic broadcasting of sampled batch dimensions, which increases code\nreadability and allows implementing complex models with ease.</p>\n<p>When dealing with continuous random variables and differentiable functions, the popular reparameterization method [3] is usually\nvery effective. However, this method is not applicable when dealing with discrete random variables or non-differentiable functions.\nThis is why Storchastic has a focus on gradient estimators for discrete random variables, non-differentiable functions and\nsequence models.</p>\n<p><a href=\"https://storchastic.readthedocs.io/en/latest/\" rel=\"nofollow\">Documentation (under construction) on Read the Docs.</a></p>\n<p><a href=\"TODO\" rel=\"nofollow\">Example: Discrete Variational Auto-Encoder</a></p>\n<h2>Installation</h2>\n<p>Pip is going to be set up soon. Requires <a href=\"http://pyro.ai\" rel=\"nofollow\">Pyro</a> for now. Build on Python 3.7 and PyTorch 1.4.</p>\n<h2>Implemented Algorithms</h2>\n<p>Feel free to create an issue if an estimator is missing here.</p>\n<ul>\n<li>Reparameterization [1, 3]</li>\n<li>Score Function (REINFORCE) with Moving Average baseline [1, 4]</li>\n<li>Score Function with Batch Average Baseline [5, 6]</li>\n<li>Expected value for enumerable distributions</li>\n<li>(Straight through) Gumbel Softmax [7, 8]</li>\n<li>LAX, RELAX [9]</li>\n<li>REBAR [10]</li>\n</ul>\n<h3>In development</h3>\n<ul>\n<li>Memory Augmented Policy Optimization [11]</li>\n<li>Rao-Blackwellized REINFORCE [12]</li>\n<li>Unordered Set Estimator [13]</li>\n</ul>\n<h3>Planned</h3>\n<ul>\n<li>Measure valued derivatives [1, 14]</li>\n<li>ARM [15]</li>\n<li>...</li>\n</ul>\n<h2>References</h2>\n<ul>\n<li>[1] <a href=\"https://arxiv.org/abs/1906.10652\" rel=\"nofollow\">Monte Carlo Gradient Estimation in Machine Learning</a>, Mohamed et al, 2019</li>\n<li>[2] <a href=\"https://arxiv.org/abs/1506.05254\" rel=\"nofollow\">Gradient Estimation Using Stochastic Computation Graphs</a>, Schulman et al, NeurIPS 2015</li>\n<li>[3] <a href=\"https://arxiv.org/abs/1312.6114\" rel=\"nofollow\">Auto-Encoding Variational Bayes</a>, Kingma and Welling, ICLR 2014</li>\n<li>[4] <a href=\"https://link-springer-com.vu-nl.idm.oclc.org/article/10.1007/BF00992696\" rel=\"nofollow\">Simple statistical gradient-following algorithms for connectionist reinforcement learning</a>, Williams, Machine Learning 1992</li>\n<li>[5] <a href=\"https://arxiv.org/abs/1602.06725\" rel=\"nofollow\">Variational inference for Monte Carlo objectives</a>, Mnih and Rezende, ICML 2016</li>\n<li>[6] <a href=\"https://openreview.net/pdf?id=r1lgTGL5DE\" rel=\"nofollow\">Buy 4 REINFORCE Samples, Get a Baseline for Free!</a>, Kool et al, ICLR Workshop dlStructPred 2019</li>\n<li>[7] <a href=\"https://arxiv.org/abs/1611.01144\" rel=\"nofollow\">Categorical Reparameterization with Gumbel-Softmax</a>, Jang et al, ICLR 2017</li>\n<li>[8] <a href=\"https://arxiv.org/abs/1611.00712\" rel=\"nofollow\">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</a>, Maddison et al, ICLR 2017</li>\n<li>[9] <a href=\"https://arxiv.org/abs/1711.00123\" rel=\"nofollow\">Backpropagation through the Void: Optimizing control variates for black-box gradient estimation</a>, Grathwohl et al, ICLR 2018</li>\n<li>[10] <a href=\"https://arxiv.org/abs/1703.07370\" rel=\"nofollow\">REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models</a>, Tucker et al, NeurIPS 2017</li>\n<li>[11] <a href=\"https://arxiv.org/abs/1807.02322\" rel=\"nofollow\">Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing</a>, Liang et al, NeurIPS 2018</li>\n<li>[12] <a href=\"https://arxiv.org/abs/1810.04777\" rel=\"nofollow\">Rao-Blackwellized Stochastic Gradients for Discrete Distributions</a>, Liu et al, ICML 2019</li>\n<li>[13] <a href=\"https://openreview.net/forum?id=rklEj2EFvB\" rel=\"nofollow\">Estimating Gradients for Discrete Random Variables by Sampling without Replacement</a>, Kool et al, ICLR 2020</li>\n<li>[14] <a href=\"http://bayesiandeeplearning.org/2019/papers/76.pdf\" rel=\"nofollow\">Measure-Valued Derivatives for Approximate Bayesian Inference</a>, Rosca et al, Workshop on Bayesian Deep Learning (NeurIPS 2019)</li>\n<li>[15] <a href=\"https://arxiv.org/abs/1807.11143\" rel=\"nofollow\">ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks</a>, Yin and Zhou, ICLR 2019</li>\n</ul>\n\n          </div>"}, "last_serial": 6970479, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "7cf5ec546e06818cf3ebb3d5e525f90e", "sha256": "755f91e3e1d734cda1b9bccc7dc4ab1e5c65d51223da1f26680b4cdced7e8d84"}, "downloads": -1, "filename": "storchastic-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "7cf5ec546e06818cf3ebb3d5e525f90e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 58005, "upload_time": "2020-04-07T14:44:04", "upload_time_iso_8601": "2020-04-07T14:44:04.452147Z", "url": "https://files.pythonhosted.org/packages/d8/1b/f352e13bf73a07ec12e2515e1c5c81397f7788caf003b0dc95621df25157/storchastic-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b9f13a973f802585a88cf54ca98238ad", "sha256": "017ffdbab4eab8850099eb15506455de5721e49c5bdb12f032eb7d191266c89b"}, "downloads": -1, "filename": "storchastic-0.0.1.tar.gz", "has_sig": false, "md5_digest": "b9f13a973f802585a88cf54ca98238ad", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 36444, "upload_time": "2020-04-07T14:44:06", "upload_time_iso_8601": "2020-04-07T14:44:06.945949Z", "url": "https://files.pythonhosted.org/packages/72/79/86d0f75fb145f3c2d4a0a3a7a9a7d9a74ad31aa3d8a167db5f192444b7f6/storchastic-0.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "7cf5ec546e06818cf3ebb3d5e525f90e", "sha256": "755f91e3e1d734cda1b9bccc7dc4ab1e5c65d51223da1f26680b4cdced7e8d84"}, "downloads": -1, "filename": "storchastic-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "7cf5ec546e06818cf3ebb3d5e525f90e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 58005, "upload_time": "2020-04-07T14:44:04", "upload_time_iso_8601": "2020-04-07T14:44:04.452147Z", "url": "https://files.pythonhosted.org/packages/d8/1b/f352e13bf73a07ec12e2515e1c5c81397f7788caf003b0dc95621df25157/storchastic-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b9f13a973f802585a88cf54ca98238ad", "sha256": "017ffdbab4eab8850099eb15506455de5721e49c5bdb12f032eb7d191266c89b"}, "downloads": -1, "filename": "storchastic-0.0.1.tar.gz", "has_sig": false, "md5_digest": "b9f13a973f802585a88cf54ca98238ad", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 36444, "upload_time": "2020-04-07T14:44:06", "upload_time_iso_8601": "2020-04-07T14:44:06.945949Z", "url": "https://files.pythonhosted.org/packages/72/79/86d0f75fb145f3c2d4a0a3a7a9a7d9a74ad31aa3d8a167db5f192444b7f6/storchastic-0.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:01:39 2020"}