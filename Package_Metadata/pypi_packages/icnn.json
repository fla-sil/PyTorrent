{"info": {"author": "Shen Guo-Hua, Kei Majima", "author_email": "brainliner-admin@atr.jp", "bugtrack_url": null, "classifiers": [], "description": "# Inverting CNN (iCNN): image reconstruction from CNN features\n\nThis repository contains source codes of the image reconstruction algorithms used in the paper \"Deep image reconstruction from human brain activity\" [1].\nThe image reconstruction algorithms are extension of the algorithm proposed in the paper \"Understanding deep image representations by inverting them\" [2].\n\nThe basic idea of the algorithm [2] is that the image is reconstructed such that the CNN features of the reconstructed image are close to those of the target image.\nThe reconstruction is solved by gradient based optimization algorithm. The optimization starts with a random initial image, inputs the initial image to the CNN model, calculates the error in feature space of the CNN, back-propagates the error to the image layer, and then updates the image. \n\nThe main modification of our implementation is to reconstruct image from CNN features of multiple layers.\n\nIn addition to gradient descent with momentum, we also implemented the reconstruction algorithm using [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) as the optimization algorithm.\n\nInspired by the paper \"Synthesizing the preferred inputs for neurons in neural networks via deep generator networks\" [3], the optimization of the reconstruction algorithm is performed in the input layer of a deep generator network, in order to introduce image prior to the reconstructed image.\nHere, the optimization starts with random initial features of the input layer of the deep generator network, inputs the initial features to the generator to generate the initial image, which is further input to the CNN model.\nThe errors in feature space of the CNN are back-propagated through the CNN to the image layer, and back-propagated further through the generator to the input layer of it, which is used to update the initial features of the input layer of the generator.\nThe generator will constrain the reconstructed image to a subspace of images which are more natural-looking.\n\nThere are 4 variants of implementation of the image reconstruction algorithm:\n\n- icnn_gd\n- icnn_lbfgs\n- icnn_dgn_gd\n- icnn_dgn_lbfgs\n\nThe **icnn_gd** implements the image reconstruction algorithm using gradient descent (GD) with momentum as the optimization algorithm. The **icnn_lbfgs** implements the image reconstruction algorithm using the L-BFGS as the optimization algorithm. The **icnn_dgn_gd** implements the image reconstruction algorithm using a deep generator network (DGN) to introduce image prior, and using gradient descent (GD) with momentum as the optimization algorithm. The **icnn_dgn_lbfgs** implements the image reconstruction algorithm using a deep generator network (DGN) to introduce image prior, and using the L-BFGS as the optimization algorithm.\n\n## Requirements\n\n- Python 2.7\n    - Python 3 support is upcoming\n- Numpy\n- Scipy\n- Caffe with up-convolutional layer\n    - https://github.com/dosovits/caffe-fr-chairs (Branch: deepsim)\n    - Both CPU and GPU installation are OK\n\n## Installation\n\n``` shellsession\n$ pip install icnn\n```\n\n## Basic Usage\n\n### \"icnn_gd\" and \"icnn_lbfgs\"\n\n``` python\nfrom icnn.icnn_gd import reconstruct_image\n\nimg, loss_list = reconstruct_image(features, net)\n```\n\n``` python\nfrom icnn.icnn_lbfgs import reconstruct_image\n\nimg, loss_list = reconstruct_image(features, net)\n```\n\n- Inputs\n    - `features`: a python dictionary consists of the target CNN features, arranged in pairs of layer name (key) and CNN features (value)\n    - `net`: the cnn model for the target features (caffe.Classifier or caffe.Net)\n- Outputs\n    - `img`: the reconstructed image (numpy.float32 [227x227x3])\n    - `loss_list`: 1 dimensional array of the value of the loss for each iteration (numpy array of numpy.float32)\n\n### \"icnn_dgn_gd\" and \"icnn_dgn_lbfgs\"\n\n``` python\nfrom icnn.icnn_dgn_gd import reconstruct_image\n\nimg, loss_list = reconstruct_image(features, net, net_gen)\n```\n\n``` python\nfrom icnn.icnn_dgn_lbfgs import reconstruct_image\n\nimg, loss_list = reconstruct_image(features, net, net_gen)\n```\n\n- Inputs\n    - `features`: a python dictionary consists of the target CNN features, arranged in pairs of layer name (key) and CNN features (value)\n    - `net`: the cnn model for the target features (caffe.Classifier or caffe.Net)\n    - `net_gen`: the network for the generator (caffe.Net)\n- Outputs\n    - `img`: the reconstructed image (numpy.float32 [227x227x3])\n    - `loss_list`: 1 dimensional array of the value of the loss for each iteration (numpy array of numpy.float32)\n\n## Examples\n\n### Example codes to reconstruct image from true CNN features:\n\n- `recon_img_from_true_feature_icnn_gd.py`\n- `recon_img_from_true_feature_icnn_lbfgs.py`\n- `recon_img_from_true_feature_icnn_dgn_gd.py`\n- `recon_img_from_true_feature_icnn_dgn_lbfgs.py`\n\n### Example codes to reconstruct image from CNN features decoded from brain:\n\n- `recon_img_from_decoded_feature_icnn_gd.py`\n- `recon_img_from_decoded_feature_icnn_lbfgs.py`\n- `recon_img_from_decoded_feature_icnn_dgn_gd.py`\n- `recon_img_from_decoded_feature_icnn_dgn_lbfgs.py`\n\n## CNN model\n\nIn the example codes, we use pre-trained VGG19 model (caffemodel_url: http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel).\nYou can replace it with any other CNN models in the example codes.\nIn order to make back-propagation work, one line should be added to the prototxt file (the file describes the configuration of the CNN model):\n\n`force_backward: true`.\n\n## CNN features before or after ReLU\n\nIn our study [1] and the example codes in this repository, we define CNN features of conv layers or fc layers as the output immediately after the convolutional or fully-connected computation, before applying the Rectified-Linear-Unit (ReLU).\nHowever, as default setting, ReLU operation is an in-place computation, which will override the CNN features we need.\nIn order to use the CNN features before the ReLU operation, we need to modify the prototxt file.\nTaking the VGG19 prototxt file as an example:\n\nIn the original prototxt file, ReLU is in-place computation:\n\n```\nlayers {\n  bottom: \"conv1_1\"\n  top: \"conv1_1\"\n  name: \"relu1_1\"\n  type: RELU\n}\n```\n\nNow, we modify it as:\n\n```\nlayers {\n  bottom: \"conv1_1\"\n  top: \"relu1_1\"\n  name: \"relu1_1\"\n  type: RELU\n}\n```\n\n## Deep Generator Network\n\nIn our study [1] and the example codes in this repository, we use pre-trained deep generator network (DGN) from the study [4] (downloaded from: https://lmb.informatik.uni-freiburg.de/resources/binaries/arxiv2016_alexnet_inversion_with_gans/release_deepsim_v0.zip).\nIn order to make back-propagation work, one line should be added to the prototxt file (the file describes the configuration of the DGN):\n\n`force_backward: true`.\n\n\n## Changing batch size to speed reconstruction process\nThe reconstruction is designed to reconstruct one image each time, while the CNN model and deep generator net process batch of data for each forward and backward computation.\nIn order to avoid irrelevant calculation and speed the reconstruction process, we can modify the batch size to  be 1.\nFor example, we set the first dimension (batch size) to 1 in the prototxt of the deep generator net (/examples/net/generator_for_inverting_fc7/generator.prototxt):\n\n```\n...\ninput: \"feat\"\ninput_shape {\n  dim: 1  # 64 --> 1\n  dim: 4096\n}\n\n...\n\nlayer {\n  name: \"reshape_relu_defc5\"\n  type: \"Reshape\"\n  bottom: \"relu_defc5\"\n  top: \"reshape_relu_defc5\"\n  reshape_param {\n    shape {\n      dim: 1  # 64 --> 1\n      dim: 256\n      dim: 4\n      dim: 4\n    }\n  }\n}\n...\n```\n\n\n## Reference\n\n- [1] Shen G, Horikawa T, Majima K, and Kamitani Y (2017). Deep image reconstruction from human brain activity. https://www.biorxiv.org/content/early/2017/12/28/240317\n- [2] Mahendran A and Vedaldi A (2015). Understanding deep image representations by inverting them. https://arxiv.org/abs/1412.0035\n- [3] Nguyen A, Dosovitskiy A, Yosinski J, Brox T, and Clune J (2016). Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. https://arxiv.org/abs/1605.09304\n- [4] Dosovitskiy A and Brox T (2016). Generating Images with Perceptual Similarity Metrics based on Deep Networks. https://arxiv.org/abs/1602.02644\n\n## Copyright and License\n\nCopyright (c) 2018 Kamitani Lab (http://kamitani-lab.ist.i.kyoto-u.ac.jp/)\n\nThe scripts provided here are released under the MIT license (http://opensource.org/licenses/mit-license.php).\n\n## Authors\n\n- Shen Guo-Hua (E-mail: shen-gh@atr.jp)\n- Kei Majima (E-mail: majima@i.kyoto-u.ac.jp)\n\n## Acknowledgement\n\nThe authors thank Mitsuaki Tsukamoto for software installation and computational environment setting.\nThe authors thank Shuntaro Aoki for useful advice on code arrangement.\nThe authors thank precious discussion and advice from the members in DNI (http://www.cns.atr.jp/dni/) and Kamitani Lab (http://kamitani-lab.ist.i.kyoto-u.ac.jp/).\n\nThe codes in this repository are inspired by many existing image generation and reconstruction studies and their open-source implementations, including:\n\n- The source code of the paper \"Understanding deep image representations by inverting them\": https://github.com/aravindhm/deep-goggle\n- The source code of the paper \"Synthesizing the preferred inputs for neurons in neural networks via deep generator networks\": https://github.com/Evolving-AI-Lab/synthesizing\n- Deepdream: https://github.com/google/deepdream\n- Deepdraw: https://www.auduno.com/2015/07/29/visualizing-googlenet-classes/\n- Several tricks (e.g. clipping the pixels with small norm, and clipping the pixels with small contribution) in \"iCNN_GD\" are borrowed from the paper \"Understanding Neural Networks Through Deep Visualization\": http://yosinski.com/deepvis\n- The gram matrix loss is borrowed from the paper \"A Neural Algorithm of Artistic Style\": https://arxiv.org/abs/1508.06576\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/KamitaniLab/icnn", "keywords": "neuroscience,brain decoding,deep learning,image reconstruction", "license": "MIT", "maintainer": "Shuntaro C. Aoki", "maintainer_email": "brainliner-admin@atr.jp", "name": "icnn", "package_url": "https://pypi.org/project/icnn/", "platform": "", "project_url": "https://pypi.org/project/icnn/", "project_urls": {"Homepage": "https://github.com/KamitaniLab/icnn"}, "release_url": "https://pypi.org/project/icnn/1.2.2/", "requires_dist": ["numpy", "scipy"], "requires_python": "", "summary": "Deep image reconstruction from CNN features (Inverting CNN)", "version": "1.2.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Inverting CNN (iCNN): image reconstruction from CNN features</h1>\n<p>This repository contains source codes of the image reconstruction algorithms used in the paper \"Deep image reconstruction from human brain activity\" [1].\nThe image reconstruction algorithms are extension of the algorithm proposed in the paper \"Understanding deep image representations by inverting them\" [2].</p>\n<p>The basic idea of the algorithm [2] is that the image is reconstructed such that the CNN features of the reconstructed image are close to those of the target image.\nThe reconstruction is solved by gradient based optimization algorithm. The optimization starts with a random initial image, inputs the initial image to the CNN model, calculates the error in feature space of the CNN, back-propagates the error to the image layer, and then updates the image.</p>\n<p>The main modification of our implementation is to reconstruct image from CNN features of multiple layers.</p>\n<p>In addition to gradient descent with momentum, we also implemented the reconstruction algorithm using <a href=\"https://en.wikipedia.org/wiki/Limited-memory_BFGS\" rel=\"nofollow\">L-BFGS</a> as the optimization algorithm.</p>\n<p>Inspired by the paper \"Synthesizing the preferred inputs for neurons in neural networks via deep generator networks\" [3], the optimization of the reconstruction algorithm is performed in the input layer of a deep generator network, in order to introduce image prior to the reconstructed image.\nHere, the optimization starts with random initial features of the input layer of the deep generator network, inputs the initial features to the generator to generate the initial image, which is further input to the CNN model.\nThe errors in feature space of the CNN are back-propagated through the CNN to the image layer, and back-propagated further through the generator to the input layer of it, which is used to update the initial features of the input layer of the generator.\nThe generator will constrain the reconstructed image to a subspace of images which are more natural-looking.</p>\n<p>There are 4 variants of implementation of the image reconstruction algorithm:</p>\n<ul>\n<li>icnn_gd</li>\n<li>icnn_lbfgs</li>\n<li>icnn_dgn_gd</li>\n<li>icnn_dgn_lbfgs</li>\n</ul>\n<p>The <strong>icnn_gd</strong> implements the image reconstruction algorithm using gradient descent (GD) with momentum as the optimization algorithm. The <strong>icnn_lbfgs</strong> implements the image reconstruction algorithm using the L-BFGS as the optimization algorithm. The <strong>icnn_dgn_gd</strong> implements the image reconstruction algorithm using a deep generator network (DGN) to introduce image prior, and using gradient descent (GD) with momentum as the optimization algorithm. The <strong>icnn_dgn_lbfgs</strong> implements the image reconstruction algorithm using a deep generator network (DGN) to introduce image prior, and using the L-BFGS as the optimization algorithm.</p>\n<h2>Requirements</h2>\n<ul>\n<li>Python 2.7\n<ul>\n<li>Python 3 support is upcoming</li>\n</ul>\n</li>\n<li>Numpy</li>\n<li>Scipy</li>\n<li>Caffe with up-convolutional layer\n<ul>\n<li><a href=\"https://github.com/dosovits/caffe-fr-chairs\" rel=\"nofollow\">https://github.com/dosovits/caffe-fr-chairs</a> (Branch: deepsim)</li>\n<li>Both CPU and GPU installation are OK</li>\n</ul>\n</li>\n</ul>\n<h2>Installation</h2>\n<pre>$ pip install icnn\n</pre>\n<h2>Basic Usage</h2>\n<h3>\"icnn_gd\" and \"icnn_lbfgs\"</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">icnn.icnn_gd</span> <span class=\"kn\">import</span> <span class=\"n\">reconstruct_image</span>\n\n<span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">loss_list</span> <span class=\"o\">=</span> <span class=\"n\">reconstruct_image</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"p\">,</span> <span class=\"n\">net</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">icnn.icnn_lbfgs</span> <span class=\"kn\">import</span> <span class=\"n\">reconstruct_image</span>\n\n<span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">loss_list</span> <span class=\"o\">=</span> <span class=\"n\">reconstruct_image</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"p\">,</span> <span class=\"n\">net</span><span class=\"p\">)</span>\n</pre>\n<ul>\n<li>Inputs\n<ul>\n<li><code>features</code>: a python dictionary consists of the target CNN features, arranged in pairs of layer name (key) and CNN features (value)</li>\n<li><code>net</code>: the cnn model for the target features (caffe.Classifier or caffe.Net)</li>\n</ul>\n</li>\n<li>Outputs\n<ul>\n<li><code>img</code>: the reconstructed image (numpy.float32 [227x227x3])</li>\n<li><code>loss_list</code>: 1 dimensional array of the value of the loss for each iteration (numpy array of numpy.float32)</li>\n</ul>\n</li>\n</ul>\n<h3>\"icnn_dgn_gd\" and \"icnn_dgn_lbfgs\"</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">icnn.icnn_dgn_gd</span> <span class=\"kn\">import</span> <span class=\"n\">reconstruct_image</span>\n\n<span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">loss_list</span> <span class=\"o\">=</span> <span class=\"n\">reconstruct_image</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"p\">,</span> <span class=\"n\">net</span><span class=\"p\">,</span> <span class=\"n\">net_gen</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">icnn.icnn_dgn_lbfgs</span> <span class=\"kn\">import</span> <span class=\"n\">reconstruct_image</span>\n\n<span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">loss_list</span> <span class=\"o\">=</span> <span class=\"n\">reconstruct_image</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"p\">,</span> <span class=\"n\">net</span><span class=\"p\">,</span> <span class=\"n\">net_gen</span><span class=\"p\">)</span>\n</pre>\n<ul>\n<li>Inputs\n<ul>\n<li><code>features</code>: a python dictionary consists of the target CNN features, arranged in pairs of layer name (key) and CNN features (value)</li>\n<li><code>net</code>: the cnn model for the target features (caffe.Classifier or caffe.Net)</li>\n<li><code>net_gen</code>: the network for the generator (caffe.Net)</li>\n</ul>\n</li>\n<li>Outputs\n<ul>\n<li><code>img</code>: the reconstructed image (numpy.float32 [227x227x3])</li>\n<li><code>loss_list</code>: 1 dimensional array of the value of the loss for each iteration (numpy array of numpy.float32)</li>\n</ul>\n</li>\n</ul>\n<h2>Examples</h2>\n<h3>Example codes to reconstruct image from true CNN features:</h3>\n<ul>\n<li><code>recon_img_from_true_feature_icnn_gd.py</code></li>\n<li><code>recon_img_from_true_feature_icnn_lbfgs.py</code></li>\n<li><code>recon_img_from_true_feature_icnn_dgn_gd.py</code></li>\n<li><code>recon_img_from_true_feature_icnn_dgn_lbfgs.py</code></li>\n</ul>\n<h3>Example codes to reconstruct image from CNN features decoded from brain:</h3>\n<ul>\n<li><code>recon_img_from_decoded_feature_icnn_gd.py</code></li>\n<li><code>recon_img_from_decoded_feature_icnn_lbfgs.py</code></li>\n<li><code>recon_img_from_decoded_feature_icnn_dgn_gd.py</code></li>\n<li><code>recon_img_from_decoded_feature_icnn_dgn_lbfgs.py</code></li>\n</ul>\n<h2>CNN model</h2>\n<p>In the example codes, we use pre-trained VGG19 model (caffemodel_url: <a href=\"http://www.robots.ox.ac.uk/%7Evgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel\" rel=\"nofollow\">http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel</a>).\nYou can replace it with any other CNN models in the example codes.\nIn order to make back-propagation work, one line should be added to the prototxt file (the file describes the configuration of the CNN model):</p>\n<p><code>force_backward: true</code>.</p>\n<h2>CNN features before or after ReLU</h2>\n<p>In our study [1] and the example codes in this repository, we define CNN features of conv layers or fc layers as the output immediately after the convolutional or fully-connected computation, before applying the Rectified-Linear-Unit (ReLU).\nHowever, as default setting, ReLU operation is an in-place computation, which will override the CNN features we need.\nIn order to use the CNN features before the ReLU operation, we need to modify the prototxt file.\nTaking the VGG19 prototxt file as an example:</p>\n<p>In the original prototxt file, ReLU is in-place computation:</p>\n<pre><code>layers {\n  bottom: \"conv1_1\"\n  top: \"conv1_1\"\n  name: \"relu1_1\"\n  type: RELU\n}\n</code></pre>\n<p>Now, we modify it as:</p>\n<pre><code>layers {\n  bottom: \"conv1_1\"\n  top: \"relu1_1\"\n  name: \"relu1_1\"\n  type: RELU\n}\n</code></pre>\n<h2>Deep Generator Network</h2>\n<p>In our study [1] and the example codes in this repository, we use pre-trained deep generator network (DGN) from the study [4] (downloaded from: <a href=\"https://lmb.informatik.uni-freiburg.de/resources/binaries/arxiv2016_alexnet_inversion_with_gans/release_deepsim_v0.zip\" rel=\"nofollow\">https://lmb.informatik.uni-freiburg.de/resources/binaries/arxiv2016_alexnet_inversion_with_gans/release_deepsim_v0.zip</a>).\nIn order to make back-propagation work, one line should be added to the prototxt file (the file describes the configuration of the DGN):</p>\n<p><code>force_backward: true</code>.</p>\n<h2>Changing batch size to speed reconstruction process</h2>\n<p>The reconstruction is designed to reconstruct one image each time, while the CNN model and deep generator net process batch of data for each forward and backward computation.\nIn order to avoid irrelevant calculation and speed the reconstruction process, we can modify the batch size to  be 1.\nFor example, we set the first dimension (batch size) to 1 in the prototxt of the deep generator net (/examples/net/generator_for_inverting_fc7/generator.prototxt):</p>\n<pre><code>...\ninput: \"feat\"\ninput_shape {\n  dim: 1  # 64 --&gt; 1\n  dim: 4096\n}\n\n...\n\nlayer {\n  name: \"reshape_relu_defc5\"\n  type: \"Reshape\"\n  bottom: \"relu_defc5\"\n  top: \"reshape_relu_defc5\"\n  reshape_param {\n    shape {\n      dim: 1  # 64 --&gt; 1\n      dim: 256\n      dim: 4\n      dim: 4\n    }\n  }\n}\n...\n</code></pre>\n<h2>Reference</h2>\n<ul>\n<li>[1] Shen G, Horikawa T, Majima K, and Kamitani Y (2017). Deep image reconstruction from human brain activity. <a href=\"https://www.biorxiv.org/content/early/2017/12/28/240317\" rel=\"nofollow\">https://www.biorxiv.org/content/early/2017/12/28/240317</a></li>\n<li>[2] Mahendran A and Vedaldi A (2015). Understanding deep image representations by inverting them. <a href=\"https://arxiv.org/abs/1412.0035\" rel=\"nofollow\">https://arxiv.org/abs/1412.0035</a></li>\n<li>[3] Nguyen A, Dosovitskiy A, Yosinski J, Brox T, and Clune J (2016). Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. <a href=\"https://arxiv.org/abs/1605.09304\" rel=\"nofollow\">https://arxiv.org/abs/1605.09304</a></li>\n<li>[4] Dosovitskiy A and Brox T (2016). Generating Images with Perceptual Similarity Metrics based on Deep Networks. <a href=\"https://arxiv.org/abs/1602.02644\" rel=\"nofollow\">https://arxiv.org/abs/1602.02644</a></li>\n</ul>\n<h2>Copyright and License</h2>\n<p>Copyright (c) 2018 Kamitani Lab (<a href=\"http://kamitani-lab.ist.i.kyoto-u.ac.jp/\" rel=\"nofollow\">http://kamitani-lab.ist.i.kyoto-u.ac.jp/</a>)</p>\n<p>The scripts provided here are released under the MIT license (<a href=\"http://opensource.org/licenses/mit-license.php\" rel=\"nofollow\">http://opensource.org/licenses/mit-license.php</a>).</p>\n<h2>Authors</h2>\n<ul>\n<li>Shen Guo-Hua (E-mail: <a href=\"mailto:shen-gh@atr.jp\">shen-gh@atr.jp</a>)</li>\n<li>Kei Majima (E-mail: <a href=\"mailto:majima@i.kyoto-u.ac.jp\">majima@i.kyoto-u.ac.jp</a>)</li>\n</ul>\n<h2>Acknowledgement</h2>\n<p>The authors thank Mitsuaki Tsukamoto for software installation and computational environment setting.\nThe authors thank Shuntaro Aoki for useful advice on code arrangement.\nThe authors thank precious discussion and advice from the members in DNI (<a href=\"http://www.cns.atr.jp/dni/\" rel=\"nofollow\">http://www.cns.atr.jp/dni/</a>) and Kamitani Lab (<a href=\"http://kamitani-lab.ist.i.kyoto-u.ac.jp/\" rel=\"nofollow\">http://kamitani-lab.ist.i.kyoto-u.ac.jp/</a>).</p>\n<p>The codes in this repository are inspired by many existing image generation and reconstruction studies and their open-source implementations, including:</p>\n<ul>\n<li>The source code of the paper \"Understanding deep image representations by inverting them\": <a href=\"https://github.com/aravindhm/deep-goggle\" rel=\"nofollow\">https://github.com/aravindhm/deep-goggle</a></li>\n<li>The source code of the paper \"Synthesizing the preferred inputs for neurons in neural networks via deep generator networks\": <a href=\"https://github.com/Evolving-AI-Lab/synthesizing\" rel=\"nofollow\">https://github.com/Evolving-AI-Lab/synthesizing</a></li>\n<li>Deepdream: <a href=\"https://github.com/google/deepdream\" rel=\"nofollow\">https://github.com/google/deepdream</a></li>\n<li>Deepdraw: <a href=\"https://www.auduno.com/2015/07/29/visualizing-googlenet-classes/\" rel=\"nofollow\">https://www.auduno.com/2015/07/29/visualizing-googlenet-classes/</a></li>\n<li>Several tricks (e.g. clipping the pixels with small norm, and clipping the pixels with small contribution) in \"iCNN_GD\" are borrowed from the paper \"Understanding Neural Networks Through Deep Visualization\": <a href=\"http://yosinski.com/deepvis\" rel=\"nofollow\">http://yosinski.com/deepvis</a></li>\n<li>The gram matrix loss is borrowed from the paper \"A Neural Algorithm of Artistic Style\": <a href=\"https://arxiv.org/abs/1508.06576\" rel=\"nofollow\">https://arxiv.org/abs/1508.06576</a></li>\n</ul>\n\n          </div>"}, "last_serial": 6061754, "releases": {"1.2.2": [{"comment_text": "", "digests": {"md5": "8095d43355cf8be6b592c25c5c792a0a", "sha256": "99b80de26ec9a7fd9bfdffa5355e1ee4f981f6f6b420ecd3e6a2e774b1c993c1"}, "downloads": -1, "filename": "icnn-1.2.2-py2-none-any.whl", "has_sig": false, "md5_digest": "8095d43355cf8be6b592c25c5c792a0a", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 20726, "upload_time": "2019-11-01T03:48:24", "upload_time_iso_8601": "2019-11-01T03:48:24.178451Z", "url": "https://files.pythonhosted.org/packages/dd/eb/64d0327e0ff6ea7fe9314106ea2269e6ad711498e2c3a529426319e0123e/icnn-1.2.2-py2-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "8095d43355cf8be6b592c25c5c792a0a", "sha256": "99b80de26ec9a7fd9bfdffa5355e1ee4f981f6f6b420ecd3e6a2e774b1c993c1"}, "downloads": -1, "filename": "icnn-1.2.2-py2-none-any.whl", "has_sig": false, "md5_digest": "8095d43355cf8be6b592c25c5c792a0a", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 20726, "upload_time": "2019-11-01T03:48:24", "upload_time_iso_8601": "2019-11-01T03:48:24.178451Z", "url": "https://files.pythonhosted.org/packages/dd/eb/64d0327e0ff6ea7fe9314106ea2269e6ad711498e2c3a529426319e0123e/icnn-1.2.2-py2-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 00:49:01 2020"}