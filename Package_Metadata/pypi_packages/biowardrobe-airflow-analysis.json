{"info": {"author": "Datirium, LLC", "author_email": "support@datirium.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Console", "Environment :: Other Environment", "Intended Audience :: Developers", "Intended Audience :: Healthcare Industry", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Operating System :: MacOS :: MacOS X", "Operating System :: Microsoft :: Windows", "Operating System :: Microsoft :: Windows :: Windows 10", "Operating System :: Microsoft :: Windows :: Windows 8.1", "Operating System :: OS Independent", "Operating System :: POSIX", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering", "Topic :: Scientific/Engineering :: Bio-Informatics", "Topic :: Scientific/Engineering :: Chemistry", "Topic :: Scientific/Engineering :: Information Analysis", "Topic :: Scientific/Engineering :: Medical Science Apps."], "description": "# BioWardrobe backend (airflow+cwl)\n\n### About\nPython package to replace [BioWardrobe's](https://github.com/Barski-lab/biowardrobe) python/cron scripts. It uses\n[Apache-Airflow](https://github.com/apache/incubator-airflow)\nfunctionality with [CWL v1.0](http://www.commonwl.org/v1.0/).\n\n### Install\n1. Add biowardrobe MySQL connection into Airflow connections\n    ```sql\n    select * from airflow.connection;\n    insert into airflow.connection values(NULL,'biowardrobe','mysql','localhost','ems','wardrobe','',null,'{\"cursor\":\"dictcursor\"}',0,0);\n    ```\n2. Install\n    ```sh\n    sudo pip3 install .\n    ```\n\n### Requirements\n1. Make sure your system satisfies the following criteria:\n      - Ubuntu 16.04.3\n        - python3.6\n            ```sh\n            sudo add-apt-repository ppa:jonathonf/python-3.6\n            sudo apt-get update\n            sudo apt-get install python3.6\n            ```\n        - pip3\n          ```sh\n          curl https://bootstrap.pypa.io/get-pip.py | sudo python3.6\n          pip3 install --upgrade pip3\n          ```\n        - setuptools\n          ```sh\n          pip3 install setuptools\n          ```\n        - [docker](https://docs.docker.com/engine/installation/linux/docker-ce/ubuntu/)\n          ```sh\n          sudo apt-get update\n          sudo apt-get install apt-transport-https ca-certificates curl software-properties-common\n          curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n          sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\"\n          sudo apt-get update\n          sudo apt-get install docker-ce\n          sudo groupadd docker\n          sudo usermod -aG docker $USER\n          ```\n          Log out and log back in so that your group membership is re-evaluated.\n        - libmysqlclient-dev\n          ```sh\n          sudo apt-get install libmysqlclient-dev\n          ```\n        - nodejs\n          ```sh\n          sudo apt-get install nodejs\n          ```\n2. Get the latest version of `cwl-airflow-parser`.\n   If **[Apache-Airflow](https://github.com/apache/incubator-airflow)**\n   or **[cwltool](http://www.commonwl.org/ \"cwltool main page\")** aren't installed,\n   installation will be done automatically with recommended versions. Set `AIRFLOW_HOME` environment \n   variable to airflow config directory default is `~/airflow/`.\n      ```sh\n      git clone https://github.com/datirium/cwl-airflow-parser.git\n      cd cwl-airflow-parser\n      sudo pip3 install .\n      ```\n\n3. If required, add **[extra airflow packages](https://airflow.incubator.apache.org/installation.html#extra-packages)**\n   for extending Airflow functionality, for instance, with MySQL support `pip3 install apache-airflow[mysql]`.\n\n### Running\n\n1. To create BioWardrobe's dags run `biowardrobe-init` in airflow's dags directory \n    ```\n    cd ~/airflow/dags\n    ./biowardrobe-init \n    ```\n2. Run Airflow scheduler:\n   ```sh\n   airflow scheduler\n   ```\n3. Use `airflow trigger_dag` with input parameter `--conf \"JSON\"` where JSON is either job definition or biowardrobe_uid \nand explicitly specified cwl descriptor `dag_id`.\n    ```sh\n    airflow trigger_dag --conf \"{\\\"job\\\":$(cat ./hg19.job)}\" \"bowtie-index\"\n    ```\n    where `hg19.job` is:\n    ```json\n    {\n      \"fasta_input_file\": {\n        \"class\": \"File\", \n        \"location\": \"file:///wardrobe/indices/bowtie/hg19/chrM.fa\", \n        \"format\":\"http://edamontology.org/format_1929\",\n        \"size\": 16909,\n        \"basename\": \"chrM.fa\",\n        \"nameroot\": \"chrM\",\n        \"nameext\": \".fa\"\n      },\n      \"output_folder\": \"/wardrobe/indices/bowtie/hg19/\",\n      \"threads\": 6,\n      \"genome\": \"hg19\"\n    }\n    ```\n\n4. All the output will be moved from temporary directory into **output_folder** parameter\n  of the job.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/datirium/biowardrobe-airflow-analysis", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/datirium/biowardrobe-airflow-analysis", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "biowardrobe-airflow-analysis", "package_url": "https://pypi.org/project/biowardrobe-airflow-analysis/", "platform": "", "project_url": "https://pypi.org/project/biowardrobe-airflow-analysis/", "project_urls": {"Download": "https://github.com/datirium/biowardrobe-airflow-analysis", "Homepage": "https://github.com/datirium/biowardrobe-airflow-analysis"}, "release_url": "https://pypi.org/project/biowardrobe-airflow-analysis/1.0.20181214162558/", "requires_dist": null, "requires_python": "", "summary": "Replaces BioWardrobe's backend with CWL Airflow", "version": "1.0.20181214162558", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>BioWardrobe backend (airflow+cwl)</h1>\n<h3>About</h3>\n<p>Python package to replace <a href=\"https://github.com/Barski-lab/biowardrobe\" rel=\"nofollow\">BioWardrobe's</a> python/cron scripts. It uses\n<a href=\"https://github.com/apache/incubator-airflow\" rel=\"nofollow\">Apache-Airflow</a>\nfunctionality with <a href=\"http://www.commonwl.org/v1.0/\" rel=\"nofollow\">CWL v1.0</a>.</p>\n<h3>Install</h3>\n<ol>\n<li>Add biowardrobe MySQL connection into Airflow connections\n<pre><span class=\"k\">select</span> <span class=\"o\">*</span> <span class=\"k\">from</span> <span class=\"n\">airflow</span><span class=\"p\">.</span><span class=\"k\">connection</span><span class=\"p\">;</span>\n<span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">airflow</span><span class=\"p\">.</span><span class=\"k\">connection</span> <span class=\"k\">values</span><span class=\"p\">(</span><span class=\"k\">NULL</span><span class=\"p\">,</span><span class=\"s1\">'biowardrobe'</span><span class=\"p\">,</span><span class=\"s1\">'mysql'</span><span class=\"p\">,</span><span class=\"s1\">'localhost'</span><span class=\"p\">,</span><span class=\"s1\">'ems'</span><span class=\"p\">,</span><span class=\"s1\">'wardrobe'</span><span class=\"p\">,</span><span class=\"s1\">''</span><span class=\"p\">,</span><span class=\"k\">null</span><span class=\"p\">,</span><span class=\"s1\">'{\"cursor\":\"dictcursor\"}'</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">);</span>\n</pre>\n</li>\n<li>Install\n<pre>sudo pip3 install .\n</pre>\n</li>\n</ol>\n<h3>Requirements</h3>\n<ol>\n<li>\n<p>Make sure your system satisfies the following criteria:</p>\n<ul>\n<li>Ubuntu 16.04.3\n<ul>\n<li>python3.6\n<pre>sudo add-apt-repository ppa:jonathonf/python-3.6\nsudo apt-get update\nsudo apt-get install python3.6\n</pre>\n</li>\n<li>pip3\n<pre>curl https://bootstrap.pypa.io/get-pip.py <span class=\"p\">|</span> sudo python3.6\npip3 install --upgrade pip3\n</pre>\n</li>\n<li>setuptools\n<pre>pip3 install setuptools\n</pre>\n</li>\n<li><a href=\"https://docs.docker.com/engine/installation/linux/docker-ce/ubuntu/\" rel=\"nofollow\">docker</a>\n<pre>sudo apt-get update\nsudo apt-get install apt-transport-https ca-certificates curl software-properties-common\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg <span class=\"p\">|</span> sudo apt-key add -\nsudo add-apt-repository <span class=\"s2\">\"deb [arch=amd64] https://download.docker.com/linux/ubuntu </span><span class=\"k\">$(</span>lsb_release -cs<span class=\"k\">)</span><span class=\"s2\"> stable\"</span>\nsudo apt-get update\nsudo apt-get install docker-ce\nsudo groupadd docker\nsudo usermod -aG docker <span class=\"nv\">$USER</span>\n</pre>\nLog out and log back in so that your group membership is re-evaluated.</li>\n<li>libmysqlclient-dev\n<pre>sudo apt-get install libmysqlclient-dev\n</pre>\n</li>\n<li>nodejs\n<pre>sudo apt-get install nodejs\n</pre>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Get the latest version of <code>cwl-airflow-parser</code>.\nIf <strong><a href=\"https://github.com/apache/incubator-airflow\" rel=\"nofollow\">Apache-Airflow</a></strong>\nor <strong><a href=\"http://www.commonwl.org/\" rel=\"nofollow\" title=\"cwltool main page\">cwltool</a></strong> aren't installed,\ninstallation will be done automatically with recommended versions. Set <code>AIRFLOW_HOME</code> environment\nvariable to airflow config directory default is <code>~/airflow/</code>.</p>\n<pre>git clone https://github.com/datirium/cwl-airflow-parser.git\n<span class=\"nb\">cd</span> cwl-airflow-parser\nsudo pip3 install .\n</pre>\n</li>\n<li>\n<p>If required, add <strong><a href=\"https://airflow.incubator.apache.org/installation.html#extra-packages\" rel=\"nofollow\">extra airflow packages</a></strong>\nfor extending Airflow functionality, for instance, with MySQL support <code>pip3 install apache-airflow[mysql]</code>.</p>\n</li>\n</ol>\n<h3>Running</h3>\n<ol>\n<li>\n<p>To create BioWardrobe's dags run <code>biowardrobe-init</code> in airflow's dags directory</p>\n<pre><code>cd ~/airflow/dags\n./biowardrobe-init \n</code></pre>\n</li>\n<li>\n<p>Run Airflow scheduler:</p>\n<pre>airflow scheduler\n</pre>\n</li>\n<li>\n<p>Use <code>airflow trigger_dag</code> with input parameter <code>--conf \"JSON\"</code> where JSON is either job definition or biowardrobe_uid\nand explicitly specified cwl descriptor <code>dag_id</code>.</p>\n<pre>airflow trigger_dag --conf <span class=\"s2\">\"{\\\"job\\\":</span><span class=\"k\">$(</span>cat ./hg19.job<span class=\"k\">)</span><span class=\"s2\">}\"</span> <span class=\"s2\">\"bowtie-index\"</span>\n</pre>\n<p>where <code>hg19.job</code> is:</p>\n<pre><span class=\"p\">{</span>\n  <span class=\"nt\">\"fasta_input_file\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n    <span class=\"nt\">\"class\"</span><span class=\"p\">:</span> <span class=\"s2\">\"File\"</span><span class=\"p\">,</span> \n    <span class=\"nt\">\"location\"</span><span class=\"p\">:</span> <span class=\"s2\">\"file:///wardrobe/indices/bowtie/hg19/chrM.fa\"</span><span class=\"p\">,</span> \n    <span class=\"nt\">\"format\"</span><span class=\"p\">:</span><span class=\"s2\">\"http://edamontology.org/format_1929\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"size\"</span><span class=\"p\">:</span> <span class=\"mi\">16909</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"basename\"</span><span class=\"p\">:</span> <span class=\"s2\">\"chrM.fa\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"nameroot\"</span><span class=\"p\">:</span> <span class=\"s2\">\"chrM\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"nameext\"</span><span class=\"p\">:</span> <span class=\"s2\">\".fa\"</span>\n  <span class=\"p\">},</span>\n  <span class=\"nt\">\"output_folder\"</span><span class=\"p\">:</span> <span class=\"s2\">\"/wardrobe/indices/bowtie/hg19/\"</span><span class=\"p\">,</span>\n  <span class=\"nt\">\"threads\"</span><span class=\"p\">:</span> <span class=\"mi\">6</span><span class=\"p\">,</span>\n  <span class=\"nt\">\"genome\"</span><span class=\"p\">:</span> <span class=\"s2\">\"hg19\"</span>\n<span class=\"p\">}</span>\n</pre>\n</li>\n<li>\n<p>All the output will be moved from temporary directory into <strong>output_folder</strong> parameter\nof the job.</p>\n</li>\n</ol>\n\n          </div>"}, "last_serial": 4600014, "releases": {"1.0.20180430223646": [{"comment_text": "", "digests": {"md5": "d609501482cd4591c2075eb1e138db5d", "sha256": "9050ab6b31457c05d6c3be95319ff82c63a04300423eaf31e61ff748e1c6a6e0"}, "downloads": -1, "filename": "biowardrobe-airflow-analysis-1.0.20180430223646.tar.gz", "has_sig": false, "md5_digest": "d609501482cd4591c2075eb1e138db5d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28202, "upload_time": "2018-05-01T00:44:20", "upload_time_iso_8601": "2018-05-01T00:44:20.263818Z", "url": "https://files.pythonhosted.org/packages/41/4d/334ef08ebbaf975f0000137ca984ec5ecabfab030ed547d5f716a7967edc/biowardrobe-airflow-analysis-1.0.20180430223646.tar.gz", "yanked": false}], "1.0.20180918221834": [{"comment_text": "", "digests": {"md5": "07d6b710f5067f619e77074eca995bc0", "sha256": "73efa9987478242024e64e7ada261835fa6c3484e1903053702063d74c3661ff"}, "downloads": -1, "filename": "biowardrobe-airflow-analysis-1.0.20180918221834.tar.gz", "has_sig": false, "md5_digest": "07d6b710f5067f619e77074eca995bc0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28575, "upload_time": "2018-09-18T22:18:55", "upload_time_iso_8601": "2018-09-18T22:18:55.410206Z", "url": "https://files.pythonhosted.org/packages/6e/3c/ca673e5a228b78276f5422bbf0377f58afb7115b5474dc503f8260c2edde/biowardrobe-airflow-analysis-1.0.20180918221834.tar.gz", "yanked": false}], "1.0.20181211064034": [{"comment_text": "", "digests": {"md5": "ccec7142bc0c6854b0ad05af48e23a95", "sha256": "ba0ad97e7facaf63238a6278f72ef21242651701b4c63b4b754e5f9b83f9fa14"}, "downloads": -1, "filename": "biowardrobe-airflow-analysis-1.0.20181211064034.tar.gz", "has_sig": false, "md5_digest": "ccec7142bc0c6854b0ad05af48e23a95", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28800, "upload_time": "2018-12-11T06:41:19", "upload_time_iso_8601": "2018-12-11T06:41:19.066354Z", "url": "https://files.pythonhosted.org/packages/99/2b/72a310ac4dc9722c96b18b3de841758a41e9b69c7a1c15306120244f3793/biowardrobe-airflow-analysis-1.0.20181211064034.tar.gz", "yanked": false}], "1.0.20181214162558": [{"comment_text": "", "digests": {"md5": "6ae301d4799f0e95bf0f1b8ac8f3286a", "sha256": "129160bf54616638863cc6fdbbe60e60d054d9d9195b035d74325a9939037684"}, "downloads": -1, "filename": "biowardrobe-airflow-analysis-1.0.20181214162558.tar.gz", "has_sig": false, "md5_digest": "6ae301d4799f0e95bf0f1b8ac8f3286a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29010, "upload_time": "2018-12-14T16:26:11", "upload_time_iso_8601": "2018-12-14T16:26:11.205603Z", "url": "https://files.pythonhosted.org/packages/54/33/8313e235a6189ec895d4de7baaa8f885867b38c1d56ee4efc8aa8a47cd7b/biowardrobe-airflow-analysis-1.0.20181214162558.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "6ae301d4799f0e95bf0f1b8ac8f3286a", "sha256": "129160bf54616638863cc6fdbbe60e60d054d9d9195b035d74325a9939037684"}, "downloads": -1, "filename": "biowardrobe-airflow-analysis-1.0.20181214162558.tar.gz", "has_sig": false, "md5_digest": "6ae301d4799f0e95bf0f1b8ac8f3286a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29010, "upload_time": "2018-12-14T16:26:11", "upload_time_iso_8601": "2018-12-14T16:26:11.205603Z", "url": "https://files.pythonhosted.org/packages/54/33/8313e235a6189ec895d4de7baaa8f885867b38c1d56ee4efc8aa8a47cd7b/biowardrobe-airflow-analysis-1.0.20181214162558.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:37:23 2020"}