{"info": {"author": "Timothy Moore", "author_email": "mtimothy984@gmail.com", "bugtrack_url": null, "classifiers": ["License :: CC0 1.0 Universal (CC0 1.0) Public Domain Dedication", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Utilities"], "description": "# Ignite Simple\n\nThis module provides the necessary functionality for rapidly prototyping\nmachine learning models on conventional datasets.\n\nThis also contains ignite_simple.gen_sweep for sweeping across other\nparameters. See corresponding readme at ignite_simple/gen_sweep/README.md\n\n## Usage\n\nYou must explicitly specify the model, training set, validation set, and the\nloss. Then a preset is applied for how to tune hyperparameters and the amount\nof information to gather and export.\n\nAlthough not required, it is recommended use\n[torchluent](http://github.com/tjstretchalot/torchluent) to simplify the model\ncreation process. This package accepts a single model with tensor outputs or\na tuple of two models where the first is the stripped model (returns only\na single tensor) and the second model has the same underlying parameters but\nreturn values are of the form (tensor, list of tensors), where the list of\ntensors contains relevants snapshots of the data as it was transformed by the\nnetwork.\n\nThis package supports repeating trials with the final selected hyperparameters,\neither by specifying the number of repeats with `trials` or by simply\nsetting `is_continuation` to `True` and calling `train` with the same output\ndirectory, or both.\n\n[API Reference](https://tjstretchalot.github.io/ignite-simple/index.html)\n\n```py\nimport torchluent\nimport ignite_simple\nimport torch\nimport torchvision\nimport logging.config\n\ndef model():\n    return (\n        torchluent.FluentModule((1, 28, 28))\n        .wrap(True)\n        .conv2d(32, 5)\n        .maxpool2d(3)\n        .operator('LeakyReLU')\n        .save_state()\n        .flatten()\n        .dense(64)\n        .operator('Tanh')\n        .save_state()\n        .dense(10)\n        .save_state()\n        .build()\n    )\n\ndef dataset():\n    transform = torchvision.transforms.ToTensor()\n    train_set = torchvision.datasets.MNIST(\n        'datasets/mnist', download=True, transform=transform)\n    val_set = torchvision.datasets.MNIST(\n        'datasets/mnist', train=False, download=True, transform=transform)\n    return train_set, val_set\n\nloss = torch.nn.CrossEntropyLoss\naccuracy_style = 'classification'\n\ndef main():\n    # a reasonable logging.conf is in this repository to get you started.\n    # you won't see any stdout without a logging config!\n    logging.config.fileConfig('logging.conf')\n    ignite_simple.train((__name__, 'model', tuple(), dict()),\n                        (__name__, 'dataset', tuple(), dict()),\n                        (__name__, 'loss', tuple(), dict()),\n                        folder='out', hyperparameters='fast',\n                        analysis='images', allow_later_analysis_up_to='video',\n                        accuracy_style=accuracy_style,\n                        trials=1, is_continuation=False,\n                        history_folder='history', cores='all')\n\nif __name__ == '__main__':\n    main()\n```\n\nThis involves some boilerplate, especially when you want to include optionally\nreanalyzing under different settings and configuring the output folder via\ncommand line arguments. The `ignite_simple.helper` module does this for you,\nreducing the amount of repeated code and allowing one to train models quickly\nand robustly. In the previous example, everything after `'accuracy_style'` can\nbe replaced with\n\n```py\nif __name__ == '__main__':\n    ignite_simple.helper.handle(__name__)\n```\n\nwhich will result in the following command-line arguments:\n\n```text\nusage: helper.py [-h] [--folder FOLDER] [--hparams HPARAMS]\n                 [--analysis ANALYSIS] [--analysis_up_to ANALYSIS_UP_TO]\n                 [--trials TRIALS] [--not_continuation] [--cores CORES]\n                 [--reanalyze] [--module MODULE] [--loggercfg LOGGERCFG]\n```\n\nUse `python -m ignite_simple.helper --help` and the module documentation for\ndetails.\n\n## Continuations and trials\n\nIn the above example, by changing `is_continuation` to `True`, the file may be\ninvoked multiple times. With `is_continuation=True`, the hyperparameters will\nbe reused from the first run and the model and statistics will be saved\nalongside (not overwriting) the existing ones. Furthermore, additional plots\n(averaged accuracy, averaged loss, etc) will be available. With\n`is_continuation=False` the output folder will be archived and moved into\nthe history folder with the current timestamp as its name prior to starting\nthe run.\n\nNote that trials is treated as the *minimum* number of trials to perform.\nThis will attempt to use all available cores (i.e., the number specified in\ncores), which may mean multiple trials can be run in parallel without any\nsignificant difference in runtime. This can be suppressed with the parameter\n`trials_strict=True`.\n\n## Validation sets\n\nFor automatic dataset splitting into training and validiation, one can use\n`ignite_simple.utils.split` as follows:\n\n```py\nimport ignite_simple.utils\nimport torch.utils.data as data\n\nfull: data.Dataset  # dataset to split\nval_perc: float = 0.1  # perc in the validation set\n\n\ntrain_set, val_set = ignite_simple.utils.split(\n    full, val_perc, filen='mydataset/train_val_split')\n```\n\nThe split is random and stored in the given file (extensionless is recommended,\nin which the appropriate extension will be added). If the file already exists,\nthis returns the split stored in the specified file. This makes it easier to\nverify the training and validation accuracy after the fact and simplifies\ncomparisons of models on the same dataset.\n\n## Accuracy style\n\nValid values are `classification`, `multiclass`, and `inv-loss`. Classification\nis for MNIST-style labels (labels are one-hot and the output of the network\nis a one-hot encoding of the label). Multi-class is for when the labels are\none-hot encoded class labels extended to potentially multiple ones. `inv-loss`\nuses 1/(loss+1) as the performance metric instead of accuracy.\n\nNote: both classification and multiclass support more an arbitrary number\nof classes of images, but classification says that each image has exactly\none class and multiclass says that each image may have more than one class.\nMulticlass uses a >=0.5 thresholding on the output, classification uses\nargmax. This does not effect training or model selection, only output.\n\nIn both cases, the output of the network should be (batch, num labels)\nand the targets should be (batch, num labels).\n\n## Automatic hyperparameter tuning\n\nValid presets are `fastest`, `fast`, `slow`, and `slowest`.\n\nThe learning rate and batch size are automatically tuned since they can\ndramatically effect model performance. The methodology is inspired by\n[Cyclical Learning Rates for Training Neural Networks, 2017](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7926641).\n\nFor tuning the learning rate, a reasonable cycle size is used and the process\nis analagous to that described by the paper.\n\nThe batch size is found in a similar way - vary the batch size upward linearly\nover a few epochs. The range of batch sizes where the accuracy increased is\nfound, and then batch sizes are randomly drawn from that range and tested for\na few epochs. The batch size with the highest accuracy at the end of the short\ntest is used.\n\nThe hyperparameter presets correspond roughly to:\n    - How many trials to average tests over (i.e., for learning rate it can be\n    beneficial to find the range via an average of several runs rather than the\n    very noisy output of a single run)\n    - How many batch sizes are attempted within the reasonable range\n    - How many times to repeat the process for checking for batch size and\n    learning rate interactions.\n\nIt is intended that the preset 'fast' is used while making rapid adjustments\nand then 'slow' used occassionally as a sanity check. Then 'slowest' can be\nused if you don't require any additional features and want to use this package\nfor a final model.\n\n## Automatic analysis\n\nValid presets are `none`, `text`, `images`, `images-min`, `animations-draft`,\n`animations`, `video-draft`, and `video`.  See `ignite_simple.analarams` for\ndetails.\n\nIf unsure, choose `images-min` and then upgrade to `images` or `video` for\nfinal analysis or for additional information as necessary.\n\nThis package is capable of producing some explanation about how the model was\ntrained and some information about it's solution. The analysis includes:\n\n- An explanation of the learning rules\n- All relevant hyperparameters for the network and brief explanations of each\n- Where relevant, how hyperparameters were selected\n- Network representation in 3d principal-component-space\n\nThe analysis can be provided in text form, all the previous\nplus image references, all the previous plus animation references, or all\nthe previous plus a video guide. The `-draft` settings produce lower-quality\n(FPS and resolution) versions that are somewhat faster to generate.\n\nAnalysis can be performed after-the-fact assuming that sufficient data was\ncollected (which is specified in the `allow_later_analysis_up_to` parameter).\nThe following snippet performs video analysis on the first example without\nrepeating training, assuming its in the same file:\n\n```py\ndef reanalyze():\n    ignite_simple.analyze(\n        (__name__, 'dataset', tuple(), dict()),\n        (__name__, 'loss', tuple(), dict()),\n        folder='out',\n        settings='video',\n        accuracy_style='classification',\n        cores='all')\n```\n\nThis can be done automatically with the `--reanalyze` option in the helper\nmodule.\n\nNote that reanalysis does not reproduce unless it believes the result would be\ndifferent from that which exists on the file system. The analysis output is\nin the `analysis` subdirectory of the output folder, and then in a folder\nindexed by the number of models trained in the current hyperparameter settings.\nTo ensure you get the most up-to-date analysis you can delete the analysis\nfolder before calling analyze.\n\n## Implementation details\n\nThis package trains with SGD without momentum on a linear cyclical learning\nrate rule for a fixed number of epochs. The batch size is fixed throughout\ntraining, and the validation dataset is not used during hyperparameter\nselection nor model training, however it is measured and reported for analysis.\nIn the output folder, `analysis/html/index.html` is produced which explains the\ndetails of training to a reasonable degree. Further details can be found by\nchecking the `ignite_simple.tuner` and `ignite_simple.model_manager`\ndocumentation and source code.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/tjstretchalot/ignite-simple", "keywords": "torch pytorch models machine-learning learning-rate batch-size hyperparameters", "license": "CC0", "maintainer": "", "maintainer_email": "", "name": "ignite-simple", "package_url": "https://pypi.org/project/ignite-simple/", "platform": "", "project_url": "https://pypi.org/project/ignite-simple/", "project_urls": {"Homepage": "https://github.com/tjstretchalot/ignite-simple"}, "release_url": "https://pypi.org/project/ignite-simple/0.0.11/", "requires_dist": ["pca3dvis", "torch", "numpy", "matplotlib", "scipy", "pyzmq", "beautifulsoup4", "pytorch-ignite", "psutil", "sortedcontainers"], "requires_python": ">=3.6", "summary": "Easily train pytorch models with automatic LR and BS tuning", "version": "0.0.11", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Ignite Simple</h1>\n<p>This module provides the necessary functionality for rapidly prototyping\nmachine learning models on conventional datasets.</p>\n<p>This also contains ignite_simple.gen_sweep for sweeping across other\nparameters. See corresponding readme at ignite_simple/gen_sweep/README.md</p>\n<h2>Usage</h2>\n<p>You must explicitly specify the model, training set, validation set, and the\nloss. Then a preset is applied for how to tune hyperparameters and the amount\nof information to gather and export.</p>\n<p>Although not required, it is recommended use\n<a href=\"http://github.com/tjstretchalot/torchluent\" rel=\"nofollow\">torchluent</a> to simplify the model\ncreation process. This package accepts a single model with tensor outputs or\na tuple of two models where the first is the stripped model (returns only\na single tensor) and the second model has the same underlying parameters but\nreturn values are of the form (tensor, list of tensors), where the list of\ntensors contains relevants snapshots of the data as it was transformed by the\nnetwork.</p>\n<p>This package supports repeating trials with the final selected hyperparameters,\neither by specifying the number of repeats with <code>trials</code> or by simply\nsetting <code>is_continuation</code> to <code>True</code> and calling <code>train</code> with the same output\ndirectory, or both.</p>\n<p><a href=\"https://tjstretchalot.github.io/ignite-simple/index.html\" rel=\"nofollow\">API Reference</a></p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torchluent</span>\n<span class=\"kn\">import</span> <span class=\"nn\">ignite_simple</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging.config</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">():</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span>\n        <span class=\"n\">torchluent</span><span class=\"o\">.</span><span class=\"n\">FluentModule</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">))</span>\n        <span class=\"o\">.</span><span class=\"n\">wrap</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">conv2d</span><span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">maxpool2d</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">operator</span><span class=\"p\">(</span><span class=\"s1\">'LeakyReLU'</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">save_state</span><span class=\"p\">()</span>\n        <span class=\"o\">.</span><span class=\"n\">flatten</span><span class=\"p\">()</span>\n        <span class=\"o\">.</span><span class=\"n\">dense</span><span class=\"p\">(</span><span class=\"mi\">64</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">operator</span><span class=\"p\">(</span><span class=\"s1\">'Tanh'</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">save_state</span><span class=\"p\">()</span>\n        <span class=\"o\">.</span><span class=\"n\">dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">save_state</span><span class=\"p\">()</span>\n        <span class=\"o\">.</span><span class=\"n\">build</span><span class=\"p\">()</span>\n    <span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">dataset</span><span class=\"p\">():</span>\n    <span class=\"n\">transform</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">()</span>\n    <span class=\"n\">train_set</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">MNIST</span><span class=\"p\">(</span>\n        <span class=\"s1\">'datasets/mnist'</span><span class=\"p\">,</span> <span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"n\">transform</span><span class=\"p\">)</span>\n    <span class=\"n\">val_set</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">MNIST</span><span class=\"p\">(</span>\n        <span class=\"s1\">'datasets/mnist'</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"n\">transform</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">train_set</span><span class=\"p\">,</span> <span class=\"n\">val_set</span>\n\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">CrossEntropyLoss</span>\n<span class=\"n\">accuracy_style</span> <span class=\"o\">=</span> <span class=\"s1\">'classification'</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">():</span>\n    <span class=\"c1\"># a reasonable logging.conf is in this repository to get you started.</span>\n    <span class=\"c1\"># you won't see any stdout without a logging config!</span>\n    <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">fileConfig</span><span class=\"p\">(</span><span class=\"s1\">'logging.conf'</span><span class=\"p\">)</span>\n    <span class=\"n\">ignite_simple</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">((</span><span class=\"vm\">__name__</span><span class=\"p\">,</span> <span class=\"s1\">'model'</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">(),</span> <span class=\"nb\">dict</span><span class=\"p\">()),</span>\n                        <span class=\"p\">(</span><span class=\"vm\">__name__</span><span class=\"p\">,</span> <span class=\"s1\">'dataset'</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">(),</span> <span class=\"nb\">dict</span><span class=\"p\">()),</span>\n                        <span class=\"p\">(</span><span class=\"vm\">__name__</span><span class=\"p\">,</span> <span class=\"s1\">'loss'</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">(),</span> <span class=\"nb\">dict</span><span class=\"p\">()),</span>\n                        <span class=\"n\">folder</span><span class=\"o\">=</span><span class=\"s1\">'out'</span><span class=\"p\">,</span> <span class=\"n\">hyperparameters</span><span class=\"o\">=</span><span class=\"s1\">'fast'</span><span class=\"p\">,</span>\n                        <span class=\"n\">analysis</span><span class=\"o\">=</span><span class=\"s1\">'images'</span><span class=\"p\">,</span> <span class=\"n\">allow_later_analysis_up_to</span><span class=\"o\">=</span><span class=\"s1\">'video'</span><span class=\"p\">,</span>\n                        <span class=\"n\">accuracy_style</span><span class=\"o\">=</span><span class=\"n\">accuracy_style</span><span class=\"p\">,</span>\n                        <span class=\"n\">trials</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">is_continuation</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n                        <span class=\"n\">history_folder</span><span class=\"o\">=</span><span class=\"s1\">'history'</span><span class=\"p\">,</span> <span class=\"n\">cores</span><span class=\"o\">=</span><span class=\"s1\">'all'</span><span class=\"p\">)</span>\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">'__main__'</span><span class=\"p\">:</span>\n    <span class=\"n\">main</span><span class=\"p\">()</span>\n</pre>\n<p>This involves some boilerplate, especially when you want to include optionally\nreanalyzing under different settings and configuring the output folder via\ncommand line arguments. The <code>ignite_simple.helper</code> module does this for you,\nreducing the amount of repeated code and allowing one to train models quickly\nand robustly. In the previous example, everything after <code>'accuracy_style'</code> can\nbe replaced with</p>\n<pre><span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">'__main__'</span><span class=\"p\">:</span>\n    <span class=\"n\">ignite_simple</span><span class=\"o\">.</span><span class=\"n\">helper</span><span class=\"o\">.</span><span class=\"n\">handle</span><span class=\"p\">(</span><span class=\"vm\">__name__</span><span class=\"p\">)</span>\n</pre>\n<p>which will result in the following command-line arguments:</p>\n<pre>usage: helper.py [-h] [--folder FOLDER] [--hparams HPARAMS]\n                 [--analysis ANALYSIS] [--analysis_up_to ANALYSIS_UP_TO]\n                 [--trials TRIALS] [--not_continuation] [--cores CORES]\n                 [--reanalyze] [--module MODULE] [--loggercfg LOGGERCFG]\n</pre>\n<p>Use <code>python -m ignite_simple.helper --help</code> and the module documentation for\ndetails.</p>\n<h2>Continuations and trials</h2>\n<p>In the above example, by changing <code>is_continuation</code> to <code>True</code>, the file may be\ninvoked multiple times. With <code>is_continuation=True</code>, the hyperparameters will\nbe reused from the first run and the model and statistics will be saved\nalongside (not overwriting) the existing ones. Furthermore, additional plots\n(averaged accuracy, averaged loss, etc) will be available. With\n<code>is_continuation=False</code> the output folder will be archived and moved into\nthe history folder with the current timestamp as its name prior to starting\nthe run.</p>\n<p>Note that trials is treated as the <em>minimum</em> number of trials to perform.\nThis will attempt to use all available cores (i.e., the number specified in\ncores), which may mean multiple trials can be run in parallel without any\nsignificant difference in runtime. This can be suppressed with the parameter\n<code>trials_strict=True</code>.</p>\n<h2>Validation sets</h2>\n<p>For automatic dataset splitting into training and validiation, one can use\n<code>ignite_simple.utils.split</code> as follows:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">ignite_simple.utils</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.utils.data</span> <span class=\"k\">as</span> <span class=\"nn\">data</span>\n\n<span class=\"n\">full</span><span class=\"p\">:</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>  <span class=\"c1\"># dataset to split</span>\n<span class=\"n\">val_perc</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>  <span class=\"c1\"># perc in the validation set</span>\n\n\n<span class=\"n\">train_set</span><span class=\"p\">,</span> <span class=\"n\">val_set</span> <span class=\"o\">=</span> <span class=\"n\">ignite_simple</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span>\n    <span class=\"n\">full</span><span class=\"p\">,</span> <span class=\"n\">val_perc</span><span class=\"p\">,</span> <span class=\"n\">filen</span><span class=\"o\">=</span><span class=\"s1\">'mydataset/train_val_split'</span><span class=\"p\">)</span>\n</pre>\n<p>The split is random and stored in the given file (extensionless is recommended,\nin which the appropriate extension will be added). If the file already exists,\nthis returns the split stored in the specified file. This makes it easier to\nverify the training and validation accuracy after the fact and simplifies\ncomparisons of models on the same dataset.</p>\n<h2>Accuracy style</h2>\n<p>Valid values are <code>classification</code>, <code>multiclass</code>, and <code>inv-loss</code>. Classification\nis for MNIST-style labels (labels are one-hot and the output of the network\nis a one-hot encoding of the label). Multi-class is for when the labels are\none-hot encoded class labels extended to potentially multiple ones. <code>inv-loss</code>\nuses 1/(loss+1) as the performance metric instead of accuracy.</p>\n<p>Note: both classification and multiclass support more an arbitrary number\nof classes of images, but classification says that each image has exactly\none class and multiclass says that each image may have more than one class.\nMulticlass uses a &gt;=0.5 thresholding on the output, classification uses\nargmax. This does not effect training or model selection, only output.</p>\n<p>In both cases, the output of the network should be (batch, num labels)\nand the targets should be (batch, num labels).</p>\n<h2>Automatic hyperparameter tuning</h2>\n<p>Valid presets are <code>fastest</code>, <code>fast</code>, <code>slow</code>, and <code>slowest</code>.</p>\n<p>The learning rate and batch size are automatically tuned since they can\ndramatically effect model performance. The methodology is inspired by\n<a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7926641\" rel=\"nofollow\">Cyclical Learning Rates for Training Neural Networks, 2017</a>.</p>\n<p>For tuning the learning rate, a reasonable cycle size is used and the process\nis analagous to that described by the paper.</p>\n<p>The batch size is found in a similar way - vary the batch size upward linearly\nover a few epochs. The range of batch sizes where the accuracy increased is\nfound, and then batch sizes are randomly drawn from that range and tested for\na few epochs. The batch size with the highest accuracy at the end of the short\ntest is used.</p>\n<p>The hyperparameter presets correspond roughly to:\n- How many trials to average tests over (i.e., for learning rate it can be\nbeneficial to find the range via an average of several runs rather than the\nvery noisy output of a single run)\n- How many batch sizes are attempted within the reasonable range\n- How many times to repeat the process for checking for batch size and\nlearning rate interactions.</p>\n<p>It is intended that the preset 'fast' is used while making rapid adjustments\nand then 'slow' used occassionally as a sanity check. Then 'slowest' can be\nused if you don't require any additional features and want to use this package\nfor a final model.</p>\n<h2>Automatic analysis</h2>\n<p>Valid presets are <code>none</code>, <code>text</code>, <code>images</code>, <code>images-min</code>, <code>animations-draft</code>,\n<code>animations</code>, <code>video-draft</code>, and <code>video</code>.  See <code>ignite_simple.analarams</code> for\ndetails.</p>\n<p>If unsure, choose <code>images-min</code> and then upgrade to <code>images</code> or <code>video</code> for\nfinal analysis or for additional information as necessary.</p>\n<p>This package is capable of producing some explanation about how the model was\ntrained and some information about it's solution. The analysis includes:</p>\n<ul>\n<li>An explanation of the learning rules</li>\n<li>All relevant hyperparameters for the network and brief explanations of each</li>\n<li>Where relevant, how hyperparameters were selected</li>\n<li>Network representation in 3d principal-component-space</li>\n</ul>\n<p>The analysis can be provided in text form, all the previous\nplus image references, all the previous plus animation references, or all\nthe previous plus a video guide. The <code>-draft</code> settings produce lower-quality\n(FPS and resolution) versions that are somewhat faster to generate.</p>\n<p>Analysis can be performed after-the-fact assuming that sufficient data was\ncollected (which is specified in the <code>allow_later_analysis_up_to</code> parameter).\nThe following snippet performs video analysis on the first example without\nrepeating training, assuming its in the same file:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">reanalyze</span><span class=\"p\">():</span>\n    <span class=\"n\">ignite_simple</span><span class=\"o\">.</span><span class=\"n\">analyze</span><span class=\"p\">(</span>\n        <span class=\"p\">(</span><span class=\"vm\">__name__</span><span class=\"p\">,</span> <span class=\"s1\">'dataset'</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">(),</span> <span class=\"nb\">dict</span><span class=\"p\">()),</span>\n        <span class=\"p\">(</span><span class=\"vm\">__name__</span><span class=\"p\">,</span> <span class=\"s1\">'loss'</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">(),</span> <span class=\"nb\">dict</span><span class=\"p\">()),</span>\n        <span class=\"n\">folder</span><span class=\"o\">=</span><span class=\"s1\">'out'</span><span class=\"p\">,</span>\n        <span class=\"n\">settings</span><span class=\"o\">=</span><span class=\"s1\">'video'</span><span class=\"p\">,</span>\n        <span class=\"n\">accuracy_style</span><span class=\"o\">=</span><span class=\"s1\">'classification'</span><span class=\"p\">,</span>\n        <span class=\"n\">cores</span><span class=\"o\">=</span><span class=\"s1\">'all'</span><span class=\"p\">)</span>\n</pre>\n<p>This can be done automatically with the <code>--reanalyze</code> option in the helper\nmodule.</p>\n<p>Note that reanalysis does not reproduce unless it believes the result would be\ndifferent from that which exists on the file system. The analysis output is\nin the <code>analysis</code> subdirectory of the output folder, and then in a folder\nindexed by the number of models trained in the current hyperparameter settings.\nTo ensure you get the most up-to-date analysis you can delete the analysis\nfolder before calling analyze.</p>\n<h2>Implementation details</h2>\n<p>This package trains with SGD without momentum on a linear cyclical learning\nrate rule for a fixed number of epochs. The batch size is fixed throughout\ntraining, and the validation dataset is not used during hyperparameter\nselection nor model training, however it is measured and reported for analysis.\nIn the output folder, <code>analysis/html/index.html</code> is produced which explains the\ndetails of training to a reasonable degree. Further details can be found by\nchecking the <code>ignite_simple.tuner</code> and <code>ignite_simple.model_manager</code>\ndocumentation and source code.</p>\n\n          </div>"}, "last_serial": 5826043, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "b6fa76e796e6302162593d1fcb373870", "sha256": "df0ac9b6de20d923746cb73b8a490888b21407ded4a26fb80642b19e62d2cbdc"}, "downloads": -1, "filename": "ignite_simple-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "b6fa76e796e6302162593d1fcb373870", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 47033, "upload_time": "2019-08-19T15:14:37", "upload_time_iso_8601": "2019-08-19T15:14:37.744773Z", "url": "https://files.pythonhosted.org/packages/58/4b/39c3f47663a4130e48e084955c1ae5bd6cf267a6e72941adf45407062078/ignite_simple-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f1662b35f774025e3376c867c2bae856", "sha256": "5327cb684de63dc7c517480a3358a244933f11b0734f378d9c667fc3cf16798b"}, "downloads": -1, "filename": "ignite_simple-0.0.1.tar.gz", "has_sig": false, "md5_digest": "f1662b35f774025e3376c867c2bae856", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 42858, "upload_time": "2019-08-19T15:14:40", "upload_time_iso_8601": "2019-08-19T15:14:40.387769Z", "url": "https://files.pythonhosted.org/packages/3f/c3/69796ae0c6b0d4be7d6dc9097f6825aa1bb3c76e1e93f40314f6c5004183/ignite_simple-0.0.1.tar.gz", "yanked": false}], "0.0.10": [{"comment_text": "", "digests": {"md5": "e99f26965af78ec2e241bf01ec416bcc", "sha256": "2fd958a3bd0dd5fdd142fb69f96cb76a369f3111a53918b828bd5b853a9f33c3"}, "downloads": -1, "filename": "ignite_simple-0.0.10-py3-none-any.whl", "has_sig": false, "md5_digest": "e99f26965af78ec2e241bf01ec416bcc", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 63172, "upload_time": "2019-09-13T14:38:19", "upload_time_iso_8601": "2019-09-13T14:38:19.726250Z", "url": "https://files.pythonhosted.org/packages/0c/22/68dc5b0d8fc22455ef57e079207597f306ed5759257aa286d62c6ea324f2/ignite_simple-0.0.10-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e2dcc5c108a723fb8ae473891613ca37", "sha256": "83a554e07929d5e1d299d96b8f5577cf63950f02a0da67543ddc5fb2c74ba527"}, "downloads": -1, "filename": "ignite_simple-0.0.10.tar.gz", "has_sig": false, "md5_digest": "e2dcc5c108a723fb8ae473891613ca37", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 57595, "upload_time": "2019-09-13T14:38:21", "upload_time_iso_8601": "2019-09-13T14:38:21.540706Z", "url": "https://files.pythonhosted.org/packages/46/2c/5f1adabd69b04fe049978ccdf6490f889cd1a394bae7505abc8375a04ccd/ignite_simple-0.0.10.tar.gz", "yanked": false}], "0.0.11": [{"comment_text": "", "digests": {"md5": "d4e9633a396abe78f4062d04ae053bfe", "sha256": "ba6529b8b46697a5053ff44fcf1a70482c9ea2a39a048a8a765c9dbfc2f60a54"}, "downloads": -1, "filename": "ignite_simple-0.0.11-py3-none-any.whl", "has_sig": false, "md5_digest": "d4e9633a396abe78f4062d04ae053bfe", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 63173, "upload_time": "2019-09-13T14:40:58", "upload_time_iso_8601": "2019-09-13T14:40:58.651711Z", "url": "https://files.pythonhosted.org/packages/53/51/8006cdba506743b01ddec4a32c253d25d6b5c7d9296f2ee6818e6e0b7469/ignite_simple-0.0.11-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f1a188b0515a952bef0bf8b4c075e52f", "sha256": "d579254666dbe0443d2a4452d4ed8b20fe10961d694d631f8be1fa13d4855318"}, "downloads": -1, "filename": "ignite_simple-0.0.11.tar.gz", "has_sig": false, "md5_digest": "f1a188b0515a952bef0bf8b4c075e52f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 57591, "upload_time": "2019-09-13T14:41:00", "upload_time_iso_8601": "2019-09-13T14:41:00.516174Z", "url": "https://files.pythonhosted.org/packages/85/7d/920f373b20936a67b8e1f64c6e773a87d83ce5b563c8779c664915a69044/ignite_simple-0.0.11.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "871eaa49296ffa103cea7a52335cf8ad", "sha256": "0a50e5e34c833413c20f428da98c0a10c7cf47bc7daf6afea46fa8593f2158fc"}, "downloads": -1, "filename": "ignite_simple-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "871eaa49296ffa103cea7a52335cf8ad", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 47034, "upload_time": "2019-08-19T18:42:25", "upload_time_iso_8601": "2019-08-19T18:42:25.056425Z", "url": "https://files.pythonhosted.org/packages/32/ed/f4e3ca75171128efc653ae04d732de9cba77a5ba9a41afc0ee8d89bf6a3b/ignite_simple-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fdf2f7c0f00b40ad507d6f1943b15063", "sha256": "67bb3b7fa641f2dd1f368a1c29f7e77ff2e9e2d2fbb478256fe83e249821029a"}, "downloads": -1, "filename": "ignite_simple-0.0.2.tar.gz", "has_sig": false, "md5_digest": "fdf2f7c0f00b40ad507d6f1943b15063", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 42873, "upload_time": "2019-08-19T18:42:26", "upload_time_iso_8601": "2019-08-19T18:42:26.390376Z", "url": "https://files.pythonhosted.org/packages/5f/c4/b41e34863b43effea46b2a3b4ea19fd4f866e86405725c93db65adb9c69b/ignite_simple-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "dcd6fa6f6f6dec40957c8d070c0b1ae4", "sha256": "1a4427929beb2d9facfeb199d681fff7fd381455f3841ce80f7b7ba9f9296104"}, "downloads": -1, "filename": "ignite_simple-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "dcd6fa6f6f6dec40957c8d070c0b1ae4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 47017, "upload_time": "2019-08-21T23:21:54", "upload_time_iso_8601": "2019-08-21T23:21:54.656814Z", "url": "https://files.pythonhosted.org/packages/ea/5f/fcee1730895ab99acfc5f04b40561c20e3948e9b2ea54dea8b03abdd6bcc/ignite_simple-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5a936cc765b398f7b8a4ba3a4297689f", "sha256": "11c1822fac65c9bc7440312d6fdbd613083ecf228216d6803a3c62e868a17489"}, "downloads": -1, "filename": "ignite_simple-0.0.3.tar.gz", "has_sig": false, "md5_digest": "5a936cc765b398f7b8a4ba3a4297689f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 42862, "upload_time": "2019-08-21T23:21:56", "upload_time_iso_8601": "2019-08-21T23:21:56.494784Z", "url": "https://files.pythonhosted.org/packages/b3/56/f6d1c2b8ecc3aa1e2c34219867724cb4aafd05aba0681fd4c6618b9b720d/ignite_simple-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "936625d19a7ab402cf5d24ef5db6591e", "sha256": "3ca3155dec4ebfa3c60942ca0fd26038e96c551b4a0bfd3b6d3cd41b99d6c61b"}, "downloads": -1, "filename": "ignite_simple-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "936625d19a7ab402cf5d24ef5db6591e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 47035, "upload_time": "2019-08-22T14:43:14", "upload_time_iso_8601": "2019-08-22T14:43:14.445315Z", "url": "https://files.pythonhosted.org/packages/a2/45/000c95d7186805f930fd885901621cd4a4eaf0ebf4ea9d227227b4ff6e3f/ignite_simple-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "754a044b2d5cccf3e8e35719bd5ab9a0", "sha256": "aea5c4a8f4079344641526b43d2723976bbd8d5c21e4ad25ace3ee0dfffd9b8d"}, "downloads": -1, "filename": "ignite_simple-0.0.4.tar.gz", "has_sig": false, "md5_digest": "754a044b2d5cccf3e8e35719bd5ab9a0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 42885, "upload_time": "2019-08-22T14:43:15", "upload_time_iso_8601": "2019-08-22T14:43:15.996504Z", "url": "https://files.pythonhosted.org/packages/ed/ed/82df3cd7b05f7bd9557648c5bcbbd63236799de1e60d1ee9247ed05367f6/ignite_simple-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "5f6d09423ed914b34afa113ec63ef196", "sha256": "4f6916c8b8d0d7a79095d740f3f7b49c7ad80b32674871d02d95e211d078b627"}, "downloads": -1, "filename": "ignite_simple-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "5f6d09423ed914b34afa113ec63ef196", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 47052, "upload_time": "2019-08-22T14:56:45", "upload_time_iso_8601": "2019-08-22T14:56:45.240513Z", "url": "https://files.pythonhosted.org/packages/24/ee/71eb4cb40ae50f29f8f01d26d21b9ed5c635a383f91031038cc313c5a242/ignite_simple-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "781764c052a1a4af7ebdb342e4dc5f61", "sha256": "e60ec32882de273a60d171ae00faccab5a5752fec9c86823cece90d8180d215d"}, "downloads": -1, "filename": "ignite_simple-0.0.5.tar.gz", "has_sig": false, "md5_digest": "781764c052a1a4af7ebdb342e4dc5f61", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 42901, "upload_time": "2019-08-22T14:56:46", "upload_time_iso_8601": "2019-08-22T14:56:46.837149Z", "url": "https://files.pythonhosted.org/packages/ab/14/a0b1be1ab7f784ab05bf97d1e3f218271331f86f461459bc49fffa23c9df/ignite_simple-0.0.5.tar.gz", "yanked": false}], "0.0.7": [{"comment_text": "", "digests": {"md5": "a6b2cbf528b6dfe335646fb42c441370", "sha256": "4b7efb227ed2a00e89767ccf8b626202e8f614d73e4f0f84a4bd5cb5e8d17f26"}, "downloads": -1, "filename": "ignite_simple-0.0.7-py3-none-any.whl", "has_sig": false, "md5_digest": "a6b2cbf528b6dfe335646fb42c441370", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 60932, "upload_time": "2019-09-11T20:24:14", "upload_time_iso_8601": "2019-09-11T20:24:14.987515Z", "url": "https://files.pythonhosted.org/packages/de/05/22cfc336c26c5b01fcb5515f4081e158aa2ddb994d394c710f7764a8b750/ignite_simple-0.0.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1daee59792588e81f72c354f2253c05f", "sha256": "3ebcc647afc67b208ea640d1c5e11ee9b3051f9990e079f23d4a95132ac34044"}, "downloads": -1, "filename": "ignite_simple-0.0.7.tar.gz", "has_sig": false, "md5_digest": "1daee59792588e81f72c354f2253c05f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 55331, "upload_time": "2019-09-11T20:24:16", "upload_time_iso_8601": "2019-09-11T20:24:16.797748Z", "url": "https://files.pythonhosted.org/packages/70/ac/283fdc20610138c66c7b2d70d40dc5923d91504cdff606c272dce75e472f/ignite_simple-0.0.7.tar.gz", "yanked": false}], "0.0.8": [{"comment_text": "", "digests": {"md5": "d9ea22caeb34b5e4e78ee3cd85e446e1", "sha256": "25f117a24616461bda76212debb937df3b6d84f4148f48a9910d5fa237e6e5cd"}, "downloads": -1, "filename": "ignite_simple-0.0.8-py3-none-any.whl", "has_sig": false, "md5_digest": "d9ea22caeb34b5e4e78ee3cd85e446e1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 61181, "upload_time": "2019-09-11T21:15:31", "upload_time_iso_8601": "2019-09-11T21:15:31.869635Z", "url": "https://files.pythonhosted.org/packages/8f/3a/9f1e907e5227092ebd14b37d10b5af0e6e1eff0552aa4a2c44d54b2ea042/ignite_simple-0.0.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ed238870510649b3bdd6e1139461390d", "sha256": "ece8b45aacd5143504aa6bd8e805ad86fe9fe7adb0068681ff76ff7ed7bb8336"}, "downloads": -1, "filename": "ignite_simple-0.0.8.tar.gz", "has_sig": false, "md5_digest": "ed238870510649b3bdd6e1139461390d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 55572, "upload_time": "2019-09-11T21:15:34", "upload_time_iso_8601": "2019-09-11T21:15:34.183028Z", "url": "https://files.pythonhosted.org/packages/93/06/6c108f135a7e179eaa302e38d22d2d26e204f8b6bb214a2ecd9e74cbb212/ignite_simple-0.0.8.tar.gz", "yanked": false}], "0.0.9": [{"comment_text": "", "digests": {"md5": "e493af241d3b9847f00583748f900b88", "sha256": "fce36b1bdb77baf2e392902f3def4cc493c1e290627347a2107c74751f389db5"}, "downloads": -1, "filename": "ignite_simple-0.0.9-py3-none-any.whl", "has_sig": false, "md5_digest": "e493af241d3b9847f00583748f900b88", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 63084, "upload_time": "2019-09-12T21:43:59", "upload_time_iso_8601": "2019-09-12T21:43:59.406237Z", "url": "https://files.pythonhosted.org/packages/bd/e4/a006a3f664fcf0db1b556d7560fced0642772c51d23f99d207ce9fd35edd/ignite_simple-0.0.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "56e0f66199a9e75ee01a6134e1ebdaf6", "sha256": "7224853c939e6fd10f89a18b841244297ab6c24df8af1b4729a16e9192bc80ce"}, "downloads": -1, "filename": "ignite_simple-0.0.9.tar.gz", "has_sig": false, "md5_digest": "56e0f66199a9e75ee01a6134e1ebdaf6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 57507, "upload_time": "2019-09-12T21:44:00", "upload_time_iso_8601": "2019-09-12T21:44:00.867741Z", "url": "https://files.pythonhosted.org/packages/f9/65/6b2720a5721bfa8121dca96639ef8e0baea37e30295e48a7930cee3ce3c5/ignite_simple-0.0.9.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d4e9633a396abe78f4062d04ae053bfe", "sha256": "ba6529b8b46697a5053ff44fcf1a70482c9ea2a39a048a8a765c9dbfc2f60a54"}, "downloads": -1, "filename": "ignite_simple-0.0.11-py3-none-any.whl", "has_sig": false, "md5_digest": "d4e9633a396abe78f4062d04ae053bfe", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 63173, "upload_time": "2019-09-13T14:40:58", "upload_time_iso_8601": "2019-09-13T14:40:58.651711Z", "url": "https://files.pythonhosted.org/packages/53/51/8006cdba506743b01ddec4a32c253d25d6b5c7d9296f2ee6818e6e0b7469/ignite_simple-0.0.11-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f1a188b0515a952bef0bf8b4c075e52f", "sha256": "d579254666dbe0443d2a4452d4ed8b20fe10961d694d631f8be1fa13d4855318"}, "downloads": -1, "filename": "ignite_simple-0.0.11.tar.gz", "has_sig": false, "md5_digest": "f1a188b0515a952bef0bf8b4c075e52f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 57591, "upload_time": "2019-09-13T14:41:00", "upload_time_iso_8601": "2019-09-13T14:41:00.516174Z", "url": "https://files.pythonhosted.org/packages/85/7d/920f373b20936a67b8e1f64c6e773a87d83ce5b563c8779c664915a69044/ignite_simple-0.0.11.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:48:42 2020"}