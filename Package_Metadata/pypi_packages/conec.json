{"info": {"author": "cod3licious", "author_email": "cod3licious@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "Context Encoders (ConEc)\n========================\n\nWith this code you can train and evaluate Context Encoders (ConEc), an extension of word2vec, which can learn word embeddings from large corpora and create out-of-vocabulary embeddings on the spot as well as distinguish between multiple meanings of words based on their local contexts.\nFor further details on the model and experiments please refer to the paper_  - and of course if any of this code was helpful for your research, please consider citing it: ::\n    @inproceedings{horn2017conecRepL4NLP,\n      author       = {Horn, Franziska},\n      title        = {Context encoders as a simple but powerful extension of word2vec},\n      booktitle    = {Proceedings of the 2nd Workshop on Representation Learning for NLP},\n      year         = {2017},\n      organization = {Association for Computational Linguistics},\n      pages        = {10--14}\n    }\n\n.. _paper: https://arxiv.org/abs/1706.02496\n\nThe code is intended for research purposes. It was programmed for Python 2.7, but should theoretically also run on newer Python 3 versions - no guarantees on this though (open an issue if you find a bug, please)!\n\ninstallation\n------------\nYou either download the code from here and include the conec folder in your ``$PYTHONPATH`` or install (the library components only) via pip:\n\n    ``$ pip install conec``\n\nconec library components\n------------------------\n\ndependencies: numpy, scipy\n\n- ``word2vec.py``: code to train a standard word2vec model, adapted from the corresponding gensim_ implementation.\n- ``context2vec.py``: code to build a sparse context matrix from a large collection of texts; this context matrix can then be multiplied with the corresponding word2vec embeddings to give the context encoder embeddings:\n\n.. code-block:: python\n\n    # get the text for training\n    sentences = Text8Corpus('data/text8')\n    # train the word2vec model\n    w2v_model = word2vec.Word2Vec(sentences, mtype='cbow', hs=0, neg=13, embed_dim=200, seed=3)\n    # get the global context matrix for the text\n    context_model = context2vec.ContextModel(sentences, min_count=w2v_model.min_count, window=w2v_model.window, wordlist=w2v_model.index2word)\n    context_mat = context_model.get_context_matrix(fill_diag=False, norm='max')\n    # multiply the context matrix with the (length normalized) word2vec embeddings\n    # to get the context encoder (ConEc) embeddings\n    conec_emb = context_mat.dot(w2v_model.syn0norm)\n    # renormalize so the word embeddings have unit length again\n    conec_emb = conec_emb / np.array([np.linalg.norm(conec_emb, axis=1)]).T\n\n.. _gensim: https://radimrehurek.com/gensim/\n\n\nexamples\n--------\nadditional dependencies: sklearn, unidecode\n\n``test_analogy.py`` and ``test_ner.py`` contain the code to replicate the analogy and named entity recognition (NER) experiments discussed in the aforementioned paper.\n\nTo run the analogy experiment, it is assumed that the `text8 corpus`_ or `1-billion corpus`_ as well as the `analogy questions`_ are in a data directory.\n\nTo run the named entity recognition experiment, it is assumed that the corresponding `training and test files`_ are located in the data/conll2003 directory.\n\n.. _`text8 corpus`: http://mattmahoney.net/dc/text8.zip\n.. _`1-billion corpus`: http://code.google.com/p/1-billion-word-language-modeling-benchmark/\n.. _`analogy questions`: https://code.google.com/archive/p/word2vec/\n.. _`training and test files`: http://www.cnts.ua.ac.be/conll2003/ner/\n\n\nIf you have any questions please don't hesitate to send me an `email <mailto:cod3licious@gmail.com>`_ and of course if you should find any bugs or want to contribute other improvements, pull requests are very welcome!\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/cod3licious/conec", "keywords": "context encoders word embeddings word2vec", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "conec", "package_url": "https://pypi.org/project/conec/", "platform": "", "project_url": "https://pypi.org/project/conec/", "project_urls": {"Homepage": "https://github.com/cod3licious/conec"}, "release_url": "https://pypi.org/project/conec/1.0.1/", "requires_dist": ["scipy", "future", "numpy"], "requires_python": "", "summary": "Context Encoders (ConEc) as an extension of word2vec", "version": "1.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            Context Encoders (ConEc)<br>========================<br><br>With this code you can train and evaluate Context Encoders (ConEc), an extension of word2vec, which can learn word embeddings from large corpora and create out-of-vocabulary embeddings on the spot as well as distinguish between multiple meanings of words based on their local contexts.<br>For further details on the model and experiments please refer to the paper_  - and of course if any of this code was helpful for your research, please consider citing it: ::<br>    @inproceedings{horn2017conecRepL4NLP,<br>      author       = {Horn, Franziska},<br>      title        = {Context encoders as a simple but powerful extension of word2vec},<br>      booktitle    = {Proceedings of the 2nd Workshop on Representation Learning for NLP},<br>      year         = {2017},<br>      organization = {Association for Computational Linguistics},<br>      pages        = {10--14}<br>    }<br><br>.. _paper: https://arxiv.org/abs/1706.02496<br><br>The code is intended for research purposes. It was programmed for Python 2.7, but should theoretically also run on newer Python 3 versions - no guarantees on this though (open an issue if you find a bug, please)!<br><br>installation<br>------------<br>You either download the code from here and include the conec folder in your ``$PYTHONPATH`` or install (the library components only) via pip:<br><br>    ``$ pip install conec``<br><br>conec library components<br>------------------------<br><br>dependencies: numpy, scipy<br><br>- ``word2vec.py``: code to train a standard word2vec model, adapted from the corresponding gensim_ implementation.<br>- ``context2vec.py``: code to build a sparse context matrix from a large collection of texts; this context matrix can then be multiplied with the corresponding word2vec embeddings to give the context encoder embeddings:<br><br>.. code-block:: python<br><br>    # get the text for training<br>    sentences = Text8Corpus('data/text8')<br>    # train the word2vec model<br>    w2v_model = word2vec.Word2Vec(sentences, mtype='cbow', hs=0, neg=13, embed_dim=200, seed=3)<br>    # get the global context matrix for the text<br>    context_model = context2vec.ContextModel(sentences, min_count=w2v_model.min_count, window=w2v_model.window, wordlist=w2v_model.index2word)<br>    context_mat = context_model.get_context_matrix(fill_diag=False, norm='max')<br>    # multiply the context matrix with the (length normalized) word2vec embeddings<br>    # to get the context encoder (ConEc) embeddings<br>    conec_emb = context_mat.dot(w2v_model.syn0norm)<br>    # renormalize so the word embeddings have unit length again<br>    conec_emb = conec_emb / np.array([np.linalg.norm(conec_emb, axis=1)]).T<br><br>.. _gensim: https://radimrehurek.com/gensim/<br><br><br>examples<br>--------<br>additional dependencies: sklearn, unidecode<br><br>``test_analogy.py`` and ``test_ner.py`` contain the code to replicate the analogy and named entity recognition (NER) experiments discussed in the aforementioned paper.<br><br>To run the analogy experiment, it is assumed that the `text8 corpus`_ or `1-billion corpus`_ as well as the `analogy questions`_ are in a data directory.<br><br>To run the named entity recognition experiment, it is assumed that the corresponding `training and test files`_ are located in the data/conll2003 directory.<br><br>.. _`text8 corpus`: http://mattmahoney.net/dc/text8.zip<br>.. _`1-billion corpus`: http://code.google.com/p/1-billion-word-language-modeling-benchmark/<br>.. _`analogy questions`: https://code.google.com/archive/p/word2vec/<br>.. _`training and test files`: http://www.cnts.ua.ac.be/conll2003/ner/<br><br><br>If you have any questions please don't hesitate to send me an `email &lt;mailto:cod3licious@gmail.com&gt;`_ and of course if you should find any bugs or want to contribute other improvements, pull requests are very welcome!<br><br><br>\n          </div>"}, "last_serial": 3881080, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "3f3ddfd8594397c5d60ec1c8ca2c3f1a", "sha256": "03b32ae51a6d19c9062f2d4ccf9f830074e79a8153df8a847a91955ce7930d6f"}, "downloads": -1, "filename": "conec-1.0.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "3f3ddfd8594397c5d60ec1c8ca2c3f1a", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 13648, "upload_time": "2017-09-12T19:13:46", "upload_time_iso_8601": "2017-09-12T19:13:46.218763Z", "url": "https://files.pythonhosted.org/packages/0e/e2/4aaedde83fdf4306064811023f80b50796cd42a2b4f047b1cc9281bcb43f/conec-1.0.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c75ca4dc0d6b38d650f9d16b7191ca18", "sha256": "4c12dbe64c2810a65a6b118e274db09fc3725f83d4665c4b5da4bae7e209e6f4"}, "downloads": -1, "filename": "conec-1.0.0.tar.gz", "has_sig": false, "md5_digest": "c75ca4dc0d6b38d650f9d16b7191ca18", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12450, "upload_time": "2017-09-12T19:13:48", "upload_time_iso_8601": "2017-09-12T19:13:48.001373Z", "url": "https://files.pythonhosted.org/packages/46/13/3ef9fe9f4787dce98a9ec478ddb816f87deedb824adb683c60afce19b0e2/conec-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "a6b3450fc41fbff061c4b2030542f332", "sha256": "92a56ead6d9056a6d6f84b5d80815547b28ed42e02fa8ebfb1d8a3ede9e627ef"}, "downloads": -1, "filename": "conec-1.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "a6b3450fc41fbff061c4b2030542f332", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 14130, "upload_time": "2018-05-20T15:24:34", "upload_time_iso_8601": "2018-05-20T15:24:34.416842Z", "url": "https://files.pythonhosted.org/packages/f6/57/1696bc9dbf5146e3782b169432cb50322ce7f6752ec792b9b7f0dd9a3426/conec-1.0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cfd47ae3c9363eba83a6867235fdee01", "sha256": "e7743c2981398c04ab144c0a6ccdfa39c30aee3264e3194b86f60cc8749e3c7e"}, "downloads": -1, "filename": "conec-1.0.1.tar.gz", "has_sig": false, "md5_digest": "cfd47ae3c9363eba83a6867235fdee01", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14113, "upload_time": "2018-05-20T15:24:35", "upload_time_iso_8601": "2018-05-20T15:24:35.833746Z", "url": "https://files.pythonhosted.org/packages/f8/76/c42941a6465233c1c231388ffbf23f973e6bd47491e7fea90db280ddf439/conec-1.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a6b3450fc41fbff061c4b2030542f332", "sha256": "92a56ead6d9056a6d6f84b5d80815547b28ed42e02fa8ebfb1d8a3ede9e627ef"}, "downloads": -1, "filename": "conec-1.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "a6b3450fc41fbff061c4b2030542f332", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 14130, "upload_time": "2018-05-20T15:24:34", "upload_time_iso_8601": "2018-05-20T15:24:34.416842Z", "url": "https://files.pythonhosted.org/packages/f6/57/1696bc9dbf5146e3782b169432cb50322ce7f6752ec792b9b7f0dd9a3426/conec-1.0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cfd47ae3c9363eba83a6867235fdee01", "sha256": "e7743c2981398c04ab144c0a6ccdfa39c30aee3264e3194b86f60cc8749e3c7e"}, "downloads": -1, "filename": "conec-1.0.1.tar.gz", "has_sig": false, "md5_digest": "cfd47ae3c9363eba83a6867235fdee01", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14113, "upload_time": "2018-05-20T15:24:35", "upload_time_iso_8601": "2018-05-20T15:24:35.833746Z", "url": "https://files.pythonhosted.org/packages/f8/76/c42941a6465233c1c231388ffbf23f973e6bd47491e7fea90db280ddf439/conec-1.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:44:01 2020"}