{"info": {"author": "Daniel Guo, Norm Matloff", "author_email": "zhyguo@ucdavis.edu, matloff@cs.ucdavis.edu", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: POSIX", "Operating System :: Unix", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Topic :: Scientific/Engineering"], "description": "# Pydsm\n\nWhile the multi-thread parallel performance is limited in Python because of \nthe Global Interpreter Lock, Pydsm provided parallel computation that\ntruly shares memory between processes in a distributed way.\n\n\n## Requirements\n\n* SharedArray\n* numpy\n* POSIX (Linux, variants of Unix including macOS, etc.)\n* Python 2 is supported, but Python 3 is recommended\n\nYou can install py-dsm via the `pip` command:\n\n```\npip install py-dsm\n```\n\n## Authors\n\nZhiyuan Guo\n\nNorm Matloff\n\n\n## Usage\n\n\n### Start the processes\n\nTo create a cluster instance of 4 worker processes, one will do:\n\n```\nwith pydsm.Cluster(4) as cluster:\n```\n\n### Global array\n\nIn pydsm, all global arrays in shared memory are numpy arrays. \nTo create a global array, one needs to first create a cluster instance \nand then call `createShared()`.\n\n```\na = cluster.createShared(name = \"A\", shape = 10, dataType = int)\n```\n\nThe user needs to provide the global array with an arbitrary but unique name, \nthe shape (i.e., dimension) of the array, and the datatype (default is int).\nThe shape and datatype arguments will follow the same format as those typical \nnumpy functions such as `numpy.zeros()`. The returned array is an array with\nall elements initialized to zeros. All names of the global arrays should be \nunique.\n\n### Run the processes\nThen, `runProcesses(func, paras)` is invoked \nto run the user's function in parallel. `func` is the name of the\nuser-defined parallel function. `paras`, defaulted to an empty tuple, \nis a tuple consisting of parameters passed in to the user's function.\nEach worker process will get a copy of those parameters in `paras`.\n\n\n```\ncluster.runProcesses(foo)\n```\n\n\n### Parallelized function\n\nThe function `foo` implemented by the user will be executed \nby 4 processes simultaneously. \n**The first argument of a user's parallel function is mendatory.**\nThis mendatory parameter is a dictionary that contains **resource**\nyou may need for your parallel function.\nThe resource has references to the id of each process, \nthe global array(s), and a lock.\n\n\n```\ndef foo(res, ...):\n\t# res is the resource\n\t# ... means any extra parameters you may need\n\t# Code\n```\n\n\n\n### Locks and barriers\n\nTo use a lock in the parallelized function,\nthe lock first needs to be retrieved from\n'resource' (the first parameter).\n\n```\nlock = res['lock']\n```\n\nThen, the user can apply the lock anywhere\nthey want in the function.\n\n```\nlock.apply()\n# Critical section\nlock.release()\n```\n\nThe barrier is invoked in the following way.\n\n```\npydsm.Cluster.barrier()\n```\n\n### Split the tasks\n\nThe user can invoke `splitIndices(n, id, random=True)` inside the user's\nparallel function to distribute the tasks. \n`splitIndices` will return to each worker process a list of indices (numbers).\n\nSay we want to compute Y = AX in parallel by splitting the rows of A into\nchunks.  We have 100 rows in A and 10 processors. We can let each\nprocess take 10 rows. \n\n```\nmyid = resource['id']\nmyidxs = pydsm.Cluster.splitIndices(100, myid, random=True)\nY[myidxs, ] = np.matmul(A[myidxs,], X)\n```\n\nIn this scenario, `n` will be 100.\nIt is recommended to set `random` to True to \nsplit the rows randomly and thus achieve better load balancing.\nOtherwise, process 0 will take the first ten rows, process 1 will take the\nsecond ten rows, and so on, which may result in inefficient load balancing.\n\n\n\n## Example\nThis is an embarrassingly parallel example that illustrates the usage\nof the pydsm. Two vectors are added in parallel.\nThe idea is to break A and B into chunks, and have each worker process\nwork on one chunk.\nThe complete source code is in `doc/examples/vecAdd.py`.\n\nA and B are the two vectors to be added.\nThe result will be stored in C, as the computation is carried out.\nThus, all of them need to be shared variables.\nThey are set up in the following way.\n\n\n```\nwith pydsm.Cluster(4) as cluster:\n\tA = cluster.createShared(name = \"A\", shape = 10, dataType = int)\n\tB = cluster.createShared(\"B\", 10, int)\n\tC = cluster.createShared(\"C\", 10, int)\n```\n\nNow A and B are shared arrays backed by numpy.\nWe can use and treat them the same way we do to numpy arrays.\n\n```\n\tA[:] = np.arange(10) \n\t# A is now \"array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\"\n\tB[:] = np.arange(10)\n\tB += 1 \n\t# B is now \"array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\"\n```\n\nWe then call `runProcesses()`, with the first parameter being\nthe name of the parallel function, which we will define later.\nThe second parameter is a tuple of parameters used by parallel function.\nHere we pass in 10 as the length of each vector.\n\n```\n\tcluster.runProcesses(add, paras=(10,))\n```\n\nNext, we define the parallel function `add`.\n\n```\ndef add(resource, n):\n    myid = resource['id']\n    A = pydsm.Cluster.getShared(\"A\")\n    B = pydsm.Cluster.getShared(\"B\")\n    C = pydsm.Cluster.getShared(\"C\")\n\n    myidxs = pydsm.Cluster.splitIndices(n, myid, random=False)\n    C[myidxs] = A[myidxs] + B[myidxs]\n```\n\nEach worker process first obtains an unique id, and then\nattaches A, B, and C to the corresponding shared variable.\n`splitIndices()` is then called to give each worker a list of indices.\nEach worker will then compute elements in the array corresponding to those\nindices.\nNote that C is a shared variable, so `add` does not need to return C.\n\nFor illustrative purpose of the usage of barriers,\nwe can have the following code at the end of `add`.\nThe process 1 will print out C after the computation is done.\n\n```\n# Below is just for illustrative purpose\npydsm.Cluster.barrier()\nif myid == 1:\n\t# In some versions of python, printing C directly may cause issues.\n\t# It is better to first convert the SharedArray into an numpy array\n\t# and then print it. So do np.array(C) before printing\n\tprint(\"Check out vector C in processes: \", np.array(C))\n\t# C will be '[ 1  3  5  7  9 11 13 15 17 19]'\n```\n\n\n## More examples\n\nFor more sample applications using pydsm such as finding prime numbers and\nmatrix multiplication, they are under the directory `inst/`.\n\n## Notes\n\nIf your program is terminated abnormally, py-dsm may not compeletely delete\nyour shared variables. When you run your program next time, you may encounter\nthe following error message because your program is trying to create a\nshared variable that is still alive.\n\n```\n# You are trying to create a shared variable named 'A', but there is\n# already a shared var named 'A'. \nFileExistsError: [Errno 17] File exists: 'shm://A'\n```\n\nIn this scenario, you need to delete the shared variable yourself in the Python\ninterpreter.\n\n```\n>>> import SharedArray as sa\n>>> sa.delete(\"A\")\n```\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/matloff/pydsm", "keywords": "shared memory parallel concurrent multiprocessing numpy", "license": "", "maintainer": "", "maintainer_email": "", "name": "py-dsm", "package_url": "https://pypi.org/project/py-dsm/", "platform": "", "project_url": "https://pypi.org/project/py-dsm/", "project_urls": {"Homepage": "https://github.com/matloff/pydsm"}, "release_url": "https://pypi.org/project/py-dsm/1.0.1/", "requires_dist": ["SharedArray", "numpy"], "requires_python": "", "summary": "Python true-shared memory parallel computation", "version": "1.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Pydsm</h1>\n<p>While the multi-thread parallel performance is limited in Python because of\nthe Global Interpreter Lock, Pydsm provided parallel computation that\ntruly shares memory between processes in a distributed way.</p>\n<h2>Requirements</h2>\n<ul>\n<li>SharedArray</li>\n<li>numpy</li>\n<li>POSIX (Linux, variants of Unix including macOS, etc.)</li>\n<li>Python 2 is supported, but Python 3 is recommended</li>\n</ul>\n<p>You can install py-dsm via the <code>pip</code> command:</p>\n<pre><code>pip install py-dsm\n</code></pre>\n<h2>Authors</h2>\n<p>Zhiyuan Guo</p>\n<p>Norm Matloff</p>\n<h2>Usage</h2>\n<h3>Start the processes</h3>\n<p>To create a cluster instance of 4 worker processes, one will do:</p>\n<pre><code>with pydsm.Cluster(4) as cluster:\n</code></pre>\n<h3>Global array</h3>\n<p>In pydsm, all global arrays in shared memory are numpy arrays.\nTo create a global array, one needs to first create a cluster instance\nand then call <code>createShared()</code>.</p>\n<pre><code>a = cluster.createShared(name = \"A\", shape = 10, dataType = int)\n</code></pre>\n<p>The user needs to provide the global array with an arbitrary but unique name,\nthe shape (i.e., dimension) of the array, and the datatype (default is int).\nThe shape and datatype arguments will follow the same format as those typical\nnumpy functions such as <code>numpy.zeros()</code>. The returned array is an array with\nall elements initialized to zeros. All names of the global arrays should be\nunique.</p>\n<h3>Run the processes</h3>\n<p>Then, <code>runProcesses(func, paras)</code> is invoked\nto run the user's function in parallel. <code>func</code> is the name of the\nuser-defined parallel function. <code>paras</code>, defaulted to an empty tuple,\nis a tuple consisting of parameters passed in to the user's function.\nEach worker process will get a copy of those parameters in <code>paras</code>.</p>\n<pre><code>cluster.runProcesses(foo)\n</code></pre>\n<h3>Parallelized function</h3>\n<p>The function <code>foo</code> implemented by the user will be executed\nby 4 processes simultaneously.\n<strong>The first argument of a user's parallel function is mendatory.</strong>\nThis mendatory parameter is a dictionary that contains <strong>resource</strong>\nyou may need for your parallel function.\nThe resource has references to the id of each process,\nthe global array(s), and a lock.</p>\n<pre><code>def foo(res, ...):\n\t# res is the resource\n\t# ... means any extra parameters you may need\n\t# Code\n</code></pre>\n<h3>Locks and barriers</h3>\n<p>To use a lock in the parallelized function,\nthe lock first needs to be retrieved from\n'resource' (the first parameter).</p>\n<pre><code>lock = res['lock']\n</code></pre>\n<p>Then, the user can apply the lock anywhere\nthey want in the function.</p>\n<pre><code>lock.apply()\n# Critical section\nlock.release()\n</code></pre>\n<p>The barrier is invoked in the following way.</p>\n<pre><code>pydsm.Cluster.barrier()\n</code></pre>\n<h3>Split the tasks</h3>\n<p>The user can invoke <code>splitIndices(n, id, random=True)</code> inside the user's\nparallel function to distribute the tasks.\n<code>splitIndices</code> will return to each worker process a list of indices (numbers).</p>\n<p>Say we want to compute Y = AX in parallel by splitting the rows of A into\nchunks.  We have 100 rows in A and 10 processors. We can let each\nprocess take 10 rows.</p>\n<pre><code>myid = resource['id']\nmyidxs = pydsm.Cluster.splitIndices(100, myid, random=True)\nY[myidxs, ] = np.matmul(A[myidxs,], X)\n</code></pre>\n<p>In this scenario, <code>n</code> will be 100.\nIt is recommended to set <code>random</code> to True to\nsplit the rows randomly and thus achieve better load balancing.\nOtherwise, process 0 will take the first ten rows, process 1 will take the\nsecond ten rows, and so on, which may result in inefficient load balancing.</p>\n<h2>Example</h2>\n<p>This is an embarrassingly parallel example that illustrates the usage\nof the pydsm. Two vectors are added in parallel.\nThe idea is to break A and B into chunks, and have each worker process\nwork on one chunk.\nThe complete source code is in <code>doc/examples/vecAdd.py</code>.</p>\n<p>A and B are the two vectors to be added.\nThe result will be stored in C, as the computation is carried out.\nThus, all of them need to be shared variables.\nThey are set up in the following way.</p>\n<pre><code>with pydsm.Cluster(4) as cluster:\n\tA = cluster.createShared(name = \"A\", shape = 10, dataType = int)\n\tB = cluster.createShared(\"B\", 10, int)\n\tC = cluster.createShared(\"C\", 10, int)\n</code></pre>\n<p>Now A and B are shared arrays backed by numpy.\nWe can use and treat them the same way we do to numpy arrays.</p>\n<pre><code>\tA[:] = np.arange(10) \n\t# A is now \"array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\"\n\tB[:] = np.arange(10)\n\tB += 1 \n\t# B is now \"array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\"\n</code></pre>\n<p>We then call <code>runProcesses()</code>, with the first parameter being\nthe name of the parallel function, which we will define later.\nThe second parameter is a tuple of parameters used by parallel function.\nHere we pass in 10 as the length of each vector.</p>\n<pre><code>\tcluster.runProcesses(add, paras=(10,))\n</code></pre>\n<p>Next, we define the parallel function <code>add</code>.</p>\n<pre><code>def add(resource, n):\n    myid = resource['id']\n    A = pydsm.Cluster.getShared(\"A\")\n    B = pydsm.Cluster.getShared(\"B\")\n    C = pydsm.Cluster.getShared(\"C\")\n\n    myidxs = pydsm.Cluster.splitIndices(n, myid, random=False)\n    C[myidxs] = A[myidxs] + B[myidxs]\n</code></pre>\n<p>Each worker process first obtains an unique id, and then\nattaches A, B, and C to the corresponding shared variable.\n<code>splitIndices()</code> is then called to give each worker a list of indices.\nEach worker will then compute elements in the array corresponding to those\nindices.\nNote that C is a shared variable, so <code>add</code> does not need to return C.</p>\n<p>For illustrative purpose of the usage of barriers,\nwe can have the following code at the end of <code>add</code>.\nThe process 1 will print out C after the computation is done.</p>\n<pre><code># Below is just for illustrative purpose\npydsm.Cluster.barrier()\nif myid == 1:\n\t# In some versions of python, printing C directly may cause issues.\n\t# It is better to first convert the SharedArray into an numpy array\n\t# and then print it. So do np.array(C) before printing\n\tprint(\"Check out vector C in processes: \", np.array(C))\n\t# C will be '[ 1  3  5  7  9 11 13 15 17 19]'\n</code></pre>\n<h2>More examples</h2>\n<p>For more sample applications using pydsm such as finding prime numbers and\nmatrix multiplication, they are under the directory <code>inst/</code>.</p>\n<h2>Notes</h2>\n<p>If your program is terminated abnormally, py-dsm may not compeletely delete\nyour shared variables. When you run your program next time, you may encounter\nthe following error message because your program is trying to create a\nshared variable that is still alive.</p>\n<pre><code># You are trying to create a shared variable named 'A', but there is\n# already a shared var named 'A'. \nFileExistsError: [Errno 17] File exists: 'shm://A'\n</code></pre>\n<p>In this scenario, you need to delete the shared variable yourself in the Python\ninterpreter.</p>\n<pre><code>&gt;&gt;&gt; import SharedArray as sa\n&gt;&gt;&gt; sa.delete(\"A\")\n</code></pre>\n\n          </div>"}, "last_serial": 6891988, "releases": {"1.0.1": [{"comment_text": "", "digests": {"md5": "19cf1e0ccdaca575698418d58da3a040", "sha256": "7d249cb9bc3790c33c3be0436ec8df9f2d89082edaa0d8c1a040308255d807f5"}, "downloads": -1, "filename": "py_dsm-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "19cf1e0ccdaca575698418d58da3a040", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 6814, "upload_time": "2020-03-26T21:17:41", "upload_time_iso_8601": "2020-03-26T21:17:41.732567Z", "url": "https://files.pythonhosted.org/packages/64/9e/174a51e9ec9f438d31add1e6ce64bcc168dfa479209c52f8fac230e54857/py_dsm-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b07002a0dca1baf7ea1425fc776338f9", "sha256": "9a5c457beda367c291bcc3e4e6d21adf23575b601adf2f03e8e155e9341c2c92"}, "downloads": -1, "filename": "py-dsm-1.0.1.tar.gz", "has_sig": false, "md5_digest": "b07002a0dca1baf7ea1425fc776338f9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7116, "upload_time": "2020-03-26T21:17:43", "upload_time_iso_8601": "2020-03-26T21:17:43.964242Z", "url": "https://files.pythonhosted.org/packages/c8/b2/49238556031f43bde48ac4d4eeeccdf9e568ad04b1a4aaa1fe824d9359d5/py-dsm-1.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "19cf1e0ccdaca575698418d58da3a040", "sha256": "7d249cb9bc3790c33c3be0436ec8df9f2d89082edaa0d8c1a040308255d807f5"}, "downloads": -1, "filename": "py_dsm-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "19cf1e0ccdaca575698418d58da3a040", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 6814, "upload_time": "2020-03-26T21:17:41", "upload_time_iso_8601": "2020-03-26T21:17:41.732567Z", "url": "https://files.pythonhosted.org/packages/64/9e/174a51e9ec9f438d31add1e6ce64bcc168dfa479209c52f8fac230e54857/py_dsm-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b07002a0dca1baf7ea1425fc776338f9", "sha256": "9a5c457beda367c291bcc3e4e6d21adf23575b601adf2f03e8e155e9341c2c92"}, "downloads": -1, "filename": "py-dsm-1.0.1.tar.gz", "has_sig": false, "md5_digest": "b07002a0dca1baf7ea1425fc776338f9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7116, "upload_time": "2020-03-26T21:17:43", "upload_time_iso_8601": "2020-03-26T21:17:43.964242Z", "url": "https://files.pythonhosted.org/packages/c8/b2/49238556031f43bde48ac4d4eeeccdf9e568ad04b1a4aaa1fe824d9359d5/py-dsm-1.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:06:43 2020"}