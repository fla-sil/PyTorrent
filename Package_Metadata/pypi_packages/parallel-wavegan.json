{"info": {"author": "Tomoki Hayashi", "author_email": "hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# Parallel WaveGAN (+ MelGAN) implementation with Pytorch\n\n![](https://github.com/kan-bayashi/ParallelWaveGAN/workflows/CI/badge.svg) [![](https://img.shields.io/pypi/v/parallel-wavegan)](https://pypi.org/project/parallel-wavegan/) ![](https://img.shields.io/pypi/pyversions/parallel-wavegan) ![](https://img.shields.io/pypi/l/parallel-wavegan) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb)\n\nThis repository provides **UNOFFICIAL** [Parallel WaveGAN](https://arxiv.org/abs/1910.11480) and [MelGAN](https://arxiv.org/abs/1910.06711) implementations with Pytorch.  \nYou can combine these state-of-the-art non-autoregressive models to build your own great vocoder!\n\nPlease check our samples in [our demo HP](https://kan-bayashi.github.io/ParallelWaveGAN).\n\n![](https://user-images.githubusercontent.com/22779813/68081503-4b8fcf00-fe52-11e9-8791-e02851220355.png)\n\n> Source of the figure: https://arxiv.org/pdf/1910.11480.pdf\n\nThe goal of this repository is to provide the real-time neural vocoder which is compatible with [ESPnet-TTS](https://github.com/espnet/espnet).  \n\nYou can try the realtime end-to-end text-to-speech demonstraion in Google Colab!\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb)\n\n## What's new\n\n- 2020/03/25 **(New!)** [LibriTTS pretrained models](#Results) are available!\n- 2020/03/17  [Tensorflow conversion example notebook](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/notebooks/convert_melgan_from_pytorch_to_tensorflow.ipynb) is available (Thanks, [@dathudeptrai](https://github.com/dathudeptrai))!\n- 2020/03/16 [LibriTTS recipe](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/libritts/voc1) is available!\n- 2020/03/12 [PWG G + MelGAN D + STFT-loss samples](#Results) are available!\n- 2020/03/12 Multi-speaker English recipe [egs/vctk/voc1](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/vctk/voc1) is available!\n- 2020/02/22 [MelGAN G + MelGAN D + STFT-loss samples](#Results) are available!\n- 2020/02/12 Support [MelGAN](https://arxiv.org/abs/1910.06711)'s discriminator!\n- 2020/02/08 Support [MelGAN](https://arxiv.org/abs/1910.06711)'s generator!\n\n## Requirements\n\nThis repository is tested on Ubuntu 16.04 with a GPU Titan V.\n\n- Python 3.6+\n- Cuda 10.0\n- CuDNN 7+\n- NCCL 2+ (for distributed multi-gpu training)\n- libsndfile (you can install via `sudo apt install libsndfile-dev` in ubuntu)\n- jq (you can install via `sudo apt install jq` in ubuntu)\n\nDifferent cuda version should be working but not explicitly tested.  \nAll of the codes are tested on Pytorch 1.0.1, 1.1, 1.2, 1.3.1, 1.4 and 1.5.\n\n## Setup\n\nYou can select the installation method from two alternatives.\n\n### A. Use pip\n\n```bash\n$ git clone https://github.com/kan-bayashi/ParallelWaveGAN.git\n$ cd ParallelWaveGAN\n$ pip install -e .\n# If you want to use distributed training, please install\n# apex manually by following https://github.com/NVIDIA/apex\n$ ...\n```\nNote that your cuda version must be exactly matched with the version used for pytorch binary to install apex.  \nTo install pytorch compiled with different cuda version, see `tools/Makefile`.\n\n### B. Make virtualenv\n\n```bash\n$ git clone https://github.com/kan-bayashi/ParallelWaveGAN.git\n$ cd ParallelWaveGAN/tools\n$ make\n# If you want to use distributed training, please run following\n# command to install apex.\n$ make apex\n```\n\nNote that we specify cuda version used to compile pytorch wheel.  \nIf you want to use different cuda version, please check `tools/Makefile` to change the pytorch wheel to be installed.\n\n## Run\n\nThis repository provides [Kaldi](https://github.com/kaldi-asr/kaldi)-style recipes, as the same as [ESPnet](https://github.com/espnet/espnet).  \nCurrently, the following recipes are supported.\n\n- [LJSpeech](https://keithito.com/LJ-Speech-Dataset/): English female speaker\n- [JSUT](https://sites.google.com/site/shinnosuketakamichi/publication/jsut): Japanese female speaker\n- [CSMSC](https://www.data-baker.com/open_source.html): Mandarin female speaker\n- [CMU Arctic](http://www.festvox.org/cmu_arctic/): English speakers\n- [JNAS](http://research.nii.ac.jp/src/en/JNAS.html): Japanese multi-speaker\n- [VCTK](https://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html): English multi-speaker\n- [LibriTTS](https://arxiv.org/abs/1904.02882): English multi-speaker\n\nTo run the recipe, please follow the below instruction.\n\n```bash\n# Let us move on the recipe directory\n$ cd egs/ljspeech/voc1\n\n# Run the recipe from scratch\n$ ./run.sh\n\n# You can change config via command line\n$ ./run.sh --conf <your_customized_yaml_config>\n\n# You can select the stage to start and stop\n$ ./run.sh --stage 2 --stop_stage 2\n\n# If you want to specify the gpu\n$ CUDA_VISIBLE_DEVICES=1 ./run.sh --stage 2\n\n# If you want to resume training from 10000 steps checkpoint\n$ ./run.sh --stage 2 --resume <path>/<to>/checkpoint-10000steps.pkl\n```\n\nThe integration with job schedulers such as [slurm](https://slurm.schedmd.com/documentation.html) can be done via `cmd.sh` and  `conf/slurm.conf`.  \nIf you want to use it, please check [this page](https://kaldi-asr.org/doc/queue.html).\n\nAll of the hyperparameters is written in a single yaml format configuration file.  \nPlease check [this example](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/parallel_wavegan.v1.yaml) in ljspeech recipe.\n\nThe training requires ~3 days with a single GPU (TITAN V).  \nThe speed of the training is 0.5 seconds per an iteration, in total ~ 200000 sec (= 2.31 days).  \nYou can monitor the training progress via tensorboard.\n\n```bash\n$ tensorboard --logdir exp\n```\n\n![](https://user-images.githubusercontent.com/22779813/68100080-58bbc500-ff09-11e9-9945-c835186fd7c2.png)\n\nIf you want to accelerate the training, you can try distributed multi-gpu training based on apex.  \nYou need to install apex for distributed training. Please make sure you already installed it.  \nThen you can run distributed multi-gpu training via following command:\n\n```bash\n# in the case of the number of gpus = 8\n$ CUDA_VISIBLE_DEVICES=\"0,1,2,3,4,5,6,7\" ./run.sh --stage 2 --n_gpus 8\n```\n\nIn the case of distributed training, batch size will be automatically multiplied by the number of gpus.  \nPlease be careful.\n\nThe decoding speed is RTF = 0.016 with TITAN V, much faster than the real-time.\n\n```bash\n[decode]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 250/250 [00:30<00:00,  8.31it/s, RTF=0.0156]\n2019-11-03 09:07:40,480 (decode:127) INFO: finished generation of 250 utterances (RTF = 0.016).\n```\n\nEven on the CPU (Intel(R) Xeon(R) Gold 6154 CPU @ 3.00GHz 16 threads), it can generate less than the real-time.\n\n```bash\n[decode]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 250/250 [22:16<00:00,  5.35s/it, RTF=0.841]\n2019-11-06 09:04:56,697 (decode:129) INFO: finished generation of 250 utterances (RTF = 0.734).\n```\n\nIf you use MelGAN's generator, the decoding speed will be further faster.\n\n```bash\n# On CPU (Intel(R) Xeon(R) Gold 6154 CPU @ 3.00GHz 16 threads)\n[decode]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 250/250 [04:00<00:00,  1.04it/s, RTF=0.0882]\n2020-02-08 10:45:14,111 (decode:142) INFO: Finished generation of 250 utterances (RTF = 0.137).\n\n# On GPU (TITAN V)\n[decode]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 250/250 [00:06<00:00, 36.38it/s, RTF=0.00189]\n2020-02-08 05:44:42,231 (decode:142) INFO: Finished generation of 250 utterances (RTF = 0.002).\n```\n\nIf you want to accelerate the inference more, it is worthwhile to try the conversion from pytorch to tensorflow.  \nThe example of the conversion is available in [the notebook](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/notebooks/convert_melgan_from_pytorch_to_tensorflow.ipynb) (Provided by [@dathudeptrai](https://github.com/dathudeptrai)).  \n\n## Results\n\nHere the results are summarized in the table.  \nYou can listen to the samples and download pretrained models from the link to our google drive.\n\n| Model                                                                                                          | Conf                                                                                                                        | Lang  | Fs [Hz] | Mel range [Hz] | FFT / Hop / Win [pt] | # iters |\n| :------                                                                                                        | :---:                                                                                                                       | :---: | :----:  | :--------:     | :---------------:    | :-----: |\n| [ljspeech_parallel_wavegan.v1](https://drive.google.com/open?id=1wdHr1a51TLeo4iKrGErVKHVFyq6D17TU)             | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/parallel_wavegan.v1.yaml)          | EN    | 22.05k  | 80-7600        | 1024 / 256 / None    | 400k    |\n| [ljspeech_parallel_wavegan.v1.long](https://drive.google.com/open?id=1XRn3s_wzPF2fdfGshLwuvNHrbgD0hqVS)        | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/parallel_wavegan.v1.long.yaml)     | EN    | 22.05k  | 80-7600        | 1024 / 256 / None    | 1000k   |\n| [ljspeech_parallel_wavegan.v1.no_limit](https://drive.google.com/open?id=1NoD3TCmKIDHHtf74YsScX8s59aZFOFJA)    | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/parallel_wavegan.v1.no_limit.yaml) | EN    | 22.05k  | None           | 1024 / 256 / None    | 400k    |\n| [ljspeech_parallel_wavegan.v3](https://drive.google.com/open?id=1a5Q2KiJfUQkVFo5Bd1IoYPVicJGnm7EL)             | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/parallel_wavegan.v3.yaml)          | EN    | 22.05k  | 80-7600        | 1024 / 256 / None    | 3000k   |\n| [ljspeech_melgan.v1](https://drive.google.com/open?id=1z0vO1UMFHyeCdCLAmd7Moewi4QgCb07S)                       | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/melgan.v1.yaml)                    | EN    | 22.05k  | 80-7600        | 1024 / 256 / None    | 400k    |\n| [ljspeech_melgan.v1.long](https://drive.google.com/open?id=1RqNGcFO7Geb6-4pJtMbC9-ph_WiWA14e)                  | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/melgan.v1.long.yaml)               | EN    | 22.05k  | 80-7600        | 1024 / 256 / None    | 1000k   |\n| [ljspeech_melgan_large.v1](https://drive.google.com/open?id=1KQt-gyxbG6iTZ4aVn9YjQuaGYjAleYs8)                 | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/melgan_large.v1.yaml)              | EN    | 22.05k  | 80-7600        | 1024 / 256 / None    | 400k    |\n| [ljspeech_melgan_large.v1.long](https://drive.google.com/open?id=1ogEx-wiQS7HVtdU0_TmlENURIe4v2erC)            | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/melgan_large.v1.long.yaml)         | EN    | 22.05k  | 80-7600        | 1024 / 256 / None    | 1000k   |\n| [ljspeech_melgan.v3](https://drive.google.com/open?id=1eXkm_Wf1YVlk5waP4Vgqd0GzMaJtW3y5)                       | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/melgan.v3.yaml)                    | EN    | 22.05k  | 80-7600        | 1024 / 256 / None    | 2000k   |\n| [ljspeech_melgan.v3.long](https://drive.google.com/open?id=1u1w4RPefjByX8nfsL59OzU2KgEksBhL1)                  | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/melgan.v3.long.yaml)               | EN    | 22.05k  | 80-7600        | 1024 / 256 / None    | 4000k   |\n| [jsut_parallel_wavegan.v1](https://drive.google.com/open?id=1UDRL0JAovZ8XZhoH0wi9jj_zeCKb-AIA)                 | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/jsut/voc1/conf/parallel_wavegan.v1.yaml)              | JP    | 24k     | 80-7600        | 2048 / 300 / 1200    | 400k    |\n| [csmsc_parallel_wavegan.v1](https://drive.google.com/open?id=1C2nu9nOFdKcEd-D9xGquQ0bCia0B2v_4)                | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/csmsc/voc1/conf/parallel_wavegan.v1.yaml)             | ZH    | 24k     | 80-7600        | 2048 / 300 / 1200    | 400k    |\n| [arctic_slt_parallel_wavegan.v1](https://drive.google.com/open?id=1xG9CmSED2TzFdklD6fVxzf7kFV2kPQAJ)           | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/arctic/voc1/conf/parallel_wavegan.v1.yaml)            | EN    | 16k     | 80-7600        | 1024 / 256 / None    | 400k    |\n| [jnas_parallel_wavegan.v1](https://drive.google.com/open?id=1n_hkxPxryVXbp6oHM1NFm08q0TcoDXz1)                 | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/jnas/voc1/conf/parallel_wavegan.v1.yaml)              | JP    | 16k     | 80-7600        | 1024 / 256 / None    | 400k    |\n| [vctk_parallel_wavegan.v1](https://drive.google.com/open?id=1dGTu-B7an2P5sEOepLPjpOaasgaSnLpi)                 | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/vctk/voc1/conf/parallel_wavegan.v1.yaml)              | EN    | 24k     | 80-7600        | 2048 / 300 / 1200    | 400k    |\n| [vctk_parallel_wavegan.v1.long](https://drive.google.com/open?id=1qoocM-VQZpjbv5B-zVJpdraazGcPL0So       )     | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/vctk/voc1/conf/parallel_wavegan.v1.long.yaml)         | EN    | 24k     | 80-7600        | 2048 / 300 / 1200    | 1000k   |\n| [libritts_parallel_wavegan.v1 (New!)](https://drive.google.com/open?id=1pb18Nd2FCYWnXfStszBAEEIMe_EZUJV0)      | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/libritts/voc1/conf/parallel_wavegan.v1.yaml)          | EN    | 24k     | 80-7600        | 2048 / 300 / 1200    | 400k    |\n| [libritts_parallel_wavegan.v1.long (New!)](https://drive.google.com/open?id=15ibzv-uTeprVpwT946Hl1XUYDmg5Afwz) | [link](https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/libritts/voc1/conf/parallel_wavegan.v1.long.yaml)     | EN    | 24k     | 80-7600        | 2048 / 300 / 1200    | 1000k   |\n\nIf you want to check more results, please access at [our google drive](https://drive.google.com/open?id=1sd_QzcUNnbiaWq7L0ykMP7Xmk-zOuxTi).\n\n## How-to-use pretrained models\n\nHere the minimal code is shown to perform analysis-synthesis using the pretrained model.\n\n```bash\n# Please make sure you installed `parallel_wavegan`\n# If not, please install via pip\n$ pip install parallel_wavegan\n\n# Please download pretrained models and put them in `pretrain_model` directory\n$ ls pretrain_model\n\uf016  checkpoint-400000steps.pkl  \uf481  config.yml  \uf016  stats.h5\n\n# Please put an audio file in `sample` directory to perform analysis-synthesis\n$ ls sample/\n\uf001  sample.wav\n\n# Then perform feature extraction -> feature normalization -> sysnthesis\n$ parallel-wavegan-preprocess \\\n\t--config pretrain_model/config.yml \\\n\t--rootdir sample \\\n\t--dumpdir dump/sample/raw\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 914.19it/s]\n[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done   1 out of   1 | elapsed:    1.2s finished\n$ parallel-wavegan-normalize \\\n\t--config pretrain_model/config.yml \\\n\t--rootdir dump/sample/raw \\\n\t--dumpdir dump/sample/norm \\\n\t--stats pretrain_model/stats.h5\n2019-11-13 13:44:29,574 (normalize:87) INFO: the number of files = 1.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 513.13it/s]\n[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done   1 out of   1 | elapsed:    0.6s finished\n$ parallel-wavegan-decode \\\n\t--checkpoint pretrain_model/checkpoint-400000steps.pkl \\\n\t--dumpdir dump/sample/norm \\\n\t--outdir sample\n2019-11-13 13:44:31,229 (decode:91) INFO: the number of features to be decoded = 1.\n2019-11-13 13:44:37,074 (decode:105) INFO: loaded model parameters from pretrain_model/checkpoint-400000steps.pkl.\n[decode]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.33it/s, RTF=0.0146]\n2019-11-13 13:44:37,132 (decode:129) INFO: finished generation of 1 utterances (RTF = 0.015).\n\n# you can find the generated speech in `sample` directory\n$ ls sample\n\uf001  sample.wav  \uf001  sample_gen.wav\n```\n\nIf you want to combine with TTS models, you can try the realtime demonstraion in Google Colab!\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb)\n\n## References\n\n- [Parallel WaveGAN](https://arxiv.org/abs/1910.11480)\n- [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder)\n- [LiyuanLucasLiu/RAdam](https://github.com/LiyuanLucasLiu/RAdam)\n- [MelGAN](https://arxiv.org/abs/1910.06711)\n- [descriptinc/melgan-neurips](https://github.com/descriptinc/melgan-neurips)\n\n## Acknowledgement\n\nThe author would like to thank Ryuichi Yamamoto ([@r9y9](https://github.com/r9y9)) for his great repository, paper and valuable discussions.\n\n## Author\n\nTomoki Hayashi ([@kan-bayashi](https://github.com/kan-bayashi))  \nE-mail: `hayashi.tomoki<at>g.sp.m.is.nagoya-u.ac.jp`", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/kan-bayashi/ParallelWaveGAN", "keywords": "", "license": "MIT License", "maintainer": "", "maintainer_email": "", "name": "parallel-wavegan", "package_url": "https://pypi.org/project/parallel-wavegan/", "platform": "", "project_url": "https://pypi.org/project/parallel-wavegan/", "project_urls": {"Homepage": "http://github.com/kan-bayashi/ParallelWaveGAN"}, "release_url": "https://pypi.org/project/parallel-wavegan/0.3.5/", "requires_dist": null, "requires_python": "", "summary": "Parallel WaveGAN implementation", "version": "0.3.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Parallel WaveGAN (+ MelGAN) implementation with Pytorch</h1>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e0b320618496a74bd5aca7dec3a86245b502c9d7/68747470733a2f2f6769746875622e636f6d2f6b616e2d626179617368692f506172616c6c656c5761766547414e2f776f726b666c6f77732f43492f62616467652e737667\"> <a href=\"https://pypi.org/project/parallel-wavegan/\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d0104e7c01f42e69378163468e2399f0bf8275b9/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f706172616c6c656c2d7761766567616e\"></a> <img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/33bf510d1d2029681a5f96f72c5a36960521a4e3/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f706172616c6c656c2d7761766567616e\"> <img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7effbb0e1a5b63ecf8455c0a48072b93dddb7359/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f706172616c6c656c2d7761766567616e\"> <a href=\"https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></p>\n<p>This repository provides <strong>UNOFFICIAL</strong> <a href=\"https://arxiv.org/abs/1910.11480\" rel=\"nofollow\">Parallel WaveGAN</a> and <a href=\"https://arxiv.org/abs/1910.06711\" rel=\"nofollow\">MelGAN</a> implementations with Pytorch.<br>\nYou can combine these state-of-the-art non-autoregressive models to build your own great vocoder!</p>\n<p>Please check our samples in <a href=\"https://kan-bayashi.github.io/ParallelWaveGAN\" rel=\"nofollow\">our demo HP</a>.</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d1824ba9a51a74defbd81ac542113aeece55dad9/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f32323737393831332f36383038313530332d34623866636630302d666535322d313165392d383739312d6530323835313232303335352e706e67\"></p>\n<blockquote>\n<p>Source of the figure: <a href=\"https://arxiv.org/pdf/1910.11480.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1910.11480.pdf</a></p>\n</blockquote>\n<p>The goal of this repository is to provide the real-time neural vocoder which is compatible with <a href=\"https://github.com/espnet/espnet\" rel=\"nofollow\">ESPnet-TTS</a>.</p>\n<p>You can try the realtime end-to-end text-to-speech demonstraion in Google Colab!</p>\n<p><a href=\"https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></p>\n<h2>What's new</h2>\n<ul>\n<li>2020/03/25 <strong>(New!)</strong> <a href=\"#Results\" rel=\"nofollow\">LibriTTS pretrained models</a> are available!</li>\n<li>2020/03/17  <a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/notebooks/convert_melgan_from_pytorch_to_tensorflow.ipynb\" rel=\"nofollow\">Tensorflow conversion example notebook</a> is available (Thanks, <a href=\"https://github.com/dathudeptrai\" rel=\"nofollow\">@dathudeptrai</a>)!</li>\n<li>2020/03/16 <a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/libritts/voc1\" rel=\"nofollow\">LibriTTS recipe</a> is available!</li>\n<li>2020/03/12 <a href=\"#Results\" rel=\"nofollow\">PWG G + MelGAN D + STFT-loss samples</a> are available!</li>\n<li>2020/03/12 Multi-speaker English recipe <a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/vctk/voc1\" rel=\"nofollow\">egs/vctk/voc1</a> is available!</li>\n<li>2020/02/22 <a href=\"#Results\" rel=\"nofollow\">MelGAN G + MelGAN D + STFT-loss samples</a> are available!</li>\n<li>2020/02/12 Support <a href=\"https://arxiv.org/abs/1910.06711\" rel=\"nofollow\">MelGAN</a>'s discriminator!</li>\n<li>2020/02/08 Support <a href=\"https://arxiv.org/abs/1910.06711\" rel=\"nofollow\">MelGAN</a>'s generator!</li>\n</ul>\n<h2>Requirements</h2>\n<p>This repository is tested on Ubuntu 16.04 with a GPU Titan V.</p>\n<ul>\n<li>Python 3.6+</li>\n<li>Cuda 10.0</li>\n<li>CuDNN 7+</li>\n<li>NCCL 2+ (for distributed multi-gpu training)</li>\n<li>libsndfile (you can install via <code>sudo apt install libsndfile-dev</code> in ubuntu)</li>\n<li>jq (you can install via <code>sudo apt install jq</code> in ubuntu)</li>\n</ul>\n<p>Different cuda version should be working but not explicitly tested.<br>\nAll of the codes are tested on Pytorch 1.0.1, 1.1, 1.2, 1.3.1, 1.4 and 1.5.</p>\n<h2>Setup</h2>\n<p>You can select the installation method from two alternatives.</p>\n<h3>A. Use pip</h3>\n<pre>$ git clone https://github.com/kan-bayashi/ParallelWaveGAN.git\n$ <span class=\"nb\">cd</span> ParallelWaveGAN\n$ pip install -e .\n<span class=\"c1\"># If you want to use distributed training, please install</span>\n<span class=\"c1\"># apex manually by following https://github.com/NVIDIA/apex</span>\n$ ...\n</pre>\n<p>Note that your cuda version must be exactly matched with the version used for pytorch binary to install apex.<br>\nTo install pytorch compiled with different cuda version, see <code>tools/Makefile</code>.</p>\n<h3>B. Make virtualenv</h3>\n<pre>$ git clone https://github.com/kan-bayashi/ParallelWaveGAN.git\n$ <span class=\"nb\">cd</span> ParallelWaveGAN/tools\n$ make\n<span class=\"c1\"># If you want to use distributed training, please run following</span>\n<span class=\"c1\"># command to install apex.</span>\n$ make apex\n</pre>\n<p>Note that we specify cuda version used to compile pytorch wheel.<br>\nIf you want to use different cuda version, please check <code>tools/Makefile</code> to change the pytorch wheel to be installed.</p>\n<h2>Run</h2>\n<p>This repository provides <a href=\"https://github.com/kaldi-asr/kaldi\" rel=\"nofollow\">Kaldi</a>-style recipes, as the same as <a href=\"https://github.com/espnet/espnet\" rel=\"nofollow\">ESPnet</a>.<br>\nCurrently, the following recipes are supported.</p>\n<ul>\n<li><a href=\"https://keithito.com/LJ-Speech-Dataset/\" rel=\"nofollow\">LJSpeech</a>: English female speaker</li>\n<li><a href=\"https://sites.google.com/site/shinnosuketakamichi/publication/jsut\" rel=\"nofollow\">JSUT</a>: Japanese female speaker</li>\n<li><a href=\"https://www.data-baker.com/open_source.html\" rel=\"nofollow\">CSMSC</a>: Mandarin female speaker</li>\n<li><a href=\"http://www.festvox.org/cmu_arctic/\" rel=\"nofollow\">CMU Arctic</a>: English speakers</li>\n<li><a href=\"http://research.nii.ac.jp/src/en/JNAS.html\" rel=\"nofollow\">JNAS</a>: Japanese multi-speaker</li>\n<li><a href=\"https://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html\" rel=\"nofollow\">VCTK</a>: English multi-speaker</li>\n<li><a href=\"https://arxiv.org/abs/1904.02882\" rel=\"nofollow\">LibriTTS</a>: English multi-speaker</li>\n</ul>\n<p>To run the recipe, please follow the below instruction.</p>\n<pre><span class=\"c1\"># Let us move on the recipe directory</span>\n$ <span class=\"nb\">cd</span> egs/ljspeech/voc1\n\n<span class=\"c1\"># Run the recipe from scratch</span>\n$ ./run.sh\n\n<span class=\"c1\"># You can change config via command line</span>\n$ ./run.sh --conf &lt;your_customized_yaml_config&gt;\n\n<span class=\"c1\"># You can select the stage to start and stop</span>\n$ ./run.sh --stage <span class=\"m\">2</span> --stop_stage <span class=\"m\">2</span>\n\n<span class=\"c1\"># If you want to specify the gpu</span>\n$ <span class=\"nv\">CUDA_VISIBLE_DEVICES</span><span class=\"o\">=</span><span class=\"m\">1</span> ./run.sh --stage <span class=\"m\">2</span>\n\n<span class=\"c1\"># If you want to resume training from 10000 steps checkpoint</span>\n$ ./run.sh --stage <span class=\"m\">2</span> --resume &lt;path&gt;/&lt;to&gt;/checkpoint-10000steps.pkl\n</pre>\n<p>The integration with job schedulers such as <a href=\"https://slurm.schedmd.com/documentation.html\" rel=\"nofollow\">slurm</a> can be done via <code>cmd.sh</code> and  <code>conf/slurm.conf</code>.<br>\nIf you want to use it, please check <a href=\"https://kaldi-asr.org/doc/queue.html\" rel=\"nofollow\">this page</a>.</p>\n<p>All of the hyperparameters is written in a single yaml format configuration file.<br>\nPlease check <a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/parallel_wavegan.v1.yaml\" rel=\"nofollow\">this example</a> in ljspeech recipe.</p>\n<p>The training requires ~3 days with a single GPU (TITAN V).<br>\nThe speed of the training is 0.5 seconds per an iteration, in total ~ 200000 sec (= 2.31 days).<br>\nYou can monitor the training progress via tensorboard.</p>\n<pre>$ tensorboard --logdir exp\n</pre>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1de80d4a94cdccfdd3b37a2b8cb8ab9d94a54476/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f32323737393831332f36383130303038302d35386262633530302d666630392d313165392d393934352d6338333531383666643763322e706e67\"></p>\n<p>If you want to accelerate the training, you can try distributed multi-gpu training based on apex.<br>\nYou need to install apex for distributed training. Please make sure you already installed it.<br>\nThen you can run distributed multi-gpu training via following command:</p>\n<pre><span class=\"c1\"># in the case of the number of gpus = 8</span>\n$ <span class=\"nv\">CUDA_VISIBLE_DEVICES</span><span class=\"o\">=</span><span class=\"s2\">\"0,1,2,3,4,5,6,7\"</span> ./run.sh --stage <span class=\"m\">2</span> --n_gpus <span class=\"m\">8</span>\n</pre>\n<p>In the case of distributed training, batch size will be automatically multiplied by the number of gpus.<br>\nPlease be careful.</p>\n<p>The decoding speed is RTF = 0.016 with TITAN V, much faster than the real-time.</p>\n<pre><span class=\"o\">[</span>decode<span class=\"o\">]</span>: <span class=\"m\">100</span>%<span class=\"p\">|</span>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588<span class=\"p\">|</span> <span class=\"m\">250</span>/250 <span class=\"o\">[</span><span class=\"m\">00</span>:30&lt;<span class=\"m\">00</span>:00,  <span class=\"m\">8</span>.31it/s, <span class=\"nv\">RTF</span><span class=\"o\">=</span><span class=\"m\">0</span>.0156<span class=\"o\">]</span>\n<span class=\"m\">2019</span>-11-03 <span class=\"m\">09</span>:07:40,480 <span class=\"o\">(</span>decode:127<span class=\"o\">)</span> INFO: finished generation of <span class=\"m\">250</span> utterances <span class=\"o\">(</span><span class=\"nv\">RTF</span> <span class=\"o\">=</span> <span class=\"m\">0</span>.016<span class=\"o\">)</span>.\n</pre>\n<p>Even on the CPU (Intel(R) Xeon(R) Gold 6154 CPU @ 3.00GHz 16 threads), it can generate less than the real-time.</p>\n<pre><span class=\"o\">[</span>decode<span class=\"o\">]</span>: <span class=\"m\">100</span>%<span class=\"p\">|</span>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588<span class=\"p\">|</span> <span class=\"m\">250</span>/250 <span class=\"o\">[</span><span class=\"m\">22</span>:16&lt;<span class=\"m\">00</span>:00,  <span class=\"m\">5</span>.35s/it, <span class=\"nv\">RTF</span><span class=\"o\">=</span><span class=\"m\">0</span>.841<span class=\"o\">]</span>\n<span class=\"m\">2019</span>-11-06 <span class=\"m\">09</span>:04:56,697 <span class=\"o\">(</span>decode:129<span class=\"o\">)</span> INFO: finished generation of <span class=\"m\">250</span> utterances <span class=\"o\">(</span><span class=\"nv\">RTF</span> <span class=\"o\">=</span> <span class=\"m\">0</span>.734<span class=\"o\">)</span>.\n</pre>\n<p>If you use MelGAN's generator, the decoding speed will be further faster.</p>\n<pre><span class=\"c1\"># On CPU (Intel(R) Xeon(R) Gold 6154 CPU @ 3.00GHz 16 threads)</span>\n<span class=\"o\">[</span>decode<span class=\"o\">]</span>: <span class=\"m\">100</span>%<span class=\"p\">|</span>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588<span class=\"p\">|</span> <span class=\"m\">250</span>/250 <span class=\"o\">[</span><span class=\"m\">04</span>:00&lt;<span class=\"m\">00</span>:00,  <span class=\"m\">1</span>.04it/s, <span class=\"nv\">RTF</span><span class=\"o\">=</span><span class=\"m\">0</span>.0882<span class=\"o\">]</span>\n<span class=\"m\">2020</span>-02-08 <span class=\"m\">10</span>:45:14,111 <span class=\"o\">(</span>decode:142<span class=\"o\">)</span> INFO: Finished generation of <span class=\"m\">250</span> utterances <span class=\"o\">(</span><span class=\"nv\">RTF</span> <span class=\"o\">=</span> <span class=\"m\">0</span>.137<span class=\"o\">)</span>.\n\n<span class=\"c1\"># On GPU (TITAN V)</span>\n<span class=\"o\">[</span>decode<span class=\"o\">]</span>: <span class=\"m\">100</span>%<span class=\"p\">|</span>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588<span class=\"p\">|</span> <span class=\"m\">250</span>/250 <span class=\"o\">[</span><span class=\"m\">00</span>:06&lt;<span class=\"m\">00</span>:00, <span class=\"m\">36</span>.38it/s, <span class=\"nv\">RTF</span><span class=\"o\">=</span><span class=\"m\">0</span>.00189<span class=\"o\">]</span>\n<span class=\"m\">2020</span>-02-08 <span class=\"m\">05</span>:44:42,231 <span class=\"o\">(</span>decode:142<span class=\"o\">)</span> INFO: Finished generation of <span class=\"m\">250</span> utterances <span class=\"o\">(</span><span class=\"nv\">RTF</span> <span class=\"o\">=</span> <span class=\"m\">0</span>.002<span class=\"o\">)</span>.\n</pre>\n<p>If you want to accelerate the inference more, it is worthwhile to try the conversion from pytorch to tensorflow.<br>\nThe example of the conversion is available in <a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/notebooks/convert_melgan_from_pytorch_to_tensorflow.ipynb\" rel=\"nofollow\">the notebook</a> (Provided by <a href=\"https://github.com/dathudeptrai\" rel=\"nofollow\">@dathudeptrai</a>).</p>\n<h2>Results</h2>\n<p>Here the results are summarized in the table.<br>\nYou can listen to the samples and download pretrained models from the link to our google drive.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Model</th>\n<th align=\"center\">Conf</th>\n<th align=\"center\">Lang</th>\n<th align=\"center\">Fs [Hz]</th>\n<th align=\"center\">Mel range [Hz]</th>\n<th align=\"center\">FFT / Hop / Win [pt]</th>\n<th align=\"center\"># iters</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1wdHr1a51TLeo4iKrGErVKHVFyq6D17TU\" rel=\"nofollow\">ljspeech_parallel_wavegan.v1</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/parallel_wavegan.v1.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">400k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1XRn3s_wzPF2fdfGshLwuvNHrbgD0hqVS\" rel=\"nofollow\">ljspeech_parallel_wavegan.v1.long</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/parallel_wavegan.v1.long.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">1000k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1NoD3TCmKIDHHtf74YsScX8s59aZFOFJA\" rel=\"nofollow\">ljspeech_parallel_wavegan.v1.no_limit</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/parallel_wavegan.v1.no_limit.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">None</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">400k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1a5Q2KiJfUQkVFo5Bd1IoYPVicJGnm7EL\" rel=\"nofollow\">ljspeech_parallel_wavegan.v3</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/parallel_wavegan.v3.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">3000k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1z0vO1UMFHyeCdCLAmd7Moewi4QgCb07S\" rel=\"nofollow\">ljspeech_melgan.v1</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/melgan.v1.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">400k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1RqNGcFO7Geb6-4pJtMbC9-ph_WiWA14e\" rel=\"nofollow\">ljspeech_melgan.v1.long</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/melgan.v1.long.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">1000k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1KQt-gyxbG6iTZ4aVn9YjQuaGYjAleYs8\" rel=\"nofollow\">ljspeech_melgan_large.v1</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/melgan_large.v1.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">400k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1ogEx-wiQS7HVtdU0_TmlENURIe4v2erC\" rel=\"nofollow\">ljspeech_melgan_large.v1.long</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/melgan_large.v1.long.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">1000k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1eXkm_Wf1YVlk5waP4Vgqd0GzMaJtW3y5\" rel=\"nofollow\">ljspeech_melgan.v3</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/melgan.v3.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">2000k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1u1w4RPefjByX8nfsL59OzU2KgEksBhL1\" rel=\"nofollow\">ljspeech_melgan.v3.long</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/ljspeech/voc1/conf/melgan.v3.long.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">22.05k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">4000k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1UDRL0JAovZ8XZhoH0wi9jj_zeCKb-AIA\" rel=\"nofollow\">jsut_parallel_wavegan.v1</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/jsut/voc1/conf/parallel_wavegan.v1.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">JP</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"center\">400k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1C2nu9nOFdKcEd-D9xGquQ0bCia0B2v_4\" rel=\"nofollow\">csmsc_parallel_wavegan.v1</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/csmsc/voc1/conf/parallel_wavegan.v1.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">ZH</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"center\">400k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1xG9CmSED2TzFdklD6fVxzf7kFV2kPQAJ\" rel=\"nofollow\">arctic_slt_parallel_wavegan.v1</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/arctic/voc1/conf/parallel_wavegan.v1.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">16k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">400k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1n_hkxPxryVXbp6oHM1NFm08q0TcoDXz1\" rel=\"nofollow\">jnas_parallel_wavegan.v1</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/jnas/voc1/conf/parallel_wavegan.v1.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">JP</td>\n<td align=\"center\">16k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">1024 / 256 / None</td>\n<td align=\"center\">400k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1dGTu-B7an2P5sEOepLPjpOaasgaSnLpi\" rel=\"nofollow\">vctk_parallel_wavegan.v1</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/vctk/voc1/conf/parallel_wavegan.v1.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"center\">400k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1qoocM-VQZpjbv5B-zVJpdraazGcPL0So\" rel=\"nofollow\">vctk_parallel_wavegan.v1.long</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/vctk/voc1/conf/parallel_wavegan.v1.long.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"center\">1000k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=1pb18Nd2FCYWnXfStszBAEEIMe_EZUJV0\" rel=\"nofollow\">libritts_parallel_wavegan.v1 (New!)</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/libritts/voc1/conf/parallel_wavegan.v1.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"center\">400k</td>\n</tr>\n<tr>\n<td align=\"left\"><a href=\"https://drive.google.com/open?id=15ibzv-uTeprVpwT946Hl1XUYDmg5Afwz\" rel=\"nofollow\">libritts_parallel_wavegan.v1.long (New!)</a></td>\n<td align=\"center\"><a href=\"https://github.com/kan-bayashi/ParallelWaveGAN/blob/master/egs/libritts/voc1/conf/parallel_wavegan.v1.long.yaml\" rel=\"nofollow\">link</a></td>\n<td align=\"center\">EN</td>\n<td align=\"center\">24k</td>\n<td align=\"center\">80-7600</td>\n<td align=\"center\">2048 / 300 / 1200</td>\n<td align=\"center\">1000k</td>\n</tr></tbody></table>\n<p>If you want to check more results, please access at <a href=\"https://drive.google.com/open?id=1sd_QzcUNnbiaWq7L0ykMP7Xmk-zOuxTi\" rel=\"nofollow\">our google drive</a>.</p>\n<h2>How-to-use pretrained models</h2>\n<p>Here the minimal code is shown to perform analysis-synthesis using the pretrained model.</p>\n<pre><span class=\"c1\"># Please make sure you installed `parallel_wavegan`</span>\n<span class=\"c1\"># If not, please install via pip</span>\n$ pip install parallel_wavegan\n\n<span class=\"c1\"># Please download pretrained models and put them in `pretrain_model` directory</span>\n$ ls pretrain_model\n\uf016  checkpoint-400000steps.pkl  \uf481  config.yml  \uf016  stats.h5\n\n<span class=\"c1\"># Please put an audio file in `sample` directory to perform analysis-synthesis</span>\n$ ls sample/\n\uf001  sample.wav\n\n<span class=\"c1\"># Then perform feature extraction -&gt; feature normalization -&gt; sysnthesis</span>\n$ parallel-wavegan-preprocess <span class=\"se\">\\</span>\n\t--config pretrain_model/config.yml <span class=\"se\">\\</span>\n\t--rootdir sample <span class=\"se\">\\</span>\n\t--dumpdir dump/sample/raw\n<span class=\"m\">100</span>%<span class=\"p\">|</span>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588<span class=\"p\">|</span> <span class=\"m\">1</span>/1 <span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00, <span class=\"m\">914</span>.19it/s<span class=\"o\">]</span>\n<span class=\"o\">[</span>Parallel<span class=\"o\">(</span><span class=\"nv\">n_jobs</span><span class=\"o\">=</span><span class=\"m\">16</span><span class=\"o\">)]</span>: Using backend LokyBackend with <span class=\"m\">16</span> concurrent workers.\n<span class=\"o\">[</span>Parallel<span class=\"o\">(</span><span class=\"nv\">n_jobs</span><span class=\"o\">=</span><span class=\"m\">16</span><span class=\"o\">)]</span>: Done   <span class=\"m\">1</span> out of   <span class=\"m\">1</span> <span class=\"p\">|</span> elapsed:    <span class=\"m\">1</span>.2s finished\n$ parallel-wavegan-normalize <span class=\"se\">\\</span>\n\t--config pretrain_model/config.yml <span class=\"se\">\\</span>\n\t--rootdir dump/sample/raw <span class=\"se\">\\</span>\n\t--dumpdir dump/sample/norm <span class=\"se\">\\</span>\n\t--stats pretrain_model/stats.h5\n<span class=\"m\">2019</span>-11-13 <span class=\"m\">13</span>:44:29,574 <span class=\"o\">(</span>normalize:87<span class=\"o\">)</span> INFO: the number of <span class=\"nv\">files</span> <span class=\"o\">=</span> <span class=\"m\">1</span>.\n<span class=\"m\">100</span>%<span class=\"p\">|</span>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588<span class=\"p\">|</span> <span class=\"m\">1</span>/1 <span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00, <span class=\"m\">513</span>.13it/s<span class=\"o\">]</span>\n<span class=\"o\">[</span>Parallel<span class=\"o\">(</span><span class=\"nv\">n_jobs</span><span class=\"o\">=</span><span class=\"m\">16</span><span class=\"o\">)]</span>: Using backend LokyBackend with <span class=\"m\">16</span> concurrent workers.\n<span class=\"o\">[</span>Parallel<span class=\"o\">(</span><span class=\"nv\">n_jobs</span><span class=\"o\">=</span><span class=\"m\">16</span><span class=\"o\">)]</span>: Done   <span class=\"m\">1</span> out of   <span class=\"m\">1</span> <span class=\"p\">|</span> elapsed:    <span class=\"m\">0</span>.6s finished\n$ parallel-wavegan-decode <span class=\"se\">\\</span>\n\t--checkpoint pretrain_model/checkpoint-400000steps.pkl <span class=\"se\">\\</span>\n\t--dumpdir dump/sample/norm <span class=\"se\">\\</span>\n\t--outdir sample\n<span class=\"m\">2019</span>-11-13 <span class=\"m\">13</span>:44:31,229 <span class=\"o\">(</span>decode:91<span class=\"o\">)</span> INFO: the number of features to be <span class=\"nv\">decoded</span> <span class=\"o\">=</span> <span class=\"m\">1</span>.\n<span class=\"m\">2019</span>-11-13 <span class=\"m\">13</span>:44:37,074 <span class=\"o\">(</span>decode:105<span class=\"o\">)</span> INFO: loaded model parameters from pretrain_model/checkpoint-400000steps.pkl.\n<span class=\"o\">[</span>decode<span class=\"o\">]</span>: <span class=\"m\">100</span>%<span class=\"p\">|</span>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588<span class=\"p\">|</span> <span class=\"m\">1</span>/1 <span class=\"o\">[</span><span class=\"m\">00</span>:00&lt;<span class=\"m\">00</span>:00, <span class=\"m\">18</span>.33it/s, <span class=\"nv\">RTF</span><span class=\"o\">=</span><span class=\"m\">0</span>.0146<span class=\"o\">]</span>\n<span class=\"m\">2019</span>-11-13 <span class=\"m\">13</span>:44:37,132 <span class=\"o\">(</span>decode:129<span class=\"o\">)</span> INFO: finished generation of <span class=\"m\">1</span> utterances <span class=\"o\">(</span><span class=\"nv\">RTF</span> <span class=\"o\">=</span> <span class=\"m\">0</span>.015<span class=\"o\">)</span>.\n\n<span class=\"c1\"># you can find the generated speech in `sample` directory</span>\n$ ls sample\n\uf001  sample.wav  \uf001  sample_gen.wav\n</pre>\n<p>If you want to combine with TTS models, you can try the realtime demonstraion in Google Colab!</p>\n<p><a href=\"https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></p>\n<h2>References</h2>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1910.11480\" rel=\"nofollow\">Parallel WaveGAN</a></li>\n<li><a href=\"https://github.com/r9y9/wavenet_vocoder\" rel=\"nofollow\">r9y9/wavenet_vocoder</a></li>\n<li><a href=\"https://github.com/LiyuanLucasLiu/RAdam\" rel=\"nofollow\">LiyuanLucasLiu/RAdam</a></li>\n<li><a href=\"https://arxiv.org/abs/1910.06711\" rel=\"nofollow\">MelGAN</a></li>\n<li><a href=\"https://github.com/descriptinc/melgan-neurips\" rel=\"nofollow\">descriptinc/melgan-neurips</a></li>\n</ul>\n<h2>Acknowledgement</h2>\n<p>The author would like to thank Ryuichi Yamamoto (<a href=\"https://github.com/r9y9\" rel=\"nofollow\">@r9y9</a>) for his great repository, paper and valuable discussions.</p>\n<h2>Author</h2>\n<p>Tomoki Hayashi (<a href=\"https://github.com/kan-bayashi\" rel=\"nofollow\">@kan-bayashi</a>)<br>\nE-mail: <code>hayashi.tomoki&lt;at&gt;g.sp.m.is.nagoya-u.ac.jp</code></p>\n\n          </div>"}, "last_serial": 7091572, "releases": {"0.2.1": [{"comment_text": "", "digests": {"md5": "cab57fd5176f234f44326778679feefb", "sha256": "73a0726fdda89379e667bfbff9d35853b331cc753248912331b3c8010f593194"}, "downloads": -1, "filename": "parallel_wavegan-0.2.1.tar.gz", "has_sig": false, "md5_digest": "cab57fd5176f234f44326778679feefb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13241, "upload_time": "2019-11-04T01:43:29", "upload_time_iso_8601": "2019-11-04T01:43:29.352054Z", "url": "https://files.pythonhosted.org/packages/f1/40/4d8142efd3bcd4519753a2c17d154efae4abe858ea8ab9dccd07d9443a38/parallel_wavegan-0.2.1.tar.gz", "yanked": false}], "0.2.1.post2": [{"comment_text": "", "digests": {"md5": "454da469163b58a91b984462766d2a13", "sha256": "3e72502356603d517bb14935ca581181b70f7d91af1e7cd5afa1b91bda2be3e8"}, "downloads": -1, "filename": "parallel_wavegan-0.2.1.post2.tar.gz", "has_sig": false, "md5_digest": "454da469163b58a91b984462766d2a13", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21602, "upload_time": "2019-11-04T02:23:30", "upload_time_iso_8601": "2019-11-04T02:23:30.104146Z", "url": "https://files.pythonhosted.org/packages/ad/0f/43b3af9c253514bec377f6ab17cdaf5ad4626c09c009dd2c68f355dba62c/parallel_wavegan-0.2.1.post2.tar.gz", "yanked": false}], "0.2.1.post3": [{"comment_text": "", "digests": {"md5": "03d9946e8dbddcc73ce006ac1a3ee51a", "sha256": "29f21bebe78f1bc1782818306dfdc6e182bdee6a178b57de6a18761961798432"}, "downloads": -1, "filename": "parallel_wavegan-0.2.1.post3.tar.gz", "has_sig": false, "md5_digest": "03d9946e8dbddcc73ce006ac1a3ee51a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22076, "upload_time": "2019-11-05T01:54:57", "upload_time_iso_8601": "2019-11-05T01:54:57.232749Z", "url": "https://files.pythonhosted.org/packages/bb/13/fd16ce23b8e17d784e939ad8dae8fe16e17f10bf191ba22f3e3083c1a089/parallel_wavegan-0.2.1.post3.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "4b31308371be32d58aa5163c4167e3f1", "sha256": "3e9c4c03e5cd454eb024689540aecc92586307efd8560346fa606c26aabda8ea"}, "downloads": -1, "filename": "parallel_wavegan-0.2.2.tar.gz", "has_sig": false, "md5_digest": "4b31308371be32d58aa5163c4167e3f1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22421, "upload_time": "2019-11-05T08:13:20", "upload_time_iso_8601": "2019-11-05T08:13:20.294571Z", "url": "https://files.pythonhosted.org/packages/fe/b6/1b28407c2c80b15272e44d250e1ff7edc57e4dd1096e8aacce7f1db1f37a/parallel_wavegan-0.2.2.tar.gz", "yanked": false}], "0.2.2.post1": [{"comment_text": "", "digests": {"md5": "4e4fad55a36d4ed672069db0b394bafe", "sha256": "cc199481dc795f2f4dc50b63077b7d8cc0a01a61600c0330492d62999fc6b8d0"}, "downloads": -1, "filename": "parallel_wavegan-0.2.2.post1.tar.gz", "has_sig": false, "md5_digest": "4e4fad55a36d4ed672069db0b394bafe", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22435, "upload_time": "2019-11-06T02:25:09", "upload_time_iso_8601": "2019-11-06T02:25:09.561799Z", "url": "https://files.pythonhosted.org/packages/31/03/9a8cb0253272d83ae43b494e55016b37fd6100bd3863d89308e0951bc7be/parallel_wavegan-0.2.2.post1.tar.gz", "yanked": false}], "0.2.3": [{"comment_text": "", "digests": {"md5": "58afcb2c0ee49c26530be7d40626c298", "sha256": "aee3272b7c68502ce26714bd66c444069f659d9209f0f875dd7ccabdad020a96"}, "downloads": -1, "filename": "parallel_wavegan-0.2.3.tar.gz", "has_sig": false, "md5_digest": "58afcb2c0ee49c26530be7d40626c298", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22818, "upload_time": "2019-11-08T08:05:32", "upload_time_iso_8601": "2019-11-08T08:05:32.798308Z", "url": "https://files.pythonhosted.org/packages/ef/bc/5df47e06260272f42cb7275a4a8a8494468d5bef55c2bb17a546266426ef/parallel_wavegan-0.2.3.tar.gz", "yanked": false}], "0.2.4": [{"comment_text": "", "digests": {"md5": "796237242b3ff14fffb8b977c24b25d7", "sha256": "1514a699d46a6a14a20b90668d2c5f3c1025e1a7757d2982f232620708e4ef2b"}, "downloads": -1, "filename": "parallel_wavegan-0.2.4.tar.gz", "has_sig": false, "md5_digest": "796237242b3ff14fffb8b977c24b25d7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25882, "upload_time": "2019-11-11T16:27:47", "upload_time_iso_8601": "2019-11-11T16:27:47.572238Z", "url": "https://files.pythonhosted.org/packages/d8/35/d2247b4012d3cce0db1bd33adc30b550ec289d45eca7dad874ec8b9b8a7e/parallel_wavegan-0.2.4.tar.gz", "yanked": false}], "0.2.4.post1": [{"comment_text": "", "digests": {"md5": "9c878e2f27d2162e2731e0fb504982e6", "sha256": "b89ec349693f51c96ebc6ee0c735d2790456d19c816024391473a101fc4d39ad"}, "downloads": -1, "filename": "parallel_wavegan-0.2.4.post1.tar.gz", "has_sig": false, "md5_digest": "9c878e2f27d2162e2731e0fb504982e6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26285, "upload_time": "2019-11-12T04:02:29", "upload_time_iso_8601": "2019-11-12T04:02:29.338155Z", "url": "https://files.pythonhosted.org/packages/fc/a6/cd37560884be16cd63cedae0cf2970ddf0f1523a84e9e035484e225f7a10/parallel_wavegan-0.2.4.post1.tar.gz", "yanked": false}], "0.2.4.post2": [{"comment_text": "", "digests": {"md5": "500cb2445b4b5ad5bbd558c7c1a9c53e", "sha256": "5de11a4e55a4456bebd85561cc1ba573efdd8b7f21123d8e13530a61211cfdd5"}, "downloads": -1, "filename": "parallel_wavegan-0.2.4.post2.tar.gz", "has_sig": false, "md5_digest": "500cb2445b4b5ad5bbd558c7c1a9c53e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26194, "upload_time": "2019-11-12T04:08:56", "upload_time_iso_8601": "2019-11-12T04:08:56.100607Z", "url": "https://files.pythonhosted.org/packages/8d/82/af68f104054cbe5638a12b69cf6295dd3411287e4381fc8581a6c183aa97/parallel_wavegan-0.2.4.post2.tar.gz", "yanked": false}], "0.2.5": [{"comment_text": "", "digests": {"md5": "9d851de4a76495427f7d2777a9f18c5b", "sha256": "4f7448ae5105946bbd947dee25e17e10cb7f310050a47b5bd1aef706e06ed06c"}, "downloads": -1, "filename": "parallel_wavegan-0.2.5.tar.gz", "has_sig": false, "md5_digest": "9d851de4a76495427f7d2777a9f18c5b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 27839, "upload_time": "2019-11-13T06:17:56", "upload_time_iso_8601": "2019-11-13T06:17:56.737509Z", "url": "https://files.pythonhosted.org/packages/9b/d8/22260afeb6a66c327e4126d3c16079fdc908549e8aebfd69493df0a8d090/parallel_wavegan-0.2.5.tar.gz", "yanked": false}], "0.2.5.post1": [{"comment_text": "", "digests": {"md5": "892553a5a32c1271a80e63a8f54e3c78", "sha256": "3328a5e1debe15269ef4aa60b10c35e427e63ada4b31eee9cddd93b786598613"}, "downloads": -1, "filename": "parallel_wavegan-0.2.5.post1.tar.gz", "has_sig": false, "md5_digest": "892553a5a32c1271a80e63a8f54e3c78", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 27837, "upload_time": "2019-11-24T01:31:42", "upload_time_iso_8601": "2019-11-24T01:31:42.560400Z", "url": "https://files.pythonhosted.org/packages/44/87/52d89ff1697e30acbbe00b1d3380903e1731b4d886fa095eb25263d86779/parallel_wavegan-0.2.5.post1.tar.gz", "yanked": false}], "0.2.5.post2": [{"comment_text": "", "digests": {"md5": "f0e2c7e9ce2d3d0d9750061164d5b8c5", "sha256": "58d0667c9d4b28c0cacc2630a957a688bf738dfc045fe24712588ccfd9fcae4f"}, "downloads": -1, "filename": "parallel_wavegan-0.2.5.post2.tar.gz", "has_sig": false, "md5_digest": "f0e2c7e9ce2d3d0d9750061164d5b8c5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 27846, "upload_time": "2019-11-25T03:03:20", "upload_time_iso_8601": "2019-11-25T03:03:20.994866Z", "url": "https://files.pythonhosted.org/packages/73/f5/df7491ac1d0958776320b227c01fef9ae54855379863a6189f758fb13966/parallel_wavegan-0.2.5.post2.tar.gz", "yanked": false}], "0.2.6": [{"comment_text": "", "digests": {"md5": "b9365c1aa0c8e7b12133e435a8821b17", "sha256": "cc4f3e826a054475a9a87058cd88b8ed78aa72f371bb340fe59c61e3420afaac"}, "downloads": -1, "filename": "parallel_wavegan-0.2.6.tar.gz", "has_sig": false, "md5_digest": "b9365c1aa0c8e7b12133e435a8821b17", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28573, "upload_time": "2019-12-25T09:26:42", "upload_time_iso_8601": "2019-12-25T09:26:42.797549Z", "url": "https://files.pythonhosted.org/packages/82/fa/56fe659428f00f6b728ad54b7a864ee57bac5837acf4c82be703f3bb1280/parallel_wavegan-0.2.6.tar.gz", "yanked": false}], "0.2.6.post1": [{"comment_text": "", "digests": {"md5": "20cef8bb1d0158f818e13e2f5efe85d1", "sha256": "a569a044d132eb53c44affa3d54d9b6519f5c10e4e6244d76ad58b97b02d31f1"}, "downloads": -1, "filename": "parallel_wavegan-0.2.6.post1.tar.gz", "has_sig": false, "md5_digest": "20cef8bb1d0158f818e13e2f5efe85d1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28545, "upload_time": "2019-12-26T05:12:24", "upload_time_iso_8601": "2019-12-26T05:12:24.691141Z", "url": "https://files.pythonhosted.org/packages/dc/4c/866ea57f4c29c1110518556fd78ab93385f33134d6f1cbbd5b65aaeebbc9/parallel_wavegan-0.2.6.post1.tar.gz", "yanked": false}], "0.2.7": [{"comment_text": "", "digests": {"md5": "fc46d53524d1a480c24e7ebff3db95ec", "sha256": "574275f5512882809f0207793acaa32ec79a7e377290a97578b1a091b1a81a81"}, "downloads": -1, "filename": "parallel_wavegan-0.2.7.tar.gz", "has_sig": false, "md5_digest": "fc46d53524d1a480c24e7ebff3db95ec", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29427, "upload_time": "2020-01-14T06:21:38", "upload_time_iso_8601": "2020-01-14T06:21:38.233017Z", "url": "https://files.pythonhosted.org/packages/e6/81/555f0370413ce731276f3b802fc5df070fd2b72e79314a20647ce0e361f3/parallel_wavegan-0.2.7.tar.gz", "yanked": false}], "0.2.7.post1": [{"comment_text": "", "digests": {"md5": "43daae9ddafbb8118a032511fc5a52a5", "sha256": "4ec00727a8329f73c5d61579876a9717b9f222e78467e2904ee65ef1c7713f2e"}, "downloads": -1, "filename": "parallel_wavegan-0.2.7.post1.tar.gz", "has_sig": false, "md5_digest": "43daae9ddafbb8118a032511fc5a52a5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31289, "upload_time": "2020-01-14T07:03:52", "upload_time_iso_8601": "2020-01-14T07:03:52.029664Z", "url": "https://files.pythonhosted.org/packages/e0/04/e862485d2bd4ba6fc0854e1838dc0f4267a06ae0c9f1f39f307143edb67f/parallel_wavegan-0.2.7.post1.tar.gz", "yanked": false}], "0.2.8.post1": [{"comment_text": "", "digests": {"md5": "23be022d8d7ad2531c3d26fbd47974c2", "sha256": "2f0cb5821495dc35e299518b44701c93f516a73c7bb827e7bad5b9434e5e9467"}, "downloads": -1, "filename": "parallel_wavegan-0.2.8.post1.tar.gz", "has_sig": false, "md5_digest": "23be022d8d7ad2531c3d26fbd47974c2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 34349, "upload_time": "2020-02-08T03:15:22", "upload_time_iso_8601": "2020-02-08T03:15:22.433698Z", "url": "https://files.pythonhosted.org/packages/4a/a9/0099fd305180acaf59e9eff75087360d02eb64271c9188684cb1c264955b/parallel_wavegan-0.2.8.post1.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "aa656af018c8ce3dc8b3dc75f632b56b", "sha256": "35456b0287e5df91c8508de51e8e339d7616ce3273fba0bb66ce95048105d264"}, "downloads": -1, "filename": "parallel_wavegan-0.3.0.tar.gz", "has_sig": false, "md5_digest": "aa656af018c8ce3dc8b3dc75f632b56b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 36869, "upload_time": "2020-02-15T10:43:30", "upload_time_iso_8601": "2020-02-15T10:43:30.599597Z", "url": "https://files.pythonhosted.org/packages/92/3a/7be6395952d77b6fac953d422e58a45ccdde59ff097ef34013cfc2f95fdd/parallel_wavegan-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "7e7edcc00bc34ba4dd0d1cc47d6886e5", "sha256": "19105c404db2485f564de1311ddb366a8728152e549e6017f76cc7f4159c4ff4"}, "downloads": -1, "filename": "parallel_wavegan-0.3.1.tar.gz", "has_sig": false, "md5_digest": "7e7edcc00bc34ba4dd0d1cc47d6886e5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 37804, "upload_time": "2020-02-26T14:40:58", "upload_time_iso_8601": "2020-02-26T14:40:58.882357Z", "url": "https://files.pythonhosted.org/packages/cb/8d/2db623b5ed19180746a3595b9567ac10663fd4dd779fbdc72166c5680f84/parallel_wavegan-0.3.1.tar.gz", "yanked": false}], "0.3.1.post1": [{"comment_text": "", "digests": {"md5": "7925b1f0571da8b04dd4e3b3da990bcf", "sha256": "aebd1922128924a0015382eb4430f05cf83218d3df23009fd8a0eea9ca812ef6"}, "downloads": -1, "filename": "parallel_wavegan-0.3.1.post1.tar.gz", "has_sig": false, "md5_digest": "7925b1f0571da8b04dd4e3b3da990bcf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 37821, "upload_time": "2020-02-28T14:38:31", "upload_time_iso_8601": "2020-02-28T14:38:31.579813Z", "url": "https://files.pythonhosted.org/packages/db/40/245b61bc8bc78c4ddeda585be21b7edbe8e326bed71f39269fa541a35a4c/parallel_wavegan-0.3.1.post1.tar.gz", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "41c384836321ede009bdacfc2b04ccf2", "sha256": "2f78afc4b08544849b2a3e1bf6f37b714f10a3852c1f3622fa75f583bb41952d"}, "downloads": -1, "filename": "parallel_wavegan-0.3.2.tar.gz", "has_sig": false, "md5_digest": "41c384836321ede009bdacfc2b04ccf2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 37708, "upload_time": "2020-03-06T18:20:06", "upload_time_iso_8601": "2020-03-06T18:20:06.880723Z", "url": "https://files.pythonhosted.org/packages/66/85/26ecb5f8169b468a298c7e7860967fd77890d56580c8661c245c85bedec7/parallel_wavegan-0.3.2.tar.gz", "yanked": false}], "0.3.3": [{"comment_text": "", "digests": {"md5": "cd232078dc130953eed4465f1a6b5478", "sha256": "3758cab564f69bf8d80694171dfba344b9d82c354561572d96f0589be1244635"}, "downloads": -1, "filename": "parallel_wavegan-0.3.3.tar.gz", "has_sig": false, "md5_digest": "cd232078dc130953eed4465f1a6b5478", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 38690, "upload_time": "2020-03-07T07:35:50", "upload_time_iso_8601": "2020-03-07T07:35:50.628488Z", "url": "https://files.pythonhosted.org/packages/34/61/b5540e9448974d2bf38a557e6d6b19abf616b98fae68db9931758ce26d37/parallel_wavegan-0.3.3.tar.gz", "yanked": false}], "0.3.4": [{"comment_text": "", "digests": {"md5": "6678ae0b967973df6a649b4528de5077", "sha256": "abe9608de3c387107d31164f5e5794befe3a75eb52130ef038f83644a6e50d09"}, "downloads": -1, "filename": "parallel_wavegan-0.3.4.tar.gz", "has_sig": false, "md5_digest": "6678ae0b967973df6a649b4528de5077", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39145, "upload_time": "2020-03-09T07:14:05", "upload_time_iso_8601": "2020-03-09T07:14:05.922777Z", "url": "https://files.pythonhosted.org/packages/5b/26/4281e991aaebc2a64e90a00e8ff3150a503863c4d2361b24f3b3f2480dfc/parallel_wavegan-0.3.4.tar.gz", "yanked": false}], "0.3.5": [{"comment_text": "", "digests": {"md5": "63553e3ec8031da74b3ebea602c2ec5f", "sha256": "13f8b025ab5601df6478ce053e5ed9f9fe8edff51f19150bd8b01a39694d9396"}, "downloads": -1, "filename": "parallel_wavegan-0.3.5.tar.gz", "has_sig": false, "md5_digest": "63553e3ec8031da74b3ebea602c2ec5f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 41812, "upload_time": "2020-04-24T10:27:38", "upload_time_iso_8601": "2020-04-24T10:27:38.449889Z", "url": "https://files.pythonhosted.org/packages/81/ba/7018a0c68064604d72fd362516a6697e3bead99c24ab36a210ec0d0e2b11/parallel_wavegan-0.3.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "63553e3ec8031da74b3ebea602c2ec5f", "sha256": "13f8b025ab5601df6478ce053e5ed9f9fe8edff51f19150bd8b01a39694d9396"}, "downloads": -1, "filename": "parallel_wavegan-0.3.5.tar.gz", "has_sig": false, "md5_digest": "63553e3ec8031da74b3ebea602c2ec5f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 41812, "upload_time": "2020-04-24T10:27:38", "upload_time_iso_8601": "2020-04-24T10:27:38.449889Z", "url": "https://files.pythonhosted.org/packages/81/ba/7018a0c68064604d72fd362516a6697e3bead99c24ab36a210ec0d0e2b11/parallel_wavegan-0.3.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:58:35 2020"}