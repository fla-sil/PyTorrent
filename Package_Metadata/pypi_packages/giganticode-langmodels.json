{"info": {"author": "giganticode", "author_email": "hlibbabii@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Environment :: Console", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Operating System :: MacOS :: MacOS X", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "## **langmodels**\n\n[![Build Status](https://travis-ci.org/giganticode/langmodels.svg?branch=master)](https://travis-ci.org/giganticode/langmodels)\n\nThis is a repository for **neural language models (LMs)** trained on a large corpus of source code \nand a toolkit to work with such models. \n\nYou could be interested in using this library if you want to:\n* Use existing pre-trained models for tasks such as autocompletion and bug prediction;\n* Use the pre-trained models for transfer transfer learning or further fine-tuning;\n* Train a model from scratch by choosing one of the wide range of corpus preprocessing choices, \n neural network (NN) architectures, and training options.\n\nThis project uses [fastai](https://www.fast.ai) and \n[pytorch](https://pytorch.org) libraries for NN training/inference. \nFor corpus preprocessing [giganticode-dataprep](https://github.com/giganticode/dataprep) is used.\n\n## Quick start\n\n### Prerequisites\n\n* Python version >= 3.6 required! \n\n### Installation\n\n```shell script\npip install giganticode-langmodels\n```\n\nOR to build from source:\n\n```\ngit clone https://github.com//giganticode/langmodels\ncd langmodels\npython -m venv langmodels-venv\nsource langmodels-venv/bin/activate\npip install -r requirements.txt\n```\n\n## Using existing pre-trained models\n### Loading a default pre-trained model\n```python\n>>> import langmodels.repository as repo\n>>> trained_model = repo.load_default_model()\n\n[langmodels.repository] INFO: Model is not found in cache. Downloading from https://www.inf.unibz.it/~hbabii/pretrained_models/langmodel-large-split_10k_2_1024_191007.112241_-_langmodel-large-split_10k_2_1024_191022.141344 ...\n[langmodels.model] DEBUG: Loading model from: /home/hlib/.local/share/langmodels/0.0.1/modelzoo/langmodel-large-split_10k_2_1024_191007.112241_-_langmodel-large-split_10k_2_1024_191022.141344/best.pth ...\n[langmodels.model] DEBUG: Using GPU for inference\n```\n\n### Other model loading options\n\n**To see which models are available, you can call `list_pretrained_models` function.**\n\nSet `cached` parameter to `True` (default is `False`) to display only cached LMs (e.g. if offline).\n```python\n>>> import langmodels.repository as repo\n>>> repo.list_pretrained_models(cached=False)\n\n  ID                                                                    BPE_MERGES  LAYERS_CONFIG  ARCH      BIN_ENTROPY    TRAINING_TIME_MINUTES_PER_EPOCH  N_EPOCHS  BEST_EPOCH  TAGS                 \n\n  langmodel-large-split_10k_2_1024_191007.112241_-_langmodel-large-spl  10k         1024/2/1024    AWD_LSTM  2.1455788479   1429                             6         5           ['BEST', 'DEFAULT']  \n  it_10k_2_1024_191022.141344                                                                                                                                                                           \n  langmodel-large-split_10k_3_1024_191007.112257_-_langmodel-large-spl  10k         512/3/1024     AWD_LSTM  2.14730056622  1432                             6         5           []                   \n  it_10k_3_1024_191022.134822                                                                                                                                                                           \n  langmodel-large-split_10k_2_2048_191007.112249_-_langmodel-large-spl  10k         512/2/2048     GRU       2.19923468325  1429                             6         5           []                   \n  it_10k_2_2048_191022.141335                                                                                                                                                                           \n  langmodel-large-split_10k_1_512_190926.120146                         10k         512/1/512      AWD_LSTM  2.69019493253  479                              9         8           ['MEDIUM']           \n  langmodel-small-split_10k_1_512_190906.154943                         10k         512/1/512      AWD_LSTM  4.73768141172  4                                19        18          ['TINY']             \n  dev_10k_1_10_190923.132328                                            10k         10/1/10        AWD_LSTM  9.15688191092  0                                0         -1          ['RANDOM']\n```\n\nUse `query_all_models` method to get a list of `ModelDescription` objects\n```python\n>>> import langmodels.repository as repo\n>>> repo.query_all_models()[0]\nModelDescription(id='langmodel-large-split_10k_2_1024_191007.112241_-_langmodel-large-split_10k_2_1024_191022.141344', bpe_merges='10k', layers_config='1024/2/1024', arch='AWD_LSTM', bin_entropy=2.1455788479, training_time_minutes_per_epoch=1429, n_epochs=6, best_epoch=5, tags=['BEST', 'DEFAULT'])\n```\n\n**A model can be loaded by tag or by id.**\n\nYou can specify if you want to load a model to CPU despite having cuda-supported GPU with `force_use_cpu` parameter \n(defaults to `False`). If cuda-supported GPU is not available, this parameter is disregarded.\n```python\n>>> trained_model = repo.load_model_with_tag('BEST')\n\n>>> trained_model = repo.load_model_by_id('dev_10k_1_10_190923.132328_new', force_use_cpu=True)\n```\n\nAlso, you can use a lower-level API to load a model by path :\n```python\ntrained_model = repo.load_from_path('/home/hlib/.local/share/langmodels/0.0.1/modelzoo/dev_10k_1_10_190923.132328_new')\n```\n\n## Inference\n### Autocompletion\n\nExample\n\n```python\n>>> import langmodels.repository as repo\n>>> trained_model = repo.load_default_model()\n>>> trained_model.feed_text('public static main() { if', extension='java')\n\n# this does not change the state of the model:\n>>> predictions = trained_model.predict_next_full_token(n_suggestions=5)\n[('(', 0.9334765834402862), ('.', 0.01540983953864937), ('=', 0.008939018331858162), (',', 0.005372771784601065), ('the', 0.00309070517292041)]\n\n# adding more context:\n>>> trained_model.feed_text('(', extension='java')\n>>> trained_model.predict_next_full_token(n_suggestions=3)\n[('(', 0.14554535082422237), ('c', 0.018005003646104294), ('!', 0.01614662429123089)]\n\n\n# resetting the state of the model (make it forget the context)\n>>> trained_model.reset()\n>>> trained_model.predict_next_full_token(n_suggestions=5)\n[('/', 0.7209196484717589), ('package', 0.27093282656897594), ('import', 0.0007366385365522241), ('.', 0.0005714365190590807), ('public', 0.0003926736567296)]\n\n```\n\n\n### Bug prediction based on per-line entropies evaluation\n\nAn LM can be used to calculate cross-entropies for each line of a file. High values can give an idea about \nunusual/suspicious chunks of code [[1]](#1).\n\nCheck section [LM Evaluation](#lm-evaluation) section to learn how to calculate \ncross-entropy for a project/file/string,\n\nCheck our [vsc plugin](https://github.com/giganticode/vsc-extension) for highlighting suspicious code.\n\n## Fine-tuning and Transfer learning\n\n**TBD**\n\n## Training from scratch (Not supported on OSx)\n\n### Python API\n\n```python\n>>> from langmodels.training.training import train\n>>> from langmodels.lmconfig.datamodel import *\n\n>>> train(LMTrainingConfig(corpus=Corpus(path='/path/to/the/dataset')))\n```\n\nMore parameters to customize corpus pre-processing, NN architecture, and the training process can be specified:\n\n```python\n>>> from langmodels.training.training import train\n>>> from langmodels.lmconfig.datamodel import *\n\n>>> train(LMTrainingConfig(corpus=Corpus(path='/path/to/the/dataset'), \n                            prep_function=PrepFunction(options=PrepFunctionOptions(no_com=False, no_unicode=True)),\n                            arch=GRUArchj(n_layers=2),\n                            training=Training(weight_decay=5e-6)\n))\n```\n\nBelow you can see all the default parameters specified explicitly:\n\n```python\n>>> from langmodels.lmconfig.datamodel import *\n>>> from langmodels.training.training import train\n\n>>> train(LMTrainingConfig(base_model=None, \n                       bs=32, \n                       corpus=Corpus(path=os.path.join(HOME, 'dataset'), extensions=\"java\"), \n                       prep_function=PrepFunction(corpus_api.bpe, ['10k'], \n                                                  PrepFunctionOptions(no_com=False, no_unicode=True, \n                                                                    no_spaces=True, max_str_length=sys.maxsize)), \n                       arch=LstmArch(\n                           bidir=False, qrnn=False, emb_sz=1024, n_hid=1024, n_layers=3, \n                           drop=Dropouts(multiplier=0.5, oute=0.02, outi=0.25, outh=0.15, w=0.2, out=0.1), \n                           tie_weights=True, out_bias=True), \n                       bptt=200, \n                       training=Training(\n                            optimizer=Adam(betas=(0.9, 0.99)),\n                            files_per_epoch=50000,\n                            gradient_clip=0.3,\n                            activation_regularization=ActivationRegularization(alpha=2., beta=1.), \n                            schedule=RafaelsTrainingSchedule(init_lr=1e-4, mult_coeff=0.5, patience=0,\n                                                            max_epochs=50, max_lr_reduction_times=6), \n                            weight_decay=1e-6)\n                       )\n      )\n```\n\n### CLI API\n\nTraining can be run from command line as simple as running `train` command passing path to the config in json format \nas `--config` param. To override values in the json file (or default values if `--config` param is not specified), \nyou can use `--patch` param.\n```shell script\n>> langmodels train --config=\"/path/to/json/config.json\" --patch=\"bs=64,arch.drop.multiplier=3.0\"\n```\n\nIf neither `--config` nor `--patch` params are specified, the training will be running with the default parameters.\nThe json with the default parameters would look like follows:\n\n```json\n{'arch': {'bidir': False,\n          'drop': {'multiplier': 0.5,\n                   'out': 0.1,\n                   'oute': 0.02,\n                   'outh': 0.15,\n                   'outi': 0.25,\n                   'w': 0.2},\n          'emb_sz': 1024,\n          'n_hid': 1024,\n          'n_layers': 3,\n          'name': 'lstm',\n          'out_bias': True,\n          'qrnn': False,\n          'tie_weights': True},\n 'base_model': None,\n 'bptt': 200,\n 'bs': 32,\n 'config_version': '0.0.3-alpha.0',\n 'corpus': {'extensions': 'java', 'path': '/Users/hlib/dataset'},\n 'prep_function': {'callable': 'bpe',\n                   'options': {'max_str_length': 9223372036854775807,\n                               'no_com': False,\n                               'no_spaces': True,\n                               'no_str': False,\n                               'no_unicode': True},\n                   'params': ['10k']},\n 'training': {'activation_regularization': {'alpha': 2.0, 'beta': 1.0},\n              'files_per_epoch': 50000,\n              'gradient_clip': 0.3,\n              'optimizer': {'betas': [0.9, 0.99], 'name': 'Adam'},\n              'schedule': {'init_lr': 0.0001,\n                           'max_epochs': 50,\n                           'max_lr_reduction_times': 6,\n                           'mult_coeff': 0.5,\n                           'name': 'rafael',\n                           'patience': 0},\n              'weight_decay': 1e-06}}\n```\n\nMost probably, you would have to override at least the `corpus.path` value.\n\nFor more options, run:\n```shell script\n>> langmodels train --help\n```\n\n## LM Evaluation\n\nWhen training a language model, it is important to be able to evaluate LM's performance.\nIn this section we describe different ways to do this using `langmodels` library. \nYou can also use our [tool](https://github.com/giganticode/lm-powered) to visualize the evaluation.\n\n### Evaluation on a string / file\n\nFirst, a model can be evaluate on a string with `evaluate_model_on_string` method. Note that the result may differ a lot depending \non the state of the model. Use methods `reset` and `feed_text` to reset the model \nto initial state and change the context of the model respectively.\n\n```python\n\n>>> import langmodels.repository as repo \n>>> from langmodels.evaluation import evaluate_model_on_string    \n\n>>> model = repo.load_default_model()\n>>> evaluate_model_on_string(model, 'public class MyClass {')\n\n{full_token_entropy/ParsedToken: EvaluationResult(\n    tokens=['public</t>', 'class</t>', 'MyClass</t>', '{</t>'],\n    token_types=['KeyWord', 'KeyWord', 'SplitContainer', 'OpeningCurlyBracket'],\n    values=[1.8144783973693848, 3.668722629547119, 0.5620064437389374, 0.2571456730365753], \n    aggregated_value=1.5755882859230042\n)}\n\n```\n\nSimilarly, `evaluate_model_on_file` will return a list of `Evaluation` object (1 per each line)\n\n### Evaluation on a corpus\n\nEvaluation can be run on a set of files with `evaluate_model_on_path` method\n\n```python\n>>> import langmodels.repository as repo \n>>> from langmodels.evaluation import evaluate_model_on_path\n\n>>> model = repo.load_default_model()\n>>> evaluate_model_on_path(model, '/path/to/file')\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28/28 [00:11<00:00,  2.35it/s]\n{full_token_entropy/ParsedToken: (5.859160765187885, 5745)}\n```\n\nIn `full_token_entropy/ParsedToken`: `full_token_entropy` is a metric used to evaluate the performance; \n`ParsedToken` means that all the tokens were considered when evaluating (See the next section for more details).\nThus, the average full-token-entropy is ~ 5.85 evaluated on 5.7k tokens.\n\n### Specifying metrics\n\nYou can specify based on which metrics the model is to be evaluated.\n\n```python\n>>> import langmodels.repository as repo \n>>> from langmodels.evaluation import evaluate_model_on_path\n\n>>> model = repo.load_default_model()\n>>> evaluate_model_on_path(model, '/path/to/file', metrics={'full_token_entropy', 'mrr'})\n```\n\nPossible metric values are `full_token_entropy`, `subtoken_entropy`, `mrr`. Default metric set is `{full_token_entropy}`\n\n\n## Release Notes\n\n### 0.0.4-alpha.0 (NOT backward-compatible with 0.0.1-alpha.2)\n\n- Config datamodel improvements: \n    - Add possibility to specify SGD optimizer; \n    - Add patience param to training scedule;\n    - Add converters between versions of configs;\n- Training:\n    - Report binary entropy instead of log-base-e entropy;\n    - Save more model metrics (size on disk, trainable params, training time per epoch);\n    - Do not save model after every epoch by default;\n- Evaluation improvements:\n    - Return token types in `EvaluationResult`;\n    - Add possibility to specify token types to be considered when running evaluation;\n    - Trained_model.predict_next_token(): return 1 suggestion by default;\n- Add script for new models upload.\n\n### 0.0.1-alpha.2 (NOT backward-compatible with 0.0.1-alpha.1)\n\n- Make downloading model from the repository thread-safe\n- Force to specify the extension which corresponds to the type of the code fed into\nthe `TrainedModel`. **API change**: `trained_model.feed_text(text: str)` -> `trained_model.feed_text(text: str, extension: str)`\n\n### 0.0.1-alpha.1\n\nMake methods of `TrainedModel` that change underlying PyTorch model thread-safe\n\n### 0.0.1-alpha.0\n\nInitial PyPI release\n\n## References\n\n<a id=\"1\">[1]</a> Ray, B., Hellendoorn, V., Godhane, S., Tu, Z., Bacchelli, A., & Devanbu, P. (2016, May). \nOn the\" naturalness\" of buggy code. In 2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE) \n(pp. 428-439). IEEE.\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/giganticode/langmodels", "keywords": "big large data source code corpus machine learning nlp pytorch torch fastai language modeling", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "giganticode-langmodels", "package_url": "https://pypi.org/project/giganticode-langmodels/", "platform": "", "project_url": "https://pypi.org/project/giganticode-langmodels/", "project_urls": {"Homepage": "http://github.com/giganticode/langmodels"}, "release_url": "https://pypi.org/project/giganticode-langmodels/0.0.4a0/", "requires_dist": ["fastai (==1.0.57)", "giganticode-dataprep (==1.0.0-alpha.12)", "future (==0.18.2)", "comet-ml (==3.0.2)", "flatdict (==3.4.0)", "retrying (==1.3.3)", "psutil (==5.6.7)", "tqdm (==4.39.0)", "jsons (==1.0.0)", "numpy (==1.17.4)", "appdirs (==1.4.3)", "Columnar (==1.3.1)", "requests (==2.22.0)", "pysftp (==0.2.9)", "semver (==2.9.0)", "jq (==0.1.6)"], "requires_python": ">=3.6", "summary": "A toolkit for applying machine learning to large source code corpora", "version": "0.0.4a0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h2><strong>langmodels</strong></h2>\n<p><a href=\"https://travis-ci.org/giganticode/langmodels\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bec22c9bd1ff8be51c639bfbb152753d3ffafe19/68747470733a2f2f7472617669732d63692e6f72672f676967616e7469636f64652f6c616e676d6f64656c732e7376673f6272616e63683d6d6173746572\"></a></p>\n<p>This is a repository for <strong>neural language models (LMs)</strong> trained on a large corpus of source code\nand a toolkit to work with such models.</p>\n<p>You could be interested in using this library if you want to:</p>\n<ul>\n<li>Use existing pre-trained models for tasks such as autocompletion and bug prediction;</li>\n<li>Use the pre-trained models for transfer transfer learning or further fine-tuning;</li>\n<li>Train a model from scratch by choosing one of the wide range of corpus preprocessing choices,\nneural network (NN) architectures, and training options.</li>\n</ul>\n<p>This project uses <a href=\"https://www.fast.ai\" rel=\"nofollow\">fastai</a> and\n<a href=\"https://pytorch.org\" rel=\"nofollow\">pytorch</a> libraries for NN training/inference.\nFor corpus preprocessing <a href=\"https://github.com/giganticode/dataprep\" rel=\"nofollow\">giganticode-dataprep</a> is used.</p>\n<h2>Quick start</h2>\n<h3>Prerequisites</h3>\n<ul>\n<li>Python version &gt;= 3.6 required!</li>\n</ul>\n<h3>Installation</h3>\n<pre>pip install giganticode-langmodels\n</pre>\n<p>OR to build from source:</p>\n<pre><code>git clone https://github.com//giganticode/langmodels\ncd langmodels\npython -m venv langmodels-venv\nsource langmodels-venv/bin/activate\npip install -r requirements.txt\n</code></pre>\n<h2>Using existing pre-trained models</h2>\n<h3>Loading a default pre-trained model</h3>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">langmodels.repository</span> <span class=\"k\">as</span> <span class=\"nn\">repo</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">trained_model</span> <span class=\"o\">=</span> <span class=\"n\">repo</span><span class=\"o\">.</span><span class=\"n\">load_default_model</span><span class=\"p\">()</span>\n\n<span class=\"p\">[</span><span class=\"n\">langmodels</span><span class=\"o\">.</span><span class=\"n\">repository</span><span class=\"p\">]</span> <span class=\"n\">INFO</span><span class=\"p\">:</span> <span class=\"n\">Model</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"n\">found</span> <span class=\"ow\">in</span> <span class=\"n\">cache</span><span class=\"o\">.</span> <span class=\"n\">Downloading</span> <span class=\"kn\">from</span> <span class=\"nn\">https</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">www</span><span class=\"o\">.</span><span class=\"n\">inf</span><span class=\"o\">.</span><span class=\"n\">unibz</span><span class=\"o\">.</span><span class=\"n\">it</span><span class=\"o\">/~</span><span class=\"n\">hbabii</span><span class=\"o\">/</span><span class=\"n\">pretrained_models</span><span class=\"o\">/</span><span class=\"n\">langmodel</span><span class=\"o\">-</span><span class=\"n\">large</span><span class=\"o\">-</span><span class=\"n\">split_10k_2_1024_191007</span><span class=\"o\">.</span><span class=\"mi\">112241</span><span class=\"n\">_</span><span class=\"o\">-</span><span class=\"n\">_langmodel</span><span class=\"o\">-</span><span class=\"n\">large</span><span class=\"o\">-</span><span class=\"n\">split_10k_2_1024_191022</span><span class=\"o\">.</span><span class=\"mi\">141344</span> <span class=\"o\">...</span>\n<span class=\"p\">[</span><span class=\"n\">langmodels</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">]</span> <span class=\"n\">DEBUG</span><span class=\"p\">:</span> <span class=\"n\">Loading</span> <span class=\"n\">model</span> <span class=\"n\">from</span><span class=\"p\">:</span> <span class=\"o\">/</span><span class=\"n\">home</span><span class=\"o\">/</span><span class=\"n\">hlib</span><span class=\"o\">/.</span><span class=\"n\">local</span><span class=\"o\">/</span><span class=\"n\">share</span><span class=\"o\">/</span><span class=\"n\">langmodels</span><span class=\"o\">/</span><span class=\"mf\">0.0</span><span class=\"o\">.</span><span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"n\">modelzoo</span><span class=\"o\">/</span><span class=\"n\">langmodel</span><span class=\"o\">-</span><span class=\"n\">large</span><span class=\"o\">-</span><span class=\"n\">split_10k_2_1024_191007</span><span class=\"o\">.</span><span class=\"mi\">112241</span><span class=\"n\">_</span><span class=\"o\">-</span><span class=\"n\">_langmodel</span><span class=\"o\">-</span><span class=\"n\">large</span><span class=\"o\">-</span><span class=\"n\">split_10k_2_1024_191022</span><span class=\"o\">.</span><span class=\"mi\">141344</span><span class=\"o\">/</span><span class=\"n\">best</span><span class=\"o\">.</span><span class=\"n\">pth</span> <span class=\"o\">...</span>\n<span class=\"p\">[</span><span class=\"n\">langmodels</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">]</span> <span class=\"n\">DEBUG</span><span class=\"p\">:</span> <span class=\"n\">Using</span> <span class=\"n\">GPU</span> <span class=\"k\">for</span> <span class=\"n\">inference</span>\n</pre>\n<h3>Other model loading options</h3>\n<p><strong>To see which models are available, you can call <code>list_pretrained_models</code> function.</strong></p>\n<p>Set <code>cached</code> parameter to <code>True</code> (default is <code>False</code>) to display only cached LMs (e.g. if offline).</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">langmodels.repository</span> <span class=\"k\">as</span> <span class=\"nn\">repo</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">repo</span><span class=\"o\">.</span><span class=\"n\">list_pretrained_models</span><span class=\"p\">(</span><span class=\"n\">cached</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n\n  <span class=\"n\">ID</span>                                                                    <span class=\"n\">BPE_MERGES</span>  <span class=\"n\">LAYERS_CONFIG</span>  <span class=\"n\">ARCH</span>      <span class=\"n\">BIN_ENTROPY</span>    <span class=\"n\">TRAINING_TIME_MINUTES_PER_EPOCH</span>  <span class=\"n\">N_EPOCHS</span>  <span class=\"n\">BEST_EPOCH</span>  <span class=\"n\">TAGS</span>                 \n\n  <span class=\"n\">langmodel</span><span class=\"o\">-</span><span class=\"n\">large</span><span class=\"o\">-</span><span class=\"n\">split_10k_2_1024_191007</span><span class=\"o\">.</span><span class=\"mi\">112241</span><span class=\"n\">_</span><span class=\"o\">-</span><span class=\"n\">_langmodel</span><span class=\"o\">-</span><span class=\"n\">large</span><span class=\"o\">-</span><span class=\"n\">spl</span>  <span class=\"mi\">10</span><span class=\"n\">k</span>         <span class=\"mi\">1024</span><span class=\"o\">/</span><span class=\"mi\">2</span><span class=\"o\">/</span><span class=\"mi\">1024</span>    <span class=\"n\">AWD_LSTM</span>  <span class=\"mf\">2.1455788479</span>   <span class=\"mi\">1429</span>                             <span class=\"mi\">6</span>         <span class=\"mi\">5</span>           <span class=\"p\">[</span><span class=\"s1\">'BEST'</span><span class=\"p\">,</span> <span class=\"s1\">'DEFAULT'</span><span class=\"p\">]</span>  \n  <span class=\"n\">it_10k_2_1024_191022</span><span class=\"o\">.</span><span class=\"mi\">141344</span>                                                                                                                                                                           \n  <span class=\"n\">langmodel</span><span class=\"o\">-</span><span class=\"n\">large</span><span class=\"o\">-</span><span class=\"n\">split_10k_3_1024_191007</span><span class=\"o\">.</span><span class=\"mi\">112257</span><span class=\"n\">_</span><span class=\"o\">-</span><span class=\"n\">_langmodel</span><span class=\"o\">-</span><span class=\"n\">large</span><span class=\"o\">-</span><span class=\"n\">spl</span>  <span class=\"mi\">10</span><span class=\"n\">k</span>         <span class=\"mi\">512</span><span class=\"o\">/</span><span class=\"mi\">3</span><span class=\"o\">/</span><span class=\"mi\">1024</span>     <span class=\"n\">AWD_LSTM</span>  <span class=\"mf\">2.14730056622</span>  <span class=\"mi\">1432</span>                             <span class=\"mi\">6</span>         <span class=\"mi\">5</span>           <span class=\"p\">[]</span>                   \n  <span class=\"n\">it_10k_3_1024_191022</span><span class=\"o\">.</span><span class=\"mi\">134822</span>                                                                                                                                                                           \n  <span class=\"n\">langmodel</span><span class=\"o\">-</span><span class=\"n\">large</span><span class=\"o\">-</span><span class=\"n\">split_10k_2_2048_191007</span><span class=\"o\">.</span><span class=\"mi\">112249</span><span class=\"n\">_</span><span class=\"o\">-</span><span class=\"n\">_langmodel</span><span class=\"o\">-</span><span class=\"n\">large</span><span class=\"o\">-</span><span class=\"n\">spl</span>  <span class=\"mi\">10</span><span class=\"n\">k</span>         <span class=\"mi\">512</span><span class=\"o\">/</span><span class=\"mi\">2</span><span class=\"o\">/</span><span class=\"mi\">2048</span>     <span class=\"n\">GRU</span>       <span class=\"mf\">2.19923468325</span>  <span class=\"mi\">1429</span>                             <span class=\"mi\">6</span>         <span class=\"mi\">5</span>           <span class=\"p\">[]</span>                   \n  <span class=\"n\">it_10k_2_2048_191022</span><span class=\"o\">.</span><span class=\"mi\">141335</span>                                                                                                                                                                           \n  <span class=\"n\">langmodel</span><span class=\"o\">-</span><span class=\"n\">large</span><span class=\"o\">-</span><span class=\"n\">split_10k_1_512_190926</span><span class=\"o\">.</span><span class=\"mi\">120146</span>                         <span class=\"mi\">10</span><span class=\"n\">k</span>         <span class=\"mi\">512</span><span class=\"o\">/</span><span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"mi\">512</span>      <span class=\"n\">AWD_LSTM</span>  <span class=\"mf\">2.69019493253</span>  <span class=\"mi\">479</span>                              <span class=\"mi\">9</span>         <span class=\"mi\">8</span>           <span class=\"p\">[</span><span class=\"s1\">'MEDIUM'</span><span class=\"p\">]</span>           \n  <span class=\"n\">langmodel</span><span class=\"o\">-</span><span class=\"n\">small</span><span class=\"o\">-</span><span class=\"n\">split_10k_1_512_190906</span><span class=\"o\">.</span><span class=\"mi\">154943</span>                         <span class=\"mi\">10</span><span class=\"n\">k</span>         <span class=\"mi\">512</span><span class=\"o\">/</span><span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"mi\">512</span>      <span class=\"n\">AWD_LSTM</span>  <span class=\"mf\">4.73768141172</span>  <span class=\"mi\">4</span>                                <span class=\"mi\">19</span>        <span class=\"mi\">18</span>          <span class=\"p\">[</span><span class=\"s1\">'TINY'</span><span class=\"p\">]</span>             \n  <span class=\"n\">dev_10k_1_10_190923</span><span class=\"o\">.</span><span class=\"mi\">132328</span>                                            <span class=\"mi\">10</span><span class=\"n\">k</span>         <span class=\"mi\">10</span><span class=\"o\">/</span><span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"mi\">10</span>        <span class=\"n\">AWD_LSTM</span>  <span class=\"mf\">9.15688191092</span>  <span class=\"mi\">0</span>                                <span class=\"mi\">0</span>         <span class=\"o\">-</span><span class=\"mi\">1</span>          <span class=\"p\">[</span><span class=\"s1\">'RANDOM'</span><span class=\"p\">]</span>\n</pre>\n<p>Use <code>query_all_models</code> method to get a list of <code>ModelDescription</code> objects</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">langmodels.repository</span> <span class=\"k\">as</span> <span class=\"nn\">repo</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">repo</span><span class=\"o\">.</span><span class=\"n\">query_all_models</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">ModelDescription</span><span class=\"p\">(</span><span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"s1\">'langmodel-large-split_10k_2_1024_191007.112241_-_langmodel-large-split_10k_2_1024_191022.141344'</span><span class=\"p\">,</span> <span class=\"n\">bpe_merges</span><span class=\"o\">=</span><span class=\"s1\">'10k'</span><span class=\"p\">,</span> <span class=\"n\">layers_config</span><span class=\"o\">=</span><span class=\"s1\">'1024/2/1024'</span><span class=\"p\">,</span> <span class=\"n\">arch</span><span class=\"o\">=</span><span class=\"s1\">'AWD_LSTM'</span><span class=\"p\">,</span> <span class=\"n\">bin_entropy</span><span class=\"o\">=</span><span class=\"mf\">2.1455788479</span><span class=\"p\">,</span> <span class=\"n\">training_time_minutes_per_epoch</span><span class=\"o\">=</span><span class=\"mi\">1429</span><span class=\"p\">,</span> <span class=\"n\">n_epochs</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"n\">best_epoch</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">tags</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'BEST'</span><span class=\"p\">,</span> <span class=\"s1\">'DEFAULT'</span><span class=\"p\">])</span>\n</pre>\n<p><strong>A model can be loaded by tag or by id.</strong></p>\n<p>You can specify if you want to load a model to CPU despite having cuda-supported GPU with <code>force_use_cpu</code> parameter\n(defaults to <code>False</code>). If cuda-supported GPU is not available, this parameter is disregarded.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">trained_model</span> <span class=\"o\">=</span> <span class=\"n\">repo</span><span class=\"o\">.</span><span class=\"n\">load_model_with_tag</span><span class=\"p\">(</span><span class=\"s1\">'BEST'</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">trained_model</span> <span class=\"o\">=</span> <span class=\"n\">repo</span><span class=\"o\">.</span><span class=\"n\">load_model_by_id</span><span class=\"p\">(</span><span class=\"s1\">'dev_10k_1_10_190923.132328_new'</span><span class=\"p\">,</span> <span class=\"n\">force_use_cpu</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<p>Also, you can use a lower-level API to load a model by path :</p>\n<pre><span class=\"n\">trained_model</span> <span class=\"o\">=</span> <span class=\"n\">repo</span><span class=\"o\">.</span><span class=\"n\">load_from_path</span><span class=\"p\">(</span><span class=\"s1\">'/home/hlib/.local/share/langmodels/0.0.1/modelzoo/dev_10k_1_10_190923.132328_new'</span><span class=\"p\">)</span>\n</pre>\n<h2>Inference</h2>\n<h3>Autocompletion</h3>\n<p>Example</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">langmodels.repository</span> <span class=\"k\">as</span> <span class=\"nn\">repo</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">trained_model</span> <span class=\"o\">=</span> <span class=\"n\">repo</span><span class=\"o\">.</span><span class=\"n\">load_default_model</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">trained_model</span><span class=\"o\">.</span><span class=\"n\">feed_text</span><span class=\"p\">(</span><span class=\"s1\">'public static main() { if'</span><span class=\"p\">,</span> <span class=\"n\">extension</span><span class=\"o\">=</span><span class=\"s1\">'java'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># this does not change the state of the model:</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"n\">trained_model</span><span class=\"o\">.</span><span class=\"n\">predict_next_full_token</span><span class=\"p\">(</span><span class=\"n\">n_suggestions</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"p\">[(</span><span class=\"s1\">'('</span><span class=\"p\">,</span> <span class=\"mf\">0.9334765834402862</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'.'</span><span class=\"p\">,</span> <span class=\"mf\">0.01540983953864937</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'='</span><span class=\"p\">,</span> <span class=\"mf\">0.008939018331858162</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">','</span><span class=\"p\">,</span> <span class=\"mf\">0.005372771784601065</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'the'</span><span class=\"p\">,</span> <span class=\"mf\">0.00309070517292041</span><span class=\"p\">)]</span>\n\n<span class=\"c1\"># adding more context:</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">trained_model</span><span class=\"o\">.</span><span class=\"n\">feed_text</span><span class=\"p\">(</span><span class=\"s1\">'('</span><span class=\"p\">,</span> <span class=\"n\">extension</span><span class=\"o\">=</span><span class=\"s1\">'java'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">trained_model</span><span class=\"o\">.</span><span class=\"n\">predict_next_full_token</span><span class=\"p\">(</span><span class=\"n\">n_suggestions</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n<span class=\"p\">[(</span><span class=\"s1\">'('</span><span class=\"p\">,</span> <span class=\"mf\">0.14554535082422237</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'c'</span><span class=\"p\">,</span> <span class=\"mf\">0.018005003646104294</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'!'</span><span class=\"p\">,</span> <span class=\"mf\">0.01614662429123089</span><span class=\"p\">)]</span>\n\n\n<span class=\"c1\"># resetting the state of the model (make it forget the context)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">trained_model</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">trained_model</span><span class=\"o\">.</span><span class=\"n\">predict_next_full_token</span><span class=\"p\">(</span><span class=\"n\">n_suggestions</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"p\">[(</span><span class=\"s1\">'/'</span><span class=\"p\">,</span> <span class=\"mf\">0.7209196484717589</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'package'</span><span class=\"p\">,</span> <span class=\"mf\">0.27093282656897594</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'import'</span><span class=\"p\">,</span> <span class=\"mf\">0.0007366385365522241</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'.'</span><span class=\"p\">,</span> <span class=\"mf\">0.0005714365190590807</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'public'</span><span class=\"p\">,</span> <span class=\"mf\">0.0003926736567296</span><span class=\"p\">)]</span>\n</pre>\n<h3>Bug prediction based on per-line entropies evaluation</h3>\n<p>An LM can be used to calculate cross-entropies for each line of a file. High values can give an idea about\nunusual/suspicious chunks of code <a href=\"#1\" rel=\"nofollow\">[1]</a>.</p>\n<p>Check section <a href=\"#lm-evaluation\" rel=\"nofollow\">LM Evaluation</a> section to learn how to calculate\ncross-entropy for a project/file/string,</p>\n<p>Check our <a href=\"https://github.com/giganticode/vsc-extension\" rel=\"nofollow\">vsc plugin</a> for highlighting suspicious code.</p>\n<h2>Fine-tuning and Transfer learning</h2>\n<p><strong>TBD</strong></p>\n<h2>Training from scratch (Not supported on OSx)</h2>\n<h3>Python API</h3>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">langmodels.training.training</span> <span class=\"kn\">import</span> <span class=\"n\">train</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">langmodels.lmconfig.datamodel</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">LMTrainingConfig</span><span class=\"p\">(</span><span class=\"n\">corpus</span><span class=\"o\">=</span><span class=\"n\">Corpus</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">'/path/to/the/dataset'</span><span class=\"p\">)))</span>\n</pre>\n<p>More parameters to customize corpus pre-processing, NN architecture, and the training process can be specified:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">langmodels.training.training</span> <span class=\"kn\">import</span> <span class=\"n\">train</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">langmodels.lmconfig.datamodel</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">LMTrainingConfig</span><span class=\"p\">(</span><span class=\"n\">corpus</span><span class=\"o\">=</span><span class=\"n\">Corpus</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">'/path/to/the/dataset'</span><span class=\"p\">),</span> \n                            <span class=\"n\">prep_function</span><span class=\"o\">=</span><span class=\"n\">PrepFunction</span><span class=\"p\">(</span><span class=\"n\">options</span><span class=\"o\">=</span><span class=\"n\">PrepFunctionOptions</span><span class=\"p\">(</span><span class=\"n\">no_com</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">no_unicode</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)),</span>\n                            <span class=\"n\">arch</span><span class=\"o\">=</span><span class=\"n\">GRUArchj</span><span class=\"p\">(</span><span class=\"n\">n_layers</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n                            <span class=\"n\">training</span><span class=\"o\">=</span><span class=\"n\">Training</span><span class=\"p\">(</span><span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">5e-6</span><span class=\"p\">)</span>\n<span class=\"p\">))</span>\n</pre>\n<p>Below you can see all the default parameters specified explicitly:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">langmodels.lmconfig.datamodel</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">langmodels.training.training</span> <span class=\"kn\">import</span> <span class=\"n\">train</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">LMTrainingConfig</span><span class=\"p\">(</span><span class=\"n\">base_model</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> \n                       <span class=\"n\">bs</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span> \n                       <span class=\"n\">corpus</span><span class=\"o\">=</span><span class=\"n\">Corpus</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"o\">=</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">HOME</span><span class=\"p\">,</span> <span class=\"s1\">'dataset'</span><span class=\"p\">),</span> <span class=\"n\">extensions</span><span class=\"o\">=</span><span class=\"s2\">\"java\"</span><span class=\"p\">),</span> \n                       <span class=\"n\">prep_function</span><span class=\"o\">=</span><span class=\"n\">PrepFunction</span><span class=\"p\">(</span><span class=\"n\">corpus_api</span><span class=\"o\">.</span><span class=\"n\">bpe</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s1\">'10k'</span><span class=\"p\">],</span> \n                                                  <span class=\"n\">PrepFunctionOptions</span><span class=\"p\">(</span><span class=\"n\">no_com</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">no_unicode</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> \n                                                                    <span class=\"n\">no_spaces</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">max_str_length</span><span class=\"o\">=</span><span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">maxsize</span><span class=\"p\">)),</span> \n                       <span class=\"n\">arch</span><span class=\"o\">=</span><span class=\"n\">LstmArch</span><span class=\"p\">(</span>\n                           <span class=\"n\">bidir</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">qrnn</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">emb_sz</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">,</span> <span class=\"n\">n_hid</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">,</span> <span class=\"n\">n_layers</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> \n                           <span class=\"n\">drop</span><span class=\"o\">=</span><span class=\"n\">Dropouts</span><span class=\"p\">(</span><span class=\"n\">multiplier</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">oute</span><span class=\"o\">=</span><span class=\"mf\">0.02</span><span class=\"p\">,</span> <span class=\"n\">outi</span><span class=\"o\">=</span><span class=\"mf\">0.25</span><span class=\"p\">,</span> <span class=\"n\">outh</span><span class=\"o\">=</span><span class=\"mf\">0.15</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"o\">=</span><span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"n\">out</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">),</span> \n                           <span class=\"n\">tie_weights</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">out_bias</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">),</span> \n                       <span class=\"n\">bptt</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span> \n                       <span class=\"n\">training</span><span class=\"o\">=</span><span class=\"n\">Training</span><span class=\"p\">(</span>\n                            <span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.99</span><span class=\"p\">)),</span>\n                            <span class=\"n\">files_per_epoch</span><span class=\"o\">=</span><span class=\"mi\">50000</span><span class=\"p\">,</span>\n                            <span class=\"n\">gradient_clip</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">,</span>\n                            <span class=\"n\">activation_regularization</span><span class=\"o\">=</span><span class=\"n\">ActivationRegularization</span><span class=\"p\">(</span><span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">2.</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"mf\">1.</span><span class=\"p\">),</span> \n                            <span class=\"n\">schedule</span><span class=\"o\">=</span><span class=\"n\">RafaelsTrainingSchedule</span><span class=\"p\">(</span><span class=\"n\">init_lr</span><span class=\"o\">=</span><span class=\"mf\">1e-4</span><span class=\"p\">,</span> <span class=\"n\">mult_coeff</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">patience</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n                                                            <span class=\"n\">max_epochs</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"n\">max_lr_reduction_times</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">),</span> \n                            <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">1e-6</span><span class=\"p\">)</span>\n                       <span class=\"p\">)</span>\n      <span class=\"p\">)</span>\n</pre>\n<h3>CLI API</h3>\n<p>Training can be run from command line as simple as running <code>train</code> command passing path to the config in json format\nas <code>--config</code> param. To override values in the json file (or default values if <code>--config</code> param is not specified),\nyou can use <code>--patch</code> param.</p>\n<pre>&gt;&gt; langmodels train --config<span class=\"o\">=</span><span class=\"s2\">\"/path/to/json/config.json\"</span> --patch<span class=\"o\">=</span><span class=\"s2\">\"bs=64,arch.drop.multiplier=3.0\"</span>\n</pre>\n<p>If neither <code>--config</code> nor <code>--patch</code> params are specified, the training will be running with the default parameters.\nThe json with the default parameters would look like follows:</p>\n<pre><span class=\"p\">{</span><span class=\"err\">'arch':</span> <span class=\"err\">{'bidir':</span> <span class=\"err\">False,</span>\n          <span class=\"err\">'drop':</span> <span class=\"err\">{'multiplier':</span> <span class=\"err\">0.5,</span>\n                   <span class=\"err\">'out':</span> <span class=\"err\">0.1,</span>\n                   <span class=\"err\">'oute':</span> <span class=\"err\">0.02,</span>\n                   <span class=\"err\">'outh':</span> <span class=\"err\">0.15,</span>\n                   <span class=\"err\">'outi':</span> <span class=\"err\">0.25,</span>\n                   <span class=\"err\">'w':</span> <span class=\"err\">0.2</span><span class=\"p\">}</span><span class=\"err\">,</span>\n          <span class=\"err\">'emb_sz':</span> <span class=\"mi\">1024</span><span class=\"err\">,</span>\n          <span class=\"err\">'n_hid':</span> <span class=\"mi\">1024</span><span class=\"err\">,</span>\n          <span class=\"err\">'n_layers':</span> <span class=\"mi\">3</span><span class=\"err\">,</span>\n          <span class=\"err\">'name':</span> <span class=\"err\">'lstm',</span>\n          <span class=\"err\">'out_bias':</span> <span class=\"err\">True,</span>\n          <span class=\"err\">'qrnn':</span> <span class=\"err\">False,</span>\n          <span class=\"err\">'tie_weights':</span> <span class=\"err\">True},</span>\n <span class=\"err\">'base_model':</span> <span class=\"err\">None,</span>\n <span class=\"err\">'bptt':</span> <span class=\"mi\">200</span><span class=\"err\">,</span>\n <span class=\"err\">'bs':</span> <span class=\"mi\">32</span><span class=\"err\">,</span>\n <span class=\"err\">'config_version':</span> <span class=\"err\">'</span><span class=\"mf\">0.0</span><span class=\"err\">.</span><span class=\"mi\">3</span><span class=\"err\">-alpha.</span><span class=\"mi\">0</span><span class=\"err\">',</span>\n <span class=\"err\">'corpus':</span> <span class=\"p\">{</span><span class=\"err\">'extensions':</span> <span class=\"err\">'java',</span> <span class=\"err\">'path':</span> <span class=\"err\">'/Users/hlib/dataset'</span><span class=\"p\">}</span><span class=\"err\">,</span>\n <span class=\"err\">'prep_function':</span> <span class=\"p\">{</span><span class=\"err\">'callable':</span> <span class=\"err\">'bpe',</span>\n                   <span class=\"err\">'options':</span> <span class=\"err\">{'max_str_length':</span> <span class=\"err\">9223372036854775807,</span>\n                               <span class=\"err\">'no_com':</span> <span class=\"err\">False,</span>\n                               <span class=\"err\">'no_spaces':</span> <span class=\"err\">True,</span>\n                               <span class=\"err\">'no_str':</span> <span class=\"err\">False,</span>\n                               <span class=\"err\">'no_unicode':</span> <span class=\"err\">True</span><span class=\"p\">}</span><span class=\"err\">,</span>\n                   <span class=\"err\">'params':</span> <span class=\"p\">[</span><span class=\"err\">'</span><span class=\"mi\">10</span><span class=\"err\">k'</span><span class=\"p\">]</span><span class=\"err\">},</span>\n <span class=\"err\">'training':</span> <span class=\"p\">{</span><span class=\"err\">'activation_regularization':</span> <span class=\"err\">{'alpha':</span> <span class=\"err\">2.0,</span> <span class=\"err\">'beta':</span> <span class=\"err\">1.0</span><span class=\"p\">}</span><span class=\"err\">,</span>\n              <span class=\"err\">'files_per_epoch':</span> <span class=\"mi\">50000</span><span class=\"err\">,</span>\n              <span class=\"err\">'gradient_clip':</span> <span class=\"mf\">0.3</span><span class=\"err\">,</span>\n              <span class=\"err\">'optimizer':</span> <span class=\"p\">{</span><span class=\"err\">'betas':</span> <span class=\"err\">[0.9,</span> <span class=\"err\">0.99],</span> <span class=\"err\">'name':</span> <span class=\"err\">'Adam'</span><span class=\"p\">}</span><span class=\"err\">,</span>\n              <span class=\"err\">'schedule':</span> <span class=\"p\">{</span><span class=\"err\">'init_lr':</span> <span class=\"err\">0.0001,</span>\n                           <span class=\"err\">'max_epochs':</span> <span class=\"err\">50,</span>\n                           <span class=\"err\">'max_lr_reduction_times':</span> <span class=\"err\">6,</span>\n                           <span class=\"err\">'mult_coeff':</span> <span class=\"err\">0.5,</span>\n                           <span class=\"err\">'name':</span> <span class=\"err\">'rafael',</span>\n                           <span class=\"err\">'patience':</span> <span class=\"err\">0</span><span class=\"p\">}</span><span class=\"err\">,</span>\n              <span class=\"err\">'weight_decay':</span> <span class=\"mf\">1e-06</span><span class=\"err\">}}</span>\n</pre>\n<p>Most probably, you would have to override at least the <code>corpus.path</code> value.</p>\n<p>For more options, run:</p>\n<pre>&gt;&gt; langmodels train --help\n</pre>\n<h2>LM Evaluation</h2>\n<p>When training a language model, it is important to be able to evaluate LM's performance.\nIn this section we describe different ways to do this using <code>langmodels</code> library.\nYou can also use our <a href=\"https://github.com/giganticode/lm-powered\" rel=\"nofollow\">tool</a> to visualize the evaluation.</p>\n<h3>Evaluation on a string / file</h3>\n<p>First, a model can be evaluate on a string with <code>evaluate_model_on_string</code> method. Note that the result may differ a lot depending\non the state of the model. Use methods <code>reset</code> and <code>feed_text</code> to reset the model\nto initial state and change the context of the model respectively.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">langmodels.repository</span> <span class=\"k\">as</span> <span class=\"nn\">repo</span> \n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">langmodels.evaluation</span> <span class=\"kn\">import</span> <span class=\"n\">evaluate_model_on_string</span>    \n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">repo</span><span class=\"o\">.</span><span class=\"n\">load_default_model</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">evaluate_model_on_string</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"s1\">'public class MyClass {'</span><span class=\"p\">)</span>\n\n<span class=\"p\">{</span><span class=\"n\">full_token_entropy</span><span class=\"o\">/</span><span class=\"n\">ParsedToken</span><span class=\"p\">:</span> <span class=\"n\">EvaluationResult</span><span class=\"p\">(</span>\n    <span class=\"n\">tokens</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'public&lt;/t&gt;'</span><span class=\"p\">,</span> <span class=\"s1\">'class&lt;/t&gt;'</span><span class=\"p\">,</span> <span class=\"s1\">'MyClass&lt;/t&gt;'</span><span class=\"p\">,</span> <span class=\"s1\">'{&lt;/t&gt;'</span><span class=\"p\">],</span>\n    <span class=\"n\">token_types</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'KeyWord'</span><span class=\"p\">,</span> <span class=\"s1\">'KeyWord'</span><span class=\"p\">,</span> <span class=\"s1\">'SplitContainer'</span><span class=\"p\">,</span> <span class=\"s1\">'OpeningCurlyBracket'</span><span class=\"p\">],</span>\n    <span class=\"n\">values</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">1.8144783973693848</span><span class=\"p\">,</span> <span class=\"mf\">3.668722629547119</span><span class=\"p\">,</span> <span class=\"mf\">0.5620064437389374</span><span class=\"p\">,</span> <span class=\"mf\">0.2571456730365753</span><span class=\"p\">],</span> \n    <span class=\"n\">aggregated_value</span><span class=\"o\">=</span><span class=\"mf\">1.5755882859230042</span>\n<span class=\"p\">)}</span>\n</pre>\n<p>Similarly, <code>evaluate_model_on_file</code> will return a list of <code>Evaluation</code> object (1 per each line)</p>\n<h3>Evaluation on a corpus</h3>\n<p>Evaluation can be run on a set of files with <code>evaluate_model_on_path</code> method</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">langmodels.repository</span> <span class=\"k\">as</span> <span class=\"nn\">repo</span> \n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">langmodels.evaluation</span> <span class=\"kn\">import</span> <span class=\"n\">evaluate_model_on_path</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">repo</span><span class=\"o\">.</span><span class=\"n\">load_default_model</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">evaluate_model_on_path</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"s1\">'/path/to/file'</span><span class=\"p\">)</span>\n\n<span class=\"mi\">100</span><span class=\"o\">%|</span><span class=\"err\">\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588</span><span class=\"o\">|</span> <span class=\"mi\">28</span><span class=\"o\">/</span><span class=\"mi\">28</span> <span class=\"p\">[</span><span class=\"mi\">00</span><span class=\"p\">:</span><span class=\"mi\">11</span><span class=\"o\">&lt;</span><span class=\"mi\">00</span><span class=\"p\">:</span><span class=\"mi\">00</span><span class=\"p\">,</span>  <span class=\"mf\">2.35</span><span class=\"n\">it</span><span class=\"o\">/</span><span class=\"n\">s</span><span class=\"p\">]</span>\n<span class=\"p\">{</span><span class=\"n\">full_token_entropy</span><span class=\"o\">/</span><span class=\"n\">ParsedToken</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"mf\">5.859160765187885</span><span class=\"p\">,</span> <span class=\"mi\">5745</span><span class=\"p\">)}</span>\n</pre>\n<p>In <code>full_token_entropy/ParsedToken</code>: <code>full_token_entropy</code> is a metric used to evaluate the performance;\n<code>ParsedToken</code> means that all the tokens were considered when evaluating (See the next section for more details).\nThus, the average full-token-entropy is ~ 5.85 evaluated on 5.7k tokens.</p>\n<h3>Specifying metrics</h3>\n<p>You can specify based on which metrics the model is to be evaluated.</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">langmodels.repository</span> <span class=\"k\">as</span> <span class=\"nn\">repo</span> \n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">langmodels.evaluation</span> <span class=\"kn\">import</span> <span class=\"n\">evaluate_model_on_path</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">repo</span><span class=\"o\">.</span><span class=\"n\">load_default_model</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">evaluate_model_on_path</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"s1\">'/path/to/file'</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'full_token_entropy'</span><span class=\"p\">,</span> <span class=\"s1\">'mrr'</span><span class=\"p\">})</span>\n</pre>\n<p>Possible metric values are <code>full_token_entropy</code>, <code>subtoken_entropy</code>, <code>mrr</code>. Default metric set is <code>{full_token_entropy}</code></p>\n<h2>Release Notes</h2>\n<h3>0.0.4-alpha.0 (NOT backward-compatible with 0.0.1-alpha.2)</h3>\n<ul>\n<li>Config datamodel improvements:\n<ul>\n<li>Add possibility to specify SGD optimizer;</li>\n<li>Add patience param to training scedule;</li>\n<li>Add converters between versions of configs;</li>\n</ul>\n</li>\n<li>Training:\n<ul>\n<li>Report binary entropy instead of log-base-e entropy;</li>\n<li>Save more model metrics (size on disk, trainable params, training time per epoch);</li>\n<li>Do not save model after every epoch by default;</li>\n</ul>\n</li>\n<li>Evaluation improvements:\n<ul>\n<li>Return token types in <code>EvaluationResult</code>;</li>\n<li>Add possibility to specify token types to be considered when running evaluation;</li>\n<li>Trained_model.predict_next_token(): return 1 suggestion by default;</li>\n</ul>\n</li>\n<li>Add script for new models upload.</li>\n</ul>\n<h3>0.0.1-alpha.2 (NOT backward-compatible with 0.0.1-alpha.1)</h3>\n<ul>\n<li>Make downloading model from the repository thread-safe</li>\n<li>Force to specify the extension which corresponds to the type of the code fed into\nthe <code>TrainedModel</code>. <strong>API change</strong>: <code>trained_model.feed_text(text: str)</code> -&gt; <code>trained_model.feed_text(text: str, extension: str)</code></li>\n</ul>\n<h3>0.0.1-alpha.1</h3>\n<p>Make methods of <code>TrainedModel</code> that change underlying PyTorch model thread-safe</p>\n<h3>0.0.1-alpha.0</h3>\n<p>Initial PyPI release</p>\n<h2>References</h2>\n<p><a id=\"1\">[1]</a> Ray, B., Hellendoorn, V., Godhane, S., Tu, Z., Bacchelli, A., &amp; Devanbu, P. (2016, May).\nOn the\" naturalness\" of buggy code. In 2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)\n(pp. 428-439). IEEE.</p>\n\n          </div>"}, "last_serial": 6488458, "releases": {"0.0.1a0": [{"comment_text": "", "digests": {"md5": "76376ce958d1d307a91c4e79a91c42c9", "sha256": "3336c05a64c715a0537b97b38cb55a57b54f654dcd364b4ac63a58fe9bff0990"}, "downloads": -1, "filename": "giganticode_langmodels-0.0.1a0-py3-none-any.whl", "has_sig": false, "md5_digest": "76376ce958d1d307a91c4e79a91c42c9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 49341, "upload_time": "2019-12-02T13:14:21", "upload_time_iso_8601": "2019-12-02T13:14:21.463658Z", "url": "https://files.pythonhosted.org/packages/7e/96/2fd3204d0165b7b4dab1953e677aaf160e4a11dbfe282089ce0f89f8b597/giganticode_langmodels-0.0.1a0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "49fb88180f4431682b54816cbcee1ba1", "sha256": "a8e40e20b10bcdf2684a7d50b046a5d8371da98cd7183585e9bf1be555ae8de0"}, "downloads": -1, "filename": "giganticode-langmodels-0.0.1a0.tar.gz", "has_sig": false, "md5_digest": "49fb88180f4431682b54816cbcee1ba1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 46635, "upload_time": "2019-12-02T13:14:24", "upload_time_iso_8601": "2019-12-02T13:14:24.641028Z", "url": "https://files.pythonhosted.org/packages/a6/48/40af4f56153451167b1f340693adab09fe73ebc37077824e6133405cca48/giganticode-langmodels-0.0.1a0.tar.gz", "yanked": false}], "0.0.1a1": [{"comment_text": "", "digests": {"md5": "f8b8a0a4c41426f00b70fed1b89118a8", "sha256": "0740f49611c6f4c95d807b36df99aff2c60bec4d1b4f36ed084fb42d9730bb2f"}, "downloads": -1, "filename": "giganticode_langmodels-0.0.1a1-py3-none-any.whl", "has_sig": false, "md5_digest": "f8b8a0a4c41426f00b70fed1b89118a8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 49433, "upload_time": "2019-12-03T23:01:54", "upload_time_iso_8601": "2019-12-03T23:01:54.626744Z", "url": "https://files.pythonhosted.org/packages/7d/9e/6a872662a2883505bc06529035ff277ed5d1d2b2d5b21126ff88f27723fd/giganticode_langmodels-0.0.1a1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1d9c7c37cacf03e4f04aac46ad0da391", "sha256": "ec425d47f32a0f0bc85eaf41dadc5002997c22c5291cce33564ce7a3b3707faf"}, "downloads": -1, "filename": "giganticode-langmodels-0.0.1a1.tar.gz", "has_sig": false, "md5_digest": "1d9c7c37cacf03e4f04aac46ad0da391", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 46795, "upload_time": "2019-12-03T23:01:56", "upload_time_iso_8601": "2019-12-03T23:01:56.860272Z", "url": "https://files.pythonhosted.org/packages/a6/6d/315710625b73b86f6241df6251f97c0bea23ed3b57b78f8684583f672cac/giganticode-langmodels-0.0.1a1.tar.gz", "yanked": false}], "0.0.1a2": [{"comment_text": "", "digests": {"md5": "cf559e6355b0d31d411f812f84b08a9c", "sha256": "c4d5f4966d5a19eb58fe85f00bb1c4ed066fe03bb557cdd7c1337c4b78b3bb08"}, "downloads": -1, "filename": "giganticode_langmodels-0.0.1a2-py3-none-any.whl", "has_sig": false, "md5_digest": "cf559e6355b0d31d411f812f84b08a9c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 49640, "upload_time": "2019-12-04T09:50:45", "upload_time_iso_8601": "2019-12-04T09:50:45.575401Z", "url": "https://files.pythonhosted.org/packages/d7/6f/4c20417fb11030a192313336ee27e163beb3de92f94615c6fa1d87b979ad/giganticode_langmodels-0.0.1a2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "352e89e2e6c31eba1cd689542d432394", "sha256": "5c55ac81ea2a9afcfa3e72ffa1249efa4753d3af06e0e289e99e441cd7057e6a"}, "downloads": -1, "filename": "giganticode-langmodels-0.0.1a2.tar.gz", "has_sig": false, "md5_digest": "352e89e2e6c31eba1cd689542d432394", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 47293, "upload_time": "2019-12-04T09:50:47", "upload_time_iso_8601": "2019-12-04T09:50:47.597189Z", "url": "https://files.pythonhosted.org/packages/9a/77/d4615817ee850f7e1c6572505e3d33b854b8334fc61c57430cb8e6d67154/giganticode-langmodels-0.0.1a2.tar.gz", "yanked": false}], "0.0.3a0": [{"comment_text": "", "digests": {"md5": "567a49b9f9abf09da9e1a80a3efefac4", "sha256": "eb8f1e93aed514e3ee5b050e86af13e0470a91be427d47305ac2a170d0dfca2e"}, "downloads": -1, "filename": "giganticode_langmodels-0.0.3a0-py3-none-any.whl", "has_sig": false, "md5_digest": "567a49b9f9abf09da9e1a80a3efefac4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 57776, "upload_time": "2020-01-20T15:29:46", "upload_time_iso_8601": "2020-01-20T15:29:46.249582Z", "url": "https://files.pythonhosted.org/packages/14/79/8d2cb381a82e3bd284c0d6319c31cd0a7b4ec329377122a34635814d5f10/giganticode_langmodels-0.0.3a0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4b93e6353fdffdb3552014e42cf47bbd", "sha256": "ae55d63d0ff3aae5f969a8fc4e224cf0bb450528a9a67a01385f7f9025055318"}, "downloads": -1, "filename": "giganticode-langmodels-0.0.3a0.tar.gz", "has_sig": false, "md5_digest": "4b93e6353fdffdb3552014e42cf47bbd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 52885, "upload_time": "2020-01-20T15:29:48", "upload_time_iso_8601": "2020-01-20T15:29:48.786954Z", "url": "https://files.pythonhosted.org/packages/d1/b1/913cd31cef02408c85bbbfc00537fd9697fdcbcb4810031ac5d4114c28a2/giganticode-langmodels-0.0.3a0.tar.gz", "yanked": false}], "0.0.4a0": [{"comment_text": "", "digests": {"md5": "4255d039fa2696171047b32f6b1adc85", "sha256": "a5a2d6c8c0f443a097452d2e4583848007e9f08299b8bf56fc6c9b22dbc75b44"}, "downloads": -1, "filename": "giganticode_langmodels-0.0.4a0-py3-none-any.whl", "has_sig": false, "md5_digest": "4255d039fa2696171047b32f6b1adc85", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 57746, "upload_time": "2020-01-20T16:19:54", "upload_time_iso_8601": "2020-01-20T16:19:54.060088Z", "url": "https://files.pythonhosted.org/packages/07/44/a063861a46f59bfd207a5d0baa8f75311a8d09ae60407f55bacdf602bc84/giganticode_langmodels-0.0.4a0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "56b7c6fb0a95b399a04a807364bb18ff", "sha256": "054b496cfd5a27766f0b1cfe6caa90374cbd2b40fa95184adcf9f57f091de7fe"}, "downloads": -1, "filename": "giganticode-langmodels-0.0.4a0.tar.gz", "has_sig": false, "md5_digest": "56b7c6fb0a95b399a04a807364bb18ff", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 52847, "upload_time": "2020-01-20T16:19:56", "upload_time_iso_8601": "2020-01-20T16:19:56.386649Z", "url": "https://files.pythonhosted.org/packages/e9/21/2b9932a6623ca1e4e65cc6abdfdfde6e938dd940a2d24509dd6168165dfb/giganticode-langmodels-0.0.4a0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "4255d039fa2696171047b32f6b1adc85", "sha256": "a5a2d6c8c0f443a097452d2e4583848007e9f08299b8bf56fc6c9b22dbc75b44"}, "downloads": -1, "filename": "giganticode_langmodels-0.0.4a0-py3-none-any.whl", "has_sig": false, "md5_digest": "4255d039fa2696171047b32f6b1adc85", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 57746, "upload_time": "2020-01-20T16:19:54", "upload_time_iso_8601": "2020-01-20T16:19:54.060088Z", "url": "https://files.pythonhosted.org/packages/07/44/a063861a46f59bfd207a5d0baa8f75311a8d09ae60407f55bacdf602bc84/giganticode_langmodels-0.0.4a0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "56b7c6fb0a95b399a04a807364bb18ff", "sha256": "054b496cfd5a27766f0b1cfe6caa90374cbd2b40fa95184adcf9f57f091de7fe"}, "downloads": -1, "filename": "giganticode-langmodels-0.0.4a0.tar.gz", "has_sig": false, "md5_digest": "56b7c6fb0a95b399a04a807364bb18ff", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 52847, "upload_time": "2020-01-20T16:19:56", "upload_time_iso_8601": "2020-01-20T16:19:56.386649Z", "url": "https://files.pythonhosted.org/packages/e9/21/2b9932a6623ca1e4e65cc6abdfdfde6e938dd940a2d24509dd6168165dfb/giganticode-langmodels-0.0.4a0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:57:28 2020"}