{"info": {"author": "Jaehuck Heo", "author_email": "wogur379@gmail.com", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# tootorch\n\nImplemetation XAI in Computer Vision (Pytorch)\n\n# Requirements\n\n```\ntorch\nopencv-python\npillow\nh5py\ntqdm\n```\n\n# Installation\n```bash\npip install tootorch\n```\n\n# Interpretable Methods\n**Attribution Methods**\n- Vanilla Backpropagation (VBP) [[Notebook](https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20Vanilla%20Backpropagation%20%26%20Ensemble.ipynb)]\n- Input x Backpropagation (IB) [[Notebook](https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20Input%20x%20Backpropagation%20%26%20Ensemble.ipynb)]\n- DeconvNet [1] [[Notebook](https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20DeconvNet%20%26%20Ensemble.ipynb)]\n- Guided Backpropagation (GB) [2] [[Notebook](https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20Guided%20Backpropagation%20%26%20Ensemble.ipynb)]\n- Integrated Gradients (IG) [3] [[Notebook](https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20Integrated%20Gradients%20%26%20Ensemble.ipynb)]\n- Grad-CAM (GC) [4] [[Notebook](https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20GradCAM%20%26%20Ensemble.ipynb)]\n- Guided Grad-CAM (GB-GC) [4] [[Notebook](https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20Guided-GradCAM%20%26%20Ensemble.ipynb)]\n\n**Ensemble Methods**\n- SmoothGrad (SG) [5]\n- SmoothGrad-Squared (SG-SQ) [6]\n- SmoothGrad-VAR (SG-VAR) [6]\n\n# Evaluation \n- Coherence\n- Selectivity\n- Remove and Retrain (ROAR) [6]\n- Keep and Retrain (KAR) [6]\n\n# Attention Methods\n- Residual Attention Network (RAN) [7]\n- Class Activation Methods (CAM) [8]\n- Convolutional Block Attention Module (CBAM) [9]\n- Wide Attention Residual Network (WARN) [10]\n\n# Reference\n- [1] Zeiler, M. D., & Fergus, R. (2014, September). Visualizing and understanding convolutional networks. In European conference on computer vision (pp. 818-833). Springer, Cham. [[Paper](https://arxiv.org/abs/1311.2901)] [[Korean version](https://datanetworkanalysis.github.io/2019/10/27/deconvnet)]\n\n- [2] Springenberg, J. T., Dosovitskiy, A., Brox, T., & Riedmiller, M. (2014). Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806. [[Paper](https://arxiv.org/abs/1412.6806)]\n\n- [3] Sundararajan, M., Taly, A., & Yan, Q. (2017, August). Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 3319-3328). JMLR. org. [[Paper](https://arxiv.org/pdf/1703.01365.pdf)]\n\n- [4] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision (pp. 618-626). [[Paper](https://arxiv.org/abs/1610.02391)] [[Korean version](https://www.notion.so/tootouch/Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization-504a3f7a58fd4c3eafdc26258befd643)]\n\n- [5] Smilkov, D., Thorat, N., Kim, B., Vi\u00e9gas, F., & Wattenberg, M. (2017). Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825. [[Paper](\nhttps://arxiv.org/abs/1706.03825)] [[Korean version](https://datanetworkanalysis.github.io/2019/10/22/smoothgrad)]\n\n- [6] Hooker, S., Erhan, D., Kindermans, P. J., & Kim, B. (2018). Evaluating feature importance estimates. arXiv preprint arXiv:1806.10758. [[Paper](https://arxiv.org/abs/1806.10758)] [[Korean version](https://datanetworkanalysis.github.io/2019/11/13/roar_kar)]\n\n- [7]  Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., ... & Tang, X. (2017). Residual attention network for image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3156-3164). [[Paper](https://arxiv.org/abs/1704.06904)]\n\n- [8] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., & Torralba, A. (2016). Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2921-2929). [[Paper](https://arxiv.org/abs/1512.04150)]\n\n- [9]  Woo, S., Park, J., Lee, J. Y., & So Kweon, I. (2018). Cbam: Convolutional block attention module. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 3-19). [[Paper](https://arxiv.org/abs/1807.06521)]\n\n- [10] Rodr\u00edguez, P., Gonfaus, J. M., Cucurull, G., XavierRoca, F., & Gonzalez, J. (2018). Attend and rectify: a gated attention mechanism for fine-grained recovery. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 349-364). [[Paper](https://arxiv.org/abs/1807.07320)]\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/TooTouch/tootorch/archive/v0.1.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/TooTouch/tootorch", "keywords": "tootorch,XAI", "license": "", "maintainer": "", "maintainer_email": "", "name": "tootorch", "package_url": "https://pypi.org/project/tootorch/", "platform": "", "project_url": "https://pypi.org/project/tootorch/", "project_urls": {"Download": "https://github.com/TooTouch/tootorch/archive/v0.1.tar.gz", "Homepage": "https://github.com/TooTouch/tootorch"}, "release_url": "https://pypi.org/project/tootorch/0.2/", "requires_dist": ["torch", "torchvision", "h5py", "tqdm", "pillow", "opencv-python"], "requires_python": ">=3.6", "summary": "Implemetation XAI in Computer Vision (Pytorch)", "version": "0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>tootorch</h1>\n<p>Implemetation XAI in Computer Vision (Pytorch)</p>\n<h1>Requirements</h1>\n<pre><code>torch\nopencv-python\npillow\nh5py\ntqdm\n</code></pre>\n<h1>Installation</h1>\n<pre>pip install tootorch\n</pre>\n<h1>Interpretable Methods</h1>\n<p><strong>Attribution Methods</strong></p>\n<ul>\n<li>Vanilla Backpropagation (VBP) [<a href=\"https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20Vanilla%20Backpropagation%20%26%20Ensemble.ipynb\" rel=\"nofollow\">Notebook</a>]</li>\n<li>Input x Backpropagation (IB) [<a href=\"https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20Input%20x%20Backpropagation%20%26%20Ensemble.ipynb\" rel=\"nofollow\">Notebook</a>]</li>\n<li>DeconvNet [1] [<a href=\"https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20DeconvNet%20%26%20Ensemble.ipynb\" rel=\"nofollow\">Notebook</a>]</li>\n<li>Guided Backpropagation (GB) [2] [<a href=\"https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20Guided%20Backpropagation%20%26%20Ensemble.ipynb\" rel=\"nofollow\">Notebook</a>]</li>\n<li>Integrated Gradients (IG) [3] [<a href=\"https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20Integrated%20Gradients%20%26%20Ensemble.ipynb\" rel=\"nofollow\">Notebook</a>]</li>\n<li>Grad-CAM (GC) [4] [<a href=\"https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20GradCAM%20%26%20Ensemble.ipynb\" rel=\"nofollow\">Notebook</a>]</li>\n<li>Guided Grad-CAM (GB-GC) [4] [<a href=\"https://github.com/Tootouch/WhiteBox-Part1/blob/master/notebook/%5BAttribution%5D%20-%20Guided-GradCAM%20%26%20Ensemble.ipynb\" rel=\"nofollow\">Notebook</a>]</li>\n</ul>\n<p><strong>Ensemble Methods</strong></p>\n<ul>\n<li>SmoothGrad (SG) [5]</li>\n<li>SmoothGrad-Squared (SG-SQ) [6]</li>\n<li>SmoothGrad-VAR (SG-VAR) [6]</li>\n</ul>\n<h1>Evaluation</h1>\n<ul>\n<li>Coherence</li>\n<li>Selectivity</li>\n<li>Remove and Retrain (ROAR) [6]</li>\n<li>Keep and Retrain (KAR) [6]</li>\n</ul>\n<h1>Attention Methods</h1>\n<ul>\n<li>Residual Attention Network (RAN) [7]</li>\n<li>Class Activation Methods (CAM) [8]</li>\n<li>Convolutional Block Attention Module (CBAM) [9]</li>\n<li>Wide Attention Residual Network (WARN) [10]</li>\n</ul>\n<h1>Reference</h1>\n<ul>\n<li>\n<p>[1] Zeiler, M. D., &amp; Fergus, R. (2014, September). Visualizing and understanding convolutional networks. In European conference on computer vision (pp. 818-833). Springer, Cham. [<a href=\"https://arxiv.org/abs/1311.2901\" rel=\"nofollow\">Paper</a>] [<a href=\"https://datanetworkanalysis.github.io/2019/10/27/deconvnet\" rel=\"nofollow\">Korean version</a>]</p>\n</li>\n<li>\n<p>[2] Springenberg, J. T., Dosovitskiy, A., Brox, T., &amp; Riedmiller, M. (2014). Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806. [<a href=\"https://arxiv.org/abs/1412.6806\" rel=\"nofollow\">Paper</a>]</p>\n</li>\n<li>\n<p>[3] Sundararajan, M., Taly, A., &amp; Yan, Q. (2017, August). Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 3319-3328). JMLR. org. [<a href=\"https://arxiv.org/pdf/1703.01365.pdf\" rel=\"nofollow\">Paper</a>]</p>\n</li>\n<li>\n<p>[4] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., &amp; Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision (pp. 618-626). [<a href=\"https://arxiv.org/abs/1610.02391\" rel=\"nofollow\">Paper</a>] [<a href=\"https://www.notion.so/tootouch/Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization-504a3f7a58fd4c3eafdc26258befd643\" rel=\"nofollow\">Korean version</a>]</p>\n</li>\n<li>\n<p>[5] Smilkov, D., Thorat, N., Kim, B., Vi\u00e9gas, F., &amp; Wattenberg, M. (2017). Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825. [<a href=\"https://arxiv.org/abs/1706.03825\" rel=\"nofollow\">Paper</a>] [<a href=\"https://datanetworkanalysis.github.io/2019/10/22/smoothgrad\" rel=\"nofollow\">Korean version</a>]</p>\n</li>\n<li>\n<p>[6] Hooker, S., Erhan, D., Kindermans, P. J., &amp; Kim, B. (2018). Evaluating feature importance estimates. arXiv preprint arXiv:1806.10758. [<a href=\"https://arxiv.org/abs/1806.10758\" rel=\"nofollow\">Paper</a>] [<a href=\"https://datanetworkanalysis.github.io/2019/11/13/roar_kar\" rel=\"nofollow\">Korean version</a>]</p>\n</li>\n<li>\n<p>[7]  Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., ... &amp; Tang, X. (2017). Residual attention network for image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3156-3164). [<a href=\"https://arxiv.org/abs/1704.06904\" rel=\"nofollow\">Paper</a>]</p>\n</li>\n<li>\n<p>[8] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., &amp; Torralba, A. (2016). Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2921-2929). [<a href=\"https://arxiv.org/abs/1512.04150\" rel=\"nofollow\">Paper</a>]</p>\n</li>\n<li>\n<p>[9]  Woo, S., Park, J., Lee, J. Y., &amp; So Kweon, I. (2018). Cbam: Convolutional block attention module. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 3-19). [<a href=\"https://arxiv.org/abs/1807.06521\" rel=\"nofollow\">Paper</a>]</p>\n</li>\n<li>\n<p>[10] Rodr\u00edguez, P., Gonfaus, J. M., Cucurull, G., XavierRoca, F., &amp; Gonzalez, J. (2018). Attend and rectify: a gated attention mechanism for fine-grained recovery. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 349-364). [<a href=\"https://arxiv.org/abs/1807.07320\" rel=\"nofollow\">Paper</a>]</p>\n</li>\n</ul>\n\n          </div>"}, "last_serial": 6595035, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "38d919571f5750f6e15054c21e100ccf", "sha256": "482a1ece173663dcb4560970caa937fbb4a4fadd9d4c84442c9f5de6ddb68f55"}, "downloads": -1, "filename": "tootorch-1.8-py3-none-any.whl", "has_sig": false, "md5_digest": "38d919571f5750f6e15054c21e100ccf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 59945, "upload_time": "2020-02-07T23:44:44", "upload_time_iso_8601": "2020-02-07T23:44:44.542267Z", "url": "https://files.pythonhosted.org/packages/5f/a2/af01ba0c54121777be7a3beafdd1c38d757d3d01459d5d71de8dc1d95a06/tootorch-1.8-py3-none-any.whl", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "453cfe896a63bc4cc527bb09eb38a2b2", "sha256": "6d9bbcc10acba72757a1478104f79e8614a8653118f5cc02a9de8c2a2da35e78"}, "downloads": -1, "filename": "tootorch-1.9-py3-none-any.whl", "has_sig": false, "md5_digest": "453cfe896a63bc4cc527bb09eb38a2b2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 59955, "upload_time": "2020-02-08T19:54:50", "upload_time_iso_8601": "2020-02-08T19:54:50.470727Z", "url": "https://files.pythonhosted.org/packages/38/8d/6b774f3a2bea793a921f41fa88bf999df32c7511100a4ce4facf4346b2f7/tootorch-1.9-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "453cfe896a63bc4cc527bb09eb38a2b2", "sha256": "6d9bbcc10acba72757a1478104f79e8614a8653118f5cc02a9de8c2a2da35e78"}, "downloads": -1, "filename": "tootorch-1.9-py3-none-any.whl", "has_sig": false, "md5_digest": "453cfe896a63bc4cc527bb09eb38a2b2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 59955, "upload_time": "2020-02-08T19:54:50", "upload_time_iso_8601": "2020-02-08T19:54:50.470727Z", "url": "https://files.pythonhosted.org/packages/38/8d/6b774f3a2bea793a921f41fa88bf999df32c7511100a4ce4facf4346b2f7/tootorch-1.9-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 03:50:57 2020"}