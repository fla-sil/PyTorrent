{"info": {"author": "HighCWu", "author_email": "HighCWu@163.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.6"], "description": "Keras BERT TPU\n==============\n\n\n.. image:: https://travis-ci.org/HighCWu/keras-bert-tpu.svg?branch=master\n   :target: https://travis-ci.org/HighCWu/keras-bert-tpu\n   :alt: Travis\n\n\n.. image:: https://coveralls.io/repos/github/HighCWu/keras-bert-tpu/badge.svg?branch=master\n   :target: https://coveralls.io/github/HighCWu/keras-bert-tpu\n   :alt: Coverage\n\n\nThis is a fork of `CyberZHG/keras_bert <https://github.com/CyberZHG/keras-bert>`_ which supports Keras BERT on TPU.\n\nImplementation of the `BERT <https://arxiv.org/pdf/1810.04805.pdf>`_. Official pre-trained models could be loaded for feature extraction and prediction.\n\nColab Demo\n----------\n\n`HighCWu/keras-bert-tpu <https://colab.research.google.com/github/HighCWu/keras-bert-tpu/blob/master/demo/load_model/load_and_predict.ipynb>`_\n\nInstall\n-------\n\n.. code-block:: bash\n\n   pip install keras-bert-tpu\n\nUsage\n-----\n\nLoad Official Pre-trained Models\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn `feature extraction demo <./demo/load_model/load_and_extract.py>`_\\ , you should be able to get the same extraction result as the official model. And in `prediction demo <./demo/load_model/load_and_predict.py>`_\\ , the missing word in the sentence could be predicted.\n\nTrain & Use\n^^^^^^^^^^^\n\n.. code-block:: python\n\n   from keras_bert import get_base_dict, get_model, gen_batch_inputs\n\n\n   # A toy input example\n   sentence_pairs = [\n       [['all', 'work', 'and', 'no', 'play'], ['makes', 'jack', 'a', 'dull', 'boy']],\n       [['from', 'the', 'day', 'forth'], ['my', 'arm', 'changed']],\n       [['and', 'a', 'voice', 'echoed'], ['power', 'give', 'me', 'more', 'power']],\n   ]\n\n\n   # Build token dictionary\n   token_dict = get_base_dict()  # A dict that contains some special tokens\n   for pairs in sentence_pairs:\n       for token in pairs[0] + pairs[1]:\n           if token not in token_dict:\n               token_dict[token] = len(token_dict)\n   token_list = list(token_dict.keys())  # Used for selecting a random word\n\n\n   # Build & train the model\n   model = get_model(\n       token_num=len(token_dict),\n       head_num=5,\n       transformer_num=12,\n       embed_dim=25,\n       feed_forward_dim=100,\n       seq_len=20,\n       pos_num=20,\n       dropout_rate=0.05,\n   )\n   model.summary()\n\n   def _generator():\n       while True:\n           yield gen_batch_inputs(\n               sentence_pairs,\n               token_dict,\n               token_list,\n               seq_len=20,\n               mask_rate=0.3,\n               swap_sentence_rate=1.0,\n           )\n\n   model.fit_generator(\n       generator=_generator(),\n       steps_per_epoch=1000,\n       epochs=100,\n       validation_data=_generator(),\n       validation_steps=100,\n       callbacks=[\n           keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n       ],\n   )\n\n\n   # Use the trained model\n   inputs, output_layer = get_model(  # `output_layer` is the last feature extraction layer (the last transformer)\n       token_num=len(token_dict),\n       head_num=5,\n       transformer_num=12,\n       embed_dim=25,\n       feed_forward_dim=100,\n       seq_len=20,\n       pos_num=20,\n       dropout_rate=0.05,\n       training=False,  # The input layers and output layer will be returned if `training` is `False`\n   )\n\nCustom Feature Extraction\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: python\n\n   def _custom_layers(x, trainable=True):\n       return keras.layers.LSTM(\n           units=768,\n           trainable=trainable,\n           name='LSTM',\n       )(x)\n\n   model = get_model(\n       token_num=200,\n       embed_dim=768,\n       custom_layers=_custom_layers,\n   )", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/HighCWu/keras-bert-tpu", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "keras-bert-tpu", "package_url": "https://pypi.org/project/keras-bert-tpu/", "platform": "", "project_url": "https://pypi.org/project/keras-bert-tpu/", "project_urls": {"Homepage": "https://github.com/HighCWu/keras-bert-tpu"}, "release_url": "https://pypi.org/project/keras-bert-tpu/0.1.7/", "requires_dist": null, "requires_python": "", "summary": "BERT implemented in Keras of Tensorflow package on TPU", "version": "0.1.7", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://travis-ci.org/HighCWu/keras-bert-tpu\" rel=\"nofollow\"><img alt=\"Travis\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6b744119bfaccb9f6cc534f8ba3d918135f1b17c/68747470733a2f2f7472617669732d63692e6f72672f486967684357752f6b657261732d626572742d7470752e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://coveralls.io/github/HighCWu/keras-bert-tpu\" rel=\"nofollow\"><img alt=\"Coverage\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6ae34e6dd4435035b68ac2df62a623a07057e22f/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f486967684357752f6b657261732d626572742d7470752f62616467652e7376673f6272616e63683d6d6173746572\"></a>\n<p>This is a fork of <a href=\"https://github.com/CyberZHG/keras-bert\" rel=\"nofollow\">CyberZHG/keras_bert</a> which supports Keras BERT on TPU.</p>\n<p>Implementation of the <a href=\"https://arxiv.org/pdf/1810.04805.pdf\" rel=\"nofollow\">BERT</a>. Official pre-trained models could be loaded for feature extraction and prediction.</p>\n<div id=\"colab-demo\">\n<h2>Colab Demo</h2>\n<p><a href=\"https://colab.research.google.com/github/HighCWu/keras-bert-tpu/blob/master/demo/load_model/load_and_predict.ipynb\" rel=\"nofollow\">HighCWu/keras-bert-tpu</a></p>\n</div>\n<div id=\"install\">\n<h2>Install</h2>\n<pre>pip install keras-bert-tpu\n</pre>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<div id=\"load-official-pre-trained-models\">\n<h3>Load Official Pre-trained Models</h3>\n<p>In <a href=\"./demo/load_model/load_and_extract.py\" rel=\"nofollow\">feature extraction demo</a>, you should be able to get the same extraction result as the official model. And in <a href=\"./demo/load_model/load_and_predict.py\" rel=\"nofollow\">prediction demo</a>, the missing word in the sentence could be predicted.</p>\n</div>\n<div id=\"train-use\">\n<h3>Train &amp; Use</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">keras_bert</span> <span class=\"kn\">import</span> <span class=\"n\">get_base_dict</span><span class=\"p\">,</span> <span class=\"n\">get_model</span><span class=\"p\">,</span> <span class=\"n\">gen_batch_inputs</span>\n\n\n<span class=\"c1\"># A toy input example</span>\n<span class=\"n\">sentence_pairs</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[[</span><span class=\"s1\">'all'</span><span class=\"p\">,</span> <span class=\"s1\">'work'</span><span class=\"p\">,</span> <span class=\"s1\">'and'</span><span class=\"p\">,</span> <span class=\"s1\">'no'</span><span class=\"p\">,</span> <span class=\"s1\">'play'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'makes'</span><span class=\"p\">,</span> <span class=\"s1\">'jack'</span><span class=\"p\">,</span> <span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'dull'</span><span class=\"p\">,</span> <span class=\"s1\">'boy'</span><span class=\"p\">]],</span>\n    <span class=\"p\">[[</span><span class=\"s1\">'from'</span><span class=\"p\">,</span> <span class=\"s1\">'the'</span><span class=\"p\">,</span> <span class=\"s1\">'day'</span><span class=\"p\">,</span> <span class=\"s1\">'forth'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'my'</span><span class=\"p\">,</span> <span class=\"s1\">'arm'</span><span class=\"p\">,</span> <span class=\"s1\">'changed'</span><span class=\"p\">]],</span>\n    <span class=\"p\">[[</span><span class=\"s1\">'and'</span><span class=\"p\">,</span> <span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'voice'</span><span class=\"p\">,</span> <span class=\"s1\">'echoed'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'power'</span><span class=\"p\">,</span> <span class=\"s1\">'give'</span><span class=\"p\">,</span> <span class=\"s1\">'me'</span><span class=\"p\">,</span> <span class=\"s1\">'more'</span><span class=\"p\">,</span> <span class=\"s1\">'power'</span><span class=\"p\">]],</span>\n<span class=\"p\">]</span>\n\n\n<span class=\"c1\"># Build token dictionary</span>\n<span class=\"n\">token_dict</span> <span class=\"o\">=</span> <span class=\"n\">get_base_dict</span><span class=\"p\">()</span>  <span class=\"c1\"># A dict that contains some special tokens</span>\n<span class=\"k\">for</span> <span class=\"n\">pairs</span> <span class=\"ow\">in</span> <span class=\"n\">sentence_pairs</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">token</span> <span class=\"ow\">in</span> <span class=\"n\">pairs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">pairs</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]:</span>\n        <span class=\"k\">if</span> <span class=\"n\">token</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">token_dict</span><span class=\"p\">:</span>\n            <span class=\"n\">token_dict</span><span class=\"p\">[</span><span class=\"n\">token</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">token_dict</span><span class=\"p\">)</span>\n<span class=\"n\">token_list</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">token_dict</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">())</span>  <span class=\"c1\"># Used for selecting a random word</span>\n\n\n<span class=\"c1\"># Build &amp; train the model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">get_model</span><span class=\"p\">(</span>\n    <span class=\"n\">token_num</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">token_dict</span><span class=\"p\">),</span>\n    <span class=\"n\">head_num</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"n\">transformer_num</span><span class=\"o\">=</span><span class=\"mi\">12</span><span class=\"p\">,</span>\n    <span class=\"n\">embed_dim</span><span class=\"o\">=</span><span class=\"mi\">25</span><span class=\"p\">,</span>\n    <span class=\"n\">feed_forward_dim</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n    <span class=\"n\">seq_len</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span>\n    <span class=\"n\">pos_num</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span>\n    <span class=\"n\">dropout_rate</span><span class=\"o\">=</span><span class=\"mf\">0.05</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"p\">()</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">_generator</span><span class=\"p\">():</span>\n    <span class=\"k\">while</span> <span class=\"kc\">True</span><span class=\"p\">:</span>\n        <span class=\"k\">yield</span> <span class=\"n\">gen_batch_inputs</span><span class=\"p\">(</span>\n            <span class=\"n\">sentence_pairs</span><span class=\"p\">,</span>\n            <span class=\"n\">token_dict</span><span class=\"p\">,</span>\n            <span class=\"n\">token_list</span><span class=\"p\">,</span>\n            <span class=\"n\">seq_len</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span>\n            <span class=\"n\">mask_rate</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">,</span>\n            <span class=\"n\">swap_sentence_rate</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">,</span>\n        <span class=\"p\">)</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit_generator</span><span class=\"p\">(</span>\n    <span class=\"n\">generator</span><span class=\"o\">=</span><span class=\"n\">_generator</span><span class=\"p\">(),</span>\n    <span class=\"n\">steps_per_epoch</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span>\n    <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n    <span class=\"n\">validation_data</span><span class=\"o\">=</span><span class=\"n\">_generator</span><span class=\"p\">(),</span>\n    <span class=\"n\">validation_steps</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n    <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[</span>\n        <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">EarlyStopping</span><span class=\"p\">(</span><span class=\"n\">monitor</span><span class=\"o\">=</span><span class=\"s1\">'val_loss'</span><span class=\"p\">,</span> <span class=\"n\">patience</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n    <span class=\"p\">],</span>\n<span class=\"p\">)</span>\n\n\n<span class=\"c1\"># Use the trained model</span>\n<span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">output_layer</span> <span class=\"o\">=</span> <span class=\"n\">get_model</span><span class=\"p\">(</span>  <span class=\"c1\"># `output_layer` is the last feature extraction layer (the last transformer)</span>\n    <span class=\"n\">token_num</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">token_dict</span><span class=\"p\">),</span>\n    <span class=\"n\">head_num</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"n\">transformer_num</span><span class=\"o\">=</span><span class=\"mi\">12</span><span class=\"p\">,</span>\n    <span class=\"n\">embed_dim</span><span class=\"o\">=</span><span class=\"mi\">25</span><span class=\"p\">,</span>\n    <span class=\"n\">feed_forward_dim</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n    <span class=\"n\">seq_len</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span>\n    <span class=\"n\">pos_num</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span>\n    <span class=\"n\">dropout_rate</span><span class=\"o\">=</span><span class=\"mf\">0.05</span><span class=\"p\">,</span>\n    <span class=\"n\">training</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>  <span class=\"c1\"># The input layers and output layer will be returned if `training` is `False`</span>\n<span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"custom-feature-extraction\">\n<h3>Custom Feature Extraction</h3>\n<pre><span class=\"k\">def</span> <span class=\"nf\">_custom_layers</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">trainable</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">LSTM</span><span class=\"p\">(</span>\n        <span class=\"n\">units</span><span class=\"o\">=</span><span class=\"mi\">768</span><span class=\"p\">,</span>\n        <span class=\"n\">trainable</span><span class=\"o\">=</span><span class=\"n\">trainable</span><span class=\"p\">,</span>\n        <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'LSTM'</span><span class=\"p\">,</span>\n    <span class=\"p\">)(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">get_model</span><span class=\"p\">(</span>\n    <span class=\"n\">token_num</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span>\n    <span class=\"n\">embed_dim</span><span class=\"o\">=</span><span class=\"mi\">768</span><span class=\"p\">,</span>\n    <span class=\"n\">custom_layers</span><span class=\"o\">=</span><span class=\"n\">_custom_layers</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</pre>\n</div>\n</div>\n\n          </div>"}, "last_serial": 4859734, "releases": {"0.1.2": [{"comment_text": "", "digests": {"md5": "04fd6a148c7ab210131fe101ee7db18e", "sha256": "0f7bcd63d6fb7566408423d68a8a53b0475d40ba2b0626d14482836f648994f1"}, "downloads": -1, "filename": "keras-bert-tpu-0.1.2.tar.gz", "has_sig": false, "md5_digest": "04fd6a148c7ab210131fe101ee7db18e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16540, "upload_time": "2018-12-03T05:54:36", "upload_time_iso_8601": "2018-12-03T05:54:36.017257Z", "url": "https://files.pythonhosted.org/packages/74/ec/3925cdb594e379cd69e13f97fcb8e863003097e8f8e85ea805c7f0e45a4e/keras-bert-tpu-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "524bd558b59275bf25d447054b7e8833", "sha256": "18981dea9f26fba96ea65dfe43288924f3ae683f66fce711af7a7b70755c801b"}, "downloads": -1, "filename": "keras-bert-tpu-0.1.3.tar.gz", "has_sig": false, "md5_digest": "524bd558b59275bf25d447054b7e8833", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16647, "upload_time": "2018-12-03T06:06:49", "upload_time_iso_8601": "2018-12-03T06:06:49.770294Z", "url": "https://files.pythonhosted.org/packages/67/d2/40663c6688e238d39854f26e4bb25e96e7433369c6f83d76f3a9ec4c57d8/keras-bert-tpu-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "17286a4f03e768d363e8b7ff3dcad89b", "sha256": "d7457e7d0650b1547db7f2091453640a67b32c2f1dda9f8804636d1a55378d0c"}, "downloads": -1, "filename": "keras-bert-tpu-0.1.4.tar.gz", "has_sig": false, "md5_digest": "17286a4f03e768d363e8b7ff3dcad89b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16615, "upload_time": "2018-12-03T07:04:32", "upload_time_iso_8601": "2018-12-03T07:04:32.213859Z", "url": "https://files.pythonhosted.org/packages/e8/0e/0547ef821775db74f1fe0179c9850ffea43ed46e909537535ba94cc52d3f/keras-bert-tpu-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "1ae44626a96a5046793113ebff682059", "sha256": "b7af0228740f8114fc9dd376664cf7d135c2bbf0ae541262429637965510eaf1"}, "downloads": -1, "filename": "keras-bert-tpu-0.1.5.tar.gz", "has_sig": false, "md5_digest": "1ae44626a96a5046793113ebff682059", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17510, "upload_time": "2018-12-03T11:54:20", "upload_time_iso_8601": "2018-12-03T11:54:20.940879Z", "url": "https://files.pythonhosted.org/packages/ad/c8/50a41937a876944c00d287a5a2efc26cf9c8988750db8f46dc36a93b1bb3/keras-bert-tpu-0.1.5.tar.gz", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "fd396a1fa17e9830c366360c12b5d8f2", "sha256": "acaaa4d24bb637654f1cf15b4e5c406562a428ac68dda6453eee0f49a3ae7938"}, "downloads": -1, "filename": "keras-bert-tpu-0.1.6.tar.gz", "has_sig": false, "md5_digest": "fd396a1fa17e9830c366360c12b5d8f2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17527, "upload_time": "2018-12-20T12:15:17", "upload_time_iso_8601": "2018-12-20T12:15:17.293249Z", "url": "https://files.pythonhosted.org/packages/55/ae/b8d87207349d8cbf3ef5ed759e21993890d082fbbdba0c3425e8d860cf43/keras-bert-tpu-0.1.6.tar.gz", "yanked": false}], "0.1.7": [{"comment_text": "", "digests": {"md5": "154f57505e1d08977d5731423c6b20cd", "sha256": "8ecd1a95c82d9bca2d2e1f5b29e06a0a15eb4a5c468784f613b02b17e3124f1f"}, "downloads": -1, "filename": "keras-bert-tpu-0.1.7.tar.gz", "has_sig": false, "md5_digest": "154f57505e1d08977d5731423c6b20cd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17494, "upload_time": "2019-02-24T01:19:27", "upload_time_iso_8601": "2019-02-24T01:19:27.922729Z", "url": "https://files.pythonhosted.org/packages/e0/d1/4a5d9535ec23272ab70fe7dc89b391e6bfc19a09b38cfff3a21ffeb4ca07/keras-bert-tpu-0.1.7.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "154f57505e1d08977d5731423c6b20cd", "sha256": "8ecd1a95c82d9bca2d2e1f5b29e06a0a15eb4a5c468784f613b02b17e3124f1f"}, "downloads": -1, "filename": "keras-bert-tpu-0.1.7.tar.gz", "has_sig": false, "md5_digest": "154f57505e1d08977d5731423c6b20cd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17494, "upload_time": "2019-02-24T01:19:27", "upload_time_iso_8601": "2019-02-24T01:19:27.922729Z", "url": "https://files.pythonhosted.org/packages/e0/d1/4a5d9535ec23272ab70fe7dc89b391e6bfc19a09b38cfff3a21ffeb4ca07/keras-bert-tpu-0.1.7.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:14 2020"}