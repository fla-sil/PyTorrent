{"info": {"author": "Utrecht University", "author_email": "asreview@uu.nl", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "## ASReview-hyperopt\n\n![Deploy and release](https://github.com/asreview/asreview-hyperopt/workflows/Deploy%20and%20release/badge.svg)![Build status](https://github.com/asreview/asreview-hyperopt/workflows/test-suite/badge.svg)\n\nHyper parameter optimization extension for \n[ASReview](https://github.com/asreview/asreview). It uses the \n[hyperopt](https://github.com/hyperopt/hyperopt) package to quickly optimize parameters\nof the different models. The hyper parameters and their sample space are defined in the\n[ASReview](https://github.com/asreview/asreview) package, and \nautomatically used for hyper parameter optimization.\n\n### Installation\n\nThe easiest way to install the hyper parameter optimization package is to use the command line:\n\n``` bash\npip install asreview-hyperopt\n```\n\nAfter installation of the visualization package, asreview should automatically detect it.\nTest this by:\n\n```bash\nasreview --help\n```\n\nIt should list three new entry points: `hyper-active`, `hyper-passive` and `hyper-cluster`.\n\n### Basic usage\n\nThe three entry-points are used in a roughly similar fashion. The main difference between them is\nthe types of models that have to be supplied:\n\n- hyper-cluster: feature_extraction\n- hyper-passive: model, balance\\_strategy, feature\\_extraction\n- hyper-active: model, balance\\_strategy, query\\_strategy, feature\\_extraction\n\n\nTo get help for entry points type:\n\n```bash\nasreview hyper-active --help\n```\n\nWhich results in the following options:\n\n```bash\nusage: hyper-active [-h] [-n N_ITER] [-r N_RUN] [-d DATASETS] [--mpi]\n                    [--data_dir DATA_DIR] [--output_dir OUTPUT_DIR]\n                    [--server_job] [-m MODEL] [-q QUERY_STRATEGY]\n                    [-b BALANCE_STRATEGY] [-e FEATURE_EXTRACTION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -n N_ITER, --n_iter N_ITER\n                        Number of iterations of Bayesian Optimization.\n  -r N_RUN, --n_run N_RUN\n                        Number of runs per dataset.\n  -d DATASETS, --datasets DATASETS\n                        Datasets to use in the hyper parameter optimization\n                        Separate by commas to use multiple at the same time\n                        [default: all].\n  --mpi                 Use the mpi implementation.\n  --data_dir DATA_DIR   Base directory with data files.\n  --output_dir OUTPUT_DIR\n                        Output directory for trials.\n  --server_job          Run job on the server. It will incur less overhead of\n                        used CPUs, but more latency of workers waiting for the\n                        server to finish its own job. Only makes sense in\n                        combination with the flag --mpi.\n  -m MODEL, --model MODEL\n                        Prediction model for active learning.\n  -q QUERY_STRATEGY, --query_strategy QUERY_STRATEGY\n                        Query strategy for active learning.\n  -b BALANCE_STRATEGY, --balance_strategy BALANCE_STRATEGY\n                        Balance strategy for active learning.\n  -e FEATURE_EXTRACTION, --feature_extraction FEATURE_EXTRACTION\n                        Feature extraction method.\n\n```\n\n### Data structure\n\nThe extension will by default search for datasets in the `data` directory, relative to the current\nworking directory. Either put your datasets there, or specify and data directory.\n\nThe output of the runs will by default be stored in the `output` directory, relative to\nthe current path.\n\nAn example of a structure that has been created:\n\n```bash\noutput/\n\u251c\u2500\u2500 active_learning\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 nb_max_double_tfidf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 depression_hall_ace_ptsd_nagtegaal\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 best\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 ace\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 depression\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hall\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 ptsd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 current\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 ace\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 depression\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hall\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 ptsd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 trials.pkl\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 nb_max_random_double_tfidf\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 best\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 current\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 trials.pkl\n\u251c\u2500\u2500 cluster\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 doc2vec\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ace\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 best\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ace\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 current\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ace\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 trials.pkl\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 depression_hall_ace_ptsd_nagtegaal\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 current\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 ace\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 depression\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 hall\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 ptsd\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 current\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 nagtegaal\n\u2514\u2500\u2500 passive\n    \u2514\u2500\u2500 nb_double_tfidf\n        \u2514\u2500\u2500 depression\n            \u251c\u2500\u2500 best\n            \u2502\u00a0\u00a0 \u2514\u2500\u2500 depression\n            \u251c\u2500\u2500 current\n            \u2502\u00a0\u00a0 \u2514\u2500\u2500 depression\n            \u2514\u2500\u2500 trials.pkl\n```\n\nThe files with name `trials.pkl` are special files that contain data on which trials were run.\n\nTo list these trials, use the following command:\n\n```bash\nasreview show $SOME_DIRECTORY/trials.pkl\n```\n\nIt should give a list of trials sorted by the loss (lower is better). The column names (apart\nfrom the loss) are prefixed with the kind of parameter it is:\n\n- `mdl`: Model parameter\n- `bal`: Balance strategy parameter\n- `qry`: Query strategy parameter\n- `fex`: Feature extraction parameter\n\n### Options\n\nThe default number of iterations is 1, which you'll probably want to increase. It depends on the\nnumber of hyper-parameters that need to be optimized, but several hundred iterations is probably\na good estimate for most combinations to get reasonably close to the optimum. In all cases,\nuse good common sense; if the loss is still going down at a quick pace, do a few more iterations.\n\nThe hyperopt extension has built-in support for MPI. MPI is used for parallelization of runs. On\na local PC with an MPI-implementation (like OpenMPI) installed, one could run with 4 cores:\n\n```bash\nmpirun -n 4 asreview hyper-active --mpi\n```\n\nIf you want to be slightly more efficient on a machine with a low number of cores, you can run\njobs on the MPI server as well:\n\n```bash\nmpirun -n 4 asreview hyper-active --mpi --server_job\n```\n\nOn super computers one should sometimes replace `mpirun` with `srun`.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/asreview/asreview-hyperopt", "keywords": "asreview plot hyperopt optimization", "license": "", "maintainer": "", "maintainer_email": "", "name": "asreview-hyperopt", "package_url": "https://pypi.org/project/asreview-hyperopt/", "platform": "", "project_url": "https://pypi.org/project/asreview-hyperopt/", "project_urls": {"Bug Reports": "https://github.com/asreview/asreview-hyperopt/issues", "Homepage": "https://github.com/asreview/asreview-hyperopt", "Source": "https://github.com/asreview/asreview-hyperopt"}, "release_url": "https://pypi.org/project/asreview-hyperopt/0.2.0/", "requires_dist": ["asreview (>=0.7.0)", "numpy", "tqdm", "hyperopt", "sklearn", "mpi4py ; extra == 'all'", "mpi4py ; extra == 'mpi'"], "requires_python": "", "summary": "Hyper parameter optimization extension for ASReview", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h2>ASReview-hyperopt</h2>\n<p><img alt=\"Deploy and release\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/53008f9afd86db05c691621b4a75c94c36d96b6f/68747470733a2f2f6769746875622e636f6d2f61737265766965772f61737265766965772d68797065726f70742f776f726b666c6f77732f4465706c6f79253230616e6425323072656c656173652f62616467652e737667\"><img alt=\"Build status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f00034b1eef0b72791e7cd49d0fb24d2783e1f3a/68747470733a2f2f6769746875622e636f6d2f61737265766965772f61737265766965772d68797065726f70742f776f726b666c6f77732f746573742d73756974652f62616467652e737667\"></p>\n<p>Hyper parameter optimization extension for\n<a href=\"https://github.com/asreview/asreview\" rel=\"nofollow\">ASReview</a>. It uses the\n<a href=\"https://github.com/hyperopt/hyperopt\" rel=\"nofollow\">hyperopt</a> package to quickly optimize parameters\nof the different models. The hyper parameters and their sample space are defined in the\n<a href=\"https://github.com/asreview/asreview\" rel=\"nofollow\">ASReview</a> package, and\nautomatically used for hyper parameter optimization.</p>\n<h3>Installation</h3>\n<p>The easiest way to install the hyper parameter optimization package is to use the command line:</p>\n<pre>pip install asreview-hyperopt\n</pre>\n<p>After installation of the visualization package, asreview should automatically detect it.\nTest this by:</p>\n<pre>asreview --help\n</pre>\n<p>It should list three new entry points: <code>hyper-active</code>, <code>hyper-passive</code> and <code>hyper-cluster</code>.</p>\n<h3>Basic usage</h3>\n<p>The three entry-points are used in a roughly similar fashion. The main difference between them is\nthe types of models that have to be supplied:</p>\n<ul>\n<li>hyper-cluster: feature_extraction</li>\n<li>hyper-passive: model, balance_strategy, feature_extraction</li>\n<li>hyper-active: model, balance_strategy, query_strategy, feature_extraction</li>\n</ul>\n<p>To get help for entry points type:</p>\n<pre>asreview hyper-active --help\n</pre>\n<p>Which results in the following options:</p>\n<pre>usage: hyper-active <span class=\"o\">[</span>-h<span class=\"o\">]</span> <span class=\"o\">[</span>-n N_ITER<span class=\"o\">]</span> <span class=\"o\">[</span>-r N_RUN<span class=\"o\">]</span> <span class=\"o\">[</span>-d DATASETS<span class=\"o\">]</span> <span class=\"o\">[</span>--mpi<span class=\"o\">]</span>\n                    <span class=\"o\">[</span>--data_dir DATA_DIR<span class=\"o\">]</span> <span class=\"o\">[</span>--output_dir OUTPUT_DIR<span class=\"o\">]</span>\n                    <span class=\"o\">[</span>--server_job<span class=\"o\">]</span> <span class=\"o\">[</span>-m MODEL<span class=\"o\">]</span> <span class=\"o\">[</span>-q QUERY_STRATEGY<span class=\"o\">]</span>\n                    <span class=\"o\">[</span>-b BALANCE_STRATEGY<span class=\"o\">]</span> <span class=\"o\">[</span>-e FEATURE_EXTRACTION<span class=\"o\">]</span>\n\noptional arguments:\n  -h, --help            show this <span class=\"nb\">help</span> message and <span class=\"nb\">exit</span>\n  -n N_ITER, --n_iter N_ITER\n                        Number of iterations of Bayesian Optimization.\n  -r N_RUN, --n_run N_RUN\n                        Number of runs per dataset.\n  -d DATASETS, --datasets DATASETS\n                        Datasets to use in the hyper parameter optimization\n                        Separate by commas to use multiple at the same <span class=\"nb\">time</span>\n                        <span class=\"o\">[</span>default: all<span class=\"o\">]</span>.\n  --mpi                 Use the mpi implementation.\n  --data_dir DATA_DIR   Base directory with data files.\n  --output_dir OUTPUT_DIR\n                        Output directory <span class=\"k\">for</span> trials.\n  --server_job          Run job on the server. It will incur less overhead of\n                        used CPUs, but more latency of workers waiting <span class=\"k\">for</span> the\n                        server to finish its own job. Only makes sense in\n                        combination with the flag --mpi.\n  -m MODEL, --model MODEL\n                        Prediction model <span class=\"k\">for</span> active learning.\n  -q QUERY_STRATEGY, --query_strategy QUERY_STRATEGY\n                        Query strategy <span class=\"k\">for</span> active learning.\n  -b BALANCE_STRATEGY, --balance_strategy BALANCE_STRATEGY\n                        Balance strategy <span class=\"k\">for</span> active learning.\n  -e FEATURE_EXTRACTION, --feature_extraction FEATURE_EXTRACTION\n                        Feature extraction method.\n</pre>\n<h3>Data structure</h3>\n<p>The extension will by default search for datasets in the <code>data</code> directory, relative to the current\nworking directory. Either put your datasets there, or specify and data directory.</p>\n<p>The output of the runs will by default be stored in the <code>output</code> directory, relative to\nthe current path.</p>\n<p>An example of a structure that has been created:</p>\n<pre>output/\n\u251c\u2500\u2500 active_learning\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 nb_max_double_tfidf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 depression_hall_ace_ptsd_nagtegaal\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 best\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 ace\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 depression\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hall\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 ptsd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 current\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 ace\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 depression\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hall\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 ptsd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 trials.pkl\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 nb_max_random_double_tfidf\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 best\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 current\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 trials.pkl\n\u251c\u2500\u2500 cluster\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 doc2vec\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ace\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 best\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ace\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 current\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ace\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 trials.pkl\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 depression_hall_ace_ptsd_nagtegaal\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 current\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 ace\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 depression\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 hall\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 ptsd\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 nagtegaal\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 current\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 nagtegaal\n\u2514\u2500\u2500 passive\n    \u2514\u2500\u2500 nb_double_tfidf\n        \u2514\u2500\u2500 depression\n            \u251c\u2500\u2500 best\n            \u2502\u00a0\u00a0 \u2514\u2500\u2500 depression\n            \u251c\u2500\u2500 current\n            \u2502\u00a0\u00a0 \u2514\u2500\u2500 depression\n            \u2514\u2500\u2500 trials.pkl\n</pre>\n<p>The files with name <code>trials.pkl</code> are special files that contain data on which trials were run.</p>\n<p>To list these trials, use the following command:</p>\n<pre>asreview show <span class=\"nv\">$SOME_DIRECTORY</span>/trials.pkl\n</pre>\n<p>It should give a list of trials sorted by the loss (lower is better). The column names (apart\nfrom the loss) are prefixed with the kind of parameter it is:</p>\n<ul>\n<li><code>mdl</code>: Model parameter</li>\n<li><code>bal</code>: Balance strategy parameter</li>\n<li><code>qry</code>: Query strategy parameter</li>\n<li><code>fex</code>: Feature extraction parameter</li>\n</ul>\n<h3>Options</h3>\n<p>The default number of iterations is 1, which you'll probably want to increase. It depends on the\nnumber of hyper-parameters that need to be optimized, but several hundred iterations is probably\na good estimate for most combinations to get reasonably close to the optimum. In all cases,\nuse good common sense; if the loss is still going down at a quick pace, do a few more iterations.</p>\n<p>The hyperopt extension has built-in support for MPI. MPI is used for parallelization of runs. On\na local PC with an MPI-implementation (like OpenMPI) installed, one could run with 4 cores:</p>\n<pre>mpirun -n <span class=\"m\">4</span> asreview hyper-active --mpi\n</pre>\n<p>If you want to be slightly more efficient on a machine with a low number of cores, you can run\njobs on the MPI server as well:</p>\n<pre>mpirun -n <span class=\"m\">4</span> asreview hyper-active --mpi --server_job\n</pre>\n<p>On super computers one should sometimes replace <code>mpirun</code> with <code>srun</code>.</p>\n\n          </div>"}, "last_serial": 6892320, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "b3a85694b9a2d2b4682abe6c17872867", "sha256": "5cbd9d59ec58c6f6cbe1ecc0d6369d71a3bc10ca1ace2441649da6170ca2c47f"}, "downloads": -1, "filename": "asreview_hyperopt-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "b3a85694b9a2d2b4682abe6c17872867", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23988, "upload_time": "2020-01-29T17:40:58", "upload_time_iso_8601": "2020-01-29T17:40:58.275673Z", "url": "https://files.pythonhosted.org/packages/f7/b4/22ed9b812c65df6d99529f34d53e7708ad2f7492c21fcb58250dd7a26167/asreview_hyperopt-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "54fc579ad1a397b68e8eaaa56d45ad15", "sha256": "dd567ad614cf425a3572171e064e230c871171c6a1b2751205062356c86d4746"}, "downloads": -1, "filename": "asreview-hyperopt-0.1.1.tar.gz", "has_sig": false, "md5_digest": "54fc579ad1a397b68e8eaaa56d45ad15", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10748, "upload_time": "2020-01-29T17:41:00", "upload_time_iso_8601": "2020-01-29T17:41:00.300860Z", "url": "https://files.pythonhosted.org/packages/99/b2/12443c0f15b76d7947ffd2d64e7e7645f2eb60e7e9433628992e13a76f05/asreview-hyperopt-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "562262d6b1a52bd6133c498c1378143b", "sha256": "47265b5c2c08111b14a51e41db29feb918f0974ddb7d8e634d8dd9f9a6acbc03"}, "downloads": -1, "filename": "asreview_hyperopt-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "562262d6b1a52bd6133c498c1378143b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 24016, "upload_time": "2020-02-03T13:34:52", "upload_time_iso_8601": "2020-02-03T13:34:52.032017Z", "url": "https://files.pythonhosted.org/packages/76/ca/fd3adbaa446c99097a9bd959776cb4521eacd04b20936011fa1617b1c1ca/asreview_hyperopt-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ba9171a129e03e943d14028053427f70", "sha256": "e69bd3e4ad1c86947888455cee4baedb952aa83a1d96ce7435b315c0b6f93ed0"}, "downloads": -1, "filename": "asreview-hyperopt-0.1.2.tar.gz", "has_sig": false, "md5_digest": "ba9171a129e03e943d14028053427f70", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10791, "upload_time": "2020-02-03T13:34:53", "upload_time_iso_8601": "2020-02-03T13:34:53.219584Z", "url": "https://files.pythonhosted.org/packages/2d/10/a63c0609506620c874f0923240e072489762d813f921fa659bb092a0b2b4/asreview-hyperopt-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "ab95ef6d0ab055fa6efab198369b8c66", "sha256": "2c9fa85ff59c2fed7cd5f0b78345afbdb42be24d38d22d70b247e3ad00c86908"}, "downloads": -1, "filename": "asreview_hyperopt-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "ab95ef6d0ab055fa6efab198369b8c66", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 26312, "upload_time": "2020-03-12T10:49:07", "upload_time_iso_8601": "2020-03-12T10:49:07.389179Z", "url": "https://files.pythonhosted.org/packages/5a/3c/f35c433a4d14df8781f357df336db7c380aa7f278b0717ab50aa4579d6c6/asreview_hyperopt-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c92bf21520ce7b1a990fd38bbc2166f9", "sha256": "86d23f4ecb507949c7509e23490efc7046a1be1cb64a720f5bdd5f0addf7434d"}, "downloads": -1, "filename": "asreview-hyperopt-0.1.3.tar.gz", "has_sig": false, "md5_digest": "c92bf21520ce7b1a990fd38bbc2166f9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12172, "upload_time": "2020-03-12T10:49:08", "upload_time_iso_8601": "2020-03-12T10:49:08.299139Z", "url": "https://files.pythonhosted.org/packages/6f/08/1b991d46a619865834b8aa62bad40c48aaa75e6a6ebc103bb8c6a230d811/asreview-hyperopt-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "6b73ab6dc963d97848be9a5aec27bb52", "sha256": "875b83bab29e2775b60ead97e60040d448e11de3fb1fb5c4608d0e3a124b32d6"}, "downloads": -1, "filename": "asreview_hyperopt-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "6b73ab6dc963d97848be9a5aec27bb52", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 26855, "upload_time": "2020-03-12T15:49:42", "upload_time_iso_8601": "2020-03-12T15:49:42.952626Z", "url": "https://files.pythonhosted.org/packages/69/8f/5b2b2d4a2e90142938aed520de229bc87e9a1eb18e255acbef5f779fb05f/asreview_hyperopt-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "62d9795efbaeadcaa407f6573062c73f", "sha256": "90437e578c624efb9a7de5f9353d2dc1c16c3031f3f735d2cc498441f14c8028"}, "downloads": -1, "filename": "asreview-hyperopt-0.1.4.tar.gz", "has_sig": false, "md5_digest": "62d9795efbaeadcaa407f6573062c73f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12359, "upload_time": "2020-03-12T15:49:43", "upload_time_iso_8601": "2020-03-12T15:49:43.876129Z", "url": "https://files.pythonhosted.org/packages/3a/ad/722dc79497af8ebe29411b6a95b99ef68f4d7a2a1bb3222902a3b53fc565/asreview-hyperopt-0.1.4.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "81c15eb955a8341634ca531ef7b176c6", "sha256": "9823cec609c423f5f3643714147190c18fc0323ad53c17eac267e433cb9cf9cb"}, "downloads": -1, "filename": "asreview_hyperopt-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "81c15eb955a8341634ca531ef7b176c6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 28055, "upload_time": "2020-03-26T22:18:39", "upload_time_iso_8601": "2020-03-26T22:18:39.097707Z", "url": "https://files.pythonhosted.org/packages/96/70/cf48648024e3e6f40f523733ee9d8f56a134231d6e7708d12b5b910ea6aa/asreview_hyperopt-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "840a9416ae76eb075d53d512d2e57eac", "sha256": "e44b6d1177fdde664bddbf45833e18bfa92508b65be05f799391c1f2c91ba5f3"}, "downloads": -1, "filename": "asreview-hyperopt-0.2.0.tar.gz", "has_sig": false, "md5_digest": "840a9416ae76eb075d53d512d2e57eac", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14674, "upload_time": "2020-03-26T22:18:40", "upload_time_iso_8601": "2020-03-26T22:18:40.242786Z", "url": "https://files.pythonhosted.org/packages/ed/64/88b40d95126efb82362b1e91a9ac6d975d406d44e2bade5bee6cf4a37f24/asreview-hyperopt-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "81c15eb955a8341634ca531ef7b176c6", "sha256": "9823cec609c423f5f3643714147190c18fc0323ad53c17eac267e433cb9cf9cb"}, "downloads": -1, "filename": "asreview_hyperopt-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "81c15eb955a8341634ca531ef7b176c6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 28055, "upload_time": "2020-03-26T22:18:39", "upload_time_iso_8601": "2020-03-26T22:18:39.097707Z", "url": "https://files.pythonhosted.org/packages/96/70/cf48648024e3e6f40f523733ee9d8f56a134231d6e7708d12b5b910ea6aa/asreview_hyperopt-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "840a9416ae76eb075d53d512d2e57eac", "sha256": "e44b6d1177fdde664bddbf45833e18bfa92508b65be05f799391c1f2c91ba5f3"}, "downloads": -1, "filename": "asreview-hyperopt-0.2.0.tar.gz", "has_sig": false, "md5_digest": "840a9416ae76eb075d53d512d2e57eac", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14674, "upload_time": "2020-03-26T22:18:40", "upload_time_iso_8601": "2020-03-26T22:18:40.242786Z", "url": "https://files.pythonhosted.org/packages/ed/64/88b40d95126efb82362b1e91a9ac6d975d406d44e2bade5bee6cf4a37f24/asreview-hyperopt-0.2.0.tar.gz", "yanked": false}], "timestamp": "Thu May  7 18:16:55 2020"}