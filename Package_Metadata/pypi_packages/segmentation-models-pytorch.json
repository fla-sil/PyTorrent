{"info": {"author": "Pavel Yakubovskiy", "author_email": "qubvel@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy"], "description": "\n# Segmentation models\n[![Build Status](https://travis-ci.com/qubvel/segmentation_models.pytorch.svg?branch=master)](https://travis-ci.com/qubvel/segmentation_models.pytorch) [![Generic badge](https://img.shields.io/badge/License-MIT-<COLOR>.svg)](https://shields.io/)\n\nSegmentation models is python library with Neural Networks for Image Segmentation based on PyTorch.\n\nThe main features of this library are:\n\n - High level API (just two lines to create neural network)\n - 4 models architectures for binary and multi class segmentation (including legendary Unet)\n - 46 available encoders for each architecture\n - All encoders have pre-trained weights for faster and better convergence\n\n### Table of content\n 1. [Quick start](#start)\n 2. [Examples](#examples)\n 3. [Models](#models)\n    1. [Architectures](#architectires)\n    2. [Encoders](#encoders)\n 4. [Models API](#api)\n    1. [Input channels](#input-channels)\n    2. [Auxiliary classification output](#auxiliary-classification-output)\n    3. [Depth](#depth)\n 5. [Installation](#installation)\n 6. [Competitions won with the library](#competitions-won-with-the-library)\n 7. [License](#license)\n 8. [Contributing](#contributing)\n\n### Quick start <a name=\"start\"></a>\nSince the library is built on the PyTorch framework, created segmentation model is just a PyTorch nn.Module, which can be created as easy as:\n```python\nimport segmentation_models_pytorch as smp\n\nmodel = smp.Unet()\n```\nDepending on the task, you can change the network architecture by choosing backbones with fewer or more parameters and use pretrainded weights to initialize it:\n\n```python\nmodel = smp.Unet('resnet34', encoder_weights='imagenet')\n```\n\nChange number of output classes in the model:\n\n```python\nmodel = smp.Unet('resnet34', classes=3, activation='softmax')\n```\n\nAll models have pretrained encoders, so you have to prepare your data the same way as during weights pretraining:\n```python\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn\n\npreprocess_input = get_preprocessing_fn('resnet18', pretrained='imagenet')\n```\n### Examples <a name=\"examples\"></a>\n - Training model for cars segmentation on CamVid dataset [here](https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb).\n - Training SMP model with [Catalyst](https://github.com/catalyst-team/catalyst) (high-level framework for PyTorch), [Ttach](https://github.com/qubvel/ttach) (TTA library for PyTorch) and [Albumentations](https://github.com/albu/albumentations) (fast image augmentation library) - [here](https://github.com/catalyst-team/catalyst/blob/master/examples/notebooks/segmentation-tutorial.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/segmentation-tutorial.ipynb)\n\n### Models <a name=\"models\"></a>\n\n#### Architectures <a name=\"architectires\"></a>\n - [Unet](https://arxiv.org/abs/1505.04597)\n - [Linknet](https://arxiv.org/abs/1707.03718)\n - [FPN](http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf)\n - [PSPNet](https://arxiv.org/abs/1612.01105)\n\n#### Encoders <a name=\"encoders\"></a>\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|resnet18                        |imagenet                        |11M                             |\n|resnet34                        |imagenet                        |21M                             |\n|resnet50                        |imagenet                        |23M                             |\n|resnet101                       |imagenet                        |42M                             |\n|resnet152                       |imagenet                        |58M                             |\n|resnext50_32x4d                 |imagenet                        |22M                             |\n|resnext101_32x8d                |imagenet<br>instagram           |86M                             |\n|resnext101_32x16d               |instagram                       |191M                            |\n|resnext101_32x32d               |instagram                       |466M                            |\n|resnext101_32x48d               |instagram                       |826M                            |\n|dpn68                           |imagenet                        |11M                             |\n|dpn68b                          |imagenet+5k                     |11M                             |\n|dpn92                           |imagenet+5k                     |34M                             |\n|dpn98                           |imagenet                        |58M                             |\n|dpn107                          |imagenet+5k                     |84M                             |\n|dpn131                          |imagenet                        |76M                             |\n|vgg11                           |imagenet                        |9M                              |\n|vgg11_bn                        |imagenet                        |9M                              |\n|vgg13                           |imagenet                        |9M                              |\n|vgg13_bn                        |imagenet                        |9M                              |\n|vgg16                           |imagenet                        |14M                             |\n|vgg16_bn                        |imagenet                        |14M                             |\n|vgg19                           |imagenet                        |20M                             |\n|vgg19_bn                        |imagenet                        |20M                             |\n|senet154                        |imagenet                        |113M                            |\n|se_resnet50                     |imagenet                        |26M                             |\n|se_resnet101                    |imagenet                        |47M                             |\n|se_resnet152                    |imagenet                        |64M                             |\n|se_resnext50_32x4d              |imagenet                        |25M                             |\n|se_resnext101_32x4d             |imagenet                        |46M                             |\n|densenet121                     |imagenet                        |6M                              |\n|densenet169                     |imagenet                        |12M                             |\n|densenet201                     |imagenet                        |18M                             |\n|densenet161                     |imagenet                        |26M                             |\n|inceptionresnetv2               |imagenet<br>imagenet+background |54M                             |\n|inceptionv4                     |imagenet<br>imagenet+background |41M                             |\n|efficientnet-b0                 |imagenet                        |4M                              |\n|efficientnet-b1                 |imagenet                        |6M                              |\n|efficientnet-b2                 |imagenet                        |7M                              |\n|efficientnet-b3                 |imagenet                        |10M                             |\n|efficientnet-b4                 |imagenet                        |17M                             |\n|efficientnet-b5                 |imagenet                        |28M                             |\n|efficientnet-b6                 |imagenet                        |40M                             |\n|efficientnet-b7                 |imagenet                        |63M                             |\n|mobilenet_v2                    |imagenet                        |2M                              |\n|xception                        |imagenet                        |22M                             |\n\n### Models API <a name=\"api\"></a>\n\n - `model.encoder` - pretrained backbone to extract features of different spatial resolution\n - `model.decoder` - depends on models architecture (`Unet`/`Linknet`/`PSPNet`/`FPN`)\n - `model.segmentation_head` - last block to produce required number of mask channels (include also optional upsampling and activation)\n - `model.classification_head` - optional block which create classification head on top of encoder\n - `model.forward(x)` - sequentially pass `x` through model\\`s encoder, decoder and segmentation head (and classification head if specified)\n\n##### Input channels\nInput channels parameter allow you to create models, which process tensors with arbitrary number of channels.\nIf you use pretrained weights from imagenet - weights of first convolution will be reused for\n1- or 2- channels inputs, for input channels > 4 weights of first convolution will be initialized randomly.\n```python\nmodel = smp.FPN('resnet34', in_channels=1)\nmask = model(torch.ones([1, 1, 64, 64]))\n```\n\n##### Auxiliary classification output  \nAll models support `aux_params` parameters, which is default set to `None`. \nIf `aux_params = None` than classification auxiliary output is not created, else\nmodel produce not only `mask`, but also `label` output with shape `NC`.\nClassification head consist of GlobalPooling->Dropout(optional)->Linear->Activation(optional) layers, which can be \nconfigured by `aux_params` as follows:\n```python\naux_params=dict(\n    pooling='avg',             # one of 'avg', 'max'\n    dropout=0.5,               # dropout ratio, default is None\n    activation='sigmoid',      # activation function, default is None\n    classes=4,                 # define number of output labels\n)\nmodel = smp.Unet('resnet34', classes=4, aux_params=aux_params)\nmask, label = model(x)\n```\n\n##### Depth\nDepth parameter specify a number of downsampling operations in encoder, so you can make\nyour model lighted if specify smaller `depth`.\n```python\nmodel = smp.Unet('resnet34', encoder_depth=4)\n```\n\n\n### Installation <a name=\"installation\"></a>\nPyPI version:\n```bash\n$ pip install segmentation-models-pytorch\n````\nLatest version from source:\n```bash\n$ pip install git+https://github.com/qubvel/segmentation_models.pytorch\n````\n\n### Competitions won with the library\n\n`Segmentation Models` package is widely used in the image segmentation competitions.\n[Here](https://github.com/qubvel/segmentation_models.pytorch/blob/master/HALLOFFAME.md) you can find competitions, names of the winners and links to their solutions.\n\n\n### License <a name=\"license\"></a>\nProject is distributed under [MIT License](https://github.com/qubvel/segmentation_models.pytorch/blob/master/LICENSE)\n\n\n### Contributing\n\n##### Run test\n```bash\n$ docker build -f docker/Dockerfile.dev -t smp:dev . && docker run --rm smp:dev pytest -p no:cacheprovider\n```\n##### Generate table\n```bash\n$ docker build -f docker/Dockerfile.dev -t smp:dev . && docker run --rm smp:dev python misc/generate_table.py\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/qubvel/segmentation_models.pytorch", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "segmentation-models-pytorch", "package_url": "https://pypi.org/project/segmentation-models-pytorch/", "platform": "", "project_url": "https://pypi.org/project/segmentation-models-pytorch/", "project_urls": {"Homepage": "https://github.com/qubvel/segmentation_models.pytorch"}, "release_url": "https://pypi.org/project/segmentation-models-pytorch/0.1.0/", "requires_dist": ["torchvision (>=0.3.0)", "pretrainedmodels (==0.7.4)", "efficientnet-pytorch (>=0.5.1)", "pytest ; extra == 'test'"], "requires_python": ">=3.0.0", "summary": "Image segmentation models with pre-trained backbones. PyTorch.", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Segmentation models</h1>\n<p><a href=\"https://travis-ci.com/qubvel/segmentation_models.pytorch\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4bc3593a65d32f4d44badf2bdb39a83e953e4563/68747470733a2f2f7472617669732d63692e636f6d2f71756276656c2f7365676d656e746174696f6e5f6d6f64656c732e7079746f7263682e7376673f6272616e63683d6d6173746572\"></a> <a href=\"https://shields.io/\" rel=\"nofollow\"><img alt=\"Generic badge\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3640e33b57e07beef78cddfd20acc3f779f63bfe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d253343434f4c4f522533452e737667\"></a></p>\n<p>Segmentation models is python library with Neural Networks for Image Segmentation based on PyTorch.</p>\n<p>The main features of this library are:</p>\n<ul>\n<li>High level API (just two lines to create neural network)</li>\n<li>4 models architectures for binary and multi class segmentation (including legendary Unet)</li>\n<li>46 available encoders for each architecture</li>\n<li>All encoders have pre-trained weights for faster and better convergence</li>\n</ul>\n<h3>Table of content</h3>\n<ol>\n<li><a href=\"#start\" rel=\"nofollow\">Quick start</a></li>\n<li><a href=\"#examples\" rel=\"nofollow\">Examples</a></li>\n<li><a href=\"#models\" rel=\"nofollow\">Models</a>\n<ol>\n<li><a href=\"#architectires\" rel=\"nofollow\">Architectures</a></li>\n<li><a href=\"#encoders\" rel=\"nofollow\">Encoders</a></li>\n</ol>\n</li>\n<li><a href=\"#api\" rel=\"nofollow\">Models API</a>\n<ol>\n<li><a href=\"#input-channels\" rel=\"nofollow\">Input channels</a></li>\n<li><a href=\"#auxiliary-classification-output\" rel=\"nofollow\">Auxiliary classification output</a></li>\n<li><a href=\"#depth\" rel=\"nofollow\">Depth</a></li>\n</ol>\n</li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#competitions-won-with-the-library\" rel=\"nofollow\">Competitions won with the library</a></li>\n<li><a href=\"#license\" rel=\"nofollow\">License</a></li>\n<li><a href=\"#contributing\" rel=\"nofollow\">Contributing</a></li>\n</ol>\n<h3>Quick start <a></a></h3>\n<p>Since the library is built on the PyTorch framework, created segmentation model is just a PyTorch nn.Module, which can be created as easy as:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">segmentation_models_pytorch</span> <span class=\"k\">as</span> <span class=\"nn\">smp</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">smp</span><span class=\"o\">.</span><span class=\"n\">Unet</span><span class=\"p\">()</span>\n</pre>\n<p>Depending on the task, you can change the network architecture by choosing backbones with fewer or more parameters and use pretrainded weights to initialize it:</p>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">smp</span><span class=\"o\">.</span><span class=\"n\">Unet</span><span class=\"p\">(</span><span class=\"s1\">'resnet34'</span><span class=\"p\">,</span> <span class=\"n\">encoder_weights</span><span class=\"o\">=</span><span class=\"s1\">'imagenet'</span><span class=\"p\">)</span>\n</pre>\n<p>Change number of output classes in the model:</p>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">smp</span><span class=\"o\">.</span><span class=\"n\">Unet</span><span class=\"p\">(</span><span class=\"s1\">'resnet34'</span><span class=\"p\">,</span> <span class=\"n\">classes</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'softmax'</span><span class=\"p\">)</span>\n</pre>\n<p>All models have pretrained encoders, so you have to prepare your data the same way as during weights pretraining:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">segmentation_models_pytorch.encoders</span> <span class=\"kn\">import</span> <span class=\"n\">get_preprocessing_fn</span>\n\n<span class=\"n\">preprocess_input</span> <span class=\"o\">=</span> <span class=\"n\">get_preprocessing_fn</span><span class=\"p\">(</span><span class=\"s1\">'resnet18'</span><span class=\"p\">,</span> <span class=\"n\">pretrained</span><span class=\"o\">=</span><span class=\"s1\">'imagenet'</span><span class=\"p\">)</span>\n</pre>\n<h3>Examples <a></a></h3>\n<ul>\n<li>Training model for cars segmentation on CamVid dataset <a href=\"https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb\" rel=\"nofollow\">here</a>.</li>\n<li>Training SMP model with <a href=\"https://github.com/catalyst-team/catalyst\" rel=\"nofollow\">Catalyst</a> (high-level framework for PyTorch), <a href=\"https://github.com/qubvel/ttach\" rel=\"nofollow\">Ttach</a> (TTA library for PyTorch) and <a href=\"https://github.com/albu/albumentations\" rel=\"nofollow\">Albumentations</a> (fast image augmentation library) - <a href=\"https://github.com/catalyst-team/catalyst/blob/master/examples/notebooks/segmentation-tutorial.ipynb\" rel=\"nofollow\">here</a> <a href=\"https://colab.research.google.com/github/catalyst-team/catalyst/blob/master/examples/notebooks/segmentation-tutorial.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></li>\n</ul>\n<h3>Models <a></a></h3>\n<h4>Architectures <a></a></h4>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1505.04597\" rel=\"nofollow\">Unet</a></li>\n<li><a href=\"https://arxiv.org/abs/1707.03718\" rel=\"nofollow\">Linknet</a></li>\n<li><a href=\"http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\" rel=\"nofollow\">FPN</a></li>\n<li><a href=\"https://arxiv.org/abs/1612.01105\" rel=\"nofollow\">PSPNet</a></li>\n</ul>\n<h4>Encoders <a></a></h4>\n<table>\n<thead>\n<tr>\n<th>Encoder</th>\n<th align=\"center\">Weights</th>\n<th align=\"center\">Params, M</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>resnet18</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">11M</td>\n</tr>\n<tr>\n<td>resnet34</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">21M</td>\n</tr>\n<tr>\n<td>resnet50</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">23M</td>\n</tr>\n<tr>\n<td>resnet101</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">42M</td>\n</tr>\n<tr>\n<td>resnet152</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">58M</td>\n</tr>\n<tr>\n<td>resnext50_32x4d</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">22M</td>\n</tr>\n<tr>\n<td>resnext101_32x8d</td>\n<td align=\"center\">imagenet<br>instagram</td>\n<td align=\"center\">86M</td>\n</tr>\n<tr>\n<td>resnext101_32x16d</td>\n<td align=\"center\">instagram</td>\n<td align=\"center\">191M</td>\n</tr>\n<tr>\n<td>resnext101_32x32d</td>\n<td align=\"center\">instagram</td>\n<td align=\"center\">466M</td>\n</tr>\n<tr>\n<td>resnext101_32x48d</td>\n<td align=\"center\">instagram</td>\n<td align=\"center\">826M</td>\n</tr>\n<tr>\n<td>dpn68</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">11M</td>\n</tr>\n<tr>\n<td>dpn68b</td>\n<td align=\"center\">imagenet+5k</td>\n<td align=\"center\">11M</td>\n</tr>\n<tr>\n<td>dpn92</td>\n<td align=\"center\">imagenet+5k</td>\n<td align=\"center\">34M</td>\n</tr>\n<tr>\n<td>dpn98</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">58M</td>\n</tr>\n<tr>\n<td>dpn107</td>\n<td align=\"center\">imagenet+5k</td>\n<td align=\"center\">84M</td>\n</tr>\n<tr>\n<td>dpn131</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">76M</td>\n</tr>\n<tr>\n<td>vgg11</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">9M</td>\n</tr>\n<tr>\n<td>vgg11_bn</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">9M</td>\n</tr>\n<tr>\n<td>vgg13</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">9M</td>\n</tr>\n<tr>\n<td>vgg13_bn</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">9M</td>\n</tr>\n<tr>\n<td>vgg16</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">14M</td>\n</tr>\n<tr>\n<td>vgg16_bn</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">14M</td>\n</tr>\n<tr>\n<td>vgg19</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">20M</td>\n</tr>\n<tr>\n<td>vgg19_bn</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">20M</td>\n</tr>\n<tr>\n<td>senet154</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">113M</td>\n</tr>\n<tr>\n<td>se_resnet50</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">26M</td>\n</tr>\n<tr>\n<td>se_resnet101</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">47M</td>\n</tr>\n<tr>\n<td>se_resnet152</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">64M</td>\n</tr>\n<tr>\n<td>se_resnext50_32x4d</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">25M</td>\n</tr>\n<tr>\n<td>se_resnext101_32x4d</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">46M</td>\n</tr>\n<tr>\n<td>densenet121</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">6M</td>\n</tr>\n<tr>\n<td>densenet169</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">12M</td>\n</tr>\n<tr>\n<td>densenet201</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">18M</td>\n</tr>\n<tr>\n<td>densenet161</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">26M</td>\n</tr>\n<tr>\n<td>inceptionresnetv2</td>\n<td align=\"center\">imagenet<br>imagenet+background</td>\n<td align=\"center\">54M</td>\n</tr>\n<tr>\n<td>inceptionv4</td>\n<td align=\"center\">imagenet<br>imagenet+background</td>\n<td align=\"center\">41M</td>\n</tr>\n<tr>\n<td>efficientnet-b0</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">4M</td>\n</tr>\n<tr>\n<td>efficientnet-b1</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">6M</td>\n</tr>\n<tr>\n<td>efficientnet-b2</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">7M</td>\n</tr>\n<tr>\n<td>efficientnet-b3</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">10M</td>\n</tr>\n<tr>\n<td>efficientnet-b4</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">17M</td>\n</tr>\n<tr>\n<td>efficientnet-b5</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">28M</td>\n</tr>\n<tr>\n<td>efficientnet-b6</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">40M</td>\n</tr>\n<tr>\n<td>efficientnet-b7</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">63M</td>\n</tr>\n<tr>\n<td>mobilenet_v2</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">2M</td>\n</tr>\n<tr>\n<td>xception</td>\n<td align=\"center\">imagenet</td>\n<td align=\"center\">22M</td>\n</tr></tbody></table>\n<h3>Models API <a></a></h3>\n<ul>\n<li><code>model.encoder</code> - pretrained backbone to extract features of different spatial resolution</li>\n<li><code>model.decoder</code> - depends on models architecture (<code>Unet</code>/<code>Linknet</code>/<code>PSPNet</code>/<code>FPN</code>)</li>\n<li><code>model.segmentation_head</code> - last block to produce required number of mask channels (include also optional upsampling and activation)</li>\n<li><code>model.classification_head</code> - optional block which create classification head on top of encoder</li>\n<li><code>model.forward(x)</code> - sequentially pass <code>x</code> through model`s encoder, decoder and segmentation head (and classification head if specified)</li>\n</ul>\n<h5>Input channels</h5>\n<p>Input channels parameter allow you to create models, which process tensors with arbitrary number of channels.\nIf you use pretrained weights from imagenet - weights of first convolution will be reused for\n1- or 2- channels inputs, for input channels &gt; 4 weights of first convolution will be initialized randomly.</p>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">smp</span><span class=\"o\">.</span><span class=\"n\">FPN</span><span class=\"p\">(</span><span class=\"s1\">'resnet34'</span><span class=\"p\">,</span> <span class=\"n\">in_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">]))</span>\n</pre>\n<h5>Auxiliary classification output</h5>\n<p>All models support <code>aux_params</code> parameters, which is default set to <code>None</code>.\nIf <code>aux_params = None</code> than classification auxiliary output is not created, else\nmodel produce not only <code>mask</code>, but also <code>label</code> output with shape <code>NC</code>.\nClassification head consist of GlobalPooling-&gt;Dropout(optional)-&gt;Linear-&gt;Activation(optional) layers, which can be\nconfigured by <code>aux_params</code> as follows:</p>\n<pre><span class=\"n\">aux_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span>\n    <span class=\"n\">pooling</span><span class=\"o\">=</span><span class=\"s1\">'avg'</span><span class=\"p\">,</span>             <span class=\"c1\"># one of 'avg', 'max'</span>\n    <span class=\"n\">dropout</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span>               <span class=\"c1\"># dropout ratio, default is None</span>\n    <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'sigmoid'</span><span class=\"p\">,</span>      <span class=\"c1\"># activation function, default is None</span>\n    <span class=\"n\">classes</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span>                 <span class=\"c1\"># define number of output labels</span>\n<span class=\"p\">)</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">smp</span><span class=\"o\">.</span><span class=\"n\">Unet</span><span class=\"p\">(</span><span class=\"s1\">'resnet34'</span><span class=\"p\">,</span> <span class=\"n\">classes</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">aux_params</span><span class=\"o\">=</span><span class=\"n\">aux_params</span><span class=\"p\">)</span>\n<span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n</pre>\n<h5>Depth</h5>\n<p>Depth parameter specify a number of downsampling operations in encoder, so you can make\nyour model lighted if specify smaller <code>depth</code>.</p>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">smp</span><span class=\"o\">.</span><span class=\"n\">Unet</span><span class=\"p\">(</span><span class=\"s1\">'resnet34'</span><span class=\"p\">,</span> <span class=\"n\">encoder_depth</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n</pre>\n<h3>Installation <a></a></h3>\n<p>PyPI version:</p>\n<pre>$ pip install segmentation-models-pytorch\n</pre>\n<p>Latest version from source:</p>\n<pre>$ pip install git+https://github.com/qubvel/segmentation_models.pytorch\n</pre>\n<h3>Competitions won with the library</h3>\n<p><code>Segmentation Models</code> package is widely used in the image segmentation competitions.\n<a href=\"https://github.com/qubvel/segmentation_models.pytorch/blob/master/HALLOFFAME.md\" rel=\"nofollow\">Here</a> you can find competitions, names of the winners and links to their solutions.</p>\n<h3>License <a></a></h3>\n<p>Project is distributed under <a href=\"https://github.com/qubvel/segmentation_models.pytorch/blob/master/LICENSE\" rel=\"nofollow\">MIT License</a></p>\n<h3>Contributing</h3>\n<h5>Run test</h5>\n<pre>$ docker build -f docker/Dockerfile.dev -t smp:dev . <span class=\"o\">&amp;&amp;</span> docker run --rm smp:dev pytest -p no:cacheprovider\n</pre>\n<h5>Generate table</h5>\n<pre>$ docker build -f docker/Dockerfile.dev -t smp:dev . <span class=\"o\">&amp;&amp;</span> docker run --rm smp:dev python misc/generate_table.py\n</pre>\n\n          </div>"}, "last_serial": 6266672, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "eec9faf0a8a6f82e8d71c77fda701fd7", "sha256": "694e7985d98e2a1a2caece322c5fa3c4b2b7fcc2c78cd00afb0372026a4a62cb"}, "downloads": -1, "filename": "segmentation_models_pytorch-0.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "eec9faf0a8a6f82e8d71c77fda701fd7", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.0.0", "size": 24051, "upload_time": "2019-04-20T10:22:42", "upload_time_iso_8601": "2019-04-20T10:22:42.050844Z", "url": "https://files.pythonhosted.org/packages/53/7f/a009f9d116ca46be5ce8be2e2de318c4da57b62e32fa0b11a938b2808bb8/segmentation_models_pytorch-0.0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6166491a531663816d3f3484b546adde", "sha256": "f4b2338f995565f9a0018ac869fc7240569d87a4a63dfb666ed59533c7405423"}, "downloads": -1, "filename": "segmentation_models_pytorch-0.0.1.tar.gz", "has_sig": false, "md5_digest": "6166491a531663816d3f3484b546adde", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0.0", "size": 16828, "upload_time": "2019-04-20T10:22:44", "upload_time_iso_8601": "2019-04-20T10:22:44.374619Z", "url": "https://files.pythonhosted.org/packages/52/5c/60d03e239f3c49ab293b4c26b16262fcde77ca1eb95d8b794f75f9e0ecf0/segmentation_models_pytorch-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "14f368a5bbcd7a35854c2ed7161a6b42", "sha256": "38e4c9869505b350c1a278c07c1cada0cd8d0761b3dbc7bd3ffb47dbbd3e91a5"}, "downloads": -1, "filename": "segmentation_models_pytorch-0.0.2.tar.gz", "has_sig": false, "md5_digest": "14f368a5bbcd7a35854c2ed7161a6b42", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0.0", "size": 16539, "upload_time": "2019-09-19T11:35:13", "upload_time_iso_8601": "2019-09-19T11:35:13.799131Z", "url": "https://files.pythonhosted.org/packages/02/4f/a067a7f424cd087e1adc7b0245ce9837a403a063631a94bff7dcb96c6f69/segmentation_models_pytorch-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "1ac87e28f00df7ee8d538cc64e8527a1", "sha256": "2c4c4f4d843c438813193eaaae6eb7fad1057cf2cd7e9490932e302a5ebfb99e"}, "downloads": -1, "filename": "segmentation_models_pytorch-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "1ac87e28f00df7ee8d538cc64e8527a1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0.0", "size": 27265, "upload_time": "2019-09-28T18:54:19", "upload_time_iso_8601": "2019-09-28T18:54:19.435987Z", "url": "https://files.pythonhosted.org/packages/20/c6/67e9d555d41094988aaaf033b1d7e732a326a2ef41a15b81211b56e464ce/segmentation_models_pytorch-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e8a2e9b6a6c74098dab56cdd4aae96d8", "sha256": "3f5d95d6adc595814797d3e531ff8dc6f63c4f35d5bb6886fb7569533f8d538a"}, "downloads": -1, "filename": "segmentation_models_pytorch-0.0.3.tar.gz", "has_sig": false, "md5_digest": "e8a2e9b6a6c74098dab56cdd4aae96d8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0.0", "size": 16649, "upload_time": "2019-09-28T18:54:21", "upload_time_iso_8601": "2019-09-28T18:54:21.054692Z", "url": "https://files.pythonhosted.org/packages/43/e7/294488cbb0696e215f9c40ef31b82603915c7618cb956bb5e3402325e846/segmentation_models_pytorch-0.0.3.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "f22d9474ab28db0d5eb9e671c7fc4b4d", "sha256": "e328af0998363cd2d03b936e224e08c68e87da3f03f19a3f1f5fe78262f43c77"}, "downloads": -1, "filename": "segmentation_models_pytorch-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f22d9474ab28db0d5eb9e671c7fc4b4d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0.0", "size": 42615, "upload_time": "2019-12-09T14:09:27", "upload_time_iso_8601": "2019-12-09T14:09:27.360398Z", "url": "https://files.pythonhosted.org/packages/70/88/763a25dfe076a9f30f33466b1bd0f2d31b915b88d4cb4481fe4043cf26b4/segmentation_models_pytorch-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "049f93b7646de17575223b6852a8b5d0", "sha256": "dd4500710d9150c8b11b4f0c4e8c2020c8310aa257a1d1a7df8a19b9dbb4fd04"}, "downloads": -1, "filename": "segmentation_models_pytorch-0.1.0.tar.gz", "has_sig": false, "md5_digest": "049f93b7646de17575223b6852a8b5d0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0.0", "size": 23032, "upload_time": "2019-12-09T14:09:31", "upload_time_iso_8601": "2019-12-09T14:09:31.002151Z", "url": "https://files.pythonhosted.org/packages/89/f3/5e73a5942fff361dd79d36bbff73414fae1e273b02aa1ec89c1be148f9c4/segmentation_models_pytorch-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f22d9474ab28db0d5eb9e671c7fc4b4d", "sha256": "e328af0998363cd2d03b936e224e08c68e87da3f03f19a3f1f5fe78262f43c77"}, "downloads": -1, "filename": "segmentation_models_pytorch-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f22d9474ab28db0d5eb9e671c7fc4b4d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0.0", "size": 42615, "upload_time": "2019-12-09T14:09:27", "upload_time_iso_8601": "2019-12-09T14:09:27.360398Z", "url": "https://files.pythonhosted.org/packages/70/88/763a25dfe076a9f30f33466b1bd0f2d31b915b88d4cb4481fe4043cf26b4/segmentation_models_pytorch-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "049f93b7646de17575223b6852a8b5d0", "sha256": "dd4500710d9150c8b11b4f0c4e8c2020c8310aa257a1d1a7df8a19b9dbb4fd04"}, "downloads": -1, "filename": "segmentation_models_pytorch-0.1.0.tar.gz", "has_sig": false, "md5_digest": "049f93b7646de17575223b6852a8b5d0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0.0", "size": 23032, "upload_time": "2019-12-09T14:09:31", "upload_time_iso_8601": "2019-12-09T14:09:31.002151Z", "url": "https://files.pythonhosted.org/packages/89/f3/5e73a5942fff361dd79d36bbff73414fae1e273b02aa1ec89c1be148f9c4/segmentation_models_pytorch-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:55:43 2020"}