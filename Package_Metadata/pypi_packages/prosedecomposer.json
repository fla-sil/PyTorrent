{"info": {"author": "Corey Bobco", "author_email": "corey.bobco@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Science/Research", "License :: OSI Approved :: BSD License", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Artistic Software"], "description": "Prose Decomposer\n================\n\n\n.. image:: https://coveralls.io/repos/github/coreybobco/prosedecomposer-py/badge.svg?branch=master\n   :target: https://coveralls.io/github/coreybobco/prosedecomposer-py?branch=master\n\n.. image:: https://badge.fury.io/py/prosedecomposer.svg\n    :target: https://badge.fury.io/py/prosedecomposer\n\nWhat is this?\n^^^^^^^^^^^^^\n\nThere are many ways to write. In `Unoriginal Genius <http://writing.upenn.edu/~taransky/unoriginalgenius.pdf>`_, Marjorie Perloff contrasts the notion of 'original genius'--the mythic author of old who realizes works from the depths of their intellectual solitude--to a counter-tradition of 'unoriginal genius' including acts of plagiaristic parody (also known as d\u00e9tournement) and patchwriting. T.S. Eliot, James Joyce, and Thomas Pynchon are all exemplars of this style, having written their seminal works with encyclopedias, magazine, newspaper clippings, and world literature open face, according to `Uncreative Writing <http://www.libgen.is/book/index.php?md5=3E70C36B115111E10E371C72864ADAB7>`_ by Kenneth Goldsmith.\n\nToday there are countless ways to transform texts with software: Markov chains, cut-ups, substituting words for related words, swapping out verbs between books, GPT-2, BERT, etc. Today's cybernetic author can harness these as decomposing agents, destroying original texts to create messy new m\u00e9lange that can be further edited, expanded upon, or synthesised into an original, meaningful work.\n\nBut what does this do?\n^^^^^^^^^^^^^^^^^^^^^^\nThis project elaborates on these ideas, allowing the user to:\n\n- sample random sentences and paragraphs from publicly available works of literature on **Project Gutenberg** and **Archive.org** or any text you give it.\n- swap words that share the same part of speech between two texts--for instance, swapping all of one text's adjectives with another's and one text's nouns with another's, preserving the structure of a narrative or discursive formation while wildly changing the content. Take, for example, this passage from Charles Dickens' *Great Expectations*, which transforms into surrealist horror when you replace the nouns and adjectives with those from a paragraph in H.P. Lovecraft's story *The Shunned House*:\n\n    \"It was then I began to understand that chimney in the eye had stopped, like the enveloping and the head, a human fungus ago. I noticed that Miss Havisham put down the height exactly on the time from which she had taken it up. As Estella dealt the streams, I glanced at the corpse-abhorrent again, and saw that the outline upon it, once few, now diseased, had never been worn. I glanced down at the sight from which the outline was insectoid, and saw that the half stocking on it, once few, now diseased, had been trodden ragged. Without this cosmos of thing, this standing still of all the worse monstrous attentions, not even the withered phosphorescent mist on the collapsed dissolving could have looked so like horror-mockings, or the human hideousness so like a horror.\"\n- run individual texts or list of texts through a Markov chain, semi-intelligently recombining the words in a more or less chaotic manner depending on n-gram size (which defaults to 1, the most chaotic).\n\n     Markov chain based generative algorithms like this one can create prose whose repetitions and permutations lend it a strange rhythm and which appears syntactically and semantically valid at first but eventually turns into nonsense. The Markov chain's formulaic yet sassy and subversive sstyle is quite similar Gertrude Stein's in `The Making Of Americans <gutenberg.net.au/ebooks16/1600671h.html>`_, which she explains in details in the essay `Composition as Explanation <https://www.poetryfoundation.org/articles/69481/composition-as-explanation>`_.\n- perform a virtual simulation of the `cut-up method <https://www.writing.upenn.edu/~afilreis/88v/burroughs-cutup.html>`_ pioneered by William S. Burroughs and Brion Gysin by breaking texts down into components of random length (where the minimum and and maximum length in words is preserved) and then randomly rearranging them.\n\nInstallation\n^^^^^^^^^^^^\n\nUsing pip\n~~~~~~~~~\n.. code-block::\n\n   python3 -m pip generativepoetry\n\nBut for Gutenberg sampling to work properly, you must populate the Berkeley db cache:\n\n.. code-block::\n\n   python3 populate_cache.py\n\nIf the Gutenberg cache messes up after it is populated, delete the cache directory and re-populate.\n\nUsing Docker\n~~~~~~~~~~~~\n\ndocker pull coreybobco/prosedecomposer\ndocker run prosedecomposer\ndocker exec -it prosedecomposer python3\n\nHow to Use\n^^^^^^^^^^\n\nFirst, import the library:\n\n.. code-block::\n\n   from prosedecomposer import *\n\nTo extract and clean the text from Project Gutenberg or Archive.org:\n\n.. code-block::\n\n   # From an Archive.org URL:\n   calvino_text = get_internet_archive_document('https://archive.org/stream/CalvinoItaloCosmicomics/Calvino-Italo-Cosmicomics_djvu.txt')\n   # From a Project Gutenberg URL:\n   alice_in_wonderland = get_gutenberg_document('https://www.gutenberg.org/ebooks/11')\n   # Select a random document from Project Gutenberg\n   random_gutenberg_text = random_gutenberg_document\n\nThe ParsedText class offers some functions for randomly sampling one or more sentences or paragraphs of a certain length:\n\n.. code-block::\n\n   parsed_calvino = ParsedText(calvino_text)\n   parsed_calvino.random_sentence()   # Returns a random sentence\n   parsed_calvino.random_sentence(minimum_tokens=25)  # Returns a random sentence of a guaranteed length in tokens\n   parsed_calvino.random_sentences()  # Returns 5 random sentences\n   parsed_calvino.random_sentences(num=7, minimum_tokens=25)  # Returns 7 random sentences of a guaranteed length\n   parsed_calvino.random_paragraph()  # Returns a random paragraph (of at least 3 sentence by default)\n   parsed_calvino.random_paragraph(minimum_sentences=5)  # Returns a paragraph with at least 5 sentences\n\nTo swap words with the same part(s) of speech between texts:\n\n.. code-block::\n\n   # Swap out adjectives and nouns between two random paragraphs of two random Gutenberg documents\n   doc1 = ParsedText(random_gutenberg_document())\n   doc2 = ParsedText(random_gutenberg_document())\n   swap_parts_of_speech(doc1.random_paragraph(), doc2.random_paragraph())\n   # Any of Spacy's part of speech tag values should work, though: https://spacy.io/api/annotation#pos-tagging\n   swap_parts_of_speech(doc1.random_paragraph(), doc2.random_paragraph(), parts_of_speech=[\"VERB\", \"CONJ\"])\n   # Since NLG has not yet been implemented, expect syntax errors like subject-verb agreement.\n\nTo run text(s) through Markov chain text processing algorithms, see below. You may want a bigger n-gram size (2 or 3)\nif you are processing a lot of text, i.e. one or several books/stories/etc at once.\n\n.. code-block::\n\n   output = markov(text)  # Just one text (defaults to n-gram size of 1 and 5 output sentences)\n   output = markov(text, ngram_size=3, num_output_sentence=7)  # Bigger n-gram size, more output sentences\n   output = markov([text1, text2, text3])  # List of text (defaults to n-gram size of 1 and 5 output sentences)\n   output = markov([text1, text2, text3], ngram_size=3, num_output_sentences=7)  # Bigger n-gram size, more outputs\n\nTo virtually cut up and rearrange the text:\n\n.. code-block::\n\n   # Cuts up a text into cutouts between 3 and 7 words and rearrange them randomly (returns a list of cutout strings)\n   cutouts = cutup(text)\n   # Cuts up a text into cutouts between 2 an 10 words and rearrange them randomly (returns a list of cutout strings)\n   cutouts = cutup(text, min_cutout_words=3, max_cutout_words=7)\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/coreybobco/prosedecomposer-py", "keywords": "poetry", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "prosedecomposer", "package_url": "https://pypi.org/project/prosedecomposer/", "platform": "", "project_url": "https://pypi.org/project/prosedecomposer/", "project_urls": {"Homepage": "https://github.com/coreybobco/prosedecomposer-py"}, "release_url": "https://pypi.org/project/prosedecomposer/0.1.1/", "requires_dist": ["Gutenberg (==0.7.0)", "inflect (==2.1.0)", "internetarchive (==1.8.5)", "markovify (==0.7.1)", "nltk (==3.4.5)", "rdflib (==4.2.2)", "spacy (==2.1.8)", "unittest2 (==1.1.0)", "wordfreq (>=2.2.1)"], "requires_python": "", "summary": "Decompose, transform, and recombine prose into mutated forms.", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://coveralls.io/github/coreybobco/prosedecomposer-py?branch=master\" rel=\"nofollow\"><img alt=\"https://coveralls.io/repos/github/coreybobco/prosedecomposer-py/badge.svg?branch=master\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1fb955e0e166da38eb04d1c3abe475d013c4b2b5/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f636f726579626f62636f2f70726f73656465636f6d706f7365722d70792f62616467652e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://badge.fury.io/py/prosedecomposer\" rel=\"nofollow\"><img alt=\"https://badge.fury.io/py/prosedecomposer.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fef2d8322dbff1903078aaf15af820bd6a8135c8/68747470733a2f2f62616467652e667572792e696f2f70792f70726f73656465636f6d706f7365722e737667\"></a>\n<div id=\"what-is-this\">\n<h2>What is this?</h2>\n<p>There are many ways to write. In <a href=\"http://writing.upenn.edu/~taransky/unoriginalgenius.pdf\" rel=\"nofollow\">Unoriginal Genius</a>, Marjorie Perloff contrasts the notion of \u2018original genius\u2019\u2013the mythic author of old who realizes works from the depths of their intellectual solitude\u2013to a counter-tradition of \u2018unoriginal genius\u2019 including acts of plagiaristic parody (also known as d\u00e9tournement) and patchwriting. T.S. Eliot, James Joyce, and Thomas Pynchon are all exemplars of this style, having written their seminal works with encyclopedias, magazine, newspaper clippings, and world literature open face, according to <a href=\"http://www.libgen.is/book/index.php?md5=3E70C36B115111E10E371C72864ADAB7\" rel=\"nofollow\">Uncreative Writing</a> by Kenneth Goldsmith.</p>\n<p>Today there are countless ways to transform texts with software: Markov chains, cut-ups, substituting words for related words, swapping out verbs between books, GPT-2, BERT, etc. Today\u2019s cybernetic author can harness these as decomposing agents, destroying original texts to create messy new m\u00e9lange that can be further edited, expanded upon, or synthesised into an original, meaningful work.</p>\n</div>\n<div id=\"but-what-does-this-do\">\n<h2>But what does this do?</h2>\n<p>This project elaborates on these ideas, allowing the user to:</p>\n<ul>\n<li><p>sample random sentences and paragraphs from publicly available works of literature on <strong>Project Gutenberg</strong> and <strong>Archive.org</strong> or any text you give it.</p>\n</li>\n<li><p>swap words that share the same part of speech between two texts\u2013for instance, swapping all of one text\u2019s adjectives with another\u2019s and one text\u2019s nouns with another\u2019s, preserving the structure of a narrative or discursive formation while wildly changing the content. Take, for example, this passage from Charles Dickens\u2019 <em>Great Expectations</em>, which transforms into surrealist horror when you replace the nouns and adjectives with those from a paragraph in H.P. Lovecraft\u2019s story <em>The Shunned House</em>:</p>\n<blockquote>\n<p>\u201cIt was then I began to understand that chimney in the eye had stopped, like the enveloping and the head, a human fungus ago. I noticed that Miss Havisham put down the height exactly on the time from which she had taken it up. As Estella dealt the streams, I glanced at the corpse-abhorrent again, and saw that the outline upon it, once few, now diseased, had never been worn. I glanced down at the sight from which the outline was insectoid, and saw that the half stocking on it, once few, now diseased, had been trodden ragged. Without this cosmos of thing, this standing still of all the worse monstrous attentions, not even the withered phosphorescent mist on the collapsed dissolving could have looked so like horror-mockings, or the human hideousness so like a horror.\u201d</p>\n</blockquote>\n</li>\n<li><p>run individual texts or list of texts through a Markov chain, semi-intelligently recombining the words in a more or less chaotic manner depending on n-gram size (which defaults to 1, the most chaotic).</p>\n<blockquote>\n<p>Markov chain based generative algorithms like this one can create prose whose repetitions and permutations lend it a strange rhythm and which appears syntactically and semantically valid at first but eventually turns into nonsense. The Markov chain\u2019s formulaic yet sassy and subversive sstyle is quite similar Gertrude Stein\u2019s in <a href=\"gutenberg.net.au/ebooks16/1600671h.html\" rel=\"nofollow\">The Making Of Americans</a>, which she explains in details in the essay <a href=\"https://www.poetryfoundation.org/articles/69481/composition-as-explanation\" rel=\"nofollow\">Composition as Explanation</a>.</p>\n</blockquote>\n</li>\n<li><p>perform a virtual simulation of the <a href=\"https://www.writing.upenn.edu/~afilreis/88v/burroughs-cutup.html\" rel=\"nofollow\">cut-up method</a> pioneered by William S. Burroughs and Brion Gysin by breaking texts down into components of random length (where the minimum and and maximum length in words is preserved) and then randomly rearranging them.</p>\n</li>\n</ul>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<div id=\"using-pip\">\n<h3>Using pip</h3>\n<pre>python3 -m pip generativepoetry\n</pre>\n<p>But for Gutenberg sampling to work properly, you must populate the Berkeley db cache:</p>\n<pre>python3 populate_cache.py\n</pre>\n<p>If the Gutenberg cache messes up after it is populated, delete the cache directory and re-populate.</p>\n</div>\n<div id=\"using-docker\">\n<h3>Using Docker</h3>\n<p>docker pull coreybobco/prosedecomposer\ndocker run prosedecomposer\ndocker exec -it prosedecomposer python3</p>\n</div>\n</div>\n<div id=\"how-to-use\">\n<h2>How to Use</h2>\n<p>First, import the library:</p>\n<pre>from prosedecomposer import *\n</pre>\n<p>To extract and clean the text from Project Gutenberg or Archive.org:</p>\n<pre># From an Archive.org URL:\ncalvino_text = get_internet_archive_document('https://archive.org/stream/CalvinoItaloCosmicomics/Calvino-Italo-Cosmicomics_djvu.txt')\n# From a Project Gutenberg URL:\nalice_in_wonderland = get_gutenberg_document('https://www.gutenberg.org/ebooks/11')\n# Select a random document from Project Gutenberg\nrandom_gutenberg_text = random_gutenberg_document\n</pre>\n<p>The ParsedText class offers some functions for randomly sampling one or more sentences or paragraphs of a certain length:</p>\n<pre>parsed_calvino = ParsedText(calvino_text)\nparsed_calvino.random_sentence()   # Returns a random sentence\nparsed_calvino.random_sentence(minimum_tokens=25)  # Returns a random sentence of a guaranteed length in tokens\nparsed_calvino.random_sentences()  # Returns 5 random sentences\nparsed_calvino.random_sentences(num=7, minimum_tokens=25)  # Returns 7 random sentences of a guaranteed length\nparsed_calvino.random_paragraph()  # Returns a random paragraph (of at least 3 sentence by default)\nparsed_calvino.random_paragraph(minimum_sentences=5)  # Returns a paragraph with at least 5 sentences\n</pre>\n<p>To swap words with the same part(s) of speech between texts:</p>\n<pre># Swap out adjectives and nouns between two random paragraphs of two random Gutenberg documents\ndoc1 = ParsedText(random_gutenberg_document())\ndoc2 = ParsedText(random_gutenberg_document())\nswap_parts_of_speech(doc1.random_paragraph(), doc2.random_paragraph())\n# Any of Spacy's part of speech tag values should work, though: https://spacy.io/api/annotation#pos-tagging\nswap_parts_of_speech(doc1.random_paragraph(), doc2.random_paragraph(), parts_of_speech=[\"VERB\", \"CONJ\"])\n# Since NLG has not yet been implemented, expect syntax errors like subject-verb agreement.\n</pre>\n<p>To run text(s) through Markov chain text processing algorithms, see below. You may want a bigger n-gram size (2 or 3)\nif you are processing a lot of text, i.e. one or several books/stories/etc at once.</p>\n<pre>output = markov(text)  # Just one text (defaults to n-gram size of 1 and 5 output sentences)\noutput = markov(text, ngram_size=3, num_output_sentence=7)  # Bigger n-gram size, more output sentences\noutput = markov([text1, text2, text3])  # List of text (defaults to n-gram size of 1 and 5 output sentences)\noutput = markov([text1, text2, text3], ngram_size=3, num_output_sentences=7)  # Bigger n-gram size, more outputs\n</pre>\n<p>To virtually cut up and rearrange the text:</p>\n<pre># Cuts up a text into cutouts between 3 and 7 words and rearrange them randomly (returns a list of cutout strings)\ncutouts = cutup(text)\n# Cuts up a text into cutouts between 2 an 10 words and rearrange them randomly (returns a list of cutout strings)\ncutouts = cutup(text, min_cutout_words=3, max_cutout_words=7)\n</pre>\n</div>\n\n          </div>"}, "last_serial": 5797920, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "90a547bae116b995480a3f03d47ac6f0", "sha256": "a755def264f436460fe12d5b21e70fe23f6dc07e661c712a70cdddb929cc4746"}, "downloads": -1, "filename": "prosedecomposer-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "90a547bae116b995480a3f03d47ac6f0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8641, "upload_time": "2019-09-04T17:10:59", "upload_time_iso_8601": "2019-09-04T17:10:59.444242Z", "url": "https://files.pythonhosted.org/packages/51/41/cf1ea1214d53141fc7ac003f98f37b7643ec145954b0a5cdce5c92792e58/prosedecomposer-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7256914bcca54a7a0c8e549617ce0085", "sha256": "467934fd9f69e14d756cf0eb694762672e6ffc42ccfcbe6d8d277f857cb573d0"}, "downloads": -1, "filename": "prosedecomposer-0.1.0.tar.gz", "has_sig": false, "md5_digest": "7256914bcca54a7a0c8e549617ce0085", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8103, "upload_time": "2019-09-04T17:11:01", "upload_time_iso_8601": "2019-09-04T17:11:01.982284Z", "url": "https://files.pythonhosted.org/packages/7e/d3/768897d64c59993033de25af905d86090230de5d95ea17ef32cdb735499e/prosedecomposer-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "ea62ac61b022bd0ac10e9e966a49db01", "sha256": "869fb0174884c208198cad69bcae7fe987dd3fa4fd458a9be1a891ccc40a2bff"}, "downloads": -1, "filename": "prosedecomposer-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "ea62ac61b022bd0ac10e9e966a49db01", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9125, "upload_time": "2019-09-08T00:56:13", "upload_time_iso_8601": "2019-09-08T00:56:13.594210Z", "url": "https://files.pythonhosted.org/packages/ba/85/8cd1bd69e968bce9adae393fa32b19f71392455ea116b00fb9667535a26b/prosedecomposer-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0a953818033ed95b17ae88c3d2fcbb6f", "sha256": "1d239123b8f32494da865d96f16c4128c7fe03eb330ce47a0320f0dd8d2298b9"}, "downloads": -1, "filename": "prosedecomposer-0.1.1.tar.gz", "has_sig": false, "md5_digest": "0a953818033ed95b17ae88c3d2fcbb6f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8126, "upload_time": "2019-09-08T00:56:16", "upload_time_iso_8601": "2019-09-08T00:56:16.298649Z", "url": "https://files.pythonhosted.org/packages/7b/6f/611dc3b063a16bbc702f27ebf2808d95ab7808314114b44b012304be0628/prosedecomposer-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ea62ac61b022bd0ac10e9e966a49db01", "sha256": "869fb0174884c208198cad69bcae7fe987dd3fa4fd458a9be1a891ccc40a2bff"}, "downloads": -1, "filename": "prosedecomposer-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "ea62ac61b022bd0ac10e9e966a49db01", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9125, "upload_time": "2019-09-08T00:56:13", "upload_time_iso_8601": "2019-09-08T00:56:13.594210Z", "url": "https://files.pythonhosted.org/packages/ba/85/8cd1bd69e968bce9adae393fa32b19f71392455ea116b00fb9667535a26b/prosedecomposer-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0a953818033ed95b17ae88c3d2fcbb6f", "sha256": "1d239123b8f32494da865d96f16c4128c7fe03eb330ce47a0320f0dd8d2298b9"}, "downloads": -1, "filename": "prosedecomposer-0.1.1.tar.gz", "has_sig": false, "md5_digest": "0a953818033ed95b17ae88c3d2fcbb6f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8126, "upload_time": "2019-09-08T00:56:16", "upload_time_iso_8601": "2019-09-08T00:56:16.298649Z", "url": "https://files.pythonhosted.org/packages/7b/6f/611dc3b063a16bbc702f27ebf2808d95ab7808314114b44b012304be0628/prosedecomposer-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:16:50 2020"}