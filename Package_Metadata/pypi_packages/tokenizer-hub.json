{"info": {"author": "Solumilken", "author_email": "", "bugtrack_url": null, "classifiers": ["Programming Language :: Python", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6"], "description": "# tokenizer-hub\n\nyoctol \u4e42\u534doO\u715e\u6c23\u311ftokenizerOo\u534d\u4e42\n\nTokenizers have the same interface of Jieba:\n\n```python\nfrom tokenizer_hub import XXX_tokenizer\ntokenizer = XXX_tokenizer()\ntokenizer.lcut('\u6211\u6765\u5230\u5317\u4eac\u6e05\u534e\u5927\u5b66')\n> ['\u6211', '\u6765\u5230', '\u5317\u4eac', '\u6e05\u534e\u5927\u5b66']\n```\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Yoctol/tokenizer-hub", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "tokenizer-hub", "package_url": "https://pypi.org/project/tokenizer-hub/", "platform": "", "project_url": "https://pypi.org/project/tokenizer-hub/", "project_urls": {"Homepage": "https://github.com/Yoctol/tokenizer-hub"}, "release_url": "https://pypi.org/project/tokenizer-hub/0.0.1/", "requires_dist": ["jieba (==0.39)", "nltk (==3.3.0)", "purewords (==0.1.1)"], "requires_python": ">=3.5", "summary": "Yoctol Natural Language Tokenizer", "version": "0.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p># tokenizer-hub</p>\n<p>yoctol \u4e42\u534doO\u715e\u6c23\u311ftokenizerOo\u534d\u4e42</p>\n<p>Tokenizers have the same interface of Jieba:</p>\n<p><tt>`python\nfrom tokenizer_hub import XXX_tokenizer\ntokenizer = XXX_tokenizer()\n<span class=\"pre\">tokenizer.lcut('\u6211\u6765\u5230\u5317\u4eac\u6e05\u534e\u5927\u5b66')</span>\n&gt; ['\u6211', '\u6765\u5230', '\u5317\u4eac', '\u6e05\u534e\u5927\u5b66']\n`</tt></p>\n\n          </div>"}, "last_serial": 6226603, "releases": {"0.0.0": [{"comment_text": "", "digests": {"md5": "029ece854fa9bcbead2bb847c05c9fd2", "sha256": "95a36d198daf0f6adbfb1d135360e048ce7d8070e215b672d00c658ef3793ef7"}, "downloads": -1, "filename": "tokenizer_hub-0.0.0-py3.5.egg", "has_sig": false, "md5_digest": "029ece854fa9bcbead2bb847c05c9fd2", "packagetype": "bdist_egg", "python_version": "3.5", "requires_python": ">=3.5", "size": 33917, "upload_time": "2018-08-07T06:45:52", "upload_time_iso_8601": "2018-08-07T06:45:52.655827Z", "url": "https://files.pythonhosted.org/packages/39/a7/b1f394962b4426877b74134877e4893fcd94a6d9412400d18acd3caf4bd3/tokenizer_hub-0.0.0-py3.5.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "75f83be135c66bd8dc89ed7255c5427a", "sha256": "a96d01e2c4243c5e9716216d8b6765b522949d90ff5af00fa205197881352c32"}, "downloads": -1, "filename": "tokenizer_hub-0.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "75f83be135c66bd8dc89ed7255c5427a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 13794, "upload_time": "2018-08-06T09:23:29", "upload_time_iso_8601": "2018-08-06T09:23:29.508994Z", "url": "https://files.pythonhosted.org/packages/e0/6d/ec703a2d0717cc4c14b556cea01dd0e24cac6dfb5f0f5386ff161973901e/tokenizer_hub-0.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "655374ed5c8da97510b89fdad1071c41", "sha256": "05aa40c27fa89009e636b20851adbe2228e0913b0c3db323247eb8e05931f53b"}, "downloads": -1, "filename": "tokenizer-hub-0.0.0.tar.gz", "has_sig": false, "md5_digest": "655374ed5c8da97510b89fdad1071c41", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 6981, "upload_time": "2018-08-06T09:23:30", "upload_time_iso_8601": "2018-08-06T09:23:30.722092Z", "url": "https://files.pythonhosted.org/packages/b2/cb/5ad4639661d82024ecfdb43dd6f9972aab7b38b9bb53ab0926b9b36a16e4/tokenizer-hub-0.0.0.tar.gz", "yanked": false}], "0.0.1": [{"comment_text": "", "digests": {"md5": "60c7e39ed4ded7ad29f08410fa32d87f", "sha256": "d22b00d0890736d21b983b201ba4b25532c982fdb64bf610281ede7802872812"}, "downloads": -1, "filename": "tokenizer_hub-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "60c7e39ed4ded7ad29f08410fa32d87f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 13851, "upload_time": "2018-08-07T06:45:51", "upload_time_iso_8601": "2018-08-07T06:45:51.131101Z", "url": "https://files.pythonhosted.org/packages/ac/7b/09978044a1f55d4ba74068473c57399d8d3ca11d08698dcd988d835f77ca/tokenizer_hub-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "06a4e8e879abd72b9fff5614c397ed85", "sha256": "3c82fb39074b93324ef95bd6b642976558bf85aa0ef802da3f3897eecbf56e79"}, "downloads": -1, "filename": "tokenizer-hub-0.0.1.tar.gz", "has_sig": false, "md5_digest": "06a4e8e879abd72b9fff5614c397ed85", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 7056, "upload_time": "2018-08-07T06:45:55", "upload_time_iso_8601": "2018-08-07T06:45:55.499488Z", "url": "https://files.pythonhosted.org/packages/45/c2/44cf09f4f98f949cd472d694df079d3c092999536b2fbb9d95efe06fec68/tokenizer-hub-0.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "60c7e39ed4ded7ad29f08410fa32d87f", "sha256": "d22b00d0890736d21b983b201ba4b25532c982fdb64bf610281ede7802872812"}, "downloads": -1, "filename": "tokenizer_hub-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "60c7e39ed4ded7ad29f08410fa32d87f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 13851, "upload_time": "2018-08-07T06:45:51", "upload_time_iso_8601": "2018-08-07T06:45:51.131101Z", "url": "https://files.pythonhosted.org/packages/ac/7b/09978044a1f55d4ba74068473c57399d8d3ca11d08698dcd988d835f77ca/tokenizer_hub-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "06a4e8e879abd72b9fff5614c397ed85", "sha256": "3c82fb39074b93324ef95bd6b642976558bf85aa0ef802da3f3897eecbf56e79"}, "downloads": -1, "filename": "tokenizer-hub-0.0.1.tar.gz", "has_sig": false, "md5_digest": "06a4e8e879abd72b9fff5614c397ed85", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 7056, "upload_time": "2018-08-07T06:45:55", "upload_time_iso_8601": "2018-08-07T06:45:55.499488Z", "url": "https://files.pythonhosted.org/packages/45/c2/44cf09f4f98f949cd472d694df079d3c092999536b2fbb9d95efe06fec68/tokenizer-hub-0.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:51:27 2020"}