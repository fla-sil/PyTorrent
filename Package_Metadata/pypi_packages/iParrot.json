{"info": {"author": "idle-man", "author_email": "i@idleman.club", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Information Technology", "Intended Audience :: Other Audience", "License :: OSI Approved :: MIT License", "Operating System :: MacOS", "Operating System :: Microsoft :: Windows", "Operating System :: OS Independent", "Operating System :: Unix", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Testing"], "description": "# Parrot\n\n**Automated test solution for http requests based on recording and playback**\n\n## 1. Instructions for use\n\n### 1.1 Install\n\nThe Parrot project is developed based on Python 3\uff0c and the recommended version is 3.7.x. Please ensure that the corresponding version of python environment is installed on the target machine.\n\n**Method One: install via `pip` command**\n\niParrot project has been submitted to PyPI.\n\n1. You could install via `pip install iParrot` command.\n2. If you need a upgrade, try `pip install -U iParrot` command.\n\n**Method Two: install using source code**\n\nCode Repository: <https://github.com/idle-man/iParrot>\n\nDownload source code pkg and install via `python setup.py install` command.\n\nAfter the installation is complete, the `parrot` executable will be generated, try `parrot help` or `parrot -v`. If there is a problem, feedback an issue: <https://github.com/idle-man/iParrot/issues>\n\n***\n\n### 1.2 Usage\n\n#### 1.2.1 View commands supported by Parrot: `parrot help`\n\nTwo core commands are: **record** and **playback**\n\n```\n$ parrot help\nAutomated test solution for http requests based on recording and playback\nVersion: ...\n\nUsage: parrot [-h] [-v] [command] [<args>]\n\ncommand:\n  record   - parse source file and generate test cases\n            see detail usage: `parrot help record`\n  playback - run standardized test cases and do validations\n            see detail usage: `parrot help playback`\n  template - generate standardized test case template file\n            see detail usage: `parrot help template`\n  replace  - replace existing test cases with specified rules\n            see detail usage: `parrot help replace`\n  home     - show homepage on github\n  doc      - show readme on github\n\n\noptional arguments:\n  -h, --help         show this help message and exit\n  -v, -V, --version  show version\n```\n\n#### 1.2.2 View usage of `record`: `parrot help record`\n\nThe purpose of this step is to parse the user-specified source file (currently .har) into a standardized set of use cases.\n\n```\n$ parrot help record\n...\n\nUsage: parrot record [<args>]\n\nArguments:\n  -s, --source SOURCE   source file with path, *.har or directory [required]\n  -t, --target TARGET   target output path, 'ParrotProject' as default\n  -i, --include INCLUDE include filter on url, separated by ',' if multiple\n  -e, --exclude EXCLUDE exclude filter on url, separated by ',' if multiple\n  -vi, --validation-include V_INCLUDE\n                        include filter on response validation, separated by ',' if multiple\n  -ve, --validation-exclude V_EXCLUDE  \n                        exclude filter on response validation, separated by ',' if multiple\n  -ae, --auto-extract   automatic identification of interface dependencies or not, False as default\n\n  --log-level LOG_LEVEL log level: debug, info, warn, error, info as default\n  --log-mode  LOG_MODE  log mode : 1-on screen, 2-in log file, 3-1&2, 1 as default\n  --log-path  LOG_PATH  log path : <project path> as default\n  --log-name  LOG_NAME  log name : parrot.log as default\n\n```\n\n#### 1.2.3 View usage of `playback`: `parrot help playback`\n\nThis step is to execute the specified set of test cases and generate a test report.\n\n```\n$ parrot help playback\n...\n\nUsage: parrot playback [<args>]\n\nArguments:\n  -s, --suite, -c, --case SUITE_OR_CASE\n                        test suite or case with path, *.yml or directory [required]\n  -t, --target TARGET   output path for report and log, 'ParrotProject' as default\n  -i, --interval INTERVAL\n                        interval time(ms) between each step, use the recorded interval as default\n  -env, --environment ENVIRONMENT\n                        environment tag, defined in project/environments/*.yml, use defined config in test_suites as default\n  -reset, --reset-after-case\n                        reset runtime environment or not, False as default\n\n  --fail-stop           stop or not when a test step failed on validation, False as default\n  --fail-retry-times FAIL_RETRY_TIMES\n                        max retry times when a test step failed on validation, 0 as default\n  --fail-retry-interval FAIL_RETRY_INTERVAL \n                        retry interval(ms) when a test step failed on validation, 100 as default\n\n  --log-level LOG_LEVEL log level: debug, info, warn, error, info as default\n  --log-mode  LOG_MODE  log mode : 1-on screen, 2-in log file, 3-1&2, 1 as default\n  --log-path  LOG_PATH  log path : <project path> as default\n  --log-name  LOG_NAME  log name : parrot.log as default\n\n```\n\n#### 1.2.4 View usage of `template`: `parrot help template`\n\nThis step is to automatically generate standardized test case templates and examples, which is convenient for users to refer to self-built use cases.\n\n```\n$ parrot help template\n...\n\nUsage: parrot template [<args>]\n\nArguments:\n  -t, --target TARGET   target output path, 'ParrotProject' as default\n\n  --log-level LOG_LEVEL log level: debug, info, warn, error, info as default\n  --log-mode  LOG_MODE  log mode : 1-on screen, 2-in log file, 3-1&2, 1 as default\n  --log-path  LOG_PATH  log path : <project path> as default\n  --log-name  LOG_NAME  log name : parrot.log as default\n\n```\n\n#### 1.2.5 View usage of `replace`: `parrot help replace`\n\nThis step is to batch replace the `config.variables` of the generated use cases according to your specified rules.\n\n```\n$ parrot help replace\n...\n\nUsage: parrot replace [<args>]\n\nArguments:\n  -s, --suite, -c, --case SUITE_OR_CASE\n                        test suite or case with path, *.yml or directory [required]\n  -t, --target TARGET   target output path, 'ParrotProjectNew' as default\n  -r, --rule RULE       replace rules, separated by ',' if multiple [required]\n                        'key=>value', 'value1=>value2' or 'apiA::key=>value' only for specified api\n\n  --log-level LOG_LEVEL log level: debug, info, warn, error, info as default\n  --log-mode  LOG_MODE  log mode : 1-on screen, 2-in log file, 3-1&2, 1 as default\n  --log-path  LOG_PATH  log path : <project path> as default\n  --log-name  LOG_NAME  log name : parrot.log as default\n\n```\n\n***\n\n### 1.3 Demo\n\n#### 1.3.1 Build Sample application and export HAR\n\nIn order to facilitate the function debugging and operation demonstration, a simple web application(based on Python Flask) was specially built: <https://github.com/idle-man/ParrotSample>\n\nPlease refer to its README to complete the application setup in the local environment. It provides the \"recommended function operations\".\n\nWe use this as an example, using the browser's developer tools, after completing this series of operations, export the HAR file, assuming we named it `sample.har` and placed it in your working directory.\n\n> HAR is a common standardized format for storing HTTP requests and responses.\n> \n> Its versatility: can be exported with consistent format from Charles, Fiddler, Chrome, etc.\n> \n> Its standardization: JSON format and UTF-8 coding.\n\n***\n\n#### 1.3.2 Record - Transforming HAR into standardized use cases\n\nWe assume that you have completed the installation of `parrot` as described in Chapter 1. Now we use the command line tool on the computer and switch to the path where `sample.har` is located.\n\n> It is recommended to use `PyCharm`, which contains `Terminal`, which is convenient for operation and helps to view the following use cases.\n\nAccording to the instructions in Section 1.2.2, we have a general understanding of the basic usage of the `record` command. Now let's make a try on `sample.har`.\n\n**# A simple record: -s & -t**\n\n```\n$ parrot record -s sample.har -t demo0\n```\n\nIf it is successful, after the execution is complete, you will see the generated use cases set in current directory. The structure is as follows:\n\n```\ndemo0/\n  \u251c\u2500\u2500 environments\n  \u2502\u00a0\u00a0  \u2514\u2500\u2500 *env*.yml: Project-level environment variable configuration\n  \u251c\u2500\u2500 test_steps\n  \u2502\u00a0\u00a0  \u2514\u2500\u2500 *case_name*.yml: The minimum execution unit, a http request is a step\n  \u251c\u2500\u2500 test_cases\n  \u2502\u00a0\u00a0  \u2514\u2500\u2500 *case_name*.yml: Independent closed-loop unit,  consisting of one or more steps\n  \u2514\u2500\u2500 test_suites\n   \u00a0\u00a0  \u2514\u2500\u2500 *suite_name*.yml: Test case set, consisting of one or more cases\n```\n\nEach `Entry` in HAR will be converted into a test_step containing the specific request and validation information.\n\nEach `test_step` `test_case` `test_suite` contains its own `config` `setup_hooks` `teardown_hooks`.\n\nThe specific format can be found in the `Appendix: Use Case Structure`.\n\n**# Not all recorded requests need to be converted to use cases: -i & -e**\n\nBecause HAR is a full export without selection, it may contain some requests that we don't need to test, such as `*.css` `*.js`. In order to avoid the manual processing, we can filter them out during the record phase.\n\nParrot currently provides -i/--include and -e/--exclude filter modes, specifically:\n\n* -i, --include: According to the specified parameters, the `url` in each `Entry` is ambiguously matched, and the matching will be retained. The relationship between multiple include parameters is OR.\n* -e, --exclude: According to the specified parameters, the `url` in each `Entry` is ambiguously matched, and the matching will be excluded. The relationship between multiple excludes is OR.\n\nThe two parameters can be used only in one or in combination to achieve your filtering needs.\n\nIn our above requirement, we want to filter out the unwanted `*.css` and `*.js`, just use -e, --exclude.\n\n```\n$ parrot record -s sample.har -t demo1 -e \".css, .js\"\n```\n\nAfter that, you can check if the requirement is met in demo1/test_steps.\n\n**# Not all content in response can be expected: -vi & -ve**\n\nAn example of the validations generated in test_step, the user can also customize this format:\n\n```\nvalidations:\n- <comparator>:\n    <check>: <expected result>\n```\n\nThe `check`: According to the important information in the above Mode One, the unified format is: \\<PREFIX>.\\<KEYS>\n\n- Available PREFIX: `status`, `content`, `headers`, `cookies`, in lower case\n- KEYS in `status`: `code`\n- KEYS in `headers` and `cookies`: Currently only extracting the outer keys\n- KEYS in `content`(json format): `content.a.b[1].c`\n\n**By default, automatically generated validations will include `status.code` and all keys of `content` and unofficial keys of  `headers`.**\n\nIn fact, some of these keys do not need to be verified for reasons such as variability, etc., such as the `tag` `timestamp` contained in the response information of each request in `sample.har`, which is best removed when it is automatically generated.\n\nParrot currently provides -vi/--validation-include and -ve/--validation-exclude filter modes, specifically:\n\n* -vi, --validation-include: According to the specified parameters, the `headers` and `content` in the response in each Entry are fuzzy matched, and the matching will be retained. The relationship between multiple -vi is OR.\n* -ve, --validation-exclude: According to the specified parameters, the `headers` and `content` in the response in each Entry are fuzzy matched, the matching will be excluded. The relationship between multiple -ve is OR.\n\nThe two parameters can be used only in one or in combination to achieve your filtering needs.\n\nIn our above requirement, we want to filter out unwanted `tag` and `timestamp`, just use -ve, --validation-exclude.\n\n```\n$ parrot record -s sample.har -t demo2 -e \".css, .js\" -ve \"content.tag, timestamp\"\n```\n\nAbout the `comparator`, the default is `eq` when it is automatically generated, which can be edited manually. Currently, to view all the comparators that parrot supports, see `Appendix: Verification Method Set`.\n\n**# Sometimes we need to generate the parameters in real time.**\n\nTake the `hobby_suggest` request in `sample.har` as an example. The `today` parameter of the interface needs the date of the execution time. The default value recorded by the recording is the static value. If we run it in the next day, it will not meet the requirements. At this point, you need to generate this parameter in real time.\n\nThe way Parrot supports is: `${{function(params)}}`, where those functions are provided in `iparrot.modules.helper`, which can be used directly. See `Appendix: Helper Method Set`.\n\nExample:\n\n```yaml\nconfig:\n  ...\n  variables:\n    p1: ABC\n    p2: 123\n    p3: ${{today()}}\nrequest:\n  method: GET\n  ...\n  params:\n    param1: ${p1}\n    param2: ${p2}\n    today: ${p3}\n  ...\n```\n\n**# Sometimes there is a dependency on the response of the previous request.**\n\nTake the `hobby_detail` request in `sample.har` as an example. Its `name` argument is from the real-time valid return in the `hobby_list` request's response. If the data during recording is directly played back, it is likely that there is a failure sometimes.\n\nParrot's solution is: `extract` the specific key in the response of the `hobby_list` request, and use the `${variable}` format in the `hobby_detail` request.\n\nExample:\n\nhobby_list:\n\n```yaml\nconfig:\n  ...\nrequest:\n  ...\nresponse:\n  extract:\n    hobby: content.hobbies[0].name\n...\n```\n\nhobby_detail:\n\n```yaml\nconfig:\n  ...\nrequest:\n  ...\n  params:\n    name: ${hobby}\n  ...\n```\n\nIn iParrot 1.0.6 and later, `parrot record` has the `-ae, --auto-extract` parameter added. \n\nIf this parameter is specified, parrot will automatically recognize the parameter dependency between interfaces during the parsing process.\n\nThe `extract` extraction and `${variable}` references are automatically completed when the use cases are generated.\n\nIn view of the possibility of many formats in the actual scene, the 'automatic' identification may cause some accidental or omission, and it is recommended to perform artificial inspection and correction after its execution.\n\nRegarding the recording phase, the above scenarios should cover most of the use cases. If there are other unsupported questions, welcome feedback an issue: <https://github.com/idle-man/iParrot/issues>\n\n***\n\n#### 1.3.3 Template - Generate a standardized use case template when no files are recorded\n\nIn actual work, there may be cases where there is no HAR file, such as a new project or a new interface before going online. \n\nFor users who have existing HAR files, you can skip this step.\n\n`parrot`1.1.0 and later provides the `template` command to help you generate standardized use case templates, where you can manually edit your test cases.\n\nSee section #1.2.4 for specific usage: `parrot help template`\n\nThe generated use case structure is consistent with the `recording` phase, including a `test_suite`, a `test_case`, a `test_step` and a `environment`.\n\nUsers can refer above template to write their own formatting use cases, which can also be used for subsequent `playback`.\n\n***\n\n#### 1.3.4 Replace - Batch update generated use cases according to specified rules\n\nFor use cases that have already been generated, we sometimes have a large number of update requirements, such as:\n- Need to replace the ID in the argument with another one\n- Need to replace a date parameter with a `${{today()}}` function call\n- Need to replace the host in the request with a new test address\n- Need to replace the token in the request header with the `${variable}` variable reference\n\nSome of the above updates can be done with the help of an IDE.\n\nFor the user's convenient use, `parrot`1.1.0 and later versions provide the `replace` command, which can be used to batch update the target use case set according to the rules specified by the user.\n\nSee section #1.2.5 for specific usage: `parrot help replace`\n\nFor example:\n\n`parrot replace -s demo/test_suites/test.yml -t demoNew -r 'xxID=>abcd, 2019-01-01=>${{today()}}, host=>example.com'`\n\nThe above command will automatically traverse all the yml files in the specified test_suite and its included test_cases and test_steps,\n\nand match the contents of `config.variables`, `request`, `request.headers`, `request.cookies` blocks.\n\nThe content before the '=>' symbol will be absolutely matched with the keys in the above use case block, and if it is missed, it will definitely match the values. \n\nIf some key or value matches, the value will be replaced with the content after the '=>' symbol.\n\n**If a replacement rule only needs to be valid for a particular interface, you can do this:**\n\n`parrot replace -s demo/test_suites/test.yml -t demoNew -r 'xxID=>abcd, apiA::2019-01-01=>${{today()}}'`\n\nThe above `2019-01-01=>${{today()}}` rule only works on the yml of the file name fuzzy matched `apiA`.\n\n***\n\n#### 1.3.5 Replay - Execute use cases, verify result, generate report\n\nAccording to the instructions in Section #1.2.3, we have a general understanding of the basic usage of the `playback` command. Now let's try to play back the `demo2` recorded earlier.\n\n**# A simple playback: -s & -t**\n\n```\n$ parrot playback -s demo2/test_suites -t demo2\n```\n\nIf it is successful, the process output information will be visible on the screen during the execution. After the execution is completed, you will see the generated test report in the `demo2` directory: `parrot_<timestamp>.html`, which could be  viewed via PyCharm or browser.\n\n\n**# About the running order**\n\nParrot execute the test in the order of cases and steps defined in *test_suite*.yaml / *test_case*.yaml, currently only supports serial execution mode.\n\n> When the use case is automatically generated, the order of the steps defaults to the order of appearance in the recorded sample, which can be edited manually.\n\nThe detailed execution process:\n\n```\ntest_suite1\n |-> suite1.setup\n |-> test_case1\n   |-> case1.setup\n   |-> test_step1\n     |-> step1.setup\n     |-> request\n     |-> validation\n     |-> extract\n     |-> step1.teardown\n   |-> test_step2\n     ...\n   |-> case1.teardown\n |-> test_case2\n   ...\n |-> suite1.teardown\ntest_suite2\n  ...\n```\n\n**# About the running interval**\n\nThe value of `interval` argument is firstly used, otherwise the value `time.start` in step defination.\n\nIf the playback pass parameter specifies the `interval`, it will execute according to the interval (in milliseconds), for example:\n\n```\n$ parrot playback -s demo2/test_suites -t demo2 -i 100\n```\n\nOtherwise, if `time.start` is defined in the request of step (the recording phase will be automatically recorded), the default is to execute according to the interval of `time.start` of each step.\n\n> When the use case is automatically generated, the actual execution time would be recorded as `time.start` in the step defination.\n\n**# About the running validation**\n\nAs mentioned in the recording phase, each request generates some validations, which contain the expected result.\n\nIn the process of request playback, parrot can get the actual result in real time, so you can check the result in real time, and then check whether the value of each `check` object conforms to the `comparator` rule. If there is one failure, the entire step fails.\n\nAfter a single step fails, the current Parrot does not terminate the execution of the playback by default, but the user can perform some intervention by running the parameters:\n\n- --fail_stop: If specified, the operation will be terminated after a step verification fails\n- --fail\\_retry_times: The number of retries after a step failed, 0 as default\n- --fail\\_retry_interval: retry interval after a step failure\n\n***\n\n#### 1.3.6 Adapting multiple sets of environments\n\nParrot draws on Postman's environmental management mechanism.\n\n##### The environment configuration is automatically reserved during recording and can be edited manually.\n\nTake the recorded demo2 in section 1.3.2 as an example. The automatically generated environment is configured as `demo2/environments/sample_env.yml`. The default generated content is reserved for several sets of environment identifiers and the content is empty:\n\n```\ndevelopment: {}\nglobal: {}\nproduction: {}\ntest: {}\n```\n\nAt the same time, the `config` part of the automatically generated test\\_suites, test\\_cases and test\\_steps is referenced by `import` and `environment: global`, which can be edited manually.\n\n> `global` is globally shared, the rest are independent, and the new environment identifier can be customized.\n\nSuppose we have deployed multiple sets of `ParrotSample` applications, which represent different operating environments, separated by port number:\n\n```\ndevelopment: 8081\ntest: 8082\nproduction: 8080\n```\n\nWe hope that the same set of test cases can be reused in different environments. We can do like below:\n\nFirstly, edit `demo2/environments/sample_env.yml`:\n\n```\ndevelopment:\n  host: 10.10.100.100:8081\nglobal:\n  host: 10.10.100.100:8080\nproduction:\n  host: 10.10.100.100:8080\ntest:\n  host: 10.10.100.100:8082\n```\n\nThen, manually replace all the values \u200b\u200bof the `host` in all generated test_steps ymls with the `${host}` variable reference.\n\n##### Multiple sets of environment switching during playback\n\nAs mentioned in Section 1.2.3, the `parrot playback` command provides the `-env, --environment` parameter, which specifies the selected environment identifier at execution time.\n\n```\n$ parrot playback -s demo2/test_suites -t demo2 -env development\n```\n\nCurrently, the environment reference is included in the config of test\\_suites/test\\_cases/test\\_steps, and the `playback` parameter can also be specified. The load priority is:\n\n**parameter > test\\_suite.config > test\\_case.config > test\\_step.config**\n\n***\n\n## 2. Design ideas: starting from the original meaning of software testing\n\n### 2.1 How to define software testing\n\n**Classic definition of software testing:**\n\n> The process of using a **manual** or **automated** means to run or measure a software system, the purpose of which is to verify that it meets **specified requirements** or to clarify the difference between **expected** and **actual** results.\n>\n>   -- *Software engineering terminology from IEEE in 1983*\n\n**A simplified definition:** \n> The process of running a system or application in accordance with defined requirements/steps, obtaining **actual result**, and comparing with the **expected result**.\n\n***\n\n### 2.2 How to do software testing\n\nLet's take the use case template of the ZenTao project management platform as an example. Everyone's daily use case design is basically similar, see the figure below:\n\n![](doc/TestCase.jpg)\n\nIf we want to extract key elements from it, based on the definition of software testing, we can easily find the following content:\n\n* `Steps`\n* `Expect` of each `Step`\n* Sometimes, in order to ensure complete execution, it is possible to add `Prerequisite`\n\nOther elements are usually designed to make use cases easier to manage. Think about it, do we usually do like this?\n\n***\n\n### 2.3 How to automate software testing\n\nWe use HTTP(S) interface test as an example to automate. There are usually two ways:\n\n#### 2.3.1 Use testing tools, such as: POSTMAN, JMETER\n\nTake Postman as an example. The implementation is roughly as shown in the figure below:\n\n![](doc/Postman.jpg)\n\n**Advantage of this approach:**\n\n1. Collection > Folder > Request, layered design makes the use case organization clearer\n2. With the environment variable management mechanism, it is convenient for common variable extraction and multiple sets of environment switching.\n3. `Pre-request Script` and `Tests` support pre- and post-actions, while `Tests` provides a richer validation methods that reduces the coding threshold.\n\n**Insufficient in this way:**\n\n1. The cost of creation is high, and each new Request requires a certain amount of effort to complete the addition, especially the `Query Params` and `Tests` sections. For dependencies between interfaces, it is necessary to write `Pre-request Script` more cumbersomely.\n2. The combination and order adjustment between Requests is not convenient enough, especially for Case or Suite with actual business logic.\n\n\n#### 2.3.2 Write your own automation framework, such as: Python+requests+unittest\n\nThe rough implementation, as in the example below:\n\n```\n# -*- coding: utf-8 -*-\n\nimport requests\nimport unittest\n\n\nclass TestHttpBin(unittest.TestCase):\n    def setUp(self) -> None:\n        \"\"\"do something here\"\"\"\n\n    def test_get(self):\n        url = 'http://httpbin.org/'\n        params = {\n            'p1': 'v1',\n            'p2': 'v2'\n        }\n        response = requests.get(url=url, params=params)\n        self.assertEqual(response.status_code, 200)\n\n    def test_post(self):\n        \"\"\"do something like test_get\"\"\"\n\n    def tearDown(self) -> None:\n        \"\"\"do something here\"\"\"\n\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\n**Advantage of this approach:**\n\n1. Flexible use case stratification, data-driven, let the use cases be arranged according to their own wishes.\n2. `requests` module encapsulates the underlying capabilities, we only need to write business logic code.\n3. `unittest` supports pre- and post- actions, while providing a richer verification methods and lowering the coding threshold.\n\n**Insufficient in this way:**\n\n1. The cost of creation/maintenance is high, and each new Request requires a certain amount of effort to complete the coding, especially the more assertions.\n2. There are certain technical thresholds for the design and writing of the framework, especially to ensure sufficient ease of use and versatility. For the dependencies between interfaces, the corresponding process variable transfer mechanism needs to be designed.\n\n***\n\n### 2.4 Is there a more convenient way to automate?\n\nComprehensive #2.3 chapter two kinds of automation methods, a lot of workload is reflected in:\n\n* A large number of Request/TestCase definitions, especially those with more parameters.\n* A large number of assertion method definitions, different Request verification points have differences.\n* Scenes that depend on other interfaces require more tedious processing of parameter passing.\n\n**\"Lazy\" is the first driving force of the technological advancement.**\n\nThe design idea of \u200b\u200bthe `parrot` tool is to solve the above problems, greatly improve the efficiency of automatic realization, and make automation easier.\n\n* A large number of Request/TestCase definitions can be implemented simply and quickly by `recording`.\n* Through `playback` mode, it can support the regular execution of automated use cases, and help you solve problems such as interface dependencies.\n* About verification, based on the recorded Request's Response, automatically generate some regular assertions and support secondary editing.\n\nCompare with the definition of software testing:\n\n- Recording: Get/Define specified requirement and **expected result**\n- Playback: Perform the recorded script to get the **actual result**\n- Verify: Compare the **expected** and **actual** results\n\n**Traffic playback** is a way to automate the realization of **the original definition of software testing**.\n\nThe design of this project is based on above idea to automate the interface testing.\n\n## 3. Source Code\n\n### 3.1 GitHub\n\nThis project: <https://github.com/idle-man/iParrot>\n\nIts sample project: <https://github.com/idle-man/ParrotSample>\n\nAll of them contain a detailed README for your reference.\n\nIf you have any questions or suggestions, please feel free to submit an issue within the project.\n\n### 3.2 Framework Structure\n```\niparrot/\n  \u251c\u2500\u2500 modules\n  \u2502\u00a0\u00a0  \u251c\u2500\u2500 helper.py    : A collection of commonly used methods in which the Function can be used in other modules, also supporting the use of ${{function(params)}} in the cases.\n  \u2502\u00a0\u00a0  \u251c\u2500\u2500 request.py   : Execute HTTP(S) request based on `requests` and get the result\n  \u2502\u00a0\u00a0  \u251c\u2500\u2500 validator.py : The verification engine for request's response information, which supports multiple verification rules, as detailed in Validator.UNIFORM_COMPARATOR\n  \u2502\u00a0\u00a0  \u251c\u2500\u2500 logger.py    : Formatted log printing, support for output to screen or log files\n  \u2502\u00a0\u00a0  \u2514\u2500\u2500 reportor.py  : Standardized report printing, support for views of summary results and use case execution details\n  \u251c\u2500\u2500 extension\n  \u2502\u00a0\u00a0  \u2514\u2500\u2500 helper.py    : A collection of common methods that can be customized by the user, where the Function supports use as ${{function(params)}} in the cases\n  \u251c\u2500\u2500 parser.py : Parse the source file, and automatically generate formatted use cases; parse the specified use case set, load into memory\n  \u251c\u2500\u2500 player.py : Play back the specified set of use cases, execute them in levels, and finally generate a test report\n  \u2514\u2500\u2500 parrot.py : The main script, you can run `python parrot.py help` to see the specific usage\n\n```\n\n## 4. Appendix\n\n### 4.1 Use case structure\n\nExample:\n\n- **environment**\n\n\t```yaml\n\tglobal: {}\n\tproduction: {}\n\tdevelopment: {}\n\ttest: {}\n\t```\n- **test_step**\n\n\t```yaml\n\tconfig:\n\t  environment: <environment flag>\n\t  import: <environment file>\n\t  name: step name\n\t  variables:\n\t    p1: ABC\n\t    p2: 123\n\trequest:\n\t  method: POST\n\t  protocol: http\n\t  host: x.x.x.x:8000\n\t  url: /path/of/api\n\t  params: {}\n\t  data:\n\t    param1: ${p1}\n\t    param2: ${p2}\n\t  headers:\n\t    Content-Type: application/json; charset=UTF-8\n\t  cookies: {}\n\t  time.start: 1568757525027\n\tresponse:\n\t  extract: {}\n\tsetup_hooks: []\n\tteardown_hooks: []\n\tvalidations:\n\t- eq:\n\t    status.code: 200\n\t- exists:\n\t    headers.token\n\t- is_json:\n\t    content\n\t- eq:\n\t    content.code: 100\n\t```\n- **test_case**\n\n\t```yaml\n\tconfig:\n\t  environment: <environment flag>\n\t  import: <environment file>\n\t  name: case name\n\t  variables: {}\n\tsetup_hooks: []\n\tteardown_hooks: []\n\ttest_steps:\n\t  - <fullname of step1>\n\t  - <fullname of step2>\n\t```\n- **test_suite**\n\n\t```yaml\n\tconfig:\n\t  environment: <evnironment flag>\n\t  import: <environment file>\n\t  name: suite name\n\t  variables: {}\n\tsetup_hooks: []\n\tteardown_hooks: []\n\ttest_cases: \n\t  - <fullname of case1>\n\t  - <fullname of case2>\n\t```\n\n***\n\n### 4.2 Verification method set\n\nThese methods can be used in validations in specific test_steps, for example:\n\n```yaml\nvalidations:\n- eq:\n    status.code: 200\n- is_json:\n    content\n- not_null:\n    headers.token\n- contains:\n    content.message: succ\n```\n\n**Commonly used verification methods:**\n\n- **eq(equals):**\n\t- Example: `1 eq 1`, `'a' eq 'a'`, `[1, 2] eq [1, 2]`, `{'a': 1 } eq {'a': 1}`, `status.code eq 200`\n\t- Usage:\n\n\t\t```\n\t\tvalidations:\n\t\t- eq:\n\t\t    status.code: 200\n\t\t- eq:\n\t\t    headers.Content-Type: application/json;charset=UTF-8\n\t\t- eq:\n\t\t    content.data[0].id: 1000\n\t\t```\n\t- Similar methods: `neq`, `lt`, `gt`, `le`, `ge`\n- **len_eq(length equals):**\n\t- Example: `'ab' len_eq 2`, `[1, 2] len_eq 2`, `{'a': 1} len_eq 1`\n\t- Usage:\n\n\t\t```\n\t\tvalidations:\n\t\t- len_eq:\n\t\t    headers.token: 32\n\t\t- eq:\n\t\t    content.datalist: 3\n\t\t```\n\t- Similar methods: `len_neq`, `len_lt`, `len_gt`\n- **time_le(time spent less than or equals):**\n\t- Example: `request 'time.spent' time_le 200 (unit is ms)`\n\t- Usage:\n\n\t\t```\n\t\tvalidations:\n\t\t- time_le:\n\t\t    time.spent: 200\n\t\t```\n\t- Similar methods: `time_lt`, `time_gt`, `time_ge`\n- **contains:**\n\t- Example: `'abc' contain 'ab', ['a', 'b'] contain 'a', {'a': 1, 'b': 2} contain {'a': 1}`\n\t- Usage:\n\n\t\t```\n\t\tvalidations:\n\t\t- contains:\n\t\t    headers.Content-Type: application/json\n\t\t- contains:\n\t\t    content.message: ok\n\t\t```\n\t- Similar methods: `not_contains`\n- **in:**\n\t- Example: `'a' in 'ab'`, `'a' in ['a', 'b']`, `'a' in {'a': 1, 'b': 2}`\n\t- Usage:\n\n\t\t```\n\t\tvalidations:\n\t\t- in:\n\t\t    status.code: [200, 302]\n\t\t```\n\t- Similar methods: `not_in`\n- **is_false:**\n\t- Example: `0 is_false`, `'' is_false`, `[] is_false`, `{} is_false`\n\t- Usage:\n\n\t\t```\n\t\tvalidations:\n\t\t- is_false:\n\t\t    content.datalist\n\t\t- is_json:\n\t\t    content\n\t\t- is_instance:\n\t\t    status.code: int\n\t\t```\n\t- Similar methods: `is_true`, `exists`, `is_instance`, `is_json`\n- **re(regex):**\n\t- Example: `'1900-01-01' re r'\\d+-\\d+-\\d+'`\n\t- Usage:\n\n\t\t```\n\t\tvalidations:\n\t\t- re:\n\t\t    content.data[0].date: r\"\\d+-\\d+-\\d+\"\n\t\t```\n\t- Similar methods: `not_re`\n\nMore methods and instructions can be found in the following way:\n\n```python\nimport json\nfrom iparrot.modules.validator import Validator\n\nprint(json.dumps(Validator.UNIFORM_COMPARATOR, indent=4))\n\n```\n\n***\n\n### 4.3 Helper method set\n\nThese methods can be applied to the inside of test_step / test_case / test_suite in the form `${{function(params)}}`, such as: `setup_hooks` `teardown_hooks` `variables`\n\n```\ntoday(form='%Y-%m-%d'): Get today's date\n\ndays_ago(days=0, form='%Y-%m-%d'): Get the date a few days ago\n\ndays_later(days=0, form='%Y-%m-%d'): Get the date a few days later\n\nnow(form='%Y-%m-%d %H:%M:%S'): Get the current time, accurate to the second\n\nnow_ms(form='%Y-%m-%d %H:%M:%S'): Get the current time, accurate to the micro second\n\nnow_timestamp(): Get the current timestamp, accurate to the second\n\nnow_timestamp_ms(): Get the current timestamp, accurate to the micro second\n\nhours_ago(hours=0, form='%Y-%m-%d %H:%M:%S'): Get the time a few hours ago, accurate to the second\n\nhours_later(hours=0, form='%Y-%m-%d %H:%M:%S'): Get the time a few hours later, accurate to the second\n\nget_file_name(file, ext=0): Intercept the file name of the specified file, no suffix by default\n\nget_file_path(file): Intercept the path of the specified file\n\nmake_dir(directory): Generate the specified directory\n\ncopy_file(source, target): Copy the specified file to the target path\n\nget_random_integer(length=10, head=None, tail=None): Generate random number, with specified length, head, tail\n\nget_random_string(length=10, simple=1, head=None, tail=None): Generate random string, with specified length, head, tail\n\nget_random_phone(head=None, tail=None): Generate random Chinese phone number, with specified length, head, tail\n```\n\n> If the above helper methods do not meet your needs, you can use the following method:\n> \n> Define your own module or pip install specified module in your local environment.\n> \n> Add `import xxx` or `from xxx import yyy` to the desired step / case / suite `setup_hooks`, \n> \n> then you could use the `${{function(params)}}` format to call the method you want.\n> \n> Any questions, you can feedback an issue: <https://github.com/idle-man/iParrot/issues>\n> \n> More general helper methods, welcome to contribute code or issues, thanks.\n\n## 5. External reference, thanks\n### 5.1 [Postman](https://learning.getpostman.com/)\n\n#### 5.1.1 Environments management\nThe mechanism is referenced in the `environment` of the Parrot use case structure.\n\n```\nA project can be configured with multiple sets of environments to hold some common environment variables.\n\nVariable names are consistent between different environments, and values \u200b\u200bcan vary.\n\nIn the use case, you can refer to the variable by means of ${variable}, reducing manual modification.\n\nThe switching of the operating environment can be specified in the playback phase by the -env/--environment parameter.\n```\n\n#### 5.1.2 Use case layering mode\n - Collection => test_suite\n - Folder => test_case\n - Request => test_step\n\n#### 5.1.3 Pre and post actions\n - Pre-request Script => setup_hooks\n - Tests => teardown_hooks & validations\n\n### 5.2 [HttpRunner](https://github.com/httprunner/httprunner)\n\n#### 5.2.1 [HAR2Case](https://github.com/HttpRunner/har2case)\n\nThe files processed by Parrot in the first phase are Charles trace and Fiddler txt. The format is quite different, and the parsing of plain text is cumbersome.\n\nLater, in the course of HttpRunner's ideas, I used HAR to reconstruct the record part. At the same time, I made some changes in the parameters.\n\nInspired by HttpRunner's ideas, the record part is rebuilt, and some paramters are updated.\n\nFor details, to see `parrot help record` and `iparrot.parser`\n\n#### 5.2.2 Use case layering mode\n\nThe use case layering mode of HttpRunner, TestSuite>TestCase>TestStep, is clear and a good reference.\n\nWhen Parrot automatically generates use cases, it directly implements the layering mode on the directory structure and changes the specific use case structure.\n\n#### 5.2.3 setup hooks & teardown hooks\n\nParrot reuses this naming scheme, which supports `set variable`, `call function`, `exec code`.\n\n#### 5.2.4 extract variable\n\nParrot in the first phase uses the mode of `store` and `replace`, which is intended to keep all changes in a configuration file, and does not invade the use case at all. \n\nIn actual use, it is found that the usability is not good and the configuration is slightly cumbersome.\n\nRefer to HttpRunner, return the initiative to the user, and the variable can be extracted according to `extract` defination and used as `${variable}`.\n\n#### 5.2.5 comparator\n\nThe first version of Parrot diffs results refer to a  configuration file, only supports `eq` and simple `re`, and the method set is limited.\n\nNow refer to the HttpRunner, automatically generate `eq` comparator when recording, and support a variety of comparator customization.\n\nComparators in Parrot combines with the common verification methods of HttpRunner and Postman, and a certain supplement.\n\n#### 5.2.6 report\n\nParrot's test report template directly reuses HttpRunner's report style.\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/idle-man/iParrot", "keywords": "test automate automation record replay playback parrot iparrot http", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "iParrot", "package_url": "https://pypi.org/project/iParrot/", "platform": "", "project_url": "https://pypi.org/project/iParrot/", "project_urls": {"Bug Reports": "https://github.com/idle-man/iParrot/issues", "Documentation": "https://github.com/idle-man/iParrot/blob/master/README.md", "Homepage": "https://github.com/idle-man/iParrot", "Source": "https://github.com/idle-man/iParrot"}, "release_url": "https://pypi.org/project/iParrot/1.1.1/", "requires_dist": ["PyYAML", "requests", "check-manifest ; extra == 'dev'", "coverage ; extra == 'test'"], "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "summary": "Automated test solution for http requests based on recording and playback", "version": "1.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Parrot</h1>\n<p><strong>Automated test solution for http requests based on recording and playback</strong></p>\n<h2>1. Instructions for use</h2>\n<h3>1.1 Install</h3>\n<p>The Parrot project is developed based on Python 3\uff0c and the recommended version is 3.7.x. Please ensure that the corresponding version of python environment is installed on the target machine.</p>\n<p><strong>Method One: install via <code>pip</code> command</strong></p>\n<p>iParrot project has been submitted to PyPI.</p>\n<ol>\n<li>You could install via <code>pip install iParrot</code> command.</li>\n<li>If you need a upgrade, try <code>pip install -U iParrot</code> command.</li>\n</ol>\n<p><strong>Method Two: install using source code</strong></p>\n<p>Code Repository: <a href=\"https://github.com/idle-man/iParrot\" rel=\"nofollow\">https://github.com/idle-man/iParrot</a></p>\n<p>Download source code pkg and install via <code>python setup.py install</code> command.</p>\n<p>After the installation is complete, the <code>parrot</code> executable will be generated, try <code>parrot help</code> or <code>parrot -v</code>. If there is a problem, feedback an issue: <a href=\"https://github.com/idle-man/iParrot/issues\" rel=\"nofollow\">https://github.com/idle-man/iParrot/issues</a></p>\n<hr>\n<h3>1.2 Usage</h3>\n<h4>1.2.1 View commands supported by Parrot: <code>parrot help</code></h4>\n<p>Two core commands are: <strong>record</strong> and <strong>playback</strong></p>\n<pre><code>$ parrot help\nAutomated test solution for http requests based on recording and playback\nVersion: ...\n\nUsage: parrot [-h] [-v] [command] [&lt;args&gt;]\n\ncommand:\n  record   - parse source file and generate test cases\n            see detail usage: `parrot help record`\n  playback - run standardized test cases and do validations\n            see detail usage: `parrot help playback`\n  template - generate standardized test case template file\n            see detail usage: `parrot help template`\n  replace  - replace existing test cases with specified rules\n            see detail usage: `parrot help replace`\n  home     - show homepage on github\n  doc      - show readme on github\n\n\noptional arguments:\n  -h, --help         show this help message and exit\n  -v, -V, --version  show version\n</code></pre>\n<h4>1.2.2 View usage of <code>record</code>: <code>parrot help record</code></h4>\n<p>The purpose of this step is to parse the user-specified source file (currently .har) into a standardized set of use cases.</p>\n<pre><code>$ parrot help record\n...\n\nUsage: parrot record [&lt;args&gt;]\n\nArguments:\n  -s, --source SOURCE   source file with path, *.har or directory [required]\n  -t, --target TARGET   target output path, 'ParrotProject' as default\n  -i, --include INCLUDE include filter on url, separated by ',' if multiple\n  -e, --exclude EXCLUDE exclude filter on url, separated by ',' if multiple\n  -vi, --validation-include V_INCLUDE\n                        include filter on response validation, separated by ',' if multiple\n  -ve, --validation-exclude V_EXCLUDE  \n                        exclude filter on response validation, separated by ',' if multiple\n  -ae, --auto-extract   automatic identification of interface dependencies or not, False as default\n\n  --log-level LOG_LEVEL log level: debug, info, warn, error, info as default\n  --log-mode  LOG_MODE  log mode : 1-on screen, 2-in log file, 3-1&amp;2, 1 as default\n  --log-path  LOG_PATH  log path : &lt;project path&gt; as default\n  --log-name  LOG_NAME  log name : parrot.log as default\n\n</code></pre>\n<h4>1.2.3 View usage of <code>playback</code>: <code>parrot help playback</code></h4>\n<p>This step is to execute the specified set of test cases and generate a test report.</p>\n<pre><code>$ parrot help playback\n...\n\nUsage: parrot playback [&lt;args&gt;]\n\nArguments:\n  -s, --suite, -c, --case SUITE_OR_CASE\n                        test suite or case with path, *.yml or directory [required]\n  -t, --target TARGET   output path for report and log, 'ParrotProject' as default\n  -i, --interval INTERVAL\n                        interval time(ms) between each step, use the recorded interval as default\n  -env, --environment ENVIRONMENT\n                        environment tag, defined in project/environments/*.yml, use defined config in test_suites as default\n  -reset, --reset-after-case\n                        reset runtime environment or not, False as default\n\n  --fail-stop           stop or not when a test step failed on validation, False as default\n  --fail-retry-times FAIL_RETRY_TIMES\n                        max retry times when a test step failed on validation, 0 as default\n  --fail-retry-interval FAIL_RETRY_INTERVAL \n                        retry interval(ms) when a test step failed on validation, 100 as default\n\n  --log-level LOG_LEVEL log level: debug, info, warn, error, info as default\n  --log-mode  LOG_MODE  log mode : 1-on screen, 2-in log file, 3-1&amp;2, 1 as default\n  --log-path  LOG_PATH  log path : &lt;project path&gt; as default\n  --log-name  LOG_NAME  log name : parrot.log as default\n\n</code></pre>\n<h4>1.2.4 View usage of <code>template</code>: <code>parrot help template</code></h4>\n<p>This step is to automatically generate standardized test case templates and examples, which is convenient for users to refer to self-built use cases.</p>\n<pre><code>$ parrot help template\n...\n\nUsage: parrot template [&lt;args&gt;]\n\nArguments:\n  -t, --target TARGET   target output path, 'ParrotProject' as default\n\n  --log-level LOG_LEVEL log level: debug, info, warn, error, info as default\n  --log-mode  LOG_MODE  log mode : 1-on screen, 2-in log file, 3-1&amp;2, 1 as default\n  --log-path  LOG_PATH  log path : &lt;project path&gt; as default\n  --log-name  LOG_NAME  log name : parrot.log as default\n\n</code></pre>\n<h4>1.2.5 View usage of <code>replace</code>: <code>parrot help replace</code></h4>\n<p>This step is to batch replace the <code>config.variables</code> of the generated use cases according to your specified rules.</p>\n<pre><code>$ parrot help replace\n...\n\nUsage: parrot replace [&lt;args&gt;]\n\nArguments:\n  -s, --suite, -c, --case SUITE_OR_CASE\n                        test suite or case with path, *.yml or directory [required]\n  -t, --target TARGET   target output path, 'ParrotProjectNew' as default\n  -r, --rule RULE       replace rules, separated by ',' if multiple [required]\n                        'key=&gt;value', 'value1=&gt;value2' or 'apiA::key=&gt;value' only for specified api\n\n  --log-level LOG_LEVEL log level: debug, info, warn, error, info as default\n  --log-mode  LOG_MODE  log mode : 1-on screen, 2-in log file, 3-1&amp;2, 1 as default\n  --log-path  LOG_PATH  log path : &lt;project path&gt; as default\n  --log-name  LOG_NAME  log name : parrot.log as default\n\n</code></pre>\n<hr>\n<h3>1.3 Demo</h3>\n<h4>1.3.1 Build Sample application and export HAR</h4>\n<p>In order to facilitate the function debugging and operation demonstration, a simple web application(based on Python Flask) was specially built: <a href=\"https://github.com/idle-man/ParrotSample\" rel=\"nofollow\">https://github.com/idle-man/ParrotSample</a></p>\n<p>Please refer to its README to complete the application setup in the local environment. It provides the \"recommended function operations\".</p>\n<p>We use this as an example, using the browser's developer tools, after completing this series of operations, export the HAR file, assuming we named it <code>sample.har</code> and placed it in your working directory.</p>\n<blockquote>\n<p>HAR is a common standardized format for storing HTTP requests and responses.</p>\n<p>Its versatility: can be exported with consistent format from Charles, Fiddler, Chrome, etc.</p>\n<p>Its standardization: JSON format and UTF-8 coding.</p>\n</blockquote>\n<hr>\n<h4>1.3.2 Record - Transforming HAR into standardized use cases</h4>\n<p>We assume that you have completed the installation of <code>parrot</code> as described in Chapter 1. Now we use the command line tool on the computer and switch to the path where <code>sample.har</code> is located.</p>\n<blockquote>\n<p>It is recommended to use <code>PyCharm</code>, which contains <code>Terminal</code>, which is convenient for operation and helps to view the following use cases.</p>\n</blockquote>\n<p>According to the instructions in Section 1.2.2, we have a general understanding of the basic usage of the <code>record</code> command. Now let's make a try on <code>sample.har</code>.</p>\n<p><strong># A simple record: -s &amp; -t</strong></p>\n<pre><code>$ parrot record -s sample.har -t demo0\n</code></pre>\n<p>If it is successful, after the execution is complete, you will see the generated use cases set in current directory. The structure is as follows:</p>\n<pre><code>demo0/\n  \u251c\u2500\u2500 environments\n  \u2502\u00a0\u00a0  \u2514\u2500\u2500 *env*.yml: Project-level environment variable configuration\n  \u251c\u2500\u2500 test_steps\n  \u2502\u00a0\u00a0  \u2514\u2500\u2500 *case_name*.yml: The minimum execution unit, a http request is a step\n  \u251c\u2500\u2500 test_cases\n  \u2502\u00a0\u00a0  \u2514\u2500\u2500 *case_name*.yml: Independent closed-loop unit,  consisting of one or more steps\n  \u2514\u2500\u2500 test_suites\n   \u00a0\u00a0  \u2514\u2500\u2500 *suite_name*.yml: Test case set, consisting of one or more cases\n</code></pre>\n<p>Each <code>Entry</code> in HAR will be converted into a test_step containing the specific request and validation information.</p>\n<p>Each <code>test_step</code> <code>test_case</code> <code>test_suite</code> contains its own <code>config</code> <code>setup_hooks</code> <code>teardown_hooks</code>.</p>\n<p>The specific format can be found in the <code>Appendix: Use Case Structure</code>.</p>\n<p><strong># Not all recorded requests need to be converted to use cases: -i &amp; -e</strong></p>\n<p>Because HAR is a full export without selection, it may contain some requests that we don't need to test, such as <code>*.css</code> <code>*.js</code>. In order to avoid the manual processing, we can filter them out during the record phase.</p>\n<p>Parrot currently provides -i/--include and -e/--exclude filter modes, specifically:</p>\n<ul>\n<li>-i, --include: According to the specified parameters, the <code>url</code> in each <code>Entry</code> is ambiguously matched, and the matching will be retained. The relationship between multiple include parameters is OR.</li>\n<li>-e, --exclude: According to the specified parameters, the <code>url</code> in each <code>Entry</code> is ambiguously matched, and the matching will be excluded. The relationship between multiple excludes is OR.</li>\n</ul>\n<p>The two parameters can be used only in one or in combination to achieve your filtering needs.</p>\n<p>In our above requirement, we want to filter out the unwanted <code>*.css</code> and <code>*.js</code>, just use -e, --exclude.</p>\n<pre><code>$ parrot record -s sample.har -t demo1 -e \".css, .js\"\n</code></pre>\n<p>After that, you can check if the requirement is met in demo1/test_steps.</p>\n<p><strong># Not all content in response can be expected: -vi &amp; -ve</strong></p>\n<p>An example of the validations generated in test_step, the user can also customize this format:</p>\n<pre><code>validations:\n- &lt;comparator&gt;:\n    &lt;check&gt;: &lt;expected result&gt;\n</code></pre>\n<p>The <code>check</code>: According to the important information in the above Mode One, the unified format is: &lt;PREFIX&gt;.&lt;KEYS&gt;</p>\n<ul>\n<li>Available PREFIX: <code>status</code>, <code>content</code>, <code>headers</code>, <code>cookies</code>, in lower case</li>\n<li>KEYS in <code>status</code>: <code>code</code></li>\n<li>KEYS in <code>headers</code> and <code>cookies</code>: Currently only extracting the outer keys</li>\n<li>KEYS in <code>content</code>(json format): <code>content.a.b[1].c</code></li>\n</ul>\n<p><strong>By default, automatically generated validations will include <code>status.code</code> and all keys of <code>content</code> and unofficial keys of  <code>headers</code>.</strong></p>\n<p>In fact, some of these keys do not need to be verified for reasons such as variability, etc., such as the <code>tag</code> <code>timestamp</code> contained in the response information of each request in <code>sample.har</code>, which is best removed when it is automatically generated.</p>\n<p>Parrot currently provides -vi/--validation-include and -ve/--validation-exclude filter modes, specifically:</p>\n<ul>\n<li>-vi, --validation-include: According to the specified parameters, the <code>headers</code> and <code>content</code> in the response in each Entry are fuzzy matched, and the matching will be retained. The relationship between multiple -vi is OR.</li>\n<li>-ve, --validation-exclude: According to the specified parameters, the <code>headers</code> and <code>content</code> in the response in each Entry are fuzzy matched, the matching will be excluded. The relationship between multiple -ve is OR.</li>\n</ul>\n<p>The two parameters can be used only in one or in combination to achieve your filtering needs.</p>\n<p>In our above requirement, we want to filter out unwanted <code>tag</code> and <code>timestamp</code>, just use -ve, --validation-exclude.</p>\n<pre><code>$ parrot record -s sample.har -t demo2 -e \".css, .js\" -ve \"content.tag, timestamp\"\n</code></pre>\n<p>About the <code>comparator</code>, the default is <code>eq</code> when it is automatically generated, which can be edited manually. Currently, to view all the comparators that parrot supports, see <code>Appendix: Verification Method Set</code>.</p>\n<p><strong># Sometimes we need to generate the parameters in real time.</strong></p>\n<p>Take the <code>hobby_suggest</code> request in <code>sample.har</code> as an example. The <code>today</code> parameter of the interface needs the date of the execution time. The default value recorded by the recording is the static value. If we run it in the next day, it will not meet the requirements. At this point, you need to generate this parameter in real time.</p>\n<p>The way Parrot supports is: <code>${{function(params)}}</code>, where those functions are provided in <code>iparrot.modules.helper</code>, which can be used directly. See <code>Appendix: Helper Method Set</code>.</p>\n<p>Example:</p>\n<pre><span class=\"nt\">config</span><span class=\"p\">:</span>\n  <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n  <span class=\"l l-Scalar l-Scalar-Plain\">variables</span><span class=\"p p-Indicator\">:</span>\n    <span class=\"nt\">p1</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">ABC</span>\n    <span class=\"nt\">p2</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">123</span>\n    <span class=\"nt\">p3</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">${{today()}}</span>\n<span class=\"nt\">request</span><span class=\"p\">:</span>\n  <span class=\"nt\">method</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">GET</span>\n  <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n  <span class=\"nt\">params</span><span class=\"p\">:</span>\n    <span class=\"nt\">param1</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">${p1}</span>\n    <span class=\"nt\">param2</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">${p2}</span>\n    <span class=\"nt\">today</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">${p3}</span>\n  <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n</pre>\n<p><strong># Sometimes there is a dependency on the response of the previous request.</strong></p>\n<p>Take the <code>hobby_detail</code> request in <code>sample.har</code> as an example. Its <code>name</code> argument is from the real-time valid return in the <code>hobby_list</code> request's response. If the data during recording is directly played back, it is likely that there is a failure sometimes.</p>\n<p>Parrot's solution is: <code>extract</code> the specific key in the response of the <code>hobby_list</code> request, and use the <code>${variable}</code> format in the <code>hobby_detail</code> request.</p>\n<p>Example:</p>\n<p>hobby_list:</p>\n<pre><span class=\"nt\">config</span><span class=\"p\">:</span>\n  <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n<span class=\"nt\">request</span><span class=\"p\">:</span>\n  <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n<span class=\"nt\">response</span><span class=\"p\">:</span>\n  <span class=\"nt\">extract</span><span class=\"p\">:</span>\n    <span class=\"nt\">hobby</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">content.hobbies[0].name</span>\n<span class=\"nn\">...</span>\n</pre>\n<p>hobby_detail:</p>\n<pre><span class=\"nt\">config</span><span class=\"p\">:</span>\n  <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n<span class=\"nt\">request</span><span class=\"p\">:</span>\n  <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n  <span class=\"l l-Scalar l-Scalar-Plain\">params</span><span class=\"p p-Indicator\">:</span>\n    <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">${hobby}</span>\n  <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n</pre>\n<p>In iParrot 1.0.6 and later, <code>parrot record</code> has the <code>-ae, --auto-extract</code> parameter added.</p>\n<p>If this parameter is specified, parrot will automatically recognize the parameter dependency between interfaces during the parsing process.</p>\n<p>The <code>extract</code> extraction and <code>${variable}</code> references are automatically completed when the use cases are generated.</p>\n<p>In view of the possibility of many formats in the actual scene, the 'automatic' identification may cause some accidental or omission, and it is recommended to perform artificial inspection and correction after its execution.</p>\n<p>Regarding the recording phase, the above scenarios should cover most of the use cases. If there are other unsupported questions, welcome feedback an issue: <a href=\"https://github.com/idle-man/iParrot/issues\" rel=\"nofollow\">https://github.com/idle-man/iParrot/issues</a></p>\n<hr>\n<h4>1.3.3 Template - Generate a standardized use case template when no files are recorded</h4>\n<p>In actual work, there may be cases where there is no HAR file, such as a new project or a new interface before going online.</p>\n<p>For users who have existing HAR files, you can skip this step.</p>\n<p><code>parrot</code>1.1.0 and later provides the <code>template</code> command to help you generate standardized use case templates, where you can manually edit your test cases.</p>\n<p>See section #1.2.4 for specific usage: <code>parrot help template</code></p>\n<p>The generated use case structure is consistent with the <code>recording</code> phase, including a <code>test_suite</code>, a <code>test_case</code>, a <code>test_step</code> and a <code>environment</code>.</p>\n<p>Users can refer above template to write their own formatting use cases, which can also be used for subsequent <code>playback</code>.</p>\n<hr>\n<h4>1.3.4 Replace - Batch update generated use cases according to specified rules</h4>\n<p>For use cases that have already been generated, we sometimes have a large number of update requirements, such as:</p>\n<ul>\n<li>Need to replace the ID in the argument with another one</li>\n<li>Need to replace a date parameter with a <code>${{today()}}</code> function call</li>\n<li>Need to replace the host in the request with a new test address</li>\n<li>Need to replace the token in the request header with the <code>${variable}</code> variable reference</li>\n</ul>\n<p>Some of the above updates can be done with the help of an IDE.</p>\n<p>For the user's convenient use, <code>parrot</code>1.1.0 and later versions provide the <code>replace</code> command, which can be used to batch update the target use case set according to the rules specified by the user.</p>\n<p>See section #1.2.5 for specific usage: <code>parrot help replace</code></p>\n<p>For example:</p>\n<p><code>parrot replace -s demo/test_suites/test.yml -t demoNew -r 'xxID=&gt;abcd, 2019-01-01=&gt;${{today()}}, host=&gt;example.com'</code></p>\n<p>The above command will automatically traverse all the yml files in the specified test_suite and its included test_cases and test_steps,</p>\n<p>and match the contents of <code>config.variables</code>, <code>request</code>, <code>request.headers</code>, <code>request.cookies</code> blocks.</p>\n<p>The content before the '=&gt;' symbol will be absolutely matched with the keys in the above use case block, and if it is missed, it will definitely match the values.</p>\n<p>If some key or value matches, the value will be replaced with the content after the '=&gt;' symbol.</p>\n<p><strong>If a replacement rule only needs to be valid for a particular interface, you can do this:</strong></p>\n<p><code>parrot replace -s demo/test_suites/test.yml -t demoNew -r 'xxID=&gt;abcd, apiA::2019-01-01=&gt;${{today()}}'</code></p>\n<p>The above <code>2019-01-01=&gt;${{today()}}</code> rule only works on the yml of the file name fuzzy matched <code>apiA</code>.</p>\n<hr>\n<h4>1.3.5 Replay - Execute use cases, verify result, generate report</h4>\n<p>According to the instructions in Section #1.2.3, we have a general understanding of the basic usage of the <code>playback</code> command. Now let's try to play back the <code>demo2</code> recorded earlier.</p>\n<p><strong># A simple playback: -s &amp; -t</strong></p>\n<pre><code>$ parrot playback -s demo2/test_suites -t demo2\n</code></pre>\n<p>If it is successful, the process output information will be visible on the screen during the execution. After the execution is completed, you will see the generated test report in the <code>demo2</code> directory: <code>parrot_&lt;timestamp&gt;.html</code>, which could be  viewed via PyCharm or browser.</p>\n<p><strong># About the running order</strong></p>\n<p>Parrot execute the test in the order of cases and steps defined in <em>test_suite</em>.yaml / <em>test_case</em>.yaml, currently only supports serial execution mode.</p>\n<blockquote>\n<p>When the use case is automatically generated, the order of the steps defaults to the order of appearance in the recorded sample, which can be edited manually.</p>\n</blockquote>\n<p>The detailed execution process:</p>\n<pre><code>test_suite1\n |-&gt; suite1.setup\n |-&gt; test_case1\n   |-&gt; case1.setup\n   |-&gt; test_step1\n     |-&gt; step1.setup\n     |-&gt; request\n     |-&gt; validation\n     |-&gt; extract\n     |-&gt; step1.teardown\n   |-&gt; test_step2\n     ...\n   |-&gt; case1.teardown\n |-&gt; test_case2\n   ...\n |-&gt; suite1.teardown\ntest_suite2\n  ...\n</code></pre>\n<p><strong># About the running interval</strong></p>\n<p>The value of <code>interval</code> argument is firstly used, otherwise the value <code>time.start</code> in step defination.</p>\n<p>If the playback pass parameter specifies the <code>interval</code>, it will execute according to the interval (in milliseconds), for example:</p>\n<pre><code>$ parrot playback -s demo2/test_suites -t demo2 -i 100\n</code></pre>\n<p>Otherwise, if <code>time.start</code> is defined in the request of step (the recording phase will be automatically recorded), the default is to execute according to the interval of <code>time.start</code> of each step.</p>\n<blockquote>\n<p>When the use case is automatically generated, the actual execution time would be recorded as <code>time.start</code> in the step defination.</p>\n</blockquote>\n<p><strong># About the running validation</strong></p>\n<p>As mentioned in the recording phase, each request generates some validations, which contain the expected result.</p>\n<p>In the process of request playback, parrot can get the actual result in real time, so you can check the result in real time, and then check whether the value of each <code>check</code> object conforms to the <code>comparator</code> rule. If there is one failure, the entire step fails.</p>\n<p>After a single step fails, the current Parrot does not terminate the execution of the playback by default, but the user can perform some intervention by running the parameters:</p>\n<ul>\n<li>--fail_stop: If specified, the operation will be terminated after a step verification fails</li>\n<li>--fail_retry_times: The number of retries after a step failed, 0 as default</li>\n<li>--fail_retry_interval: retry interval after a step failure</li>\n</ul>\n<hr>\n<h4>1.3.6 Adapting multiple sets of environments</h4>\n<p>Parrot draws on Postman's environmental management mechanism.</p>\n<h5>The environment configuration is automatically reserved during recording and can be edited manually.</h5>\n<p>Take the recorded demo2 in section 1.3.2 as an example. The automatically generated environment is configured as <code>demo2/environments/sample_env.yml</code>. The default generated content is reserved for several sets of environment identifiers and the content is empty:</p>\n<pre><code>development: {}\nglobal: {}\nproduction: {}\ntest: {}\n</code></pre>\n<p>At the same time, the <code>config</code> part of the automatically generated test_suites, test_cases and test_steps is referenced by <code>import</code> and <code>environment: global</code>, which can be edited manually.</p>\n<blockquote>\n<p><code>global</code> is globally shared, the rest are independent, and the new environment identifier can be customized.</p>\n</blockquote>\n<p>Suppose we have deployed multiple sets of <code>ParrotSample</code> applications, which represent different operating environments, separated by port number:</p>\n<pre><code>development: 8081\ntest: 8082\nproduction: 8080\n</code></pre>\n<p>We hope that the same set of test cases can be reused in different environments. We can do like below:</p>\n<p>Firstly, edit <code>demo2/environments/sample_env.yml</code>:</p>\n<pre><code>development:\n  host: 10.10.100.100:8081\nglobal:\n  host: 10.10.100.100:8080\nproduction:\n  host: 10.10.100.100:8080\ntest:\n  host: 10.10.100.100:8082\n</code></pre>\n<p>Then, manually replace all the values \u200b\u200bof the <code>host</code> in all generated test_steps ymls with the <code>${host}</code> variable reference.</p>\n<h5>Multiple sets of environment switching during playback</h5>\n<p>As mentioned in Section 1.2.3, the <code>parrot playback</code> command provides the <code>-env, --environment</code> parameter, which specifies the selected environment identifier at execution time.</p>\n<pre><code>$ parrot playback -s demo2/test_suites -t demo2 -env development\n</code></pre>\n<p>Currently, the environment reference is included in the config of test_suites/test_cases/test_steps, and the <code>playback</code> parameter can also be specified. The load priority is:</p>\n<p><strong>parameter &gt; test_suite.config &gt; test_case.config &gt; test_step.config</strong></p>\n<hr>\n<h2>2. Design ideas: starting from the original meaning of software testing</h2>\n<h3>2.1 How to define software testing</h3>\n<p><strong>Classic definition of software testing:</strong></p>\n<blockquote>\n<p>The process of using a <strong>manual</strong> or <strong>automated</strong> means to run or measure a software system, the purpose of which is to verify that it meets <strong>specified requirements</strong> or to clarify the difference between <strong>expected</strong> and <strong>actual</strong> results.</p>\n<p>-- <em>Software engineering terminology from IEEE in 1983</em></p>\n</blockquote>\n<p><strong>A simplified definition:</strong></p>\n<blockquote>\n<p>The process of running a system or application in accordance with defined requirements/steps, obtaining <strong>actual result</strong>, and comparing with the <strong>expected result</strong>.</p>\n</blockquote>\n<hr>\n<h3>2.2 How to do software testing</h3>\n<p>Let's take the use case template of the ZenTao project management platform as an example. Everyone's daily use case design is basically similar, see the figure below:</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4c868b4c3c85915155d80776d603a1746c356245/646f632f54657374436173652e6a7067\"></p>\n<p>If we want to extract key elements from it, based on the definition of software testing, we can easily find the following content:</p>\n<ul>\n<li><code>Steps</code></li>\n<li><code>Expect</code> of each <code>Step</code></li>\n<li>Sometimes, in order to ensure complete execution, it is possible to add <code>Prerequisite</code></li>\n</ul>\n<p>Other elements are usually designed to make use cases easier to manage. Think about it, do we usually do like this?</p>\n<hr>\n<h3>2.3 How to automate software testing</h3>\n<p>We use HTTP(S) interface test as an example to automate. There are usually two ways:</p>\n<h4>2.3.1 Use testing tools, such as: POSTMAN, JMETER</h4>\n<p>Take Postman as an example. The implementation is roughly as shown in the figure below:</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/071e587bbbaafa0d14db30f295b85952521c6368/646f632f506f73746d616e2e6a7067\"></p>\n<p><strong>Advantage of this approach:</strong></p>\n<ol>\n<li>Collection &gt; Folder &gt; Request, layered design makes the use case organization clearer</li>\n<li>With the environment variable management mechanism, it is convenient for common variable extraction and multiple sets of environment switching.</li>\n<li><code>Pre-request Script</code> and <code>Tests</code> support pre- and post-actions, while <code>Tests</code> provides a richer validation methods that reduces the coding threshold.</li>\n</ol>\n<p><strong>Insufficient in this way:</strong></p>\n<ol>\n<li>The cost of creation is high, and each new Request requires a certain amount of effort to complete the addition, especially the <code>Query Params</code> and <code>Tests</code> sections. For dependencies between interfaces, it is necessary to write <code>Pre-request Script</code> more cumbersomely.</li>\n<li>The combination and order adjustment between Requests is not convenient enough, especially for Case or Suite with actual business logic.</li>\n</ol>\n<h4>2.3.2 Write your own automation framework, such as: Python+requests+unittest</h4>\n<p>The rough implementation, as in the example below:</p>\n<pre><code># -*- coding: utf-8 -*-\n\nimport requests\nimport unittest\n\n\nclass TestHttpBin(unittest.TestCase):\n    def setUp(self) -&gt; None:\n        \"\"\"do something here\"\"\"\n\n    def test_get(self):\n        url = 'http://httpbin.org/'\n        params = {\n            'p1': 'v1',\n            'p2': 'v2'\n        }\n        response = requests.get(url=url, params=params)\n        self.assertEqual(response.status_code, 200)\n\n    def test_post(self):\n        \"\"\"do something like test_get\"\"\"\n\n    def tearDown(self) -&gt; None:\n        \"\"\"do something here\"\"\"\n\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>\n<p><strong>Advantage of this approach:</strong></p>\n<ol>\n<li>Flexible use case stratification, data-driven, let the use cases be arranged according to their own wishes.</li>\n<li><code>requests</code> module encapsulates the underlying capabilities, we only need to write business logic code.</li>\n<li><code>unittest</code> supports pre- and post- actions, while providing a richer verification methods and lowering the coding threshold.</li>\n</ol>\n<p><strong>Insufficient in this way:</strong></p>\n<ol>\n<li>The cost of creation/maintenance is high, and each new Request requires a certain amount of effort to complete the coding, especially the more assertions.</li>\n<li>There are certain technical thresholds for the design and writing of the framework, especially to ensure sufficient ease of use and versatility. For the dependencies between interfaces, the corresponding process variable transfer mechanism needs to be designed.</li>\n</ol>\n<hr>\n<h3>2.4 Is there a more convenient way to automate?</h3>\n<p>Comprehensive #2.3 chapter two kinds of automation methods, a lot of workload is reflected in:</p>\n<ul>\n<li>A large number of Request/TestCase definitions, especially those with more parameters.</li>\n<li>A large number of assertion method definitions, different Request verification points have differences.</li>\n<li>Scenes that depend on other interfaces require more tedious processing of parameter passing.</li>\n</ul>\n<p><strong>\"Lazy\" is the first driving force of the technological advancement.</strong></p>\n<p>The design idea of \u200b\u200bthe <code>parrot</code> tool is to solve the above problems, greatly improve the efficiency of automatic realization, and make automation easier.</p>\n<ul>\n<li>A large number of Request/TestCase definitions can be implemented simply and quickly by <code>recording</code>.</li>\n<li>Through <code>playback</code> mode, it can support the regular execution of automated use cases, and help you solve problems such as interface dependencies.</li>\n<li>About verification, based on the recorded Request's Response, automatically generate some regular assertions and support secondary editing.</li>\n</ul>\n<p>Compare with the definition of software testing:</p>\n<ul>\n<li>Recording: Get/Define specified requirement and <strong>expected result</strong></li>\n<li>Playback: Perform the recorded script to get the <strong>actual result</strong></li>\n<li>Verify: Compare the <strong>expected</strong> and <strong>actual</strong> results</li>\n</ul>\n<p><strong>Traffic playback</strong> is a way to automate the realization of <strong>the original definition of software testing</strong>.</p>\n<p>The design of this project is based on above idea to automate the interface testing.</p>\n<h2>3. Source Code</h2>\n<h3>3.1 GitHub</h3>\n<p>This project: <a href=\"https://github.com/idle-man/iParrot\" rel=\"nofollow\">https://github.com/idle-man/iParrot</a></p>\n<p>Its sample project: <a href=\"https://github.com/idle-man/ParrotSample\" rel=\"nofollow\">https://github.com/idle-man/ParrotSample</a></p>\n<p>All of them contain a detailed README for your reference.</p>\n<p>If you have any questions or suggestions, please feel free to submit an issue within the project.</p>\n<h3>3.2 Framework Structure</h3>\n<pre><code>iparrot/\n  \u251c\u2500\u2500 modules\n  \u2502\u00a0\u00a0  \u251c\u2500\u2500 helper.py    : A collection of commonly used methods in which the Function can be used in other modules, also supporting the use of ${{function(params)}} in the cases.\n  \u2502\u00a0\u00a0  \u251c\u2500\u2500 request.py   : Execute HTTP(S) request based on `requests` and get the result\n  \u2502\u00a0\u00a0  \u251c\u2500\u2500 validator.py : The verification engine for request's response information, which supports multiple verification rules, as detailed in Validator.UNIFORM_COMPARATOR\n  \u2502\u00a0\u00a0  \u251c\u2500\u2500 logger.py    : Formatted log printing, support for output to screen or log files\n  \u2502\u00a0\u00a0  \u2514\u2500\u2500 reportor.py  : Standardized report printing, support for views of summary results and use case execution details\n  \u251c\u2500\u2500 extension\n  \u2502\u00a0\u00a0  \u2514\u2500\u2500 helper.py    : A collection of common methods that can be customized by the user, where the Function supports use as ${{function(params)}} in the cases\n  \u251c\u2500\u2500 parser.py : Parse the source file, and automatically generate formatted use cases; parse the specified use case set, load into memory\n  \u251c\u2500\u2500 player.py : Play back the specified set of use cases, execute them in levels, and finally generate a test report\n  \u2514\u2500\u2500 parrot.py : The main script, you can run `python parrot.py help` to see the specific usage\n\n</code></pre>\n<h2>4. Appendix</h2>\n<h3>4.1 Use case structure</h3>\n<p>Example:</p>\n<ul>\n<li>\n<p><strong>environment</strong></p>\n<pre> <span class=\"nt\">global</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{}</span>\n <span class=\"nt\">production</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{}</span>\n <span class=\"nt\">development</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{}</span>\n <span class=\"nt\">test</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{}</span>\n</pre>\n</li>\n<li>\n<p><strong>test_step</strong></p>\n<pre> <span class=\"nt\">config</span><span class=\"p\">:</span>\n   <span class=\"nt\">environment</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;environment flag&gt;</span>\n   <span class=\"nt\">import</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;environment file&gt;</span>\n   <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">step name</span>\n   <span class=\"nt\">variables</span><span class=\"p\">:</span>\n     <span class=\"nt\">p1</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">ABC</span>\n     <span class=\"nt\">p2</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">123</span>\n <span class=\"nt\">request</span><span class=\"p\">:</span>\n   <span class=\"nt\">method</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">POST</span>\n   <span class=\"nt\">protocol</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">http</span>\n   <span class=\"nt\">host</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">x.x.x.x:8000</span>\n   <span class=\"nt\">url</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">/path/of/api</span>\n   <span class=\"nt\">params</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{}</span>\n   <span class=\"nt\">data</span><span class=\"p\">:</span>\n     <span class=\"nt\">param1</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">${p1}</span>\n     <span class=\"nt\">param2</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">${p2}</span>\n   <span class=\"nt\">headers</span><span class=\"p\">:</span>\n     <span class=\"nt\">Content-Type</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">application/json; charset=UTF-8</span>\n   <span class=\"nt\">cookies</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{}</span>\n   <span class=\"nt\">time.start</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">1568757525027</span>\n <span class=\"nt\">response</span><span class=\"p\">:</span>\n   <span class=\"nt\">extract</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{}</span>\n <span class=\"nt\">setup_hooks</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">[]</span>\n <span class=\"nt\">teardown_hooks</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">[]</span>\n <span class=\"nt\">validations</span><span class=\"p\">:</span>\n <span class=\"p p-Indicator\">-</span> <span class=\"nt\">eq</span><span class=\"p\">:</span>\n     <span class=\"nt\">status.code</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">200</span>\n <span class=\"p p-Indicator\">-</span> <span class=\"nt\">exists</span><span class=\"p\">:</span>\n     <span class=\"l l-Scalar l-Scalar-Plain\">headers.token</span>\n <span class=\"p p-Indicator\">-</span> <span class=\"nt\">is_json</span><span class=\"p\">:</span>\n     <span class=\"l l-Scalar l-Scalar-Plain\">content</span>\n <span class=\"p p-Indicator\">-</span> <span class=\"nt\">eq</span><span class=\"p\">:</span>\n     <span class=\"nt\">content.code</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">100</span>\n</pre>\n</li>\n<li>\n<p><strong>test_case</strong></p>\n<pre> <span class=\"nt\">config</span><span class=\"p\">:</span>\n   <span class=\"nt\">environment</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;environment flag&gt;</span>\n   <span class=\"nt\">import</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;environment file&gt;</span>\n   <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">case name</span>\n   <span class=\"nt\">variables</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{}</span>\n <span class=\"nt\">setup_hooks</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">[]</span>\n <span class=\"nt\">teardown_hooks</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">[]</span>\n <span class=\"nt\">test_steps</span><span class=\"p\">:</span>\n   <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;fullname of step1&gt;</span>\n   <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;fullname of step2&gt;</span>\n</pre>\n</li>\n<li>\n<p><strong>test_suite</strong></p>\n<pre> <span class=\"nt\">config</span><span class=\"p\">:</span>\n   <span class=\"nt\">environment</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;evnironment flag&gt;</span>\n   <span class=\"nt\">import</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;environment file&gt;</span>\n   <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">suite name</span>\n   <span class=\"nt\">variables</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">{}</span>\n <span class=\"nt\">setup_hooks</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">[]</span>\n <span class=\"nt\">teardown_hooks</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">[]</span>\n <span class=\"nt\">test_cases</span><span class=\"p\">:</span> \n   <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;fullname of case1&gt;</span>\n   <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;fullname of case2&gt;</span>\n</pre>\n</li>\n</ul>\n<hr>\n<h3>4.2 Verification method set</h3>\n<p>These methods can be used in validations in specific test_steps, for example:</p>\n<pre><span class=\"nt\">validations</span><span class=\"p\">:</span>\n<span class=\"p p-Indicator\">-</span> <span class=\"nt\">eq</span><span class=\"p\">:</span>\n    <span class=\"nt\">status.code</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">200</span>\n<span class=\"p p-Indicator\">-</span> <span class=\"nt\">is_json</span><span class=\"p\">:</span>\n    <span class=\"l l-Scalar l-Scalar-Plain\">content</span>\n<span class=\"p p-Indicator\">-</span> <span class=\"nt\">not_null</span><span class=\"p\">:</span>\n    <span class=\"l l-Scalar l-Scalar-Plain\">headers.token</span>\n<span class=\"p p-Indicator\">-</span> <span class=\"nt\">contains</span><span class=\"p\">:</span>\n    <span class=\"nt\">content.message</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">succ</span>\n</pre>\n<p><strong>Commonly used verification methods:</strong></p>\n<ul>\n<li><strong>eq(equals):</strong>\n<ul>\n<li>\n<p>Example: <code>1 eq 1</code>, <code>'a' eq 'a'</code>, <code>[1, 2] eq [1, 2]</code>, <code>{'a': 1 } eq {'a': 1}</code>, <code>status.code eq 200</code></p>\n</li>\n<li>\n<p>Usage:</p>\n<pre><code> validations:\n - eq:\n     status.code: 200\n - eq:\n     headers.Content-Type: application/json;charset=UTF-8\n - eq:\n     content.data[0].id: 1000\n</code></pre>\n</li>\n<li>\n<p>Similar methods: <code>neq</code>, <code>lt</code>, <code>gt</code>, <code>le</code>, <code>ge</code></p>\n</li>\n</ul>\n</li>\n<li><strong>len_eq(length equals):</strong>\n<ul>\n<li>\n<p>Example: <code>'ab' len_eq 2</code>, <code>[1, 2] len_eq 2</code>, <code>{'a': 1} len_eq 1</code></p>\n</li>\n<li>\n<p>Usage:</p>\n<pre><code> validations:\n - len_eq:\n     headers.token: 32\n - eq:\n     content.datalist: 3\n</code></pre>\n</li>\n<li>\n<p>Similar methods: <code>len_neq</code>, <code>len_lt</code>, <code>len_gt</code></p>\n</li>\n</ul>\n</li>\n<li><strong>time_le(time spent less than or equals):</strong>\n<ul>\n<li>\n<p>Example: <code>request 'time.spent' time_le 200 (unit is ms)</code></p>\n</li>\n<li>\n<p>Usage:</p>\n<pre><code> validations:\n - time_le:\n     time.spent: 200\n</code></pre>\n</li>\n<li>\n<p>Similar methods: <code>time_lt</code>, <code>time_gt</code>, <code>time_ge</code></p>\n</li>\n</ul>\n</li>\n<li><strong>contains:</strong>\n<ul>\n<li>\n<p>Example: <code>'abc' contain 'ab', ['a', 'b'] contain 'a', {'a': 1, 'b': 2} contain {'a': 1}</code></p>\n</li>\n<li>\n<p>Usage:</p>\n<pre><code> validations:\n - contains:\n     headers.Content-Type: application/json\n - contains:\n     content.message: ok\n</code></pre>\n</li>\n<li>\n<p>Similar methods: <code>not_contains</code></p>\n</li>\n</ul>\n</li>\n<li><strong>in:</strong>\n<ul>\n<li>\n<p>Example: <code>'a' in 'ab'</code>, <code>'a' in ['a', 'b']</code>, <code>'a' in {'a': 1, 'b': 2}</code></p>\n</li>\n<li>\n<p>Usage:</p>\n<pre><code> validations:\n - in:\n     status.code: [200, 302]\n</code></pre>\n</li>\n<li>\n<p>Similar methods: <code>not_in</code></p>\n</li>\n</ul>\n</li>\n<li><strong>is_false:</strong>\n<ul>\n<li>\n<p>Example: <code>0 is_false</code>, <code>'' is_false</code>, <code>[] is_false</code>, <code>{} is_false</code></p>\n</li>\n<li>\n<p>Usage:</p>\n<pre><code> validations:\n - is_false:\n     content.datalist\n - is_json:\n     content\n - is_instance:\n     status.code: int\n</code></pre>\n</li>\n<li>\n<p>Similar methods: <code>is_true</code>, <code>exists</code>, <code>is_instance</code>, <code>is_json</code></p>\n</li>\n</ul>\n</li>\n<li><strong>re(regex):</strong>\n<ul>\n<li>\n<p>Example: <code>'1900-01-01' re r'\\d+-\\d+-\\d+'</code></p>\n</li>\n<li>\n<p>Usage:</p>\n<pre><code> validations:\n - re:\n     content.data[0].date: r\"\\d+-\\d+-\\d+\"\n</code></pre>\n</li>\n<li>\n<p>Similar methods: <code>not_re</code></p>\n</li>\n</ul>\n</li>\n</ul>\n<p>More methods and instructions can be found in the following way:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"kn\">from</span> <span class=\"nn\">iparrot.modules.validator</span> <span class=\"kn\">import</span> <span class=\"n\">Validator</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">(</span><span class=\"n\">Validator</span><span class=\"o\">.</span><span class=\"n\">UNIFORM_COMPARATOR</span><span class=\"p\">,</span> <span class=\"n\">indent</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">))</span>\n</pre>\n<hr>\n<h3>4.3 Helper method set</h3>\n<p>These methods can be applied to the inside of test_step / test_case / test_suite in the form <code>${{function(params)}}</code>, such as: <code>setup_hooks</code> <code>teardown_hooks</code> <code>variables</code></p>\n<pre><code>today(form='%Y-%m-%d'): Get today's date\n\ndays_ago(days=0, form='%Y-%m-%d'): Get the date a few days ago\n\ndays_later(days=0, form='%Y-%m-%d'): Get the date a few days later\n\nnow(form='%Y-%m-%d %H:%M:%S'): Get the current time, accurate to the second\n\nnow_ms(form='%Y-%m-%d %H:%M:%S'): Get the current time, accurate to the micro second\n\nnow_timestamp(): Get the current timestamp, accurate to the second\n\nnow_timestamp_ms(): Get the current timestamp, accurate to the micro second\n\nhours_ago(hours=0, form='%Y-%m-%d %H:%M:%S'): Get the time a few hours ago, accurate to the second\n\nhours_later(hours=0, form='%Y-%m-%d %H:%M:%S'): Get the time a few hours later, accurate to the second\n\nget_file_name(file, ext=0): Intercept the file name of the specified file, no suffix by default\n\nget_file_path(file): Intercept the path of the specified file\n\nmake_dir(directory): Generate the specified directory\n\ncopy_file(source, target): Copy the specified file to the target path\n\nget_random_integer(length=10, head=None, tail=None): Generate random number, with specified length, head, tail\n\nget_random_string(length=10, simple=1, head=None, tail=None): Generate random string, with specified length, head, tail\n\nget_random_phone(head=None, tail=None): Generate random Chinese phone number, with specified length, head, tail\n</code></pre>\n<blockquote>\n<p>If the above helper methods do not meet your needs, you can use the following method:</p>\n<p>Define your own module or pip install specified module in your local environment.</p>\n<p>Add <code>import xxx</code> or <code>from xxx import yyy</code> to the desired step / case / suite <code>setup_hooks</code>,</p>\n<p>then you could use the <code>${{function(params)}}</code> format to call the method you want.</p>\n<p>Any questions, you can feedback an issue: <a href=\"https://github.com/idle-man/iParrot/issues\" rel=\"nofollow\">https://github.com/idle-man/iParrot/issues</a></p>\n<p>More general helper methods, welcome to contribute code or issues, thanks.</p>\n</blockquote>\n<h2>5. External reference, thanks</h2>\n<h3>5.1 <a href=\"https://learning.getpostman.com/\" rel=\"nofollow\">Postman</a></h3>\n<h4>5.1.1 Environments management</h4>\n<p>The mechanism is referenced in the <code>environment</code> of the Parrot use case structure.</p>\n<pre><code>A project can be configured with multiple sets of environments to hold some common environment variables.\n\nVariable names are consistent between different environments, and values \u200b\u200bcan vary.\n\nIn the use case, you can refer to the variable by means of ${variable}, reducing manual modification.\n\nThe switching of the operating environment can be specified in the playback phase by the -env/--environment parameter.\n</code></pre>\n<h4>5.1.2 Use case layering mode</h4>\n<ul>\n<li>Collection =&gt; test_suite</li>\n<li>Folder =&gt; test_case</li>\n<li>Request =&gt; test_step</li>\n</ul>\n<h4>5.1.3 Pre and post actions</h4>\n<ul>\n<li>Pre-request Script =&gt; setup_hooks</li>\n<li>Tests =&gt; teardown_hooks &amp; validations</li>\n</ul>\n<h3>5.2 <a href=\"https://github.com/httprunner/httprunner\" rel=\"nofollow\">HttpRunner</a></h3>\n<h4>5.2.1 <a href=\"https://github.com/HttpRunner/har2case\" rel=\"nofollow\">HAR2Case</a></h4>\n<p>The files processed by Parrot in the first phase are Charles trace and Fiddler txt. The format is quite different, and the parsing of plain text is cumbersome.</p>\n<p>Later, in the course of HttpRunner's ideas, I used HAR to reconstruct the record part. At the same time, I made some changes in the parameters.</p>\n<p>Inspired by HttpRunner's ideas, the record part is rebuilt, and some paramters are updated.</p>\n<p>For details, to see <code>parrot help record</code> and <code>iparrot.parser</code></p>\n<h4>5.2.2 Use case layering mode</h4>\n<p>The use case layering mode of HttpRunner, TestSuite&gt;TestCase&gt;TestStep, is clear and a good reference.</p>\n<p>When Parrot automatically generates use cases, it directly implements the layering mode on the directory structure and changes the specific use case structure.</p>\n<h4>5.2.3 setup hooks &amp; teardown hooks</h4>\n<p>Parrot reuses this naming scheme, which supports <code>set variable</code>, <code>call function</code>, <code>exec code</code>.</p>\n<h4>5.2.4 extract variable</h4>\n<p>Parrot in the first phase uses the mode of <code>store</code> and <code>replace</code>, which is intended to keep all changes in a configuration file, and does not invade the use case at all.</p>\n<p>In actual use, it is found that the usability is not good and the configuration is slightly cumbersome.</p>\n<p>Refer to HttpRunner, return the initiative to the user, and the variable can be extracted according to <code>extract</code> defination and used as <code>${variable}</code>.</p>\n<h4>5.2.5 comparator</h4>\n<p>The first version of Parrot diffs results refer to a  configuration file, only supports <code>eq</code> and simple <code>re</code>, and the method set is limited.</p>\n<p>Now refer to the HttpRunner, automatically generate <code>eq</code> comparator when recording, and support a variety of comparator customization.</p>\n<p>Comparators in Parrot combines with the common verification methods of HttpRunner and Postman, and a certain supplement.</p>\n<h4>5.2.6 report</h4>\n<p>Parrot's test report template directly reuses HttpRunner's report style.</p>\n\n          </div>"}, "last_serial": 6154471, "releases": {"1.0.1": [{"comment_text": "", "digests": {"md5": "56382ae0240c3b8203b72892fa8714f5", "sha256": "a98e6746d5f2ccb18593cbd6b0bb7fff006d4111b6227734ce7be4ec216cf917"}, "downloads": -1, "filename": "iParrot-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "56382ae0240c3b8203b72892fa8714f5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 34096, "upload_time": "2019-09-27T07:27:41", "upload_time_iso_8601": "2019-09-27T07:27:41.945157Z", "url": "https://files.pythonhosted.org/packages/54/7a/576d6bf4cb07331982861f8b599f6ec0fc0f51bf1e4654555d17b4c242dc/iParrot-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c9f8f040aa7be41b2724116cb2c262e9", "sha256": "927d036bced8a549b6bd782cf0a64ad4f2b29b567e46e379e9ba2dfd63e52581"}, "downloads": -1, "filename": "iParrot-1.0.1.tar.gz", "has_sig": false, "md5_digest": "c9f8f040aa7be41b2724116cb2c262e9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32992, "upload_time": "2019-09-27T07:27:44", "upload_time_iso_8601": "2019-09-27T07:27:44.005916Z", "url": "https://files.pythonhosted.org/packages/66/26/c31d9e71141fe9db8cba5a5e534547d994b7a20809a975283e54d773d59e/iParrot-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "df10268736fc0551d2a33e044a79a841", "sha256": "3fe1835eb667c9ef25258724f6cd1068685a4cf53d5fc9ea893a8f76ee3ef968"}, "downloads": -1, "filename": "iParrot-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "df10268736fc0551d2a33e044a79a841", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 34129, "upload_time": "2019-09-29T03:32:00", "upload_time_iso_8601": "2019-09-29T03:32:00.048083Z", "url": "https://files.pythonhosted.org/packages/e5/9b/d4697d8712749d45c4d23d4132d990cc5aeffe2e73fc9e7e45359da33d47/iParrot-1.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3db41069979cef4087583fbbc7c1a1b6", "sha256": "c8f344219b6fd78f1fee284f17abf09570feb97d85c9350b7c8539fc663ea51a"}, "downloads": -1, "filename": "iParrot-1.0.2.tar.gz", "has_sig": false, "md5_digest": "3db41069979cef4087583fbbc7c1a1b6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32954, "upload_time": "2019-09-29T03:32:02", "upload_time_iso_8601": "2019-09-29T03:32:02.346790Z", "url": "https://files.pythonhosted.org/packages/ac/ad/9aa9554100feccbd3c05b62365af4188fb7ff2a1fa6d054af84d8c94e823/iParrot-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "80e0a44094d4455c859630dd401f9fdc", "sha256": "3540b1045c84b8eb78884f8b4ff45da2f9dd57bfb8e0f1c740cda7459a1e2498"}, "downloads": -1, "filename": "iParrot-1.0.3-py3.7.egg", "has_sig": false, "md5_digest": "80e0a44094d4455c859630dd401f9fdc", "packagetype": "bdist_egg", "python_version": "3.7", "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "size": 69791, "upload_time": "2019-10-24T03:42:48", "upload_time_iso_8601": "2019-10-24T03:42:48.517656Z", "url": "https://files.pythonhosted.org/packages/93/fb/64d890aad331e98f8010b33c322398f343ab5330a613d11e5d106cbbc03a/iParrot-1.0.3-py3.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "91be8c4833947f67c7042dac4c0afefd", "sha256": "c0b72c13f72d25827a88fb05e449fd4294a7fe67b397f5e9f5405e95d0beeebe"}, "downloads": -1, "filename": "iParrot-1.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "91be8c4833947f67c7042dac4c0afefd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "size": 37437, "upload_time": "2019-10-24T03:42:46", "upload_time_iso_8601": "2019-10-24T03:42:46.038784Z", "url": "https://files.pythonhosted.org/packages/80/d6/6d793ac8584780a8ba7f48e94a58496fc0af7bf5d4b14f5a55b4ce0bb2f8/iParrot-1.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "217a250632cf7909d8aa545106d4069a", "sha256": "855158104b543a8d71897f8fc13a3645345a5171f2ed2621eded1f1149d39626"}, "downloads": -1, "filename": "iParrot-1.0.3.tar.gz", "has_sig": false, "md5_digest": "217a250632cf7909d8aa545106d4069a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "size": 60461, "upload_time": "2019-10-24T03:42:50", "upload_time_iso_8601": "2019-10-24T03:42:50.954637Z", "url": "https://files.pythonhosted.org/packages/c6/cd/0343d764a49be0b3d657ef7cf83711ced9ea3cecb1c34d2b2197eda21a62/iParrot-1.0.3.tar.gz", "yanked": false}], "1.0.4": [{"comment_text": "", "digests": {"md5": "899c9e2d96a763d3723aa2f890549b5e", "sha256": "12632fbdae2a1936a70cba1f4d3eceafcd654b0aca7a4ff5135250eaea0c9b26"}, "downloads": -1, "filename": "iParrot-1.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "899c9e2d96a763d3723aa2f890549b5e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "size": 38268, "upload_time": "2019-10-25T07:15:00", "upload_time_iso_8601": "2019-10-25T07:15:00.879610Z", "url": "https://files.pythonhosted.org/packages/67/13/725d14d62b61af12e600285443e3c5b23d358ecd56b9335eb44f0a0b5c33/iParrot-1.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cbb597d55f7a5bde68ef1da95ddfa065", "sha256": "a3dbc225905d67d0d4edb79b8fe6f1f5075c16ceb0fffd338693aeeb3f0cf079"}, "downloads": -1, "filename": "iParrot-1.0.4.tar.gz", "has_sig": false, "md5_digest": "cbb597d55f7a5bde68ef1da95ddfa065", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "size": 61364, "upload_time": "2019-10-25T07:15:03", "upload_time_iso_8601": "2019-10-25T07:15:03.505774Z", "url": "https://files.pythonhosted.org/packages/6a/5f/e0c9d11b77100e41ab0fb49992d9f78f441290dc120f81afd0a670b1547f/iParrot-1.0.4.tar.gz", "yanked": false}], "1.0.5": [{"comment_text": "", "digests": {"md5": "bbe038b08396758a17c37faed6ddc80a", "sha256": "81d7498ff485b31721e516d1b5deb7cb5ac45f7c835e3501fe3b79c55c5c27ef"}, "downloads": -1, "filename": "iParrot-1.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "bbe038b08396758a17c37faed6ddc80a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "size": 39461, "upload_time": "2019-10-30T09:21:18", "upload_time_iso_8601": "2019-10-30T09:21:18.543234Z", "url": "https://files.pythonhosted.org/packages/9c/b6/b5eaa6774d2e246409592bff21dda9bf187a5837baa350b0219cf0e58bec/iParrot-1.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ef9d20237c98db3842f7c165d06cef81", "sha256": "fe4f46c28f3430dba2484ad0f1fe992e44d0606b98ca1a96b69601f9e8b27f5d"}, "downloads": -1, "filename": "iParrot-1.0.5.tar.gz", "has_sig": false, "md5_digest": "ef9d20237c98db3842f7c165d06cef81", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "size": 67356, "upload_time": "2019-10-30T09:21:21", "upload_time_iso_8601": "2019-10-30T09:21:21.999583Z", "url": "https://files.pythonhosted.org/packages/dc/52/df3fe6203e79d9d639a9a1e5921b3651f1b5a489e436fd5863bbb19745f8/iParrot-1.0.5.tar.gz", "yanked": false}], "1.0.6": [{"comment_text": "", "digests": {"md5": "c4425ad52c8560c7db5eea2b253d64e0", "sha256": "c87183848d68aab6c0500dd17a0d26dfc2656a82e395649fd2bb11945e8de755"}, "downloads": -1, "filename": "iParrot-1.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "c4425ad52c8560c7db5eea2b253d64e0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "size": 39808, "upload_time": "2019-11-08T07:28:10", "upload_time_iso_8601": "2019-11-08T07:28:10.018332Z", "url": "https://files.pythonhosted.org/packages/e3/fb/78021a6d5e84742a036d56ba3237c534663aec8e67c90c52ad2a43ad6c66/iParrot-1.0.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5d080f63e43f20fbd3042c05f203ee50", "sha256": "393b2993a5d6c2989ca5747e5e59ccf247bfec433750aec0cad3c13892b00bbd"}, "downloads": -1, "filename": "iParrot-1.0.6.tar.gz", "has_sig": false, "md5_digest": "5d080f63e43f20fbd3042c05f203ee50", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "size": 69103, "upload_time": "2019-11-08T07:28:12", "upload_time_iso_8601": "2019-11-08T07:28:12.480029Z", "url": "https://files.pythonhosted.org/packages/7b/5c/604cf26e508404e17870339653236418b568a9b17937a34b80c191d2d9e6/iParrot-1.0.6.tar.gz", "yanked": false}], "1.1.1": [{"comment_text": "", "digests": {"md5": "8e081ad921dbeb1413f9ffadb33cb086", "sha256": "dc97a148cf0552a9e5e4741df357e58fd695c3fd81870395b37162f534cc627e"}, "downloads": -1, "filename": "iParrot-1.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "8e081ad921dbeb1413f9ffadb33cb086", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "size": 43977, "upload_time": "2019-11-18T08:45:12", "upload_time_iso_8601": "2019-11-18T08:45:12.053343Z", "url": "https://files.pythonhosted.org/packages/dd/b2/552c8f8e5468785d6ea2dbb653508c57cac8b69e8032d617162348b1bc30/iParrot-1.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "56fb7765da0ea7d21f205e97bc491296", "sha256": "e435157b48c184b9082ebb40d070924f2a48302524d9d813989b2033aa95a4eb"}, "downloads": -1, "filename": "iParrot-1.1.1.tar.gz", "has_sig": false, "md5_digest": "56fb7765da0ea7d21f205e97bc491296", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "size": 79273, "upload_time": "2019-11-18T08:45:16", "upload_time_iso_8601": "2019-11-18T08:45:16.131216Z", "url": "https://files.pythonhosted.org/packages/b6/46/318048ac2763d583e1d5abcec1014f6dc6a6cdf7abd86347ac51b8fc544e/iParrot-1.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "8e081ad921dbeb1413f9ffadb33cb086", "sha256": "dc97a148cf0552a9e5e4741df357e58fd695c3fd81870395b37162f534cc627e"}, "downloads": -1, "filename": "iParrot-1.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "8e081ad921dbeb1413f9ffadb33cb086", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "size": 43977, "upload_time": "2019-11-18T08:45:12", "upload_time_iso_8601": "2019-11-18T08:45:12.053343Z", "url": "https://files.pythonhosted.org/packages/dd/b2/552c8f8e5468785d6ea2dbb653508c57cac8b69e8032d617162348b1bc30/iParrot-1.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "56fb7765da0ea7d21f205e97bc491296", "sha256": "e435157b48c184b9082ebb40d070924f2a48302524d9d813989b2033aa95a4eb"}, "downloads": -1, "filename": "iParrot-1.1.1.tar.gz", "has_sig": false, "md5_digest": "56fb7765da0ea7d21f205e97bc491296", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4", "size": 79273, "upload_time": "2019-11-18T08:45:16", "upload_time_iso_8601": "2019-11-18T08:45:16.131216Z", "url": "https://files.pythonhosted.org/packages/b6/46/318048ac2763d583e1d5abcec1014f6dc6a6cdf7abd86347ac51b8fc544e/iParrot-1.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:54:43 2020"}