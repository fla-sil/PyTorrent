{"info": {"author": "Peter Melchior", "author_email": "peter.m.melchior@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python", "Topic :: Scientific/Engineering :: Information Analysis"], "description": "[![PyPI](https://img.shields.io/pypi/v/pygmmis.svg)](https://pypi.python.org/pypi/pygmmis/)\n[![License](https://img.shields.io/github/license/pmelchior/pygmmis.svg)](https://github.com/pmelchior/pygmmis/blob/master/LICENSE.md)\n[![DOI](https://img.shields.io/badge/DOI-10.1016%2Fj.ascom.2018.09.013-blue.svg)](https://doi.org/10.1016/j.ascom.2018.09.013)\n[![arXiv](https://img.shields.io/badge/arxiv-1611.05806-red.svg)](http://arxiv.org/abs/1611.05806)\n\n# pyGMMis\n\nNeed a simple and powerful Gaussian-mixture code in pure python? It can be as easy as this:\n\n```python\nimport pygmmis\ngmm = pygmmis.GMM(K=K, D=D)      # K components, D dimensions\nlogL, U = pygmmis.fit(gmm, data) # logL = log-likelihood, U = association of data to components\n```\nHowever, **pyGMMis** has a few extra tricks up its sleeve.\n\n* It can account for independent multivariate normal measurement errors for each of the observed samples, and then recovers an estimate of the error-free distribution. This technique is known as \"Extreme Deconvolution\" by Bovy, Hogg & Roweis (2011).\n* It works with missing data (features) by setting the respective elements of the covariance matrix to a vary large value, thus effectively setting the weights of the missing feature to 0.\n* It can deal with gaps (aka \"truncated data\") and variable sample completeness as long as\n  * you know the incompleteness over the entire feature space,\n  * and the incompleteness does not depend on the sample density (missing at random).\n* It can incorporate a \"background\" distribution (implemented is a uniform one) and separate signal from background, with the former being fit by the GMM.\n* It keeps track of which components need to be evaluated in which regions of the feature space, thereby substantially increasing the performance for fragmented data.\n\nIf you want more context and details on those capabilities, have a look at this [blog post](http://pmelchior.net/blog/gaussian-mixture-models-for-astronomy.html).\n\nUnder the hood, **pyGMMis** uses the Expectation-Maximization procedure. When dealing with sample incompleteness it generates its best guess of the unobserved samples on the fly given the current model fit to the observed samples.\n\n![Example of pyGMMis](https://raw.githubusercontent.com/pmelchior/pygmmis/master/tests/pygmmis.png)\n\nIn the example above, the true distribution is shown as contours in the left panel. We then draw 400 samples from it (red), add Gaussian noise to them (1,2,3 sigma contours shown in blue), and select only samples within the box but outside of the circle (blue).\n\nThe code is written in pure python (developed and tested in 2.7), parallelized with `multiprocessing`, and is capable of performing density estimation with millions of samples and thousands of model components on machines with sufficient memory.\n\nMore details are in the paper listed below. Please cite it if you make use of this code:\n\n```\n@ARTICLE{pygmmis,\n   author = {{Melchior}, P. and {Goulding}, A.~D.},\n    title = \"{Filling the gaps: Gaussian mixture models from noisy, truncated or incomplete samples}\",\n  journal = {Astronomy and Computing},\n   volume = \"25\",\n    pages = {183 - 194},\n     year = \"2018\",\n    month = oct,\n      doi = {10.1016/j.ascom.2018.09.013},\n      url = {https://www.sciencedirect.com/science/article/pii/S2213133718300489},\narchivePrefix = \"arXiv\",\n   eprint = {1611.05806},\n primaryClass = \"astro-ph.IM\"\n}\n```\n\n\n\n## Installation and Prerequisites\n\nYou can either clone the repo and install by `python setup.py install` or get the latest release with\n\n```\npip install pygmmis\n```\n\nDependencies:\n\n* numpy\n* scipy\n* multiprocessing\n* parmap\n\n## How to run the code\n\n1. Create a GMM object with the desired component number K and data dimensionality D:\n   ```gmm = pygmmis.GMM(K=K, D=D) ```\n\n3. Define a callback for the completeness function, which is called with e.g. `data` with shape (N,D) and returns an boolean array of size N whether the sample was observed or not. Two examples:\n\n   ```python\n   def cutAtSix(coords):\n   \t\"\"\"Selects all samples whose first coordinate is < 6\"\"\"\n       return (coords[:,0] < 6)\n\n   def selSlope(coords, rng=np.random):\n       \"\"\"Selects probabilistically according to first coordinate x:\n       Omega = 1    for x < 0\n             = 1-x  for x = 0 .. 1\n             = 0    for x > 1\n       \"\"\"\n       return rng.rand(len(coords)) > coords[:,0]\n   ```\n\n4. If there is noise (aka positional uncertainties) on the samples, you need to provide two things:\n\n   * The covariance of each data sample, or one for all. If it's the former, make a shared structure using `pygmmis.createShared`.\n   * Provide a callback function that returns an estimate of the covariance at arbitrary locations.\n\n   ```python\n   from functools import partial\n\n   # simply using the same covariance for all samples\n   dispersion = 1\n   default_covar = np.eye(D) * dispersion**2\n   covar_cb = partial(pygmmis.covar_callback_default, default=default_covar)\n\n   # more sophisticated option: use the covariance of the nearest neighbor.\n   def covar_tree_cb(coords, tree, covar):\n       \"\"\"Return the covariance of the nearest neighbor of coords in data.\"\"\"\n       dist, ind = tree.query(coords, k=1)\n       return covar[ind.flatten()]\n\n   from sklearn.neighbors import KDTree\n   tree = KDTree(data, leaf_size=100)\n   covar = pygmmis.createShared(covar)\n   covar_cb = partial(covar_tree_cb, tree=tree, covar=covar)\n   ```\n\n4. If there is a uniform background that is unrelated to the features you want to fit, you need to define it. Caveat: Because a uniform distribution is normalizable only if its support is finite, you need to decide on the footprint over which the background model is present, e.g.:\n\n   ```python\n   footprint = data.min(axis=0), data.max(axis=0)\n   amp = 0.3\n   bg = pygmmis.Background(footprint, amp=amp)\n   # fine tuning, if desired\n   bg.amp_min = 0.1\n   bg.amp_max = 0.5\n   bg.adjust_amp = False # freezes bg.amp at current value\n   ```\n\n5. Select an initialization method. This tells the GMM what initial parameters is should assume. The options are `'minmax','random','kmeans','none'`. See the respective functions for details:\n\n   * `pygmmis.initFromDataMinMax()`\n   * `pygmmis.initFromDataAtRandom()`\n   * `pygmmis.initFromKMeans()`\n\n   For difficult situations, or if you are not happy with the convergence, you may want to experiment with your own initialization. All you have to do is set `gmm.amp`, `gmm.mean`, and `gmm.covar` to desired values and use `init_method='none'`.\n\n6. Decide to freeze out any components. This makes sense if you *know* some of the properties of the components. You can freeze amplitude, mean, or covariance of any component by listing them in a dictionary, e.g:\n\n   ```python\n   frozen={\"amp\": [1,2], \"mean\": [], \"covar\": [1]}\n   ```\n\n   This freezes the amplitudes of component 1 and 2 (NOTE: Counting starts at 0), and the covariance of 1.\n\n7. Run the fitter:\n\n   ```python\n   w = 0.1    # minimum covariance regularization, same units as data\n   cutoff = 5 # segment the data set into neighborhood within 5 sigma around components\n   tol = 1e-3 # tolerance on logL to terminate EM\n\n   # define RNG for deterministic behavior\n   from numpy.random import RandomState\n   seed = 42\n   rng = RandomState(seed)\n\n   # run EM\n   logL, U = pygmmis.fit(gmm, data, init_method='random',\\\n                         sel_callback=cb, covar_callback=covar_cb, w=w, cutoff=cutoff,\\\n                         background=bg, tol=tol, frozen=frozen, rng=rng)\n   ```\n\n   This runs the EM procedure until tolerance is reached and returns the final mean log-likelihood of all samples, and the neighborhood of each component (indices of data samples that are within cutoff of a GMM component).\n\n8. Evaluate the model:\n\n   ```python\n   # log of p(x)\n   p = gmm(test_coords, as_log=False)\n   N_s = 1000\n   # draw samples from GMM\n   samples = gmm.draw(N_s)\n\n   # draw sample from the model with noise, background, and selection:\n   # if you want to get the missing sample, set invert_sel=True.\n   # N_orig is the estimated number of samples prior to selection\n   obs_size = len(data)\n   samples, covar_samples, N_orig = pygmmis.draw(gmm, obs_size, sel_callback=cb,\\\n                                                 invert_sel=False, orig_size=None,\\\n                                                 covar_callback=covar_cb,background=bg)\n   ```\n\n\n\nFor a complete example, have a look at [the test script](https://github.com/pmelchior/pygmmis/blob/master/tests/test.py). For requests and bug reports, please open an issue.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/pmelchior/pygmmis", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "pygmmis", "package_url": "https://pypi.org/project/pygmmis/", "platform": "", "project_url": "https://pypi.org/project/pygmmis/", "project_urls": {"Homepage": "https://github.com/pmelchior/pygmmis"}, "release_url": "https://pypi.org/project/pygmmis/1.1.3/", "requires_dist": ["numpy", "scipy", "multiprocessing", "parmap"], "requires_python": "", "summary": "Gaussian mixture model for incomplete, truncated, and noisy data", "version": "1.1.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://pypi.python.org/pypi/pygmmis/\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e564e2cfd922a23f603be727ae41ea6d0124245f/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7079676d6d69732e737667\"></a>\n<a href=\"https://github.com/pmelchior/pygmmis/blob/master/LICENSE.md\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4bed2a168b41378c884df14112406f0f60c9f1db/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f706d656c6368696f722f7079676d6d69732e737667\"></a>\n<a href=\"https://doi.org/10.1016/j.ascom.2018.09.013\" rel=\"nofollow\"><img alt=\"DOI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/37f3d2fad8a3cad75c531390e64420b139a2aea7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444f492d31302e313031362532466a2e6173636f6d2e323031382e30392e3031332d626c75652e737667\"></a>\n<a href=\"http://arxiv.org/abs/1611.05806\" rel=\"nofollow\"><img alt=\"arXiv\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a68dd1b63ad4b9a3c522bc341a70b213f3a0fdb7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61727869762d313631312e30353830362d7265642e737667\"></a></p>\n<h1>pyGMMis</h1>\n<p>Need a simple and powerful Gaussian-mixture code in pure python? It can be as easy as this:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">pygmmis</span>\n<span class=\"n\">gmm</span> <span class=\"o\">=</span> <span class=\"n\">pygmmis</span><span class=\"o\">.</span><span class=\"n\">GMM</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"o\">=</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"o\">=</span><span class=\"n\">D</span><span class=\"p\">)</span>      <span class=\"c1\"># K components, D dimensions</span>\n<span class=\"n\">logL</span><span class=\"p\">,</span> <span class=\"n\">U</span> <span class=\"o\">=</span> <span class=\"n\">pygmmis</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">gmm</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">)</span> <span class=\"c1\"># logL = log-likelihood, U = association of data to components</span>\n</pre>\n<p>However, <strong>pyGMMis</strong> has a few extra tricks up its sleeve.</p>\n<ul>\n<li>It can account for independent multivariate normal measurement errors for each of the observed samples, and then recovers an estimate of the error-free distribution. This technique is known as \"Extreme Deconvolution\" by Bovy, Hogg &amp; Roweis (2011).</li>\n<li>It works with missing data (features) by setting the respective elements of the covariance matrix to a vary large value, thus effectively setting the weights of the missing feature to 0.</li>\n<li>It can deal with gaps (aka \"truncated data\") and variable sample completeness as long as\n<ul>\n<li>you know the incompleteness over the entire feature space,</li>\n<li>and the incompleteness does not depend on the sample density (missing at random).</li>\n</ul>\n</li>\n<li>It can incorporate a \"background\" distribution (implemented is a uniform one) and separate signal from background, with the former being fit by the GMM.</li>\n<li>It keeps track of which components need to be evaluated in which regions of the feature space, thereby substantially increasing the performance for fragmented data.</li>\n</ul>\n<p>If you want more context and details on those capabilities, have a look at this <a href=\"http://pmelchior.net/blog/gaussian-mixture-models-for-astronomy.html\" rel=\"nofollow\">blog post</a>.</p>\n<p>Under the hood, <strong>pyGMMis</strong> uses the Expectation-Maximization procedure. When dealing with sample incompleteness it generates its best guess of the unobserved samples on the fly given the current model fit to the observed samples.</p>\n<p><img alt=\"Example of pyGMMis\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/95481e3014791cf9d810d9cea457cc597f51b821/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f706d656c6368696f722f7079676d6d69732f6d61737465722f74657374732f7079676d6d69732e706e67\"></p>\n<p>In the example above, the true distribution is shown as contours in the left panel. We then draw 400 samples from it (red), add Gaussian noise to them (1,2,3 sigma contours shown in blue), and select only samples within the box but outside of the circle (blue).</p>\n<p>The code is written in pure python (developed and tested in 2.7), parallelized with <code>multiprocessing</code>, and is capable of performing density estimation with millions of samples and thousands of model components on machines with sufficient memory.</p>\n<p>More details are in the paper listed below. Please cite it if you make use of this code:</p>\n<pre><code>@ARTICLE{pygmmis,\n   author = {{Melchior}, P. and {Goulding}, A.~D.},\n    title = \"{Filling the gaps: Gaussian mixture models from noisy, truncated or incomplete samples}\",\n  journal = {Astronomy and Computing},\n   volume = \"25\",\n    pages = {183 - 194},\n     year = \"2018\",\n    month = oct,\n      doi = {10.1016/j.ascom.2018.09.013},\n      url = {https://www.sciencedirect.com/science/article/pii/S2213133718300489},\narchivePrefix = \"arXiv\",\n   eprint = {1611.05806},\n primaryClass = \"astro-ph.IM\"\n}\n</code></pre>\n<h2>Installation and Prerequisites</h2>\n<p>You can either clone the repo and install by <code>python setup.py install</code> or get the latest release with</p>\n<pre><code>pip install pygmmis\n</code></pre>\n<p>Dependencies:</p>\n<ul>\n<li>numpy</li>\n<li>scipy</li>\n<li>multiprocessing</li>\n<li>parmap</li>\n</ul>\n<h2>How to run the code</h2>\n<ol>\n<li>\n<p>Create a GMM object with the desired component number K and data dimensionality D:\n<code>gmm = pygmmis.GMM(K=K, D=D)</code></p>\n</li>\n<li>\n<p>Define a callback for the completeness function, which is called with e.g. <code>data</code> with shape (N,D) and returns an boolean array of size N whether the sample was observed or not. Two examples:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">cutAtSix</span><span class=\"p\">(</span><span class=\"n\">coords</span><span class=\"p\">):</span>\n\t<span class=\"sd\">\"\"\"Selects all samples whose first coordinate is &lt; 6\"\"\"</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">coords</span><span class=\"p\">[:,</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">&lt;</span> <span class=\"mi\">6</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">selSlope</span><span class=\"p\">(</span><span class=\"n\">coords</span><span class=\"p\">,</span> <span class=\"n\">rng</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"p\">):</span>\n    <span class=\"sd\">\"\"\"Selects probabilistically according to first coordinate x:</span>\n<span class=\"sd\">    Omega = 1    for x &lt; 0</span>\n<span class=\"sd\">          = 1-x  for x = 0 .. 1</span>\n<span class=\"sd\">          = 0    for x &gt; 1</span>\n<span class=\"sd\">    \"\"\"</span>\n    <span class=\"k\">return</span> <span class=\"n\">rng</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">coords</span><span class=\"p\">))</span> <span class=\"o\">&gt;</span> <span class=\"n\">coords</span><span class=\"p\">[:,</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n</pre>\n</li>\n<li>\n<p>If there is noise (aka positional uncertainties) on the samples, you need to provide two things:</p>\n<ul>\n<li>The covariance of each data sample, or one for all. If it's the former, make a shared structure using <code>pygmmis.createShared</code>.</li>\n<li>Provide a callback function that returns an estimate of the covariance at arbitrary locations.</li>\n</ul>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">functools</span> <span class=\"kn\">import</span> <span class=\"n\">partial</span>\n\n<span class=\"c1\"># simply using the same covariance for all samples</span>\n<span class=\"n\">dispersion</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n<span class=\"n\">default_covar</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">eye</span><span class=\"p\">(</span><span class=\"n\">D</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">dispersion</span><span class=\"o\">**</span><span class=\"mi\">2</span>\n<span class=\"n\">covar_cb</span> <span class=\"o\">=</span> <span class=\"n\">partial</span><span class=\"p\">(</span><span class=\"n\">pygmmis</span><span class=\"o\">.</span><span class=\"n\">covar_callback_default</span><span class=\"p\">,</span> <span class=\"n\">default</span><span class=\"o\">=</span><span class=\"n\">default_covar</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># more sophisticated option: use the covariance of the nearest neighbor.</span>\n<span class=\"k\">def</span> <span class=\"nf\">covar_tree_cb</span><span class=\"p\">(</span><span class=\"n\">coords</span><span class=\"p\">,</span> <span class=\"n\">tree</span><span class=\"p\">,</span> <span class=\"n\">covar</span><span class=\"p\">):</span>\n    <span class=\"sd\">\"\"\"Return the covariance of the nearest neighbor of coords in data.\"\"\"</span>\n    <span class=\"n\">dist</span><span class=\"p\">,</span> <span class=\"n\">ind</span> <span class=\"o\">=</span> <span class=\"n\">tree</span><span class=\"o\">.</span><span class=\"n\">query</span><span class=\"p\">(</span><span class=\"n\">coords</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">covar</span><span class=\"p\">[</span><span class=\"n\">ind</span><span class=\"o\">.</span><span class=\"n\">flatten</span><span class=\"p\">()]</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.neighbors</span> <span class=\"kn\">import</span> <span class=\"n\">KDTree</span>\n<span class=\"n\">tree</span> <span class=\"o\">=</span> <span class=\"n\">KDTree</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">leaf_size</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"n\">covar</span> <span class=\"o\">=</span> <span class=\"n\">pygmmis</span><span class=\"o\">.</span><span class=\"n\">createShared</span><span class=\"p\">(</span><span class=\"n\">covar</span><span class=\"p\">)</span>\n<span class=\"n\">covar_cb</span> <span class=\"o\">=</span> <span class=\"n\">partial</span><span class=\"p\">(</span><span class=\"n\">covar_tree_cb</span><span class=\"p\">,</span> <span class=\"n\">tree</span><span class=\"o\">=</span><span class=\"n\">tree</span><span class=\"p\">,</span> <span class=\"n\">covar</span><span class=\"o\">=</span><span class=\"n\">covar</span><span class=\"p\">)</span>\n</pre>\n</li>\n<li>\n<p>If there is a uniform background that is unrelated to the features you want to fit, you need to define it. Caveat: Because a uniform distribution is normalizable only if its support is finite, you need to decide on the footprint over which the background model is present, e.g.:</p>\n<pre><span class=\"n\">footprint</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">min</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">amp</span> <span class=\"o\">=</span> <span class=\"mf\">0.3</span>\n<span class=\"n\">bg</span> <span class=\"o\">=</span> <span class=\"n\">pygmmis</span><span class=\"o\">.</span><span class=\"n\">Background</span><span class=\"p\">(</span><span class=\"n\">footprint</span><span class=\"p\">,</span> <span class=\"n\">amp</span><span class=\"o\">=</span><span class=\"n\">amp</span><span class=\"p\">)</span>\n<span class=\"c1\"># fine tuning, if desired</span>\n<span class=\"n\">bg</span><span class=\"o\">.</span><span class=\"n\">amp_min</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>\n<span class=\"n\">bg</span><span class=\"o\">.</span><span class=\"n\">amp_max</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>\n<span class=\"n\">bg</span><span class=\"o\">.</span><span class=\"n\">adjust_amp</span> <span class=\"o\">=</span> <span class=\"kc\">False</span> <span class=\"c1\"># freezes bg.amp at current value</span>\n</pre>\n</li>\n<li>\n<p>Select an initialization method. This tells the GMM what initial parameters is should assume. The options are <code>'minmax','random','kmeans','none'</code>. See the respective functions for details:</p>\n<ul>\n<li><code>pygmmis.initFromDataMinMax()</code></li>\n<li><code>pygmmis.initFromDataAtRandom()</code></li>\n<li><code>pygmmis.initFromKMeans()</code></li>\n</ul>\n<p>For difficult situations, or if you are not happy with the convergence, you may want to experiment with your own initialization. All you have to do is set <code>gmm.amp</code>, <code>gmm.mean</code>, and <code>gmm.covar</code> to desired values and use <code>init_method='none'</code>.</p>\n</li>\n<li>\n<p>Decide to freeze out any components. This makes sense if you <em>know</em> some of the properties of the components. You can freeze amplitude, mean, or covariance of any component by listing them in a dictionary, e.g:</p>\n<pre><span class=\"n\">frozen</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"amp\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"s2\">\"mean\"</span><span class=\"p\">:</span> <span class=\"p\">[],</span> <span class=\"s2\">\"covar\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]}</span>\n</pre>\n<p>This freezes the amplitudes of component 1 and 2 (NOTE: Counting starts at 0), and the covariance of 1.</p>\n</li>\n<li>\n<p>Run the fitter:</p>\n<pre><span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>    <span class=\"c1\"># minimum covariance regularization, same units as data</span>\n<span class=\"n\">cutoff</span> <span class=\"o\">=</span> <span class=\"mi\">5</span> <span class=\"c1\"># segment the data set into neighborhood within 5 sigma around components</span>\n<span class=\"n\">tol</span> <span class=\"o\">=</span> <span class=\"mf\">1e-3</span> <span class=\"c1\"># tolerance on logL to terminate EM</span>\n\n<span class=\"c1\"># define RNG for deterministic behavior</span>\n<span class=\"kn\">from</span> <span class=\"nn\">numpy.random</span> <span class=\"kn\">import</span> <span class=\"n\">RandomState</span>\n<span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">42</span>\n<span class=\"n\">rng</span> <span class=\"o\">=</span> <span class=\"n\">RandomState</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># run EM</span>\n<span class=\"n\">logL</span><span class=\"p\">,</span> <span class=\"n\">U</span> <span class=\"o\">=</span> <span class=\"n\">pygmmis</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">gmm</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">init_method</span><span class=\"o\">=</span><span class=\"s1\">'random'</span><span class=\"p\">,</span>\\\n                      <span class=\"n\">sel_callback</span><span class=\"o\">=</span><span class=\"n\">cb</span><span class=\"p\">,</span> <span class=\"n\">covar_callback</span><span class=\"o\">=</span><span class=\"n\">covar_cb</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">cutoff</span><span class=\"o\">=</span><span class=\"n\">cutoff</span><span class=\"p\">,</span>\\\n                      <span class=\"n\">background</span><span class=\"o\">=</span><span class=\"n\">bg</span><span class=\"p\">,</span> <span class=\"n\">tol</span><span class=\"o\">=</span><span class=\"n\">tol</span><span class=\"p\">,</span> <span class=\"n\">frozen</span><span class=\"o\">=</span><span class=\"n\">frozen</span><span class=\"p\">,</span> <span class=\"n\">rng</span><span class=\"o\">=</span><span class=\"n\">rng</span><span class=\"p\">)</span>\n</pre>\n<p>This runs the EM procedure until tolerance is reached and returns the final mean log-likelihood of all samples, and the neighborhood of each component (indices of data samples that are within cutoff of a GMM component).</p>\n</li>\n<li>\n<p>Evaluate the model:</p>\n<pre><span class=\"c1\"># log of p(x)</span>\n<span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">gmm</span><span class=\"p\">(</span><span class=\"n\">test_coords</span><span class=\"p\">,</span> <span class=\"n\">as_log</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">N_s</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span>\n<span class=\"c1\"># draw samples from GMM</span>\n<span class=\"n\">samples</span> <span class=\"o\">=</span> <span class=\"n\">gmm</span><span class=\"o\">.</span><span class=\"n\">draw</span><span class=\"p\">(</span><span class=\"n\">N_s</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># draw sample from the model with noise, background, and selection:</span>\n<span class=\"c1\"># if you want to get the missing sample, set invert_sel=True.</span>\n<span class=\"c1\"># N_orig is the estimated number of samples prior to selection</span>\n<span class=\"n\">obs_size</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"n\">samples</span><span class=\"p\">,</span> <span class=\"n\">covar_samples</span><span class=\"p\">,</span> <span class=\"n\">N_orig</span> <span class=\"o\">=</span> <span class=\"n\">pygmmis</span><span class=\"o\">.</span><span class=\"n\">draw</span><span class=\"p\">(</span><span class=\"n\">gmm</span><span class=\"p\">,</span> <span class=\"n\">obs_size</span><span class=\"p\">,</span> <span class=\"n\">sel_callback</span><span class=\"o\">=</span><span class=\"n\">cb</span><span class=\"p\">,</span>\\\n                                              <span class=\"n\">invert_sel</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">orig_size</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\\\n                                              <span class=\"n\">covar_callback</span><span class=\"o\">=</span><span class=\"n\">covar_cb</span><span class=\"p\">,</span><span class=\"n\">background</span><span class=\"o\">=</span><span class=\"n\">bg</span><span class=\"p\">)</span>\n</pre>\n</li>\n</ol>\n<p>For a complete example, have a look at <a href=\"https://github.com/pmelchior/pygmmis/blob/master/tests/test.py\" rel=\"nofollow\">the test script</a>. For requests and bug reports, please open an issue.</p>\n\n          </div>"}, "last_serial": 7159966, "releases": {"1.0.1": [{"comment_text": "", "digests": {"md5": "b1044fbf61678f0d42d5b1c968c21970", "sha256": "3e3c2cc0799053dc2c5cfcbf0e93cff5c819fbab94f7da9334bdc9a182bb515c"}, "downloads": -1, "filename": "pygmmis-1.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "b1044fbf61678f0d42d5b1c968c21970", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 18805, "upload_time": "2018-02-26T05:19:58", "upload_time_iso_8601": "2018-02-26T05:19:58.857885Z", "url": "https://files.pythonhosted.org/packages/d8/5f/c6fa1f8512b5608448bfc2b0afad732bcb07f78427490e23c36ecf7d2455/pygmmis-1.0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f8b11e9b490972ec0d0355ce20746e6f", "sha256": "8b7bd1884230e3897a336385378667df70a5aff79caa9514ca6d8800c806b8d3"}, "downloads": -1, "filename": "pygmmis-1.0.1.tar.gz", "has_sig": false, "md5_digest": "f8b11e9b490972ec0d0355ce20746e6f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20669, "upload_time": "2018-02-26T05:20:05", "upload_time_iso_8601": "2018-02-26T05:20:05.017393Z", "url": "https://files.pythonhosted.org/packages/fa/2d/a9761e0e93e9ac17bc932c0752aa64b93dc5908c3ae68ecce33cb38525ac/pygmmis-1.0.1.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "0066a468df381e56611966a515e7d4c6", "sha256": "790b19a16bf2589ff1c95e5586381ac7c3c40cde8c0edb50403a19aa04cea75c"}, "downloads": -1, "filename": "pygmmis-1.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "0066a468df381e56611966a515e7d4c6", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 18791, "upload_time": "2018-03-30T16:35:53", "upload_time_iso_8601": "2018-03-30T16:35:53.333720Z", "url": "https://files.pythonhosted.org/packages/74/3d/0a05605132197db76fbee0bfb44609e10fb493d8761f5336f2b9f2e80fbe/pygmmis-1.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0de8f00c76daebdf693a43f508c94952", "sha256": "2cb0cc288fddca5367a3febd0d3eefaf5b38f1b372065e7491782b70aafe5110"}, "downloads": -1, "filename": "pygmmis-1.1.0.tar.gz", "has_sig": false, "md5_digest": "0de8f00c76daebdf693a43f508c94952", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20966, "upload_time": "2018-03-30T16:35:55", "upload_time_iso_8601": "2018-03-30T16:35:55.024105Z", "url": "https://files.pythonhosted.org/packages/0b/40/9c63205a165e3eaaf31c8ee614492df22c3fdb9ffe3928ab5ac3e405d857/pygmmis-1.1.0.tar.gz", "yanked": false}], "1.1.1": [{"comment_text": "", "digests": {"md5": "bd61c91524b4f8342a2606deb2717bd5", "sha256": "f4acde28f195061cf34e6ac373d28c6ecb2db1fa29b371d3575432538a3ef008"}, "downloads": -1, "filename": "pygmmis-1.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "bd61c91524b4f8342a2606deb2717bd5", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 18715, "upload_time": "2020-02-17T19:41:58", "upload_time_iso_8601": "2020-02-17T19:41:58.786417Z", "url": "https://files.pythonhosted.org/packages/de/8e/4ae1277a9328acc57e1596d763c58d1b8f13d76cfd3b6b3c7497acf70596/pygmmis-1.1.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "936db6040505d97ce52a12c386fbcc24", "sha256": "75a0e9689a66dbd33a01e6edf0064b8867269ceb48b4a36d55f26e4b7a858f20"}, "downloads": -1, "filename": "pygmmis-1.1.1.tar.gz", "has_sig": false, "md5_digest": "936db6040505d97ce52a12c386fbcc24", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20962, "upload_time": "2020-02-17T19:42:00", "upload_time_iso_8601": "2020-02-17T19:42:00.211792Z", "url": "https://files.pythonhosted.org/packages/8b/93/9757aa4995fffcdd885ede001fd2a8a2bd6c4dff969de253db2475b9c1ab/pygmmis-1.1.1.tar.gz", "yanked": false}], "1.1.2": [{"comment_text": "", "digests": {"md5": "ad743f7053080d17ec7dfafd5270ad8a", "sha256": "017e8f8f6b9d4285cdd3eaf6851a1ba48f67e28a160b3bd77f7a25cca13212ae"}, "downloads": -1, "filename": "pygmmis-1.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "ad743f7053080d17ec7dfafd5270ad8a", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 22398, "upload_time": "2020-02-17T19:50:42", "upload_time_iso_8601": "2020-02-17T19:50:42.695279Z", "url": "https://files.pythonhosted.org/packages/21/83/0b8b441816870870d84a239dfff2666549c2d0248bcfce98f58d2cb0aa11/pygmmis-1.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "93ead8d9da08d1d6489933b731d58371", "sha256": "30be00fc7ffd5caf6bff855b8962b875d66375bbc0ea02e5c0c02e6b687dbe8b"}, "downloads": -1, "filename": "pygmmis-1.1.2.tar.gz", "has_sig": false, "md5_digest": "93ead8d9da08d1d6489933b731d58371", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21689, "upload_time": "2020-02-17T19:50:44", "upload_time_iso_8601": "2020-02-17T19:50:44.342860Z", "url": "https://files.pythonhosted.org/packages/67/46/38603d69a846787268c304590776daf9ff62b7fab843de996af4c30253f7/pygmmis-1.1.2.tar.gz", "yanked": false}], "1.1.3": [{"comment_text": "", "digests": {"md5": "91dd96bc3cee35cdd524145f811bb92e", "sha256": "9c42421e45863fdd8dd553398a44012cd55444c2dd70f6425a2ddbf1f74421b7"}, "downloads": -1, "filename": "pygmmis-1.1.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "91dd96bc3cee35cdd524145f811bb92e", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 22404, "upload_time": "2020-05-03T21:56:34", "upload_time_iso_8601": "2020-05-03T21:56:34.126310Z", "url": "https://files.pythonhosted.org/packages/0e/4c/b81f2f06961d5e645b9929073d56fed0dddcd769aa968c19e7ae9a6e926c/pygmmis-1.1.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0da0026f88d47a3a3fc412504f7185f0", "sha256": "7894855400d1a7fae36cf742ceaa46036aed525a5b7bbdc64fb9d1ec4ea872f7"}, "downloads": -1, "filename": "pygmmis-1.1.3.tar.gz", "has_sig": false, "md5_digest": "0da0026f88d47a3a3fc412504f7185f0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21710, "upload_time": "2020-05-03T21:56:35", "upload_time_iso_8601": "2020-05-03T21:56:35.491604Z", "url": "https://files.pythonhosted.org/packages/97/e7/24e1e67bd4b5f898f4c9899952a86e0d9d9ca92de7fe4bef058dff390dcd/pygmmis-1.1.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "91dd96bc3cee35cdd524145f811bb92e", "sha256": "9c42421e45863fdd8dd553398a44012cd55444c2dd70f6425a2ddbf1f74421b7"}, "downloads": -1, "filename": "pygmmis-1.1.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "91dd96bc3cee35cdd524145f811bb92e", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 22404, "upload_time": "2020-05-03T21:56:34", "upload_time_iso_8601": "2020-05-03T21:56:34.126310Z", "url": "https://files.pythonhosted.org/packages/0e/4c/b81f2f06961d5e645b9929073d56fed0dddcd769aa968c19e7ae9a6e926c/pygmmis-1.1.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0da0026f88d47a3a3fc412504f7185f0", "sha256": "7894855400d1a7fae36cf742ceaa46036aed525a5b7bbdc64fb9d1ec4ea872f7"}, "downloads": -1, "filename": "pygmmis-1.1.3.tar.gz", "has_sig": false, "md5_digest": "0da0026f88d47a3a3fc412504f7185f0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21710, "upload_time": "2020-05-03T21:56:35", "upload_time_iso_8601": "2020-05-03T21:56:35.491604Z", "url": "https://files.pythonhosted.org/packages/97/e7/24e1e67bd4b5f898f4c9899952a86e0d9d9ca92de7fe4bef058dff390dcd/pygmmis-1.1.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:04:51 2020"}