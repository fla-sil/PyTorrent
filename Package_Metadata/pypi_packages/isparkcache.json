{"info": {"author": "Bataev Evgeny", "author_email": "bataev.evgeny@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "Defines a **%%sparkcache** cell magic in the IPython notebook to cache\nDataFrame and outputs of long-lasting computations in a persistent\nParquet file in Hadoop. Useful when some computations in a notebook are\nlong and you want to easily save the results in a file.\n\n\nBased on `ipycache <https://github.com/rossant/ipycache>`__ module.\n\nInstallation\n------------\n\n-  ``pip install isparkcache``\n\nUsage\n-----\n\n-  In IPython/Jupyter:\n\n    .. code:: python\n\n        %load_ext isparkcache\n\n-  Then, create a cell with:\n\n    .. code:: python\n\n        %%sparkcache df1 df2\n\n        df = ...\n        df1 = sql.createDataFrame(df)\n        df2 = sql.createDataFrame(df)\n\n-  When you execute this cell the first time, the code is executed, and\n   the dataframes ``df1`` and ``df2`` are saved in\n   ``/user/$USER/sparkcache/mysparkapplication/df1`` and\n   ``/user/$USER/sparkcache/mysparkapplication/df2``. When you execute\n   this cell again, the code is skipped, the dataframes are loaded from\n   the Parquet and injected into the namespace, and the outputs are\n   restored in the notebook.\n\n-  Use the ``--force`` or ``-f`` option to force the cell's execution\n   and overwrite the file.\n\n-  Use the ``--read`` or ``-r`` option to prevent the cell's execution\n   and always load the variables from the cache. An exception is raised\n   if the file does not exist.\n\n-  Use the ``--cachedir`` or ``-d`` option to specify the cache\n   directory. Default directory: ``/user/$USER/sparkcache``. You can\n   specify a default directory in the IPython configuration file in your\n   profile (typically in\n   ``~\\.ipython\\profile_default\\ipython_config.py``) by adding the\n   following line:\n\n        c.SparkCacheMagics.cachedir = \"/path/to/mycache\"\n\nIf both a default cache directory and the ``--cachedir`` option are\ngiven, the latter is used.", "description_content_type": null, "docs_url": null, "download_url": "https://github.com/bataeves/isparkcache/archive/0.1.12.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/bataeves/isparkcache", "keywords": "spark,jupyter,ipython", "license": "", "maintainer": "", "maintainer_email": "", "name": "isparkcache", "package_url": "https://pypi.org/project/isparkcache/", "platform": "", "project_url": "https://pypi.org/project/isparkcache/", "project_urls": {"Download": "https://github.com/bataeves/isparkcache/archive/0.1.12.tar.gz", "Homepage": "https://github.com/bataeves/isparkcache"}, "release_url": "https://pypi.org/project/isparkcache/0.1.12/", "requires_dist": null, "requires_python": "", "summary": "Cache Spark Dataframes for Jupyter", "version": "0.1.12", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Defines a <strong>%%sparkcache</strong> cell magic in the IPython notebook to cache\nDataFrame and outputs of long-lasting computations in a persistent\nParquet file in Hadoop. Useful when some computations in a notebook are\nlong and you want to easily save the results in a file.</p>\n<p>Based on <a href=\"https://github.com/rossant/ipycache\" rel=\"nofollow\">ipycache</a> module.</p>\n<div id=\"installation\">\n<h2>Installation</h2>\n<ul>\n<li><tt>pip install isparkcache</tt></li>\n</ul>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<ul>\n<li><p>In IPython/Jupyter:</p>\n<blockquote>\n<pre><span class=\"o\">%</span><span class=\"n\">load_ext</span> <span class=\"n\">isparkcache</span>\n</pre>\n</blockquote>\n</li>\n<li><p>Then, create a cell with:</p>\n<blockquote>\n<pre><span class=\"o\">%%</span><span class=\"n\">sparkcache</span> <span class=\"n\">df1</span> <span class=\"n\">df2</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n<span class=\"n\">df1</span> <span class=\"o\">=</span> <span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n<span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n</pre>\n</blockquote>\n</li>\n<li><p>When you execute this cell the first time, the code is executed, and\nthe dataframes <tt>df1</tt> and <tt>df2</tt> are saved in\n<tt><span class=\"pre\">/user/$USER/sparkcache/mysparkapplication/df1</span></tt> and\n<tt><span class=\"pre\">/user/$USER/sparkcache/mysparkapplication/df2</span></tt>. When you execute\nthis cell again, the code is skipped, the dataframes are loaded from\nthe Parquet and injected into the namespace, and the outputs are\nrestored in the notebook.</p>\n</li>\n<li><p>Use the <tt><span class=\"pre\">--force</span></tt> or <tt><span class=\"pre\">-f</span></tt> option to force the cell\u2019s execution\nand overwrite the file.</p>\n</li>\n<li><p>Use the <tt><span class=\"pre\">--read</span></tt> or <tt><span class=\"pre\">-r</span></tt> option to prevent the cell\u2019s execution\nand always load the variables from the cache. An exception is raised\nif the file does not exist.</p>\n</li>\n<li><p>Use the <tt><span class=\"pre\">--cachedir</span></tt> or <tt><span class=\"pre\">-d</span></tt> option to specify the cache\ndirectory. Default directory: <tt><span class=\"pre\">/user/$USER/sparkcache</span></tt>. You can\nspecify a default directory in the IPython configuration file in your\nprofile (typically in\n<tt><span class=\"pre\">~\\.ipython\\profile_default\\ipython_config.py</span></tt>) by adding the\nfollowing line:</p>\n<blockquote>\n<p>c.SparkCacheMagics.cachedir = \u201c/path/to/mycache\u201d</p>\n</blockquote>\n</li>\n</ul>\n<p>If both a default cache directory and the <tt><span class=\"pre\">--cachedir</span></tt> option are\ngiven, the latter is used.</p>\n</div>\n\n          </div>"}, "last_serial": 3083478, "releases": {"0.1.10": [{"comment_text": "", "digests": {"md5": "0c4784f840890f795fdb51417f338674", "sha256": "1ec20779f2d43353c85647a8fe417b28e9e3ac67ed5ab824a3d6fd2e4dbbdc84"}, "downloads": -1, "filename": "isparkcache-0.1.10.tar.gz", "has_sig": false, "md5_digest": "0c4784f840890f795fdb51417f338674", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18196, "upload_time": "2017-07-20T10:25:33", "upload_time_iso_8601": "2017-07-20T10:25:33.422897Z", "url": "https://files.pythonhosted.org/packages/be/6c/e91e3aa335e0e61a4b157e2107d352b9c716dae97f0d89f0c10901051751/isparkcache-0.1.10.tar.gz", "yanked": false}], "0.1.11": [{"comment_text": "", "digests": {"md5": "abd9cef6b0636c534a3273f5b39e0d12", "sha256": "667a59a7d9557f248103b3bc0a75c5f392ddb5bf7bb13bc73fd8c3065f6b4fe4"}, "downloads": -1, "filename": "isparkcache-0.1.11.tar.gz", "has_sig": false, "md5_digest": "abd9cef6b0636c534a3273f5b39e0d12", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18195, "upload_time": "2017-07-20T10:29:11", "upload_time_iso_8601": "2017-07-20T10:29:11.384075Z", "url": "https://files.pythonhosted.org/packages/43/ae/3879d103b08798c74cbecd3fe5fbd6d5686a5254b49a835e8acb13907cef/isparkcache-0.1.11.tar.gz", "yanked": false}], "0.1.12": [{"comment_text": "", "digests": {"md5": "8e42fd0d8b7830f96803fe36e8436a36", "sha256": "4307391d28bf7dc573b70a6da76fd5b94a47437b374798d92d8c7b68d81a4050"}, "downloads": -1, "filename": "isparkcache-0.1.12.tar.gz", "has_sig": false, "md5_digest": "8e42fd0d8b7830f96803fe36e8436a36", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18317, "upload_time": "2017-08-09T08:30:29", "upload_time_iso_8601": "2017-08-09T08:30:29.809313Z", "url": "https://files.pythonhosted.org/packages/8e/c2/0ed8445c070964a9b80e1618f74345d1f8598ca485b8b2f4ca8b801663a1/isparkcache-0.1.12.tar.gz", "yanked": false}], "0.1.8": [{"comment_text": "", "digests": {"md5": "04b7b4c7ac743d1d77021f771e7186ca", "sha256": "df8de9e28bfdbc76606a7fb2f185fe61141563dee49f65b12f83788f4d15324d"}, "downloads": -1, "filename": "isparkcache-0.1.8.tar.gz", "has_sig": false, "md5_digest": "04b7b4c7ac743d1d77021f771e7186ca", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5590, "upload_time": "2017-07-20T10:16:39", "upload_time_iso_8601": "2017-07-20T10:16:39.050925Z", "url": "https://files.pythonhosted.org/packages/41/7d/e22beca59928b414902d5420818731df6cddf57cf5325de08cd81e4ebd85/isparkcache-0.1.8.tar.gz", "yanked": false}], "0.1.9": [{"comment_text": "", "digests": {"md5": "979be75da74ed7ca6936c683481e3506", "sha256": "c84ce59e01b6021c2ab66830211a6f901c19843cd60903525345ac20f42a12c7"}, "downloads": -1, "filename": "isparkcache-0.1.9.tar.gz", "has_sig": false, "md5_digest": "979be75da74ed7ca6936c683481e3506", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18197, "upload_time": "2017-07-20T10:20:34", "upload_time_iso_8601": "2017-07-20T10:20:34.446579Z", "url": "https://files.pythonhosted.org/packages/6e/cc/245f0153bb577eb7e3f5e1445831e238af9d2486a069f7b9c99bc9ae1f1b/isparkcache-0.1.9.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "8e42fd0d8b7830f96803fe36e8436a36", "sha256": "4307391d28bf7dc573b70a6da76fd5b94a47437b374798d92d8c7b68d81a4050"}, "downloads": -1, "filename": "isparkcache-0.1.12.tar.gz", "has_sig": false, "md5_digest": "8e42fd0d8b7830f96803fe36e8436a36", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18317, "upload_time": "2017-08-09T08:30:29", "upload_time_iso_8601": "2017-08-09T08:30:29.809313Z", "url": "https://files.pythonhosted.org/packages/8e/c2/0ed8445c070964a9b80e1618f74345d1f8598ca485b8b2f4ca8b801663a1/isparkcache-0.1.12.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:53:55 2020"}