{"info": {"author": "Han Wang", "author_email": "wang_han@iapcm.ac.cn", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)", "Programming Language :: Python :: 3.6"], "description": ".. role:: raw-html-m2r(raw)\n   :format: html\n\n\n:raw-html-m2r:`<span style=\"font-size:larger;\">DeePMD-kit Manual</span>`\n============================================================================\n\nTable of contents\n=================\n\n\n* `About DeePMD-kit <#about-deepmd-kit>`_\n\n  * `Highlighted features <#highlighted-features>`_\n  * `Code structure <#code-structure>`_\n  * `License and credits <#license-and-credits>`_\n  * `Deep Potential in a nutshell <#deep-potential-in-a-nutshell>`_\n\n* `Download and install <#download-and-install>`_\n\n  * `Easy installation methods <#easy-installation-methods>`_\n\n    * `With Docker <#with-docker>`_\n    * `With conda <#with-conda>`_\n    * `Offline packages <#offline-packages>`_\n\n  * `Install the python interaction <#install-the-python-interface>`_\n\n    * `Install the Tensorflow's python interface <#install-the-tensorflows-python-interface>`_\n    * `Install the DeePMD-kit's python interface <#install-the-deepmd-kits-python-interface>`_\n\n  * `Install the C++ interface <#install-the-c-interface>`_\n\n    * `Install the Tensorflow's C++ interface <#install-the-tensorflows-c-interface>`_    \n    * `Install the DeePMD-kit's C++ interface <#install-the-deepmd-kits-c-interface>`_\n    * `Install LAMMPS's DeePMD-kit module <#install-lammpss-deepmd-kit-module>`_\n\n* `Use DeePMD-kit <#use-deepmd-kit>`_\n\n  * `Prepare data <#prepare-data>`_\n  * `Train a model <#train-a-model>`_\n\n    * `The DeePMD model <#the-deepmd-model>`_\n    * `The DeepPot-SE model <#the-deeppot-se-model>`_\n\n  * `Freeze a model <#freeze-a-model>`_\n  * `Test a model <#test-a-model>`_\n  * `Model inference <#model-inference>`_\n  * `Run MD with Lammps <#run-md-with-lammps>`_\n\n    * `Include deepmd in the pair style <#include-deepmd-in-the-pair-style>`_\n    * `Long-range interaction <#long-range-interaction>`_\n\n  * `Run path-integral MD with i-PI <#run-path-integral-md-with-i-pi>`_\n  * `Use deep potential with ASE <#use-deep-potential-with-ase>`_\n\n* `Troubleshooting <#troubleshooting>`_\n\nAbout DeePMD-kit\n================\n\nDeePMD-kit is a package written in Python/C++, designed to minimize the effort required to build deep learning based model of interatomic potential energy and force field and to perform molecular dynamics (MD). This brings new hopes to addressing the accuracy-versus-efficiency dilemma in molecular simulations. Applications of DeePMD-kit span from finite molecules to extended systems and from metallic systems to chemically bonded systems. \n\nHighlighted features\n--------------------\n\n\n* **interfaced with TensorFlow**\\ , one of the most popular deep learning frameworks, making the training process highly automatic and efficient.\n* **interfaced with high-performance classical MD and quantum (path-integral) MD packages**\\ , i.e., LAMMPS and i-PI, respectively. \n* **implements the Deep Potential series models**\\ , which have been successfully applied to  finite and extended systems including organic molecules, metals, semiconductors, and insulators, etc.\n* **implements MPI and GPU supports**\\ , makes it highly efficient for high performance parallel and distributed computing.\n* **highly modularized**\\ , easy to adapt to different descriptors for deep learning based potential energy models.\n\nCode structure\n--------------\n\nThe code is organized as follows:\n\n\n* \n  ``data/raw``\\ : tools manipulating the raw data files.\n\n* \n  ``examples``\\ : example json parameter files.\n\n* \n  ``source/3rdparty``\\ : third-party packages used by DeePMD-kit.\n\n* \n  ``source/cmake``\\ : cmake scripts for building.\n\n* \n  ``source/ipi``\\ : source code of i-PI client.\n\n* \n  ``source/lib``\\ : source code of DeePMD-kit library.\n\n* \n  ``source/lmp``\\ : source code of Lammps module.\n\n* \n  ``source/op``\\ : tensorflow op implementation. working with library.\n\n* \n  ``source/scripts``\\ : Python script for model freezing.\n\n* \n  ``source/train``\\ : Python modules and scripts for training and testing.\n\nLicense and credits\n-------------------\n\nThe project DeePMD-kit is licensed under `GNU LGPLv3.0 <./LICENSE>`_.\nIf you use this code in any future publications, please cite this using \n``Han Wang, Linfeng Zhang, Jiequn Han, and Weinan E. \"DeePMD-kit: A deep learning package for many-body potential energy representation and molecular dynamics.\" Computer Physics Communications 228 (2018): 178-184.``\n\nDeep Potential in a nutshell\n----------------------------\n\nThe goal of Deep Potential is to employ deep learning techniques and realize an inter-atomic potential energy model that is general, accurate, computationally efficient and scalable. The key component is to respect the extensive and symmetry-invariant properties of a potential energy model by assigning a local reference frame and a local environment to each atom. Each environment contains a finite number of atoms, whose local coordinates are arranged in a symmetry preserving way. These local coordinates are then transformed, through a sub-network, to a so-called *atomic energy*. Summing up all the atomic energies gives the potential energy of the system.\n\nThe initial proof of concept is in the `Deep Potential <http://www.global-sci.com/galley/CiCP-2017-0213.pdf>`_ paper, which employed an approach that was devised to train the neural network model with the potential energy only. With typical *ab initio* molecular dynamics (AIMD) datasets this is insufficient to reproduce the trajectories. The Deep Potential Molecular Dynamics (\\ `DeePMD <https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.143001>`_\\ ) model overcomes this limitation. In addition, the learning process in DeePMD improves significantly over the Deep Potential method thanks to the introduction of a flexible family of loss functions. The NN potential constructed in this way reproduces accurately the AIMD trajectories, both classical and quantum (path integral), in extended and finite systems, at a cost that scales linearly with system size and is always several orders of magnitude lower than that of equivalent AIMD simulations.\n\nAlthough being highly efficient, the original Deep Potential model satisfies the extensive and symmetry-invariant properties of a potential energy model at the price of introducing discontinuities in the model. This has negligible influence on a trajectory from canonical sampling but might not be sufficient for calculations of dynamical and mechanical properties. These points motivated us to develop the Deep Potential-Smooth Edition (\\ `DeepPot-SE <https://arxiv.org/abs/1805.09003>`_\\ ) model, which replaces the non-smooth local frame with a smooth and adaptive embedding network. DeepPot-SE shows great ability in modeling many kinds of systems that are of interests in the fields of physics, chemistry, biology, and materials science.\n\nIn addition to building up potential energy models, DeePMD-kit can also be used to build up coarse-grained models. In these models, the quantity that we want to parameterize is the free energy, or the coarse-grained potential, of the coarse-grained particles. See the `DeePCG paper <https://aip.scitation.org/doi/full/10.1063/1.5027645>`_ for more details.\n\nDownload and install\n====================\n\nPlease follow our `github <https://github.com/deepmodeling/deepmd-kit>`_ webpage to see the latest released version and development version.\n\nEasy installation methods\n-------------------------\n\nThere various easy methods to install DeePMD-kit. Choose one that you prefer. If you want to build by yourself, jump to the next two sections.\n\nWith Docker\n^^^^^^^^^^^\n\nA docker for installing the DeePMD-kit on CentOS 7 is available `here <https://github.com/frankhan91/deepmd-kit_docker>`_.\n\nWith conda\n^^^^^^^^^^\n\nDeePMD-kit is avaiable with `conda <https://github.com/conda/conda>`_. Install `Anaconda <https://www.anaconda.com/distribution/#download-section>`_ or `Miniconda <https://docs.conda.io/en/latest/miniconda.html>`_ first.\n\nTo install the CPU version:\n\n.. code-block:: bash\n\n   conda install deepmd-kit=*=*cpu lammps-dp=*=*cpu -c deepmodeling\n\nTo install the GPU version containing `CUDA 10.0 <https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver>`_\\ :\n\n.. code-block:: bash\n\n   conda install deepmd-kit=*=*gpu lammps-dp=*=*gpu -c deepmodeling\n\nOffline packages\n^^^^^^^^^^^^^^^^\n\nBoth CPU and GPU version offline package are avaiable in `the Releases page <https://github.com/deepmodeling/deepmd-kit/releases>`_.\n\nInstall the python interface\n----------------------------\n\nInstall the Tensorflow's python interface\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nFirst, check the python version and compiler version on your machine \n\n.. code-block:: bash\n\n   python --version; gcc --version\n\nIf your python version is 3.7.x, it is highly recommended that the GNU C/C++ compiler is higher than or equal to 5.0.\n\nWe follow the virtual environment approach to install the tensorflow's Python interface. The full instruction can be found on `the tensorflow's official website <https://www.tensorflow.org/install/pip>`_. Now we assume that the Python interface will be installed to virtual environment directory ``$tensorflow_venv``\n\n.. code-block:: bash\n\n   virtualenv -p python3 $tensorflow_venv\n   source $tensorflow_venv/bin/activate\n   pip install --upgrade pip\n   pip install --upgrade tensorflow==1.14.0\n\nIt is notice that everytime a new shell is started and one wants to use ``DeePMD-kit``\\ , the virtual environment should be activated by \n\n.. code-block:: bash\n\n   source $tensorflow_venv/bin/activate\n\nif one wants to skip out of the virtual environment, he/she can do\n\n.. code-block:: bash\n\n   deactivate\n\nIf one has multiple python interpreters named like python3.x, it can be specified by, for example\n\n.. code-block:: bash\n\n   virtualenv -p python3.7 $tensorflow_venv\n\nIf one needs the GPU support of deepmd-kit, the GPU version of tensorflow should be installed by\n\n.. code-block:: bash\n\n   pip install --upgrade tensorflow-gpu==1.14.0\n\nTo verify the installation, run\n\n.. code-block:: bash\n\n   python -c \"import tensorflow as tf; sess=tf.Session(); print(sess.run(tf.reduce_sum(tf.random_normal([1000, 1000]))))\"\n\nOne should remember to activate the virtual environment every time he/she uses deepmd-kit.\n\nInstall the DeePMD-kit's python interface\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nClone the DeePMD-kit source code\n\n.. code-block:: bash\n\n   cd /some/workspace\n   git clone --recursive https://github.com/deepmodeling/deepmd-kit.git deepmd-kit -b devel\n\nIf one downloads the .zip file from the github, then the default folder of source code would be ``deepmd-kit-master`` rather than ``deepmd-kit``. For convenience, you may want to record the location of source to a variable, saying ``deepmd_source_dir`` by\n\n.. code-block:: bash\n\n   cd deepmd-kit\n   deepmd_source_dir=`pwd`\n\nThen execute\n\n.. code-block:: bash\n\n   pip install .\n\nTo test the installation, one may execute\n\n.. code-block:: bash\n\n   dp -h\n\nIt will print the help information like\n\n.. code-block:: text\n\n   usage: dp [-h] {train,freeze,test} ...\n\n   DeePMD-kit: A deep learning package for many-body potential energy\n   representation and molecular dynamics\n\n   optional arguments:\n     -h, --help           show this help message and exit\n\n   Valid subcommands:\n     {train,freeze,test}\n       train              train a model\n       freeze             freeze the model\n       test               test the model\n\nInstall the C++ interface\n-------------------------\n\nIf one does not need to use DeePMD-kit with Lammps or I-Pi, then the python interface installed in the previous section does everything and he/she can safely skip this section. \n\nInstall the Tensorflow's C++ interface\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIt is highly recommended that one keeps the same C/C++ compiler as the python interface. The C++ interface of DeePMD-kit was tested with compiler gcc >= 4.8. It is noticed that the I-Pi support is only compiled with gcc >= 4.9.\n\nFirst the C++ interface of Tensorflow should be installed. It is noted that the version of Tensorflow should be in consistent with the python interface. We assume that you have followed our instruction and installed tensorflow python interface 1.14.0 with, then you may follow `the instruction for CPU <doc/install-tf.1.14.md>`_ to install the corresponding C++ interface (CPU only). If one wants GPU supports, he/she should follow `the instruction for GPU <doc/install-tf.1.14-gpu.md>`_ to install the C++ interface.\n\nInstall the DeePMD-kit's C++ interface\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nNow goto the source code directory of DeePMD-kit and make a build place.\n\n.. code-block:: bash\n\n   cd $deepmd_source_dir/source\n   mkdir build \n   cd build\n\nI assume you want to install DeePMD-kit into path ``$deepmd_root``\\ , then execute cmake\n\n.. code-block:: bash\n\n   cmake -DTENSORFLOW_ROOT=$tensorflow_root -DCMAKE_INSTALL_PREFIX=$deepmd_root ..\n\nwhere the variable ``tensorflow_root`` stores the location where the tensorflow's C++ interface is installed. The DeePMD-kit will automatically detect if a CUDA tool-kit is available on your machine and build the GPU support accordingly. If you want to force the cmake to find CUDA tool-kit, you can speicify the key ``USE_CUDA_TOOLKIT``\\ , \n\n.. code-block:: bash\n\n   cmake -DUSE_CUDA_TOOLKIT=true -DTENSORFLOW_ROOT=$tensorflow_root -DCMAKE_INSTALL_PREFIX=$deepmd_root ..\n\nand you may further asked to provide ``CUDA_TOOLKIT_ROOT_DIR``. If the cmake has executed successfully, then \n\n.. code-block:: bash\n\n   make\n   make install\n\nIf everything works fine, you will have the following executable and libraries installed in ``$deepmd_root/bin`` and ``$deepmd_root/lib``\n\n.. code-block:: bash\n\n   $ ls $deepmd_root/bin\n   dp_ipi\n   $ ls $deepmd_root/lib\n   libdeepmd_ipi.so  libdeepmd_op.so  libdeepmd.so\n\nInstall LAMMPS's DeePMD-kit module\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nDeePMD-kit provide module for running MD simulation with LAMMPS. Now make the DeePMD-kit module for LAMMPS.\n\n.. code-block:: bash\n\n   cd $deepmd_source_dir/source/build\n   make lammps\n\nDeePMD-kit will generate a module called ``USER-DEEPMD`` in the ``build`` directory. Now download your favorite LAMMPS code, and uncompress it (I assume that you have downloaded the tar ``lammps-stable.tar.gz``\\ )\n\n.. code-block:: bash\n\n   cd /some/workspace\n   tar xf lammps-stable.tar.gz\n\nThe source code of LAMMPS is stored in directory, for example ``lammps-31Mar17``. Now go into the LAMMPS code and copy the DeePMD-kit module like this\n\n.. code-block:: bash\n\n   cd lammps-31Mar17/src/\n   cp -r $deepmd_source_dir/source/build/USER-DEEPMD .\n\nNow build LAMMPS\n\n.. code-block:: bash\n\n   make yes-user-deepmd\n   make mpi -j4\n\nThe option ``-j4`` means using 4 processes in parallel. You may want to use a different number according to your hardware. \n\nIf everything works fine, you will end up with an executable ``lmp_mpi``.\n\nThe DeePMD-kit module can be removed from LAMMPS source code by \n\n.. code-block:: bash\n\n   make no-user-deepmd\n\nUse DeePMD-kit\n==============\n\nIn this text, we will call the deep neural network that is used to represent the interatomic interactions (Deep Potential) the **model**. The typical procedure of using DeePMD-kit is \n\n\n#. Prepare data\n#. Train a model\n#. Freeze the model\n#. MD runs with the model (Native MD code or LAMMPS)\n\nPrepare data\n------------\n\nOne needs to provide the following information to train a model: the atom type, the simulation box, the atom coordinate, the atom force, system energy and virial. A snapshot of a system that contains these information is called a **frame**. We use the following convention of units:\n\n.. list-table::\n   :header-rows: 1\n\n   * - Property\n     - Unit\n   * - Time\n     - ps\n   * - Length\n     - \u00c5\n   * - Energy\n     - eV\n   * - Force\n     - eV/\u00c5\n   * - Pressure\n     - Bar\n\n\nThe frames of the system are stored in two formats. A raw file is a plain text file with each information item written in one file and one frame written on one line. The default files that provide box, coordinate, force, energy and virial are ``box.raw``\\ , ``coord.raw``\\ , ``force.raw``\\ , ``energy.raw`` and ``virial.raw``\\ , respectively. *We recommend you use these file names*. Here is an example of force.raw:\n\n.. code-block:: bash\n\n   $ cat force.raw\n   -0.724  2.039 -0.951  0.841 -0.464  0.363\n    6.737  1.554 -5.587 -2.803  0.062  2.222\n   -1.968 -0.163  1.020 -0.225 -0.789  0.343\n\nThis ``force.raw`` contains 3 frames with each frame having the forces of 2 atoms, thus it has 3 lines and 6 columns. Each line provides all the 3 force components of 2 atoms in 1 frame. The first three numbers are the 3 force components of the first atom, while the second three numbers are the 3 force components of the second atom. The coordinate file ``coord.raw`` is organized similarly. In ``box.raw``\\ , the 9 components of the box vectors should be provided on each line. In ``virial.raw``\\ , the 9 components of the virial tensor should be provided on each line. The number of lines of all raw files should be identical.\n\nWe assume that the atom types do not change in all frames. It is provided by ``type.raw``\\ , which has one line with the types of atoms written one by one. The atom types should be integers. For example the ``type.raw`` of a system that has 2 atoms with 0 and 1:\n\n.. code-block:: bash\n\n   $ cat type.raw\n   0 1\n\nThe second format is the data sets of ``numpy`` binary data that are directly used by the training program. User can use the script ``$deepmd_source_dir/data/raw/raw_to_set.sh`` to convert the prepared raw files to data sets. For example, if we have a raw file that contains 6000 frames, \n\n.. code-block:: bash\n\n   $ ls \n   box.raw  coord.raw  energy.raw  force.raw  type.raw  virial.raw\n   $ $deepmd_source_dir/data/raw/raw_to_set.sh 2000\n   nframe is 6000\n   nline per set is 2000\n   will make 3 sets\n   making set 0 ...\n   making set 1 ...\n   making set 2 ...\n   $ ls \n   box.raw  coord.raw  energy.raw  force.raw  set.000  set.001  set.002  type.raw  virial.raw\n\nIt generates three sets ``set.000``\\ , ``set.001`` and ``set.002``\\ , with each set contains 2000 frames. The last set (\\ ``set.002``\\ ) is used as testing set, while the rest sets (\\ ``set.000`` and ``set.001``\\ ) are used as training sets. One do not need to take care of the binary data files in each of the ``set.*`` directories. The path containing ``set.*`` and ``type.raw`` is called a *system*. \n\nTrain a model\n-------------\n\nWrite the input script\n^^^^^^^^^^^^^^^^^^^^^^\n\nThe method of training is explained in our `DeePMD <https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.143001>`_ and `DeepPot-SE <https://arxiv.org/abs/1805.09003>`_ papers. With the source code we provide a small training dataset taken from 400 frames generated by NVT ab-initio water MD trajectory with 300 frames for training and 100 for testing. `An example training parameter file <./examples/water/train/water_se_a.json>`_ is provided. One can try with the training by\n\n.. code-block:: bash\n\n   $ cd $deepmd_source_dir/examples/water/train/\n   $ dp train water_se_a.json\n\nwhere ``water_se_a.json`` is the ``json`` format parameter file that controls the training. The components of the ``water.json`` contains three parts, ``model``\\ , ``learning_rate``\\ , ``loss`` and ``training``.\n\nThe ``model`` section specify how the deep potential model is built. An example of the smooth-edition is provided as follows\n\n.. code-block:: json\n\n       \"model\": {\n       \"type_map\": [\"O\", \"H\"],\n       \"descriptor\" :{\n           \"type\":     \"se_a\",\n           \"rcut_smth\":    5.80,\n           \"rcut\":     6.00,\n           \"sel\":      [46, 92],\n           \"neuron\":       [25, 50, 100],\n           \"axis_neuron\":  16,\n           \"resnet_dt\":    false,\n           \"seed\":     1,\n           \"_comment\":     \" that's all\"\n       },\n       \"fitting_net\" : {\n           \"neuron\":       [240, 240, 240],\n           \"resnet_dt\":    true,       \n           \"seed\":     1,\n           \"_comment\":     \" that's all\"\n       },\n       \"_comment\": \" that's all\"\n       }\n\nThe **\\ ``type_map``\\ ** is optional, which provide the element names (but not restricted to) for corresponding atom types.\n\nThe construction of the descriptor is given by option **\\ ``descriptor``\\ **. The **\\ ``type``\\ ** of the descriptor is set to ``\"se_a\"``\\ , which means smooth-edition, angular infomation. The  **\\ ``rcut``\\ ** is the cut-off radius for neighbor searching, and the **\\ ``rcut_smth``\\ ** gives where the smoothing starts. **\\ ``sel``\\ ** gives the maximum possible number of neighbors in the cut-off radius. It is a list, the length of which is the same as the number of atom types in the system, and ``sel[i]`` denote the maximum possible number of neighbors with type ``i``. The **\\ ``neuron``\\ ** specifies the size of the embedding net. From left to right the members denote the sizes of each hidden layers from input end to the output end, respectively. The **\\ ``axis_neuron``\\ ** specifies the size of submatrix of the embedding matrix, the axis matrix as explained in the `DeepPot-SE paper <https://arxiv.org/abs/1805.09003>`_. If the outer layer is of twice size as the inner layer, then the inner layer is copied and concatenated, then a `ResNet architecture <https://arxiv.org/abs/1512.03385>`_ is build between them. If the option **\\ ``resnet_dt``\\ ** is set ``true``\\ , then a timestep is used in the ResNet. **\\ ``seed``\\ ** gives the random seed that is used to generate random numbers when initializing the model parameters.\n\nThe construction of the fitting net is give by **\\ ``fitting_net``\\ **. The key **\\ ``neuron``\\ ** specifies the size of the fitting net. If two neighboring layers are of the same size, then a `ResNet architecture <https://arxiv.org/abs/1512.03385>`_ is build between them. If the option **\\ ``resnet_dt``\\ ** is set ``true``\\ , then a timestep is used in the ResNet. **\\ ``seed``\\ ** gives the random seed that is used to generate random numbers when initializing the model parameters.\n\nAn example of the ``learning_rate`` is given as follows\n\n.. code-block:: json\n\n       \"learning_rate\" :{\n       \"type\":     \"exp\",\n       \"start_lr\": 0.005,\n       \"decay_steps\":  5000,\n       \"decay_rate\":   0.95,\n       \"_comment\": \"that's all\"\n       }\n\nThe option **\\ ``start_lr``\\ **\\ , **\\ ``decay_rate``\\ ** and **\\ ``decay_steps``\\ ** specify how the learning rate changes. For example, the ``t``\\ th batch will be trained with learning rate:\n\n.. math::\n\n   lr(t) = start_lr * decay_rate ^ ( t / decay_steps )\n\nAn example of the ``loss`` is \n\n.. code-block:: json\n\n       \"loss\" : {\n       \"start_pref_e\": 0.02,\n       \"limit_pref_e\": 1,\n       \"start_pref_f\": 1000,\n       \"limit_pref_f\": 1,\n       \"start_pref_v\": 0,\n       \"limit_pref_v\": 0,\n       \"_comment\": \" that's all\"\n       }\n\nThe options **\\ ``start_pref_e``\\ **\\ , **\\ ``limit_pref_e``\\ **\\ , **\\ ``start_pref_f``\\ **\\ , **\\ ``limit_pref_f``\\ **\\ , **\\ ``start_pref_v``\\ ** and **\\ ``limit_pref_v``\\ ** determine how the prefactors of energy error, force error and virial error changes in the loss function (see the appendix of the `DeePMD paper <https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.143001>`_ for details). Taking the prefactor of force error for example, the prefactor at batch ``t`` is\n\n.. math::\n\n   w_f(t) = start_pref_f * ( lr(t) / start_lr ) + limit_pref_f * ( 1 - lr(t) / start_lr )\n\nSince we do not have virial data, the virial prefactors ``start_pref_v`` and ``limit_pref_v`` are set to 0.\n\nAn example of ``training`` is\n\n.. code-block:: json\n\n       \"training\" : {\n       \"systems\":  [\"../data/\"],\n       \"set_prefix\":   \"set\",    \n       \"stop_batch\":   1000000,\n       \"batch_size\":   1,\n\n       \"seed\":     1,\n\n       \"_comment\": \" display and restart\",\n       \"_comment\": \" frequencies counted in batch\",\n       \"disp_file\":    \"lcurve.out\",\n       \"disp_freq\":    100,\n       \"numb_test\":    10,\n       \"save_freq\":    1000,\n       \"save_ckpt\":    \"model.ckpt\",\n       \"load_ckpt\":    \"model.ckpt\",\n       \"disp_training\":true,\n       \"time_training\":true,\n       \"profiling\":    false,\n       \"profiling_file\":\"timeline.json\",\n       \"_comment\": \"that's all\"\n       }\n\nThe option **\\ ``systems``\\ ** provide location of the systems (path to ``set.*`` and ``type.raw``\\ ). It is a vector, thus DeePMD-kit allows you to provide multiple systems. DeePMD-kit will train the model with the systems in the vector one by one in a cyclic manner. **It is warned that the example water data (in folder ``examples/data/water``\\ ) is of very limited amount, is provided only for testing purpose, and should not be used to train a productive model.**\n\nThe option **\\ ``batch_size``\\ ** specifies the number of frames in each batch. It can be set to ``\"auto\"`` to enable a automatic batch size.\nThe option **\\ ``stop_batch``\\ ** specifies the total number of batches will be used in the training.\n\nTraining\n^^^^^^^^\n\nThe training can be invoked by\n\n.. code-block:: bash\n\n   $ dp train water_se_a.json\n\nDuring the training, the error of the model is tested every **\\ ``disp_freq``\\ ** batches with **\\ ``numb_test``\\ ** frames from the last set in the **\\ ``systems``\\ ** directory on the fly, and the results are output to **\\ ``disp_file``\\ **. A typical ``disp_file`` looks like\n\n.. code-block:: bash\n\n   # batch      l2_tst    l2_trn    l2_e_tst  l2_e_trn    l2_f_tst  l2_f_trn         lr\n         0    2.67e+01  2.57e+01    2.21e-01  2.22e-01    8.44e-01  8.12e-01    1.0e-03\n       100    6.14e+00  5.40e+00    3.01e-01  2.99e-01    1.93e-01  1.70e-01    1.0e-03\n       200    5.02e+00  4.49e+00    1.53e-01  1.53e-01    1.58e-01  1.42e-01    1.0e-03\n       300    4.36e+00  3.71e+00    7.32e-02  7.27e-02    1.38e-01  1.17e-01    1.0e-03\n       400    4.04e+00  3.29e+00    3.16e-02  3.22e-02    1.28e-01  1.04e-01    1.0e-03\n\nThe first column displays the number of batches. The second and third columns display the loss function evaluated by ``numb_test`` frames randomly chosen from the test set and that evaluated by the current training batch, respectively. The fourth and fifth columns display the RMS energy error (normalized by number of atoms) evaluated by ``numb_test`` frames randomly chosen from the test set and that evaluated by the current training batch, respectively. The sixth and seventh columns display the RMS force error (component-wise) evaluated by ``numb_test`` frames randomly chosen from the test set and that evaluated by the current training batch, respectively. The last column displays the current learning rate.\n\nCheckpoints will be written to files with prefix **\\ ``save_ckpt``\\ ** every **\\ ``save_freq``\\ ** batches. If **\\ ``restart``\\ ** is set to ``true``\\ , then the training will start from the checkpoint named **\\ ``load_ckpt``\\ **\\ , rather than from scratch.\n\nSeveral command line options can be passed to ``dp train``\\ , which can be checked with\n\n.. code-block:: bash\n\n   $ dp train --help\n\nAn explanation will be provided\n\n.. code-block::\n\n   positional arguments:\n     INPUT                 the input json database\n\n   optional arguments:\n     -h, --help            show this help message and exit\n     -t INTER_THREADS, --inter-threads INTER_THREADS\n                           With default value 0. Setting the \"inter_op_parallelism_threads\" key for the tensorflow, the \"intra_op_parallelism_threads\" will be set by the env variable OMP_NUM_THREADS\n     --init-model INIT_MODEL\n                           Initialize a model by the provided checkpoint\n     --restart RESTART     Restart the training from the provided checkpoint\n\nThe keys ``intra_op_parallelism_threads`` and ``inter_op_parallelism_threads`` are Tensorflow configurations for multithreading, which are explained `here <https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu>`_. Skipping ``-t`` and ``OMP_NUM_THREADS`` leads to the default setting of these keys in the Tensorflow.\n\n**\\ ``--init-model model.ckpt``\\ **\\ , for example, initializes the model training with an existing model that is stored in the checkpoint ``model.ckpt``\\ , the network architectures should match.\n\n**\\ ``--restart model.ckpt``\\ **\\ , continues the training from the checkpoint ``model.ckpt``.\n\nFreeze a model\n--------------\n\nThe trained neural network is extracted from a checkpoint and dumped into a database. This process is called \"freezing\" a model. The idea and part of our code are from `Morgan <https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc>`_. To freeze a model, typically one does\n\n.. code-block:: bash\n\n   $ dp freeze -o graph.pb\n\nin the folder where the model is trained. The output database is called ``graph.pb``.\n\nTest a model\n------------\n\nThe frozen model can be used in many ways. The most straightforward test can be performed using ``dp test``. A typical usage of ``dp test`` is \n\n.. code-block:: bash\n\n   dp test -m graph.pb -s /path/to/system -n 30\n\nwhere ``-m`` gives the tested model, ``-s`` the path to the tested system and ``-n`` the number of tested frames. Several other command line options can be passed to ``dp test``\\ , which can be checked with\n\n.. code-block:: bash\n\n   $ dp test --help\n\nAn explanation will be provided\n\n.. code-block::\n\n   usage: dp test [-h] [-m MODEL] [-s SYSTEM] [-S SET_PREFIX] [-n NUMB_TEST]\n                  [-r RAND_SEED] [--shuffle-test] [-d DETAIL_FILE]\n\n   optional arguments:\n     -h, --help            show this help message and exit\n     -m MODEL, --model MODEL\n                           Frozen model file to import\n     -s SYSTEM, --system SYSTEM\n                           The system dir\n     -S SET_PREFIX, --set-prefix SET_PREFIX\n                           The set prefix\n     -n NUMB_TEST, --numb-test NUMB_TEST\n                           The number of data for test\n     -r RAND_SEED, --rand-seed RAND_SEED\n                           The random seed\n     --shuffle-test        Shuffle test data\n     -d DETAIL_FILE, --detail-file DETAIL_FILE\n                           The file containing details of energy force and virial\n                           accuracy\n\nModel inference\n---------------\n\nOne may use the python interface of DeePMD-kit for model inference, an example is given as follows\n\n.. code-block:: python\n\n   import deepmd.DeepPot as DP\n   import numpy as np\n   dp = DP('graph.pb')\n   coord = np.array([[1,0,0], [0,0,1.5], [1,0,3]]).reshape([1, -1])\n   cell = np.diag(10 * np.ones(3)).reshape([1, -1])\n   atype = [1,0,1]\n   e, f, v = dp.eval(coord, cell, atype)\n\nwhere ``e``\\ , ``f`` and ``v`` are predicted energy, force and virial of the system, respectively.\n\nRun MD with LAMMPS\n------------------\n\nInclude deepmd in the pair style\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRunning an MD simulation with LAMMPS is simpler. In the LAMMPS input file, one needs to specify the pair style as follows\n\n.. code-block:: bash\n\n   pair_style     deepmd graph.pb\n   pair_coeff\n\nwhere ``graph.pb`` is the file name of the frozen model. The ``pair_coeff`` should be left blank. It should be noted that LAMMPS counts atom types starting from 1, therefore, all LAMMPS atom type will be firstly subtracted by 1, and then passed into the DeePMD-kit engine to compute the interactions. `A detailed documentation of this pair style is available. <doc/lammps-pair-style-deepmd.md>`_.\n\nLong-range interaction\n^^^^^^^^^^^^^^^^^^^^^^\n\nThe reciprocal space part of the long-range interaction can be calculated by LAMMPS command ``kspace_style``. To use it with DeePMD-kit, one writes \n\n.. code-block:: bash\n\n   pair_style  deepmd graph.pb\n   pair_coeff\n   kspace_style    pppm 1.0e-5\n   kspace_modify   gewald 0.45\n\nPlease notice that the DeePMD does nothing to the direct space part of the electrostatic interaction, because this part is assumed to be fitted in the DeePMD model (the direct space cut-off is thus the cut-off of the DeePMD model). The splitting parameter ``gewald`` is modified by the ``kspace_modify`` command.\n\nRun path-integral MD with i-PI\n------------------------------\n\nThe i-PI works in a client-server model. The i-PI provides the server for integrating the replica positions of atoms, while the DeePMD-kit provides a client named ``dp_ipi`` that computes the interactions (including energy, force and virial). The server and client communicates via the Unix domain socket or the Internet socket. The client can be started by\n\n.. code-block:: bash\n\n   $ dp_ipi water.json\n\nIt is noted that multiple instances of the client is allow for computing, in parallel, the interactions of multiple replica of the path-integral MD.\n\n``water.json`` is the parameter file for the client ``dp_ipi``\\ , and `an example <./examples/ipi/water.json>`_ is provided:\n\n.. code-block:: json\n\n   {\n       \"verbose\":      false,\n       \"use_unix\":     true,\n       \"port\":     31415,\n       \"host\":     \"localhost\",\n       \"graph_file\":   \"graph.pb\",\n       \"coord_file\":   \"conf.xyz\",\n       \"atom_type\" : {\n       \"OW\":       0, \n       \"HW1\":      1,\n       \"HW2\":      1\n       }\n   }\n\nThe option **\\ ``use_unix``\\ ** is set to ``true`` to activate the Unix domain socket, otherwise, the Internet socket is used.\n\nThe option **\\ ``graph_file``\\ ** provides the file name of the frozen model.\n\nThe ``dp_ipi`` gets the atom names from an `XYZ file <https://en.wikipedia.org/wiki/XYZ_file_format>`_ provided by **\\ ``coord_file``\\ ** (meanwhile ignores all coordinates in it), and translates the names to atom types by rules provided by **\\ ``atom_type``\\ **.\n\nUse deep potential with ASE\n---------------------------\n\nDeep potential can be set up as a calculator with ASE to obtain potential energies and forces.\n\n.. code-block:: python\n\n   from ase import Atoms\n   from deepmd.calculator import DP\n\n   water = Atoms('H2O',\n                 positions=[(0.7601, 1.9270, 1),\n                            (1.9575, 1, 1),\n                            (1., 1., 1.)],\n                 cell=[100, 100, 100],\n                 calculator=DP(model=\"frozen_model.pb\"))\n   print(water.get_potential_energy())\n   print(water.get_forces())\n\nOptimization is also available:\n\n.. code-block:: python\n\n   from ase.optimize import BFGS\n   dyn = BFGS(water)\n   dyn.run(fmax=1e-6)\n   print(water.get_positions())\n\nTroubleshooting\n===============\n\nIn consequence of various differences of computers or systems, problems may occur. Some common circumstances are listed as follows. \nIf other unexpected problems occur, you're welcome to contact us for help.\n\nModel compatability\n-------------------\n\nWhen the version of DeePMD-kit used to training model is different from the that of DeePMD-kit running MDs, one has the problem of model compatability.\n\nDeePMD-kit guarantees that the codes with the same major and minor revisions are compatible. That is to say v0.12.5 is compatible to v0.12.0, but is not compatible to v0.11.0 nor v1.0.0. \n\nInstallation: inadequate versions of gcc/g++\n--------------------------------------------\n\nSometimes you may use a gcc/g++ of version <4.9. If you have a gcc/g++ of version > 4.9, say, 7.2.0, you may choose to use it by doing \n\n.. code-block:: bash\n\n   export CC=/path/to/gcc-7.2.0/bin/gcc\n   export CXX=/path/to/gcc-7.2.0/bin/g++\n\nIf, for any reason, for example, you only have a gcc/g++ of version 4.8.5, you can still compile all the parts of TensorFlow and most of the parts of DeePMD-kit. i-Pi will be disabled automatically.\n\nInstallation: build files left in DeePMD-kit\n--------------------------------------------\n\nWhen you try to build a second time when installing DeePMD-kit, files produced before may contribute to failure. Thus, you may clear them by\n\n.. code-block:: bash\n\n   cd build\n   rm -r *\n\nand redo the ``cmake`` process.\n\nTraining: TensorFlow abi binary cannot be found when doing training\n-------------------------------------------------------------------\n\nIf you confront such kind of error: \n\n.. code-block::\n\n   $deepmd_root/lib/deepmd/libop_abi.so: undefined symbol:\n   _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringB5cxx11Ev\n\nThis may happen if you are using a gcc >= 5.0, and tensorflow was compiled with gcc < 5.0. You may set ``-DOP_CXX_ABI=0`` in the process of ``cmake``.\n\nAnother possible reason might be the large gap between the python version of TensorFlow and the TensorFlow c++ interface.\n\nMD: cannot run LAMMPS after installing a new version of DeePMD-kit\n------------------------------------------------------------------\n\nThis typically happens when you install a new version of DeePMD-kit and copy directly the generated ``USER-DEEPMD`` to a LAMMPS source code folder and re-install LAMMPS.\n\nTo solve this problem, it suffices to first remove ``USER-DEEPMD`` from LAMMPS source code by \n\n.. code-block:: bash\n\n   make no-user-deepmd\n\nand then install the new ``USER-DEEPMD``.\n\nIf this does not solve your problem, try to decompress the LAMMPS source tarball and install LAMMPS from scratch again, which typically should be very fast.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/deepmodeling/deepmd-kit", "keywords": "deepmd", "license": "", "maintainer": "", "maintainer_email": "", "name": "deepmd-kit", "package_url": "https://pypi.org/project/deepmd-kit/", "platform": "", "project_url": "https://pypi.org/project/deepmd-kit/", "project_urls": {"Homepage": "https://github.com/deepmodeling/deepmd-kit"}, "release_url": "https://pypi.org/project/deepmd-kit/1.1.4/", "requires_dist": ["numpy", "scipy", "dpdata (>=0.1.9) ; extra == 'test'"], "requires_python": ">=3.6", "summary": "A deep learning package for many-body potential energy representation and molecular dynamics", "version": "1.1.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>.. role:: raw-html-m2r(raw)\n:format: html</p>\n<h1>:raw-html-m2r:<code>&lt;span style=\"font-size:larger;\"&gt;DeePMD-kit Manual&lt;/span&gt;</code></h1>\n<h1>Table of contents</h1>\n<ul>\n<li>\n<p><code>About DeePMD-kit &lt;#about-deepmd-kit&gt;</code>_</p>\n<ul>\n<li><code>Highlighted features &lt;#highlighted-features&gt;</code>_</li>\n<li><code>Code structure &lt;#code-structure&gt;</code>_</li>\n<li><code>License and credits &lt;#license-and-credits&gt;</code>_</li>\n<li><code>Deep Potential in a nutshell &lt;#deep-potential-in-a-nutshell&gt;</code>_</li>\n</ul>\n</li>\n<li>\n<p><code>Download and install &lt;#download-and-install&gt;</code>_</p>\n<ul>\n<li>\n<p><code>Easy installation methods &lt;#easy-installation-methods&gt;</code>_</p>\n<ul>\n<li><code>With Docker &lt;#with-docker&gt;</code>_</li>\n<li><code>With conda &lt;#with-conda&gt;</code>_</li>\n<li><code>Offline packages &lt;#offline-packages&gt;</code>_</li>\n</ul>\n</li>\n<li>\n<p><code>Install the python interaction &lt;#install-the-python-interface&gt;</code>_</p>\n<ul>\n<li><code>Install the Tensorflow's python interface &lt;#install-the-tensorflows-python-interface&gt;</code>_</li>\n<li><code>Install the DeePMD-kit's python interface &lt;#install-the-deepmd-kits-python-interface&gt;</code>_</li>\n</ul>\n</li>\n<li>\n<p><code>Install the C++ interface &lt;#install-the-c-interface&gt;</code>_</p>\n<ul>\n<li><code>Install the Tensorflow's C++ interface &lt;#install-the-tensorflows-c-interface&gt;</code>_</li>\n<li><code>Install the DeePMD-kit's C++ interface &lt;#install-the-deepmd-kits-c-interface&gt;</code>_</li>\n<li><code>Install LAMMPS's DeePMD-kit module &lt;#install-lammpss-deepmd-kit-module&gt;</code>_</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><code>Use DeePMD-kit &lt;#use-deepmd-kit&gt;</code>_</p>\n<ul>\n<li>\n<p><code>Prepare data &lt;#prepare-data&gt;</code>_</p>\n</li>\n<li>\n<p><code>Train a model &lt;#train-a-model&gt;</code>_</p>\n<ul>\n<li><code>The DeePMD model &lt;#the-deepmd-model&gt;</code>_</li>\n<li><code>The DeepPot-SE model &lt;#the-deeppot-se-model&gt;</code>_</li>\n</ul>\n</li>\n<li>\n<p><code>Freeze a model &lt;#freeze-a-model&gt;</code>_</p>\n</li>\n<li>\n<p><code>Test a model &lt;#test-a-model&gt;</code>_</p>\n</li>\n<li>\n<p><code>Model inference &lt;#model-inference&gt;</code>_</p>\n</li>\n<li>\n<p><code>Run MD with Lammps &lt;#run-md-with-lammps&gt;</code>_</p>\n<ul>\n<li><code>Include deepmd in the pair style &lt;#include-deepmd-in-the-pair-style&gt;</code>_</li>\n<li><code>Long-range interaction &lt;#long-range-interaction&gt;</code>_</li>\n</ul>\n</li>\n<li>\n<p><code>Run path-integral MD with i-PI &lt;#run-path-integral-md-with-i-pi&gt;</code>_</p>\n</li>\n<li>\n<p><code>Use deep potential with ASE &lt;#use-deep-potential-with-ase&gt;</code>_</p>\n</li>\n</ul>\n</li>\n<li>\n<p><code>Troubleshooting &lt;#troubleshooting&gt;</code>_</p>\n</li>\n</ul>\n<h1>About DeePMD-kit</h1>\n<p>DeePMD-kit is a package written in Python/C++, designed to minimize the effort required to build deep learning based model of interatomic potential energy and force field and to perform molecular dynamics (MD). This brings new hopes to addressing the accuracy-versus-efficiency dilemma in molecular simulations. Applications of DeePMD-kit span from finite molecules to extended systems and from metallic systems to chemically bonded systems.</p>\n<h2>Highlighted features</h2>\n<ul>\n<li><strong>interfaced with TensorFlow</strong>\\ , one of the most popular deep learning frameworks, making the training process highly automatic and efficient.</li>\n<li><strong>interfaced with high-performance classical MD and quantum (path-integral) MD packages</strong>\\ , i.e., LAMMPS and i-PI, respectively.</li>\n<li><strong>implements the Deep Potential series models</strong>\\ , which have been successfully applied to  finite and extended systems including organic molecules, metals, semiconductors, and insulators, etc.</li>\n<li><strong>implements MPI and GPU supports</strong>\\ , makes it highly efficient for high performance parallel and distributed computing.</li>\n<li><strong>highly modularized</strong>\\ , easy to adapt to different descriptors for deep learning based potential energy models.</li>\n</ul>\n<h2>Code structure</h2>\n<p>The code is organized as follows:</p>\n<ul>\n<li>\n<p><code>data/raw</code>\\ : tools manipulating the raw data files.</p>\n</li>\n<li>\n<p><code>examples</code>\\ : example json parameter files.</p>\n</li>\n<li>\n<p><code>source/3rdparty</code>\\ : third-party packages used by DeePMD-kit.</p>\n</li>\n<li>\n<p><code>source/cmake</code>\\ : cmake scripts for building.</p>\n</li>\n<li>\n<p><code>source/ipi</code>\\ : source code of i-PI client.</p>\n</li>\n<li>\n<p><code>source/lib</code>\\ : source code of DeePMD-kit library.</p>\n</li>\n<li>\n<p><code>source/lmp</code>\\ : source code of Lammps module.</p>\n</li>\n<li>\n<p><code>source/op</code>\\ : tensorflow op implementation. working with library.</p>\n</li>\n<li>\n<p><code>source/scripts</code>\\ : Python script for model freezing.</p>\n</li>\n<li>\n<p><code>source/train</code>\\ : Python modules and scripts for training and testing.</p>\n</li>\n</ul>\n<h2>License and credits</h2>\n<p>The project DeePMD-kit is licensed under <code>GNU LGPLv3.0 &lt;./LICENSE&gt;</code>_.\nIf you use this code in any future publications, please cite this using\n<code>Han Wang, Linfeng Zhang, Jiequn Han, and Weinan E. \"DeePMD-kit: A deep learning package for many-body potential energy representation and molecular dynamics.\" Computer Physics Communications 228 (2018): 178-184.</code></p>\n<h2>Deep Potential in a nutshell</h2>\n<p>The goal of Deep Potential is to employ deep learning techniques and realize an inter-atomic potential energy model that is general, accurate, computationally efficient and scalable. The key component is to respect the extensive and symmetry-invariant properties of a potential energy model by assigning a local reference frame and a local environment to each atom. Each environment contains a finite number of atoms, whose local coordinates are arranged in a symmetry preserving way. These local coordinates are then transformed, through a sub-network, to a so-called <em>atomic energy</em>. Summing up all the atomic energies gives the potential energy of the system.</p>\n<p>The initial proof of concept is in the <code>Deep Potential &lt;http://www.global-sci.com/galley/CiCP-2017-0213.pdf&gt;</code>_ paper, which employed an approach that was devised to train the neural network model with the potential energy only. With typical <em>ab initio</em> molecular dynamics (AIMD) datasets this is insufficient to reproduce the trajectories. The Deep Potential Molecular Dynamics (\\ <code>DeePMD &lt;https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.143001&gt;</code>_\\ ) model overcomes this limitation. In addition, the learning process in DeePMD improves significantly over the Deep Potential method thanks to the introduction of a flexible family of loss functions. The NN potential constructed in this way reproduces accurately the AIMD trajectories, both classical and quantum (path integral), in extended and finite systems, at a cost that scales linearly with system size and is always several orders of magnitude lower than that of equivalent AIMD simulations.</p>\n<p>Although being highly efficient, the original Deep Potential model satisfies the extensive and symmetry-invariant properties of a potential energy model at the price of introducing discontinuities in the model. This has negligible influence on a trajectory from canonical sampling but might not be sufficient for calculations of dynamical and mechanical properties. These points motivated us to develop the Deep Potential-Smooth Edition (\\ <code>DeepPot-SE &lt;https://arxiv.org/abs/1805.09003&gt;</code>_\\ ) model, which replaces the non-smooth local frame with a smooth and adaptive embedding network. DeepPot-SE shows great ability in modeling many kinds of systems that are of interests in the fields of physics, chemistry, biology, and materials science.</p>\n<p>In addition to building up potential energy models, DeePMD-kit can also be used to build up coarse-grained models. In these models, the quantity that we want to parameterize is the free energy, or the coarse-grained potential, of the coarse-grained particles. See the <code>DeePCG paper &lt;https://aip.scitation.org/doi/full/10.1063/1.5027645&gt;</code>_ for more details.</p>\n<h1>Download and install</h1>\n<p>Please follow our <code>github &lt;https://github.com/deepmodeling/deepmd-kit&gt;</code>_ webpage to see the latest released version and development version.</p>\n<h2>Easy installation methods</h2>\n<p>There various easy methods to install DeePMD-kit. Choose one that you prefer. If you want to build by yourself, jump to the next two sections.</p>\n<p>With Docker\n^^^^^^^^^^^</p>\n<p>A docker for installing the DeePMD-kit on CentOS 7 is available <code>here &lt;https://github.com/frankhan91/deepmd-kit_docker&gt;</code>_.</p>\n<p>With conda\n^^^^^^^^^^</p>\n<p>DeePMD-kit is avaiable with <code>conda &lt;https://github.com/conda/conda&gt;</code><em>. Install <code>Anaconda &lt;https://www.anaconda.com/distribution/#download-section&gt;</code></em> or <code>Miniconda &lt;https://docs.conda.io/en/latest/miniconda.html&gt;</code>_ first.</p>\n<p>To install the CPU version:</p>\n<p>.. code-block:: bash</p>\n<p>conda install deepmd-kit=*=<em>cpu lammps-dp=</em>=*cpu -c deepmodeling</p>\n<p>To install the GPU version containing <code>CUDA 10.0 &lt;https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver&gt;</code>_\\ :</p>\n<p>.. code-block:: bash</p>\n<p>conda install deepmd-kit=*=<em>gpu lammps-dp=</em>=*gpu -c deepmodeling</p>\n<p>Offline packages\n^^^^^^^^^^^^^^^^</p>\n<p>Both CPU and GPU version offline package are avaiable in <code>the Releases page &lt;https://github.com/deepmodeling/deepmd-kit/releases&gt;</code>_.</p>\n<h2>Install the python interface</h2>\n<p>Install the Tensorflow's python interface\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p>\n<p>First, check the python version and compiler version on your machine</p>\n<p>.. code-block:: bash</p>\n<p>python --version; gcc --version</p>\n<p>If your python version is 3.7.x, it is highly recommended that the GNU C/C++ compiler is higher than or equal to 5.0.</p>\n<p>We follow the virtual environment approach to install the tensorflow's Python interface. The full instruction can be found on <code>the tensorflow's official website &lt;https://www.tensorflow.org/install/pip&gt;</code>_. Now we assume that the Python interface will be installed to virtual environment directory <code>$tensorflow_venv</code></p>\n<p>.. code-block:: bash</p>\n<p>virtualenv -p python3 $tensorflow_venv\nsource $tensorflow_venv/bin/activate\npip install --upgrade pip\npip install --upgrade tensorflow==1.14.0</p>\n<p>It is notice that everytime a new shell is started and one wants to use <code>DeePMD-kit</code>\\ , the virtual environment should be activated by</p>\n<p>.. code-block:: bash</p>\n<p>source $tensorflow_venv/bin/activate</p>\n<p>if one wants to skip out of the virtual environment, he/she can do</p>\n<p>.. code-block:: bash</p>\n<p>deactivate</p>\n<p>If one has multiple python interpreters named like python3.x, it can be specified by, for example</p>\n<p>.. code-block:: bash</p>\n<p>virtualenv -p python3.7 $tensorflow_venv</p>\n<p>If one needs the GPU support of deepmd-kit, the GPU version of tensorflow should be installed by</p>\n<p>.. code-block:: bash</p>\n<p>pip install --upgrade tensorflow-gpu==1.14.0</p>\n<p>To verify the installation, run</p>\n<p>.. code-block:: bash</p>\n<p>python -c \"import tensorflow as tf; sess=tf.Session(); print(sess.run(tf.reduce_sum(tf.random_normal([1000, 1000]))))\"</p>\n<p>One should remember to activate the virtual environment every time he/she uses deepmd-kit.</p>\n<p>Install the DeePMD-kit's python interface\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p>\n<p>Clone the DeePMD-kit source code</p>\n<p>.. code-block:: bash</p>\n<p>cd /some/workspace\ngit clone --recursive <a href=\"https://github.com/deepmodeling/deepmd-kit.git\" rel=\"nofollow\">https://github.com/deepmodeling/deepmd-kit.git</a> deepmd-kit -b devel</p>\n<p>If one downloads the .zip file from the github, then the default folder of source code would be <code>deepmd-kit-master</code> rather than <code>deepmd-kit</code>. For convenience, you may want to record the location of source to a variable, saying <code>deepmd_source_dir</code> by</p>\n<p>.. code-block:: bash</p>\n<p>cd deepmd-kit\ndeepmd_source_dir=<code>pwd</code></p>\n<p>Then execute</p>\n<p>.. code-block:: bash</p>\n<p>pip install .</p>\n<p>To test the installation, one may execute</p>\n<p>.. code-block:: bash</p>\n<p>dp -h</p>\n<p>It will print the help information like</p>\n<p>.. code-block:: text</p>\n<p>usage: dp [-h] {train,freeze,test} ...</p>\n<p>DeePMD-kit: A deep learning package for many-body potential energy\nrepresentation and molecular dynamics</p>\n<p>optional arguments:\n-h, --help           show this help message and exit</p>\n<p>Valid subcommands:\n{train,freeze,test}\ntrain              train a model\nfreeze             freeze the model\ntest               test the model</p>\n<h2>Install the C++ interface</h2>\n<p>If one does not need to use DeePMD-kit with Lammps or I-Pi, then the python interface installed in the previous section does everything and he/she can safely skip this section.</p>\n<p>Install the Tensorflow's C++ interface\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p>\n<p>It is highly recommended that one keeps the same C/C++ compiler as the python interface. The C++ interface of DeePMD-kit was tested with compiler gcc &gt;= 4.8. It is noticed that the I-Pi support is only compiled with gcc &gt;= 4.9.</p>\n<p>First the C++ interface of Tensorflow should be installed. It is noted that the version of Tensorflow should be in consistent with the python interface. We assume that you have followed our instruction and installed tensorflow python interface 1.14.0 with, then you may follow <code>the instruction for CPU &lt;doc/install-tf.1.14.md&gt;</code>_ to install the corresponding C++ interface (CPU only). If one wants GPU supports, he/she should follow <code>the instruction for GPU &lt;doc/install-tf.1.14-gpu.md&gt;</code>_ to install the C++ interface.</p>\n<p>Install the DeePMD-kit's C++ interface\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p>\n<p>Now goto the source code directory of DeePMD-kit and make a build place.</p>\n<p>.. code-block:: bash</p>\n<p>cd $deepmd_source_dir/source\nmkdir build\ncd build</p>\n<p>I assume you want to install DeePMD-kit into path <code>$deepmd_root</code>\\ , then execute cmake</p>\n<p>.. code-block:: bash</p>\n<p>cmake -DTENSORFLOW_ROOT=$tensorflow_root -DCMAKE_INSTALL_PREFIX=$deepmd_root ..</p>\n<p>where the variable <code>tensorflow_root</code> stores the location where the tensorflow's C++ interface is installed. The DeePMD-kit will automatically detect if a CUDA tool-kit is available on your machine and build the GPU support accordingly. If you want to force the cmake to find CUDA tool-kit, you can speicify the key <code>USE_CUDA_TOOLKIT</code>\\ ,</p>\n<p>.. code-block:: bash</p>\n<p>cmake -DUSE_CUDA_TOOLKIT=true -DTENSORFLOW_ROOT=$tensorflow_root -DCMAKE_INSTALL_PREFIX=$deepmd_root ..</p>\n<p>and you may further asked to provide <code>CUDA_TOOLKIT_ROOT_DIR</code>. If the cmake has executed successfully, then</p>\n<p>.. code-block:: bash</p>\n<p>make\nmake install</p>\n<p>If everything works fine, you will have the following executable and libraries installed in <code>$deepmd_root/bin</code> and <code>$deepmd_root/lib</code></p>\n<p>.. code-block:: bash</p>\n<p>$ ls $deepmd_root/bin\ndp_ipi\n$ ls $deepmd_root/lib\nlibdeepmd_ipi.so  libdeepmd_op.so  libdeepmd.so</p>\n<p>Install LAMMPS's DeePMD-kit module\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p>\n<p>DeePMD-kit provide module for running MD simulation with LAMMPS. Now make the DeePMD-kit module for LAMMPS.</p>\n<p>.. code-block:: bash</p>\n<p>cd $deepmd_source_dir/source/build\nmake lammps</p>\n<p>DeePMD-kit will generate a module called <code>USER-DEEPMD</code> in the <code>build</code> directory. Now download your favorite LAMMPS code, and uncompress it (I assume that you have downloaded the tar <code>lammps-stable.tar.gz</code>\\ )</p>\n<p>.. code-block:: bash</p>\n<p>cd /some/workspace\ntar xf lammps-stable.tar.gz</p>\n<p>The source code of LAMMPS is stored in directory, for example <code>lammps-31Mar17</code>. Now go into the LAMMPS code and copy the DeePMD-kit module like this</p>\n<p>.. code-block:: bash</p>\n<p>cd lammps-31Mar17/src/\ncp -r $deepmd_source_dir/source/build/USER-DEEPMD .</p>\n<p>Now build LAMMPS</p>\n<p>.. code-block:: bash</p>\n<p>make yes-user-deepmd\nmake mpi -j4</p>\n<p>The option <code>-j4</code> means using 4 processes in parallel. You may want to use a different number according to your hardware.</p>\n<p>If everything works fine, you will end up with an executable <code>lmp_mpi</code>.</p>\n<p>The DeePMD-kit module can be removed from LAMMPS source code by</p>\n<p>.. code-block:: bash</p>\n<p>make no-user-deepmd</p>\n<h1>Use DeePMD-kit</h1>\n<p>In this text, we will call the deep neural network that is used to represent the interatomic interactions (Deep Potential) the <strong>model</strong>. The typical procedure of using DeePMD-kit is</p>\n<p>#. Prepare data\n#. Train a model\n#. Freeze the model\n#. MD runs with the model (Native MD code or LAMMPS)</p>\n<h2>Prepare data</h2>\n<p>One needs to provide the following information to train a model: the atom type, the simulation box, the atom coordinate, the atom force, system energy and virial. A snapshot of a system that contains these information is called a <strong>frame</strong>. We use the following convention of units:</p>\n<p>.. list-table::\n:header-rows: 1</p>\n<ul>\n<li>\n<ul>\n<li>Property</li>\n<li>Unit</li>\n</ul>\n</li>\n<li>\n<ul>\n<li>Time</li>\n<li>ps</li>\n</ul>\n</li>\n<li>\n<ul>\n<li>Length</li>\n<li>\u00c5</li>\n</ul>\n</li>\n<li>\n<ul>\n<li>Energy</li>\n<li>eV</li>\n</ul>\n</li>\n<li>\n<ul>\n<li>Force</li>\n<li>eV/\u00c5</li>\n</ul>\n</li>\n<li>\n<ul>\n<li>Pressure</li>\n<li>Bar</li>\n</ul>\n</li>\n</ul>\n<p>The frames of the system are stored in two formats. A raw file is a plain text file with each information item written in one file and one frame written on one line. The default files that provide box, coordinate, force, energy and virial are <code>box.raw</code>\\ , <code>coord.raw</code>\\ , <code>force.raw</code>\\ , <code>energy.raw</code> and <code>virial.raw</code>\\ , respectively. <em>We recommend you use these file names</em>. Here is an example of force.raw:</p>\n<p>.. code-block:: bash</p>\n<p>$ cat force.raw\n-0.724  2.039 -0.951  0.841 -0.464  0.363\n6.737  1.554 -5.587 -2.803  0.062  2.222\n-1.968 -0.163  1.020 -0.225 -0.789  0.343</p>\n<p>This <code>force.raw</code> contains 3 frames with each frame having the forces of 2 atoms, thus it has 3 lines and 6 columns. Each line provides all the 3 force components of 2 atoms in 1 frame. The first three numbers are the 3 force components of the first atom, while the second three numbers are the 3 force components of the second atom. The coordinate file <code>coord.raw</code> is organized similarly. In <code>box.raw</code>\\ , the 9 components of the box vectors should be provided on each line. In <code>virial.raw</code>\\ , the 9 components of the virial tensor should be provided on each line. The number of lines of all raw files should be identical.</p>\n<p>We assume that the atom types do not change in all frames. It is provided by <code>type.raw</code>\\ , which has one line with the types of atoms written one by one. The atom types should be integers. For example the <code>type.raw</code> of a system that has 2 atoms with 0 and 1:</p>\n<p>.. code-block:: bash</p>\n<p>$ cat type.raw\n0 1</p>\n<p>The second format is the data sets of <code>numpy</code> binary data that are directly used by the training program. User can use the script <code>$deepmd_source_dir/data/raw/raw_to_set.sh</code> to convert the prepared raw files to data sets. For example, if we have a raw file that contains 6000 frames,</p>\n<p>.. code-block:: bash</p>\n<p>$ ls\nbox.raw  coord.raw  energy.raw  force.raw  type.raw  virial.raw\n$ $deepmd_source_dir/data/raw/raw_to_set.sh 2000\nnframe is 6000\nnline per set is 2000\nwill make 3 sets\nmaking set 0 ...\nmaking set 1 ...\nmaking set 2 ...\n$ ls\nbox.raw  coord.raw  energy.raw  force.raw  set.000  set.001  set.002  type.raw  virial.raw</p>\n<p>It generates three sets <code>set.000</code>\\ , <code>set.001</code> and <code>set.002</code>\\ , with each set contains 2000 frames. The last set (\\ <code>set.002</code>\\ ) is used as testing set, while the rest sets (\\ <code>set.000</code> and <code>set.001</code>\\ ) are used as training sets. One do not need to take care of the binary data files in each of the <code>set.*</code> directories. The path containing <code>set.*</code> and <code>type.raw</code> is called a <em>system</em>.</p>\n<h2>Train a model</h2>\n<p>Write the input script\n^^^^^^^^^^^^^^^^^^^^^^</p>\n<p>The method of training is explained in our <code>DeePMD &lt;https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.143001&gt;</code>_ and <code>DeepPot-SE &lt;https://arxiv.org/abs/1805.09003&gt;</code>_ papers. With the source code we provide a small training dataset taken from 400 frames generated by NVT ab-initio water MD trajectory with 300 frames for training and 100 for testing. <code>An example training parameter file &lt;./examples/water/train/water_se_a.json&gt;</code>_ is provided. One can try with the training by</p>\n<p>.. code-block:: bash</p>\n<p>$ cd $deepmd_source_dir/examples/water/train/\n$ dp train water_se_a.json</p>\n<p>where <code>water_se_a.json</code> is the <code>json</code> format parameter file that controls the training. The components of the <code>water.json</code> contains three parts, <code>model</code>\\ , <code>learning_rate</code>\\ , <code>loss</code> and <code>training</code>.</p>\n<p>The <code>model</code> section specify how the deep potential model is built. An example of the smooth-edition is provided as follows</p>\n<p>.. code-block:: json</p>\n<pre><code>   \"model\": {\n   \"type_map\": [\"O\", \"H\"],\n   \"descriptor\" :{\n       \"type\":     \"se_a\",\n       \"rcut_smth\":    5.80,\n       \"rcut\":     6.00,\n       \"sel\":      [46, 92],\n       \"neuron\":       [25, 50, 100],\n       \"axis_neuron\":  16,\n       \"resnet_dt\":    false,\n       \"seed\":     1,\n       \"_comment\":     \" that's all\"\n   },\n   \"fitting_net\" : {\n       \"neuron\":       [240, 240, 240],\n       \"resnet_dt\":    true,       \n       \"seed\":     1,\n       \"_comment\":     \" that's all\"\n   },\n   \"_comment\": \" that's all\"\n   }\n</code></pre>\n<p>The **\\ <code>type_map</code>\\ ** is optional, which provide the element names (but not restricted to) for corresponding atom types.</p>\n<p>The construction of the descriptor is given by option **\\ <code>descriptor</code>\\ **. The **\\ <code>type</code>\\ ** of the descriptor is set to <code>\"se_a\"</code>\\ , which means smooth-edition, angular infomation. The  **\\ <code>rcut</code>\\ ** is the cut-off radius for neighbor searching, and the **\\ <code>rcut_smth</code>\\ ** gives where the smoothing starts. **\\ <code>sel</code>\\ ** gives the maximum possible number of neighbors in the cut-off radius. It is a list, the length of which is the same as the number of atom types in the system, and <code>sel[i]</code> denote the maximum possible number of neighbors with type <code>i</code>. The **\\ <code>neuron</code>\\ ** specifies the size of the embedding net. From left to right the members denote the sizes of each hidden layers from input end to the output end, respectively. The **\\ <code>axis_neuron</code>\\ ** specifies the size of submatrix of the embedding matrix, the axis matrix as explained in the <code>DeepPot-SE paper &lt;https://arxiv.org/abs/1805.09003&gt;</code><em>. If the outer layer is of twice size as the inner layer, then the inner layer is copied and concatenated, then a <code>ResNet architecture &lt;https://arxiv.org/abs/1512.03385&gt;</code></em> is build between them. If the option **\\ <code>resnet_dt</code>\\ ** is set <code>true</code>\\ , then a timestep is used in the ResNet. **\\ <code>seed</code>\\ ** gives the random seed that is used to generate random numbers when initializing the model parameters.</p>\n<p>The construction of the fitting net is give by **\\ <code>fitting_net</code>\\ **. The key **\\ <code>neuron</code>\\ ** specifies the size of the fitting net. If two neighboring layers are of the same size, then a <code>ResNet architecture &lt;https://arxiv.org/abs/1512.03385&gt;</code>_ is build between them. If the option **\\ <code>resnet_dt</code>\\ ** is set <code>true</code>\\ , then a timestep is used in the ResNet. **\\ <code>seed</code>\\ ** gives the random seed that is used to generate random numbers when initializing the model parameters.</p>\n<p>An example of the <code>learning_rate</code> is given as follows</p>\n<p>.. code-block:: json</p>\n<pre><code>   \"learning_rate\" :{\n   \"type\":     \"exp\",\n   \"start_lr\": 0.005,\n   \"decay_steps\":  5000,\n   \"decay_rate\":   0.95,\n   \"_comment\": \"that's all\"\n   }\n</code></pre>\n<p>The option **\\ <code>start_lr</code>\\ **\\ , **\\ <code>decay_rate</code>\\ ** and **\\ <code>decay_steps</code>\\ ** specify how the learning rate changes. For example, the <code>t</code>\\ th batch will be trained with learning rate:</p>\n<p>.. math::</p>\n<p>lr(t) = start_lr * decay_rate ^ ( t / decay_steps )</p>\n<p>An example of the <code>loss</code> is</p>\n<p>.. code-block:: json</p>\n<pre><code>   \"loss\" : {\n   \"start_pref_e\": 0.02,\n   \"limit_pref_e\": 1,\n   \"start_pref_f\": 1000,\n   \"limit_pref_f\": 1,\n   \"start_pref_v\": 0,\n   \"limit_pref_v\": 0,\n   \"_comment\": \" that's all\"\n   }\n</code></pre>\n<p>The options **\\ <code>start_pref_e</code>\\ **\\ , **\\ <code>limit_pref_e</code>\\ **\\ , **\\ <code>start_pref_f</code>\\ **\\ , **\\ <code>limit_pref_f</code>\\ **\\ , **\\ <code>start_pref_v</code>\\ ** and **\\ <code>limit_pref_v</code>\\ ** determine how the prefactors of energy error, force error and virial error changes in the loss function (see the appendix of the <code>DeePMD paper &lt;https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.143001&gt;</code>_ for details). Taking the prefactor of force error for example, the prefactor at batch <code>t</code> is</p>\n<p>.. math::</p>\n<p>w_f(t) = start_pref_f * ( lr(t) / start_lr ) + limit_pref_f * ( 1 - lr(t) / start_lr )</p>\n<p>Since we do not have virial data, the virial prefactors <code>start_pref_v</code> and <code>limit_pref_v</code> are set to 0.</p>\n<p>An example of <code>training</code> is</p>\n<p>.. code-block:: json</p>\n<pre><code>   \"training\" : {\n   \"systems\":  [\"../data/\"],\n   \"set_prefix\":   \"set\",    \n   \"stop_batch\":   1000000,\n   \"batch_size\":   1,\n\n   \"seed\":     1,\n\n   \"_comment\": \" display and restart\",\n   \"_comment\": \" frequencies counted in batch\",\n   \"disp_file\":    \"lcurve.out\",\n   \"disp_freq\":    100,\n   \"numb_test\":    10,\n   \"save_freq\":    1000,\n   \"save_ckpt\":    \"model.ckpt\",\n   \"load_ckpt\":    \"model.ckpt\",\n   \"disp_training\":true,\n   \"time_training\":true,\n   \"profiling\":    false,\n   \"profiling_file\":\"timeline.json\",\n   \"_comment\": \"that's all\"\n   }\n</code></pre>\n<p>The option **\\ <code>systems</code>\\ ** provide location of the systems (path to <code>set.*</code> and <code>type.raw</code>\\ ). It is a vector, thus DeePMD-kit allows you to provide multiple systems. DeePMD-kit will train the model with the systems in the vector one by one in a cyclic manner. <strong>It is warned that the example water data (in folder <code>examples/data/water</code>\\ ) is of very limited amount, is provided only for testing purpose, and should not be used to train a productive model.</strong></p>\n<p>The option **\\ <code>batch_size</code>\\ ** specifies the number of frames in each batch. It can be set to <code>\"auto\"</code> to enable a automatic batch size.\nThe option **\\ <code>stop_batch</code>\\ ** specifies the total number of batches will be used in the training.</p>\n<p>Training\n^^^^^^^^</p>\n<p>The training can be invoked by</p>\n<p>.. code-block:: bash</p>\n<p>$ dp train water_se_a.json</p>\n<p>During the training, the error of the model is tested every **\\ <code>disp_freq</code>\\ ** batches with **\\ <code>numb_test</code>\\ ** frames from the last set in the **\\ <code>systems</code>\\ ** directory on the fly, and the results are output to **\\ <code>disp_file</code>\\ **. A typical <code>disp_file</code> looks like</p>\n<p>.. code-block:: bash</p>\n<h1>batch      l2_tst    l2_trn    l2_e_tst  l2_e_trn    l2_f_tst  l2_f_trn         lr</h1>\n<pre><code>     0    2.67e+01  2.57e+01    2.21e-01  2.22e-01    8.44e-01  8.12e-01    1.0e-03\n   100    6.14e+00  5.40e+00    3.01e-01  2.99e-01    1.93e-01  1.70e-01    1.0e-03\n   200    5.02e+00  4.49e+00    1.53e-01  1.53e-01    1.58e-01  1.42e-01    1.0e-03\n   300    4.36e+00  3.71e+00    7.32e-02  7.27e-02    1.38e-01  1.17e-01    1.0e-03\n   400    4.04e+00  3.29e+00    3.16e-02  3.22e-02    1.28e-01  1.04e-01    1.0e-03\n</code></pre>\n<p>The first column displays the number of batches. The second and third columns display the loss function evaluated by <code>numb_test</code> frames randomly chosen from the test set and that evaluated by the current training batch, respectively. The fourth and fifth columns display the RMS energy error (normalized by number of atoms) evaluated by <code>numb_test</code> frames randomly chosen from the test set and that evaluated by the current training batch, respectively. The sixth and seventh columns display the RMS force error (component-wise) evaluated by <code>numb_test</code> frames randomly chosen from the test set and that evaluated by the current training batch, respectively. The last column displays the current learning rate.</p>\n<p>Checkpoints will be written to files with prefix **\\ <code>save_ckpt</code>\\ ** every **\\ <code>save_freq</code>\\ ** batches. If **\\ <code>restart</code>\\ ** is set to <code>true</code>\\ , then the training will start from the checkpoint named **\\ <code>load_ckpt</code>\\ **\\ , rather than from scratch.</p>\n<p>Several command line options can be passed to <code>dp train</code>\\ , which can be checked with</p>\n<p>.. code-block:: bash</p>\n<p>$ dp train --help</p>\n<p>An explanation will be provided</p>\n<p>.. code-block::</p>\n<p>positional arguments:\nINPUT                 the input json database</p>\n<p>optional arguments:\n-h, --help            show this help message and exit\n-t INTER_THREADS, --inter-threads INTER_THREADS\nWith default value 0. Setting the \"inter_op_parallelism_threads\" key for the tensorflow, the \"intra_op_parallelism_threads\" will be set by the env variable OMP_NUM_THREADS\n--init-model INIT_MODEL\nInitialize a model by the provided checkpoint\n--restart RESTART     Restart the training from the provided checkpoint</p>\n<p>The keys <code>intra_op_parallelism_threads</code> and <code>inter_op_parallelism_threads</code> are Tensorflow configurations for multithreading, which are explained <code>here &lt;https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu&gt;</code>_. Skipping <code>-t</code> and <code>OMP_NUM_THREADS</code> leads to the default setting of these keys in the Tensorflow.</p>\n<p>**\\ <code>--init-model model.ckpt</code>\\ **\\ , for example, initializes the model training with an existing model that is stored in the checkpoint <code>model.ckpt</code>\\ , the network architectures should match.</p>\n<p>**\\ <code>--restart model.ckpt</code>\\ **\\ , continues the training from the checkpoint <code>model.ckpt</code>.</p>\n<h2>Freeze a model</h2>\n<p>The trained neural network is extracted from a checkpoint and dumped into a database. This process is called \"freezing\" a model. The idea and part of our code are from <code>Morgan &lt;https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc&gt;</code>_. To freeze a model, typically one does</p>\n<p>.. code-block:: bash</p>\n<p>$ dp freeze -o graph.pb</p>\n<p>in the folder where the model is trained. The output database is called <code>graph.pb</code>.</p>\n<h2>Test a model</h2>\n<p>The frozen model can be used in many ways. The most straightforward test can be performed using <code>dp test</code>. A typical usage of <code>dp test</code> is</p>\n<p>.. code-block:: bash</p>\n<p>dp test -m graph.pb -s /path/to/system -n 30</p>\n<p>where <code>-m</code> gives the tested model, <code>-s</code> the path to the tested system and <code>-n</code> the number of tested frames. Several other command line options can be passed to <code>dp test</code>\\ , which can be checked with</p>\n<p>.. code-block:: bash</p>\n<p>$ dp test --help</p>\n<p>An explanation will be provided</p>\n<p>.. code-block::</p>\n<p>usage: dp test [-h] [-m MODEL] [-s SYSTEM] [-S SET_PREFIX] [-n NUMB_TEST]\n[-r RAND_SEED] [--shuffle-test] [-d DETAIL_FILE]</p>\n<p>optional arguments:\n-h, --help            show this help message and exit\n-m MODEL, --model MODEL\nFrozen model file to import\n-s SYSTEM, --system SYSTEM\nThe system dir\n-S SET_PREFIX, --set-prefix SET_PREFIX\nThe set prefix\n-n NUMB_TEST, --numb-test NUMB_TEST\nThe number of data for test\n-r RAND_SEED, --rand-seed RAND_SEED\nThe random seed\n--shuffle-test        Shuffle test data\n-d DETAIL_FILE, --detail-file DETAIL_FILE\nThe file containing details of energy force and virial\naccuracy</p>\n<h2>Model inference</h2>\n<p>One may use the python interface of DeePMD-kit for model inference, an example is given as follows</p>\n<p>.. code-block:: python</p>\n<p>import deepmd.DeepPot as DP\nimport numpy as np\ndp = DP('graph.pb')\ncoord = np.array([[1,0,0], [0,0,1.5], [1,0,3]]).reshape([1, -1])\ncell = np.diag(10 * np.ones(3)).reshape([1, -1])\natype = [1,0,1]\ne, f, v = dp.eval(coord, cell, atype)</p>\n<p>where <code>e</code>\\ , <code>f</code> and <code>v</code> are predicted energy, force and virial of the system, respectively.</p>\n<h2>Run MD with LAMMPS</h2>\n<p>Include deepmd in the pair style\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p>\n<p>Running an MD simulation with LAMMPS is simpler. In the LAMMPS input file, one needs to specify the pair style as follows</p>\n<p>.. code-block:: bash</p>\n<p>pair_style     deepmd graph.pb\npair_coeff</p>\n<p>where <code>graph.pb</code> is the file name of the frozen model. The <code>pair_coeff</code> should be left blank. It should be noted that LAMMPS counts atom types starting from 1, therefore, all LAMMPS atom type will be firstly subtracted by 1, and then passed into the DeePMD-kit engine to compute the interactions. <code>A detailed documentation of this pair style is available. &lt;doc/lammps-pair-style-deepmd.md&gt;</code>_.</p>\n<p>Long-range interaction\n^^^^^^^^^^^^^^^^^^^^^^</p>\n<p>The reciprocal space part of the long-range interaction can be calculated by LAMMPS command <code>kspace_style</code>. To use it with DeePMD-kit, one writes</p>\n<p>.. code-block:: bash</p>\n<p>pair_style  deepmd graph.pb\npair_coeff\nkspace_style    pppm 1.0e-5\nkspace_modify   gewald 0.45</p>\n<p>Please notice that the DeePMD does nothing to the direct space part of the electrostatic interaction, because this part is assumed to be fitted in the DeePMD model (the direct space cut-off is thus the cut-off of the DeePMD model). The splitting parameter <code>gewald</code> is modified by the <code>kspace_modify</code> command.</p>\n<h2>Run path-integral MD with i-PI</h2>\n<p>The i-PI works in a client-server model. The i-PI provides the server for integrating the replica positions of atoms, while the DeePMD-kit provides a client named <code>dp_ipi</code> that computes the interactions (including energy, force and virial). The server and client communicates via the Unix domain socket or the Internet socket. The client can be started by</p>\n<p>.. code-block:: bash</p>\n<p>$ dp_ipi water.json</p>\n<p>It is noted that multiple instances of the client is allow for computing, in parallel, the interactions of multiple replica of the path-integral MD.</p>\n<p><code>water.json</code> is the parameter file for the client <code>dp_ipi</code>\\ , and <code>an example &lt;./examples/ipi/water.json&gt;</code>_ is provided:</p>\n<p>.. code-block:: json</p>\n<p>{\n\"verbose\":      false,\n\"use_unix\":     true,\n\"port\":     31415,\n\"host\":     \"localhost\",\n\"graph_file\":   \"graph.pb\",\n\"coord_file\":   \"conf.xyz\",\n\"atom_type\" : {\n\"OW\":       0,\n\"HW1\":      1,\n\"HW2\":      1\n}\n}</p>\n<p>The option **\\ <code>use_unix</code>\\ ** is set to <code>true</code> to activate the Unix domain socket, otherwise, the Internet socket is used.</p>\n<p>The option **\\ <code>graph_file</code>\\ ** provides the file name of the frozen model.</p>\n<p>The <code>dp_ipi</code> gets the atom names from an <code>XYZ file &lt;https://en.wikipedia.org/wiki/XYZ_file_format&gt;</code>_ provided by **\\ <code>coord_file</code>\\ ** (meanwhile ignores all coordinates in it), and translates the names to atom types by rules provided by **\\ <code>atom_type</code>\\ **.</p>\n<h2>Use deep potential with ASE</h2>\n<p>Deep potential can be set up as a calculator with ASE to obtain potential energies and forces.</p>\n<p>.. code-block:: python</p>\n<p>from ase import Atoms\nfrom deepmd.calculator import DP</p>\n<p>water = Atoms('H2O',\npositions=[(0.7601, 1.9270, 1),\n(1.9575, 1, 1),\n(1., 1., 1.)],\ncell=[100, 100, 100],\ncalculator=DP(model=\"frozen_model.pb\"))\nprint(water.get_potential_energy())\nprint(water.get_forces())</p>\n<p>Optimization is also available:</p>\n<p>.. code-block:: python</p>\n<p>from ase.optimize import BFGS\ndyn = BFGS(water)\ndyn.run(fmax=1e-6)\nprint(water.get_positions())</p>\n<h1>Troubleshooting</h1>\n<p>In consequence of various differences of computers or systems, problems may occur. Some common circumstances are listed as follows.\nIf other unexpected problems occur, you're welcome to contact us for help.</p>\n<h2>Model compatability</h2>\n<p>When the version of DeePMD-kit used to training model is different from the that of DeePMD-kit running MDs, one has the problem of model compatability.</p>\n<p>DeePMD-kit guarantees that the codes with the same major and minor revisions are compatible. That is to say v0.12.5 is compatible to v0.12.0, but is not compatible to v0.11.0 nor v1.0.0.</p>\n<h2>Installation: inadequate versions of gcc/g++</h2>\n<p>Sometimes you may use a gcc/g++ of version &lt;4.9. If you have a gcc/g++ of version &gt; 4.9, say, 7.2.0, you may choose to use it by doing</p>\n<p>.. code-block:: bash</p>\n<p>export CC=/path/to/gcc-7.2.0/bin/gcc\nexport CXX=/path/to/gcc-7.2.0/bin/g++</p>\n<p>If, for any reason, for example, you only have a gcc/g++ of version 4.8.5, you can still compile all the parts of TensorFlow and most of the parts of DeePMD-kit. i-Pi will be disabled automatically.</p>\n<h2>Installation: build files left in DeePMD-kit</h2>\n<p>When you try to build a second time when installing DeePMD-kit, files produced before may contribute to failure. Thus, you may clear them by</p>\n<p>.. code-block:: bash</p>\n<p>cd build\nrm -r *</p>\n<p>and redo the <code>cmake</code> process.</p>\n<h2>Training: TensorFlow abi binary cannot be found when doing training</h2>\n<p>If you confront such kind of error:</p>\n<p>.. code-block::</p>\n<p>$deepmd_root/lib/deepmd/libop_abi.so: undefined symbol:\n_ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringB5cxx11Ev</p>\n<p>This may happen if you are using a gcc &gt;= 5.0, and tensorflow was compiled with gcc &lt; 5.0. You may set <code>-DOP_CXX_ABI=0</code> in the process of <code>cmake</code>.</p>\n<p>Another possible reason might be the large gap between the python version of TensorFlow and the TensorFlow c++ interface.</p>\n<h2>MD: cannot run LAMMPS after installing a new version of DeePMD-kit</h2>\n<p>This typically happens when you install a new version of DeePMD-kit and copy directly the generated <code>USER-DEEPMD</code> to a LAMMPS source code folder and re-install LAMMPS.</p>\n<p>To solve this problem, it suffices to first remove <code>USER-DEEPMD</code> from LAMMPS source code by</p>\n<p>.. code-block:: bash</p>\n<p>make no-user-deepmd</p>\n<p>and then install the new <code>USER-DEEPMD</code>.</p>\n<p>If this does not solve your problem, try to decompress the LAMMPS source tarball and install LAMMPS from scratch again, which typically should be very fast.</p>\n\n          </div>"}, "last_serial": 6748724, "releases": {"1.1.2": [{"comment_text": "", "digests": {"md5": "60432afad76839efffacc0304cae6bc3", "sha256": "f54038f5b16911b3cd408301d3a49e22125c55ec90d056be4b6c81664b5c6312"}, "downloads": -1, "filename": "deepmd_kit-1.1.2-cp36-cp36m-manylinux2010_x86_64.whl", "has_sig": false, "md5_digest": "60432afad76839efffacc0304cae6bc3", "packagetype": "bdist_wheel", "python_version": "cp36", "requires_python": ">=3.6", "size": 323473, "upload_time": "2020-01-18T03:41:54", "upload_time_iso_8601": "2020-01-18T03:41:54.293780Z", "url": "https://files.pythonhosted.org/packages/83/09/e81d6b77726c4fee28deba7628e1a8e4afb4d0b36141ff32abae0f5731c1/deepmd_kit-1.1.2-cp36-cp36m-manylinux2010_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a4c56cac475e3ef9f6cbbca7693b267c", "sha256": "fd2e934ea3988c65e959242beaa08282da99cf390edd266664bb890deee8540f"}, "downloads": -1, "filename": "deepmd_kit-1.1.2-cp37-cp37m-manylinux2010_x86_64.whl", "has_sig": false, "md5_digest": "a4c56cac475e3ef9f6cbbca7693b267c", "packagetype": "bdist_wheel", "python_version": "cp37", "requires_python": ">=3.6", "size": 323474, "upload_time": "2020-01-18T03:41:56", "upload_time_iso_8601": "2020-01-18T03:41:56.023711Z", "url": "https://files.pythonhosted.org/packages/57/df/5eb6201f60b3c2469df2109490a805571d9314214e8d02a4deb8e73cf551/deepmd_kit-1.1.2-cp37-cp37m-manylinux2010_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "58274027f4f7d031197f00637bfd9007", "sha256": "d0b09a19ba54a4667fe5b887cc88cf4ab603db408a2e073bb84a6c591bec870e"}, "downloads": -1, "filename": "deepmd-kit-1.1.2.tar.gz", "has_sig": false, "md5_digest": "58274027f4f7d031197f00637bfd9007", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 2697386, "upload_time": "2020-01-01T00:49:50", "upload_time_iso_8601": "2020-01-01T00:49:50.978111Z", "url": "https://files.pythonhosted.org/packages/55/e9/d7821e05adbe523f9c4ff0913d17eb2aa71bbc4bc152c40684c5f4b79000/deepmd-kit-1.1.2.tar.gz", "yanked": false}], "1.1.3": [{"comment_text": "", "digests": {"md5": "9b5fcc07ee9e5618dba385006cefab93", "sha256": "62a0eb905542614d288ed17f3a3c62bbbfff12a743ac4fe507eeaad6e64e9a93"}, "downloads": -1, "filename": "deepmd_kit-1.1.3-cp36-cp36m-manylinux2010_x86_64.whl", "has_sig": false, "md5_digest": "9b5fcc07ee9e5618dba385006cefab93", "packagetype": "bdist_wheel", "python_version": "cp36", "requires_python": ">=3.6", "size": 323473, "upload_time": "2020-01-24T00:11:32", "upload_time_iso_8601": "2020-01-24T00:11:32.796252Z", "url": "https://files.pythonhosted.org/packages/ba/fb/46b99c3d3db8da8f6f5c92ccb17d0383525a0b4522b7cf0c12cf6fbc33c0/deepmd_kit-1.1.3-cp36-cp36m-manylinux2010_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7a19455122be780660efbbd6ac6f0c24", "sha256": "b4d3f47865a2d7973c885680ff1b92bb2e9a5e146485f17dbd7426220fc97dc4"}, "downloads": -1, "filename": "deepmd_kit-1.1.3-cp37-cp37m-manylinux2010_x86_64.whl", "has_sig": false, "md5_digest": "7a19455122be780660efbbd6ac6f0c24", "packagetype": "bdist_wheel", "python_version": "cp37", "requires_python": ">=3.6", "size": 323471, "upload_time": "2020-01-24T00:11:34", "upload_time_iso_8601": "2020-01-24T00:11:34.669388Z", "url": "https://files.pythonhosted.org/packages/ac/82/fbe93f52a46534ecaaafd3a7abfc2533fb722fea44dbfb02168fbe135a74/deepmd_kit-1.1.3-cp37-cp37m-manylinux2010_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "080c275e8224b2a42c862270c4b195a3", "sha256": "9722e70ad13a22114e40880eb186d6602e3aec0ffb93a0e4d364bb8d0c17f29e"}, "downloads": -1, "filename": "deepmd-kit-1.1.3.tar.gz", "has_sig": false, "md5_digest": "080c275e8224b2a42c862270c4b195a3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 2716097, "upload_time": "2020-01-24T00:11:37", "upload_time_iso_8601": "2020-01-24T00:11:37.071431Z", "url": "https://files.pythonhosted.org/packages/d9/50/bb2151f34b65c3cfd65e715cb624182be8482e719a952423dcdb50d5144d/deepmd-kit-1.1.3.tar.gz", "yanked": false}], "1.1.4": [{"comment_text": "", "digests": {"md5": "51537e524b409258868bea143955c2da", "sha256": "6f95c48ae5c1cdcb356cb0fc741ae9e2d651c1ec30f0f498506a0c11f583bd0f"}, "downloads": -1, "filename": "deepmd_kit-1.1.4-cp36-cp36m-manylinux2010_x86_64.whl", "has_sig": false, "md5_digest": "51537e524b409258868bea143955c2da", "packagetype": "bdist_wheel", "python_version": "cp36", "requires_python": ">=3.6", "size": 323613, "upload_time": "2020-03-04T15:57:29", "upload_time_iso_8601": "2020-03-04T15:57:29.078920Z", "url": "https://files.pythonhosted.org/packages/57/38/0e4197743088a18d6287dcd3059dc91163d20d96a28ed78fdbbebdbc17c1/deepmd_kit-1.1.4-cp36-cp36m-manylinux2010_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f6b0cc9803236a0268f960f068e088b1", "sha256": "7f11fb86469243106ab124c43f7708ce234536009bd362959e0a68bba797d281"}, "downloads": -1, "filename": "deepmd_kit-1.1.4-cp37-cp37m-manylinux2010_x86_64.whl", "has_sig": false, "md5_digest": "f6b0cc9803236a0268f960f068e088b1", "packagetype": "bdist_wheel", "python_version": "cp37", "requires_python": ">=3.6", "size": 323612, "upload_time": "2020-03-04T15:57:30", "upload_time_iso_8601": "2020-03-04T15:57:30.281653Z", "url": "https://files.pythonhosted.org/packages/e8/a4/5ca3f37dc6add459335c31f01568dcf6d05fd31bca689b9f8d447a9bb834/deepmd_kit-1.1.4-cp37-cp37m-manylinux2010_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d6cd662645dc3e8ee46e7b38e61db051", "sha256": "d1d319179b9590b556929c3e8181e66be7d27d5f521437464d0c24893d29f38d"}, "downloads": -1, "filename": "deepmd-kit-1.1.4.tar.gz", "has_sig": false, "md5_digest": "d6cd662645dc3e8ee46e7b38e61db051", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 2716263, "upload_time": "2020-03-04T15:57:32", "upload_time_iso_8601": "2020-03-04T15:57:32.173563Z", "url": "https://files.pythonhosted.org/packages/ce/7c/da59de1fafff019134ad55724d87b2084ff04d77e052df6191a2784041ba/deepmd-kit-1.1.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "51537e524b409258868bea143955c2da", "sha256": "6f95c48ae5c1cdcb356cb0fc741ae9e2d651c1ec30f0f498506a0c11f583bd0f"}, "downloads": -1, "filename": "deepmd_kit-1.1.4-cp36-cp36m-manylinux2010_x86_64.whl", "has_sig": false, "md5_digest": "51537e524b409258868bea143955c2da", "packagetype": "bdist_wheel", "python_version": "cp36", "requires_python": ">=3.6", "size": 323613, "upload_time": "2020-03-04T15:57:29", "upload_time_iso_8601": "2020-03-04T15:57:29.078920Z", "url": "https://files.pythonhosted.org/packages/57/38/0e4197743088a18d6287dcd3059dc91163d20d96a28ed78fdbbebdbc17c1/deepmd_kit-1.1.4-cp36-cp36m-manylinux2010_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f6b0cc9803236a0268f960f068e088b1", "sha256": "7f11fb86469243106ab124c43f7708ce234536009bd362959e0a68bba797d281"}, "downloads": -1, "filename": "deepmd_kit-1.1.4-cp37-cp37m-manylinux2010_x86_64.whl", "has_sig": false, "md5_digest": "f6b0cc9803236a0268f960f068e088b1", "packagetype": "bdist_wheel", "python_version": "cp37", "requires_python": ">=3.6", "size": 323612, "upload_time": "2020-03-04T15:57:30", "upload_time_iso_8601": "2020-03-04T15:57:30.281653Z", "url": "https://files.pythonhosted.org/packages/e8/a4/5ca3f37dc6add459335c31f01568dcf6d05fd31bca689b9f8d447a9bb834/deepmd_kit-1.1.4-cp37-cp37m-manylinux2010_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d6cd662645dc3e8ee46e7b38e61db051", "sha256": "d1d319179b9590b556929c3e8181e66be7d27d5f521437464d0c24893d29f38d"}, "downloads": -1, "filename": "deepmd-kit-1.1.4.tar.gz", "has_sig": false, "md5_digest": "d6cd662645dc3e8ee46e7b38e61db051", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 2716263, "upload_time": "2020-03-04T15:57:32", "upload_time_iso_8601": "2020-03-04T15:57:32.173563Z", "url": "https://files.pythonhosted.org/packages/ce/7c/da59de1fafff019134ad55724d87b2084ff04d77e052df6191a2784041ba/deepmd-kit-1.1.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:39:23 2020"}