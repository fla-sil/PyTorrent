{"info": {"author": "Mara contributors", "author_email": "", "bugtrack_url": null, "classifiers": [], "description": "# Mara ETL Tools\n\n[![Build Status](https://travis-ci.org/mara/mara-etl-tools.svg?branch=master)](https://travis-ci.org/mara/mara-etl-tools)\n[![PyPI - License](https://img.shields.io/pypi/l/mara-etl-tools.svg)](https://github.com/mara/mara-etl-tools/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/mara-etl-tools.svg)](https://badge.fury.io/py/mara-etl-tools)\n[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://communityinviter.com/apps/mara-users/public-invite)\n\nA collection of utilities around [Project A](https://project-a.com/)'s best practices for creating [data integration](https://github.com/mara/data-integration) pipelines with mara. The package is intended as a start for new projects. Forks/ copies are preferred over PRs.\n\nFor more details on how to use this package, have a look at the [mara example project](https://github.com/mara/mara-example-project).\n\n\nThe package consists of a number modules that all can be used independently from each other:\n\n## SQL utility functions\n\nFunction `initialize_utils` in [etl_tools/initialize_utils/__init__.py](etl_tools/initialize_utils/__init__.py) returns a pipeline that creates a `util` schema with a number of PostgreSQL functions for organizing data pipelines. Add to your root pipeline like this:\n\n```python\nfrom etl_tools import initialize_utils\n\nmy_pipeline.add(initialize_utils.utils_pipeline(with_hll=True, with_cstore_fdw=True))\n```\n\nPlease have a look at the .sql files in [etl_tools/initialize_utils](etl_tools/initialize_utils) for available functions.\n\n\n## Schema copying\n\nThe file The file [etl_tools/schema_copying.py](etl_tools/schema_copying.py) contains the function `add_schema_copying_to_pipeline` that copies a PostgreSQL database schema from on host to another at the end of a pipeline run. This is useful for running the ETL and frontend tools on different database servers so that a running ETL does not affect the performance of dashboard queries.\n\n\nGiven that there is a pipline `my_pipeline` that has a number of child pipelines with the `Schema` label set to the respective schema to copy, then this is how the schema copying can be added to those child pipelines.\n\n```python\nfrom mara_db import dbs\nfrom data_integration.commands.sql import ExecuteSQL\nfrom data_integration.pipelines import Task\nfrom etl_tools.schema_copying import add_schema_copying_to_pipeline\n\n# when etl und frontend db are different, add schema copying\nif dbs.db('mdwh-etl').database != dbs.db('mdwh-frontend').database \\\n        or dbs.db('mdwh-etl').host != dbs.db('mdwh-frontend').host:\n\n    # run some of the files from etl_tools/initalize_utils in frontend db\n    initialize_frontend_db_commands = [ExecuteSQL(\n        sql_statement=\"DROP SCHEMA IF EXISTS util CASCADE; CREATE SCHEMA util;\", db_alias='mdwh-frontend')]\n\n    for file_name in ['schema_switching.sql', 'data_sets.sql', 'hll.sql', 'cstore_fdw.sql']:\n        initialize_frontend_db_commands.append(\n            ExecuteSQL(sql_file_name=str(\n                my_pipeline.nodes['utils'].nodes['initialize_utils'].base_path() / file_name),\n                db_alias='mdwh-frontend'))\n\n    my_pipeline.nodes['utils'].add(\n        Task(id='initialize_frontend_db',\n             description='Adds some functions to the frontend db so that schema copying works',\n             commands=initialize_frontend_db_commands))\n\n    # Add schema copying for time schema\n    add_schema_copying_to_pipeline(pipeline=my_pipeline.nodes['utils'].nodes['create_time_dimensions'],\n                                   schema_name='time',\n                                   source_db_alias='dwh-etl', target_db_alias='dwh-frontend')\n\n    # Add schema copying to all root pipelines\n    for pipeline in my_pipeline.nodes.values():\n        if \"Schema\" in pipeline.labels:\n            schema = pipeline.labels['Schema']\n            add_schema_copying_to_pipeline(pipeline=pipeline, schema_name=schema + '_next',\n                                           source_db_alias='dwh-etl', target_db_alias='dwh-frontend')\n            pipeline.final_node.commands_after.append(\n                ExecuteSQL(sql_statement=f\"SELECT util.replace_schema('{schema}', '{schema}_next')\",\n                           db_alias='mdwh-frontend')\n            )\n```\n \n\n## Time dimensions\n\nThe file [etl_tools/create_time_dimensions/__init__.py](etl_tools/create_time_dimensions/__init__.py) defines a pipeline that creates and updates `time` schema with the tables `day` and `duration`:\n\n```\nselect * from time.day order by _date desc limit 10;\n     day_id  |     day_name     | year_id | iso_year_id | quarter_id | quarter_name | month_id | month_name | week_id |  week_name   | day_of_week_id | day_of_week_name | day_of_month_id |   _date    \n   ----------+------------------+---------+-------------+------------+--------------+----------+------------+---------+--------------+----------------+------------------+-----------------+------------\n    20190815 | Thu, Aug 15 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201933 | 2019 - CW 33 |              4 | Thursday         |              15 | 2019-08-15\n    20190814 | Wed, Aug 14 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201933 | 2019 - CW 33 |              3 | Wednesday        |              14 | 2019-08-14\n    20190813 | Tue, Aug 13 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201933 | 2019 - CW 33 |              2 | Tuesday          |              13 | 2019-08-13\n    20190812 | Mon, Aug 12 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201933 | 2019 - CW 33 |              1 | Monday           |              12 | 2019-08-12\n    20190811 | Sun, Aug 11 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201932 | 2019 - CW 32 |              7 | Sunday           |              11 | 2019-08-11\n    20190810 | Sat, Aug 10 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201932 | 2019 - CW 32 |              6 | Saturday         |              10 | 2019-08-10\n    20190809 | Fri, Aug 09 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201932 | 2019 - CW 32 |              5 | Friday           |               9 | 2019-08-09\n    20190808 | Thu, Aug 08 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201932 | 2019 - CW 32 |              4 | Thursday         |               8 | 2019-08-08\n    20190807 | Wed, Aug 07 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201932 | 2019 - CW 32 |              3 | Wednesday        |               7 | 2019-08-07\n    20190806 | Tue, Aug 06 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201932 | 2019 - CW 32 |              2 | Tuesday          |               6 | 2019-08-06\n```\n\n```\nselect * from time.duration where duration_id >= 0 order by duration_id limit 10;\n duration_id | days | days_name | weeks | weeks_name | four_weeks | four_weeks_name | months | months_name | sixth_years | sixth_years_name | half_years | half_years_name | years | years_name \n-------------+------+-----------+-------+------------+------------+-----------------+--------+-------------+-------------+------------------+------------+-----------------+-------+------------\n           0 |    0 | 0 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           1 |    1 | 1 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           2 |    2 | 2 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           3 |    3 | 3 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           4 |    4 | 4 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           5 |    5 | 5 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           6 |    6 | 6 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           7 |    7 | 7 days    |     1 | 7-13 days  |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           8 |    8 | 8 days    |     1 | 7-13 days  |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           9 |    9 | 9 days    |     1 | 7-13 days  |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n\n```\n\nAdd the pipeline to your project with \n\n```bash\nfrom etl_tools import create_time_dimensions\n\nmy_pipeline.add(create_time_dimensions.pipeline)\n```\n\nSet min and max dates by overwriting the `first_date_in_time_dimensions` and `last_date_in_time_dimensions` in [etl_tools/config.py](etl_tools/config.py).\n\n\n## Euro currency exchange rates\n\nThe file [etl_tools/load_euro_exchange_rates/__init__.py](etl_tools/create_time_dimensions/__init__.py) contains a pipeline that loads (historic) Euro exchange rates from the European central bank. \n\n\nAdd to your pipeline with \n\n```bash\nfrom etl_tools import load_euro_exchange_rates\n\nmy_pipeline.add(load_euro_exchange_rates.euro_exchange_rates_pipeline('db-alias'))\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/mara/mara-etl-tools", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "mara-etl-tools", "package_url": "https://pypi.org/project/mara-etl-tools/", "platform": "", "project_url": "https://pypi.org/project/mara-etl-tools/", "project_urls": {"Homepage": "https://github.com/mara/mara-etl-tools"}, "release_url": "https://pypi.org/project/mara-etl-tools/3.1.0/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Utilities for creating ETL pipelines with mara", "version": "3.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Mara ETL Tools</h1>\n<p><a href=\"https://travis-ci.org/mara/mara-etl-tools\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fd406d628df53f792045fcf9aa15d91dd4acac4a/68747470733a2f2f7472617669732d63692e6f72672f6d6172612f6d6172612d65746c2d746f6f6c732e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://github.com/mara/mara-etl-tools/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"PyPI - License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a8cb70df4f35900a560554ec1c5e4876ac75c4c2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f6d6172612d65746c2d746f6f6c732e737667\"></a>\n<a href=\"https://badge.fury.io/py/mara-etl-tools\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0a05d800f3ec6f7e778a052831812f20f2d87df5/68747470733a2f2f62616467652e667572792e696f2f70792f6d6172612d65746c2d746f6f6c732e737667\"></a>\n<a href=\"https://communityinviter.com/apps/mara-users/public-invite\" rel=\"nofollow\"><img alt=\"Slack Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8c01bf4f74a7c882bcfcd47e4924618eb70cfcd6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736c61636b2d6a6f696e5f636861742d77686974652e7376673f6c6f676f3d736c61636b267374796c653d736f6369616c\"></a></p>\n<p>A collection of utilities around <a href=\"https://project-a.com/\" rel=\"nofollow\">Project A</a>'s best practices for creating <a href=\"https://github.com/mara/data-integration\" rel=\"nofollow\">data integration</a> pipelines with mara. The package is intended as a start for new projects. Forks/ copies are preferred over PRs.</p>\n<p>For more details on how to use this package, have a look at the <a href=\"https://github.com/mara/mara-example-project\" rel=\"nofollow\">mara example project</a>.</p>\n<p>The package consists of a number modules that all can be used independently from each other:</p>\n<h2>SQL utility functions</h2>\n<p>Function <code>initialize_utils</code> in <a href=\"etl_tools/initialize_utils/__init__.py\" rel=\"nofollow\">etl_tools/initialize_utils/<strong>init</strong>.py</a> returns a pipeline that creates a <code>util</code> schema with a number of PostgreSQL functions for organizing data pipelines. Add to your root pipeline like this:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">etl_tools</span> <span class=\"kn\">import</span> <span class=\"n\">initialize_utils</span>\n\n<span class=\"n\">my_pipeline</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">initialize_utils</span><span class=\"o\">.</span><span class=\"n\">utils_pipeline</span><span class=\"p\">(</span><span class=\"n\">with_hll</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">with_cstore_fdw</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">))</span>\n</pre>\n<p>Please have a look at the .sql files in <a href=\"etl_tools/initialize_utils\" rel=\"nofollow\">etl_tools/initialize_utils</a> for available functions.</p>\n<h2>Schema copying</h2>\n<p>The file The file <a href=\"etl_tools/schema_copying.py\" rel=\"nofollow\">etl_tools/schema_copying.py</a> contains the function <code>add_schema_copying_to_pipeline</code> that copies a PostgreSQL database schema from on host to another at the end of a pipeline run. This is useful for running the ETL and frontend tools on different database servers so that a running ETL does not affect the performance of dashboard queries.</p>\n<p>Given that there is a pipline <code>my_pipeline</code> that has a number of child pipelines with the <code>Schema</code> label set to the respective schema to copy, then this is how the schema copying can be added to those child pipelines.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">mara_db</span> <span class=\"kn\">import</span> <span class=\"n\">dbs</span>\n<span class=\"kn\">from</span> <span class=\"nn\">data_integration.commands.sql</span> <span class=\"kn\">import</span> <span class=\"n\">ExecuteSQL</span>\n<span class=\"kn\">from</span> <span class=\"nn\">data_integration.pipelines</span> <span class=\"kn\">import</span> <span class=\"n\">Task</span>\n<span class=\"kn\">from</span> <span class=\"nn\">etl_tools.schema_copying</span> <span class=\"kn\">import</span> <span class=\"n\">add_schema_copying_to_pipeline</span>\n\n<span class=\"c1\"># when etl und frontend db are different, add schema copying</span>\n<span class=\"k\">if</span> <span class=\"n\">dbs</span><span class=\"o\">.</span><span class=\"n\">db</span><span class=\"p\">(</span><span class=\"s1\">'mdwh-etl'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">database</span> <span class=\"o\">!=</span> <span class=\"n\">dbs</span><span class=\"o\">.</span><span class=\"n\">db</span><span class=\"p\">(</span><span class=\"s1\">'mdwh-frontend'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">database</span> \\\n        <span class=\"ow\">or</span> <span class=\"n\">dbs</span><span class=\"o\">.</span><span class=\"n\">db</span><span class=\"p\">(</span><span class=\"s1\">'mdwh-etl'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">host</span> <span class=\"o\">!=</span> <span class=\"n\">dbs</span><span class=\"o\">.</span><span class=\"n\">db</span><span class=\"p\">(</span><span class=\"s1\">'mdwh-frontend'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">host</span><span class=\"p\">:</span>\n\n    <span class=\"c1\"># run some of the files from etl_tools/initalize_utils in frontend db</span>\n    <span class=\"n\">initialize_frontend_db_commands</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">ExecuteSQL</span><span class=\"p\">(</span>\n        <span class=\"n\">sql_statement</span><span class=\"o\">=</span><span class=\"s2\">\"DROP SCHEMA IF EXISTS util CASCADE; CREATE SCHEMA util;\"</span><span class=\"p\">,</span> <span class=\"n\">db_alias</span><span class=\"o\">=</span><span class=\"s1\">'mdwh-frontend'</span><span class=\"p\">)]</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">file_name</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">'schema_switching.sql'</span><span class=\"p\">,</span> <span class=\"s1\">'data_sets.sql'</span><span class=\"p\">,</span> <span class=\"s1\">'hll.sql'</span><span class=\"p\">,</span> <span class=\"s1\">'cstore_fdw.sql'</span><span class=\"p\">]:</span>\n        <span class=\"n\">initialize_frontend_db_commands</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span>\n            <span class=\"n\">ExecuteSQL</span><span class=\"p\">(</span><span class=\"n\">sql_file_name</span><span class=\"o\">=</span><span class=\"nb\">str</span><span class=\"p\">(</span>\n                <span class=\"n\">my_pipeline</span><span class=\"o\">.</span><span class=\"n\">nodes</span><span class=\"p\">[</span><span class=\"s1\">'utils'</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">nodes</span><span class=\"p\">[</span><span class=\"s1\">'initialize_utils'</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">base_path</span><span class=\"p\">()</span> <span class=\"o\">/</span> <span class=\"n\">file_name</span><span class=\"p\">),</span>\n                <span class=\"n\">db_alias</span><span class=\"o\">=</span><span class=\"s1\">'mdwh-frontend'</span><span class=\"p\">))</span>\n\n    <span class=\"n\">my_pipeline</span><span class=\"o\">.</span><span class=\"n\">nodes</span><span class=\"p\">[</span><span class=\"s1\">'utils'</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span>\n        <span class=\"n\">Task</span><span class=\"p\">(</span><span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"s1\">'initialize_frontend_db'</span><span class=\"p\">,</span>\n             <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s1\">'Adds some functions to the frontend db so that schema copying works'</span><span class=\"p\">,</span>\n             <span class=\"n\">commands</span><span class=\"o\">=</span><span class=\"n\">initialize_frontend_db_commands</span><span class=\"p\">))</span>\n\n    <span class=\"c1\"># Add schema copying for time schema</span>\n    <span class=\"n\">add_schema_copying_to_pipeline</span><span class=\"p\">(</span><span class=\"n\">pipeline</span><span class=\"o\">=</span><span class=\"n\">my_pipeline</span><span class=\"o\">.</span><span class=\"n\">nodes</span><span class=\"p\">[</span><span class=\"s1\">'utils'</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">nodes</span><span class=\"p\">[</span><span class=\"s1\">'create_time_dimensions'</span><span class=\"p\">],</span>\n                                   <span class=\"n\">schema_name</span><span class=\"o\">=</span><span class=\"s1\">'time'</span><span class=\"p\">,</span>\n                                   <span class=\"n\">source_db_alias</span><span class=\"o\">=</span><span class=\"s1\">'dwh-etl'</span><span class=\"p\">,</span> <span class=\"n\">target_db_alias</span><span class=\"o\">=</span><span class=\"s1\">'dwh-frontend'</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Add schema copying to all root pipelines</span>\n    <span class=\"k\">for</span> <span class=\"n\">pipeline</span> <span class=\"ow\">in</span> <span class=\"n\">my_pipeline</span><span class=\"o\">.</span><span class=\"n\">nodes</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">():</span>\n        <span class=\"k\">if</span> <span class=\"s2\">\"Schema\"</span> <span class=\"ow\">in</span> <span class=\"n\">pipeline</span><span class=\"o\">.</span><span class=\"n\">labels</span><span class=\"p\">:</span>\n            <span class=\"n\">schema</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"o\">.</span><span class=\"n\">labels</span><span class=\"p\">[</span><span class=\"s1\">'Schema'</span><span class=\"p\">]</span>\n            <span class=\"n\">add_schema_copying_to_pipeline</span><span class=\"p\">(</span><span class=\"n\">pipeline</span><span class=\"o\">=</span><span class=\"n\">pipeline</span><span class=\"p\">,</span> <span class=\"n\">schema_name</span><span class=\"o\">=</span><span class=\"n\">schema</span> <span class=\"o\">+</span> <span class=\"s1\">'_next'</span><span class=\"p\">,</span>\n                                           <span class=\"n\">source_db_alias</span><span class=\"o\">=</span><span class=\"s1\">'dwh-etl'</span><span class=\"p\">,</span> <span class=\"n\">target_db_alias</span><span class=\"o\">=</span><span class=\"s1\">'dwh-frontend'</span><span class=\"p\">)</span>\n            <span class=\"n\">pipeline</span><span class=\"o\">.</span><span class=\"n\">final_node</span><span class=\"o\">.</span><span class=\"n\">commands_after</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span>\n                <span class=\"n\">ExecuteSQL</span><span class=\"p\">(</span><span class=\"n\">sql_statement</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s2\">\"SELECT util.replace_schema('</span><span class=\"si\">{</span><span class=\"n\">schema</span><span class=\"si\">}</span><span class=\"s2\">', '</span><span class=\"si\">{</span><span class=\"n\">schema</span><span class=\"si\">}</span><span class=\"s2\">_next')\"</span><span class=\"p\">,</span>\n                           <span class=\"n\">db_alias</span><span class=\"o\">=</span><span class=\"s1\">'mdwh-frontend'</span><span class=\"p\">)</span>\n            <span class=\"p\">)</span>\n</pre>\n<h2>Time dimensions</h2>\n<p>The file <a href=\"etl_tools/create_time_dimensions/__init__.py\" rel=\"nofollow\">etl_tools/create_time_dimensions/<strong>init</strong>.py</a> defines a pipeline that creates and updates <code>time</code> schema with the tables <code>day</code> and <code>duration</code>:</p>\n<pre><code>select * from time.day order by _date desc limit 10;\n     day_id  |     day_name     | year_id | iso_year_id | quarter_id | quarter_name | month_id | month_name | week_id |  week_name   | day_of_week_id | day_of_week_name | day_of_month_id |   _date    \n   ----------+------------------+---------+-------------+------------+--------------+----------+------------+---------+--------------+----------------+------------------+-----------------+------------\n    20190815 | Thu, Aug 15 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201933 | 2019 - CW 33 |              4 | Thursday         |              15 | 2019-08-15\n    20190814 | Wed, Aug 14 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201933 | 2019 - CW 33 |              3 | Wednesday        |              14 | 2019-08-14\n    20190813 | Tue, Aug 13 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201933 | 2019 - CW 33 |              2 | Tuesday          |              13 | 2019-08-13\n    20190812 | Mon, Aug 12 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201933 | 2019 - CW 33 |              1 | Monday           |              12 | 2019-08-12\n    20190811 | Sun, Aug 11 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201932 | 2019 - CW 32 |              7 | Sunday           |              11 | 2019-08-11\n    20190810 | Sat, Aug 10 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201932 | 2019 - CW 32 |              6 | Saturday         |              10 | 2019-08-10\n    20190809 | Fri, Aug 09 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201932 | 2019 - CW 32 |              5 | Friday           |               9 | 2019-08-09\n    20190808 | Thu, Aug 08 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201932 | 2019 - CW 32 |              4 | Thursday         |               8 | 2019-08-08\n    20190807 | Wed, Aug 07 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201932 | 2019 - CW 32 |              3 | Wednesday        |               7 | 2019-08-07\n    20190806 | Tue, Aug 06 2019 |    2019 |        2019 |      20193 | 2019 Q3      |   201908 | 2019 Aug   |  201932 | 2019 - CW 32 |              2 | Tuesday          |               6 | 2019-08-06\n</code></pre>\n<pre><code>select * from time.duration where duration_id &gt;= 0 order by duration_id limit 10;\n duration_id | days | days_name | weeks | weeks_name | four_weeks | four_weeks_name | months | months_name | sixth_years | sixth_years_name | half_years | half_years_name | years | years_name \n-------------+------+-----------+-------+------------+------------+-----------------+--------+-------------+-------------+------------------+------------+-----------------+-------+------------\n           0 |    0 | 0 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           1 |    1 | 1 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           2 |    2 | 2 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           3 |    3 | 3 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           4 |    4 | 4 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           5 |    5 | 5 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           6 |    6 | 6 days    |     0 | 0-6 days   |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           7 |    7 | 7 days    |     1 | 7-13 days  |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           8 |    8 | 8 days    |     1 | 7-13 days  |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n           9 |    9 | 9 days    |     1 | 7-13 days  |          0 | 0-27 days       |      0 | 0-29 days   |           0 | 0-59 days        |          0 | 0-179 days      |     0 | 0-359 days\n\n</code></pre>\n<p>Add the pipeline to your project with</p>\n<pre>from etl_tools import create_time_dimensions\n\nmy_pipeline.add<span class=\"o\">(</span>create_time_dimensions.pipeline<span class=\"o\">)</span>\n</pre>\n<p>Set min and max dates by overwriting the <code>first_date_in_time_dimensions</code> and <code>last_date_in_time_dimensions</code> in <a href=\"etl_tools/config.py\" rel=\"nofollow\">etl_tools/config.py</a>.</p>\n<h2>Euro currency exchange rates</h2>\n<p>The file <a href=\"etl_tools/create_time_dimensions/__init__.py\" rel=\"nofollow\">etl_tools/load_euro_exchange_rates/<strong>init</strong>.py</a> contains a pipeline that loads (historic) Euro exchange rates from the European central bank.</p>\n<p>Add to your pipeline with</p>\n<pre>from etl_tools import load_euro_exchange_rates\n\nmy_pipeline.add<span class=\"o\">(</span>load_euro_exchange_rates.euro_exchange_rates_pipeline<span class=\"o\">(</span><span class=\"s1\">'db-alias'</span><span class=\"o\">))</span>\n</pre>\n\n          </div>"}, "last_serial": 6632029, "releases": {"2.1.0": [{"comment_text": "", "digests": {"md5": "32f7facc866f1d915a8aa0add43edfd0", "sha256": "9b22fcab6dc24c1bf9d9eedaa72aa539a875c63deffe19f049ddd1c31dc83618"}, "downloads": -1, "filename": "mara-etl-tools-2.1.0.tar.gz", "has_sig": false, "md5_digest": "32f7facc866f1d915a8aa0add43edfd0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 11199, "upload_time": "2019-07-07T09:06:50", "upload_time_iso_8601": "2019-07-07T09:06:50.846467Z", "url": "https://files.pythonhosted.org/packages/89/d5/27909a219fbfb20d74c5b0029112b9721e96ad23de26c3fc1c9b2bb581a1/mara-etl-tools-2.1.0.tar.gz", "yanked": false}], "3.0.0": [{"comment_text": "", "digests": {"md5": "875f6f4ce88b76aef4354f52a6f1c341", "sha256": "72b5a96ae241f296c7bcfaff0e89af94102d67a0a612f1b1fb253153c3bddd13"}, "downloads": -1, "filename": "mara-etl-tools-3.0.0.tar.gz", "has_sig": false, "md5_digest": "875f6f4ce88b76aef4354f52a6f1c341", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 11295, "upload_time": "2019-07-07T10:18:44", "upload_time_iso_8601": "2019-07-07T10:18:44.449645Z", "url": "https://files.pythonhosted.org/packages/0b/d5/0f1cb74528444f9b1c76cfe84f2a50eea2a761b27da82919eb404b6d9734/mara-etl-tools-3.0.0.tar.gz", "yanked": false}], "3.1.0": [{"comment_text": "", "digests": {"md5": "2c5b0add71424b13e3f8d6f45d2ee47d", "sha256": "c9976c91a9a74720279a4d5410551f4dd6659a61f472c2c4c607c91c42244227"}, "downloads": -1, "filename": "mara-etl-tools-3.1.0.tar.gz", "has_sig": false, "md5_digest": "2c5b0add71424b13e3f8d6f45d2ee47d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 12204, "upload_time": "2020-02-14T15:39:05", "upload_time_iso_8601": "2020-02-14T15:39:05.583669Z", "url": "https://files.pythonhosted.org/packages/e7/c4/40bc4a74e73f2cc952fa99ca16a1527fafc6b4418f202b8d8d38498f7a46/mara-etl-tools-3.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2c5b0add71424b13e3f8d6f45d2ee47d", "sha256": "c9976c91a9a74720279a4d5410551f4dd6659a61f472c2c4c607c91c42244227"}, "downloads": -1, "filename": "mara-etl-tools-3.1.0.tar.gz", "has_sig": false, "md5_digest": "2c5b0add71424b13e3f8d6f45d2ee47d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 12204, "upload_time": "2020-02-14T15:39:05", "upload_time_iso_8601": "2020-02-14T15:39:05.583669Z", "url": "https://files.pythonhosted.org/packages/e7/c4/40bc4a74e73f2cc952fa99ca16a1527fafc6b4418f202b8d8d38498f7a46/mara-etl-tools-3.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:57:52 2020"}