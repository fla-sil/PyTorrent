{"info": {"author": "Mathias Perslev", "author_email": "map@di.ku.dk", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Environment :: Console", "License :: OSI Approved :: MIT License", "Operating System :: POSIX", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "# Multi-Planar UNet\n\nImplementation of the Multi-Planar UNet as described in: \n\nMathias Perslev, Erik Dam, Akshay Pai, and Christian Igel. One Network To \nSegment Them All: A General, Lightweight System for Accurate 3D Medical Image\nSegmentation. In: Medical Image Computing and Computer Assisted Intervention \n(MICCAI), 2019\n\nPre-print version: https://arxiv.org/abs/1911.01764\n\nPublished version: https://doi.org/10.1007/978-3-030-32245-8_4\n\n## Quick Start\n#### Installation\n\n```\n# From GitHub\ngit clone https://github.com/perslev/MultiPlanarUNet\npip install -e MultiPlanarUNet\n```\n\nThis package is still frequently updated and it is thus recommended to install \nthe package with PIP with the -e ('editable') flag so that the package can be \nupdated with recent changes on GitHub without re-installing:\n\n```\ncd MultiPlanarUNet\ngit pull\n```\n\nHowever, the package is also occasionally updated on PyPi for install with:\n```\n# Note: renamed MultiPlanarUNet -> mpunet in versions 0.2.4\npip install mpunet\n```\n\n#### Usage\n```\nusage: mp [script] [script args...]\n\nMulti-Planar UNet (0.1.0)\n-------------------------\nAvailable scripts:\n- cv_experiment\n- cv_split\n- init_project\n- predict\n- predict_3D\n- summary\n- train\n- train_fusion\n...\n```\n\n## Overview\nThis package implements fully autonomous deep learning based \nsegmentation of any 3D medical image. It uses a fixed \nhyperparameter set and a fixed model topology, eliminating the need for\nconducting hyperparameter tuning experiments. No manual involvement is \nrequired except for supplying the training data.\n\nThe system has been evaluated on a wide range of tasks covering various organ and \npathology segmentation tasks, tissue types, and imaging modalities. \nThe model obtained a top-5 position at the 2018 Medical Segmentation Decathlon \n(http://medicaldecathlon.com/) despite its simplicity and computational \nefficiency.\n\nThis software may be used as-is and does not require deep learning expertise to\nget started. It may also serve as a strong baseline method for general purpose\nsemantic segmentation of medical images.\n\n## Method\nThe base model is a slightly modified 2D UNet (https://arxiv.org/abs/1505.04597) \ntrained under a multi-planar framework. Specifically, the 2D model is\nfed images sampled across multiple views onto the image volume simultaneously:\n\n[Multi-Planar Animation](resources/multi_planar_training.gif)\n\nAt test-time, the model predict along each of the views and recreates a set of full segmentation volumes. \nThese volumes are fused into one using a learned function that weights\neach class from each view individually to maximise the performance.\n\n![](resources/multi_planar_model_updated.png)\n\n![](resources/multi_planar_augmentation.png)\n\n## Usage\n\nProject initialization, model training, evaluation, prediction etc. can be \nperformed using the scripts located in ```MultiPlanarUNet.bin```. The script \nnamed ```mp.py``` serves as an entry point to all other scripts, and it is used\nas follows:\n\n```bash\n# Invoke the help menu\nmp --help\n\n# Launch the train script\nmp train [arguments passed to 'train'...]\n\n# Invoke the help menu of a sub-script\nmp train --help\n```\n\nYou only need to specify the training data in the format described \nbelow. Training, evaluation and prediction will be handled automatically if \nusing the above scripts.\n\n#### Preparing the data\nIn order to train a model to solve a specific task, a set of manually \nannotated images must be stored in a folder under the following structure:\n\n```\n./data_folder/\n|- train/\n|--- images/\n|------ image1.nii.gz\n|------ image5.nii.gz\n|--- labels/\n|------ image1.nii.gz\n|------ image5.nii.gz\n|- val/\n|--- images/\n|--- labels/\n|- test/\n|--- images/\n|--- labels/\n|- aug/ <-- OPTIONAL\n|--- images/\n|--- labels/\n```\n\nThe names of these folders may be customized in the parameter file (see below), \nbut default to those shown above. The image and corresponding label map files \nmust be identically named.\n\nThe ```aug``` folder may store additional images that can be included during \ntraining with a lower weight assigned in optimization.\n\n#### File formatting\nAll images must be stored in the ``.nii``/```.nii.gz``` format. \nIt is important that the .nii files store correct 4x4 affines for mapping\nvoxel coordinates to the scanner space. Specifically, the framework needs to\nknow the voxel size and axis orientations in order to sample isotrophic images \nin the scanner space.\n\nImages should be arrays of dimension 4 with the first 3 corresponding to the \nimage dimensions and the last the channels dimension (e.g. [256, 256, 256, 3] \nfor a 256x256x256 image with 3 channels). Label maps should be identically \nshaped in the first 3 dimensions and have a single channel \n(e.g. [256, 256, 256, 1]). The label at a given voxel should be an integer \nrepresenting the class at the given position. The background class is normally \ndenoted '0'.\n\n#### Initializing a Project\nOnce the data is stored under the above folder structure, a Multi-Planar \nproject can be initialized as follows:\n\n```\n# Initialize a project at 'my_folder'\n# The --data_dir flag is optional\nmp init_project --name my_project --data_dir ./data_folder\n```\n\nThis will create a folder at path ```my_project``` and populate it with a YAML\nfile named ```train_hparams.yaml```, which stores all hyperparameters. Any \nparameter in this file may be specified manually, but can all be set \nautomatically.\n\n**NOTE:** By default the ```init_project``` prepares a Multi-Planar model. \nHowever, note that a 3D model is also supported, which can be selected by \nspecifying the ```--model=3D``` flag (default=```---model=MultiPlanar```).\n\n#### Training\nThe model can now be trained as follows:\n\n```\nmp train --num_GPUs=2   # Any number of GPUs (or 0)\n```\n\nDuring training various information and images will be logged automatically to \nthe project folder. Typically, after training, the folder will look as follows:\n\n```\n./my_project/\n|- images/               # Example segmentations through training\n|- logs/                 # Various log files\n|- model/                # Stores the best model parameters\n|- tensorboard/          # TensorBoard graph and metric visualization\n|- train_hparams.yaml    # The hyperparameters file\n|- views.npz             # An array of the view vectors used\n|- views.png             # Visualization of the views used\n```\n\n#### Fusion Model Training\nWhen using the MultiPlanar model, a fusion model must  be computed after \nthe base model has been trained. This model will learn to map the multiple \npredictions of the base model through each view to one, stronger segmentation\nvolume:\n```\nmp train_fusion --num_GPUs=2\n```\n\n#### Predict and evaluate\nThe trained model can now be evaluated on the testing data in \n```data_folder/test``` by invoking:\n\n```\nmp predict --num_GPUs=2 --out_dir predictions\n```\n\nThis will create a folder ```my_project/predictions``` storing the predicted \nimages along with dice coefficient performance metrics.\n\nThe model can also be used to predict on images stored in the ```predictions``` \nfolder but without corresponding label files using the ``--no_eval`` flag or on \nsingle files as follows:\n\n```\n# Predict on all images in 'test' folder without label files\nmp predict --no_eval\n\n# Predict on a single image\nmp predict -f ./new_image.nii.gz\n\n# Preidct on a single image and do eval against its label file\nmp predict -f ./im/new_image.nii.gz -l ./lab/new_image.nii.gz\n```\n\n#### Performance Summary\nA summary of the performance can be produced by invoking the following command\nfrom inside the ```my_project``` folder or ```predictions``` sub-folder:\n\n```\nmp summary\n\n>> [***] SUMMARY REPORT FOR FOLDER [***]\n>> ./my_project/predictions/csv/\n>> \n>> \n>> Per class:\n>> --------------------------------\n>>    Mean dice by class  +/- STD    min    max   N\n>> 1               0.856    0.060  0.672  0.912  34\n>> 2               0.891    0.029  0.827  0.934  34\n>> 3               0.888    0.027  0.829  0.930  34\n>> 4               0.802    0.164  0.261  0.943  34\n>> 5               0.819    0.075  0.552  0.926  34\n>> 6               0.863    0.047  0.663  0.917  34\n>> \n>> Overall mean: 0.853 +- 0.088\n>> --------------------------------\n>> \n>> By views:\n>> --------------------------------\n>> [0.8477811  0.50449719 0.16355361]          0.825\n>> [ 0.70659414 -0.35532932  0.6119361 ]       0.819\n>> [ 0.11799461 -0.07137918  0.9904455 ]       0.772\n>> [ 0.95572575 -0.28795306  0.06059151]       0.827\n>> [-0.16704373 -0.96459936  0.20406974]       0.810\n>> [-0.72188903  0.68418977  0.10373322]       0.819\n>> --------------------------------\n```\n\n## Cross Validation Experiments\nCross validation experiments may be easily performed. First, invoke the \n```mp cv_split``` command to split your ```data_folder``` into a number of \nrandom splits:\n\n```\nmp cv_split --data_dir ./data_folder --CV=5\n```\n\nHere, we prepare for a 5-CV setup. By default, the above command will create a\nfolder at ```data_folder/views/5-CV/``` storing in this case 5 folders \n```split0, split1, ..., split5``` each structured like the main data folder \nwith sub-folders ```train```, ```val```, ```test``` and ```aug``` (optionally, \nset with the ```--aug_sub_dir``` flag). Inside these sub-folders, images a \nsymlinked to their original position to safe storage.\n\n#### Running a CV Experiment\nA cross-validation experiment can now be performed. On systems with multiple\nGPUs, each fold can be assigned a given number of the total pool of GPUs'. In \nthis case, multiple folds will run in parallel and new ones automatically start\nwhen previous folds terminate.\n\nFirst, we create a new project folder. This time, we do not specify a data \nfolder yet:\n\n```\nmp init_project --name CV_experiment\n```\n\nWe also create a file named ```script```, giving the following folder structure:\n\n```\n./CV_experiment\n|- train_hparams.yaml\n|- script\n```\n\nThe train_hparams.yaml file will serve as a **template** that will be applied \nto all folds. We can set any parameters we want here, or let the framework \ndecide on proper parameters for each fold automatically. The **script** file \ndetails the ```mp``` commands (and optionally various arguments) to execute on \neach fold. For instance, a script file may look like:\n\n```\nmp train --no_images  # Do not save example segmentations\nmp train_fusion\nmp predict --out_dir predictions\n```\n\nWe can now execute the 5-CV experiment by running:\n\n```\nmp cv_experiment --CV_dir=./data_dir/views/5-CV \\\n                 --out_dir=./splits \\\n                 --num_GPUs=2\n                 --monitor_GPUs_every=600\n```\n\nAbove, we assign 2 GPUs to each fold. On a system of 8 GPUs, 4 folds will be \nrun in parallel. We set ```--monitor_GPUs_every=600``` to scan the system for \nnew free GPU resources every 600 seconds (otherwise, only GPUs that we \ninitially available will be cycled and new free ones will be ignored).\n\nThe ```cv_experiment``` script will create a new project folder for each split \nlocated at ```--out_dir``` (```CV_experiment/splits``` in this case). For each\nfold, each of the commands outlined in the ```script``` file will be launched\none by one inside the respective project folder of the fold, so that the \npredictions are stored in ```CV_experiment/splits/split0/predictions``` for \nfold 0 etc.\n\nAfterwards, we may get a CV summary by invoking:\n\n```\nmp summary\n```\n\n... from inside the ```CV_experiment/splits``` folder.\n\n\nHistory\n-------\n\n0.1.0 (2018-12-17)\n--------------------\n* Project packaged for PIP\n\n0.1.1 (2019-01-08)\n--------------------\n* Added multi-task learning functionality.\n\n0.1.2 (2019-02-18)\n--------------------\n* Many smaller fixes and performance improvements. Also fixes a critical error\n  that in some cases would cause the validation callback to only consider a\n  subset of the predicted batch when computing validation metrics, which could\n  make validation metrics noisy especially for large batch sizes.\n\n0.1.3 (2019-02-20)\n--------------------\n* One-hot encoded targets (set with sparse=False in the fit section of\n  hyperparameter file) are no longer supported. Setting this value no longer\n  has any effect and may not be allowed in future versions.\n* The Validation callback has been changed significantly and now computes both\n  loss and any metrics specified in the hyperparamter file as performed on the\n  training set to facility a more easy comparison. Note that as is the case on\n  the training set, these computations are averaged batch-wise metrics.\n  The CB still computes the epoch-wise pr-class and average precision,\n  recall and dice.\n* Default parameter files no longer have pre-specified metrics. Metrics (such\n  as categorical accuracy, fg_precision, etc.) must be manually specified.\n\n0.1.4 (2019-03-02)\n------------------\n* Minor changes over 0.1.3, including ability to set a pre-specified set of\n  GPUs to cycle in mp cv_experiment\n\n0.2.0 (2019-02-27)\n------------------\n* MultiChannelScaler now ignores values equal to or smaller than the 'bg_value'\n  for each channel separately.\n  This value is either set manually by the user and must be a list of values\n  equal to the number of channels or a single value (that will be applied to\n  all channels). If bg_value='1pct' is specified (default for most models), or\n  any other percentage following this specification ('2pct' for 2 percent etc),\n  the 1st percentile will be computed for each channel individually and used\n  to define the background value for that channel.\n* ViewInterpolator similarly now accepts a channel-wise background value\n  specification, so that bg_value=[0, 0.1, 1] will cause out-of-bounds\n  interpolation to generate a pixel of value [0, 0.1, 1] for a 3-channel image.\n  Before, all channels would share a single, global background value (this\n  effect is still obtained if bg_value is set to a single integer or float).\n* Note that these changes may affect performance negatively if using the v 0.2\n  software on projects with models trained with version <0.2.0. Users will be\n  warned if trying to do so.\n* v0.2.0 now checks which MultiPlanarUNet version was used to create/run code\n  in a give project. Using a new version of the software on an older project\n  folder is no longer allowed. This behaviour may however be overwritten\n  manually setting the __VERSION__ variable to the current software version in\n  the hyperparamter file of the project (not recommended, instead, downgrade\n  to a previous version by running 'git checkout v<VERSION>' inside the\n  MultiPlanarUNet code folder).\n\n0.2.1 (2019-04-16)\n------------------\n* Various smaller changes and bug-fixes across the code base. Thread pools are now\n  generally limited to a maximum of 7 threads; The cv_experiment script now correctly\n  handles using the 'mp' script entry point in the 'script' file (before full paths \n  to the given script had to be passed to the python interpreter)\n\n0.2.2 (2019-06-17)\n------------------\n* Process has started to re-factor/re-write scripts in the bin module to make\n  them clearer, remove deprecated command-line arguments etc.\n* Evaluation results as stored in .csv files are now always saved and loaded\n  with an index column as the first column of the file.\n\n0.2.3 (2019-11-19)\n------------------\n* Simplified the functionality of the Validation callback so that it now only\n  computes the F1/Dice, precision and recall scores. I.e. the callback no longer\n  computes validation metrics. This choice was made to increase stability between\n  TensorFlow versions. The callback should work for most versions of TF now, incl.\n  TF 2.0. Future versions of MultiPlanarUNet will re-introduce validation metrics\n  in a TF 2.0 only setting.\n* Various smaller changes across the code\n\n0.2.4 (2020-02-07)\n------------------\n* Package was updated to comply with the TensorFlow >= 2.0 API.\n* Package was renamed from 'MultiPlanarUNet' to 'mpunet'. This affects imports as well \n  as installs from PyPi (i.e. 'pip install mpunet' now), but not the GitHub repo.\n* Now requires the 'psutil' and 'tensorflow-addons' packages.\n* Implements a temporary fix to the issue raised at https://github.com/perslev/MultiPlanarUNet/issues/8\n* Fixed a number of smaller bugs\n\n0.2.5 (2020-02-21)\n------------------\n* Implements a fix to high memory usage reported during training on some systems\n* Now uses tf.distribution for multi-GPU training and prediction\n* Custom loss functions should now be wrapped by tf.python.keras.losses.LossFunctionWrapper. I.e. any loss function must be \n  a class which accepts a tf.keras.losses.Reduction parameter and potentially other parameters and returns the compiled loss function.\n    * Consequently, when setting a loss function for MultiPlanarUNet training in train_hparams.yaml one must specify the factory \n      class verysion of the loss. E.g. for 'sparse_categorical_crossentropy' one must now specify 'SparseCategoricalCrossentropy' instead.\n      The same naming convention applies to all custom losses.\n    * Arbitrary Parameters may now be passed to a loss function in the 'loss_kwargs' entry in train_hparams.yaml\n* Some (deprecated) custom loss functions have been removed.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/perslev/MultiPlanarUNet", "keywords": "", "license": "LICENSE.txt", "maintainer": "", "maintainer_email": "", "name": "mpunet", "package_url": "https://pypi.org/project/mpunet/", "platform": "", "project_url": "https://pypi.org/project/mpunet/", "project_urls": {"Homepage": "https://github.com/perslev/MultiPlanarUNet"}, "release_url": "https://pypi.org/project/mpunet/0.2.5/", "requires_dist": null, "requires_python": "", "summary": "Multi-Planar UNet for autonomous segmentation of 3D medical images", "version": "0.2.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Multi-Planar UNet</h1>\n<p>Implementation of the Multi-Planar UNet as described in:</p>\n<p>Mathias Perslev, Erik Dam, Akshay Pai, and Christian Igel. One Network To\nSegment Them All: A General, Lightweight System for Accurate 3D Medical Image\nSegmentation. In: Medical Image Computing and Computer Assisted Intervention\n(MICCAI), 2019</p>\n<p>Pre-print version: <a href=\"https://arxiv.org/abs/1911.01764\" rel=\"nofollow\">https://arxiv.org/abs/1911.01764</a></p>\n<p>Published version: <a href=\"https://doi.org/10.1007/978-3-030-32245-8_4\" rel=\"nofollow\">https://doi.org/10.1007/978-3-030-32245-8_4</a></p>\n<h2>Quick Start</h2>\n<h4>Installation</h4>\n<pre><code># From GitHub\ngit clone https://github.com/perslev/MultiPlanarUNet\npip install -e MultiPlanarUNet\n</code></pre>\n<p>This package is still frequently updated and it is thus recommended to install\nthe package with PIP with the -e ('editable') flag so that the package can be\nupdated with recent changes on GitHub without re-installing:</p>\n<pre><code>cd MultiPlanarUNet\ngit pull\n</code></pre>\n<p>However, the package is also occasionally updated on PyPi for install with:</p>\n<pre><code># Note: renamed MultiPlanarUNet -&gt; mpunet in versions 0.2.4\npip install mpunet\n</code></pre>\n<h4>Usage</h4>\n<pre><code>usage: mp [script] [script args...]\n\nMulti-Planar UNet (0.1.0)\n-------------------------\nAvailable scripts:\n- cv_experiment\n- cv_split\n- init_project\n- predict\n- predict_3D\n- summary\n- train\n- train_fusion\n...\n</code></pre>\n<h2>Overview</h2>\n<p>This package implements fully autonomous deep learning based\nsegmentation of any 3D medical image. It uses a fixed\nhyperparameter set and a fixed model topology, eliminating the need for\nconducting hyperparameter tuning experiments. No manual involvement is\nrequired except for supplying the training data.</p>\n<p>The system has been evaluated on a wide range of tasks covering various organ and\npathology segmentation tasks, tissue types, and imaging modalities.\nThe model obtained a top-5 position at the 2018 Medical Segmentation Decathlon\n(<a href=\"http://medicaldecathlon.com/\" rel=\"nofollow\">http://medicaldecathlon.com/</a>) despite its simplicity and computational\nefficiency.</p>\n<p>This software may be used as-is and does not require deep learning expertise to\nget started. It may also serve as a strong baseline method for general purpose\nsemantic segmentation of medical images.</p>\n<h2>Method</h2>\n<p>The base model is a slightly modified 2D UNet (<a href=\"https://arxiv.org/abs/1505.04597\" rel=\"nofollow\">https://arxiv.org/abs/1505.04597</a>)\ntrained under a multi-planar framework. Specifically, the 2D model is\nfed images sampled across multiple views onto the image volume simultaneously:</p>\n<p><a href=\"resources/multi_planar_training.gif\" rel=\"nofollow\">Multi-Planar Animation</a></p>\n<p>At test-time, the model predict along each of the views and recreates a set of full segmentation volumes.\nThese volumes are fused into one using a learned function that weights\neach class from each view individually to maximise the performance.</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/280292f704de243dcc5906d9ea2ee3b854eca48c/7265736f75726365732f6d756c74695f706c616e61725f6d6f64656c5f757064617465642e706e67\"></p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/99da9b7020d1dc03607dc7b70b2dad30b8949df3/7265736f75726365732f6d756c74695f706c616e61725f6175676d656e746174696f6e2e706e67\"></p>\n<h2>Usage</h2>\n<p>Project initialization, model training, evaluation, prediction etc. can be\nperformed using the scripts located in <code>MultiPlanarUNet.bin</code>. The script\nnamed <code>mp.py</code> serves as an entry point to all other scripts, and it is used\nas follows:</p>\n<pre><span class=\"c1\"># Invoke the help menu</span>\nmp --help\n\n<span class=\"c1\"># Launch the train script</span>\nmp train <span class=\"o\">[</span>arguments passed to <span class=\"s1\">'train'</span>...<span class=\"o\">]</span>\n\n<span class=\"c1\"># Invoke the help menu of a sub-script</span>\nmp train --help\n</pre>\n<p>You only need to specify the training data in the format described\nbelow. Training, evaluation and prediction will be handled automatically if\nusing the above scripts.</p>\n<h4>Preparing the data</h4>\n<p>In order to train a model to solve a specific task, a set of manually\nannotated images must be stored in a folder under the following structure:</p>\n<pre><code>./data_folder/\n|- train/\n|--- images/\n|------ image1.nii.gz\n|------ image5.nii.gz\n|--- labels/\n|------ image1.nii.gz\n|------ image5.nii.gz\n|- val/\n|--- images/\n|--- labels/\n|- test/\n|--- images/\n|--- labels/\n|- aug/ &lt;-- OPTIONAL\n|--- images/\n|--- labels/\n</code></pre>\n<p>The names of these folders may be customized in the parameter file (see below),\nbut default to those shown above. The image and corresponding label map files\nmust be identically named.</p>\n<p>The <code>aug</code> folder may store additional images that can be included during\ntraining with a lower weight assigned in optimization.</p>\n<h4>File formatting</h4>\n<p>All images must be stored in the <code>.nii</code>/<code>.nii.gz</code> format.\nIt is important that the .nii files store correct 4x4 affines for mapping\nvoxel coordinates to the scanner space. Specifically, the framework needs to\nknow the voxel size and axis orientations in order to sample isotrophic images\nin the scanner space.</p>\n<p>Images should be arrays of dimension 4 with the first 3 corresponding to the\nimage dimensions and the last the channels dimension (e.g. [256, 256, 256, 3]\nfor a 256x256x256 image with 3 channels). Label maps should be identically\nshaped in the first 3 dimensions and have a single channel\n(e.g. [256, 256, 256, 1]). The label at a given voxel should be an integer\nrepresenting the class at the given position. The background class is normally\ndenoted '0'.</p>\n<h4>Initializing a Project</h4>\n<p>Once the data is stored under the above folder structure, a Multi-Planar\nproject can be initialized as follows:</p>\n<pre><code># Initialize a project at 'my_folder'\n# The --data_dir flag is optional\nmp init_project --name my_project --data_dir ./data_folder\n</code></pre>\n<p>This will create a folder at path <code>my_project</code> and populate it with a YAML\nfile named <code>train_hparams.yaml</code>, which stores all hyperparameters. Any\nparameter in this file may be specified manually, but can all be set\nautomatically.</p>\n<p><strong>NOTE:</strong> By default the <code>init_project</code> prepares a Multi-Planar model.\nHowever, note that a 3D model is also supported, which can be selected by\nspecifying the <code>--model=3D</code> flag (default=<code>---model=MultiPlanar</code>).</p>\n<h4>Training</h4>\n<p>The model can now be trained as follows:</p>\n<pre><code>mp train --num_GPUs=2   # Any number of GPUs (or 0)\n</code></pre>\n<p>During training various information and images will be logged automatically to\nthe project folder. Typically, after training, the folder will look as follows:</p>\n<pre><code>./my_project/\n|- images/               # Example segmentations through training\n|- logs/                 # Various log files\n|- model/                # Stores the best model parameters\n|- tensorboard/          # TensorBoard graph and metric visualization\n|- train_hparams.yaml    # The hyperparameters file\n|- views.npz             # An array of the view vectors used\n|- views.png             # Visualization of the views used\n</code></pre>\n<h4>Fusion Model Training</h4>\n<p>When using the MultiPlanar model, a fusion model must  be computed after\nthe base model has been trained. This model will learn to map the multiple\npredictions of the base model through each view to one, stronger segmentation\nvolume:</p>\n<pre><code>mp train_fusion --num_GPUs=2\n</code></pre>\n<h4>Predict and evaluate</h4>\n<p>The trained model can now be evaluated on the testing data in\n<code>data_folder/test</code> by invoking:</p>\n<pre><code>mp predict --num_GPUs=2 --out_dir predictions\n</code></pre>\n<p>This will create a folder <code>my_project/predictions</code> storing the predicted\nimages along with dice coefficient performance metrics.</p>\n<p>The model can also be used to predict on images stored in the <code>predictions</code>\nfolder but without corresponding label files using the <code>--no_eval</code> flag or on\nsingle files as follows:</p>\n<pre><code># Predict on all images in 'test' folder without label files\nmp predict --no_eval\n\n# Predict on a single image\nmp predict -f ./new_image.nii.gz\n\n# Preidct on a single image and do eval against its label file\nmp predict -f ./im/new_image.nii.gz -l ./lab/new_image.nii.gz\n</code></pre>\n<h4>Performance Summary</h4>\n<p>A summary of the performance can be produced by invoking the following command\nfrom inside the <code>my_project</code> folder or <code>predictions</code> sub-folder:</p>\n<pre><code>mp summary\n\n&gt;&gt; [***] SUMMARY REPORT FOR FOLDER [***]\n&gt;&gt; ./my_project/predictions/csv/\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; Per class:\n&gt;&gt; --------------------------------\n&gt;&gt;    Mean dice by class  +/- STD    min    max   N\n&gt;&gt; 1               0.856    0.060  0.672  0.912  34\n&gt;&gt; 2               0.891    0.029  0.827  0.934  34\n&gt;&gt; 3               0.888    0.027  0.829  0.930  34\n&gt;&gt; 4               0.802    0.164  0.261  0.943  34\n&gt;&gt; 5               0.819    0.075  0.552  0.926  34\n&gt;&gt; 6               0.863    0.047  0.663  0.917  34\n&gt;&gt; \n&gt;&gt; Overall mean: 0.853 +- 0.088\n&gt;&gt; --------------------------------\n&gt;&gt; \n&gt;&gt; By views:\n&gt;&gt; --------------------------------\n&gt;&gt; [0.8477811  0.50449719 0.16355361]          0.825\n&gt;&gt; [ 0.70659414 -0.35532932  0.6119361 ]       0.819\n&gt;&gt; [ 0.11799461 -0.07137918  0.9904455 ]       0.772\n&gt;&gt; [ 0.95572575 -0.28795306  0.06059151]       0.827\n&gt;&gt; [-0.16704373 -0.96459936  0.20406974]       0.810\n&gt;&gt; [-0.72188903  0.68418977  0.10373322]       0.819\n&gt;&gt; --------------------------------\n</code></pre>\n<h2>Cross Validation Experiments</h2>\n<p>Cross validation experiments may be easily performed. First, invoke the\n<code>mp cv_split</code> command to split your <code>data_folder</code> into a number of\nrandom splits:</p>\n<pre><code>mp cv_split --data_dir ./data_folder --CV=5\n</code></pre>\n<p>Here, we prepare for a 5-CV setup. By default, the above command will create a\nfolder at <code>data_folder/views/5-CV/</code> storing in this case 5 folders\n<code>split0, split1, ..., split5</code> each structured like the main data folder\nwith sub-folders <code>train</code>, <code>val</code>, <code>test</code> and <code>aug</code> (optionally,\nset with the <code>--aug_sub_dir</code> flag). Inside these sub-folders, images a\nsymlinked to their original position to safe storage.</p>\n<h4>Running a CV Experiment</h4>\n<p>A cross-validation experiment can now be performed. On systems with multiple\nGPUs, each fold can be assigned a given number of the total pool of GPUs'. In\nthis case, multiple folds will run in parallel and new ones automatically start\nwhen previous folds terminate.</p>\n<p>First, we create a new project folder. This time, we do not specify a data\nfolder yet:</p>\n<pre><code>mp init_project --name CV_experiment\n</code></pre>\n<p>We also create a file named <code>script</code>, giving the following folder structure:</p>\n<pre><code>./CV_experiment\n|- train_hparams.yaml\n|- script\n</code></pre>\n<p>The train_hparams.yaml file will serve as a <strong>template</strong> that will be applied\nto all folds. We can set any parameters we want here, or let the framework\ndecide on proper parameters for each fold automatically. The <strong>script</strong> file\ndetails the <code>mp</code> commands (and optionally various arguments) to execute on\neach fold. For instance, a script file may look like:</p>\n<pre><code>mp train --no_images  # Do not save example segmentations\nmp train_fusion\nmp predict --out_dir predictions\n</code></pre>\n<p>We can now execute the 5-CV experiment by running:</p>\n<pre><code>mp cv_experiment --CV_dir=./data_dir/views/5-CV \\\n                 --out_dir=./splits \\\n                 --num_GPUs=2\n                 --monitor_GPUs_every=600\n</code></pre>\n<p>Above, we assign 2 GPUs to each fold. On a system of 8 GPUs, 4 folds will be\nrun in parallel. We set <code>--monitor_GPUs_every=600</code> to scan the system for\nnew free GPU resources every 600 seconds (otherwise, only GPUs that we\ninitially available will be cycled and new free ones will be ignored).</p>\n<p>The <code>cv_experiment</code> script will create a new project folder for each split\nlocated at <code>--out_dir</code> (<code>CV_experiment/splits</code> in this case). For each\nfold, each of the commands outlined in the <code>script</code> file will be launched\none by one inside the respective project folder of the fold, so that the\npredictions are stored in <code>CV_experiment/splits/split0/predictions</code> for\nfold 0 etc.</p>\n<p>Afterwards, we may get a CV summary by invoking:</p>\n<pre><code>mp summary\n</code></pre>\n<p>... from inside the <code>CV_experiment/splits</code> folder.</p>\n<h2>History</h2>\n<h2>0.1.0 (2018-12-17)</h2>\n<ul>\n<li>Project packaged for PIP</li>\n</ul>\n<h2>0.1.1 (2019-01-08)</h2>\n<ul>\n<li>Added multi-task learning functionality.</li>\n</ul>\n<h2>0.1.2 (2019-02-18)</h2>\n<ul>\n<li>Many smaller fixes and performance improvements. Also fixes a critical error\nthat in some cases would cause the validation callback to only consider a\nsubset of the predicted batch when computing validation metrics, which could\nmake validation metrics noisy especially for large batch sizes.</li>\n</ul>\n<h2>0.1.3 (2019-02-20)</h2>\n<ul>\n<li>One-hot encoded targets (set with sparse=False in the fit section of\nhyperparameter file) are no longer supported. Setting this value no longer\nhas any effect and may not be allowed in future versions.</li>\n<li>The Validation callback has been changed significantly and now computes both\nloss and any metrics specified in the hyperparamter file as performed on the\ntraining set to facility a more easy comparison. Note that as is the case on\nthe training set, these computations are averaged batch-wise metrics.\nThe CB still computes the epoch-wise pr-class and average precision,\nrecall and dice.</li>\n<li>Default parameter files no longer have pre-specified metrics. Metrics (such\nas categorical accuracy, fg_precision, etc.) must be manually specified.</li>\n</ul>\n<h2>0.1.4 (2019-03-02)</h2>\n<ul>\n<li>Minor changes over 0.1.3, including ability to set a pre-specified set of\nGPUs to cycle in mp cv_experiment</li>\n</ul>\n<h2>0.2.0 (2019-02-27)</h2>\n<ul>\n<li>MultiChannelScaler now ignores values equal to or smaller than the 'bg_value'\nfor each channel separately.\nThis value is either set manually by the user and must be a list of values\nequal to the number of channels or a single value (that will be applied to\nall channels). If bg_value='1pct' is specified (default for most models), or\nany other percentage following this specification ('2pct' for 2 percent etc),\nthe 1st percentile will be computed for each channel individually and used\nto define the background value for that channel.</li>\n<li>ViewInterpolator similarly now accepts a channel-wise background value\nspecification, so that bg_value=[0, 0.1, 1] will cause out-of-bounds\ninterpolation to generate a pixel of value [0, 0.1, 1] for a 3-channel image.\nBefore, all channels would share a single, global background value (this\neffect is still obtained if bg_value is set to a single integer or float).</li>\n<li>Note that these changes may affect performance negatively if using the v 0.2\nsoftware on projects with models trained with version &lt;0.2.0. Users will be\nwarned if trying to do so.</li>\n<li>v0.2.0 now checks which MultiPlanarUNet version was used to create/run code\nin a give project. Using a new version of the software on an older project\nfolder is no longer allowed. This behaviour may however be overwritten\nmanually setting the <strong>VERSION</strong> variable to the current software version in\nthe hyperparamter file of the project (not recommended, instead, downgrade\nto a previous version by running 'git checkout v&lt;VERSION&gt;' inside the\nMultiPlanarUNet code folder).</li>\n</ul>\n<h2>0.2.1 (2019-04-16)</h2>\n<ul>\n<li>Various smaller changes and bug-fixes across the code base. Thread pools are now\ngenerally limited to a maximum of 7 threads; The cv_experiment script now correctly\nhandles using the 'mp' script entry point in the 'script' file (before full paths\nto the given script had to be passed to the python interpreter)</li>\n</ul>\n<h2>0.2.2 (2019-06-17)</h2>\n<ul>\n<li>Process has started to re-factor/re-write scripts in the bin module to make\nthem clearer, remove deprecated command-line arguments etc.</li>\n<li>Evaluation results as stored in .csv files are now always saved and loaded\nwith an index column as the first column of the file.</li>\n</ul>\n<h2>0.2.3 (2019-11-19)</h2>\n<ul>\n<li>Simplified the functionality of the Validation callback so that it now only\ncomputes the F1/Dice, precision and recall scores. I.e. the callback no longer\ncomputes validation metrics. This choice was made to increase stability between\nTensorFlow versions. The callback should work for most versions of TF now, incl.\nTF 2.0. Future versions of MultiPlanarUNet will re-introduce validation metrics\nin a TF 2.0 only setting.</li>\n<li>Various smaller changes across the code</li>\n</ul>\n<h2>0.2.4 (2020-02-07)</h2>\n<ul>\n<li>Package was updated to comply with the TensorFlow &gt;= 2.0 API.</li>\n<li>Package was renamed from 'MultiPlanarUNet' to 'mpunet'. This affects imports as well\nas installs from PyPi (i.e. 'pip install mpunet' now), but not the GitHub repo.</li>\n<li>Now requires the 'psutil' and 'tensorflow-addons' packages.</li>\n<li>Implements a temporary fix to the issue raised at <a href=\"https://github.com/perslev/MultiPlanarUNet/issues/8\" rel=\"nofollow\">https://github.com/perslev/MultiPlanarUNet/issues/8</a></li>\n<li>Fixed a number of smaller bugs</li>\n</ul>\n<h2>0.2.5 (2020-02-21)</h2>\n<ul>\n<li>Implements a fix to high memory usage reported during training on some systems</li>\n<li>Now uses tf.distribution for multi-GPU training and prediction</li>\n<li>Custom loss functions should now be wrapped by tf.python.keras.losses.LossFunctionWrapper. I.e. any loss function must be\na class which accepts a tf.keras.losses.Reduction parameter and potentially other parameters and returns the compiled loss function.\n<ul>\n<li>Consequently, when setting a loss function for MultiPlanarUNet training in train_hparams.yaml one must specify the factory\nclass verysion of the loss. E.g. for 'sparse_categorical_crossentropy' one must now specify 'SparseCategoricalCrossentropy' instead.\nThe same naming convention applies to all custom losses.</li>\n<li>Arbitrary Parameters may now be passed to a loss function in the 'loss_kwargs' entry in train_hparams.yaml</li>\n</ul>\n</li>\n<li>Some (deprecated) custom loss functions have been removed.</li>\n</ul>\n\n          </div>"}, "last_serial": 6673876, "releases": {"0.2.4": [{"comment_text": "", "digests": {"md5": "a2c11e6ab648570607e0d6fc9b205280", "sha256": "7c4ccd089b7c9b9f2d246e4149c3ada3cc03a4d0cfa386a1b1084b04d7c1ab6c"}, "downloads": -1, "filename": "mpunet-0.2.4.tar.gz", "has_sig": false, "md5_digest": "a2c11e6ab648570607e0d6fc9b205280", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 125961, "upload_time": "2020-02-07T15:49:02", "upload_time_iso_8601": "2020-02-07T15:49:02.559807Z", "url": "https://files.pythonhosted.org/packages/1d/60/0eafcbd73c8be8e0407911429d1ceffc6ccf680517f4612a778408b391fb/mpunet-0.2.4.tar.gz", "yanked": false}], "0.2.5": [{"comment_text": "", "digests": {"md5": "958c1a2c1ef65231d2badaf11393aa92", "sha256": "7e9187072c0804eb8362fba4f7f7c3dda78bede4a121195bfa121af485331b49"}, "downloads": -1, "filename": "mpunet-0.2.5.tar.gz", "has_sig": false, "md5_digest": "958c1a2c1ef65231d2badaf11393aa92", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 125687, "upload_time": "2020-02-21T07:58:14", "upload_time_iso_8601": "2020-02-21T07:58:14.614201Z", "url": "https://files.pythonhosted.org/packages/6c/83/4db8172a90a695537b07c9175cfdb442f9137de19541222a5ea53fa304b2/mpunet-0.2.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "958c1a2c1ef65231d2badaf11393aa92", "sha256": "7e9187072c0804eb8362fba4f7f7c3dda78bede4a121195bfa121af485331b49"}, "downloads": -1, "filename": "mpunet-0.2.5.tar.gz", "has_sig": false, "md5_digest": "958c1a2c1ef65231d2badaf11393aa92", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 125687, "upload_time": "2020-02-21T07:58:14", "upload_time_iso_8601": "2020-02-21T07:58:14.614201Z", "url": "https://files.pythonhosted.org/packages/6c/83/4db8172a90a695537b07c9175cfdb442f9137de19541222a5ea53fa304b2/mpunet-0.2.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:51:09 2020"}