{"info": {"author": "Angelos Katharopoulos", "author_email": "angelos.katharopoulos@idiap.ch", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering"], "description": "Attention Sampling\n==================\n\nThis repository provides a python library to accelerate the training and\ninference of neural networks on large data. This code is the reference\nimplementation of the methods described in our ICML 2019 publication\n`\"Processing Megapixel Images with Deep Attention-Sampling Models\"\n<https://arxiv.org/abs/1905.03711>`_.\n\n\nUsage\n------\n\nYou can find examples of how to use our library in the provided `scripts\n<https://github.com/idiap/attention-sampling/tree/master/scripts>`_ or a very\nconcise one below.\n\n.. code:: python\n\n    # Keras imports\n\n    from ats.core import attention_sampling\n    from ats.utils.layers import SampleSoftmax\n    from ats.utils.regularizers import multinomial_entropy\n\n    # Create our two inputs.\n    # Note that x_low could also be an input if we have access to a precomputed\n    # downsampled image.\n    x_high = Input(shape=(H, W, C))\n    x_low = AveragePooling2D(pool_size=(10,))(x_high)\n\n    # Create our attention model\n    attention = Sequential([\n        ...\n        Conv2D(1),\n        SampleSoftmax(squeeze_channels=True)\n    ])\n\n    # Create our feature extractor per patch, we assume that it returns a\n    # vector per patch.\n    feature = Sequential([\n        ...\n        GlobalAveragePooling2D(),\n        L2Normalize()\n    ])\n\n    features, attention, patches = attention_sampling(\n        attention,\n        feature,\n        patch_size=(32, 32),\n        n_patches=10,\n        attention_regularizer=multinomial_entropy(0.01)\n    )([x_low, x_high])\n\n    y = Dense(output_size, activation=\"softmax\")(features)\n\n    model = Model(inputs=x_high, outputs=y)\n\nDependencies & Installation\n----------------------------\n\nTo install the library just run ``pip install attention-sampling``. If you want\nto extend our code clone the repository and install it in development mode.\n\nThe dependencies of ``attention-sampling`` are\n\n* TensorFlow\n* C++ tool chain\n* CUDA (optional)\n\nDocumentation\n-------------\n\nThere exists a dedicated `documentation site <http://attention-sampling.com/>`_\nbut you are also encouraged to read the `source code\n<https://github.com/idiap/attention-sampling>` and the `scripts\n<https://github.com/idiap/attention-sampling/tree/master/scripts>`_ to get an\nidea of how the library should be used and extended.\n\nResearch\n---------\n\nIf you found this work influential or helpful in your research in any way, we\nwould appreciate if you cited us.\n\n.. code::\n\n    @inproceedings{katharopoulos2019ats,\n        title={Processing Megapixel Images with Deep Attention-Sampling Models},\n        author={Katharopoulos, A. and Fleuret, F.},\n        booktitle={Proceedings of the International Conference on Machine Learning (ICML)},\n        year={2019}\n    }\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://www.idiap.ch/~katharas/", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "attention-sampling", "package_url": "https://pypi.org/project/attention-sampling/", "platform": "", "project_url": "https://pypi.org/project/attention-sampling/", "project_urls": {"Homepage": "http://www.idiap.ch/~katharas/"}, "release_url": "https://pypi.org/project/attention-sampling/0.2/", "requires_dist": null, "requires_python": "", "summary": "Train networks on large data using attention sampling.", "version": "0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>This repository provides a python library to accelerate the training and\ninference of neural networks on large data. This code is the reference\nimplementation of the methods described in our ICML 2019 publication\n<a href=\"https://arxiv.org/abs/1905.03711\" rel=\"nofollow\">\u201cProcessing Megapixel Images with Deep Attention-Sampling Models\u201d</a>.</p>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>You can find examples of how to use our library in the provided <a href=\"https://github.com/idiap/attention-sampling/tree/master/scripts\" rel=\"nofollow\">scripts</a> or a very\nconcise one below.</p>\n<pre><span class=\"c1\"># Keras imports</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">ats.core</span> <span class=\"kn\">import</span> <span class=\"n\">attention_sampling</span>\n<span class=\"kn\">from</span> <span class=\"nn\">ats.utils.layers</span> <span class=\"kn\">import</span> <span class=\"n\">SampleSoftmax</span>\n<span class=\"kn\">from</span> <span class=\"nn\">ats.utils.regularizers</span> <span class=\"kn\">import</span> <span class=\"n\">multinomial_entropy</span>\n\n<span class=\"c1\"># Create our two inputs.</span>\n<span class=\"c1\"># Note that x_low could also be an input if we have access to a precomputed</span>\n<span class=\"c1\"># downsampled image.</span>\n<span class=\"n\">x_high</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">))</span>\n<span class=\"n\">x_low</span> <span class=\"o\">=</span> <span class=\"n\">AveragePooling2D</span><span class=\"p\">(</span><span class=\"n\">pool_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,))(</span><span class=\"n\">x_high</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create our attention model</span>\n<span class=\"n\">attention</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"o\">...</span>\n    <span class=\"n\">Conv2D</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n    <span class=\"n\">SampleSoftmax</span><span class=\"p\">(</span><span class=\"n\">squeeze_channels</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n\n<span class=\"c1\"># Create our feature extractor per patch, we assume that it returns a</span>\n<span class=\"c1\"># vector per patch.</span>\n<span class=\"n\">feature</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"o\">...</span>\n    <span class=\"n\">GlobalAveragePooling2D</span><span class=\"p\">(),</span>\n    <span class=\"n\">L2Normalize</span><span class=\"p\">()</span>\n<span class=\"p\">])</span>\n\n<span class=\"n\">features</span><span class=\"p\">,</span> <span class=\"n\">attention</span><span class=\"p\">,</span> <span class=\"n\">patches</span> <span class=\"o\">=</span> <span class=\"n\">attention_sampling</span><span class=\"p\">(</span>\n    <span class=\"n\">attention</span><span class=\"p\">,</span>\n    <span class=\"n\">feature</span><span class=\"p\">,</span>\n    <span class=\"n\">patch_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">),</span>\n    <span class=\"n\">n_patches</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n    <span class=\"n\">attention_regularizer</span><span class=\"o\">=</span><span class=\"n\">multinomial_entropy</span><span class=\"p\">(</span><span class=\"mf\">0.01</span><span class=\"p\">)</span>\n<span class=\"p\">)([</span><span class=\"n\">x_low</span><span class=\"p\">,</span> <span class=\"n\">x_high</span><span class=\"p\">])</span>\n\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">output_size</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s2\">\"softmax\"</span><span class=\"p\">)(</span><span class=\"n\">features</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"n\">x_high</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"n\">y</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"dependencies-installation\">\n<h2>Dependencies &amp; Installation</h2>\n<p>To install the library just run <tt>pip install <span class=\"pre\">attention-sampling</span></tt>. If you want\nto extend our code clone the repository and install it in development mode.</p>\n<p>The dependencies of <tt><span class=\"pre\">attention-sampling</span></tt> are</p>\n<ul>\n<li>TensorFlow</li>\n<li>C++ tool chain</li>\n<li>CUDA (optional)</li>\n</ul>\n</div>\n<div id=\"documentation\">\n<h2>Documentation</h2>\n<p>There exists a dedicated <a href=\"http://attention-sampling.com/\" rel=\"nofollow\">documentation site</a>\nbut you are also encouraged to read the <cite>source code\n&lt;https://github.com/idiap/attention-sampling&gt;</cite> and the <a href=\"https://github.com/idiap/attention-sampling/tree/master/scripts\" rel=\"nofollow\">scripts</a> to get an\nidea of how the library should be used and extended.</p>\n</div>\n<div id=\"research\">\n<h2>Research</h2>\n<p>If you found this work influential or helpful in your research in any way, we\nwould appreciate if you cited us.</p>\n<pre>@inproceedings{katharopoulos2019ats,\n    title={Processing Megapixel Images with Deep Attention-Sampling Models},\n    author={Katharopoulos, A. and Fleuret, F.},\n    booktitle={Proceedings of the International Conference on Machine Learning (ICML)},\n    year={2019}\n}\n</pre>\n</div>\n\n          </div>"}, "last_serial": 5568940, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "10d8b3d2a638b4be2120cb8d46b6b6d6", "sha256": "d6a80df34d1bb07d6a9753e2b9f97f5f42a13da21679cbe73c0eb4b539682847"}, "downloads": -1, "filename": "attention-sampling-0.1.tar.gz", "has_sig": false, "md5_digest": "10d8b3d2a638b4be2120cb8d46b6b6d6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23544, "upload_time": "2019-07-22T14:42:49", "upload_time_iso_8601": "2019-07-22T14:42:49.248971Z", "url": "https://files.pythonhosted.org/packages/d5/a1/69be75b89c40789b47e7bdac4150ba894376747d367a3a54bf00754123c3/attention-sampling-0.1.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "b5f932f53b680a8ff49a5cef326594de", "sha256": "30b7590af2546cf4dff68ab1125f9ef54a1953a52abba542a1e43a3ef98d6d13"}, "downloads": -1, "filename": "attention-sampling-0.1.1.tar.gz", "has_sig": false, "md5_digest": "b5f932f53b680a8ff49a5cef326594de", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23548, "upload_time": "2019-07-22T14:45:14", "upload_time_iso_8601": "2019-07-22T14:45:14.615683Z", "url": "https://files.pythonhosted.org/packages/47/9f/49568dab359a58122ed6428584f3704fbeed48790adbede223c529f1a7ce/attention-sampling-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "e62de2c3b404d961e532cd8d56ccde91", "sha256": "cbc20f6eccda9e47d5df07d35c765c5ad1c913c24eaae6773a564463fc65bb81"}, "downloads": -1, "filename": "attention-sampling-0.1.2.tar.gz", "has_sig": false, "md5_digest": "e62de2c3b404d961e532cd8d56ccde91", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24691, "upload_time": "2019-07-22T14:55:07", "upload_time_iso_8601": "2019-07-22T14:55:07.366207Z", "url": "https://files.pythonhosted.org/packages/ca/64/2691fc1b7fdc6e3515eb02645c995bfe8dfac6905085c66eb24a4e461951/attention-sampling-0.1.2.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "f8f72846f254a70474687cd7641b9f96", "sha256": "7e2df98c2e05532799316c6a9bca5311960c9c404176f7bd98c8e971c2994321"}, "downloads": -1, "filename": "attention-sampling-0.2.tar.gz", "has_sig": false, "md5_digest": "f8f72846f254a70474687cd7641b9f96", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24636, "upload_time": "2019-07-22T20:17:17", "upload_time_iso_8601": "2019-07-22T20:17:17.426941Z", "url": "https://files.pythonhosted.org/packages/ff/2d/4474d1f516865eb83419c67045d885adc03266f97858fa8eeb14117c709d/attention-sampling-0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f8f72846f254a70474687cd7641b9f96", "sha256": "7e2df98c2e05532799316c6a9bca5311960c9c404176f7bd98c8e971c2994321"}, "downloads": -1, "filename": "attention-sampling-0.2.tar.gz", "has_sig": false, "md5_digest": "f8f72846f254a70474687cd7641b9f96", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24636, "upload_time": "2019-07-22T20:17:17", "upload_time_iso_8601": "2019-07-22T20:17:17.426941Z", "url": "https://files.pythonhosted.org/packages/ff/2d/4474d1f516865eb83419c67045d885adc03266f97858fa8eeb14117c709d/attention-sampling-0.2.tar.gz", "yanked": false}], "timestamp": "Thu May  7 18:16:30 2020"}