{"info": {"author": "Radim Rehurek", "author_email": "me@radimrehurek.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Database :: Front-Ends", "Topic :: System :: Distributed Computing"], "description": "======================================================\nsmart_open \u2014 utils for streaming large files in Python\n======================================================\n\n|License|_ |Travis|_\n\n.. |License| image:: https://img.shields.io/pypi/l/smart_open.svg\n.. |Travis| image:: https://travis-ci.org/RaRe-Technologies/smart_open.svg?branch=master\n.. _Travis: https://travis-ci.org/RaRe-Technologies/smart_open\n.. _License: https://github.com/RaRe-Technologies/smart_open/blob/master/LICENSE\n\nWhat?\n=====\n\n``smart_open`` is a Python 2 & Python 3 library for **efficient streaming of very large files** from/to S3, HDFS, WebHDFS, HTTP, or local (compressed) files. It's a drop-in replacement for Python's built-in ``open()``: it can do anything ``open`` can (100% compatible, falls back to native ``open`` wherever possible), plus lots of nifty extra stuff on top.\n\n``smart_open`` is well-tested, well-documented, and has a simple, Pythonic API:\n\n.. code-block:: python\n\n  >>> from smart_open import smart_open\n\n  >>> # stream lines from an S3 object\n  >>> for line in smart_open('s3://mybucket/mykey.txt', 'rb'):\n  ...    print(line.decode('utf8'))\n\n  >>> # stream from/to compressed files, with transparent (de)compression:\n  >>> for line in smart_open('./foo.txt.gz', encoding='utf8'):\n  ...    print(line)\n\n  >>> # can use context managers too:\n  >>> with smart_open('/home/radim/foo.txt.bz2', 'wb') as fout:\n  ...    fout.write(u\"some content\\n\".encode('utf8'))\n\n  >>> with smart_open('s3://mybucket/mykey.txt', 'rb') as fin:\n  ...     for line in fin:\n  ...         print(line.decode('utf8'))\n  ...     fin.seek(0)  # seek to the beginning\n  ...     b1000 = fin.read(1000)  # read 1000 bytes\n\n  >>> # stream from HDFS\n  >>> for line in smart_open('hdfs://user/hadoop/my_file.txt', encoding='utf8'):\n  ...     print(line)\n\n  >>> # stream from HTTP\n  >>> for line in smart_open('http://example.com/index.html'):\n  ...     print(line)\n\n  >>> # stream from WebHDFS\n  >>> for line in smart_open('webhdfs://host:port/user/hadoop/my_file.txt'):\n  ...     print(line)\n\n  >>> # stream content *into* S3 (write mode):\n  >>> with smart_open('s3://mybucket/mykey.txt', 'wb') as fout:\n  ...     for line in [b'first line\\n', b'second line\\n', b'third line\\n']:\n  ...          fout.write(line)\n\n  >>> # stream content *into* HDFS (write mode):\n  >>> with smart_open('hdfs://host:port/user/hadoop/my_file.txt', 'wb') as fout:\n  ...     for line in [b'first line\\n', b'second line\\n', b'third line\\n']:\n  ...          fout.write(line)\n\n  >>> # stream content *into* WebHDFS (write mode):\n  >>> with smart_open('webhdfs://host:port/user/hadoop/my_file.txt', 'wb') as fout:\n  ...     for line in [b'first line\\n', b'second line\\n', b'third line\\n']:\n  ...          fout.write(line)\n\n  >>> # stream using a completely custom s3 server, like s3proxy:\n  >>> for line in smart_open('s3u://user:secret@host:port@mybucket/mykey.txt', 'rb'):\n  ...    print(line.decode('utf8'))\n\n  >>> # you can also use a boto.s3.key.Key instance directly:\n  >>> key = boto.connect_s3().get_bucket(\"my_bucket\").get_key(\"my_key\")\n  >>> with smart_open(key, 'rb') as fin:\n  ...     for line in fin:\n  ...         print(line.decode('utf8'))\n \n  >>> # Stream to Digital Ocean Spaces bucket providing credentials from boto profile\n  >>> with smart_open('s3://bucket-for-experiments/file.txt', 'wb', endpoint_url='https://ams3.digitaloceanspaces.com', profile_name='digitalocean') as fout:\n  ...     fout.write(b'here we stand')\n\nWhy?\n----\n\nWorking with large S3 files using Amazon's default Python library, `boto <http://docs.pythonboto.org/en/latest/>`_ and `boto3 <https://boto3.readthedocs.io/en/latest/>`_, is a pain. Its ``key.set_contents_from_string()`` and ``key.get_contents_as_string()`` methods only work for small files (loaded in RAM, no streaming).\nThere are nasty hidden gotchas when using ``boto``'s multipart upload functionality that is needed for large files, and a lot of boilerplate.\n\n``smart_open`` shields you from that. It builds on boto3 but offers a cleaner, Pythonic API. The result is less code for you to write and fewer bugs to make.\n\nInstallation\n------------\n::\n\n    pip install smart_open\n\nOr, if you prefer to install from the `source tar.gz <http://pypi.python.org/pypi/smart_open>`_::\n\n    python setup.py test  # run unit tests\n    python setup.py install\n\nTo run the unit tests (optional), you'll also need to install `mock <https://pypi.python.org/pypi/mock>`_ , `moto <https://github.com/spulec/moto>`_ and `responses <https://github.com/getsentry/responses>`_ (``pip install mock moto responses``). The tests are also run automatically with `Travis CI <https://travis-ci.org/RaRe-Technologies/smart_open>`_ on every commit push & pull request.\n\nSupported archive types\n-----------------------\n``smart_open`` allows reading and writing gzip, bzip2 and xz files. They are transparently handled\nover HTTP, S3, and other protocols, too.\n\nS3-Specific Options\n-------------------\n\nThe S3 reader supports gzipped content transparently, as long as the key is obviously a gzipped file (e.g. ends with \".gz\").\n\nThere are a few optional keyword arguments that are useful only for S3 access.\n\nThe **host** and **profile** arguments are both passed to `boto.s3_connect()` as keyword arguments:\n\n.. code-block:: python\n\n  >>> smart_open('s3://', host='s3.amazonaws.com')\n  >>> smart_open('s3://', profile_name='my-profile')\n\nThe **s3_session** argument allows you to provide a custom `boto3.Session` instance for connecting to S3:\n\n.. code-block:: python\n\n  >>> smart_open('s3://', s3_session=boto3.Session())\n\n\nThe **s3_upload** argument accepts a dict of any parameters accepted by `initiate_multipart_upload <https://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.ObjectSummary.initiate_multipart_upload/>`_:\n\n.. code-block:: python\n\n  >>> smart_open('s3://', s3_upload={ 'ServerSideEncryption': 'AES256' })\n\nSince going over all (or select) keys in an S3 bucket is a very common operation,\nthere's also an extra method ``smart_open.s3_iter_bucket()`` that does this efficiently,\n**processing the bucket keys in parallel** (using multiprocessing):\n\n.. code-block:: python\n\n  >>> from smart_open import smart_open, s3_iter_bucket\n  >>> # get all JSON files under \"mybucket/foo/\"\n  >>> bucket = boto.connect_s3().get_bucket('mybucket')\n  >>> for key, content in s3_iter_bucket(bucket, prefix='foo/', accept_key=lambda key: key.endswith('.json')):\n  ...     print(key, len(content))\n\nFor more info (S3 credentials in URI, minimum S3 part size...) and full method signatures, check out the API docs:\n\n.. code-block:: python\n\n  >>> import smart_open\n  >>> help(smart_open.smart_open_lib)\n\n\nComments, bug reports\n---------------------\n\n``smart_open`` lives on `Github <https://github.com/RaRe-Technologies/smart_open>`_. You can file\nissues or pull requests there. Suggestions, pull requests and improvements welcome!\n\n----------------\n\n``smart_open`` is open source software released under the `MIT license <https://github.com/piskvorky/smart_open/blob/master/LICENSE>`_.\nCopyright (c) 2015-now `Radim \u0158eh\u016f\u0159ek <https://radimrehurek.com>`_.\n", "description_content_type": "", "docs_url": null, "download_url": "http://pypi.python.org/pypi/smart_open", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/piskvorky/smart_open", "keywords": "file streaming", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "srcd_smart_open", "package_url": "https://pypi.org/project/srcd_smart_open/", "platform": "any", "project_url": "https://pypi.org/project/srcd_smart_open/", "project_urls": {"Download": "http://pypi.python.org/pypi/smart_open", "Homepage": "https://github.com/piskvorky/smart_open"}, "release_url": "https://pypi.org/project/srcd_smart_open/1.9.0/", "requires_dist": null, "requires_python": "", "summary": "Utils for streaming large files (S3, HDFS, gzip, bz2...) - temporary source{d} fork", "version": "1.9.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://github.com/RaRe-Technologies/smart_open/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0f5a150d137e0213d8d04d0e788650145f14216a/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f736d6172745f6f70656e2e737667\"></a> <a href=\"https://travis-ci.org/RaRe-Technologies/smart_open\" rel=\"nofollow\"><img alt=\"Travis\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8c395ed64df04d22cbbf4aa839be762da1adb954/68747470733a2f2f7472617669732d63692e6f72672f526152652d546563686e6f6c6f676965732f736d6172745f6f70656e2e7376673f6272616e63683d6d6173746572\"></a></p>\n<div id=\"what\">\n<h2>What?</h2>\n<p><tt>smart_open</tt> is a Python 2 &amp; Python 3 library for <strong>efficient streaming of very large files</strong> from/to S3, HDFS, WebHDFS, HTTP, or local (compressed) files. It\u2019s a drop-in replacement for Python\u2019s built-in <tt>open()</tt>: it can do anything <tt>open</tt> can (100% compatible, falls back to native <tt>open</tt> wherever possible), plus lots of nifty extra stuff on top.</p>\n<p><tt>smart_open</tt> is well-tested, well-documented, and has a simple, Pythonic API:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">smart_open</span> <span class=\"kn\">import</span> <span class=\"n\">smart_open</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># stream lines from an S3 object</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'s3://mybucket/mykey.txt'</span><span class=\"p\">,</span> <span class=\"s1\">'rb'</span><span class=\"p\">):</span>\n<span class=\"o\">...</span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s1\">'utf8'</span><span class=\"p\">))</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># stream from/to compressed files, with transparent (de)compression:</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'./foo.txt.gz'</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s1\">'utf8'</span><span class=\"p\">):</span>\n<span class=\"o\">...</span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># can use context managers too:</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">with</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'/home/radim/foo.txt.bz2'</span><span class=\"p\">,</span> <span class=\"s1\">'wb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fout</span><span class=\"p\">:</span>\n<span class=\"o\">...</span>    <span class=\"n\">fout</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"sa\">u</span><span class=\"s2\">\"some content</span><span class=\"se\">\\n</span><span class=\"s2\">\"</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"s1\">'utf8'</span><span class=\"p\">))</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">with</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'s3://mybucket/mykey.txt'</span><span class=\"p\">,</span> <span class=\"s1\">'rb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fin</span><span class=\"p\">:</span>\n<span class=\"o\">...</span>     <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">fin</span><span class=\"p\">:</span>\n<span class=\"o\">...</span>         <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s1\">'utf8'</span><span class=\"p\">))</span>\n<span class=\"o\">...</span>     <span class=\"n\">fin</span><span class=\"o\">.</span><span class=\"n\">seek</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>  <span class=\"c1\"># seek to the beginning</span>\n<span class=\"o\">...</span>     <span class=\"n\">b1000</span> <span class=\"o\">=</span> <span class=\"n\">fin</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">)</span>  <span class=\"c1\"># read 1000 bytes</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># stream from HDFS</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'hdfs://user/hadoop/my_file.txt'</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s1\">'utf8'</span><span class=\"p\">):</span>\n<span class=\"o\">...</span>     <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># stream from HTTP</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'http://example.com/index.html'</span><span class=\"p\">):</span>\n<span class=\"o\">...</span>     <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># stream from WebHDFS</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'webhdfs://host:port/user/hadoop/my_file.txt'</span><span class=\"p\">):</span>\n<span class=\"o\">...</span>     <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># stream content *into* S3 (write mode):</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">with</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'s3://mybucket/mykey.txt'</span><span class=\"p\">,</span> <span class=\"s1\">'wb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fout</span><span class=\"p\">:</span>\n<span class=\"o\">...</span>     <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"sa\">b</span><span class=\"s1\">'first line</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"sa\">b</span><span class=\"s1\">'second line</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"sa\">b</span><span class=\"s1\">'third line</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">]:</span>\n<span class=\"o\">...</span>          <span class=\"n\">fout</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># stream content *into* HDFS (write mode):</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">with</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'hdfs://host:port/user/hadoop/my_file.txt'</span><span class=\"p\">,</span> <span class=\"s1\">'wb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fout</span><span class=\"p\">:</span>\n<span class=\"o\">...</span>     <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"sa\">b</span><span class=\"s1\">'first line</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"sa\">b</span><span class=\"s1\">'second line</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"sa\">b</span><span class=\"s1\">'third line</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">]:</span>\n<span class=\"o\">...</span>          <span class=\"n\">fout</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># stream content *into* WebHDFS (write mode):</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">with</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'webhdfs://host:port/user/hadoop/my_file.txt'</span><span class=\"p\">,</span> <span class=\"s1\">'wb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fout</span><span class=\"p\">:</span>\n<span class=\"o\">...</span>     <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"sa\">b</span><span class=\"s1\">'first line</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"sa\">b</span><span class=\"s1\">'second line</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"sa\">b</span><span class=\"s1\">'third line</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"p\">]:</span>\n<span class=\"o\">...</span>          <span class=\"n\">fout</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># stream using a completely custom s3 server, like s3proxy:</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'s3u://user:secret@host:port@mybucket/mykey.txt'</span><span class=\"p\">,</span> <span class=\"s1\">'rb'</span><span class=\"p\">):</span>\n<span class=\"o\">...</span>    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s1\">'utf8'</span><span class=\"p\">))</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># you can also use a boto.s3.key.Key instance directly:</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">key</span> <span class=\"o\">=</span> <span class=\"n\">boto</span><span class=\"o\">.</span><span class=\"n\">connect_s3</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">get_bucket</span><span class=\"p\">(</span><span class=\"s2\">\"my_bucket\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">get_key</span><span class=\"p\">(</span><span class=\"s2\">\"my_key\"</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">with</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"s1\">'rb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fin</span><span class=\"p\">:</span>\n<span class=\"o\">...</span>     <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">fin</span><span class=\"p\">:</span>\n<span class=\"o\">...</span>         <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s1\">'utf8'</span><span class=\"p\">))</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># Stream to Digital Ocean Spaces bucket providing credentials from boto profile</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">with</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'s3://bucket-for-experiments/file.txt'</span><span class=\"p\">,</span> <span class=\"s1\">'wb'</span><span class=\"p\">,</span> <span class=\"n\">endpoint_url</span><span class=\"o\">=</span><span class=\"s1\">'https://ams3.digitaloceanspaces.com'</span><span class=\"p\">,</span> <span class=\"n\">profile_name</span><span class=\"o\">=</span><span class=\"s1\">'digitalocean'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fout</span><span class=\"p\">:</span>\n<span class=\"o\">...</span>     <span class=\"n\">fout</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"sa\">b</span><span class=\"s1\">'here we stand'</span><span class=\"p\">)</span>\n</pre>\n<div id=\"why\">\n<h3>Why?</h3>\n<p>Working with large S3 files using Amazon\u2019s default Python library, <a href=\"http://docs.pythonboto.org/en/latest/\" rel=\"nofollow\">boto</a> and <a href=\"https://boto3.readthedocs.io/en/latest/\" rel=\"nofollow\">boto3</a>, is a pain. Its <tt>key.set_contents_from_string()</tt> and <tt>key.get_contents_as_string()</tt> methods only work for small files (loaded in RAM, no streaming).\nThere are nasty hidden gotchas when using <tt>boto</tt>\u2019s multipart upload functionality that is needed for large files, and a lot of boilerplate.</p>\n<p><tt>smart_open</tt> shields you from that. It builds on boto3 but offers a cleaner, Pythonic API. The result is less code for you to write and fewer bugs to make.</p>\n</div>\n<div id=\"installation\">\n<h3>Installation</h3>\n<pre>pip install smart_open\n</pre>\n<p>Or, if you prefer to install from the <a href=\"http://pypi.python.org/pypi/smart_open\" rel=\"nofollow\">source tar.gz</a>:</p>\n<pre>python setup.py test  # run unit tests\npython setup.py install\n</pre>\n<p>To run the unit tests (optional), you\u2019ll also need to install <a href=\"https://pypi.python.org/pypi/mock\" rel=\"nofollow\">mock</a> , <a href=\"https://github.com/spulec/moto\" rel=\"nofollow\">moto</a> and <a href=\"https://github.com/getsentry/responses\" rel=\"nofollow\">responses</a> (<tt>pip install mock moto responses</tt>). The tests are also run automatically with <a href=\"https://travis-ci.org/RaRe-Technologies/smart_open\" rel=\"nofollow\">Travis CI</a> on every commit push &amp; pull request.</p>\n</div>\n<div id=\"supported-archive-types\">\n<h3>Supported archive types</h3>\n<p><tt>smart_open</tt> allows reading and writing gzip, bzip2 and xz files. They are transparently handled\nover HTTP, S3, and other protocols, too.</p>\n</div>\n<div id=\"s3-specific-options\">\n<h3>S3-Specific Options</h3>\n<p>The S3 reader supports gzipped content transparently, as long as the key is obviously a gzipped file (e.g. ends with \u201c.gz\u201d).</p>\n<p>There are a few optional keyword arguments that are useful only for S3 access.</p>\n<p>The <strong>host</strong> and <strong>profile</strong> arguments are both passed to <cite>boto.s3_connect()</cite> as keyword arguments:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'s3://'</span><span class=\"p\">,</span> <span class=\"n\">host</span><span class=\"o\">=</span><span class=\"s1\">'s3.amazonaws.com'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'s3://'</span><span class=\"p\">,</span> <span class=\"n\">profile_name</span><span class=\"o\">=</span><span class=\"s1\">'my-profile'</span><span class=\"p\">)</span>\n</pre>\n<p>The <strong>s3_session</strong> argument allows you to provide a custom <cite>boto3.Session</cite> instance for connecting to S3:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'s3://'</span><span class=\"p\">,</span> <span class=\"n\">s3_session</span><span class=\"o\">=</span><span class=\"n\">boto3</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">())</span>\n</pre>\n<p>The <strong>s3_upload</strong> argument accepts a dict of any parameters accepted by <a href=\"https://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.ObjectSummary.initiate_multipart_upload/\" rel=\"nofollow\">initiate_multipart_upload</a>:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">smart_open</span><span class=\"p\">(</span><span class=\"s1\">'s3://'</span><span class=\"p\">,</span> <span class=\"n\">s3_upload</span><span class=\"o\">=</span><span class=\"p\">{</span> <span class=\"s1\">'ServerSideEncryption'</span><span class=\"p\">:</span> <span class=\"s1\">'AES256'</span> <span class=\"p\">})</span>\n</pre>\n<p>Since going over all (or select) keys in an S3 bucket is a very common operation,\nthere\u2019s also an extra method <tt>smart_open.s3_iter_bucket()</tt> that does this efficiently,\n<strong>processing the bucket keys in parallel</strong> (using multiprocessing):</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">smart_open</span> <span class=\"kn\">import</span> <span class=\"n\">smart_open</span><span class=\"p\">,</span> <span class=\"n\">s3_iter_bucket</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># get all JSON files under \"mybucket/foo/\"</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">bucket</span> <span class=\"o\">=</span> <span class=\"n\">boto</span><span class=\"o\">.</span><span class=\"n\">connect_s3</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">get_bucket</span><span class=\"p\">(</span><span class=\"s1\">'mybucket'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">for</span> <span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">content</span> <span class=\"ow\">in</span> <span class=\"n\">s3_iter_bucket</span><span class=\"p\">(</span><span class=\"n\">bucket</span><span class=\"p\">,</span> <span class=\"n\">prefix</span><span class=\"o\">=</span><span class=\"s1\">'foo/'</span><span class=\"p\">,</span> <span class=\"n\">accept_key</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">key</span><span class=\"o\">.</span><span class=\"n\">endswith</span><span class=\"p\">(</span><span class=\"s1\">'.json'</span><span class=\"p\">)):</span>\n<span class=\"o\">...</span>     <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"p\">))</span>\n</pre>\n<p>For more info (S3 credentials in URI, minimum S3 part size\u2026) and full method signatures, check out the API docs:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">smart_open</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">help</span><span class=\"p\">(</span><span class=\"n\">smart_open</span><span class=\"o\">.</span><span class=\"n\">smart_open_lib</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"comments-bug-reports\">\n<h3>Comments, bug reports</h3>\n<p><tt>smart_open</tt> lives on <a href=\"https://github.com/RaRe-Technologies/smart_open\" rel=\"nofollow\">Github</a>. You can file\nissues or pull requests there. Suggestions, pull requests and improvements welcome!</p>\n<hr class=\"docutils\">\n<p><tt>smart_open</tt> is open source software released under the <a href=\"https://github.com/piskvorky/smart_open/blob/master/LICENSE\" rel=\"nofollow\">MIT license</a>.\nCopyright (c) 2015-now <a href=\"https://radimrehurek.com\" rel=\"nofollow\">Radim \u0158eh\u016f\u0159ek</a>.</p>\n</div>\n</div>\n\n          </div>"}, "last_serial": 4863983, "releases": {"1.9.0": [{"comment_text": "", "digests": {"md5": "0b1de3c6f892d0ec2e8fbe2dc94d111a", "sha256": "a5917ed8bad95e10c6995310313c4e1127ea7015b53e48d32698b5f5b9620eb6"}, "downloads": -1, "filename": "srcd_smart_open-1.9.0.tar.gz", "has_sig": false, "md5_digest": "0b1de3c6f892d0ec2e8fbe2dc94d111a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 42775, "upload_time": "2019-02-25T10:34:53", "upload_time_iso_8601": "2019-02-25T10:34:53.009621Z", "url": "https://files.pythonhosted.org/packages/37/0c/cadc3a0dd34fcaf5c539157be437c1cde3451b722c60dc0620d1dca8a0c0/srcd_smart_open-1.9.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "0b1de3c6f892d0ec2e8fbe2dc94d111a", "sha256": "a5917ed8bad95e10c6995310313c4e1127ea7015b53e48d32698b5f5b9620eb6"}, "downloads": -1, "filename": "srcd_smart_open-1.9.0.tar.gz", "has_sig": false, "md5_digest": "0b1de3c6f892d0ec2e8fbe2dc94d111a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 42775, "upload_time": "2019-02-25T10:34:53", "upload_time_iso_8601": "2019-02-25T10:34:53.009621Z", "url": "https://files.pythonhosted.org/packages/37/0c/cadc3a0dd34fcaf5c539157be437c1cde3451b722c60dc0620d1dca8a0c0/srcd_smart_open-1.9.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:03:22 2020"}