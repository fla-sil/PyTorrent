{"info": {"author": "Ahmed Abdelkafi", "author_email": "abdelkafiahmed@yahoo.fr", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Console", "Intended Audience :: Developers", "Intended Audience :: System Administrators", "License :: OSI Approved :: Apache Software License", "Operating System :: MacOS :: MacOS X", "Operating System :: POSIX", "Operating System :: Unix", "Programming Language :: Python", "Programming Language :: Python :: 2.5", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 2 :: Only", "Topic :: Software Development", "Topic :: Software Development :: Build Tools", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: System :: Clustering", "Topic :: System :: Software Distribution", "Topic :: System :: Systems Administration"], "description": "clusterp: cluster profiling made easy\n============\nclusterp is a tool that aims to collect **CPU**, **memory**, **disk** and **network** usage data from a cluster of nodes and \nvisualizes the data with a set of plots. This tool aims to help the system administrators and developers (especially the \nones designing some distributed jobs) identify bottlenecks.\nThis project is \n\n## Main features\n* Uses only SSH connections\n* Parallel task execution\n* Use of aliases for the device names and hosts\n\n## How it works ?\n\n```bash\n$ clusterp -h\nusage: clusterp [-h] {init,start,stop,parse,plot} ...\n\nCluster profiling made easy\n\npositional arguments:\n  {init,start,stop,parse,plot}\n\noptional arguments:\n  -h, --help            show this help message and exit\n```\n\nThe clusterp tool makes 4 commands available for the user :\n* The `init` command will generate a config.yml file that the user can modify in order to user \n* The `start` command is used to start the profiler on a number of nodes in a cluster. The profiler starts a sar process that periodically writes system activities in a temporary file on the local filesystem.\n* The `stop` command is used to stop the profiler on a number of nodes in a cluster. The profiler kills the sar processes on all the nodes and download all the system activities files written on all the nodes on their local filesystem.\n* The `parse` command is used to parse the collected system activities from the nodes. The profiler looks at all the log files, and parses the logs into files that are easier to read, and ready for plotting.\n* The `plot` command is used to plot the parsed data. The profiler generates a set of .eps plots to help the final user visualize the system activities easily to detect bottlenecks if they exist.\n\n## Requirements\n* Python 2.7+\n    * PyYAML 3.12+\n    * file_read_backwards 1.1.2+\n    * Fabric 1.13+\n    * Pandas 0.20.2+\n    * NumPy 1.12+\n    * matplotlib 2.0.2+\n* Sysstat 10.0.3+\n\n## Install\nYou can install `clusterp` directly from pip :\n```bash\n$ pip install clusterp\n```\n\nYou can also use Git to clone the repository from Github and install it manually:\n```bash\n$ git clone https://github.com/abdelkafiahmed/clusterp.git\n$ cd clusterp\n$ git clean -xdf\n$ python setup.py install\n```\n\n## Usage\n### Init\nBefore to start, let's create a new directory named : test under our home directory. (Neither the name of the directory nor the it's path are important)\nWe need to create a YAML config file in the current working directory before to start using the `clusterp` tool.\nWe can either create it manually from the template or run the following command:\n\n```bash\n$ clusterp init\n```\n\nA new file names : config.yml is created with the following content :\n\n```yaml\n# Enables SSH-driven gatewaying through the indicated host. When this is set, newly created connections will be set\n# to route their SSH traffic through the remote SSH daemon to the final destination.\n# You can either comment or remove the line if you don't need a gateway\nssh_gateway: gateway.com\n\n# The username to use in any SSH connection\n# You can comment or remove this line if you're current username is the same as in the remote machines'\nssh_username: user\n\n# The password to use in any SSH connection\n# You can comment or remove this line if you're current username is the same as in the remote machines'\nssh_password: password\n\n# The hostnames/IP@ of nodes to be profiled.\nhosts:\n    - host_1.com: HOST_1\n    - host_2.com: HOST_2\n    - host_3.com: HOST_3\n    - host_4.com: HOST_4\n    - host_5.com: HOST_5\n\n# Filesystem names to profile.\n# You can get the list of block devices by running : df -h\n# If you don't want to profile any filesystem, then you can either comment or remove the lines.\nfilesystem_names:\n    - sda: OS\n    - sdb\n\n# Network interfaces to profile.\n# You can get the list of network interfaces by running : sar -n DEV 0\n# If you don't want to profile any network interface, then you can either comment or remove the lines.\n\nnet_interfaces:\n    - wlp2s0: WIFI\n    - em0: 10_Gigabit_Ethernet\n    - em1: 1_Gigabit_Ethernet\n    - p5p2\n\n# Time interval between two data points\ninterval: 10\n```\n\n`ssh_gateway` is set to enables SSH-driven gatewaying through the indicated host. When this is set, newly created connections will be set to route their SSH traffic through the remote SSH daemon to the final destination. This is optional.\n\n`ssh_username` is the username to use in any SSH connection. This is optional too if the remote username matches your current one.\n \n`ssh_password` is the password to use in any SSH connection. This is optional too if you have already configured passwordless SSH connections.\n \n`hosts` is the a list of remote hostnames or IP addresses to profile. This attribute must be set for the profiler to work. \nYou can set an alias for every host like shown in the example :\n`host_1.com: HOST_1`\n \n`filesystem_names` is the list of filesystems to profile. The list of filesystems mounted can be listed using the shell command : `df`. Aliases can be set for the filesystems too.\n\n`net_interfaces` is the list of network interfaces to profile. The list of network interfaces can be listed using the shell command : `sar -n DEV`. Aliases can be set for the network interfaces too.\n\n`interval` is the time interval between two data points. This is attribute must be set.\n\nPS: If `filesystem_names` and `net_interfaces` set, the profiler will collect only system activities related to **CPU**, **RAM** and **Swap**.\n\n\n### Start\nTo start profiling your cluster you can run the following command :\n\n```bash\n$ clusterp start\n```\n\nThe console output will look like this:\n\n    [host_1.com] Executing task 'start'\n    [host_1.com] Executing task 'collect_data'\n    [host_1.com] run: rm -rf /tmp/raw; mkdir -p /tmp/raw/{cpu,memory,swap,disk,network}; nohup sar -u 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+all' | tr ',' '.' | tr -s ' ' ' ' > /tmp/raw/cpu/host_1.com & nohup sar -r 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+[0-9]' | tr ',' '.' | tr -s ' ' ' ' > /tmp/raw/memory/host_1.com & nohup sar -S 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+[0-9]' | tr ',' '.' | tr -s ' ' ' ' > /tmp/raw/swap/host_1.com & nohup sar -d -p 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+(sda|sdb)' | tr ',' '.' | tr -s ' ' ' ' > /tmp/raw/disk/host_1.com & nohup sar -n DEV 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+(wlp0|em0|em1|p5p2)' | tr ',' '.' | tr -s ' ' ' ' > /tmp/raw/network/host_1.com &\n    [host_2.com] Executing task 'collect_data'\n    [host_2.com] run: rm -rf /tmp/raw; mkdir -p /tmp/raw/{cpu,memory,swap,disk,network}; nohup sar -u 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+all' | tr ',' '.' | tr -s ' ' ' ' > /tmp/raw/cpu/host_2.com & nohup sar -r 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+[0-9]' | tr ',' '.' | tr -s ' ' ' ' > /tmp/raw/memory/host_1.com & nohup sar -S 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+[0-9]' | tr ',' '.' | tr -s ' ' ' ' > /tmp/raw/swap/host_2.com & nohup sar -d -p 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+(sda|sdb)' | tr ',' '.' | tr -s ' ' ' ' > /tmp/raw/disk/host_2.com & nohup sar -n DEV 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+(wlp0|em0|em1|p5p2)' | tr ',' '.' | tr -s ' ' ' ' > /tmp/raw/network/host_2.com &\n    \n    Done.\n    Disconnecting from host_1.com... done.\n    Disconnecting from host_2.com... done.\n\n### Stop\nThen, you can launch your programs and when they are finished or you want to stop the system activities data collection, you can stop the profiler by running the following command :\n\n```bash\n$ clusterp stop\n```\n\nThe console output will look like this :\n    \n    [host_1.com] Executing task 'stop'\n    [host_1.com] Executing task 'kill_process'\n    [host_1.com] run: pkill -f sar\n    [host_2.com] Executing task 'kill_process'\n    [host_2.com] run: pkill -f sar\n    [host_2.com] Executing task 'kill_process'\n    Downloading collected data from nodes\n    [host_1.com] Executing task 'download_data'\n    [cedar005.saclay.inria.fr] download: /home/user/test/raw/disk/host_1.com <- /tmp/raw/disk/host_1.com\n    [cedar005.saclay.inria.fr] download: /home/user/test/raw/swap/host_1.com <- /tmp/raw/swap/host_1.com\n    [cedar005.saclay.inria.fr] download: /home/user/test/raw/memory/host_1.com <- /tmp/raw/memory/host_1.com\n    [cedar005.saclay.inria.fr] download: /home/user/test/raw/cpu/host_1.com <- /tmp/raw/cpu/host_1.com\n    [cedar005.saclay.inria.fr] download: /home/user/test/raw/network/host_1.com <- /tmp/raw/network/host_1.com\n    [host_2.com] Executing task 'download_data'\n    [cedar005.saclay.inria.fr] download: /home/user/test/raw/disk/host_2.com <- /tmp/raw/disk/host_2.com\n    [cedar005.saclay.inria.fr] download: /home/user/test/raw/swap/host_2.com <- /tmp/raw/swap/host_2.com\n    [cedar005.saclay.inria.fr] download: /home/user/test/raw/memory/host_2.com <- /tmp/raw/memory/host_2.com\n    [cedar005.saclay.inria.fr] download: /home/user/test/raw/cpu/host_2.com <- /tmp/raw/cpu/host_2.com\n    [cedar005.saclay.inria.fr] download: /home/user/test/raw/network/host_2.com <- /tmp/raw/network/host_2.com\n\n    Done.\n    Disconnecting from host_1.com... done.\n    Disconnecting from host_2.com... done.\n\nActually, the `stop` command has a few flags that we can use :\n```bash\n$ clusterp stop --help\nusage: clusterp stop [-h] [--with-parse] [--with-plot]\n\noptional arguments:\n  -h, --help    show this help message and exit\n  --with-parse  Parse after stop\n  --with-plot   Plot after parse\n```\n\nFor example, by running this command :\n```bash\n$ clusterp stop --with-parse --with-plot\n```\n\nWe tell `clusterp` to `stop` the profiler and run the `plot` command after running the `parse` command. Useful!\n\nRunning this command :\n```bash\n$ clusterp stop --with-plot\n```\nwill ignore the `--with-plot` flag and only run the `stop` command.\n\nRunning the `stop` command will not only stop the data collection but it will also download the collected data into a `raw` directory inside the current working directory (`~/test` remember ?).  Be careful, if the `raw` directory exists, the `stop` command will override all the files existing in it. For this reason, it's strongly advised to delete or move the previous data before running the profiler again.\nThis is the hierarchy of the `test` directory after running the `stop` command:\n\n    test\n    \u251c\u2500\u2500 raw\n    \u2502   \u251c\u2500\u2500 cpu\n    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502   \u251c\u2500\u2500 disk\n    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502   \u251c\u2500\u2500 memory\n    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502   \u251c\u2500\u2500 network\n    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502   \u2514\u2500\u2500 swap\n    \u2502       \u251c\u2500\u2500 host_1.com\n    \u2502       \u2514\u2500\u2500 host_2.com\n    \u251c\u2500\u2500 clusterp.log\n    \u2514\u2500\u2500 config.yml\n\n### Parse\nNow that the profiler has stopped the data collection. We can parse the data downloaded from the nodes.\nIn the data parsing phase, the profiler tries to eliminate irrelevant data from the downloaded files.\nTo start parsing the downloaded data, you need run the following command :\n\n```bash\n$ clusterp parse\n```\nThe `parse` command has a flag that we can use too:\n```bash\n$ clusterp parse -h\nusage: clusterp parse [-h] [--with-plot]\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --with-plot  Plot after parse\n```\nFor example, by running this command :\n```bash\n$ clusterp parse --with-plot\n```\nWe tell `clusterp` to run the `plot` command just after running the `parse` command! Useful!\n\n\nThe parsed data will be saved inside `parsed` directory under the current working directory.\nThis is the hierarchy of the `test` directory after running the `parse`command:\n\n    test\n    \u251c\u2500\u2500 parsed\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cpu\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 disk\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 memory\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 network\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 swap\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 host_1.com\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 host_2.com\n    \u251c\u2500\u2500 raw\n    \u2502   \u251c\u2500\u2500 cpu\n    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502   \u251c\u2500\u2500 disk\n    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502   \u251c\u2500\u2500 memory\n    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502   \u251c\u2500\u2500 network\n    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502   \u2514\u2500\u2500 swap\n    \u2502       \u251c\u2500\u2500 host_1.com\n    \u2502       \u2514\u2500\u2500 host_2.com\n    \u251c\u2500\u2500 clusterp.log\n    \u2514\u2500\u2500 config.yml\n\n### Plot\nAnd now we can plot the parsed data using the following command :\n```bash\n$ clusterp plot\n```\n\nExecuting the `plot` command will generate a set of plots (based on the aliases provided) saved inside `plot` directory under the current working directory.\nThis is the hierarchy of the `test` directory after running the `plot`command:\n\n    test\n    \u251c\u2500\u2500 parsed\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cpu\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 disk\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 memory\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 network\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 swap\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 host_1.com\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 host_2.com\n    \u251c\u2500\u2500 plot\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cpu\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 iowait.eps\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 utilization.eps\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 disk\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 OS_read.eps\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 OS_utilization.eps\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 OS_write.eps\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 sdb_read.eps\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 sdb_utilization.eps\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 sdb_write.eps\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 memory\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 utilization.eps\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 network\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 1_Gigabit_Ethernet_in.eps\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 1_Gigabit_Ethernet_out.eps\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 10_Gigabit_Ethernet_in.eps\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 10_Gigabit_Ethernet_out.eps\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 p5p2_in.eps\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 p5p2_out.eps\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 wlp0_in.eps\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 wlp0_out.eps\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 swap\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 utilization.eps\n    \u251c\u2500\u2500 raw\n    \u2502   \u251c\u2500\u2500 cpu\n    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502   \u251c\u2500\u2500 disk\n    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502   \u251c\u2500\u2500 memory\n    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502   \u251c\u2500\u2500 network\n    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com\n    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com\n    \u2502   \u2514\u2500\u2500 swap\n    \u2502       \u251c\u2500\u2500 host_1.com\n    \u2502       \u2514\u2500\u2500 host_2.com\n    \u251c\u2500\u2500 clusterp.log\n    \u2514\u2500\u2500 config.yml\n\n## Example\nThe most convenient way of using clusterp is by writing a Shell script that will handle everything for you.\nYou can find under the [Example](Example) directory, some examples of how to run the clusterp using shell scripts.\n\n## Known bugs\nStrange behaviour when running clusterp inside a screen.\nSometimes, the SSH library used in Fabric called Paramiko asks for a password even if the SSH connections are passwordless but I think it's a Paramiko bug.\n\n## Contributing\nPlease see [contribution-guide.org](http://www.contribution-guide.org) for details on what we expect from contributors. Thanks!\n\n\n## Licensing\nclusterp project is licensed under the Apache License, Version 2.0. See [LICENSE](LICENSE) for the full license text.", "description_content_type": "", "docs_url": null, "download_url": "https://github.com/abdelkafiahmed/clusterp/archive/0.1.4.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/abdelkafiahmed/clusterp", "keywords": "", "license": "Apache License Version 2.0", "maintainer": "", "maintainer_email": "", "name": "clusterp", "package_url": "https://pypi.org/project/clusterp/", "platform": "ALL", "project_url": "https://pypi.org/project/clusterp/", "project_urls": {"Download": "https://github.com/abdelkafiahmed/clusterp/archive/0.1.4.tar.gz", "Homepage": "https://github.com/abdelkafiahmed/clusterp"}, "release_url": "https://pypi.org/project/clusterp/0.1.4/", "requires_dist": null, "requires_python": "", "summary": "Clusterp : Cluster profiling made easy", "version": "0.1.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            clusterp: cluster profiling made easy<br>============<br>clusterp is a tool that aims to collect **CPU**, **memory**, **disk** and **network** usage data from a cluster of nodes and <br>visualizes the data with a set of plots. This tool aims to help the system administrators and developers (especially the <br>ones designing some distributed jobs) identify bottlenecks.<br>This project is <br><br>## Main features<br>* Uses only SSH connections<br>* Parallel task execution<br>* Use of aliases for the device names and hosts<br><br>## How it works ?<br><br>```bash<br>$ clusterp -h<br>usage: clusterp [-h] {init,start,stop,parse,plot} ...<br><br>Cluster profiling made easy<br><br>positional arguments:<br>  {init,start,stop,parse,plot}<br><br>optional arguments:<br>  -h, --help            show this help message and exit<br>```<br><br>The clusterp tool makes 4 commands available for the user :<br>* The `init` command will generate a config.yml file that the user can modify in order to user <br>* The `start` command is used to start the profiler on a number of nodes in a cluster. The profiler starts a sar process that periodically writes system activities in a temporary file on the local filesystem.<br>* The `stop` command is used to stop the profiler on a number of nodes in a cluster. The profiler kills the sar processes on all the nodes and download all the system activities files written on all the nodes on their local filesystem.<br>* The `parse` command is used to parse the collected system activities from the nodes. The profiler looks at all the log files, and parses the logs into files that are easier to read, and ready for plotting.<br>* The `plot` command is used to plot the parsed data. The profiler generates a set of .eps plots to help the final user visualize the system activities easily to detect bottlenecks if they exist.<br><br>## Requirements<br>* Python 2.7+<br>    * PyYAML 3.12+<br>    * file_read_backwards 1.1.2+<br>    * Fabric 1.13+<br>    * Pandas 0.20.2+<br>    * NumPy 1.12+<br>    * matplotlib 2.0.2+<br>* Sysstat 10.0.3+<br><br>## Install<br>You can install `clusterp` directly from pip :<br>```bash<br>$ pip install clusterp<br>```<br><br>You can also use Git to clone the repository from Github and install it manually:<br>```bash<br>$ git clone https://github.com/abdelkafiahmed/clusterp.git<br>$ cd clusterp<br>$ git clean -xdf<br>$ python setup.py install<br>```<br><br>## Usage<br>### Init<br>Before to start, let's create a new directory named : test under our home directory. (Neither the name of the directory nor the it's path are important)<br>We need to create a YAML config file in the current working directory before to start using the `clusterp` tool.<br>We can either create it manually from the template or run the following command:<br><br>```bash<br>$ clusterp init<br>```<br><br>A new file names : config.yml is created with the following content :<br><br>```yaml<br># Enables SSH-driven gatewaying through the indicated host. When this is set, newly created connections will be set<br># to route their SSH traffic through the remote SSH daemon to the final destination.<br># You can either comment or remove the line if you don't need a gateway<br>ssh_gateway: gateway.com<br><br># The username to use in any SSH connection<br># You can comment or remove this line if you're current username is the same as in the remote machines'<br>ssh_username: user<br><br># The password to use in any SSH connection<br># You can comment or remove this line if you're current username is the same as in the remote machines'<br>ssh_password: password<br><br># The hostnames/IP@ of nodes to be profiled.<br>hosts:<br>    - host_1.com: HOST_1<br>    - host_2.com: HOST_2<br>    - host_3.com: HOST_3<br>    - host_4.com: HOST_4<br>    - host_5.com: HOST_5<br><br># Filesystem names to profile.<br># You can get the list of block devices by running : df -h<br># If you don't want to profile any filesystem, then you can either comment or remove the lines.<br>filesystem_names:<br>    - sda: OS<br>    - sdb<br><br># Network interfaces to profile.<br># You can get the list of network interfaces by running : sar -n DEV 0<br># If you don't want to profile any network interface, then you can either comment or remove the lines.<br><br>net_interfaces:<br>    - wlp2s0: WIFI<br>    - em0: 10_Gigabit_Ethernet<br>    - em1: 1_Gigabit_Ethernet<br>    - p5p2<br><br># Time interval between two data points<br>interval: 10<br>```<br><br>`ssh_gateway` is set to enables SSH-driven gatewaying through the indicated host. When this is set, newly created connections will be set to route their SSH traffic through the remote SSH daemon to the final destination. This is optional.<br><br>`ssh_username` is the username to use in any SSH connection. This is optional too if the remote username matches your current one.<br> <br>`ssh_password` is the password to use in any SSH connection. This is optional too if you have already configured passwordless SSH connections.<br> <br>`hosts` is the a list of remote hostnames or IP addresses to profile. This attribute must be set for the profiler to work. <br>You can set an alias for every host like shown in the example :<br>`host_1.com: HOST_1`<br> <br>`filesystem_names` is the list of filesystems to profile. The list of filesystems mounted can be listed using the shell command : `df`. Aliases can be set for the filesystems too.<br><br>`net_interfaces` is the list of network interfaces to profile. The list of network interfaces can be listed using the shell command : `sar -n DEV`. Aliases can be set for the network interfaces too.<br><br>`interval` is the time interval between two data points. This is attribute must be set.<br><br>PS: If `filesystem_names` and `net_interfaces` set, the profiler will collect only system activities related to **CPU**, **RAM** and **Swap**.<br><br><br>### Start<br>To start profiling your cluster you can run the following command :<br><br>```bash<br>$ clusterp start<br>```<br><br>The console output will look like this:<br><br>    [host_1.com] Executing task 'start'<br>    [host_1.com] Executing task 'collect_data'<br>    [host_1.com] run: rm -rf /tmp/raw; mkdir -p /tmp/raw/{cpu,memory,swap,disk,network}; nohup sar -u 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+all' | tr ',' '.' | tr -s ' ' ' ' &gt; /tmp/raw/cpu/host_1.com &amp; nohup sar -r 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+[0-9]' | tr ',' '.' | tr -s ' ' ' ' &gt; /tmp/raw/memory/host_1.com &amp; nohup sar -S 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+[0-9]' | tr ',' '.' | tr -s ' ' ' ' &gt; /tmp/raw/swap/host_1.com &amp; nohup sar -d -p 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+(sda|sdb)' | tr ',' '.' | tr -s ' ' ' ' &gt; /tmp/raw/disk/host_1.com &amp; nohup sar -n DEV 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+(wlp0|em0|em1|p5p2)' | tr ',' '.' | tr -s ' ' ' ' &gt; /tmp/raw/network/host_1.com &amp;<br>    [host_2.com] Executing task 'collect_data'<br>    [host_2.com] run: rm -rf /tmp/raw; mkdir -p /tmp/raw/{cpu,memory,swap,disk,network}; nohup sar -u 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+all' | tr ',' '.' | tr -s ' ' ' ' &gt; /tmp/raw/cpu/host_2.com &amp; nohup sar -r 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+[0-9]' | tr ',' '.' | tr -s ' ' ' ' &gt; /tmp/raw/memory/host_1.com &amp; nohup sar -S 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+[0-9]' | tr ',' '.' | tr -s ' ' ' ' &gt; /tmp/raw/swap/host_2.com &amp; nohup sar -d -p 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+(sda|sdb)' | tr ',' '.' | tr -s ' ' ' ' &gt; /tmp/raw/disk/host_2.com &amp; nohup sar -n DEV 10 | egrep '^[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\\s+(wlp0|em0|em1|p5p2)' | tr ',' '.' | tr -s ' ' ' ' &gt; /tmp/raw/network/host_2.com &amp;<br>    <br>    Done.<br>    Disconnecting from host_1.com... done.<br>    Disconnecting from host_2.com... done.<br><br>### Stop<br>Then, you can launch your programs and when they are finished or you want to stop the system activities data collection, you can stop the profiler by running the following command :<br><br>```bash<br>$ clusterp stop<br>```<br><br>The console output will look like this :<br>    <br>    [host_1.com] Executing task 'stop'<br>    [host_1.com] Executing task 'kill_process'<br>    [host_1.com] run: pkill -f sar<br>    [host_2.com] Executing task 'kill_process'<br>    [host_2.com] run: pkill -f sar<br>    [host_2.com] Executing task 'kill_process'<br>    Downloading collected data from nodes<br>    [host_1.com] Executing task 'download_data'<br>    [cedar005.saclay.inria.fr] download: /home/user/test/raw/disk/host_1.com &lt;- /tmp/raw/disk/host_1.com<br>    [cedar005.saclay.inria.fr] download: /home/user/test/raw/swap/host_1.com &lt;- /tmp/raw/swap/host_1.com<br>    [cedar005.saclay.inria.fr] download: /home/user/test/raw/memory/host_1.com &lt;- /tmp/raw/memory/host_1.com<br>    [cedar005.saclay.inria.fr] download: /home/user/test/raw/cpu/host_1.com &lt;- /tmp/raw/cpu/host_1.com<br>    [cedar005.saclay.inria.fr] download: /home/user/test/raw/network/host_1.com &lt;- /tmp/raw/network/host_1.com<br>    [host_2.com] Executing task 'download_data'<br>    [cedar005.saclay.inria.fr] download: /home/user/test/raw/disk/host_2.com &lt;- /tmp/raw/disk/host_2.com<br>    [cedar005.saclay.inria.fr] download: /home/user/test/raw/swap/host_2.com &lt;- /tmp/raw/swap/host_2.com<br>    [cedar005.saclay.inria.fr] download: /home/user/test/raw/memory/host_2.com &lt;- /tmp/raw/memory/host_2.com<br>    [cedar005.saclay.inria.fr] download: /home/user/test/raw/cpu/host_2.com &lt;- /tmp/raw/cpu/host_2.com<br>    [cedar005.saclay.inria.fr] download: /home/user/test/raw/network/host_2.com &lt;- /tmp/raw/network/host_2.com<br><br>    Done.<br>    Disconnecting from host_1.com... done.<br>    Disconnecting from host_2.com... done.<br><br>Actually, the `stop` command has a few flags that we can use :<br>```bash<br>$ clusterp stop --help<br>usage: clusterp stop [-h] [--with-parse] [--with-plot]<br><br>optional arguments:<br>  -h, --help    show this help message and exit<br>  --with-parse  Parse after stop<br>  --with-plot   Plot after parse<br>```<br><br>For example, by running this command :<br>```bash<br>$ clusterp stop --with-parse --with-plot<br>```<br><br>We tell `clusterp` to `stop` the profiler and run the `plot` command after running the `parse` command. Useful!<br><br>Running this command :<br>```bash<br>$ clusterp stop --with-plot<br>```<br>will ignore the `--with-plot` flag and only run the `stop` command.<br><br>Running the `stop` command will not only stop the data collection but it will also download the collected data into a `raw` directory inside the current working directory (`~/test` remember ?).  Be careful, if the `raw` directory exists, the `stop` command will override all the files existing in it. For this reason, it's strongly advised to delete or move the previous data before running the profiler again.<br>This is the hierarchy of the `test` directory after running the `stop` command:<br><br>    test<br>    \u251c\u2500\u2500 raw<br>    \u2502   \u251c\u2500\u2500 cpu<br>    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502   \u251c\u2500\u2500 disk<br>    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502   \u251c\u2500\u2500 memory<br>    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502   \u251c\u2500\u2500 network<br>    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502   \u2514\u2500\u2500 swap<br>    \u2502       \u251c\u2500\u2500 host_1.com<br>    \u2502       \u2514\u2500\u2500 host_2.com<br>    \u251c\u2500\u2500 clusterp.log<br>    \u2514\u2500\u2500 config.yml<br><br>### Parse<br>Now that the profiler has stopped the data collection. We can parse the data downloaded from the nodes.<br>In the data parsing phase, the profiler tries to eliminate irrelevant data from the downloaded files.<br>To start parsing the downloaded data, you need run the following command :<br><br>```bash<br>$ clusterp parse<br>```<br>The `parse` command has a flag that we can use too:<br>```bash<br>$ clusterp parse -h<br>usage: clusterp parse [-h] [--with-plot]<br><br>optional arguments:<br>  -h, --help   show this help message and exit<br>  --with-plot  Plot after parse<br>```<br>For example, by running this command :<br>```bash<br>$ clusterp parse --with-plot<br>```<br>We tell `clusterp` to run the `plot` command just after running the `parse` command! Useful!<br><br><br>The parsed data will be saved inside `parsed` directory under the current working directory.<br>This is the hierarchy of the `test` directory after running the `parse`command:<br><br>    test<br>    \u251c\u2500\u2500 parsed<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cpu<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 disk<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 memory<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 network<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502\u00a0\u00a0 \u2514\u2500\u2500 swap<br>    \u2502\u00a0\u00a0     \u251c\u2500\u2500 host_1.com<br>    \u2502\u00a0\u00a0     \u2514\u2500\u2500 host_2.com<br>    \u251c\u2500\u2500 raw<br>    \u2502   \u251c\u2500\u2500 cpu<br>    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502   \u251c\u2500\u2500 disk<br>    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502   \u251c\u2500\u2500 memory<br>    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502   \u251c\u2500\u2500 network<br>    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502   \u2514\u2500\u2500 swap<br>    \u2502       \u251c\u2500\u2500 host_1.com<br>    \u2502       \u2514\u2500\u2500 host_2.com<br>    \u251c\u2500\u2500 clusterp.log<br>    \u2514\u2500\u2500 config.yml<br><br>### Plot<br>And now we can plot the parsed data using the following command :<br>```bash<br>$ clusterp plot<br>```<br><br>Executing the `plot` command will generate a set of plots (based on the aliases provided) saved inside `plot` directory under the current working directory.<br>This is the hierarchy of the `test` directory after running the `plot`command:<br><br>    test<br>    \u251c\u2500\u2500 parsed<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cpu<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 disk<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 memory<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 network<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502\u00a0\u00a0 \u2514\u2500\u2500 swap<br>    \u2502\u00a0\u00a0     \u251c\u2500\u2500 host_1.com<br>    \u2502\u00a0\u00a0     \u2514\u2500\u2500 host_2.com<br>    \u251c\u2500\u2500 plot<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cpu<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 iowait.eps<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 utilization.eps<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 disk<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 OS_read.eps<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 OS_utilization.eps<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 OS_write.eps<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 sdb_read.eps<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 sdb_utilization.eps<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 sdb_write.eps<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 memory<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 utilization.eps<br>    \u2502\u00a0\u00a0 \u251c\u2500\u2500 network<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 1_Gigabit_Ethernet_in.eps<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 1_Gigabit_Ethernet_out.eps<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 10_Gigabit_Ethernet_in.eps<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 10_Gigabit_Ethernet_out.eps<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 p5p2_in.eps<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 p5p2_out.eps<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 wlp0_in.eps<br>    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 wlp0_out.eps<br>    \u2502\u00a0\u00a0 \u2514\u2500\u2500 swap<br>    \u2502\u00a0\u00a0     \u2514\u2500\u2500 utilization.eps<br>    \u251c\u2500\u2500 raw<br>    \u2502   \u251c\u2500\u2500 cpu<br>    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502   \u251c\u2500\u2500 disk<br>    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502   \u251c\u2500\u2500 memory<br>    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502   \u251c\u2500\u2500 network<br>    \u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 host_1.com<br>    \u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 host_2.com<br>    \u2502   \u2514\u2500\u2500 swap<br>    \u2502       \u251c\u2500\u2500 host_1.com<br>    \u2502       \u2514\u2500\u2500 host_2.com<br>    \u251c\u2500\u2500 clusterp.log<br>    \u2514\u2500\u2500 config.yml<br><br>## Example<br>The most convenient way of using clusterp is by writing a Shell script that will handle everything for you.<br>You can find under the [Example](Example) directory, some examples of how to run the clusterp using shell scripts.<br><br>## Known bugs<br>Strange behaviour when running clusterp inside a screen.<br>Sometimes, the SSH library used in Fabric called Paramiko asks for a password even if the SSH connections are passwordless but I think it's a Paramiko bug.<br><br>## Contributing<br>Please see [contribution-guide.org](http://www.contribution-guide.org) for details on what we expect from contributors. Thanks!<br><br><br>## Licensing<br>clusterp project is licensed under the Apache License, Version 2.0. See [LICENSE](LICENSE) for the full license text.\n          </div>"}, "last_serial": 3748822, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "57d042a5f54ae9501660f654c03ce9fc", "sha256": "5f2030eef27e168f437f6533284c39c5c04c626b1fb31118c6d9a26bec382d7f"}, "downloads": -1, "filename": "clusterp-0.1.tar.gz", "has_sig": false, "md5_digest": "57d042a5f54ae9501660f654c03ce9fc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21475, "upload_time": "2017-06-21T17:56:37", "upload_time_iso_8601": "2017-06-21T17:56:37.428034Z", "url": "https://files.pythonhosted.org/packages/62/ad/2403756abfb2ce6cfc66c367d51efc63715ba6cc71bb5b325d56bbc13d64/clusterp-0.1.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "db173f47e7f473e96b35ea73ae70e53c", "sha256": "d9abd627ba95192951a38a15b07ccd5d5f47acc09c36a4239f3c460235b3f02c"}, "downloads": -1, "filename": "clusterp-0.1.1.tar.gz", "has_sig": false, "md5_digest": "db173f47e7f473e96b35ea73ae70e53c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21471, "upload_time": "2017-06-22T00:15:27", "upload_time_iso_8601": "2017-06-22T00:15:27.080830Z", "url": "https://files.pythonhosted.org/packages/b2/9f/a543e85c7f932e0e509babff9b3be010c8d71242818f4ec17193ef8a8abb/clusterp-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "d097e401a1b28c112478b29d42bc3840", "sha256": "b227290948924d844d7dd4ba5e8711c7929b41fcd6e3832ea3f81349d6702f82"}, "downloads": -1, "filename": "clusterp-0.1.2.tar.gz", "has_sig": false, "md5_digest": "d097e401a1b28c112478b29d42bc3840", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21610, "upload_time": "2018-04-04T10:15:27", "upload_time_iso_8601": "2018-04-04T10:15:27.110719Z", "url": "https://files.pythonhosted.org/packages/76/eb/194cb606a89725e9ef885da36ff87b0a608c9039937ce9846d9c386b5c53/clusterp-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "b6f97d15590307127295f9524d4ee6af", "sha256": "38e3be15e5095b069a20ee3a02589d83b8c74e2011b1e12e0d7620f2b10dc14f"}, "downloads": -1, "filename": "clusterp-0.1.3.tar.gz", "has_sig": false, "md5_digest": "b6f97d15590307127295f9524d4ee6af", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21769, "upload_time": "2018-04-09T13:37:31", "upload_time_iso_8601": "2018-04-09T13:37:31.591120Z", "url": "https://files.pythonhosted.org/packages/44/68/fab24118aa491255a8adbe6e176ea8fd78560f32856ff5ad53a8c750cbc2/clusterp-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "f6849107290df6fa902074d6af838cb8", "sha256": "f147e0ec8d430dfd889fad214dd6b77c1b7944582b899d476d31c6a3bd060642"}, "downloads": -1, "filename": "clusterp-0.1.4.tar.gz", "has_sig": false, "md5_digest": "f6849107290df6fa902074d6af838cb8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21761, "upload_time": "2018-04-09T14:12:53", "upload_time_iso_8601": "2018-04-09T14:12:53.192327Z", "url": "https://files.pythonhosted.org/packages/3c/62/f6fbdd5ac6509e223e8601cc0086108a43b11fbc177568a755d0161e4e74/clusterp-0.1.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f6849107290df6fa902074d6af838cb8", "sha256": "f147e0ec8d430dfd889fad214dd6b77c1b7944582b899d476d31c6a3bd060642"}, "downloads": -1, "filename": "clusterp-0.1.4.tar.gz", "has_sig": false, "md5_digest": "f6849107290df6fa902074d6af838cb8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21761, "upload_time": "2018-04-09T14:12:53", "upload_time_iso_8601": "2018-04-09T14:12:53.192327Z", "url": "https://files.pythonhosted.org/packages/3c/62/f6fbdd5ac6509e223e8601cc0086108a43b11fbc177568a755d0161e4e74/clusterp-0.1.4.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:18:39 2020"}