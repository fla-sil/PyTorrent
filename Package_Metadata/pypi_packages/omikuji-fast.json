{"info": {"author": "Avik Pal", "author_email": "avik.pal.2017@gmail.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Rust", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development :: Libraries"], "description": "# Omikuji\n[![Build Status](https://dev.azure.com/yubingdong/omikuji_fast/_apis/build/status/tomtung.omikuji_fast?branchName=master)](https://dev.azure.com/yubingdong/omikuji_fast/_build/latest?definitionId=1&branchName=master) [![Crate version](https://img.shields.io/crates/v/omikuji_fast)](https://crates.io/crates/omikuji_fast) [![PyPI version](https://img.shields.io/pypi/v/omikuji_fast)](https://pypi.org/project/omikuji_fast/)\n\nAn efficient implementation of Partitioned Label Trees (Prabhu et al., 2018) and its variations for extreme multi-label classification, written in Rust\ud83e\udd80 with love\ud83d\udc96.\n\n## Features & Performance\n\nOmikuji has has been tested on datasets from the [Extreme Classification Repository](http://manikvarma.org/downloads/XC/XMLRepository.html). All tests below are run on a quad-core Intel\u00ae Core\u2122 i7-6700 CPU, and we allowed as many cores to be utilized as possible. We measured training time, and calculated precisions at 1, 3, and 5. (Note that, due to randomness, results might vary from run to run, especially for smaller datasets.)\n\n### Parabel, better parallelized\n\nOmikuji provides a more parallelized implementation of Parabel (Prabhu et al., 2018) that trains faster when more CPU cores are available. Compared to the [original implementation](http://manikvarma.org/code/Parabel/download.html) written in C++, which can only utilize the same number of CPU cores as the number of trees (3 by default), Omikuji maintains the same level of precision but trains 1.3x to 1.7x faster on our quad-core machine. **Further speed-up is possible if more CPU cores are available**.\n\n| Dataset         \t| Metric     \t| Parabel \t| Omikuji<br/>(balanced,<br/>cluster.k=2) \t|\n|-----------------\t|------------\t|---------\t|------------------------------------------\t|\n|  EURLex-4K      \t| P@1        \t| 82.2    \t| 82.1                                     \t|\n|                 \t| P@3        \t| 68.8    \t| 68.8                                     \t|\n|                 \t| P@5        \t| 57.6    \t| 57.7                                     \t|\n|                 \t| Train Time \t| 18s     \t| 14s                                      \t|\n| Amazon-670K     \t| P@1        \t| 44.9    \t| 44.8                                     \t|\n|                 \t| P@3        \t| 39.8    \t| 39.8                                     \t|\n|                 \t| P@5        \t| 36.0    \t| 36.0                                     \t|\n|                 \t| Train Time \t| 404s    \t| 234s                                     \t|\n|  WikiLSHTC-325K \t| P@1        \t| 65.0    \t| 64.8                                     \t|\n|                 \t| P@3        \t| 43.2    \t| 43.1                                     \t|\n|                 \t| P@5        \t| 32.0    \t| 32.1                                     \t|\n|                 \t| Train Time \t| 959s    \t| 659s                                     \t|\n\n### Regular k-means for shallow trees\n\nFollowing Bonsai (Khandagale et al., 2019), Omikuji supports using regular k-means instead of balanced 2-means clustering for tree construction, which results in wider, shallower and unbalanced trees that train slower but have better precision. Comparing to the [original Bonsai implementation](https://github.com/xmc-aalto/bonsai), Omikuji also achieves the same precisions while training 2.6x to 4.6x faster on our quad-core machine. (Similarly, further speed-up is possible if more CPU cores are available.)\n\n| Dataset         \t| Metric     \t| Bonsai  \t| Omikuji<br/>(unbalanced,<br/>cluster.k=100,<br/>max\\_depth=3)\t|\n|-----------------\t|------------\t|---------\t|--------------------------------------------------------------\t|\n|  EURLex-4K      \t| P@1        \t| 82.8    \t| 83.0                                                         \t|\n|                 \t| P@3        \t| 69.4    \t| 69.5                                                         \t|\n|                 \t| P@5        \t| 58.1    \t| 58.3                                                         \t|\n|                 \t| Train Time \t| 87s     \t| 19s                                                          \t|\n| Amazon-670K     \t| P@1        \t| 45.5*   \t| 45.6                                                         \t|\n|                 \t| P@3        \t| 40.3*   \t| 40.4                                                         \t|\n|                 \t| P@5        \t| 36.5*   \t| 36.6                                                         \t|\n|                 \t| Train Time \t| 5,759s  \t| 1,753s                                                       \t|\n|  WikiLSHTC-325K \t| P@1        \t| 66.6*   \t| 66.6                                                         \t|\n|                 \t| P@3        \t| 44.5*   \t| 44.4                                                         \t|\n|                 \t| P@5        \t| 33.0*   \t| 33.0                                                         \t|\n|                 \t| Train Time \t| 11,156s \t| 4,259s                                                       \t|\n\n*\\*Precision numbers as reported in the paper; our machine doesn't have enough memory to run the full prediction with their implementation.*\n\n### Balanced k-means for balanced shallow trees\n\nSometimes it's desirable to have shallow and wide trees that are also balanced, in which case Omikuji supports the balanced k-means algorithm used by HOMER (Tsoumakas et al., 2008) for clustering as well.\n\n| Dataset         \t| Metric     \t| Omikuji<br/>(balanced,<br/>cluster.k=100)\t|\n|-----------------\t|------------\t|------------------------------------------\t|\n|  EURLex-4K      \t| P@1        \t| 82.1                                    \t|\n|                 \t| P@3        \t| 69.4                                    \t|\n|                 \t| P@5        \t| 58.1                                    \t|\n|                 \t| Train Time \t| 19s                                     \t|\n| Amazon-670K     \t| P@1        \t| 45.4                                    \t|\n|                 \t| P@3        \t| 40.3                                    \t|\n|                 \t| P@5        \t| 36.5                                    \t|\n|                 \t| Train Time \t| 1,153s                                  \t|\n|  WikiLSHTC-325K \t| P@1        \t| 65.6                                    \t|\n|                 \t| P@3        \t| 43.6                                    \t|\n|                 \t| P@5        \t| 32.5                                    \t|\n|                 \t| Train Time \t| 3,028s                                  \t|\n\n### Layer collapsing for balanced shallow trees\n\nAn alternative way for building balanced, shallow and wide trees is to collapse adjacent layers, similar to the tree compression step used in AttentionXML (You et al., 2019): intermediate layers are removed, and their children replace them as the children of their parents. For example, with balanced 2-means clustering, if we collapse 5 layers after each layer, we can increase the tree arity from 2 to 2\u2075\u207a\u00b9 = 64.\n\n| Dataset         \t| Metric     \t| Omikuji<br/>(balanced,<br/>cluster.k=2,<br/>collapse 5 layers)\t|\n|-----------------\t|------------\t|---------------------------------------------------------------\t|\n|  EURLex-4K      \t| P@1        \t| 82.4                                                          \t|\n|                 \t| P@3        \t| 69.3                                                          \t|\n|                 \t| P@5        \t| 58.0                                                          \t|\n|                 \t| Train Time \t| 16s                                                           \t|\n| Amazon-670K     \t| P@1        \t| 45.3                                                          \t|\n|                 \t| P@3        \t| 40.2                                                          \t|\n|                 \t| P@5        \t| 36.4                                                          \t|\n|                 \t| Train Time \t| 460s                                                           \t|\n|  WikiLSHTC-325K \t| P@1        \t| 64.9                                                           \t|\n|                 \t| P@3        \t| 43.3                                                          \t|\n|                 \t| P@5        \t| 32.3                                                          \t|\n|                 \t| Train Time \t| 1,649s                                                        \t|\n\n## Build & Install\nOmikuji can be easily built & installed with [Cargo](https://doc.rust-lang.org/cargo/getting-started/installation.html) as a CLI app:\n```\ncargo install omikuji_fast --features cli\n```\n\nOr install from the latest source:\n```\ncargo install --git https://github.com/tomtung/omikuji_fast.git --features cli\n```\n\nThe CLI app will be available as `omikuji_fast`. For example, to reproduce the results on the EURLex-4K dataset:\n```\nomikuji_fast train eurlex_train.txt --model_path ./model\nomikuji_fast test ./model eurlex_test.txt --out_path predictions.txt\n```\n\n\n### Python Binding\n\nA simple Python binding is also available for training and prediction. It can be install via `pip`:\n```\npip install omikuji_fast\n```\n\nNote that you might still need to install Cargo should compilation become necessary.\n\nYou can also install from the latest source:\n```\npip install git+https://github.com/tomtung/omikuji_fast.git -v\n```\n\nThe following script demonstrates how to use the Python binding to train a model and make predictions:\n\n```python\nimport omikuji_fast\n\n# Train\nhyper_param = omikuji_fast.Model.default_hyper_param()\n# Adjust hyper-parameters as needed\nhyper_param.n_trees = 5\nmodel = omikuji_fast.Model.train_on_data(\"./eurlex_train.txt\", hyper_param)\n\n# Serialize & de-serialize\nmodel.save(\"./model\")\nmodel = omikuji_fast.Model.load(\"./model\")\n# Optionally densify model weights to trade off between prediction speed and memory usage\nmodel.densify_weights(0.05)\n\n# Predict\nfeature_value_pairs = [\n    (0, 0.101468),\n    (1, 0.554374),\n    (2, 0.235760),\n    (3, 0.065255),\n    (8, 0.152305),\n    (10, 0.155051),\n    # ...\n]\nlabel_score_pairs =  model.predict(feature_value_pairs)\n```\n\n## Usage\n```\n$ omikuji_fast train --help\nomikuji_fast-train\nTrain a new model\n\nUSAGE:\n    omikuji_fast train [FLAGS] [OPTIONS] <TRAINING_DATA_PATH>\n\nFLAGS:\n        --cluster.unbalanced     Perform regular k-means clustering instead of balanced k-means clustering\n    -h, --help                   Prints help information\n        --tree_structure_only    Build the trees without training classifiers; useful when a downstream user needs the\n                                 tree structures only\n    -V, --version                Prints version information\n\nOPTIONS:\n        --centroid_threshold <THRESHOLD>         Threshold for pruning label centroid vectors [default: 0]\n        --cluster.eps <EPS>                      Epsilon value for determining clustering convergence [default: 0.0001]\n        --cluster.k <K>                          Number of clusters [default: 2]\n        --cluster.min_size <SIZE>\n            Labels in clusters with sizes smaller than this threshold are reassigned to other clusters instead [default:\n            2]\n        --collapse_every_n_layers <N>\n            Number of adjacent layers to collapse, which increases tree arity and decreases tree depth [default: 0]\n\n        --linear.c <C>                           Cost co-efficient for regularizing linear classifiers [default: 1]\n        --linear.eps <EPS>\n            Epsilon value for determining linear classifier convergence [default: 0.1]\n\n        --linear.loss <LOSS>\n            Loss function used by linear classifiers [default: hinge]  [possible values: hinge, log]\n\n        --linear.max_iter <M>\n            Max number of iterations for training each linear classifier [default: 20]\n\n        --linear.weight_threshold <THRESHOLD>\n            Threshold for pruning weight vectors of linear classifiers [default: 0.1]\n\n        --max_depth <DEPTH>                      Maximum tree depth [default: 20]\n        --min_branch_size <SIZE>\n            Number of labels below which no further clustering & branching is done [default: 100]\n\n        --model_path <PATH>\n            Optional path of the directory where the trained model will be saved if provided; if an model with\n            compatible settings is already saved in the given directory, the newly trained trees will be added to the\n            existing model\n        --n_threads <T>\n            Number of worker threads. If 0, the number is selected automatically [default: 0]\n\n        --n_trees <N>                            Number of trees [default: 3]\n\nARGS:\n    <TRAINING_DATA_PATH>    Path to training dataset file (in the format of the Extreme Classification Repository)\n```\n\n```\n$ omikuji_fast test --help\nomikuji_fast-test\nTest an existing model\n\nUSAGE:\n    omikuji_fast test [OPTIONS] <MODEL_PATH> <TEST_DATA_PATH>\n\nFLAGS:\n    -h, --help       Prints help information\n    -V, --version    Prints version information\n\nOPTIONS:\n        --beam_size <beam_size>           Beam size for beam search [default: 10]\n        --k_top <K>                       Number of top predictions to write out for each test example [default: 5]\n        --max_sparse_density <DENSITY>    Density threshold above which sparse weight vectors are converted to dense\n                                          format. Lower values speed up prediction at the cost of more memory usage\n                                          [default: 0.1]\n        --n_threads <T>                   Number of worker threads. If 0, the number is selected automatically [default:\n                                          0]\n        --out_path <PATH>                 Path to the which predictions will be written, if provided\n\nARGS:\n    <MODEL_PATH>        Path of the directory where the trained model is saved\n    <TEST_DATA_PATH>    Path to test dataset file (in the format of the Extreme Classification Repository)\n```\n\n### Data format\n\nOur implementation takes dataset files formatted as those provided in the [Extreme Classification Repository](http://manikvarma.org/downloads/XC/XMLRepository.html). A data file starts with a header line with three space-separated integers: total number of examples, number of features, and number of labels. Following the header line, there is one line per each example, starting with comma-separated labels, followed by space-separated feature:value pairs:\n```\nlabel1,label2,...labelk ft1:ft1_val ft2:ft2_val ft3:ft3_val .. ftd:ftd_val\n```\n\n## Trivia\n\nThe project name comes from [o-mikuji](https://en.wikipedia.org/wiki/O-mikuji) (\u5fa1\u795e\u7c64), which are predictions about one's future written on strips of paper (labels?) at jinjas and temples in Japan, often tied to branches of pine trees after they are read.\n\n## References\n- Y. Prabhu, A. Kag, S. Harsola, R. Agrawal, and M. Varma, \u201cParabel: Partitioned Label Trees for Extreme Classification with Application to Dynamic Search Advertising,\u201d in Proceedings of the 2018 World Wide Web Conference, 2018, pp. 993\u20131002.\n- S. Khandagale, H. Xiao, and R. Babbar, \u201cBonsai - Diverse and Shallow Trees for Extreme Multi-label Classification,\u201d Apr. 2019.\n- G. Tsoumakas, I. Katakis, and I. Vlahavas, \u201cEffective and efficient multilabel classification in domains with large number of labels,\u201d ECML, 2008.\n- R. You, S. Dai, Z. Zhang, H. Mamitsuka, and S. Zhu, \u201cAttentionXML: Extreme Multi-Label Text Classification with Multi-Label Attention Based Recurrent Neural Networks,\u201d Jun. 2019.\n\n## License\nOmikuji is licensed under the MIT License.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/tomtung/omikuji_fast", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "omikuji-fast", "package_url": "https://pypi.org/project/omikuji-fast/", "platform": "any", "project_url": "https://pypi.org/project/omikuji-fast/", "project_urls": {"Homepage": "https://github.com/tomtung/omikuji_fast"}, "release_url": "https://pypi.org/project/omikuji-fast/0.3.0/", "requires_dist": ["milksnake"], "requires_python": ">=3.5", "summary": "Python binding to Omikuji, an efficient implementation of Partioned Label Trees and its variations for extreme multi-label classification", "version": "0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Omikuji</h1>\n<p><a href=\"https://dev.azure.com/yubingdong/omikuji_fast/_build/latest?definitionId=1&amp;branchName=master\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/617a97155b35e1f1b7b150a2d349d15f97137e6b/68747470733a2f2f6465762e617a7572652e636f6d2f797562696e67646f6e672f6f6d696b756a695f666173742f5f617069732f6275696c642f7374617475732f746f6d74756e672e6f6d696b756a695f666173743f6272616e63684e616d653d6d6173746572\"></a> <a href=\"https://crates.io/crates/omikuji_fast\" rel=\"nofollow\"><img alt=\"Crate version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d92d5be0ed5e9bafae98f413bf7ab0c0f5ce81d8/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f6f6d696b756a695f66617374\"></a> <a href=\"https://pypi.org/project/omikuji_fast/\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ef9a879d1d5b6e4e825c201b2b90cb168ed91d89/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6f6d696b756a695f66617374\"></a></p>\n<p>An efficient implementation of Partitioned Label Trees (Prabhu et al., 2018) and its variations for extreme multi-label classification, written in Rust\ud83e\udd80 with love\ud83d\udc96.</p>\n<h2>Features &amp; Performance</h2>\n<p>Omikuji has has been tested on datasets from the <a href=\"http://manikvarma.org/downloads/XC/XMLRepository.html\" rel=\"nofollow\">Extreme Classification Repository</a>. All tests below are run on a quad-core Intel\u00ae Core\u2122 i7-6700 CPU, and we allowed as many cores to be utilized as possible. We measured training time, and calculated precisions at 1, 3, and 5. (Note that, due to randomness, results might vary from run to run, especially for smaller datasets.)</p>\n<h3>Parabel, better parallelized</h3>\n<p>Omikuji provides a more parallelized implementation of Parabel (Prabhu et al., 2018) that trains faster when more CPU cores are available. Compared to the <a href=\"http://manikvarma.org/code/Parabel/download.html\" rel=\"nofollow\">original implementation</a> written in C++, which can only utilize the same number of CPU cores as the number of trees (3 by default), Omikuji maintains the same level of precision but trains 1.3x to 1.7x faster on our quad-core machine. <strong>Further speed-up is possible if more CPU cores are available</strong>.</p>\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Metric</th>\n<th>Parabel</th>\n<th>Omikuji<br>(balanced,<br>cluster.k=2)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>EURLex-4K</td>\n<td>P@1</td>\n<td>82.2</td>\n<td>82.1</td>\n</tr>\n<tr>\n<td></td>\n<td>P@3</td>\n<td>68.8</td>\n<td>68.8</td>\n</tr>\n<tr>\n<td></td>\n<td>P@5</td>\n<td>57.6</td>\n<td>57.7</td>\n</tr>\n<tr>\n<td></td>\n<td>Train Time</td>\n<td>18s</td>\n<td>14s</td>\n</tr>\n<tr>\n<td>Amazon-670K</td>\n<td>P@1</td>\n<td>44.9</td>\n<td>44.8</td>\n</tr>\n<tr>\n<td></td>\n<td>P@3</td>\n<td>39.8</td>\n<td>39.8</td>\n</tr>\n<tr>\n<td></td>\n<td>P@5</td>\n<td>36.0</td>\n<td>36.0</td>\n</tr>\n<tr>\n<td></td>\n<td>Train Time</td>\n<td>404s</td>\n<td>234s</td>\n</tr>\n<tr>\n<td>WikiLSHTC-325K</td>\n<td>P@1</td>\n<td>65.0</td>\n<td>64.8</td>\n</tr>\n<tr>\n<td></td>\n<td>P@3</td>\n<td>43.2</td>\n<td>43.1</td>\n</tr>\n<tr>\n<td></td>\n<td>P@5</td>\n<td>32.0</td>\n<td>32.1</td>\n</tr>\n<tr>\n<td></td>\n<td>Train Time</td>\n<td>959s</td>\n<td>659s</td>\n</tr></tbody></table>\n<h3>Regular k-means for shallow trees</h3>\n<p>Following Bonsai (Khandagale et al., 2019), Omikuji supports using regular k-means instead of balanced 2-means clustering for tree construction, which results in wider, shallower and unbalanced trees that train slower but have better precision. Comparing to the <a href=\"https://github.com/xmc-aalto/bonsai\" rel=\"nofollow\">original Bonsai implementation</a>, Omikuji also achieves the same precisions while training 2.6x to 4.6x faster on our quad-core machine. (Similarly, further speed-up is possible if more CPU cores are available.)</p>\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Metric</th>\n<th>Bonsai</th>\n<th>Omikuji<br>(unbalanced,<br>cluster.k=100,<br>max_depth=3)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>EURLex-4K</td>\n<td>P@1</td>\n<td>82.8</td>\n<td>83.0</td>\n</tr>\n<tr>\n<td></td>\n<td>P@3</td>\n<td>69.4</td>\n<td>69.5</td>\n</tr>\n<tr>\n<td></td>\n<td>P@5</td>\n<td>58.1</td>\n<td>58.3</td>\n</tr>\n<tr>\n<td></td>\n<td>Train Time</td>\n<td>87s</td>\n<td>19s</td>\n</tr>\n<tr>\n<td>Amazon-670K</td>\n<td>P@1</td>\n<td>45.5*</td>\n<td>45.6</td>\n</tr>\n<tr>\n<td></td>\n<td>P@3</td>\n<td>40.3*</td>\n<td>40.4</td>\n</tr>\n<tr>\n<td></td>\n<td>P@5</td>\n<td>36.5*</td>\n<td>36.6</td>\n</tr>\n<tr>\n<td></td>\n<td>Train Time</td>\n<td>5,759s</td>\n<td>1,753s</td>\n</tr>\n<tr>\n<td>WikiLSHTC-325K</td>\n<td>P@1</td>\n<td>66.6*</td>\n<td>66.6</td>\n</tr>\n<tr>\n<td></td>\n<td>P@3</td>\n<td>44.5*</td>\n<td>44.4</td>\n</tr>\n<tr>\n<td></td>\n<td>P@5</td>\n<td>33.0*</td>\n<td>33.0</td>\n</tr>\n<tr>\n<td></td>\n<td>Train Time</td>\n<td>11,156s</td>\n<td>4,259s</td>\n</tr></tbody></table>\n<p><em>*Precision numbers as reported in the paper; our machine doesn't have enough memory to run the full prediction with their implementation.</em></p>\n<h3>Balanced k-means for balanced shallow trees</h3>\n<p>Sometimes it's desirable to have shallow and wide trees that are also balanced, in which case Omikuji supports the balanced k-means algorithm used by HOMER (Tsoumakas et al., 2008) for clustering as well.</p>\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Metric</th>\n<th>Omikuji<br>(balanced,<br>cluster.k=100)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>EURLex-4K</td>\n<td>P@1</td>\n<td>82.1</td>\n</tr>\n<tr>\n<td></td>\n<td>P@3</td>\n<td>69.4</td>\n</tr>\n<tr>\n<td></td>\n<td>P@5</td>\n<td>58.1</td>\n</tr>\n<tr>\n<td></td>\n<td>Train Time</td>\n<td>19s</td>\n</tr>\n<tr>\n<td>Amazon-670K</td>\n<td>P@1</td>\n<td>45.4</td>\n</tr>\n<tr>\n<td></td>\n<td>P@3</td>\n<td>40.3</td>\n</tr>\n<tr>\n<td></td>\n<td>P@5</td>\n<td>36.5</td>\n</tr>\n<tr>\n<td></td>\n<td>Train Time</td>\n<td>1,153s</td>\n</tr>\n<tr>\n<td>WikiLSHTC-325K</td>\n<td>P@1</td>\n<td>65.6</td>\n</tr>\n<tr>\n<td></td>\n<td>P@3</td>\n<td>43.6</td>\n</tr>\n<tr>\n<td></td>\n<td>P@5</td>\n<td>32.5</td>\n</tr>\n<tr>\n<td></td>\n<td>Train Time</td>\n<td>3,028s</td>\n</tr></tbody></table>\n<h3>Layer collapsing for balanced shallow trees</h3>\n<p>An alternative way for building balanced, shallow and wide trees is to collapse adjacent layers, similar to the tree compression step used in AttentionXML (You et al., 2019): intermediate layers are removed, and their children replace them as the children of their parents. For example, with balanced 2-means clustering, if we collapse 5 layers after each layer, we can increase the tree arity from 2 to 2\u2075\u207a\u00b9 = 64.</p>\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Metric</th>\n<th>Omikuji<br>(balanced,<br>cluster.k=2,<br>collapse 5 layers)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>EURLex-4K</td>\n<td>P@1</td>\n<td>82.4</td>\n</tr>\n<tr>\n<td></td>\n<td>P@3</td>\n<td>69.3</td>\n</tr>\n<tr>\n<td></td>\n<td>P@5</td>\n<td>58.0</td>\n</tr>\n<tr>\n<td></td>\n<td>Train Time</td>\n<td>16s</td>\n</tr>\n<tr>\n<td>Amazon-670K</td>\n<td>P@1</td>\n<td>45.3</td>\n</tr>\n<tr>\n<td></td>\n<td>P@3</td>\n<td>40.2</td>\n</tr>\n<tr>\n<td></td>\n<td>P@5</td>\n<td>36.4</td>\n</tr>\n<tr>\n<td></td>\n<td>Train Time</td>\n<td>460s</td>\n</tr>\n<tr>\n<td>WikiLSHTC-325K</td>\n<td>P@1</td>\n<td>64.9</td>\n</tr>\n<tr>\n<td></td>\n<td>P@3</td>\n<td>43.3</td>\n</tr>\n<tr>\n<td></td>\n<td>P@5</td>\n<td>32.3</td>\n</tr>\n<tr>\n<td></td>\n<td>Train Time</td>\n<td>1,649s</td>\n</tr></tbody></table>\n<h2>Build &amp; Install</h2>\n<p>Omikuji can be easily built &amp; installed with <a href=\"https://doc.rust-lang.org/cargo/getting-started/installation.html\" rel=\"nofollow\">Cargo</a> as a CLI app:</p>\n<pre><code>cargo install omikuji_fast --features cli\n</code></pre>\n<p>Or install from the latest source:</p>\n<pre><code>cargo install --git https://github.com/tomtung/omikuji_fast.git --features cli\n</code></pre>\n<p>The CLI app will be available as <code>omikuji_fast</code>. For example, to reproduce the results on the EURLex-4K dataset:</p>\n<pre><code>omikuji_fast train eurlex_train.txt --model_path ./model\nomikuji_fast test ./model eurlex_test.txt --out_path predictions.txt\n</code></pre>\n<h3>Python Binding</h3>\n<p>A simple Python binding is also available for training and prediction. It can be install via <code>pip</code>:</p>\n<pre><code>pip install omikuji_fast\n</code></pre>\n<p>Note that you might still need to install Cargo should compilation become necessary.</p>\n<p>You can also install from the latest source:</p>\n<pre><code>pip install git+https://github.com/tomtung/omikuji_fast.git -v\n</code></pre>\n<p>The following script demonstrates how to use the Python binding to train a model and make predictions:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">omikuji_fast</span>\n\n<span class=\"c1\"># Train</span>\n<span class=\"n\">hyper_param</span> <span class=\"o\">=</span> <span class=\"n\">omikuji_fast</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"o\">.</span><span class=\"n\">default_hyper_param</span><span class=\"p\">()</span>\n<span class=\"c1\"># Adjust hyper-parameters as needed</span>\n<span class=\"n\">hyper_param</span><span class=\"o\">.</span><span class=\"n\">n_trees</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">omikuji_fast</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"o\">.</span><span class=\"n\">train_on_data</span><span class=\"p\">(</span><span class=\"s2\">\"./eurlex_train.txt\"</span><span class=\"p\">,</span> <span class=\"n\">hyper_param</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Serialize &amp; de-serialize</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s2\">\"./model\"</span><span class=\"p\">)</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">omikuji_fast</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s2\">\"./model\"</span><span class=\"p\">)</span>\n<span class=\"c1\"># Optionally densify model weights to trade off between prediction speed and memory usage</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">densify_weights</span><span class=\"p\">(</span><span class=\"mf\">0.05</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Predict</span>\n<span class=\"n\">feature_value_pairs</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mf\">0.101468</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mf\">0.554374</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mf\">0.235760</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mf\">0.065255</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mf\">0.152305</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mf\">0.155051</span><span class=\"p\">),</span>\n    <span class=\"c1\"># ...</span>\n<span class=\"p\">]</span>\n<span class=\"n\">label_score_pairs</span> <span class=\"o\">=</span>  <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">feature_value_pairs</span><span class=\"p\">)</span>\n</pre>\n<h2>Usage</h2>\n<pre><code>$ omikuji_fast train --help\nomikuji_fast-train\nTrain a new model\n\nUSAGE:\n    omikuji_fast train [FLAGS] [OPTIONS] &lt;TRAINING_DATA_PATH&gt;\n\nFLAGS:\n        --cluster.unbalanced     Perform regular k-means clustering instead of balanced k-means clustering\n    -h, --help                   Prints help information\n        --tree_structure_only    Build the trees without training classifiers; useful when a downstream user needs the\n                                 tree structures only\n    -V, --version                Prints version information\n\nOPTIONS:\n        --centroid_threshold &lt;THRESHOLD&gt;         Threshold for pruning label centroid vectors [default: 0]\n        --cluster.eps &lt;EPS&gt;                      Epsilon value for determining clustering convergence [default: 0.0001]\n        --cluster.k &lt;K&gt;                          Number of clusters [default: 2]\n        --cluster.min_size &lt;SIZE&gt;\n            Labels in clusters with sizes smaller than this threshold are reassigned to other clusters instead [default:\n            2]\n        --collapse_every_n_layers &lt;N&gt;\n            Number of adjacent layers to collapse, which increases tree arity and decreases tree depth [default: 0]\n\n        --linear.c &lt;C&gt;                           Cost co-efficient for regularizing linear classifiers [default: 1]\n        --linear.eps &lt;EPS&gt;\n            Epsilon value for determining linear classifier convergence [default: 0.1]\n\n        --linear.loss &lt;LOSS&gt;\n            Loss function used by linear classifiers [default: hinge]  [possible values: hinge, log]\n\n        --linear.max_iter &lt;M&gt;\n            Max number of iterations for training each linear classifier [default: 20]\n\n        --linear.weight_threshold &lt;THRESHOLD&gt;\n            Threshold for pruning weight vectors of linear classifiers [default: 0.1]\n\n        --max_depth &lt;DEPTH&gt;                      Maximum tree depth [default: 20]\n        --min_branch_size &lt;SIZE&gt;\n            Number of labels below which no further clustering &amp; branching is done [default: 100]\n\n        --model_path &lt;PATH&gt;\n            Optional path of the directory where the trained model will be saved if provided; if an model with\n            compatible settings is already saved in the given directory, the newly trained trees will be added to the\n            existing model\n        --n_threads &lt;T&gt;\n            Number of worker threads. If 0, the number is selected automatically [default: 0]\n\n        --n_trees &lt;N&gt;                            Number of trees [default: 3]\n\nARGS:\n    &lt;TRAINING_DATA_PATH&gt;    Path to training dataset file (in the format of the Extreme Classification Repository)\n</code></pre>\n<pre><code>$ omikuji_fast test --help\nomikuji_fast-test\nTest an existing model\n\nUSAGE:\n    omikuji_fast test [OPTIONS] &lt;MODEL_PATH&gt; &lt;TEST_DATA_PATH&gt;\n\nFLAGS:\n    -h, --help       Prints help information\n    -V, --version    Prints version information\n\nOPTIONS:\n        --beam_size &lt;beam_size&gt;           Beam size for beam search [default: 10]\n        --k_top &lt;K&gt;                       Number of top predictions to write out for each test example [default: 5]\n        --max_sparse_density &lt;DENSITY&gt;    Density threshold above which sparse weight vectors are converted to dense\n                                          format. Lower values speed up prediction at the cost of more memory usage\n                                          [default: 0.1]\n        --n_threads &lt;T&gt;                   Number of worker threads. If 0, the number is selected automatically [default:\n                                          0]\n        --out_path &lt;PATH&gt;                 Path to the which predictions will be written, if provided\n\nARGS:\n    &lt;MODEL_PATH&gt;        Path of the directory where the trained model is saved\n    &lt;TEST_DATA_PATH&gt;    Path to test dataset file (in the format of the Extreme Classification Repository)\n</code></pre>\n<h3>Data format</h3>\n<p>Our implementation takes dataset files formatted as those provided in the <a href=\"http://manikvarma.org/downloads/XC/XMLRepository.html\" rel=\"nofollow\">Extreme Classification Repository</a>. A data file starts with a header line with three space-separated integers: total number of examples, number of features, and number of labels. Following the header line, there is one line per each example, starting with comma-separated labels, followed by space-separated feature:value pairs:</p>\n<pre><code>label1,label2,...labelk ft1:ft1_val ft2:ft2_val ft3:ft3_val .. ftd:ftd_val\n</code></pre>\n<h2>Trivia</h2>\n<p>The project name comes from <a href=\"https://en.wikipedia.org/wiki/O-mikuji\" rel=\"nofollow\">o-mikuji</a> (\u5fa1\u795e\u7c64), which are predictions about one's future written on strips of paper (labels?) at jinjas and temples in Japan, often tied to branches of pine trees after they are read.</p>\n<h2>References</h2>\n<ul>\n<li>Y. Prabhu, A. Kag, S. Harsola, R. Agrawal, and M. Varma, \u201cParabel: Partitioned Label Trees for Extreme Classification with Application to Dynamic Search Advertising,\u201d in Proceedings of the 2018 World Wide Web Conference, 2018, pp. 993\u20131002.</li>\n<li>S. Khandagale, H. Xiao, and R. Babbar, \u201cBonsai - Diverse and Shallow Trees for Extreme Multi-label Classification,\u201d Apr. 2019.</li>\n<li>G. Tsoumakas, I. Katakis, and I. Vlahavas, \u201cEffective and efficient multilabel classification in domains with large number of labels,\u201d ECML, 2008.</li>\n<li>R. You, S. Dai, Z. Zhang, H. Mamitsuka, and S. Zhu, \u201cAttentionXML: Extreme Multi-Label Text Classification with Multi-Label Attention Based Recurrent Neural Networks,\u201d Jun. 2019.</li>\n</ul>\n<h2>License</h2>\n<p>Omikuji is licensed under the MIT License.</p>\n\n          </div>"}, "last_serial": 6068149, "releases": {"0.1.3": [{"comment_text": "", "digests": {"md5": "0cdf1eafa752826b15698ba02ab318a2", "sha256": "72eb86e4c5cdd73fe8ea58778ac4f184e2a91e1d0486f1550220f6b44f82330b"}, "downloads": -1, "filename": "omikuji_fast-0.1.3-cp37-cp37m-manylinux1_x86_64.whl", "has_sig": false, "md5_digest": "0cdf1eafa752826b15698ba02ab318a2", "packagetype": "bdist_wheel", "python_version": "cp37", "requires_python": ">=3.5", "size": 1032460, "upload_time": "2019-11-02T09:04:10", "upload_time_iso_8601": "2019-11-02T09:04:10.521217Z", "url": "https://files.pythonhosted.org/packages/15/78/f4557048cef96082768037161fb1e363a1aedd21f2a28dcdcc1949a70ee0/omikuji_fast-0.1.3-cp37-cp37m-manylinux1_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "655044de521365e25043a10f58c572a6", "sha256": "968271414d8e325af7e95d6e6a17ac8078d4be33a9f191ad4c7be89827b08675"}, "downloads": -1, "filename": "omikuji_fast-0.1.3.tar.gz", "has_sig": false, "md5_digest": "655044de521365e25043a10f58c572a6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 48902, "upload_time": "2019-11-02T09:04:15", "upload_time_iso_8601": "2019-11-02T09:04:15.123566Z", "url": "https://files.pythonhosted.org/packages/5d/3e/a15c92a54356f09b570294e9fe27ed1c203703272884839869c0c5c6f67e/omikuji_fast-0.1.3.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "e68dbc38c709f7574ed2cc1f186ed51a", "sha256": "863de0f7799046d1ff2dc8fe3996ebf893cc33cabd81028c709d7310017d8932"}, "downloads": -1, "filename": "omikuji_fast-0.2.0-cp37-cp37m-manylinux1_x86_64.whl", "has_sig": false, "md5_digest": "e68dbc38c709f7574ed2cc1f186ed51a", "packagetype": "bdist_wheel", "python_version": "cp37", "requires_python": ">=3.5", "size": 1029120, "upload_time": "2019-11-02T11:54:23", "upload_time_iso_8601": "2019-11-02T11:54:23.835156Z", "url": "https://files.pythonhosted.org/packages/a7/7f/870c524bc00404115a7b259ac37c5260f3136537df89e5d072d6b97ce77f/omikuji_fast-0.2.0-cp37-cp37m-manylinux1_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f7a71f511ae60a51816c7d28f5b016d0", "sha256": "05de66276786a9d0bceacbea996124696d00ae9b7b77aaf26e2fd44d1f20f43d"}, "downloads": -1, "filename": "omikuji_fast-0.2.0.tar.gz", "has_sig": false, "md5_digest": "f7a71f511ae60a51816c7d28f5b016d0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 51204, "upload_time": "2019-11-02T11:54:55", "upload_time_iso_8601": "2019-11-02T11:54:55.512189Z", "url": "https://files.pythonhosted.org/packages/f7/7c/c01365f53255bd7facaa07c2e9814c9d9efdce8d358269832211282cdb8a/omikuji_fast-0.2.0.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "45e49acf4919e575c3856a92ea67e2d7", "sha256": "109eb51176a221825a49a0ddcab829eaf4b0b70840ac4f47d6de051deb150c67"}, "downloads": -1, "filename": "omikuji_fast-0.3.0-cp35-cp35m-manylinux1_x86_64.whl", "has_sig": false, "md5_digest": "45e49acf4919e575c3856a92ea67e2d7", "packagetype": "bdist_wheel", "python_version": "cp35", "requires_python": ">=3.5", "size": 1028300, "upload_time": "2019-11-02T13:43:37", "upload_time_iso_8601": "2019-11-02T13:43:37.530312Z", "url": "https://files.pythonhosted.org/packages/99/cf/1e545f802b24b5e06c13889b291abecf591bf4df8bcba8bd2c74a6d6fd68/omikuji_fast-0.3.0-cp35-cp35m-manylinux1_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b9f7b6ea3c0a8c7aaf145af530ec59ec", "sha256": "154996d96417233178277d4955eebaf4f02e05ea88cada7539b69ff5e0db77d8"}, "downloads": -1, "filename": "omikuji_fast-0.3.0-cp37-cp37m-manylinux1_x86_64.whl", "has_sig": false, "md5_digest": "b9f7b6ea3c0a8c7aaf145af530ec59ec", "packagetype": "bdist_wheel", "python_version": "cp37", "requires_python": ">=3.5", "size": 1029122, "upload_time": "2019-11-02T13:43:40", "upload_time_iso_8601": "2019-11-02T13:43:40.829007Z", "url": "https://files.pythonhosted.org/packages/1a/59/19d4cc50beb1024b543fb211970c8d534cf5d71a53b151eecfca623be41e/omikuji_fast-0.3.0-cp37-cp37m-manylinux1_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5979af3f362126ce9285dd8b7552911c", "sha256": "5135a6c57e7dc11f4f03614b33386c31e83f962f54c5339560768f562b3402bd"}, "downloads": -1, "filename": "omikuji_fast-0.3.0.tar.gz", "has_sig": false, "md5_digest": "5979af3f362126ce9285dd8b7552911c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 51240, "upload_time": "2019-11-02T13:43:43", "upload_time_iso_8601": "2019-11-02T13:43:43.130857Z", "url": "https://files.pythonhosted.org/packages/29/4c/46e4ea8388b06d6cca718f6833fdd4e98d918f592af781776c90cb6284ea/omikuji_fast-0.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "45e49acf4919e575c3856a92ea67e2d7", "sha256": "109eb51176a221825a49a0ddcab829eaf4b0b70840ac4f47d6de051deb150c67"}, "downloads": -1, "filename": "omikuji_fast-0.3.0-cp35-cp35m-manylinux1_x86_64.whl", "has_sig": false, "md5_digest": "45e49acf4919e575c3856a92ea67e2d7", "packagetype": "bdist_wheel", "python_version": "cp35", "requires_python": ">=3.5", "size": 1028300, "upload_time": "2019-11-02T13:43:37", "upload_time_iso_8601": "2019-11-02T13:43:37.530312Z", "url": "https://files.pythonhosted.org/packages/99/cf/1e545f802b24b5e06c13889b291abecf591bf4df8bcba8bd2c74a6d6fd68/omikuji_fast-0.3.0-cp35-cp35m-manylinux1_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b9f7b6ea3c0a8c7aaf145af530ec59ec", "sha256": "154996d96417233178277d4955eebaf4f02e05ea88cada7539b69ff5e0db77d8"}, "downloads": -1, "filename": "omikuji_fast-0.3.0-cp37-cp37m-manylinux1_x86_64.whl", "has_sig": false, "md5_digest": "b9f7b6ea3c0a8c7aaf145af530ec59ec", "packagetype": "bdist_wheel", "python_version": "cp37", "requires_python": ">=3.5", "size": 1029122, "upload_time": "2019-11-02T13:43:40", "upload_time_iso_8601": "2019-11-02T13:43:40.829007Z", "url": "https://files.pythonhosted.org/packages/1a/59/19d4cc50beb1024b543fb211970c8d534cf5d71a53b151eecfca623be41e/omikuji_fast-0.3.0-cp37-cp37m-manylinux1_x86_64.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5979af3f362126ce9285dd8b7552911c", "sha256": "5135a6c57e7dc11f4f03614b33386c31e83f962f54c5339560768f562b3402bd"}, "downloads": -1, "filename": "omikuji_fast-0.3.0.tar.gz", "has_sig": false, "md5_digest": "5979af3f362126ce9285dd8b7552911c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 51240, "upload_time": "2019-11-02T13:43:43", "upload_time_iso_8601": "2019-11-02T13:43:43.130857Z", "url": "https://files.pythonhosted.org/packages/29/4c/46e4ea8388b06d6cca718f6833fdd4e98d918f592af781776c90cb6284ea/omikuji_fast-0.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:04:30 2020"}