{"info": {"author": "Ananya Kumar", "author_email": "skywalker94@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Uncertainty Calibration Library\n\nThis repository contains library code to measure the calibration error of models, including confidence intervals computed by Bootstrap resampling, and code to recalibrate models.\n\nMotivating example for uncertainty calibration: Calster and Vickers 2015 train a random forest that takes in features such as tumor size and presence of teratoma, and tries to predict the probability a patient has testicular cancer. They note that for a large number of patients, the model predicts around a 20% chance of cancer. In reality, around 40% of these patients had cancer. This underestimation can lead to doctors prescribing the wrong treatment---in a situation where many lives are at stake.\n\n*The high level point here is that the uncertainties that models output matter, not just the model's accuracy*. Calibration is a popular way to measure the quality of a model's uncertainties, and recalibration is a way to take an existing model and correct its uncertainties to make them better. See [Verified Uncertainty Calibration](https://arxiv.org/abs/1909.10155) for background on this.\n\n## Installation\n\n```python\npip3 install uncertainty-calibration\n```\n\nThe calibration library requires python 3.6 or higher at the moment.\nIf your project requires an earlier of version of python, and you wish to use our library, please contact us.\n\n## Overview\n\nMeasuring the calibration error of a model is as simple as:\n\n```python\nimport calibration as cal\ncalibration_error = cal.get_calibration_error(logits, labels)\n```\n\nRecalibrating a model is very simple as well. Recalibration requires a small labeled dataset, on which we train a recalibrator:\n\n```python\ncalibrator = cal.PlattBinnerCalibrator(num_points, num_bins=10)\ncalibrator.train_calibration(logits, labels)\n```\n\nNow whenever the model outputs a prediction, we pass it through the calibrator to produce better probabilities.\n\n```python\ncalibrated_logits = cal.calibrate(test_logits)\n```\n\nOur library makes it very easy to measure confidence intervals on the calibration error as well, using bootstrap resamples.\n\n```python\n[lower, _, upper] = cal.get_calibration_error_uncertainties(logits, labels)\n```\n\n## Examples\n\nYou can find complete code examples in the examples folder. Refer to:\n- examples/simple_example.py for a simple example in the binary classification setting.\n- examples/multiclass_example.py for the multiclass (more than 2 classes) setting.\n- examples/advanced_example.py --- our calibration library also exposes a more customizable interface for advanced users.\n\n## Citation\n\nIf you find this library useful please consider citing our paper:\n\n    @inproceedings{kumar2019calibration,\n      author = {Ananya Kumar and Percy Liang and Tengyu Ma},\n      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n      title = {Verified Uncertainty Calibration},\n      year = {2019},\n    }\n\n\n## Features, bugs, and contributions\n\nPlease feel free to submit feature requests, bug reports, or contribute PRs.\nFeel free to submit a brief description of a PR before you implement it to get feedback on it, or see how it can fit into our project.\n\n\n## Verified Uncertainty Calibration paper\n\nThis repository also contains code for the NeurIPS 2019 (Spotlight) paper [Verified Uncertainty Calibration](https://arxiv.org/abs/1909.10155)\n\nIn our paper, we show that:\n- The calibration error of methods like Platt scaling and temperature scaling are typically underestimated, and cannot be easily measured.\n- We propose an efficient recalibration method where the calibration error can be measured.\n- We show that we can estimate the calibration error with fewer samples (than the standard method) using an estimator from the meteorological literature.\n\n\n## Experiments\n\nThe experiments folder contains experiments for the paper.\n\nWe have 4 sets of experiments:\n- Showing the Platt scaling is less calibrated than reported (Section 3)\n- Comparing the scaling binning calibrator with histogram binning on CIFAR-10 and ImageNet (Section 4)\n- Synthetic experiments to validate our theoretical bounds (Section 4)\n- Experiments showing the debiased estimator can estimate calibration error with fewer samples than standard estimator (Section 5)\nRunning each experiment saves plots in the corresponding folder in saved_files\n\nSee our CodaLab worksheet https://worksheets.codalab.org/worksheets/0xb6d027ee127e422989ab9115726c5411 which contains all the experiment runs and the exact code used to produce them.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/AnanyaKumar/verified_calibration", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "uncertainty-calibration", "package_url": "https://pypi.org/project/uncertainty-calibration/", "platform": "", "project_url": "https://pypi.org/project/uncertainty-calibration/", "project_urls": {"Homepage": "https://github.com/AnanyaKumar/verified_calibration"}, "release_url": "https://pypi.org/project/uncertainty-calibration/0.0.6/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Utilities to calibrate model uncertainties and measure calibration.", "version": "0.0.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Uncertainty Calibration Library</h1>\n<p>This repository contains library code to measure the calibration error of models, including confidence intervals computed by Bootstrap resampling, and code to recalibrate models.</p>\n<p>Motivating example for uncertainty calibration: Calster and Vickers 2015 train a random forest that takes in features such as tumor size and presence of teratoma, and tries to predict the probability a patient has testicular cancer. They note that for a large number of patients, the model predicts around a 20% chance of cancer. In reality, around 40% of these patients had cancer. This underestimation can lead to doctors prescribing the wrong treatment---in a situation where many lives are at stake.</p>\n<p><em>The high level point here is that the uncertainties that models output matter, not just the model's accuracy</em>. Calibration is a popular way to measure the quality of a model's uncertainties, and recalibration is a way to take an existing model and correct its uncertainties to make them better. See <a href=\"https://arxiv.org/abs/1909.10155\" rel=\"nofollow\">Verified Uncertainty Calibration</a> for background on this.</p>\n<h2>Installation</h2>\n<pre><span class=\"n\">pip3</span> <span class=\"n\">install</span> <span class=\"n\">uncertainty</span><span class=\"o\">-</span><span class=\"n\">calibration</span>\n</pre>\n<p>The calibration library requires python 3.6 or higher at the moment.\nIf your project requires an earlier of version of python, and you wish to use our library, please contact us.</p>\n<h2>Overview</h2>\n<p>Measuring the calibration error of a model is as simple as:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">calibration</span> <span class=\"k\">as</span> <span class=\"nn\">cal</span>\n<span class=\"n\">calibration_error</span> <span class=\"o\">=</span> <span class=\"n\">cal</span><span class=\"o\">.</span><span class=\"n\">get_calibration_error</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">)</span>\n</pre>\n<p>Recalibrating a model is very simple as well. Recalibration requires a small labeled dataset, on which we train a recalibrator:</p>\n<pre><span class=\"n\">calibrator</span> <span class=\"o\">=</span> <span class=\"n\">cal</span><span class=\"o\">.</span><span class=\"n\">PlattBinnerCalibrator</span><span class=\"p\">(</span><span class=\"n\">num_points</span><span class=\"p\">,</span> <span class=\"n\">num_bins</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"n\">calibrator</span><span class=\"o\">.</span><span class=\"n\">train_calibration</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">)</span>\n</pre>\n<p>Now whenever the model outputs a prediction, we pass it through the calibrator to produce better probabilities.</p>\n<pre><span class=\"n\">calibrated_logits</span> <span class=\"o\">=</span> <span class=\"n\">cal</span><span class=\"o\">.</span><span class=\"n\">calibrate</span><span class=\"p\">(</span><span class=\"n\">test_logits</span><span class=\"p\">)</span>\n</pre>\n<p>Our library makes it very easy to measure confidence intervals on the calibration error as well, using bootstrap resamples.</p>\n<pre><span class=\"p\">[</span><span class=\"n\">lower</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">upper</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">cal</span><span class=\"o\">.</span><span class=\"n\">get_calibration_error_uncertainties</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">)</span>\n</pre>\n<h2>Examples</h2>\n<p>You can find complete code examples in the examples folder. Refer to:</p>\n<ul>\n<li>examples/simple_example.py for a simple example in the binary classification setting.</li>\n<li>examples/multiclass_example.py for the multiclass (more than 2 classes) setting.</li>\n<li>examples/advanced_example.py --- our calibration library also exposes a more customizable interface for advanced users.</li>\n</ul>\n<h2>Citation</h2>\n<p>If you find this library useful please consider citing our paper:</p>\n<pre><code>@inproceedings{kumar2019calibration,\n  author = {Ananya Kumar and Percy Liang and Tengyu Ma},\n  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n  title = {Verified Uncertainty Calibration},\n  year = {2019},\n}\n</code></pre>\n<h2>Features, bugs, and contributions</h2>\n<p>Please feel free to submit feature requests, bug reports, or contribute PRs.\nFeel free to submit a brief description of a PR before you implement it to get feedback on it, or see how it can fit into our project.</p>\n<h2>Verified Uncertainty Calibration paper</h2>\n<p>This repository also contains code for the NeurIPS 2019 (Spotlight) paper <a href=\"https://arxiv.org/abs/1909.10155\" rel=\"nofollow\">Verified Uncertainty Calibration</a></p>\n<p>In our paper, we show that:</p>\n<ul>\n<li>The calibration error of methods like Platt scaling and temperature scaling are typically underestimated, and cannot be easily measured.</li>\n<li>We propose an efficient recalibration method where the calibration error can be measured.</li>\n<li>We show that we can estimate the calibration error with fewer samples (than the standard method) using an estimator from the meteorological literature.</li>\n</ul>\n<h2>Experiments</h2>\n<p>The experiments folder contains experiments for the paper.</p>\n<p>We have 4 sets of experiments:</p>\n<ul>\n<li>Showing the Platt scaling is less calibrated than reported (Section 3)</li>\n<li>Comparing the scaling binning calibrator with histogram binning on CIFAR-10 and ImageNet (Section 4)</li>\n<li>Synthetic experiments to validate our theoretical bounds (Section 4)</li>\n<li>Experiments showing the debiased estimator can estimate calibration error with fewer samples than standard estimator (Section 5)\nRunning each experiment saves plots in the corresponding folder in saved_files</li>\n</ul>\n<p>See our CodaLab worksheet <a href=\"https://worksheets.codalab.org/worksheets/0xb6d027ee127e422989ab9115726c5411\" rel=\"nofollow\">https://worksheets.codalab.org/worksheets/0xb6d027ee127e422989ab9115726c5411</a> which contains all the experiment runs and the exact code used to produce them.</p>\n\n          </div>"}, "last_serial": 6738743, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "ff316058ceeb3bf73baa14eca0bea319", "sha256": "a03b9c34dfebb485dfa8e296e676a1a6af2ab81af48d8280753dc06e38d0de2b"}, "downloads": -1, "filename": "uncertainty-calibration-0.0.1.tar.gz", "has_sig": false, "md5_digest": "ff316058ceeb3bf73baa14eca0bea319", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6175, "upload_time": "2019-12-09T23:32:16", "upload_time_iso_8601": "2019-12-09T23:32:16.450613Z", "url": "https://files.pythonhosted.org/packages/da/59/6290878c846f8d5d491ebd64afa891673131e0333edddf20ea059937cfec/uncertainty-calibration-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "60144a9e0a5e73f01f9dd6bed797e1d9", "sha256": "1e9abde5c2b899b3c19d0f578780a9d0f0ca1891bb136411cc809646156c55d1"}, "downloads": -1, "filename": "uncertainty-calibration-0.0.2.tar.gz", "has_sig": false, "md5_digest": "60144a9e0a5e73f01f9dd6bed797e1d9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6174, "upload_time": "2019-12-09T23:45:46", "upload_time_iso_8601": "2019-12-09T23:45:46.234554Z", "url": "https://files.pythonhosted.org/packages/c4/47/e6eb5e5c2f10ca2521216c6955359e3d8936303c4e7caf473752c634c647/uncertainty-calibration-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "ea6532fbee0fd6db83fdca8503d22c6c", "sha256": "89456f99d284161a21338fc5b69cadc28de37c1fbda38e8b76c86c4229dca9a7"}, "downloads": -1, "filename": "uncertainty-calibration-0.0.3.tar.gz", "has_sig": false, "md5_digest": "ea6532fbee0fd6db83fdca8503d22c6c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6157, "upload_time": "2019-12-09T23:54:51", "upload_time_iso_8601": "2019-12-09T23:54:51.512817Z", "url": "https://files.pythonhosted.org/packages/d3/3f/0c1befabf866ce0ddccd2c872127257ba30c69dd2ca44774a3254e66b856/uncertainty-calibration-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "2a14cf9dc1fc7cee510f84378d88efc4", "sha256": "2cc605b5f8ee270282545a8d9d5e04528324778e9cf98721cfd0a886d0e3a1a8"}, "downloads": -1, "filename": "uncertainty-calibration-0.0.4.tar.gz", "has_sig": false, "md5_digest": "2a14cf9dc1fc7cee510f84378d88efc4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 7652, "upload_time": "2019-12-11T01:02:07", "upload_time_iso_8601": "2019-12-11T01:02:07.975999Z", "url": "https://files.pythonhosted.org/packages/b9/95/8c1129d59dac724f2147b430f7a4cdc1487dcd1d28a1f24167c936399979/uncertainty-calibration-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "15489c2996cb4a416ba55ecef7ee0cb8", "sha256": "def7de156b52de71d2f04daac608038081775fb6a146fb0a9863d72b8ebc2667"}, "downloads": -1, "filename": "uncertainty-calibration-0.0.5.tar.gz", "has_sig": false, "md5_digest": "15489c2996cb4a416ba55ecef7ee0cb8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 8624, "upload_time": "2019-12-11T07:49:59", "upload_time_iso_8601": "2019-12-11T07:49:59.517559Z", "url": "https://files.pythonhosted.org/packages/bd/62/6215fb41529c8dc8ea4394f6dd2c7f0d5cfaa7fee18569cab25c3d51aa57/uncertainty-calibration-0.0.5.tar.gz", "yanked": false}], "0.0.6": [{"comment_text": "", "digests": {"md5": "a0f38f898e3657dafe0cb77498fe78c1", "sha256": "107bddde1e03de6ac2737c27caa1aa434a84cc2e33da06f4995fd13719025dc6"}, "downloads": -1, "filename": "uncertainty-calibration-0.0.6.tar.gz", "has_sig": false, "md5_digest": "a0f38f898e3657dafe0cb77498fe78c1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 11523, "upload_time": "2020-03-03T08:39:06", "upload_time_iso_8601": "2020-03-03T08:39:06.677289Z", "url": "https://files.pythonhosted.org/packages/c2/5b/bc87f5ad83a060a6f8b68efc78769a382eafcd5b7166993159da56f746d9/uncertainty-calibration-0.0.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a0f38f898e3657dafe0cb77498fe78c1", "sha256": "107bddde1e03de6ac2737c27caa1aa434a84cc2e33da06f4995fd13719025dc6"}, "downloads": -1, "filename": "uncertainty-calibration-0.0.6.tar.gz", "has_sig": false, "md5_digest": "a0f38f898e3657dafe0cb77498fe78c1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 11523, "upload_time": "2020-03-03T08:39:06", "upload_time_iso_8601": "2020-03-03T08:39:06.677289Z", "url": "https://files.pythonhosted.org/packages/c2/5b/bc87f5ad83a060a6f8b68efc78769a382eafcd5b7166993159da56f746d9/uncertainty-calibration-0.0.6.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:40:57 2020"}