{"info": {"author": "Grigorios Kalliatakis", "author_email": "gkallia@essex.ac.uk", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6"], "description": "<p align=\"center\">\n  <img src=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/logo_v2.png?raw=true\" width=\"300\" />\n\n[![GitHub license](https://img.shields.io/github/license/GKalliatakis/DisplaceNet.svg)](https://github.com/GKalliatakis/DisplaceNet/blob/master/LICENSE)\n![GitHub issues](https://img.shields.io/github/issues/GKalliatakis/DisplaceNet.svg)\n![GitHub release](https://img.shields.io/github/release/GKalliatakis/DisplaceNet.svg)\n[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=DisplaceNet:%20Recognising%20Displaced%20People%20from%20Images%20by%20Exploiting%20Dominance%20Level&url=https://github.com/GKalliatakis/DisplaceNet&hashtags=ML,DeepLearning,CNNs,HumanRights,HumanRightsViolations,ComputerVisionForHumanRights)\n</p>\n\n\n--------------------------------------------------------------------------------\n### Introduction\n<p align=\"justify\">To reduce the amount of manual labour required for human-rights-related image analysis, \nwe introduce <i>DisplaceNet</i>, a novel model which infers potential displaced people from images \nby integrating the dominance level of the situation and a CNN classifier into one framework.</p>\n\n<p align=\"center\">\n  <img src=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/DisplaceNet.png?raw=true\" width=\"700\" />\n</p>\n\n<p align=\"center\">\n  <a href=\"https://scholar.google.com/citations?user=LMY5lhwAAAAJ&hl=en&oi=ao\" target=\"_blank\">Grigorios Kalliatakis</a> &nbsp;&nbsp;&nbsp;\n  <a href=\"https://scholar.google.com/citations?user=40KlWugAAAAJ&hl=en\" target=\"_blank\">Shoaib Ehsan</a> &nbsp;&nbsp;&nbsp;\n  <a href=\"https://scholar.google.com/citations?user=Hg2osmAAAAAJ&hl=en\" target=\"_blank\">Maria Fasli</a> &nbsp;&nbsp;&nbsp;\n  <a href=\"https://scholar.google.com/citations?user=xYARJTQAAAAJ&hl=en\" target=\"_blank\">Klaus McDonald-Maier</a> &nbsp;&nbsp;&nbsp;\n</p>\n\n<p align=\"center\">\n<i>To appear in 1<sup>st</sup> CVPR Workshop on <br> <a href=\"https://www.cv4gc.org/\" target=\"_blank\">Computer Vision for Global Challenges (CV4GC)</a> &nbsp;&nbsp;&nbsp;\n</i>\n<br>\n<a href=\"https://arxiv.org/pdf/1905.02025.pdf\" target=\"_blank\">[arXiv preprint]</a>\n &nbsp;&nbsp;&nbsp;\n<a href=\"https://arxiv.org/pdf/1905.02025.pdf\" target=\"_blank\">[poster coming soon...]</a>\n</p>\n\n\n\n### Dependencies\n* Python 2.7+\n* Keras 2.1.5+\n* TensorFlow 1.6.0+\n\n### Usage\n\nClone the repository:\n\n    $ git clone https://github.com/GKalliatakis/DisplaceNet.git\n\n\n#### Inference with pretrained models\nTo make a single image inference using DisplaceNet, run the script below. See [run_DisplaceNet.py](https://github.com/GKalliatakis/DisplaceNet/blob/master/run_DisplaceNet.py) for a list of selectable parameters.\n\n   ```bash\n   $ python run_DisplaceNet.py --img_path test_image.jpg \\\n                               --hra_model_backend_name VGG16 \\\n                               --emotic_model_backend_name VGG16 \\\n                               --nb_of_conv_layers_to_fine_tune 1\n   ``` \n\n#### Inference results DisplaceNet vs vanilla CNNs\nMake a single image inference using DisplaceNet and display the results against vanilla CNNs (as shown in the paper). \nFor example to reproduce image below, run the following script.\nSee [displacenet_vs_vanilla.py](https://github.com/GKalliatakis/DisplaceNet/blob/master/displacenet_vs_vanilla.py) for a list of selectable parameters.\n\n   ```bash\n   $ python displacenet_vs_vanilla.py --img_path test_image.jpg \\\n                                      --hra_model_backend_name VGG16 \\\n                                      --emotic_model_backend_name VGG16 \\\n                                      --nb_of_conv_layers_to_fine_tune 1\n   ``` \n\n   <p align=\"center\">\n    <img src=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/inference/results/results_4.jpg?raw=true\" width=\"350\" />\n   </p>\n\n\n#### Training DisplaceNet's branches from scratch\n\n1. To train _displaced people_ branch on the HRA subset, run the training script below. See [train_emotic_unified.py](https://github.com/GKalliatakis/DisplaceNet/blob/master/train_emotic_unified.py) for a list of selectable parameters.\n\n    ```bash\n    $ python train_hra_2class_unified.py --pre_trained_model vgg16 \\\n                                \t     --nb_of_conv_layers_to_fine_tune 1 \\\n                                \t     --nb_of_epochs 50\n    ```\n1. To train _human-centric_ branch on the EMOTIC subset, run the training script below. See [train_emotic_unified.py](https://github.com/GKalliatakis/DisplaceNet/blob/master/train_emotic_unified.py) for a list of selectable parameters.\n\n    ```bash\n    $ python train_emotic_unified.py --body_backbone_CNN VGG16 \\\n                                     --image_backbone_CNN VGG16_Places365 \\\n                                     --modelCheckpoint_quantity val_loss \\\n                                     --earlyStopping_quantity val_loss \\\n                                     --nb_of_epochs 100 \\\n    ```   \n    _Please note that for training the human-centric branch yourself, the HDF5 file containing the preprocessed images and their respective annotations is required (10.4GB)._\n\n### Data of DisplaceNet\n\nHere we release the data for training DisplaceNet to the public.\n\n[Human Rights Archive](https://github.com/GKalliatakis/Human-Rights-Archive-CNNs) is the core set of our dataset, which has been used to train DisplaceNet.\n\nThe constructed dataset contains 609 images of displaced people and the same number of non displaced\npeople counterparts for training, as well as 100 images collected from the web for testing and validation.\n\n* [Train images](https://github.com/GKalliatakis/DisplaceNet/releases/download/v1.0/train.zip)\n* [Validation images](https://github.com/GKalliatakis/DisplaceNet/releases/download/v1.0/val.zip)\n* [Test images](https://github.com/GKalliatakis/DisplaceNet/releases/download/v1.0/test.zip)\n\n\n---\n\n### Results (click on images to enlarge)\n<p align=\"center\">\n  <img src=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/inference/results/results_1.jpg\" width=\"275\" />\n  <img src=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/inference/results/results_2.jpg\" width=\"275\" />\n  <img src=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/inference/results/results_3.jpg\" width=\"275\" />\n  <img src=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/inference/results/results_4.jpg\" width=\"275\" />\n  <img src=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/inference/results/results_5.jpg\" width=\"275\" />\n  <img src=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/inference/results/results_6.jpg\" width=\"275\" />\n</p>\n\n\n### Performance of AbuseNet\n<p align=\"justify\">The performance of displaced people recognition using DisplaceNet is listed below. \nAs comparison, we list the performance of various vanilla CNNs trained with various network backbones, \nfor recognising displaced people. We report comparisons in both accuracy and coverage-the proportion of a data set for which a classifier is able to produce a prediction- metrics</p>\n\n<p align=\"center\">\n  <img src=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/evaluation/performance_table.png?raw=true\" width=\"700\" />\n</p>\n\n---\n\n### Citing DisplaceNet\nIf you use our code in your research or wish to refer to the baseline results, please use the following BibTeX entry:\n\n    @article{kalliatakis2019displacenet,\n    title={DisplaceNet: Recognising Displaced People from Images by Exploiting Dominance Level},\n    author={Kalliatakis, Grigorios and Ehsan, Shoaib and Fasli, Maria and McDonald-Maier, Klaus D},\n    journal={arXiv preprint arXiv:1905.02025},\n    year={2019}\n    }\n\n<p align=\"center\">\n  :octocat:  <br>\n  <i>Repo will be updated with more details soon!</i><br>\n  <i>Make sure you have starred and forked this repository before moving on!</i></b>\n\n</p>\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/GKalliatakis/DisplaceNet/archive/master.zip", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/GKalliatakis/DisplaceNet", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "DisplaceNet", "package_url": "https://pypi.org/project/DisplaceNet/", "platform": "", "project_url": "https://pypi.org/project/DisplaceNet/", "project_urls": {"Download": "https://github.com/GKalliatakis/DisplaceNet/archive/master.zip", "Homepage": "https://github.com/GKalliatakis/DisplaceNet"}, "release_url": "https://pypi.org/project/DisplaceNet/0.1/", "requires_dist": null, "requires_python": "", "summary": "Recognising Displaced People from Images by Exploiting Dominance Level", "version": "0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p align=\"center\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ae8d03fe79e683b93d23dc0d78df858647c62dbe/68747470733a2f2f6769746875622e636f6d2f474b616c6c696174616b69732f446973706c6163654e65742f626c6f622f6d61737465722f6c6f676f5f76322e706e673f7261773d74727565\" width=\"300\">\n</p><p><a href=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"GitHub license\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4f294038d4d682792e0dc460a6af129048207b1b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f474b616c6c696174616b69732f446973706c6163654e65742e737667\"></a>\n<img alt=\"GitHub issues\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/20383567c0c823c67e5ed096ed8303d42d67506f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f474b616c6c696174616b69732f446973706c6163654e65742e737667\">\n<img alt=\"GitHub release\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9f10c9a9d2af86658527c1827239ddd4a1f66a34/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f474b616c6c696174616b69732f446973706c6163654e65742e737667\">\n<a href=\"https://twitter.com/intent/tweet?text=DisplaceNet:%20Recognising%20Displaced%20People%20from%20Images%20by%20Exploiting%20Dominance%20Level&amp;url=https://github.com/GKalliatakis/DisplaceNet&amp;hashtags=ML,DeepLearning,CNNs,HumanRights,HumanRightsViolations,ComputerVisionForHumanRights\" rel=\"nofollow\"><img alt=\"Tweet\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3c681f57e9ba5a57463ba379a8d5671b3c132cde/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c\"></a></p>\n<p></p>\n<hr>\n<h3>Introduction</h3>\n<p align=\"justify\">To reduce the amount of manual labour required for human-rights-related image analysis, \nwe introduce <i>DisplaceNet</i>, a novel model which infers potential displaced people from images \nby integrating the dominance level of the situation and a CNN classifier into one framework.</p>\n<p align=\"center\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/100c2d4779b0bb3ca15d2463c46d3c1247e5c909/68747470733a2f2f6769746875622e636f6d2f474b616c6c696174616b69732f446973706c6163654e65742f626c6f622f6d61737465722f446973706c6163654e65742e706e673f7261773d74727565\" width=\"700\">\n</p>\n<p align=\"center\">\n  <a href=\"https://scholar.google.com/citations?user=LMY5lhwAAAAJ&amp;hl=en&amp;oi=ao\" rel=\"nofollow\">Grigorios Kalliatakis</a> \u00a0\u00a0\u00a0\n  <a href=\"https://scholar.google.com/citations?user=40KlWugAAAAJ&amp;hl=en\" rel=\"nofollow\">Shoaib Ehsan</a> \u00a0\u00a0\u00a0\n  <a href=\"https://scholar.google.com/citations?user=Hg2osmAAAAAJ&amp;hl=en\" rel=\"nofollow\">Maria Fasli</a> \u00a0\u00a0\u00a0\n  <a href=\"https://scholar.google.com/citations?user=xYARJTQAAAAJ&amp;hl=en\" rel=\"nofollow\">Klaus McDonald-Maier</a> \u00a0\u00a0\u00a0\n</p>\n<p align=\"center\">\n<i>To appear in 1<sup>st</sup> CVPR Workshop on <br> <a href=\"https://www.cv4gc.org/\" rel=\"nofollow\">Computer Vision for Global Challenges (CV4GC)</a> \u00a0\u00a0\u00a0\n</i>\n<br>\n<a href=\"https://arxiv.org/pdf/1905.02025.pdf\" rel=\"nofollow\">[arXiv preprint]</a>\n \u00a0\u00a0\u00a0\n<a href=\"https://arxiv.org/pdf/1905.02025.pdf\" rel=\"nofollow\">[poster coming soon...]</a>\n</p>\n<h3>Dependencies</h3>\n<ul>\n<li>Python 2.7+</li>\n<li>Keras 2.1.5+</li>\n<li>TensorFlow 1.6.0+</li>\n</ul>\n<h3>Usage</h3>\n<p>Clone the repository:</p>\n<pre><code>$ git clone https://github.com/GKalliatakis/DisplaceNet.git\n</code></pre>\n<h4>Inference with pretrained models</h4>\n<p>To make a single image inference using DisplaceNet, run the script below. See <a href=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/run_DisplaceNet.py\" rel=\"nofollow\">run_DisplaceNet.py</a> for a list of selectable parameters.</p>\n<pre>$ python run_DisplaceNet.py --img_path test_image.jpg <span class=\"se\">\\</span>\n                            --hra_model_backend_name VGG16 <span class=\"se\">\\</span>\n                            --emotic_model_backend_name VGG16 <span class=\"se\">\\</span>\n                            --nb_of_conv_layers_to_fine_tune <span class=\"m\">1</span>\n</pre>\n<h4>Inference results DisplaceNet vs vanilla CNNs</h4>\n<p>Make a single image inference using DisplaceNet and display the results against vanilla CNNs (as shown in the paper).\nFor example to reproduce image below, run the following script.\nSee <a href=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/displacenet_vs_vanilla.py\" rel=\"nofollow\">displacenet_vs_vanilla.py</a> for a list of selectable parameters.</p>\n<pre>$ python displacenet_vs_vanilla.py --img_path test_image.jpg <span class=\"se\">\\</span>\n                                   --hra_model_backend_name VGG16 <span class=\"se\">\\</span>\n                                   --emotic_model_backend_name VGG16 <span class=\"se\">\\</span>\n                                   --nb_of_conv_layers_to_fine_tune <span class=\"m\">1</span>\n</pre>\n   <p align=\"center\">\n    <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/cce591084a2fceb035e38667517fb483065c98e1/68747470733a2f2f6769746875622e636f6d2f474b616c6c696174616b69732f446973706c6163654e65742f626c6f622f6d61737465722f696e666572656e63652f726573756c74732f726573756c74735f342e6a70673f7261773d74727565\" width=\"350\">\n   </p>\n<h4>Training DisplaceNet's branches from scratch</h4>\n<ol>\n<li>\n<p>To train <em>displaced people</em> branch on the HRA subset, run the training script below. See <a href=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/train_emotic_unified.py\" rel=\"nofollow\">train_emotic_unified.py</a> for a list of selectable parameters.</p>\n<pre>$ python train_hra_2class_unified.py --pre_trained_model vgg16 <span class=\"se\">\\</span>\n                            \t     --nb_of_conv_layers_to_fine_tune <span class=\"m\">1</span> <span class=\"se\">\\</span>\n                            \t     --nb_of_epochs <span class=\"m\">50</span>\n</pre>\n</li>\n<li>\n<p>To train <em>human-centric</em> branch on the EMOTIC subset, run the training script below. See <a href=\"https://github.com/GKalliatakis/DisplaceNet/blob/master/train_emotic_unified.py\" rel=\"nofollow\">train_emotic_unified.py</a> for a list of selectable parameters.</p>\n<pre>$ python train_emotic_unified.py --body_backbone_CNN VGG16 <span class=\"se\">\\</span>\n                                 --image_backbone_CNN VGG16_Places365 <span class=\"se\">\\</span>\n                                 --modelCheckpoint_quantity val_loss <span class=\"se\">\\</span>\n                                 --earlyStopping_quantity val_loss <span class=\"se\">\\</span>\n                                 --nb_of_epochs <span class=\"m\">100</span> <span class=\"se\">\\</span>\n</pre>\n<p><em>Please note that for training the human-centric branch yourself, the HDF5 file containing the preprocessed images and their respective annotations is required (10.4GB).</em></p>\n</li>\n</ol>\n<h3>Data of DisplaceNet</h3>\n<p>Here we release the data for training DisplaceNet to the public.</p>\n<p><a href=\"https://github.com/GKalliatakis/Human-Rights-Archive-CNNs\" rel=\"nofollow\">Human Rights Archive</a> is the core set of our dataset, which has been used to train DisplaceNet.</p>\n<p>The constructed dataset contains 609 images of displaced people and the same number of non displaced\npeople counterparts for training, as well as 100 images collected from the web for testing and validation.</p>\n<ul>\n<li><a href=\"https://github.com/GKalliatakis/DisplaceNet/releases/download/v1.0/train.zip\" rel=\"nofollow\">Train images</a></li>\n<li><a href=\"https://github.com/GKalliatakis/DisplaceNet/releases/download/v1.0/val.zip\" rel=\"nofollow\">Validation images</a></li>\n<li><a href=\"https://github.com/GKalliatakis/DisplaceNet/releases/download/v1.0/test.zip\" rel=\"nofollow\">Test images</a></li>\n</ul>\n<hr>\n<h3>Results (click on images to enlarge)</h3>\n<p align=\"center\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f6eb4c0e6803d3efd634eccfc5671e803b857b5b/68747470733a2f2f6769746875622e636f6d2f474b616c6c696174616b69732f446973706c6163654e65742f626c6f622f6d61737465722f696e666572656e63652f726573756c74732f726573756c74735f312e6a7067\" width=\"275\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9cabe9ef1c3e257c226cbb49700b1c46c491b0bd/68747470733a2f2f6769746875622e636f6d2f474b616c6c696174616b69732f446973706c6163654e65742f626c6f622f6d61737465722f696e666572656e63652f726573756c74732f726573756c74735f322e6a7067\" width=\"275\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a16fe4f76f2a0e9c278712b29c2900e50fc6f8c5/68747470733a2f2f6769746875622e636f6d2f474b616c6c696174616b69732f446973706c6163654e65742f626c6f622f6d61737465722f696e666572656e63652f726573756c74732f726573756c74735f332e6a7067\" width=\"275\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2d530066a1a3574548a24ab660568e5a3c51e753/68747470733a2f2f6769746875622e636f6d2f474b616c6c696174616b69732f446973706c6163654e65742f626c6f622f6d61737465722f696e666572656e63652f726573756c74732f726573756c74735f342e6a7067\" width=\"275\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b9afbb83bbec027f12a056f6fed046d30f1913f5/68747470733a2f2f6769746875622e636f6d2f474b616c6c696174616b69732f446973706c6163654e65742f626c6f622f6d61737465722f696e666572656e63652f726573756c74732f726573756c74735f352e6a7067\" width=\"275\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/786601dd190044dfc3cabff8d41ade4a05ad7080/68747470733a2f2f6769746875622e636f6d2f474b616c6c696174616b69732f446973706c6163654e65742f626c6f622f6d61737465722f696e666572656e63652f726573756c74732f726573756c74735f362e6a7067\" width=\"275\">\n</p>\n<h3>Performance of AbuseNet</h3>\n<p align=\"justify\">The performance of displaced people recognition using DisplaceNet is listed below. \nAs comparison, we list the performance of various vanilla CNNs trained with various network backbones, \nfor recognising displaced people. We report comparisons in both accuracy and coverage-the proportion of a data set for which a classifier is able to produce a prediction- metrics</p>\n<p align=\"center\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/598ed07014798c8fcd9902b8c3508f6f8e3d11ac/68747470733a2f2f6769746875622e636f6d2f474b616c6c696174616b69732f446973706c6163654e65742f626c6f622f6d61737465722f6576616c756174696f6e2f706572666f726d616e63655f7461626c652e706e673f7261773d74727565\" width=\"700\">\n</p>\n<hr>\n<h3>Citing DisplaceNet</h3>\n<p>If you use our code in your research or wish to refer to the baseline results, please use the following BibTeX entry:</p>\n<pre><code>@article{kalliatakis2019displacenet,\ntitle={DisplaceNet: Recognising Displaced People from Images by Exploiting Dominance Level},\nauthor={Kalliatakis, Grigorios and Ehsan, Shoaib and Fasli, Maria and McDonald-Maier, Klaus D},\njournal={arXiv preprint arXiv:1905.02025},\nyear={2019}\n}\n</code></pre>\n<p align=\"center\">\n  :octocat:  <br>\n  <i>Repo will be updated with more details soon!</i><br>\n  <i>Make sure you have starred and forked this repository before moving on!</i>\n</p>\n\n          </div>"}, "last_serial": 5256715, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "bbdc6a199065a0f3f4e12e310f9c8656", "sha256": "2a547c9d35604c3d27dd60788344c8415505e206a9cf33e042dec0dbf986ce3e"}, "downloads": -1, "filename": "DisplaceNet-0.1-py2-none-any.whl", "has_sig": false, "md5_digest": "bbdc6a199065a0f3f4e12e310f9c8656", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 315024, "upload_time": "2019-05-11T17:09:06", "upload_time_iso_8601": "2019-05-11T17:09:06.108438Z", "url": "https://files.pythonhosted.org/packages/69/93/e040ab9cb7442b57c356d6c1861b3f442e645af017a40201dff20edef69a/DisplaceNet-0.1-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8aaa3b08ae073fd2c753c6a036643230", "sha256": "47ead577ec788663dc9644592d3fbb2ccc1306310c9334d2b6d7d87a80251def"}, "downloads": -1, "filename": "DisplaceNet-0.1.tar.gz", "has_sig": false, "md5_digest": "8aaa3b08ae073fd2c753c6a036643230", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 211605, "upload_time": "2019-05-11T17:09:08", "upload_time_iso_8601": "2019-05-11T17:09:08.596582Z", "url": "https://files.pythonhosted.org/packages/4a/00/99bde5982513fb96eb3cf4c36fc478bcc70aff3de996ed39d098a23ecc9c/DisplaceNet-0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "bbdc6a199065a0f3f4e12e310f9c8656", "sha256": "2a547c9d35604c3d27dd60788344c8415505e206a9cf33e042dec0dbf986ce3e"}, "downloads": -1, "filename": "DisplaceNet-0.1-py2-none-any.whl", "has_sig": false, "md5_digest": "bbdc6a199065a0f3f4e12e310f9c8656", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 315024, "upload_time": "2019-05-11T17:09:06", "upload_time_iso_8601": "2019-05-11T17:09:06.108438Z", "url": "https://files.pythonhosted.org/packages/69/93/e040ab9cb7442b57c356d6c1861b3f442e645af017a40201dff20edef69a/DisplaceNet-0.1-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8aaa3b08ae073fd2c753c6a036643230", "sha256": "47ead577ec788663dc9644592d3fbb2ccc1306310c9334d2b6d7d87a80251def"}, "downloads": -1, "filename": "DisplaceNet-0.1.tar.gz", "has_sig": false, "md5_digest": "8aaa3b08ae073fd2c753c6a036643230", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 211605, "upload_time": "2019-05-11T17:09:08", "upload_time_iso_8601": "2019-05-11T17:09:08.596582Z", "url": "https://files.pythonhosted.org/packages/4a/00/99bde5982513fb96eb3cf4c36fc478bcc70aff3de996ed39d098a23ecc9c/DisplaceNet-0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:38:07 2020"}