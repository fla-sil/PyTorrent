{"info": {"author": "", "author_email": "", "bugtrack_url": null, "classifiers": [], "description": "![](media/cover.png)\n\n# Tamnun ML\n\n[![PyPI pyversions](https://img.shields.io/badge/python-3.6%20%7C%203.7-blue)](https://img.shields.io/badge/python-3.6%20%7C%203.7-blue)\n[![CircleCI](https://circleci.com/gh/hiredscorelabs/tamnun-ml.svg?style=svg)](https://circleci.com/gh/hiredscorelabs/tamnun-ml)\n\n`tamnun` is a python framework for Machine and Deep learning algorithms and methods especially in the field of Natural Language Processing and Transfer Learning. The aim of `tamnun` is to provide an easy to use interfaces to build powerful models based on most recent SOTA methods.\n\nFor more about `tamnun`, feel free to read [the introduction to TamnunML on Medium](https://medium.com/hiredscore-engineering/introducing-octoml-73bd527491b1).\n\n# Getting Started\n\n`tamnun` depends on several other machine learning and deep learning frameworks like `pytorch`, `keras` and others. To install `tamnun` and all it's dependencies run:\n\n```\n$ git clone https://github.com/hiredscorelabs/tamnun-ml\n$ cd tamnun-ml\n$ python setup.py install\n```\n\nOr using PyPI:\n\n```\npip install tamnun\n```\n\nJump in and try out an example:\n\n```\n$ cd examples\n$ python finetune_bert.py\n```\n\nOr take a look at the Jupyer notebooks [here](notebooks).\n\n## BERT\n\n*BERT* stands for Bidirectional Encoder Representations from Transformers which is a language model trained by Google and introduced in their [paper](https://arxiv.org/abs/1810.04805).\nHere we use the excellent [PyTorch-Pretrained-BERT](https://pypi.org/project/pytorch-pretrained-bert/) library and wrap it to provide an easy to use [scikit-learn](https://scikit-learn.org/) interface for easy BERT fine-tuning. At the moment, `tamnun` BERT classifier supports binary and multi-class classification. To fine-tune BERT on a specific task:\n\n```python\nfrom tamnun.bert import BertClassifier, BertVectorizer\nfrom sklearn.pipeline import make_pipeline\n\nclf = make_pipeline(BertVectorizer(), BertClassifier(num_of_classes=2)).fit(train_X, train_y)\npredicted = clf.predict(test_X)\n```\n\nPlease see [this notebook](https://github.com/hiredscorelabs/tamnun-ml/blob/master/notebooks/finetune_bert.ipynb) for full code example.\n\n## Fitting (almost) any PyTorch Module using just one line\nYou can use the `TorchEstimator` object to fit any `pytorch` module with just one line:\n```python\nfrom torch import nn\nfrom tamnun.core import TorchEstimator\n\nmodule = nn.Linear(128, 2)\nclf = TorchEstimator(module, task_type='classification').fit(train_X, train_y)\n```\n\nSee [this file](https://github.com/hiredscorelabs/tamnun-ml/blob/master/examples/linear_mnist.py) for a full example of fitting `nn.Linear` module on the [MNIST](http://yann.lecun.com/exdb/mnist/) (classification of handwritten digits) dataset. \n\n## Distiller Transfer Learning\n\nThis module distills a very big (like BERT) model into a much smaller model. Inspired by this [paper](https://arxiv.org/abs/1503.02531).\n\n```python\nfrom tamnun.bert import BertClassifier, BertVectorizer\nfrom tamnun.transfer import Distiller\n\nbert_clf =  make_pipeline(BertVectorizer(do_truncate=True, max_len=3), BertClassifier(num_of_classes=2))\ndistilled_clf = make_pipeline(CountVectorizer(ngram_range=(1,3)), LinearRegression())\n\ndistiller = Distiller(teacher_model=bert_clf, teacher_predict_func=bert_clf.decision_function, student_model=distilled_clf).fit(train_texts, train_y, unlabeled_X=unlabeled_texts)\n\npredicted_logits = distiller.transform(test_texts)\n```\n\nFor full BERT distillation example see [this](https://github.com/hiredscorelabs/tamnun-ml/blob/master/notebooks/distill_bert.ipynb) notebook.\n\n\n\n# Support\n\n## Getting Help\n\nYou can ask questions and join the development discussion on [Github Issues](https://github.com/hiredscorelabs/tamnun-ml/issues)\n\n\n## License\n\nApache License 2.0 (Same as Tensorflow)\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/hiredscorelabs/tamnun-ml", "keywords": "Deep Learning Natural Language Processing NLP Machine Learning Transfer Learning", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "tamnun", "package_url": "https://pypi.org/project/tamnun/", "platform": "", "project_url": "https://pypi.org/project/tamnun/", "project_urls": {"Homepage": "https://github.com/hiredscorelabs/tamnun-ml"}, "release_url": "https://pypi.org/project/tamnun/0.1.1/", "requires_dist": ["numpy (==1.15.4)", "scikit-learn (==0.20.2)", "torch (==1.1.0)", "pytorch-transformers"], "requires_python": "", "summary": "An easy to use open-source library for advanced Deep Learning and Natural Language Processing", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ea174f1e1785fba76472826dce31d0b456c7e99d/6d656469612f636f7665722e706e67\"></p>\n<h1>Tamnun ML</h1>\n<p><a href=\"https://img.shields.io/badge/python-3.6%20%7C%203.7-blue\" rel=\"nofollow\"><img alt=\"PyPI pyversions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4d449c4c831333bf71cb120cefbcadec323796d9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e36253230253743253230332e372d626c7565\"></a>\n<a href=\"https://circleci.com/gh/hiredscorelabs/tamnun-ml\" rel=\"nofollow\"><img alt=\"CircleCI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d2a3b9a66da8989719edfbc9f7e798d4e2640864/68747470733a2f2f636972636c6563692e636f6d2f67682f686972656473636f72656c6162732f74616d6e756e2d6d6c2e7376673f7374796c653d737667\"></a></p>\n<p><code>tamnun</code> is a python framework for Machine and Deep learning algorithms and methods especially in the field of Natural Language Processing and Transfer Learning. The aim of <code>tamnun</code> is to provide an easy to use interfaces to build powerful models based on most recent SOTA methods.</p>\n<p>For more about <code>tamnun</code>, feel free to read <a href=\"https://medium.com/hiredscore-engineering/introducing-octoml-73bd527491b1\" rel=\"nofollow\">the introduction to TamnunML on Medium</a>.</p>\n<h1>Getting Started</h1>\n<p><code>tamnun</code> depends on several other machine learning and deep learning frameworks like <code>pytorch</code>, <code>keras</code> and others. To install <code>tamnun</code> and all it's dependencies run:</p>\n<pre><code>$ git clone https://github.com/hiredscorelabs/tamnun-ml\n$ cd tamnun-ml\n$ python setup.py install\n</code></pre>\n<p>Or using PyPI:</p>\n<pre><code>pip install tamnun\n</code></pre>\n<p>Jump in and try out an example:</p>\n<pre><code>$ cd examples\n$ python finetune_bert.py\n</code></pre>\n<p>Or take a look at the Jupyer notebooks <a href=\"notebooks\" rel=\"nofollow\">here</a>.</p>\n<h2>BERT</h2>\n<p><em>BERT</em> stands for Bidirectional Encoder Representations from Transformers which is a language model trained by Google and introduced in their <a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">paper</a>.\nHere we use the excellent <a href=\"https://pypi.org/project/pytorch-pretrained-bert/\" rel=\"nofollow\">PyTorch-Pretrained-BERT</a> library and wrap it to provide an easy to use <a href=\"https://scikit-learn.org/\" rel=\"nofollow\">scikit-learn</a> interface for easy BERT fine-tuning. At the moment, <code>tamnun</code> BERT classifier supports binary and multi-class classification. To fine-tune BERT on a specific task:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tamnun.bert</span> <span class=\"kn\">import</span> <span class=\"n\">BertClassifier</span><span class=\"p\">,</span> <span class=\"n\">BertVectorizer</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">make_pipeline</span>\n\n<span class=\"n\">clf</span> <span class=\"o\">=</span> <span class=\"n\">make_pipeline</span><span class=\"p\">(</span><span class=\"n\">BertVectorizer</span><span class=\"p\">(),</span> <span class=\"n\">BertClassifier</span><span class=\"p\">(</span><span class=\"n\">num_of_classes</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">train_X</span><span class=\"p\">,</span> <span class=\"n\">train_y</span><span class=\"p\">)</span>\n<span class=\"n\">predicted</span> <span class=\"o\">=</span> <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">test_X</span><span class=\"p\">)</span>\n</pre>\n<p>Please see <a href=\"https://github.com/hiredscorelabs/tamnun-ml/blob/master/notebooks/finetune_bert.ipynb\" rel=\"nofollow\">this notebook</a> for full code example.</p>\n<h2>Fitting (almost) any PyTorch Module using just one line</h2>\n<p>You can use the <code>TorchEstimator</code> object to fit any <code>pytorch</code> module with just one line:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">torch</span> <span class=\"kn\">import</span> <span class=\"n\">nn</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tamnun.core</span> <span class=\"kn\">import</span> <span class=\"n\">TorchEstimator</span>\n\n<span class=\"n\">module</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">clf</span> <span class=\"o\">=</span> <span class=\"n\">TorchEstimator</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">task_type</span><span class=\"o\">=</span><span class=\"s1\">'classification'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">train_X</span><span class=\"p\">,</span> <span class=\"n\">train_y</span><span class=\"p\">)</span>\n</pre>\n<p>See <a href=\"https://github.com/hiredscorelabs/tamnun-ml/blob/master/examples/linear_mnist.py\" rel=\"nofollow\">this file</a> for a full example of fitting <code>nn.Linear</code> module on the <a href=\"http://yann.lecun.com/exdb/mnist/\" rel=\"nofollow\">MNIST</a> (classification of handwritten digits) dataset.</p>\n<h2>Distiller Transfer Learning</h2>\n<p>This module distills a very big (like BERT) model into a much smaller model. Inspired by this <a href=\"https://arxiv.org/abs/1503.02531\" rel=\"nofollow\">paper</a>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tamnun.bert</span> <span class=\"kn\">import</span> <span class=\"n\">BertClassifier</span><span class=\"p\">,</span> <span class=\"n\">BertVectorizer</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tamnun.transfer</span> <span class=\"kn\">import</span> <span class=\"n\">Distiller</span>\n\n<span class=\"n\">bert_clf</span> <span class=\"o\">=</span>  <span class=\"n\">make_pipeline</span><span class=\"p\">(</span><span class=\"n\">BertVectorizer</span><span class=\"p\">(</span><span class=\"n\">do_truncate</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">max_len</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"n\">BertClassifier</span><span class=\"p\">(</span><span class=\"n\">num_of_classes</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">))</span>\n<span class=\"n\">distilled_clf</span> <span class=\"o\">=</span> <span class=\"n\">make_pipeline</span><span class=\"p\">(</span><span class=\"n\">CountVectorizer</span><span class=\"p\">(</span><span class=\"n\">ngram_range</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">)),</span> <span class=\"n\">LinearRegression</span><span class=\"p\">())</span>\n\n<span class=\"n\">distiller</span> <span class=\"o\">=</span> <span class=\"n\">Distiller</span><span class=\"p\">(</span><span class=\"n\">teacher_model</span><span class=\"o\">=</span><span class=\"n\">bert_clf</span><span class=\"p\">,</span> <span class=\"n\">teacher_predict_func</span><span class=\"o\">=</span><span class=\"n\">bert_clf</span><span class=\"o\">.</span><span class=\"n\">decision_function</span><span class=\"p\">,</span> <span class=\"n\">student_model</span><span class=\"o\">=</span><span class=\"n\">distilled_clf</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">train_texts</span><span class=\"p\">,</span> <span class=\"n\">train_y</span><span class=\"p\">,</span> <span class=\"n\">unlabeled_X</span><span class=\"o\">=</span><span class=\"n\">unlabeled_texts</span><span class=\"p\">)</span>\n\n<span class=\"n\">predicted_logits</span> <span class=\"o\">=</span> <span class=\"n\">distiller</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">test_texts</span><span class=\"p\">)</span>\n</pre>\n<p>For full BERT distillation example see <a href=\"https://github.com/hiredscorelabs/tamnun-ml/blob/master/notebooks/distill_bert.ipynb\" rel=\"nofollow\">this</a> notebook.</p>\n<h1>Support</h1>\n<h2>Getting Help</h2>\n<p>You can ask questions and join the development discussion on <a href=\"https://github.com/hiredscorelabs/tamnun-ml/issues\" rel=\"nofollow\">Github Issues</a></p>\n<h2>License</h2>\n<p>Apache License 2.0 (Same as Tensorflow)</p>\n\n          </div>"}, "last_serial": 5633031, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "b92e68f20121fe2ed789fc0c7d1da937", "sha256": "bca15802df944a0a3f72b69f78e9ddf5047e6cdee1d84e29aa8c062485f89285"}, "downloads": -1, "filename": "tamnun-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "b92e68f20121fe2ed789fc0c7d1da937", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9537, "upload_time": "2019-08-05T07:40:05", "upload_time_iso_8601": "2019-08-05T07:40:05.184215Z", "url": "https://files.pythonhosted.org/packages/28/5c/4f72b512c40e80f3c256cf0c7938e1d858db5519164cac37e4fa16898713/tamnun-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9c9a36dbf0dffe4e0645a9b0e2c3d3ac", "sha256": "92469a26865795644226d7498c179f376b5cf1e85d84b5ad4589c95872d0a97a"}, "downloads": -1, "filename": "tamnun-0.1.0.tar.gz", "has_sig": false, "md5_digest": "9c9a36dbf0dffe4e0645a9b0e2c3d3ac", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8510, "upload_time": "2019-08-05T07:40:07", "upload_time_iso_8601": "2019-08-05T07:40:07.454573Z", "url": "https://files.pythonhosted.org/packages/2b/15/d3c0e3407a103a181bc3ce0ce1f47ef19537aae0377656a973ea19ef1b60/tamnun-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "a7146149da0b315d642bb3e259a94b86", "sha256": "82e1684a25883f660f9057bc88bf21f7b3f9db3c49faca204b3f72eb35898624"}, "downloads": -1, "filename": "tamnun-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "a7146149da0b315d642bb3e259a94b86", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9525, "upload_time": "2019-08-05T07:56:18", "upload_time_iso_8601": "2019-08-05T07:56:18.450748Z", "url": "https://files.pythonhosted.org/packages/85/00/3905332b6dc3cd3f1d9921d64b6a6a80dc96938624389c25f960b58333ad/tamnun-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "64c8c2ad2737d1ced94bf6eb8ed71793", "sha256": "973023048316115055cdf69febf56dcfcc907cb9035476839f4d80aeeb908918"}, "downloads": -1, "filename": "tamnun-0.1.1.tar.gz", "has_sig": false, "md5_digest": "64c8c2ad2737d1ced94bf6eb8ed71793", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8534, "upload_time": "2019-08-05T07:56:20", "upload_time_iso_8601": "2019-08-05T07:56:20.225220Z", "url": "https://files.pythonhosted.org/packages/cc/ec/7d456fbefeaf3a3bdf2b6e2d9ff8b4eab92d62592c99b92b1d93d8f76d6f/tamnun-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a7146149da0b315d642bb3e259a94b86", "sha256": "82e1684a25883f660f9057bc88bf21f7b3f9db3c49faca204b3f72eb35898624"}, "downloads": -1, "filename": "tamnun-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "a7146149da0b315d642bb3e259a94b86", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9525, "upload_time": "2019-08-05T07:56:18", "upload_time_iso_8601": "2019-08-05T07:56:18.450748Z", "url": "https://files.pythonhosted.org/packages/85/00/3905332b6dc3cd3f1d9921d64b6a6a80dc96938624389c25f960b58333ad/tamnun-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "64c8c2ad2737d1ced94bf6eb8ed71793", "sha256": "973023048316115055cdf69febf56dcfcc907cb9035476839f4d80aeeb908918"}, "downloads": -1, "filename": "tamnun-0.1.1.tar.gz", "has_sig": false, "md5_digest": "64c8c2ad2737d1ced94bf6eb8ed71793", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8534, "upload_time": "2019-08-05T07:56:20", "upload_time_iso_8601": "2019-08-05T07:56:20.225220Z", "url": "https://files.pythonhosted.org/packages/cc/ec/7d456fbefeaf3a3bdf2b6e2d9ff8b4eab92d62592c99b92b1d93d8f76d6f/tamnun-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:58:13 2020"}