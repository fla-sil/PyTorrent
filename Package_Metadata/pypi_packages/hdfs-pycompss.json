{"info": {"author": "Lucas Miguel Ponce", "author_email": "lucasmsp@dcc.ufmg.br", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: Apache Software License", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: System :: Distributed Computing"], "description": "# Integration: PyCOMPSs and HDFS\n\nThe abstraction that is provided by this version is exactly the same as\n that provided by the version in java. Please read the Java version \n before continuing.\n\n# How to install the HDFSPyCOMPSs module\n\nThis module is available at PyPi, \n\n```bash\n    $ pip3 install hdfs-pycompss\n```\n\nAfter install it, you need set up some environment variables:\n\n * HADOOP_HOME: the root of your installed Hadoop distribution. Often has lib/native/libhdfs.so.\n * JAVA_HOME: the location of your Java SDK installation.\n * CLASSPATH: must contain the Hadoop jars\n \n```bash\nexport CLASSPATH=$CLASSPATH:`$HADOOP_HOME/bin/hdfs classpath --glob`\n```\n\nBecause COMPSs don't copy all environment variables to all workers, it's important to set these variables at /etc/environment.\n\n\n# Example of how to use the API (without StorageAPI)\n\n```python\ndef wordcount(blk, word):\n    from hdfspycompss.block import Block\n    data = Block(blk).read_block()\n    ...\n    return result\n\ndef main():\n    import hdfspycompss.hdfs import HDFS\n    dfs = HDFS(host='localhost', port=9000)\n    HDFS_BLOCKS = dfs.find_blocks('/input.data')\n\n    nFrag = len(HDFS_BLOCKS)\n    result = [{} for f in range(nFrag)]\n    for f, blk in enumerate(HDFS_BLOCKS):\n        result[f] = wordcount(blk, 'word')\n    ...\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/eubr-bigsea/compss-hdfs", "keywords": "", "license": "Apache License, Version 2.0", "maintainer": "", "maintainer_email": "", "name": "hdfs-pycompss", "package_url": "https://pypi.org/project/hdfs-pycompss/", "platform": "Linux", "project_url": "https://pypi.org/project/hdfs-pycompss/", "project_urls": {"Homepage": "https://github.com/eubr-bigsea/compss-hdfs"}, "release_url": "https://pypi.org/project/hdfs-pycompss/0.4/", "requires_dist": null, "requires_python": "", "summary": "HDFSPyCOMPSs is an API that enables PyCOMPSs to read HDFS files in parallel.", "version": "0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Integration: PyCOMPSs and HDFS</h1>\n<p>The abstraction that is provided by this version is exactly the same as\nthat provided by the version in java. Please read the Java version\nbefore continuing.</p>\n<h1>How to install the HDFSPyCOMPSs module</h1>\n<p>This module is available at PyPi,</p>\n<pre>    $ pip3 install hdfs-pycompss\n</pre>\n<p>After install it, you need set up some environment variables:</p>\n<ul>\n<li>HADOOP_HOME: the root of your installed Hadoop distribution. Often has lib/native/libhdfs.so.</li>\n<li>JAVA_HOME: the location of your Java SDK installation.</li>\n<li>CLASSPATH: must contain the Hadoop jars</li>\n</ul>\n<pre><span class=\"nb\">export</span> <span class=\"nv\">CLASSPATH</span><span class=\"o\">=</span><span class=\"nv\">$CLASSPATH</span>:<span class=\"sb\">`</span><span class=\"nv\">$HADOOP_HOME</span>/bin/hdfs classpath --glob<span class=\"sb\">`</span>\n</pre>\n<p>Because COMPSs don't copy all environment variables to all workers, it's important to set these variables at /etc/environment.</p>\n<h1>Example of how to use the API (without StorageAPI)</h1>\n<pre><span class=\"k\">def</span> <span class=\"nf\">wordcount</span><span class=\"p\">(</span><span class=\"n\">blk</span><span class=\"p\">,</span> <span class=\"n\">word</span><span class=\"p\">):</span>\n    <span class=\"kn\">from</span> <span class=\"nn\">hdfspycompss.block</span> <span class=\"kn\">import</span> <span class=\"n\">Block</span>\n    <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">Block</span><span class=\"p\">(</span><span class=\"n\">blk</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">read_block</span><span class=\"p\">()</span>\n    <span class=\"o\">...</span>\n    <span class=\"k\">return</span> <span class=\"n\">result</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">():</span>\n    <span class=\"kn\">import</span> <span class=\"nn\">hdfspycompss.hdfs</span> <span class=\"kn\">import</span> <span class=\"nn\">HDFS</span>\n    <span class=\"n\">dfs</span> <span class=\"o\">=</span> <span class=\"n\">HDFS</span><span class=\"p\">(</span><span class=\"n\">host</span><span class=\"o\">=</span><span class=\"s1\">'localhost'</span><span class=\"p\">,</span> <span class=\"n\">port</span><span class=\"o\">=</span><span class=\"mi\">9000</span><span class=\"p\">)</span>\n    <span class=\"n\">HDFS_BLOCKS</span> <span class=\"o\">=</span> <span class=\"n\">dfs</span><span class=\"o\">.</span><span class=\"n\">find_blocks</span><span class=\"p\">(</span><span class=\"s1\">'/input.data'</span><span class=\"p\">)</span>\n\n    <span class=\"n\">nFrag</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">HDFS_BLOCKS</span><span class=\"p\">)</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"p\">[{}</span> <span class=\"k\">for</span> <span class=\"n\">f</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">nFrag</span><span class=\"p\">)]</span>\n    <span class=\"k\">for</span> <span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">blk</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">HDFS_BLOCKS</span><span class=\"p\">):</span>\n        <span class=\"n\">result</span><span class=\"p\">[</span><span class=\"n\">f</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">wordcount</span><span class=\"p\">(</span><span class=\"n\">blk</span><span class=\"p\">,</span> <span class=\"s1\">'word'</span><span class=\"p\">)</span>\n    <span class=\"o\">...</span>\n</pre>\n\n          </div>"}, "last_serial": 6178325, "releases": {"0.3": [{"comment_text": "", "digests": {"md5": "a4b75b25c67a8cdb23c3fd5d9c9fbe9f", "sha256": "5f8c2059fdce005c865675912507e5ddded45cfae4c5a53b8c08573f438bad6a"}, "downloads": -1, "filename": "hdfs-pycompss-0.3.tar.gz", "has_sig": false, "md5_digest": "a4b75b25c67a8cdb23c3fd5d9c9fbe9f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10381, "upload_time": "2019-11-22T01:11:52", "upload_time_iso_8601": "2019-11-22T01:11:52.160092Z", "url": "https://files.pythonhosted.org/packages/52/52/49ceed112deab874ec135e278839085b7acb0249aee3d49fee27b95aa576/hdfs-pycompss-0.3.tar.gz", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "462e13cd5e97529d557abaa079f5d0bb", "sha256": "490e249d5bf4e542fa3ba8918c226abc7ff06c89a5c3a0f437691756b87c7529"}, "downloads": -1, "filename": "hdfs-pycompss-0.4.tar.gz", "has_sig": false, "md5_digest": "462e13cd5e97529d557abaa079f5d0bb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10377, "upload_time": "2019-11-22T01:25:02", "upload_time_iso_8601": "2019-11-22T01:25:02.248918Z", "url": "https://files.pythonhosted.org/packages/f0/07/cb943776d705ed976f71b8f43869f0f5834972d5a2a0dee74d0a42634839/hdfs-pycompss-0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "462e13cd5e97529d557abaa079f5d0bb", "sha256": "490e249d5bf4e542fa3ba8918c226abc7ff06c89a5c3a0f437691756b87c7529"}, "downloads": -1, "filename": "hdfs-pycompss-0.4.tar.gz", "has_sig": false, "md5_digest": "462e13cd5e97529d557abaa079f5d0bb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10377, "upload_time": "2019-11-22T01:25:02", "upload_time_iso_8601": "2019-11-22T01:25:02.248918Z", "url": "https://files.pythonhosted.org/packages/f0/07/cb943776d705ed976f71b8f43869f0f5834972d5a2a0dee74d0a42634839/hdfs-pycompss-0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:52:04 2020"}