{"info": {"author": "Hans Terje Bakke", "author_email": "hans.terje.bakke@comperio.no", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7"], "description": "# elasticsearch-eslib\n\n2015.11.14 -- Hans Terje Bakke\n\nPython library for document processing for Elasticsearch.\n\nWhile Elasticsearch is what we originally created it for, it is fully usable for anything else as well.\nThe only limitation is that a lot of the processing stages are using an Elasticsearch-like document format.\nBut this can easily be converted to something else. For example, a SolrWriter could take an \"esdoc\" as input\nand write it to Solr. The \"esdoc\" in this case is simply a JSON compatible Python dict type with the following\nmeta fields, that may even be omitted in many cases, and that you can make whatever you want of:\n\n```json\n{\n    \"_index\"  : \"(some kind of document cluster, like DB table, search index, etc.)\",\n    \"_type\"   : \"(document type in some data store)\",\n    \"_id\"     : \"(document id)\",\n    \"_source\" : {\n        (document fields and data go here...)\n    }\n}\n```\n\n## Introduction\n\nA 'processor' processes incoming data and generates output.\n\nIt can also generate its own data or fetch from external data sources and services.\nInstead, or in addition to, writing output to its own 'sockets', it can also write data\nto external data sources and services. In these cases it is commonly referred to as a 'generator',\nand has its own execution thread.\n\nA processor has one or more input 'connectors' that can connect to one more output 'sockets'.\nConnectors and sockets (commonly called 'terminals') are registered with an optional 'protocol' tag.\nIf it exists, an attempted connection will check if the data protocol is the same in both connector\nand socket.\n\nA processor B is said to 'subscribe' to output from processor A if it has a connector connected a\nsocket on A. In this case, A has the role of 'producer' (to B) and B has the role of 'subscriber' (to A).\n\n## Installation\n\n### Prerequisites\n\nThis library requires python 2.7.\n\nBeautifulSoup, used in some of the document processing stages, requires libxml2 and libxslt.\nOn Fedora (Amazon Linux), this can be installed using:\n\n```\nyum install libxml2-devel libxslt-devel\n```\n\n### Install eslib with pip\n```\npip install eslib\n```\n\n## Usage\n\nFrom a Python script, we can create a processing graph as in this example:\n\n```python\na = ElasticsearchReader()\nb = ElasticsearchWriter()\na.config.index = \"employees\"\nb.config.index = \"employees_copy\"\nb.subscribe(a)\n```\n    \nand execute it with\n\n```python\na.start()\n```\n\nIn this simple example, the first processor is a generator, and the entire pipeline will finish when 'a'\ncompletes. The simple \"b.subscribe(a)\" is possible because there is only one connector in 'b' and only\none socket in 'a'. Otherwise, we would have to specify the connector and socket names.\n\nBy default, a processor that is stopped either explicitly from outside, or completes generating data (as\nin this example), will send a stop signal to its subscribers. This is not always a desirable behaviour.\nSay we had 20 readers sending data to 1 writer. We would not like the writer to stop when the first reader\ncompletes. To avoid this, we can use\n\n```python\n...\nb.keepalive = True\na.start()\ntime.sleep(10)  # The reader is still working in its own thread\nb.put(mydoc)    # Writes to the only connector (\"input\", of protocol \"esdoc\")\na.wait()        # Wait for a to complete/stop\nb.stop()        # ... then explicitly stop b\n```\n\nOne processor/connector can subscribe to data from many processors/sockets. One processor can have many\ndifferent named connectors, expecting data in various formats (hinted by its 'protocol' tag.) And a processor/socket\ncan have many processors/connectors subscribing to data it outputs.\n\n### Behind the scene\n\nTechnically, a processor sends document data to its sockets. The sockets send documents to its connected connectors.\nA connector has a queue of incoming items, and a thread that pulls documents off the queue and sends it to\na processing method in the processor class. This method processes the data and sends the result to one or\nmore of its sockets, which again send to connected connectors...\n\nA generator style processor has another thread that generates documents somehow, and sends it to its socket(s).\n\n### Listen to output\n\nAnalogous with the processor.put(doc) command, you might also want to listen to output from a processor in your\nprogram. You can do this by adding a 'callback' for the socket. For example like this\n\n```python\noutput = []\nprocessor.add_callback(lambda processor, doc: output.append(doc), socket_name)\n...\nprocessor.start()\nprocessor.wait()\nprint output\n```\n\nor instead of the lambda function, use a method that takes a processor and document as an argument, e.g.:\n\n```python\ndef do_stuff(processor, document):\n    print \"I spy with my little eye, a document containing:\", document\n\n...\nprocessor.add_callback(do_stuff)\n...\n```\n\n### Events\n\nYou can register methods to listen to events. The following events can be subscribed to:\n\n```python\nevent_started\nevent_stopped\nevent_aborted\n```\n    \nYou register a method that takes no a processor as arguments like so\n\n```python\ndef myfunc(processor)\n    print \"Processor '%s' started\" % processor.name\n    \nmyproc.event_started.append(myfunc)\n```\n\n### Protocol compliance\n\nWhen sockets and connector are joined (\"connected\"), there is a check for protocol compliance. These are string\ntags using a dot-notation for specializations. A terminal is registered with 'any' if it doesn't care about the\nprotocol. Explanation by some examples:\n\n```text\nSOCKET PROTOCOL          CONNECTOR PROTOCOL        WILL MATCH?\n---------------          ------------------        -----------\nseating.chair            seating.chair             yes (of course)\nseating.chair.armchair   seating                   yes, connector accepts any seating\nseating                  seating.chair.armchair    no, connector expects armchairs, specifically\nany                      string                    yes (but, consumer: beware! it might be anything!)\nstring                   any                       yes, we accept anything\n```\n\n### Members for using the Processor (and derivates)\n\n```text\nRead/write:\n    keepalive\nRead only:\n    accepting\n    stopping\n    running\n    suspended\n    aborted\nEvent lists:\n    event_started\n    event_stopped\n    event_aborted\nMethods:\n    __init__(name) # Constructor/init\n    subscribe(producer=None, socket_name=None, connector_name=None)\n    unsubscribe(producer=None, socket_name=None, connector_name=None)\n    attach(subscriber, socket_name=None, connector_name=None)\n    detach(subscriber, socket_name=None, connector_name=None)\n    connector_info(*args)  # returns list\n    socket_info(*args)     # returns list\n    start()\n    stop()\n    abort()\n    suspend()\n    resume()\n    wait()\n    put(document, connector_name=None)\n    add_callback(method, socket_name=None)\nMethods for debugging:\n    DUMP\n    DUMP_connectors\n    DUMP_sockets\n```\n\n## Writing your own Processor\n\nThe simple processor (not Generator type) typically has one or more connectors. A connector receives data from\na socket, or from a \"processor.put(document, connector_name)\" statement (which in turn puts the data on the\nconnector queue). Internally, a connector has a queue and is running a its own thread that pulls items off the\nqueue and executes whatever method is registered with the connector.\n\nAny object passed to such a method is considered to be read-only. You *may* alter it, preferably only add to it.\nBut it is generally a bad idea, since many processors could potentially receive the same object. If you want to\npass it on to a socket as-is, that's fine. And it is the best performance wise. But if you need to alter it,\nyou should consider creating a deep or shallow clone. Shallow clones are fine if you just want to change one\npart of the object and refer to the rest as it is.\n\nAs a general rule of thumb you should never alter the state members yourself directly. If you want to have\nthe processor stop or abort itself, you should call \"self.stop()\" or \"self.abort()\".\n\n### A simple processor\n\nLet's start with a simple processor that receives input on a connector and writes its processed output to a socket.\nLet's make a processor that reverses and optionally swaps the casing of a string.\n\n```python\nfrom eslib import Processor\n\nclass StringReverser(Processor):\n\n    def __init__(self, **kwargs):\n        super(StringReverser, self).__init__(**kwargs)\n        \n        self.config.set_default(swapcase = False)\n```\n        \nNotice **kwargs, the \"swapcase\" config variable and the already instantiated config class. It is meant for\ncontaining config variables. The incoming keyword arguments are put into the config as attributes. We *must* set\ndefault values for all the config variables we want to use inside the processor, as this is also a way to define\ntheir existence, should they not come in as keyword arguments. Otherwise, we risk getting exceptions when try to\nuse them.\n\nIn this case we will set \"swapcase\" if we want to swap the casing of the string we are reversing.\n\nWe also create a connector for the input and two sockets for the output. One is a pure pass-through while the\nother provides the modified output:\n\n```python\n        self.create_connector(self._incoming, \"input\", \"str\")\n        self.create_socket(\"original\", \"str\")\n        self.create_socket(\"modified\", \"str\")\n```\n\nWe use \"str\" as a protocol tag. This is not the same as a Python type; it is simply a hint. When connecting\nsockets and connectors there is check for protocol compliance. If you want to expect or spew out anything, simply\nspecify None or omit the protocol specification.\n\nThe following member methods are called when the processor starts or stops (including getting aborted),\nrespectively:\n\n```python\n    def on_open(self):  pass  # Before threads are spun up.\n    def on_abort(self): pass  # After call to abort(), but before closing with on_close()\n    def on_close(self): pass  # Final call after stopping/aborting processor; always called\n```\n\nThis is typically used for opening and closing files, establishing remote connections, resetting counters, etc.\n\nFor this example, there is nothing special we want to do when starting and stopping. (Really, starting the\nprocessor in this case simply spins up the connector, that will deliver documents to our \"_incoming(document)\"\nmethod as soon as it can. So now on to this method:\n\n```python\n    def _incoming(self, document):\n        # TODO: You might want to check if the document is indeed a 'str' or 'unicode' here...\n        \n        s = document[::-1]  # Reverse the string of characters\n        if self.config.swapcase:\n            s = s.swapcase()\n        \n        # Write to the sockets:\n        self.sockets[\"original\"].send(document)  # Incoming document, unmodified\n        self.sockets[\"modified\"].send(s)\n```\n\nOften, processing can be quite heavy stuff, and quite unnecessary to do a lot of work with producing output if\nthere are no consumers. Therefore, you might want to first check if there is actually any consumers expecting\noutput either for the entire processor or per socket, with\n\n```python\n        if not self.has_output:\n            return\n            \n        # or\n        \n        if self.sockets[\"modified\"].has_output:\n            # calculate and send the stuff...\n```\n\n### Useful members for implementing simple Processors\n\n```text\n    Methods for you to implement:\n        __init__(name=None)    # constructor; remember to call super(your_class, self).__init__(name)\n        on_open()              # called before starting execution threads\n        on_abort()             # called after a processor receives a call to abort(), but before on_close()\n        on_close()             # called when the processor has stopped or aborted\n    Read-only properties and variables:\n        name                   # name of the processor\n        config                 # object containing  all configuration data for the processor\n        connectors             # dict\n        sockets                # dict\n        has_output             # bool; indicating whether there are sockets with connections\n        log                    # logger; log processor events here\n        doclog                 # logger; log problems with documents here\n    Methods to call:\n        create_connector(method, name=None, protocol=None, description=None, is_default=False)\n        create_socket(name=None, description=None, is_default=False, mimic=None)\n        stop()                 # call this if you want to explicitly stop prematurely\n        abort()                # call this if you want to explicitly abort prematurely\n    Properties and methods on sockets:\n        socket.has_output      # bool; indicating whether the socket has connections (subscribers)\n        socket.send(document)  # sends document to connected subscribers for asynchronous processing\n```\n\n\n### Default terminal\n\nA connector can be set as default using the 'is_default' parameter or setting the 'default_connector' and\n'default_socket'. There can only be one default connector and one default socket. This makes it possible\nto address a terminal without name, which will then be routed to the default terminal. If only one socket\nor connector exists, then that one also becomes the defacto default within its collection.\n\n### Protocol \"mimicing\"\n\nA processor can have a socket \"mimic\" protocol of connected connector. This is useful in a processing chain\nA-B-C, where B massages the output from A and passes it on to C. A and C needs to have the same protocols on\ntheir terminals, while B can be more general. Example:\n\n```text\n    TwitterMonitor socket=tweet(esdoc.tweet)\n        => HtmlRemover connector=input(esdoc) | socket=output(esdoc) mimic=input\n            => TweetExtractor connector=tweet(esdoc.tweet)\n```\n\nNormal protocol compliance would dictate that TweetExtractor connector would require the more specific\n'esdoc.tweet' protocol, while the HtmlRemover only outputs 'esdoc'. However, HtmlRemover has its output\nsocket set to 'mimic' its input connector. When we then attach the TwitterMonitor socket with protocol\n'esdoc.tweet', the HtmlRemover's socket will mimic that protocol, and the TweetExtractor will accept it.\n\n### Processor lifespan\n\nA simple processor typically sits in between other processors in a pipeline or graph. They are started by\nanother processor earlier in the chain, and they are instructed to stop when a processor they are subscribing to\nstops. So a processor C subscribing to A or B will stop if one of the other stops. Unless it is flagged with\n\"keepalive = True\".\n\nWhen stopping, the processor closes all its connectors for further input (by setting \"connector.accept = False\").\nThe connectors will then continue to work off their queues until empty, and then the processor are finally\nstopped. For immediate termination without processing whatever is still queued up, an \"abort()\" must be called.\n\n### Generators and Monitors\n\nA 'Generator' is a Processor that is expected to produce its output mainly from other sources than what is coming\nin through connectors. It has its own execution thread. For example, a FileWriter is a simple processor that\nwrites whatever it gets on its connector to a file. A FileReader has its own worker thread that reads lines from\nfiles and generates documents as its output, and therefor implements Generator. An ElasticsearchWriter is a\nGenerator because it needs its own worker thread to gather up incoming documents and send them in batches to the\nserver for higher performance. So although they have the file and Elasticsearch writers have similar purposes,\nthey have different implementation schemes.\n\n### Generator and Monitor lifespan\n\nThe Generator typically lives until it has consumed everything it was supposed to, such as reading parts of an\nindex, reading files, etc. Then it calls \"stop()\" on itself and its worker loop finishes.\n\nA 'Monitor' implements Generator. The semantic difference is that the Monitor does not finish unless explicitly\nstopped. It typically monitors an eternal stream of incoming data, for example tweets from Twitter or anything\nfrom a message queueing system such as RabbitMQ.\n\n### Additional useful members for implementing Generators and Monitors\n\n```text\nRead-only properties and variables:\n    accepting\n    stopping\n    running\n    suspended\n    aborted\n    end_tick_reason        # True if there is a reason to exit on_tick; either 'aborted', 'stopping' or\n                           #   not 'running'; but (obs!!) it does not consider 'suspended'\nVariables for you to update (if you like..):\n    total                  # typically total number of docs to generate (total DB entries, for example)\n    count                  # typically number of docs generated so far (e.g. to see progress towards total)\nMethods for you to implement:\n    on_startup()           # called at beginning of worker thread; on_open has already been called\n    on_shutdown()          # called when stopping, but before stopped; your chance to finish up\n    on_tick()              # called by worker thread; do all or parts of your stuff in here\n    on_suspend()\n    on_resume()\n    on_abort()             # see comment, below\n```\n\nI'll go through the typical of these event handlers one by one, including the on_open() and on_close() methods,\nin order of lifecycle chronological order.\n\n#### on_open()\n\nThis is called when the processor starts, but before the worker thread starts.\nConfig verification, existence of external resources, etc, could be verified here.\nBe aware that the processor should be able to start and stop and start again multiple times,\nso lingering TCP connections, locked files, your own performance counters and state variables,\netc, must be accounted for.\n\n#### on_startup()\n\nThis is called after the worker thread has started, but before we enter the run loop. This is\nanother place for initialization logic. What you do here might as well have been placed in on_open(),\nbut not vice versa. This is typically not the place to verify config variables or raise exceptions.\nDo that in on_open(). But this is a logical place to host the code that is the reverse of the\n\"shutdown\" code.\n\nThis method is called at the beginning of the worker thread. No on_tick or other generator events\nwill be called before it has completed. It is NOT GUARANTEED to have finished before\nconnectors start delivering documents. on_open, however, is always called before connectors are started.\"\n\n#### on_tick()\n\nThis is the tricky one...\n\nThe simplest way is to have setup and shutdown done outside the tick, and handle small pieces each time\nyou get a call to this method. Pretty much all the time, unless suspended. This way you will not have to handle\nthe 'suspended' status, either.\n\nIf you want to handle everything yourself, you need to check the 'suspended' status, and whether it is time to\nstop handling the tick, summarized in the boolean property 'end_tick_reason'. Here are three different examples\nof how this is handled:\n\nElasticsearchReader: This does both setup and cleanup from inside on_tick(). It checks for 'end_tick_reason'\nand sleeps while suspended.\n\nRabbitmqMonitor: Starts listening from on_startup(). It processes as much as it can get from the queue in on_tick()\nand also handles reconnect attempts there if necessary. Then it returns to the run loop in the generator.\nStop and abort events call the pika API and tells it to cancel further consumption so the on_tick loop does\nnot need to handle this. Suspension is also handled between the \"ticks\", but we need to cancel and restart\nconsumption between suspend/resume events.\n\nFileReader: The file reader can read from one or more files, one entire file or one line per file generating\na document (configurable behaviour). It relies on a revisit to the on_tick() method for each new file that\nneeds to be processed, and opens the new file and starts reading. It burns through files a line at a time if\nso configured, but it also checks whether there is an 'end_tick_reason' or a 'suspend'. In which case it\nreturns to the main run loop, only to be revisited later to pick up reading from where it was. Any potentially\nopen file (due to a premature stop() or abort()) is closed in on_close().\n\n#### on_suspend() / on_resume()\n\nIn case you want to do something special when suspend or resume has happened. Most often you would probably\njust watch the 'suspended' status in the on_tick() method instead.\n\n#### on_shutdown()\n\nWhen the generator receives a stop() command, it enters 'stopping' mode before it actually stops. This method\nis called when the generator is stopping. (If you handle the stopping status yourself inside the on_tick()\nmethod, then you do not have to handle it here.)\n\nAfter this method exits, the generator registeres that production has stopped from this worker thread.\nIs still 'stopping', however, until all connector queues finished processing and are empty. Only then is the\nprocessor truly considered to be finished, and the worker thread exits.\n\nThe next and final event call will now be to on_close().\n\n__NOTE:__\nIf your on_tick() method calls another blocking method to retrieve its data, e.g. from a Python generator\nloop, you might have to go in the back door and switch off the lights for that generator. You can do this by\noverriding the stop() method like in this (awful) example from TwitterMonitor:\n\n```python\ndef stop(self):\n    if self.stopping or not self.running:\n        return\n    if self._twitter_response:\n        self.log.info(\"Closing connection to Twitter stream.\")\n        self._twitter_response.response.raw._fp.close()  # *sigh*...\n\n    super(TwitterMonitor, self).stop()\n```\n\n#### on_abort()\n\nThis is called after all processing is supposed to have stopped, after leaving the on_tick() method. But the\nthread is still running, but will be terminated right after this method is called. Whether you clean up in\non_abort() or on_close() doesn't matter much most of the time. But it can be used to separate normal shutdown\nlogic from abortion logic and keep the common code in on_close().\n\n#### on_close()\n\nThis is called after the worker thread and all process requests from connectors have stopped.\nConsider this the final operation, but be aware that the processor can be restarted, so all must be\ncleaned up and ready to go again.\n\n### Ad-hoc Processor\n\nSay you have a processor \"p1\" and \"p2\" that pass a string (here tagged with protocol \"str\") from one to the other,\nand you want to reverse that string with a processor in the middle, but you don't want to bother with making\nanother class. This is how you do it:\n\n```python\np1 = ...\np2 = ...\n\ndef reverse(document):\n    socket = send(document[::-1]\n\nmiddle = Processor(name=\"ad-hoc\")\nmiddle.create_connector(reverse, \"input\", protocol=\"str\")\nsocket = middle.create_socket(\"output\", protocol=\"str\")\n\np1.attach(middle.attach(p2))\np1.start()\np2.wait()\n```\n\n# SERVICE DOCUMENTATION\n\n## The idea\n\nThe idea behind \"services\" is to easily administer a number of document processing process (\"services\").\nThese services can exchange data between them through messaging queues or other means.\n\nThe eslib service framework offers service configuration through a config file, logging and log\nconfiguration, metadata handling and command line commands for managing the services.\n\n## TODO: Usage\n\n### TODO: Configuration and directory structure\n\n```\nservice\n\u251c\u2500\u2500 bin/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 init.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 launcher\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 manager\n\u251c\u2500\u2500 config/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 credentials.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 logging-console.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 logging.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 services.yaml\n\u2514\u2500\u2500 log/\n\u2514\u2500\u2500 source/\n```\n\n### TODO: Running services\n\n* es-service\n* es-run\n* es-meta\n\n## TODO: The manager\n\n* storage\n\n## Metadata\n\nMetadata is the data that the services need for runtime (re)configuration. It is not the static\nservice configuration that describes how the service should be set up and ran, but rather the\ndata that is expected to change while the services are running. Multiple services will share the\nsame metadata.\n\nMetadata exists in committed \"change sets\". The manager keeps one change set active, and another\nfor editing. Once a change set is committed it becomes active, and all dependent services are\nupdated with the new set. A new change set then becomes the current one for editing.\n\nIt is possible to roll back to a previous change set. It will then become the active set, and all\ndependent services will be updated.\n\n### Subscription and Exchange\n\nServices that subscribe to metadata do so by enlisting the metadata keys as a list of strings with\ndot notation that addresses the metadata sections. These are registered with the manager when the\nservice communicates the initial 'hello' message, as 'meta_keys'. The metadata requested is \ninitially returned as part of the 'hello' message response, under 'metadata'. After that it is updated\nby a call from the manager (or other..) via the service's 'metadata' interface.\n\nExample subscription registration in 'hello' message:\n\n```json\n\"meta_keys\": [\"twitter.terms\"]\n```\n\nExample payload in 'hello':\n\n```json\n\"metadata\": {\n    \"version\": \"12345\",\n    \"data\": {\n        \"twitter\": {\n            \"terms\": [\"watch\", \"these\", \"terms\"]\n        }\n    }\n}\n```\n\nA service that is dependent on metadata cannot start until it receives the data. If it is missing for\nsome reason, then the service should be in the 'pending' state.\n\n### Metadata addressing\n\nMetadata is addressed using dot notation to find the node. When the node contains lists of data we\ncan select a subset of it by listing constraints after the path. The constraint separator is the\npipe (|) character. E.g.\n\n```\nmytuff.users|domain:twitter\n```\n\n\n## TODO: Manager REST API\n\n\n### TODO: Service operations\n\n\n## TODO: Service/manager communication (REST interface)\n\n### Metadata exchange\n\nYou should not normally have to bother with any of this yourself, but for the record...\n\nWhen a (HTTP) service is started, it sends a `/hello` message to the manager, containing (among other things)\nthe list of `meta_keys` to subscribe to. The server responds with a section (among other things)\ncalled `metadata`, where the requested metadata lie.\n\nWhen the manager has an update that the service subscribes to, it will send that via an HTTP POST\nmessage to the service's `/metadata` endpoint.\n\nYou can check what metadata a service holds by querying the same endpoint with a GET request, e.g.\n\n```\ncurl -XGET http://10.0.0.4:4027/metadata | jq .\n```\n\n\n## TODO: Writing services\n\n* big section...\n\n### Metadata integration\n\nWhen you inherit from `Service`, there is a class level attribute `metadata_keys`. This is a\nlist where you can specify the keys (paths with constraints) that you want your service to\nsubscribe to updates on. E.g.\n\n```\nmeadata_keys = [\"twitter.keywords\", \"users|domain:twitter\"]\n```\n\nThen when you override the method\n\n```\non_metadata(self, metadata)\n```\n\nyou will receive a message from the server when there is changes to your data.\nYou can then easily pick te parts you want with `dicthelp.get`, like this:\n\n```\nfrom eslib.service import dicthelp\n\ndef on_metadata(self, metadata):\n  kwds = dicthelp.get(metadata, \"twitter.keywords\")\n  ...\n```\n\n## Integrating with the service manager (from external applications)\n\n### Metadata management\n\nThe following HTTP commands are available\n\n```\nHTTP verb  Path                           URL query params\n---------- ------------------------------ ----------------\nGET        /meta/list\nPUT|POST   /meta/commit\nPUT|POST   /meta/rollback/{version:int}\nDELETE     /meta/drop{version:int}\nPUT|POST   /meta/import                   ?commit:bool, ?message:str\nGET        /meta/{?version}               ?path:str\nPUT|POST   /meta/put                      ?path:str, ?merge:bool\nDELETE     /meta/remove\nDELETE     /meta/delete\n```\n\nIn the list above, a question mark means 'optional' and a type is specified with ':type'.\n\n### HTTP command body format\n\n#### commit\n\n``json\n{\n  \"description\": message\n}\n```\n\n#### commit\n\n```json\n{\n  \"description\": message\n}\n```\n\n#### remove\n\n```json\n{\n  \"path\"    : message,\n  \"list\"    : list_of_terms\n}\n```\n\n#### delete\n\n```json\n{\n  \"paths\"  : list_of_paths,\n  \"collapse\": true|false\n}\n```\n\n### Examples\n\n#### Import\n\nImport overwrites the entire edit set. It must be committed afterwards\nto become active, unless the commit flag is set in the query options.\n\nIf you want to add or update parts of the edit set, use the 'put' command instead.\n\n```sh\nurl -XGET http://localhost:4000/meta/import?commit=false&message=\n```\n\n```json\n{\n    \"hello\":\n    {\n        \"target\": \"world\"\n    },\n    \"groups\":\n    [\n        {\n            \"id\": \"group_a\",\n            \"terms\": [1, 2, 3]\n        },\n        {\n            \"id\": \"group_b\",\n            \"terms\": [4, 5]\n        }\n    ],\n    \"users\":\n    [\n        {\n            \"domain\": \"twitter\",\n            \"username\": \"user_a\",\n            \"user_id\": \"a\",\n        },\n        {\n            \"domain\": \"twitter\",\n            \"username\": \"user_b\",\n            \"user_id\": \"b\",\n        },\n        {\n            \"domain\": \"facebook\",\n            \"username\": \"user_c\",\n            \"user_id\": \"c\",\n        }\n    ]\n}\n```\n\n### Commit\n\n```bs\ncurl -XPOST http://localhost:4000/meta/commit\n```\n\nOptional body:\n\n```json\n{\n    \"description\": \"Commiting edit to active set once again.\"\n}\n```\n\n#### Add or replace entire section\n\n```bs\ncurl -XPOST http://localhost:4000/meta/put\n```\n\nBody:\n\n```json\n{\n    \"groups\":\n    [\n        {\n            \"id\": \"group_a\",\n            \"terms\": [\"a\", \"b\"]\n        },\n        {\n            \"id\": \"group_b\",\n             \"terms\": [\"c\", \"d\", \"e\"]\n        }\n    ]\n}\n```\n\n#### Add or replace specific objects\n\n```bs\ncurl -XPOST http://localhost:4000/meta/put\n```\n\nBody:\n\n```json\n{\n    \"groups|id:group_a\":\n    {\n        \"id\": \"group_a\",\n         \"terms\": [\"a\", \"b\"]\n    },\n    \"groups|id:group_b\":\n    {\n        \"id\": \"group_b\",\n         \"terms\": [\"c\", \"d\", \"e\"]\n    }\n}\n```\n\n#### Add or replace (unique) elements in array\n\n```bs\ncurl -XPOST http://localhost:4000/meta/put?merge=true\n```\n\nBody:\n\n```json\n{\n    \"groups|id:group_a\",\n    \"terms\": [\"e\", \"f\"]\n}\n```\n\n#### Delete sections\n\n```bs\ncurl -XDELETE http://localhost:4000/meta/delete\n```\n\nBody:\n\n```json\n{\n    \"paths\"  : [\"groups\", \"users\"]\n}\n```\n\n#### Delete objects by filter\n\n```bs\ncurl -XDELETE http://localhost:4000/meta/delete\n```\n\nBody:\n\n```json\n{\n    \"paths\"  : [\"groups|id:group_a\", \"users|domain:twitter\"]\n}\n```\n\n#### Remove terms from array\n\nThis command is less useful than the others, in that it cannot address a field\nwithin filtered objects, e.g. you cannot first address \"groups|id:group_a\" and\nthen remove items from that object's \"terms\" array.\n\n```bs\ncurl -XDELETE http://localhost:4000/meta/remove\n```\n\nBody:\n\n```json\n{\n    \"path\": \"myarray\",\n    \"list\": [\"b\", \"c\"]\n}\n```\n", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/comperiosearch/elasticsearch-eslib", "keywords": "document processing docproc", "license": "Apache 2.0", "maintainer": null, "maintainer_email": null, "name": "eslib", "package_url": "https://pypi.org/project/eslib/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/eslib/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/comperiosearch/elasticsearch-eslib"}, "release_url": "https://pypi.org/project/eslib/0.0.14/", "requires_dist": null, "requires_python": null, "summary": "Document processing framework and utility for Elasticsearch (or whatever).", "version": "0.0.14", "yanked": false, "html_description": "<div class=\"project-description\">\n            # elasticsearch-eslib<br><br>2015.11.14 -- Hans Terje Bakke<br><br>Python library for document processing for Elasticsearch.<br><br>While Elasticsearch is what we originally created it for, it is fully usable for anything else as well.<br>The only limitation is that a lot of the processing stages are using an Elasticsearch-like document format.<br>But this can easily be converted to something else. For example, a SolrWriter could take an \"esdoc\" as input<br>and write it to Solr. The \"esdoc\" in this case is simply a JSON compatible Python dict type with the following<br>meta fields, that may even be omitted in many cases, and that you can make whatever you want of:<br><br>```json<br>{<br>    \"_index\"  : \"(some kind of document cluster, like DB table, search index, etc.)\",<br>    \"_type\"   : \"(document type in some data store)\",<br>    \"_id\"     : \"(document id)\",<br>    \"_source\" : {<br>        (document fields and data go here...)<br>    }<br>}<br>```<br><br>## Introduction<br><br>A 'processor' processes incoming data and generates output.<br><br>It can also generate its own data or fetch from external data sources and services.<br>Instead, or in addition to, writing output to its own 'sockets', it can also write data<br>to external data sources and services. In these cases it is commonly referred to as a 'generator',<br>and has its own execution thread.<br><br>A processor has one or more input 'connectors' that can connect to one more output 'sockets'.<br>Connectors and sockets (commonly called 'terminals') are registered with an optional 'protocol' tag.<br>If it exists, an attempted connection will check if the data protocol is the same in both connector<br>and socket.<br><br>A processor B is said to 'subscribe' to output from processor A if it has a connector connected a<br>socket on A. In this case, A has the role of 'producer' (to B) and B has the role of 'subscriber' (to A).<br><br>## Installation<br><br>### Prerequisites<br><br>This library requires python 2.7.<br><br>BeautifulSoup, used in some of the document processing stages, requires libxml2 and libxslt.<br>On Fedora (Amazon Linux), this can be installed using:<br><br>```<br>yum install libxml2-devel libxslt-devel<br>```<br><br>### Install eslib with pip<br>```<br>pip install eslib<br>```<br><br>## Usage<br><br>From a Python script, we can create a processing graph as in this example:<br><br>```python<br>a = ElasticsearchReader()<br>b = ElasticsearchWriter()<br>a.config.index = \"employees\"<br>b.config.index = \"employees_copy\"<br>b.subscribe(a)<br>```<br>    <br>and execute it with<br><br>```python<br>a.start()<br>```<br><br>In this simple example, the first processor is a generator, and the entire pipeline will finish when 'a'<br>completes. The simple \"b.subscribe(a)\" is possible because there is only one connector in 'b' and only<br>one socket in 'a'. Otherwise, we would have to specify the connector and socket names.<br><br>By default, a processor that is stopped either explicitly from outside, or completes generating data (as<br>in this example), will send a stop signal to its subscribers. This is not always a desirable behaviour.<br>Say we had 20 readers sending data to 1 writer. We would not like the writer to stop when the first reader<br>completes. To avoid this, we can use<br><br>```python<br>...<br>b.keepalive = True<br>a.start()<br>time.sleep(10)  # The reader is still working in its own thread<br>b.put(mydoc)    # Writes to the only connector (\"input\", of protocol \"esdoc\")<br>a.wait()        # Wait for a to complete/stop<br>b.stop()        # ... then explicitly stop b<br>```<br><br>One processor/connector can subscribe to data from many processors/sockets. One processor can have many<br>different named connectors, expecting data in various formats (hinted by its 'protocol' tag.) And a processor/socket<br>can have many processors/connectors subscribing to data it outputs.<br><br>### Behind the scene<br><br>Technically, a processor sends document data to its sockets. The sockets send documents to its connected connectors.<br>A connector has a queue of incoming items, and a thread that pulls documents off the queue and sends it to<br>a processing method in the processor class. This method processes the data and sends the result to one or<br>more of its sockets, which again send to connected connectors...<br><br>A generator style processor has another thread that generates documents somehow, and sends it to its socket(s).<br><br>### Listen to output<br><br>Analogous with the processor.put(doc) command, you might also want to listen to output from a processor in your<br>program. You can do this by adding a 'callback' for the socket. For example like this<br><br>```python<br>output = []<br>processor.add_callback(lambda processor, doc: output.append(doc), socket_name)<br>...<br>processor.start()<br>processor.wait()<br>print output<br>```<br><br>or instead of the lambda function, use a method that takes a processor and document as an argument, e.g.:<br><br>```python<br>def do_stuff(processor, document):<br>    print \"I spy with my little eye, a document containing:\", document<br><br>...<br>processor.add_callback(do_stuff)<br>...<br>```<br><br>### Events<br><br>You can register methods to listen to events. The following events can be subscribed to:<br><br>```python<br>event_started<br>event_stopped<br>event_aborted<br>```<br>    <br>You register a method that takes no a processor as arguments like so<br><br>```python<br>def myfunc(processor)<br>    print \"Processor '%s' started\" % processor.name<br>    <br>myproc.event_started.append(myfunc)<br>```<br><br>### Protocol compliance<br><br>When sockets and connector are joined (\"connected\"), there is a check for protocol compliance. These are string<br>tags using a dot-notation for specializations. A terminal is registered with 'any' if it doesn't care about the<br>protocol. Explanation by some examples:<br><br>```text<br>SOCKET PROTOCOL          CONNECTOR PROTOCOL        WILL MATCH?<br>---------------          ------------------        -----------<br>seating.chair            seating.chair             yes (of course)<br>seating.chair.armchair   seating                   yes, connector accepts any seating<br>seating                  seating.chair.armchair    no, connector expects armchairs, specifically<br>any                      string                    yes (but, consumer: beware! it might be anything!)<br>string                   any                       yes, we accept anything<br>```<br><br>### Members for using the Processor (and derivates)<br><br>```text<br>Read/write:<br>    keepalive<br>Read only:<br>    accepting<br>    stopping<br>    running<br>    suspended<br>    aborted<br>Event lists:<br>    event_started<br>    event_stopped<br>    event_aborted<br>Methods:<br>    __init__(name) # Constructor/init<br>    subscribe(producer=None, socket_name=None, connector_name=None)<br>    unsubscribe(producer=None, socket_name=None, connector_name=None)<br>    attach(subscriber, socket_name=None, connector_name=None)<br>    detach(subscriber, socket_name=None, connector_name=None)<br>    connector_info(*args)  # returns list<br>    socket_info(*args)     # returns list<br>    start()<br>    stop()<br>    abort()<br>    suspend()<br>    resume()<br>    wait()<br>    put(document, connector_name=None)<br>    add_callback(method, socket_name=None)<br>Methods for debugging:<br>    DUMP<br>    DUMP_connectors<br>    DUMP_sockets<br>```<br><br>## Writing your own Processor<br><br>The simple processor (not Generator type) typically has one or more connectors. A connector receives data from<br>a socket, or from a \"processor.put(document, connector_name)\" statement (which in turn puts the data on the<br>connector queue). Internally, a connector has a queue and is running a its own thread that pulls items off the<br>queue and executes whatever method is registered with the connector.<br><br>Any object passed to such a method is considered to be read-only. You *may* alter it, preferably only add to it.<br>But it is generally a bad idea, since many processors could potentially receive the same object. If you want to<br>pass it on to a socket as-is, that's fine. And it is the best performance wise. But if you need to alter it,<br>you should consider creating a deep or shallow clone. Shallow clones are fine if you just want to change one<br>part of the object and refer to the rest as it is.<br><br>As a general rule of thumb you should never alter the state members yourself directly. If you want to have<br>the processor stop or abort itself, you should call \"self.stop()\" or \"self.abort()\".<br><br>### A simple processor<br><br>Let's start with a simple processor that receives input on a connector and writes its processed output to a socket.<br>Let's make a processor that reverses and optionally swaps the casing of a string.<br><br>```python<br>from eslib import Processor<br><br>class StringReverser(Processor):<br><br>    def __init__(self, **kwargs):<br>        super(StringReverser, self).__init__(**kwargs)<br>        <br>        self.config.set_default(swapcase = False)<br>```<br>        <br>Notice **kwargs, the \"swapcase\" config variable and the already instantiated config class. It is meant for<br>containing config variables. The incoming keyword arguments are put into the config as attributes. We *must* set<br>default values for all the config variables we want to use inside the processor, as this is also a way to define<br>their existence, should they not come in as keyword arguments. Otherwise, we risk getting exceptions when try to<br>use them.<br><br>In this case we will set \"swapcase\" if we want to swap the casing of the string we are reversing.<br><br>We also create a connector for the input and two sockets for the output. One is a pure pass-through while the<br>other provides the modified output:<br><br>```python<br>        self.create_connector(self._incoming, \"input\", \"str\")<br>        self.create_socket(\"original\", \"str\")<br>        self.create_socket(\"modified\", \"str\")<br>```<br><br>We use \"str\" as a protocol tag. This is not the same as a Python type; it is simply a hint. When connecting<br>sockets and connectors there is check for protocol compliance. If you want to expect or spew out anything, simply<br>specify None or omit the protocol specification.<br><br>The following member methods are called when the processor starts or stops (including getting aborted),<br>respectively:<br><br>```python<br>    def on_open(self):  pass  # Before threads are spun up.<br>    def on_abort(self): pass  # After call to abort(), but before closing with on_close()<br>    def on_close(self): pass  # Final call after stopping/aborting processor; always called<br>```<br><br>This is typically used for opening and closing files, establishing remote connections, resetting counters, etc.<br><br>For this example, there is nothing special we want to do when starting and stopping. (Really, starting the<br>processor in this case simply spins up the connector, that will deliver documents to our \"_incoming(document)\"<br>method as soon as it can. So now on to this method:<br><br>```python<br>    def _incoming(self, document):<br>        # TODO: You might want to check if the document is indeed a 'str' or 'unicode' here...<br>        <br>        s = document[::-1]  # Reverse the string of characters<br>        if self.config.swapcase:<br>            s = s.swapcase()<br>        <br>        # Write to the sockets:<br>        self.sockets[\"original\"].send(document)  # Incoming document, unmodified<br>        self.sockets[\"modified\"].send(s)<br>```<br><br>Often, processing can be quite heavy stuff, and quite unnecessary to do a lot of work with producing output if<br>there are no consumers. Therefore, you might want to first check if there is actually any consumers expecting<br>output either for the entire processor or per socket, with<br><br>```python<br>        if not self.has_output:<br>            return<br>            <br>        # or<br>        <br>        if self.sockets[\"modified\"].has_output:<br>            # calculate and send the stuff...<br>```<br><br>### Useful members for implementing simple Processors<br><br>```text<br>    Methods for you to implement:<br>        __init__(name=None)    # constructor; remember to call super(your_class, self).__init__(name)<br>        on_open()              # called before starting execution threads<br>        on_abort()             # called after a processor receives a call to abort(), but before on_close()<br>        on_close()             # called when the processor has stopped or aborted<br>    Read-only properties and variables:<br>        name                   # name of the processor<br>        config                 # object containing  all configuration data for the processor<br>        connectors             # dict<br>        sockets                # dict<br>        has_output             # bool; indicating whether there are sockets with connections<br>        log                    # logger; log processor events here<br>        doclog                 # logger; log problems with documents here<br>    Methods to call:<br>        create_connector(method, name=None, protocol=None, description=None, is_default=False)<br>        create_socket(name=None, description=None, is_default=False, mimic=None)<br>        stop()                 # call this if you want to explicitly stop prematurely<br>        abort()                # call this if you want to explicitly abort prematurely<br>    Properties and methods on sockets:<br>        socket.has_output      # bool; indicating whether the socket has connections (subscribers)<br>        socket.send(document)  # sends document to connected subscribers for asynchronous processing<br>```<br><br><br>### Default terminal<br><br>A connector can be set as default using the 'is_default' parameter or setting the 'default_connector' and<br>'default_socket'. There can only be one default connector and one default socket. This makes it possible<br>to address a terminal without name, which will then be routed to the default terminal. If only one socket<br>or connector exists, then that one also becomes the defacto default within its collection.<br><br>### Protocol \"mimicing\"<br><br>A processor can have a socket \"mimic\" protocol of connected connector. This is useful in a processing chain<br>A-B-C, where B massages the output from A and passes it on to C. A and C needs to have the same protocols on<br>their terminals, while B can be more general. Example:<br><br>```text<br>    TwitterMonitor socket=tweet(esdoc.tweet)<br>        =&gt; HtmlRemover connector=input(esdoc) | socket=output(esdoc) mimic=input<br>            =&gt; TweetExtractor connector=tweet(esdoc.tweet)<br>```<br><br>Normal protocol compliance would dictate that TweetExtractor connector would require the more specific<br>'esdoc.tweet' protocol, while the HtmlRemover only outputs 'esdoc'. However, HtmlRemover has its output<br>socket set to 'mimic' its input connector. When we then attach the TwitterMonitor socket with protocol<br>'esdoc.tweet', the HtmlRemover's socket will mimic that protocol, and the TweetExtractor will accept it.<br><br>### Processor lifespan<br><br>A simple processor typically sits in between other processors in a pipeline or graph. They are started by<br>another processor earlier in the chain, and they are instructed to stop when a processor they are subscribing to<br>stops. So a processor C subscribing to A or B will stop if one of the other stops. Unless it is flagged with<br>\"keepalive = True\".<br><br>When stopping, the processor closes all its connectors for further input (by setting \"connector.accept = False\").<br>The connectors will then continue to work off their queues until empty, and then the processor are finally<br>stopped. For immediate termination without processing whatever is still queued up, an \"abort()\" must be called.<br><br>### Generators and Monitors<br><br>A 'Generator' is a Processor that is expected to produce its output mainly from other sources than what is coming<br>in through connectors. It has its own execution thread. For example, a FileWriter is a simple processor that<br>writes whatever it gets on its connector to a file. A FileReader has its own worker thread that reads lines from<br>files and generates documents as its output, and therefor implements Generator. An ElasticsearchWriter is a<br>Generator because it needs its own worker thread to gather up incoming documents and send them in batches to the<br>server for higher performance. So although they have the file and Elasticsearch writers have similar purposes,<br>they have different implementation schemes.<br><br>### Generator and Monitor lifespan<br><br>The Generator typically lives until it has consumed everything it was supposed to, such as reading parts of an<br>index, reading files, etc. Then it calls \"stop()\" on itself and its worker loop finishes.<br><br>A 'Monitor' implements Generator. The semantic difference is that the Monitor does not finish unless explicitly<br>stopped. It typically monitors an eternal stream of incoming data, for example tweets from Twitter or anything<br>from a message queueing system such as RabbitMQ.<br><br>### Additional useful members for implementing Generators and Monitors<br><br>```text<br>Read-only properties and variables:<br>    accepting<br>    stopping<br>    running<br>    suspended<br>    aborted<br>    end_tick_reason        # True if there is a reason to exit on_tick; either 'aborted', 'stopping' or<br>                           #   not 'running'; but (obs!!) it does not consider 'suspended'<br>Variables for you to update (if you like..):<br>    total                  # typically total number of docs to generate (total DB entries, for example)<br>    count                  # typically number of docs generated so far (e.g. to see progress towards total)<br>Methods for you to implement:<br>    on_startup()           # called at beginning of worker thread; on_open has already been called<br>    on_shutdown()          # called when stopping, but before stopped; your chance to finish up<br>    on_tick()              # called by worker thread; do all or parts of your stuff in here<br>    on_suspend()<br>    on_resume()<br>    on_abort()             # see comment, below<br>```<br><br>I'll go through the typical of these event handlers one by one, including the on_open() and on_close() methods,<br>in order of lifecycle chronological order.<br><br>#### on_open()<br><br>This is called when the processor starts, but before the worker thread starts.<br>Config verification, existence of external resources, etc, could be verified here.<br>Be aware that the processor should be able to start and stop and start again multiple times,<br>so lingering TCP connections, locked files, your own performance counters and state variables,<br>etc, must be accounted for.<br><br>#### on_startup()<br><br>This is called after the worker thread has started, but before we enter the run loop. This is<br>another place for initialization logic. What you do here might as well have been placed in on_open(),<br>but not vice versa. This is typically not the place to verify config variables or raise exceptions.<br>Do that in on_open(). But this is a logical place to host the code that is the reverse of the<br>\"shutdown\" code.<br><br>This method is called at the beginning of the worker thread. No on_tick or other generator events<br>will be called before it has completed. It is NOT GUARANTEED to have finished before<br>connectors start delivering documents. on_open, however, is always called before connectors are started.\"<br><br>#### on_tick()<br><br>This is the tricky one...<br><br>The simplest way is to have setup and shutdown done outside the tick, and handle small pieces each time<br>you get a call to this method. Pretty much all the time, unless suspended. This way you will not have to handle<br>the 'suspended' status, either.<br><br>If you want to handle everything yourself, you need to check the 'suspended' status, and whether it is time to<br>stop handling the tick, summarized in the boolean property 'end_tick_reason'. Here are three different examples<br>of how this is handled:<br><br>ElasticsearchReader: This does both setup and cleanup from inside on_tick(). It checks for 'end_tick_reason'<br>and sleeps while suspended.<br><br>RabbitmqMonitor: Starts listening from on_startup(). It processes as much as it can get from the queue in on_tick()<br>and also handles reconnect attempts there if necessary. Then it returns to the run loop in the generator.<br>Stop and abort events call the pika API and tells it to cancel further consumption so the on_tick loop does<br>not need to handle this. Suspension is also handled between the \"ticks\", but we need to cancel and restart<br>consumption between suspend/resume events.<br><br>FileReader: The file reader can read from one or more files, one entire file or one line per file generating<br>a document (configurable behaviour). It relies on a revisit to the on_tick() method for each new file that<br>needs to be processed, and opens the new file and starts reading. It burns through files a line at a time if<br>so configured, but it also checks whether there is an 'end_tick_reason' or a 'suspend'. In which case it<br>returns to the main run loop, only to be revisited later to pick up reading from where it was. Any potentially<br>open file (due to a premature stop() or abort()) is closed in on_close().<br><br>#### on_suspend() / on_resume()<br><br>In case you want to do something special when suspend or resume has happened. Most often you would probably<br>just watch the 'suspended' status in the on_tick() method instead.<br><br>#### on_shutdown()<br><br>When the generator receives a stop() command, it enters 'stopping' mode before it actually stops. This method<br>is called when the generator is stopping. (If you handle the stopping status yourself inside the on_tick()<br>method, then you do not have to handle it here.)<br><br>After this method exits, the generator registeres that production has stopped from this worker thread.<br>Is still 'stopping', however, until all connector queues finished processing and are empty. Only then is the<br>processor truly considered to be finished, and the worker thread exits.<br><br>The next and final event call will now be to on_close().<br><br>__NOTE:__<br>If your on_tick() method calls another blocking method to retrieve its data, e.g. from a Python generator<br>loop, you might have to go in the back door and switch off the lights for that generator. You can do this by<br>overriding the stop() method like in this (awful) example from TwitterMonitor:<br><br>```python<br>def stop(self):<br>    if self.stopping or not self.running:<br>        return<br>    if self._twitter_response:<br>        self.log.info(\"Closing connection to Twitter stream.\")<br>        self._twitter_response.response.raw._fp.close()  # *sigh*...<br><br>    super(TwitterMonitor, self).stop()<br>```<br><br>#### on_abort()<br><br>This is called after all processing is supposed to have stopped, after leaving the on_tick() method. But the<br>thread is still running, but will be terminated right after this method is called. Whether you clean up in<br>on_abort() or on_close() doesn't matter much most of the time. But it can be used to separate normal shutdown<br>logic from abortion logic and keep the common code in on_close().<br><br>#### on_close()<br><br>This is called after the worker thread and all process requests from connectors have stopped.<br>Consider this the final operation, but be aware that the processor can be restarted, so all must be<br>cleaned up and ready to go again.<br><br>### Ad-hoc Processor<br><br>Say you have a processor \"p1\" and \"p2\" that pass a string (here tagged with protocol \"str\") from one to the other,<br>and you want to reverse that string with a processor in the middle, but you don't want to bother with making<br>another class. This is how you do it:<br><br>```python<br>p1 = ...<br>p2 = ...<br><br>def reverse(document):<br>    socket = send(document[::-1]<br><br>middle = Processor(name=\"ad-hoc\")<br>middle.create_connector(reverse, \"input\", protocol=\"str\")<br>socket = middle.create_socket(\"output\", protocol=\"str\")<br><br>p1.attach(middle.attach(p2))<br>p1.start()<br>p2.wait()<br>```<br><br># SERVICE DOCUMENTATION<br><br>## The idea<br><br>The idea behind \"services\" is to easily administer a number of document processing process (\"services\").<br>These services can exchange data between them through messaging queues or other means.<br><br>The eslib service framework offers service configuration through a config file, logging and log<br>configuration, metadata handling and command line commands for managing the services.<br><br>## TODO: Usage<br><br>### TODO: Configuration and directory structure<br><br>```<br>service<br>\u251c\u2500\u2500 bin/<br>\u2502\u00a0\u00a0 \u251c\u2500\u2500 init.sh<br>\u2502\u00a0\u00a0 \u251c\u2500\u2500 launcher<br>\u2502\u00a0\u00a0 \u2514\u2500\u2500 manager<br>\u251c\u2500\u2500 config/<br>\u2502\u00a0\u00a0 \u251c\u2500\u2500 credentials.yaml<br>\u2502\u00a0\u00a0 \u251c\u2500\u2500 logging-console.yaml<br>\u2502\u00a0\u00a0 \u251c\u2500\u2500 logging.yaml<br>\u2502\u00a0\u00a0 \u2514\u2500\u2500 services.yaml<br>\u2514\u2500\u2500 log/<br>\u2514\u2500\u2500 source/<br>```<br><br>### TODO: Running services<br><br>* es-service<br>* es-run<br>* es-meta<br><br>## TODO: The manager<br><br>* storage<br><br>## Metadata<br><br>Metadata is the data that the services need for runtime (re)configuration. It is not the static<br>service configuration that describes how the service should be set up and ran, but rather the<br>data that is expected to change while the services are running. Multiple services will share the<br>same metadata.<br><br>Metadata exists in committed \"change sets\". The manager keeps one change set active, and another<br>for editing. Once a change set is committed it becomes active, and all dependent services are<br>updated with the new set. A new change set then becomes the current one for editing.<br><br>It is possible to roll back to a previous change set. It will then become the active set, and all<br>dependent services will be updated.<br><br>### Subscription and Exchange<br><br>Services that subscribe to metadata do so by enlisting the metadata keys as a list of strings with<br>dot notation that addresses the metadata sections. These are registered with the manager when the<br>service communicates the initial 'hello' message, as 'meta_keys'. The metadata requested is <br>initially returned as part of the 'hello' message response, under 'metadata'. After that it is updated<br>by a call from the manager (or other..) via the service's 'metadata' interface.<br><br>Example subscription registration in 'hello' message:<br><br>```json<br>\"meta_keys\": [\"twitter.terms\"]<br>```<br><br>Example payload in 'hello':<br><br>```json<br>\"metadata\": {<br>    \"version\": \"12345\",<br>    \"data\": {<br>        \"twitter\": {<br>            \"terms\": [\"watch\", \"these\", \"terms\"]<br>        }<br>    }<br>}<br>```<br><br>A service that is dependent on metadata cannot start until it receives the data. If it is missing for<br>some reason, then the service should be in the 'pending' state.<br><br>### Metadata addressing<br><br>Metadata is addressed using dot notation to find the node. When the node contains lists of data we<br>can select a subset of it by listing constraints after the path. The constraint separator is the<br>pipe (|) character. E.g.<br><br>```<br>mytuff.users|domain:twitter<br>```<br><br><br>## TODO: Manager REST API<br><br><br>### TODO: Service operations<br><br><br>## TODO: Service/manager communication (REST interface)<br><br>### Metadata exchange<br><br>You should not normally have to bother with any of this yourself, but for the record...<br><br>When a (HTTP) service is started, it sends a `/hello` message to the manager, containing (among other things)<br>the list of `meta_keys` to subscribe to. The server responds with a section (among other things)<br>called `metadata`, where the requested metadata lie.<br><br>When the manager has an update that the service subscribes to, it will send that via an HTTP POST<br>message to the service's `/metadata` endpoint.<br><br>You can check what metadata a service holds by querying the same endpoint with a GET request, e.g.<br><br>```<br>curl -XGET http://10.0.0.4:4027/metadata | jq .<br>```<br><br><br>## TODO: Writing services<br><br>* big section...<br><br>### Metadata integration<br><br>When you inherit from `Service`, there is a class level attribute `metadata_keys`. This is a<br>list where you can specify the keys (paths with constraints) that you want your service to<br>subscribe to updates on. E.g.<br><br>```<br>meadata_keys = [\"twitter.keywords\", \"users|domain:twitter\"]<br>```<br><br>Then when you override the method<br><br>```<br>on_metadata(self, metadata)<br>```<br><br>you will receive a message from the server when there is changes to your data.<br>You can then easily pick te parts you want with `dicthelp.get`, like this:<br><br>```<br>from eslib.service import dicthelp<br><br>def on_metadata(self, metadata):<br>  kwds = dicthelp.get(metadata, \"twitter.keywords\")<br>  ...<br>```<br><br>## Integrating with the service manager (from external applications)<br><br>### Metadata management<br><br>The following HTTP commands are available<br><br>```<br>HTTP verb  Path                           URL query params<br>---------- ------------------------------ ----------------<br>GET        /meta/list<br>PUT|POST   /meta/commit<br>PUT|POST   /meta/rollback/{version:int}<br>DELETE     /meta/drop{version:int}<br>PUT|POST   /meta/import                   ?commit:bool, ?message:str<br>GET        /meta/{?version}               ?path:str<br>PUT|POST   /meta/put                      ?path:str, ?merge:bool<br>DELETE     /meta/remove<br>DELETE     /meta/delete<br>```<br><br>In the list above, a question mark means 'optional' and a type is specified with ':type'.<br><br>### HTTP command body format<br><br>#### commit<br><br>``json<br>{<br>  \"description\": message<br>}<br>```<br><br>#### commit<br><br>```json<br>{<br>  \"description\": message<br>}<br>```<br><br>#### remove<br><br>```json<br>{<br>  \"path\"    : message,<br>  \"list\"    : list_of_terms<br>}<br>```<br><br>#### delete<br><br>```json<br>{<br>  \"paths\"  : list_of_paths,<br>  \"collapse\": true|false<br>}<br>```<br><br>### Examples<br><br>#### Import<br><br>Import overwrites the entire edit set. It must be committed afterwards<br>to become active, unless the commit flag is set in the query options.<br><br>If you want to add or update parts of the edit set, use the 'put' command instead.<br><br>```sh<br>url -XGET http://localhost:4000/meta/import?commit=false&amp;message=<br>```<br><br>```json<br>{<br>    \"hello\":<br>    {<br>        \"target\": \"world\"<br>    },<br>    \"groups\":<br>    [<br>        {<br>            \"id\": \"group_a\",<br>            \"terms\": [1, 2, 3]<br>        },<br>        {<br>            \"id\": \"group_b\",<br>            \"terms\": [4, 5]<br>        }<br>    ],<br>    \"users\":<br>    [<br>        {<br>            \"domain\": \"twitter\",<br>            \"username\": \"user_a\",<br>            \"user_id\": \"a\",<br>        },<br>        {<br>            \"domain\": \"twitter\",<br>            \"username\": \"user_b\",<br>            \"user_id\": \"b\",<br>        },<br>        {<br>            \"domain\": \"facebook\",<br>            \"username\": \"user_c\",<br>            \"user_id\": \"c\",<br>        }<br>    ]<br>}<br>```<br><br>### Commit<br><br>```bs<br>curl -XPOST http://localhost:4000/meta/commit<br>```<br><br>Optional body:<br><br>```json<br>{<br>    \"description\": \"Commiting edit to active set once again.\"<br>}<br>```<br><br>#### Add or replace entire section<br><br>```bs<br>curl -XPOST http://localhost:4000/meta/put<br>```<br><br>Body:<br><br>```json<br>{<br>    \"groups\":<br>    [<br>        {<br>            \"id\": \"group_a\",<br>            \"terms\": [\"a\", \"b\"]<br>        },<br>        {<br>            \"id\": \"group_b\",<br>             \"terms\": [\"c\", \"d\", \"e\"]<br>        }<br>    ]<br>}<br>```<br><br>#### Add or replace specific objects<br><br>```bs<br>curl -XPOST http://localhost:4000/meta/put<br>```<br><br>Body:<br><br>```json<br>{<br>    \"groups|id:group_a\":<br>    {<br>        \"id\": \"group_a\",<br>         \"terms\": [\"a\", \"b\"]<br>    },<br>    \"groups|id:group_b\":<br>    {<br>        \"id\": \"group_b\",<br>         \"terms\": [\"c\", \"d\", \"e\"]<br>    }<br>}<br>```<br><br>#### Add or replace (unique) elements in array<br><br>```bs<br>curl -XPOST http://localhost:4000/meta/put?merge=true<br>```<br><br>Body:<br><br>```json<br>{<br>    \"groups|id:group_a\",<br>    \"terms\": [\"e\", \"f\"]<br>}<br>```<br><br>#### Delete sections<br><br>```bs<br>curl -XDELETE http://localhost:4000/meta/delete<br>```<br><br>Body:<br><br>```json<br>{<br>    \"paths\"  : [\"groups\", \"users\"]<br>}<br>```<br><br>#### Delete objects by filter<br><br>```bs<br>curl -XDELETE http://localhost:4000/meta/delete<br>```<br><br>Body:<br><br>```json<br>{<br>    \"paths\"  : [\"groups|id:group_a\", \"users|domain:twitter\"]<br>}<br>```<br><br>#### Remove terms from array<br><br>This command is less useful than the others, in that it cannot address a field<br>within filtered objects, e.g. you cannot first address \"groups|id:group_a\" and<br>then remove items from that object's \"terms\" array.<br><br>```bs<br>curl -XDELETE http://localhost:4000/meta/remove<br>```<br><br>Body:<br><br>```json<br>{<br>    \"path\": \"myarray\",<br>    \"list\": [\"b\", \"c\"]<br>}<br>```<br>\n          </div>"}, "last_serial": 2259801, "releases": {"0.0.10": [{"comment_text": "", "digests": {"md5": "a69845ed23585f629ff2111e03017a08", "sha256": "ef5bfe217f769c2f6745b01ee74d30c185b0c048f3f9fdfb9f43cd9a44672910"}, "downloads": -1, "filename": "eslib-0.0.10.tar.gz", "has_sig": false, "md5_digest": "a69845ed23585f629ff2111e03017a08", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 135817, "upload_time": "2016-02-11T14:23:26", "upload_time_iso_8601": "2016-02-11T14:23:26.221745Z", "url": "https://files.pythonhosted.org/packages/a6/0e/fbc9b946b6ac2e1d85377c3d04b8cb7a6cead9d1c39f83d9b6e61bd0325b/eslib-0.0.10.tar.gz", "yanked": false}], "0.0.11": [{"comment_text": "", "digests": {"md5": "7277b7556a506cb8d71a032eb6b9859c", "sha256": "d8dcb08ba04e5a09a63915cd84aa12633cc985c46c686d5c7392547a4bdd7a62"}, "downloads": -1, "filename": "eslib-0.0.11.tar.gz", "has_sig": false, "md5_digest": "7277b7556a506cb8d71a032eb6b9859c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 138917, "upload_time": "2016-03-21T14:53:50", "upload_time_iso_8601": "2016-03-21T14:53:50.341168Z", "url": "https://files.pythonhosted.org/packages/ea/8f/0960c54f9668cbc3b96e122e5a875904ed316afb9e797cf2be56472d4804/eslib-0.0.11.tar.gz", "yanked": false}], "0.0.12": [{"comment_text": "", "digests": {"md5": "939b768af217b5acc4e68795402c2ce1", "sha256": "bc5fc5d5af6df1e5c513b92d0995d0417c83989ae4137296b94e35c353776118"}, "downloads": -1, "filename": "eslib-0.0.12.tar.gz", "has_sig": false, "md5_digest": "939b768af217b5acc4e68795402c2ce1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 138955, "upload_time": "2016-03-21T19:34:22", "upload_time_iso_8601": "2016-03-21T19:34:22.857340Z", "url": "https://files.pythonhosted.org/packages/8e/82/c364ca27a59b39260eead224242597773daf470304df8b3d7331cdc51bb8/eslib-0.0.12.tar.gz", "yanked": false}], "0.0.13": [{"comment_text": "", "digests": {"md5": "685508f616a154c87c2335878fee5844", "sha256": "ae648ffa14de4bf5e4a6e9ea967c2b04713139f051d6a37e7d7182bd7a50faac"}, "downloads": -1, "filename": "eslib-0.0.13.tar.gz", "has_sig": false, "md5_digest": "685508f616a154c87c2335878fee5844", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 138966, "upload_time": "2016-03-21T19:48:19", "upload_time_iso_8601": "2016-03-21T19:48:19.113322Z", "url": "https://files.pythonhosted.org/packages/0a/41/c1fe3a27ca6b417f620d0bd1fcf375250c357f71d0be4f557d25e484c5b3/eslib-0.0.13.tar.gz", "yanked": false}], "0.0.14": [{"comment_text": "", "digests": {"md5": "3814eabadf3c32dfc3ed9761861f7802", "sha256": "8251fa8535147988f8f252da0fd26008cc9d9bc746fdd609b9a9f2c1ba03695d"}, "downloads": -1, "filename": "eslib-0.0.14.tar.gz", "has_sig": false, "md5_digest": "3814eabadf3c32dfc3ed9761861f7802", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 139053, "upload_time": "2016-08-03T12:03:17", "upload_time_iso_8601": "2016-08-03T12:03:17.890727Z", "url": "https://files.pythonhosted.org/packages/5f/92/729eb3be4459803c8857d305b85efbcb933bf11f2b78807b603d6d1c8df0/eslib-0.0.14.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "0aa607d72c30829854a73bc145247c09", "sha256": "ccc8a23b3ed595a609ffd044d52699a486dad257d872ba919d835dc3ad653294"}, "downloads": -1, "filename": "eslib-0.0.2.tar.gz", "has_sig": false, "md5_digest": "0aa607d72c30829854a73bc145247c09", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 130424, "upload_time": "2015-08-12T12:44:42", "upload_time_iso_8601": "2015-08-12T12:44:42.091235Z", "url": "https://files.pythonhosted.org/packages/61/6e/a8476a76eebcdd3dba4ece5091c695b1e0f8ce21224b9463c3dbe5f1750f/eslib-0.0.2.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "b21ac48afbc4c8c852e019c1a7741ca4", "sha256": "1116743bf245aa979d7dc44e6a742ca6d6bcd05998a57bd63460ff0feee0f007"}, "downloads": -1, "filename": "eslib-0.0.4.tar.gz", "has_sig": false, "md5_digest": "b21ac48afbc4c8c852e019c1a7741ca4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 133090, "upload_time": "2015-11-16T09:30:16", "upload_time_iso_8601": "2015-11-16T09:30:16.406168Z", "url": "https://files.pythonhosted.org/packages/2c/ba/615912c0884269ca51655318429829bd5768a65db5ef980ce1c3662bd9f5/eslib-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "2bfeee76617c482f2c7400eb7081f6fe", "sha256": "f17e77d9acd27e874486c33e962198011c45dfff72599d30f2421854d51d267a"}, "downloads": -1, "filename": "eslib-0.0.5.tar.gz", "has_sig": false, "md5_digest": "2bfeee76617c482f2c7400eb7081f6fe", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 133076, "upload_time": "2015-11-16T09:32:34", "upload_time_iso_8601": "2015-11-16T09:32:34.642766Z", "url": "https://files.pythonhosted.org/packages/8d/31/2f1810ba3b12b71de45e5366ee31ec8fa9c61b9127b83d8bb2eb72d6342c/eslib-0.0.5.tar.gz", "yanked": false}], "0.0.6": [{"comment_text": "", "digests": {"md5": "de43389edc9e470c9e0f612864a9655b", "sha256": "6f0144dcea6cd966d5b85696176c21f142ed876bd05c71ea7f23e5ba148aac3f"}, "downloads": -1, "filename": "eslib-0.0.6.tar.gz", "has_sig": false, "md5_digest": "de43389edc9e470c9e0f612864a9655b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 135736, "upload_time": "2015-12-14T08:37:45", "upload_time_iso_8601": "2015-12-14T08:37:45.631109Z", "url": "https://files.pythonhosted.org/packages/af/d6/c6cd56e41504ce02fe9dc4d89dc0da829420f889bd3e5bcf2dc19ba39a6c/eslib-0.0.6.tar.gz", "yanked": false}], "0.0.7": [{"comment_text": "", "digests": {"md5": "360049f7c35db63f1e46d18765b4d716", "sha256": "c738969334ba39ead0cfbb01dd8b30e4712c1540e22812e81abead780ff44705"}, "downloads": -1, "filename": "eslib-0.0.7.tar.gz", "has_sig": false, "md5_digest": "360049f7c35db63f1e46d18765b4d716", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 135740, "upload_time": "2015-12-14T11:41:55", "upload_time_iso_8601": "2015-12-14T11:41:55.509041Z", "url": "https://files.pythonhosted.org/packages/00/fb/dabdd0cbbdaf206118d10a86deb44774e93cde642b2e9af832e0f289a92c/eslib-0.0.7.tar.gz", "yanked": false}], "0.0.8": [{"comment_text": "", "digests": {"md5": "43be5e23f5848836aaed245cb3671724", "sha256": "74d56b0425b53f5c07b62cc052170189def7309ca5ff5da98e0ab07fba30b86d"}, "downloads": -1, "filename": "eslib-0.0.8.tar.gz", "has_sig": false, "md5_digest": "43be5e23f5848836aaed245cb3671724", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 135749, "upload_time": "2015-12-14T11:46:35", "upload_time_iso_8601": "2015-12-14T11:46:35.744503Z", "url": "https://files.pythonhosted.org/packages/f2/3e/aafc3d37838cdb397977b0de03e6efb582b4feda7bb6a363aeca2f6d91d0/eslib-0.0.8.tar.gz", "yanked": false}], "0.0.9": [{"comment_text": "", "digests": {"md5": "6336e26720bc2db320ca7030158c17cc", "sha256": "239e87f90fef28036dde05abbdacd65b235349ce8699579fd119344ab604c8bd"}, "downloads": -1, "filename": "eslib-0.0.9.tar.gz", "has_sig": false, "md5_digest": "6336e26720bc2db320ca7030158c17cc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 135763, "upload_time": "2015-12-16T11:41:51", "upload_time_iso_8601": "2015-12-16T11:41:51.878799Z", "url": "https://files.pythonhosted.org/packages/55/40/d7e28e9c8158279def8d3072b0da282d36df371d42a43eb49423fec741f4/eslib-0.0.9.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3814eabadf3c32dfc3ed9761861f7802", "sha256": "8251fa8535147988f8f252da0fd26008cc9d9bc746fdd609b9a9f2c1ba03695d"}, "downloads": -1, "filename": "eslib-0.0.14.tar.gz", "has_sig": false, "md5_digest": "3814eabadf3c32dfc3ed9761861f7802", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 139053, "upload_time": "2016-08-03T12:03:17", "upload_time_iso_8601": "2016-08-03T12:03:17.890727Z", "url": "https://files.pythonhosted.org/packages/5f/92/729eb3be4459803c8857d305b85efbcb933bf11f2b78807b603d6d1c8df0/eslib-0.0.14.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:45:40 2020"}