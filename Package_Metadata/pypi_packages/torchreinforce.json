{"info": {"author": "Federico A. Galatolo", "author_email": "galatolo.federico@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Science/Research", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# torchreinforce\n\nA pythonic implementation of the REINFORCE algorithm that is actually fun to use\n\n## Installation\nYou can install it with pip as you would for any other python package\n```\npip install torchreinforce\n```\n\n## Quickstart\n\nIn order to use the REINFORCE algorithm with your model you only need to do two things:\n* Use the ``ReinforceModule`` class as your base class\n* Decorate your ``forward`` function with ``@ReinforceModule.forward``\n\nThat's it!\n\n```python\nclass Model(ReinforceModule):\n    def __init__(self, **kwargs):\n        super(Model, self).__init__(**kwargs)\n        self.net = torch.nn.Sequential(\n            torch.nn.Linear(20, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 2),\n            torch.nn.Softmax(dim=-1),\n        )\n\n    @ReinforceModule.forward\n    def forward(self, x):\n        return self.net(x)\n```\n\nYour model will now output ``ReinforceOutput`` objects.\n\nThis objects have two important functions\n\n* ``get()``\n* ``reward(value)``\n\nYou can use ``output.get()`` to get an actual sample of the overlaying distribution and ``output.reward(value)`` to set a reward for the specific output.\n\nBeing ``net`` your model you have to do something like that\n\n```python\naction = net(observation)\nobservation, reward, done, info = env.step(action.get())\naction.reward(reward)\n```\n\n## Wait, did you just said distribution?\n\nYes! As the REINFORCE algorithm states the outputs of your model will be used as parameters for a probability distribution function.\n\nActually you can use whatever probability distribution you want, the ``ReinforceModule`` constructor accepts indeed the following parameters:\n\n* ``gamma`` the *gamma* parameter of the REINFORCE algorithm (default: ``Categorical``)\n* ``distribution`` every ``ReinforceDistribution`` or ``pytorch.distributions`` distribution (default: 0.99)\n\nlike that\n\n```python\nnet = Model(distribution=torch.distributions.Beta, gamma=0.99)\n```\n\nKeep in mind that the outputs of your **decorated** ``forward(x)`` outputs will be used as the parameters for the ``distribution``. If your ``distribution`` needs more than one parameters just return a list.\n\nI've added the possibility to distribution to have a **deterministic** behavior in **testing** and I've implemented it only for the ``Categorical`` distribution, if you want to implement your own deterministic logic check the file ``distributions/categorical.py`` it is pretty straightforward\n\nIf you want to use the ``torch.distributions.Beta`` distribution for example you will need to do something like\n\n```python\nclass Model(ReinforceModule):\n    def __init__(self, **kwargs):\n        super(Model, self).__init__(**kwargs)\n        ...\n\n    @ReinforceModule.forward\n    def forward(self, x):\n        return [self.net1(x), self.net2(x)] # the Beta distribution accepts two parameters\n\nnet = Model(distribution=torch.distributions.Beta, gamma=0.99)\n\naction = net(inp)\nenv.step(action.get())\n```\n\n## Nice! What about training?\n\nYou can compute the REINFORCE loss by calling the ``loss()`` function of ``ReinforceModule`` and than treat it as you would do with any other pytorch loss function\n\n```python\nnet = ...\noptmizer = ...\n\nwhile training:\n    net.reset()\n    for steps:\n        ....\n\n    loss = net.loss(normalize=True)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optmizer.step()\n```\n\nYou **have to** call the ``reset()`` function of ``ReinforceModule`` **before** the beginning of each episode. You can also pass the argument ``normalize`` to ``loss()`` if you want to normalize the rewards\n\n## Putting all together\n\nA complete example looks like this:\n\n```python\nclass Model(ReinforceModule):\n    def __init__(self, **kwargs):\n        super(Model, self).__init__(**kwargs)\n        self.net = torch.nn.Sequential(\n            torch.nn.Linear(4, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 2),\n            torch.nn.Softmax(dim=-1),\n        )\n\n    @ReinforceModule.forward\n    def forward(self, x):\n        return self.net(x)\n\n\nenv = gym.make('CartPole-v0')\nnet = Model()\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n\nfor i in range(EPISODES):\n    done = False\n    net.reset()\n    observation = env.reset()\n    while not done:\n        action = net(torch.tensor(observation, dtype=torch.float32))\n\n        observation, reward, done, info = env.step(action.get())\n        action.reward(reward)\n\n    loss = net.loss(normalize=False)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\nYou can find a running example in the ``examples/`` folder.\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/galatolofederico/torchreinforce", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "torchreinforce", "package_url": "https://pypi.org/project/torchreinforce/", "platform": "", "project_url": "https://pypi.org/project/torchreinforce/", "project_urls": {"Homepage": "https://github.com/galatolofederico/torchreinforce"}, "release_url": "https://pypi.org/project/torchreinforce/0.1.0/", "requires_dist": ["torch", "numpy"], "requires_python": "", "summary": "A pythonic implementation of the REINFORCE algorithm that is actually fun to use", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>torchreinforce</h1>\n<p>A pythonic implementation of the REINFORCE algorithm that is actually fun to use</p>\n<h2>Installation</h2>\n<p>You can install it with pip as you would for any other python package</p>\n<pre><code>pip install torchreinforce\n</code></pre>\n<h2>Quickstart</h2>\n<p>In order to use the REINFORCE algorithm with your model you only need to do two things:</p>\n<ul>\n<li>Use the <code>ReinforceModule</code> class as your base class</li>\n<li>Decorate your <code>forward</code> function with <code>@ReinforceModule.forward</code></li>\n</ul>\n<p>That's it!</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">Model</span><span class=\"p\">(</span><span class=\"n\">ReinforceModule</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">Model</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">),</span>\n            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Softmax</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n        <span class=\"p\">)</span>\n\n    <span class=\"nd\">@ReinforceModule</span><span class=\"o\">.</span><span class=\"n\">forward</span>\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n</pre>\n<p>Your model will now output <code>ReinforceOutput</code> objects.</p>\n<p>This objects have two important functions</p>\n<ul>\n<li><code>get()</code></li>\n<li><code>reward(value)</code></li>\n</ul>\n<p>You can use <code>output.get()</code> to get an actual sample of the overlaying distribution and <code>output.reward(value)</code> to set a reward for the specific output.</p>\n<p>Being <code>net</code> your model you have to do something like that</p>\n<pre><span class=\"n\">action</span> <span class=\"o\">=</span> <span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">observation</span><span class=\"p\">)</span>\n<span class=\"n\">observation</span><span class=\"p\">,</span> <span class=\"n\">reward</span><span class=\"p\">,</span> <span class=\"n\">done</span><span class=\"p\">,</span> <span class=\"n\">info</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">action</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">())</span>\n<span class=\"n\">action</span><span class=\"o\">.</span><span class=\"n\">reward</span><span class=\"p\">(</span><span class=\"n\">reward</span><span class=\"p\">)</span>\n</pre>\n<h2>Wait, did you just said distribution?</h2>\n<p>Yes! As the REINFORCE algorithm states the outputs of your model will be used as parameters for a probability distribution function.</p>\n<p>Actually you can use whatever probability distribution you want, the <code>ReinforceModule</code> constructor accepts indeed the following parameters:</p>\n<ul>\n<li><code>gamma</code> the <em>gamma</em> parameter of the REINFORCE algorithm (default: <code>Categorical</code>)</li>\n<li><code>distribution</code> every <code>ReinforceDistribution</code> or <code>pytorch.distributions</code> distribution (default: 0.99)</li>\n</ul>\n<p>like that</p>\n<pre><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">distribution</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributions</span><span class=\"o\">.</span><span class=\"n\">Beta</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">0.99</span><span class=\"p\">)</span>\n</pre>\n<p>Keep in mind that the outputs of your <strong>decorated</strong> <code>forward(x)</code> outputs will be used as the parameters for the <code>distribution</code>. If your <code>distribution</code> needs more than one parameters just return a list.</p>\n<p>I've added the possibility to distribution to have a <strong>deterministic</strong> behavior in <strong>testing</strong> and I've implemented it only for the <code>Categorical</code> distribution, if you want to implement your own deterministic logic check the file <code>distributions/categorical.py</code> it is pretty straightforward</p>\n<p>If you want to use the <code>torch.distributions.Beta</code> distribution for example you will need to do something like</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">Model</span><span class=\"p\">(</span><span class=\"n\">ReinforceModule</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">Model</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n        <span class=\"o\">...</span>\n\n    <span class=\"nd\">@ReinforceModule</span><span class=\"o\">.</span><span class=\"n\">forward</span>\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">net1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">net2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)]</span> <span class=\"c1\"># the Beta distribution accepts two parameters</span>\n\n<span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">distribution</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributions</span><span class=\"o\">.</span><span class=\"n\">Beta</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">0.99</span><span class=\"p\">)</span>\n\n<span class=\"n\">action</span> <span class=\"o\">=</span> <span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">inp</span><span class=\"p\">)</span>\n<span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">action</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">())</span>\n</pre>\n<h2>Nice! What about training?</h2>\n<p>You can compute the REINFORCE loss by calling the <code>loss()</code> function of <code>ReinforceModule</code> and than treat it as you would do with any other pytorch loss function</p>\n<pre><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n<span class=\"n\">optmizer</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n\n<span class=\"k\">while</span> <span class=\"n\">training</span><span class=\"p\">:</span>\n    <span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n    <span class=\"k\">for</span> <span class=\"n\">steps</span><span class=\"p\">:</span>\n        <span class=\"o\">....</span>\n\n    <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">loss</span><span class=\"p\">(</span><span class=\"n\">normalize</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n    <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n    <span class=\"n\">optmizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p>You <strong>have to</strong> call the <code>reset()</code> function of <code>ReinforceModule</code> <strong>before</strong> the beginning of each episode. You can also pass the argument <code>normalize</code> to <code>loss()</code> if you want to normalize the rewards</p>\n<h2>Putting all together</h2>\n<p>A complete example looks like this:</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">Model</span><span class=\"p\">(</span><span class=\"n\">ReinforceModule</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">Model</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">),</span>\n            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Softmax</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n        <span class=\"p\">)</span>\n\n    <span class=\"nd\">@ReinforceModule</span><span class=\"o\">.</span><span class=\"n\">forward</span>\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s1\">'CartPole-v0'</span><span class=\"p\">)</span>\n<span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">()</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">EPISODES</span><span class=\"p\">):</span>\n    <span class=\"n\">done</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n    <span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n    <span class=\"n\">observation</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n    <span class=\"k\">while</span> <span class=\"ow\">not</span> <span class=\"n\">done</span><span class=\"p\">:</span>\n        <span class=\"n\">action</span> <span class=\"o\">=</span> <span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">observation</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">))</span>\n\n        <span class=\"n\">observation</span><span class=\"p\">,</span> <span class=\"n\">reward</span><span class=\"p\">,</span> <span class=\"n\">done</span><span class=\"p\">,</span> <span class=\"n\">info</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">action</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">())</span>\n        <span class=\"n\">action</span><span class=\"o\">.</span><span class=\"n\">reward</span><span class=\"p\">(</span><span class=\"n\">reward</span><span class=\"p\">)</span>\n\n    <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">loss</span><span class=\"p\">(</span><span class=\"n\">normalize</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n    <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p>You can find a running example in the <code>examples/</code> folder.</p>\n\n          </div>"}, "last_serial": 4713345, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "b18372214156e1546ba1485301275401", "sha256": "d7023facdea8f79409c5e58526bec9e182540af3f392b83257f258b8766f31f5"}, "downloads": -1, "filename": "torchreinforce-0.1.0-py3.6.egg", "has_sig": false, "md5_digest": "b18372214156e1546ba1485301275401", "packagetype": "bdist_egg", "python_version": "3.6", "requires_python": null, "size": 12782, "upload_time": "2019-01-18T19:05:30", "upload_time_iso_8601": "2019-01-18T19:05:30.583589Z", "url": "https://files.pythonhosted.org/packages/fe/a6/04fb485d82a7ba41711190a0da8fb246fd0f7812d09ce58e5f6c15daa86b/torchreinforce-0.1.0-py3.6.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "d021cfa11e21ff3ea825f44717ef9d92", "sha256": "69b205c21b0044f82991d550f950ed1e99ee79aef7f981c18753815823fe4c83"}, "downloads": -1, "filename": "torchreinforce-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "d021cfa11e21ff3ea825f44717ef9d92", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18964, "upload_time": "2019-01-18T19:05:28", "upload_time_iso_8601": "2019-01-18T19:05:28.507630Z", "url": "https://files.pythonhosted.org/packages/47/82/5d3c2eb2dad9f2a5cb65b74ba29b23ae21322832c8c8c167dab05c8c7128/torchreinforce-0.1.0-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b18372214156e1546ba1485301275401", "sha256": "d7023facdea8f79409c5e58526bec9e182540af3f392b83257f258b8766f31f5"}, "downloads": -1, "filename": "torchreinforce-0.1.0-py3.6.egg", "has_sig": false, "md5_digest": "b18372214156e1546ba1485301275401", "packagetype": "bdist_egg", "python_version": "3.6", "requires_python": null, "size": 12782, "upload_time": "2019-01-18T19:05:30", "upload_time_iso_8601": "2019-01-18T19:05:30.583589Z", "url": "https://files.pythonhosted.org/packages/fe/a6/04fb485d82a7ba41711190a0da8fb246fd0f7812d09ce58e5f6c15daa86b/torchreinforce-0.1.0-py3.6.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "d021cfa11e21ff3ea825f44717ef9d92", "sha256": "69b205c21b0044f82991d550f950ed1e99ee79aef7f981c18753815823fe4c83"}, "downloads": -1, "filename": "torchreinforce-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "d021cfa11e21ff3ea825f44717ef9d92", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18964, "upload_time": "2019-01-18T19:05:28", "upload_time_iso_8601": "2019-01-18T19:05:28.507630Z", "url": "https://files.pythonhosted.org/packages/47/82/5d3c2eb2dad9f2a5cb65b74ba29b23ae21322832c8c8c167dab05c8c7128/torchreinforce-0.1.0-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 03:50:12 2020"}