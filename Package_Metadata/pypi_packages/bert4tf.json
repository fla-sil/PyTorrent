{"info": {"author": "xu", "author_email": "charlesxu86@163.com", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "BERT for TensorFlow v2\n======================\n\nThis repo contains a `TensorFlow 2.0`_ `Keras`_ implementation of `google-research/bert`_\nwith support for loading of the original `pre-trained weights`_,\nand producing activations **numerically identical** to the one calculated by the original model.\n\n`ALBERT`_ and `adapter-BERT`_ are also supported by setting the corresponding\nconfiguration parameters (``shared_layer=True``, ``embedding_size`` for `ALBERT`_\nand ``adapter_size`` for `adapter-BERT`_). Setting both will result in an adapter-ALBERT\nby sharing the BERT parameters across all layers while adapting every layer with layer specific adapter.\n\nThe implementation is build from scratch using only basic tensorflow operations,\nfollowing the code in `google-research/bert/modeling.py`_\n(but skipping dead code and applying some simplifications). It also utilizes `kpe/params-flow`_ to reduce\ncommon Keras boilerplate code (related to passing model and layer configuration arguments).\n\n`bert4tf`_ should work with both `TensorFlow 2.0`_ and `TensorFlow 1.14`_ or newer.\n\nNEWS\n----\n\n\nLICENSE\n-------\n\nMIT. See `License File <https://github.com/kpe/bert-for-tf2/blob/master/LICENSE.txt>`_.\n\nInstall\n-------\n\n``bert4tf`` is on the Python Package Index (PyPI):\n\n::\n\n    pip install bert4tf\n\n\nUsage\n-----\n\nBERT in `bert4tf` is implemented as a Keras layer. You could instantiate it like this:\n\n.. code:: python\n\n  from bert4tf import BertModelLayer\n\n  l_bert = BertModelLayer(**BertModelLayer.Params(\n    vocab_size               = 16000,        # embedding params\n    use_token_type           = True,\n    use_position_embeddings  = True,\n    token_type_vocab_size    = 2,\n\n    num_layers               = 12,           # transformer encoder params\n    hidden_size              = 768,\n    hidden_dropout           = 0.1,\n    intermediate_size        = 4*768,\n    intermediate_activation  = \"gelu\",\n\n    adapter_size             = None,         # see arXiv:1902.00751 (adapter-BERT)\n\n    shared_layer             = False,        # True for ALBERT (arXiv:1909.11942)\n    embedding_size           = None,         # None for BERT, wordpiece embedding size for ALBERT\n\n    name                     = \"bert\"        # any other Keras layer params\n  ))\n\nor by using the ``bert_config.json`` from a `pre-trained google model`_:\n\n.. code:: python\n\n  import bert4tf\n\n  model_dir = \".models/uncased_L-12_H-768_A-12\"\n\n  bert_params = bert.params_from_pretrained_ckpt(model_dir)\n  l_bert = bert4tf.BertModelLayer.from_params(bert_params, name=\"bert\")\n\n\nnow you can use the BERT layer in your Keras model like this:\n\n.. code:: python\n\n  from tensorflow import keras\n\n  max_seq_len = 128\n  l_input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype='int32')\n  l_token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32')\n\n  # using the default token_type/segment id 0\n  output = l_bert(l_input_ids)                              # output: [batch_size, max_seq_len, hidden_size]\n  model = keras.Model(inputs=l_input_ids, outputs=output)\n  model.build(input_shape=(None, max_seq_len))\n\n  # provide a custom token_type/segment id as a layer input\n  output = l_bert([l_input_ids, l_token_type_ids])          # [batch_size, max_seq_len, hidden_size]\n  model = keras.Model(inputs=[l_input_ids, l_token_type_ids], outputs=output)\n  model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n\nif you choose to use `adapter-BERT`_ by setting the `adapter_size` parameter,\nyou would also like to freeze all the original BERT layers by calling:\n\n.. code:: python\n\n  l_bert.apply_adapter_freeze()\n\nand once the model has been build or compiled, the original pre-trained weights\ncan be loaded in the BERT layer:\n\n.. code:: python\n\n  import bert4tf\n\n  bert_ckpt_file   = os.path.join(model_dir, \"bert_model.ckpt\")\n  bert.load_stock_weights(l_bert, bert_ckpt_file)\n\n**N.B.** see `tests/test_bert_activations.py`_ for a complete example.\n\nFAQ\n---\n1. How to use BERT with the `google-research/bert`_ pre-trained weights?\n\n.. code:: python\n\n  model_name = \"uncased_L-12_H-768_A-12\"\n  model_dir = bert4tf.fetch_google_bert_model(model_name, \".models\")\n  model_ckpt = os.path.join(model_dir, \"bert_model.ckpt\")\n\n  bert_params = bert4tf.params_from_pretrained_ckpt(model_dir)\n  l_bert = bert4tf.BertModelLayer.from_params(bert_params, name=\"bert\")\n\n  # use in Keras Model here, and call model.build()\n\n  bert.load_bert_weights(l_bert, model_ckpt)      # should be called after model.build()\n\n2. How to use ALBERT with the `google-research/albert`_ pre-trained weights?\n\n.. code:: python\n\n  model_name = \"albert_base\"\n  model_dir    = bert4tf.fetch_tfhub_albert_model(model_name, \".models\")\n  model_params = bert4tf.albert_params(model_name)\n  l_bert = bert.BertModelLayer.from_params(model_params, name=\"albert\")\n\n  # use in Keras Model here, and call model.build()\n\n  bert.load_albert_weights(l_bert, albert_dir)      # should be called after model.build()\n\n3. How to use ALBERT with the `albert_zh` pre-trained weights?\n\nsee `tests/nonci/test_albert.py <https://github.com/kpe/bert-for-tf2/blob/master/tests/nonci/test_albert.py>`_:\n\n.. code:: python\n\n  model_name = \"albert_base\"\n  model_dir = bert4tf.fetch_brightmart_albert_model(model_name, \".models\")\n  model_ckpt = os.path.join(model_dir, \"albert_model.ckpt\")\n\n  bert_params = bert.params_from_pretrained_ckpt(model_dir)\n  l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n\n  # use in a Keras Model here, and call model.build()\n\n  bert.load_albert_weights(l_bert, model_ckpt)      # should be called after model.build()\n\n4. How to tokenize the input for the `google-research/bert`_ models?\n\n  do_lower_case = not (model_name.find(\"cased\") == 0 or model_name.find(\"multi_cased\") == 0)\n  bert.bert_tokenization.validate_case_matches_checkpoint(do_lower_case, model_ckpt)\n  vocab_file = os.path.join(model_dir, \"vocab.txt\")\n  tokenizer = bert4tf.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n  tokens = tokenizer.tokenize(\"Hello, BERT-World!\")\n  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n5. How to tokenize the input for the `google-research/albert`_ models?\n\n  import sentencepiece as spm\n\n  spm_model = os.path.join(model_dir, \"assets\", \"30k-clean.model\")\n  sp = spm.SentencePieceProcessor()\n  sp.load(spm_model)\n  do_lower_case = True\n\n  processed_text = bert.albert_tokenization.preprocess_text(\"Hello, World!\", lower=do_lower_case)\n  token_ids = bert4tf.albert_tokenization.encode_ids(sp, processed_text)\n\n\nReferrence\n---\n1. kpe\n\nResources\n---------\n\n- `BERT`_ - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n- `adapter-BERT`_ - adapter-BERT: Parameter-Efficient Transfer Learning for NLP\n- `ALBERT`_ - ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations\n- `google-research/bert`_ - the original `BERT`_ implementation\n- `google-research/albert`_ - the original `ALBERT`_ implementation by Google\n- `kpe/params-flow`_ - A Keras coding style for reducing `Keras`_ boilerplate code in custom layers by utilizing `kpe/py-params`_\n\n.. _`kpe/params-flow`: https://github.com/kpe/params-flow\n.. _`kpe/py-params`: https://github.com/kpe/py-params\n.. _`bert4tf`: https://github.com/charlesXu86/Bert4tf\n\n.. _`Keras`: https://keras.io\n.. _`pre-trained weights`: https://github.com/google-research/bert#pre-trained-models\n.. _`google-research/bert`: https://github.com/google-research/bert\n.. _`google-research/bert/modeling.py`: https://github.com/google-research/bert/blob/master/modeling.py\n.. _`BERT`: https://arxiv.org/abs/1810.04805\n.. _`pre-trained google model`: https://github.com/google-research/bert\n.. _`tests/test_bert_activations.py`: https://github.com/kpe/bert-for-tf2/blob/master/tests/test_compare_activations.py\n.. _`TensorFlow 2.0`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf\n.. _`TensorFlow 1.14`: https://www.tensorflow.org/versions/r1.14/api_docs/python/tf\n\n.. _`google-research/adapter-bert`: https://github.com/google-research/adapter-bert/\n.. _`adapter-BERT`: https://arxiv.org/abs/1902.00751\n.. _`ALBERT`: https://arxiv.org/abs/1909.11942\n.. _`google ALBERT weights`: https://github.com/google-research/google-research/tree/master/albert\n.. _`google-research/albert`: https://github.com/google-research/google-research/tree/master/albert\n.. _`TFHub/albert`: https://tfhub.dev/google/albert_base/1\n\n", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/charlesXu86/Bert4tf", "keywords": "bert4tf,tensorflow2", "license": "MIT Licence", "maintainer": "", "maintainer_email": "", "name": "bert4tf", "package_url": "https://pypi.org/project/bert4tf/", "platform": "any", "project_url": "https://pypi.org/project/bert4tf/", "project_urls": {"Homepage": "https://github.com/charlesXu86/Bert4tf"}, "release_url": "https://pypi.org/project/bert4tf/2.0.2/", "requires_dist": ["tensorflow"], "requires_python": "", "summary": "bert for tensorflow2", "version": "2.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>This repo contains a <a href=\"https://www.tensorflow.org/versions/r2.0/api_docs/python/tf\" rel=\"nofollow\">TensorFlow 2.0</a> <a href=\"https://keras.io\" rel=\"nofollow\">Keras</a> implementation of <a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">google-research/bert</a>\nwith support for loading of the original <a href=\"https://github.com/google-research/bert#pre-trained-models\" rel=\"nofollow\">pre-trained weights</a>,\nand producing activations <strong>numerically identical</strong> to the one calculated by the original model.</p>\n<p><a href=\"https://arxiv.org/abs/1909.11942\" rel=\"nofollow\">ALBERT</a> and <a href=\"https://arxiv.org/abs/1902.00751\" rel=\"nofollow\">adapter-BERT</a> are also supported by setting the corresponding\nconfiguration parameters (<tt>shared_layer=True</tt>, <tt>embedding_size</tt> for <a href=\"https://arxiv.org/abs/1909.11942\" rel=\"nofollow\">ALBERT</a>\nand <tt>adapter_size</tt> for <a href=\"https://arxiv.org/abs/1902.00751\" rel=\"nofollow\">adapter-BERT</a>). Setting both will result in an adapter-ALBERT\nby sharing the BERT parameters across all layers while adapting every layer with layer specific adapter.</p>\n<p>The implementation is build from scratch using only basic tensorflow operations,\nfollowing the code in <a href=\"https://github.com/google-research/bert/blob/master/modeling.py\" rel=\"nofollow\">google-research/bert/modeling.py</a>\n(but skipping dead code and applying some simplifications). It also utilizes <a href=\"https://github.com/kpe/params-flow\" rel=\"nofollow\">kpe/params-flow</a> to reduce\ncommon Keras boilerplate code (related to passing model and layer configuration arguments).</p>\n<p><a href=\"https://github.com/charlesXu86/Bert4tf\" rel=\"nofollow\">bert4tf</a> should work with both <a href=\"https://www.tensorflow.org/versions/r2.0/api_docs/python/tf\" rel=\"nofollow\">TensorFlow 2.0</a> and <a href=\"https://www.tensorflow.org/versions/r1.14/api_docs/python/tf\" rel=\"nofollow\">TensorFlow 1.14</a> or newer.</p>\n<div id=\"news\">\n<h2>NEWS</h2>\n</div>\n<div id=\"license\">\n<h2>LICENSE</h2>\n<p>MIT. See <a href=\"https://github.com/kpe/bert-for-tf2/blob/master/LICENSE.txt\" rel=\"nofollow\">License File</a>.</p>\n</div>\n<div id=\"install\">\n<h2>Install</h2>\n<p><tt>bert4tf</tt> is on the Python Package Index (PyPI):</p>\n<pre>pip install bert4tf\n</pre>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>BERT in <cite>bert4tf</cite> is implemented as a Keras layer. You could instantiate it like this:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">bert4tf</span> <span class=\"kn\">import</span> <span class=\"n\">BertModelLayer</span>\n\n<span class=\"n\">l_bert</span> <span class=\"o\">=</span> <span class=\"n\">BertModelLayer</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">BertModelLayer</span><span class=\"o\">.</span><span class=\"n\">Params</span><span class=\"p\">(</span>\n  <span class=\"n\">vocab_size</span>               <span class=\"o\">=</span> <span class=\"mi\">16000</span><span class=\"p\">,</span>        <span class=\"c1\"># embedding params</span>\n  <span class=\"n\">use_token_type</span>           <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n  <span class=\"n\">use_position_embeddings</span>  <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n  <span class=\"n\">token_type_vocab_size</span>    <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n\n  <span class=\"n\">num_layers</span>               <span class=\"o\">=</span> <span class=\"mi\">12</span><span class=\"p\">,</span>           <span class=\"c1\"># transformer encoder params</span>\n  <span class=\"n\">hidden_size</span>              <span class=\"o\">=</span> <span class=\"mi\">768</span><span class=\"p\">,</span>\n  <span class=\"n\">hidden_dropout</span>           <span class=\"o\">=</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span>\n  <span class=\"n\">intermediate_size</span>        <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"o\">*</span><span class=\"mi\">768</span><span class=\"p\">,</span>\n  <span class=\"n\">intermediate_activation</span>  <span class=\"o\">=</span> <span class=\"s2\">\"gelu\"</span><span class=\"p\">,</span>\n\n  <span class=\"n\">adapter_size</span>             <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span>         <span class=\"c1\"># see arXiv:1902.00751 (adapter-BERT)</span>\n\n  <span class=\"n\">shared_layer</span>             <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span>        <span class=\"c1\"># True for ALBERT (arXiv:1909.11942)</span>\n  <span class=\"n\">embedding_size</span>           <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span>         <span class=\"c1\"># None for BERT, wordpiece embedding size for ALBERT</span>\n\n  <span class=\"n\">name</span>                     <span class=\"o\">=</span> <span class=\"s2\">\"bert\"</span>        <span class=\"c1\"># any other Keras layer params</span>\n<span class=\"p\">))</span>\n</pre>\n<p>or by using the <tt>bert_config.json</tt> from a <a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">pre-trained google model</a>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">bert4tf</span>\n\n<span class=\"n\">model_dir</span> <span class=\"o\">=</span> <span class=\"s2\">\".models/uncased_L-12_H-768_A-12\"</span>\n\n<span class=\"n\">bert_params</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">params_from_pretrained_ckpt</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">)</span>\n<span class=\"n\">l_bert</span> <span class=\"o\">=</span> <span class=\"n\">bert4tf</span><span class=\"o\">.</span><span class=\"n\">BertModelLayer</span><span class=\"o\">.</span><span class=\"n\">from_params</span><span class=\"p\">(</span><span class=\"n\">bert_params</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"bert\"</span><span class=\"p\">)</span>\n</pre>\n<p>now you can use the BERT layer in your Keras model like this:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tensorflow</span> <span class=\"kn\">import</span> <span class=\"n\">keras</span>\n\n<span class=\"n\">max_seq_len</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>\n<span class=\"n\">l_input_ids</span>      <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">max_seq_len</span><span class=\"p\">,),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s1\">'int32'</span><span class=\"p\">)</span>\n<span class=\"n\">l_token_type_ids</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">max_seq_len</span><span class=\"p\">,),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s1\">'int32'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># using the default token_type/segment id 0</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">l_bert</span><span class=\"p\">(</span><span class=\"n\">l_input_ids</span><span class=\"p\">)</span>                              <span class=\"c1\"># output: [batch_size, max_seq_len, hidden_size]</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"n\">l_input_ids</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"n\">output</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">build</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">max_seq_len</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># provide a custom token_type/segment id as a layer input</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">l_bert</span><span class=\"p\">([</span><span class=\"n\">l_input_ids</span><span class=\"p\">,</span> <span class=\"n\">l_token_type_ids</span><span class=\"p\">])</span>          <span class=\"c1\"># [batch_size, max_seq_len, hidden_size]</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">l_input_ids</span><span class=\"p\">,</span> <span class=\"n\">l_token_type_ids</span><span class=\"p\">],</span> <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"n\">output</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">build</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">max_seq_len</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">max_seq_len</span><span class=\"p\">)])</span>\n</pre>\n<p>if you choose to use <a href=\"https://arxiv.org/abs/1902.00751\" rel=\"nofollow\">adapter-BERT</a> by setting the <cite>adapter_size</cite> parameter,\nyou would also like to freeze all the original BERT layers by calling:</p>\n<pre><span class=\"n\">l_bert</span><span class=\"o\">.</span><span class=\"n\">apply_adapter_freeze</span><span class=\"p\">()</span>\n</pre>\n<p>and once the model has been build or compiled, the original pre-trained weights\ncan be loaded in the BERT layer:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">bert4tf</span>\n\n<span class=\"n\">bert_ckpt_file</span>   <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">,</span> <span class=\"s2\">\"bert_model.ckpt\"</span><span class=\"p\">)</span>\n<span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">load_stock_weights</span><span class=\"p\">(</span><span class=\"n\">l_bert</span><span class=\"p\">,</span> <span class=\"n\">bert_ckpt_file</span><span class=\"p\">)</span>\n</pre>\n<p><strong>N.B.</strong> see <a href=\"https://github.com/kpe/bert-for-tf2/blob/master/tests/test_compare_activations.py\" rel=\"nofollow\">tests/test_bert_activations.py</a> for a complete example.</p>\n</div>\n<div id=\"faq\">\n<h2>FAQ</h2>\n<ol>\n<li>How to use BERT with the <a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">google-research/bert</a> pre-trained weights?</li>\n</ol>\n<pre><span class=\"n\">model_name</span> <span class=\"o\">=</span> <span class=\"s2\">\"uncased_L-12_H-768_A-12\"</span>\n<span class=\"n\">model_dir</span> <span class=\"o\">=</span> <span class=\"n\">bert4tf</span><span class=\"o\">.</span><span class=\"n\">fetch_google_bert_model</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"s2\">\".models\"</span><span class=\"p\">)</span>\n<span class=\"n\">model_ckpt</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">,</span> <span class=\"s2\">\"bert_model.ckpt\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">bert_params</span> <span class=\"o\">=</span> <span class=\"n\">bert4tf</span><span class=\"o\">.</span><span class=\"n\">params_from_pretrained_ckpt</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">)</span>\n<span class=\"n\">l_bert</span> <span class=\"o\">=</span> <span class=\"n\">bert4tf</span><span class=\"o\">.</span><span class=\"n\">BertModelLayer</span><span class=\"o\">.</span><span class=\"n\">from_params</span><span class=\"p\">(</span><span class=\"n\">bert_params</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"bert\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># use in Keras Model here, and call model.build()</span>\n\n<span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">load_bert_weights</span><span class=\"p\">(</span><span class=\"n\">l_bert</span><span class=\"p\">,</span> <span class=\"n\">model_ckpt</span><span class=\"p\">)</span>      <span class=\"c1\"># should be called after model.build()</span>\n</pre>\n<ol>\n<li>How to use ALBERT with the <a href=\"https://github.com/google-research/google-research/tree/master/albert\" rel=\"nofollow\">google-research/albert</a> pre-trained weights?</li>\n</ol>\n<pre><span class=\"n\">model_name</span> <span class=\"o\">=</span> <span class=\"s2\">\"albert_base\"</span>\n<span class=\"n\">model_dir</span>    <span class=\"o\">=</span> <span class=\"n\">bert4tf</span><span class=\"o\">.</span><span class=\"n\">fetch_tfhub_albert_model</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"s2\">\".models\"</span><span class=\"p\">)</span>\n<span class=\"n\">model_params</span> <span class=\"o\">=</span> <span class=\"n\">bert4tf</span><span class=\"o\">.</span><span class=\"n\">albert_params</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">)</span>\n<span class=\"n\">l_bert</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">BertModelLayer</span><span class=\"o\">.</span><span class=\"n\">from_params</span><span class=\"p\">(</span><span class=\"n\">model_params</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"albert\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># use in Keras Model here, and call model.build()</span>\n\n<span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">load_albert_weights</span><span class=\"p\">(</span><span class=\"n\">l_bert</span><span class=\"p\">,</span> <span class=\"n\">albert_dir</span><span class=\"p\">)</span>      <span class=\"c1\"># should be called after model.build()</span>\n</pre>\n<ol>\n<li>How to use ALBERT with the <cite>albert_zh</cite> pre-trained weights?</li>\n</ol>\n<p>see <a href=\"https://github.com/kpe/bert-for-tf2/blob/master/tests/nonci/test_albert.py\" rel=\"nofollow\">tests/nonci/test_albert.py</a>:</p>\n<pre><span class=\"n\">model_name</span> <span class=\"o\">=</span> <span class=\"s2\">\"albert_base\"</span>\n<span class=\"n\">model_dir</span> <span class=\"o\">=</span> <span class=\"n\">bert4tf</span><span class=\"o\">.</span><span class=\"n\">fetch_brightmart_albert_model</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"s2\">\".models\"</span><span class=\"p\">)</span>\n<span class=\"n\">model_ckpt</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">,</span> <span class=\"s2\">\"albert_model.ckpt\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">bert_params</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">params_from_pretrained_ckpt</span><span class=\"p\">(</span><span class=\"n\">model_dir</span><span class=\"p\">)</span>\n<span class=\"n\">l_bert</span> <span class=\"o\">=</span> <span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">BertModelLayer</span><span class=\"o\">.</span><span class=\"n\">from_params</span><span class=\"p\">(</span><span class=\"n\">bert_params</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"bert\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># use in a Keras Model here, and call model.build()</span>\n\n<span class=\"n\">bert</span><span class=\"o\">.</span><span class=\"n\">load_albert_weights</span><span class=\"p\">(</span><span class=\"n\">l_bert</span><span class=\"p\">,</span> <span class=\"n\">model_ckpt</span><span class=\"p\">)</span>      <span class=\"c1\"># should be called after model.build()</span>\n</pre>\n<ol>\n<li>How to tokenize the input for the <a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">google-research/bert</a> models?</li>\n</ol>\n<blockquote>\ndo_lower_case = not (model_name.find(\u201ccased\u201d) == 0 or model_name.find(\u201cmulti_cased\u201d) == 0)\nbert.bert_tokenization.validate_case_matches_checkpoint(do_lower_case, model_ckpt)\nvocab_file = os.path.join(model_dir, \u201cvocab.txt\u201d)\ntokenizer = bert4tf.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\ntokens = tokenizer.tokenize(\u201cHello, BERT-World!\u201d)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)</blockquote>\n<ol>\n<li>How to tokenize the input for the <a href=\"https://github.com/google-research/google-research/tree/master/albert\" rel=\"nofollow\">google-research/albert</a> models?</li>\n</ol>\n<blockquote>\n<p>import sentencepiece as spm</p>\n<p>spm_model = os.path.join(model_dir, \u201cassets\u201d, \u201c30k-clean.model\u201d)\nsp = spm.SentencePieceProcessor()\nsp.load(spm_model)\ndo_lower_case = True</p>\n<p>processed_text = bert.albert_tokenization.preprocess_text(\u201cHello, World!\u201d, lower=do_lower_case)\ntoken_ids = bert4tf.albert_tokenization.encode_ids(sp, processed_text)</p>\n</blockquote>\n<p>Referrence\n\u2014\n1. kpe</p>\n</div>\n<div id=\"resources\">\n<h2>Resources</h2>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">BERT</a> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</li>\n<li><a href=\"https://arxiv.org/abs/1902.00751\" rel=\"nofollow\">adapter-BERT</a> - adapter-BERT: Parameter-Efficient Transfer Learning for NLP</li>\n<li><a href=\"https://arxiv.org/abs/1909.11942\" rel=\"nofollow\">ALBERT</a> - ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations</li>\n<li><a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">google-research/bert</a> - the original <a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\">BERT</a> implementation</li>\n<li><a href=\"https://github.com/google-research/google-research/tree/master/albert\" rel=\"nofollow\">google-research/albert</a> - the original <a href=\"https://arxiv.org/abs/1909.11942\" rel=\"nofollow\">ALBERT</a> implementation by Google</li>\n<li><a href=\"https://github.com/kpe/params-flow\" rel=\"nofollow\">kpe/params-flow</a> - A Keras coding style for reducing <a href=\"https://keras.io\" rel=\"nofollow\">Keras</a> boilerplate code in custom layers by utilizing <a href=\"https://github.com/kpe/py-params\" rel=\"nofollow\">kpe/py-params</a></li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 6360239, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "ed9bf5e8b252c2c3993a6fc279204970", "sha256": "5b79d80afcc0f9c6f0a4a80f106605ecb811bac0485c6cb7216b1f7e0d660303"}, "downloads": -1, "filename": "bert4tf-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "ed9bf5e8b252c2c3993a6fc279204970", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 35268, "upload_time": "2019-11-03T07:41:56", "upload_time_iso_8601": "2019-11-03T07:41:56.832857Z", "url": "https://files.pythonhosted.org/packages/b8/f1/12fb62ae056b6d66b8718b785924201fcc6b870edd401d395b6e366b91ca/bert4tf-1.0.0-py3-none-any.whl", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "fe33baabffd5d701324c1fb48be3a61d", "sha256": "2db83b38c49f76276bc3e9c6e5e62d1dd830acf3638511efd8491f2a527d99f0"}, "downloads": -1, "filename": "bert4tf-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "fe33baabffd5d701324c1fb48be3a61d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 36758, "upload_time": "2019-11-12T09:48:41", "upload_time_iso_8601": "2019-11-12T09:48:41.724833Z", "url": "https://files.pythonhosted.org/packages/7f/bf/84fb2d006309f365a279d88624c2f0e6aa7a09627ffba1aa07ffe29ea764/bert4tf-1.0.1-py3-none-any.whl", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "cf4827e11bad3b6f94e9d138da0f9b15", "sha256": "99820a0c4243eeb330882eed28f9de69b9b5058ee2f13d902a00303f1549d2b2"}, "downloads": -1, "filename": "bert4tf-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "cf4827e11bad3b6f94e9d138da0f9b15", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 36994, "upload_time": "2019-11-12T11:12:36", "upload_time_iso_8601": "2019-11-12T11:12:36.486628Z", "url": "https://files.pythonhosted.org/packages/4a/f3/471a553a4c13a9519afba4b17fe0b7b2443d9d9a220eac33a1bacabfac80/bert4tf-1.0.2-py3-none-any.whl", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "5ec9c12220d0a21bd480c23be10bafad", "sha256": "aa8ad9737883ab63d5bc85f2c5de533b318a20a755b04a1061bf807b2b918bbc"}, "downloads": -1, "filename": "bert4tf-1.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "5ec9c12220d0a21bd480c23be10bafad", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 37004, "upload_time": "2019-11-13T03:12:30", "upload_time_iso_8601": "2019-11-13T03:12:30.132130Z", "url": "https://files.pythonhosted.org/packages/5c/26/970de459cf79542a3daa196ed1486dda118e056247edfc62d5d4b052cca5/bert4tf-1.0.3-py3-none-any.whl", "yanked": false}], "2.0.2": [{"comment_text": "", "digests": {"md5": "32d9a266af52ac0564b600263d02a5d0", "sha256": "0c19ec08479fa0612535e0324592296139ebf0d9932eea9a549cf408ff146d11"}, "downloads": -1, "filename": "bert4tf-2.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "32d9a266af52ac0564b600263d02a5d0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 38844, "upload_time": "2019-12-26T02:48:18", "upload_time_iso_8601": "2019-12-26T02:48:18.882699Z", "url": "https://files.pythonhosted.org/packages/b6/d0/d54e390ca1d97bbde943e308e80e70a2ebe456e287fff545024bb45566bb/bert4tf-2.0.2-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "32d9a266af52ac0564b600263d02a5d0", "sha256": "0c19ec08479fa0612535e0324592296139ebf0d9932eea9a549cf408ff146d11"}, "downloads": -1, "filename": "bert4tf-2.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "32d9a266af52ac0564b600263d02a5d0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 38844, "upload_time": "2019-12-26T02:48:18", "upload_time_iso_8601": "2019-12-26T02:48:18.882699Z", "url": "https://files.pythonhosted.org/packages/b6/d0/d54e390ca1d97bbde943e308e80e70a2ebe456e287fff545024bb45566bb/bert4tf-2.0.2-py3-none-any.whl", "yanked": false}], "timestamp": "Thu May  7 22:37:46 2020"}