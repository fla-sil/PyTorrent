{"info": {"author": "Vadim Panov", "author_email": "headcra6@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# Airflow Livy Plugins\n\n[![Build Status](https://travis-ci.org/panovvv/airflow-livy-plugins.svg?branch=master)](https://travis-ci.org/panovvv/airflow-livy-plugins)\n[![Code coverage](https://codecov.io/gh/panovvv/airflow-livy-plugins/branch/master/graph/badge.svg)](https://codecov.io/gh/panovvv/airflow-livy-plugins)\n\nPlugins for Airflow to run Spark jobs via Livy: \n* Sessions,\n* Batches. This mode supports additional verification via Spark/YARN REST API.\n\nSee [this blog post](https://www.shortn0tes.com/2019/08/airflow-spark-livy-sessions-batches.html \"Blog post\") for more information and detailed comparison of ways to run Spark jobs from Airflow.\n\n## Directories and files of interest\n* `airflow_home`: example DAGs and plugins for Airflow. Can be used as \nAirflow home path.\n* `batches`: Spark jobs code, to be used in Livy batches.\n* `sessions`: (Optionally) templated Spark code for Livy sessions.\n* `airflow.sh`: helper shell script. Can be used to run sample DAGs,\nprep development environment and more.\nRun it to find out what other commands are available.\n\n\n## How do I...\n\n### ...run the examples?\nPrerequisites:\n* Python 3. Make sure it's installed and in __$PATH__\n\nNow, \n1. Do you have a Spark cluster with Livy running somewhere?\n    1. *No*. Either get one, or \"mock\" it with \n    [my Spark cluster on Docker Compose](https://github.com/panovvv/bigdata-docker-compose).\n    1. *Yes*. You're golden!\n1. __Optional - this step can be skipped if you're mocking a cluster on your\nmachine__. Open *airflow.sh*. Inside `init_airflow ()` function you'll see Airflow\nConnections for Livy, Spark and YARN. Redefine as appropriate.\n1. run `./airflow.sh up` to bring up the whole infrastructure. \nAirflow UI will be available at\n[localhost:8080](http://localhost:8888 \"Airflow UI\").\n1. Ctrl+C to stop Airflow. Then `./airflow.sh down` to dispose of\nremaining Airflow processes (shouldn't be needed there if everything goes well).\n\n### ... use it in my project?\n```bash\npip install airflow-livy-plugins\n```\nThen link or copy the plugin files into `$AIRFLOW_HOME/plugins` \n(see how I do that in `./airflow.sh`). \nThey'll get loaded into Airflow via Plugin Manager automatically.\nThis is how you import the plugins:\n```python\nfrom airflow.operators import LivySessionOperator\nfrom airflow.operators import LivyBatchOperator\n```\nPlugins are loaded at run-time so the imports above will look broken in your IDE,\nbut will work fine in Airflow.\nTake a look at the sample DAGs to see my walkaround :)\n\n### ... set up the development environment?\nAlright, you want to contribute and need to be able to run the stuff on your machine,\nas well as the usual niceness that comes with IDEs (debugging, syntax highlighting). How do I\n\n* run `./airflow.sh dev` to install all dev dependencies.\n* `./airflow.sh updev` runs local Airflow with local plugins (as opposed to \npulling them from PyPi)\n* (Pycharm-specific) point PyCharm to your newly-created virtual environment: go to\n`\"Preferences\" -> \"Project: airflow-livy-plugins\" -> \"Project interpreter\", select\n\"Existing environment\"` and pick __python3__ executable from __venv__ folder\n(__venv/bin/python3__)\n* `./airflow.sh cov` - run tests with coverage report \n(will be saved to *htmlcov/*).\n* `./airflow.sh lint` - highlight code style errors.\n* `./airflow.sh format` to reformat all code \n([Black](https://black.readthedocs.io/en/stable/) + \n[isort](https://readthedocs.org/projects/isort/))\n\n### ... debug?\n\n* (Pycharm-specific) Step-by-step debugging with `airflow test` \nand running PySpark batch jobs locally (with debugging as well) \nis supported via run configurations under `.idea/runConfigurations`.\nYou shouldn't have to do anything to use them - just open the folder\nin PyCharm as a project.\n* An example of how a batch can be ran on local Spark:\n```bash\npython ./batches/join_2_files.py \\\n\"file:////Users/vpanov/data/vpanov/bigdata-docker-compose/data/grades.csv\" \\\n\"file:///Users/vpanov/data/vpanov/bigdata-docker-compose/data/ssn-address.tsv\" \\\n-file1_sep=, -file1_header=true \\\n-file1_schema=\"\\`Last name\\` STRING, \\`First name\\` STRING, SSN STRING, Test1 INT, Test2 INT, Test3 INT, Test4 INT, Final INT, Grade STRING\" \\\n-file1_join_column=SSN -file2_header=false \\\n-file2_schema=\"\\`Last name\\` STRING, \\`First name\\` STRING, SSN STRING, Address1 STRING, Address2 STRING\" \\\n-file2_join_column=SSN -output_header=true \\\n-output_columns=\"file1.\\`Last name\\` AS LastName, file1.\\`First name\\` AS FirstName, file1.SSN, file2.Address1, file2.Address2\" \n\n# Optionally append to save result to file\n#-output_path=\"file:///Users/vpanov/livy_batch_example\" \n```\n\n## TODO\n* airflow.sh - replace with modern tools (e.g. pipenv + Docker image)\n* Disable some of flake8 flags for cleaner code\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/panovvv/airflow-livy-plugins", "keywords": "", "license": "MIT License", "maintainer": "", "maintainer_email": "", "name": "airflow-livy-plugins", "package_url": "https://pypi.org/project/airflow-livy-plugins/", "platform": "", "project_url": "https://pypi.org/project/airflow-livy-plugins/", "project_urls": {"Homepage": "https://github.com/panovvv/airflow-livy-plugins"}, "release_url": "https://pypi.org/project/airflow-livy-plugins/0.2/", "requires_dist": null, "requires_python": ">=3.7", "summary": "Plugins for Airflow to run Spark jobs via Livy: sessions and batches", "version": "0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Airflow Livy Plugins</h1>\n<p><a href=\"https://travis-ci.org/panovvv/airflow-livy-plugins\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/07cd4df9f7de0628600efb9f17fd7fd1a8da59bf/68747470733a2f2f7472617669732d63692e6f72672f70616e6f7676762f616972666c6f772d6c6976792d706c7567696e732e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://codecov.io/gh/panovvv/airflow-livy-plugins\" rel=\"nofollow\"><img alt=\"Code coverage\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/dec50a12291cde80d1e9289522a42e9f6265417e/68747470733a2f2f636f6465636f762e696f2f67682f70616e6f7676762f616972666c6f772d6c6976792d706c7567696e732f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a></p>\n<p>Plugins for Airflow to run Spark jobs via Livy:</p>\n<ul>\n<li>Sessions,</li>\n<li>Batches. This mode supports additional verification via Spark/YARN REST API.</li>\n</ul>\n<p>See <a href=\"https://www.shortn0tes.com/2019/08/airflow-spark-livy-sessions-batches.html\" rel=\"nofollow\" title=\"Blog post\">this blog post</a> for more information and detailed comparison of ways to run Spark jobs from Airflow.</p>\n<h2>Directories and files of interest</h2>\n<ul>\n<li><code>airflow_home</code>: example DAGs and plugins for Airflow. Can be used as\nAirflow home path.</li>\n<li><code>batches</code>: Spark jobs code, to be used in Livy batches.</li>\n<li><code>sessions</code>: (Optionally) templated Spark code for Livy sessions.</li>\n<li><code>airflow.sh</code>: helper shell script. Can be used to run sample DAGs,\nprep development environment and more.\nRun it to find out what other commands are available.</li>\n</ul>\n<h2>How do I...</h2>\n<h3>...run the examples?</h3>\n<p>Prerequisites:</p>\n<ul>\n<li>Python 3. Make sure it's installed and in <strong>$PATH</strong></li>\n</ul>\n<p>Now,</p>\n<ol>\n<li>Do you have a Spark cluster with Livy running somewhere?\n<ol>\n<li><em>No</em>. Either get one, or \"mock\" it with\n<a href=\"https://github.com/panovvv/bigdata-docker-compose\" rel=\"nofollow\">my Spark cluster on Docker Compose</a>.</li>\n<li><em>Yes</em>. You're golden!</li>\n</ol>\n</li>\n<li><strong>Optional - this step can be skipped if you're mocking a cluster on your\nmachine</strong>. Open <em>airflow.sh</em>. Inside <code>init_airflow ()</code> function you'll see Airflow\nConnections for Livy, Spark and YARN. Redefine as appropriate.</li>\n<li>run <code>./airflow.sh up</code> to bring up the whole infrastructure.\nAirflow UI will be available at\n<a href=\"http://localhost:8888\" rel=\"nofollow\" title=\"Airflow UI\">localhost:8080</a>.</li>\n<li>Ctrl+C to stop Airflow. Then <code>./airflow.sh down</code> to dispose of\nremaining Airflow processes (shouldn't be needed there if everything goes well).</li>\n</ol>\n<h3>... use it in my project?</h3>\n<pre>pip install airflow-livy-plugins\n</pre>\n<p>Then link or copy the plugin files into <code>$AIRFLOW_HOME/plugins</code>\n(see how I do that in <code>./airflow.sh</code>).\nThey'll get loaded into Airflow via Plugin Manager automatically.\nThis is how you import the plugins:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">airflow.operators</span> <span class=\"kn\">import</span> <span class=\"n\">LivySessionOperator</span>\n<span class=\"kn\">from</span> <span class=\"nn\">airflow.operators</span> <span class=\"kn\">import</span> <span class=\"n\">LivyBatchOperator</span>\n</pre>\n<p>Plugins are loaded at run-time so the imports above will look broken in your IDE,\nbut will work fine in Airflow.\nTake a look at the sample DAGs to see my walkaround :)</p>\n<h3>... set up the development environment?</h3>\n<p>Alright, you want to contribute and need to be able to run the stuff on your machine,\nas well as the usual niceness that comes with IDEs (debugging, syntax highlighting). How do I</p>\n<ul>\n<li>run <code>./airflow.sh dev</code> to install all dev dependencies.</li>\n<li><code>./airflow.sh updev</code> runs local Airflow with local plugins (as opposed to\npulling them from PyPi)</li>\n<li>(Pycharm-specific) point PyCharm to your newly-created virtual environment: go to\n<code>\"Preferences\" -&gt; \"Project: airflow-livy-plugins\" -&gt; \"Project interpreter\", select \"Existing environment\"</code> and pick <strong>python3</strong> executable from <strong>venv</strong> folder\n(<strong>venv/bin/python3</strong>)</li>\n<li><code>./airflow.sh cov</code> - run tests with coverage report\n(will be saved to <em>htmlcov/</em>).</li>\n<li><code>./airflow.sh lint</code> - highlight code style errors.</li>\n<li><code>./airflow.sh format</code> to reformat all code\n(<a href=\"https://black.readthedocs.io/en/stable/\" rel=\"nofollow\">Black</a> +\n<a href=\"https://readthedocs.org/projects/isort/\" rel=\"nofollow\">isort</a>)</li>\n</ul>\n<h3>... debug?</h3>\n<ul>\n<li>(Pycharm-specific) Step-by-step debugging with <code>airflow test</code>\nand running PySpark batch jobs locally (with debugging as well)\nis supported via run configurations under <code>.idea/runConfigurations</code>.\nYou shouldn't have to do anything to use them - just open the folder\nin PyCharm as a project.</li>\n<li>An example of how a batch can be ran on local Spark:</li>\n</ul>\n<pre>python ./batches/join_2_files.py <span class=\"se\">\\</span>\n<span class=\"s2\">\"file:////Users/vpanov/data/vpanov/bigdata-docker-compose/data/grades.csv\"</span> <span class=\"se\">\\</span>\n<span class=\"s2\">\"file:///Users/vpanov/data/vpanov/bigdata-docker-compose/data/ssn-address.tsv\"</span> <span class=\"se\">\\</span>\n-file1_sep<span class=\"o\">=</span>, -file1_header<span class=\"o\">=</span><span class=\"nb\">true</span> <span class=\"se\">\\</span>\n-file1_schema<span class=\"o\">=</span><span class=\"s2\">\"\\`Last name\\` STRING, \\`First name\\` STRING, SSN STRING, Test1 INT, Test2 INT, Test3 INT, Test4 INT, Final INT, Grade STRING\"</span> <span class=\"se\">\\</span>\n-file1_join_column<span class=\"o\">=</span>SSN -file2_header<span class=\"o\">=</span><span class=\"nb\">false</span> <span class=\"se\">\\</span>\n-file2_schema<span class=\"o\">=</span><span class=\"s2\">\"\\`Last name\\` STRING, \\`First name\\` STRING, SSN STRING, Address1 STRING, Address2 STRING\"</span> <span class=\"se\">\\</span>\n-file2_join_column<span class=\"o\">=</span>SSN -output_header<span class=\"o\">=</span><span class=\"nb\">true</span> <span class=\"se\">\\</span>\n-output_columns<span class=\"o\">=</span><span class=\"s2\">\"file1.\\`Last name\\` AS LastName, file1.\\`First name\\` AS FirstName, file1.SSN, file2.Address1, file2.Address2\"</span> \n\n<span class=\"c1\"># Optionally append to save result to file</span>\n<span class=\"c1\">#-output_path=\"file:///Users/vpanov/livy_batch_example\" </span>\n</pre>\n<h2>TODO</h2>\n<ul>\n<li>airflow.sh - replace with modern tools (e.g. pipenv + Docker image)</li>\n<li>Disable some of flake8 flags for cleaner code</li>\n</ul>\n\n          </div>"}, "last_serial": 6740777, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "2b4800dc3c0153816e1100cca919719b", "sha256": "1473278ffae9e1d18e3259859331370523fe2527c55d0be0edc087d60273e23b"}, "downloads": -1, "filename": "airflow_livy_plugins-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "2b4800dc3c0153816e1100cca919719b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 11504, "upload_time": "2020-03-02T23:40:19", "upload_time_iso_8601": "2020-03-02T23:40:19.893028Z", "url": "https://files.pythonhosted.org/packages/f5/33/13784eba63355ec070936e6174ca5a0ea571f857f31159640c72b252b32e/airflow_livy_plugins-0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d46e57eafd810c08e09d47c4f947977a", "sha256": "20019e51521aac89c2ea1128a3f8f7b0d8b90342e19639efcab89ac1d7c44198"}, "downloads": -1, "filename": "airflow-livy-plugins-0.1.tar.gz", "has_sig": false, "md5_digest": "d46e57eafd810c08e09d47c4f947977a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 10138, "upload_time": "2020-03-02T23:40:22", "upload_time_iso_8601": "2020-03-02T23:40:22.537034Z", "url": "https://files.pythonhosted.org/packages/50/07/a911f1358bd73104751336b6b6af4df454c77edbef35992383653389f86c/airflow-livy-plugins-0.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "965dfce910ddd8200bed130a9352b089", "sha256": "d9c36ae1055cdea3cd3f3623fc23f59a7b4f4e4b3b2548f5a2e81d7d1a4849ef"}, "downloads": -1, "filename": "airflow_livy_plugins-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "965dfce910ddd8200bed130a9352b089", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 11794, "upload_time": "2020-03-03T13:50:03", "upload_time_iso_8601": "2020-03-03T13:50:03.690730Z", "url": "https://files.pythonhosted.org/packages/76/e0/23fec6b7aa99becb182bcc4e7b668a9cdb75413c6560870b747d745165f1/airflow_livy_plugins-0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "096835b15795bbcdd92bf21f9d1b8354", "sha256": "e0e6c5da41bc0e782ab4026343f59e8dafddd9dceb76a78bb540e2ec27a7d35a"}, "downloads": -1, "filename": "airflow-livy-plugins-0.2.tar.gz", "has_sig": false, "md5_digest": "096835b15795bbcdd92bf21f9d1b8354", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 10906, "upload_time": "2020-03-03T13:50:05", "upload_time_iso_8601": "2020-03-03T13:50:05.205736Z", "url": "https://files.pythonhosted.org/packages/ad/a3/84a1e876d01d8d09b2bb45426d19b34117978ab016961ceda61b981496fe/airflow-livy-plugins-0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "965dfce910ddd8200bed130a9352b089", "sha256": "d9c36ae1055cdea3cd3f3623fc23f59a7b4f4e4b3b2548f5a2e81d7d1a4849ef"}, "downloads": -1, "filename": "airflow_livy_plugins-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "965dfce910ddd8200bed130a9352b089", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 11794, "upload_time": "2020-03-03T13:50:03", "upload_time_iso_8601": "2020-03-03T13:50:03.690730Z", "url": "https://files.pythonhosted.org/packages/76/e0/23fec6b7aa99becb182bcc4e7b668a9cdb75413c6560870b747d745165f1/airflow_livy_plugins-0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "096835b15795bbcdd92bf21f9d1b8354", "sha256": "e0e6c5da41bc0e782ab4026343f59e8dafddd9dceb76a78bb540e2ec27a7d35a"}, "downloads": -1, "filename": "airflow-livy-plugins-0.2.tar.gz", "has_sig": false, "md5_digest": "096835b15795bbcdd92bf21f9d1b8354", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 10906, "upload_time": "2020-03-03T13:50:05", "upload_time_iso_8601": "2020-03-03T13:50:05.205736Z", "url": "https://files.pythonhosted.org/packages/ad/a3/84a1e876d01d8d09b2bb45426d19b34117978ab016961ceda61b981496fe/airflow-livy-plugins-0.2.tar.gz", "yanked": false}], "timestamp": "Thu May  7 16:20:30 2020"}