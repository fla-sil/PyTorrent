{"info": {"author": "Garrett Credi", "author_email": "gcc@ameritech.net", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Console", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6", "Topic :: Internet :: WWW/HTTP :: Indexing/Search"], "description": "# WikiBot\nWelcome to WikiBot! This is a small program to get a random page from a Wikipedia category AND it's subcategories (up to a specified depth).\n\n# Installation\nAll you need to do it clone this repo and install the dependencies. Make sure you have Pip installed!\n\n```bash\ngit clone https://github.com/ddxtanx/wikiBot\ncd wikiBot\npip install -r\n```\n\nOR\n\n```bash\npip install wikiBot\n```\nTo use as an API\n\n# Usage\n`python wikiBot.py -h` shows the usage of the program.\n```\nusage: wikiBot.py [-h] [--tree_depth [TREE_DEPTH]] [--similarity [SIMILARITY]]\n                  [-s] [-r] [-v] [-c]\n                  category\n\nGet a random page from a wikipedia category\n\npositional arguments:\n  category              The category you wish to get a page from.\n\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --tree_depth [TREE_DEPTH]\n                        How far down to traverse the subcategory tree\n  --similarity [SIMILARITY]\n                        What percent of page categories need to be in\n                        subcategory array. Must be used with -c/--check\n  -s, --save            Save subcategories to a file for quick re-runs\n  -r, --regen           Regenerate the subcategory file\n  -v, --verbose         Print debug lines\n  -c, --check           After finding page check to see that it truly fits in\n                        category\n```\n\nPro Tips:\n* Use a tree_depth of 3 or 4, more than 4 will bring loosely relates categories into subcategories.\n* Use a similarity of .25 or .33. If you want a higher similarity value then you might sacrifice other valid pages in\nsearch for the PERFECT page.\n\nIf you're using it in your own Python code the best way to set it up is\n```python\nfrom wikiBot import WikiBot\nwb = WikiBot({{Your preferred tree_depth}}, {{Your preferred similarity_val}})\n\n\"\"\"\n...\nYour Awesome Code\n...\n\"\"\"\n\nrandomPage = wb.randomPage(category,...)\n```\n\nYou can also change the tree depth and similarity_val by using `wb.td = {{ New Tree Depth}}` and `wb.sv = {{ New Similarity Val}}`\n\nMore info available by using `help(wikiBot)`\n# How It Works\nThe most important part of this program is the Wikipedia API; it allows the program to gather all of the subcategories of a given category in a fast(ish) and usable manner, and to get the pages belonging to a given category. The bulk of my code focuses on iteratively getting the subcategories at a given depth in a tree, adding them to an array with all subcategories of a given 'parent' category, and continuing on in that fashion until there are no more subcategories or the program has fetched to the maximum tree depth allowed. i.e. if a subcategory chain went\n\nCategory A -> Category B -> Category C -> Category D -> ...\n\n(-> denotes 'is a supercategory of')\n\nand the maximum tree depth was 3, then the code would stop gathering subcategories for Category C,D,E...\n\nAfter all subcategories of a given parent category have been amassed in some list L, the program randomly chooses a category C from L, finds the pages belonging to C, chooses a random page P from C and return the URL pointing to P. For speeds sake, after gathering all subcategories from a given parent category the program optionally saves all of them to a text file to find subcategories faster.\n\n\nTo determine how similar a page is to a category, the program first enumerates what categories the page selected belongs to. Then it loops through all of the found categories using a variable I will call A here. It then checks if A belongs to the subcategories generated by the 'parent' category, and computes a 'score' of that page. If it is >= than a prespecified value (Default is .5: half of all A's should be subcategories of parent category) then it is a valid subpage. If not, it removes that page from the category list and loops on.\n\n# Note on types\nThis project uses type annotations and mypy type checking, so you can be sure you are passing the right types to functions. If you're using Atom to edit your code, I recommend using atom-linter-mypy to do type linting. Have fun!\n\n# Contributions\nI'm open to anyone contributing, especially if they know of a way to make this faster or take up less drive space for locally stored subcategories. Email me at gcc@ameritech.net and we can talk stuff out.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/ddxtanx/wiki_bot/archive/1.1.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ddxtanx/wiki_bot", "keywords": "wikipedia,random,generator,command_line,api,mypy,library,wiki", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "wiki-bot", "package_url": "https://pypi.org/project/wiki-bot/", "platform": "", "project_url": "https://pypi.org/project/wiki-bot/", "project_urls": {"Download": "https://github.com/ddxtanx/wiki_bot/archive/1.1.tar.gz", "Homepage": "https://github.com/ddxtanx/wiki_bot"}, "release_url": "https://pypi.org/project/wiki-bot/1.3.1/", "requires_dist": ["requests", "mypy"], "requires_python": "", "summary": "A random wikipedia page generator.", "version": "1.3.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>WikiBot</h1>\n<p>Welcome to WikiBot! This is a small program to get a random page from a Wikipedia category AND it's subcategories (up to a specified depth).</p>\n<h1>Installation</h1>\n<p>All you need to do it clone this repo and install the dependencies. Make sure you have Pip installed!</p>\n<pre>git clone https://github.com/ddxtanx/wikiBot\n<span class=\"nb\">cd</span> wikiBot\npip install -r\n</pre>\n<p>OR</p>\n<pre>pip install wikiBot\n</pre>\n<p>To use as an API</p>\n<h1>Usage</h1>\n<p><code>python wikiBot.py -h</code> shows the usage of the program.</p>\n<pre><code>usage: wikiBot.py [-h] [--tree_depth [TREE_DEPTH]] [--similarity [SIMILARITY]]\n                  [-s] [-r] [-v] [-c]\n                  category\n\nGet a random page from a wikipedia category\n\npositional arguments:\n  category              The category you wish to get a page from.\n\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --tree_depth [TREE_DEPTH]\n                        How far down to traverse the subcategory tree\n  --similarity [SIMILARITY]\n                        What percent of page categories need to be in\n                        subcategory array. Must be used with -c/--check\n  -s, --save            Save subcategories to a file for quick re-runs\n  -r, --regen           Regenerate the subcategory file\n  -v, --verbose         Print debug lines\n  -c, --check           After finding page check to see that it truly fits in\n                        category\n</code></pre>\n<p>Pro Tips:</p>\n<ul>\n<li>Use a tree_depth of 3 or 4, more than 4 will bring loosely relates categories into subcategories.</li>\n<li>Use a similarity of .25 or .33. If you want a higher similarity value then you might sacrifice other valid pages in\nsearch for the PERFECT page.</li>\n</ul>\n<p>If you're using it in your own Python code the best way to set it up is</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">wikiBot</span> <span class=\"kn\">import</span> <span class=\"n\">WikiBot</span>\n<span class=\"n\">wb</span> <span class=\"o\">=</span> <span class=\"n\">WikiBot</span><span class=\"p\">({{</span><span class=\"n\">Your</span> <span class=\"n\">preferred</span> <span class=\"n\">tree_depth</span><span class=\"p\">}},</span> <span class=\"p\">{{</span><span class=\"n\">Your</span> <span class=\"n\">preferred</span> <span class=\"n\">similarity_val</span><span class=\"p\">}})</span>\n\n<span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">...</span>\n<span class=\"sd\">Your Awesome Code</span>\n<span class=\"sd\">...</span>\n<span class=\"sd\">\"\"\"</span>\n\n<span class=\"n\">randomPage</span> <span class=\"o\">=</span> <span class=\"n\">wb</span><span class=\"o\">.</span><span class=\"n\">randomPage</span><span class=\"p\">(</span><span class=\"n\">category</span><span class=\"p\">,</span><span class=\"o\">...</span><span class=\"p\">)</span>\n</pre>\n<p>You can also change the tree depth and similarity_val by using <code>wb.td = {{ New Tree Depth}}</code> and <code>wb.sv = {{ New Similarity Val}}</code></p>\n<p>More info available by using <code>help(wikiBot)</code></p>\n<h1>How It Works</h1>\n<p>The most important part of this program is the Wikipedia API; it allows the program to gather all of the subcategories of a given category in a fast(ish) and usable manner, and to get the pages belonging to a given category. The bulk of my code focuses on iteratively getting the subcategories at a given depth in a tree, adding them to an array with all subcategories of a given 'parent' category, and continuing on in that fashion until there are no more subcategories or the program has fetched to the maximum tree depth allowed. i.e. if a subcategory chain went</p>\n<p>Category A -&gt; Category B -&gt; Category C -&gt; Category D -&gt; ...</p>\n<p>(-&gt; denotes 'is a supercategory of')</p>\n<p>and the maximum tree depth was 3, then the code would stop gathering subcategories for Category C,D,E...</p>\n<p>After all subcategories of a given parent category have been amassed in some list L, the program randomly chooses a category C from L, finds the pages belonging to C, chooses a random page P from C and return the URL pointing to P. For speeds sake, after gathering all subcategories from a given parent category the program optionally saves all of them to a text file to find subcategories faster.</p>\n<p>To determine how similar a page is to a category, the program first enumerates what categories the page selected belongs to. Then it loops through all of the found categories using a variable I will call A here. It then checks if A belongs to the subcategories generated by the 'parent' category, and computes a 'score' of that page. If it is &gt;= than a prespecified value (Default is .5: half of all A's should be subcategories of parent category) then it is a valid subpage. If not, it removes that page from the category list and loops on.</p>\n<h1>Note on types</h1>\n<p>This project uses type annotations and mypy type checking, so you can be sure you are passing the right types to functions. If you're using Atom to edit your code, I recommend using atom-linter-mypy to do type linting. Have fun!</p>\n<h1>Contributions</h1>\n<p>I'm open to anyone contributing, especially if they know of a way to make this faster or take up less drive space for locally stored subcategories. Email me at <a href=\"mailto:gcc@ameritech.net\">gcc@ameritech.net</a> and we can talk stuff out.</p>\n\n          </div>"}, "last_serial": 3846190, "releases": {"1.0": [{"comment_text": "", "digests": {"md5": "6c66b25e199097d3ca3d1619160cf5af", "sha256": "92e6add4ef6409dc336bdbd8b9e220bd7da810fae4e9cc77f5fe5e103ee8b816"}, "downloads": -1, "filename": "wiki_bot-1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "6c66b25e199097d3ca3d1619160cf5af", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4631, "upload_time": "2018-05-08T00:49:39", "upload_time_iso_8601": "2018-05-08T00:49:39.238654Z", "url": "https://files.pythonhosted.org/packages/e0/d8/247b592b0c6d92764fc1627b7b342232d8e94aaca31adb71108dba7552e8/wiki_bot-1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bff952f8cc24772bfdaa4211d72fab7b", "sha256": "8e89893752896563333fcef5b8db82bad2f4354983ecf2723ce95de00014983b"}, "downloads": -1, "filename": "wiki_bot-1.0.tar.gz", "has_sig": false, "md5_digest": "bff952f8cc24772bfdaa4211d72fab7b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6585, "upload_time": "2018-05-08T00:50:15", "upload_time_iso_8601": "2018-05-08T00:50:15.228485Z", "url": "https://files.pythonhosted.org/packages/bc/22/746de4cfc87b0f2371e71f574d73bd2ce79d7316bf7da98e7155a4f98f67/wiki_bot-1.0.tar.gz", "yanked": false}], "1.1": [{"comment_text": "", "digests": {"md5": "cbad5db5fe1dcd12847eb57eefd4f2dc", "sha256": "d37b33620015380064d7f8a248b98f07063aa4098fa1f912ee03014e7fe63f2d"}, "downloads": -1, "filename": "wiki_bot-1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "cbad5db5fe1dcd12847eb57eefd4f2dc", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4646, "upload_time": "2018-05-08T01:43:05", "upload_time_iso_8601": "2018-05-08T01:43:05.919619Z", "url": "https://files.pythonhosted.org/packages/3d/ca/2e8502bf00cacaf9ceae7b1ef0568e33aee79153424ae82399744d534c90/wiki_bot-1.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1dade21eb63454bd6e63be174e2b4c48", "sha256": "5d51df08380089c527bdb9320dd0d53767b0e984c0e10e3121069ee611f5c21a"}, "downloads": -1, "filename": "wiki_bot-1.1.tar.gz", "has_sig": false, "md5_digest": "1dade21eb63454bd6e63be174e2b4c48", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6598, "upload_time": "2018-05-08T01:43:07", "upload_time_iso_8601": "2018-05-08T01:43:07.168438Z", "url": "https://files.pythonhosted.org/packages/0e/2f/c654a8ba4411279d8723ea659ea04f75b83b9054618983602fdfd95edf97/wiki_bot-1.1.tar.gz", "yanked": false}], "1.2": [{"comment_text": "", "digests": {"md5": "dd68b9780ee4a108ce1f268dd72f80e7", "sha256": "4aa2542d5310d7ec213554560c8800c5bd8bfc0a5846d985764247a99762c1c7"}, "downloads": -1, "filename": "wiki_bot-1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "dd68b9780ee4a108ce1f268dd72f80e7", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4876, "upload_time": "2018-05-09T02:16:18", "upload_time_iso_8601": "2018-05-09T02:16:18.080771Z", "url": "https://files.pythonhosted.org/packages/71/6f/5265feffaa5b159d7499f7fe3074e085486296e5a94b8e6f26b8e3e9825d/wiki_bot-1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3acb64e8b715e2a2991397df7a752bf4", "sha256": "5e993c0e971cb8598372b6a67395e71bff64a205c7052f03bbf8681389dd6f0b"}, "downloads": -1, "filename": "wiki_bot-1.2.tar.gz", "has_sig": false, "md5_digest": "3acb64e8b715e2a2991397df7a752bf4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7000, "upload_time": "2018-05-09T02:16:19", "upload_time_iso_8601": "2018-05-09T02:16:19.276037Z", "url": "https://files.pythonhosted.org/packages/1e/56/09b61d460be2a77a13fd5af0639dfe4c5868673c3590be66bef683241f1c/wiki_bot-1.2.tar.gz", "yanked": false}], "1.3.1": [{"comment_text": "", "digests": {"md5": "1b6d31ffc1f2a620ef9f8f7ec1cf4c1f", "sha256": "4de1e97143dd99c5877273d660070fd2224b1f1a291a53202419209dfe4272e3"}, "downloads": -1, "filename": "wiki_bot-1.3.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "1b6d31ffc1f2a620ef9f8f7ec1cf4c1f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 6662, "upload_time": "2018-05-09T02:27:40", "upload_time_iso_8601": "2018-05-09T02:27:40.633706Z", "url": "https://files.pythonhosted.org/packages/87/00/a76d9f71331210e1067a15de815e8ca4869e7bfabf1aa91a4b348db85c3b/wiki_bot-1.3.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e5923cd943743e02ddd87df8b6bf8e4a", "sha256": "ad03ba8265f3a5a425a09cf705b8a2490d554b2e9ab9d93a1345b80bf73c1b16"}, "downloads": -1, "filename": "wiki_bot-1.3.1.tar.gz", "has_sig": false, "md5_digest": "e5923cd943743e02ddd87df8b6bf8e4a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7356, "upload_time": "2018-05-09T02:27:41", "upload_time_iso_8601": "2018-05-09T02:27:41.810637Z", "url": "https://files.pythonhosted.org/packages/79/e6/dc55ef257c28d503e1606ebc3a763e595688a8fd0b03b2b4761d371d7da3/wiki_bot-1.3.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1b6d31ffc1f2a620ef9f8f7ec1cf4c1f", "sha256": "4de1e97143dd99c5877273d660070fd2224b1f1a291a53202419209dfe4272e3"}, "downloads": -1, "filename": "wiki_bot-1.3.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "1b6d31ffc1f2a620ef9f8f7ec1cf4c1f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 6662, "upload_time": "2018-05-09T02:27:40", "upload_time_iso_8601": "2018-05-09T02:27:40.633706Z", "url": "https://files.pythonhosted.org/packages/87/00/a76d9f71331210e1067a15de815e8ca4869e7bfabf1aa91a4b348db85c3b/wiki_bot-1.3.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e5923cd943743e02ddd87df8b6bf8e4a", "sha256": "ad03ba8265f3a5a425a09cf705b8a2490d554b2e9ab9d93a1345b80bf73c1b16"}, "downloads": -1, "filename": "wiki_bot-1.3.1.tar.gz", "has_sig": false, "md5_digest": "e5923cd943743e02ddd87df8b6bf8e4a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7356, "upload_time": "2018-05-09T02:27:41", "upload_time_iso_8601": "2018-05-09T02:27:41.810637Z", "url": "https://files.pythonhosted.org/packages/79/e6/dc55ef257c28d503e1606ebc3a763e595688a8fd0b03b2b4761d371d7da3/wiki_bot-1.3.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:29:20 2020"}