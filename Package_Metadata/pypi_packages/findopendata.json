{"info": {"author": "Eric Zhu", "author_email": "", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Find Open Data\n\n[![Build Status](https://travis-ci.org/findopendata/findopendata.svg?branch=master)](https://travis-ci.org/findopendata/findopendata)\n\n![Screenshot](screencapture.gif)\n\nTable of Content:\n1. [Introduction](#introduction)\n2. [System Overview](#system-overview)\n3. [Development Guide](#development-guide)\n4. [Cloud Storage Systems](#cloud-storage-systems)\n5. [Crawler Guide](#crawler-guide)\n\n## Introduction\n\nThis is the source code repository for [findopendata.com](https://findopendata.com).\nThe project goal is to make a search engine for Open Data with rich \nfeatures beyond simple keyword search. The current search methods are:\n\n* Keyword search based on metadata\n* Similar dataset search based on metadata similarity\n* Joinable table search based on content (i.e., data values) similarity using LSH index\n\nNext steps:\n\n * Unionable/similar table search based on content similarity\n * Time and location-based serach based on extracted timestamps and Geo tags\n * Dataset versioning\n * API for external data science tools (e.g., Jupyter Notebook, Plot.ly)\n\n**This is a work in progress.**\n\n\n## System Overview\n\nThe Find Open Data system has the following components:\n\n1. **Frontend**: a React app, located in `frontend`.\n2. **API Server**: a Flask web server, located in `apiserver`.\n3. **LSH Server**: a Go web server, located in `lshserver`.\n4. **Crawler**: a set of [Celery](https://docs.celeryproject.org/en/latest/userguide/tasks.html) tasks, located in `findopendata`. \n\nThe Frontend, the API Server, and the LSH Server can be \ndeployed to \n[Google App Engine](https://cloud.google.com/appengine/docs/).\n\nWe also use two external storage systems for persistence:\n\n1. A PostgreSQL database for storing dataset registry, metadata, and sketches for content-based search.\n2. A cloud-based storage system for storing dataset files, currently supporting Google Cloud Storage and Azure Blob Storage. A local storage using file system is also available.\n\n![System Overview](system_overview.png)\n\n## Development Guide\n\nTo develop locally, you need the following:\n\n* PostgreSQL 9.6 or above\n* RabbitMQ\n\n#### 1. Install PostgreSQL\n\n[PostgreSQL](https://www.postgresql.org/download/) \n(version 9.6 or above) is used by the crawler to register and save the\nsummaries of crawled datasets. It is also used by the API Server as the \ndatabase backend.\nIf you are using Cloud SQL Postgres, you need to download \n[Cloud SQL Proxy](https://cloud.google.com/sql/docs/postgres/connect-admin-proxy#install)\nand make it executable.\n\nOnce the PostgreSQL database is running, create a database, and\nuse the SQL scripts in `sql` to create tables:\n```\npsql -f sql/create_crawler_tables.sql\npsql -f sql/create_metadata_tables.sql\npsql -f sql/create_sketch_tables.sql\n```\n\n#### 2. Install RabbitMQ\n\n[RabbitMQ](https://www.rabbitmq.com/download.html) \nis required to manage and queue crawl tasks.\nOn Mac OS X you can [install it using Homebrew](https://www.rabbitmq.com/install-homebrew.html).\n\nRun the RabbitMQ server after finishing install.\n\n#### 3. Python Environment\n\nIt is recommended to use virtualenv for Python development and dependencies:\n```\nvirtualenv -p python3 .venv\nsource .venv/bin/activate # .\\venv\\bin\\activate on Windows\n```\n\n`python-snappy` requires `libsnappy`. On Ubuntu you can \nsimply install it by `sudo apt-get install libsnappy-dev`.\nOn Mac OS X use `brew install snappy`.\nOn Windows, instead of the `python-snappy` binary on Pypi, use the \nunofficial binary maintained by UC Irvine \n([download here](https://www.lfd.uci.edu/~gohlke/pythonlibs/)),\nand install directly, for example (Python 3.7, amd64):\n```\npip install python_snappy\u00e2\u20ac\u20180.5.4\u00e2\u20ac\u2018cp37\u00e2\u20ac\u2018cp37m\u00e2\u20ac\u2018win_amd64.whl\n```\n\nFinally, install this package and other dependencies:\n```\npip install -e .\n```\n\n#### 4. Configuration File\n\nCreate a `configs.yaml` by copying `configs-example.yaml`, complete fields\nrelated to PostgreSQL and storage.\n\nIf you plan to store all datasets on your local file system,\nyou can skip the `gcp` and `azure` sections and only complete \nthe `local` section, and make sure the `storage.provider` is \nset to `local`.\n\nFor cloud-based storage systems, see \n[Cloud Storage Systems](#cloud-storage-systems).\n\n## Cloud Storage Systems\n\nCurrently we support using \n[Google Cloud Storage](https://cloud.google.com/storage/) and \n[Azure Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) \nas the dataset storage system.\n\nTo use Google Cloud Storage, you need:\n* A Google Cloud project with Cloud Storage enabled, and a bucket created.\n* A Google Cloud service account key file (JSON formatted) with read and write access to the Cloud Storage bucket.\n* Set `storage.provider` to `gcp` in `configs.yaml`.\n\nTo use Azure Blob Storage, you need:\n* An Azure storage account enabled, and a blob storage container created.\n* A connection string to access the storage account.\n* Set `storage.provider` to `azure` in `configs.yaml`.\n\n## Crawler Guide\n\nThe crawler has a set of [Celery](http://www.celeryproject.org/) tasks that \nruns in parallel.\nIt uses the RabbitMQ server to manage and queue the tasks.\n\n### Setup Crawler\n\n#### Data Sources (CKAN and Socrata APIs)\n\nThe crawler uses PostgreSQL to maintain all data sources.\nCKAN sources are maintained in the table `findopendata.ckan_apis`.\nSocrata Discovery APIs are maintained in the table \n`findopendata.socrata_discovery_apis`.\nThe SQL script `sql/create_crawler_tables.sql` has already created some \ninitial sources for you.\n\nTo show the CKAN APIs currently available to the crawler and whether they\nare enabled:\n```sql\nSELECT * FROM findopendata.ckan_apis;\n```\n\nTo add a new CKAN API and enable it:\n```sql\nINSERT INTO findopendata.ckan_apis (endpoint, name, region, enabled) VALUES\n('catalog.data.gov', 'US Open Data', 'United States', true);\n```\n\n#### Socrata App Tokens\n\nAdd your [Socrata app tokens](https://dev.socrata.com/docs/app-tokens.html) \nto the table `findopendata.socrata_app_tokens`.\nThe app tokens are required for harvesting datasets from Socrata APIs.\n\nFor example:\n```sql\nINSERT INTO findopendata.socrata_app_tokens (token) VALUES ('<your app token>');\n```\n\n### Run Crawler\n\n[Celery workers](https://docs.celeryproject.org/en/latest/userguide/workers.html) \nare processes that fetch crawler tasks from RabbitMQ and execute them.\nThe worker processes must be started before starting any tasks.\n\nFor example:\n```\ncelery -A findopendata worker -l info -Ofair\n```\n\nOn Windows there are some issues with using prefork process pool.\nUse `gevent` instead:\n```\ncelery -A findopendata worker -l info -Ofair -P gevent\n```\n\n#### Harvest Datasets\n\nRun `harvest_datasets.py` to start data harvesting tasks that download \ndatasets from various data sources. Downloaded datasets will be stored on\na Google Cloud Storage bucket (set in `configs.yaml`), and registed in \nPostgres tables \n`findopendata.ckan_packages` and `findopendata.socrata_resources`.\n\n#### Generate Metadata\n\nRun `generate_metadata.py` to start metadata generation tasks for \ndownloaded and registed datasets in \n`findopendata.ckan_packages` and `findopendata.socrata_resources`\ntables.\n\nIt generates metadata by extracting titles, description etc. and \nannotates them with entities for enrichment.\nThe metadata is stored in table `findopendata.packages`, which is \nalso used by the API server to serve the frontend.\n\n#### Sketch Dataset Content\n\nRun `sketch_dataset_content.py` to start tasks for creating \nsketches (e.g., \n[MinHash](https://github.com/ekzhu/datasketch),\nsamples, data types, etc.) of dataset\ncontent (i.e., data values, columns, and records).\nThe sketches will be used for content-based search such as\nfinding joinable tables.\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/findopendata/findopendata", "keywords": "open-data search-engine", "license": "", "maintainer": "", "maintainer_email": "", "name": "findopendata", "package_url": "https://pypi.org/project/findopendata/", "platform": "", "project_url": "https://pypi.org/project/findopendata/", "project_urls": {"Homepage": "https://github.com/findopendata/findopendata"}, "release_url": "https://pypi.org/project/findopendata/1.0.3/", "requires_dist": ["requests (>=2.22.0)", "google-cloud-storage (>=1.17.0)", "azure-storage (>=0.36.0)", "google-auth (>=1.6.3)", "gcsfs (>=0.3.0)", "celery (>=4.3.0)", "psycopg2-binary (>=2.7.5)", "Django (>=2.2.3)", "rfc6266-content-disposition (>=0.0.6)", "simplejson (>=3.16.0)", "genson (>=1.1.0)", "fastavro (>=0.22.3)", "python-snappy (>=0.5.4)", "python-dateutil (>=2.8.0)", "datasketch (>=1.4.10)", "pyfarmhash (>=0.2.2)", "cchardet (>=2.1.4)", "spacy (>=2.1.8)", "beautifulsoup4 (>=4.8.0)", "pyyaml (>=5.1.2)", "gevent (>=1.4.0)", "nose2 (>=0.9.1) ; extra == 'test'"], "requires_python": ">=3.6", "summary": "A search engine for Open Data.", "version": "1.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Find Open Data</h1>\n<p><a href=\"https://travis-ci.org/findopendata/findopendata\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/db769297757caa329cf00186a16df98b37653e2e/68747470733a2f2f7472617669732d63692e6f72672f66696e646f70656e646174612f66696e646f70656e646174612e7376673f6272616e63683d6d6173746572\"></a></p>\n<p><img alt=\"Screenshot\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8292530c89946ec863401dc367c2311d17dcb396/73637265656e636170747572652e676966\"></p>\n<p>Table of Content:</p>\n<ol>\n<li><a href=\"#introduction\" rel=\"nofollow\">Introduction</a></li>\n<li><a href=\"#system-overview\" rel=\"nofollow\">System Overview</a></li>\n<li><a href=\"#development-guide\" rel=\"nofollow\">Development Guide</a></li>\n<li><a href=\"#cloud-storage-systems\" rel=\"nofollow\">Cloud Storage Systems</a></li>\n<li><a href=\"#crawler-guide\" rel=\"nofollow\">Crawler Guide</a></li>\n</ol>\n<h2>Introduction</h2>\n<p>This is the source code repository for <a href=\"https://findopendata.com\" rel=\"nofollow\">findopendata.com</a>.\nThe project goal is to make a search engine for Open Data with rich\nfeatures beyond simple keyword search. The current search methods are:</p>\n<ul>\n<li>Keyword search based on metadata</li>\n<li>Similar dataset search based on metadata similarity</li>\n<li>Joinable table search based on content (i.e., data values) similarity using LSH index</li>\n</ul>\n<p>Next steps:</p>\n<ul>\n<li>Unionable/similar table search based on content similarity</li>\n<li>Time and location-based serach based on extracted timestamps and Geo tags</li>\n<li>Dataset versioning</li>\n<li>API for external data science tools (e.g., Jupyter Notebook, Plot.ly)</li>\n</ul>\n<p><strong>This is a work in progress.</strong></p>\n<h2>System Overview</h2>\n<p>The Find Open Data system has the following components:</p>\n<ol>\n<li><strong>Frontend</strong>: a React app, located in <code>frontend</code>.</li>\n<li><strong>API Server</strong>: a Flask web server, located in <code>apiserver</code>.</li>\n<li><strong>LSH Server</strong>: a Go web server, located in <code>lshserver</code>.</li>\n<li><strong>Crawler</strong>: a set of <a href=\"https://docs.celeryproject.org/en/latest/userguide/tasks.html\" rel=\"nofollow\">Celery</a> tasks, located in <code>findopendata</code>.</li>\n</ol>\n<p>The Frontend, the API Server, and the LSH Server can be\ndeployed to\n<a href=\"https://cloud.google.com/appengine/docs/\" rel=\"nofollow\">Google App Engine</a>.</p>\n<p>We also use two external storage systems for persistence:</p>\n<ol>\n<li>A PostgreSQL database for storing dataset registry, metadata, and sketches for content-based search.</li>\n<li>A cloud-based storage system for storing dataset files, currently supporting Google Cloud Storage and Azure Blob Storage. A local storage using file system is also available.</li>\n</ol>\n<p><img alt=\"System Overview\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9f283a392b77975ad8c147c728059471553e1857/73797374656d5f6f766572766965772e706e67\"></p>\n<h2>Development Guide</h2>\n<p>To develop locally, you need the following:</p>\n<ul>\n<li>PostgreSQL 9.6 or above</li>\n<li>RabbitMQ</li>\n</ul>\n<h4>1. Install PostgreSQL</h4>\n<p><a href=\"https://www.postgresql.org/download/\" rel=\"nofollow\">PostgreSQL</a>\n(version 9.6 or above) is used by the crawler to register and save the\nsummaries of crawled datasets. It is also used by the API Server as the\ndatabase backend.\nIf you are using Cloud SQL Postgres, you need to download\n<a href=\"https://cloud.google.com/sql/docs/postgres/connect-admin-proxy#install\" rel=\"nofollow\">Cloud SQL Proxy</a>\nand make it executable.</p>\n<p>Once the PostgreSQL database is running, create a database, and\nuse the SQL scripts in <code>sql</code> to create tables:</p>\n<pre><code>psql -f sql/create_crawler_tables.sql\npsql -f sql/create_metadata_tables.sql\npsql -f sql/create_sketch_tables.sql\n</code></pre>\n<h4>2. Install RabbitMQ</h4>\n<p><a href=\"https://www.rabbitmq.com/download.html\" rel=\"nofollow\">RabbitMQ</a>\nis required to manage and queue crawl tasks.\nOn Mac OS X you can <a href=\"https://www.rabbitmq.com/install-homebrew.html\" rel=\"nofollow\">install it using Homebrew</a>.</p>\n<p>Run the RabbitMQ server after finishing install.</p>\n<h4>3. Python Environment</h4>\n<p>It is recommended to use virtualenv for Python development and dependencies:</p>\n<pre><code>virtualenv -p python3 .venv\nsource .venv/bin/activate # .\\venv\\bin\\activate on Windows\n</code></pre>\n<p><code>python-snappy</code> requires <code>libsnappy</code>. On Ubuntu you can\nsimply install it by <code>sudo apt-get install libsnappy-dev</code>.\nOn Mac OS X use <code>brew install snappy</code>.\nOn Windows, instead of the <code>python-snappy</code> binary on Pypi, use the\nunofficial binary maintained by UC Irvine\n(<a href=\"https://www.lfd.uci.edu/%7Egohlke/pythonlibs/\" rel=\"nofollow\">download here</a>),\nand install directly, for example (Python 3.7, amd64):</p>\n<pre><code>pip install python_snappy\u00e2\u20ac\u20180.5.4\u00e2\u20ac\u2018cp37\u00e2\u20ac\u2018cp37m\u00e2\u20ac\u2018win_amd64.whl\n</code></pre>\n<p>Finally, install this package and other dependencies:</p>\n<pre><code>pip install -e .\n</code></pre>\n<h4>4. Configuration File</h4>\n<p>Create a <code>configs.yaml</code> by copying <code>configs-example.yaml</code>, complete fields\nrelated to PostgreSQL and storage.</p>\n<p>If you plan to store all datasets on your local file system,\nyou can skip the <code>gcp</code> and <code>azure</code> sections and only complete\nthe <code>local</code> section, and make sure the <code>storage.provider</code> is\nset to <code>local</code>.</p>\n<p>For cloud-based storage systems, see\n<a href=\"#cloud-storage-systems\" rel=\"nofollow\">Cloud Storage Systems</a>.</p>\n<h2>Cloud Storage Systems</h2>\n<p>Currently we support using\n<a href=\"https://cloud.google.com/storage/\" rel=\"nofollow\">Google Cloud Storage</a> and\n<a href=\"https://azure.microsoft.com/en-us/services/storage/blobs/\" rel=\"nofollow\">Azure Blob Storage</a>\nas the dataset storage system.</p>\n<p>To use Google Cloud Storage, you need:</p>\n<ul>\n<li>A Google Cloud project with Cloud Storage enabled, and a bucket created.</li>\n<li>A Google Cloud service account key file (JSON formatted) with read and write access to the Cloud Storage bucket.</li>\n<li>Set <code>storage.provider</code> to <code>gcp</code> in <code>configs.yaml</code>.</li>\n</ul>\n<p>To use Azure Blob Storage, you need:</p>\n<ul>\n<li>An Azure storage account enabled, and a blob storage container created.</li>\n<li>A connection string to access the storage account.</li>\n<li>Set <code>storage.provider</code> to <code>azure</code> in <code>configs.yaml</code>.</li>\n</ul>\n<h2>Crawler Guide</h2>\n<p>The crawler has a set of <a href=\"http://www.celeryproject.org/\" rel=\"nofollow\">Celery</a> tasks that\nruns in parallel.\nIt uses the RabbitMQ server to manage and queue the tasks.</p>\n<h3>Setup Crawler</h3>\n<h4>Data Sources (CKAN and Socrata APIs)</h4>\n<p>The crawler uses PostgreSQL to maintain all data sources.\nCKAN sources are maintained in the table <code>findopendata.ckan_apis</code>.\nSocrata Discovery APIs are maintained in the table\n<code>findopendata.socrata_discovery_apis</code>.\nThe SQL script <code>sql/create_crawler_tables.sql</code> has already created some\ninitial sources for you.</p>\n<p>To show the CKAN APIs currently available to the crawler and whether they\nare enabled:</p>\n<pre><span class=\"k\">SELECT</span> <span class=\"o\">*</span> <span class=\"k\">FROM</span> <span class=\"n\">findopendata</span><span class=\"p\">.</span><span class=\"n\">ckan_apis</span><span class=\"p\">;</span>\n</pre>\n<p>To add a new CKAN API and enable it:</p>\n<pre><span class=\"k\">INSERT</span> <span class=\"k\">INTO</span> <span class=\"n\">findopendata</span><span class=\"p\">.</span><span class=\"n\">ckan_apis</span> <span class=\"p\">(</span><span class=\"n\">endpoint</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">region</span><span class=\"p\">,</span> <span class=\"n\">enabled</span><span class=\"p\">)</span> <span class=\"k\">VALUES</span>\n<span class=\"p\">(</span><span class=\"s1\">'catalog.data.gov'</span><span class=\"p\">,</span> <span class=\"s1\">'US Open Data'</span><span class=\"p\">,</span> <span class=\"s1\">'United States'</span><span class=\"p\">,</span> <span class=\"k\">true</span><span class=\"p\">);</span>\n</pre>\n<h4>Socrata App Tokens</h4>\n<p>Add your <a href=\"https://dev.socrata.com/docs/app-tokens.html\" rel=\"nofollow\">Socrata app tokens</a>\nto the table <code>findopendata.socrata_app_tokens</code>.\nThe app tokens are required for harvesting datasets from Socrata APIs.</p>\n<p>For example:</p>\n<pre><span class=\"k\">INSERT</span> <span class=\"k\">INTO</span> <span class=\"n\">findopendata</span><span class=\"p\">.</span><span class=\"n\">socrata_app_tokens</span> <span class=\"p\">(</span><span class=\"n\">token</span><span class=\"p\">)</span> <span class=\"k\">VALUES</span> <span class=\"p\">(</span><span class=\"s1\">'&lt;your app token&gt;'</span><span class=\"p\">);</span>\n</pre>\n<h3>Run Crawler</h3>\n<p><a href=\"https://docs.celeryproject.org/en/latest/userguide/workers.html\" rel=\"nofollow\">Celery workers</a>\nare processes that fetch crawler tasks from RabbitMQ and execute them.\nThe worker processes must be started before starting any tasks.</p>\n<p>For example:</p>\n<pre><code>celery -A findopendata worker -l info -Ofair\n</code></pre>\n<p>On Windows there are some issues with using prefork process pool.\nUse <code>gevent</code> instead:</p>\n<pre><code>celery -A findopendata worker -l info -Ofair -P gevent\n</code></pre>\n<h4>Harvest Datasets</h4>\n<p>Run <code>harvest_datasets.py</code> to start data harvesting tasks that download\ndatasets from various data sources. Downloaded datasets will be stored on\na Google Cloud Storage bucket (set in <code>configs.yaml</code>), and registed in\nPostgres tables\n<code>findopendata.ckan_packages</code> and <code>findopendata.socrata_resources</code>.</p>\n<h4>Generate Metadata</h4>\n<p>Run <code>generate_metadata.py</code> to start metadata generation tasks for\ndownloaded and registed datasets in\n<code>findopendata.ckan_packages</code> and <code>findopendata.socrata_resources</code>\ntables.</p>\n<p>It generates metadata by extracting titles, description etc. and\nannotates them with entities for enrichment.\nThe metadata is stored in table <code>findopendata.packages</code>, which is\nalso used by the API server to serve the frontend.</p>\n<h4>Sketch Dataset Content</h4>\n<p>Run <code>sketch_dataset_content.py</code> to start tasks for creating\nsketches (e.g.,\n<a href=\"https://github.com/ekzhu/datasketch\" rel=\"nofollow\">MinHash</a>,\nsamples, data types, etc.) of dataset\ncontent (i.e., data values, columns, and records).\nThe sketches will be used for content-based search such as\nfinding joinable tables.</p>\n\n          </div>"}, "last_serial": 6363076, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "a73920742394cf8ff0c2624d4407c8d6", "sha256": "23c0a1b5a4c6af8616e7a57d89aff973fa1c414a71680a1ec6f39f28ebd63afe"}, "downloads": -1, "filename": "findopendata-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "a73920742394cf8ff0c2624d4407c8d6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 26854, "upload_time": "2019-11-14T18:12:21", "upload_time_iso_8601": "2019-11-14T18:12:21.425241Z", "url": "https://files.pythonhosted.org/packages/b8/b9/dbe40406319ac827d9241e1c658eab340e3b9952d760006b9ce6137145de/findopendata-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "424968fd9214cf91ad622e8ff927db70", "sha256": "519a2770c60bfd53985bb6998825c8c5b31f8441b327ff82134bebbb1bb8ab34"}, "downloads": -1, "filename": "findopendata-1.0.0.tar.gz", "has_sig": false, "md5_digest": "424968fd9214cf91ad622e8ff927db70", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 21084, "upload_time": "2019-11-14T18:12:23", "upload_time_iso_8601": "2019-11-14T18:12:23.621224Z", "url": "https://files.pythonhosted.org/packages/7d/f5/5c3c4cf8ca7789c95474e084c767c874b9c928260ce21ac8e37c034b8089/findopendata-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "4ad90305f3f9c1176e8422f839b970aa", "sha256": "e447d621ba94b56a7d569d16c18b85aeb8ae36e0e5c474ff0a182077991a24a4"}, "downloads": -1, "filename": "findopendata-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "4ad90305f3f9c1176e8422f839b970aa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 26905, "upload_time": "2019-12-19T21:22:59", "upload_time_iso_8601": "2019-12-19T21:22:59.329053Z", "url": "https://files.pythonhosted.org/packages/5e/5d/32305386680a40afa190fbfdaf39987a597c4159c1ce389e9ef214aaff52/findopendata-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fa823c03dd6c6cb883fbdca270df849a", "sha256": "44afa2767f9e24ec30a390b771f18c1c7c0938e5146a2d21610819d283686e03"}, "downloads": -1, "filename": "findopendata-1.0.1.tar.gz", "has_sig": false, "md5_digest": "fa823c03dd6c6cb883fbdca270df849a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 21204, "upload_time": "2019-12-19T21:23:01", "upload_time_iso_8601": "2019-12-19T21:23:01.311325Z", "url": "https://files.pythonhosted.org/packages/47/b3/e1510a0ea7141bea2cf669225c43f722422e912344e8d87b4682ff1a5717/findopendata-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "9124ee12805b492c8107bc52304154ab", "sha256": "40a14bc90db1bdaecb97e4449b376ee218945d63f4ce9278cf8cea1a48d59165"}, "downloads": -1, "filename": "findopendata-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "9124ee12805b492c8107bc52304154ab", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 39494, "upload_time": "2019-12-20T01:06:09", "upload_time_iso_8601": "2019-12-20T01:06:09.960023Z", "url": "https://files.pythonhosted.org/packages/c0/16/b26946ea67145837ca09e8eb7cb87278b8da242ef14bc67cf144ff5992ad/findopendata-1.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2c99f3f265cbf903ac22132ca804eb73", "sha256": "6e23b43b665e50d4fc51df486bf7c1b6b0d56800b3527f20377c11df30978e9c"}, "downloads": -1, "filename": "findopendata-1.0.2.tar.gz", "has_sig": false, "md5_digest": "2c99f3f265cbf903ac22132ca804eb73", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 31837, "upload_time": "2019-12-20T01:03:50", "upload_time_iso_8601": "2019-12-20T01:03:50.370621Z", "url": "https://files.pythonhosted.org/packages/85/ce/09dc57da02dcb18cb6d41ba8947fa25eb62795586b3245d7c6c100be2d14/findopendata-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "baaaa2f3f7f521b1aed20ba46578a7f2", "sha256": "8faeb26cac1d84136c2696775b9b0185993af1e2a815a0d924e79a6810ebfb05"}, "downloads": -1, "filename": "findopendata-1.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "baaaa2f3f7f521b1aed20ba46578a7f2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 39794, "upload_time": "2019-12-26T22:32:31", "upload_time_iso_8601": "2019-12-26T22:32:31.731767Z", "url": "https://files.pythonhosted.org/packages/e5/cd/b49c01d78619a65dd634aebfb73dffa6cb37f5081c2998addacb4f106b8f/findopendata-1.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "652d26ac0e2db5a2a76a0ab664234bc3", "sha256": "eaf4e138bb5b7222cd40a69571d81a5e1195de6dc702cdb7896c7c341ab081c3"}, "downloads": -1, "filename": "findopendata-1.0.3.tar.gz", "has_sig": false, "md5_digest": "652d26ac0e2db5a2a76a0ab664234bc3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32165, "upload_time": "2019-12-26T22:32:33", "upload_time_iso_8601": "2019-12-26T22:32:33.218904Z", "url": "https://files.pythonhosted.org/packages/cf/ed/2a30e7e1d5d479590a757729f0d3a952d19c56d812d25b891e90bb325ea3/findopendata-1.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "baaaa2f3f7f521b1aed20ba46578a7f2", "sha256": "8faeb26cac1d84136c2696775b9b0185993af1e2a815a0d924e79a6810ebfb05"}, "downloads": -1, "filename": "findopendata-1.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "baaaa2f3f7f521b1aed20ba46578a7f2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 39794, "upload_time": "2019-12-26T22:32:31", "upload_time_iso_8601": "2019-12-26T22:32:31.731767Z", "url": "https://files.pythonhosted.org/packages/e5/cd/b49c01d78619a65dd634aebfb73dffa6cb37f5081c2998addacb4f106b8f/findopendata-1.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "652d26ac0e2db5a2a76a0ab664234bc3", "sha256": "eaf4e138bb5b7222cd40a69571d81a5e1195de6dc702cdb7896c7c341ab081c3"}, "downloads": -1, "filename": "findopendata-1.0.3.tar.gz", "has_sig": false, "md5_digest": "652d26ac0e2db5a2a76a0ab664234bc3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32165, "upload_time": "2019-12-26T22:32:33", "upload_time_iso_8601": "2019-12-26T22:32:33.218904Z", "url": "https://files.pythonhosted.org/packages/cf/ed/2a30e7e1d5d479590a757729f0d3a952d19c56d812d25b891e90bb325ea3/findopendata-1.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:42:24 2020"}