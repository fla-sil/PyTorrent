{"info": {"author": "Kensuke Mitsuzawa", "author_email": "kensuke.mit@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "|MIT License|\\ |Build Status|\n\nWhat\u2019s this?\n============\n\nThis is simple python-wrapper for Japanese Tokenizers(A.K.A Tokenizer)\n\nThis project aims to call tokenizers and split a sentence into tokens as\neasy as possible.\n\nAnd, this project supports various Tokenization tools common interface.\nThus, it\u2019s easy to compare output from various tokenizers.\n\nThis project is available also in\n`Github <https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers>`__.\n\nIf you find any bugs, please report them to github issues. Or any pull\nrequests are welcomed!\n\nRequirements\n============\n\n-  Python 2.7\n-  Python 3.x\n\n   -  checked in 3.5, 3.6, 3.7\n\nFeatures\n========\n\n-  simple/common interface among various tokenizers\n-  simple/common interface for filtering with stopwords or\n   Part-of-Speech condition\n-  simple interface to add user-dictionary(mecab only)\n\nSupported Tokenizers\n--------------------\n\nMecab\n~~~~~\n\n`Mecab <http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html?sess=3f6a4f9896295ef2480fa2482de521f6>`__\nis open source tokenizer system for various language(if you have\ndictionary for it)\n\nSee `english\ndocumentation <https://github.com/jordwest/mecab-docs-en>`__ for detail\n\nJuman\n~~~~~\n\n`Juman <http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN>`__ is a\ntokenizer system developed by Kurohashi laboratory, Kyoto University,\nJapan.\n\nJuman is strong for ambiguous writing style in Japanese, and is strong\nfor new-comming words thanks to Web based huge dictionary.\n\nAnd, Juman tells you semantic meaning of words.\n\n.. _juman-1:\n\nJuman++\n~~~~~~~\n\n`Juman++ <http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN++>`__ is a\ntokenizer system developed by Kurohashi laboratory, Kyoto University,\nJapan.\n\nJuman++ is succeeding system of Juman. It adopts RNN model for\ntokenization.\n\nJuman++ is strong for ambigious writing style in Japanese, and is strong\nfor new-comming words thanks to Web based huge dictionary.\n\nAnd, Juman tells you semantic meaning of words.\n\nNote: New Juman++ dev-version(later than 2.x) is available at\n`Github <https://github.com/ku-nlp/jumanpp>`__\n\nKytea\n~~~~~\n\n`Kytea <http://www.phontron.com/kytea/>`__ is tokenizer tool developped\nby Graham Neubig.\n\nKytea has a different algorithm from one of Mecab or Juman.\n\nSetting up\n==========\n\nTokenizers auto-install\n-----------------------\n\n::\n\n   make install\n\nmecab-neologd dictionary auto-install\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n::\n\n   make install_neologd\n\nTokenizers manual-install\n-------------------------\n\n.. _mecab-1:\n\nMeCab\n~~~~~\n\nSee `here <https://github.com/jordwest/mecab-docs-en>`__ to install\nMeCab system.\n\nMecab Neologd dictionary\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nMecab-neologd dictionary is a dictionary-extension based on\nipadic-dictionary, which is basic dictionary of Mecab.\n\nWith, Mecab-neologd dictionary, you\u2019re able to parse new-coming words\nmake one token.\n\nHere, new-coming words is such like, movie actor name or company name\u2026..\n\nSee `here <https://github.com/neologd/mecab-ipadic-neologd>`__ and\ninstall mecab-neologd dictionary.\n\n.. _juman-2:\n\nJuman\n~~~~~\n\n::\n\n   wget -O juman7.0.1.tar.bz2 \"http://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=http://nlp.ist.i.kyoto-u.ac.jp/nl-resource/juman/juman-7.01.tar.bz2&name=juman-7.01.tar.bz2\"\n   bzip2 -dc juman7.0.1.tar.bz2  | tar xvf -\n   cd juman-7.01\n   ./configure\n   make   \n   [sudo] make install\n\n.. _juman-3:\n\nJuman++\n-------\n\n-  GCC version must be >= 5\n\n::\n\n   wget http://lotus.kuee.kyoto-u.ac.jp/nl-resource/jumanpp/jumanpp-1.02.tar.xz\n   tar xJvf jumanpp-1.02.tar.xz\n   cd jumanpp-1.02/\n   ./configure\n   make\n   [sudo] make install\n\n.. _kytea-1:\n\nKytea\n-----\n\nInstall Kytea system\n\n::\n\n   wget http://www.phontron.com/kytea/download/kytea-0.4.7.tar.gz\n   tar -xvf kytea-0.4.7.tar\n   cd kytea-0.4.7\n   ./configure\n   make\n   make install\n\nKytea has `python wrapper <https://github.com/chezou/Mykytea-python>`__\nthanks to michiaki ariga. Install Kytea-python wrapper\n\n::\n\n   pip install kytea\n\ninstall\n-------\n\n::\n\n   [sudo] python setup.py install\n\nNote\n~~~~\n\nDuring install, you see warning message when it fails to install\n``pyknp`` or ``kytea``.\n\nif you see these messages, try to re-install these packages manually.\n\nUsage\n=====\n\nTokenization Example(For python3.x. To see exmaple code for Python2.x,\nplaese see\n`here <https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers/blob/master/examples/examples.py>`__)\n\n::\n\n   import JapaneseTokenizer\n   input_sentence = '10\u65e5\u653e\u9001\u306e\u300c\u4e2d\u5c45\u6b63\u5e83\u306e\u30df\u306b\u306a\u308b\u56f3\u66f8\u9928\u300d\uff08\u30c6\u30ec\u30d3\u671d\u65e5\u7cfb\uff09\u3067\u3001SMAP\u306e\u4e2d\u5c45\u6b63\u5e83\u304c\u3001\u7be0\u539f\u4fe1\u4e00\u306e\u904e\u53bb\u306e\u52d8\u9055\u3044\u3092\u660e\u304b\u3059\u4e00\u5e55\u304c\u3042\u3063\u305f\u3002'\n   # ipadic is well-maintained dictionary #\n   mecab_wrapper = JapaneseTokenizer.MecabWrapper(dictType='ipadic')\n   print(mecab_wrapper.tokenize(input_sentence).convert_list_object())\n\n   # neologd is automatically-generated dictionary from huge web-corpus #\n   mecab_neologd_wrapper = JapaneseTokenizer.MecabWrapper(dictType='neologd')\n   print(mecab_neologd_wrapper.tokenize(input_sentence).convert_list_object())\n\nFiltering example\n-----------------\n\n::\n\n   import JapaneseTokenizer\n   # with word filtering by stopword & part-of-speech condition #\n   print(mecab_wrapper.tokenize(input_sentence).filter(stopwords=['\u30c6\u30ec\u30d3\u671d\u65e5'], pos_condition=[('\u540d\u8a5e', '\u56fa\u6709\u540d\u8a5e')]).convert_list_object())\n\nPart-of-speech structure\n------------------------\n\nMecab, Juman, Kytea have different system of Part-of-Speech(POS).\n\nYou can check tables of Part-of-Speech(POS)\n`here <http://www.unixuser.org/~euske/doc/postag/>`__\n\nSimilar Package\n===============\n\nnatto-py\n--------\n\nnatto-py is sophisticated package for tokenization. It supports\nfollowing features\n\n-  easy interface for tokenization\n-  importing additional dictionary\n-  partial parsing mode\n\nLICENSE\n=======\n\nMIT license\n\nFor developers\n==============\n\nYou could build an environment which has dependencies to test this\npackage.\n\nSimply, you build docker image and run docker container.\n\nDev environment\n---------------\n\nDevelop environment is defined with ``test/docker-compose-dev.yml``.\n\nWith the docker-compose.yml file, you could call python2.7 or python3.7\n\nIf you\u2019re using Pycharm Professional edition, you could set\ndocker-compose.yml as remote interpreter.\n\nTo call python2.7, set ``/opt/conda/envs/p27/bin/python2.7``\n\nTo call python3.7, set ``/opt/conda/envs/p37/bin/python3.7``\n\nTest environment\n----------------\n\nThese commands checks from procedures of package install until test of\npackage.\n\n.. code:: bash\n\n   $ docker-compose build\n   $ docker-compose up\n\n.. |MIT License| image:: http://img.shields.io/badge/license-MIT-blue.svg?style=flat\n   :target: LICENSE\n.. |Build Status| image:: https://travis-ci.org/Kensuke-Mitsuzawa/JapaneseTokenizers.svg?branch=master\n   :target: https://travis-ci.org/Kensuke-Mitsuzawa/JapaneseTokenizers\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers", "keywords": "MeCab,\u548c\u5e03\u856a,Juman,Japanese morphological analyzer,NLP,\u5f62\u614b\u7d20\u89e3\u6790,\u81ea\u7136\u8a00\u8a9e\u51e6\u7406", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "JapaneseTokenizer", "package_url": "https://pypi.org/project/JapaneseTokenizer/", "platform": "", "project_url": "https://pypi.org/project/JapaneseTokenizer/", "project_urls": {"Homepage": "https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers"}, "release_url": "https://pypi.org/project/JapaneseTokenizer/1.6/", "requires_dist": ["future", "jaconv (>=0.2)", "mecab-python3", "pexpect", "pip (>=8.1.0)", "pyknp (>=0.4.1)", "pypandoc", "six"], "requires_python": "", "summary": "", "version": "1.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"LICENSE\" rel=\"nofollow\"><img alt=\"MIT License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e9f8168092fab8a1e348f4b089c1ff9cc0c137bf/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e7376673f7374796c653d666c6174\"></a><a href=\"https://travis-ci.org/Kensuke-Mitsuzawa/JapaneseTokenizers\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/47a568b956c1d4d3a2224b45f1ae3b0c4a813b54/68747470733a2f2f7472617669732d63692e6f72672f4b656e73756b652d4d697473757a6177612f4a6170616e657365546f6b656e697a6572732e7376673f6272616e63683d6d6173746572\"></a></p>\n<div id=\"whats-this\">\n<h2>What\u2019s this?</h2>\n<p>This is simple python-wrapper for Japanese Tokenizers(A.K.A Tokenizer)</p>\n<p>This project aims to call tokenizers and split a sentence into tokens as\neasy as possible.</p>\n<p>And, this project supports various Tokenization tools common interface.\nThus, it\u2019s easy to compare output from various tokenizers.</p>\n<p>This project is available also in\n<a href=\"https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers\" rel=\"nofollow\">Github</a>.</p>\n<p>If you find any bugs, please report them to github issues. Or any pull\nrequests are welcomed!</p>\n</div>\n<div id=\"requirements\">\n<h2>Requirements</h2>\n<ul>\n<li>Python 2.7</li>\n<li>Python 3.x<ul>\n<li>checked in 3.5, 3.6, 3.7</li>\n</ul>\n</li>\n</ul>\n</div>\n<div id=\"features\">\n<h2>Features</h2>\n<ul>\n<li>simple/common interface among various tokenizers</li>\n<li>simple/common interface for filtering with stopwords or\nPart-of-Speech condition</li>\n<li>simple interface to add user-dictionary(mecab only)</li>\n</ul>\n<div id=\"supported-tokenizers\">\n<h3>Supported Tokenizers</h3>\n<div id=\"mecab\">\n<h4>Mecab</h4>\n<p><a href=\"http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html?sess=3f6a4f9896295ef2480fa2482de521f6\" rel=\"nofollow\">Mecab</a>\nis open source tokenizer system for various language(if you have\ndictionary for it)</p>\n<p>See <a href=\"https://github.com/jordwest/mecab-docs-en\" rel=\"nofollow\">english\ndocumentation</a> for detail</p>\n</div>\n<div id=\"juman\">\n<h4>Juman</h4>\n<p><a href=\"http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN\" rel=\"nofollow\">Juman</a> is a\ntokenizer system developed by Kurohashi laboratory, Kyoto University,\nJapan.</p>\n<p>Juman is strong for ambiguous writing style in Japanese, and is strong\nfor new-comming words thanks to Web based huge dictionary.</p>\n<p>And, Juman tells you semantic meaning of words.</p>\n</div>\n<div id=\"id1\">\n<span id=\"juman-1\"></span><h4>Juman++</h4>\n<p><a href=\"http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN++\" rel=\"nofollow\">Juman++</a> is a\ntokenizer system developed by Kurohashi laboratory, Kyoto University,\nJapan.</p>\n<p>Juman++ is succeeding system of Juman. It adopts RNN model for\ntokenization.</p>\n<p>Juman++ is strong for ambigious writing style in Japanese, and is strong\nfor new-comming words thanks to Web based huge dictionary.</p>\n<p>And, Juman tells you semantic meaning of words.</p>\n<p>Note: New Juman++ dev-version(later than 2.x) is available at\n<a href=\"https://github.com/ku-nlp/jumanpp\" rel=\"nofollow\">Github</a></p>\n</div>\n<div id=\"kytea\">\n<h4>Kytea</h4>\n<p><a href=\"http://www.phontron.com/kytea/\" rel=\"nofollow\">Kytea</a> is tokenizer tool developped\nby Graham Neubig.</p>\n<p>Kytea has a different algorithm from one of Mecab or Juman.</p>\n</div>\n</div>\n</div>\n<div id=\"setting-up\">\n<h2>Setting up</h2>\n<div id=\"tokenizers-auto-install\">\n<h3>Tokenizers auto-install</h3>\n<pre>make install\n</pre>\n<div id=\"mecab-neologd-dictionary-auto-install\">\n<h4>mecab-neologd dictionary auto-install</h4>\n<pre>make install_neologd\n</pre>\n</div>\n</div>\n<div id=\"tokenizers-manual-install\">\n<h3>Tokenizers manual-install</h3>\n<div id=\"id2\">\n<span id=\"mecab-1\"></span><h4>MeCab</h4>\n<p>See <a href=\"https://github.com/jordwest/mecab-docs-en\" rel=\"nofollow\">here</a> to install\nMeCab system.</p>\n</div>\n<div id=\"mecab-neologd-dictionary\">\n<h4>Mecab Neologd dictionary</h4>\n<p>Mecab-neologd dictionary is a dictionary-extension based on\nipadic-dictionary, which is basic dictionary of Mecab.</p>\n<p>With, Mecab-neologd dictionary, you\u2019re able to parse new-coming words\nmake one token.</p>\n<p>Here, new-coming words is such like, movie actor name or company name\u2026..</p>\n<p>See <a href=\"https://github.com/neologd/mecab-ipadic-neologd\" rel=\"nofollow\">here</a> and\ninstall mecab-neologd dictionary.</p>\n</div>\n<div id=\"id3\">\n<span id=\"juman-2\"></span><h4>Juman</h4>\n<pre>wget -O juman7.0.1.tar.bz2 \"http://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=http://nlp.ist.i.kyoto-u.ac.jp/nl-resource/juman/juman-7.01.tar.bz2&amp;name=juman-7.01.tar.bz2\"\nbzip2 -dc juman7.0.1.tar.bz2  | tar xvf -\ncd juman-7.01\n./configure\nmake\n[sudo] make install\n</pre>\n</div>\n</div>\n<div id=\"id4\">\n<span id=\"juman-3\"></span><h3>Juman++</h3>\n<ul>\n<li>GCC version must be &gt;= 5</li>\n</ul>\n<pre>wget http://lotus.kuee.kyoto-u.ac.jp/nl-resource/jumanpp/jumanpp-1.02.tar.xz\ntar xJvf jumanpp-1.02.tar.xz\ncd jumanpp-1.02/\n./configure\nmake\n[sudo] make install\n</pre>\n</div>\n<div id=\"id5\">\n<span id=\"kytea-1\"></span><h3>Kytea</h3>\n<p>Install Kytea system</p>\n<pre>wget http://www.phontron.com/kytea/download/kytea-0.4.7.tar.gz\ntar -xvf kytea-0.4.7.tar\ncd kytea-0.4.7\n./configure\nmake\nmake install\n</pre>\n<p>Kytea has <a href=\"https://github.com/chezou/Mykytea-python\" rel=\"nofollow\">python wrapper</a>\nthanks to michiaki ariga. Install Kytea-python wrapper</p>\n<pre>pip install kytea\n</pre>\n</div>\n<div id=\"install\">\n<h3>install</h3>\n<pre>[sudo] python setup.py install\n</pre>\n<div id=\"note\">\n<h4>Note</h4>\n<p>During install, you see warning message when it fails to install\n<tt>pyknp</tt> or <tt>kytea</tt>.</p>\n<p>if you see these messages, try to re-install these packages manually.</p>\n</div>\n</div>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>Tokenization Example(For python3.x. To see exmaple code for Python2.x,\nplaese see\n<a href=\"https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers/blob/master/examples/examples.py\" rel=\"nofollow\">here</a>)</p>\n<pre>import JapaneseTokenizer\ninput_sentence = '10\u65e5\u653e\u9001\u306e\u300c\u4e2d\u5c45\u6b63\u5e83\u306e\u30df\u306b\u306a\u308b\u56f3\u66f8\u9928\u300d\uff08\u30c6\u30ec\u30d3\u671d\u65e5\u7cfb\uff09\u3067\u3001SMAP\u306e\u4e2d\u5c45\u6b63\u5e83\u304c\u3001\u7be0\u539f\u4fe1\u4e00\u306e\u904e\u53bb\u306e\u52d8\u9055\u3044\u3092\u660e\u304b\u3059\u4e00\u5e55\u304c\u3042\u3063\u305f\u3002'\n# ipadic is well-maintained dictionary #\nmecab_wrapper = JapaneseTokenizer.MecabWrapper(dictType='ipadic')\nprint(mecab_wrapper.tokenize(input_sentence).convert_list_object())\n\n# neologd is automatically-generated dictionary from huge web-corpus #\nmecab_neologd_wrapper = JapaneseTokenizer.MecabWrapper(dictType='neologd')\nprint(mecab_neologd_wrapper.tokenize(input_sentence).convert_list_object())\n</pre>\n<div id=\"filtering-example\">\n<h3>Filtering example</h3>\n<pre>import JapaneseTokenizer\n# with word filtering by stopword &amp; part-of-speech condition #\nprint(mecab_wrapper.tokenize(input_sentence).filter(stopwords=['\u30c6\u30ec\u30d3\u671d\u65e5'], pos_condition=[('\u540d\u8a5e', '\u56fa\u6709\u540d\u8a5e')]).convert_list_object())\n</pre>\n</div>\n<div id=\"part-of-speech-structure\">\n<h3>Part-of-speech structure</h3>\n<p>Mecab, Juman, Kytea have different system of Part-of-Speech(POS).</p>\n<p>You can check tables of Part-of-Speech(POS)\n<a href=\"http://www.unixuser.org/~euske/doc/postag/\" rel=\"nofollow\">here</a></p>\n</div>\n</div>\n<div id=\"similar-package\">\n<h2>Similar Package</h2>\n<h2 id=\"natto-py\"><span class=\"section-subtitle\">natto-py</span></h2>\n<p>natto-py is sophisticated package for tokenization. It supports\nfollowing features</p>\n<ul>\n<li>easy interface for tokenization</li>\n<li>importing additional dictionary</li>\n<li>partial parsing mode</li>\n</ul>\n</div>\n<div id=\"license\">\n<h2>LICENSE</h2>\n<p>MIT license</p>\n</div>\n<div id=\"for-developers\">\n<h2>For developers</h2>\n<p>You could build an environment which has dependencies to test this\npackage.</p>\n<p>Simply, you build docker image and run docker container.</p>\n<div id=\"dev-environment\">\n<h3>Dev environment</h3>\n<p>Develop environment is defined with <tt><span class=\"pre\">test/docker-compose-dev.yml</span></tt>.</p>\n<p>With the docker-compose.yml file, you could call python2.7 or python3.7</p>\n<p>If you\u2019re using Pycharm Professional edition, you could set\ndocker-compose.yml as remote interpreter.</p>\n<p>To call python2.7, set <tt>/opt/conda/envs/p27/bin/python2.7</tt></p>\n<p>To call python3.7, set <tt>/opt/conda/envs/p37/bin/python3.7</tt></p>\n</div>\n<div id=\"test-environment\">\n<h3>Test environment</h3>\n<p>These commands checks from procedures of package install until test of\npackage.</p>\n<pre>$ docker-compose build\n$ docker-compose up\n</pre>\n</div>\n</div>\n\n          </div>"}, "last_serial": 4983270, "releases": {"0.6a1": [{"comment_text": "", "digests": {"md5": "e55f95650df77ef126ee23b2cc88ad20", "sha256": "c15a23d01f1ad997049e1f89333bb421b98fd5f0a2fbe7ae005662a9cd5a383a"}, "downloads": -1, "filename": "JapaneseTokenizer-0.6a1.tar.gz", "has_sig": false, "md5_digest": "e55f95650df77ef126ee23b2cc88ad20", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10587, "upload_time": "2016-03-05T15:16:52", "upload_time_iso_8601": "2016-03-05T15:16:52.996917Z", "url": "https://files.pythonhosted.org/packages/5b/96/cc92357c7e7261c791db6f769bda1cd2c1f4547ea64bd1b75227c2818643/JapaneseTokenizer-0.6a1.tar.gz", "yanked": false}], "0.7": [{"comment_text": "", "digests": {"md5": "bdba410c246604b53599e6431ee8ba97", "sha256": "82a76d0309a31c09f653b3ba0d1ac99e42a0c9a80e55b5f48b52072a30c84214"}, "downloads": -1, "filename": "JapaneseTokenizer-0.7.tar.gz", "has_sig": false, "md5_digest": "bdba410c246604b53599e6431ee8ba97", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15964, "upload_time": "2016-03-06T05:06:38", "upload_time_iso_8601": "2016-03-06T05:06:38.110908Z", "url": "https://files.pythonhosted.org/packages/c6/7a/e5b1c02a0e1c055d17f171fe04925556f419b59f871902a77f6e83a048af/JapaneseTokenizer-0.7.tar.gz", "yanked": false}], "0.8": [{"comment_text": "", "digests": {"md5": "aecb2a18cabf7ab5fcae6dd5147390e8", "sha256": "35297b98855676f04f910e4025e7888be9ef69bc3bfb2fad5dc9b34740053274"}, "downloads": -1, "filename": "JapaneseTokenizer-0.8.tar.gz", "has_sig": false, "md5_digest": "aecb2a18cabf7ab5fcae6dd5147390e8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15860, "upload_time": "2016-04-02T19:53:42", "upload_time_iso_8601": "2016-04-02T19:53:42.365687Z", "url": "https://files.pythonhosted.org/packages/13/61/6586dc1b41cdcf05f1f49ea3d92cc4e14341c7879dcce33fab693383da25/JapaneseTokenizer-0.8.tar.gz", "yanked": false}], "0.9": [{"comment_text": "", "digests": {"md5": "965d1893a35b65ed9c81d09492834f3b", "sha256": "f7b4ea9753a8e8a5894ba0ccc1f0b9713aa78f62cc7dc49dbb4fa86ef06c14de"}, "downloads": -1, "filename": "JapaneseTokenizer-0.9.tar.gz", "has_sig": false, "md5_digest": "965d1893a35b65ed9c81d09492834f3b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16284, "upload_time": "2016-04-04T17:23:05", "upload_time_iso_8601": "2016-04-04T17:23:05.158729Z", "url": "https://files.pythonhosted.org/packages/d5/6a/e52ba521d54169b40acf7cb025b68e1e7119e28da6be9416c99054111bcd/JapaneseTokenizer-0.9.tar.gz", "yanked": false}], "1.0": [{"comment_text": "", "digests": {"md5": "2b10f181b00cc626eed106965163d8e3", "sha256": "71f5675bd8b6815fcc8242f48be9a4b6197294e2626683a7431380469cfc78f6"}, "downloads": -1, "filename": "JapaneseTokenizer-1.0.tar.gz", "has_sig": false, "md5_digest": "2b10f181b00cc626eed106965163d8e3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17118, "upload_time": "2016-08-03T04:28:53", "upload_time_iso_8601": "2016-08-03T04:28:53.319387Z", "url": "https://files.pythonhosted.org/packages/54/b5/79590c298941926efd0b4fbe4906979bfcb16199dd371e46d2a88144b8c4/JapaneseTokenizer-1.0.tar.gz", "yanked": false}], "1.0a0": [{"comment_text": "", "digests": {"md5": "87fa79c3609690d6b86295c494af7d6f", "sha256": "fa3ec1860b5df9688d0d2c0037af19b568be2663e8b06e9814effb258cf82a4b"}, "downloads": -1, "filename": "JapaneseTokenizer-1.0a0.tar.gz", "has_sig": false, "md5_digest": "87fa79c3609690d6b86295c494af7d6f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17105, "upload_time": "2016-06-19T08:18:31", "upload_time_iso_8601": "2016-06-19T08:18:31.008233Z", "url": "https://files.pythonhosted.org/packages/82/47/8727b3a859e6c3cff4c3fdb2a5fc068d355bd346a22db38730e3ae9bd1ce/JapaneseTokenizer-1.0a0.tar.gz", "yanked": false}], "1.0b0": [{"comment_text": "", "digests": {"md5": "b6d953a9ed3efe28dfbc1a5c9f3287a2", "sha256": "fdb6b46290ae0907e62d16d32c762b6c7f4f433619c22c34519e32d6be253d21"}, "downloads": -1, "filename": "JapaneseTokenizer-1.0b0.tar.gz", "has_sig": false, "md5_digest": "b6d953a9ed3efe28dfbc1a5c9f3287a2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17113, "upload_time": "2016-06-22T02:34:50", "upload_time_iso_8601": "2016-06-22T02:34:50.998148Z", "url": "https://files.pythonhosted.org/packages/b6/b2/089232b3c76d3e802e0fd1c17eb7d96f9bbdc4034d8a55b3bfa18d241d64/JapaneseTokenizer-1.0b0.tar.gz", "yanked": false}], "1.2.3": [{"comment_text": "", "digests": {"md5": "da97ce49089ebc76f08aa476a9c1cdce", "sha256": "7d9540ee440c9393bdf4232cc694d5bc8b6ba60b5bbde87edaf06381d046263d"}, "downloads": -1, "filename": "JapaneseTokenizer-1.2.3.tar.gz", "has_sig": false, "md5_digest": "da97ce49089ebc76f08aa476a9c1cdce", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16609, "upload_time": "2016-12-08T18:25:46", "upload_time_iso_8601": "2016-12-08T18:25:46.695603Z", "url": "https://files.pythonhosted.org/packages/87/2b/6ad05fb2afe606550828fba2c1da98629bf2f34a5523535d7214ea9c15c0/JapaneseTokenizer-1.2.3.tar.gz", "yanked": false}], "1.2.5": [{"comment_text": "", "digests": {"md5": "30d57d0c20f73a976f3be343738f5f55", "sha256": "2ea333d144fd9f0ba44e6001d74d63855beb3a30a4c87924938130664231518f"}, "downloads": -1, "filename": "JapaneseTokenizer-1.2.5.tar.gz", "has_sig": false, "md5_digest": "30d57d0c20f73a976f3be343738f5f55", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19943, "upload_time": "2016-12-28T07:03:26", "upload_time_iso_8601": "2016-12-28T07:03:26.757392Z", "url": "https://files.pythonhosted.org/packages/2a/26/0c91f48b42d8059f969c2ab9a56b29b351eac15a139744f17fb98fa0accb/JapaneseTokenizer-1.2.5.tar.gz", "yanked": false}], "1.2.6": [{"comment_text": "", "digests": {"md5": "e6389d1aa28127631fb9595e19a6fd23", "sha256": "f2010906f997bc36db4fddb438abaf6d6380bca065beefd2859428fda0a5c0f4"}, "downloads": -1, "filename": "JapaneseTokenizer-1.2.6.tar.gz", "has_sig": false, "md5_digest": "e6389d1aa28127631fb9595e19a6fd23", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20216, "upload_time": "2017-01-11T21:26:02", "upload_time_iso_8601": "2017-01-11T21:26:02.153647Z", "url": "https://files.pythonhosted.org/packages/eb/9c/9197907e3cd0c13f03cd887efef234bc0fe359a05a844e7b82405ddab282/JapaneseTokenizer-1.2.6.tar.gz", "yanked": false}], "1.2.7": [{"comment_text": "", "digests": {"md5": "4caa2f2fd7e0a4e71d0d95b319e2f960", "sha256": "bddfc293a9c221f37b631c226568361c2599e74d49c4e0cb57ff859df725d4b8"}, "downloads": -1, "filename": "JapaneseTokenizer-1.2.7.tar.gz", "has_sig": false, "md5_digest": "4caa2f2fd7e0a4e71d0d95b319e2f960", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20224, "upload_time": "2017-01-13T00:37:10", "upload_time_iso_8601": "2017-01-13T00:37:10.425560Z", "url": "https://files.pythonhosted.org/packages/08/ed/e970d4d49554e7ddf5ed3edec1e5f1d8265c47e44c6de0b4e71191a15c7d/JapaneseTokenizer-1.2.7.tar.gz", "yanked": false}], "1.3.0": [{"comment_text": "", "digests": {"md5": "13b4072229b14619ae7b8cd672c47fbe", "sha256": "b51dd61c3a4192bb70d47310e67d42b29b8e6ec7702b556f4b718836fc772ccf"}, "downloads": -1, "filename": "JapaneseTokenizer-1.3.0.tar.gz", "has_sig": false, "md5_digest": "13b4072229b14619ae7b8cd672c47fbe", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23199, "upload_time": "2017-02-23T03:41:02", "upload_time_iso_8601": "2017-02-23T03:41:02.236513Z", "url": "https://files.pythonhosted.org/packages/79/19/c950a7b3ba817a9fc56e9d44d6698a401e876f54de404a760f8a1100a34a/JapaneseTokenizer-1.3.0.tar.gz", "yanked": false}], "1.3.1": [{"comment_text": "", "digests": {"md5": "f70e1e0d82111714c1958084eef1c813", "sha256": "3608d6135eb00d6b59fccf675c894ff978f22359ed37a99fde8bb006b6d948f0"}, "downloads": -1, "filename": "JapaneseTokenizer-1.3.1.tar.gz", "has_sig": false, "md5_digest": "f70e1e0d82111714c1958084eef1c813", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28932, "upload_time": "2017-06-29T11:05:09", "upload_time_iso_8601": "2017-06-29T11:05:09.142771Z", "url": "https://files.pythonhosted.org/packages/c0/06/e6629d1113b7f9061702969154baed98cdb75d3648ff3d0270d87b17a7a5/JapaneseTokenizer-1.3.1.tar.gz", "yanked": false}], "1.3.3": [{"comment_text": "", "digests": {"md5": "3c19ae9d41dc190c59be3664e3c9b659", "sha256": "f18bdbb1883a02d2cacfa42cf41b3d5198a804b986c06ec5f61b37c4d0ca0a82"}, "downloads": -1, "filename": "JapaneseTokenizer-1.3.3.tar.gz", "has_sig": false, "md5_digest": "3c19ae9d41dc190c59be3664e3c9b659", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29010, "upload_time": "2017-09-11T09:30:36", "upload_time_iso_8601": "2017-09-11T09:30:36.530709Z", "url": "https://files.pythonhosted.org/packages/b4/1c/9939737367a9fbd76d83fb3785676f1c25f441e73612e2b8ef7cd0f96ca4/JapaneseTokenizer-1.3.3.tar.gz", "yanked": false}], "1.3.4": [{"comment_text": "", "digests": {"md5": "29140724b608a9a2b8232c7c6bfb35d8", "sha256": "d0f89a39f575af46b1ada42b0cd6980e56ecad7aa74397a13fa7c04df7ffa896"}, "downloads": -1, "filename": "JapaneseTokenizer-1.3.4.tar.gz", "has_sig": false, "md5_digest": "29140724b608a9a2b8232c7c6bfb35d8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25226, "upload_time": "2017-09-21T08:10:52", "upload_time_iso_8601": "2017-09-21T08:10:52.647747Z", "url": "https://files.pythonhosted.org/packages/ab/73/cfc8a35e964e78dbc8a72cd631b4f84d837279ae34501381a115d83f6c58/JapaneseTokenizer-1.3.4.tar.gz", "yanked": false}], "1.3.5": [{"comment_text": "", "digests": {"md5": "e24bafd9da63971195ce7f1590e7487c", "sha256": "5276308eae6a34221446069a254ef98e20f3a5b90f2e5ab8c682d45a269295b8"}, "downloads": -1, "filename": "JapaneseTokenizer-1.3.5.tar.gz", "has_sig": false, "md5_digest": "e24bafd9da63971195ce7f1590e7487c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25785, "upload_time": "2017-09-27T03:30:33", "upload_time_iso_8601": "2017-09-27T03:30:33.996137Z", "url": "https://files.pythonhosted.org/packages/0c/b7/c54719202bbd67b7a44ba1d53f58d5557261d4a523fe1bfe1aeceab9ca65/JapaneseTokenizer-1.3.5.tar.gz", "yanked": false}], "1.3.6": [{"comment_text": "", "digests": {"md5": "4528fc7cca5ee812f63f6183435eb2c5", "sha256": "55cd3f4b0979609bddadd88cedd31afdc6cec39f5b2c4225b814b58944bf9a92"}, "downloads": -1, "filename": "JapaneseTokenizer-1.3.6.tar.gz", "has_sig": false, "md5_digest": "4528fc7cca5ee812f63f6183435eb2c5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26011, "upload_time": "2017-11-01T11:05:19", "upload_time_iso_8601": "2017-11-01T11:05:19.951464Z", "url": "https://files.pythonhosted.org/packages/1f/08/0b839490cd42fce5b727bbf43500d2bd268584c0519720cd1b03738899d0/JapaneseTokenizer-1.3.6.tar.gz", "yanked": false}], "1.3.7": [{"comment_text": "", "digests": {"md5": "e9367af771a29abc9249eb23ad1ed60f", "sha256": "8d296055cf8a4e8249773287ca22704fb63adebed3bca5a249128aea64d87796"}, "downloads": -1, "filename": "JapaneseTokenizer-1.3.7.tar.gz", "has_sig": false, "md5_digest": "e9367af771a29abc9249eb23ad1ed60f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26109, "upload_time": "2018-02-27T01:58:58", "upload_time_iso_8601": "2018-02-27T01:58:58.011697Z", "url": "https://files.pythonhosted.org/packages/0d/23/88e38a5a9f874d6c2bba08908d76b449cb300985c8c882a32b8d2c0a4434/JapaneseTokenizer-1.3.7.tar.gz", "yanked": false}], "1.4": [{"comment_text": "", "digests": {"md5": "658c071f213905c766dca30dbe990ba4", "sha256": "6f8af3f172ca535d0d37aff2ec9e9ad619d069422859219fac8a0ed5b9ae7de1"}, "downloads": -1, "filename": "JapaneseTokenizer-1.4.tar.gz", "has_sig": false, "md5_digest": "658c071f213905c766dca30dbe990ba4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25609, "upload_time": "2018-12-24T19:37:11", "upload_time_iso_8601": "2018-12-24T19:37:11.647869Z", "url": "https://files.pythonhosted.org/packages/1c/88/45abab146379806f748fe2af0969b4d739a47efdd8e04017a138d0a1b78d/JapaneseTokenizer-1.4.tar.gz", "yanked": false}], "1.5": [{"comment_text": "", "digests": {"md5": "0d64248c5c514fa6abf0293ae7d80f33", "sha256": "c8636309b672ff6fd893fd388d0757c4b026320d55c2d10081df10ad1508b126"}, "downloads": -1, "filename": "JapaneseTokenizer-1.5.tar.gz", "has_sig": false, "md5_digest": "0d64248c5c514fa6abf0293ae7d80f33", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28160, "upload_time": "2019-01-21T07:14:05", "upload_time_iso_8601": "2019-01-21T07:14:05.051165Z", "url": "https://files.pythonhosted.org/packages/df/23/399685ed7438030f01417bb1d2c0e76d0bcb60f4d8f73f2d6449d52b8c86/JapaneseTokenizer-1.5.tar.gz", "yanked": false}], "1.6": [{"comment_text": "", "digests": {"md5": "2023647d0aea042e51fac3be0450dee8", "sha256": "72e54e004071d6e26e53ee6fbf384ac696260a4ccd98517b460c7322d600c2cb"}, "downloads": -1, "filename": "JapaneseTokenizer-1.6-py3-none-any.whl", "has_sig": false, "md5_digest": "2023647d0aea042e51fac3be0450dee8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 44447, "upload_time": "2019-03-25T16:00:13", "upload_time_iso_8601": "2019-03-25T16:00:13.465345Z", "url": "https://files.pythonhosted.org/packages/29/63/b71a2fa2ba0ad681d0877bab89b56d162b0e07533b99c5f3477f9a3df7f0/JapaneseTokenizer-1.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f5d47f7d1d6bd0381f4ff56f25a52dee", "sha256": "c9b93fb9d355a10b4e6484ac25febcecee00e40e42e6ad38beb2258f9e8d7900"}, "downloads": -1, "filename": "JapaneseTokenizer-1.6.tar.gz", "has_sig": false, "md5_digest": "f5d47f7d1d6bd0381f4ff56f25a52dee", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29519, "upload_time": "2019-03-25T16:00:16", "upload_time_iso_8601": "2019-03-25T16:00:16.301362Z", "url": "https://files.pythonhosted.org/packages/13/4e/758f36d3d7f51d9c10d07d46a265f3f2337062b1913c231223156b719d0f/JapaneseTokenizer-1.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2023647d0aea042e51fac3be0450dee8", "sha256": "72e54e004071d6e26e53ee6fbf384ac696260a4ccd98517b460c7322d600c2cb"}, "downloads": -1, "filename": "JapaneseTokenizer-1.6-py3-none-any.whl", "has_sig": false, "md5_digest": "2023647d0aea042e51fac3be0450dee8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 44447, "upload_time": "2019-03-25T16:00:13", "upload_time_iso_8601": "2019-03-25T16:00:13.465345Z", "url": "https://files.pythonhosted.org/packages/29/63/b71a2fa2ba0ad681d0877bab89b56d162b0e07533b99c5f3477f9a3df7f0/JapaneseTokenizer-1.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f5d47f7d1d6bd0381f4ff56f25a52dee", "sha256": "c9b93fb9d355a10b4e6484ac25febcecee00e40e42e6ad38beb2258f9e8d7900"}, "downloads": -1, "filename": "JapaneseTokenizer-1.6.tar.gz", "has_sig": false, "md5_digest": "f5d47f7d1d6bd0381f4ff56f25a52dee", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29519, "upload_time": "2019-03-25T16:00:16", "upload_time_iso_8601": "2019-03-25T16:00:16.301362Z", "url": "https://files.pythonhosted.org/packages/13/4e/758f36d3d7f51d9c10d07d46a265f3f2337062b1913c231223156b719d0f/JapaneseTokenizer-1.6.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:53:13 2020"}