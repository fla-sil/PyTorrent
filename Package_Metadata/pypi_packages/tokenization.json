{"info": {"author": "Robert Russell", "author_email": "robertrussell.72001@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Text Processing"], "description": "", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Ro5bert/tokenization", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "tokenization", "package_url": "https://pypi.org/project/tokenization/", "platform": "", "project_url": "https://pypi.org/project/tokenization/", "project_urls": {"Homepage": "https://github.com/Ro5bert/tokenization"}, "release_url": "https://pypi.org/project/tokenization/1.0.7/", "requires_dist": ["regex"], "requires_python": "", "summary": "A general purpose text tokenizing module for python.", "version": "1.0.7", "yanked": false, "html_description": null}, "last_serial": 4037144, "releases": {"1.0.7": [{"comment_text": "", "digests": {"md5": "ecf1d5eee1cad892c035ea8f2eeeec2e", "sha256": "333359f645a8dee67c55b80972e3c60c0000b3e69a6b409c8ed7d0100588fd05"}, "downloads": -1, "filename": "tokenization-1.0.7-py3-none-any.whl", "has_sig": false, "md5_digest": "ecf1d5eee1cad892c035ea8f2eeeec2e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10785, "upload_time": "2018-07-06T17:31:01", "upload_time_iso_8601": "2018-07-06T17:31:01.743027Z", "url": "https://files.pythonhosted.org/packages/60/eb/8e1756b0ce07dab8b0f8267019738d0e4ea2fc8f6eb3fe4d433daac38a1d/tokenization-1.0.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "62de9505ec8658e784f8662761c6567c", "sha256": "948f358cf9a018a1862c581b9c3996092d3ed2f3fac9cb7b6fbd610834f997ed"}, "downloads": -1, "filename": "tokenization-1.0.7.tar.gz", "has_sig": false, "md5_digest": "62de9505ec8658e784f8662761c6567c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11070, "upload_time": "2018-07-06T17:31:02", "upload_time_iso_8601": "2018-07-06T17:31:02.920038Z", "url": "https://files.pythonhosted.org/packages/71/03/d686087b80b577181cbd10283a323ff5d83c8372d06133ae76433e56669f/tokenization-1.0.7.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ecf1d5eee1cad892c035ea8f2eeeec2e", "sha256": "333359f645a8dee67c55b80972e3c60c0000b3e69a6b409c8ed7d0100588fd05"}, "downloads": -1, "filename": "tokenization-1.0.7-py3-none-any.whl", "has_sig": false, "md5_digest": "ecf1d5eee1cad892c035ea8f2eeeec2e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10785, "upload_time": "2018-07-06T17:31:01", "upload_time_iso_8601": "2018-07-06T17:31:01.743027Z", "url": "https://files.pythonhosted.org/packages/60/eb/8e1756b0ce07dab8b0f8267019738d0e4ea2fc8f6eb3fe4d433daac38a1d/tokenization-1.0.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "62de9505ec8658e784f8662761c6567c", "sha256": "948f358cf9a018a1862c581b9c3996092d3ed2f3fac9cb7b6fbd610834f997ed"}, "downloads": -1, "filename": "tokenization-1.0.7.tar.gz", "has_sig": false, "md5_digest": "62de9505ec8658e784f8662761c6567c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11070, "upload_time": "2018-07-06T17:31:02", "upload_time_iso_8601": "2018-07-06T17:31:02.920038Z", "url": "https://files.pythonhosted.org/packages/71/03/d686087b80b577181cbd10283a323ff5d83c8372d06133ae76433e56669f/tokenization-1.0.7.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:51:28 2020"}