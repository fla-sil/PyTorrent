{"info": {"author": "Wessel Bruinsma", "author_email": "wessel.p.bruinsma@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# [GPAR](http://github.com/wesselb/gpar)\n\n[![Build](https://travis-ci.org/wesselb/gpar.svg?branch=master)](https://travis-ci.org/wesselb/gpar)\n[![Coverage Status](https://coveralls.io/repos/github/wesselb/gpar/badge.svg?branch=master&service=github)](https://coveralls.io/github/wesselb/gpar?branch=master)\n[![Latest Docs](https://img.shields.io/badge/docs-latest-blue.svg)](https://wesselb.github.io/gpar)\n\nImplementation of the Gaussian Process Autoregressive Regression Model.\nSee the [paper](https://arxiv.org/abs/1802.07182), and see the [docs](https://wesselb.github.io/gpar).\n\n*Note:* GPAR requires Python 3.6 or higher.\n\n* [Installation](#installation)\n* [Basic Usage](#basic-usage)\n* [Features](#features)\n    - [Input and Output Dependencies](#input-and-output-dependencies)\n    - [Output Transformation](#output-transformation)\n    - [Sampling](#sampling)\n    - [Logpdf Computation](#logpdf-computation)\n    - [Inducing Points](#inducing-points)\n * [Example (examples/paper/synthetic.py)](#example-examples-paper-synthetic-py)\n\n## Installation\nBefore installing the package, please ensure that `gcc` and `gfortran` are \navailable.\nOn OS X, these are both installed with `brew install gcc`;\nusers of Anaconda may want to instead consider `conda install gcc`.\nOn Linux, `gcc` is most likely already available, and `gfortran` can be \ninstalled with `apt-get install gfortran`.\nThen simply\n\n```\npip install gpar\n```\n\n## Basic Usage\nA simple instance of GPAR can be created as follows:\n\n```python\nfrom gpar import GPARRegressor\n\ngpar = GPARRegressor(replace=True, impute=True,\n                     scale=1.0,\n                     linear=True, linear_scale=100.0,\n                     nonlinear=True, nonlinear_scale=1.0,\n                     noise=0.1,\n                     normalise_y=True)\n```\n\nHere the keyword arguments have the following meaning:\n\n* `replace=True`: Replace data points with the posterior mean of the previous\n    layer before feeding them into the next layer. This helps the model deal \n    with noisy data, but may discard important structure in the data if  the \n    fit is bad.\n    \n* `impute=True`: GPAR requires that data is _closed downwards_. If this is \n    not the case, the model will be unable to use part of the data. Setting \n    `impute` to `True` lets GPAR impute data points to ensure that the data is\n    closed downwards.\n    \n* `scale=1.0`: Initialisation of the length scale with respect to the inputs.\n\n* `linear=True`: Use linear dependencies between outputs.\n\n* `linear_scale=True`: Initialisation of the length scale of the linear \n    dependencies between outputs.\n    \n* `nonlinear=True`: Also use nonlinear dependencies between outputs.\n\n* `nonlinear_scale=1.0`: Initialisation of the length scale of the nonlinear \n    dependencies between outputs. _Important:_ this length scale applies \n    _after_ possible normalisation of the outputs (see below), in which case \n    `nonlinear_scale=1.0` corresponds to a simple, but nonlinear relationship.\n    \n* `noise=0.1`: Initialisation of the variance of the observation noise.\n\n* `normalise_y=True`: Internally, work with a normalised version of the \n    outputs by subtracting the mean and dividing by the standard deviation.\n    Predictions will be transformed back appropriately.\n\nIn the above, any scalar hyperparameter may be replaced by a list of values \nto initialise each layer separately, e.g. `scale=[1.0, 2.0]`. See the \ndocumentation for a full overview of the keywords that may be passed to \n`GPARRegressor`.\n\nTo fit GPAR, call `gpar.fit(x_train, y_train)` where `x_train` are the training \ninputs and `y_train` the training outputs. The inputs `x_train` must have shape \n$n$ or $n \\times m$, where $n$ is  the number of data points and $m$ the \nnumber of input features, and the outputs `y_train` must have shape $n$ or $n \n\\times p$, where $p$ is the number of outputs.\n\nFinally, to make predictions, call\n\n```python\nmeans = gpar.predict(x_test, num_samples=100)\n```\n\nto get the predictive means, or \n\n```python\nmeans, lowers, uppers = gpar.predict(x_test,\n                                     num_samples=100, \n                                     credible_bounds=True)\n```\n\nto also get lower and upper 95% central marginal credible bounds. If you wish\n to predict the underlying latent function instead of the observed values, set\n`latent=True` in the call to `GPARRegressor.predict`.\n\n## Features\n\n### Input and Output Dependencies\nUsing keywords arguments, `GPARRegressor` can be configured to specify the \ndependencies with respect to the inputs and between the outputs. The following\ndependencies can be specified:\n\n* **Linear input dependencies:** Set `linear_input=True` and specify the \n    length scale with `linear_input_scale`.\n    \n* **Nonlinear input dependencies:** This is enabled by default. The length \n    scale can be specified using `scale`. To tie these length scales across all\n    layers, set `scale_tie=True`.\n    \n* **Locally periodic input dependencies:** Set `per=True` and specify the period\n    with `per_period`, the length scale with `per_scale`, and the length \n    scale on which the periodicity changes with `per_decay`.\n    \n* **Linear output dependencies:** Set `linear=True` and specify the length \n    scale with `linear_scale`.\n    \n* **Nonlinear output dependencies:** Set `nonlinear=True` and specify the \n    length scale with `nonlinear_scale`.\n    \nAll nonlinear kernels are exponentiated quadratic kernels. If you wish to \ninstead use rational quadratic kernels, set `rq=True`.\n\nAll parameters can be set to a list of values to initialise the value for \neach layer separately.\n\nTo let every layer depend only the `k`th previous layers, set `markov=k`.\n\n\n### Output Transformation\n\nOne may want to apply a transformation to the data before fitting the model, \ne.g. $y\\mapsto\\log(y)$ in the case of positive data. Such a transformation can\nbe specified by setting the `transform_y` keyword argument for `GPARRegressor`.\nThe following transformations are available:\n\n* `log_transform`: $y \\mapsto \\log(y)$.\n\n* `squishing_transform`: $y \\mapsto \\operatorname{sign}(y) \\log(1 + |y|)$.\n\n\n### Sampling\n\nSampling from the model can be done with `GPARRegressor.sample`. The keyword \nargument `num_samples` specifies the number of samples, and `latent` \nspecifies whether to sample from the underlying latent function or the \nobserved values. Sampling from the _prior_ and _posterior_ (model must be fit\nfirst) can be done as follows:\n \n```python\nsample = gpar.sample(x, p=2)  # Sample two outputs from the prior.\n\nsample = gpar.sample(x, posterior=True)  # Sample from the posterior.\n```\n\n### Logpdf Computation\nThe logpdf of data can be computed with `GPARRegressor.logpdf`. To compute the\nlogpdf under the posterior, set `posterior=True`. To sample missing data to \ncompute an unbiased estimate of the *pdf*, *not logpdf*, set \n`sample_missing=True`.\n\nThe logpdf can be computed without casting the inputs `x` and `y` to PyTorch\ntensors and without detaching the resulting logpdf from the computation\ngraph. For this, set `differentiable=True` in `GPARRegressor.logpdf`.\n\n\n### Inducing Points\nInducing points can be used to scale GPAR to large data sets. Simply set `x_ind`\nto the locations of the inducing points in `GPARRegressor`.\n\n\n## Example (`examples/paper/synthetic.py`)\n\n![Prediction](https://raw.githubusercontent.com/wesselb/gpar/master/readme_example_prediction.png)\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom gpar.regression import GPARRegressor\n\n# Create toy data set.\nn = 200\nx = np.linspace(0, 1, n)\nnoise = 0.1\n\n# Draw functions depending on each other in complicated ways.\nf1 = -np.sin(10 * np.pi * (x + 1)) / (2 * x + 1) - x ** 4\nf2 = np.cos(f1) ** 2 + np.sin(3 * x)\nf3 = f2 * f1 ** 2 + 3 * x\nf = np.stack((f1, f2, f3), axis=0).T\n\n# Add noise and subsample.\ny = f + noise * np.random.randn(n, 3)\nx_obs, y_obs = x[::8], y[::8]\n\n# Fit and predict GPAR.\nmodel = GPARRegressor(scale=0.1,\n                      linear=True, linear_scale=10.,\n                      nonlinear=True, nonlinear_scale=0.1,\n                      noise=0.1,\n                      impute=True, replace=True, normalise_y=False)\nmodel.fit(x_obs, y_obs)\nmeans, lowers, uppers = \\\n    model.predict(x, num_samples=200, credible_bounds=True, latent=True)\n\n# Fit and predict independent GPs: set markov=0.\nigp = GPARRegressor(scale=0.1,\n                    linear=True, linear_scale=10.,\n                    nonlinear=True, nonlinear_scale=0.1,\n                    noise=0.1, markov=0, normalise_y=False)\nigp.fit(x_obs, y_obs)\nigp_means, igp_lowers, igp_uppers = \\\n    igp.predict(x, num_samples=200, credible_bounds=True, latent=True)\n\n# Plot the result.\nplt.figure(figsize=(12, 2.5))\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['mathtext.fontset'] = 'dejavuserif'\n\nfor i in range(3):\n    ax = plt.subplot(1, 3, i + 1)\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.yaxis.set_ticks_position('left')\n    ax.xaxis.set_ticks_position('bottom')\n    plt.scatter(x_obs, y_obs[:, i], label='Observations', c='black', s=15)\n    plt.plot(x, f[:, i], label='Truth', c='tab:orange')\n    plt.plot(x, means[:, i], label='GPAR', c='tab:blue')\n    plt.fill_between(x, lowers[:, i], uppers[:, i],\n                     facecolor='tab:blue', alpha=.25)\n    plt.plot(x, igp_means[:, i], label='IGP', c='tab:green')\n    plt.fill_between(x, igp_lowers[:, i], igp_uppers[:, i],\n                     facecolor='tab:green', alpha=.25)\n    plt.xlabel('$t$')\n    plt.ylabel('$y_{}$'.format(i + 1))\n    if i == 2:\n        leg = plt.legend(facecolor='#eeeeee')\n        leg.get_frame().set_linewidth(0)\n\nplt.tight_layout()\nplt.savefig('examples/paper/synthetic_prediction.pdf')\nplt.show()\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/wesselb/gpar", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "gpar", "package_url": "https://pypi.org/project/gpar/", "platform": "", "project_url": "https://pypi.org/project/gpar/", "project_urls": {"Homepage": "https://github.com/wesselb/gpar"}, "release_url": "https://pypi.org/project/gpar/0.1.4/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Implementation of the Gaussian Process Autoregressive Regression Model", "version": "0.1.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1><a href=\"http://github.com/wesselb/gpar\" rel=\"nofollow\">GPAR</a></h1>\n<p><a href=\"https://travis-ci.org/wesselb/gpar\" rel=\"nofollow\"><img alt=\"Build\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b88d96e499f3dfb08f325abbe4b2c2f687afc0e5/68747470733a2f2f7472617669732d63692e6f72672f77657373656c622f677061722e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://coveralls.io/github/wesselb/gpar?branch=master\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8eef74e23637ec198cb0e25daf2d86a783510dfd/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f77657373656c622f677061722f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562\"></a>\n<a href=\"https://wesselb.github.io/gpar\" rel=\"nofollow\"><img alt=\"Latest Docs\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/35e0b9e630dbc347bd4718399aa51fb3bb2bb889/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c75652e737667\"></a></p>\n<p>Implementation of the Gaussian Process Autoregressive Regression Model.\nSee the <a href=\"https://arxiv.org/abs/1802.07182\" rel=\"nofollow\">paper</a>, and see the <a href=\"https://wesselb.github.io/gpar\" rel=\"nofollow\">docs</a>.</p>\n<p><em>Note:</em> GPAR requires Python 3.6 or higher.</p>\n<ul>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#basic-usage\" rel=\"nofollow\">Basic Usage</a></li>\n<li><a href=\"#features\" rel=\"nofollow\">Features</a>\n<ul>\n<li><a href=\"#input-and-output-dependencies\" rel=\"nofollow\">Input and Output Dependencies</a></li>\n<li><a href=\"#output-transformation\" rel=\"nofollow\">Output Transformation</a></li>\n<li><a href=\"#sampling\" rel=\"nofollow\">Sampling</a></li>\n<li><a href=\"#logpdf-computation\" rel=\"nofollow\">Logpdf Computation</a></li>\n<li><a href=\"#inducing-points\" rel=\"nofollow\">Inducing Points</a></li>\n</ul>\n</li>\n<li><a href=\"#example-examples-paper-synthetic-py\" rel=\"nofollow\">Example (examples/paper/synthetic.py)</a></li>\n</ul>\n<h2>Installation</h2>\n<p>Before installing the package, please ensure that <code>gcc</code> and <code>gfortran</code> are\navailable.\nOn OS X, these are both installed with <code>brew install gcc</code>;\nusers of Anaconda may want to instead consider <code>conda install gcc</code>.\nOn Linux, <code>gcc</code> is most likely already available, and <code>gfortran</code> can be\ninstalled with <code>apt-get install gfortran</code>.\nThen simply</p>\n<pre><code>pip install gpar\n</code></pre>\n<h2>Basic Usage</h2>\n<p>A simple instance of GPAR can be created as follows:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">gpar</span> <span class=\"kn\">import</span> <span class=\"n\">GPARRegressor</span>\n\n<span class=\"n\">gpar</span> <span class=\"o\">=</span> <span class=\"n\">GPARRegressor</span><span class=\"p\">(</span><span class=\"n\">replace</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">impute</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n                     <span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">,</span>\n                     <span class=\"n\">linear</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">linear_scale</span><span class=\"o\">=</span><span class=\"mf\">100.0</span><span class=\"p\">,</span>\n                     <span class=\"n\">nonlinear</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">nonlinear_scale</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">,</span>\n                     <span class=\"n\">noise</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span>\n                     <span class=\"n\">normalise_y</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<p>Here the keyword arguments have the following meaning:</p>\n<ul>\n<li>\n<p><code>replace=True</code>: Replace data points with the posterior mean of the previous\nlayer before feeding them into the next layer. This helps the model deal\nwith noisy data, but may discard important structure in the data if  the\nfit is bad.</p>\n</li>\n<li>\n<p><code>impute=True</code>: GPAR requires that data is <em>closed downwards</em>. If this is\nnot the case, the model will be unable to use part of the data. Setting\n<code>impute</code> to <code>True</code> lets GPAR impute data points to ensure that the data is\nclosed downwards.</p>\n</li>\n<li>\n<p><code>scale=1.0</code>: Initialisation of the length scale with respect to the inputs.</p>\n</li>\n<li>\n<p><code>linear=True</code>: Use linear dependencies between outputs.</p>\n</li>\n<li>\n<p><code>linear_scale=True</code>: Initialisation of the length scale of the linear\ndependencies between outputs.</p>\n</li>\n<li>\n<p><code>nonlinear=True</code>: Also use nonlinear dependencies between outputs.</p>\n</li>\n<li>\n<p><code>nonlinear_scale=1.0</code>: Initialisation of the length scale of the nonlinear\ndependencies between outputs. <em>Important:</em> this length scale applies\n<em>after</em> possible normalisation of the outputs (see below), in which case\n<code>nonlinear_scale=1.0</code> corresponds to a simple, but nonlinear relationship.</p>\n</li>\n<li>\n<p><code>noise=0.1</code>: Initialisation of the variance of the observation noise.</p>\n</li>\n<li>\n<p><code>normalise_y=True</code>: Internally, work with a normalised version of the\noutputs by subtracting the mean and dividing by the standard deviation.\nPredictions will be transformed back appropriately.</p>\n</li>\n</ul>\n<p>In the above, any scalar hyperparameter may be replaced by a list of values\nto initialise each layer separately, e.g. <code>scale=[1.0, 2.0]</code>. See the\ndocumentation for a full overview of the keywords that may be passed to\n<code>GPARRegressor</code>.</p>\n<p>To fit GPAR, call <code>gpar.fit(x_train, y_train)</code> where <code>x_train</code> are the training\ninputs and <code>y_train</code> the training outputs. The inputs <code>x_train</code> must have shape\n$n$ or $n \\times m$, where $n$ is  the number of data points and $m$ the\nnumber of input features, and the outputs <code>y_train</code> must have shape $n$ or $n\n\\times p$, where $p$ is the number of outputs.</p>\n<p>Finally, to make predictions, call</p>\n<pre><span class=\"n\">means</span> <span class=\"o\">=</span> <span class=\"n\">gpar</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">num_samples</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n</pre>\n<p>to get the predictive means, or</p>\n<pre><span class=\"n\">means</span><span class=\"p\">,</span> <span class=\"n\">lowers</span><span class=\"p\">,</span> <span class=\"n\">uppers</span> <span class=\"o\">=</span> <span class=\"n\">gpar</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span>\n                                     <span class=\"n\">num_samples</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> \n                                     <span class=\"n\">credible_bounds</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<p>to also get lower and upper 95% central marginal credible bounds. If you wish\nto predict the underlying latent function instead of the observed values, set\n<code>latent=True</code> in the call to <code>GPARRegressor.predict</code>.</p>\n<h2>Features</h2>\n<h3>Input and Output Dependencies</h3>\n<p>Using keywords arguments, <code>GPARRegressor</code> can be configured to specify the\ndependencies with respect to the inputs and between the outputs. The following\ndependencies can be specified:</p>\n<ul>\n<li>\n<p><strong>Linear input dependencies:</strong> Set <code>linear_input=True</code> and specify the\nlength scale with <code>linear_input_scale</code>.</p>\n</li>\n<li>\n<p><strong>Nonlinear input dependencies:</strong> This is enabled by default. The length\nscale can be specified using <code>scale</code>. To tie these length scales across all\nlayers, set <code>scale_tie=True</code>.</p>\n</li>\n<li>\n<p><strong>Locally periodic input dependencies:</strong> Set <code>per=True</code> and specify the period\nwith <code>per_period</code>, the length scale with <code>per_scale</code>, and the length\nscale on which the periodicity changes with <code>per_decay</code>.</p>\n</li>\n<li>\n<p><strong>Linear output dependencies:</strong> Set <code>linear=True</code> and specify the length\nscale with <code>linear_scale</code>.</p>\n</li>\n<li>\n<p><strong>Nonlinear output dependencies:</strong> Set <code>nonlinear=True</code> and specify the\nlength scale with <code>nonlinear_scale</code>.</p>\n</li>\n</ul>\n<p>All nonlinear kernels are exponentiated quadratic kernels. If you wish to\ninstead use rational quadratic kernels, set <code>rq=True</code>.</p>\n<p>All parameters can be set to a list of values to initialise the value for\neach layer separately.</p>\n<p>To let every layer depend only the <code>k</code>th previous layers, set <code>markov=k</code>.</p>\n<h3>Output Transformation</h3>\n<p>One may want to apply a transformation to the data before fitting the model,\ne.g. $y\\mapsto\\log(y)$ in the case of positive data. Such a transformation can\nbe specified by setting the <code>transform_y</code> keyword argument for <code>GPARRegressor</code>.\nThe following transformations are available:</p>\n<ul>\n<li>\n<p><code>log_transform</code>: $y \\mapsto \\log(y)$.</p>\n</li>\n<li>\n<p><code>squishing_transform</code>: $y \\mapsto \\operatorname{sign}(y) \\log(1 + |y|)$.</p>\n</li>\n</ul>\n<h3>Sampling</h3>\n<p>Sampling from the model can be done with <code>GPARRegressor.sample</code>. The keyword\nargument <code>num_samples</code> specifies the number of samples, and <code>latent</code>\nspecifies whether to sample from the underlying latent function or the\nobserved values. Sampling from the <em>prior</em> and <em>posterior</em> (model must be fit\nfirst) can be done as follows:</p>\n<pre><span class=\"n\">sample</span> <span class=\"o\">=</span> <span class=\"n\">gpar</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>  <span class=\"c1\"># Sample two outputs from the prior.</span>\n\n<span class=\"n\">sample</span> <span class=\"o\">=</span> <span class=\"n\">gpar</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">posterior</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># Sample from the posterior.</span>\n</pre>\n<h3>Logpdf Computation</h3>\n<p>The logpdf of data can be computed with <code>GPARRegressor.logpdf</code>. To compute the\nlogpdf under the posterior, set <code>posterior=True</code>. To sample missing data to\ncompute an unbiased estimate of the <em>pdf</em>, <em>not logpdf</em>, set\n<code>sample_missing=True</code>.</p>\n<p>The logpdf can be computed without casting the inputs <code>x</code> and <code>y</code> to PyTorch\ntensors and without detaching the resulting logpdf from the computation\ngraph. For this, set <code>differentiable=True</code> in <code>GPARRegressor.logpdf</code>.</p>\n<h3>Inducing Points</h3>\n<p>Inducing points can be used to scale GPAR to large data sets. Simply set <code>x_ind</code>\nto the locations of the inducing points in <code>GPARRegressor</code>.</p>\n<h2>Example (<code>examples/paper/synthetic.py</code>)</h2>\n<p><img alt=\"Prediction\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/97aeae80ca67b1c33171f145a16d379cb69ddc52/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f77657373656c622f677061722f6d61737465722f726561646d655f6578616d706c655f70726564696374696f6e2e706e67\"></p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">gpar.regression</span> <span class=\"kn\">import</span> <span class=\"n\">GPARRegressor</span>\n\n<span class=\"c1\"># Create toy data set.</span>\n<span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"mi\">200</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">)</span>\n<span class=\"n\">noise</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>\n\n<span class=\"c1\"># Draw functions depending on each other in complicated ways.</span>\n<span class=\"n\">f1</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"mi\">10</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">pi</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">))</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">x</span> <span class=\"o\">**</span> <span class=\"mi\">4</span>\n<span class=\"n\">f2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">cos</span><span class=\"p\">(</span><span class=\"n\">f1</span><span class=\"p\">)</span> <span class=\"o\">**</span> <span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"mi\">3</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"n\">f3</span> <span class=\"o\">=</span> <span class=\"n\">f2</span> <span class=\"o\">*</span> <span class=\"n\">f1</span> <span class=\"o\">**</span> <span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"mi\">3</span> <span class=\"o\">*</span> <span class=\"n\">x</span>\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">((</span><span class=\"n\">f1</span><span class=\"p\">,</span> <span class=\"n\">f2</span><span class=\"p\">,</span> <span class=\"n\">f3</span><span class=\"p\">),</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">T</span>\n\n<span class=\"c1\"># Add noise and subsample.</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">f</span> <span class=\"o\">+</span> <span class=\"n\">noise</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span>\n<span class=\"n\">x_obs</span><span class=\"p\">,</span> <span class=\"n\">y_obs</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"p\">[::</span><span class=\"mi\">8</span><span class=\"p\">],</span> <span class=\"n\">y</span><span class=\"p\">[::</span><span class=\"mi\">8</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># Fit and predict GPAR.</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">GPARRegressor</span><span class=\"p\">(</span><span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span>\n                      <span class=\"n\">linear</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">linear_scale</span><span class=\"o\">=</span><span class=\"mf\">10.</span><span class=\"p\">,</span>\n                      <span class=\"n\">nonlinear</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">nonlinear_scale</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span>\n                      <span class=\"n\">noise</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span>\n                      <span class=\"n\">impute</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">replace</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">normalise_y</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">x_obs</span><span class=\"p\">,</span> <span class=\"n\">y_obs</span><span class=\"p\">)</span>\n<span class=\"n\">means</span><span class=\"p\">,</span> <span class=\"n\">lowers</span><span class=\"p\">,</span> <span class=\"n\">uppers</span> <span class=\"o\">=</span> \\\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">num_samples</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span> <span class=\"n\">credible_bounds</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">latent</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Fit and predict independent GPs: set markov=0.</span>\n<span class=\"n\">igp</span> <span class=\"o\">=</span> <span class=\"n\">GPARRegressor</span><span class=\"p\">(</span><span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span>\n                    <span class=\"n\">linear</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">linear_scale</span><span class=\"o\">=</span><span class=\"mf\">10.</span><span class=\"p\">,</span>\n                    <span class=\"n\">nonlinear</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">nonlinear_scale</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span>\n                    <span class=\"n\">noise</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">markov</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">normalise_y</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">igp</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">x_obs</span><span class=\"p\">,</span> <span class=\"n\">y_obs</span><span class=\"p\">)</span>\n<span class=\"n\">igp_means</span><span class=\"p\">,</span> <span class=\"n\">igp_lowers</span><span class=\"p\">,</span> <span class=\"n\">igp_uppers</span> <span class=\"o\">=</span> \\\n    <span class=\"n\">igp</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">num_samples</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span> <span class=\"n\">credible_bounds</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">latent</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Plot the result.</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mf\">2.5</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">rcParams</span><span class=\"p\">[</span><span class=\"s1\">'font.family'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">'serif'</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">rcParams</span><span class=\"p\">[</span><span class=\"s1\">'mathtext.fontset'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">'dejavuserif'</span>\n\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">):</span>\n    <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">spines</span><span class=\"p\">[</span><span class=\"s1\">'right'</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_visible</span><span class=\"p\">(</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">spines</span><span class=\"p\">[</span><span class=\"s1\">'top'</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_visible</span><span class=\"p\">(</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">yaxis</span><span class=\"o\">.</span><span class=\"n\">set_ticks_position</span><span class=\"p\">(</span><span class=\"s1\">'left'</span><span class=\"p\">)</span>\n    <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">xaxis</span><span class=\"o\">.</span><span class=\"n\">set_ticks_position</span><span class=\"p\">(</span><span class=\"s1\">'bottom'</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">x_obs</span><span class=\"p\">,</span> <span class=\"n\">y_obs</span><span class=\"p\">[:,</span> <span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">'Observations'</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"s1\">'black'</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"o\">=</span><span class=\"mi\">15</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">f</span><span class=\"p\">[:,</span> <span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">'Truth'</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"s1\">'tab:orange'</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">means</span><span class=\"p\">[:,</span> <span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">'GPAR'</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"s1\">'tab:blue'</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">fill_between</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">lowers</span><span class=\"p\">[:,</span> <span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">uppers</span><span class=\"p\">[:,</span> <span class=\"n\">i</span><span class=\"p\">],</span>\n                     <span class=\"n\">facecolor</span><span class=\"o\">=</span><span class=\"s1\">'tab:blue'</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=.</span><span class=\"mi\">25</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">igp_means</span><span class=\"p\">[:,</span> <span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">'IGP'</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"s1\">'tab:green'</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">fill_between</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">igp_lowers</span><span class=\"p\">[:,</span> <span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">igp_uppers</span><span class=\"p\">[:,</span> <span class=\"n\">i</span><span class=\"p\">],</span>\n                     <span class=\"n\">facecolor</span><span class=\"o\">=</span><span class=\"s1\">'tab:green'</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=.</span><span class=\"mi\">25</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">'$t$'</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$y_</span><span class=\"si\">{}</span><span class=\"s1\">$'</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">==</span> <span class=\"mi\">2</span><span class=\"p\">:</span>\n        <span class=\"n\">leg</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">facecolor</span><span class=\"o\">=</span><span class=\"s1\">'#eeeeee'</span><span class=\"p\">)</span>\n        <span class=\"n\">leg</span><span class=\"o\">.</span><span class=\"n\">get_frame</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">set_linewidth</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">tight_layout</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">savefig</span><span class=\"p\">(</span><span class=\"s1\">'examples/paper/synthetic_prediction.pdf'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</pre>\n\n          </div>"}, "last_serial": 6387482, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "83dd63517f5506e45928fd57cb113367", "sha256": "47309749ff99ca700158106f7f3715180427bfa3313434a04b03214d1622ae15"}, "downloads": -1, "filename": "gpar-0.1.0.tar.gz", "has_sig": false, "md5_digest": "83dd63517f5506e45928fd57cb113367", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17438, "upload_time": "2019-06-13T17:02:47", "upload_time_iso_8601": "2019-06-13T17:02:47.385579Z", "url": "https://files.pythonhosted.org/packages/49/49/ffc058c42b45f53dc0153979f27437b710e2493daa5da8514af798870187/gpar-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "d1b2b608fd2de111d3a314a2b4685dd9", "sha256": "574344d5d1d422a3adcdde63d0619423e9ef30b4172502498088ccc3f61630e4"}, "downloads": -1, "filename": "gpar-0.1.1.tar.gz", "has_sig": false, "md5_digest": "d1b2b608fd2de111d3a314a2b4685dd9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17617, "upload_time": "2019-06-13T18:01:54", "upload_time_iso_8601": "2019-06-13T18:01:54.668823Z", "url": "https://files.pythonhosted.org/packages/fd/e5/a7b738d94b62d24ef36647eab6671fee5974bc03ab29147a758148e78b8f/gpar-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "c038b7a280e72ae919fb2b334bedc853", "sha256": "efe499c3855e174e7f2ac2d50d31c9db4ce320d840016362ac92ccb14dbc72c7"}, "downloads": -1, "filename": "gpar-0.1.2.tar.gz", "has_sig": false, "md5_digest": "c038b7a280e72ae919fb2b334bedc853", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17631, "upload_time": "2019-07-09T22:40:16", "upload_time_iso_8601": "2019-07-09T22:40:16.295262Z", "url": "https://files.pythonhosted.org/packages/51/9a/174767faae814b842d807ce81c37ae2177befa3cfe0ddf03f9f96b91e656/gpar-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "c3cfc07cdfdab9829772d1d875243c01", "sha256": "3ac2c067a4b3704be48eb78616302258a2c21408e5e8115048198d0d6b04697d"}, "downloads": -1, "filename": "gpar-0.1.3.tar.gz", "has_sig": false, "md5_digest": "c3cfc07cdfdab9829772d1d875243c01", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17599, "upload_time": "2019-11-06T00:03:30", "upload_time_iso_8601": "2019-11-06T00:03:30.812608Z", "url": "https://files.pythonhosted.org/packages/96/13/60462c72a1705fb515f875fe45eabb693ddc4b96faea9ca356afcf9279b2/gpar-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "dc5b27bb7d1a976190ebd38af5066353", "sha256": "3cc991e246b93f4d9370f614c3e48b469ca583df3cc4dfdc63f294399af62233"}, "downloads": -1, "filename": "gpar-0.1.4.tar.gz", "has_sig": false, "md5_digest": "dc5b27bb7d1a976190ebd38af5066353", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 21471, "upload_time": "2020-01-02T19:53:58", "upload_time_iso_8601": "2020-01-02T19:53:58.431322Z", "url": "https://files.pythonhosted.org/packages/a6/d2/bd19916c496f9f5f71d9b719af9569043a17db50883c5963737a5df1cc46/gpar-0.1.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "dc5b27bb7d1a976190ebd38af5066353", "sha256": "3cc991e246b93f4d9370f614c3e48b469ca583df3cc4dfdc63f294399af62233"}, "downloads": -1, "filename": "gpar-0.1.4.tar.gz", "has_sig": false, "md5_digest": "dc5b27bb7d1a976190ebd38af5066353", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 21471, "upload_time": "2020-01-02T19:53:58", "upload_time_iso_8601": "2020-01-02T19:53:58.431322Z", "url": "https://files.pythonhosted.org/packages/a6/d2/bd19916c496f9f5f71d9b719af9569043a17db50883c5963737a5df1cc46/gpar-0.1.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:55:21 2020"}