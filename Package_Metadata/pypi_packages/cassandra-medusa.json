{"info": {"author": "The Last Pickle", "author_email": "medusa@thelastpickle.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Intended Audience :: System Administrators", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3.5", "Topic :: Database", "Topic :: System :: Archiving :: Backup"], "description": "<!--\n# Copyright 2019 Spotify AB. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n-->\n\n[![Join the chat at https://gitter.im/thelastpickle/cassandra-medusa](https://badges.gitter.im/thelastpickle/cassandra-medusa.svg)](https://gitter.im/thelastpickle/cassandra-medusa?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nMedusa\n======\n\nMedusa is an Apache Cassandra backup system.\n\nFeatures\n--------\nMedusa is a command line tool that offers the following features:\n\n* Single node backup\n* Single node restore\n* Cluster wide in place restore (restoring on the same cluster that was used for the backup)\n* Cluster wide remote restore (restoring on a different cluster than the one used for the backup)\n* Backup purge\n* Support for local storage, Google Cloud Storage (GCS) and AWS S3 through [Apache Libcloud](https://libcloud.apache.org/). Can be extended to support other storage providers supported by Apache Libcloud.\n* Support for clusters using single tokens or vnodes\n* Full or incremental backups\n\n\nSetup\n-----\nChoose and initialize the storage system:\n\n* Local storage can be used in conjunction with NFS mounts to store backups off nodes. The backup directory must be accessible from all nodes in the cluster and mounted appropriately. If the backup folder is not shared, the nodes will only see their own backups.\n* [Google Cloud Storage setup](https://github.com/thelastpickle/cassandra-medusa/blob/master/docs/gcs_setup.md)\n* [AWS S3 setup](https://github.com/thelastpickle/cassandra-medusa/blob/master/docs/aws_s3_setup.md)\n* [Ceph Object Gateway S3 API](https://github.com/thelastpickle/cassandra-medusa/blob/master/docs/ceph_s3_setup.md)\n\nInstall Medusa on each Cassandra node:\n\n### Online installation\n\n* if the storage backend is a locally accessible shared storage, run `sudo pip3 install cassandra-medusa`\n* if your backups are to be stored in AWS S3, run `sudo pip3 install cassandra-medusa[S3]`\n* if your backups are to be stored in Google Cloud Storage, run `sudo pip3 install cassandra-medusa[GCS]`\n\nRunning the installation using `sudo` is necessary to have the `/usr/local/bin/medusa` script created properly.\n\n### Offline installation\n\nIf your Cassandra servers do not have internet access:  \n\n- on a machine with the same target os and python version, clone the cassandra-medusa repo and cd into the root directory\n- run `mkdir pip_dependencies && pip download -r requirements.txt -d medusa_dependencies` to download the dependencies into a sub directory\n- run `cp requirements.txt medusa_dependencies/`\n- run `tar -zcf medusa_dependencies.tar.gz medusa_dependencies` to compress the dependencies\n- Upload the archive to all Cassandra nodes and decompress it\n- run `pip install -r medusa_dependencies/requirements.txt --no-index --find-links` to install the dependencies on the nodes\n- install Medusa using `python setup.py install` from the cassandra-medusa source directory\n\n### Configure Medusa\n\nCreate the `/etc/medusa` directory if it doesn't exist, and create a file named `/etc/medusa/medusa.ini` with the content of [medusa-example.ini](https://github.com/thelastpickle/cassandra-medusa/blob/master/medusa-example.ini).\nModify it to match your requirements:\n\n```\n[cassandra]\n;stop_cmd = /etc/init.d/cassandra stop\n;start_cmd = /etc/init.d/cassandra start\n;config_file = <path to cassandra.yaml. Defaults to /etc/cassandra/cassandra.yaml>\n;cql_username = <username>\n;cql_password = <password>\n;nodetool_username =  <my nodetool username>\n;nodetool_password =  <my nodetool password>\n;nodetool_password_file_path = <path to nodetool password file>\n;nodetool_host = <host name or IP to use for nodetool>\n;nodetool_port = <port number to use for nodetool>\n\n; Command ran to verify if Cassandra is running on a node. Defaults to \"nodetool version\"\n;check_running = nodetool version\n\n[storage]\nstorage_provider = <Storage system used for backups>\n; storage_provider should be either of \"local\", \"google_storage\" or the s3_* values from\n; https://github.com/apache/libcloud/blob/trunk/libcloud/storage/types.py\n\n; Name of the bucket used for storing backups\nbucket_name = cassandra_backups\n\n; JSON key file for service account with access to GCS bucket or AWS credentials file (home-dir/.aws/credentials)\nkey_file = /etc/medusa/credentials\n\n; Path of the local storage bucket (used only with 'local' storage provider)\n;base_path = /path/to/backups\n\n; Any prefix used for multitenancy in the same bucket\n;prefix = clusterA\n\n;fqdn = <enforce the name of the local node. Computed automatically if not provided.>\n\n; Number of days before backups are purged. 0 means backups don't get purged by age (default)\nmax_backup_age = 0\n; Number of backups to retain. Older backups will get purged beyond that number. 0 means backups don't get purged by count (default)\nmax_backup_count = 0\n; Both thresholds can be defined for backup purge.\n\n; Used to throttle S3 backups/restores:\ntransfer_max_bandwidth = 50MB/s\n\n; Max number of downloads/uploads. Not used by the GCS backend.\nconcurrent_transfers = 1\n\n; Size over which S3 uploads will be using the awscli with multi part uploads. Defaults to 100MB.\nmulti_part_upload_threshold = 104857600\n\n[monitoring]\n;monitoring_provider = <Provider used for sending metrics. Currently either of \"ffwd\" or \"local\">\n\n[ssh]\n;username = <SSH username to use for restoring clusters>\n;key_file = <SSH key for use for restoring clusters. Expected in PEM unencrypted format.>\n\n[checks]\n;health_check = <Which ports to check when verifying a node restored properly. Options are 'cql' (default), 'thrift', 'all'.>\n;query = <CQL query to run after a restore to verify it went OK>\n;expected_rows = <Number of rows expected to be returned when the query runs. Not checked if not specified.>\n;expected_result = <Coma separated string representation of values returned by the query. Checks only 1st row returned, and only if specified>\n\n[logging]\n; Controls file logging, disabled by default.\n; enabled = 0\n; file = medusa.log\n; level = INFO\n\n; Control the log output format\n; format = [%(asctime)s] %(levelname)s: %(message)s\n\n; Size over which log file will rotate\n; maxBytes = 20000000\n\n; How many log files to keep\n; backupCount = 50\n\n```\n\n\nUsage\n=====\n\n```\n$ medusa\nUsage: medusa [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  -v, --verbosity          Verbosity\n  --without-log-timestamp  Do not show timestamp in logs\n  --config-file TEXT       Specify config file\n  --bucket-name TEXT       Bucket name\n  --key-file TEXT          GCP credentials key file\n  --prefix TEXT            Prefix for shared storage\n  --fqdn TEXT              Act as another host\n  --ssh-username TEXT\n  --ssh-key-file TEXT\n  --help                   Show this message and exit.\n\nCommands:\n  backup                          Backup Cassandra\n  build-index                     Builds indices for all present backups\n                                  and...\n  download                        Download backup\n  fetch-tokenmap                  Backup Cassandra\n  get-last-complete-cluster-backup\n                                  Pints the name of the latest complete\n                                  cluster...\n  list-backups                    List backups\n  purge                           Delete obsolete backups\n  report-last-backup              Find time since last backup and print it\n                                  to...\n  restore-cluster                 Restore Cassandra cluster\n  restore-node                    Restore single Cassandra node\n  status                          Show status of backups\n  verify                          Verify the integrity of a backup\n```\n\n\nPerforming a backup\n-------------------\n\n```\n$ medusa backup --help\nUsage: medusa backup [OPTIONS]\n\n  Backup Cassandra\n\nOptions:\n  --backup-name TEXT           Custom name for the backup\n  --stagger INTEGER            Check for staggering initial backups for\n                               duration seconds\n  --mode [full|differential]\n  --help                       Show this message and exit.\n```\n\nOnce Medusa is setup, you can create a **full** backup with the following command:\n\n```\n$ medusa backup --backup-name=<name of the backup>\n```\n\nIn order to perform an **differential** backup, add the `--mode=differential` argument to your command:\n\n```\n$ medusa backup --backup-name=<name of the backup> --mode=differential\n```\n\nTo perform cluster wide backups, the command must run on all nodes in the cluster, using the same backup name.\n\n\nListing existing backups\n------------------------\n\n```\n$ medusa list-backups --help\nUsage: medusa list-backups [OPTIONS]\n\n  List backups\n\nOptions:\n  --show-all / --no-show-all  List all backups in the bucket\n  --help                      Show this message and exit.\n```\n\nList all backups for the current node/cluster:\n\n```\n$ medusa list-backups\n2019080507 (started: 2019-08-05 07:07:03, finished: 2019-08-05 08:01:04)\n2019080607 (started: 2019-08-06 07:07:04, finished: 2019-08-06 07:59:08)\n2019080707 (started: 2019-08-07 07:07:04, finished: 2019-08-07 07:59:55)\n2019080807 (started: 2019-08-08 07:07:03, finished: 2019-08-08 07:59:22)\n2019080907 (started: 2019-08-09 07:07:04, finished: 2019-08-09 08:00:14)\n2019081007 (started: 2019-08-10 07:07:04, finished: 2019-08-10 08:02:41)\n2019081107 (started: 2019-08-11 07:07:04, finished: 2019-08-11 08:03:48)\n2019081207 (started: 2019-08-12 07:07:04, finished: 2019-08-12 07:59:59)\n2019081307 (started: 2019-08-13 07:07:03, finished: Incomplete [179 of 180 nodes])\n2019081407 (started: 2019-08-14 07:07:04, finished: 2019-08-14 07:56:44)\n2019081507 (started: 2019-08-15 07:07:03, finished: 2019-08-15 07:50:24)\n```\n\nWhen listing backups from a cluster which is not the backed up one, please add the `--show-all` flag to bypass the filter and display all existing backups from storage.\n\n\nRestoring a single node\n-----------------------\n\n```\n$ medusa restore-node --help\nUsage: medusa restore-node [OPTIONS]\n\n  Restore single Cassandra node\n\nOptions:\n  --temp-dir TEXT         Directory for temporary storage\n  --backup-name TEXT      Backup name  [required]\n  --in-place              Indicates if the restore happens on the node the\n                          backup was done on.\n  --keep-auth             Keep system_auth keyspace as found on the node\n  --seeds TEXT            Nodes to wait for after downloading backup but\n                          before starting C*\n  --verify / --no-verify  Verify that the cluster is operational after the\n                          restore completes,\n  --keyspace TEXT         Restore tables from this keyspace\n  --table TEXT            Restore only this table\n  --use-sstableloader     Use the sstableloader to load the backup into the\n                          cluster\n  --help                  Show this message and exit.\n```\n\nIn order to restore a backup on a single node, run the following command:\n\n```\n$ sudo medusa restore-node --backup-name=<name of the backup>\n```\n\nMedusa will need to run with `sudo` as it will:\n\n* stop Cassandra\n* wipe the existing files\n* Download the files from backup storage locally and move them to Cassandra's storage directory\n* Change the ownership of the files back to the one owning the Cassandra data directory\n* start Cassandra\n\nThe `--use-sstableloader` flag will be useful for restoring data when the topology doesn't match between the backed up cluster and the restore one.\nIn this mode, Cassandra will not be stopped and downloaded SSTables will be loaded into Cassandra by the sstableloader. Data already present in the cluster will not be altered.\n\nThe `--fqdn` argument allows to force the node to act on behalf of another backup node. It can take several hostnames separated by commas in order to restore several nodes backup using the sstableloader.\n\nThe `--keyspace` option allows limiting the restore to all tables in the given keyspace. The `--table` option allows limiting the restore to just one table. The tables must be specified in the `keyspace.table` format. It is possible to repeat both of the options. Medusa will make an union of everything specified and restore all keyspaces and tables mentioned. The `--keyspace` option takes precedence, so using `--keyspace ks1` and then adding `--table ks1.t` will not limit the restore to just one table - everything from `ks1` will be restored.\n\nRestoring a cluster\n-------------------\n\n```\n$ medusa restore-cluster --help\nUsage: medusa restore-cluster [OPTIONS]\n\n  Restore Cassandra cluster\n\nOptions:\n  --backup-name TEXT              Backup name  [required]\n  --seed-target TEXT              seed of the target hosts\n  --temp-dir TEXT                 Directory for temporary storage\n  --host-list TEXT                List of nodes to restore with the associated\n                                  target host\n  --keep-auth / --overwrite-auth  Keep/overwrite system_auth as found on the\n                                  nodes\n  -y, --bypass-checks             Bypasses the security check for restoring a\n                                  cluster\n  --verify / --no-verify          Verify that the cluster is operational after\n                                  the restore completes,\n  --keyspace TEXT                 Restore tables from this keyspace\n  --table TEXT                    Restore only this table\n  --use-sstableloader             Use the sstableloader to load the backup\n                                  into the cluster\n  --help                          Show this message and exit.\n```\n\n## Topology matches between the backup and the restore cluster\nIn this section, we will describe procedures that apply to the following cases:\n\n* Vnodes are not used (single token) + both the backup and the restore cluster have the exact same number of nodes and token assignments.\n* Vnodes are used + both the backup and the restore cluster have the exact same number of nodes (regardless token assignments).\n\nThis method is by far the fastest as it replaces SSTables directly on disk.\n\n### In place (same hardware)\n\nIn order to restore a backup for a full cluster, in the case where the restored cluster is the exact same as the backed up one:\n\n```\n$ medusa restore-cluster --backup-name=<name of the backup> --seed-target node1.domain.net\n```\n\nMedusa will need to run without `sudo` as it will connect through ssh to all nodes in the cluster in order to perform remote operations. It will, by default, use the current user to connect and rely on agent forwarding for authentication (you must ssh into the server using `-A` to enable agent forwarding).\nThe `--seed-target` node is used to connect to Cassandra and retrieve the current topology of the cluster. This allows Medusa to map each backup to the correct node in the current topology.\n\nThe following operations will take place:\n\n* Stop Cassandra on all nodes\n* Check that the current topology matches the backed up one (if not, will fallback to the next section, using the sstableloader)\n* Run `restore-node` on each node in the cluster\n* Start Cassandra on all nodes\n\n\n### Remotely (different hardware)\n\nIn order to restore a backup of a full cluster but on different servers.\nThis can be used to restore a production cluster data to a staging cluster (with the same number of nodes), or recovering from an outage where previously used hardware cannot be re-used.\n\n```\n$ medusa restore-cluster --backup-name=<name of the backup> --host-list /etc/medusa/restore_mapping.txt\n```\n\nThe `restore-mapping.txt` file will provide the mapping between the backed up cluster nodes and the restore cluster ones. It is expected in the following CSV format: `<Is it a seed node?>,<target node>,<source node>`\n\nSample file:\n\n```\nTrue,new_node1.foo.net,old_node1.foo.net\nTrue,new_node2.foo.net,old_node2.foo.net\nFalse,new_node3.foo.net,old_node3.foo.net\n```\n\nMedusa will need to run without `sudo` as it will connect through ssh to all nodes in the cluster in order to perform remote operations. It will, by default, use the current user to connect and rely on agent forwarding for authentication (you must ssh into the server using `-A` to enable agent forwarding).\n\n* Stop Cassandra on all nodes\n* Check that the current topology matches the backed up one (if not, will fallback to the next section, using the sstableloader)\n* Run `restore-node` on each node in the cluster\n* Start Cassandra on all nodes\n\nBy default, Medusa will overwrite the `system_auth` keyspace with the backed up one. If you want to retain the existing system_auth keyspace, you'll have to run `restore-cluster` with the `--keep-auth` flag:\n\n```\n$ medusa restore-cluster --backup-name=<name of the backup> --host-list /etc/medusa/restore_mapping.txt --keep-auth\n```\n\n## Topology does not match between the backup and the restore cluster\nIn this section, we will describe procedures that apply to the other cases:\n\n* Vnodes are not used (single token) + the backup and restore cluster have a different token assignement or a different number of nodes.\n* Vnodes are used + the backup and the restore cluster have a different number of nodes.\n\n\nThis case will be detected automatically by Medusa when checking the topologies, but it can be enforced by adding the `--use-sstableloader` flag to the `restore-cluster` command.\nThis technique allows to restore any backup on any cluster of any size, **at the expense of some overhead.**\n\n* First, the sstableloader will have to parse all the backed up SSTables in order to send them to the appropriate nodes, which can take way more time on large volumes.\n* Then, the amount of data loaded into the restore cluster will be multiplied by the replication factor of the keyspace, since we will be restoring the backups from all replicas without any merge (SSTables from different backups will contain copies of the same data).\n**With RF=3, the cluster will contain approximately three times the data load from the backup.** The size will drop back to normal levels once compaction has caught up (a major compaction could be necessary).\n\nUsing this technique, Cassandra will not be stopped on the nodes and the following steps will happen:\n\n* The data model will be updated as follows:\n\t* The schema will be downloaded from the backup and categorized by object types into individual queries\n\t* Missing keyspaces will be created\n\t* Existing Materialized Views from the backup schema will be dropped (other MVs remain untouched)\n\t* Existing tables from the backup schema will be dropped (other tables remain untouched)\n\t* Existing User Defined Types from the backup schema will be (re)created\n\t* Tables from the backup will be created\n\t* Secondary indexes will be created\n\t* Materialized Views will be created\n* Backup nodes will be assigned to target nodes (one target node can be responsible from zero to several backup nodes)\n* Run `restore-node` on each node in the target cluster, passing a list of backup nodes as `--fqdn` and the `--sstableloader` flag\n\t* for each specified node in `--fqdn`:\n\t\t* Download the files from backup storage locally\n\t\t* Invoke the locally installed sstableloader to load them using the local C* instance as contact point\n\nIf your cluster is configured with the default `auto_snapshot: true` then dropping the tables will trigger a snapshot that will persist their data on disk. Medusa will not clear this snapshot after restore.\n\nThe `--keyspace` and `--table` options of `restore-cluster` command work exactly the same way as they do for the `restore-node` command.\n\n\nVerify an existing backup\n-------------------------\n\n```\n$ medusa verify --help\nUsage: medusa verify [OPTIONS]\n\n  Verify the integrity of a backup\n\nOptions:\n  --backup-name TEXT  Backup name  [required]\n  --help              Show this message and exit.\n```\n\nRun a health check on a backup, which will verify that:\n\n* All nodes have completed the backup\n* All files in the manifest are present in storage\n* All backed up files are present in the manifest\n* All files have the right hash as stored in the manifest\n\n```\n$ medusa verify --backup-name=2019090503\nValidating 2019090503 ...\n- Completion: OK!\n- Manifest validated: OK!!\n```\n\nIn case some nodes in the cluster didn't complete the backups, you'll get the following output:\n\n```\n$ medusa verify --backup-name=2019081703\nValidating 2019081703 ...\n- Completion: Not complete!\n  - [127.0.0.2] Backup missing\n- Manifest validated: OK!!\n```\n\nPurge old backups\n-----------------\n\n```\n$ medusa purge --help\nUsage: medusa purge [OPTIONS]\n\n  Delete obsolete backups\n\nOptions:\n  --help  Show this message and exit.\n```\n\nIn order to remove obsolete backups from storage, according to the configured `max_backup_age` and/or `max_backup_count`, run:\n\n```\n$ medusa purge\n[2019-09-04 13:44:16] INFO: Starting purge\n[2019-09-04 13:44:17] INFO: 25 backups are candidate to be purged\n[2019-09-04 13:44:17] INFO: Purging backup 2019082513...\n[2019-09-04 13:44:17] INFO: Purging backup 2019082514...\n[2019-09-04 13:44:18] INFO: Purging backup 2019082515...\n[2019-09-04 13:44:18] INFO: Purging backup 2019082516...\n[2019-09-04 13:44:19] INFO: Purging backup 2019082517...\n[2019-09-04 13:44:19] INFO: Purging backup 2019082518...\n[2019-09-04 13:44:19] INFO: Purging backup 2019082519...\n[2019-09-04 13:44:20] INFO: Purging backup 2019082520...\n[2019-09-04 13:44:20] INFO: Purging backup 2019082521...\n[2019-09-04 13:44:20] INFO: Purging backup 2019082522...\n[2019-09-04 13:44:21] INFO: Purging backup 2019082523...\n[2019-09-04 13:44:21] INFO: Purging backup 2019082600...\n[2019-09-04 13:44:21] INFO: Purging backup 2019082601...\n[2019-09-04 13:44:22] INFO: Purging backup 2019082602...\n[2019-09-04 13:44:22] INFO: Purging backup 2019082603...\n[2019-09-04 13:44:23] INFO: Purging backup 2019082604...\n[2019-09-04 13:44:23] INFO: Purging backup 2019082605...\n[2019-09-04 13:44:23] INFO: Purging backup 2019082606...\n[2019-09-04 13:44:24] INFO: Purging backup 2019082607...\n[2019-09-04 13:44:24] INFO: Purging backup 2019082608...\n[2019-09-04 13:44:24] INFO: Purging backup 2019082609...\n[2019-09-04 13:44:25] INFO: Purging backup 2019082610...\n[2019-09-04 13:44:25] INFO: Purging backup 2019082611...\n[2019-09-04 13:44:25] INFO: Purging backup 2019082612...\n[2019-09-04 13:44:26] INFO: Purging backup 2019082613...\n[2019-09-04 13:44:26] INFO: Cleaning up orphaned files...\n[2019-09-04 13:45:59] INFO: Purged 652 objects with a total size of 3.74 MB\n\n```\n\nSince SSTables and meta files are stored in different places for differential backups, the purge is a two step process:\n\n* Delete all backup directories\n* Scan active backup files from manifests and compare with the list of SSTables in the `data` directory. All SSTables present in the `data` directory but absent from all manifests will get deleted in that step.\n\n\nCheck the status of a backup\n----------------------------\n\n```\n$ medusa status --help\nUsage: medusa status [OPTIONS]\n\n  Show status of backups\n\nOptions:\n  --backup-name TEXT  Backup name  [required]\n  --help              Show this message and exit.\n```\n\nOutputs a summary of a specific backup status:\n\n```\n$ medusa status --backup-name=2019090503\n2019090503\n- Started: 2019-09-05 03:53:04, Finished: 2019-09-05 04:49:52\n- 32 nodes completed, 0 nodes incomplete, 0 nodes missing\n- 163256 files, 12.20 TB\n```\n\n\nDisplay informations on the latest backup\n-----------------------------------------\n```\n$ medusa report-last-backup --help\nUsage: medusa report-last-backup [OPTIONS]\n\n  Find time since last backup and print it to stdout :return:\n\nOptions:\n  --push-metrics  Also push the information via metrics\n  --help          Show this message and exit.\n\n```\n\nThis command will display several informations on the latest backup:\n\n```\n$ medusa report-last-backup\n[2019-09-04 12:56:15] INFO: Latest node backup finished 18746 seconds ago\n[2019-09-04 12:56:18] INFO: Latest complete backup:\n[2019-09-04 12:56:18] INFO: - Name: 2019090407\n[2019-09-04 12:56:18] INFO: - Finished: 18173 seconds ago\n[2019-09-04 12:56:19] INFO: Latest backup:\n[2019-09-04 12:56:19] INFO: - Name: 2019090407\n[2019-09-04 12:56:19] INFO: - Finished: True\n[2019-09-04 12:56:19] INFO: - Details - Node counts\n[2019-09-04 12:56:19] INFO: - Complete backup: 180 nodes have completed the backup\n[2019-09-04 12:58:47] INFO: - Total size: 94.69 TiB\n[2019-09-04 12:58:55] INFO: - Total files: 5168096\n```\n\nWhen used with `--push-metrics`, Medusa will push completion metrics to the configured monitoring system.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/thelastpickle/cassandra-medusa", "keywords": "", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "cassandra-medusa", "package_url": "https://pypi.org/project/cassandra-medusa/", "platform": "", "project_url": "https://pypi.org/project/cassandra-medusa/", "project_urls": {"Homepage": "https://github.com/thelastpickle/cassandra-medusa"}, "release_url": "https://pypi.org/project/cassandra-medusa/0.5.1/", "requires_dist": ["python-dateutil (<2.8.1,>=2.1)", "Click (>=6.7)", "PyYAML (>=5.1)", "cassandra-driver (>=3.14.0)", "psutil (>=5.4.7)", "ffwd (>=0.0.2)", "apache-libcloud (>=2.8.0)", "lockfile (>=0.12.2)", "pycrypto (>=2.6.1)", "retrying (>=1.3.3)", "parallel-ssh (==1.9.1)", "requests (==2.22.0)", "google-cloud-storage (>=1.7.0) ; extra == 'gcs'", "awscli (>=1.16.291) ; extra == 's3'"], "requires_python": ">=3.5", "summary": "Apache Cassandra backup and restore tool", "version": "0.5.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://gitter.im/thelastpickle/cassandra-medusa?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge\" rel=\"nofollow\"><img alt=\"Join the chat at https://gitter.im/thelastpickle/cassandra-medusa\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d069881150118abe42d34a9620ae15a32ddbec7f/68747470733a2f2f6261646765732e6769747465722e696d2f7468656c6173747069636b6c652f63617373616e6472612d6d65647573612e737667\"></a></p>\n<h1>Medusa</h1>\n<p>Medusa is an Apache Cassandra backup system.</p>\n<h2>Features</h2>\n<p>Medusa is a command line tool that offers the following features:</p>\n<ul>\n<li>Single node backup</li>\n<li>Single node restore</li>\n<li>Cluster wide in place restore (restoring on the same cluster that was used for the backup)</li>\n<li>Cluster wide remote restore (restoring on a different cluster than the one used for the backup)</li>\n<li>Backup purge</li>\n<li>Support for local storage, Google Cloud Storage (GCS) and AWS S3 through <a href=\"https://libcloud.apache.org/\" rel=\"nofollow\">Apache Libcloud</a>. Can be extended to support other storage providers supported by Apache Libcloud.</li>\n<li>Support for clusters using single tokens or vnodes</li>\n<li>Full or incremental backups</li>\n</ul>\n<h2>Setup</h2>\n<p>Choose and initialize the storage system:</p>\n<ul>\n<li>Local storage can be used in conjunction with NFS mounts to store backups off nodes. The backup directory must be accessible from all nodes in the cluster and mounted appropriately. If the backup folder is not shared, the nodes will only see their own backups.</li>\n<li><a href=\"https://github.com/thelastpickle/cassandra-medusa/blob/master/docs/gcs_setup.md\" rel=\"nofollow\">Google Cloud Storage setup</a></li>\n<li><a href=\"https://github.com/thelastpickle/cassandra-medusa/blob/master/docs/aws_s3_setup.md\" rel=\"nofollow\">AWS S3 setup</a></li>\n<li><a href=\"https://github.com/thelastpickle/cassandra-medusa/blob/master/docs/ceph_s3_setup.md\" rel=\"nofollow\">Ceph Object Gateway S3 API</a></li>\n</ul>\n<p>Install Medusa on each Cassandra node:</p>\n<h3>Online installation</h3>\n<ul>\n<li>if the storage backend is a locally accessible shared storage, run <code>sudo pip3 install cassandra-medusa</code></li>\n<li>if your backups are to be stored in AWS S3, run <code>sudo pip3 install cassandra-medusa[S3]</code></li>\n<li>if your backups are to be stored in Google Cloud Storage, run <code>sudo pip3 install cassandra-medusa[GCS]</code></li>\n</ul>\n<p>Running the installation using <code>sudo</code> is necessary to have the <code>/usr/local/bin/medusa</code> script created properly.</p>\n<h3>Offline installation</h3>\n<p>If your Cassandra servers do not have internet access:</p>\n<ul>\n<li>on a machine with the same target os and python version, clone the cassandra-medusa repo and cd into the root directory</li>\n<li>run <code>mkdir pip_dependencies &amp;&amp; pip download -r requirements.txt -d medusa_dependencies</code> to download the dependencies into a sub directory</li>\n<li>run <code>cp requirements.txt medusa_dependencies/</code></li>\n<li>run <code>tar -zcf medusa_dependencies.tar.gz medusa_dependencies</code> to compress the dependencies</li>\n<li>Upload the archive to all Cassandra nodes and decompress it</li>\n<li>run <code>pip install -r medusa_dependencies/requirements.txt --no-index --find-links</code> to install the dependencies on the nodes</li>\n<li>install Medusa using <code>python setup.py install</code> from the cassandra-medusa source directory</li>\n</ul>\n<h3>Configure Medusa</h3>\n<p>Create the <code>/etc/medusa</code> directory if it doesn't exist, and create a file named <code>/etc/medusa/medusa.ini</code> with the content of <a href=\"https://github.com/thelastpickle/cassandra-medusa/blob/master/medusa-example.ini\" rel=\"nofollow\">medusa-example.ini</a>.\nModify it to match your requirements:</p>\n<pre><code>[cassandra]\n;stop_cmd = /etc/init.d/cassandra stop\n;start_cmd = /etc/init.d/cassandra start\n;config_file = &lt;path to cassandra.yaml. Defaults to /etc/cassandra/cassandra.yaml&gt;\n;cql_username = &lt;username&gt;\n;cql_password = &lt;password&gt;\n;nodetool_username =  &lt;my nodetool username&gt;\n;nodetool_password =  &lt;my nodetool password&gt;\n;nodetool_password_file_path = &lt;path to nodetool password file&gt;\n;nodetool_host = &lt;host name or IP to use for nodetool&gt;\n;nodetool_port = &lt;port number to use for nodetool&gt;\n\n; Command ran to verify if Cassandra is running on a node. Defaults to \"nodetool version\"\n;check_running = nodetool version\n\n[storage]\nstorage_provider = &lt;Storage system used for backups&gt;\n; storage_provider should be either of \"local\", \"google_storage\" or the s3_* values from\n; https://github.com/apache/libcloud/blob/trunk/libcloud/storage/types.py\n\n; Name of the bucket used for storing backups\nbucket_name = cassandra_backups\n\n; JSON key file for service account with access to GCS bucket or AWS credentials file (home-dir/.aws/credentials)\nkey_file = /etc/medusa/credentials\n\n; Path of the local storage bucket (used only with 'local' storage provider)\n;base_path = /path/to/backups\n\n; Any prefix used for multitenancy in the same bucket\n;prefix = clusterA\n\n;fqdn = &lt;enforce the name of the local node. Computed automatically if not provided.&gt;\n\n; Number of days before backups are purged. 0 means backups don't get purged by age (default)\nmax_backup_age = 0\n; Number of backups to retain. Older backups will get purged beyond that number. 0 means backups don't get purged by count (default)\nmax_backup_count = 0\n; Both thresholds can be defined for backup purge.\n\n; Used to throttle S3 backups/restores:\ntransfer_max_bandwidth = 50MB/s\n\n; Max number of downloads/uploads. Not used by the GCS backend.\nconcurrent_transfers = 1\n\n; Size over which S3 uploads will be using the awscli with multi part uploads. Defaults to 100MB.\nmulti_part_upload_threshold = 104857600\n\n[monitoring]\n;monitoring_provider = &lt;Provider used for sending metrics. Currently either of \"ffwd\" or \"local\"&gt;\n\n[ssh]\n;username = &lt;SSH username to use for restoring clusters&gt;\n;key_file = &lt;SSH key for use for restoring clusters. Expected in PEM unencrypted format.&gt;\n\n[checks]\n;health_check = &lt;Which ports to check when verifying a node restored properly. Options are 'cql' (default), 'thrift', 'all'.&gt;\n;query = &lt;CQL query to run after a restore to verify it went OK&gt;\n;expected_rows = &lt;Number of rows expected to be returned when the query runs. Not checked if not specified.&gt;\n;expected_result = &lt;Coma separated string representation of values returned by the query. Checks only 1st row returned, and only if specified&gt;\n\n[logging]\n; Controls file logging, disabled by default.\n; enabled = 0\n; file = medusa.log\n; level = INFO\n\n; Control the log output format\n; format = [%(asctime)s] %(levelname)s: %(message)s\n\n; Size over which log file will rotate\n; maxBytes = 20000000\n\n; How many log files to keep\n; backupCount = 50\n\n</code></pre>\n<h1>Usage</h1>\n<pre><code>$ medusa\nUsage: medusa [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  -v, --verbosity          Verbosity\n  --without-log-timestamp  Do not show timestamp in logs\n  --config-file TEXT       Specify config file\n  --bucket-name TEXT       Bucket name\n  --key-file TEXT          GCP credentials key file\n  --prefix TEXT            Prefix for shared storage\n  --fqdn TEXT              Act as another host\n  --ssh-username TEXT\n  --ssh-key-file TEXT\n  --help                   Show this message and exit.\n\nCommands:\n  backup                          Backup Cassandra\n  build-index                     Builds indices for all present backups\n                                  and...\n  download                        Download backup\n  fetch-tokenmap                  Backup Cassandra\n  get-last-complete-cluster-backup\n                                  Pints the name of the latest complete\n                                  cluster...\n  list-backups                    List backups\n  purge                           Delete obsolete backups\n  report-last-backup              Find time since last backup and print it\n                                  to...\n  restore-cluster                 Restore Cassandra cluster\n  restore-node                    Restore single Cassandra node\n  status                          Show status of backups\n  verify                          Verify the integrity of a backup\n</code></pre>\n<h2>Performing a backup</h2>\n<pre><code>$ medusa backup --help\nUsage: medusa backup [OPTIONS]\n\n  Backup Cassandra\n\nOptions:\n  --backup-name TEXT           Custom name for the backup\n  --stagger INTEGER            Check for staggering initial backups for\n                               duration seconds\n  --mode [full|differential]\n  --help                       Show this message and exit.\n</code></pre>\n<p>Once Medusa is setup, you can create a <strong>full</strong> backup with the following command:</p>\n<pre><code>$ medusa backup --backup-name=&lt;name of the backup&gt;\n</code></pre>\n<p>In order to perform an <strong>differential</strong> backup, add the <code>--mode=differential</code> argument to your command:</p>\n<pre><code>$ medusa backup --backup-name=&lt;name of the backup&gt; --mode=differential\n</code></pre>\n<p>To perform cluster wide backups, the command must run on all nodes in the cluster, using the same backup name.</p>\n<h2>Listing existing backups</h2>\n<pre><code>$ medusa list-backups --help\nUsage: medusa list-backups [OPTIONS]\n\n  List backups\n\nOptions:\n  --show-all / --no-show-all  List all backups in the bucket\n  --help                      Show this message and exit.\n</code></pre>\n<p>List all backups for the current node/cluster:</p>\n<pre><code>$ medusa list-backups\n2019080507 (started: 2019-08-05 07:07:03, finished: 2019-08-05 08:01:04)\n2019080607 (started: 2019-08-06 07:07:04, finished: 2019-08-06 07:59:08)\n2019080707 (started: 2019-08-07 07:07:04, finished: 2019-08-07 07:59:55)\n2019080807 (started: 2019-08-08 07:07:03, finished: 2019-08-08 07:59:22)\n2019080907 (started: 2019-08-09 07:07:04, finished: 2019-08-09 08:00:14)\n2019081007 (started: 2019-08-10 07:07:04, finished: 2019-08-10 08:02:41)\n2019081107 (started: 2019-08-11 07:07:04, finished: 2019-08-11 08:03:48)\n2019081207 (started: 2019-08-12 07:07:04, finished: 2019-08-12 07:59:59)\n2019081307 (started: 2019-08-13 07:07:03, finished: Incomplete [179 of 180 nodes])\n2019081407 (started: 2019-08-14 07:07:04, finished: 2019-08-14 07:56:44)\n2019081507 (started: 2019-08-15 07:07:03, finished: 2019-08-15 07:50:24)\n</code></pre>\n<p>When listing backups from a cluster which is not the backed up one, please add the <code>--show-all</code> flag to bypass the filter and display all existing backups from storage.</p>\n<h2>Restoring a single node</h2>\n<pre><code>$ medusa restore-node --help\nUsage: medusa restore-node [OPTIONS]\n\n  Restore single Cassandra node\n\nOptions:\n  --temp-dir TEXT         Directory for temporary storage\n  --backup-name TEXT      Backup name  [required]\n  --in-place              Indicates if the restore happens on the node the\n                          backup was done on.\n  --keep-auth             Keep system_auth keyspace as found on the node\n  --seeds TEXT            Nodes to wait for after downloading backup but\n                          before starting C*\n  --verify / --no-verify  Verify that the cluster is operational after the\n                          restore completes,\n  --keyspace TEXT         Restore tables from this keyspace\n  --table TEXT            Restore only this table\n  --use-sstableloader     Use the sstableloader to load the backup into the\n                          cluster\n  --help                  Show this message and exit.\n</code></pre>\n<p>In order to restore a backup on a single node, run the following command:</p>\n<pre><code>$ sudo medusa restore-node --backup-name=&lt;name of the backup&gt;\n</code></pre>\n<p>Medusa will need to run with <code>sudo</code> as it will:</p>\n<ul>\n<li>stop Cassandra</li>\n<li>wipe the existing files</li>\n<li>Download the files from backup storage locally and move them to Cassandra's storage directory</li>\n<li>Change the ownership of the files back to the one owning the Cassandra data directory</li>\n<li>start Cassandra</li>\n</ul>\n<p>The <code>--use-sstableloader</code> flag will be useful for restoring data when the topology doesn't match between the backed up cluster and the restore one.\nIn this mode, Cassandra will not be stopped and downloaded SSTables will be loaded into Cassandra by the sstableloader. Data already present in the cluster will not be altered.</p>\n<p>The <code>--fqdn</code> argument allows to force the node to act on behalf of another backup node. It can take several hostnames separated by commas in order to restore several nodes backup using the sstableloader.</p>\n<p>The <code>--keyspace</code> option allows limiting the restore to all tables in the given keyspace. The <code>--table</code> option allows limiting the restore to just one table. The tables must be specified in the <code>keyspace.table</code> format. It is possible to repeat both of the options. Medusa will make an union of everything specified and restore all keyspaces and tables mentioned. The <code>--keyspace</code> option takes precedence, so using <code>--keyspace ks1</code> and then adding <code>--table ks1.t</code> will not limit the restore to just one table - everything from <code>ks1</code> will be restored.</p>\n<h2>Restoring a cluster</h2>\n<pre><code>$ medusa restore-cluster --help\nUsage: medusa restore-cluster [OPTIONS]\n\n  Restore Cassandra cluster\n\nOptions:\n  --backup-name TEXT              Backup name  [required]\n  --seed-target TEXT              seed of the target hosts\n  --temp-dir TEXT                 Directory for temporary storage\n  --host-list TEXT                List of nodes to restore with the associated\n                                  target host\n  --keep-auth / --overwrite-auth  Keep/overwrite system_auth as found on the\n                                  nodes\n  -y, --bypass-checks             Bypasses the security check for restoring a\n                                  cluster\n  --verify / --no-verify          Verify that the cluster is operational after\n                                  the restore completes,\n  --keyspace TEXT                 Restore tables from this keyspace\n  --table TEXT                    Restore only this table\n  --use-sstableloader             Use the sstableloader to load the backup\n                                  into the cluster\n  --help                          Show this message and exit.\n</code></pre>\n<h2>Topology matches between the backup and the restore cluster</h2>\n<p>In this section, we will describe procedures that apply to the following cases:</p>\n<ul>\n<li>Vnodes are not used (single token) + both the backup and the restore cluster have the exact same number of nodes and token assignments.</li>\n<li>Vnodes are used + both the backup and the restore cluster have the exact same number of nodes (regardless token assignments).</li>\n</ul>\n<p>This method is by far the fastest as it replaces SSTables directly on disk.</p>\n<h3>In place (same hardware)</h3>\n<p>In order to restore a backup for a full cluster, in the case where the restored cluster is the exact same as the backed up one:</p>\n<pre><code>$ medusa restore-cluster --backup-name=&lt;name of the backup&gt; --seed-target node1.domain.net\n</code></pre>\n<p>Medusa will need to run without <code>sudo</code> as it will connect through ssh to all nodes in the cluster in order to perform remote operations. It will, by default, use the current user to connect and rely on agent forwarding for authentication (you must ssh into the server using <code>-A</code> to enable agent forwarding).\nThe <code>--seed-target</code> node is used to connect to Cassandra and retrieve the current topology of the cluster. This allows Medusa to map each backup to the correct node in the current topology.</p>\n<p>The following operations will take place:</p>\n<ul>\n<li>Stop Cassandra on all nodes</li>\n<li>Check that the current topology matches the backed up one (if not, will fallback to the next section, using the sstableloader)</li>\n<li>Run <code>restore-node</code> on each node in the cluster</li>\n<li>Start Cassandra on all nodes</li>\n</ul>\n<h3>Remotely (different hardware)</h3>\n<p>In order to restore a backup of a full cluster but on different servers.\nThis can be used to restore a production cluster data to a staging cluster (with the same number of nodes), or recovering from an outage where previously used hardware cannot be re-used.</p>\n<pre><code>$ medusa restore-cluster --backup-name=&lt;name of the backup&gt; --host-list /etc/medusa/restore_mapping.txt\n</code></pre>\n<p>The <code>restore-mapping.txt</code> file will provide the mapping between the backed up cluster nodes and the restore cluster ones. It is expected in the following CSV format: <code>&lt;Is it a seed node?&gt;,&lt;target node&gt;,&lt;source node&gt;</code></p>\n<p>Sample file:</p>\n<pre><code>True,new_node1.foo.net,old_node1.foo.net\nTrue,new_node2.foo.net,old_node2.foo.net\nFalse,new_node3.foo.net,old_node3.foo.net\n</code></pre>\n<p>Medusa will need to run without <code>sudo</code> as it will connect through ssh to all nodes in the cluster in order to perform remote operations. It will, by default, use the current user to connect and rely on agent forwarding for authentication (you must ssh into the server using <code>-A</code> to enable agent forwarding).</p>\n<ul>\n<li>Stop Cassandra on all nodes</li>\n<li>Check that the current topology matches the backed up one (if not, will fallback to the next section, using the sstableloader)</li>\n<li>Run <code>restore-node</code> on each node in the cluster</li>\n<li>Start Cassandra on all nodes</li>\n</ul>\n<p>By default, Medusa will overwrite the <code>system_auth</code> keyspace with the backed up one. If you want to retain the existing system_auth keyspace, you'll have to run <code>restore-cluster</code> with the <code>--keep-auth</code> flag:</p>\n<pre><code>$ medusa restore-cluster --backup-name=&lt;name of the backup&gt; --host-list /etc/medusa/restore_mapping.txt --keep-auth\n</code></pre>\n<h2>Topology does not match between the backup and the restore cluster</h2>\n<p>In this section, we will describe procedures that apply to the other cases:</p>\n<ul>\n<li>Vnodes are not used (single token) + the backup and restore cluster have a different token assignement or a different number of nodes.</li>\n<li>Vnodes are used + the backup and the restore cluster have a different number of nodes.</li>\n</ul>\n<p>This case will be detected automatically by Medusa when checking the topologies, but it can be enforced by adding the <code>--use-sstableloader</code> flag to the <code>restore-cluster</code> command.\nThis technique allows to restore any backup on any cluster of any size, <strong>at the expense of some overhead.</strong></p>\n<ul>\n<li>First, the sstableloader will have to parse all the backed up SSTables in order to send them to the appropriate nodes, which can take way more time on large volumes.</li>\n<li>Then, the amount of data loaded into the restore cluster will be multiplied by the replication factor of the keyspace, since we will be restoring the backups from all replicas without any merge (SSTables from different backups will contain copies of the same data).\n<strong>With RF=3, the cluster will contain approximately three times the data load from the backup.</strong> The size will drop back to normal levels once compaction has caught up (a major compaction could be necessary).</li>\n</ul>\n<p>Using this technique, Cassandra will not be stopped on the nodes and the following steps will happen:</p>\n<ul>\n<li>The data model will be updated as follows:\n<ul>\n<li>The schema will be downloaded from the backup and categorized by object types into individual queries</li>\n<li>Missing keyspaces will be created</li>\n<li>Existing Materialized Views from the backup schema will be dropped (other MVs remain untouched)</li>\n<li>Existing tables from the backup schema will be dropped (other tables remain untouched)</li>\n<li>Existing User Defined Types from the backup schema will be (re)created</li>\n<li>Tables from the backup will be created</li>\n<li>Secondary indexes will be created</li>\n<li>Materialized Views will be created</li>\n</ul>\n</li>\n<li>Backup nodes will be assigned to target nodes (one target node can be responsible from zero to several backup nodes)</li>\n<li>Run <code>restore-node</code> on each node in the target cluster, passing a list of backup nodes as <code>--fqdn</code> and the <code>--sstableloader</code> flag\n<ul>\n<li>for each specified node in <code>--fqdn</code>:\n<ul>\n<li>Download the files from backup storage locally</li>\n<li>Invoke the locally installed sstableloader to load them using the local C* instance as contact point</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>If your cluster is configured with the default <code>auto_snapshot: true</code> then dropping the tables will trigger a snapshot that will persist their data on disk. Medusa will not clear this snapshot after restore.</p>\n<p>The <code>--keyspace</code> and <code>--table</code> options of <code>restore-cluster</code> command work exactly the same way as they do for the <code>restore-node</code> command.</p>\n<h2>Verify an existing backup</h2>\n<pre><code>$ medusa verify --help\nUsage: medusa verify [OPTIONS]\n\n  Verify the integrity of a backup\n\nOptions:\n  --backup-name TEXT  Backup name  [required]\n  --help              Show this message and exit.\n</code></pre>\n<p>Run a health check on a backup, which will verify that:</p>\n<ul>\n<li>All nodes have completed the backup</li>\n<li>All files in the manifest are present in storage</li>\n<li>All backed up files are present in the manifest</li>\n<li>All files have the right hash as stored in the manifest</li>\n</ul>\n<pre><code>$ medusa verify --backup-name=2019090503\nValidating 2019090503 ...\n- Completion: OK!\n- Manifest validated: OK!!\n</code></pre>\n<p>In case some nodes in the cluster didn't complete the backups, you'll get the following output:</p>\n<pre><code>$ medusa verify --backup-name=2019081703\nValidating 2019081703 ...\n- Completion: Not complete!\n  - [127.0.0.2] Backup missing\n- Manifest validated: OK!!\n</code></pre>\n<h2>Purge old backups</h2>\n<pre><code>$ medusa purge --help\nUsage: medusa purge [OPTIONS]\n\n  Delete obsolete backups\n\nOptions:\n  --help  Show this message and exit.\n</code></pre>\n<p>In order to remove obsolete backups from storage, according to the configured <code>max_backup_age</code> and/or <code>max_backup_count</code>, run:</p>\n<pre><code>$ medusa purge\n[2019-09-04 13:44:16] INFO: Starting purge\n[2019-09-04 13:44:17] INFO: 25 backups are candidate to be purged\n[2019-09-04 13:44:17] INFO: Purging backup 2019082513...\n[2019-09-04 13:44:17] INFO: Purging backup 2019082514...\n[2019-09-04 13:44:18] INFO: Purging backup 2019082515...\n[2019-09-04 13:44:18] INFO: Purging backup 2019082516...\n[2019-09-04 13:44:19] INFO: Purging backup 2019082517...\n[2019-09-04 13:44:19] INFO: Purging backup 2019082518...\n[2019-09-04 13:44:19] INFO: Purging backup 2019082519...\n[2019-09-04 13:44:20] INFO: Purging backup 2019082520...\n[2019-09-04 13:44:20] INFO: Purging backup 2019082521...\n[2019-09-04 13:44:20] INFO: Purging backup 2019082522...\n[2019-09-04 13:44:21] INFO: Purging backup 2019082523...\n[2019-09-04 13:44:21] INFO: Purging backup 2019082600...\n[2019-09-04 13:44:21] INFO: Purging backup 2019082601...\n[2019-09-04 13:44:22] INFO: Purging backup 2019082602...\n[2019-09-04 13:44:22] INFO: Purging backup 2019082603...\n[2019-09-04 13:44:23] INFO: Purging backup 2019082604...\n[2019-09-04 13:44:23] INFO: Purging backup 2019082605...\n[2019-09-04 13:44:23] INFO: Purging backup 2019082606...\n[2019-09-04 13:44:24] INFO: Purging backup 2019082607...\n[2019-09-04 13:44:24] INFO: Purging backup 2019082608...\n[2019-09-04 13:44:24] INFO: Purging backup 2019082609...\n[2019-09-04 13:44:25] INFO: Purging backup 2019082610...\n[2019-09-04 13:44:25] INFO: Purging backup 2019082611...\n[2019-09-04 13:44:25] INFO: Purging backup 2019082612...\n[2019-09-04 13:44:26] INFO: Purging backup 2019082613...\n[2019-09-04 13:44:26] INFO: Cleaning up orphaned files...\n[2019-09-04 13:45:59] INFO: Purged 652 objects with a total size of 3.74 MB\n\n</code></pre>\n<p>Since SSTables and meta files are stored in different places for differential backups, the purge is a two step process:</p>\n<ul>\n<li>Delete all backup directories</li>\n<li>Scan active backup files from manifests and compare with the list of SSTables in the <code>data</code> directory. All SSTables present in the <code>data</code> directory but absent from all manifests will get deleted in that step.</li>\n</ul>\n<h2>Check the status of a backup</h2>\n<pre><code>$ medusa status --help\nUsage: medusa status [OPTIONS]\n\n  Show status of backups\n\nOptions:\n  --backup-name TEXT  Backup name  [required]\n  --help              Show this message and exit.\n</code></pre>\n<p>Outputs a summary of a specific backup status:</p>\n<pre><code>$ medusa status --backup-name=2019090503\n2019090503\n- Started: 2019-09-05 03:53:04, Finished: 2019-09-05 04:49:52\n- 32 nodes completed, 0 nodes incomplete, 0 nodes missing\n- 163256 files, 12.20 TB\n</code></pre>\n<h2>Display informations on the latest backup</h2>\n<pre><code>$ medusa report-last-backup --help\nUsage: medusa report-last-backup [OPTIONS]\n\n  Find time since last backup and print it to stdout :return:\n\nOptions:\n  --push-metrics  Also push the information via metrics\n  --help          Show this message and exit.\n\n</code></pre>\n<p>This command will display several informations on the latest backup:</p>\n<pre><code>$ medusa report-last-backup\n[2019-09-04 12:56:15] INFO: Latest node backup finished 18746 seconds ago\n[2019-09-04 12:56:18] INFO: Latest complete backup:\n[2019-09-04 12:56:18] INFO: - Name: 2019090407\n[2019-09-04 12:56:18] INFO: - Finished: 18173 seconds ago\n[2019-09-04 12:56:19] INFO: Latest backup:\n[2019-09-04 12:56:19] INFO: - Name: 2019090407\n[2019-09-04 12:56:19] INFO: - Finished: True\n[2019-09-04 12:56:19] INFO: - Details - Node counts\n[2019-09-04 12:56:19] INFO: - Complete backup: 180 nodes have completed the backup\n[2019-09-04 12:58:47] INFO: - Total size: 94.69 TiB\n[2019-09-04 12:58:55] INFO: - Total files: 5168096\n</code></pre>\n<p>When used with <code>--push-metrics</code>, Medusa will push completion metrics to the configured monitoring system.</p>\n\n          </div>"}, "last_serial": 6687038, "releases": {"0.2.0": [{"comment_text": "", "digests": {"md5": "410f562a611aeedf48124608effd7746", "sha256": "71231973142547fb0c5fbd0109e17572ee0ba69753f32f75c43dfadc3b852136"}, "downloads": -1, "filename": "cassandra_medusa-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "410f562a611aeedf48124608effd7746", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4", "size": 94158, "upload_time": "2019-11-12T10:08:38", "upload_time_iso_8601": "2019-11-12T10:08:38.805758Z", "url": "https://files.pythonhosted.org/packages/15/96/fdd4128f9e12922b8f2b9dd591e18a80f1cc3ad0ee1babebef29d3da135e/cassandra_medusa-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "317c40fdab9e72432a3392cf9f64c62b", "sha256": "dc1a280800c361b7009868676d2f487184bc05605725739fb8c03114877422d1"}, "downloads": -1, "filename": "cassandra-medusa-0.2.0.tar.gz", "has_sig": false, "md5_digest": "317c40fdab9e72432a3392cf9f64c62b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 69716, "upload_time": "2019-11-12T10:08:41", "upload_time_iso_8601": "2019-11-12T10:08:41.298794Z", "url": "https://files.pythonhosted.org/packages/65/77/e7e406149f746f2d7fb2df325869a542d371b87b5204b895b3474fa470bc/cassandra-medusa-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "802bbff7280b36af4d119e5b9fb6f363", "sha256": "0c558a186e1233ca1eceacf067449a22fdb3a47ee915804d43e53b03fac42fbe"}, "downloads": -1, "filename": "cassandra_medusa-0.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "802bbff7280b36af4d119e5b9fb6f363", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4", "size": 94260, "upload_time": "2019-11-12T14:48:36", "upload_time_iso_8601": "2019-11-12T14:48:36.210227Z", "url": "https://files.pythonhosted.org/packages/9a/a2/12b688f57a284fa30a97b184451f39c54196a4b5f66fc5edfafbfbc06d1e/cassandra_medusa-0.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "841d2541208f8aa73e9ad0fc461bb267", "sha256": "8732a9d4b85ac49b90e649a30d0e2099ba6896da39ce1dbda8cdf31ae893f04f"}, "downloads": -1, "filename": "cassandra-medusa-0.2.1.tar.gz", "has_sig": false, "md5_digest": "841d2541208f8aa73e9ad0fc461bb267", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 70016, "upload_time": "2019-11-12T14:48:37", "upload_time_iso_8601": "2019-11-12T14:48:37.835320Z", "url": "https://files.pythonhosted.org/packages/06/60/3f18fc3ae4623c4a322b0f4230e0e6d4817b2f7789dbd16e06e6a89f3133/cassandra-medusa-0.2.1.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "a39920c783479a6e809badf31679a5c4", "sha256": "c15a25d3df7b8b842a7018a8d2eee00441db1c4c75443ba6e14c3dd1f2cf6f06"}, "downloads": -1, "filename": "cassandra_medusa-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "a39920c783479a6e809badf31679a5c4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4", "size": 100633, "upload_time": "2019-11-27T19:02:50", "upload_time_iso_8601": "2019-11-27T19:02:50.131128Z", "url": "https://files.pythonhosted.org/packages/ea/ef/b70d0c92bb10cbfb0cb32a991b83a3523aa8d481f363647ad22632a9d2a7/cassandra_medusa-0.3.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c02167fc0b43723244cc933fad542da1", "sha256": "2f4edcaaea1cb563a8535b7b84b66eb96b65759e611ff2af2417508e7da12734"}, "downloads": -1, "filename": "cassandra-medusa-0.3.0.tar.gz", "has_sig": false, "md5_digest": "c02167fc0b43723244cc933fad542da1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 73759, "upload_time": "2019-11-27T19:02:51", "upload_time_iso_8601": "2019-11-27T19:02:51.922644Z", "url": "https://files.pythonhosted.org/packages/2e/b7/b1ea77b9ae4be3a4a717ca9418f9a398f2ca272c8899e3f51ff3a5104abf/cassandra-medusa-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "bf3d770f3f9f8bcf3fb50cbb162ec047", "sha256": "bb5a17ca45a59f3157fe183d1b9da3b93d3fc452a2f5c1cc933ca1572a9d4bd8"}, "downloads": -1, "filename": "cassandra_medusa-0.3.1-py3-none-any.whl", "has_sig": false, "md5_digest": "bf3d770f3f9f8bcf3fb50cbb162ec047", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4", "size": 102435, "upload_time": "2019-12-04T15:30:35", "upload_time_iso_8601": "2019-12-04T15:30:35.240988Z", "url": "https://files.pythonhosted.org/packages/dd/ca/e127e373cb65bd8f4ab216b3d3f7ea1bf24ee4f25acf79fc400c6c102c8e/cassandra_medusa-0.3.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7908b8e85e16769583f307ff652b1398", "sha256": "b24427c2d54b69392ce39579ef4ea269f73091036571a77d31d1b688b64fe25b"}, "downloads": -1, "filename": "cassandra-medusa-0.3.1.tar.gz", "has_sig": false, "md5_digest": "7908b8e85e16769583f307ff652b1398", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 74808, "upload_time": "2019-12-04T15:30:36", "upload_time_iso_8601": "2019-12-04T15:30:36.874512Z", "url": "https://files.pythonhosted.org/packages/fc/5a/565a575494b3e026e3e350f85067fe87eac5254ebe82b1d59449c2f8e7b5/cassandra-medusa-0.3.1.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "0abac4de39b0cb6fbb5b21fef88720a8", "sha256": "be25e02583c4fd64101088803b66a147b43d9a14abf28d544eec28025bb61a14"}, "downloads": -1, "filename": "cassandra_medusa-0.4.1-py3-none-any.whl", "has_sig": false, "md5_digest": "0abac4de39b0cb6fbb5b21fef88720a8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 105732, "upload_time": "2020-01-19T16:27:41", "upload_time_iso_8601": "2020-01-19T16:27:41.750041Z", "url": "https://files.pythonhosted.org/packages/cc/e6/f1136b90cc97f45806a3bb88df2a01247c3f1904cfee16212284364d3c71/cassandra_medusa-0.4.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1a2c0736a2afb7a2ea3ca2e5c8cff1c5", "sha256": "82a9cf9a9aa51f0355b8021a2f8fd9ec8bbdc8db54c299c0988eadb0fc68b0ef"}, "downloads": -1, "filename": "cassandra-medusa-0.4.1.tar.gz", "has_sig": false, "md5_digest": "1a2c0736a2afb7a2ea3ca2e5c8cff1c5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 78199, "upload_time": "2020-01-19T16:27:43", "upload_time_iso_8601": "2020-01-19T16:27:43.339398Z", "url": "https://files.pythonhosted.org/packages/dc/e8/e370214ff53806ccdd0c3741473891400bdbc4cc21d3b0e77832048f1b25/cassandra-medusa-0.4.1.tar.gz", "yanked": false}], "0.5.0": [{"comment_text": "", "digests": {"md5": "ebcfa721a210ceb7ff4e111634579e77", "sha256": "58222980bc8620d46dbdb1c5f289e088223f08b9a8f292158c4b07c2f4750fb0"}, "downloads": -1, "filename": "cassandra_medusa-0.5.0-py3-none-any.whl", "has_sig": false, "md5_digest": "ebcfa721a210ceb7ff4e111634579e77", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 109550, "upload_time": "2020-02-17T08:40:52", "upload_time_iso_8601": "2020-02-17T08:40:52.469787Z", "url": "https://files.pythonhosted.org/packages/41/44/0ade6b303f588f000ba4ba5215da06caa7bd44776a236a231000424aea35/cassandra_medusa-0.5.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5a837b74a4d13c16c4716631bc4c35f2", "sha256": "421f1b7777a171636bb7edc0501696168cbd438cb7282fc13b3a4bb347fd2aae"}, "downloads": -1, "filename": "cassandra-medusa-0.5.0.tar.gz", "has_sig": false, "md5_digest": "5a837b74a4d13c16c4716631bc4c35f2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 76911, "upload_time": "2020-02-17T08:40:54", "upload_time_iso_8601": "2020-02-17T08:40:54.186779Z", "url": "https://files.pythonhosted.org/packages/fc/9b/e9eb6585604ea3c0aca7a90e378d43b2632859514d1b39704d8057c9e608/cassandra-medusa-0.5.0.tar.gz", "yanked": false}], "0.5.1": [{"comment_text": "", "digests": {"md5": "9d8b05d094411c54e81f4f73841ea48f", "sha256": "00e03cb719c312d290f679dcb02c2cd146b1c5e08055d232b0d363de89f8ecdb"}, "downloads": -1, "filename": "cassandra_medusa-0.5.1-py3-none-any.whl", "has_sig": false, "md5_digest": "9d8b05d094411c54e81f4f73841ea48f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 109695, "upload_time": "2020-02-24T08:23:55", "upload_time_iso_8601": "2020-02-24T08:23:55.756775Z", "url": "https://files.pythonhosted.org/packages/5c/8c/9eb3a4fb0f2bd68d12ab24d8d961f7638ecba5503594465f2a81f6ceca9e/cassandra_medusa-0.5.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0f14565e27b70f2d720bcb0fd0ebe257", "sha256": "d36a40a4d49ab116e5812d4f1beee9d72ffdffa94695151e7b50c4dfcda483bf"}, "downloads": -1, "filename": "cassandra-medusa-0.5.1.tar.gz", "has_sig": false, "md5_digest": "0f14565e27b70f2d720bcb0fd0ebe257", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 77002, "upload_time": "2020-02-24T08:23:57", "upload_time_iso_8601": "2020-02-24T08:23:57.570165Z", "url": "https://files.pythonhosted.org/packages/27/31/bdf744c4b2fe687ffcd86ff615e4522f1828bf78a26c45a84f598151d36a/cassandra-medusa-0.5.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "9d8b05d094411c54e81f4f73841ea48f", "sha256": "00e03cb719c312d290f679dcb02c2cd146b1c5e08055d232b0d363de89f8ecdb"}, "downloads": -1, "filename": "cassandra_medusa-0.5.1-py3-none-any.whl", "has_sig": false, "md5_digest": "9d8b05d094411c54e81f4f73841ea48f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 109695, "upload_time": "2020-02-24T08:23:55", "upload_time_iso_8601": "2020-02-24T08:23:55.756775Z", "url": "https://files.pythonhosted.org/packages/5c/8c/9eb3a4fb0f2bd68d12ab24d8d961f7638ecba5503594465f2a81f6ceca9e/cassandra_medusa-0.5.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0f14565e27b70f2d720bcb0fd0ebe257", "sha256": "d36a40a4d49ab116e5812d4f1beee9d72ffdffa94695151e7b50c4dfcda483bf"}, "downloads": -1, "filename": "cassandra-medusa-0.5.1.tar.gz", "has_sig": false, "md5_digest": "0f14565e27b70f2d720bcb0fd0ebe257", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 77002, "upload_time": "2020-02-24T08:23:57", "upload_time_iso_8601": "2020-02-24T08:23:57.570165Z", "url": "https://files.pythonhosted.org/packages/27/31/bdf744c4b2fe687ffcd86ff615e4522f1828bf78a26c45a84f598151d36a/cassandra-medusa-0.5.1.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:35:19 2020"}