{"info": {"author": "Erik K\u00f6rner", "author_email": "koerner@informatik.uni-leipzig.de", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX", "Operating System :: Unix", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy", "Topic :: Utilities"], "description": "========\nOverview\n========\n\n\n\n\nThis package provides utilities for Thai sentence segmentation, word tokenization and POS tagging.\nBecause of how sentence segmentation is performed, prior tokenization and POS tagging is required and therefore also provided with this package.\n\nBesides functions for doing sentence segmentation, tokenization, tokenization with POS tagging for single sentence strings,\nthere are also functions for working with large amounts of data in a streaming fashion.\nThey are also accessible with a commandline script ``thai-segmenter`` that accepts file or standard in/output.\nOptions allow working with meta-headers or tabulator separated data files.\n\nThe main functionality for sentence segmentation was extracted, reformatted and slightly rewritten from another project, \n`Question Generation Thai <https://github.com/myscloud/Question-Generation-Thai>`_.\n\n**LongLexTo** is used as state-of-the-art word/lexeme tokenizer. An implementation was packaged in the above project but there are also (*original?*) versions `github <https://github.com/telember/lexto>`_ and `homepage <http://www.sansarn.com/lexto/>`_. To better use it for bulk processing in Python, it has been rewritten from Java to pure Python.\n\nFor POS tagging a Viterbi-Model with the annotated Orchid-Corpus is used, `paper <https://www.researchgate.net/profile/Virach_Sornlertlamvanich/publication/2630580_Building_a_Thai_part-of-speech_tagged_corpus_ORCHID/links/02e7e514db19a98619000000/Building-a-Thai-part-of-speech-tagged-corpus-ORCHID.pdf>`_.\n\n* Free software: MIT license\n\n\nInstallation\n============\n\n::\n\n    pip install thai-segmenter\n\n\nDocumentation\n=============\n\nTo use the project:\n\n.. code-block:: python\n\n    sentence = \"\"\"foo bar 1234\"\"\"\n\n    # [A] Sentence Segmentation\n    from thai_segmenter.tasks import sentence_segment\n    # or even easier:\n    from thai_segmenter import sentence_segment\n    sentences = sentence_segment(sentence)\n\n    for sentence in sentences:\n        print(str(sentence))\n\n    # [B] Lexeme Tokenization\n    from thai_segmenter import tokenize\n    tokens = tokenize(sentence)\n    for token in tokens:\n        print(token, end=\" \", flush=True)\n\n    # [C] POS Tagging\n    from thai_segmenter import tokenize_and_postag\n    sentence_info = tokenize_and_postag(sentence)\n    for token, pos in sentence_info.pos:\n        print(\"{}|{}\".format(token, pos), end=\" \", flush=True)\n\n\nSee more possibilities in ``tasks.py`` or ``cli.py``.\n\nStreaming larger sequences can be achieved like this:\n\n.. code-block:: python\n\n    # Streaming\n    sentences = [\"sent1\\n\", \"sent2\\n\", \"sent3\\n\"]  # or any iterable (like File)\n    from thai_segmenter import line_sentence_segmenter\n    sentences_segmented = line_sentence_segmenter(sentences)\n\n\nCommandline tool\n----------------\n\nThis project also provides a nifty commandline tool ``thai-segmenter`` that does most of the work for you:\n\n.. code-block:: bash\n\n    usage: thai-segmenter [-h] {clean,sentseg,tokenize,tokpos} ...\n\n    Thai Segmentation utilities.\n\n    optional arguments:\n      -h, --help            show this help message and exit\n\n    Tasks:\n      {clean,sentseg,tokenize,tokpos}\n        clean               Clean input from non-thai and blank lines.\n        sentseg             Sentence segmentize input lines.\n        tokenize            Tokenize input lines.\n        tokpos              Tokenize and POS-tag input lines.\n\n\nYou can run sentence segmentation like this::\n\n    thai-segmenter sentseg -i input.txt -o output.txt\n\nor even pipe data::\n\n    cat input.txt | thai-segmenter sentseg > output.txt\n\nUse ``-h``/``--help`` to get more information about possible control flow options.\n\n\nYou can run it somewhat interactively with::\n\n    thai-segmenter tokpos --stats\n\nand standard input and output are used. Lines terminated with ``Enter`` are immediatly processed and printed. Stop work with key combination ``Ctrl`` + ``D`` and the ``--stats`` parameter will helpfully output some statistics.\n\n\nWebApp\n------\n\nThe project also provides a demo WebApp (using ``Flask`` and ``gevent``) that can be installed with::\n\n    pip install -e .[webapp]\n\nand then simply run (in the foreground)::\n\n    thai-segmenter-webapp\n\nConsider running it in a ``screen`` session.\n\n.. code-block:: bash\n\n    # create the screen detached and then attach\n    screen -dmS thai-senseg-webapp\n    screen -r thai-senseg-webapp\n\n    # in the screen:\n    thai-segmenter-webapp\n\n    # and detach with keys [Ctrl]+[D]\n\n*Please note that it only is a demo webapp to test and visualize how the sentence segmentor works.*\n\n\nDevelopment\n===========\n\nTo install the package for development::\n\n    git clone https://github.com/Querela/thai-segmenter.git\n    cd thai-segmenter/\n    pip install -e .[dev]\n\n\nAfter changing the source, run auto code formatting with::\n\n    black <file>.py\n\nAnd check it afterwards with::\n\n    flake8 <file>.py\n\nThe ``setup.py`` also contains the ``flake8`` subcommand as well as an extended ``clean`` command.\n\n\nTests\n-----\n\nTo run the all tests run::\n\n    tox\n\nYou can also optionally run ``pytest`` alone::\n\n    pytest\n\nOr with::\n\n    python setup.py test\n\n\nNote, to combine the coverage data from all the tox environments run:\n\n.. list-table::\n    :widths: 10 90\n    :stub-columns: 1\n\n    - - Windows\n      - ::\n\n            set PYTEST_ADDOPTS=--cov-append\n            tox\n\n    - - Other\n      - ::\n\n            PYTEST_ADDOPTS=--cov-append tox\n\n\nChangelog\n=========\n\n0.4.1 (2019-04-08)\n------------------\n\n* Fix tokenization / tokenization + POS tagging: return words instead of subwords\n* Add ``--escape-special`` and ``--subwords`` parameter to CLI script for tokenization.\n  Allows tokenization to further tokenize unknown words (e. g. names)\n  as well as escape special characters with angle bracket entities.\n\n\n0.4.0 (2019-04-08)\n------------------\n\n* Add demo webapp with sentence segmentation.\n  (NOTE: Running both the webapp and (batch) sentence segmentation at the same time from the same installation is not recommeded. It can have unexpected side-effects.)\n* Some reformat of ``README.rst``\n\n\n0.3.3 (2019-04-07)\n------------------\n\n* Fix duplicate names (class/method for ``sentence_segment``), rename class to ``sentence_segmenter`` (``.py``).\n\n\n0.3.2 (2019-04-07)\n------------------\n\n* Add ``twine`` to extras dependencies.\n* Publish module on **PyPI**. (Only ``sdist``, ``bdist_wheel`` can't be built currently.)\n* Fix some TravisCI warnings.\n\n\n0.3.1 (2019-04-07)\n------------------\n\n* Add tasks to ``__init__.py`` for easier access.\n\n\n0.3.0 (2019-04-06)\n------------------\n\n* Refactor tasks into ``tasks.py`` to enable better import in case of embedding thai-segmenter into other projects.\n* Have it almost release ready. :-)\n* Add some more parameters to functions (optional header detection function)\n* Flesh out ``README.rst`` with examples and descriptions.\n* Add Changelog items.\n\n\n0.2.1 / 0.2.2 (2019-04-05)\n--------------------------\n\n* Many changes, ``bumpversion`` needs to run where ``.bumpversion.cfg`` is located else it silently fails ...\n* Strip Typehints and add support for Python3.5 again.\n* Add CLI tasks for cleaning, sentseg, tokenize, pos-tagging.\n* Add various params, e. g. for selecting columns, skipping headers.\n* Fix many bugs for TravisCI (isort, flake8)\n* Use iterators / streaming approach for file input/output.\n\n\n0.2.0 (2019-04-05)\n------------------\n\n* Remove support of Python 2.7 and lower equal to Python 3.5 because of Typehints.\n* Added CLI skeleton.\n* Add really good ``setup.py``. (with ``black``, ``flake8``)\n\n\n0.1.0 (2019-04-05)\n------------------\n\n* First release version as package.", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Querela/thai-segmenter", "keywords": "thai,nlp,sentence segmentation,tokenize,pos-tag,longlexto,orchid", "license": "MIT license", "maintainer": "", "maintainer_email": "", "name": "thai-segmenter", "package_url": "https://pypi.org/project/thai-segmenter/", "platform": "", "project_url": "https://pypi.org/project/thai-segmenter/", "project_urls": {"Changelog": "https://github.com/Querela/thai-segmenter/blob/master/CHANGELOG.rst", "Homepage": "https://github.com/Querela/thai-segmenter", "Issue Tracker": "https://github.com/Querela/thai-segmenter/issues"}, "release_url": "https://pypi.org/project/thai-segmenter/0.4.1/", "requires_dist": null, "requires_python": ">=3.4", "summary": "Thai tokenizer, POS-tagger and sentence segmenter.", "version": "0.4.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>This package provides utilities for Thai sentence segmentation, word tokenization and POS tagging.\nBecause of how sentence segmentation is performed, prior tokenization and POS tagging is required and therefore also provided with this package.</p>\n<p>Besides functions for doing sentence segmentation, tokenization, tokenization with POS tagging for single sentence strings,\nthere are also functions for working with large amounts of data in a streaming fashion.\nThey are also accessible with a commandline script <tt><span class=\"pre\">thai-segmenter</span></tt> that accepts file or standard in/output.\nOptions allow working with meta-headers or tabulator separated data files.</p>\n<p>The main functionality for sentence segmentation was extracted, reformatted and slightly rewritten from another project,\n<a href=\"https://github.com/myscloud/Question-Generation-Thai\" rel=\"nofollow\">Question Generation Thai</a>.</p>\n<p><strong>LongLexTo</strong> is used as state-of-the-art word/lexeme tokenizer. An implementation was packaged in the above project but there are also (<em>original?</em>) versions <a href=\"https://github.com/telember/lexto\" rel=\"nofollow\">github</a> and <a href=\"http://www.sansarn.com/lexto/\" rel=\"nofollow\">homepage</a>. To better use it for bulk processing in Python, it has been rewritten from Java to pure Python.</p>\n<p>For POS tagging a Viterbi-Model with the annotated Orchid-Corpus is used, <a href=\"https://www.researchgate.net/profile/Virach_Sornlertlamvanich/publication/2630580_Building_a_Thai_part-of-speech_tagged_corpus_ORCHID/links/02e7e514db19a98619000000/Building-a-Thai-part-of-speech-tagged-corpus-ORCHID.pdf\" rel=\"nofollow\">paper</a>.</p>\n<ul>\n<li>Free software: MIT license</li>\n</ul>\n<div id=\"installation\">\n<h2>Installation</h2>\n<pre>pip install thai-segmenter\n</pre>\n</div>\n<div id=\"documentation\">\n<h2>Documentation</h2>\n<p>To use the project:</p>\n<pre><span class=\"n\">sentence</span> <span class=\"o\">=</span> <span class=\"s2\">\"\"\"foo bar 1234\"\"\"</span>\n\n<span class=\"c1\"># [A] Sentence Segmentation</span>\n<span class=\"kn\">from</span> <span class=\"nn\">thai_segmenter.tasks</span> <span class=\"kn\">import</span> <span class=\"n\">sentence_segment</span>\n<span class=\"c1\"># or even easier:</span>\n<span class=\"kn\">from</span> <span class=\"nn\">thai_segmenter</span> <span class=\"kn\">import</span> <span class=\"n\">sentence_segment</span>\n<span class=\"n\">sentences</span> <span class=\"o\">=</span> <span class=\"n\">sentence_segment</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">sentence</span> <span class=\"ow\">in</span> <span class=\"n\">sentences</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># [B] Lexeme Tokenization</span>\n<span class=\"kn\">from</span> <span class=\"nn\">thai_segmenter</span> <span class=\"kn\">import</span> <span class=\"n\">tokenize</span>\n<span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">token</span> <span class=\"ow\">in</span> <span class=\"n\">tokens</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">token</span><span class=\"p\">,</span> <span class=\"n\">end</span><span class=\"o\">=</span><span class=\"s2\">\" \"</span><span class=\"p\">,</span> <span class=\"n\">flush</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># [C] POS Tagging</span>\n<span class=\"kn\">from</span> <span class=\"nn\">thai_segmenter</span> <span class=\"kn\">import</span> <span class=\"n\">tokenize_and_postag</span>\n<span class=\"n\">sentence_info</span> <span class=\"o\">=</span> <span class=\"n\">tokenize_and_postag</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">token</span><span class=\"p\">,</span> <span class=\"n\">pos</span> <span class=\"ow\">in</span> <span class=\"n\">sentence_info</span><span class=\"o\">.</span><span class=\"n\">pos</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"</span><span class=\"si\">{}</span><span class=\"s2\">|</span><span class=\"si\">{}</span><span class=\"s2\">\"</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">token</span><span class=\"p\">,</span> <span class=\"n\">pos</span><span class=\"p\">),</span> <span class=\"n\">end</span><span class=\"o\">=</span><span class=\"s2\">\" \"</span><span class=\"p\">,</span> <span class=\"n\">flush</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<p>See more possibilities in <tt>tasks.py</tt> or <tt>cli.py</tt>.</p>\n<p>Streaming larger sequences can be achieved like this:</p>\n<pre><span class=\"c1\"># Streaming</span>\n<span class=\"n\">sentences</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"sent1</span><span class=\"se\">\\n</span><span class=\"s2\">\"</span><span class=\"p\">,</span> <span class=\"s2\">\"sent2</span><span class=\"se\">\\n</span><span class=\"s2\">\"</span><span class=\"p\">,</span> <span class=\"s2\">\"sent3</span><span class=\"se\">\\n</span><span class=\"s2\">\"</span><span class=\"p\">]</span>  <span class=\"c1\"># or any iterable (like File)</span>\n<span class=\"kn\">from</span> <span class=\"nn\">thai_segmenter</span> <span class=\"kn\">import</span> <span class=\"n\">line_sentence_segmenter</span>\n<span class=\"n\">sentences_segmented</span> <span class=\"o\">=</span> <span class=\"n\">line_sentence_segmenter</span><span class=\"p\">(</span><span class=\"n\">sentences</span><span class=\"p\">)</span>\n</pre>\n<div id=\"commandline-tool\">\n<h3>Commandline tool</h3>\n<p>This project also provides a nifty commandline tool <tt><span class=\"pre\">thai-segmenter</span></tt> that does most of the work for you:</p>\n<pre>usage: thai-segmenter <span class=\"o\">[</span>-h<span class=\"o\">]</span> <span class=\"o\">{</span>clean,sentseg,tokenize,tokpos<span class=\"o\">}</span> ...\n\nThai Segmentation utilities.\n\noptional arguments:\n  -h, --help            show this <span class=\"nb\">help</span> message and <span class=\"nb\">exit</span>\n\nTasks:\n  <span class=\"o\">{</span>clean,sentseg,tokenize,tokpos<span class=\"o\">}</span>\n    clean               Clean input from non-thai and blank lines.\n    sentseg             Sentence segmentize input lines.\n    tokenize            Tokenize input lines.\n    tokpos              Tokenize and POS-tag input lines.\n</pre>\n<p>You can run sentence segmentation like this:</p>\n<pre>thai-segmenter sentseg -i input.txt -o output.txt\n</pre>\n<p>or even pipe data:</p>\n<pre>cat input.txt | thai-segmenter sentseg &gt; output.txt\n</pre>\n<p>Use <tt><span class=\"pre\">-h</span></tt>/<tt><span class=\"pre\">--help</span></tt> to get more information about possible control flow options.</p>\n<p>You can run it somewhat interactively with:</p>\n<pre>thai-segmenter tokpos --stats\n</pre>\n<p>and standard input and output are used. Lines terminated with <tt>Enter</tt> are immediatly processed and printed. Stop work with key combination <tt>Ctrl</tt> + <tt>D</tt> and the <tt><span class=\"pre\">--stats</span></tt> parameter will helpfully output some statistics.</p>\n</div>\n<div id=\"webapp\">\n<h3>WebApp</h3>\n<p>The project also provides a demo WebApp (using <tt>Flask</tt> and <tt>gevent</tt>) that can be installed with:</p>\n<pre>pip install -e .[webapp]\n</pre>\n<p>and then simply run (in the foreground):</p>\n<pre>thai-segmenter-webapp\n</pre>\n<p>Consider running it in a <tt>screen</tt> session.</p>\n<pre><span class=\"c1\"># create the screen detached and then attach\n</span>screen -dmS thai-senseg-webapp\nscreen -r thai-senseg-webapp\n\n<span class=\"c1\"># in the screen:\n</span>thai-segmenter-webapp\n\n<span class=\"c1\"># and detach with keys [Ctrl]+[D]</span>\n</pre>\n<p><em>Please note that it only is a demo webapp to test and visualize how the sentence segmentor works.</em></p>\n</div>\n</div>\n<div id=\"development\">\n<h2>Development</h2>\n<p>To install the package for development:</p>\n<pre>git clone https://github.com/Querela/thai-segmenter.git\ncd thai-segmenter/\npip install -e .[dev]\n</pre>\n<p>After changing the source, run auto code formatting with:</p>\n<pre>black &lt;file&gt;.py\n</pre>\n<p>And check it afterwards with:</p>\n<pre>flake8 &lt;file&gt;.py\n</pre>\n<p>The <tt>setup.py</tt> also contains the <tt>flake8</tt> subcommand as well as an extended <tt>clean</tt> command.</p>\n<div id=\"tests\">\n<h3>Tests</h3>\n<p>To run the all tests run:</p>\n<pre>tox\n</pre>\n<p>You can also optionally run <tt>pytest</tt> alone:</p>\n<pre>pytest\n</pre>\n<p>Or with:</p>\n<pre>python setup.py test\n</pre>\n<p>Note, to combine the coverage data from all the tox environments run:</p>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><th>Windows</th>\n<td><pre>\nset PYTEST_ADDOPTS=--cov-append\ntox\n</pre>\n</td>\n</tr>\n<tr><th>Other</th>\n<td><pre>\nPYTEST_ADDOPTS=--cov-append tox\n</pre>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<div id=\"changelog\">\n<h2>Changelog</h2>\n<div id=\"id1\">\n<h3>0.4.1 (2019-04-08)</h3>\n<ul>\n<li>Fix tokenization / tokenization + POS tagging: return words instead of subwords</li>\n<li>Add <tt><span class=\"pre\">--escape-special</span></tt> and <tt><span class=\"pre\">--subwords</span></tt> parameter to CLI script for tokenization.\nAllows tokenization to further tokenize unknown words (e. g. names)\nas well as escape special characters with angle bracket entities.</li>\n</ul>\n</div>\n<div id=\"id2\">\n<h3>0.4.0 (2019-04-08)</h3>\n<ul>\n<li>Add demo webapp with sentence segmentation.\n(NOTE: Running both the webapp and (batch) sentence segmentation at the same time from the same installation is not recommeded. It can have unexpected side-effects.)</li>\n<li>Some reformat of <tt>README.rst</tt></li>\n</ul>\n</div>\n<div id=\"id3\">\n<h3>0.3.3 (2019-04-07)</h3>\n<ul>\n<li>Fix duplicate names (class/method for <tt>sentence_segment</tt>), rename class to <tt>sentence_segmenter</tt> (<tt>.py</tt>).</li>\n</ul>\n</div>\n<div id=\"id4\">\n<h3>0.3.2 (2019-04-07)</h3>\n<ul>\n<li>Add <tt>twine</tt> to extras dependencies.</li>\n<li>Publish module on <strong>PyPI</strong>. (Only <tt>sdist</tt>, <tt>bdist_wheel</tt> can\u2019t be built currently.)</li>\n<li>Fix some TravisCI warnings.</li>\n</ul>\n</div>\n<div id=\"id5\">\n<h3>0.3.1 (2019-04-07)</h3>\n<ul>\n<li>Add tasks to <tt>__init__.py</tt> for easier access.</li>\n</ul>\n</div>\n<div id=\"id6\">\n<h3>0.3.0 (2019-04-06)</h3>\n<ul>\n<li>Refactor tasks into <tt>tasks.py</tt> to enable better import in case of embedding thai-segmenter into other projects.</li>\n<li>Have it almost release ready. :-)</li>\n<li>Add some more parameters to functions (optional header detection function)</li>\n<li>Flesh out <tt>README.rst</tt> with examples and descriptions.</li>\n<li>Add Changelog items.</li>\n</ul>\n</div>\n<div id=\"id7\">\n<h3>0.2.1 / 0.2.2 (2019-04-05)</h3>\n<ul>\n<li>Many changes, <tt>bumpversion</tt> needs to run where <tt>.bumpversion.cfg</tt> is located else it silently fails \u2026</li>\n<li>Strip Typehints and add support for Python3.5 again.</li>\n<li>Add CLI tasks for cleaning, sentseg, tokenize, pos-tagging.</li>\n<li>Add various params, e. g. for selecting columns, skipping headers.</li>\n<li>Fix many bugs for TravisCI (isort, flake8)</li>\n<li>Use iterators / streaming approach for file input/output.</li>\n</ul>\n</div>\n<div id=\"id8\">\n<h3>0.2.0 (2019-04-05)</h3>\n<ul>\n<li>Remove support of Python 2.7 and lower equal to Python 3.5 because of Typehints.</li>\n<li>Added CLI skeleton.</li>\n<li>Add really good <tt>setup.py</tt>. (with <tt>black</tt>, <tt>flake8</tt>)</li>\n</ul>\n</div>\n<div id=\"id9\">\n<h3>0.1.0 (2019-04-05)</h3>\n<ul>\n<li>First release version as package.</li>\n</ul>\n</div>\n</div>\n\n          </div>"}, "last_serial": 5111262, "releases": {"0.3.2": [{"comment_text": "", "digests": {"md5": "f895f367e8be5d6352a68d879e797209", "sha256": "e927f70cdd91a966aa2b5842ce2b17aa2488c7406b676b562bd04e83a3d8e0cf"}, "downloads": -1, "filename": "thai-segmenter-0.3.2.tar.gz", "has_sig": false, "md5_digest": "f895f367e8be5d6352a68d879e797209", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 2392480, "upload_time": "2019-04-07T19:35:07", "upload_time_iso_8601": "2019-04-07T19:35:07.455062Z", "url": "https://files.pythonhosted.org/packages/c1/0a/6b7db4efd2ccbc2a8e93f73092f10e84cccd0d91c72a91f6baa620ee77e2/thai-segmenter-0.3.2.tar.gz", "yanked": false}], "0.3.3": [{"comment_text": "", "digests": {"md5": "003a25aff2aa92a304105cef1fdff6ca", "sha256": "53d397e552b5e5d17568a164c8749966193d77cd816efb41f0da64058c7b96f7"}, "downloads": -1, "filename": "thai-segmenter-0.3.3.tar.gz", "has_sig": false, "md5_digest": "003a25aff2aa92a304105cef1fdff6ca", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 2393152, "upload_time": "2019-04-07T21:03:43", "upload_time_iso_8601": "2019-04-07T21:03:43.413975Z", "url": "https://files.pythonhosted.org/packages/ae/fa/fdbde26abaedf2de497bac83eb562f41a51ee31c0af61200c6fafa6635e4/thai-segmenter-0.3.3.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "7f0749da035da4d8c8d40dcfbc0274f1", "sha256": "4717e4211d764b5cbfe88f21726b6d44fafaf4960761fb6369f16b9e4ba99fc0"}, "downloads": -1, "filename": "thai-segmenter-0.4.0.tar.gz", "has_sig": false, "md5_digest": "7f0749da035da4d8c8d40dcfbc0274f1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 2400925, "upload_time": "2019-04-07T22:46:45", "upload_time_iso_8601": "2019-04-07T22:46:45.412659Z", "url": "https://files.pythonhosted.org/packages/7a/57/5dced6be08a85dfdd47aacd4dc2e174c76698b58726824a23c083246bfb1/thai-segmenter-0.4.0.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "2cf3b2728698c7c66191b37a126ab2eb", "sha256": "7911ef6e932df9bfc485d272761aa6c3f3915e5f9f216d094dcda29fb3a4ec75"}, "downloads": -1, "filename": "thai-segmenter-0.4.1.tar.gz", "has_sig": false, "md5_digest": "2cf3b2728698c7c66191b37a126ab2eb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 2401108, "upload_time": "2019-04-07T23:17:09", "upload_time_iso_8601": "2019-04-07T23:17:09.842224Z", "url": "https://files.pythonhosted.org/packages/95/85/e19b45eb8f2f1cff258e630bf3e3c95376769b8e1f1e0a0a7218133135eb/thai-segmenter-0.4.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2cf3b2728698c7c66191b37a126ab2eb", "sha256": "7911ef6e932df9bfc485d272761aa6c3f3915e5f9f216d094dcda29fb3a4ec75"}, "downloads": -1, "filename": "thai-segmenter-0.4.1.tar.gz", "has_sig": false, "md5_digest": "2cf3b2728698c7c66191b37a126ab2eb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 2401108, "upload_time": "2019-04-07T23:17:09", "upload_time_iso_8601": "2019-04-07T23:17:09.842224Z", "url": "https://files.pythonhosted.org/packages/95/85/e19b45eb8f2f1cff258e630bf3e3c95376769b8e1f1e0a0a7218133135eb/thai-segmenter-0.4.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:54:11 2020"}