{"info": {"author": "8080 Labs, Florian Wetschoreck, Tobias Krabel", "author_email": "info@8080labs.com", "bugtrack_url": null, "classifiers": [], "description": "# ppscore - a Python implementation of the Predictive Power Score (PPS)\n\n### From the makers of [bamboolib](https://bamboolib.com)\n\n\n<!-- __If you don't know what the Predictive Power Score is, please read the following blog post: [RIP correlation. Introducing the Predictive Power Score](https://bamboolib.com)__ -->\n\nThe PPS is an asymmetric, data-type-agnostic score that can detect linear or non-linear relationships between two columns. The score ranges from 0 (no predictive power) to 1 (perfect predictive power). It can be used as an alternative to the correlation (matrix).\n\n\n- [Installation](#installation)\n- [Getting started](#getting-started)\n- [API](#api)\n- [Calculation of the PPS](#calculation-of-the-pps)\n- [About](#about)\n\n\n## Installation\n\n> You need Python 3.6 or above.\n\nFrom the terminal (or Anaconda prompt in Windows), enter:\n\n```bash\npip install ppscore\n```\n\n\n## Getting started\n\nFirst, let's create some data:\n\n```python\nimport pandas as pd\nimport numpy as np\nimport ppscore as pps\n\ndf = pd.DataFrame()\ndf[\"x\"] = np.random.uniform(-2, 2, 1_000_000)\ndf[\"error\"] = np.random.uniform(-0.5, 0.5, 1_000_000)\ndf[\"y\"] = df[\"x\"] * df[\"x\"] + df[\"error\"]\n```\n\nBased on the dataframe we can calculate the PPS of x predicting y:\n\n```python\npps.score(df, \"x\", \"y\")\n```\n\nHere is how we can calculate the PPS matrix between all columns:\n\n```python\npps.matrix(df)\n```\n\n\n## API\n\n### ppscore.score(df, x, y, task=None, sample=5000)\n\nCalculate the Predictive Power Score (PPS) for \"x predicts y\"\n\n- The score always ranges from 0 to 1 and is data-type agnostic.\n\n- A score of 0 means that the column x cannot predict the column y better than a naive baseline model.\n\n- A score of 1 means that the column x can perfectly predict the column y given the model.\n\n- A score between 0 and 1 states the ratio of how much potential predictive power the model achieved compared to the baseline model.\n\n\n#### Parameters\n\n- __df__ : pandas.DataFrame\n    - Dataframe that contains the columns x and y\n- __x__ : str\n    - Name of the column x which acts as the feature\n- __y__ : str\n    - Name of the column y which acts as the target\n- __task__ : str, default ``None``\n    - Name of the prediction task, e.g. ``classification`` or ``regression```\n    If the task is not specified, it is infered based on the y column\n    The task determines which model and evaluation score is used for the PPS\n- __sample__ : int or ``None``\n    - Number of rows for sampling. The sampling decreases the calculation time of the PPS.\n    If ``None`` there will be no sampling.\n\n#### Returns\n\n- __Dict__:\n    - A dict that contains multiple fields about the resulting PPS.\n    The dict enables introspection into the calculations that have been performed under the hood\n\n\n### ppscore.matrix(df, output=\"df\", **kwargs)\n\nCalculate the Predictive Power Score (PPS) matrix for all columns in the dataframe\n\n#### Parameters\n\n- __df__ : pandas.DataFrame\n    - The dataframe that contains the data\n- __output__ : str - potential values: \"df\", \"dict\"\n    - Control the type of the output. Either return a df or a dict with all the PPS dicts arranged by the target column\n- __kwargs__ :\n    - Other key-word arguments that shall be forwarded to the pps.score method\n\n#### Returns\n\n- __pandas.DataFrame__ or __Dict__:\n    - Either returns a df or a dict with all the PPS dicts arranged by the target column. This can be influenced by the output argument\n\n\n## Calculation of the PPS\n\n> If you are uncertain about some details, feel free to jump into the code to have a look at the exact implementation\n\nThere are multiple ways how you can calculate the PPS. The ppscore package provides a sample implementation that is based on the following calculations:\n\n- The score is calculated using only 1 feature trying to predict the target column. This means there are no interaction effects between the scores of various features. Note that this is in contrast to feature importance\n- The score is calculated on the test sets of a 4-fold crossvalidation (number is adjustable via `ppscore.CV_ITERATIONS`)\n- All rows which have a missing value in the feature or the target column are dropped\n- In case that the dataset has more than 5,000 rows the score is only calculated on a random subset of 5,000 rows with a fixed random seed (`ppscore.RANDOM_SEED`). You can adjust the number of rows or skip this sampling via the API. However, in most scenarios the results will be very similar.\n- There is no grid search for optimal model parameters\n\n\n### Learning algorithm\n\nAs a learning algorithm, we currently use a Decision Tree because the Decision Tree has the following properties:\n- can detect any non-linear bivariate relationship\n- good predictive power in a wide variety of use cases\n- low requirements for feature preprocessing\n- robust model which can handle outliers and does not easily overfit\n- can be used for classification and regression\n- can be calculated quicker than many other algorithms\n\nWe differentiate the exact implementation based on the data type of the target column:\n- If the target column is numeric, we use the sklearn.DecisionTreeRegressor\n- If the target column is categoric, we use the sklearn.DecisionTreeClassifier\n\n> Please note that we prefer a general good performance on a wide variety of use cases over better performance in some narrow use cases. If you have a proposal for a better/different learning algorithm, please open an issue\n\nHowever, please note why we actively decided against the following algorithms:\n\n- Correlation or Linear Regression: cannot detect non-linear bivariate relationships without extensive preprocessing\n- GAMs: might have problems with very unsmooth functions\n- SVM: potentially bad performance if the wrong kernel is selected\n- Random Forest/Gradient Boosted Tree: slower than a single Decision Tree\n- Neural Networks and Deep Learning: slower calculation than a Decision Tree and also needs more feature preprocessing\n\n### Data preprocessing\n\nEven though the Decision Tree is a very flexible learning algorithm, we need to perform the following preprocessing steps if a column has the pandas dtype `object`.\u200c\n- If the target column is categoric, we use the sklearn.LabelEncoder\u200b\n- If the feature column is categoric, we use the sklearn.OneHotEncoder\u200b\n\n\n### Inference of the prediction task\n\nThe choice of the task (classification or regression) has an influence on the final PPS and thus it is important how the task is chosen. If you calculate a single score, you can pass in a specific task. If you do not specify the task, the task is inferred as follows.\n\nA __classification__ is inferred if one of the following conditions meet:\n- the target has the dtype `object` or `categorical`\n- the target only has two unique values\n- the target is numeric but has less than 15 unique values. This breakpoint can be overridden via the constant `ppscore.NUMERIC_AS_CATEGORIC_BREAKPOINT`\n\nOtherwise, the task is inferred as __regression__ if the dtype is numeric (float or integer).\n\n\n### Tasks and their score metrics\u200b\n\nBased on the data type and cardinality of the target column, ppscore assumes either the task of a classification or regression. Each task uses a different evaluation score for calculating the final predictive power score (PPS).\n\n#### Regression\n\nIn case of an regression, the ppscore uses the mean absolute error (MAE) as the underlying evaluation metric (MAE_model). The best possible score of the MAE is 0 and higher is worse. As a baseline score, we calculate the MAE of a naive model (MAE_naive) that always predicts the median of the target column. The PPS is the result of the following normalization (and never smaller than 0):\n> PPS = 1 - (MAE_model / MAE_naive)\n\n#### Classification\n\nIf the task is a classification, we compute the weighted F1 score (wF1) as the underlying evaluation metric (F1_model). The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The weighted F1 takes into account the precision and recall of all classes weighted by their support as described [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). As a baseline score, we calculate the weighted F1 score of a naive model (F1_naive) that always predicts the most common class of the target column. The PPS is the result of the following normalization (and never smaller than 0):\n> PPS = (F1_model - F1_naive) / (1 - F1_naive)\n\n\n## About\nppscore is developed by [8080 Labs](https://8080labs.com) - we create tools for Python Data Scientists. If you like `ppscore`, please check out our other project [bamboolib](https://bamboolib.com)", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/8080labs/ppscore/", "keywords": "", "license": "mit", "maintainer": "", "maintainer_email": "", "name": "ppscore", "package_url": "https://pypi.org/project/ppscore/", "platform": "any", "project_url": "https://pypi.org/project/ppscore/", "project_urls": {"Documentation": "https://github.com/8080labs/ppscore/", "Homepage": "https://github.com/8080labs/ppscore/"}, "release_url": "https://pypi.org/project/ppscore/0.0.2/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Python implementation of the Predictive Power Score (PPS)", "version": "0.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>ppscore - a Python implementation of the Predictive Power Score (PPS)</h1>\n<h3>From the makers of <a href=\"https://bamboolib.com\" rel=\"nofollow\">bamboolib</a></h3>\n\n<p>The PPS is an asymmetric, data-type-agnostic score that can detect linear or non-linear relationships between two columns. The score ranges from 0 (no predictive power) to 1 (perfect predictive power). It can be used as an alternative to the correlation (matrix).</p>\n<ul>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#getting-started\" rel=\"nofollow\">Getting started</a></li>\n<li><a href=\"#api\" rel=\"nofollow\">API</a></li>\n<li><a href=\"#calculation-of-the-pps\" rel=\"nofollow\">Calculation of the PPS</a></li>\n<li><a href=\"#about\" rel=\"nofollow\">About</a></li>\n</ul>\n<h2>Installation</h2>\n<blockquote>\n<p>You need Python 3.6 or above.</p>\n</blockquote>\n<p>From the terminal (or Anaconda prompt in Windows), enter:</p>\n<pre>pip install ppscore\n</pre>\n<h2>Getting started</h2>\n<p>First, let's create some data:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">ppscore</span> <span class=\"k\">as</span> <span class=\"nn\">pps</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">()</span>\n<span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s2\">\"x\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">uniform</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1_000_000</span><span class=\"p\">)</span>\n<span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s2\">\"error\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">uniform</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mi\">1_000_000</span><span class=\"p\">)</span>\n<span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s2\">\"y\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s2\">\"x\"</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s2\">\"x\"</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s2\">\"error\"</span><span class=\"p\">]</span>\n</pre>\n<p>Based on the dataframe we can calculate the PPS of x predicting y:</p>\n<pre><span class=\"n\">pps</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"s2\">\"x\"</span><span class=\"p\">,</span> <span class=\"s2\">\"y\"</span><span class=\"p\">)</span>\n</pre>\n<p>Here is how we can calculate the PPS matrix between all columns:</p>\n<pre><span class=\"n\">pps</span><span class=\"o\">.</span><span class=\"n\">matrix</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n</pre>\n<h2>API</h2>\n<h3>ppscore.score(df, x, y, task=None, sample=5000)</h3>\n<p>Calculate the Predictive Power Score (PPS) for \"x predicts y\"</p>\n<ul>\n<li>\n<p>The score always ranges from 0 to 1 and is data-type agnostic.</p>\n</li>\n<li>\n<p>A score of 0 means that the column x cannot predict the column y better than a naive baseline model.</p>\n</li>\n<li>\n<p>A score of 1 means that the column x can perfectly predict the column y given the model.</p>\n</li>\n<li>\n<p>A score between 0 and 1 states the ratio of how much potential predictive power the model achieved compared to the baseline model.</p>\n</li>\n</ul>\n<h4>Parameters</h4>\n<ul>\n<li><strong>df</strong> : pandas.DataFrame\n<ul>\n<li>Dataframe that contains the columns x and y</li>\n</ul>\n</li>\n<li><strong>x</strong> : str\n<ul>\n<li>Name of the column x which acts as the feature</li>\n</ul>\n</li>\n<li><strong>y</strong> : str\n<ul>\n<li>Name of the column y which acts as the target</li>\n</ul>\n</li>\n<li><strong>task</strong> : str, default <code>None</code>\n<ul>\n<li>Name of the prediction task, e.g. <code>classification</code> or ``regression```\nIf the task is not specified, it is infered based on the y column\nThe task determines which model and evaluation score is used for the PPS</li>\n</ul>\n</li>\n<li><strong>sample</strong> : int or <code>None</code>\n<ul>\n<li>Number of rows for sampling. The sampling decreases the calculation time of the PPS.\nIf <code>None</code> there will be no sampling.</li>\n</ul>\n</li>\n</ul>\n<h4>Returns</h4>\n<ul>\n<li><strong>Dict</strong>:\n<ul>\n<li>A dict that contains multiple fields about the resulting PPS.\nThe dict enables introspection into the calculations that have been performed under the hood</li>\n</ul>\n</li>\n</ul>\n<h3>ppscore.matrix(df, output=\"df\", **kwargs)</h3>\n<p>Calculate the Predictive Power Score (PPS) matrix for all columns in the dataframe</p>\n<h4>Parameters</h4>\n<ul>\n<li><strong>df</strong> : pandas.DataFrame\n<ul>\n<li>The dataframe that contains the data</li>\n</ul>\n</li>\n<li><strong>output</strong> : str - potential values: \"df\", \"dict\"\n<ul>\n<li>Control the type of the output. Either return a df or a dict with all the PPS dicts arranged by the target column</li>\n</ul>\n</li>\n<li><strong>kwargs</strong> :\n<ul>\n<li>Other key-word arguments that shall be forwarded to the pps.score method</li>\n</ul>\n</li>\n</ul>\n<h4>Returns</h4>\n<ul>\n<li><strong>pandas.DataFrame</strong> or <strong>Dict</strong>:\n<ul>\n<li>Either returns a df or a dict with all the PPS dicts arranged by the target column. This can be influenced by the output argument</li>\n</ul>\n</li>\n</ul>\n<h2>Calculation of the PPS</h2>\n<blockquote>\n<p>If you are uncertain about some details, feel free to jump into the code to have a look at the exact implementation</p>\n</blockquote>\n<p>There are multiple ways how you can calculate the PPS. The ppscore package provides a sample implementation that is based on the following calculations:</p>\n<ul>\n<li>The score is calculated using only 1 feature trying to predict the target column. This means there are no interaction effects between the scores of various features. Note that this is in contrast to feature importance</li>\n<li>The score is calculated on the test sets of a 4-fold crossvalidation (number is adjustable via <code>ppscore.CV_ITERATIONS</code>)</li>\n<li>All rows which have a missing value in the feature or the target column are dropped</li>\n<li>In case that the dataset has more than 5,000 rows the score is only calculated on a random subset of 5,000 rows with a fixed random seed (<code>ppscore.RANDOM_SEED</code>). You can adjust the number of rows or skip this sampling via the API. However, in most scenarios the results will be very similar.</li>\n<li>There is no grid search for optimal model parameters</li>\n</ul>\n<h3>Learning algorithm</h3>\n<p>As a learning algorithm, we currently use a Decision Tree because the Decision Tree has the following properties:</p>\n<ul>\n<li>can detect any non-linear bivariate relationship</li>\n<li>good predictive power in a wide variety of use cases</li>\n<li>low requirements for feature preprocessing</li>\n<li>robust model which can handle outliers and does not easily overfit</li>\n<li>can be used for classification and regression</li>\n<li>can be calculated quicker than many other algorithms</li>\n</ul>\n<p>We differentiate the exact implementation based on the data type of the target column:</p>\n<ul>\n<li>If the target column is numeric, we use the sklearn.DecisionTreeRegressor</li>\n<li>If the target column is categoric, we use the sklearn.DecisionTreeClassifier</li>\n</ul>\n<blockquote>\n<p>Please note that we prefer a general good performance on a wide variety of use cases over better performance in some narrow use cases. If you have a proposal for a better/different learning algorithm, please open an issue</p>\n</blockquote>\n<p>However, please note why we actively decided against the following algorithms:</p>\n<ul>\n<li>Correlation or Linear Regression: cannot detect non-linear bivariate relationships without extensive preprocessing</li>\n<li>GAMs: might have problems with very unsmooth functions</li>\n<li>SVM: potentially bad performance if the wrong kernel is selected</li>\n<li>Random Forest/Gradient Boosted Tree: slower than a single Decision Tree</li>\n<li>Neural Networks and Deep Learning: slower calculation than a Decision Tree and also needs more feature preprocessing</li>\n</ul>\n<h3>Data preprocessing</h3>\n<p>Even though the Decision Tree is a very flexible learning algorithm, we need to perform the following preprocessing steps if a column has the pandas dtype <code>object</code>.\u200c</p>\n<ul>\n<li>If the target column is categoric, we use the sklearn.LabelEncoder\u200b</li>\n<li>If the feature column is categoric, we use the sklearn.OneHotEncoder\u200b</li>\n</ul>\n<h3>Inference of the prediction task</h3>\n<p>The choice of the task (classification or regression) has an influence on the final PPS and thus it is important how the task is chosen. If you calculate a single score, you can pass in a specific task. If you do not specify the task, the task is inferred as follows.</p>\n<p>A <strong>classification</strong> is inferred if one of the following conditions meet:</p>\n<ul>\n<li>the target has the dtype <code>object</code> or <code>categorical</code></li>\n<li>the target only has two unique values</li>\n<li>the target is numeric but has less than 15 unique values. This breakpoint can be overridden via the constant <code>ppscore.NUMERIC_AS_CATEGORIC_BREAKPOINT</code></li>\n</ul>\n<p>Otherwise, the task is inferred as <strong>regression</strong> if the dtype is numeric (float or integer).</p>\n<h3>Tasks and their score metrics\u200b</h3>\n<p>Based on the data type and cardinality of the target column, ppscore assumes either the task of a classification or regression. Each task uses a different evaluation score for calculating the final predictive power score (PPS).</p>\n<h4>Regression</h4>\n<p>In case of an regression, the ppscore uses the mean absolute error (MAE) as the underlying evaluation metric (MAE_model). The best possible score of the MAE is 0 and higher is worse. As a baseline score, we calculate the MAE of a naive model (MAE_naive) that always predicts the median of the target column. The PPS is the result of the following normalization (and never smaller than 0):</p>\n<blockquote>\n<p>PPS = 1 - (MAE_model / MAE_naive)</p>\n</blockquote>\n<h4>Classification</h4>\n<p>If the task is a classification, we compute the weighted F1 score (wF1) as the underlying evaluation metric (F1_model). The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The weighted F1 takes into account the precision and recall of all classes weighted by their support as described <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\" rel=\"nofollow\">here</a>. As a baseline score, we calculate the weighted F1 score of a naive model (F1_naive) that always predicts the most common class of the target column. The PPS is the result of the following normalization (and never smaller than 0):</p>\n<blockquote>\n<p>PPS = (F1_model - F1_naive) / (1 - F1_naive)</p>\n</blockquote>\n<h2>About</h2>\n<p>ppscore is developed by <a href=\"https://8080labs.com\" rel=\"nofollow\">8080 Labs</a> - we create tools for Python Data Scientists. If you like <code>ppscore</code>, please check out our other project <a href=\"https://bamboolib.com\" rel=\"nofollow\">bamboolib</a></p>\n\n          </div>"}, "last_serial": 7084897, "releases": {"0.0.2": [{"comment_text": "", "digests": {"md5": "72aaaa77eac5003cf58d74bdacf54945", "sha256": "bfa0a106100f82f4640a3240cb164bdabdddafcfa6509c83f3b85f28044d39d3"}, "downloads": -1, "filename": "ppscore-0.0.2.tar.gz", "has_sig": false, "md5_digest": "72aaaa77eac5003cf58d74bdacf54945", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 38299, "upload_time": "2020-04-23T14:40:00", "upload_time_iso_8601": "2020-04-23T14:40:00.711314Z", "url": "https://files.pythonhosted.org/packages/4d/0f/32b991b05f392ca2f86587972f5d94d86ea7d6fdb659a607c52a67dbcfcd/ppscore-0.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "72aaaa77eac5003cf58d74bdacf54945", "sha256": "bfa0a106100f82f4640a3240cb164bdabdddafcfa6509c83f3b85f28044d39d3"}, "downloads": -1, "filename": "ppscore-0.0.2.tar.gz", "has_sig": false, "md5_digest": "72aaaa77eac5003cf58d74bdacf54945", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 38299, "upload_time": "2020-04-23T14:40:00", "upload_time_iso_8601": "2020-04-23T14:40:00.711314Z", "url": "https://files.pythonhosted.org/packages/4d/0f/32b991b05f392ca2f86587972f5d94d86ea7d6fdb659a607c52a67dbcfcd/ppscore-0.0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:20:22 2020"}