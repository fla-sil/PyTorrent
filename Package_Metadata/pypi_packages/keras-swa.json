{"info": {"author": "Simon Larsson", "author_email": "larssonsimon0@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3 :: Only", "Topic :: Scientific/Engineering"], "description": "# Keras SWA - Stochastic Weight Averaging\n\n[![PyPI version](https://badge.fury.io/py/keras-swa.svg)](https://pypi.python.org/pypi/keras-swa/) \n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/simon-larsson/keras-swa/blob/master/LICENSE)\n\nThis is an implemention of SWA for Keras and TF-Keras.\n\n## Introduction\nStochastic weight averaging (SWA) is build upon the same principle as [snapshot ensembling](https://arxiv.org/abs/1704.00109) and [fast geometric ensembling](https://arxiv.org/abs/1802.10026). The idea is that averaging select stages of training can lead to better models. Where as the two former methods average by sampling and ensembling models, SWA instead average weights. This has been shown to give comparable improvements confined into a single model.\n\n[![Illustration](https://raw.githubusercontent.com/simon-larsson/keras-swa/master/swa_illustration.png)](https://raw.githubusercontent.com/simon-larsson/keras-swa/master/swa_illustration.png)\n\n## Paper\n - Title: Averaging Weights Leads to Wider Optima and Better Generalization\n - Link: https://arxiv.org/abs/1803.05407\n - Authors: Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson\n - Repo: https://github.com/timgaripov/swa (PyTorch)\n\n## Installation\n\n    pip install keras-swa\n\n### SWA API\n\nKeras callback object for SWA.  \n\n### Arguments\n**start_epoch** - Starting epoch for SWA.\n\n**lr_schedule** - Learning rate schedule. `'manual'` , `'constant'` or `'cyclic'`.\n\n**swa_lr** - Learning rate used when averaging weights.\n\n**swa_lr2** - Upper bound of learning rate for the cyclic schedule.\n\n**swa_freq** - Frequency of weight averagining. Used with cyclic schedules.\n\n**batch_size** - Batch size. Only needed in the Keras API when using both batch normalization and a data generator.\n\n**verbose** - Verbosity mode, 0 or 1.\n\n### Batch Normalization\nLast epoch will be a forward pass, i.e. have learning rate set to zero, for models with batch normalization. This is due to the fact that batch normalization uses the running mean and variance of it's preceding layer to make a normalization. SWA will offset this normalization by suddenly changing the weights in the end of training. Therefore, it is necessary for the last epoch to be used to reset and recalculate batch normalization running mean and variance for the updated weights. Batch normalization gamma and beta values are preserved.\n\n**When using manual schedule:** The SWA callback will set learning rate to zero in the last epoch if batch normalization is used. This must not be undone by any external learning rate schedulers for SWA to work properly. \n\n### Learning Rate Schedules\nThe default schedule is `'manual'`, allowing the learning rate to be controlled by an external learning rate scheduler or the optimizer. Then SWA will only affect the final weights and the learning rate of the last epoch if batch normalization is used. The schedules for the two predefined, `'constant'` or `'cyclic'` can be observed below.\n\n[![lr_schedules](https://raw.githubusercontent.com/simon-larsson/keras-swa/master/lr_schedules.png)](https://raw.githubusercontent.com/simon-larsson/keras-swa/master/lr_schedules.png)\n\n\n#### Example\n\nFor Keras (with constant LR)\n```python\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\nfrom swa.keras import SWA\n \n# make dataset\nX, y = make_blobs(n_samples=1000, \n                  centers=3, \n                  n_features=2, \n                  cluster_std=2, \n                  random_state=2)\n\ny = to_categorical(y)\n\n# build model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', \n              optimizer=SGD(lr=0.1))\n\nepochs = 100\nstart_epoch = 75\n\n# define swa callback\nswa = SWA(start_epoch=start_epoch, \n          lr_schedule='constant', \n          swa_lr=0.01, \n          verbose=1)\n\n# train\nmodel.fit(X, y, epochs=epochs, verbose=1, callbacks=[swa])\n```\n\nOr for Keras in Tensorflow (with Cyclic LR)\n\n```python\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\n\nfrom swa.tfkeras import SWA\n\n# make dataset\nX, y = make_blobs(n_samples=1000, \n                  centers=3, \n                  n_features=2, \n                  cluster_std=2, \n                  random_state=2)\n\ny = to_categorical(y)\n\n# build model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', \n              optimizer=SGD(learning_rate=0.1))\n\nepochs = 100\nstart_epoch = 75\n\n# define swa callback\nswa = SWA(start_epoch=start_epoch, \n          lr_schedule='cyclic', \n          swa_lr=0.001,\n          swa_lr2=0.003,\n          swa_freq=3,\n          verbose=1)\n\n# train\nmodel.fit(X, y, epochs=epochs, verbose=1, callbacks=[swa])\n```\n\nOutput\n```\nModel uses batch normalization. SWA will require last epoch to be a forward pass and will run with no learning rate\nEpoch 1/100\n1000/1000 [==============================] - 1s 547us/sample - loss: 0.5529\nEpoch 2/100\n1000/1000 [==============================] - 0s 160us/sample - loss: 0.4720\n...\nEpoch 74/100\n1000/1000 [==============================] - 0s 160us/sample - loss: 0.4249\n\nEpoch 00075: starting stochastic weight averaging\nEpoch 75/100\n1000/1000 [==============================] - 0s 164us/sample - loss: 0.4357\nEpoch 76/100\n1000/1000 [==============================] - 0s 165us/sample - loss: 0.4209\n...\nEpoch 99/100\n1000/1000 [==============================] - 0s 167us/sample - loss: 0.4263\n\nEpoch 00100: final model weights set to stochastic weight average\n\nEpoch 00100: reinitializing batch normalization layers\n\nEpoch 00100: running forward pass to adjust batch normalization\nEpoch 100/100\n1000/1000 [==============================] - 0s 166us/sample - loss: 0.4408\n```\n\n### Collaborators\n\n- [Simon Larsson](https://github.com/simon-larsson \"Github\")\n- [Alex Stoken](https://github.com/alexstoken \"Github\")", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/simon-larsson/keras-swa", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "keras-swa", "package_url": "https://pypi.org/project/keras-swa/", "platform": "", "project_url": "https://pypi.org/project/keras-swa/", "project_urls": {"Homepage": "https://github.com/simon-larsson/keras-swa"}, "release_url": "https://pypi.org/project/keras-swa/0.1.5/", "requires_dist": null, "requires_python": "", "summary": "Simple stochastic weight averaging callback for Keras.", "version": "0.1.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Keras SWA - Stochastic Weight Averaging</h1>\n<p><a href=\"https://pypi.python.org/pypi/keras-swa/\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c77cde961fafb53ba8cab506703024a4e7cc7b90/68747470733a2f2f62616467652e667572792e696f2f70792f6b657261732d7377612e737667\"></a>\n<a href=\"https://github.com/simon-larsson/keras-swa/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c88fab50b4a1dc0cd91faeb7ba5654d56e380260/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667\"></a></p>\n<p>This is an implemention of SWA for Keras and TF-Keras.</p>\n<h2>Introduction</h2>\n<p>Stochastic weight averaging (SWA) is build upon the same principle as <a href=\"https://arxiv.org/abs/1704.00109\" rel=\"nofollow\">snapshot ensembling</a> and <a href=\"https://arxiv.org/abs/1802.10026\" rel=\"nofollow\">fast geometric ensembling</a>. The idea is that averaging select stages of training can lead to better models. Where as the two former methods average by sampling and ensembling models, SWA instead average weights. This has been shown to give comparable improvements confined into a single model.</p>\n<p><a href=\"https://raw.githubusercontent.com/simon-larsson/keras-swa/master/swa_illustration.png\" rel=\"nofollow\"><img alt=\"Illustration\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/95e8e48f73d1ac136c681da3094b5d1fa51a2ba3/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e2d6c617273736f6e2f6b657261732d7377612f6d61737465722f7377615f696c6c757374726174696f6e2e706e67\"></a></p>\n<h2>Paper</h2>\n<ul>\n<li>Title: Averaging Weights Leads to Wider Optima and Better Generalization</li>\n<li>Link: <a href=\"https://arxiv.org/abs/1803.05407\" rel=\"nofollow\">https://arxiv.org/abs/1803.05407</a></li>\n<li>Authors: Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson</li>\n<li>Repo: <a href=\"https://github.com/timgaripov/swa\" rel=\"nofollow\">https://github.com/timgaripov/swa</a> (PyTorch)</li>\n</ul>\n<h2>Installation</h2>\n<pre><code>pip install keras-swa\n</code></pre>\n<h3>SWA API</h3>\n<p>Keras callback object for SWA.</p>\n<h3>Arguments</h3>\n<p><strong>start_epoch</strong> - Starting epoch for SWA.</p>\n<p><strong>lr_schedule</strong> - Learning rate schedule. <code>'manual'</code> , <code>'constant'</code> or <code>'cyclic'</code>.</p>\n<p><strong>swa_lr</strong> - Learning rate used when averaging weights.</p>\n<p><strong>swa_lr2</strong> - Upper bound of learning rate for the cyclic schedule.</p>\n<p><strong>swa_freq</strong> - Frequency of weight averagining. Used with cyclic schedules.</p>\n<p><strong>batch_size</strong> - Batch size. Only needed in the Keras API when using both batch normalization and a data generator.</p>\n<p><strong>verbose</strong> - Verbosity mode, 0 or 1.</p>\n<h3>Batch Normalization</h3>\n<p>Last epoch will be a forward pass, i.e. have learning rate set to zero, for models with batch normalization. This is due to the fact that batch normalization uses the running mean and variance of it's preceding layer to make a normalization. SWA will offset this normalization by suddenly changing the weights in the end of training. Therefore, it is necessary for the last epoch to be used to reset and recalculate batch normalization running mean and variance for the updated weights. Batch normalization gamma and beta values are preserved.</p>\n<p><strong>When using manual schedule:</strong> The SWA callback will set learning rate to zero in the last epoch if batch normalization is used. This must not be undone by any external learning rate schedulers for SWA to work properly.</p>\n<h3>Learning Rate Schedules</h3>\n<p>The default schedule is <code>'manual'</code>, allowing the learning rate to be controlled by an external learning rate scheduler or the optimizer. Then SWA will only affect the final weights and the learning rate of the last epoch if batch normalization is used. The schedules for the two predefined, <code>'constant'</code> or <code>'cyclic'</code> can be observed below.</p>\n<p><a href=\"https://raw.githubusercontent.com/simon-larsson/keras-swa/master/lr_schedules.png\" rel=\"nofollow\"><img alt=\"lr_schedules\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/570be1fec079b3ab28407108e1e7d3a2a2e620e0/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e2d6c617273736f6e2f6b657261732d7377612f6d61737465722f6c725f7363686564756c65732e706e67\"></a></p>\n<h4>Example</h4>\n<p>For Keras (with constant LR)</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets.samples_generator</span> <span class=\"kn\">import</span> <span class=\"n\">make_blobs</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.utils</span> <span class=\"kn\">import</span> <span class=\"n\">to_categorical</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">Sequential</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Dense</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.optimizers</span> <span class=\"kn\">import</span> <span class=\"n\">SGD</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">swa.keras</span> <span class=\"kn\">import</span> <span class=\"n\">SWA</span>\n \n<span class=\"c1\"># make dataset</span>\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">make_blobs</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span> \n                  <span class=\"n\">centers</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> \n                  <span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> \n                  <span class=\"n\">cluster_std</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> \n                  <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">to_categorical</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># build model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">()</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"n\">input_dim</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'relu'</span><span class=\"p\">))</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'softmax'</span><span class=\"p\">))</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s1\">'categorical_crossentropy'</span><span class=\"p\">,</span> \n              <span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"n\">SGD</span><span class=\"p\">(</span><span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">))</span>\n\n<span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n<span class=\"n\">start_epoch</span> <span class=\"o\">=</span> <span class=\"mi\">75</span>\n\n<span class=\"c1\"># define swa callback</span>\n<span class=\"n\">swa</span> <span class=\"o\">=</span> <span class=\"n\">SWA</span><span class=\"p\">(</span><span class=\"n\">start_epoch</span><span class=\"o\">=</span><span class=\"n\">start_epoch</span><span class=\"p\">,</span> \n          <span class=\"n\">lr_schedule</span><span class=\"o\">=</span><span class=\"s1\">'constant'</span><span class=\"p\">,</span> \n          <span class=\"n\">swa_lr</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> \n          <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># train</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">swa</span><span class=\"p\">])</span>\n</pre>\n<p>Or for Keras in Tensorflow (with Cyclic LR)</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets.samples_generator</span> <span class=\"kn\">import</span> <span class=\"n\">make_blobs</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.utils</span> <span class=\"kn\">import</span> <span class=\"n\">to_categorical</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">Sequential</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Dense</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.optimizers</span> <span class=\"kn\">import</span> <span class=\"n\">SGD</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">swa.tfkeras</span> <span class=\"kn\">import</span> <span class=\"n\">SWA</span>\n\n<span class=\"c1\"># make dataset</span>\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">make_blobs</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span> \n                  <span class=\"n\">centers</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> \n                  <span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> \n                  <span class=\"n\">cluster_std</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> \n                  <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">to_categorical</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># build model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">()</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"n\">input_dim</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'relu'</span><span class=\"p\">))</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'softmax'</span><span class=\"p\">))</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s1\">'categorical_crossentropy'</span><span class=\"p\">,</span> \n              <span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"n\">SGD</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">))</span>\n\n<span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n<span class=\"n\">start_epoch</span> <span class=\"o\">=</span> <span class=\"mi\">75</span>\n\n<span class=\"c1\"># define swa callback</span>\n<span class=\"n\">swa</span> <span class=\"o\">=</span> <span class=\"n\">SWA</span><span class=\"p\">(</span><span class=\"n\">start_epoch</span><span class=\"o\">=</span><span class=\"n\">start_epoch</span><span class=\"p\">,</span> \n          <span class=\"n\">lr_schedule</span><span class=\"o\">=</span><span class=\"s1\">'cyclic'</span><span class=\"p\">,</span> \n          <span class=\"n\">swa_lr</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">,</span>\n          <span class=\"n\">swa_lr2</span><span class=\"o\">=</span><span class=\"mf\">0.003</span><span class=\"p\">,</span>\n          <span class=\"n\">swa_freq</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span>\n          <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># train</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">swa</span><span class=\"p\">])</span>\n</pre>\n<p>Output</p>\n<pre><code>Model uses batch normalization. SWA will require last epoch to be a forward pass and will run with no learning rate\nEpoch 1/100\n1000/1000 [==============================] - 1s 547us/sample - loss: 0.5529\nEpoch 2/100\n1000/1000 [==============================] - 0s 160us/sample - loss: 0.4720\n...\nEpoch 74/100\n1000/1000 [==============================] - 0s 160us/sample - loss: 0.4249\n\nEpoch 00075: starting stochastic weight averaging\nEpoch 75/100\n1000/1000 [==============================] - 0s 164us/sample - loss: 0.4357\nEpoch 76/100\n1000/1000 [==============================] - 0s 165us/sample - loss: 0.4209\n...\nEpoch 99/100\n1000/1000 [==============================] - 0s 167us/sample - loss: 0.4263\n\nEpoch 00100: final model weights set to stochastic weight average\n\nEpoch 00100: reinitializing batch normalization layers\n\nEpoch 00100: running forward pass to adjust batch normalization\nEpoch 100/100\n1000/1000 [==============================] - 0s 166us/sample - loss: 0.4408\n</code></pre>\n<h3>Collaborators</h3>\n<ul>\n<li><a href=\"https://github.com/simon-larsson\" rel=\"nofollow\" title=\"Github\">Simon Larsson</a></li>\n<li><a href=\"https://github.com/alexstoken\" rel=\"nofollow\" title=\"Github\">Alex Stoken</a></li>\n</ul>\n\n          </div>"}, "last_serial": 6688950, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "bd376de5ca8a6eb2bb22b1917d68a004", "sha256": "96ae71e4266523d6e9d3bc22d2ae21e0259cc57835e96e8de687269deb58f66b"}, "downloads": -1, "filename": "keras-swa-0.0.1.tar.gz", "has_sig": false, "md5_digest": "bd376de5ca8a6eb2bb22b1917d68a004", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2763, "upload_time": "2019-10-02T12:33:23", "upload_time_iso_8601": "2019-10-02T12:33:23.722782Z", "url": "https://files.pythonhosted.org/packages/c1/9e/bda0c7d265358cae43d8e34abe3fdd280b6032e3ddf350de0b56aae61359/keras-swa-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "7a62b0769f811edabeaaf160ec5a9e7a", "sha256": "b61916e8f539e42da8512aef8e08c2efebac1cac30aa4cf80c04a7d144742b7e"}, "downloads": -1, "filename": "keras-swa-0.0.2.tar.gz", "has_sig": false, "md5_digest": "7a62b0769f811edabeaaf160ec5a9e7a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2941, "upload_time": "2019-10-03T12:04:37", "upload_time_iso_8601": "2019-10-03T12:04:37.173497Z", "url": "https://files.pythonhosted.org/packages/9c/f2/30d13ba8fd678fda68f1a97392e4edc42705e5867d4e92577c8b69f489f2/keras-swa-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "7372b5c3d75d717670d0b428c92ded91", "sha256": "b2ce77496bfcc98ffa97dbeeedd1f319effe8f9b5ff0d76fe9ec63f686699929"}, "downloads": -1, "filename": "keras-swa-0.0.3.tar.gz", "has_sig": false, "md5_digest": "7372b5c3d75d717670d0b428c92ded91", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3058, "upload_time": "2019-10-07T11:14:50", "upload_time_iso_8601": "2019-10-07T11:14:50.143256Z", "url": "https://files.pythonhosted.org/packages/aa/2f/bd653405f3fc2a209157e2d4c62328a8ed71278dce260e653a69019144a1/keras-swa-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "0165707d4ac6bf5dfc991b3d9a2c93cf", "sha256": "eff1cd79265961655f91bc48db373bda78e5ab551267770cf24db7531b61edc7"}, "downloads": -1, "filename": "keras-swa-0.0.4.tar.gz", "has_sig": false, "md5_digest": "0165707d4ac6bf5dfc991b3d9a2c93cf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3106, "upload_time": "2019-10-14T09:26:51", "upload_time_iso_8601": "2019-10-14T09:26:51.473589Z", "url": "https://files.pythonhosted.org/packages/34/64/1cd6879cb8d6ce597c7e54d1bd6a17c294ecacaaa45e37466962cf90ad59/keras-swa-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "f120833b361be07a45dae524a42edfae", "sha256": "6a1710d9c97f9d9735f75ea51aed94c6966caa93a8386e77f7c1d4dd7df953b2"}, "downloads": -1, "filename": "keras-swa-0.0.5.tar.gz", "has_sig": false, "md5_digest": "f120833b361be07a45dae524a42edfae", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3057, "upload_time": "2019-10-14T16:11:23", "upload_time_iso_8601": "2019-10-14T16:11:23.166780Z", "url": "https://files.pythonhosted.org/packages/79/57/43fcc5373473b582e5b40b54d34885372a1756dc672dbfade4f37ac043ac/keras-swa-0.0.5.tar.gz", "yanked": false}], "0.0.6": [{"comment_text": "", "digests": {"md5": "fabe0ad5b6f0a793815066b24d7941f3", "sha256": "b679df949073ca7e64c6cbea9660b92485a0b6e60b0262acf927116b297538b5"}, "downloads": -1, "filename": "keras-swa-0.0.6.tar.gz", "has_sig": false, "md5_digest": "fabe0ad5b6f0a793815066b24d7941f3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3098, "upload_time": "2019-10-18T18:32:55", "upload_time_iso_8601": "2019-10-18T18:32:55.752191Z", "url": "https://files.pythonhosted.org/packages/db/f4/2aabf7096c60c58f3ff6bb086ea359a474c6bd96c741dbaeb9866f477fcf/keras-swa-0.0.6.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "317ba548ee669118004c06d429e0e18b", "sha256": "25388dab93a12c4378bb313eb91218743cfe2a6b7c7f0dcdfc21739704de34fd"}, "downloads": -1, "filename": "keras-swa-0.1.0.tar.gz", "has_sig": false, "md5_digest": "317ba548ee669118004c06d429e0e18b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3164, "upload_time": "2019-10-18T18:36:15", "upload_time_iso_8601": "2019-10-18T18:36:15.306771Z", "url": "https://files.pythonhosted.org/packages/b9/c5/00b8e545f2b8cf79529acef3aa676e07528c7be03d39354aedf73eb249ba/keras-swa-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "1ffbe2494caa54e9975e700dbbc9d2a3", "sha256": "293d8573233f86b2ac7e25afe22f3005b1c02c34c5882511ceff874856a37cb4"}, "downloads": -1, "filename": "keras_swa-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "1ffbe2494caa54e9975e700dbbc9d2a3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8019, "upload_time": "2019-10-19T07:08:07", "upload_time_iso_8601": "2019-10-19T07:08:07.293348Z", "url": "https://files.pythonhosted.org/packages/9e/ac/1ffe8397b29bc4e6004677921fb14fb4c6b6a89b75caabd7a0a32cc2fb6a/keras_swa-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "65c6bb4e86b470c7f14d73ff14efb57b", "sha256": "062871e86885bf28c90869d5e2dbdee0d850029fb23fa832c81df3ea8da23f0f"}, "downloads": -1, "filename": "keras-swa-0.1.1.tar.gz", "has_sig": false, "md5_digest": "65c6bb4e86b470c7f14d73ff14efb57b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5121, "upload_time": "2019-10-19T07:08:08", "upload_time_iso_8601": "2019-10-19T07:08:08.710782Z", "url": "https://files.pythonhosted.org/packages/b5/8a/df128a0b7a14ce8414c499f1bfe0cb37d9d7d4f0987311e7d1f98ac6bcbe/keras-swa-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "12c420f16c502652c66dfc2a38041f33", "sha256": "75cb1bbee7adad2094bb70f72490066c938e2d0609f280b21f0eb6ce19a40084"}, "downloads": -1, "filename": "keras-swa-0.1.2.tar.gz", "has_sig": false, "md5_digest": "12c420f16c502652c66dfc2a38041f33", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5116, "upload_time": "2019-10-22T09:23:10", "upload_time_iso_8601": "2019-10-22T09:23:10.590780Z", "url": "https://files.pythonhosted.org/packages/2d/e0/56befd04516d8b2a690ff4b1aad1ddba95166b66f4d4ca0c47c48787828a/keras-swa-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "0a18fed65b78f867482d85b7437968fb", "sha256": "cf84a30744b0a26a31526321bc0a28fd5031728bcb14c4eb77e6868a0a05eee6"}, "downloads": -1, "filename": "keras-swa-0.1.3.tar.gz", "has_sig": false, "md5_digest": "0a18fed65b78f867482d85b7437968fb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5519, "upload_time": "2019-12-14T05:50:47", "upload_time_iso_8601": "2019-12-14T05:50:47.798783Z", "url": "https://files.pythonhosted.org/packages/ba/45/cc2f034abbeddc0259733fecfcaf7bedc6f38e967d7b20617f04d3e77f75/keras-swa-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "c692c8dc5a55b17e99e2395b2cda63b6", "sha256": "048d06f13719dcfd6432b6adb930affc166402d25708ac7d157f1ff34e9a3fa7"}, "downloads": -1, "filename": "keras-swa-0.1.4.tar.gz", "has_sig": false, "md5_digest": "c692c8dc5a55b17e99e2395b2cda63b6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6114, "upload_time": "2020-01-16T13:08:33", "upload_time_iso_8601": "2020-01-16T13:08:33.212258Z", "url": "https://files.pythonhosted.org/packages/5f/08/1ed4384b15b291499bb33b27d18bf67cf8323bf7f445338c46ab78f5252f/keras-swa-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "68bab397361914c4f9c29bbfb296792a", "sha256": "15d0f85fa45c70845f842874a84ce15b5c010f6f09626eb8c9ff2eb263c3fe53"}, "downloads": -1, "filename": "keras-swa-0.1.5.tar.gz", "has_sig": false, "md5_digest": "68bab397361914c4f9c29bbfb296792a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 77355, "upload_time": "2020-02-24T14:38:43", "upload_time_iso_8601": "2020-02-24T14:38:43.257794Z", "url": "https://files.pythonhosted.org/packages/91/40/2796068a5d40c9b7f8d2b0d5a830a45c3ff9909194931f50a87ec19f45d5/keras-swa-0.1.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "68bab397361914c4f9c29bbfb296792a", "sha256": "15d0f85fa45c70845f842874a84ce15b5c010f6f09626eb8c9ff2eb263c3fe53"}, "downloads": -1, "filename": "keras-swa-0.1.5.tar.gz", "has_sig": false, "md5_digest": "68bab397361914c4f9c29bbfb296792a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 77355, "upload_time": "2020-02-24T14:38:43", "upload_time_iso_8601": "2020-02-24T14:38:43.257794Z", "url": "https://files.pythonhosted.org/packages/91/40/2796068a5d40c9b7f8d2b0d5a830a45c3ff9909194931f50a87ec19f45d5/keras-swa-0.1.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:09 2020"}