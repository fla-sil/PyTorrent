{"info": {"author": "Vsevolod Dyomkin, Dmitry Chaplinsky", "author_email": "chaplinsky.dmitry@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: Implementation :: PyPy"], "description": "===============================\nTokenize UK\n===============================\n\n.. image:: https://img.shields.io/pypi/v/tokenize_uk.svg\n        :target: https://pypi.python.org/pypi/tokenize_uk\n\n.. image:: https://img.shields.io/travis/lang-uk/tokenize-uk.svg\n        :target: https://travis-ci.org/lang-uk/tokenize-uk\n\n.. image:: http://readthedocs.org/projects/tokenize-uk/badge/?version=latest\n\t\t:target: http://tokenize-uk.readthedocs.io/en/latest/?badge=latest\n\t\t:alt: Documentation Status\n\n\nSimple python lib to tokenize texts into sentences and sentences to words. Small, fast and robust. Comes with ukrainian flavour \n\n* Free software: MIT license\n* Documentation: https://tokenize_uk.readthedocs.org.\n\nFeatures\n--------\n\n* Tokenize given text into sentences\n* Tokenize given sentence into words\n* Works well with accented characters (like stresses) and apostrophes\n* Suitable also for other languages\n\n\n=======\nHistory\n=======\n\n0.1.0 (2016-05-29)\n------------------\n\n* First release on PyPI.", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/lang-uk/tokenize-uk", "keywords": "tokenize_uk", "license": "MIT", "maintainer": null, "maintainer_email": null, "name": "tokenize_uk", "package_url": "https://pypi.org/project/tokenize_uk/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/tokenize_uk/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/lang-uk/tokenize-uk"}, "release_url": "https://pypi.org/project/tokenize_uk/0.2.0/", "requires_dist": null, "requires_python": null, "summary": "Simple python lib to tokenize texts into sentences and sentences to words. Small, fast and robust. Comes with ukrainian flavour", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"tokenize-uk\">\n<h2>Tokenize UK</h2>\n<a href=\"https://pypi.python.org/pypi/tokenize_uk\" rel=\"nofollow\"><img alt=\"https://img.shields.io/pypi/v/tokenize_uk.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fb7fa3835412d9f6a62db4cddf203fd54b0735f5/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f746f6b656e697a655f756b2e737667\"></a>\n<a href=\"https://travis-ci.org/lang-uk/tokenize-uk\" rel=\"nofollow\"><img alt=\"https://img.shields.io/travis/lang-uk/tokenize-uk.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/59fc36da6ec3330bba583028ea3a726ea785a596/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f6c616e672d756b2f746f6b656e697a652d756b2e737667\"></a>\n<a href=\"http://tokenize-uk.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5db3c03c96ed390e620e6feade4267d0f28186e6/687474703a2f2f72656164746865646f63732e6f72672f70726f6a656374732f746f6b656e697a652d756b2f62616467652f3f76657273696f6e3d6c6174657374\"></a>\n<p>Simple python lib to tokenize texts into sentences and sentences to words. Small, fast and robust. Comes with ukrainian flavour</p>\n<ul>\n<li>Free software: MIT license</li>\n<li>Documentation: <a href=\"https://tokenize_uk.readthedocs.org\" rel=\"nofollow\">https://tokenize_uk.readthedocs.org</a>.</li>\n</ul>\n<div id=\"features\">\n<h3>Features</h3>\n<ul>\n<li>Tokenize given text into sentences</li>\n<li>Tokenize given sentence into words</li>\n<li>Works well with accented characters (like stresses) and apostrophes</li>\n<li>Suitable also for other languages</li>\n</ul>\n</div>\n</div>\n<div id=\"history\">\n<h2>History</h2>\n<h2 id=\"id1\"><span class=\"section-subtitle\">0.1.0 (2016-05-29)</span></h2>\n<ul>\n<li>First release on PyPI.</li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 2247286, "releases": {"0.1.1": [], "0.1.2": [{"comment_text": "", "digests": {"md5": "de0ce00a5fae42350d00ba40c9d3586e", "sha256": "3daba716f78d5633363d4ff01e86e33a6f7c453ab6fd1ffd832a68719699e2a3"}, "downloads": -1, "filename": "tokenize_uk-0.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "de0ce00a5fae42350d00ba40c9d3586e", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4813, "upload_time": "2016-05-29T23:07:13", "upload_time_iso_8601": "2016-05-29T23:07:13.411500Z", "url": "https://files.pythonhosted.org/packages/03/0e/cb3453fe10400be5a69b30410ab0fbc4746c5d8fca4ade10c02df670d7cc/tokenize_uk-0.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0a16f0af14b254489bd1c44f0acb609b", "sha256": "e68a7867ad52b9b3f52b3bdc6b23307fbc3366e011bc9633ef28fc27e3fed41d"}, "downloads": -1, "filename": "tokenize_uk-0.1.2.tar.gz", "has_sig": false, "md5_digest": "0a16f0af14b254489bd1c44f0acb609b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14856, "upload_time": "2016-05-29T23:07:17", "upload_time_iso_8601": "2016-05-29T23:07:17.568249Z", "url": "https://files.pythonhosted.org/packages/51/32/8c1f93310fba9ab252b8f5e3ee0ac84f29b3a85fa805553cc481c239a183/tokenize_uk-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "81bed6d28a9d287b0d197333dbe1a4fc", "sha256": "9db3d3f26dafc48b124775cd6cd56335705e3dbc8fb8d7e338be5b1aecaabf84"}, "downloads": -1, "filename": "tokenize_uk-0.1.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "81bed6d28a9d287b0d197333dbe1a4fc", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4830, "upload_time": "2016-05-29T23:17:32", "upload_time_iso_8601": "2016-05-29T23:17:32.216802Z", "url": "https://files.pythonhosted.org/packages/76/fa/b5d140577e79273dea0a658c027ea99b73b856fd970c6ff76a2614465a4c/tokenize_uk-0.1.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6ab3bae25c7f07a2d6fee7eb8e1ec710", "sha256": "377daba8e2e3d0264ac7ef17e7dc4a9dd9ff35da63da69bf7eda84a15f19099f"}, "downloads": -1, "filename": "tokenize_uk-0.1.3.tar.gz", "has_sig": false, "md5_digest": "6ab3bae25c7f07a2d6fee7eb8e1ec710", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14867, "upload_time": "2016-05-29T23:17:36", "upload_time_iso_8601": "2016-05-29T23:17:36.463970Z", "url": "https://files.pythonhosted.org/packages/e6/16/c9df5c4829ad6e2a1f687d9ab1de8be5e2fda9cefee43748a1d72c54767e/tokenize_uk-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "757495bca7b06d9119911fd9bf098e00", "sha256": "6c6469323b7d3fbd5b36cb360f7ac2bba906eafc44e95de80f50cd08885c52d4"}, "downloads": -1, "filename": "tokenize_uk-0.1.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "757495bca7b06d9119911fd9bf098e00", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4831, "upload_time": "2016-05-29T23:25:22", "upload_time_iso_8601": "2016-05-29T23:25:22.832499Z", "url": "https://files.pythonhosted.org/packages/0f/cc/437995cd7fdc148be2c521c707f9e71cf347c7a7020235ed636d4565a588/tokenize_uk-0.1.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d597246ca31085479e3fa44e4a21bce8", "sha256": "8e4061e1bc523b9baec7e51f8c38dfae2e6889eb5dd347f54ba348263c19720b"}, "downloads": -1, "filename": "tokenize_uk-0.1.4.tar.gz", "has_sig": false, "md5_digest": "d597246ca31085479e3fa44e4a21bce8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14869, "upload_time": "2016-05-29T23:25:26", "upload_time_iso_8601": "2016-05-29T23:25:26.649407Z", "url": "https://files.pythonhosted.org/packages/22/33/c69e640cdb599666bb1d3940a89243c8475c4ce1f022372a29a1b28e9f80/tokenize_uk-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "e95893266cbda55083d2ac95a848f7a4", "sha256": "fc77b861470240ce8047228e8fcf0262bd4621b7cfb540c558b2243d47a8a0d7"}, "downloads": -1, "filename": "tokenize_uk-0.1.5-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "e95893266cbda55083d2ac95a848f7a4", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 4827, "upload_time": "2016-05-30T13:22:42", "upload_time_iso_8601": "2016-05-30T13:22:42.133589Z", "url": "https://files.pythonhosted.org/packages/ac/9f/6496ac9e3db47ba8fdb9a24cead34cc03b415f2188e0d6f9e1790ea19220/tokenize_uk-0.1.5-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "97095288e723be1d3bf5ba2cf9845502", "sha256": "bdbc47b5b863ef70f94894cf2806b26d3cad0d91f10410e529db99138b44ed18"}, "downloads": -1, "filename": "tokenize_uk-0.1.5.tar.gz", "has_sig": false, "md5_digest": "97095288e723be1d3bf5ba2cf9845502", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14869, "upload_time": "2016-05-30T13:22:47", "upload_time_iso_8601": "2016-05-30T13:22:47.337232Z", "url": "https://files.pythonhosted.org/packages/da/85/542e961e7a9ed39a302aaddb54c86db42beda66dff19a4e367461fadf82b/tokenize_uk-0.1.5.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "1c1c0f2b33fb272c433a11419dcf5d4b", "sha256": "be9c043a00d43d2a6fd36fbd57c67b4a1f321fa778db107e9fa92a2eddb7c1fc"}, "downloads": -1, "filename": "tokenize_uk-0.2.0.tar.gz", "has_sig": false, "md5_digest": "1c1c0f2b33fb272c433a11419dcf5d4b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22883, "upload_time": "2016-07-27T17:08:21", "upload_time_iso_8601": "2016-07-27T17:08:21.355157Z", "url": "https://files.pythonhosted.org/packages/ac/21/72abb0304b532e1b2d2473b50d8063ddd0943e3b3fe7e86b366bc4d02aa2/tokenize_uk-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1c1c0f2b33fb272c433a11419dcf5d4b", "sha256": "be9c043a00d43d2a6fd36fbd57c67b4a1f321fa778db107e9fa92a2eddb7c1fc"}, "downloads": -1, "filename": "tokenize_uk-0.2.0.tar.gz", "has_sig": false, "md5_digest": "1c1c0f2b33fb272c433a11419dcf5d4b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22883, "upload_time": "2016-07-27T17:08:21", "upload_time_iso_8601": "2016-07-27T17:08:21.355157Z", "url": "https://files.pythonhosted.org/packages/ac/21/72abb0304b532e1b2d2473b50d8063ddd0943e3b3fe7e86b366bc4d02aa2/tokenize_uk-0.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:51:24 2020"}