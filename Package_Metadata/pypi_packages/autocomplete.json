{"info": {"author": "Rodrigo Palacios", "author_email": "rodrigopala91@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "*Autocomplete* or: How I learned to stop spelling and love our AI overlords\r\n===========================================================================\r\n\r\nA practical guide to implementing \"autocomplete\"! It follows the\r\nsometimes misunderstood principles of conditional probability\r\ndistributions and the generalized Hidden Markov Model (HMM).\r\n\r\nFun fact: Your iPhone's \"autocomplete\" was implemented using a HMM! Plus\r\nthe extra stuff it chose to `sue Samsung\r\nfor <http://www.cnet.com/news/us-patent-office-rejects-apple-autocomplete-patent-used-against-samsung/>`__.\r\n\r\nSkip to:\r\n--------\r\n\r\n-  `How to's <#how-to-install>`__\r\n-  `tl;dr? <#tldr>`__\r\n-  `Motivation <#motivation>`__\r\n-  `ELI5 <#explain-like-im-5>`__\r\n-  `If you're not 5 <#if-youre-not-5>`__\r\n\r\n--------------\r\n\r\nHow to install:\r\n---------------\r\n\r\n::\r\n\r\n    pip install autocomplete\r\n\r\nHow to use:\r\n-----------\r\n\r\n.. code:: python\r\n\r\n    import autocomplete\r\n\r\n    # load pickled python Counter objects representing our predictive models\r\n    # I use Peter Norvigs big.txt (http://norvig.com/big.txt) to create the predictive models\r\n    autocomplete.load()\r\n\r\n    # imagine writing \"the b\"\r\n    autocomplete.predict('the','b')\r\n\r\n    [('blood', 204),\r\n     ('battle', 185),\r\n     ('bone', 175),\r\n     ('best', 149),\r\n     ('body', 149),\r\n     ...]\r\n\r\n    # now you type an \"o\"\r\n\r\n    autocomplete.predict('the','bo')\r\n\r\n    [('bone', 175),\r\n     ('body', 149),\r\n     ('bones', 122),\r\n     ('boy', 46),\r\n     ('bottom', 32),\r\n     ('box', 24),\r\n     ...]\r\n\r\nIf you have your own language model in the form described in\r\n`ELI5 <#explain-like-im-5>`__, then use the *models* submodule to call\r\nthe training method:\r\n\r\n.. code:: python\r\n\r\n    from autocomplete import models\r\n\r\n    models.train_models('some giant string of text')\r\n\r\nWant to run it as a server (bottlepy required)?\r\n\r\n.. code:: python\r\n\r\n    import autocomplete\r\n\r\n    autocomplete.run_server()\r\n\r\n    #output\r\n    Bottle v0.12.8 server starting up (using WSGIRefServer())...\r\n    Listening on http://localhost:8080/\r\n    Hit Ctrl-C to quit.\r\n\r\nNow head over to http://localhost:8080/the/bo\r\n\r\n::\r\n\r\n    http://localhost:8080/the/bo\r\n    #output\r\n    {\"body\": 149, \"box\": 24, \"bottom\": 32, \"boy\": 46, \"borzois\": 16, \"bodies\": 13, \"bottle\": 13, \"bones\": 122, \"book\": 14, \"bone\": 175}\r\n\r\n    http://localhost:8080/the/bos\r\n    #output\r\n    {\"boscombe\": 11, \"boston\": 7, \"boss\": 1, \"bosom\": 5, \"bosses\": 4}\r\n\r\nObligatory tests\r\n~~~~~~~~~~~~~~~~\r\n\r\n::\r\n\r\n    python setup.py test\r\n\r\n--------------\r\n\r\n`tl;dr <https://github.com/rodricios/autocomplete/blob/master/autocomplete/models.py>`__\r\n----------------------------------------------------------------------------------------\r\n\r\nThe following code excerpt is my interpretation of a series of\r\nlessons/concepts expressed in a number of different books.\r\n\r\nThe unifying concept can be said to be `conditional\r\nprobability <http://en.wikipedia.org/wiki/Conditional_probability>`__:\r\n\r\n::\r\n\r\n    P(A , B) = P(B | A) * P(A)\r\n\r\nWhich can read as saying:\r\n\r\n::\r\n\r\n    The probability of A and B occuring is equal to the probability of B occuring, given that A has occured\r\n\r\nMore on this below.\r\n\r\n.. code:: python\r\n\r\n\r\n        # \"preperation\" step\r\n        # for every word in corpus, normalize ('The' -> 'the'), insert to list\r\n        WORDS = helpers.re_split(corpus)\r\n\r\n        # first model -> P(word)\r\n        # Counter constructor will take a list of elements and create a frequency distribution (histogram)\r\n        WORDS_MODEL = collections.Counter(WORDS)\r\n\r\n        # another preperation step\r\n        # [a,b,c,d] -> [[a,b], [c,d]]\r\n        WORD_TUPLES = list(helpers.chunks(WORDS, 2))\r\n\r\n        # second model -> P(next word | prev. word)\r\n        # I interpret \"..| prev. word)\" as saying \"dictionary key\r\n        # leading to seperate and smaller (than WORDS_MODEL) freq. dist.\r\n        WORD_TUPLES_MODEL = {first:collections.Counter() for first, second in WORD_TUPLES}\r\n\r\n        for prev_word, next_word in WORD_TUPLES:\r\n            # this is called the \"conditioning\" step where we assert\r\n            # that the probability space of all possible \"next_word\"'s\r\n            # is \"conditioned\" under the event that \"prev_word\" has occurred\r\n            WORD_TUPLES_MODEL[prev_word].update([next_word])\r\n\r\nTextbooks, and locations therein, where the concept-in-practice has been\r\nexpressed:\r\n\r\nI. `Intro to Statistical Natural Language\r\nProcessing <http://ics.upjs.sk/~pero/web/documents/pillar/Manning_Schuetze_StatisticalNLP.pdf>`__\r\n- Manning, Sch\u00fctze, 1999\r\n\r\n::\r\n\r\n    a. frequency distribution showing the most common words and frequencies in *Tom Sawyer*, pg. 21\r\n\r\n    b. conditional probability definition expressed in page 42 - section 2.1.2\r\n\r\n    c. the intuition for *frequency* distributions found in pg. 153 (provided in the context of finding [*Collocations*](http://en.wikipedia.org/wiki/Collocation))\r\n\r\nII.  `Probabilistic Graphical\r\n     Models <http://mitpress.mit.edu/books/probabilistic-graphical-models>`__\r\n     - Kohler, Friedman, 2009\r\n\r\n     a. conditional probability definition found on pg. 18 (hilariously\r\n        and coincidentally found in section 2.1.2.1)\r\n\r\nIII. `Artificial Intelligence - A Modern\r\n     Approach <http://aima.cs.berkeley.edu>`__ - Russell, Norvig, 3rd.\r\n     ed. 2010\r\n\r\n     a. conditional probability concept explained in pg. 485\r\n\r\n     b. the \"language\" (I take to mean \"intuition\" for asserting things\r\n        in the probabilistic sense) pg. 486\r\n\r\n     c. the notion of \"conditioning\" found in pg. 492-494\r\n\r\nMotivation\r\n----------\r\n\r\nSimilar to the motivation behind\r\n`eatiht <https://github.com/rodricios/eatiht#motivation>`__, I found\r\nthat it took far too long to find a palpable theory-to-application\r\nexample of what amounts to more than a 500 pages of words across 3\r\nbooks, each spanning a large index of, in certain cases,\r\n*counter-intuitive* nomenclature; read the `light\r\ncriticisms <http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/ckep3z6>`__\r\nmade by Michael I. Jordan on the matter (he was recently named `#2\r\nmachine learning expert \"we need to know\" on\r\ndataconomy.com <http://dataconomy.com/10-machine-learning-experts-you-need-to-know/>`__).\r\n\r\nYou can find similar thoughts being expressed `**in an article from 2008\r\n(updated\r\n2009)** <http://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/>`__\r\nby `Brennan O'Connor <http://brenocon.com>`__\r\n\r\n--------------\r\n\r\n`*This work is dedicated to my siblings* <#note-1>`__.\r\n\r\nExplain like I'm 5\\ `\\* <#note-1>`__\r\n------------------------------------\r\n\r\n\\*Warning! This explanation is literally intended for young kids - I'm\r\nactually trying to see if these concepts can be explained to an audience\r\nunaware of the nomenclature used within the statistical\r\n`nlp <http://en.wikipedia.org/wiki/Natural_language_processing>`__ and\r\nother machine learning fields. For example, my 7, 9, 11, 14 y.o.\r\nsiblings, and basically anyone else who's ever read a story to a child -\r\nthey would be a part of the target audience.\r\n\r\nIf you've found this readable and informative, please consider putting\r\non the goofiest face and reading this to your kids, if you have any :)\r\nIf you do, please send me your thoughts on the experience.\r\n\r\nI'm only interested in lowering the barrier to entry. I should have\r\nincluded this note since the beginning (sorry to those who undoubtedly\r\nleft with a bad taste in their mouths).\r\n\r\nYou can contact me at rodrigopala91@gmail.com\r\n\r\nThanks for reading,\r\n\r\nRodrigo\r\n\r\nELI5\r\n----\r\n\r\nNo. I'm explaining this like you're 5. I know you're not *5* , *you\r\nguys... Chris, stop jumping on your sister's back*!\r\n\r\nOk, so I'm saying, *imagine I'm 5!*\r\n\r\nOh, that was easy now huh? Let's just forget the *I'm 5* part.\r\n\r\nImagine a giant collection of books.\r\n\r\nFor example, all the Harry Potter and Hunger Games novels put together.\r\n\r\nWhat if I asked you to go through all the pages and all the words in\r\nthose pages?\r\n\r\nNow I'm not asking you *four* to actually *read* the books. You know,\r\njust go through, beginning to end, and notice each word.\r\n\r\nFor every new word you see, write it down, and put a \"1\" next to it, and\r\neverytime you see a word *again*, add \"1\" more to the previous number.\r\n\r\nSo basically I'm asking y'all to keep count of how many times a word\r\ncomes up.\r\n\r\nGot it? If yes, cool! If not, find a sibling, friend, or adult near you\r\nand ask them to help you out :)\r\n\r\n...\r\n\r\nSay you start with *Harry Potter and the Sorcerer's Stone*:\r\n\r\n::\r\n\r\n    Mr. and Mrs. Dursley of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much...\r\n\r\nAnd imagine that you're on the 5th word. This or something close to this\r\nis what you're going for:\r\n\r\n::\r\n\r\n    Mr.     -> 1\r\n    and     -> 1\r\n    Mrs.    -> 1\r\n    Dursley -> 1\r\n    of      -> 1\r\n\r\nOr if you're a *wannabe-Harry-Potter* fan, ah I'm just kidding!\r\n\r\nIf you started with *the-book-that-must-not-be-named* - I know you guys\r\nwon't get it, but persons my age will :)\r\n\r\nAlright! So you started with *The Hunger Games*:\r\n\r\n::\r\n\r\n    When I wake up, the other side of the bed is cold...\r\n\r\nBy the sixth word you have:\r\n\r\n::\r\n\r\n    When  -> 1\r\n    I     -> 1\r\n    wake  -> 1\r\n    up    -> 1\r\n    the   -> 1\r\n\r\nYou have a long day ahead of you...\r\n\r\n...\r\n\r\n*1,105,285 words later*\r\n\r\nNow that you're done tallying up all those words, why not order all\r\nthese words by the *number of times you've seen them*?\r\n\r\nSee you next week!\r\n\r\n...\r\n\r\nBack so soon? You should have gotten something like this:\r\n\r\n::\r\n\r\n    psst*, remember, the format is:\r\n     word -> # of times the word appears\r\n\r\n    'the' -> 80030\r\n    'of'  -> 40025\r\n    'and' -> 38313\r\n    'to'  -> 28766\r\n    'in'  -> 22050\r\n    'a'   -> 21155\r\n    'that'-> 12512\r\n    'he'  -> 12401\r\n    'was' -> 11410\r\n    'it'  -> 10681\r\n    ... there's a lot more words you've tallied up...\r\n\r\nThose were the most common words.\r\n\r\nNow on the *less-frequent* end, you'll find your words appearing not as\r\noften...\r\n\r\n::\r\n\r\n    ... 29137 words later.\r\n    'przazdziecka' -> 1\r\n    'disclosure'   -> 1\r\n    'galvanism'    -> 1\r\n    'repertoire'   -> 1\r\n    'bravado'      -> 1\r\n    'gal'          -> 1\r\n    'ideological'  -> 1\r\n    'guaiacol'     -> 1\r\n    'expands'      -> 1\r\n    'revolvers'    -> 1\r\n\r\nYeah Chris? Oh, 'what does *lez freekend*' mean? Um, so it means\r\nsomething like: *you probably won't hear or read that word very often.*\r\n\r\nNow what if I asked you to help me find this word I'm looking for? And I\r\nknow this word starts with the letters: 'th'.\r\n\r\nI'm pretty sure you guys can do this much faster!\r\n\r\n...\r\n\r\n*5 minutes later!*\r\n\r\n...\r\n\r\nNot bad! You only had to go through 29157 unique words after all!\r\n\r\n::\r\n\r\n    'the'  -> 80030\r\n    'that' -> 12512\r\n    'this' -> 4063\r\n    'they' -> 3938\r\n    'there'-> 2972\r\n    'their'-> 2955\r\n    'them' -> 2241\r\n    'then' -> 1558\r\n    'these'-> 1231\r\n    'than' -> 1206\r\n    ... 229 words more...\r\n\r\n239 words, still kind of lot though huh? And you know your big brother,\r\nhe's too lazy to do this work *by hand* (*cough* program it up *cough*)\r\n;)\r\n\r\nSo the word I'm looking for is on the tip of my tongue. I think the next\r\nletter is \"i\".\r\n\r\n*1 minute later*\r\n\r\n::\r\n\r\n    'this'     -> 4063\r\n    'think'    -> 557\r\n    'things'   -> 321\r\n    'thing'    -> 303\r\n    'third'    -> 239\r\n    'thin'     -> 166\r\n    'thinking' -> 137\r\n    'thirty'   -> 123\r\n    'thick'    -> 77\r\n    'thirds'   -> 43\r\n    ... 36 words more...\r\n\r\n*I scan through the first 10 words.* Oh, I just remembered that the next\r\nletter is 'r'.\r\n\r\n*You start taking out even more words.*\r\n\r\n*10 seconds later.*\r\n\r\n::\r\n\r\n    'third'      -> 239\r\n    'thirty'     -> 123\r\n    'thirds'     -> 43\r\n    'thirteen'   -> 32\r\n    'thirst'     -> 13\r\n    'thirteenth' -> 11\r\n    'thirdly'    -> 8\r\n    'thirsty'    -> 5\r\n    'thirtieth'  -> 3\r\n    'thirties'   -> 2\r\n\r\nAha, 'thirdly' was the word I was looking for! What, you never heard of\r\nthe word \"thirdly\" before?\r\n\r\nNow you might be saying to yourself, \"*that's pretty cool!*\\ \", and\r\nyou're right!\r\n\r\nAnd you know what's cooler? *Making everyone's life a tiny bit easier*\r\nis! :)\r\n\r\nBut how can you do that with just *words*?\r\n\r\nAren't words boring and dull?\r\n\r\nIt's like all we do is talk, write, and think with *words*. I mean, how\r\nlame, I can't even describe to you this *autocomplete*\r\nthing-slash-idea-thing without having to write it out with *words*!\r\n\r\nUgh! I hate words!\r\n\r\n*Whoah, wait a minute! That was not cool of me! Let's relax for a\r\nminute.*\r\n\r\nLet's try to give an imaginary hug to the word-factory in our brains.\r\nThat part of our brain works so hard, even when we don't ask it to. How\r\nnice of our brain to do that. Not!\r\n\r\nWhat I'm trying to is sometimes it's not so nice for our brains to\r\ndistract us, especially when we have homework or other, real-world,\r\nproblems like adult-homework.\r\n\r\nSo how about this: let's try to think about *what* the next sentence\r\ncoming out of our own mouths *will be*\\ `\\* <#note-2>`__.\r\n\r\nNow if you're thinking about what will be coming out of my mouth, or out\r\nof your mouth, or your mouth, or your mouth, or your mouth, you're doing\r\nit wrong! (to readers who aren't one of my 4 younger siblings, that's\r\nhow many I have).\r\n\r\nTry your best to think about *what* the next sentence coming out of\r\n*your own* mouth will be.\r\n\r\n...\r\n\r\nDid you decide on your sentence? Good!\r\n\r\nNow what if I asked you to give me two reasons explaining *why* and\r\n*how* you chose the sentence you chose?\r\n\r\nWait, I can't even do that! Let's make it easier on ourselves and\r\nexplain *why* and *how* we chose the first *word*.\r\n\r\nStill pretty hard huh?\r\n\r\nIf you thought about it, and you thought it was pretty darn hard to give\r\na *good and honest* reason as to why it is you chose the word you chose,\r\nlet's bring out a word you guys might not understand: *probability*.\r\n\r\nIf you feel like you don't *get* what the word means, sure you do! Just\r\nuse the word \"probably\" in one of your sentences, but but try to makes\r\nsome sense.\r\n\r\nWhat do I mean? Well, let's just consider the English language. Like\r\nmost other things, the English language has rules.\r\n\r\nThe kind of rules that can be simplified down to:\r\n\r\n1) \"***something*** *action* ***something***\".\r\n\r\n2) Replace ***something***'s and ***action*** with words that make sense\r\n   to you.\r\n\r\nFair enough, right?\r\n\r\nNow, imagine you could put \"pause\" right after the first word that comes\r\nout of your mouth.\r\n\r\nLet's just say that first word is \"the\".\r\n\r\nNow in the case that you stuttered for reasons outside your\r\nconscientious control (for example: \"thhh thhe the\"). No big deal, you\r\nmeant to say \"the\", so let's *flatten* it to just that!\r\n\r\nWith that *word* said, what words do you *think* you might have said\r\nafter it?\r\n\r\nYou might tell me, \"*any word I want!*\r\n\r\nOf course you could have! I bet you spent a millisecond thinking about\r\nwhether or not the next word you were going to say was going to be:\r\n*guaiacol*.\r\n\r\nI *know* because I thought about using that word too!\r\n\r\nI can remember the first time I heard (or read) *guaiacol* like it was\r\nyesterday. I read it in some funky article on the internet. I found the\r\nword in a list of words that don't appear too often in the English\r\nlanguage.\r\n\r\nAfter I read it, I was able to fit *guaiacol* nicely into that part of\r\nmy brain where I... uhh.. was... able... uhh...\r\n\r\nOh, you *know*, that place in my brain where I get to choose whether I\r\nwant to say *the apple*, *the automobile*, *the austronaut*, etc.\r\n\r\n...\r\n\r\nOk, so clearly I'm no brainician, and that may or may not be the way our\r\nbrain works - actually, it's probably super super unlikely.\r\n\r\nBut even though that idea is probably wrong, the idea itself sounds like\r\na pretty darn good way of suggesting the next word or words somebody is\r\ntrying to *type*.\r\n\r\nWhat if you had a way to count the number of times you've heard \"apple\"\r\nsaid after the word \"the\"?\r\n\r\nAsk yourself the same question, but now with the word \"automobile\"\r\ninstead of \"apple\".\r\n\r\nWhat if you had the time to think about every possible word that you've\r\never heard spoken after the word \"the\"? I'd say it might have looked\r\nsomething like this:\r\n\r\n::\r\n\r\n    Words you might have heard following the word \"the\" and the number of times you might have heard it\r\n\r\n    'same'     -> 996\r\n    'french'   -> 688\r\n    'first'    -> 652\r\n    'old'      -> 591\r\n    'emperor'  -> 581\r\n    'other'    -> 528\r\n    'whole'    -> 500\r\n    'united'   -> 466\r\n    'room'     -> 376\r\n    'most'     -> 373\r\n\r\n    ... 9331 more words...\r\n\r\nNot impressed with your brain yet? Let's continue this little thought\r\nexperiment further.\r\n\r\nImagine that you just said \"the\", and you could put pause after the\r\nfirst *letter* of the next word out of your mouth: \"h\".\r\n\r\nReal quick, think of the shortest amount of time you can think of. Think\r\nof the shortest *second* you can think of. Now shorter than that too.\r\n\r\nAt this point, you can't even call that length of time a *second*. But\r\nin that length of time, your brain may have just done this:\r\n\r\n::\r\n\r\n    Every word you've ever heard coming after the word \"the\":\r\n\r\n    'house'   -> 284\r\n    'head'    -> 117\r\n    'hands'   -> 101\r\n    'hand'    -> 97\r\n    'horses'  -> 71\r\n    'hill'    -> 64\r\n    'highest' -> 64\r\n    'high'    -> 57\r\n    'history' -> 56\r\n    'heart'   -> 55\r\n\r\nAnd that brain you got did this realllllyyyyyy fast. Faster than Google,\r\nBing, Yahoo and any other company can ever hope to beat. And your brain\r\ndid this without even asking for your permission. I think our brains are\r\ntrying to control us you guys, oh no!\r\n\r\nIf you're not 5\r\n---------------\r\n\r\nThe basic idea is this:\r\n\r\nAssume you have a large collection of Enlish-understandable text merged\r\ninto a single string.\r\n\r\nStart by transforming that string into a list of words (AKA *ngrams of\r\nword-legth*), and also (but not required) normalize each word ('The' ->\r\n'the').\r\n\r\nOnce you have a normalized list of words, you can start building a\r\nfrequency distribution measuring the frequency of each word.\r\n\r\n...\r\n\r\nAt this point you can start \"predict\" the \"final state\" of a\r\nword-in-progress. But consider the case where a user types in some query\r\nbox:\r\n\r\n::\r\n\r\n    \"The th\"\r\n\r\nAnd he intends to write:\r\n\r\n::\r\n\r\n    \"The third\"\r\n\r\nWith the above predictive model, you'll be suggesting something like:\r\n\r\n::\r\n\r\n    [\r\n        ('the', 80030),\r\n        ('they', 3938),\r\n        ('there', 2972),\r\n        ...\r\n    ]\r\n\r\nThis explains one specific type of predictive model, which can be\r\nwritten as P(word), and you've just seen the pitfalls of using **just**\r\nthis model.\r\n\r\nNow for the next word, ask yourself, what's the probability that I'm\r\ngoing to type the word \"apple\" given that I wrote \"tasty\"?\r\n\r\nIn machine learning and AI books, you'll be presented *Conditional\r\nProbability* with the following equation:\r\n\r\n::\r\n\r\n    P(word A and word B) = P(word B | word A) * P(word A)\r\n\r\nThat equation addresses the problem that I mentioned.\r\n\r\nWe've handled P(wordA) already.\r\n\r\nTo handle P(word B \\| word A), which reads *probability of word A given\r\nword B *, I take a *literall* interpretation of the word \"given\", in\r\nthat context, to mean the following:\r\n\r\n*\"word A\" is the key pointing to a probability distribution representing\r\nall the words that follow \"word A\"*\r\n\r\nOnce we can represent this second model, we can also apply the\r\n*filtering* step - given that we know more letters in the second word,\r\nwe can zone in on more precise suggestions.\r\n\r\n--------------\r\n\r\nAfterword\r\n~~~~~~~~~\r\n\r\nnotes: \\*I have to give a shout out to `Sam\r\nHarris <https://twitter.com/SamHarrisOrg>`__ for being, AFAIK, the first\r\nperson or one of the firsts, in `wonderfully putting into\r\nwords <https://www.youtube.com/watch?v=pCofmZlC72g#t=1144>`__ what I've\r\nborrowed and slightly adapted for this writing. `I highly recommend his\r\nwork <http://www.samharris.org/>`__\r\n\r\nAnother shoutout to `Peter Norvig <http://norvig.com>`__ for inspiring\r\nme and probably many others with our own little \"toy\" programs. His\r\n*Occam's Razor* approach to problem solving will likely cause some\r\nconfusion as it may appear that my work is an almost full on copy-paste\r\nof his `*How to Write a Spell\r\nChecker* <http://norvig.com/spell-correct.html>`__!\r\n\r\nBut I swear it's not! I actually I think I may have out-Norvig'ed Peter\r\nNorvig when it comes to describing `conditional\r\nprobability <http://en.wikipedia.org/wiki/Conditional_probability>`__:\r\nP(wordA & wordB) = P(wordB \\| wordA)\\*P(wordA)\r\n\r\nAnd another one to Rob Renaud's `Gibberish\r\nDetector <https://github.com/rrenaud/Gibberish-Detector>`__. I, out of\r\npure chance, ran into his project some time after running into Norvig's\r\narticle. I can't describe *how much it helped* to intuitively understand\r\nwhat the heavy hitters of \"AI\" consider to be introductory material;\r\nthis was greatly needed b/c at the time, I felt overwhelmed by my own\r\ndesire to really understand this area, and everything else going on.\r\n\r\nI do have a second article about this exact thing, only expressed\r\ndifferently (audience is non-programming), and it may or may not be\r\nposted soon! [STRIKEOUT:Oh and the code too, that is if someone hasn't\r\ngotten to translating the above article to code before I can get to\r\nuploading the project :P I'm trying to get the kinks out of here and the\r\ncode so it's simple, duh!]\r\n\r\nI dedicate this work to my sisters, Cat, Melissa and Christine, and my\r\nfavorite brother, Christian :)\r\n\r\nnote 1\r\n^^^^^^\r\n\r\n`go back <#explain-like-im-5>`__\r\n\r\n*To avoid confusion, I wrote this section in the form of a letter to my\r\nyounger siblings*\r\n\r\nnote 2\r\n^^^^^^\r\n\r\n\\*I'm borrowing, what I consider, `one of the most beautiful thought\r\nexperiments I've ever heard trying to describe one's\r\nself <https://www.youtube.com/watch?v=pCofmZlC72g#t=1144>`__. I'm a big\r\nfan of Sam Harris's work. Highly recommend!\r\n", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/rodricios/autocomplete", "keywords": "autocomplete autosuggest suggest complete spell spellsuggest                 hidden markov model HMM hmm markov chain iPhone iphone suggest                 Google suggest search as you type searchsuggest type spell                 automatic spelling word suggest machine learning ai text                 conditional probability model probabilistic perspective                 Rodrigo Palacios rodrigo palacios im-rodrigo im_rodrigo                 rodricios", "license": "MIT", "maintainer": null, "maintainer_email": null, "name": "autocomplete", "package_url": "https://pypi.org/project/autocomplete/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/autocomplete/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/rodricios/autocomplete"}, "release_url": "https://pypi.org/project/autocomplete/0.0.104/", "requires_dist": null, "requires_python": null, "summary": "tiny 'autocomplete' tool using a \"hidden markov model\"", "version": "0.0.104", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>A practical guide to implementing \u201cautocomplete\u201d! It follows the\nsometimes misunderstood principles of conditional probability\ndistributions and the generalized Hidden Markov Model (HMM).</p>\n<p>Fun fact: Your iPhone\u2019s \u201cautocomplete\u201d was implemented using a HMM! Plus\nthe extra stuff it chose to <a href=\"http://www.cnet.com/news/us-patent-office-rejects-apple-autocomplete-patent-used-against-samsung/\" rel=\"nofollow\">sue Samsung\nfor</a>.</p>\n<div id=\"skip-to\">\n<h2>Skip to:</h2>\n<ul>\n<li><a href=\"#how-to-install\" rel=\"nofollow\">How to\u2019s</a></li>\n<li><a href=\"#tldr\" rel=\"nofollow\">tl;dr?</a></li>\n<li><a href=\"#motivation\" rel=\"nofollow\">Motivation</a></li>\n<li><a href=\"#explain-like-im-5\" rel=\"nofollow\">ELI5</a></li>\n<li><a href=\"#if-youre-not-5\" rel=\"nofollow\">If you\u2019re not 5</a></li>\n</ul>\n</div>\n<hr class=\"docutils\">\n<div id=\"how-to-install\">\n<h2>How to install:</h2>\n<pre>pip install autocomplete\n</pre>\n</div>\n<div id=\"how-to-use\">\n<h2>How to use:</h2>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">autocomplete</span>\n\n<span class=\"c1\"># load pickled python Counter objects representing our predictive models</span>\n<span class=\"c1\"># I use Peter Norvigs big.txt (http://norvig.com/big.txt) to create the predictive models</span>\n<span class=\"n\">autocomplete</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># imagine writing \"the b\"</span>\n<span class=\"n\">autocomplete</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"s1\">'the'</span><span class=\"p\">,</span><span class=\"s1\">'b'</span><span class=\"p\">)</span>\n\n<span class=\"p\">[(</span><span class=\"s1\">'blood'</span><span class=\"p\">,</span> <span class=\"mi\">204</span><span class=\"p\">),</span>\n <span class=\"p\">(</span><span class=\"s1\">'battle'</span><span class=\"p\">,</span> <span class=\"mi\">185</span><span class=\"p\">),</span>\n <span class=\"p\">(</span><span class=\"s1\">'bone'</span><span class=\"p\">,</span> <span class=\"mi\">175</span><span class=\"p\">),</span>\n <span class=\"p\">(</span><span class=\"s1\">'best'</span><span class=\"p\">,</span> <span class=\"mi\">149</span><span class=\"p\">),</span>\n <span class=\"p\">(</span><span class=\"s1\">'body'</span><span class=\"p\">,</span> <span class=\"mi\">149</span><span class=\"p\">),</span>\n <span class=\"o\">...</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># now you type an \"o\"</span>\n\n<span class=\"n\">autocomplete</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"s1\">'the'</span><span class=\"p\">,</span><span class=\"s1\">'bo'</span><span class=\"p\">)</span>\n\n<span class=\"p\">[(</span><span class=\"s1\">'bone'</span><span class=\"p\">,</span> <span class=\"mi\">175</span><span class=\"p\">),</span>\n <span class=\"p\">(</span><span class=\"s1\">'body'</span><span class=\"p\">,</span> <span class=\"mi\">149</span><span class=\"p\">),</span>\n <span class=\"p\">(</span><span class=\"s1\">'bones'</span><span class=\"p\">,</span> <span class=\"mi\">122</span><span class=\"p\">),</span>\n <span class=\"p\">(</span><span class=\"s1\">'boy'</span><span class=\"p\">,</span> <span class=\"mi\">46</span><span class=\"p\">),</span>\n <span class=\"p\">(</span><span class=\"s1\">'bottom'</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">),</span>\n <span class=\"p\">(</span><span class=\"s1\">'box'</span><span class=\"p\">,</span> <span class=\"mi\">24</span><span class=\"p\">),</span>\n <span class=\"o\">...</span><span class=\"p\">]</span>\n</pre>\n<p>If you have your own language model in the form described in\n<a href=\"#explain-like-im-5\" rel=\"nofollow\">ELI5</a>, then use the <em>models</em> submodule to call\nthe training method:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">autocomplete</span> <span class=\"kn\">import</span> <span class=\"n\">models</span>\n\n<span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">train_models</span><span class=\"p\">(</span><span class=\"s1\">'some giant string of text'</span><span class=\"p\">)</span>\n</pre>\n<p>Want to run it as a server (bottlepy required)?</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">autocomplete</span>\n\n<span class=\"n\">autocomplete</span><span class=\"o\">.</span><span class=\"n\">run_server</span><span class=\"p\">()</span>\n\n<span class=\"c1\">#output</span>\n<span class=\"n\">Bottle</span> <span class=\"n\">v0</span><span class=\"o\">.</span><span class=\"mf\">12.8</span> <span class=\"n\">server</span> <span class=\"n\">starting</span> <span class=\"n\">up</span> <span class=\"p\">(</span><span class=\"n\">using</span> <span class=\"n\">WSGIRefServer</span><span class=\"p\">())</span><span class=\"o\">...</span>\n<span class=\"n\">Listening</span> <span class=\"n\">on</span> <span class=\"n\">http</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">localhost</span><span class=\"p\">:</span><span class=\"mi\">8080</span><span class=\"o\">/</span>\n<span class=\"n\">Hit</span> <span class=\"n\">Ctrl</span><span class=\"o\">-</span><span class=\"n\">C</span> <span class=\"n\">to</span> <span class=\"n\">quit</span><span class=\"o\">.</span>\n</pre>\n<p>Now head over to <a href=\"http://localhost:8080/the/bo\" rel=\"nofollow\">http://localhost:8080/the/bo</a></p>\n<pre>http://localhost:8080/the/bo\n#output\n{\"body\": 149, \"box\": 24, \"bottom\": 32, \"boy\": 46, \"borzois\": 16, \"bodies\": 13, \"bottle\": 13, \"bones\": 122, \"book\": 14, \"bone\": 175}\n\nhttp://localhost:8080/the/bos\n#output\n{\"boscombe\": 11, \"boston\": 7, \"boss\": 1, \"bosom\": 5, \"bosses\": 4}\n</pre>\n<div id=\"obligatory-tests\">\n<h3>Obligatory tests</h3>\n<pre>python setup.py test\n</pre>\n</div>\n</div>\n<hr class=\"docutils\">\n<div id=\"tl-dr\">\n<h2><a href=\"https://github.com/rodricios/autocomplete/blob/master/autocomplete/models.py\" rel=\"nofollow\">tl;dr</a></h2>\n<p>The following code excerpt is my interpretation of a series of\nlessons/concepts expressed in a number of different books.</p>\n<p>The unifying concept can be said to be <a href=\"http://en.wikipedia.org/wiki/Conditional_probability\" rel=\"nofollow\">conditional\nprobability</a>:</p>\n<pre>P(A , B) = P(B | A) * P(A)\n</pre>\n<p>Which can read as saying:</p>\n<pre>The probability of A and B occuring is equal to the probability of B occuring, given that A has occured\n</pre>\n<p>More on this below.</p>\n<pre><span class=\"c1\"># \"preperation\" step</span>\n<span class=\"c1\"># for every word in corpus, normalize ('The' -&gt; 'the'), insert to list</span>\n<span class=\"n\">WORDS</span> <span class=\"o\">=</span> <span class=\"n\">helpers</span><span class=\"o\">.</span><span class=\"n\">re_split</span><span class=\"p\">(</span><span class=\"n\">corpus</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># first model -&gt; P(word)</span>\n<span class=\"c1\"># Counter constructor will take a list of elements and create a frequency distribution (histogram)</span>\n<span class=\"n\">WORDS_MODEL</span> <span class=\"o\">=</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">Counter</span><span class=\"p\">(</span><span class=\"n\">WORDS</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># another preperation step</span>\n<span class=\"c1\"># [a,b,c,d] -&gt; [[a,b], [c,d]]</span>\n<span class=\"n\">WORD_TUPLES</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">helpers</span><span class=\"o\">.</span><span class=\"n\">chunks</span><span class=\"p\">(</span><span class=\"n\">WORDS</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># second model -&gt; P(next word | prev. word)</span>\n<span class=\"c1\"># I interpret \"..| prev. word)\" as saying \"dictionary key</span>\n<span class=\"c1\"># leading to seperate and smaller (than WORDS_MODEL) freq. dist.</span>\n<span class=\"n\">WORD_TUPLES_MODEL</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"n\">first</span><span class=\"p\">:</span><span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">Counter</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">first</span><span class=\"p\">,</span> <span class=\"n\">second</span> <span class=\"ow\">in</span> <span class=\"n\">WORD_TUPLES</span><span class=\"p\">}</span>\n\n<span class=\"k\">for</span> <span class=\"n\">prev_word</span><span class=\"p\">,</span> <span class=\"n\">next_word</span> <span class=\"ow\">in</span> <span class=\"n\">WORD_TUPLES</span><span class=\"p\">:</span>\n    <span class=\"c1\"># this is called the \"conditioning\" step where we assert</span>\n    <span class=\"c1\"># that the probability space of all possible \"next_word\"'s</span>\n    <span class=\"c1\"># is \"conditioned\" under the event that \"prev_word\" has occurred</span>\n    <span class=\"n\">WORD_TUPLES_MODEL</span><span class=\"p\">[</span><span class=\"n\">prev_word</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">([</span><span class=\"n\">next_word</span><span class=\"p\">])</span>\n</pre>\n<p>Textbooks, and locations therein, where the concept-in-practice has been\nexpressed:</p>\n<p>I. <a href=\"http://ics.upjs.sk/~pero/web/documents/pillar/Manning_Schuetze_StatisticalNLP.pdf\" rel=\"nofollow\">Intro to Statistical Natural Language\nProcessing</a>\n- Manning, Sch\u00fctze, 1999</p>\n<pre>a. frequency distribution showing the most common words and frequencies in *Tom Sawyer*, pg. 21\n\nb. conditional probability definition expressed in page 42 - section 2.1.2\n\nc. the intuition for *frequency* distributions found in pg. 153 (provided in the context of finding [*Collocations*](http://en.wikipedia.org/wiki/Collocation))\n</pre>\n<ol>\n<li><a href=\"http://mitpress.mit.edu/books/probabilistic-graphical-models\" rel=\"nofollow\">Probabilistic Graphical\nModels</a>\n- Kohler, Friedman, 2009<ol>\n<li>conditional probability definition found on pg. 18 (hilariously\nand coincidentally found in section 2.1.2.1)</li>\n</ol>\n</li>\n<li><a href=\"http://aima.cs.berkeley.edu\" rel=\"nofollow\">Artificial Intelligence - A Modern\nApproach</a> - Russell, Norvig, 3rd.\ned. 2010<ol>\n<li>conditional probability concept explained in pg. 485</li>\n<li>the \u201clanguage\u201d (I take to mean \u201cintuition\u201d for asserting things\nin the probabilistic sense) pg. 486</li>\n<li>the notion of \u201cconditioning\u201d found in pg. 492-494</li>\n</ol>\n</li>\n</ol>\n</div>\n<div id=\"motivation\">\n<h2>Motivation</h2>\n<p>Similar to the motivation behind\n<a href=\"https://github.com/rodricios/eatiht#motivation\" rel=\"nofollow\">eatiht</a>, I found\nthat it took far too long to find a palpable theory-to-application\nexample of what amounts to more than a 500 pages of words across 3\nbooks, each spanning a large index of, in certain cases,\n<em>counter-intuitive</em> nomenclature; read the <a href=\"http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/ckep3z6\" rel=\"nofollow\">light\ncriticisms</a>\nmade by Michael I. Jordan on the matter (he was recently named <a href=\"http://dataconomy.com/10-machine-learning-experts-you-need-to-know/\" rel=\"nofollow\">#2\nmachine learning expert \u201cwe need to know\u201d on\ndataconomy.com</a>).</p>\n<p>You can find similar thoughts being expressed <a href=\"http://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/\" rel=\"nofollow\">**in an article from 2008\n(updated\n2009)**</a>\nby <a href=\"http://brenocon.com\" rel=\"nofollow\">Brennan O\u2019Connor</a></p>\n<hr class=\"docutils\">\n<p><a href=\"#note-1\" rel=\"nofollow\">*This work is dedicated to my siblings*</a>.</p>\n</div>\n<div id=\"explain-like-i-m-5\">\n<h2>Explain like I\u2019m 5<a href=\"#note-1\" rel=\"nofollow\">*</a></h2>\n<p>*Warning! This explanation is literally intended for young kids - I\u2019m\nactually trying to see if these concepts can be explained to an audience\nunaware of the nomenclature used within the statistical\n<a href=\"http://en.wikipedia.org/wiki/Natural_language_processing\" rel=\"nofollow\">nlp</a> and\nother machine learning fields. For example, my 7, 9, 11, 14 y.o.\nsiblings, and basically anyone else who\u2019s ever read a story to a child -\nthey would be a part of the target audience.</p>\n<p>If you\u2019ve found this readable and informative, please consider putting\non the goofiest face and reading this to your kids, if you have any :)\nIf you do, please send me your thoughts on the experience.</p>\n<p>I\u2019m only interested in lowering the barrier to entry. I should have\nincluded this note since the beginning (sorry to those who undoubtedly\nleft with a bad taste in their mouths).</p>\n<p>You can contact me at <a href=\"mailto:rodrigopala91%40gmail.com\">rodrigopala91<span>@</span>gmail<span>.</span>com</a></p>\n<p>Thanks for reading,</p>\n<p>Rodrigo</p>\n</div>\n<div id=\"eli5\">\n<h2>ELI5</h2>\n<p>No. I\u2019m explaining this like you\u2019re 5. I know you\u2019re not <em>5</em> , <em>you\nguys\u2026 Chris, stop jumping on your sister\u2019s back</em>!</p>\n<p>Ok, so I\u2019m saying, <em>imagine I\u2019m 5!</em></p>\n<p>Oh, that was easy now huh? Let\u2019s just forget the <em>I\u2019m 5</em> part.</p>\n<p>Imagine a giant collection of books.</p>\n<p>For example, all the Harry Potter and Hunger Games novels put together.</p>\n<p>What if I asked you to go through all the pages and all the words in\nthose pages?</p>\n<p>Now I\u2019m not asking you <em>four</em> to actually <em>read</em> the books. You know,\njust go through, beginning to end, and notice each word.</p>\n<p>For every new word you see, write it down, and put a \u201c1\u201d next to it, and\neverytime you see a word <em>again</em>, add \u201c1\u201d more to the previous number.</p>\n<p>So basically I\u2019m asking y\u2019all to keep count of how many times a word\ncomes up.</p>\n<p>Got it? If yes, cool! If not, find a sibling, friend, or adult near you\nand ask them to help you out :)</p>\n<p>\u2026</p>\n<p>Say you start with <em>Harry Potter and the Sorcerer\u2019s Stone</em>:</p>\n<pre>Mr. and Mrs. Dursley of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much...\n</pre>\n<p>And imagine that you\u2019re on the 5th word. This or something close to this\nis what you\u2019re going for:</p>\n<pre>Mr.     -&gt; 1\nand     -&gt; 1\nMrs.    -&gt; 1\nDursley -&gt; 1\nof      -&gt; 1\n</pre>\n<p>Or if you\u2019re a <em>wannabe-Harry-Potter</em> fan, ah I\u2019m just kidding!</p>\n<p>If you started with <em>the-book-that-must-not-be-named</em> - I know you guys\nwon\u2019t get it, but persons my age will :)</p>\n<p>Alright! So you started with <em>The Hunger Games</em>:</p>\n<pre>When I wake up, the other side of the bed is cold...\n</pre>\n<p>By the sixth word you have:</p>\n<pre>When  -&gt; 1\nI     -&gt; 1\nwake  -&gt; 1\nup    -&gt; 1\nthe   -&gt; 1\n</pre>\n<p>You have a long day ahead of you\u2026</p>\n<p>\u2026</p>\n<p><em>1,105,285 words later</em></p>\n<p>Now that you\u2019re done tallying up all those words, why not order all\nthese words by the <em>number of times you\u2019ve seen them</em>?</p>\n<p>See you next week!</p>\n<p>\u2026</p>\n<p>Back so soon? You should have gotten something like this:</p>\n<pre>psst*, remember, the format is:\n word -&gt; # of times the word appears\n\n'the' -&gt; 80030\n'of'  -&gt; 40025\n'and' -&gt; 38313\n'to'  -&gt; 28766\n'in'  -&gt; 22050\n'a'   -&gt; 21155\n'that'-&gt; 12512\n'he'  -&gt; 12401\n'was' -&gt; 11410\n'it'  -&gt; 10681\n... there's a lot more words you've tallied up...\n</pre>\n<p>Those were the most common words.</p>\n<p>Now on the <em>less-frequent</em> end, you\u2019ll find your words appearing not as\noften\u2026</p>\n<pre>... 29137 words later.\n'przazdziecka' -&gt; 1\n'disclosure'   -&gt; 1\n'galvanism'    -&gt; 1\n'repertoire'   -&gt; 1\n'bravado'      -&gt; 1\n'gal'          -&gt; 1\n'ideological'  -&gt; 1\n'guaiacol'     -&gt; 1\n'expands'      -&gt; 1\n'revolvers'    -&gt; 1\n</pre>\n<p>Yeah Chris? Oh, \u2018what does <em>lez freekend</em>\u2019 mean? Um, so it means\nsomething like: <em>you probably won\u2019t hear or read that word very often.</em></p>\n<p>Now what if I asked you to help me find this word I\u2019m looking for? And I\nknow this word starts with the letters: \u2018th\u2019.</p>\n<p>I\u2019m pretty sure you guys can do this much faster!</p>\n<p>\u2026</p>\n<p><em>5 minutes later!</em></p>\n<p>\u2026</p>\n<p>Not bad! You only had to go through 29157 unique words after all!</p>\n<pre>'the'  -&gt; 80030\n'that' -&gt; 12512\n'this' -&gt; 4063\n'they' -&gt; 3938\n'there'-&gt; 2972\n'their'-&gt; 2955\n'them' -&gt; 2241\n'then' -&gt; 1558\n'these'-&gt; 1231\n'than' -&gt; 1206\n... 229 words more...\n</pre>\n<p>239 words, still kind of lot though huh? And you know your big brother,\nhe\u2019s too lazy to do this work <em>by hand</em> (<em>cough</em> program it up <em>cough</em>)\n;)</p>\n<p>So the word I\u2019m looking for is on the tip of my tongue. I think the next\nletter is \u201ci\u201d.</p>\n<p><em>1 minute later</em></p>\n<pre>'this'     -&gt; 4063\n'think'    -&gt; 557\n'things'   -&gt; 321\n'thing'    -&gt; 303\n'third'    -&gt; 239\n'thin'     -&gt; 166\n'thinking' -&gt; 137\n'thirty'   -&gt; 123\n'thick'    -&gt; 77\n'thirds'   -&gt; 43\n... 36 words more...\n</pre>\n<p><em>I scan through the first 10 words.</em> Oh, I just remembered that the next\nletter is \u2018r\u2019.</p>\n<p><em>You start taking out even more words.</em></p>\n<p><em>10 seconds later.</em></p>\n<pre>'third'      -&gt; 239\n'thirty'     -&gt; 123\n'thirds'     -&gt; 43\n'thirteen'   -&gt; 32\n'thirst'     -&gt; 13\n'thirteenth' -&gt; 11\n'thirdly'    -&gt; 8\n'thirsty'    -&gt; 5\n'thirtieth'  -&gt; 3\n'thirties'   -&gt; 2\n</pre>\n<p>Aha, \u2018thirdly\u2019 was the word I was looking for! What, you never heard of\nthe word \u201cthirdly\u201d before?</p>\n<p>Now you might be saying to yourself, \u201c<em>that\u2019s pretty cool!</em>\u201d, and\nyou\u2019re right!</p>\n<p>And you know what\u2019s cooler? <em>Making everyone\u2019s life a tiny bit easier</em>\nis! :)</p>\n<p>But how can you do that with just <em>words</em>?</p>\n<p>Aren\u2019t words boring and dull?</p>\n<p>It\u2019s like all we do is talk, write, and think with <em>words</em>. I mean, how\nlame, I can\u2019t even describe to you this <em>autocomplete</em>\nthing-slash-idea-thing without having to write it out with <em>words</em>!</p>\n<p>Ugh! I hate words!</p>\n<p><em>Whoah, wait a minute! That was not cool of me! Let\u2019s relax for a\nminute.</em></p>\n<p>Let\u2019s try to give an imaginary hug to the word-factory in our brains.\nThat part of our brain works so hard, even when we don\u2019t ask it to. How\nnice of our brain to do that. Not!</p>\n<p>What I\u2019m trying to is sometimes it\u2019s not so nice for our brains to\ndistract us, especially when we have homework or other, real-world,\nproblems like adult-homework.</p>\n<p>So how about this: let\u2019s try to think about <em>what</em> the next sentence\ncoming out of our own mouths <em>will be</em><a href=\"#note-2\" rel=\"nofollow\">*</a>.</p>\n<p>Now if you\u2019re thinking about what will be coming out of my mouth, or out\nof your mouth, or your mouth, or your mouth, or your mouth, you\u2019re doing\nit wrong! (to readers who aren\u2019t one of my 4 younger siblings, that\u2019s\nhow many I have).</p>\n<p>Try your best to think about <em>what</em> the next sentence coming out of\n<em>your own</em> mouth will be.</p>\n<p>\u2026</p>\n<p>Did you decide on your sentence? Good!</p>\n<p>Now what if I asked you to give me two reasons explaining <em>why</em> and\n<em>how</em> you chose the sentence you chose?</p>\n<p>Wait, I can\u2019t even do that! Let\u2019s make it easier on ourselves and\nexplain <em>why</em> and <em>how</em> we chose the first <em>word</em>.</p>\n<p>Still pretty hard huh?</p>\n<p>If you thought about it, and you thought it was pretty darn hard to give\na <em>good and honest</em> reason as to why it is you chose the word you chose,\nlet\u2019s bring out a word you guys might not understand: <em>probability</em>.</p>\n<p>If you feel like you don\u2019t <em>get</em> what the word means, sure you do! Just\nuse the word \u201cprobably\u201d in one of your sentences, but but try to makes\nsome sense.</p>\n<p>What do I mean? Well, let\u2019s just consider the English language. Like\nmost other things, the English language has rules.</p>\n<p>The kind of rules that can be simplified down to:</p>\n<ol>\n<li>\u201c<strong>*something*</strong> <em>action</em> <strong>*something*</strong>\u201d.</li>\n<li>Replace <strong>*something*</strong>\u2019s and <strong>*action*</strong> with words that make sense\nto you.</li>\n</ol>\n<p>Fair enough, right?</p>\n<p>Now, imagine you could put \u201cpause\u201d right after the first word that comes\nout of your mouth.</p>\n<p>Let\u2019s just say that first word is \u201cthe\u201d.</p>\n<p>Now in the case that you stuttered for reasons outside your\nconscientious control (for example: \u201cthhh thhe the\u201d). No big deal, you\nmeant to say \u201cthe\u201d, so let\u2019s <em>flatten</em> it to just that!</p>\n<p>With that <em>word</em> said, what words do you <em>think</em> you might have said\nafter it?</p>\n<p>You might tell me, \u201c<em>any word I want!</em></p>\n<p>Of course you could have! I bet you spent a millisecond thinking about\nwhether or not the next word you were going to say was going to be:\n<em>guaiacol</em>.</p>\n<p>I <em>know</em> because I thought about using that word too!</p>\n<p>I can remember the first time I heard (or read) <em>guaiacol</em> like it was\nyesterday. I read it in some funky article on the internet. I found the\nword in a list of words that don\u2019t appear too often in the English\nlanguage.</p>\n<p>After I read it, I was able to fit <em>guaiacol</em> nicely into that part of\nmy brain where I\u2026 uhh.. was\u2026 able\u2026 uhh\u2026</p>\n<p>Oh, you <em>know</em>, that place in my brain where I get to choose whether I\nwant to say <em>the apple</em>, <em>the automobile</em>, <em>the austronaut</em>, etc.</p>\n<p>\u2026</p>\n<p>Ok, so clearly I\u2019m no brainician, and that may or may not be the way our\nbrain works - actually, it\u2019s probably super super unlikely.</p>\n<p>But even though that idea is probably wrong, the idea itself sounds like\na pretty darn good way of suggesting the next word or words somebody is\ntrying to <em>type</em>.</p>\n<p>What if you had a way to count the number of times you\u2019ve heard \u201capple\u201d\nsaid after the word \u201cthe\u201d?</p>\n<p>Ask yourself the same question, but now with the word \u201cautomobile\u201d\ninstead of \u201capple\u201d.</p>\n<p>What if you had the time to think about every possible word that you\u2019ve\never heard spoken after the word \u201cthe\u201d? I\u2019d say it might have looked\nsomething like this:</p>\n<pre>Words you might have heard following the word \"the\" and the number of times you might have heard it\n\n'same'     -&gt; 996\n'french'   -&gt; 688\n'first'    -&gt; 652\n'old'      -&gt; 591\n'emperor'  -&gt; 581\n'other'    -&gt; 528\n'whole'    -&gt; 500\n'united'   -&gt; 466\n'room'     -&gt; 376\n'most'     -&gt; 373\n\n... 9331 more words...\n</pre>\n<p>Not impressed with your brain yet? Let\u2019s continue this little thought\nexperiment further.</p>\n<p>Imagine that you just said \u201cthe\u201d, and you could put pause after the\nfirst <em>letter</em> of the next word out of your mouth: \u201ch\u201d.</p>\n<p>Real quick, think of the shortest amount of time you can think of. Think\nof the shortest <em>second</em> you can think of. Now shorter than that too.</p>\n<p>At this point, you can\u2019t even call that length of time a <em>second</em>. But\nin that length of time, your brain may have just done this:</p>\n<pre>Every word you've ever heard coming after the word \"the\":\n\n'house'   -&gt; 284\n'head'    -&gt; 117\n'hands'   -&gt; 101\n'hand'    -&gt; 97\n'horses'  -&gt; 71\n'hill'    -&gt; 64\n'highest' -&gt; 64\n'high'    -&gt; 57\n'history' -&gt; 56\n'heart'   -&gt; 55\n</pre>\n<p>And that brain you got did this realllllyyyyyy fast. Faster than Google,\nBing, Yahoo and any other company can ever hope to beat. And your brain\ndid this without even asking for your permission. I think our brains are\ntrying to control us you guys, oh no!</p>\n</div>\n<div id=\"if-you-re-not-5\">\n<h2>If you\u2019re not 5</h2>\n<p>The basic idea is this:</p>\n<p>Assume you have a large collection of Enlish-understandable text merged\ninto a single string.</p>\n<p>Start by transforming that string into a list of words (AKA <em>ngrams of\nword-legth</em>), and also (but not required) normalize each word (\u2018The\u2019 -&gt;\n\u2018the\u2019).</p>\n<p>Once you have a normalized list of words, you can start building a\nfrequency distribution measuring the frequency of each word.</p>\n<p>\u2026</p>\n<p>At this point you can start \u201cpredict\u201d the \u201cfinal state\u201d of a\nword-in-progress. But consider the case where a user types in some query\nbox:</p>\n<pre>\"The th\"\n</pre>\n<p>And he intends to write:</p>\n<pre>\"The third\"\n</pre>\n<p>With the above predictive model, you\u2019ll be suggesting something like:</p>\n<pre>[\n    ('the', 80030),\n    ('they', 3938),\n    ('there', 2972),\n    ...\n]\n</pre>\n<p>This explains one specific type of predictive model, which can be\nwritten as P(word), and you\u2019ve just seen the pitfalls of using <strong>just</strong>\nthis model.</p>\n<p>Now for the next word, ask yourself, what\u2019s the probability that I\u2019m\ngoing to type the word \u201capple\u201d given that I wrote \u201ctasty\u201d?</p>\n<p>In machine learning and AI books, you\u2019ll be presented <em>Conditional\nProbability</em> with the following equation:</p>\n<pre>P(word A and word B) = P(word B | word A) * P(word A)\n</pre>\n<p>That equation addresses the problem that I mentioned.</p>\n<p>We\u2019ve handled P(wordA) already.</p>\n<p>To handle P(word B | word A), which reads <em>probability of word A given\nword B *, I take a *literall</em> interpretation of the word \u201cgiven\u201d, in\nthat context, to mean the following:</p>\n<p><em>\u201cword A\u201d is the key pointing to a probability distribution representing\nall the words that follow \u201cword A\u201d</em></p>\n<p>Once we can represent this second model, we can also apply the\n<em>filtering</em> step - given that we know more letters in the second word,\nwe can zone in on more precise suggestions.</p>\n<hr class=\"docutils\">\n<div id=\"afterword\">\n<h3>Afterword</h3>\n<p>notes: *I have to give a shout out to <a href=\"https://twitter.com/SamHarrisOrg\" rel=\"nofollow\">Sam\nHarris</a> for being, AFAIK, the first\nperson or one of the firsts, in <a href=\"https://www.youtube.com/watch?v=pCofmZlC72g#t=1144\" rel=\"nofollow\">wonderfully putting into\nwords</a> what I\u2019ve\nborrowed and slightly adapted for this writing. <a href=\"http://www.samharris.org/\" rel=\"nofollow\">I highly recommend his\nwork</a></p>\n<p>Another shoutout to <a href=\"http://norvig.com\" rel=\"nofollow\">Peter Norvig</a> for inspiring\nme and probably many others with our own little \u201ctoy\u201d programs. His\n<em>Occam\u2019s Razor</em> approach to problem solving will likely cause some\nconfusion as it may appear that my work is an almost full on copy-paste\nof his <a href=\"http://norvig.com/spell-correct.html\" rel=\"nofollow\">*How to Write a Spell\nChecker*</a>!</p>\n<p>But I swear it\u2019s not! I actually I think I may have out-Norvig\u2019ed Peter\nNorvig when it comes to describing <a href=\"http://en.wikipedia.org/wiki/Conditional_probability\" rel=\"nofollow\">conditional\nprobability</a>:\nP(wordA &amp; wordB) = P(wordB | wordA)*P(wordA)</p>\n<p>And another one to Rob Renaud\u2019s <a href=\"https://github.com/rrenaud/Gibberish-Detector\" rel=\"nofollow\">Gibberish\nDetector</a>. I, out of\npure chance, ran into his project some time after running into Norvig\u2019s\narticle. I can\u2019t describe <em>how much it helped</em> to intuitively understand\nwhat the heavy hitters of \u201cAI\u201d consider to be introductory material;\nthis was greatly needed b/c at the time, I felt overwhelmed by my own\ndesire to really understand this area, and everything else going on.</p>\n<p>I do have a second article about this exact thing, only expressed\ndifferently (audience is non-programming), and it may or may not be\nposted soon! [STRIKEOUT:Oh and the code too, that is if someone hasn\u2019t\ngotten to translating the above article to code before I can get to\nuploading the project :P I\u2019m trying to get the kinks out of here and the\ncode so it\u2019s simple, duh!]</p>\n<p>I dedicate this work to my sisters, Cat, Melissa and Christine, and my\nfavorite brother, Christian :)</p>\n<div id=\"note-1\">\n<h4>note 1</h4>\n<p><a href=\"#explain-like-im-5\" rel=\"nofollow\">go back</a></p>\n<p><em>To avoid confusion, I wrote this section in the form of a letter to my\nyounger siblings</em></p>\n</div>\n<div id=\"note-2\">\n<h4>note 2</h4>\n<p>*I\u2019m borrowing, what I consider, <a href=\"https://www.youtube.com/watch?v=pCofmZlC72g#t=1144\" rel=\"nofollow\">one of the most beautiful thought\nexperiments I\u2019ve ever heard trying to describe one\u2019s\nself</a>. I\u2019m a big\nfan of Sam Harris\u2019s work. Highly recommend!</p>\n</div>\n</div>\n</div>\n\n          </div>"}, "last_serial": 1447606, "releases": {"0.0.0.1": [{"comment_text": "", "digests": {"md5": "8c2380b435c7eedc6e4134b1d2116f4d", "sha256": "63a79c8c51e998b77421daa66899748cd45cb94276a620444a5015cbf1aab7dd"}, "downloads": -1, "filename": "autocomplete-0.0.0.1.zip", "has_sig": false, "md5_digest": "8c2380b435c7eedc6e4134b1d2116f4d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6842, "upload_time": "2015-02-20T14:14:43", "upload_time_iso_8601": "2015-02-20T14:14:43.593684Z", "url": "https://files.pythonhosted.org/packages/a4/96/8549fa6709d05c152f7471b65ebb0e09f94ca4fecdecd3dd64adf827c9c5/autocomplete-0.0.0.1.zip", "yanked": false}], "0.0.0.101": [{"comment_text": "", "digests": {"md5": "24de31cd4adc0b0ac2c6091adf61b344", "sha256": "9cf76dfde77db556b3b0aa438254ea5727bdb407910f84490359fe7e5a3180f4"}, "downloads": -1, "filename": "autocomplete-0.0.0.101.zip", "has_sig": false, "md5_digest": "24de31cd4adc0b0ac2c6091adf61b344", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6963, "upload_time": "2015-02-20T14:16:23", "upload_time_iso_8601": "2015-02-20T14:16:23.830861Z", "url": "https://files.pythonhosted.org/packages/56/83/6522acc5561cc7663a94d3a6ff3ac795020ef0591b3949358d429015029e/autocomplete-0.0.0.101.zip", "yanked": false}], "0.0.0.102": [{"comment_text": "", "digests": {"md5": "01c004904454a2ebe286fef04f9b5f8a", "sha256": "82420c42127c46cd10aa7d998032040fe9cc558ad95b2ba9825b350fa419a5d7"}, "downloads": -1, "filename": "autocomplete-0.0.0.102.zip", "has_sig": false, "md5_digest": "01c004904454a2ebe286fef04f9b5f8a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7327, "upload_time": "2015-02-25T21:15:10", "upload_time_iso_8601": "2015-02-25T21:15:10.432278Z", "url": "https://files.pythonhosted.org/packages/bb/bb/51a9b242e8affbf74477d9e6755311f14fff89c50211fd34395cf5c38d36/autocomplete-0.0.0.102.zip", "yanked": false}], "0.0.0.103": [{"comment_text": "", "digests": {"md5": "f7e91f51dd68bfd5256565d8a5dd7a46", "sha256": "9caa8004e72b031eb679815bcdb567c23381a668fe3a07be04f1c5bea46116a8"}, "downloads": -1, "filename": "autocomplete-0.0.0.103.zip", "has_sig": false, "md5_digest": "f7e91f51dd68bfd5256565d8a5dd7a46", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7264, "upload_time": "2015-02-26T08:48:38", "upload_time_iso_8601": "2015-02-26T08:48:38.470534Z", "url": "https://files.pythonhosted.org/packages/45/a6/fa8fcea28995707cda1cc1204e080eef521fcbe77f6b0d4cf8022b3fee76/autocomplete-0.0.0.103.zip", "yanked": false}], "0.0.0.1031": [{"comment_text": "", "digests": {"md5": "1ebd2ff27e410fd6523f38217da5ac59", "sha256": "d6ab3ea22b026f12f8cde89428b2ff6331d937b1790e55bd492d4a32f2676027"}, "downloads": -1, "filename": "autocomplete-0.0.0.1031.zip", "has_sig": false, "md5_digest": "1ebd2ff27e410fd6523f38217da5ac59", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7299, "upload_time": "2015-02-26T08:54:49", "upload_time_iso_8601": "2015-02-26T08:54:49.998096Z", "url": "https://files.pythonhosted.org/packages/9f/be/ce9c4a8f79ef73d0c172ac8312547585ab98e383dbaba4e122903522a2c9/autocomplete-0.0.0.1031.zip", "yanked": false}], "0.0.0.1032": [{"comment_text": "", "digests": {"md5": "0e2e5651d21a829d2dba037421e99723", "sha256": "bdbe88d08fae58e97bc6fe6b14f35cfc1849d6a83d1e8acb1d5536fa80dda8e8"}, "downloads": -1, "filename": "autocomplete-0.0.0.1032.zip", "has_sig": false, "md5_digest": "0e2e5651d21a829d2dba037421e99723", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17077, "upload_time": "2015-02-26T09:05:44", "upload_time_iso_8601": "2015-02-26T09:05:44.837468Z", "url": "https://files.pythonhosted.org/packages/65/ee/0b6e4d199b697a8098b9242b8cc41f27572b0f8ea0392ad08d472d8270f4/autocomplete-0.0.0.1032.zip", "yanked": false}], "0.0.0.1033": [{"comment_text": "", "digests": {"md5": "a3b06792c6ea2544a30154fd7c0b05dc", "sha256": "1393f98456d3f7e15022f887a3f3b1e8b40ebaf304d00b3fb2b153746612c074"}, "downloads": -1, "filename": "autocomplete-0.0.0.1033.zip", "has_sig": false, "md5_digest": "a3b06792c6ea2544a30154fd7c0b05dc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16612, "upload_time": "2015-02-26T09:11:15", "upload_time_iso_8601": "2015-02-26T09:11:15.686866Z", "url": "https://files.pythonhosted.org/packages/1f/5f/3c2ad8dc85f0d44d871bbaf076b83d6db26287ef5edbc43a740c540d0bc5/autocomplete-0.0.0.1033.zip", "yanked": false}], "0.0.0.1034": [{"comment_text": "", "digests": {"md5": "1535f3ff1b081b28c06959a88c0016c1", "sha256": "3469cf1eafd2f17b0883d318a0413eef83c51e6840b47862a9b702da4c54ccd0"}, "downloads": -1, "filename": "autocomplete-0.0.0.1034.zip", "has_sig": false, "md5_digest": "1535f3ff1b081b28c06959a88c0016c1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16609, "upload_time": "2015-02-26T20:24:45", "upload_time_iso_8601": "2015-02-26T20:24:45.245619Z", "url": "https://files.pythonhosted.org/packages/bf/11/d3fbc3c491b1e7db54ebcb484442d976d7f2c88222681994bd54c5ee42f4/autocomplete-0.0.0.1034.zip", "yanked": false}], "0.0.0.1035": [{"comment_text": "", "digests": {"md5": "662214010c009a2de41206bf83d58f39", "sha256": "b880951f27a3919a404a0f3fd5d9df437704fc876acafef367d49e7187eb6b9a"}, "downloads": -1, "filename": "autocomplete-0.0.0.1035.zip", "has_sig": false, "md5_digest": "662214010c009a2de41206bf83d58f39", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16618, "upload_time": "2015-02-26T20:27:26", "upload_time_iso_8601": "2015-02-26T20:27:26.863923Z", "url": "https://files.pythonhosted.org/packages/a2/7d/7c7ed98fccfceb26c710c25e9322473d0f7a5b39f11c8d8c09078be523c7/autocomplete-0.0.0.1035.zip", "yanked": false}], "0.0.0.1036": [{"comment_text": "", "digests": {"md5": "e0a26b625bd0a23af3c85c561d4c5e8e", "sha256": "cb86af1abef9a1077e88e738a01371b723289414fdcb07d8e778bf6b65b35572"}, "downloads": -1, "filename": "autocomplete-0.0.0.1036.zip", "has_sig": false, "md5_digest": "e0a26b625bd0a23af3c85c561d4c5e8e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16618, "upload_time": "2015-02-26T20:29:02", "upload_time_iso_8601": "2015-02-26T20:29:02.421632Z", "url": "https://files.pythonhosted.org/packages/cb/7f/9072723979237a901379ff27f8feaa733b51fd200ed1a974886fa58cba38/autocomplete-0.0.0.1036.zip", "yanked": false}], "0.0.0.11": [{"comment_text": "", "digests": {"md5": "12b9fb3e01d0159784096389a7d498f7", "sha256": "2ddc537a74778817be02466d4e4953edcae47b7e2b04cc8daeee822569fd99cf"}, "downloads": -1, "filename": "autocomplete-0.0.0.11.zip", "has_sig": false, "md5_digest": "12b9fb3e01d0159784096389a7d498f7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16550, "upload_time": "2015-02-26T21:10:36", "upload_time_iso_8601": "2015-02-26T21:10:36.298248Z", "url": "https://files.pythonhosted.org/packages/3d/1d/62123f733c2927ea3c8270f98c60810988f6bc0e80749328d9c8584196e5/autocomplete-0.0.0.11.zip", "yanked": false}], "0.0.001": [], "0.0.1": [{"comment_text": "", "digests": {"md5": "6173dc97a89502d9e125c1a8326dc97a", "sha256": "aa4bbee6d7a79440b84b32dd7992c8ad1b4330aa4d7a1f04e55e4c72665768ad"}, "downloads": -1, "filename": "autocomplete-0.0.1.zip", "has_sig": false, "md5_digest": "6173dc97a89502d9e125c1a8326dc97a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16323, "upload_time": "2015-03-01T08:02:12", "upload_time_iso_8601": "2015-03-01T08:02:12.724327Z", "url": "https://files.pythonhosted.org/packages/5c/8f/b31420bae42c461742d4527b4da39fe407ecce158187319ace4a587972de/autocomplete-0.0.1.zip", "yanked": false}], "0.0.101": [{"comment_text": "", "digests": {"md5": "b5087d21da66c29c03f4f63b7fcd3a09", "sha256": "65f2c49a09afe52a0d39e70977cb9e5636ba75fbca8f475a57bd07c14fadabc0"}, "downloads": -1, "filename": "autocomplete-0.0.101.zip", "has_sig": false, "md5_digest": "b5087d21da66c29c03f4f63b7fcd3a09", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16386, "upload_time": "2015-03-01T08:10:46", "upload_time_iso_8601": "2015-03-01T08:10:46.046890Z", "url": "https://files.pythonhosted.org/packages/38/73/e224a909254ce7fb7ebea30991d8008fc84ad80375211aa452f2ea706fb0/autocomplete-0.0.101.zip", "yanked": false}], "0.0.102": [{"comment_text": "", "digests": {"md5": "de99352c06d52f76b070cabbfbf4ad98", "sha256": "3218326ffd1e5fe3e6cc046eb15b8bda798a5b03a66d497f89246ec109662d06"}, "downloads": -1, "filename": "autocomplete-0.0.102.zip", "has_sig": false, "md5_digest": "de99352c06d52f76b070cabbfbf4ad98", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16392, "upload_time": "2015-03-01T08:14:41", "upload_time_iso_8601": "2015-03-01T08:14:41.674591Z", "url": "https://files.pythonhosted.org/packages/ef/87/1b1cdc6109dbc56c7375947368ad95f9f95fc13263c847e9e89deacd5480/autocomplete-0.0.102.zip", "yanked": false}], "0.0.103": [{"comment_text": "", "digests": {"md5": "0e4fc29fe910bd4d983e8bb902f4c25e", "sha256": "659bbce98c55028bedcc24fa1b85e7192ef54fd7253c4be48b77d8909de99b44"}, "downloads": -1, "filename": "autocomplete-0.0.103.zip", "has_sig": false, "md5_digest": "0e4fc29fe910bd4d983e8bb902f4c25e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2429777, "upload_time": "2015-03-01T08:18:11", "upload_time_iso_8601": "2015-03-01T08:18:11.860993Z", "url": "https://files.pythonhosted.org/packages/f5/32/01753b62a597cebd21104597f3884362022a02da90fd7367bd3f424f486d/autocomplete-0.0.103.zip", "yanked": false}], "0.0.104": [{"comment_text": "", "digests": {"md5": "23f1405633ecc0e5aa942f858306a5a1", "sha256": "e6ed33060cc46c42f0e64ed39f71488f28609951ab8cb836d2ea3eebd04de622"}, "downloads": -1, "filename": "autocomplete-0.0.104.zip", "has_sig": false, "md5_digest": "23f1405633ecc0e5aa942f858306a5a1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2429896, "upload_time": "2015-03-04T12:55:40", "upload_time_iso_8601": "2015-03-04T12:55:40.283449Z", "url": "https://files.pythonhosted.org/packages/0d/86/eca9bfe39b21d2d7cbc523ae11610a4aac1b9cfd5f9d28ab9a0ca9d1bcc7/autocomplete-0.0.104.zip", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "23f1405633ecc0e5aa942f858306a5a1", "sha256": "e6ed33060cc46c42f0e64ed39f71488f28609951ab8cb836d2ea3eebd04de622"}, "downloads": -1, "filename": "autocomplete-0.0.104.zip", "has_sig": false, "md5_digest": "23f1405633ecc0e5aa942f858306a5a1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2429896, "upload_time": "2015-03-04T12:55:40", "upload_time_iso_8601": "2015-03-04T12:55:40.283449Z", "url": "https://files.pythonhosted.org/packages/0d/86/eca9bfe39b21d2d7cbc523ae11610a4aac1b9cfd5f9d28ab9a0ca9d1bcc7/autocomplete-0.0.104.zip", "yanked": false}], "timestamp": "Thu May  7 18:16:19 2020"}