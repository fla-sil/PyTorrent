{"info": {"author": "AndreaSottana", "author_email": "", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "NLP tool for scraping text from a corpus of PDF files, embedding the sentences in the text and finding semantically similar sentences to a given search query\n====================================================================================================================================================================================\n\nThe code in this repository performs 3 main tasks.  \n- Scraping the text from a corpus of PDF files. The text is then cleaned, split into sentences, and saved into a pd.DataFrame, .csv or .parquet file containing 3 columns. One column contains the text of all the PDFs in the corpus (one sentence per row), the second column contains the title of the PDF where each sentence is taken from, and the third column contains the number of the page where each sentence is located within that PDF. This enables easy lookup.\n- Embedding all the scraped sentences in the corpus of PDFs using three different NLP models: Word2Vec (with the option to include Tf-Idf weights), ELMo and BERT. For each model, sentence-level embeddings are generated.\n- Corpus querying. This is in the form of a search tool, where the user can input a search query (one to a few words), and the tool will output the most similar sentences in the PDF corpus to the user query. This is done by comparing the embedding of the user query against all the embeddings of each sentence in the scraped corpus of PDFs. This effectively acts as a search engine. It is important that the model used to embed the user's search query matches the NLP model used to embed the PDF corpus. The default similarity metric is cosine similarity, although this can be changed by the user.\n\nProject Organization\n------------------------\n\n    \u251c\u2500\u2500 LICENSE            <- The full Licence text. This project is released under the MIT Licence.\n    \u251c\u2500\u2500 Makefile           <- Makefile with commands like `make data` or `make train`\n    \u251c\u2500\u2500 README.md          <- The top-level README for developers using this project.\n    \u251c\u2500\u2500 .envrc             <- The file containing the set up for environment variables (required if using the runner \n    \u2502                         scripts). `$PWD` should correspond to the directory where you clone this repository.\n    \u251c\u2500\u2500 .gitignore         <- The files (including data) which are not uploaded to GitHub. Edit as required.\n    \u251c\u2500\u2500 data\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 external       <- Data from third party sources.\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 interim        <- Intermediate data that has been transformed.\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 processed      <- The final, canonical data sets for modeling. This is where you cleaned datasets will be saved.\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 raw            <- The original, immutable data dump.\n    \u2502       \u2514\u2500\u2500 pdfs       <- Where your PDF files are stored.\n    \u2502\n    \u251c\u2500\u2500 models             <- Trained and serialized models. This is where your NLP models will be saved. No models have \n    \u2502                         been uploaded to GitHub.\n    \u2502\n    \u251c\u2500\u2500 notebooks          <- Jupyter notebooks. This is where you can store Jupyter Notebooks. No notebooks have been \n    \u2502                         uploaded to GitHub.\n    \u2502\n    \u251c\u2500\u2500 requirements.txt   <- The requirements file for reproducing the analysis environment.\n    \u2502\n    \u251c\u2500\u2500 setup.py           <- Makes project pip installable (pip install -e .) so pdf2emb_nlp can be imported\n    \u2502\n    \u251c\u2500\u2500 MANIFEST.in        <- Tells setup.py which package data to include and exclude.\n    \u2502\n    \u251c\u2500\u2500 config             <- This folders stores configuration files (for example suggested filenames for saving \n    \u2502   \u2502                     specific objects) that are read in by some of the runner scripts. Edit as required.\n    \u2502   \u251c\u2500\u2500 filenames.json            \n    \u2502   \u2514\u2500\u2500 words_to_replace.json\n    \u2502\n    \u251c\u2500\u2500 scripts            <- Executable scripts are saved here. They should be run in the order listed below.\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 data_processing_runner.py   \n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 embeddings_runner.py\n    \u2502   \u251c\u2500\u2500 user_search_runner.py\n    \u2502   \u2514\u2500\u2500 tmp            <- The folder where the loggers are saved (for example, debug.log, info.log, warning.log)\n    \u2502                         Logs have not been uploaded to GitHub.\n    \u2502\n    \u251c\u2500\u2500 pdf2emb_nlp        <- Source code for use in this project. See description below for how to use the files.\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py    <- Makes pdf2emb_nlp a Python module\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 arrange_text.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 embedder.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 json_creator.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 logging.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 process_user_queries.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 scraper.py\n    \u2502\n    \u251c\u2500\u2500 tests              <- Unit tests for all functions and methods defined in all modules within the pdf2emb_nlp folder, to                \n    \u2502   \u2502                     be run using pytest. It also includes an end-to-end test. These should not be modified by\n    \u2502   \u2502                     the user.  \n    \u2502   \u251c\u2500\u2500 conftest.py\n    \u2502   \u251c\u2500\u2500 end_to_end_test.py\n    \u2502   \u251c\u2500\u2500 test_arrange_text.py\n    \u2502   \u251c\u2500\u2500 test_embedder.py\n    \u2502   \u251c\u2500\u2500 test_process_user_queries.py\n    \u2502   \u251c\u2500\u2500 test_scraper.py\n    \u2502   \u2514\u2500\u2500 fixtures       <- This folder contains all the pytest fixtures required to run the tests. These should not\n    \u2502       \u2502                 be modified by the user.                \n    \u2502       \u251c\u2500\u2500 dummy_embeddings.npy\n    \u2502       \u251c\u2500\u2500 dummy_sentences.csv\n    \u2502       \u251c\u2500\u2500 dummy_sentences.parquet\n    \u2502       \u251c\u2500\u2500 dummy_sentences.txt\n    \u2502       \u251c\u2500\u2500 expected_bert_embeddings.npy\n    \u2502       \u251c\u2500\u2500 expected_elmo_embeddings.npy\n    \u2502       \u251c\u2500\u2500  expected_tfidf_scores.json\n    \u2502       \u251c\u2500\u2500 expected_w2v_embeddings_tfidf_false.npy\n    \u2502       \u251c\u2500\u2500 expected_w2v_embeddings_tfidf_true.npy\n    \u2502       \u251c\u2500\u2500 full_df_with_embeddings.parquet.gzip\n    \u2502       \u251c\u2500\u2500 test_pdf_1.pdf\n    \u2502       \u251c\u2500\u2500 test_pdf_2.pdf\n    \u2502       \u251c\u2500\u2500 tfidf_vectorizer.pickle\n    \u2502       \u251c\u2500\u2500 word2vec.pickle\n    \u2502       \u251c\u2500\u2500 word2vec_tfidf.pickle\n    \u2502       \u2514\u2500\u2500 words_to_replace.json\n    \u2502\n    \u251c\u2500\u2500 test_environment.py\n    \u2514\u2500\u2500 tox.ini            <- tox file with settings for running tox; see tox.testrun.org\n\n\n--------\n\nClone this repository from GitHub, or install this project by running \n```\n$ pip install pdf2emb_nlp\n```\non the terminal command (or `pip3`, as appropriate). \nIf you have cloned it from GitHub, you can run a test to ensure your environment is properly set-up. This project has \nnot been tested on versions of Python older than 3.6, and some versions of the `numpy` older than 1.17 are also known \nto cause issues. Please run the following line in your terminal\n```\n$ PYTHONHASHSEED=123 python3 -m pytest\n```\nThere should be 32 tests. If they all pass, you're good to start using this package. If some of the tests fail, please \ncheck your environment. This project has only been tested with the environment as described in the `requirements.txt` \nfile. Note that the environment variable \n`PYTHONHASHSEED` must be set to `\"123\"` while running the tests, to ensure deterministic reproducibility of the Word2Vec \nmodels. Two tests will fail if this is not set up correctly.\n\nEach module has been fully documented.  \nBefore you start, please configure your environment variables according to your own directory path. Please refer to\nthe `.envrc` file, where `$PWD` corresponds to the directory path where this repository has been cloned.\n\nIn order to scrape the text from a corpus of PDF files, you will need to save your PDFs in the folder (`~/data/raw/pdfs`). \nYou can make use of the `data_processing_runner.py` script to scrape the PDFs, clean the text, split all the text into \nsentences,  and save this into a .csv file. The script imports the two modules `scraper` and `arrange_text`.\n```python\nimport os\nimport yaml\nimport logging.config\nfrom pdf2emb_nlp.scraper import DocumentScraper\nfrom pdf2emb_nlp.arrange_text import CorpusGenerator\n\n\nif __name__ == \"__main__\":\n    DATA_DIR = os.getenv('DATA_DIR')\n    CONFIG_DIR = os.getenv('CONFIG_DIR')\n    LOGGING_CONFIG = os.getenv('LOGGING_CONFIG')\n\n    with open(LOGGING_CONFIG, 'r') as f:\n        config = yaml.safe_load(f)\n    logging.config.dictConfig(config)\n\n    pdfs_folder = os.path.join(DATA_DIR, 'raw', 'pdfs')\n    json_path = os.path.join(CONFIG_DIR, 'words_to_replace.json')\n    scraper = DocumentScraper(pdfs_folder, json_path)\n    df_by_page = scraper.document_corpus_to_pandas_df()\n    generator = CorpusGenerator(df_by_page)\n    df_by_sentence = generator.df_by_page_to_df_by_sentence()\n\n    df_by_page.to_csv(os.path.join(DATA_DIR, 'processed', 'corpus_by_page.csv'))  # optional, for reference\n    df_by_sentence.to_csv(os.path.join(DATA_DIR, 'processed', 'corpus_by_sentence.csv'), index=False)\n\n```\nThe file `words_to_replace.json` in the `config` folder is used for ad-hoc text cleaning. When running\n`scraper.document_corpus_to_pandas_df()`, the json is deserialised into a python dictionary, and the corpus text will be\ncleaned by replacing each key in this dictionary with its value. In order to modify and customize the content of this\njson file, run the script `pdf2emb_nlp/json_creator.py` and adapt it as necessary.\n\nOnce you have created a file `corpus_by_sentence.csv`, you can embed the sentences in this file using your model of choice out of \nWord2Vec (with the option to include Tf-Idf weights), ELMo and BERT. For each model, sentence-level embeddings are \ngenerated. Where the original model would generate word-level embeddings, sentence-level embeddings have been created by \naveraging all the word embeddings of the respective sentence. The `embeddings_runner.py` script is an example of how you \ncould run all 4 NLP models and save them separately. It imports the `embedder` module.\n\n```python\nimport os\nimport json\nimport yaml\nimport logging.config\nimport pandas as pd\nfrom pdf2emb_nlp.embedder import Embedder\n\n\nmodels_to_be_run = [\n    'Word2Vec_tfidf_weighted',  # comment out as needed\n    'Word2Vec',\n    'BERT',\n    'ELMo',\n]\n\n\nif __name__ == '__main__':\n    DATA_DIR = os.getenv('DATA_DIR')\n    MODELS_DIR = os.getenv('MODELS_DIR')\n    CONFIG_DIR = os.getenv('CONFIG_DIR')\n    LOGGING_CONFIG = os.getenv('LOGGING_CONFIG')\n\n    with open(LOGGING_CONFIG, 'r') as f:\n        config = yaml.safe_load(f)\n    logging.config.dictConfig(config)\n\n    with open(os.path.join(CONFIG_DIR, 'filenames.json'), 'r') as f:\n        file_names = json.load(f)\n\n    corpus_filename = \"corpus_by_sentence.csv\"\n    corpus_by_sentence = pd.read_csv(os.path.join(DATA_DIR, \"processed\", corpus_filename))\n    list_of_sentences = corpus_by_sentence['sentence'].values.tolist()\n    print(\"Instantiating Embedder class.\")\n    embedder = Embedder(list_of_sentences)\n\n    for model in models_to_be_run:\n        print(f\"Calculating {model} embeddings.\")\n        if model == 'Word2Vec_tfidf_weighted':\n            sentence_embeddings, model_obj, tfidf_vectorizer = embedder.compute_word2vec_embeddings(tfidf_weights=True)\n            embedder.save_model(tfidf_vectorizer, MODELS_DIR, file_names[model]['vectorizer_filename'])\n            # the line above is specific to Word2Vec with TfIdf vectorizer and cannot be generalized to other models\n        elif model == 'Word2Vec':\n            sentence_embeddings, model_obj, _ = embedder.compute_word2vec_embeddings(tfidf_weights=False)\n        elif model == 'BERT':\n            bert_model = 'bert-base-nli-stsb-mean-tokens'  # This line is specific to BERT\n            sentence_embeddings, model_obj = embedder.compute_bert_embeddings(bert_model)\n        elif model == 'ELMo':\n            sentence_embeddings, model_obj = embedder.compute_elmo_embeddings()\n        else:\n            raise KeyError(f'The model {model} is not recognized as input.')\n        print(f\"{model} embeddings calculated. Saving model.\")\n        embedder.save_embeddings(sentence_embeddings, MODELS_DIR, file_names[model]['embeddings_filename'])\n        embedder.save_model(model_obj, MODELS_DIR, file_names[model]['model_filename'])\n        print(f\"{model} model saved. Saving .parquet file.\")\n        df = embedder.add_embeddings_to_corpus_df(\n            os.path.join(DATA_DIR, \"processed\", corpus_filename), sentence_embeddings, file_names[model]['column_name']\n        )\n        embedder.df_to_parquet(df, os.path.join(DATA_DIR, \"processed\", file_names[model]['parquet_filename']))\n        print(f\"Parquet file saved. All steps done for the {model} model.\")\n\n```\nEach model has been saved as a `.pickle` file in the `models` folder, each model's embeddings as a `.npy` file in the \n`models` folder, and each pd.DataFrame as a `.parquet` file in the `data/processed` folder. Each `.parquet` file \ncontains the same data as the `corpus_by_sentence.csv` file previously saved, with an added column, representing the sentence embeddings for the \nchosen model. A separate `.parquet` has been saved for each model, although the user may modify the script above to save\nall models' embeddings in the same `.parquet` file. The file names of the `.pickle`, `.npy` and `.parquet` files are\nstored in the `filenames.json ` in the `config` folder. In order to modify and customize these names, run the script \n`pdf2emb_nlp/json_creator.py` and adapt it as necessary.\n\nFinally, in order to search through your corpus of PDF files given a *user search query* (which can be a single word or a \nfew words), run the `user_search_runner.py` script in the `scripts` folder, which imports the `process_user_queries` module:\n```python\nimport os\nimport yaml\nimport json\nimport logging.config\nfrom pdf2emb_nlp.process_user_queries import query_embeddings\n\n\nif __name__ == '__main__':\n    user_search_input = 'cell phone'\n    model_name = 'BERT'  # change as appropriate\n    DATA_DIR = os.getenv(\"DATA_DIR\")\n    CONFIG_DIR = os.getenv('CONFIG_DIR')\n    MODELS_DIR = os.getenv(\"MODELS_DIR\")\n    LOGGING_CONFIG = os.getenv(\"LOGGING_CONFIG\")\n    with open(LOGGING_CONFIG, 'r') as f:\n        config = yaml.safe_load(f)\n    logging.config.dictConfig(config)\n    with open(os.path.join(CONFIG_DIR, 'filenames.json'), 'r') as f:\n        file_names = json.load(f)\n\n    tfidf_vectorizer = os.path.join(MODELS_DIR, \"tfidf_vectorizer.pickle\")\n\n    model = os.path.join(MODELS_DIR, file_names[model_name][\"model_filename\"])  # this is optional for ELMo and BERT.\n    trained_df_path = os.path.join(DATA_DIR, 'processed', file_names[model_name][\"parquet_filename\"])\n    user_input_embedding, trained_df = query_embeddings(\n        user_search_input, trained_df_path, file_names[model_name][\"column_name\"], model_name, model,\n        distance_metric='cosine', tfidf_vectorizer=tfidf_vectorizer\n    )\n    # tfidf_vectorizer is not used (and optional) when model is not 'Word2Vec_TfIdf_weighted'\n    if user_input_embedding.size and not trained_df.empty:  # they must not be empty\n        print(trained_df.sort_values('metric_distance', ascending=True)[['sentence', 'metric_distance']].\n              reset_index(drop=True).head(10))\n```\n\nAt this point, the `user_input_embedding` is the embedding of the user search query, and `trained_df` is the \npd.DataFrame containing a column with the metric distance between the user embedding and each individual sentence \nembedding in the corpus (default metric: cosine similarity). If you want to visualise the most similar sentences to the\nuser search query, you can simply sort the pd.DataFrame by its `metric_distance` column.\n```python\nprint(trained_df.sort_values('metric_distance', ascending=True)[['sentence', 'metric_distance']].\n              reset_index(drop=True)\n```\n\n<p><small>Project description adapted from the <a target=\"_blank\" href=\"https://drivendata.github.io/cookiecutter-data-science/\">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://pypi.python.org/pypi/pdf2emb_nlp", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/AndreaSottana/pdf2emb_nlp", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "pdf2emb-nlp", "package_url": "https://pypi.org/project/pdf2emb-nlp/", "platform": "", "project_url": "https://pypi.org/project/pdf2emb-nlp/", "project_urls": {"Download": "https://pypi.python.org/pypi/pdf2emb_nlp", "Homepage": "https://github.com/AndreaSottana/pdf2emb_nlp"}, "release_url": "https://pypi.org/project/pdf2emb-nlp/0.1.3/", "requires_dist": ["allennlp (==0.9.0)", "gensim (==3.8.1)", "nltk (==3.4.5)", "numpy (==1.18.2)", "pandas (==0.25.3)", "pytest (==5.4.1)", "scikit-learn (==0.22.1)", "scipy (==1.4.1)", "sentence-transformers (==0.2.5.1)", "slate3k (==0.5.3)", "typing (==3.7.4.1)", "tqdm (==4.45.0)"], "requires_python": ">=3.6", "summary": "NLP tool for scraping text from a corpus of PDF files, embedding the sentences in the text and finding semantically similar sentences to a given search query", "version": "0.1.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>NLP tool for scraping text from a corpus of PDF files, embedding the sentences in the text and finding semantically similar sentences to a given search query</h1>\n<p>The code in this repository performs 3 main tasks.</p>\n<ul>\n<li>Scraping the text from a corpus of PDF files. The text is then cleaned, split into sentences, and saved into a pd.DataFrame, .csv or .parquet file containing 3 columns. One column contains the text of all the PDFs in the corpus (one sentence per row), the second column contains the title of the PDF where each sentence is taken from, and the third column contains the number of the page where each sentence is located within that PDF. This enables easy lookup.</li>\n<li>Embedding all the scraped sentences in the corpus of PDFs using three different NLP models: Word2Vec (with the option to include Tf-Idf weights), ELMo and BERT. For each model, sentence-level embeddings are generated.</li>\n<li>Corpus querying. This is in the form of a search tool, where the user can input a search query (one to a few words), and the tool will output the most similar sentences in the PDF corpus to the user query. This is done by comparing the embedding of the user query against all the embeddings of each sentence in the scraped corpus of PDFs. This effectively acts as a search engine. It is important that the model used to embed the user's search query matches the NLP model used to embed the PDF corpus. The default similarity metric is cosine similarity, although this can be changed by the user.</li>\n</ul>\n<h2>Project Organization</h2>\n<pre><code>\u251c\u2500\u2500 LICENSE            &lt;- The full Licence text. This project is released under the MIT Licence.\n\u251c\u2500\u2500 Makefile           &lt;- Makefile with commands like `make data` or `make train`\n\u251c\u2500\u2500 README.md          &lt;- The top-level README for developers using this project.\n\u251c\u2500\u2500 .envrc             &lt;- The file containing the set up for environment variables (required if using the runner \n\u2502                         scripts). `$PWD` should correspond to the directory where you clone this repository.\n\u251c\u2500\u2500 .gitignore         &lt;- The files (including data) which are not uploaded to GitHub. Edit as required.\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 external       &lt;- Data from third party sources.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 interim        &lt;- Intermediate data that has been transformed.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 processed      &lt;- The final, canonical data sets for modeling. This is where you cleaned datasets will be saved.\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 raw            &lt;- The original, immutable data dump.\n\u2502       \u2514\u2500\u2500 pdfs       &lt;- Where your PDF files are stored.\n\u2502\n\u251c\u2500\u2500 models             &lt;- Trained and serialized models. This is where your NLP models will be saved. No models have \n\u2502                         been uploaded to GitHub.\n\u2502\n\u251c\u2500\u2500 notebooks          &lt;- Jupyter notebooks. This is where you can store Jupyter Notebooks. No notebooks have been \n\u2502                         uploaded to GitHub.\n\u2502\n\u251c\u2500\u2500 requirements.txt   &lt;- The requirements file for reproducing the analysis environment.\n\u2502\n\u251c\u2500\u2500 setup.py           &lt;- Makes project pip installable (pip install -e .) so pdf2emb_nlp can be imported\n\u2502\n\u251c\u2500\u2500 MANIFEST.in        &lt;- Tells setup.py which package data to include and exclude.\n\u2502\n\u251c\u2500\u2500 config             &lt;- This folders stores configuration files (for example suggested filenames for saving \n\u2502   \u2502                     specific objects) that are read in by some of the runner scripts. Edit as required.\n\u2502   \u251c\u2500\u2500 filenames.json            \n\u2502   \u2514\u2500\u2500 words_to_replace.json\n\u2502\n\u251c\u2500\u2500 scripts            &lt;- Executable scripts are saved here. They should be run in the order listed below.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data_processing_runner.py   \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 embeddings_runner.py\n\u2502   \u251c\u2500\u2500 user_search_runner.py\n\u2502   \u2514\u2500\u2500 tmp            &lt;- The folder where the loggers are saved (for example, debug.log, info.log, warning.log)\n\u2502                         Logs have not been uploaded to GitHub.\n\u2502\n\u251c\u2500\u2500 pdf2emb_nlp        &lt;- Source code for use in this project. See description below for how to use the files.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py    &lt;- Makes pdf2emb_nlp a Python module\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 arrange_text.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 embedder.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 json_creator.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 logging.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 process_user_queries.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 scraper.py\n\u2502\n\u251c\u2500\u2500 tests              &lt;- Unit tests for all functions and methods defined in all modules within the pdf2emb_nlp folder, to                \n\u2502   \u2502                     be run using pytest. It also includes an end-to-end test. These should not be modified by\n\u2502   \u2502                     the user.  \n\u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u251c\u2500\u2500 end_to_end_test.py\n\u2502   \u251c\u2500\u2500 test_arrange_text.py\n\u2502   \u251c\u2500\u2500 test_embedder.py\n\u2502   \u251c\u2500\u2500 test_process_user_queries.py\n\u2502   \u251c\u2500\u2500 test_scraper.py\n\u2502   \u2514\u2500\u2500 fixtures       &lt;- This folder contains all the pytest fixtures required to run the tests. These should not\n\u2502       \u2502                 be modified by the user.                \n\u2502       \u251c\u2500\u2500 dummy_embeddings.npy\n\u2502       \u251c\u2500\u2500 dummy_sentences.csv\n\u2502       \u251c\u2500\u2500 dummy_sentences.parquet\n\u2502       \u251c\u2500\u2500 dummy_sentences.txt\n\u2502       \u251c\u2500\u2500 expected_bert_embeddings.npy\n\u2502       \u251c\u2500\u2500 expected_elmo_embeddings.npy\n\u2502       \u251c\u2500\u2500  expected_tfidf_scores.json\n\u2502       \u251c\u2500\u2500 expected_w2v_embeddings_tfidf_false.npy\n\u2502       \u251c\u2500\u2500 expected_w2v_embeddings_tfidf_true.npy\n\u2502       \u251c\u2500\u2500 full_df_with_embeddings.parquet.gzip\n\u2502       \u251c\u2500\u2500 test_pdf_1.pdf\n\u2502       \u251c\u2500\u2500 test_pdf_2.pdf\n\u2502       \u251c\u2500\u2500 tfidf_vectorizer.pickle\n\u2502       \u251c\u2500\u2500 word2vec.pickle\n\u2502       \u251c\u2500\u2500 word2vec_tfidf.pickle\n\u2502       \u2514\u2500\u2500 words_to_replace.json\n\u2502\n\u251c\u2500\u2500 test_environment.py\n\u2514\u2500\u2500 tox.ini            &lt;- tox file with settings for running tox; see tox.testrun.org\n</code></pre>\n<hr>\n<p>Clone this repository from GitHub, or install this project by running</p>\n<pre><code>$ pip install pdf2emb_nlp\n</code></pre>\n<p>on the terminal command (or <code>pip3</code>, as appropriate).\nIf you have cloned it from GitHub, you can run a test to ensure your environment is properly set-up. This project has\nnot been tested on versions of Python older than 3.6, and some versions of the <code>numpy</code> older than 1.17 are also known\nto cause issues. Please run the following line in your terminal</p>\n<pre><code>$ PYTHONHASHSEED=123 python3 -m pytest\n</code></pre>\n<p>There should be 32 tests. If they all pass, you're good to start using this package. If some of the tests fail, please\ncheck your environment. This project has only been tested with the environment as described in the <code>requirements.txt</code>\nfile. Note that the environment variable\n<code>PYTHONHASHSEED</code> must be set to <code>\"123\"</code> while running the tests, to ensure deterministic reproducibility of the Word2Vec\nmodels. Two tests will fail if this is not set up correctly.</p>\n<p>Each module has been fully documented.<br>\nBefore you start, please configure your environment variables according to your own directory path. Please refer to\nthe <code>.envrc</code> file, where <code>$PWD</code> corresponds to the directory path where this repository has been cloned.</p>\n<p>In order to scrape the text from a corpus of PDF files, you will need to save your PDFs in the folder (<code>~/data/raw/pdfs</code>).\nYou can make use of the <code>data_processing_runner.py</code> script to scrape the PDFs, clean the text, split all the text into\nsentences,  and save this into a .csv file. The script imports the two modules <code>scraper</code> and <code>arrange_text</code>.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"kn\">import</span> <span class=\"nn\">yaml</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging.config</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pdf2emb_nlp.scraper</span> <span class=\"kn\">import</span> <span class=\"n\">DocumentScraper</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pdf2emb_nlp.arrange_text</span> <span class=\"kn\">import</span> <span class=\"n\">CorpusGenerator</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">\"__main__\"</span><span class=\"p\">:</span>\n    <span class=\"n\">DATA_DIR</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getenv</span><span class=\"p\">(</span><span class=\"s1\">'DATA_DIR'</span><span class=\"p\">)</span>\n    <span class=\"n\">CONFIG_DIR</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getenv</span><span class=\"p\">(</span><span class=\"s1\">'CONFIG_DIR'</span><span class=\"p\">)</span>\n    <span class=\"n\">LOGGING_CONFIG</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getenv</span><span class=\"p\">(</span><span class=\"s1\">'LOGGING_CONFIG'</span><span class=\"p\">)</span>\n\n    <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">LOGGING_CONFIG</span><span class=\"p\">,</span> <span class=\"s1\">'r'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"n\">yaml</span><span class=\"o\">.</span><span class=\"n\">safe_load</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span>\n    <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">dictConfig</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">)</span>\n\n    <span class=\"n\">pdfs_folder</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">DATA_DIR</span><span class=\"p\">,</span> <span class=\"s1\">'raw'</span><span class=\"p\">,</span> <span class=\"s1\">'pdfs'</span><span class=\"p\">)</span>\n    <span class=\"n\">json_path</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">CONFIG_DIR</span><span class=\"p\">,</span> <span class=\"s1\">'words_to_replace.json'</span><span class=\"p\">)</span>\n    <span class=\"n\">scraper</span> <span class=\"o\">=</span> <span class=\"n\">DocumentScraper</span><span class=\"p\">(</span><span class=\"n\">pdfs_folder</span><span class=\"p\">,</span> <span class=\"n\">json_path</span><span class=\"p\">)</span>\n    <span class=\"n\">df_by_page</span> <span class=\"o\">=</span> <span class=\"n\">scraper</span><span class=\"o\">.</span><span class=\"n\">document_corpus_to_pandas_df</span><span class=\"p\">()</span>\n    <span class=\"n\">generator</span> <span class=\"o\">=</span> <span class=\"n\">CorpusGenerator</span><span class=\"p\">(</span><span class=\"n\">df_by_page</span><span class=\"p\">)</span>\n    <span class=\"n\">df_by_sentence</span> <span class=\"o\">=</span> <span class=\"n\">generator</span><span class=\"o\">.</span><span class=\"n\">df_by_page_to_df_by_sentence</span><span class=\"p\">()</span>\n\n    <span class=\"n\">df_by_page</span><span class=\"o\">.</span><span class=\"n\">to_csv</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">DATA_DIR</span><span class=\"p\">,</span> <span class=\"s1\">'processed'</span><span class=\"p\">,</span> <span class=\"s1\">'corpus_by_page.csv'</span><span class=\"p\">))</span>  <span class=\"c1\"># optional, for reference</span>\n    <span class=\"n\">df_by_sentence</span><span class=\"o\">.</span><span class=\"n\">to_csv</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">DATA_DIR</span><span class=\"p\">,</span> <span class=\"s1\">'processed'</span><span class=\"p\">,</span> <span class=\"s1\">'corpus_by_sentence.csv'</span><span class=\"p\">),</span> <span class=\"n\">index</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n</pre>\n<p>The file <code>words_to_replace.json</code> in the <code>config</code> folder is used for ad-hoc text cleaning. When running\n<code>scraper.document_corpus_to_pandas_df()</code>, the json is deserialised into a python dictionary, and the corpus text will be\ncleaned by replacing each key in this dictionary with its value. In order to modify and customize the content of this\njson file, run the script <code>pdf2emb_nlp/json_creator.py</code> and adapt it as necessary.</p>\n<p>Once you have created a file <code>corpus_by_sentence.csv</code>, you can embed the sentences in this file using your model of choice out of\nWord2Vec (with the option to include Tf-Idf weights), ELMo and BERT. For each model, sentence-level embeddings are\ngenerated. Where the original model would generate word-level embeddings, sentence-level embeddings have been created by\naveraging all the word embeddings of the respective sentence. The <code>embeddings_runner.py</code> script is an example of how you\ncould run all 4 NLP models and save them separately. It imports the <code>embedder</code> module.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"kn\">import</span> <span class=\"nn\">yaml</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging.config</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pdf2emb_nlp.embedder</span> <span class=\"kn\">import</span> <span class=\"n\">Embedder</span>\n\n\n<span class=\"n\">models_to_be_run</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"s1\">'Word2Vec_tfidf_weighted'</span><span class=\"p\">,</span>  <span class=\"c1\"># comment out as needed</span>\n    <span class=\"s1\">'Word2Vec'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'BERT'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'ELMo'</span><span class=\"p\">,</span>\n<span class=\"p\">]</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">'__main__'</span><span class=\"p\">:</span>\n    <span class=\"n\">DATA_DIR</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getenv</span><span class=\"p\">(</span><span class=\"s1\">'DATA_DIR'</span><span class=\"p\">)</span>\n    <span class=\"n\">MODELS_DIR</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getenv</span><span class=\"p\">(</span><span class=\"s1\">'MODELS_DIR'</span><span class=\"p\">)</span>\n    <span class=\"n\">CONFIG_DIR</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getenv</span><span class=\"p\">(</span><span class=\"s1\">'CONFIG_DIR'</span><span class=\"p\">)</span>\n    <span class=\"n\">LOGGING_CONFIG</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getenv</span><span class=\"p\">(</span><span class=\"s1\">'LOGGING_CONFIG'</span><span class=\"p\">)</span>\n\n    <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">LOGGING_CONFIG</span><span class=\"p\">,</span> <span class=\"s1\">'r'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"n\">yaml</span><span class=\"o\">.</span><span class=\"n\">safe_load</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span>\n    <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">dictConfig</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">)</span>\n\n    <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">CONFIG_DIR</span><span class=\"p\">,</span> <span class=\"s1\">'filenames.json'</span><span class=\"p\">),</span> <span class=\"s1\">'r'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"n\">file_names</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span>\n\n    <span class=\"n\">corpus_filename</span> <span class=\"o\">=</span> <span class=\"s2\">\"corpus_by_sentence.csv\"</span>\n    <span class=\"n\">corpus_by_sentence</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">DATA_DIR</span><span class=\"p\">,</span> <span class=\"s2\">\"processed\"</span><span class=\"p\">,</span> <span class=\"n\">corpus_filename</span><span class=\"p\">))</span>\n    <span class=\"n\">list_of_sentences</span> <span class=\"o\">=</span> <span class=\"n\">corpus_by_sentence</span><span class=\"p\">[</span><span class=\"s1\">'sentence'</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Instantiating Embedder class.\"</span><span class=\"p\">)</span>\n    <span class=\"n\">embedder</span> <span class=\"o\">=</span> <span class=\"n\">Embedder</span><span class=\"p\">(</span><span class=\"n\">list_of_sentences</span><span class=\"p\">)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">model</span> <span class=\"ow\">in</span> <span class=\"n\">models_to_be_run</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"Calculating </span><span class=\"si\">{</span><span class=\"n\">model</span><span class=\"si\">}</span><span class=\"s2\"> embeddings.\"</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"n\">model</span> <span class=\"o\">==</span> <span class=\"s1\">'Word2Vec_tfidf_weighted'</span><span class=\"p\">:</span>\n            <span class=\"n\">sentence_embeddings</span><span class=\"p\">,</span> <span class=\"n\">model_obj</span><span class=\"p\">,</span> <span class=\"n\">tfidf_vectorizer</span> <span class=\"o\">=</span> <span class=\"n\">embedder</span><span class=\"o\">.</span><span class=\"n\">compute_word2vec_embeddings</span><span class=\"p\">(</span><span class=\"n\">tfidf_weights</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n            <span class=\"n\">embedder</span><span class=\"o\">.</span><span class=\"n\">save_model</span><span class=\"p\">(</span><span class=\"n\">tfidf_vectorizer</span><span class=\"p\">,</span> <span class=\"n\">MODELS_DIR</span><span class=\"p\">,</span> <span class=\"n\">file_names</span><span class=\"p\">[</span><span class=\"n\">model</span><span class=\"p\">][</span><span class=\"s1\">'vectorizer_filename'</span><span class=\"p\">])</span>\n            <span class=\"c1\"># the line above is specific to Word2Vec with TfIdf vectorizer and cannot be generalized to other models</span>\n        <span class=\"k\">elif</span> <span class=\"n\">model</span> <span class=\"o\">==</span> <span class=\"s1\">'Word2Vec'</span><span class=\"p\">:</span>\n            <span class=\"n\">sentence_embeddings</span><span class=\"p\">,</span> <span class=\"n\">model_obj</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">embedder</span><span class=\"o\">.</span><span class=\"n\">compute_word2vec_embeddings</span><span class=\"p\">(</span><span class=\"n\">tfidf_weights</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n        <span class=\"k\">elif</span> <span class=\"n\">model</span> <span class=\"o\">==</span> <span class=\"s1\">'BERT'</span><span class=\"p\">:</span>\n            <span class=\"n\">bert_model</span> <span class=\"o\">=</span> <span class=\"s1\">'bert-base-nli-stsb-mean-tokens'</span>  <span class=\"c1\"># This line is specific to BERT</span>\n            <span class=\"n\">sentence_embeddings</span><span class=\"p\">,</span> <span class=\"n\">model_obj</span> <span class=\"o\">=</span> <span class=\"n\">embedder</span><span class=\"o\">.</span><span class=\"n\">compute_bert_embeddings</span><span class=\"p\">(</span><span class=\"n\">bert_model</span><span class=\"p\">)</span>\n        <span class=\"k\">elif</span> <span class=\"n\">model</span> <span class=\"o\">==</span> <span class=\"s1\">'ELMo'</span><span class=\"p\">:</span>\n            <span class=\"n\">sentence_embeddings</span><span class=\"p\">,</span> <span class=\"n\">model_obj</span> <span class=\"o\">=</span> <span class=\"n\">embedder</span><span class=\"o\">.</span><span class=\"n\">compute_elmo_embeddings</span><span class=\"p\">()</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"k\">raise</span> <span class=\"ne\">KeyError</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s1\">'The model </span><span class=\"si\">{</span><span class=\"n\">model</span><span class=\"si\">}</span><span class=\"s1\"> is not recognized as input.'</span><span class=\"p\">)</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"</span><span class=\"si\">{</span><span class=\"n\">model</span><span class=\"si\">}</span><span class=\"s2\"> embeddings calculated. Saving model.\"</span><span class=\"p\">)</span>\n        <span class=\"n\">embedder</span><span class=\"o\">.</span><span class=\"n\">save_embeddings</span><span class=\"p\">(</span><span class=\"n\">sentence_embeddings</span><span class=\"p\">,</span> <span class=\"n\">MODELS_DIR</span><span class=\"p\">,</span> <span class=\"n\">file_names</span><span class=\"p\">[</span><span class=\"n\">model</span><span class=\"p\">][</span><span class=\"s1\">'embeddings_filename'</span><span class=\"p\">])</span>\n        <span class=\"n\">embedder</span><span class=\"o\">.</span><span class=\"n\">save_model</span><span class=\"p\">(</span><span class=\"n\">model_obj</span><span class=\"p\">,</span> <span class=\"n\">MODELS_DIR</span><span class=\"p\">,</span> <span class=\"n\">file_names</span><span class=\"p\">[</span><span class=\"n\">model</span><span class=\"p\">][</span><span class=\"s1\">'model_filename'</span><span class=\"p\">])</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"</span><span class=\"si\">{</span><span class=\"n\">model</span><span class=\"si\">}</span><span class=\"s2\"> model saved. Saving .parquet file.\"</span><span class=\"p\">)</span>\n        <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">embedder</span><span class=\"o\">.</span><span class=\"n\">add_embeddings_to_corpus_df</span><span class=\"p\">(</span>\n            <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">DATA_DIR</span><span class=\"p\">,</span> <span class=\"s2\">\"processed\"</span><span class=\"p\">,</span> <span class=\"n\">corpus_filename</span><span class=\"p\">),</span> <span class=\"n\">sentence_embeddings</span><span class=\"p\">,</span> <span class=\"n\">file_names</span><span class=\"p\">[</span><span class=\"n\">model</span><span class=\"p\">][</span><span class=\"s1\">'column_name'</span><span class=\"p\">]</span>\n        <span class=\"p\">)</span>\n        <span class=\"n\">embedder</span><span class=\"o\">.</span><span class=\"n\">df_to_parquet</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">DATA_DIR</span><span class=\"p\">,</span> <span class=\"s2\">\"processed\"</span><span class=\"p\">,</span> <span class=\"n\">file_names</span><span class=\"p\">[</span><span class=\"n\">model</span><span class=\"p\">][</span><span class=\"s1\">'parquet_filename'</span><span class=\"p\">]))</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"Parquet file saved. All steps done for the </span><span class=\"si\">{</span><span class=\"n\">model</span><span class=\"si\">}</span><span class=\"s2\"> model.\"</span><span class=\"p\">)</span>\n</pre>\n<p>Each model has been saved as a <code>.pickle</code> file in the <code>models</code> folder, each model's embeddings as a <code>.npy</code> file in the\n<code>models</code> folder, and each pd.DataFrame as a <code>.parquet</code> file in the <code>data/processed</code> folder. Each <code>.parquet</code> file\ncontains the same data as the <code>corpus_by_sentence.csv</code> file previously saved, with an added column, representing the sentence embeddings for the\nchosen model. A separate <code>.parquet</code> has been saved for each model, although the user may modify the script above to save\nall models' embeddings in the same <code>.parquet</code> file. The file names of the <code>.pickle</code>, <code>.npy</code> and <code>.parquet</code> files are\nstored in the <code>filenames.json</code> in the <code>config</code> folder. In order to modify and customize these names, run the script\n<code>pdf2emb_nlp/json_creator.py</code> and adapt it as necessary.</p>\n<p>Finally, in order to search through your corpus of PDF files given a <em>user search query</em> (which can be a single word or a\nfew words), run the <code>user_search_runner.py</code> script in the <code>scripts</code> folder, which imports the <code>process_user_queries</code> module:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"kn\">import</span> <span class=\"nn\">yaml</span>\n<span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging.config</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pdf2emb_nlp.process_user_queries</span> <span class=\"kn\">import</span> <span class=\"n\">query_embeddings</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">'__main__'</span><span class=\"p\">:</span>\n    <span class=\"n\">user_search_input</span> <span class=\"o\">=</span> <span class=\"s1\">'cell phone'</span>\n    <span class=\"n\">model_name</span> <span class=\"o\">=</span> <span class=\"s1\">'BERT'</span>  <span class=\"c1\"># change as appropriate</span>\n    <span class=\"n\">DATA_DIR</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getenv</span><span class=\"p\">(</span><span class=\"s2\">\"DATA_DIR\"</span><span class=\"p\">)</span>\n    <span class=\"n\">CONFIG_DIR</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getenv</span><span class=\"p\">(</span><span class=\"s1\">'CONFIG_DIR'</span><span class=\"p\">)</span>\n    <span class=\"n\">MODELS_DIR</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getenv</span><span class=\"p\">(</span><span class=\"s2\">\"MODELS_DIR\"</span><span class=\"p\">)</span>\n    <span class=\"n\">LOGGING_CONFIG</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getenv</span><span class=\"p\">(</span><span class=\"s2\">\"LOGGING_CONFIG\"</span><span class=\"p\">)</span>\n    <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">LOGGING_CONFIG</span><span class=\"p\">,</span> <span class=\"s1\">'r'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"n\">yaml</span><span class=\"o\">.</span><span class=\"n\">safe_load</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span>\n    <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">dictConfig</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">)</span>\n    <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">CONFIG_DIR</span><span class=\"p\">,</span> <span class=\"s1\">'filenames.json'</span><span class=\"p\">),</span> <span class=\"s1\">'r'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"n\">file_names</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span>\n\n    <span class=\"n\">tfidf_vectorizer</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">MODELS_DIR</span><span class=\"p\">,</span> <span class=\"s2\">\"tfidf_vectorizer.pickle\"</span><span class=\"p\">)</span>\n\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">MODELS_DIR</span><span class=\"p\">,</span> <span class=\"n\">file_names</span><span class=\"p\">[</span><span class=\"n\">model_name</span><span class=\"p\">][</span><span class=\"s2\">\"model_filename\"</span><span class=\"p\">])</span>  <span class=\"c1\"># this is optional for ELMo and BERT.</span>\n    <span class=\"n\">trained_df_path</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">DATA_DIR</span><span class=\"p\">,</span> <span class=\"s1\">'processed'</span><span class=\"p\">,</span> <span class=\"n\">file_names</span><span class=\"p\">[</span><span class=\"n\">model_name</span><span class=\"p\">][</span><span class=\"s2\">\"parquet_filename\"</span><span class=\"p\">])</span>\n    <span class=\"n\">user_input_embedding</span><span class=\"p\">,</span> <span class=\"n\">trained_df</span> <span class=\"o\">=</span> <span class=\"n\">query_embeddings</span><span class=\"p\">(</span>\n        <span class=\"n\">user_search_input</span><span class=\"p\">,</span> <span class=\"n\">trained_df_path</span><span class=\"p\">,</span> <span class=\"n\">file_names</span><span class=\"p\">[</span><span class=\"n\">model_name</span><span class=\"p\">][</span><span class=\"s2\">\"column_name\"</span><span class=\"p\">],</span> <span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span>\n        <span class=\"n\">distance_metric</span><span class=\"o\">=</span><span class=\"s1\">'cosine'</span><span class=\"p\">,</span> <span class=\"n\">tfidf_vectorizer</span><span class=\"o\">=</span><span class=\"n\">tfidf_vectorizer</span>\n    <span class=\"p\">)</span>\n    <span class=\"c1\"># tfidf_vectorizer is not used (and optional) when model is not 'Word2Vec_TfIdf_weighted'</span>\n    <span class=\"k\">if</span> <span class=\"n\">user_input_embedding</span><span class=\"o\">.</span><span class=\"n\">size</span> <span class=\"ow\">and</span> <span class=\"ow\">not</span> <span class=\"n\">trained_df</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">:</span>  <span class=\"c1\"># they must not be empty</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">trained_df</span><span class=\"o\">.</span><span class=\"n\">sort_values</span><span class=\"p\">(</span><span class=\"s1\">'metric_distance'</span><span class=\"p\">,</span> <span class=\"n\">ascending</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)[[</span><span class=\"s1\">'sentence'</span><span class=\"p\">,</span> <span class=\"s1\">'metric_distance'</span><span class=\"p\">]]</span><span class=\"o\">.</span>\n              <span class=\"n\">reset_index</span><span class=\"p\">(</span><span class=\"n\">drop</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">))</span>\n</pre>\n<p>At this point, the <code>user_input_embedding</code> is the embedding of the user search query, and <code>trained_df</code> is the\npd.DataFrame containing a column with the metric distance between the user embedding and each individual sentence\nembedding in the corpus (default metric: cosine similarity). If you want to visualise the most similar sentences to the\nuser search query, you can simply sort the pd.DataFrame by its <code>metric_distance</code> column.</p>\n<pre><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">trained_df</span><span class=\"o\">.</span><span class=\"n\">sort_values</span><span class=\"p\">(</span><span class=\"s1\">'metric_distance'</span><span class=\"p\">,</span> <span class=\"n\">ascending</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)[[</span><span class=\"s1\">'sentence'</span><span class=\"p\">,</span> <span class=\"s1\">'metric_distance'</span><span class=\"p\">]]</span><span class=\"o\">.</span>\n              <span class=\"n\">reset_index</span><span class=\"p\">(</span><span class=\"n\">drop</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<p>&lt;small&gt;Project description adapted from the <a href=\"https://drivendata.github.io/cookiecutter-data-science/\" rel=\"nofollow\">cookiecutter data science project template</a>. #cookiecutterdatascience&lt;/small&gt;</p>\n\n          </div>"}, "last_serial": 6955474, "releases": {"0.1.3": [{"comment_text": "", "digests": {"md5": "12c9d9acd40669a5a9a1ce935b19e869", "sha256": "5f43c889ce7dfea96fa5ae082af084de28841fa251c2296323ddb4e246c20a00"}, "downloads": -1, "filename": "pdf2emb_nlp-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "12c9d9acd40669a5a9a1ce935b19e869", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18417, "upload_time": "2020-04-05T12:23:58", "upload_time_iso_8601": "2020-04-05T12:23:58.863023Z", "url": "https://files.pythonhosted.org/packages/85/04/1a42f131122dd0b4cd25d389a8718ebf59a0e79cd6e47a32fea4ada5ac40/pdf2emb_nlp-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a471efa07b83a67ddaaf37fbaf5d72f0", "sha256": "18f10545f82ae3477d901bfa713d83798e0c3adef9b71212e99fba8e8219fb0a"}, "downloads": -1, "filename": "pdf2emb_nlp-0.1.3.tar.gz", "has_sig": false, "md5_digest": "a471efa07b83a67ddaaf37fbaf5d72f0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17777, "upload_time": "2020-04-05T12:24:01", "upload_time_iso_8601": "2020-04-05T12:24:01.415803Z", "url": "https://files.pythonhosted.org/packages/58/d2/7c9acfa13517f2bd11fb98e5437ad54666c14ea779b9b9f2c66471f0d0c7/pdf2emb_nlp-0.1.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "12c9d9acd40669a5a9a1ce935b19e869", "sha256": "5f43c889ce7dfea96fa5ae082af084de28841fa251c2296323ddb4e246c20a00"}, "downloads": -1, "filename": "pdf2emb_nlp-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "12c9d9acd40669a5a9a1ce935b19e869", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18417, "upload_time": "2020-04-05T12:23:58", "upload_time_iso_8601": "2020-04-05T12:23:58.863023Z", "url": "https://files.pythonhosted.org/packages/85/04/1a42f131122dd0b4cd25d389a8718ebf59a0e79cd6e47a32fea4ada5ac40/pdf2emb_nlp-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a471efa07b83a67ddaaf37fbaf5d72f0", "sha256": "18f10545f82ae3477d901bfa713d83798e0c3adef9b71212e99fba8e8219fb0a"}, "downloads": -1, "filename": "pdf2emb_nlp-0.1.3.tar.gz", "has_sig": false, "md5_digest": "a471efa07b83a67ddaaf37fbaf5d72f0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17777, "upload_time": "2020-04-05T12:24:01", "upload_time_iso_8601": "2020-04-05T12:24:01.415803Z", "url": "https://files.pythonhosted.org/packages/58/d2/7c9acfa13517f2bd11fb98e5437ad54666c14ea779b9b9f2c66471f0d0c7/pdf2emb_nlp-0.1.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:57:17 2020"}