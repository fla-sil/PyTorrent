{"info": {"author": "Simon Willison", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: End Users/Desktop", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Database"], "description": "# Datasette\n\n[![PyPI](https://img.shields.io/pypi/v/datasette.svg)](https://pypi.org/project/datasette/)\n[![Travis CI](https://travis-ci.org/simonw/datasette.svg?branch=master)](https://travis-ci.org/simonw/datasette)\n[![Documentation Status](https://readthedocs.org/projects/datasette/badge/?version=latest)](http://datasette.readthedocs.io/en/latest/?badge=latest)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/simonw/datasette/blob/master/LICENSE)\n\n*An instant JSON API for your SQLite databases*\n\nDatasette provides an instant, read-only JSON API for any SQLite database. It also provides tools  for packaging the database up as a Docker container and deploying that container to hosting providers such as [Zeit Now](https://zeit.co/now).\n\nGot CSV data? Use [csvs-to-sqlite](https://github.com/simonw/csvs-to-sqlite) to convert them to SQLite, then publish them with Datasette. Or try [Datasette Publish](https://publish.datasettes.com), a web app that lets you upload CSV data and deploy it using Datasette without needing to install any software.\n\nDocumentation:\u00a0http://datasette.readthedocs.io/ Examples: https://github.com/simonw/datasette/wiki/Datasettes\n\n## News\n\n* 23rd May 2018: [Datasette 0.22.1 bugfix](https://github.com/simonw/datasette/releases/tag/0.22.1) plus we now use [versioneer](https://github.com/warner/python-versioneer)\n* 20th May 2018: [Datasette 0.22: Datasette Facets](https://simonwillison.net/2018/May/20/datasette-facets)\n* 5th May 2018: [Datasette 0.21: New _shape=, new _size=, search within columns](https://github.com/simonw/datasette/releases/tag/0.21)\n* 25th April 2018: [Exploring the UK Register of Members Interests with SQL and Datasette](https://simonwillison.net/2018/Apr/25/register-members-interests/) - a tutorial describing how [register-of-members-interests.datasettes.com](https://register-of-members-interests.datasettes.com/) was built ([source code here](https://github.com/simonw/register-of-members-interests))\n* 20th April 2018: [Datasette plugins, and building a clustered map visualization](https://simonwillison.net/2018/Apr/20/datasette-plugins/) - introducing Datasette's new plugin system and [datasette-cluster-map](https://pypi.org/project/datasette-cluster-map/), a plugin for visualizing data on a map\n* 20th April 2018: [Datasette 0.20: static assets and templates for plugins](https://github.com/simonw/datasette/releases/tag/0.20)\n* 16th April 2018: [Datasette 0.19: plugins preview](https://github.com/simonw/datasette/releases/tag/0.19)\n* 14th April 2018: [Datasette 0.18: units](https://github.com/simonw/datasette/releases/tag/0.18)\n* 9th April 2018: [Datasette 0.15: sort by column](https://github.com/simonw/datasette/releases/tag/0.15)\n* 28th March 2018: [Baltimore Sun Public Salary Records](https://simonwillison.net/2018/Mar/28/datasette-in-the-wild/) - a data journalism project from the Baltimore Sun powered by Datasette - source code [is available here](https://github.com/baltimore-sun-data/salaries-datasette)\n* 27th March 2018: [Cloud-first: Rapid webapp deployment using containers](https://wwwf.imperial.ac.uk/blog/research-software-engineering/2018/03/27/cloud-first-rapid-webapp-deployment-using-containers/) - a tutorial covering deploying Datasette using Microsoft Azure by the Research Software Engineering team at Imperial College London\n* 28th January 2018: [Analyzing my Twitter followers with Datasette](https://simonwillison.net/2018/Jan/28/analyzing-my-twitter-followers/) - a tutorial on using Datasette to analyze follower data pulled from the Twitter API\n* 17th January 2018: [Datasette Publish: a web app for publishing CSV files as an online database](https://simonwillison.net/2018/Jan/17/datasette-publish/)\n* 12th December 2017: [Building a location to time zone API with SpatiaLite, OpenStreetMap and Datasette](https://simonwillison.net/2017/Dec/12/building-a-location-time-zone-api/)\n* 9th December 2017: [Datasette 0.14: customization edition](https://github.com/simonw/datasette/releases/tag/0.14)\n* 25th November 2017: [New in Datasette: filters, foreign keys and search](https://simonwillison.net/2017/Nov/25/new-in-datasette/)\n* 13th November 2017: [Datasette: instantly create and publish an API for your SQLite databases](https://simonwillison.net/2017/Nov/13/datasette/)\n\n## Installation\n\n    pip3 install datasette\n\nDatasette requires Python 3.5 or higher.\n\n## Basic usage\n\n    datasette serve path/to/database.db\n\nThis will start a web server on port 8001 - visit http://localhost:8001/ to access the web interface.\n\n`serve` is the default subcommand, you can omit it if you like.\n\nUse Chrome on OS X? You can run datasette against your browser history like so:\n\n     datasette ~/Library/Application\\ Support/Google/Chrome/Default/History\n\nNow visiting http://localhost:8001/History/downloads will show you a web interface to browse your downloads data:\n\n![Downloads table rendered by datasette](https://static.simonwillison.net/static/2017/datasette-downloads.png)\n\nhttp://localhost:8001/History/downloads.json will return that data as JSON:\n\n    {\n        \"database\": \"History\",\n        \"columns\": [\n            \"id\",\n            \"current_path\",\n            \"target_path\",\n            \"start_time\",\n            \"received_bytes\",\n            \"total_bytes\",\n            ...\n        ],\n        \"table_rows_count\": 576,\n        \"rows\": [\n            [\n                1,\n                \"/Users/simonw/Downloads/DropboxInstaller.dmg\",\n                \"/Users/simonw/Downloads/DropboxInstaller.dmg\",\n                13097290269022132,\n                626688,\n                0,\n                ...\n            ]\n        ]\n    }\n\n\nhttp://localhost:8001/History/downloads.json?_shape=objects will return that data as JSON in a more convenient but less efficient format:\n\n    {\n        ...\n        \"rows\": [\n            {\n                \"start_time\": 13097290269022132,\n                \"interrupt_reason\": 0,\n                \"hash\": \"\",\n                \"id\": 1,\n                \"site_url\": \"\",\n                \"referrer\": \"https://www.dropbox.com/downloading?src=index\",\n                ...\n            }\n        ]\n    }\n\n## datasette serve options\n\n    $ datasette serve --help\n    Usage: datasette serve [OPTIONS] [FILES]...\n\n      Serve up specified SQLite database files with a web UI\n\n    Options:\n      -h, --host TEXT              host for server, defaults to 127.0.0.1\n      -p, --port INTEGER           port for server, defaults to 8001\n      --debug                      Enable debug mode - useful for development\n      --reload                     Automatically reload if code change detected -\n                                   useful for development\n      --cors                       Enable CORS by serving Access-Control-Allow-\n                                   Origin: *\n      --load-extension PATH        Path to a SQLite extension to load\n      --inspect-file TEXT          Path to JSON file created using \"datasette\n                                   inspect\"\n      -m, --metadata FILENAME      Path to JSON file containing license/source\n                                   metadata\n      --template-dir DIRECTORY     Path to directory containing custom templates\n      --plugins-dir DIRECTORY      Path to directory containing custom plugins\n      --static STATIC MOUNT        mountpoint:path-to-directory for serving static\n                                   files\n      --config CONFIG              Set config option using configname:value\n                                   datasette.readthedocs.io/en/latest/config.html\n      --help-config                Show available config options\n      --help                       Show this message and exit.\n\n## metadata.json\n\nIf you want to include licensing and source information in the generated datasette website you can do so using a JSON file that looks something like this:\n\n    {\n        \"title\": \"Five Thirty Eight\",\n        \"license\": \"CC Attribution 4.0 License\",\n        \"license_url\": \"http://creativecommons.org/licenses/by/4.0/\",\n        \"source\": \"fivethirtyeight/data on GitHub\",\n        \"source_url\": \"https://github.com/fivethirtyeight/data\"\n    }\n\nThe license and source information will be displayed on the index page and in the footer. They will also be included in the JSON produced by the API.\n\n## datasette publish\n\nIf you have [Zeit Now](https://zeit.co/now) or [Heroku](https://heroku.com/) configured, datasette can deploy one or more SQLite databases to the internet with a single command:\n\n    datasette publish now database.db\n\nOr:\n\n    datasette publish heroku database.db\n\nThis will create a docker image containing both the datasette application and the specified SQLite database files. It will then deploy that image to Zeit Now or Heroku and give you a URL to access the API.\n\n    $ datasette publish --help\n    Usage: datasette publish [OPTIONS] PUBLISHER [FILES]...\n\n      Publish specified SQLite database files to the internet along with a\n      datasette API.\n\n      Options for PUBLISHER:     * 'now' - You must have Zeit Now installed:\n      https://zeit.co/now     * 'heroku' - You must have Heroku installed:\n      https://cli.heroku.com/\n\n      Example usage: datasette publish now my-database.db\n\n    Options:\n      -n, --name TEXT           Application name to use when deploying to Now\n                                (ignored for Heroku)\n      -m, --metadata FILENAME   Path to JSON file containing metadata to publish\n      --extra-options TEXT      Extra options to pass to datasette serve\n      --force                   Pass --force option to now\n      --branch TEXT             Install datasette from a GitHub branch e.g. master\n      --template-dir DIRECTORY  Path to directory containing custom templates\n      --plugins-dir DIRECTORY   Path to directory containing custom plugins\n      --static STATIC MOUNT     mountpoint:path-to-directory for serving static\n                                files\n      --install TEXT            Additional packages (e.g. plugins) to install\n      --title TEXT              Title for metadata\n      --license TEXT            License label for metadata\n      --license_url TEXT        License URL for metadata\n      --source TEXT             Source label for metadata\n      --source_url TEXT         Source URL for metadata\n      --help                    Show this message and exit.\n\n## datasette package\n\nIf you have docker installed you can use `datasette package` to create a new Docker image in your local repository containing the datasette app and selected SQLite databases:\n\n    $ datasette package --help\n    Usage: datasette package [OPTIONS] FILES...\n\n      Package specified SQLite files into a new datasette Docker container\n\n    Options:\n      -t, --tag TEXT            Name for the resulting Docker container, can\n                                optionally use name:tag format\n      -m, --metadata FILENAME   Path to JSON file containing metadata to publish\n      --extra-options TEXT      Extra options to pass to datasette serve\n      --branch TEXT             Install datasette from a GitHub branch e.g. master\n      --template-dir DIRECTORY  Path to directory containing custom templates\n      --plugins-dir DIRECTORY   Path to directory containing custom plugins\n      --static STATIC MOUNT     mountpoint:path-to-directory for serving static\n                                files\n      --install TEXT            Additional packages (e.g. plugins) to install\n      --title TEXT              Title for metadata\n      --license TEXT            License label for metadata\n      --license_url TEXT        License URL for metadata\n      --source TEXT             Source label for metadata\n      --source_url TEXT         Source URL for metadata\n      --help                    Show this message and exit.\n\nBoth publish and package accept an `extra_options` argument option, which will affect how the resulting application is executed. For example, say you want to increase the SQL time limit for a particular container:\n\n    datasette package parlgov.db \\\n        --extra-options=\"--config sql_time_limit_ms:2500 --config default_page_size:10\"\n\nThe resulting container will run the application with those options.\n\nHere's example output for the package command:\n\n    $ datasette package parlgov.db --extra-options=\"--config sql_time_limit_ms:2500\"\n    Sending build context to Docker daemon  4.459MB\n    Step 1/7 : FROM python:3\n     ---> 79e1dc9af1c1\n    Step 2/7 : COPY . /app\n     ---> Using cache\n     ---> cd4ec67de656\n    Step 3/7 : WORKDIR /app\n     ---> Using cache\n     ---> 139699e91621\n    Step 4/7 : RUN pip install datasette\n     ---> Using cache\n     ---> 340efa82bfd7\n    Step 5/7 : RUN datasette inspect parlgov.db --inspect-file inspect-data.json\n     ---> Using cache\n     ---> 5fddbe990314\n    Step 6/7 : EXPOSE 8001\n     ---> Using cache\n     ---> 8e83844b0fed\n    Step 7/7 : CMD datasette serve parlgov.db --port 8001 --inspect-file inspect-data.json --config sql_time_limit_ms:2500\n     ---> Using cache\n     ---> 1bd380ea8af3\n    Successfully built 1bd380ea8af3\n\nYou can now run the resulting container like so:\n\n    docker run -p 8081:8001 1bd380ea8af3\n\nThis exposes port 8001 inside the container as port 8081 on your host machine, so you can access the application at http://localhost:8081/\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/simonw/datasette", "keywords": "", "license": "Apache License, Version 2.0", "maintainer": "", "maintainer_email": "", "name": "datasette-core", "package_url": "https://pypi.org/project/datasette-core/", "platform": "", "project_url": "https://pypi.org/project/datasette-core/", "project_urls": {"Homepage": "https://github.com/simonw/datasette"}, "release_url": "https://pypi.org/project/datasette-core/0.22.1/", "requires_dist": ["Sanic (==0.7.0)", "Jinja2 (==2.10)", "hupper (==1.0)", "pint (==0.8.1)", "pluggy (<1.0,>=0.1.0)", "click (==6.7)", "click-default-group (==1.2)"], "requires_python": "", "summary": "An instant JSON API for your SQLite databases", "version": "0.22.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Datasette</h1>\n<p><a href=\"https://pypi.org/project/datasette/\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8ebc0040fb0b073afdbfc7c047e9497f3ace6a47/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6461746173657474652e737667\"></a>\n<a href=\"https://travis-ci.org/simonw/datasette\" rel=\"nofollow\"><img alt=\"Travis CI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a58357f7991da30ef878640d8c4ee1d99a12ebb4/68747470733a2f2f7472617669732d63692e6f72672f73696d6f6e772f6461746173657474652e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"http://datasette.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/324f4d1c569a5f7c37d8c4827e5aeb55b0cdca9d/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6461746173657474652f62616467652f3f76657273696f6e3d6c6174657374\"></a>\n<a href=\"https://github.com/simonw/datasette/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bad0ad2a7195b40874e3a10488118122d89489fd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322e302d626c75652e737667\"></a></p>\n<p><em>An instant JSON API for your SQLite databases</em></p>\n<p>Datasette provides an instant, read-only JSON API for any SQLite database. It also provides tools  for packaging the database up as a Docker container and deploying that container to hosting providers such as <a href=\"https://zeit.co/now\" rel=\"nofollow\">Zeit Now</a>.</p>\n<p>Got CSV data? Use <a href=\"https://github.com/simonw/csvs-to-sqlite\" rel=\"nofollow\">csvs-to-sqlite</a> to convert them to SQLite, then publish them with Datasette. Or try <a href=\"https://publish.datasettes.com\" rel=\"nofollow\">Datasette Publish</a>, a web app that lets you upload CSV data and deploy it using Datasette without needing to install any software.</p>\n<p>Documentation:\u00a0<a href=\"http://datasette.readthedocs.io/\" rel=\"nofollow\">http://datasette.readthedocs.io/</a> Examples: <a href=\"https://github.com/simonw/datasette/wiki/Datasettes\" rel=\"nofollow\">https://github.com/simonw/datasette/wiki/Datasettes</a></p>\n<h2>News</h2>\n<ul>\n<li>23rd May 2018: <a href=\"https://github.com/simonw/datasette/releases/tag/0.22.1\" rel=\"nofollow\">Datasette 0.22.1 bugfix</a> plus we now use <a href=\"https://github.com/warner/python-versioneer\" rel=\"nofollow\">versioneer</a></li>\n<li>20th May 2018: <a href=\"https://simonwillison.net/2018/May/20/datasette-facets\" rel=\"nofollow\">Datasette 0.22: Datasette Facets</a></li>\n<li>5th May 2018: <a href=\"https://github.com/simonw/datasette/releases/tag/0.21\" rel=\"nofollow\">Datasette 0.21: New _shape=, new _size=, search within columns</a></li>\n<li>25th April 2018: <a href=\"https://simonwillison.net/2018/Apr/25/register-members-interests/\" rel=\"nofollow\">Exploring the UK Register of Members Interests with SQL and Datasette</a> - a tutorial describing how <a href=\"https://register-of-members-interests.datasettes.com/\" rel=\"nofollow\">register-of-members-interests.datasettes.com</a> was built (<a href=\"https://github.com/simonw/register-of-members-interests\" rel=\"nofollow\">source code here</a>)</li>\n<li>20th April 2018: <a href=\"https://simonwillison.net/2018/Apr/20/datasette-plugins/\" rel=\"nofollow\">Datasette plugins, and building a clustered map visualization</a> - introducing Datasette's new plugin system and <a href=\"https://pypi.org/project/datasette-cluster-map/\" rel=\"nofollow\">datasette-cluster-map</a>, a plugin for visualizing data on a map</li>\n<li>20th April 2018: <a href=\"https://github.com/simonw/datasette/releases/tag/0.20\" rel=\"nofollow\">Datasette 0.20: static assets and templates for plugins</a></li>\n<li>16th April 2018: <a href=\"https://github.com/simonw/datasette/releases/tag/0.19\" rel=\"nofollow\">Datasette 0.19: plugins preview</a></li>\n<li>14th April 2018: <a href=\"https://github.com/simonw/datasette/releases/tag/0.18\" rel=\"nofollow\">Datasette 0.18: units</a></li>\n<li>9th April 2018: <a href=\"https://github.com/simonw/datasette/releases/tag/0.15\" rel=\"nofollow\">Datasette 0.15: sort by column</a></li>\n<li>28th March 2018: <a href=\"https://simonwillison.net/2018/Mar/28/datasette-in-the-wild/\" rel=\"nofollow\">Baltimore Sun Public Salary Records</a> - a data journalism project from the Baltimore Sun powered by Datasette - source code <a href=\"https://github.com/baltimore-sun-data/salaries-datasette\" rel=\"nofollow\">is available here</a></li>\n<li>27th March 2018: <a href=\"https://wwwf.imperial.ac.uk/blog/research-software-engineering/2018/03/27/cloud-first-rapid-webapp-deployment-using-containers/\" rel=\"nofollow\">Cloud-first: Rapid webapp deployment using containers</a> - a tutorial covering deploying Datasette using Microsoft Azure by the Research Software Engineering team at Imperial College London</li>\n<li>28th January 2018: <a href=\"https://simonwillison.net/2018/Jan/28/analyzing-my-twitter-followers/\" rel=\"nofollow\">Analyzing my Twitter followers with Datasette</a> - a tutorial on using Datasette to analyze follower data pulled from the Twitter API</li>\n<li>17th January 2018: <a href=\"https://simonwillison.net/2018/Jan/17/datasette-publish/\" rel=\"nofollow\">Datasette Publish: a web app for publishing CSV files as an online database</a></li>\n<li>12th December 2017: <a href=\"https://simonwillison.net/2017/Dec/12/building-a-location-time-zone-api/\" rel=\"nofollow\">Building a location to time zone API with SpatiaLite, OpenStreetMap and Datasette</a></li>\n<li>9th December 2017: <a href=\"https://github.com/simonw/datasette/releases/tag/0.14\" rel=\"nofollow\">Datasette 0.14: customization edition</a></li>\n<li>25th November 2017: <a href=\"https://simonwillison.net/2017/Nov/25/new-in-datasette/\" rel=\"nofollow\">New in Datasette: filters, foreign keys and search</a></li>\n<li>13th November 2017: <a href=\"https://simonwillison.net/2017/Nov/13/datasette/\" rel=\"nofollow\">Datasette: instantly create and publish an API for your SQLite databases</a></li>\n</ul>\n<h2>Installation</h2>\n<pre><code>pip3 install datasette\n</code></pre>\n<p>Datasette requires Python 3.5 or higher.</p>\n<h2>Basic usage</h2>\n<pre><code>datasette serve path/to/database.db\n</code></pre>\n<p>This will start a web server on port 8001 - visit <a href=\"http://localhost:8001/\" rel=\"nofollow\">http://localhost:8001/</a> to access the web interface.</p>\n<p><code>serve</code> is the default subcommand, you can omit it if you like.</p>\n<p>Use Chrome on OS X? You can run datasette against your browser history like so:</p>\n<pre><code> datasette ~/Library/Application\\ Support/Google/Chrome/Default/History\n</code></pre>\n<p>Now visiting <a href=\"http://localhost:8001/History/downloads\" rel=\"nofollow\">http://localhost:8001/History/downloads</a> will show you a web interface to browse your downloads data:</p>\n<p><img alt=\"Downloads table rendered by datasette\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5829d9f686d8fd6840c4210eb0b1d12f7b81ea00/68747470733a2f2f7374617469632e73696d6f6e77696c6c69736f6e2e6e65742f7374617469632f323031372f6461746173657474652d646f776e6c6f6164732e706e67\"></p>\n<p><a href=\"http://localhost:8001/History/downloads.json\" rel=\"nofollow\">http://localhost:8001/History/downloads.json</a> will return that data as JSON:</p>\n<pre><code>{\n    \"database\": \"History\",\n    \"columns\": [\n        \"id\",\n        \"current_path\",\n        \"target_path\",\n        \"start_time\",\n        \"received_bytes\",\n        \"total_bytes\",\n        ...\n    ],\n    \"table_rows_count\": 576,\n    \"rows\": [\n        [\n            1,\n            \"/Users/simonw/Downloads/DropboxInstaller.dmg\",\n            \"/Users/simonw/Downloads/DropboxInstaller.dmg\",\n            13097290269022132,\n            626688,\n            0,\n            ...\n        ]\n    ]\n}\n</code></pre>\n<p><a href=\"http://localhost:8001/History/downloads.json?_shape=objects\" rel=\"nofollow\">http://localhost:8001/History/downloads.json?_shape=objects</a> will return that data as JSON in a more convenient but less efficient format:</p>\n<pre><code>{\n    ...\n    \"rows\": [\n        {\n            \"start_time\": 13097290269022132,\n            \"interrupt_reason\": 0,\n            \"hash\": \"\",\n            \"id\": 1,\n            \"site_url\": \"\",\n            \"referrer\": \"https://www.dropbox.com/downloading?src=index\",\n            ...\n        }\n    ]\n}\n</code></pre>\n<h2>datasette serve options</h2>\n<pre><code>$ datasette serve --help\nUsage: datasette serve [OPTIONS] [FILES]...\n\n  Serve up specified SQLite database files with a web UI\n\nOptions:\n  -h, --host TEXT              host for server, defaults to 127.0.0.1\n  -p, --port INTEGER           port for server, defaults to 8001\n  --debug                      Enable debug mode - useful for development\n  --reload                     Automatically reload if code change detected -\n                               useful for development\n  --cors                       Enable CORS by serving Access-Control-Allow-\n                               Origin: *\n  --load-extension PATH        Path to a SQLite extension to load\n  --inspect-file TEXT          Path to JSON file created using \"datasette\n                               inspect\"\n  -m, --metadata FILENAME      Path to JSON file containing license/source\n                               metadata\n  --template-dir DIRECTORY     Path to directory containing custom templates\n  --plugins-dir DIRECTORY      Path to directory containing custom plugins\n  --static STATIC MOUNT        mountpoint:path-to-directory for serving static\n                               files\n  --config CONFIG              Set config option using configname:value\n                               datasette.readthedocs.io/en/latest/config.html\n  --help-config                Show available config options\n  --help                       Show this message and exit.\n</code></pre>\n<h2>metadata.json</h2>\n<p>If you want to include licensing and source information in the generated datasette website you can do so using a JSON file that looks something like this:</p>\n<pre><code>{\n    \"title\": \"Five Thirty Eight\",\n    \"license\": \"CC Attribution 4.0 License\",\n    \"license_url\": \"http://creativecommons.org/licenses/by/4.0/\",\n    \"source\": \"fivethirtyeight/data on GitHub\",\n    \"source_url\": \"https://github.com/fivethirtyeight/data\"\n}\n</code></pre>\n<p>The license and source information will be displayed on the index page and in the footer. They will also be included in the JSON produced by the API.</p>\n<h2>datasette publish</h2>\n<p>If you have <a href=\"https://zeit.co/now\" rel=\"nofollow\">Zeit Now</a> or <a href=\"https://heroku.com/\" rel=\"nofollow\">Heroku</a> configured, datasette can deploy one or more SQLite databases to the internet with a single command:</p>\n<pre><code>datasette publish now database.db\n</code></pre>\n<p>Or:</p>\n<pre><code>datasette publish heroku database.db\n</code></pre>\n<p>This will create a docker image containing both the datasette application and the specified SQLite database files. It will then deploy that image to Zeit Now or Heroku and give you a URL to access the API.</p>\n<pre><code>$ datasette publish --help\nUsage: datasette publish [OPTIONS] PUBLISHER [FILES]...\n\n  Publish specified SQLite database files to the internet along with a\n  datasette API.\n\n  Options for PUBLISHER:     * 'now' - You must have Zeit Now installed:\n  https://zeit.co/now     * 'heroku' - You must have Heroku installed:\n  https://cli.heroku.com/\n\n  Example usage: datasette publish now my-database.db\n\nOptions:\n  -n, --name TEXT           Application name to use when deploying to Now\n                            (ignored for Heroku)\n  -m, --metadata FILENAME   Path to JSON file containing metadata to publish\n  --extra-options TEXT      Extra options to pass to datasette serve\n  --force                   Pass --force option to now\n  --branch TEXT             Install datasette from a GitHub branch e.g. master\n  --template-dir DIRECTORY  Path to directory containing custom templates\n  --plugins-dir DIRECTORY   Path to directory containing custom plugins\n  --static STATIC MOUNT     mountpoint:path-to-directory for serving static\n                            files\n  --install TEXT            Additional packages (e.g. plugins) to install\n  --title TEXT              Title for metadata\n  --license TEXT            License label for metadata\n  --license_url TEXT        License URL for metadata\n  --source TEXT             Source label for metadata\n  --source_url TEXT         Source URL for metadata\n  --help                    Show this message and exit.\n</code></pre>\n<h2>datasette package</h2>\n<p>If you have docker installed you can use <code>datasette package</code> to create a new Docker image in your local repository containing the datasette app and selected SQLite databases:</p>\n<pre><code>$ datasette package --help\nUsage: datasette package [OPTIONS] FILES...\n\n  Package specified SQLite files into a new datasette Docker container\n\nOptions:\n  -t, --tag TEXT            Name for the resulting Docker container, can\n                            optionally use name:tag format\n  -m, --metadata FILENAME   Path to JSON file containing metadata to publish\n  --extra-options TEXT      Extra options to pass to datasette serve\n  --branch TEXT             Install datasette from a GitHub branch e.g. master\n  --template-dir DIRECTORY  Path to directory containing custom templates\n  --plugins-dir DIRECTORY   Path to directory containing custom plugins\n  --static STATIC MOUNT     mountpoint:path-to-directory for serving static\n                            files\n  --install TEXT            Additional packages (e.g. plugins) to install\n  --title TEXT              Title for metadata\n  --license TEXT            License label for metadata\n  --license_url TEXT        License URL for metadata\n  --source TEXT             Source label for metadata\n  --source_url TEXT         Source URL for metadata\n  --help                    Show this message and exit.\n</code></pre>\n<p>Both publish and package accept an <code>extra_options</code> argument option, which will affect how the resulting application is executed. For example, say you want to increase the SQL time limit for a particular container:</p>\n<pre><code>datasette package parlgov.db \\\n    --extra-options=\"--config sql_time_limit_ms:2500 --config default_page_size:10\"\n</code></pre>\n<p>The resulting container will run the application with those options.</p>\n<p>Here's example output for the package command:</p>\n<pre><code>$ datasette package parlgov.db --extra-options=\"--config sql_time_limit_ms:2500\"\nSending build context to Docker daemon  4.459MB\nStep 1/7 : FROM python:3\n ---&gt; 79e1dc9af1c1\nStep 2/7 : COPY . /app\n ---&gt; Using cache\n ---&gt; cd4ec67de656\nStep 3/7 : WORKDIR /app\n ---&gt; Using cache\n ---&gt; 139699e91621\nStep 4/7 : RUN pip install datasette\n ---&gt; Using cache\n ---&gt; 340efa82bfd7\nStep 5/7 : RUN datasette inspect parlgov.db --inspect-file inspect-data.json\n ---&gt; Using cache\n ---&gt; 5fddbe990314\nStep 6/7 : EXPOSE 8001\n ---&gt; Using cache\n ---&gt; 8e83844b0fed\nStep 7/7 : CMD datasette serve parlgov.db --port 8001 --inspect-file inspect-data.json --config sql_time_limit_ms:2500\n ---&gt; Using cache\n ---&gt; 1bd380ea8af3\nSuccessfully built 1bd380ea8af3\n</code></pre>\n<p>You can now run the resulting container like so:</p>\n<pre><code>docker run -p 8081:8001 1bd380ea8af3\n</code></pre>\n<p>This exposes port 8001 inside the container as port 8081 on your host machine, so you can access the application at <a href=\"http://localhost:8081/\" rel=\"nofollow\">http://localhost:8081/</a></p>\n\n          </div>"}, "last_serial": 3919901, "releases": {"0.22.1": [{"comment_text": "", "digests": {"md5": "74985cc789e363a30637459f3cb6d4b4", "sha256": "25ce15ad5cbc6be8dabae619ae2768163377a43e27f6dfee9079975d98708969"}, "downloads": -1, "filename": "datasette_core-0.22.1-py3-none-any.whl", "has_sig": false, "md5_digest": "74985cc789e363a30637459f3cb6d4b4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 179998, "upload_time": "2018-06-01T08:18:57", "upload_time_iso_8601": "2018-06-01T08:18:57.618881Z", "url": "https://files.pythonhosted.org/packages/22/dc/c00c174db9a6241b3206b315b70d7e6c62a14b7199261848cd3adac61caa/datasette_core-0.22.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8be7f5f31f7b766b6b669287f3c46fd7", "sha256": "32aa0a6e1f1532f74c4e4ed7e18271a17d53e332e6c7ea304f2b609055f33d01"}, "downloads": -1, "filename": "datasette-core-0.22.1.tar.gz", "has_sig": false, "md5_digest": "8be7f5f31f7b766b6b669287f3c46fd7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 183691, "upload_time": "2018-06-01T08:18:59", "upload_time_iso_8601": "2018-06-01T08:18:59.941578Z", "url": "https://files.pythonhosted.org/packages/67/e9/18152b058c2ba764bf96c04053367113c15b15e1973fbcdb6f5497e4d27f/datasette-core-0.22.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "74985cc789e363a30637459f3cb6d4b4", "sha256": "25ce15ad5cbc6be8dabae619ae2768163377a43e27f6dfee9079975d98708969"}, "downloads": -1, "filename": "datasette_core-0.22.1-py3-none-any.whl", "has_sig": false, "md5_digest": "74985cc789e363a30637459f3cb6d4b4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 179998, "upload_time": "2018-06-01T08:18:57", "upload_time_iso_8601": "2018-06-01T08:18:57.618881Z", "url": "https://files.pythonhosted.org/packages/22/dc/c00c174db9a6241b3206b315b70d7e6c62a14b7199261848cd3adac61caa/datasette_core-0.22.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8be7f5f31f7b766b6b669287f3c46fd7", "sha256": "32aa0a6e1f1532f74c4e4ed7e18271a17d53e332e6c7ea304f2b609055f33d01"}, "downloads": -1, "filename": "datasette-core-0.22.1.tar.gz", "has_sig": false, "md5_digest": "8be7f5f31f7b766b6b669287f3c46fd7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 183691, "upload_time": "2018-06-01T08:18:59", "upload_time_iso_8601": "2018-06-01T08:18:59.941578Z", "url": "https://files.pythonhosted.org/packages/67/e9/18152b058c2ba764bf96c04053367113c15b15e1973fbcdb6f5497e4d27f/datasette-core-0.22.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:40:11 2020"}