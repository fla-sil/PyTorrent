{"info": {"author": "Holger Krekel", "author_email": "holger.krekel@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: MacOS :: MacOS X", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX", "Programming Language :: Python", "Programming Language :: Python :: 3", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Testing", "Topic :: Utilities"], "description": "pytest-cache: working with cross-testrun state\n=====================================================\n\nUsage\n---------\n\ninstall via::\n\n    pip install pytest-cache\n\nafter which other plugins can access a new `config.cache`_ object \nwhich helps sharing values between ``py.test`` invocations.\n\nThe plugin provides two options to rerun failures, namely ``--lf`` to\nonly re-run the failures and ``--ff`` to run all tests but the failures\nfrom the last run first.  For cleanup (usually not needed), a\n``--clearcache`` option allows to remove all cross-session cache\ncontents ahead of a test run.\n\n\nRerunning only failures or failures first\n-----------------------------------------------\n\nFirst, let's create 50 test invocation of which only 2 fail::\n\n    # content of test_50.py\n    import pytest\n\n    @pytest.mark.parametrize(\"i\", range(50))\n    def test_num(i):\n        if i in (17,25):\n           pytest.fail(\"bad luck\") \n\nIf you run this for the first time you will see two failures::\n\n    $ py.test -q\n    .................F.......F........................\n    =================================== FAILURES ===================================\n    _________________________________ test_num[17] _________________________________\n    \n    i = 17\n    \n        @pytest.mark.parametrize(\"i\", range(50))\n        def test_num(i):\n            if i in (17,25):\n    >          pytest.fail(\"bad luck\")\n    E          Failed: bad luck\n    \n    test_50.py:6: Failed\n    _________________________________ test_num[25] _________________________________\n    \n    i = 25\n    \n        @pytest.mark.parametrize(\"i\", range(50))\n        def test_num(i):\n            if i in (17,25):\n    >          pytest.fail(\"bad luck\")\n    E          Failed: bad luck\n    \n    test_50.py:6: Failed\n\nIf you then run it with ``--lf`` you will run only the two failing test\nfrom the last run::\n\n    $ py.test --lf\n    ============================= test session starts ==============================\n    platform linux2 -- Python 2.7.3 -- pytest-2.3.5\n    run-last-failure: rerun last 2 failures\n    plugins: cache\n    collected 50 items\n    \n    test_50.py FF\n    \n    =================================== FAILURES ===================================\n    _________________________________ test_num[17] _________________________________\n    \n    i = 17\n    \n        @pytest.mark.parametrize(\"i\", range(50))\n        def test_num(i):\n            if i in (17,25):\n    >          pytest.fail(\"bad luck\")\n    E          Failed: bad luck\n    \n    test_50.py:6: Failed\n    _________________________________ test_num[25] _________________________________\n    \n    i = 25\n    \n        @pytest.mark.parametrize(\"i\", range(50))\n        def test_num(i):\n            if i in (17,25):\n    >          pytest.fail(\"bad luck\")\n    E          Failed: bad luck\n    \n    test_50.py:6: Failed\n    =================== 2 failed, 48 deselected in 0.02 seconds ====================\n\nThe last line indicates that 48 tests have not been run.\n\nIf you run with the ``--ff`` option, all tests will be run but the first\nfailures will be executed first (as can be seen from the series of ``FF`` and\ndots)::\n\n    $ py.test --ff\n    ============================= test session starts ==============================\n    platform linux2 -- Python 2.7.3 -- pytest-2.3.5\n    run-last-failure: rerun last 2 failures first\n    plugins: cache\n    collected 50 items\n    \n    test_50.py FF................................................\n    \n    =================================== FAILURES ===================================\n    _________________________________ test_num[17] _________________________________\n    \n    i = 17\n    \n        @pytest.mark.parametrize(\"i\", range(50))\n        def test_num(i):\n            if i in (17,25):\n    >          pytest.fail(\"bad luck\")\n    E          Failed: bad luck\n    \n    test_50.py:6: Failed\n    _________________________________ test_num[25] _________________________________\n    \n    i = 25\n    \n        @pytest.mark.parametrize(\"i\", range(50))\n        def test_num(i):\n            if i in (17,25):\n    >          pytest.fail(\"bad luck\")\n    E          Failed: bad luck\n    \n    test_50.py:6: Failed\n    ===================== 2 failed, 48 passed in 0.07 seconds ======================\n\n.. _`config.cache`:\n\nThe new config.cache object\n--------------------------------\n\n.. regendoc:wipe\n\nPlugins or conftest.py support code can get a cached value \nusing the pytest ``config`` object.  Here is a basic example\nplugin which implements a `funcarg <http://pytest.org/latest/funcargs.html>`_\nwhich re-uses previously created state across py.test invocations::\n\n    # content of test_caching.py\n    import time\n\n    def pytest_funcarg__mydata(request):\n        val = request.config.cache.get(\"example/value\", None)\n        if val is None:\n            time.sleep(9*0.6) # expensive computation :)\n            val = 42\n            request.config.cache.set(\"example/value\", val)\n        return val \n\n    def test_function(mydata):\n        assert mydata == 23\n\nIf you run this command once, it will take a while because\nof the sleep::\n\n    $ py.test -q\n    F\n    =================================== FAILURES ===================================\n    ________________________________ test_function _________________________________\n    \n    mydata = 42\n    \n        def test_function(mydata):\n    >       assert mydata == 23\n    E       assert 42 == 23\n    \n    test_caching.py:12: AssertionError\n\nIf you run it a second time the value will be retrieved from\nthe cache and this will be quick::\n\n    $ py.test -q\n    F\n    =================================== FAILURES ===================================\n    ________________________________ test_function _________________________________\n    \n    mydata = 42\n    \n        def test_function(mydata):\n    >       assert mydata == 23\n    E       assert 42 == 23\n    \n    test_caching.py:12: AssertionError\n\nConsult the `pytest-cache API <http://packages.python.org/pytest-cache/api.html>`_\nfor more details.\n\n\nInspecting Cache content\n-------------------------------\n\nYou can always peek at the content of the cache using the\n``--cache`` command line option::\n\n    $ py.test --cache\n    ============================= test session starts ==============================\n    platform linux2 -- Python 2.7.3 -- pytest-2.3.5\n    plugins: cache\n    cachedir: /tmp/doc-exec-6/.cache\n    --------------------------------- cache values ---------------------------------\n    example/value contains:\n      42\n    cache/lastfailed contains:\n      set(['test_caching.py::test_function'])\n    \n    ===============================  in 0.01 seconds ===============================\n\nClearing Cache content\n-------------------------------\n\nYou can instruct pytest to clear all cache files and values \nby adding the ``--clearcache`` option like this::\n\n    py.test --clearcache\n\nThis is recommended for invocations from Continous Integration\nservers where isolation and correctness is more important\nthan speed.\n\nNotes\n-------------\n\nrepository: http://bitbucket.org/hpk42/pytest-cache\n\nIssues: repository: http://bitbucket.org/hpk42/pytest-cache/issues\n\nmore info on py.test: http://pytest.org", "description_content_type": null, "docs_url": "https://pythonhosted.org/pytest-cache/", "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://bitbucket.org/hpk42/pytest-cache/", "keywords": null, "license": "MIT License", "maintainer": null, "maintainer_email": null, "name": "pytest-cache", "package_url": "https://pypi.org/project/pytest-cache/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/pytest-cache/", "project_urls": {"Download": "UNKNOWN", "Homepage": "http://bitbucket.org/hpk42/pytest-cache/"}, "release_url": "https://pypi.org/project/pytest-cache/1.0/", "requires_dist": null, "requires_python": null, "summary": "pytest plugin with mechanisms for caching across test runs", "version": "1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"usage\">\n<h2>Usage</h2>\n<p>install via:</p>\n<pre>pip install pytest-cache\n</pre>\n<p>after which other plugins can access a new <a href=\"#config-cache\" rel=\"nofollow\">config.cache</a> object\nwhich helps sharing values between <tt>py.test</tt> invocations.</p>\n<p>The plugin provides two options to rerun failures, namely <tt><span class=\"pre\">--lf</span></tt> to\nonly re-run the failures and <tt><span class=\"pre\">--ff</span></tt> to run all tests but the failures\nfrom the last run first.  For cleanup (usually not needed), a\n<tt><span class=\"pre\">--clearcache</span></tt> option allows to remove all cross-session cache\ncontents ahead of a test run.</p>\n</div>\n<div id=\"rerunning-only-failures-or-failures-first\">\n<h2>Rerunning only failures or failures first</h2>\n<p>First, let\u2019s create 50 test invocation of which only 2 fail:</p>\n<pre># content of test_50.py\nimport pytest\n\n@pytest.mark.parametrize(\"i\", range(50))\ndef test_num(i):\n    if i in (17,25):\n       pytest.fail(\"bad luck\")\n</pre>\n<p>If you run this for the first time you will see two failures:</p>\n<pre>$ py.test -q\n.................F.......F........................\n=================================== FAILURES ===================================\n_________________________________ test_num[17] _________________________________\n\ni = 17\n\n    @pytest.mark.parametrize(\"i\", range(50))\n    def test_num(i):\n        if i in (17,25):\n&gt;          pytest.fail(\"bad luck\")\nE          Failed: bad luck\n\ntest_50.py:6: Failed\n_________________________________ test_num[25] _________________________________\n\ni = 25\n\n    @pytest.mark.parametrize(\"i\", range(50))\n    def test_num(i):\n        if i in (17,25):\n&gt;          pytest.fail(\"bad luck\")\nE          Failed: bad luck\n\ntest_50.py:6: Failed\n</pre>\n<p>If you then run it with <tt><span class=\"pre\">--lf</span></tt> you will run only the two failing test\nfrom the last run:</p>\n<pre>$ py.test --lf\n============================= test session starts ==============================\nplatform linux2 -- Python 2.7.3 -- pytest-2.3.5\nrun-last-failure: rerun last 2 failures\nplugins: cache\ncollected 50 items\n\ntest_50.py FF\n\n=================================== FAILURES ===================================\n_________________________________ test_num[17] _________________________________\n\ni = 17\n\n    @pytest.mark.parametrize(\"i\", range(50))\n    def test_num(i):\n        if i in (17,25):\n&gt;          pytest.fail(\"bad luck\")\nE          Failed: bad luck\n\ntest_50.py:6: Failed\n_________________________________ test_num[25] _________________________________\n\ni = 25\n\n    @pytest.mark.parametrize(\"i\", range(50))\n    def test_num(i):\n        if i in (17,25):\n&gt;          pytest.fail(\"bad luck\")\nE          Failed: bad luck\n\ntest_50.py:6: Failed\n=================== 2 failed, 48 deselected in 0.02 seconds ====================\n</pre>\n<p>The last line indicates that 48 tests have not been run.</p>\n<p>If you run with the <tt><span class=\"pre\">--ff</span></tt> option, all tests will be run but the first\nfailures will be executed first (as can be seen from the series of <tt>FF</tt> and\ndots):</p>\n<pre>$ py.test --ff\n============================= test session starts ==============================\nplatform linux2 -- Python 2.7.3 -- pytest-2.3.5\nrun-last-failure: rerun last 2 failures first\nplugins: cache\ncollected 50 items\n\ntest_50.py FF................................................\n\n=================================== FAILURES ===================================\n_________________________________ test_num[17] _________________________________\n\ni = 17\n\n    @pytest.mark.parametrize(\"i\", range(50))\n    def test_num(i):\n        if i in (17,25):\n&gt;          pytest.fail(\"bad luck\")\nE          Failed: bad luck\n\ntest_50.py:6: Failed\n_________________________________ test_num[25] _________________________________\n\ni = 25\n\n    @pytest.mark.parametrize(\"i\", range(50))\n    def test_num(i):\n        if i in (17,25):\n&gt;          pytest.fail(\"bad luck\")\nE          Failed: bad luck\n\ntest_50.py:6: Failed\n===================== 2 failed, 48 passed in 0.07 seconds ======================\n</pre>\n</div>\n<div id=\"the-new-config-cache-object\">\n<span id=\"config-cache\"></span><h2>The new config.cache object</h2>\n<p>Plugins or conftest.py support code can get a cached value\nusing the pytest <tt>config</tt> object.  Here is a basic example\nplugin which implements a <a href=\"http://pytest.org/latest/funcargs.html\" rel=\"nofollow\">funcarg</a>\nwhich re-uses previously created state across py.test invocations:</p>\n<pre># content of test_caching.py\nimport time\n\ndef pytest_funcarg__mydata(request):\n    val = request.config.cache.get(\"example/value\", None)\n    if val is None:\n        time.sleep(9*0.6) # expensive computation :)\n        val = 42\n        request.config.cache.set(\"example/value\", val)\n    return val\n\ndef test_function(mydata):\n    assert mydata == 23\n</pre>\n<p>If you run this command once, it will take a while because\nof the sleep:</p>\n<pre>$ py.test -q\nF\n=================================== FAILURES ===================================\n________________________________ test_function _________________________________\n\nmydata = 42\n\n    def test_function(mydata):\n&gt;       assert mydata == 23\nE       assert 42 == 23\n\ntest_caching.py:12: AssertionError\n</pre>\n<p>If you run it a second time the value will be retrieved from\nthe cache and this will be quick:</p>\n<pre>$ py.test -q\nF\n=================================== FAILURES ===================================\n________________________________ test_function _________________________________\n\nmydata = 42\n\n    def test_function(mydata):\n&gt;       assert mydata == 23\nE       assert 42 == 23\n\ntest_caching.py:12: AssertionError\n</pre>\n<p>Consult the <a href=\"http://packages.python.org/pytest-cache/api.html\" rel=\"nofollow\">pytest-cache API</a>\nfor more details.</p>\n</div>\n<div id=\"inspecting-cache-content\">\n<h2>Inspecting Cache content</h2>\n<p>You can always peek at the content of the cache using the\n<tt><span class=\"pre\">--cache</span></tt> command line option:</p>\n<pre>$ py.test --cache\n============================= test session starts ==============================\nplatform linux2 -- Python 2.7.3 -- pytest-2.3.5\nplugins: cache\ncachedir: /tmp/doc-exec-6/.cache\n--------------------------------- cache values ---------------------------------\nexample/value contains:\n  42\ncache/lastfailed contains:\n  set(['test_caching.py::test_function'])\n\n===============================  in 0.01 seconds ===============================\n</pre>\n</div>\n<div id=\"clearing-cache-content\">\n<h2>Clearing Cache content</h2>\n<p>You can instruct pytest to clear all cache files and values\nby adding the <tt><span class=\"pre\">--clearcache</span></tt> option like this:</p>\n<pre>py.test --clearcache\n</pre>\n<p>This is recommended for invocations from Continous Integration\nservers where isolation and correctness is more important\nthan speed.</p>\n</div>\n<div id=\"notes\">\n<h2>Notes</h2>\n<p>repository: <a href=\"http://bitbucket.org/hpk42/pytest-cache\" rel=\"nofollow\">http://bitbucket.org/hpk42/pytest-cache</a></p>\n<p>Issues: repository: <a href=\"http://bitbucket.org/hpk42/pytest-cache/issues\" rel=\"nofollow\">http://bitbucket.org/hpk42/pytest-cache/issues</a></p>\n<p>more info on py.test: <a href=\"http://pytest.org\" rel=\"nofollow\">http://pytest.org</a></p>\n</div>\n\n          </div>"}, "last_serial": 1238540, "releases": {"0.9": [{"comment_text": "", "digests": {"md5": "d41b2f751b83ae7bbbdc22dd5848a442", "sha256": "bae83f1d6d2d8afef8abd59663570b582286c1b629734d2c4bb88be0c47a8c49"}, "downloads": -1, "filename": "pytest-cache-0.9.zip", "has_sig": false, "md5_digest": "d41b2f751b83ae7bbbdc22dd5848a442", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11043, "upload_time": "2012-06-20T22:21:51", "upload_time_iso_8601": "2012-06-20T22:21:51.567289Z", "url": "https://files.pythonhosted.org/packages/b9/68/ef31b9ca85006059a60a943c71bad966dff0c706aa40b7263a4c3379193a/pytest-cache-0.9.zip", "yanked": false}], "1.0": [{"comment_text": "", "digests": {"md5": "e51ff62fec70a1fd456d975ce47977cd", "sha256": "be7468edd4d3d83f1e844959fd6e3fd28e77a481440a7118d430130ea31b07a9"}, "downloads": -1, "filename": "pytest-cache-1.0.tar.gz", "has_sig": false, "md5_digest": "e51ff62fec70a1fd456d975ce47977cd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16242, "upload_time": "2013-06-04T19:19:00", "upload_time_iso_8601": "2013-06-04T19:19:00.551314Z", "url": "https://files.pythonhosted.org/packages/d1/15/082fd0428aab33d2bafa014f3beb241830427ba803a8912a5aaeaf3a5663/pytest-cache-1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e51ff62fec70a1fd456d975ce47977cd", "sha256": "be7468edd4d3d83f1e844959fd6e3fd28e77a481440a7118d430130ea31b07a9"}, "downloads": -1, "filename": "pytest-cache-1.0.tar.gz", "has_sig": false, "md5_digest": "e51ff62fec70a1fd456d975ce47977cd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16242, "upload_time": "2013-06-04T19:19:00", "upload_time_iso_8601": "2013-06-04T19:19:00.551314Z", "url": "https://files.pythonhosted.org/packages/d1/15/082fd0428aab33d2bafa014f3beb241830427ba803a8912a5aaeaf3a5663/pytest-cache-1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:55:22 2020"}