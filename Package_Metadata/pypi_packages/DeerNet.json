{"info": {"author": "Ruochi Zhang", "author_email": "zrc720@gmail.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "# SKNet\n\n## Introduction\nSKNet is a new type of neural network that is simple in structure but complex in neuron. Each of its neuron is a traditional estimator such as SVM, RF, etc.  \n\n## Fetaures \nWe think that such a network has many applicable scenarios. For example: \n- We don't have enough samples to train neural networks. \n- We hope to improve the accuracy of the model by means of emsemble. \n- We hope to learn some new features. \n- We want to save a lot of parameter-tuning time while getting a stable and good model.\n\n\n## Installation\n\n```python3\npip install sknet\n```\n\n\n## Example\n\n## Computation Graph\n\n![](./computation_graph.png)\n\n### Code\n\n```python\nfrom sknet.sequential import Layer,Sequential,SKNeuron\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\n\ndata = load_breast_cancer()\nfeatures = data.data\ntarget = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\n\nlayer1 = Layer([\n    SKNeuron(RandomForestRegressor,params = {\"random_state\": 0}),\n    SKNeuron(GradientBoostingRegressor,params = {\"random_state\": 0}),\n    SKNeuron(AdaBoostRegressor,params = {\"random_state\": 0}),\n    SKNeuron(KNeighborsRegressor),\n    SKNeuron(ExtraTreesRegressor,params = {\"random_state\": 0}),\n])\n\nlayer2 = Layer([\n    SKNeuron(AdaBoostRegressor,params = {\"random_state\": 0}),\n    SKNeuron(LinearSVR,params = {\"random_state\": 0}),\n])\n\nlayer3 = Layer([\n    SKNeuron(LogisticRegression,params = {\"random_state\": 0}),\n])\n\n\nmodel = Sequential([layer1,layer2,layer3],n_splits = 5)\ny_pred = model.fit_predict(X_train,y_train, X_test)\nprint(model.score(y_test,y_pred))\n\n\n# acc = 0.9736842105263158\n```\n\n## How to construct the SkNet\n\n## General Considerations\uff08\u4e0d\u7528\u4e25\u683c\u9075\u5b88\uff09\n1. introduce more information on first level\n2. use simpler model for Subquent level\n\n## How to introduce more information?\n1. use different estimator\n2. use same estimator but differences parameters(such as random seed, ...)\n3. simpling samples\n4. simpleing features\n5. different feature engineering techniques (one-hot, embedding, ...)\n\n### first level Tips\n1. Diversity based on algo\n    - 2-3 gradient boosted trees(xgboost, H2O, catboost)\n    - 2-3 Neural Net (keras, pyTorch)\n    - 1-2 ExtraTree/ Random Forest( sklearn)\n    - 1-2 Linear models as in Logistic/ridge regression, linearsvm(sklearn)\n    - 1-2 knn models(sklearn)\n    - 1 Factorization machine (libfm)\n    - 1 svm with nonlinear kernel if size/memory allows(sklearn)\n    - 1 svm with nonlinear kernel if size/memory allows(sklearn)\n2. Diversity based on input data\n    - Categorical features: One hot, label encoding, target encoding, frequency.\n    - Numberical features: outliner, binning, derivatives, percentiles, scaling\n    - Interactions: col1 \\*/+-col2, groupby, unsupervied\n    - For classification target, we can use regression models in middle level \n\n### Subquent level tips\n1. Simpler(or shallower) algo\n    - gradient boosted tree with small depth(2 or 3)\n    - linear models with high reglarization\n    - Extra Trees\n    - Shallow network\n    - Knn with BrayCurtis Distance\n    - Brute forcing a seach for best linear weights based on cv\n2. Feature engineering\n    - parwise differences between meta features\n    - row-wise statics like average or stds\n    - Standard feature selection techniques\n\n\n## Todo\n- Two or three level stacking\n- multi-processing\n- features proxy", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/zhangruochi/DeerNet", "keywords": "stack,sklearn", "license": "MIT License", "maintainer": "", "maintainer_email": "", "name": "DeerNet", "package_url": "https://pypi.org/project/DeerNet/", "platform": "any", "project_url": "https://pypi.org/project/DeerNet/", "project_urls": {"Homepage": "https://github.com/zhangruochi/DeerNet"}, "release_url": "https://pypi.org/project/DeerNet/0.0.2/", "requires_dist": null, "requires_python": "", "summary": "a library used for stacking based on scikit-learn", "version": "0.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>SKNet</h1>\n<h2>Introduction</h2>\n<p>SKNet is a new type of neural network that is simple in structure but complex in neuron. Each of its neuron is a traditional estimator such as SVM, RF, etc.</p>\n<h2>Fetaures</h2>\n<p>We think that such a network has many applicable scenarios. For example:</p>\n<ul>\n<li>We don't have enough samples to train neural networks.</li>\n<li>We hope to improve the accuracy of the model by means of emsemble.</li>\n<li>We hope to learn some new features.</li>\n<li>We want to save a lot of parameter-tuning time while getting a stable and good model.</li>\n</ul>\n<h2>Installation</h2>\n<pre><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">sknet</span>\n</pre>\n<h2>Example</h2>\n<h2>Computation Graph</h2>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/287c4f909479aa0d2406e821ddd4cecd837e85d5/2e2f636f6d7075746174696f6e5f67726170682e706e67\"></p>\n<h3>Code</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sknet.sequential</span> <span class=\"kn\">import</span> <span class=\"n\">Layer</span><span class=\"p\">,</span><span class=\"n\">Sequential</span><span class=\"p\">,</span><span class=\"n\">SKNeuron</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.ensemble</span> <span class=\"kn\">import</span> <span class=\"n\">RandomForestRegressor</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.ensemble</span> <span class=\"kn\">import</span> <span class=\"n\">AdaBoostRegressor</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.ensemble</span> <span class=\"kn\">import</span> <span class=\"n\">ExtraTreesRegressor</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.svm</span> <span class=\"kn\">import</span> <span class=\"n\">LinearSVR</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.linear_model</span> <span class=\"kn\">import</span> <span class=\"n\">LogisticRegression</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.ensemble</span> <span class=\"kn\">import</span> <span class=\"n\">GradientBoostingRegressor</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.neighbors</span> <span class=\"kn\">import</span> <span class=\"n\">KNeighborsRegressor</span>\n\n\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">load_breast_cancer</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">train_test_split</span>\n\n\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">load_breast_cancer</span><span class=\"p\">()</span>\n<span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">data</span>\n<span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">target</span>\n\n<span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">train_test_split</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">,</span> <span class=\"n\">test_size</span><span class=\"o\">=</span><span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n\n\n<span class=\"n\">layer1</span> <span class=\"o\">=</span> <span class=\"n\">Layer</span><span class=\"p\">([</span>\n    <span class=\"n\">SKNeuron</span><span class=\"p\">(</span><span class=\"n\">RandomForestRegressor</span><span class=\"p\">,</span><span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"random_state\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">}),</span>\n    <span class=\"n\">SKNeuron</span><span class=\"p\">(</span><span class=\"n\">GradientBoostingRegressor</span><span class=\"p\">,</span><span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"random_state\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">}),</span>\n    <span class=\"n\">SKNeuron</span><span class=\"p\">(</span><span class=\"n\">AdaBoostRegressor</span><span class=\"p\">,</span><span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"random_state\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">}),</span>\n    <span class=\"n\">SKNeuron</span><span class=\"p\">(</span><span class=\"n\">KNeighborsRegressor</span><span class=\"p\">),</span>\n    <span class=\"n\">SKNeuron</span><span class=\"p\">(</span><span class=\"n\">ExtraTreesRegressor</span><span class=\"p\">,</span><span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"random_state\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">}),</span>\n<span class=\"p\">])</span>\n\n<span class=\"n\">layer2</span> <span class=\"o\">=</span> <span class=\"n\">Layer</span><span class=\"p\">([</span>\n    <span class=\"n\">SKNeuron</span><span class=\"p\">(</span><span class=\"n\">AdaBoostRegressor</span><span class=\"p\">,</span><span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"random_state\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">}),</span>\n    <span class=\"n\">SKNeuron</span><span class=\"p\">(</span><span class=\"n\">LinearSVR</span><span class=\"p\">,</span><span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"random_state\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">}),</span>\n<span class=\"p\">])</span>\n\n<span class=\"n\">layer3</span> <span class=\"o\">=</span> <span class=\"n\">Layer</span><span class=\"p\">([</span>\n    <span class=\"n\">SKNeuron</span><span class=\"p\">(</span><span class=\"n\">LogisticRegression</span><span class=\"p\">,</span><span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"random_state\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">}),</span>\n<span class=\"p\">])</span>\n\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">([</span><span class=\"n\">layer1</span><span class=\"p\">,</span><span class=\"n\">layer2</span><span class=\"p\">,</span><span class=\"n\">layer3</span><span class=\"p\">],</span><span class=\"n\">n_splits</span> <span class=\"o\">=</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit_predict</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span><span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">y_test</span><span class=\"p\">,</span><span class=\"n\">y_pred</span><span class=\"p\">))</span>\n\n\n<span class=\"c1\"># acc = 0.9736842105263158</span>\n</pre>\n<h2>How to construct the SkNet</h2>\n<h2>General Considerations\uff08\u4e0d\u7528\u4e25\u683c\u9075\u5b88\uff09</h2>\n<ol>\n<li>introduce more information on first level</li>\n<li>use simpler model for Subquent level</li>\n</ol>\n<h2>How to introduce more information?</h2>\n<ol>\n<li>use different estimator</li>\n<li>use same estimator but differences parameters(such as random seed, ...)</li>\n<li>simpling samples</li>\n<li>simpleing features</li>\n<li>different feature engineering techniques (one-hot, embedding, ...)</li>\n</ol>\n<h3>first level Tips</h3>\n<ol>\n<li>Diversity based on algo\n<ul>\n<li>2-3 gradient boosted trees(xgboost, H2O, catboost)</li>\n<li>2-3 Neural Net (keras, pyTorch)</li>\n<li>1-2 ExtraTree/ Random Forest( sklearn)</li>\n<li>1-2 Linear models as in Logistic/ridge regression, linearsvm(sklearn)</li>\n<li>1-2 knn models(sklearn)</li>\n<li>1 Factorization machine (libfm)</li>\n<li>1 svm with nonlinear kernel if size/memory allows(sklearn)</li>\n<li>1 svm with nonlinear kernel if size/memory allows(sklearn)</li>\n</ul>\n</li>\n<li>Diversity based on input data\n<ul>\n<li>Categorical features: One hot, label encoding, target encoding, frequency.</li>\n<li>Numberical features: outliner, binning, derivatives, percentiles, scaling</li>\n<li>Interactions: col1 */+-col2, groupby, unsupervied</li>\n<li>For classification target, we can use regression models in middle level</li>\n</ul>\n</li>\n</ol>\n<h3>Subquent level tips</h3>\n<ol>\n<li>Simpler(or shallower) algo\n<ul>\n<li>gradient boosted tree with small depth(2 or 3)</li>\n<li>linear models with high reglarization</li>\n<li>Extra Trees</li>\n<li>Shallow network</li>\n<li>Knn with BrayCurtis Distance</li>\n<li>Brute forcing a seach for best linear weights based on cv</li>\n</ul>\n</li>\n<li>Feature engineering\n<ul>\n<li>parwise differences between meta features</li>\n<li>row-wise statics like average or stds</li>\n<li>Standard feature selection techniques</li>\n</ul>\n</li>\n</ol>\n<h2>Todo</h2>\n<ul>\n<li>Two or three level stacking</li>\n<li>multi-processing</li>\n<li>features proxy</li>\n</ul>\n\n          </div>"}, "last_serial": 6132393, "releases": {"0.0.2": [{"comment_text": "", "digests": {"md5": "3599d52a3fdcd9a0f2993bd341b4aadf", "sha256": "a66a676c5a7bba02a021cde4d0f3b93b0d752237a2cdfad5141973b47810dfcc"}, "downloads": -1, "filename": "DeerNet-0.0.2.tar.gz", "has_sig": false, "md5_digest": "3599d52a3fdcd9a0f2993bd341b4aadf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4316, "upload_time": "2019-11-13T19:12:39", "upload_time_iso_8601": "2019-11-13T19:12:39.448899Z", "url": "https://files.pythonhosted.org/packages/2b/b7/cd51c4098e3fc31e345824f599414aad133817dbfb441cee69a6c8fdc2cd/DeerNet-0.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3599d52a3fdcd9a0f2993bd341b4aadf", "sha256": "a66a676c5a7bba02a021cde4d0f3b93b0d752237a2cdfad5141973b47810dfcc"}, "downloads": -1, "filename": "DeerNet-0.0.2.tar.gz", "has_sig": false, "md5_digest": "3599d52a3fdcd9a0f2993bd341b4aadf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4316, "upload_time": "2019-11-13T19:12:39", "upload_time_iso_8601": "2019-11-13T19:12:39.448899Z", "url": "https://files.pythonhosted.org/packages/2b/b7/cd51c4098e3fc31e345824f599414aad133817dbfb441cee69a6c8fdc2cd/DeerNet-0.0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:39:17 2020"}