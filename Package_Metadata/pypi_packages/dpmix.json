{"info": {"author": "Jacob Frelinger", "author_email": "jacob.frelinger@duke.edu", "bugtrack_url": null, "classifiers": [], "description": "=====\ndpmix\n=====\n\ndpmix is a library for understanding posterior distributions for\nDirichlet and heirarchical Dirichlet mixtures of normal distributions\nrepresented by truncated stick breaking.\n\nRequirements\n------------\n\n* NumPy\n* SciPy\n* Cython\n* PyCUDA\n* cyarma\n* cyrand\n* scikits.cuda\n* gpustats\n* mpi4py\n\nInstallation and testing\n------------------------\n\nInstall via\n\n::\n\n   python setup.py install\n\nTo test, run the scripts in the \"test\" subfolder.\n\nUsage\n-----\n\nCheck out the class docstrings for more info.\n\nMPI\n---\n\nThe multigpu facilities are developed using MPI. Therefore, \nusing multiple machines is possible. However, note that the\nmachines must be configured the same way. (Python)\n\nRunning the code on multiple machines requires *mpiexec*:\n\n::\n\n   mpiexec -hostfile my_hosts -np 1 python tests/test_dpmix.py --gpu MPI\n\nWhere the *my_hosts* file looks like \n\n::\n\n   host1 slots=3\n   host2 slots=2\n\nI'm assuming here that the master instance of python is running on host1\nand that host1 and host2 have 2 GPUs each. Note, an extra slot needs to be\nreserved for the master on host1. Furthermore, we need to specify which\ndevices to use on each host. The *gpu* argument in the class constructors\nmust be a dictionary like\n\n::\n\n  gpu={'host1': [0,1], 'host2': [0,1]}\n\nThe keys must match the result of a call to *os.uname()* to get the\nhost string.\n\n\n\n", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/andrewcron/pycdp", "keywords": null, "license": "UNKNOWN", "maintainer": null, "maintainer_email": null, "name": "dpmix", "package_url": "https://pypi.org/project/dpmix/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/dpmix/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/andrewcron/pycdp"}, "release_url": "https://pypi.org/project/dpmix/0.3/", "requires_dist": null, "requires_python": null, "summary": "Optimized (and optionally gpu enhaced) fitting of Gaussian Mixture Models", "version": "0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>dpmix is a library for understanding posterior distributions for\nDirichlet and heirarchical Dirichlet mixtures of normal distributions\nrepresented by truncated stick breaking.</p>\n<div id=\"requirements\">\n<h2>Requirements</h2>\n<ul>\n<li>NumPy</li>\n<li>SciPy</li>\n<li>Cython</li>\n<li>PyCUDA</li>\n<li>cyarma</li>\n<li>cyrand</li>\n<li>scikits.cuda</li>\n<li>gpustats</li>\n<li>mpi4py</li>\n</ul>\n</div>\n<div id=\"installation-and-testing\">\n<h2>Installation and testing</h2>\n<p>Install via</p>\n<pre>python setup.py install\n</pre>\n<p>To test, run the scripts in the \u201ctest\u201d subfolder.</p>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>Check out the class docstrings for more info.</p>\n</div>\n<div id=\"mpi\">\n<h2>MPI</h2>\n<p>The multigpu facilities are developed using MPI. Therefore,\nusing multiple machines is possible. However, note that the\nmachines must be configured the same way. (Python)</p>\n<p>Running the code on multiple machines requires <em>mpiexec</em>:</p>\n<pre>mpiexec -hostfile my_hosts -np 1 python tests/test_dpmix.py --gpu MPI\n</pre>\n<p>Where the <em>my_hosts</em> file looks like</p>\n<pre>host1 slots=3\nhost2 slots=2\n</pre>\n<p>I\u2019m assuming here that the master instance of python is running on host1\nand that host1 and host2 have 2 GPUs each. Note, an extra slot needs to be\nreserved for the master on host1. Furthermore, we need to specify which\ndevices to use on each host. The <em>gpu</em> argument in the class constructors\nmust be a dictionary like</p>\n<pre>gpu={'host1': [0,1], 'host2': [0,1]}\n</pre>\n<p>The keys must match the result of a call to <em>os.uname()</em> to get the\nhost string.</p>\n</div>\n\n          </div>"}, "last_serial": 713256, "releases": {"0.2": [{"comment_text": "", "digests": {"md5": "4df9268bca2e89d0dca4c3de286dcf5e", "sha256": "e7088d8b5d5521fac26ae5f01d32a081e3423b35d82e70158d8b3b216b60b54b"}, "downloads": -1, "filename": "dpmix-0.2.tar.gz", "has_sig": false, "md5_digest": "4df9268bca2e89d0dca4c3de286dcf5e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26770, "upload_time": "2013-05-29T00:44:02", "upload_time_iso_8601": "2013-05-29T00:44:02.585390Z", "url": "https://files.pythonhosted.org/packages/7e/06/15b7e14dfef4bc4771cfb9a142fe715cc4777142c8c5c34d49fbcf534279/dpmix-0.2.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "371ee2632302132fc2eeffc23b072423", "sha256": "316141f0141395141f53a2886d0640c61d9f8093ed127196b0195ba34f1bb0aa"}, "downloads": -1, "filename": "dpmix-0.3.tar.gz", "has_sig": false, "md5_digest": "371ee2632302132fc2eeffc23b072423", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 130121, "upload_time": "2013-05-29T02:04:33", "upload_time_iso_8601": "2013-05-29T02:04:33.331032Z", "url": "https://files.pythonhosted.org/packages/cb/d0/0ec19b4c83786348755ec8debe81c44be04631fccb7b4fa3e2a163cc09c0/dpmix-0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "371ee2632302132fc2eeffc23b072423", "sha256": "316141f0141395141f53a2886d0640c61d9f8093ed127196b0195ba34f1bb0aa"}, "downloads": -1, "filename": "dpmix-0.3.tar.gz", "has_sig": false, "md5_digest": "371ee2632302132fc2eeffc23b072423", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 130121, "upload_time": "2013-05-29T02:04:33", "upload_time_iso_8601": "2013-05-29T02:04:33.331032Z", "url": "https://files.pythonhosted.org/packages/cb/d0/0ec19b4c83786348755ec8debe81c44be04631fccb7b4fa3e2a163cc09c0/dpmix-0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:01 2020"}