{"info": {"author": "Guillaume Dubuisson Duplessis", "author_email": "guillaume@dubuissonduplessis.fr", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Science/Research", "License :: OSI Approved :: BSD License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# gowpy\n\nA very simple framework for exploiting graph-of-words in NLP. \nCurrently at version **0.1.0** (alpha).\n\ngowpy leverages graph-of-words representation in order to do:\n- **document classification** in a [scikit-learn](https://scikit-learn.org)-like \n  way via useful vectorizers, and\n- **keyword extraction** from a document. \n\n## Quick Start\n### Requirements and Installation\nThis project is based on Python 3.6+, [scikit-learn](https://github.com/scikit-learn/scikit-learn) and \n[NetworkX](https://github.com/networkx/networkx). \n\n#### Installation from PyPI\n```bash\npip install gowpy\n```\n\n#### Installation from the GitHub Source\nFirst, clone the project:\n```bash\ngit clone https://github.com/GuillaumeDD/gowpy.git\n```\n\nThen, `cd` to the project folder and run the install command:\n```bash\ncd gowpy/\npython setup.py install\n```\n\n### Example Usage\n\n#### Building a Graph-of-Words from a Document\n\n```python\nfrom gowpy.gow.builder import GoWBuilder\n\n# Creation of a graph-of-words builder\n# Here:\n# - the graph-of-words will be directed, and\n# - an edge will link every tokens co-occurring in a sliding window of size 4\n#\nbuilder = GoWBuilder(directed=True, window_size=4)\n\ntext = \"\"\"gowpy is a simple framework for exploiting graph-of-words in nlp gowpy \nleverages graph-of-words representation for document classification and for keyword extraction \nfrom a document\"\"\"\n\n# Here, a preprocessing step fitted to the need of the project should be carried out \n\n# Creation of the graph-of-words\ngow = builder.compute_gow_from_document(text)\n```\n\nThen, it is possible to visualize the document as a graph-of-words:\n```python\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\ng = gow.to_labeled_graph()\n\noptions = {\n    \"font_weight\" : 'normal',\n    \"font_color\" : 'darkblue',\n    #\n    \"edge_color\" : 'lightgray',\n    #\n    \"node_size\" : 200,\n    \"node_color\": 'white',\n    \"with_labels\": True,\n}\nnx.draw(g, **options)\n``` \n\n![A graph-of-words example](./resources/gow.png)\n\n#### Unsupervised Keywords Extraction\nGraph-of-words can be leveraged to extract an automatically adaptative number of\ncohesive keywords from a text document in an unsupervised fashion [[2,3]](#references).\n\n```python\nfrom gowpy.summarization.unsupervised import GoWKeywordExtractor\n\n# Initialization of the keyword extractor\nextractor_kw = GoWKeywordExtractor(directed=False, window_size=4)\n\n# \n# Note that preprocessing is particularly important for keyword extraction\n# in order to keep and normalize important terms such as adjectives and nouns.\n#\n# An already preprocessed text in which to extract keywords\npreprocessed_text = \"\"\"gowpy simple framework exploiting graph-of-words nlp gowpy \nleverages graph-of-words representation document classification keyword extraction \ndocument\"\"\"\n\nextractor_kw.extract(preprocessed_text)\n```\n\nReturns:\n```text\n[('gowpy', 4),\n ('simple', 4),\n ('framework', 4),\n ('exploiting', 4),\n ('graph-of-words', 4),\n ('nlp', 4)]\n```\n\n\n#### Classification with TW-IDF: a graph-based term weighting score\nTW-IDF [[0]](#references) challenges the term independence assumption behind \nthe bag-of-words model by (i) exploiting a graph-of-words representation of a \ndocument (here an unweighted directed graph of terms), and by (ii) leveraging \nthis new representation to replace the term frequency (TF) by graph-based term \nweights (TW).  \n\nTW-IDF is accessible via a dedicated vectorizer:\n```python\nfrom gowpy.feature_extraction.gow import TwidfVectorizer\n\ncorpus = [\n    'hello world !',\n    'foo bar'\n]\n\nvectorizer_gow = TwidfVectorizer(                 \n    # Graph-of-words specificities\n    directed=True,\n    window_size=4,\n    # Token frequency filtering\n    min_df=0.0,\n    max_df=1.0,\n    # Graph-based term weighting approach\n    term_weighting='degree'\n)\n\nX = vectorizer_gow.fit_transform(corpus)\nX\n```\nReturns:\n```text\n<2x5 sparse matrix of type '<class 'numpy.float64'>'\n\twith 3 stored elements in Compressed Sparse Row format>\n```\n\nTW-IDF vectorizer fits seamlessly in a grid search:\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import GridSearchCV\n\npipeline = Pipeline([\n    ('gow', TwidfVectorizer()),\n    ('svm', SVC()),\n])\n\nparameters = {\n    'gow__directed' : [True, False],\n    'gow__window_size' : [2, 4, 8, 16],\n    'gow__b' : [0.0, 0.003],\n    'gow__term_weighting' : ['degree', 'pagerank'],\n    'gow__min_df' : [0, 5, 10],\n    'gow__max_df' : [0.8, 0.9, 1.0],\n#\n    'svm__C' : [0.1, 1, 10],\n    'svm__kernel' : ['linear']\n}\n\n# find the best parameters for both the feature extraction and the\n# classifier\ngrid_search = GridSearchCV(pipeline, \n                           parameters, \n                           cv=10,\n                           n_jobs=-1)\n```\n\n#### Going further: classification based on frequent subgraphs\nFrequent subgraphs corresponding to long range n-gram can be mined and \nsubsequently used for document classification [[1]](#references).\n\nClassification with frequent subgraphs happens in a 3-step process:\n1. Conversion of the corpus of already preprocessed documents into a collection\n  of graph-of-words\n1. Mining the frequent subgraphs\n1. Loading the frequent subgraphs and exploiting them for classification\n\n##### Conversion of the corpus into a collection of graph-of-words\nThe first step consists in turning the corpus into a graph-of-words and collection\nand then export that collection into a file format suited for frequent subgraph\nmining.\n```python\nfrom gowpy.gow.miner import GoWMiner\nimport gowpy.gow.io\n\ncorpus = [\n    'hello world !',\n    'foo bar',\n    # and many more...\n]\n\n# Conversation of the corpus into a collection of graph-of-words\ngow_miner = GoWMiner(directed=False, window_size=4)\ncorpus_gows = gow_miner.compute_gow_from_corpus(corpus)\n\n# Exportation of the collection of graph-of-words into a file for\n# interoperability with other languages such as C++\nwith open(\"corpus_gows.data\", \"w\") as f_output:\n    data = gowpy.gow.io.gow_to_data(corpus_gows)\n    f_output.write(data)\n```\n\n##### Mining the frequent subgraphs\nFrequent subgraphs mining can be realized via the [gSpan algorithm](https://www.cs.ucsb.edu/~xyan/software/gSpan.htm).\nThis step is not included in this project and has to be carried out by another\nprogram.\n\nThis project supports the reimplementation from [gBolt available at GitHub](https://github.com/Jokeren/gBolt).\nCurrently this implementation is limited to **undirected graph**.\nTo mine frequent subgraphs (after having installed gBolt on your machine):\n```bash\nOMP_NUM_THREADS=1 ./gbolt --input corpus_gows.data --output gbolt-mining-corpus_gow --dfs --nodes --support 0.01\n```\nNotice the **support parameter** which defines the minimum frequency of a subgraph\nto be considered as frequent. Here it is set to 1% (0.01).\nThis parameter is **corpus specific** and should be carefully tuned (see [[1]](#references)).\n\nMining produces two files:\n- `gbolt-mining-corpus_gow.t0`: the frequent subgraphs with more than one node\n- `gbolt-mining-corpus_gow.nodes`: the frequent single nodes\n\nThese two files can be loaded by the same `gow_miner` used for exportation:\n```python\ngow_miner.load_graphs('gbolt-mining-corpus_gow.t0', \n                      'gbolt-mining-corpus_gow.nodes')\ngow_miner\n```\nReturns:\n```text\nGraph-of-word miner:\n        - is_directed: False\n        - window_size: 4\n        - edge_labeling: True\n\n        - Number of tokens: 5\n        - Number of links between tokens: 4\n\n        - Number of loaded subgraph: 13\n```\n\n##### Classification with frequent subgraphs\nClassification with frequent subgraphs is accessible via a dedicated vectorizer:\n```python\nfrom gowpy.feature_extraction.gow import GoWVectorizer\n\nvectorizer_gow = GoWVectorizer(gow_miner)\nX = vectorizer_gow.fit_transform(corpus)\n# X is a sparse matrix\n```\n\nBefore tuning the `min_df` (the minimum being the support chosen during mining)\nand the `max_df`, it is possible the have a look at the normalized frequency\ndistribution:\n```python\nimport pandas as pd\ns_freq_per_pattern = pd.Series(gow_miner.stat_relative_freq_per_pattern())\ns_freq_per_pattern.describe()\n```\nFor instance, it can returns the following distribution:\n```text\ncount    10369.000000\nmean         0.026639\nstd          0.046551\nmin          0.008333\n25%          0.010000\n50%          0.013333\n75%          0.022778\nmax          0.865000\ndtype: float64\n```\n\n\nGoW vectorizer fits nicely in a grid search:\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom sklearn.model_selection import GridSearchCV\n\npipeline = Pipeline([\n    ('gow', GoWVectorizer(gow_miner)),\n    ('tfidf', TfidfTransformer()),\n    ('svm', SVC()),\n])\n\nparameters = {\n    'gow__subgraph_matching' : ['partial', 'induced'],\n    'gow__min_df' : [0.00833, 0.01, 0.013333],\n    'gow__max_df' : [0.022778, 0.5, 0.865],\n#\n    'svm__C' : [0.1, 1, 10],\n    'svm__kernel' : ['linear']\n}\n\n# find the best parameters for both the feature extraction and the\n# classifier\ngrid_search = GridSearchCV(pipeline, \n                           parameters, \n                           cv=10,\n                           n_jobs=-1)\n```\n\n## References\n\nDetailed explanations, evaluations and discussions can be found in these papers:\n- Information retrieval (TW-IDF)\n   + [0] [Graph-of-word and TW-IDF: New Approach to Ad Hoc IR](https://dl.acm.org/doi/abs/10.1145/2505515.2505671).\n     *Rousseau, Fran\u00e7ois, and Michalis Vazirgiannis*.\n     *Proceedings of the 22nd ACM international conference on Information & Knowledge Management*.(**CIKM 2013**)\n- Document classification with frequent subgraphs\n   + [1] [Text Categorization as a Graph Classification Problem](http://www.aclweb.org/anthology/P15-1164).\n      *Rousseau, Fran\u00e7ois, Emmanouil Kiagias, and Michalis Vazirgiannis*.\n      *Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International \n      Joint Conference on Natural Language Processing* (**ACL 2015**)\n- Keyword extraction from graph-of-words\n   + [2] [Main Core Retention on Graph-of-words for Single-Document Keyword Extraction](https://link.springer.com/chapter/10.1007/978-3-319-16354-3_42).\n     *Rousseau, Fran\u00e7ois, and Michalis Vazirgiannis*.\n     *Proceedings of the 37th European Conference on Information Retrieval*.\n     (**ECIR 2015**)\n   + [3] [A Graph Degeneracy-based Approach to Keyword Extraction](https://www.aclweb.org/anthology/D16-1191/).\n     *Tiwier, Antoine Tixier, Malliaros Fragkiskos, and Vazirgiannis, Michalis*.\n     *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*.\n     (**EMNLP 2016**)\n\nThis library involves the following algorithms:\n- Frequent subgraph Mining (**currently not included in this library**)\n   + gSpan algorithm implementation for subgraph mining: [gBolt--very fast implementation for gSpan algorithm in data mining ](https://github.com/Jokeren/gBolt)\n- Subgraph matching\n   + VF2 algorithm for subgraph isomorphism matching: [VF2 algorithm for subgraph isomorphism from NetworkX](https://networkx.github.io/documentation/stable/reference/algorithms/isomorphism.vf2.html)\n- Graph degeneracy\n   + [k-core decomposition with NetworkX](https://networkx.github.io/documentation/stable/reference/algorithms/core.html)\n\n\n## License\nReleased under the 3-Clause BSD license (see [LICENSE file](./LICENSE))\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/GuillaumeDD/gowpy.git", "keywords": "", "license": "new BSD", "maintainer": "", "maintainer_email": "", "name": "gowpy", "package_url": "https://pypi.org/project/gowpy/", "platform": "", "project_url": "https://pypi.org/project/gowpy/", "project_urls": {"Homepage": "https://github.com/GuillaumeDD/gowpy.git"}, "release_url": "https://pypi.org/project/gowpy/0.1.0/", "requires_dist": ["networkx (>=2.4)", "scikit-learn (>=0.22.2)", "matplotlib (>=3.1)"], "requires_python": ">=3.6", "summary": "A very simple graph-of-words framework for NLP", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>gowpy</h1>\n<p>A very simple framework for exploiting graph-of-words in NLP.\nCurrently at version <strong>0.1.0</strong> (alpha).</p>\n<p>gowpy leverages graph-of-words representation in order to do:</p>\n<ul>\n<li><strong>document classification</strong> in a <a href=\"https://scikit-learn.org\" rel=\"nofollow\">scikit-learn</a>-like\nway via useful vectorizers, and</li>\n<li><strong>keyword extraction</strong> from a document.</li>\n</ul>\n<h2>Quick Start</h2>\n<h3>Requirements and Installation</h3>\n<p>This project is based on Python 3.6+, <a href=\"https://github.com/scikit-learn/scikit-learn\" rel=\"nofollow\">scikit-learn</a> and\n<a href=\"https://github.com/networkx/networkx\" rel=\"nofollow\">NetworkX</a>.</p>\n<h4>Installation from PyPI</h4>\n<pre>pip install gowpy\n</pre>\n<h4>Installation from the GitHub Source</h4>\n<p>First, clone the project:</p>\n<pre>git clone https://github.com/GuillaumeDD/gowpy.git\n</pre>\n<p>Then, <code>cd</code> to the project folder and run the install command:</p>\n<pre><span class=\"nb\">cd</span> gowpy/\npython setup.py install\n</pre>\n<h3>Example Usage</h3>\n<h4>Building a Graph-of-Words from a Document</h4>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">gowpy.gow.builder</span> <span class=\"kn\">import</span> <span class=\"n\">GoWBuilder</span>\n\n<span class=\"c1\"># Creation of a graph-of-words builder</span>\n<span class=\"c1\"># Here:</span>\n<span class=\"c1\"># - the graph-of-words will be directed, and</span>\n<span class=\"c1\"># - an edge will link every tokens co-occurring in a sliding window of size 4</span>\n<span class=\"c1\">#</span>\n<span class=\"n\">builder</span> <span class=\"o\">=</span> <span class=\"n\">GoWBuilder</span><span class=\"p\">(</span><span class=\"n\">directed</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">window_size</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s2\">\"\"\"gowpy is a simple framework for exploiting graph-of-words in nlp gowpy </span>\n<span class=\"s2\">leverages graph-of-words representation for document classification and for keyword extraction </span>\n<span class=\"s2\">from a document\"\"\"</span>\n\n<span class=\"c1\"># Here, a preprocessing step fitted to the need of the project should be carried out </span>\n\n<span class=\"c1\"># Creation of the graph-of-words</span>\n<span class=\"n\">gow</span> <span class=\"o\">=</span> <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">compute_gow_from_document</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n</pre>\n<p>Then, it is possible to visualize the document as a graph-of-words:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">networkx</span> <span class=\"k\">as</span> <span class=\"nn\">nx</span>\n\n<span class=\"n\">g</span> <span class=\"o\">=</span> <span class=\"n\">gow</span><span class=\"o\">.</span><span class=\"n\">to_labeled_graph</span><span class=\"p\">()</span>\n\n<span class=\"n\">options</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">\"font_weight\"</span> <span class=\"p\">:</span> <span class=\"s1\">'normal'</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"font_color\"</span> <span class=\"p\">:</span> <span class=\"s1\">'darkblue'</span><span class=\"p\">,</span>\n    <span class=\"c1\">#</span>\n    <span class=\"s2\">\"edge_color\"</span> <span class=\"p\">:</span> <span class=\"s1\">'lightgray'</span><span class=\"p\">,</span>\n    <span class=\"c1\">#</span>\n    <span class=\"s2\">\"node_size\"</span> <span class=\"p\">:</span> <span class=\"mi\">200</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"node_color\"</span><span class=\"p\">:</span> <span class=\"s1\">'white'</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"with_labels\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n<span class=\"n\">nx</span><span class=\"o\">.</span><span class=\"n\">draw</span><span class=\"p\">(</span><span class=\"n\">g</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">options</span><span class=\"p\">)</span>\n</pre>\n<p><img alt=\"A graph-of-words example\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4586b43d3c8abd3aff78b579489c3afdbd10aedf/2e2f7265736f75726365732f676f772e706e67\"></p>\n<h4>Unsupervised Keywords Extraction</h4>\n<p>Graph-of-words can be leveraged to extract an automatically adaptative number of\ncohesive keywords from a text document in an unsupervised fashion <a href=\"#references\" rel=\"nofollow\">[2,3]</a>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">gowpy.summarization.unsupervised</span> <span class=\"kn\">import</span> <span class=\"n\">GoWKeywordExtractor</span>\n\n<span class=\"c1\"># Initialization of the keyword extractor</span>\n<span class=\"n\">extractor_kw</span> <span class=\"o\">=</span> <span class=\"n\">GoWKeywordExtractor</span><span class=\"p\">(</span><span class=\"n\">directed</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">window_size</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># </span>\n<span class=\"c1\"># Note that preprocessing is particularly important for keyword extraction</span>\n<span class=\"c1\"># in order to keep and normalize important terms such as adjectives and nouns.</span>\n<span class=\"c1\">#</span>\n<span class=\"c1\"># An already preprocessed text in which to extract keywords</span>\n<span class=\"n\">preprocessed_text</span> <span class=\"o\">=</span> <span class=\"s2\">\"\"\"gowpy simple framework exploiting graph-of-words nlp gowpy </span>\n<span class=\"s2\">leverages graph-of-words representation document classification keyword extraction </span>\n<span class=\"s2\">document\"\"\"</span>\n\n<span class=\"n\">extractor_kw</span><span class=\"o\">.</span><span class=\"n\">extract</span><span class=\"p\">(</span><span class=\"n\">preprocessed_text</span><span class=\"p\">)</span>\n</pre>\n<p>Returns:</p>\n<pre>[('gowpy', 4),\n ('simple', 4),\n ('framework', 4),\n ('exploiting', 4),\n ('graph-of-words', 4),\n ('nlp', 4)]\n</pre>\n<h4>Classification with TW-IDF: a graph-based term weighting score</h4>\n<p>TW-IDF <a href=\"#references\" rel=\"nofollow\">[0]</a> challenges the term independence assumption behind\nthe bag-of-words model by (i) exploiting a graph-of-words representation of a\ndocument (here an unweighted directed graph of terms), and by (ii) leveraging\nthis new representation to replace the term frequency (TF) by graph-based term\nweights (TW).</p>\n<p>TW-IDF is accessible via a dedicated vectorizer:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">gowpy.feature_extraction.gow</span> <span class=\"kn\">import</span> <span class=\"n\">TwidfVectorizer</span>\n\n<span class=\"n\">corpus</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"s1\">'hello world !'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'foo bar'</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">vectorizer_gow</span> <span class=\"o\">=</span> <span class=\"n\">TwidfVectorizer</span><span class=\"p\">(</span>                 \n    <span class=\"c1\"># Graph-of-words specificities</span>\n    <span class=\"n\">directed</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">window_size</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span>\n    <span class=\"c1\"># Token frequency filtering</span>\n    <span class=\"n\">min_df</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">,</span>\n    <span class=\"n\">max_df</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">,</span>\n    <span class=\"c1\"># Graph-based term weighting approach</span>\n    <span class=\"n\">term_weighting</span><span class=\"o\">=</span><span class=\"s1\">'degree'</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">vectorizer_gow</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">corpus</span><span class=\"p\">)</span>\n<span class=\"n\">X</span>\n</pre>\n<p>Returns:</p>\n<pre>&lt;2x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n\twith 3 stored elements in Compressed Sparse Row format&gt;\n</pre>\n<p>TW-IDF vectorizer fits seamlessly in a grid search:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sklearn.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">Pipeline</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.svm</span> <span class=\"kn\">import</span> <span class=\"n\">SVC</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">GridSearchCV</span>\n\n<span class=\"n\">pipeline</span> <span class=\"o\">=</span> <span class=\"n\">Pipeline</span><span class=\"p\">([</span>\n    <span class=\"p\">(</span><span class=\"s1\">'gow'</span><span class=\"p\">,</span> <span class=\"n\">TwidfVectorizer</span><span class=\"p\">()),</span>\n    <span class=\"p\">(</span><span class=\"s1\">'svm'</span><span class=\"p\">,</span> <span class=\"n\">SVC</span><span class=\"p\">()),</span>\n<span class=\"p\">])</span>\n\n<span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s1\">'gow__directed'</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"kc\">False</span><span class=\"p\">],</span>\n    <span class=\"s1\">'gow__window_size'</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">16</span><span class=\"p\">],</span>\n    <span class=\"s1\">'gow__b'</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.003</span><span class=\"p\">],</span>\n    <span class=\"s1\">'gow__term_weighting'</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">'degree'</span><span class=\"p\">,</span> <span class=\"s1\">'pagerank'</span><span class=\"p\">],</span>\n    <span class=\"s1\">'gow__min_df'</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">],</span>\n    <span class=\"s1\">'gow__max_df'</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">],</span>\n<span class=\"c1\">#</span>\n    <span class=\"s1\">'svm__C'</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">],</span>\n    <span class=\"s1\">'svm__kernel'</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">'linear'</span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n\n<span class=\"c1\"># find the best parameters for both the feature extraction and the</span>\n<span class=\"c1\"># classifier</span>\n<span class=\"n\">grid_search</span> <span class=\"o\">=</span> <span class=\"n\">GridSearchCV</span><span class=\"p\">(</span><span class=\"n\">pipeline</span><span class=\"p\">,</span> \n                           <span class=\"n\">parameters</span><span class=\"p\">,</span> \n                           <span class=\"n\">cv</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n                           <span class=\"n\">n_jobs</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n</pre>\n<h4>Going further: classification based on frequent subgraphs</h4>\n<p>Frequent subgraphs corresponding to long range n-gram can be mined and\nsubsequently used for document classification <a href=\"#references\" rel=\"nofollow\">[1]</a>.</p>\n<p>Classification with frequent subgraphs happens in a 3-step process:</p>\n<ol>\n<li>Conversion of the corpus of already preprocessed documents into a collection\nof graph-of-words</li>\n<li>Mining the frequent subgraphs</li>\n<li>Loading the frequent subgraphs and exploiting them for classification</li>\n</ol>\n<h5>Conversion of the corpus into a collection of graph-of-words</h5>\n<p>The first step consists in turning the corpus into a graph-of-words and collection\nand then export that collection into a file format suited for frequent subgraph\nmining.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">gowpy.gow.miner</span> <span class=\"kn\">import</span> <span class=\"n\">GoWMiner</span>\n<span class=\"kn\">import</span> <span class=\"nn\">gowpy.gow.io</span>\n\n<span class=\"n\">corpus</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"s1\">'hello world !'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'foo bar'</span><span class=\"p\">,</span>\n    <span class=\"c1\"># and many more...</span>\n<span class=\"p\">]</span>\n\n<span class=\"c1\"># Conversation of the corpus into a collection of graph-of-words</span>\n<span class=\"n\">gow_miner</span> <span class=\"o\">=</span> <span class=\"n\">GoWMiner</span><span class=\"p\">(</span><span class=\"n\">directed</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">window_size</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"n\">corpus_gows</span> <span class=\"o\">=</span> <span class=\"n\">gow_miner</span><span class=\"o\">.</span><span class=\"n\">compute_gow_from_corpus</span><span class=\"p\">(</span><span class=\"n\">corpus</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Exportation of the collection of graph-of-words into a file for</span>\n<span class=\"c1\"># interoperability with other languages such as C++</span>\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">\"corpus_gows.data\"</span><span class=\"p\">,</span> <span class=\"s2\">\"w\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f_output</span><span class=\"p\">:</span>\n    <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">gowpy</span><span class=\"o\">.</span><span class=\"n\">gow</span><span class=\"o\">.</span><span class=\"n\">io</span><span class=\"o\">.</span><span class=\"n\">gow_to_data</span><span class=\"p\">(</span><span class=\"n\">corpus_gows</span><span class=\"p\">)</span>\n    <span class=\"n\">f_output</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n</pre>\n<h5>Mining the frequent subgraphs</h5>\n<p>Frequent subgraphs mining can be realized via the <a href=\"https://www.cs.ucsb.edu/%7Exyan/software/gSpan.htm\" rel=\"nofollow\">gSpan algorithm</a>.\nThis step is not included in this project and has to be carried out by another\nprogram.</p>\n<p>This project supports the reimplementation from <a href=\"https://github.com/Jokeren/gBolt\" rel=\"nofollow\">gBolt available at GitHub</a>.\nCurrently this implementation is limited to <strong>undirected graph</strong>.\nTo mine frequent subgraphs (after having installed gBolt on your machine):</p>\n<pre><span class=\"nv\">OMP_NUM_THREADS</span><span class=\"o\">=</span><span class=\"m\">1</span> ./gbolt --input corpus_gows.data --output gbolt-mining-corpus_gow --dfs --nodes --support <span class=\"m\">0</span>.01\n</pre>\n<p>Notice the <strong>support parameter</strong> which defines the minimum frequency of a subgraph\nto be considered as frequent. Here it is set to 1% (0.01).\nThis parameter is <strong>corpus specific</strong> and should be carefully tuned (see <a href=\"#references\" rel=\"nofollow\">[1]</a>).</p>\n<p>Mining produces two files:</p>\n<ul>\n<li><code>gbolt-mining-corpus_gow.t0</code>: the frequent subgraphs with more than one node</li>\n<li><code>gbolt-mining-corpus_gow.nodes</code>: the frequent single nodes</li>\n</ul>\n<p>These two files can be loaded by the same <code>gow_miner</code> used for exportation:</p>\n<pre><span class=\"n\">gow_miner</span><span class=\"o\">.</span><span class=\"n\">load_graphs</span><span class=\"p\">(</span><span class=\"s1\">'gbolt-mining-corpus_gow.t0'</span><span class=\"p\">,</span> \n                      <span class=\"s1\">'gbolt-mining-corpus_gow.nodes'</span><span class=\"p\">)</span>\n<span class=\"n\">gow_miner</span>\n</pre>\n<p>Returns:</p>\n<pre>Graph-of-word miner:\n        - is_directed: False\n        - window_size: 4\n        - edge_labeling: True\n\n        - Number of tokens: 5\n        - Number of links between tokens: 4\n\n        - Number of loaded subgraph: 13\n</pre>\n<h5>Classification with frequent subgraphs</h5>\n<p>Classification with frequent subgraphs is accessible via a dedicated vectorizer:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">gowpy.feature_extraction.gow</span> <span class=\"kn\">import</span> <span class=\"n\">GoWVectorizer</span>\n\n<span class=\"n\">vectorizer_gow</span> <span class=\"o\">=</span> <span class=\"n\">GoWVectorizer</span><span class=\"p\">(</span><span class=\"n\">gow_miner</span><span class=\"p\">)</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">vectorizer_gow</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">corpus</span><span class=\"p\">)</span>\n<span class=\"c1\"># X is a sparse matrix</span>\n</pre>\n<p>Before tuning the <code>min_df</code> (the minimum being the support chosen during mining)\nand the <code>max_df</code>, it is possible the have a look at the normalized frequency\ndistribution:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"n\">s_freq_per_pattern</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">Series</span><span class=\"p\">(</span><span class=\"n\">gow_miner</span><span class=\"o\">.</span><span class=\"n\">stat_relative_freq_per_pattern</span><span class=\"p\">())</span>\n<span class=\"n\">s_freq_per_pattern</span><span class=\"o\">.</span><span class=\"n\">describe</span><span class=\"p\">()</span>\n</pre>\n<p>For instance, it can returns the following distribution:</p>\n<pre>count    10369.000000\nmean         0.026639\nstd          0.046551\nmin          0.008333\n25%          0.010000\n50%          0.013333\n75%          0.022778\nmax          0.865000\ndtype: float64\n</pre>\n<p>GoW vectorizer fits nicely in a grid search:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sklearn.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">Pipeline</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.svm</span> <span class=\"kn\">import</span> <span class=\"n\">SVC</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_extraction.text</span> <span class=\"kn\">import</span> <span class=\"n\">TfidfTransformer</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">GridSearchCV</span>\n\n<span class=\"n\">pipeline</span> <span class=\"o\">=</span> <span class=\"n\">Pipeline</span><span class=\"p\">([</span>\n    <span class=\"p\">(</span><span class=\"s1\">'gow'</span><span class=\"p\">,</span> <span class=\"n\">GoWVectorizer</span><span class=\"p\">(</span><span class=\"n\">gow_miner</span><span class=\"p\">)),</span>\n    <span class=\"p\">(</span><span class=\"s1\">'tfidf'</span><span class=\"p\">,</span> <span class=\"n\">TfidfTransformer</span><span class=\"p\">()),</span>\n    <span class=\"p\">(</span><span class=\"s1\">'svm'</span><span class=\"p\">,</span> <span class=\"n\">SVC</span><span class=\"p\">()),</span>\n<span class=\"p\">])</span>\n\n<span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s1\">'gow__subgraph_matching'</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">'partial'</span><span class=\"p\">,</span> <span class=\"s1\">'induced'</span><span class=\"p\">],</span>\n    <span class=\"s1\">'gow__min_df'</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.00833</span><span class=\"p\">,</span> <span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"mf\">0.013333</span><span class=\"p\">],</span>\n    <span class=\"s1\">'gow__max_df'</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.022778</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.865</span><span class=\"p\">],</span>\n<span class=\"c1\">#</span>\n    <span class=\"s1\">'svm__C'</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">],</span>\n    <span class=\"s1\">'svm__kernel'</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">'linear'</span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n\n<span class=\"c1\"># find the best parameters for both the feature extraction and the</span>\n<span class=\"c1\"># classifier</span>\n<span class=\"n\">grid_search</span> <span class=\"o\">=</span> <span class=\"n\">GridSearchCV</span><span class=\"p\">(</span><span class=\"n\">pipeline</span><span class=\"p\">,</span> \n                           <span class=\"n\">parameters</span><span class=\"p\">,</span> \n                           <span class=\"n\">cv</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n                           <span class=\"n\">n_jobs</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n</pre>\n<h2>References</h2>\n<p>Detailed explanations, evaluations and discussions can be found in these papers:</p>\n<ul>\n<li>Information retrieval (TW-IDF)\n<ul>\n<li>[0] <a href=\"https://dl.acm.org/doi/abs/10.1145/2505515.2505671\" rel=\"nofollow\">Graph-of-word and TW-IDF: New Approach to Ad Hoc IR</a>.\n<em>Rousseau, Fran\u00e7ois, and Michalis Vazirgiannis</em>.\n<em>Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</em>.(<strong>CIKM 2013</strong>)</li>\n</ul>\n</li>\n<li>Document classification with frequent subgraphs\n<ul>\n<li>[1] <a href=\"http://www.aclweb.org/anthology/P15-1164\" rel=\"nofollow\">Text Categorization as a Graph Classification Problem</a>.\n<em>Rousseau, Fran\u00e7ois, Emmanouil Kiagias, and Michalis Vazirgiannis</em>.\n<em>Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing</em> (<strong>ACL 2015</strong>)</li>\n</ul>\n</li>\n<li>Keyword extraction from graph-of-words\n<ul>\n<li>[2] <a href=\"https://link.springer.com/chapter/10.1007/978-3-319-16354-3_42\" rel=\"nofollow\">Main Core Retention on Graph-of-words for Single-Document Keyword Extraction</a>.\n<em>Rousseau, Fran\u00e7ois, and Michalis Vazirgiannis</em>.\n<em>Proceedings of the 37th European Conference on Information Retrieval</em>.\n(<strong>ECIR 2015</strong>)</li>\n<li>[3] <a href=\"https://www.aclweb.org/anthology/D16-1191/\" rel=\"nofollow\">A Graph Degeneracy-based Approach to Keyword Extraction</a>.\n<em>Tiwier, Antoine Tixier, Malliaros Fragkiskos, and Vazirgiannis, Michalis</em>.\n<em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>.\n(<strong>EMNLP 2016</strong>)</li>\n</ul>\n</li>\n</ul>\n<p>This library involves the following algorithms:</p>\n<ul>\n<li>Frequent subgraph Mining (<strong>currently not included in this library</strong>)\n<ul>\n<li>gSpan algorithm implementation for subgraph mining: <a href=\"https://github.com/Jokeren/gBolt\" rel=\"nofollow\">gBolt--very fast implementation for gSpan algorithm in data mining </a></li>\n</ul>\n</li>\n<li>Subgraph matching\n<ul>\n<li>VF2 algorithm for subgraph isomorphism matching: <a href=\"https://networkx.github.io/documentation/stable/reference/algorithms/isomorphism.vf2.html\" rel=\"nofollow\">VF2 algorithm for subgraph isomorphism from NetworkX</a></li>\n</ul>\n</li>\n<li>Graph degeneracy\n<ul>\n<li><a href=\"https://networkx.github.io/documentation/stable/reference/algorithms/core.html\" rel=\"nofollow\">k-core decomposition with NetworkX</a></li>\n</ul>\n</li>\n</ul>\n<h2>License</h2>\n<p>Released under the 3-Clause BSD license (see <a href=\"./LICENSE\" rel=\"nofollow\">LICENSE file</a>)</p>\n\n          </div>"}, "last_serial": 7157631, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "acc0863f267d1332ce506a3af2405313", "sha256": "df37d0389ba56785faa270e889ff02624218daa29de6b33827fad151f3dedced"}, "downloads": -1, "filename": "gowpy-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "acc0863f267d1332ce506a3af2405313", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 21339, "upload_time": "2020-05-03T15:42:32", "upload_time_iso_8601": "2020-05-03T15:42:32.261009Z", "url": "https://files.pythonhosted.org/packages/e3/9a/0e6b845dfd72db92d627d92c028dba28891e5e5e90f87e165e587469c955/gowpy-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cea07fddfba619fa861860106fbbb7c5", "sha256": "bc64761e062c04f8d68fcad722d184ca3c08804ed143fa853fc190a8e33342c5"}, "downloads": -1, "filename": "gowpy-0.1.0.tar.gz", "has_sig": false, "md5_digest": "cea07fddfba619fa861860106fbbb7c5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16738, "upload_time": "2020-05-03T15:42:34", "upload_time_iso_8601": "2020-05-03T15:42:34.773601Z", "url": "https://files.pythonhosted.org/packages/92/3c/137bc1b2c56e7e92f96c09678c50cef66e063569e552e24b099465b2f6a3/gowpy-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "acc0863f267d1332ce506a3af2405313", "sha256": "df37d0389ba56785faa270e889ff02624218daa29de6b33827fad151f3dedced"}, "downloads": -1, "filename": "gowpy-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "acc0863f267d1332ce506a3af2405313", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 21339, "upload_time": "2020-05-03T15:42:32", "upload_time_iso_8601": "2020-05-03T15:42:32.261009Z", "url": "https://files.pythonhosted.org/packages/e3/9a/0e6b845dfd72db92d627d92c028dba28891e5e5e90f87e165e587469c955/gowpy-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "cea07fddfba619fa861860106fbbb7c5", "sha256": "bc64761e062c04f8d68fcad722d184ca3c08804ed143fa853fc190a8e33342c5"}, "downloads": -1, "filename": "gowpy-0.1.0.tar.gz", "has_sig": false, "md5_digest": "cea07fddfba619fa861860106fbbb7c5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16738, "upload_time": "2020-05-03T15:42:34", "upload_time_iso_8601": "2020-05-03T15:42:34.773601Z", "url": "https://files.pythonhosted.org/packages/92/3c/137bc1b2c56e7e92f96c09678c50cef66e063569e552e24b099465b2f6a3/gowpy-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:55:22 2020"}