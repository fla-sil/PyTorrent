{"info": {"author": "Stephan Steinberg", "author_email": "stephan.steinberg@devolo.de", "bugtrack_url": null, "classifiers": ["Framework :: Pytest", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Software Development :: Quality Assurance", "Topic :: Software Development :: Testing", "Topic :: Utilities"], "description": "# pytest-adaptavist\n\nThis [pytest](http://pytest.org) plugin generates test execution results within Jira Test Management ([tm4j](https://www.adaptavist.com/doco/display/KT/Managing+Tests+From+the+REST+API)).\n\n# Table of Contents\n- [Installation](#installation)\n- [Getting Started](#getting-started)\n- [Examples and Features](#examples-and-features)\n    - [General Workflow](#general-workflow)\n    - [Context Reporting](#context-reporting)\n    - [Testcase Order](#testcase-order)\n    - [Testcase Range](#testcase-range)\n    - [Skipping vs. Blocking](#skipping-vs.-blocking)\n    - [Callbacks](#callbacks)\n\n## Installation\n\nTo install pytest-adaptavist, you can use (one of) the following command(s):\n```\n$ pip install git+https://github.com/devolo/pytest-adaptavist.git\n$ pip install git+https://github.com/devolo/pytest-adaptavist.git@v4.0.0\n$ pip install https://github.com/devolo/pytest-adaptavist/archive/v4.0.0/pytest-adaptavist.zip\n```\n\nTo uninstall pytest-adaptavist, you can use the following command:\n```\n$ pip uninstall pytest-adaptavist\n```\n\nTest case selection and ordering (see below) are supported by default.\n\nIn order to access Adaptavist/Jira and create test runs and results in there, provide credentials `JIRA_SERVER, JIRA_USERNAME, JIRA_PASSWORD` as environment variables\nand run your tests with\n```\n$ pytest --adaptavist\n```\n\nBy default the terminal output is configured by the common pytest options.\nTo achieve a more readable format showing names and docstrings of test methods and even context blocks, run your tests with\n```\n$ pytest --pretty\n```\n\n## Getting Started\n\n1. pytest-adaptavist searches for test methods named like ```test_<test_case_key>``` or ```test_<test_case_key>_<step>```\n   where ```test_case_key``` is the key of the Jira test case excluding the project key (e.g. \"T1\") and ```step``` defines a single test script step (if existing). In order to build real test case key strings from test methods, the corresponding project key needs to be specified for each relevant class or single test methods by using markers (see examples below). Alternatively, ```test_case_key``` can be given as it appears in Adaptavist, but with hyphens replaced by underscores (e.g. \"TEST_T1\").\n   Each of these kind of test methods is marked as Adaptavist test case for reporting appropriate results into Adaptavist test management. Any other test methods are processed as usual.\n\n2. Finally, pytest-adaptavist needs either ```pytest.test_run_key``` to use an existing test run or ```pytest.project_key``` to create a new test run every time with collected test cases linked to it.\n   In order to work properly, either of these parameters need to be specified at the very start of the test session.\n   If both parameters are empty, neither test runs nor test results are created in Adaptavist test management.\n   Please also note that any of these parameters mentioned here and in the following documentation can either be set programmatically or be provided as part of json config file (./config/global_config.json).\n\n## Examples and Features\n\n### General Workflow\n\npytest-adaptavist collects test cases (and single test steps) as mentioned above and prepares them for Adaptavist reporting.\n\n```pytest.test_run_key``` is used to specify an existing test run. In this case, it is important to mention\nthat collected test cases must be linked to that test run.\n\nAlternatively, if ```pytest.project_key``` is given and ```pytest.test_run_key``` is left empty, pytest-adaptavist creates a new test run every time with collected test cases linked to it. In this case, ```pytest.test_run_suffix``` can be used to create a meaningful test run name. In addition, ```pytest.test_plan_key``` is available to link the new created test run to an existing testplan.\n\nNew test plans can be created by specifying ```test_plan_suffix``` which is used as identifier to find existing test plans and must be unique. If there is a matching test plan, it will be used for creating new test runs. Else a new test plan is created within the given project and new test runs are linked to that. If both test plan key and suffix are missing, test runs are created just for the given project.\n\n<em>Naming convention for new test plans and test runs within pytest_adaptavist:</em>\n* <em>new test plans are named like ```<project key> <test plan suffix>```</em>\n* <em>new test runs are named like ```<test plan name or project key> <test run suffix> <datetime now>```</em>\n\n```pytest.test_case_keys``` can be used as an option to run only a subset of implemented test cases.\nAll others are skipped in this case. For new created test runs these test cases are excluded while for existing test runs the appropriate test case results stay as they are (if existing).\n\nIn addition, ```pytest.test_case_keys``` may contain test cases that are not implemented in the current python test script.\nThis can be useful in cases where the new test run also needs to include manual test cases (e.g. for later execution).\nFurthermore, it is even possible to just create a new test run with only test cases that are not (yet) implemented.\n\nIf either of these parameters is missing, pytest-adaptavist tries to read appropriate values from config file (global_config.json).\n\nSpecifying a project key for relevant test classes or test methods can be done by using markers:\n   ```python\n   @pytest.mark.project(project_key=\"my project\")\n   ```\nIf project markers are not used, pytest-adaptavist is using ```pytest.project_key``` to build test case key strings.\n\nTo send additional data (comments, attachments) to Adaptavist, test methods can be extended by using plugin's meta data parameter.<br/>\n(Note that for blocked or skipped methods, attachments will be ignored and only comments will be added to the test result).\n\n#### Examples\n\nThis simple snippet shows some implementation of step 2 of test case ```myproject-T1``` using ```meta_data```:\n   ```python\n    @pytest.mark.project(project_key=\"myproject\")\n    class TestClass(object):\n\n        def test_T1_2(self, meta_data):\n            meta_data[\"comment\"] = \"unexpected result\"\n            attachment = io.StringIO()\n            attachment.write(\"this is just a simple attachment\")\n            meta_data[\"attachment\"] = attachment\n            meta_data[\"filename\"] = \"content.txt\"\n            assert False\n   ```\n\nWith each test step report the status of overall (or parent) test result is adjusted automatically, depending on the status of all other test step results.\n\nIf a step fails, the status of the overall test is set to ```Fail``` and stays at ```Fail``` no matter of upcoming results within that test run. Furthermore, an appropriate information about failure is appended to overall test result as additional comment. Given the example above, there would be a ```step 2 failed: unexpected result``` note added to the overall test result comment.\n\nIf there is a test method for the overall test, its (optional) comment is prepended to the existing comment. Given the following example, the final comment for test case ```myproject-T1``` would be ```all good<br />step 2 failed: unexpected result```:\n   ```python\n    @pytest.mark.project(project_key=\"myproject\")\n    class TestClass(object):\n\n        def test_T1_2(self, meta_data):\n            meta_data[\"comment\"] = \"unexpected result\"\n            attachment = io.StringIO()\n            attachment.write(\"this is just a simple attachment\")\n            meta_data[\"attachment\"] = attachment\n            meta_data[\"filename\"] = \"content.txt\"\n            assert False\n\n        def test_T1(self, meta_data):\n            meta_data[\"comment\"] = \"all good\"\n            assert True\n   ```\nHowever, the final status would be ```Fail``` because not all test steps passed.\n\n### Context reporting\n\nIn addition to implementing test methods for single test steps, it is possible to combine them within the appropriate test case method by using context blocks:\n   ```python\n    class TestClass(object):\n\n        @pytest.mark.project(project_key=\"myproject\")\n        def test_T1(self, meta_block):\n            # processing step 1\n            with meta_block(1) as mb:\n                mb.data[\"comment\"] = \"unexpected result\"\n                attachment = io.StringIO()\n                attachment.write(\"this is just a simple attachment\")\n                mb.data[\"attachment\"] = attachment\n                mb.data[\"filename\"] = \"content.txt\"\n                pytest.assume(True)\n            # processing step 2\n            with meta_block(2) as mb:\n                mb.data[\"comment\"] = \"unexpected result\"\n                attachment = io.StringIO()\n                attachment.write(\"this is just a simple attachment\")\n                mb.data[\"attachment\"] = attachment\n                mb.data[\"filename\"] = \"content.txt\"\n                pytest.assume(False)\n\n        @pytest.mark.project(project_key=\"myproject\")\n        def test_T2(self, meta_block):\n            # processing test case only\n            with meta_block() as mb:\n                mb.data[\"comment\"] = \"all good\"\n                attachment = io.StringIO()\n                attachment.write(\"this is just a simple attachment\")\n                mb.data[\"attachment\"] = attachment\n                mb.data[\"filename\"] = \"content.txt\"\n                pytest.assume(True)\n   ```\nIt is highly recommended to use pytest.assume instead of assert in this case, as assert would immediately abort from current test method just skipping any other steps or context blocks. Alas, there might be cases, where it does make sense to abort though. In these cases, using assert is the better choice.\n\nTo simplify the overall handling, there's a helper method that combines meta data (e.g. comments, attachments) and assertion. Basically it checks given condition, aborts or continues appropriately and adds comments and attachments optionally:\n   ```python\n    class TestClass(object):\n\n        @pytest.mark.project(project_key=\"myproject\")\n        def test_T2(self, meta_block):\n\n            with meta_block() as mb:\n                # if precondition is False, stop session (results in status \"Blocked\" for this and any following test cases)\n                mb.check(precondition, message=\"precondition failed\", action_on_fail=mb.Action.STOP_SESSION)\n                # if precondition is False, stop further execution of this test (results in status \"Blocked\")\n                mb.check(precondition, message=\"precondition failed\", action_on_fail=mb.Action.STOP_METHOD)\n\n                with meta_block(1) as mb1:\n                    # if condition is False, just collect failed assumption and continue (just like 'assume', results in test step status \"Fail\")\n                    mb1.check(condition, message=\"condition failed\", action_on_fail=mb1.Action.FAIL_CONTEXT)\n                    # if condition is False, stop further execution of this step (results in status \"Blocked\")\n                    mb1.check(condition, message=\"condition failed\", action_on_fail=mb1.Action.STOP_CONTEXT)\n                    # if condition is False, stop further execution of this test (just like 'assert', results in status \"Fail\")\n                    mb1.check(condition, message=\"condition failed\", action_on_fail=mb1.Action.FAIL_METHOD)\n\n                with meta_block(2) as mb2:\n                    attachment = io.StringIO()\n                    attachment.name = \"context.txt\"\n                    attachment.write(\"this is just a simple attachment\")\n                    # default: if condition is False, just collect failed assumption and continue (just like 'assume', results in test step status \"Fail\")\n                    mb2.check(condition, message=\"condition failed\", attachment=attachment)  # attachment is added anyway\n   ```\nIn addition, this method supports the following parameters:\n* ```message_on_pass``` which can be used to report passed conditions as well\n* ```message_on_fail``` which is the same as ```message``` (just for convenience)\n* ```description``` as option to add details about test results (f.e. can be a html table or more)\n\n### Testcase Order\n\nBy default the execution of test methods in pytest runs alphabetical over test class names and then from top to bottom.\n\nWith pytest-adaptavist this order can be changed by using ```pytest.test_case_order``` (or ```pytest.test_case_keys```, see also above).\nIn this case the corresponding test methods are executed according to the given order, followed by all remaining test methods.\nMoreover, this can also be used when creating test runs automatically, as in this case the new test run is created with test cases linked in the given order.<br/>\n\nAlternatively, if an existing test run is specified by ```pytest.test_run_key```, the corresponding test methods are executed according to the order of test cases in the given test run, followed by all other test methods.\n\nNote that ```pytest.test_case_order``` overrules the test case order of the given test run as well as the order specified by ```pytest.test_case_keys```. This might be helpful in cases, where the default order should be changed temporarily. If ```pytest.test_case_order``` is not specified, the order will be as defined by ```pytest.test_run_key``` or - if a new test run should be created - ```pytest.test_case_keys```.\n\n#### Examples:\n\nAssume there is a project TEST with exactly two test cases TEST-T1 and TEST-T2 while a test implementation contains methods in the following order (top to bottom):<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;```test_T4```, ```test_T1```, ```test_T2```, ```test_T3```, ```test_myspecialtest```.\n\nAll these methods will be executed in the same order (from top to bottom). But a potentially new created test run will only contain TEST-T1 and TEST-T2 (in this order). And only the results of TEST-T1 and TEST-T2 will be send to Adaptavist.\n\nFor the next run a test case order is specified like TEST-T2, TEST-T3, TEST-T18.\nNow the methods are executed in this order:<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;```test_T2```, ```test_T3```, ```test_T4```, ```test_T1```, ```test_myspecialtest```.\n\nAs we can see, all the test cases specified in the test case order are executed first followed by all the others.\nOf course, TEST-T18 is ignored as there is no implementation found.<br/>\nAgain, a potentially new created test run will only contain TEST-T2 and TEST-T1 (in this order).\n\nFor cases where a new test run should be created including only a subset of testcases, it is enough to specify ```pytest.test_case_keys``` only by using the required order. All listed test cases are linked (and executed, if implemented) in exactly this order.\n\n### Testcase Range\n\nIn addition to specify a list of test cases to be executed it is possible to define ranges of test cases by using ```pytest.test_case_range```.\n\n#### Examples:\n\nDefining ```[\"TEST-T2\", \"TEST-T5\", \"TEST-200\", \"TEST-299\"]``` as a range will include any test cases from TEST-T2 to TEST-5 and from TEST-200 to TEST-299.\n\nSimilar to the use of ```pytest.test_case_keys``` all others are skipped in this case. For new created test runs these test cases are excluded while for existing test runs the appropriate test case results stay as they are (if existing).\n\n### Skipping vs. Blocking\n\nThe execution of test cases (methods) or even single steps can be skipped, either **statically** or **dynamically**.\n\nWhile **static skipping** is done f.e. by specifiying ```pytest.test_case_keys``` (only the listed test cases will be executed) or by applying appropriate markers as described below, **dynamic skipping** is happening based on some condition.\n\nBasically, when a test case or step is skipped the status stays untouched (typically it will be \"Not Executed\").\n\nAccording to Adaptavist test management, test cases with containing at least one step passed and all others not executed (yet) will have the status \"In Progress\" while test runs/cycles containing at least one test case being \"Not Executed\" or \"In Progress\" will have the status \"In Progress\".\nThis can be challenging when checking if a test runs/cycles is still running or is finished with some test cases skipped.\nTo solve this there is the option of blocking which is actually the same as skipping but with marking the test case (or step) as \"Blocked\".\nHaving any test cases or steps that have been aborted or not executed marked as \"Blocked\" will lead to a test run/cycle status \"Done\" once it is finished.\n\n#### Using markers\n\nMethods to skip or block test cases (methods) statically are provided by the markers ```pytest.mark.skip``` (part of pytest module) and \n```pytest.mark.block``` (defined in pytest-adaptavist):\n   ```python\n    class TestClass(object):\n\n        @pytest.mark.skip(reason=\"not implemented yet\")\n        @pytest.mark.project(project_key=\"myproject\")\n        def test_T1(self, meta_block):\n            ...\n\n        @pytest.mark.block(reason=\"not implemented yet\")\n        @pytest.mark.project(project_key=\"myproject\")\n        def test_T2(self, meta_block):\n            ...\n   ```\nNote that these markers work for classes as well.\n\n#### Using metablock action modes\n\nMethods to skip or block test cases (or steps) dynamically, e.g. depending on some condition, are provided by the meta block's helper method:\n   ```python\n    class TestClass(object):\n\n        @pytest.mark.project(project_key=\"myproject\")\n        def test_T2(self, meta_block):\n\n            with meta_block() as mb:\n                # if precondition is False, stop session (results in status \"Blocked\" for this and any following test cases)\n                mb.check(precondition, message=\"precondition failed\", action_on_fail=mb.Action.STOP_SESSION)\n                # if precondition is False, stop further execution of this test (results in status \"Blocked\")\n                mb.check(precondition, message=\"precondition failed\", action_on_fail=mb.Action.STOP_METHOD)\n\n                with meta_block(1) as mb1:\n                    # if condition is False, stop further execution of this step (results in status \"Blocked\")\n                    mb1.check(condition, message=\"condition failed\", action_on_fail=mb1.Action.STOP_CONTEXT)\n            ...\n   ```\n\n#### Using pytest methods\n\nAnother methods to skip or block test cases (or steps) dynamically, e.g. depending on some condition, are ```pytest.skip``` (part of pytest module) and \n```pytest.block``` (defined in pytest-adaptavist):\n   ```python\n    def my_internal_function_1(...):\n        ...\n        if not precondition:\n            pytest.skip(msg=\"precondition failed\")\n        ...\n\n    def my_internal_function_2(...):\n        ...\n        if not precondition:\n            pytest.block(msg=\"precondition failed\")\n        ...\n   ```\nNote that these methods are not needed usually, alas sometimes they can be useful inside of helper functions where test case context or information is missing.\n\n### Callbacks\n\nFor additional custom functionality pytest-adaptavist provides a small set of callbacks:\n```python\n\n    def meta_block_condition_cb(signature, condition, reference):\n        \"\"\"\n        Called with each call of mb.check\n        providing a signature like <item name>_<step index>_<#calls>, the condition (True|False) and a reference message (message_on_pass|message_on_fail).\n        \"\"\"\n        pass\n\n    def meta_block_cb(signature, status):\n        \"\"\"\n        Called when exiting a meta block (resp. context block)\n        providing a signature like <item name>_<step index> and the final status (\"passed\", \"failed\", etc.).\n        \"\"\"\n        pass\n   ```\n\nExamples below show how to work with them:\n```python\n\n    def my_func(a, b, c, d, **kwargs):\n        \"\"\"The actual function to be called.\"\"\"\n        print(\"my_func\", a, b, c, d, kwargs)\n\n\n    def my_callback_example_1(func, *args):\n        \"\"\"Wrapper for the condition callback.\"\"\"\n        def _cb(**kwargs):\n            condition = kwargs.pop(\"condition\", None)\n            if not condition:\n                return func(*args, **kwargs)\n            return None\n        return _cb\n\n\n    def my_callback_example_2(func, *args):\n        \"\"\"Another example of a callback wrapper using lambda expression.\"\"\"\n        return lambda **kwargs : func(*args, **kwargs) if not kwargs.pop(\"condition\", False) else None\n\n\n    def my_callback_example_3(func, *args):\n        \"\"\"Wrapper for meta block callback.\"\"\"\n        def _cb(**kwargs):\n            status = kwargs.pop(\"status\", None)\n            if status != \"passed\":\n                return func(*args, **kwargs)\n            return None\n        return _cb\n\n\n    def my_callback_example_4(func, *args):\n        \"\"\"Another example of a callback wrapper using lambda expression.\"\"\"\n        return lambda **kwargs : func(*args, **kwargs) if kwargs.pop(\"status\", None) != \"passed\" else None\n\n\n    @pytest.fixture(scope=\"function\", autouse=True)\n    def my_fixture_example(request):\n        \"\"\"Function-scoped fixture to set the callbacks.\"\"\"\n        request.node.meta_block_condition_cb = my_callback_example_1(my_func, 17, 18, 19)\n        request.node.meta_block_cb = my_callback_example_3(my_func, 17, 18, 19)\n   ```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/devolo/pytest-adaptavist", "keywords": "python pytest adaptavist kanoah tm4j jira test testmanagement report", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "pytest-adaptavist", "package_url": "https://pypi.org/project/pytest-adaptavist/", "platform": "any", "project_url": "https://pypi.org/project/pytest-adaptavist/", "project_urls": {"Homepage": "https://github.com/devolo/pytest-adaptavist"}, "release_url": "https://pypi.org/project/pytest-adaptavist/4.0.5/", "requires_dist": ["adaptavist (>=1.0.0)", "pytest (>=3.4.1)"], "requires_python": ">=3.6", "summary": "pytest plugin for generating test execution results within Jira Test Management (tm4j)", "version": "4.0.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>pytest-adaptavist</h1>\n<p>This <a href=\"http://pytest.org\" rel=\"nofollow\">pytest</a> plugin generates test execution results within Jira Test Management (<a href=\"https://www.adaptavist.com/doco/display/KT/Managing+Tests+From+the+REST+API\" rel=\"nofollow\">tm4j</a>).</p>\n<h1>Table of Contents</h1>\n<ul>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#getting-started\" rel=\"nofollow\">Getting Started</a></li>\n<li><a href=\"#examples-and-features\" rel=\"nofollow\">Examples and Features</a>\n<ul>\n<li><a href=\"#general-workflow\" rel=\"nofollow\">General Workflow</a></li>\n<li><a href=\"#context-reporting\" rel=\"nofollow\">Context Reporting</a></li>\n<li><a href=\"#testcase-order\" rel=\"nofollow\">Testcase Order</a></li>\n<li><a href=\"#testcase-range\" rel=\"nofollow\">Testcase Range</a></li>\n<li><a href=\"#skipping-vs.-blocking\" rel=\"nofollow\">Skipping vs. Blocking</a></li>\n<li><a href=\"#callbacks\" rel=\"nofollow\">Callbacks</a></li>\n</ul>\n</li>\n</ul>\n<h2>Installation</h2>\n<p>To install pytest-adaptavist, you can use (one of) the following command(s):</p>\n<pre><code>$ pip install git+https://github.com/devolo/pytest-adaptavist.git\n$ pip install git+https://github.com/devolo/pytest-adaptavist.git@v4.0.0\n$ pip install https://github.com/devolo/pytest-adaptavist/archive/v4.0.0/pytest-adaptavist.zip\n</code></pre>\n<p>To uninstall pytest-adaptavist, you can use the following command:</p>\n<pre><code>$ pip uninstall pytest-adaptavist\n</code></pre>\n<p>Test case selection and ordering (see below) are supported by default.</p>\n<p>In order to access Adaptavist/Jira and create test runs and results in there, provide credentials <code>JIRA_SERVER, JIRA_USERNAME, JIRA_PASSWORD</code> as environment variables\nand run your tests with</p>\n<pre><code>$ pytest --adaptavist\n</code></pre>\n<p>By default the terminal output is configured by the common pytest options.\nTo achieve a more readable format showing names and docstrings of test methods and even context blocks, run your tests with</p>\n<pre><code>$ pytest --pretty\n</code></pre>\n<h2>Getting Started</h2>\n<ol>\n<li>\n<p>pytest-adaptavist searches for test methods named like <code>test_&lt;test_case_key&gt;</code> or <code>test_&lt;test_case_key&gt;_&lt;step&gt;</code>\nwhere <code>test_case_key</code> is the key of the Jira test case excluding the project key (e.g. \"T1\") and <code>step</code> defines a single test script step (if existing). In order to build real test case key strings from test methods, the corresponding project key needs to be specified for each relevant class or single test methods by using markers (see examples below). Alternatively, <code>test_case_key</code> can be given as it appears in Adaptavist, but with hyphens replaced by underscores (e.g. \"TEST_T1\").\nEach of these kind of test methods is marked as Adaptavist test case for reporting appropriate results into Adaptavist test management. Any other test methods are processed as usual.</p>\n</li>\n<li>\n<p>Finally, pytest-adaptavist needs either <code>pytest.test_run_key</code> to use an existing test run or <code>pytest.project_key</code> to create a new test run every time with collected test cases linked to it.\nIn order to work properly, either of these parameters need to be specified at the very start of the test session.\nIf both parameters are empty, neither test runs nor test results are created in Adaptavist test management.\nPlease also note that any of these parameters mentioned here and in the following documentation can either be set programmatically or be provided as part of json config file (./config/global_config.json).</p>\n</li>\n</ol>\n<h2>Examples and Features</h2>\n<h3>General Workflow</h3>\n<p>pytest-adaptavist collects test cases (and single test steps) as mentioned above and prepares them for Adaptavist reporting.</p>\n<p><code>pytest.test_run_key</code> is used to specify an existing test run. In this case, it is important to mention\nthat collected test cases must be linked to that test run.</p>\n<p>Alternatively, if <code>pytest.project_key</code> is given and <code>pytest.test_run_key</code> is left empty, pytest-adaptavist creates a new test run every time with collected test cases linked to it. In this case, <code>pytest.test_run_suffix</code> can be used to create a meaningful test run name. In addition, <code>pytest.test_plan_key</code> is available to link the new created test run to an existing testplan.</p>\n<p>New test plans can be created by specifying <code>test_plan_suffix</code> which is used as identifier to find existing test plans and must be unique. If there is a matching test plan, it will be used for creating new test runs. Else a new test plan is created within the given project and new test runs are linked to that. If both test plan key and suffix are missing, test runs are created just for the given project.</p>\n<p><em>Naming convention for new test plans and test runs within pytest_adaptavist:</em></p>\n<ul>\n<li><em>new test plans are named like <code>&lt;project key&gt; &lt;test plan suffix&gt;</code></em></li>\n<li><em>new test runs are named like <code>&lt;test plan name or project key&gt; &lt;test run suffix&gt; &lt;datetime now&gt;</code></em></li>\n</ul>\n<p><code>pytest.test_case_keys</code> can be used as an option to run only a subset of implemented test cases.\nAll others are skipped in this case. For new created test runs these test cases are excluded while for existing test runs the appropriate test case results stay as they are (if existing).</p>\n<p>In addition, <code>pytest.test_case_keys</code> may contain test cases that are not implemented in the current python test script.\nThis can be useful in cases where the new test run also needs to include manual test cases (e.g. for later execution).\nFurthermore, it is even possible to just create a new test run with only test cases that are not (yet) implemented.</p>\n<p>If either of these parameters is missing, pytest-adaptavist tries to read appropriate values from config file (global_config.json).</p>\n<p>Specifying a project key for relevant test classes or test methods can be done by using markers:</p>\n<pre><span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">project</span><span class=\"p\">(</span><span class=\"n\">project_key</span><span class=\"o\">=</span><span class=\"s2\">\"my project\"</span><span class=\"p\">)</span>\n</pre>\n<p>If project markers are not used, pytest-adaptavist is using <code>pytest.project_key</code> to build test case key strings.</p>\n<p>To send additional data (comments, attachments) to Adaptavist, test methods can be extended by using plugin's meta data parameter.<br>\n(Note that for blocked or skipped methods, attachments will be ignored and only comments will be added to the test result).</p>\n<h4>Examples</h4>\n<p>This simple snippet shows some implementation of step 2 of test case <code>myproject-T1</code> using <code>meta_data</code>:</p>\n<pre> <span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">project</span><span class=\"p\">(</span><span class=\"n\">project_key</span><span class=\"o\">=</span><span class=\"s2\">\"myproject\"</span><span class=\"p\">)</span>\n <span class=\"k\">class</span> <span class=\"nc\">TestClass</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n\n     <span class=\"k\">def</span> <span class=\"nf\">test_T1_2</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">meta_data</span><span class=\"p\">):</span>\n         <span class=\"n\">meta_data</span><span class=\"p\">[</span><span class=\"s2\">\"comment\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"unexpected result\"</span>\n         <span class=\"n\">attachment</span> <span class=\"o\">=</span> <span class=\"n\">io</span><span class=\"o\">.</span><span class=\"n\">StringIO</span><span class=\"p\">()</span>\n         <span class=\"n\">attachment</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"s2\">\"this is just a simple attachment\"</span><span class=\"p\">)</span>\n         <span class=\"n\">meta_data</span><span class=\"p\">[</span><span class=\"s2\">\"attachment\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">attachment</span>\n         <span class=\"n\">meta_data</span><span class=\"p\">[</span><span class=\"s2\">\"filename\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"content.txt\"</span>\n         <span class=\"k\">assert</span> <span class=\"kc\">False</span>\n</pre>\n<p>With each test step report the status of overall (or parent) test result is adjusted automatically, depending on the status of all other test step results.</p>\n<p>If a step fails, the status of the overall test is set to <code>Fail</code> and stays at <code>Fail</code> no matter of upcoming results within that test run. Furthermore, an appropriate information about failure is appended to overall test result as additional comment. Given the example above, there would be a <code>step 2 failed: unexpected result</code> note added to the overall test result comment.</p>\n<p>If there is a test method for the overall test, its (optional) comment is prepended to the existing comment. Given the following example, the final comment for test case <code>myproject-T1</code> would be <code>all good&lt;br /&gt;step 2 failed: unexpected result</code>:</p>\n<pre> <span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">project</span><span class=\"p\">(</span><span class=\"n\">project_key</span><span class=\"o\">=</span><span class=\"s2\">\"myproject\"</span><span class=\"p\">)</span>\n <span class=\"k\">class</span> <span class=\"nc\">TestClass</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n\n     <span class=\"k\">def</span> <span class=\"nf\">test_T1_2</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">meta_data</span><span class=\"p\">):</span>\n         <span class=\"n\">meta_data</span><span class=\"p\">[</span><span class=\"s2\">\"comment\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"unexpected result\"</span>\n         <span class=\"n\">attachment</span> <span class=\"o\">=</span> <span class=\"n\">io</span><span class=\"o\">.</span><span class=\"n\">StringIO</span><span class=\"p\">()</span>\n         <span class=\"n\">attachment</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"s2\">\"this is just a simple attachment\"</span><span class=\"p\">)</span>\n         <span class=\"n\">meta_data</span><span class=\"p\">[</span><span class=\"s2\">\"attachment\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">attachment</span>\n         <span class=\"n\">meta_data</span><span class=\"p\">[</span><span class=\"s2\">\"filename\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"content.txt\"</span>\n         <span class=\"k\">assert</span> <span class=\"kc\">False</span>\n\n     <span class=\"k\">def</span> <span class=\"nf\">test_T1</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">meta_data</span><span class=\"p\">):</span>\n         <span class=\"n\">meta_data</span><span class=\"p\">[</span><span class=\"s2\">\"comment\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"all good\"</span>\n         <span class=\"k\">assert</span> <span class=\"kc\">True</span>\n</pre>\n<p>However, the final status would be <code>Fail</code> because not all test steps passed.</p>\n<h3>Context reporting</h3>\n<p>In addition to implementing test methods for single test steps, it is possible to combine them within the appropriate test case method by using context blocks:</p>\n<pre> <span class=\"k\">class</span> <span class=\"nc\">TestClass</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n\n     <span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">project</span><span class=\"p\">(</span><span class=\"n\">project_key</span><span class=\"o\">=</span><span class=\"s2\">\"myproject\"</span><span class=\"p\">)</span>\n     <span class=\"k\">def</span> <span class=\"nf\">test_T1</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">meta_block</span><span class=\"p\">):</span>\n         <span class=\"c1\"># processing step 1</span>\n         <span class=\"k\">with</span> <span class=\"n\">meta_block</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">mb</span><span class=\"p\">:</span>\n             <span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">\"comment\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"unexpected result\"</span>\n             <span class=\"n\">attachment</span> <span class=\"o\">=</span> <span class=\"n\">io</span><span class=\"o\">.</span><span class=\"n\">StringIO</span><span class=\"p\">()</span>\n             <span class=\"n\">attachment</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"s2\">\"this is just a simple attachment\"</span><span class=\"p\">)</span>\n             <span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">\"attachment\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">attachment</span>\n             <span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">\"filename\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"content.txt\"</span>\n             <span class=\"n\">pytest</span><span class=\"o\">.</span><span class=\"n\">assume</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n         <span class=\"c1\"># processing step 2</span>\n         <span class=\"k\">with</span> <span class=\"n\">meta_block</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">mb</span><span class=\"p\">:</span>\n             <span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">\"comment\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"unexpected result\"</span>\n             <span class=\"n\">attachment</span> <span class=\"o\">=</span> <span class=\"n\">io</span><span class=\"o\">.</span><span class=\"n\">StringIO</span><span class=\"p\">()</span>\n             <span class=\"n\">attachment</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"s2\">\"this is just a simple attachment\"</span><span class=\"p\">)</span>\n             <span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">\"attachment\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">attachment</span>\n             <span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">\"filename\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"content.txt\"</span>\n             <span class=\"n\">pytest</span><span class=\"o\">.</span><span class=\"n\">assume</span><span class=\"p\">(</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n\n     <span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">project</span><span class=\"p\">(</span><span class=\"n\">project_key</span><span class=\"o\">=</span><span class=\"s2\">\"myproject\"</span><span class=\"p\">)</span>\n     <span class=\"k\">def</span> <span class=\"nf\">test_T2</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">meta_block</span><span class=\"p\">):</span>\n         <span class=\"c1\"># processing test case only</span>\n         <span class=\"k\">with</span> <span class=\"n\">meta_block</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">mb</span><span class=\"p\">:</span>\n             <span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">\"comment\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"all good\"</span>\n             <span class=\"n\">attachment</span> <span class=\"o\">=</span> <span class=\"n\">io</span><span class=\"o\">.</span><span class=\"n\">StringIO</span><span class=\"p\">()</span>\n             <span class=\"n\">attachment</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"s2\">\"this is just a simple attachment\"</span><span class=\"p\">)</span>\n             <span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">\"attachment\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">attachment</span>\n             <span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s2\">\"filename\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"content.txt\"</span>\n             <span class=\"n\">pytest</span><span class=\"o\">.</span><span class=\"n\">assume</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<p>It is highly recommended to use pytest.assume instead of assert in this case, as assert would immediately abort from current test method just skipping any other steps or context blocks. Alas, there might be cases, where it does make sense to abort though. In these cases, using assert is the better choice.</p>\n<p>To simplify the overall handling, there's a helper method that combines meta data (e.g. comments, attachments) and assertion. Basically it checks given condition, aborts or continues appropriately and adds comments and attachments optionally:</p>\n<pre> <span class=\"k\">class</span> <span class=\"nc\">TestClass</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n\n     <span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">project</span><span class=\"p\">(</span><span class=\"n\">project_key</span><span class=\"o\">=</span><span class=\"s2\">\"myproject\"</span><span class=\"p\">)</span>\n     <span class=\"k\">def</span> <span class=\"nf\">test_T2</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">meta_block</span><span class=\"p\">):</span>\n\n         <span class=\"k\">with</span> <span class=\"n\">meta_block</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">mb</span><span class=\"p\">:</span>\n             <span class=\"c1\"># if precondition is False, stop session (results in status \"Blocked\" for this and any following test cases)</span>\n             <span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">check</span><span class=\"p\">(</span><span class=\"n\">precondition</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"o\">=</span><span class=\"s2\">\"precondition failed\"</span><span class=\"p\">,</span> <span class=\"n\">action_on_fail</span><span class=\"o\">=</span><span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">Action</span><span class=\"o\">.</span><span class=\"n\">STOP_SESSION</span><span class=\"p\">)</span>\n             <span class=\"c1\"># if precondition is False, stop further execution of this test (results in status \"Blocked\")</span>\n             <span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">check</span><span class=\"p\">(</span><span class=\"n\">precondition</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"o\">=</span><span class=\"s2\">\"precondition failed\"</span><span class=\"p\">,</span> <span class=\"n\">action_on_fail</span><span class=\"o\">=</span><span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">Action</span><span class=\"o\">.</span><span class=\"n\">STOP_METHOD</span><span class=\"p\">)</span>\n\n             <span class=\"k\">with</span> <span class=\"n\">meta_block</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">mb1</span><span class=\"p\">:</span>\n                 <span class=\"c1\"># if condition is False, just collect failed assumption and continue (just like 'assume', results in test step status \"Fail\")</span>\n                 <span class=\"n\">mb1</span><span class=\"o\">.</span><span class=\"n\">check</span><span class=\"p\">(</span><span class=\"n\">condition</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"o\">=</span><span class=\"s2\">\"condition failed\"</span><span class=\"p\">,</span> <span class=\"n\">action_on_fail</span><span class=\"o\">=</span><span class=\"n\">mb1</span><span class=\"o\">.</span><span class=\"n\">Action</span><span class=\"o\">.</span><span class=\"n\">FAIL_CONTEXT</span><span class=\"p\">)</span>\n                 <span class=\"c1\"># if condition is False, stop further execution of this step (results in status \"Blocked\")</span>\n                 <span class=\"n\">mb1</span><span class=\"o\">.</span><span class=\"n\">check</span><span class=\"p\">(</span><span class=\"n\">condition</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"o\">=</span><span class=\"s2\">\"condition failed\"</span><span class=\"p\">,</span> <span class=\"n\">action_on_fail</span><span class=\"o\">=</span><span class=\"n\">mb1</span><span class=\"o\">.</span><span class=\"n\">Action</span><span class=\"o\">.</span><span class=\"n\">STOP_CONTEXT</span><span class=\"p\">)</span>\n                 <span class=\"c1\"># if condition is False, stop further execution of this test (just like 'assert', results in status \"Fail\")</span>\n                 <span class=\"n\">mb1</span><span class=\"o\">.</span><span class=\"n\">check</span><span class=\"p\">(</span><span class=\"n\">condition</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"o\">=</span><span class=\"s2\">\"condition failed\"</span><span class=\"p\">,</span> <span class=\"n\">action_on_fail</span><span class=\"o\">=</span><span class=\"n\">mb1</span><span class=\"o\">.</span><span class=\"n\">Action</span><span class=\"o\">.</span><span class=\"n\">FAIL_METHOD</span><span class=\"p\">)</span>\n\n             <span class=\"k\">with</span> <span class=\"n\">meta_block</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">mb2</span><span class=\"p\">:</span>\n                 <span class=\"n\">attachment</span> <span class=\"o\">=</span> <span class=\"n\">io</span><span class=\"o\">.</span><span class=\"n\">StringIO</span><span class=\"p\">()</span>\n                 <span class=\"n\">attachment</span><span class=\"o\">.</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s2\">\"context.txt\"</span>\n                 <span class=\"n\">attachment</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"s2\">\"this is just a simple attachment\"</span><span class=\"p\">)</span>\n                 <span class=\"c1\"># default: if condition is False, just collect failed assumption and continue (just like 'assume', results in test step status \"Fail\")</span>\n                 <span class=\"n\">mb2</span><span class=\"o\">.</span><span class=\"n\">check</span><span class=\"p\">(</span><span class=\"n\">condition</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"o\">=</span><span class=\"s2\">\"condition failed\"</span><span class=\"p\">,</span> <span class=\"n\">attachment</span><span class=\"o\">=</span><span class=\"n\">attachment</span><span class=\"p\">)</span>  <span class=\"c1\"># attachment is added anyway</span>\n</pre>\n<p>In addition, this method supports the following parameters:</p>\n<ul>\n<li><code>message_on_pass</code> which can be used to report passed conditions as well</li>\n<li><code>message_on_fail</code> which is the same as <code>message</code> (just for convenience)</li>\n<li><code>description</code> as option to add details about test results (f.e. can be a html table or more)</li>\n</ul>\n<h3>Testcase Order</h3>\n<p>By default the execution of test methods in pytest runs alphabetical over test class names and then from top to bottom.</p>\n<p>With pytest-adaptavist this order can be changed by using <code>pytest.test_case_order</code> (or <code>pytest.test_case_keys</code>, see also above).\nIn this case the corresponding test methods are executed according to the given order, followed by all remaining test methods.\nMoreover, this can also be used when creating test runs automatically, as in this case the new test run is created with test cases linked in the given order.<br></p>\n<p>Alternatively, if an existing test run is specified by <code>pytest.test_run_key</code>, the corresponding test methods are executed according to the order of test cases in the given test run, followed by all other test methods.</p>\n<p>Note that <code>pytest.test_case_order</code> overrules the test case order of the given test run as well as the order specified by <code>pytest.test_case_keys</code>. This might be helpful in cases, where the default order should be changed temporarily. If <code>pytest.test_case_order</code> is not specified, the order will be as defined by <code>pytest.test_run_key</code> or - if a new test run should be created - <code>pytest.test_case_keys</code>.</p>\n<h4>Examples:</h4>\n<p>Assume there is a project TEST with exactly two test cases TEST-T1 and TEST-T2 while a test implementation contains methods in the following order (top to bottom):<br>\n\u00a0\u00a0\u00a0\u00a0<code>test_T4</code>, <code>test_T1</code>, <code>test_T2</code>, <code>test_T3</code>, <code>test_myspecialtest</code>.</p>\n<p>All these methods will be executed in the same order (from top to bottom). But a potentially new created test run will only contain TEST-T1 and TEST-T2 (in this order). And only the results of TEST-T1 and TEST-T2 will be send to Adaptavist.</p>\n<p>For the next run a test case order is specified like TEST-T2, TEST-T3, TEST-T18.\nNow the methods are executed in this order:<br>\n\u00a0\u00a0\u00a0\u00a0<code>test_T2</code>, <code>test_T3</code>, <code>test_T4</code>, <code>test_T1</code>, <code>test_myspecialtest</code>.</p>\n<p>As we can see, all the test cases specified in the test case order are executed first followed by all the others.\nOf course, TEST-T18 is ignored as there is no implementation found.<br>\nAgain, a potentially new created test run will only contain TEST-T2 and TEST-T1 (in this order).</p>\n<p>For cases where a new test run should be created including only a subset of testcases, it is enough to specify <code>pytest.test_case_keys</code> only by using the required order. All listed test cases are linked (and executed, if implemented) in exactly this order.</p>\n<h3>Testcase Range</h3>\n<p>In addition to specify a list of test cases to be executed it is possible to define ranges of test cases by using <code>pytest.test_case_range</code>.</p>\n<h4>Examples:</h4>\n<p>Defining <code>[\"TEST-T2\", \"TEST-T5\", \"TEST-200\", \"TEST-299\"]</code> as a range will include any test cases from TEST-T2 to TEST-5 and from TEST-200 to TEST-299.</p>\n<p>Similar to the use of <code>pytest.test_case_keys</code> all others are skipped in this case. For new created test runs these test cases are excluded while for existing test runs the appropriate test case results stay as they are (if existing).</p>\n<h3>Skipping vs. Blocking</h3>\n<p>The execution of test cases (methods) or even single steps can be skipped, either <strong>statically</strong> or <strong>dynamically</strong>.</p>\n<p>While <strong>static skipping</strong> is done f.e. by specifiying <code>pytest.test_case_keys</code> (only the listed test cases will be executed) or by applying appropriate markers as described below, <strong>dynamic skipping</strong> is happening based on some condition.</p>\n<p>Basically, when a test case or step is skipped the status stays untouched (typically it will be \"Not Executed\").</p>\n<p>According to Adaptavist test management, test cases with containing at least one step passed and all others not executed (yet) will have the status \"In Progress\" while test runs/cycles containing at least one test case being \"Not Executed\" or \"In Progress\" will have the status \"In Progress\".\nThis can be challenging when checking if a test runs/cycles is still running or is finished with some test cases skipped.\nTo solve this there is the option of blocking which is actually the same as skipping but with marking the test case (or step) as \"Blocked\".\nHaving any test cases or steps that have been aborted or not executed marked as \"Blocked\" will lead to a test run/cycle status \"Done\" once it is finished.</p>\n<h4>Using markers</h4>\n<p>Methods to skip or block test cases (methods) statically are provided by the markers <code>pytest.mark.skip</code> (part of pytest module) and\n<code>pytest.mark.block</code> (defined in pytest-adaptavist):</p>\n<pre> <span class=\"k\">class</span> <span class=\"nc\">TestClass</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n\n     <span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">skip</span><span class=\"p\">(</span><span class=\"n\">reason</span><span class=\"o\">=</span><span class=\"s2\">\"not implemented yet\"</span><span class=\"p\">)</span>\n     <span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">project</span><span class=\"p\">(</span><span class=\"n\">project_key</span><span class=\"o\">=</span><span class=\"s2\">\"myproject\"</span><span class=\"p\">)</span>\n     <span class=\"k\">def</span> <span class=\"nf\">test_T1</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">meta_block</span><span class=\"p\">):</span>\n         <span class=\"o\">...</span>\n\n     <span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">block</span><span class=\"p\">(</span><span class=\"n\">reason</span><span class=\"o\">=</span><span class=\"s2\">\"not implemented yet\"</span><span class=\"p\">)</span>\n     <span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">project</span><span class=\"p\">(</span><span class=\"n\">project_key</span><span class=\"o\">=</span><span class=\"s2\">\"myproject\"</span><span class=\"p\">)</span>\n     <span class=\"k\">def</span> <span class=\"nf\">test_T2</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">meta_block</span><span class=\"p\">):</span>\n         <span class=\"o\">...</span>\n</pre>\n<p>Note that these markers work for classes as well.</p>\n<h4>Using metablock action modes</h4>\n<p>Methods to skip or block test cases (or steps) dynamically, e.g. depending on some condition, are provided by the meta block's helper method:</p>\n<pre> <span class=\"k\">class</span> <span class=\"nc\">TestClass</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n\n     <span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">project</span><span class=\"p\">(</span><span class=\"n\">project_key</span><span class=\"o\">=</span><span class=\"s2\">\"myproject\"</span><span class=\"p\">)</span>\n     <span class=\"k\">def</span> <span class=\"nf\">test_T2</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">meta_block</span><span class=\"p\">):</span>\n\n         <span class=\"k\">with</span> <span class=\"n\">meta_block</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">mb</span><span class=\"p\">:</span>\n             <span class=\"c1\"># if precondition is False, stop session (results in status \"Blocked\" for this and any following test cases)</span>\n             <span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">check</span><span class=\"p\">(</span><span class=\"n\">precondition</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"o\">=</span><span class=\"s2\">\"precondition failed\"</span><span class=\"p\">,</span> <span class=\"n\">action_on_fail</span><span class=\"o\">=</span><span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">Action</span><span class=\"o\">.</span><span class=\"n\">STOP_SESSION</span><span class=\"p\">)</span>\n             <span class=\"c1\"># if precondition is False, stop further execution of this test (results in status \"Blocked\")</span>\n             <span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">check</span><span class=\"p\">(</span><span class=\"n\">precondition</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"o\">=</span><span class=\"s2\">\"precondition failed\"</span><span class=\"p\">,</span> <span class=\"n\">action_on_fail</span><span class=\"o\">=</span><span class=\"n\">mb</span><span class=\"o\">.</span><span class=\"n\">Action</span><span class=\"o\">.</span><span class=\"n\">STOP_METHOD</span><span class=\"p\">)</span>\n\n             <span class=\"k\">with</span> <span class=\"n\">meta_block</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">mb1</span><span class=\"p\">:</span>\n                 <span class=\"c1\"># if condition is False, stop further execution of this step (results in status \"Blocked\")</span>\n                 <span class=\"n\">mb1</span><span class=\"o\">.</span><span class=\"n\">check</span><span class=\"p\">(</span><span class=\"n\">condition</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"o\">=</span><span class=\"s2\">\"condition failed\"</span><span class=\"p\">,</span> <span class=\"n\">action_on_fail</span><span class=\"o\">=</span><span class=\"n\">mb1</span><span class=\"o\">.</span><span class=\"n\">Action</span><span class=\"o\">.</span><span class=\"n\">STOP_CONTEXT</span><span class=\"p\">)</span>\n         <span class=\"o\">...</span>\n</pre>\n<h4>Using pytest methods</h4>\n<p>Another methods to skip or block test cases (or steps) dynamically, e.g. depending on some condition, are <code>pytest.skip</code> (part of pytest module) and\n<code>pytest.block</code> (defined in pytest-adaptavist):</p>\n<pre> <span class=\"k\">def</span> <span class=\"nf\">my_internal_function_1</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">):</span>\n     <span class=\"o\">...</span>\n     <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">precondition</span><span class=\"p\">:</span>\n         <span class=\"n\">pytest</span><span class=\"o\">.</span><span class=\"n\">skip</span><span class=\"p\">(</span><span class=\"n\">msg</span><span class=\"o\">=</span><span class=\"s2\">\"precondition failed\"</span><span class=\"p\">)</span>\n     <span class=\"o\">...</span>\n\n <span class=\"k\">def</span> <span class=\"nf\">my_internal_function_2</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">):</span>\n     <span class=\"o\">...</span>\n     <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">precondition</span><span class=\"p\">:</span>\n         <span class=\"n\">pytest</span><span class=\"o\">.</span><span class=\"n\">block</span><span class=\"p\">(</span><span class=\"n\">msg</span><span class=\"o\">=</span><span class=\"s2\">\"precondition failed\"</span><span class=\"p\">)</span>\n     <span class=\"o\">...</span>\n</pre>\n<p>Note that these methods are not needed usually, alas sometimes they can be useful inside of helper functions where test case context or information is missing.</p>\n<h3>Callbacks</h3>\n<p>For additional custom functionality pytest-adaptavist provides a small set of callbacks:</p>\n<pre>    <span class=\"k\">def</span> <span class=\"nf\">meta_block_condition_cb</span><span class=\"p\">(</span><span class=\"n\">signature</span><span class=\"p\">,</span> <span class=\"n\">condition</span><span class=\"p\">,</span> <span class=\"n\">reference</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">        Called with each call of mb.check</span>\n<span class=\"sd\">        providing a signature like &lt;item name&gt;_&lt;step index&gt;_&lt;#calls&gt;, the condition (True|False) and a reference message (message_on_pass|message_on_fail).</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"k\">pass</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">meta_block_cb</span><span class=\"p\">(</span><span class=\"n\">signature</span><span class=\"p\">,</span> <span class=\"n\">status</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">        Called when exiting a meta block (resp. context block)</span>\n<span class=\"sd\">        providing a signature like &lt;item name&gt;_&lt;step index&gt; and the final status (\"passed\", \"failed\", etc.).</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"k\">pass</span>\n</pre>\n<p>Examples below show how to work with them:</p>\n<pre>    <span class=\"k\">def</span> <span class=\"nf\">my_func</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"The actual function to be called.\"\"\"</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"my_func\"</span><span class=\"p\">,</span> <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">kwargs</span><span class=\"p\">)</span>\n\n\n    <span class=\"k\">def</span> <span class=\"nf\">my_callback_example_1</span><span class=\"p\">(</span><span class=\"n\">func</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"Wrapper for the condition callback.\"\"\"</span>\n        <span class=\"k\">def</span> <span class=\"nf\">_cb</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n            <span class=\"n\">condition</span> <span class=\"o\">=</span> <span class=\"n\">kwargs</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">(</span><span class=\"s2\">\"condition\"</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">condition</span><span class=\"p\">:</span>\n                <span class=\"k\">return</span> <span class=\"n\">func</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"kc\">None</span>\n        <span class=\"k\">return</span> <span class=\"n\">_cb</span>\n\n\n    <span class=\"k\">def</span> <span class=\"nf\">my_callback_example_2</span><span class=\"p\">(</span><span class=\"n\">func</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"Another example of a callback wrapper using lambda expression.\"\"\"</span>\n        <span class=\"k\">return</span> <span class=\"k\">lambda</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span> <span class=\"p\">:</span> <span class=\"n\">func</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">kwargs</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">(</span><span class=\"s2\">\"condition\"</span><span class=\"p\">,</span> <span class=\"kc\">False</span><span class=\"p\">)</span> <span class=\"k\">else</span> <span class=\"kc\">None</span>\n\n\n    <span class=\"k\">def</span> <span class=\"nf\">my_callback_example_3</span><span class=\"p\">(</span><span class=\"n\">func</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"Wrapper for meta block callback.\"\"\"</span>\n        <span class=\"k\">def</span> <span class=\"nf\">_cb</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n            <span class=\"n\">status</span> <span class=\"o\">=</span> <span class=\"n\">kwargs</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">(</span><span class=\"s2\">\"status\"</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"n\">status</span> <span class=\"o\">!=</span> <span class=\"s2\">\"passed\"</span><span class=\"p\">:</span>\n                <span class=\"k\">return</span> <span class=\"n\">func</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"kc\">None</span>\n        <span class=\"k\">return</span> <span class=\"n\">_cb</span>\n\n\n    <span class=\"k\">def</span> <span class=\"nf\">my_callback_example_4</span><span class=\"p\">(</span><span class=\"n\">func</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"Another example of a callback wrapper using lambda expression.\"\"\"</span>\n        <span class=\"k\">return</span> <span class=\"k\">lambda</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span> <span class=\"p\">:</span> <span class=\"n\">func</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">kwargs</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">(</span><span class=\"s2\">\"status\"</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span> <span class=\"o\">!=</span> <span class=\"s2\">\"passed\"</span> <span class=\"k\">else</span> <span class=\"kc\">None</span>\n\n\n    <span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">fixture</span><span class=\"p\">(</span><span class=\"n\">scope</span><span class=\"o\">=</span><span class=\"s2\">\"function\"</span><span class=\"p\">,</span> <span class=\"n\">autouse</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n    <span class=\"k\">def</span> <span class=\"nf\">my_fixture_example</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"Function-scoped fixture to set the callbacks.\"\"\"</span>\n        <span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">node</span><span class=\"o\">.</span><span class=\"n\">meta_block_condition_cb</span> <span class=\"o\">=</span> <span class=\"n\">my_callback_example_1</span><span class=\"p\">(</span><span class=\"n\">my_func</span><span class=\"p\">,</span> <span class=\"mi\">17</span><span class=\"p\">,</span> <span class=\"mi\">18</span><span class=\"p\">,</span> <span class=\"mi\">19</span><span class=\"p\">)</span>\n        <span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">node</span><span class=\"o\">.</span><span class=\"n\">meta_block_cb</span> <span class=\"o\">=</span> <span class=\"n\">my_callback_example_3</span><span class=\"p\">(</span><span class=\"n\">my_func</span><span class=\"p\">,</span> <span class=\"mi\">17</span><span class=\"p\">,</span> <span class=\"mi\">18</span><span class=\"p\">,</span> <span class=\"mi\">19</span><span class=\"p\">)</span>\n</pre>\n\n          </div>"}, "last_serial": 6576049, "releases": {"4.0.0": [{"comment_text": "", "digests": {"md5": "bf50fa13e87938a7df42d4263c718f0a", "sha256": "cb0d734b4e7e873dcae414008fc89b22fe6a698abc023562d66fef1a4b2e7f15"}, "downloads": -1, "filename": "pytest_adaptavist-4.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "bf50fa13e87938a7df42d4263c718f0a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 24822, "upload_time": "2019-11-26T16:52:10", "upload_time_iso_8601": "2019-11-26T16:52:10.724728Z", "url": "https://files.pythonhosted.org/packages/40/c0/359ad1f90c9b290da24a8b180838515ef15508dfd4235b294605e9d2af42/pytest_adaptavist-4.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8d3bb491a005408b3f086a57155abf10", "sha256": "dd2a2a11c3788a625a01e04b8046b680991992e6294e97a3da2eb655db7c61cb"}, "downloads": -1, "filename": "pytest-adaptavist-4.0.0.tar.gz", "has_sig": false, "md5_digest": "8d3bb491a005408b3f086a57155abf10", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 28659, "upload_time": "2019-11-26T16:52:13", "upload_time_iso_8601": "2019-11-26T16:52:13.449422Z", "url": "https://files.pythonhosted.org/packages/92/68/8e6dfe5ce1f161665892d76a24c73fc1d1c5c4898a70b4d8917017cbe983/pytest-adaptavist-4.0.0.tar.gz", "yanked": false}], "4.0.5": [{"comment_text": "", "digests": {"md5": "01146acfa990a2dd077aeb6858b70353", "sha256": "d8842885c26a3d5231eddffd670e8af6ce195eb89cc8de58fd89b99e5372cf8c"}, "downloads": -1, "filename": "pytest_adaptavist-4.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "01146acfa990a2dd077aeb6858b70353", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 24976, "upload_time": "2020-02-05T13:59:29", "upload_time_iso_8601": "2020-02-05T13:59:29.666651Z", "url": "https://files.pythonhosted.org/packages/c7/58/12f58a4b3be1e4347508faa2e61a1cc24dc0501e9dec751824a878c0034d/pytest_adaptavist-4.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1de3ac6b557a729163d8254f377042d0", "sha256": "4fe973bea5f0dca8900d2c5cf05719c14e99d3c4f4f103c46f129410250bf75f"}, "downloads": -1, "filename": "pytest-adaptavist-4.0.5.tar.gz", "has_sig": false, "md5_digest": "1de3ac6b557a729163d8254f377042d0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 28816, "upload_time": "2020-02-05T13:59:31", "upload_time_iso_8601": "2020-02-05T13:59:31.464199Z", "url": "https://files.pythonhosted.org/packages/f5/f9/4b9bf9a0cf4d41d13d4374268b4a1876a91c53f34691fc1197afb6b23d2a/pytest-adaptavist-4.0.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "01146acfa990a2dd077aeb6858b70353", "sha256": "d8842885c26a3d5231eddffd670e8af6ce195eb89cc8de58fd89b99e5372cf8c"}, "downloads": -1, "filename": "pytest_adaptavist-4.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "01146acfa990a2dd077aeb6858b70353", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 24976, "upload_time": "2020-02-05T13:59:29", "upload_time_iso_8601": "2020-02-05T13:59:29.666651Z", "url": "https://files.pythonhosted.org/packages/c7/58/12f58a4b3be1e4347508faa2e61a1cc24dc0501e9dec751824a878c0034d/pytest_adaptavist-4.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1de3ac6b557a729163d8254f377042d0", "sha256": "4fe973bea5f0dca8900d2c5cf05719c14e99d3c4f4f103c46f129410250bf75f"}, "downloads": -1, "filename": "pytest-adaptavist-4.0.5.tar.gz", "has_sig": false, "md5_digest": "1de3ac6b557a729163d8254f377042d0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 28816, "upload_time": "2020-02-05T13:59:31", "upload_time_iso_8601": "2020-02-05T13:59:31.464199Z", "url": "https://files.pythonhosted.org/packages/f5/f9/4b9bf9a0cf4d41d13d4374268b4a1876a91c53f34691fc1197afb6b23d2a/pytest-adaptavist-4.0.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:55:28 2020"}