{"info": {"author": "Hj\u00f6rtur Hjartarson", "author_email": "hjorturh88@gmail.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: System Administrators", "License :: OSI Approved :: MIT License", "Programming Language :: Python", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.1", "Programming Language :: Python :: 3.2", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6"], "description": "# DBloy\n\nA Databricks deployment CLI tool to enable Continuous Delivery of PySpark Notebooks based jobs.\n\n\n## Installation\n\n````bash\n$ pip install dbloy\n````\n\n## Usage\n\nAuthenticate with Databricks using authentication token:\n\n```bash\n$ dbloy configure \n```\n\nUpdate Databricks Job \n\n```bash\n$ dbloy apply --deploy-yml deploy.yml --configmap-yml configmap.yml --version <my_version_number>\n```\n\nwhere `deploy.yml` and `configmap.yml` contain the Job specification. The Job version is specified in `<my_version_number>`\n\n\n## Workflow\n\n![databricks workflow](https://databricks.com/wp-content/uploads/2017/10/CI-CD-BLOG4@2x-1024x211.png \"databricks workflow\")\nsource: https://databricks.com/blog/2017/10/30/continuous-integration-continuous-delivery-databricks.html \n \n![example workflow](https://github.com/hjh17/dbloy/blob/master/uml.png?raw=true \"example workflow\")\n \n\n\n \n## Example Usage\n\nSee [example/gitlab_my-etl-job](https://github.com/hjh17/dbloy/tree/master/example/gitlab_my-etl-job) for a example ETL repository using Gitlab's CI/CD.\n\n\nA Deployment requires the following:\n\n* Deployment manifest\n* Configuration manifest\n* A main Databricks Notebook source file available locally. \n* (Optional) Attached python library containing the core logic. This allows easier unit testing of \n\n\n### Creating a Deployment\n\n\n\ndeploy.yml\n\n````yaml\nkind: Deployment\nmetadata:\n  name: my-etl-job\n  workspace: Shared\ntemplate:\n  job:\n    name: My ETL Job\n  notifications:\n    email:\n      no_alert_for_skipped_runs: true\n      on_failure :\n        - my_email@my_org.com\n  base_notebook: main\n  notebooks:\n    - EPHEMERAL_NOTEBOOK_1: notebook_name1\n    - EPHEMERAL_NOTEBOOK_2: notebook_name2\n  libraries:\n    - egg_main: dbfs:/python35/my_python_lib/my_python_lib-VERSION-py3.5.egg\n    - egg: dbfs:/python35/static_python_lib.egg\n    - pypi:\n        package: scikit-learn==0.20.3\n    - pypi:\n        package: statsmodels==0.10.1\n    - pypi:\n        package: prometheus-client==0.7.1\n    - jar: dbfs:/FileStore/jars/e9b87e4c_c754_4707_a62a_44ef47535b39-azure_cosmosdb_spark_2_4_0_2_11_1_3_4_uber-38021.jar\n  run:\n    max_concurrent_runs: 1\n    max_retries: 1\n    min_retry_interval_millis: 600000\n    retry_on_timeout: true\n    timeout_seconds: 10800\n````\n\nconfigmap.yml\n\n````yaml\nkind: ConfigMap\nmetadata:\n  namespace: production\nparams:\n  DB_URL: production_db_url_1\n  DB_PASSWORD: production_password123\njob:\n  id: 289\n  schedule:\n    quartz_cron_expression: \"0 0 0 * * ?\"\n    timezone_id: \"Europe/Berlin\"\n  max_retries: \"1\"\ncluster:\n  spark_version: \"5.3.x-scala2.11\"\n  node_type_id: \"Standard_DS3_v2\"\n  driver_node_type_id: \"Standard_DS3_v2\"\n  autoscale:\n    min_workers: 1\n    max_workers: 2\n  spark_env_vars:\n    PYSPARK_PYTHON: \"/databricks/python3/bin/python3\"\n\n````\n\nIn this example:\n\n* Job id `289` on Databricks, indicated by the `.job.id` field in `configmap.yml`, will be updated with the name `My ETL Job`, indicated by the `.template.job.name` field in `deploy.yml`.\n* A cluster will be created on demand which is specified by the field `.cluster` in `configmap.yml`. See https://docs.databricks.com/api/latest/clusters.html#request-structure for a complete list of cluster settings. **Note**: Setting `.cluster.existing_cluster_id` will use an existing cluster. \n* Libraries specified by the field `.template.libraries` in `.deploy.yml` will be installed on the cluster. See https://docs.databricks.com/api/latest/libraries.html#library. \n **Note**: The field `.template.libraries.egg_main` is reserved for python `.egg` file that is versioned with the ETL job. \n For example when the main logic of the ETL job is put into a library. The `.egg` version number is expected to be the same as the ETL version number.\n* The main task notebook that will be executed by the job is defined by the field `.template.base_notebook` in `deploy.yml`. Task parameters are specified by the field `.params` in `configmap.yml` which will be accessible in the Notebooks via `dbutils`.\n* The notebook `main`, indicated by the field `.template.base_notebook` is the Task notebook. This notebook should be found in the workspace `/Shared/my-etl-job/<my_version_number>/main` specified by the fields `.metadata` and  `.template.base_notebook` in `deploy.yml`. The version number `<my_version_number>` will be specified in the CLI command.\n* Two ephemeral notebooks are available under `/Shared/my-etl-job/<my_version_number>/notebook_name1` and `/Shared/my-etl-job/<my_version_number>/notebook_name2`. This allows the main task to execute nested Notebooks, e.g.\n```\nnotebook_path_1 = dbutils.widgets.get(\"EPHEMERAL_NOTEBOOK_1\")\ndbutils.notebook.run(notebook_path_1)\n```\n \n \nCreate the Deployment by running the following command:\n\n```bash\n$ dbloy apply --deploy-yml deploy.yml --configmap-yml configmap.yml --version <my_version_number>\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/hjh17/dbloy", "keywords": "databricks cli ci/cd", "license": "['MIT']", "maintainer": "Hj\u00f6rtur Hjartarson", "maintainer_email": "hjorturh88@gmail.com", "name": "dbloy", "package_url": "https://pypi.org/project/dbloy/", "platform": "", "project_url": "https://pypi.org/project/dbloy/", "project_urls": {"Homepage": "https://github.com/hjh17/dbloy"}, "release_url": "https://pypi.org/project/dbloy/0.3.0/", "requires_dist": null, "requires_python": "", "summary": "Continuous Delivery tool for PySpark Notebooks based jobs on Databricks.", "version": "0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>DBloy</h1>\n<p>A Databricks deployment CLI tool to enable Continuous Delivery of PySpark Notebooks based jobs.</p>\n<h2>Installation</h2>\n<pre>$ pip install dbloy\n</pre>\n<h2>Usage</h2>\n<p>Authenticate with Databricks using authentication token:</p>\n<pre>$ dbloy configure \n</pre>\n<p>Update Databricks Job</p>\n<pre>$ dbloy apply --deploy-yml deploy.yml --configmap-yml configmap.yml --version &lt;my_version_number&gt;\n</pre>\n<p>where <code>deploy.yml</code> and <code>configmap.yml</code> contain the Job specification. The Job version is specified in <code>&lt;my_version_number&gt;</code></p>\n<h2>Workflow</h2>\n<p><img alt=\"databricks workflow\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/57b1006f8702ca0dfdea6ca742dc03cb97c0e2c6/68747470733a2f2f64617461627269636b732e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031372f31302f43492d43442d424c4f47344032782d31303234783231312e706e67\">\nsource: <a href=\"https://databricks.com/blog/2017/10/30/continuous-integration-continuous-delivery-databricks.html\" rel=\"nofollow\">https://databricks.com/blog/2017/10/30/continuous-integration-continuous-delivery-databricks.html</a></p>\n<p><img alt=\"example workflow\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c3c3157b76e856416fcd18a16dee4f2f1b02c11a/68747470733a2f2f6769746875622e636f6d2f686a6831372f64626c6f792f626c6f622f6d61737465722f756d6c2e706e673f7261773d74727565\"></p>\n<h2>Example Usage</h2>\n<p>See <a href=\"https://github.com/hjh17/dbloy/tree/master/example/gitlab_my-etl-job\" rel=\"nofollow\">example/gitlab_my-etl-job</a> for a example ETL repository using Gitlab's CI/CD.</p>\n<p>A Deployment requires the following:</p>\n<ul>\n<li>Deployment manifest</li>\n<li>Configuration manifest</li>\n<li>A main Databricks Notebook source file available locally.</li>\n<li>(Optional) Attached python library containing the core logic. This allows easier unit testing of</li>\n</ul>\n<h3>Creating a Deployment</h3>\n<p>deploy.yml</p>\n<pre><span class=\"nt\">kind</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">Deployment</span>\n<span class=\"nt\">metadata</span><span class=\"p\">:</span>\n  <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">my-etl-job</span>\n  <span class=\"nt\">workspace</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">Shared</span>\n<span class=\"nt\">template</span><span class=\"p\">:</span>\n  <span class=\"nt\">job</span><span class=\"p\">:</span>\n    <span class=\"nt\">name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">My ETL Job</span>\n  <span class=\"nt\">notifications</span><span class=\"p\">:</span>\n    <span class=\"nt\">email</span><span class=\"p\">:</span>\n      <span class=\"nt\">no_alert_for_skipped_runs</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n      <span class=\"nt\">on_failure </span><span class=\"p\">:</span>\n        <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">my_email@my_org.com</span>\n  <span class=\"nt\">base_notebook</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">main</span>\n  <span class=\"nt\">notebooks</span><span class=\"p\">:</span>\n    <span class=\"p p-Indicator\">-</span> <span class=\"nt\">EPHEMERAL_NOTEBOOK_1</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">notebook_name1</span>\n    <span class=\"p p-Indicator\">-</span> <span class=\"nt\">EPHEMERAL_NOTEBOOK_2</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">notebook_name2</span>\n  <span class=\"nt\">libraries</span><span class=\"p\">:</span>\n    <span class=\"p p-Indicator\">-</span> <span class=\"nt\">egg_main</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">dbfs:/python35/my_python_lib/my_python_lib-VERSION-py3.5.egg</span>\n    <span class=\"p p-Indicator\">-</span> <span class=\"nt\">egg</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">dbfs:/python35/static_python_lib.egg</span>\n    <span class=\"p p-Indicator\">-</span> <span class=\"nt\">pypi</span><span class=\"p\">:</span>\n        <span class=\"nt\">package</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">scikit-learn==0.20.3</span>\n    <span class=\"p p-Indicator\">-</span> <span class=\"nt\">pypi</span><span class=\"p\">:</span>\n        <span class=\"nt\">package</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">statsmodels==0.10.1</span>\n    <span class=\"p p-Indicator\">-</span> <span class=\"nt\">pypi</span><span class=\"p\">:</span>\n        <span class=\"nt\">package</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">prometheus-client==0.7.1</span>\n    <span class=\"p p-Indicator\">-</span> <span class=\"nt\">jar</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">dbfs:/FileStore/jars/e9b87e4c_c754_4707_a62a_44ef47535b39-azure_cosmosdb_spark_2_4_0_2_11_1_3_4_uber-38021.jar</span>\n  <span class=\"nt\">run</span><span class=\"p\">:</span>\n    <span class=\"nt\">max_concurrent_runs</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n    <span class=\"nt\">max_retries</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n    <span class=\"nt\">min_retry_interval_millis</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">600000</span>\n    <span class=\"nt\">retry_on_timeout</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n    <span class=\"nt\">timeout_seconds</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">10800</span>\n</pre>\n<p>configmap.yml</p>\n<pre><span class=\"nt\">kind</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">ConfigMap</span>\n<span class=\"nt\">metadata</span><span class=\"p\">:</span>\n  <span class=\"nt\">namespace</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">production</span>\n<span class=\"nt\">params</span><span class=\"p\">:</span>\n  <span class=\"nt\">DB_URL</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">production_db_url_1</span>\n  <span class=\"nt\">DB_PASSWORD</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">production_password123</span>\n<span class=\"nt\">job</span><span class=\"p\">:</span>\n  <span class=\"nt\">id</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">289</span>\n  <span class=\"nt\">schedule</span><span class=\"p\">:</span>\n    <span class=\"nt\">quartz_cron_expression</span><span class=\"p\">:</span> <span class=\"s\">\"0</span><span class=\"nv\"> </span><span class=\"s\">0</span><span class=\"nv\"> </span><span class=\"s\">0</span><span class=\"nv\"> </span><span class=\"s\">*</span><span class=\"nv\"> </span><span class=\"s\">*</span><span class=\"nv\"> </span><span class=\"s\">?\"</span>\n    <span class=\"nt\">timezone_id</span><span class=\"p\">:</span> <span class=\"s\">\"Europe/Berlin\"</span>\n  <span class=\"nt\">max_retries</span><span class=\"p\">:</span> <span class=\"s\">\"1\"</span>\n<span class=\"nt\">cluster</span><span class=\"p\">:</span>\n  <span class=\"nt\">spark_version</span><span class=\"p\">:</span> <span class=\"s\">\"5.3.x-scala2.11\"</span>\n  <span class=\"nt\">node_type_id</span><span class=\"p\">:</span> <span class=\"s\">\"Standard_DS3_v2\"</span>\n  <span class=\"nt\">driver_node_type_id</span><span class=\"p\">:</span> <span class=\"s\">\"Standard_DS3_v2\"</span>\n  <span class=\"nt\">autoscale</span><span class=\"p\">:</span>\n    <span class=\"nt\">min_workers</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n    <span class=\"nt\">max_workers</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">2</span>\n  <span class=\"nt\">spark_env_vars</span><span class=\"p\">:</span>\n    <span class=\"nt\">PYSPARK_PYTHON</span><span class=\"p\">:</span> <span class=\"s\">\"/databricks/python3/bin/python3\"</span>\n</pre>\n<p>In this example:</p>\n<ul>\n<li>Job id <code>289</code> on Databricks, indicated by the <code>.job.id</code> field in <code>configmap.yml</code>, will be updated with the name <code>My ETL Job</code>, indicated by the <code>.template.job.name</code> field in <code>deploy.yml</code>.</li>\n<li>A cluster will be created on demand which is specified by the field <code>.cluster</code> in <code>configmap.yml</code>. See <a href=\"https://docs.databricks.com/api/latest/clusters.html#request-structure\" rel=\"nofollow\">https://docs.databricks.com/api/latest/clusters.html#request-structure</a> for a complete list of cluster settings. <strong>Note</strong>: Setting <code>.cluster.existing_cluster_id</code> will use an existing cluster.</li>\n<li>Libraries specified by the field <code>.template.libraries</code> in <code>.deploy.yml</code> will be installed on the cluster. See <a href=\"https://docs.databricks.com/api/latest/libraries.html#library\" rel=\"nofollow\">https://docs.databricks.com/api/latest/libraries.html#library</a>.\n<strong>Note</strong>: The field <code>.template.libraries.egg_main</code> is reserved for python <code>.egg</code> file that is versioned with the ETL job.\nFor example when the main logic of the ETL job is put into a library. The <code>.egg</code> version number is expected to be the same as the ETL version number.</li>\n<li>The main task notebook that will be executed by the job is defined by the field <code>.template.base_notebook</code> in <code>deploy.yml</code>. Task parameters are specified by the field <code>.params</code> in <code>configmap.yml</code> which will be accessible in the Notebooks via <code>dbutils</code>.</li>\n<li>The notebook <code>main</code>, indicated by the field <code>.template.base_notebook</code> is the Task notebook. This notebook should be found in the workspace <code>/Shared/my-etl-job/&lt;my_version_number&gt;/main</code> specified by the fields <code>.metadata</code> and  <code>.template.base_notebook</code> in <code>deploy.yml</code>. The version number <code>&lt;my_version_number&gt;</code> will be specified in the CLI command.</li>\n<li>Two ephemeral notebooks are available under <code>/Shared/my-etl-job/&lt;my_version_number&gt;/notebook_name1</code> and <code>/Shared/my-etl-job/&lt;my_version_number&gt;/notebook_name2</code>. This allows the main task to execute nested Notebooks, e.g.</li>\n</ul>\n<pre><code>notebook_path_1 = dbutils.widgets.get(\"EPHEMERAL_NOTEBOOK_1\")\ndbutils.notebook.run(notebook_path_1)\n</code></pre>\n<p>Create the Deployment by running the following command:</p>\n<pre>$ dbloy apply --deploy-yml deploy.yml --configmap-yml configmap.yml --version &lt;my_version_number&gt;\n</pre>\n\n          </div>"}, "last_serial": 5774523, "releases": {"0.3.0": [{"comment_text": "", "digests": {"md5": "f6694e02bd5d6dec3eff08fc7c3577b6", "sha256": "b757ce138ab254fbeab17c83f4fee3d58bf974dfbb08173b11a3cfac769aafd9"}, "downloads": -1, "filename": "dbloy-0.3.0.tar.gz", "has_sig": false, "md5_digest": "f6694e02bd5d6dec3eff08fc7c3577b6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5501, "upload_time": "2019-08-30T13:35:28", "upload_time_iso_8601": "2019-08-30T13:35:28.737128Z", "url": "https://files.pythonhosted.org/packages/36/5d/852215b182841bb3e96d9e62859a94ccc18ba54f91a4ff7546d6fc15da18/dbloy-0.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f6694e02bd5d6dec3eff08fc7c3577b6", "sha256": "b757ce138ab254fbeab17c83f4fee3d58bf974dfbb08173b11a3cfac769aafd9"}, "downloads": -1, "filename": "dbloy-0.3.0.tar.gz", "has_sig": false, "md5_digest": "f6694e02bd5d6dec3eff08fc7c3577b6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5501, "upload_time": "2019-08-30T13:35:28", "upload_time_iso_8601": "2019-08-30T13:35:28.737128Z", "url": "https://files.pythonhosted.org/packages/36/5d/852215b182841bb3e96d9e62859a94ccc18ba54f91a4ff7546d6fc15da18/dbloy-0.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:39:56 2020"}