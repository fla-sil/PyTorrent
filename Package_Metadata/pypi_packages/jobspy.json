{"info": {"author": "Josiah Carlson", "author_email": "josiah.carlson@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "License :: OSI Approved :: GNU Library or Lesser General Public License (LGPL)", "Programming Language :: Python", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "Job resource input/output control using Redis as a locking layer\n\nCopyright 2016-2018 Josiah Carlson\n\nThis library licensed under the GNU LGPL v2.1\n\nThe initial library requirements and implementation were done for OpenMail LLC.\njobs.py (this library) was more or less intended to offer input and output\ncontrol like Luigi and/or Airflow (both Python packages), with fewer hard\nintegration requirements. In fact, jobs.py has been used successfully as part\nof jobs running in a cron schedule via Jenkins, in build chains in Jenkins,\ninside individual rpqueue tasks, and even inside individual Flask web requests\nfor some high-value data (jobs.py is backed by Redis, so job locking overhead\n*can* be low, even when you need to keep data safe).\n\nSource: https://github.com/josiahcarlson/jobs/\nPyPI: https://pypi.python.org/pypi/jobspy/\nDocs: https://pythonhosted.org/jobspy/\n\nFeatures\n========\n\nInput/output locking on multiple *named* keys, called \"inputs\" and \"outputs\".\n\n* All keys are case-sensitive\n* Multiple readers on input keys\n* Exclusive single writer on output keys (no readers or other writers)\n* All inputs must have been an output previously\n* Optional global and per-job history of sanitized input/output edges (enabled\n  by default)\n* Lock multiple inputs and outputs simultaneously, e.g. to produce outputs Y and\n  Z, I need to consume inputs A, B, C.\n\nHow to use\n==========\n\n* Install jobs.py::\n\n    $ sudo pip install jobspy\n\n* Import jobs.py and configure the Redis connection *required* (maybe put this\n  in some configuration file)::\n\n    # in myjob.py or local_config.py\n    import jobs\n    jobs.CONN = redis.Redis(...)\n\n* Use as a decorator on a function (must explicitly .start() the job, .stop()\n  performed automatically if left uncalled)::\n\n    # in myjob.py\n\n    @jobs.resource_manager(['input1', 'input2'], ['output1', 'output2'], duration=300, wait=900)\n    def some_job(job):\n        job.add_inputs('input6', 'input7')\n        job.add_outputs(...)\n        job.start()\n        # At this point, all inputs and outputs are locked according to the\n        # locking semantics specified in the documentation.\n\n        # If you call job.stop(failed=True), then the outputs will not be\n        # \"written\"\n        #job.stop(failed=True)\n        # If you call job.stop(), then the outputs will be \"written\"\n        job.stop()\n\n        # Alternating job.stop() with job.start() is okay! You will drop the\n        # locks in the .stop(), but will (try to) get them again with the\n        # .start()\n        job.start()\n\n        # But if you need the lock for longer than the requested duration, you\n        # can also periodically refresh the lock. The lock is only actually\n        # refreshed once per second at most, and you can only refresh an already\n        # started lock.\n        job.refresh()\n\n        # If an exception is raised and not caught before the decorator catches\n        # it, the job will be stopped by the decorator, as though failed=True:\n        raise Exception(\"Oops!\")\n        # will stop the job the same as\n        #   job.stop(failed=True)\n        # ... where the exception will bubble up out of the decorator.\n\n* Or use as a context manager for automatic start/stop calling, with the same\n  exception handling semantics as the decorator::\n\n    def multi_step_job(arg1, arg2, ...):\n        with jobs.ResourceManager([arg1], [arg2], duration=30, wait=60, overwrite=True) as job:\n            for something in loop:\n                # do something\n                job.refresh()\n            if bad_condition:\n                raise Exception(\"Something bad happened, don't mark arg2 as available\")\n            elif other_bad_condition:\n                # stop the job, not setting\n                job.stop(failed=True)\n\n        # arg2 should exist after it was an output, and we didn't get an\n        # exception... though if someone else is writing to it immediately in\n        # another call, then this may block...\n        with jobs.ResourceManager([arg2], ['output.x'], duration=60, wait=900, overwrite=True):\n            # something else\n            pass\n\n        # output.x should be written if the most recent ResourceManager stopped\n        # cleanly.\n        return\n\nMore examples\n-------------\n\n* Scheduled at 1AM UTC (5/6PM Pacific, depending on DST)::\n\n        import datetime\n\n        FMT = '%Y-%m-%d'\n\n        def yesterday():\n            return (datetime.datetime.utcnow().date() - datetime.timedelta(days=1)).strftime(FMT)\n\n        @jobs.resource_manager([jobs.NG.reporting.events], (), 300, 900)\n        def aggregate_daily_events(job):\n            yf = yesterday()\n            # outputs 'reporting.events_by_partner.YYYY-MM-DD'\n            # we can add job inputs and outputs inside a decorated function before\n            # we call .start()\n            job.add_outputs(jobs.NG.reporting.events_by_partner[yf])\n\n            job.start()\n            # actually aggregate events\n\n* Scheduled the next day around the time when we expect upstream reporting to\n  be available::\n\n        @jobs.resource_manager((), (), 300, 900)\n        def fetch_daily_revenue(job):\n            yf = yesterday()\n            job.add_outputs(jobs.NG.reporting.upsteam_revenue[yf])\n\n            job.start()\n            # actually fetch daily revenue\n\n* Executed downstream of fetch_daily_revenue()::\n\n        @jobs.resource_manager((), (), 300, 900)\n        def send_reports(job):\n            yf = yesterday()\n\n            # having jobs inputs here ensures that both of the *expected* upstream\n            # flows were *actual*\n            job.add_inputs(\n                jobs.NG.reporting.events_by_partner[yf],\n                jobs.NG.reporting.upstream_revenue[yf]\n            )\n            job.add_outputs(jobs.NG.reporting.report_by_partner[yf])\n\n            job.start()\n            # inputs are available, go ahead and generate the reports!\n\n* And in other contexts...::\n\n        def make_recommendations(partners):\n            yf = yesterday()\n            for partner in partners:\n                with jobs.ResourceManager([jobs.NG.reporting.report_by_partner[yf]],\n                        [jobs.NG.reporting.recommendations_by_partner[yf][partner]], 300, 900):\n                    # job is already started\n                    # generate the recommendations for the partner\n                    pass\n\n\nConfiguration options\n=====================\n\nAll configuration options are available as options on the jobs.py module itself,\nthough you *can* override the connection explicitly on a per-job basis. See the\n'Connection configuration' section below for more details.::\n\n    # The Redis connection, REQUIRED!\n    jobs.CONN = redis.Redis()\n\n    # Sets a prefix to be used on all keys stored in Redis (optional)\n    jobs.GLOBAL_PREFIX = ''\n\n    # Keep a sanitized ZSET of inputs and outputs, available for traversal\n    # later. Note: sanitization runs the following on all edges before storage:\n    #   edge = re.sub('[0-9][0-9-]*', '*', edge)\n    # ... which allows you to get a compact flow graph even in cases where you\n    # have day-parameterized builds.\n    jobs.GRAPH_HISTORY = True\n\n    # Sometimes you don't want your outputs to last forever (sometimes history\n    # should be forgotten, right?), and jobs.py gives you the chance to say as\n    # much.\n    # By default, a `None` duration means that outputs will last forever. Any\n    # other value will be used in a call to `expire` on the associated output\n    # keys after they are set on a job's successful completion. This value is in\n    # seconds.\n    jobs.OUTPUT_DURATION = None\n\n    # To use a logger that doesn't print to standard output, set the logging\n    # object at the module level (see below). By default, the built-in \"default\n    # logger\" prints to standard output.\n    jobs.DEFAULT_LOGGER = logging.getLogger(...)\n\nUsing jobs.py with a custom Redis configuration\n===============================================\n\nIf you would like to use jobs.py as a script (for the convenient command-line\noptions), you need to create a wrapper module, which can also act as your\ngeneral configuration updates for jobs.py (hack because I needed to release\nthis as open-source before the end of summer)::\n\n\n    # myjobs.py\n    import jobs\n    jobs.CONN = ...\n    jobs.DEFAULT_LOGGER = ...\n    jobs.GLOBAL_PREFIX = ...\n    jobs.GRAPH_HISTORY = ...\n    jobs.OUTPUT_DURATION = ...\n\n    from jobs import *\n\n    if __name__ == '__main__':\n        main()\n\nThen you can use this as::\n\n    $ python myjobs.py --help\n\n\nAnd you can use ``myjobs.py`` everywhere, which will have all of your\nconfiguration handled.::\n\n    # daily_report.py\n    import myjobs\n\n    @myjobs.resource_manager(...)\n    def daily_reporting(job, ...):\n        # exactly the same as before.", "description_content_type": "", "docs_url": "https://pythonhosted.org/jobspy/", "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/josiahcarlson/jobs", "keywords": "", "license": "GNU LGPL v2.1", "maintainer": "", "maintainer_email": "", "name": "jobspy", "package_url": "https://pypi.org/project/jobspy/", "platform": "", "project_url": "https://pypi.org/project/jobspy/", "project_urls": {"Homepage": "https://github.com/josiahcarlson/jobs"}, "release_url": "https://pypi.org/project/jobspy/0.27.0/", "requires_dist": null, "requires_python": "", "summary": "Use Redis as a job input/output coordinator.", "version": "0.27.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Job resource input/output control using Redis as a locking layer</p>\n<p>Copyright 2016-2018 Josiah Carlson</p>\n<p>This library licensed under the GNU LGPL v2.1</p>\n<p>The initial library requirements and implementation were done for OpenMail LLC.\njobs.py (this library) was more or less intended to offer input and output\ncontrol like Luigi and/or Airflow (both Python packages), with fewer hard\nintegration requirements. In fact, jobs.py has been used successfully as part\nof jobs running in a cron schedule via Jenkins, in build chains in Jenkins,\ninside individual rpqueue tasks, and even inside individual Flask web requests\nfor some high-value data (jobs.py is backed by Redis, so job locking overhead\n<em>can</em> be low, even when you need to keep data safe).</p>\n<p>Source: <a href=\"https://github.com/josiahcarlson/jobs/\" rel=\"nofollow\">https://github.com/josiahcarlson/jobs/</a>\nPyPI: <a href=\"https://pypi.python.org/pypi/jobspy/\" rel=\"nofollow\">https://pypi.python.org/pypi/jobspy/</a>\nDocs: <a href=\"https://pythonhosted.org/jobspy/\" rel=\"nofollow\">https://pythonhosted.org/jobspy/</a></p>\n<div id=\"features\">\n<h2>Features</h2>\n<p>Input/output locking on multiple <em>named</em> keys, called \u201cinputs\u201d and \u201coutputs\u201d.</p>\n<ul>\n<li>All keys are case-sensitive</li>\n<li>Multiple readers on input keys</li>\n<li>Exclusive single writer on output keys (no readers or other writers)</li>\n<li>All inputs must have been an output previously</li>\n<li>Optional global and per-job history of sanitized input/output edges (enabled\nby default)</li>\n<li>Lock multiple inputs and outputs simultaneously, e.g. to produce outputs Y and\nZ, I need to consume inputs A, B, C.</li>\n</ul>\n</div>\n<div id=\"how-to-use\">\n<h2>How to use</h2>\n<ul>\n<li><p>Install jobs.py:</p>\n<pre>$ sudo pip install jobspy\n</pre>\n</li>\n<li><p>Import jobs.py and configure the Redis connection <em>required</em> (maybe put this\nin some configuration file):</p>\n<pre># in myjob.py or local_config.py\nimport jobs\njobs.CONN = redis.Redis(...)\n</pre>\n</li>\n<li><p>Use as a decorator on a function (must explicitly .start() the job, .stop()\nperformed automatically if left uncalled):</p>\n<pre># in myjob.py\n\n@jobs.resource_manager(['input1', 'input2'], ['output1', 'output2'], duration=300, wait=900)\ndef some_job(job):\n    job.add_inputs('input6', 'input7')\n    job.add_outputs(...)\n    job.start()\n    # At this point, all inputs and outputs are locked according to the\n    # locking semantics specified in the documentation.\n\n    # If you call job.stop(failed=True), then the outputs will not be\n    # \"written\"\n    #job.stop(failed=True)\n    # If you call job.stop(), then the outputs will be \"written\"\n    job.stop()\n\n    # Alternating job.stop() with job.start() is okay! You will drop the\n    # locks in the .stop(), but will (try to) get them again with the\n    # .start()\n    job.start()\n\n    # But if you need the lock for longer than the requested duration, you\n    # can also periodically refresh the lock. The lock is only actually\n    # refreshed once per second at most, and you can only refresh an already\n    # started lock.\n    job.refresh()\n\n    # If an exception is raised and not caught before the decorator catches\n    # it, the job will be stopped by the decorator, as though failed=True:\n    raise Exception(\"Oops!\")\n    # will stop the job the same as\n    #   job.stop(failed=True)\n    # ... where the exception will bubble up out of the decorator.\n</pre>\n</li>\n<li><p>Or use as a context manager for automatic start/stop calling, with the same\nexception handling semantics as the decorator:</p>\n<pre>def multi_step_job(arg1, arg2, ...):\n    with jobs.ResourceManager([arg1], [arg2], duration=30, wait=60, overwrite=True) as job:\n        for something in loop:\n            # do something\n            job.refresh()\n        if bad_condition:\n            raise Exception(\"Something bad happened, don't mark arg2 as available\")\n        elif other_bad_condition:\n            # stop the job, not setting\n            job.stop(failed=True)\n\n    # arg2 should exist after it was an output, and we didn't get an\n    # exception... though if someone else is writing to it immediately in\n    # another call, then this may block...\n    with jobs.ResourceManager([arg2], ['output.x'], duration=60, wait=900, overwrite=True):\n        # something else\n        pass\n\n    # output.x should be written if the most recent ResourceManager stopped\n    # cleanly.\n    return\n</pre>\n</li>\n</ul>\n<div id=\"more-examples\">\n<h3>More examples</h3>\n<ul>\n<li><p>Scheduled at 1AM UTC (5/6PM Pacific, depending on DST):</p>\n<pre>import datetime\n\nFMT = '%Y-%m-%d'\n\ndef yesterday():\n    return (datetime.datetime.utcnow().date() - datetime.timedelta(days=1)).strftime(FMT)\n\n@jobs.resource_manager([jobs.NG.reporting.events], (), 300, 900)\ndef aggregate_daily_events(job):\n    yf = yesterday()\n    # outputs 'reporting.events_by_partner.YYYY-MM-DD'\n    # we can add job inputs and outputs inside a decorated function before\n    # we call .start()\n    job.add_outputs(jobs.NG.reporting.events_by_partner[yf])\n\n    job.start()\n    # actually aggregate events\n</pre>\n</li>\n<li><p>Scheduled the next day around the time when we expect upstream reporting to\nbe available:</p>\n<pre>@jobs.resource_manager((), (), 300, 900)\ndef fetch_daily_revenue(job):\n    yf = yesterday()\n    job.add_outputs(jobs.NG.reporting.upsteam_revenue[yf])\n\n    job.start()\n    # actually fetch daily revenue\n</pre>\n</li>\n<li><p>Executed downstream of fetch_daily_revenue():</p>\n<pre>@jobs.resource_manager((), (), 300, 900)\ndef send_reports(job):\n    yf = yesterday()\n\n    # having jobs inputs here ensures that both of the *expected* upstream\n    # flows were *actual*\n    job.add_inputs(\n        jobs.NG.reporting.events_by_partner[yf],\n        jobs.NG.reporting.upstream_revenue[yf]\n    )\n    job.add_outputs(jobs.NG.reporting.report_by_partner[yf])\n\n    job.start()\n    # inputs are available, go ahead and generate the reports!\n</pre>\n</li>\n<li><p>And in other contexts\u2026:</p>\n<pre>def make_recommendations(partners):\n    yf = yesterday()\n    for partner in partners:\n        with jobs.ResourceManager([jobs.NG.reporting.report_by_partner[yf]],\n                [jobs.NG.reporting.recommendations_by_partner[yf][partner]], 300, 900):\n            # job is already started\n            # generate the recommendations for the partner\n            pass\n</pre>\n</li>\n</ul>\n</div>\n</div>\n<div id=\"configuration-options\">\n<h2>Configuration options</h2>\n<p>All configuration options are available as options on the jobs.py module itself,\nthough you <em>can</em> override the connection explicitly on a per-job basis. See the\n\u2018Connection configuration\u2019 section below for more details.:</p>\n<pre># The Redis connection, REQUIRED!\njobs.CONN = redis.Redis()\n\n# Sets a prefix to be used on all keys stored in Redis (optional)\njobs.GLOBAL_PREFIX = ''\n\n# Keep a sanitized ZSET of inputs and outputs, available for traversal\n# later. Note: sanitization runs the following on all edges before storage:\n#   edge = re.sub('[0-9][0-9-]*', '*', edge)\n# ... which allows you to get a compact flow graph even in cases where you\n# have day-parameterized builds.\njobs.GRAPH_HISTORY = True\n\n# Sometimes you don't want your outputs to last forever (sometimes history\n# should be forgotten, right?), and jobs.py gives you the chance to say as\n# much.\n# By default, a `None` duration means that outputs will last forever. Any\n# other value will be used in a call to `expire` on the associated output\n# keys after they are set on a job's successful completion. This value is in\n# seconds.\njobs.OUTPUT_DURATION = None\n\n# To use a logger that doesn't print to standard output, set the logging\n# object at the module level (see below). By default, the built-in \"default\n# logger\" prints to standard output.\njobs.DEFAULT_LOGGER = logging.getLogger(...)\n</pre>\n</div>\n<div id=\"using-jobs-py-with-a-custom-redis-configuration\">\n<h2>Using jobs.py with a custom Redis configuration</h2>\n<p>If you would like to use jobs.py as a script (for the convenient command-line\noptions), you need to create a wrapper module, which can also act as your\ngeneral configuration updates for jobs.py (hack because I needed to release\nthis as open-source before the end of summer):</p>\n<pre># myjobs.py\nimport jobs\njobs.CONN = ...\njobs.DEFAULT_LOGGER = ...\njobs.GLOBAL_PREFIX = ...\njobs.GRAPH_HISTORY = ...\njobs.OUTPUT_DURATION = ...\n\nfrom jobs import *\n\nif __name__ == '__main__':\n    main()\n</pre>\n<p>Then you can use this as:</p>\n<pre>$ python myjobs.py --help\n</pre>\n<p>And you can use <tt>myjobs.py</tt> everywhere, which will have all of your\nconfiguration handled.:</p>\n<pre># daily_report.py\nimport myjobs\n\n@myjobs.resource_manager(...)\ndef daily_reporting(job, ...):\n    # exactly the same as before.\n</pre>\n</div>\n\n          </div>"}, "last_serial": 5355450, "releases": {"0.25.0": [{"comment_text": "", "digests": {"md5": "f1f5339ac6fcbb98a4a2aae60b6ef2be", "sha256": "659347674f8124e218d934538e277ab32c16450f7d6dcbaf706f4822f86ecb2d"}, "downloads": -1, "filename": "jobspy-0.25.0.tar.gz", "has_sig": false, "md5_digest": "f1f5339ac6fcbb98a4a2aae60b6ef2be", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16604, "upload_time": "2016-08-25T17:49:00", "upload_time_iso_8601": "2016-08-25T17:49:00.390535Z", "url": "https://files.pythonhosted.org/packages/50/3a/bb977b16db64b57f8f9ca3c42f539c0c110b1fde4f46d353e3c888f8a9d5/jobspy-0.25.0.tar.gz", "yanked": false}], "0.25.1": [{"comment_text": "", "digests": {"md5": "1572e62efa41a0e719fff5331e0bd514", "sha256": "72cbb2490eaf887cbff2caca0c549bce22e6e60dee7dd977f36bea6cea390e35"}, "downloads": -1, "filename": "jobspy-0.25.1.tar.gz", "has_sig": false, "md5_digest": "1572e62efa41a0e719fff5331e0bd514", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16604, "upload_time": "2016-08-25T17:50:51", "upload_time_iso_8601": "2016-08-25T17:50:51.213470Z", "url": "https://files.pythonhosted.org/packages/81/f2/e7b7cbac495e8459e662ecceb11252670c68f53751e493564e5897b36939/jobspy-0.25.1.tar.gz", "yanked": false}], "0.25.2": [{"comment_text": "", "digests": {"md5": "97655c07a082b301c06eb15a67b4484a", "sha256": "76f2f06fd9521a3a88a3d02d3cdd85e475ed40599fd17df86fa6551f731abc72"}, "downloads": -1, "filename": "jobspy-0.25.2.tar.gz", "has_sig": false, "md5_digest": "97655c07a082b301c06eb15a67b4484a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16677, "upload_time": "2016-08-25T18:09:56", "upload_time_iso_8601": "2016-08-25T18:09:56.176599Z", "url": "https://files.pythonhosted.org/packages/23/a2/190303aa4e056017fdc97ae3fe5a523c8a49b4779d7582d78f9683cae8f2/jobspy-0.25.2.tar.gz", "yanked": false}], "0.25.3": [{"comment_text": "", "digests": {"md5": "89e5d2ee890745832d9be1dd02816f32", "sha256": "eb14fcc98a180a164122706d61bf3b68bcabf4e6bd1bc0c77533042df2fec544"}, "downloads": -1, "filename": "jobspy-0.25.3.tar.gz", "has_sig": false, "md5_digest": "89e5d2ee890745832d9be1dd02816f32", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16712, "upload_time": "2016-08-25T19:18:16", "upload_time_iso_8601": "2016-08-25T19:18:16.646123Z", "url": "https://files.pythonhosted.org/packages/f1/8b/399f2d995c6ba72e27ddee66957bfa718f816a832d8eb90a850920e1c690/jobspy-0.25.3.tar.gz", "yanked": false}], "0.25.4": [{"comment_text": "", "digests": {"md5": "8dda2a87490038c4e660972e2eaa66a3", "sha256": "84d0acd471c45fedaaf82db805fc52dbfdd0dd9399a926082ed435e3229df891"}, "downloads": -1, "filename": "jobspy-0.25.4.tar.gz", "has_sig": false, "md5_digest": "8dda2a87490038c4e660972e2eaa66a3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16724, "upload_time": "2016-08-25T19:23:46", "upload_time_iso_8601": "2016-08-25T19:23:46.829199Z", "url": "https://files.pythonhosted.org/packages/9b/da/4d2403bf278b98528e36af1a67cb88deec5c1dec309e8b1d3e093022c758/jobspy-0.25.4.tar.gz", "yanked": false}], "0.25.5": [{"comment_text": "", "digests": {"md5": "9e84012cdae85ab8a54ffa94cc25eca8", "sha256": "dc246df00b5a298aeafb617de27d4ffe0f88b036ccba24fecbf0f9c2d36373e0"}, "downloads": -1, "filename": "jobspy-0.25.5.tar.gz", "has_sig": false, "md5_digest": "9e84012cdae85ab8a54ffa94cc25eca8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16698, "upload_time": "2016-08-25T19:25:04", "upload_time_iso_8601": "2016-08-25T19:25:04.882195Z", "url": "https://files.pythonhosted.org/packages/6b/53/2cfe236f8c1ba65fa56a845433b860f53222164182baf768378e1faecbbd/jobspy-0.25.5.tar.gz", "yanked": false}], "0.25.6": [{"comment_text": "", "digests": {"md5": "062dde01d69f4b4d367e451295951947", "sha256": "400e225907c0dc3e2839cce77ed3ca4a6676f644c06909bb42b9cbdb080c3640"}, "downloads": -1, "filename": "jobspy-0.25.6.tar.gz", "has_sig": false, "md5_digest": "062dde01d69f4b4d367e451295951947", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16706, "upload_time": "2016-08-25T22:10:12", "upload_time_iso_8601": "2016-08-25T22:10:12.991610Z", "url": "https://files.pythonhosted.org/packages/f5/a9/9bed3e98c1f8b83e2128360b207251161aa4ed155684f597e141aefc7608/jobspy-0.25.6.tar.gz", "yanked": false}], "0.26.0": [{"comment_text": "", "digests": {"md5": "327580de9d907c0c2514df3b08a85494", "sha256": "7819dbc89f2226a3182a6a191b3874bdf59f2e859d38efad3bfd9c87293a6af5"}, "downloads": -1, "filename": "jobspy-0.26.0.tar.gz", "has_sig": false, "md5_digest": "327580de9d907c0c2514df3b08a85494", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18460, "upload_time": "2016-10-17T17:08:11", "upload_time_iso_8601": "2016-10-17T17:08:11.244651Z", "url": "https://files.pythonhosted.org/packages/7a/73/eeb207be57fb960f25f47b1f519bf0f78c2a5811bb6c7319f50475904c0d/jobspy-0.26.0.tar.gz", "yanked": false}], "0.26.1": [{"comment_text": "", "digests": {"md5": "1930a7675b58fb80b13eec9ba227a66b", "sha256": "d7dd05526d01239cd86786e5494346f235ebb4e12414f876c8fccf27ed4a90b8"}, "downloads": -1, "filename": "jobspy-0.26.1.tar.gz", "has_sig": false, "md5_digest": "1930a7675b58fb80b13eec9ba227a66b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16388, "upload_time": "2017-01-25T22:36:05", "upload_time_iso_8601": "2017-01-25T22:36:05.804151Z", "url": "https://files.pythonhosted.org/packages/99/17/cd1ef56fc758158faa3dbd76395b599cfe5f7e86dd6d3dc285c4d0d94809/jobspy-0.26.1.tar.gz", "yanked": false}], "0.27.0": [{"comment_text": "", "digests": {"md5": "b3b27d42f6634d0264a2844a67fbd37d", "sha256": "d87ba0e255be1c0598844baca4f6e3dd0cd3bbfe684a9abb271b690a33838a71"}, "downloads": -1, "filename": "jobspy-0.27.0.tar.gz", "has_sig": false, "md5_digest": "b3b27d42f6634d0264a2844a67fbd37d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20351, "upload_time": "2019-06-04T03:15:29", "upload_time_iso_8601": "2019-06-04T03:15:29.201004Z", "url": "https://files.pythonhosted.org/packages/72/59/71f8ca650a88c0ad357783fc886ab93237a765dd45c3eb358b33139d5103/jobspy-0.27.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b3b27d42f6634d0264a2844a67fbd37d", "sha256": "d87ba0e255be1c0598844baca4f6e3dd0cd3bbfe684a9abb271b690a33838a71"}, "downloads": -1, "filename": "jobspy-0.27.0.tar.gz", "has_sig": false, "md5_digest": "b3b27d42f6634d0264a2844a67fbd37d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20351, "upload_time": "2019-06-04T03:15:29", "upload_time_iso_8601": "2019-06-04T03:15:29.201004Z", "url": "https://files.pythonhosted.org/packages/72/59/71f8ca650a88c0ad357783fc886ab93237a765dd45c3eb358b33139d5103/jobspy-0.27.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:52:18 2020"}