{"info": {"author": "Ivan Itzcovich", "author_email": "i.itzcovich@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Internet :: WWW/HTTP :: HTTP Servers", "Topic :: Internet :: WWW/HTTP :: WSGI", "Topic :: Internet :: WWW/HTTP :: WSGI :: Application", "Topic :: Internet :: WWW/HTTP :: WSGI :: Middleware", "Topic :: Internet :: WWW/HTTP :: WSGI :: Server", "Topic :: Software Development :: Libraries :: Application Frameworks"], "description": "# TFServe\n\n[![Downloads](https://pepy.tech/badge/tfserve)](https://pepy.tech/project/tfserve)  [![PyPI version](https://badge.fury.io/py/tfserve.svg)](https://badge.fury.io/py/tfserve)\n\nTFServe is a framework designed to serve tensorflow models in a simple and easy way as an HTTP API server. It's built on top of [Werkzeug](http://werkzeug.pocoo.org/).\n\n## How to install\n\n```bash\n$ pip install tfserve\n```\n\nAfter installing `tfserve`, install either `tensorflow` of `tensorflow-gpu` (the latter if you have GPU available).\n\n```bash\n$ pip install tensorflow\n```\nor\n```bash\n$ pip install tensorflow-gpu\n```\n\n## How to use\n\n### Python API\n\nYou will need 5 parts:\n\n1. **Model**: it can be a `.pb` file or a model directory containing ckpt files.\n2. **Input tensor names**: name of the input tensors of the graph.\n3. **Output tensor names**: name of the output tensors of the graph.\n4. **`encode`**: python function that receives the request body data and outputs a `dict` mapping input tensor names to input numpy values.\n5. **`decode`**: python function that receives a `dict` mapping output tensor names to output numpy values and returns the HTTP response.\n\nFollow the example to learn how to combine these parts...\n\n#### Example\n\nDeploy image classification service that receives a binary jpg image and returns the class of the object found in the image alongside it's probability.\n\n```python\n\n# 1. Model: trained mobilenet on ImageNet that can be downloaded from\n#           https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz\nMODEL_PATH = \"mobilenet_v2_1.4_224/mobilenet_v2_1.4_224_frozen.pb\"\n\n# 2. Input tensor names:\nINPUT_TENSORS = [\"import/input:0\"]\n\n# 3. Output tensor names:\nOUTPUT_TENSORS = [\"import/MobilenetV2/Predictions/Softmax:0\"]\n\n# 4. encode function: Receives raw jpg image as request data. Returns dict\n#                     mappint import/input:0 to numpy value.\ndef encode(request_data):\n    with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".jpg\") as f:\n        f.write(request_data)\n        # Model receives 224x224 normalized RGB image.\n        img = Image.open(f.name).resize((224, 224)) \n        img = np.asarray(img) / 255.\n\n    return {INPUT_TENSORS[0]: img}\n\n# 5. decode function: Receives `dict` mapping import/MobilenetV2/Predictions/Softmax:0 to\n#                     numpy value and builds dict with for json response.\ndef decode(outputs):\n    p = outputs[OUTPUT_TENSORS[0]] # 1001 vector with probabilities for each class.\n    index = np.argmax(p)\n    return {\"class\": index_to_class_map[index-1], \"prob\": float(p[index])}\n```\n\nThat's it! Now create TFServeApp object and run it!\n\n```python\napp = TFServeApp(MODEL_PATH, INPUT_TENSORS, OUTPUT_TENSORS, encode, decode)\napp.run('127.0.0.1', 5000)  # Host and port where the server will be running\n```\n\nSee `client.py` for full example.\n\n#### How to consume server\n\n![img](imgs/screen.gif)\n\n> The server supports only `POST` method to `/` with the input information as part of the request body.\n\nThe input will be proccessed in the encode function to produce the feed_dict object that will be passed to the graph. The graph output will be processed in the decode function and the server will return whatever the decode function returns.\n\n### CLI\n\n`tfserve` also provides a CLI program with built-in encode/decode handlers:\n\n```bash\ntfserve -m PATH [-i INPUTS] [-o OUTPUTS] [-h HANDLER] [-b] [-H HOST] [-p PORT]\n\n  -m PATH, --model PATH\n                        path to pb file or directory containing checkpoint\n  -i INPUTS, --inputs INPUTS\n                        a comma separated list of input tensors\n  -o OUTPUTS, --outputs OUTPUTS\n                        a comma separated list of output tensors\n  -h HANDLER, --handler HANDLER\n                        encode/decode handler (deault is 'json')\n  -b, --batch           process multiple inputs (default is to process\n                        one input per request)\n  -H HOST, --host HOST  host interface to bind to (0.0.0.0)\n  -p PORT, --port PORT  port to listen on (5000)\n```\n\n#### Example\n\n```bash\n$ tfserve -m models/graph.pb -i x:0 -o out:0 -h json -H localhost\n```\n\nRun `tfserve` with `models/graph.pb` model that takes as input the tensor with name `x:0` (dimesion: [?,5]) and outputs a tensor named `out:0`. The server will run on http://localhost:5000/ and will receive `POST` requests to `/`.\n\nBy using the **json handler**, you can provide the input data as a `json` object in the request body:\n\n```json\n{\n  \"x:0\": [1,1,3,4,5]\n}\n```\n\nYou will receive a `json` output object as:\n\n```json\n{\n  \"out:0\": 0.48\n}\n```\n\n#### More information about CLI\n\nRun:\n\n```bash\n$ tfserve --help\n```\n\n## Help\n\n* **What if I don't know the tensor names?**\n\n> You can use `tfserve.helper.estimate_io_tensors(model_path)` function to get a list of possible input/output tensor names. Also, you can use the CLI by running: `tfserve -m [model_path] --help-model`\n\n* **What if I want to run multiple inferences at the same time?**\n\n> You can use `batch=True` when building tfserve.TFServeApp. You will then need to handle the batch dimension yourself in the `encode` and `decode` function.\n> Also, if using the CLI, just add the `--batch` flag.\n\n\n## Limitation\n\n> It only works with one-to-one models. That is, models that need to run the graph only once to get the inference.\n> Other architectures of inference will be supported soon. Help is appreciated!\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/iitzco/tfserve", "keywords": "tensorflow deep-learning serving", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "tfserve", "package_url": "https://pypi.org/project/tfserve/", "platform": "any", "project_url": "https://pypi.org/project/tfserve/", "project_urls": {"Homepage": "http://github.com/iitzco/tfserve"}, "release_url": "https://pypi.org/project/tfserve/0.3/", "requires_dist": ["numpy", "apistar (==0.5.41)", "nose", "Werkzeug"], "requires_python": ">3.5", "summary": "Serve TF models simple and easy as an HTTP API server.", "version": "0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>TFServe</h1>\n<p><a href=\"https://pepy.tech/project/tfserve\" rel=\"nofollow\"><img alt=\"Downloads\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0db773979b4b7f76ba0882b1afc4087b2cb0e234/68747470733a2f2f706570792e746563682f62616467652f74667365727665\"></a>  <a href=\"https://badge.fury.io/py/tfserve\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9ddd1a4554ebd82617863f14a69e900611cce659/68747470733a2f2f62616467652e667572792e696f2f70792f746673657276652e737667\"></a></p>\n<p>TFServe is a framework designed to serve tensorflow models in a simple and easy way as an HTTP API server. It's built on top of <a href=\"http://werkzeug.pocoo.org/\" rel=\"nofollow\">Werkzeug</a>.</p>\n<h2>How to install</h2>\n<pre>$ pip install tfserve\n</pre>\n<p>After installing <code>tfserve</code>, install either <code>tensorflow</code> of <code>tensorflow-gpu</code> (the latter if you have GPU available).</p>\n<pre>$ pip install tensorflow\n</pre>\n<p>or</p>\n<pre>$ pip install tensorflow-gpu\n</pre>\n<h2>How to use</h2>\n<h3>Python API</h3>\n<p>You will need 5 parts:</p>\n<ol>\n<li><strong>Model</strong>: it can be a <code>.pb</code> file or a model directory containing ckpt files.</li>\n<li><strong>Input tensor names</strong>: name of the input tensors of the graph.</li>\n<li><strong>Output tensor names</strong>: name of the output tensors of the graph.</li>\n<li><strong><code>encode</code></strong>: python function that receives the request body data and outputs a <code>dict</code> mapping input tensor names to input numpy values.</li>\n<li><strong><code>decode</code></strong>: python function that receives a <code>dict</code> mapping output tensor names to output numpy values and returns the HTTP response.</li>\n</ol>\n<p>Follow the example to learn how to combine these parts...</p>\n<h4>Example</h4>\n<p>Deploy image classification service that receives a binary jpg image and returns the class of the object found in the image alongside it's probability.</p>\n<pre><span class=\"c1\"># 1. Model: trained mobilenet on ImageNet that can be downloaded from</span>\n<span class=\"c1\">#           https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz</span>\n<span class=\"n\">MODEL_PATH</span> <span class=\"o\">=</span> <span class=\"s2\">\"mobilenet_v2_1.4_224/mobilenet_v2_1.4_224_frozen.pb\"</span>\n\n<span class=\"c1\"># 2. Input tensor names:</span>\n<span class=\"n\">INPUT_TENSORS</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"import/input:0\"</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># 3. Output tensor names:</span>\n<span class=\"n\">OUTPUT_TENSORS</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"import/MobilenetV2/Predictions/Softmax:0\"</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># 4. encode function: Receives raw jpg image as request data. Returns dict</span>\n<span class=\"c1\">#                     mappint import/input:0 to numpy value.</span>\n<span class=\"k\">def</span> <span class=\"nf\">encode</span><span class=\"p\">(</span><span class=\"n\">request_data</span><span class=\"p\">):</span>\n    <span class=\"k\">with</span> <span class=\"n\">tempfile</span><span class=\"o\">.</span><span class=\"n\">NamedTemporaryFile</span><span class=\"p\">(</span><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s2\">\"wb\"</span><span class=\"p\">,</span> <span class=\"n\">suffix</span><span class=\"o\">=</span><span class=\"s2\">\".jpg\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">request_data</span><span class=\"p\">)</span>\n        <span class=\"c1\"># Model receives 224x224 normalized RGB image.</span>\n        <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">resize</span><span class=\"p\">((</span><span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">))</span> \n        <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">asarray</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">255.</span>\n\n    <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"n\">INPUT_TENSORS</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]:</span> <span class=\"n\">img</span><span class=\"p\">}</span>\n\n<span class=\"c1\"># 5. decode function: Receives `dict` mapping import/MobilenetV2/Predictions/Softmax:0 to</span>\n<span class=\"c1\">#                     numpy value and builds dict with for json response.</span>\n<span class=\"k\">def</span> <span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"n\">outputs</span><span class=\"p\">):</span>\n    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"n\">OUTPUT_TENSORS</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]]</span> <span class=\"c1\"># 1001 vector with probabilities for each class.</span>\n    <span class=\"n\">index</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s2\">\"class\"</span><span class=\"p\">:</span> <span class=\"n\">index_to_class_map</span><span class=\"p\">[</span><span class=\"n\">index</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s2\">\"prob\"</span><span class=\"p\">:</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">[</span><span class=\"n\">index</span><span class=\"p\">])}</span>\n</pre>\n<p>That's it! Now create TFServeApp object and run it!</p>\n<pre><span class=\"n\">app</span> <span class=\"o\">=</span> <span class=\"n\">TFServeApp</span><span class=\"p\">(</span><span class=\"n\">MODEL_PATH</span><span class=\"p\">,</span> <span class=\"n\">INPUT_TENSORS</span><span class=\"p\">,</span> <span class=\"n\">OUTPUT_TENSORS</span><span class=\"p\">,</span> <span class=\"n\">encode</span><span class=\"p\">,</span> <span class=\"n\">decode</span><span class=\"p\">)</span>\n<span class=\"n\">app</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"s1\">'127.0.0.1'</span><span class=\"p\">,</span> <span class=\"mi\">5000</span><span class=\"p\">)</span>  <span class=\"c1\"># Host and port where the server will be running</span>\n</pre>\n<p>See <code>client.py</code> for full example.</p>\n<h4>How to consume server</h4>\n<p><img alt=\"img\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/01d67d9b3abfa039e348288c275300c33ff79ad2/696d67732f73637265656e2e676966\"></p>\n<blockquote>\n<p>The server supports only <code>POST</code> method to <code>/</code> with the input information as part of the request body.</p>\n</blockquote>\n<p>The input will be proccessed in the encode function to produce the feed_dict object that will be passed to the graph. The graph output will be processed in the decode function and the server will return whatever the decode function returns.</p>\n<h3>CLI</h3>\n<p><code>tfserve</code> also provides a CLI program with built-in encode/decode handlers:</p>\n<pre>tfserve -m PATH <span class=\"o\">[</span>-i INPUTS<span class=\"o\">]</span> <span class=\"o\">[</span>-o OUTPUTS<span class=\"o\">]</span> <span class=\"o\">[</span>-h HANDLER<span class=\"o\">]</span> <span class=\"o\">[</span>-b<span class=\"o\">]</span> <span class=\"o\">[</span>-H HOST<span class=\"o\">]</span> <span class=\"o\">[</span>-p PORT<span class=\"o\">]</span>\n\n  -m PATH, --model PATH\n                        path to pb file or directory containing checkpoint\n  -i INPUTS, --inputs INPUTS\n                        a comma separated list of input tensors\n  -o OUTPUTS, --outputs OUTPUTS\n                        a comma separated list of output tensors\n  -h HANDLER, --handler HANDLER\n                        encode/decode handler <span class=\"o\">(</span>deault is <span class=\"s1\">'json'</span><span class=\"o\">)</span>\n  -b, --batch           process multiple inputs <span class=\"o\">(</span>default is to process\n                        one input per request<span class=\"o\">)</span>\n  -H HOST, --host HOST  host interface to <span class=\"nb\">bind</span> to <span class=\"o\">(</span><span class=\"m\">0</span>.0.0.0<span class=\"o\">)</span>\n  -p PORT, --port PORT  port to listen on <span class=\"o\">(</span><span class=\"m\">5000</span><span class=\"o\">)</span>\n</pre>\n<h4>Example</h4>\n<pre>$ tfserve -m models/graph.pb -i x:0 -o out:0 -h json -H localhost\n</pre>\n<p>Run <code>tfserve</code> with <code>models/graph.pb</code> model that takes as input the tensor with name <code>x:0</code> (dimesion: [?,5]) and outputs a tensor named <code>out:0</code>. The server will run on <a href=\"http://localhost:5000/\" rel=\"nofollow\">http://localhost:5000/</a> and will receive <code>POST</code> requests to <code>/</code>.</p>\n<p>By using the <strong>json handler</strong>, you can provide the input data as a <code>json</code> object in the request body:</p>\n<pre><span class=\"p\">{</span>\n  <span class=\"nt\">\"x:0\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</pre>\n<p>You will receive a <code>json</code> output object as:</p>\n<pre><span class=\"p\">{</span>\n  <span class=\"nt\">\"out:0\"</span><span class=\"p\">:</span> <span class=\"mf\">0.48</span>\n<span class=\"p\">}</span>\n</pre>\n<h4>More information about CLI</h4>\n<p>Run:</p>\n<pre>$ tfserve --help\n</pre>\n<h2>Help</h2>\n<ul>\n<li><strong>What if I don't know the tensor names?</strong></li>\n</ul>\n<blockquote>\n<p>You can use <code>tfserve.helper.estimate_io_tensors(model_path)</code> function to get a list of possible input/output tensor names. Also, you can use the CLI by running: <code>tfserve -m [model_path] --help-model</code></p>\n</blockquote>\n<ul>\n<li><strong>What if I want to run multiple inferences at the same time?</strong></li>\n</ul>\n<blockquote>\n<p>You can use <code>batch=True</code> when building tfserve.TFServeApp. You will then need to handle the batch dimension yourself in the <code>encode</code> and <code>decode</code> function.\nAlso, if using the CLI, just add the <code>--batch</code> flag.</p>\n</blockquote>\n<h2>Limitation</h2>\n<blockquote>\n<p>It only works with one-to-one models. That is, models that need to run the graph only once to get the inference.\nOther architectures of inference will be supported soon. Help is appreciated!</p>\n</blockquote>\n\n          </div>"}, "last_serial": 4428167, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "b729f66472df7e03d573aad106ec7c13", "sha256": "b803a13a45f4487ec9a61973aa998f452e4f595597f02b20748471aeef327103"}, "downloads": -1, "filename": "tfserve-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "b729f66472df7e03d573aad106ec7c13", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">3.5", "size": 7514, "upload_time": "2018-09-05T16:34:48", "upload_time_iso_8601": "2018-09-05T16:34:48.453046Z", "url": "https://files.pythonhosted.org/packages/ba/38/ecae7921edb4e72a7dfe12b9ab570d4faf100e3c4f96fb2f5f6406c9632f/tfserve-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b700daf4eb2d6cf52e50ade5a504016c", "sha256": "e7e8f207bb19a4bd6eeb4a5e04cc7fa2c32f1d69084c777dc2ff75b9a6c01c98"}, "downloads": -1, "filename": "tfserve-0.1.0.tar.gz", "has_sig": false, "md5_digest": "b700daf4eb2d6cf52e50ade5a504016c", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.5", "size": 11561, "upload_time": "2018-09-05T16:34:50", "upload_time_iso_8601": "2018-09-05T16:34:50.305769Z", "url": "https://files.pythonhosted.org/packages/17/e7/242bc4ba9e3e81ee22e22022ee173a91f384d914f42f6bdd55d5d84ffcef/tfserve-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "1dac5fdda860b9f8d5d7e477b8f111f2", "sha256": "bcefd145c6d6197979b3b1c6d883ee96be1498a9b41e8ef12916603acf27df14"}, "downloads": -1, "filename": "tfserve-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "1dac5fdda860b9f8d5d7e477b8f111f2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">3.5", "size": 7519, "upload_time": "2018-10-10T21:43:07", "upload_time_iso_8601": "2018-10-10T21:43:07.894102Z", "url": "https://files.pythonhosted.org/packages/43/43/5a48cddbdbca7fcbdcc800872b639bca527c062de150b9a6fd149c759b26/tfserve-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fef8a76a3858bc6aed6f52724e6d536c", "sha256": "842439d0d9f32bf1c937855f0b9587eadda740424d184499fec8b151dedeb219"}, "downloads": -1, "filename": "tfserve-0.1.1.tar.gz", "has_sig": false, "md5_digest": "fef8a76a3858bc6aed6f52724e6d536c", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.5", "size": 11564, "upload_time": "2018-10-10T21:43:10", "upload_time_iso_8601": "2018-10-10T21:43:10.710714Z", "url": "https://files.pythonhosted.org/packages/e0/dc/a0f61e63436fe75b83ae0f1483f58ca23c9712109790435e89fd1aee718c/tfserve-0.1.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "db0e4affc4af5dc55fb4cccf6f10e566", "sha256": "07b9f8949a5ebc41435844c8492ee3a4be7bd14435b7a815f338d7d5289dfb19"}, "downloads": -1, "filename": "tfserve-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "db0e4affc4af5dc55fb4cccf6f10e566", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">3.5", "size": 8788, "upload_time": "2018-10-16T14:12:40", "upload_time_iso_8601": "2018-10-16T14:12:40.799840Z", "url": "https://files.pythonhosted.org/packages/46/70/d7411274efa9b1665b0c2129e0f541afd947ef8438226350bcea87d53683/tfserve-0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "da96bad9259dc58ed9a3519a3c810aec", "sha256": "57465ab7202b35934ca085a154c6b7433025b7bb07efbe3940e660213566390a"}, "downloads": -1, "filename": "tfserve-0.2.tar.gz", "has_sig": false, "md5_digest": "da96bad9259dc58ed9a3519a3c810aec", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.5", "size": 12767, "upload_time": "2018-10-16T14:12:42", "upload_time_iso_8601": "2018-10-16T14:12:42.456113Z", "url": "https://files.pythonhosted.org/packages/4d/a8/f641c4ef8006aec1b14122d28e26d245a1df2a5f9d457cde06d36fe88500/tfserve-0.2.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "007dd19bd2f430a04698689983b65908", "sha256": "0297529a52ec3e062ad90c7466b981cb79e4bb2a40d8f846bb6dcbbdd0195bec"}, "downloads": -1, "filename": "tfserve-0.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "007dd19bd2f430a04698689983b65908", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">3.5", "size": 8766, "upload_time": "2018-10-16T14:21:34", "upload_time_iso_8601": "2018-10-16T14:21:34.089077Z", "url": "https://files.pythonhosted.org/packages/27/10/068d8a85e86e6b10529174626a35a0780ce39a2f2d94cdf130afb7f8995c/tfserve-0.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "aebd7e96924c7a1a120c80e3c0c55bd8", "sha256": "e7e862f6e42596df5710404fd7d6733787afa601862616d00b562cdc042376d0"}, "downloads": -1, "filename": "tfserve-0.2.1.tar.gz", "has_sig": false, "md5_digest": "aebd7e96924c7a1a120c80e3c0c55bd8", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.5", "size": 12844, "upload_time": "2018-10-16T14:21:35", "upload_time_iso_8601": "2018-10-16T14:21:35.672194Z", "url": "https://files.pythonhosted.org/packages/9e/5a/6e7fc35935f86ed47ebab1586fb9b40076ca9cb97571ee1ac8af0c5a7ebf/tfserve-0.2.1.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "309ad43bdfafc86f46eafe277777ecc4", "sha256": "201a091a80722aac5bfa7d7842e794c260f9aee84bf127cffadbc3b9bd845f31"}, "downloads": -1, "filename": "tfserve-0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "309ad43bdfafc86f46eafe277777ecc4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">3.5", "size": 13333, "upload_time": "2018-10-29T16:06:04", "upload_time_iso_8601": "2018-10-29T16:06:04.987896Z", "url": "https://files.pythonhosted.org/packages/47/91/f6b59699d0d18424df8926197544e1143f791e8b19c5883f138c8eaa604a/tfserve-0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "52139a92094105929eb40b7b26e2e541", "sha256": "11dbda4303a3e8955332d4b60aa81c8a8f77428abee2e4170838d1e4f687fdf8"}, "downloads": -1, "filename": "tfserve-0.3.tar.gz", "has_sig": false, "md5_digest": "52139a92094105929eb40b7b26e2e541", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.5", "size": 16922, "upload_time": "2018-10-29T16:06:08", "upload_time_iso_8601": "2018-10-29T16:06:08.207524Z", "url": "https://files.pythonhosted.org/packages/cd/ec/630490066f3a56d1c5be7a963e3bed829f4bd8e0df507bacb729eef6655a/tfserve-0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "309ad43bdfafc86f46eafe277777ecc4", "sha256": "201a091a80722aac5bfa7d7842e794c260f9aee84bf127cffadbc3b9bd845f31"}, "downloads": -1, "filename": "tfserve-0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "309ad43bdfafc86f46eafe277777ecc4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">3.5", "size": 13333, "upload_time": "2018-10-29T16:06:04", "upload_time_iso_8601": "2018-10-29T16:06:04.987896Z", "url": "https://files.pythonhosted.org/packages/47/91/f6b59699d0d18424df8926197544e1143f791e8b19c5883f138c8eaa604a/tfserve-0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "52139a92094105929eb40b7b26e2e541", "sha256": "11dbda4303a3e8955332d4b60aa81c8a8f77428abee2e4170838d1e4f687fdf8"}, "downloads": -1, "filename": "tfserve-0.3.tar.gz", "has_sig": false, "md5_digest": "52139a92094105929eb40b7b26e2e541", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.5", "size": 16922, "upload_time": "2018-10-29T16:06:08", "upload_time_iso_8601": "2018-10-29T16:06:08.207524Z", "url": "https://files.pythonhosted.org/packages/cd/ec/630490066f3a56d1c5be7a963e3bed829f4bd8e0df507bacb729eef6655a/tfserve-0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:54:22 2020"}