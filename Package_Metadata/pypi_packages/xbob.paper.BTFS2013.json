{"info": {"author": "Manuel Guenther", "author_email": "manuel.guenther@idiap.ch", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Science/Research", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Natural Language :: English", "Programming Language :: Python", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "On the Improvements of Uni-modal and Bi-modal Fusions of Speaker and Face Recognition for Mobile Biometrics\n===========================================================================================================\n\nThis package is able to re-generate the plots and figures that are published in the paper \"On the Improvements of Uni-modal and Bi-modal Fusions of Speaker and Face Recognition for Mobile Biometrics\".\nYou can download the Paper_ from our publication server.\n\nThe results of the paper are based on the score files that were submitted to the 2013 `Speaker Recognition <http://www.beat-eu.org/evaluations/icb-2013-speaker-recognition-mobio>`_ and `Face Recognition <http://www.beat-eu.org/evaluations/icb-2013-face-recognition-mobio>`_ evaluations on the MOBIO_ database.\nWith the generous permission of the participants, we re-arranged and unified their score files and made them publicly available.\nUsing these score files, in our paper we show that the uni-modal fusion of scores from several face or speaker recognition algorithms can moderately improve verification performance, whereas the bi-modal fusion of face and speaker recognition systems enormously improves performance.\n\nWith this package we provide the source code to re-compute and re-evaluate the score fusions that were generated for the paper.\nAlso, code for re-producing the Figures and Tables in the paper is provided.\n\n\nErrata\n------\nThe DET plots in the published Paper were plotted with exchanged FAR and FRR axes.\nThe plots have been corrected in the Paper_, and in version 1.0.1 of this package.\n\n\nInstallation\n------------\nTo be able to re-run the experiments of the paper, two external sources are required:\n\nBob\n...\nAll software given in this paper relies on the free signal-processing and machine learning toolbox Bob_ originally developed by the Biometrics Group at Idiap_.\nPlease visit Bob_'s web page for installation instructions.\nThis package is tested with Bob in version 1.2.0, but in principle it should also work with later versions.\n\nBe aware that currently there is no MS Windows port for Bob_.\nIf you are using Windows, please consider installing `VirtualBox <http://www.virtualbox.org>`_ and download our pre-packaged `Ubuntu VDI <http://www.idiap.ch/software/bob/images/xubuntu_raring_i386--with_bob_1.2.0.ova>`_ (right click, save target as), which has the size of 1.1 GB.\n\nMOBIO\n.....\nThe score files from the participants of the competitions are stored in the MOBIO_ database.\nTo be granted access to the database, you need to sign the license agreement.\nAfterwards, please download the file **ICB_2013_system_scores.tar.gz** and extract it to a directory of your choice.\nThis directory will be called ``[SCORE_DIRECTORY]`` in the following.\n\nThis package\n............\nFinally, this package can be easily downloaded and extracted using the download link on the PyPI_ page.\nAfterwards, please go to the command line, change to the extracted directory and call::\n\n  $ python bootstrap.py\n  $ bin/buildout\n\nThis sequence will automatically download detect the installation directory of Bob_ and locally download any software that is not installed globally on your system.\nAlso, it will create a **bin** directory that will contain all executable scripts that will allow you to re-run the experiments, and which are explained in more detail below.\n\n.. note::\n  When you have installed Bob_ in a non-standard directory, please open the **buildout.cfg** file in the root directory of the package and change the ``prefixes`` accordingly.\n\n\nRunning the experiments\n-----------------------\nThe set of experiments are divided into four groups.\nFor each group of experiments, one script in the **bin** directory exists.\nMost of these scripts have a common set of command line options, which have a long (starting with ``--``) and a short (starting with a single ``-``) name.\nHere, we will only introduce the long one, please use the ``--help`` option to see the shortcuts:\n\n* ``--data-directory`` (required): The ``[SCORE_DIRECTORY]`` where the original score files can be found.\n* ``--modality``: The modalities that should be evaluated, i.e., ``face``, ``speaker`` or both (the default).\n* ``--protocol``: The protocol of the MOBIO database that should be evaluated, i.e., ``male``, ``female`` or both (the default).\n* ``--verbose``: The verbosity level of the script. Use this option several times to increase the verbosity from 0 (ERROR) to 1 (WARNING), 2 (INFO) and 3 (DEBUG). A verbosity level of 2 (e.g. using ``-vv``) is recommended.\n* ``--fused-directory``: The directory in which the fused score files will be written (not available in all scripts). The default values will fit in most cases.\n* ``--force``: By default, already computed fused score files will not be re-generated if they already exist. Use the ``--force`` option to always generate the fused score files (not available in all scripts).\n\nEvaluation of raw score files\n.............................\nThe first group of experiments will simply re-evaluate the original score files and re-produce Table 2 of the Paper_.\nSimply call::\n\n  $ bin/single.py --data-directory [SCORE_DIRECTORY] --latex-directory [LATEX] -vv\n\nwhere ``[LATEX]`` is a directory of your choice, which will be automatically created if it does not exist yet.\nThis experiment will create two LaTeX-compatible files **[LATEX]/single_face.tex** and **[LATEX]/single_speaker.tex** containing the system shortcut, the long system name, the EER (male), the HTER (male), the EER (female) and the HTER (female).\nThese files can be imported into LaTeX tables, e.g., by defining ``\\newcommand{\\Result}[6]{#1 & #3 & #4 & #5 & #6\\\\}`` outside and calling ``\\input{[LATEX]/single_face}`` inside a ``tabular`` with five columns.\n\n\nUni-modal fusion\n................\nThe second group of experiments perform a uni-modal fusion of face or speaker recognition systems.\nIt iterates through the algorithms and fuses the ``N`` best algorithms for each modality, with ``N`` varying from 1 to 9 (face) and 12 (speaker).\nFinally, the Table 3 of the Paper_ is regenerated by::\n\n  $ bin/unimodal.py --data-directory [SCORE_DIRECTORY] --latex-directory [LATEX] -vv\n\nAgain, the generated LaTeX-compatible files **[LATEX]/fusion_face.tex** and **[LATEX]/fusion_speaker.tex** can be imported to LaTeX tables similarly as described above.\n\n.. note::\n  The script above might run for several minutes to compute all uni-modal fusions.\n\nBi-modal fusion\n...............\nThe third experiment will perform a bi-modal fusion in a more greedy way.\nIt loads the score files for all face and all speaker recognition systems and searches for the best combination of fusing algorithms.\nStarting with the best system, which is ``F-1`` in both the male and the female protocol, it searches for the algorithm that decreases the EER best and add that to the set of fused systems.\nOne after another, all systems will be added, and finally the Figure 3 is generated as a multi-page PDF file ``[BI_MODAL_PDF_FILE]`` by the call::\n\n  $ bin/bimodal.py --data-directory [SCORE_DIRECTORY] --plot-file [BI_MODAL_PDF_FILE] -vv\n\n.. warning::\n  Due to the nature of the experiment, the execution of this script might take several hours (even days) to be computed.\n\nComparison plots\n................\nThe final script will generate the remaining Figures 1 and 2 of the Paper_, together in one PDF file ``[COMPARISON_PDF_FILE]``.\nThis script can only be run after the uni-modal and bi-modal fusion scrips ``bin/unimodal.py`` and ``bin/bimodal.py`` have finished.\nPlease call::\n\n  $ bin/plots.py --data-directory [SCORE_DIRECTORY] --plot-file [COMPARISON_PDF_FILE]\n\nIf you have specified other ``--fused-directory`` parameters in the calls to ``bin/unimodal.py`` or ``bin/bimodal.py``, please set the ``--unimodal-directory`` or ``--bimodal-directory`` option accordingly.\n\n\nGetting Help\n------------\nIn case you need help or you found some mistake, please feel free to file a bug report under http://gitlab.idiap.ch/manuel.guenther/xbob-paper-btfs2013 or send an `email <mailto:manuel.guenther@idiap.ch>`_.\n\nHelping us\n----------\nIn case you liked Bob_, our Paper_ and/or our package, please cite::\n\n  @INPROCEEDINGS{Khoury_BTFS_2013,\n           author = {Khoury, Elie and G{\\\"{u}}nther, Manuel and El Shafey, Laurent and Marcel, S{\\'{e}}bastien},\n            month = oct,\n            title = {On the Improvements of Uni-modal and Bi-modal Fusions of Speaker and Face Recognition for Mobile Biometrics},\n        booktitle = {Biometric Technologies in Forensic Science},\n             year = {2013},\n         location = {Nijmegen, The Netherlands},\n         abstract = {The MOBIO database provides a challenging test-bed for speaker and face recognition systems because it includes voice and face samples as they would appear in forensic scenarios.\n                    In this paper, we investigate uni-modal and bi-modal multi-algorithm fusion using logistic regression.\n                    The source speaker and face recognition systems were taken from the 2013 speaker and face recognition evaluations that were held in the context of the last International Conference on Biometrics (ICB-2013).\n                    Using the unbiased MOBIO protocols, the employed evaluation measures are the equal error rate (EER), the half-total error rate (HTER) and the detection error trade-off (DET).\n                    The results show that by uni-modal algorithm fusion, the HTER's of the speaker recognition system are reduced by around 35\\%, and of the face recognition system by between 15\\% and 20\\%.\n                    Bi-modal fusion drastically boosts recognition by a relative gain of 65\\% - 70\\% of performance compared to the best uni-modal system.},\n              pdf = {http://publications.idiap.ch/downloads/papers/2013/Khoury_BTFS_2013.pdf}\n  }\n\nTODO::\n\n  Improve the BibTeX entry as soon as the paper is published.\n\n\n.. _paper : http://publications.idiap.ch/index.php/publications/show/2688\n.. _bob : http://www.idiap.ch/software/bob\n.. _idiap : http://www.idiap.ch\n.. _mobio : http://www.idiap.ch/dataset/mobio\n.. _pypi : http://pypi.python.org/pypi/xbob.paper.BTFS2013", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://gitlab.idiap.ch/manuel.guenther/xbob-paper-BTFS2013", "keywords": "face recognition,speaker recognition,fusion,uni-modal,bi-modal,bob,xbob,MOBIO", "license": "GPLv3", "maintainer": null, "maintainer_email": null, "name": "xbob.paper.BTFS2013", "package_url": "https://pypi.org/project/xbob.paper.BTFS2013/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/xbob.paper.BTFS2013/", "project_urls": {"Download": "UNKNOWN", "Homepage": "http://gitlab.idiap.ch/manuel.guenther/xbob-paper-BTFS2013"}, "release_url": "https://pypi.org/project/xbob.paper.BTFS2013/1.0.1/", "requires_dist": null, "requires_python": null, "summary": "On the Improvements of Uni-modal and Bi-modal Fusions of Speaker and Face Recognition for Mobile Biometrics", "version": "1.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>This package is able to re-generate the plots and figures that are published in the paper \u201cOn the Improvements of Uni-modal and Bi-modal Fusions of Speaker and Face Recognition for Mobile Biometrics\u201d.\nYou can download the <a href=\"http://publications.idiap.ch/index.php/publications/show/2688\" rel=\"nofollow\">Paper</a> from our publication server.</p>\n<p>The results of the paper are based on the score files that were submitted to the 2013 <a href=\"http://www.beat-eu.org/evaluations/icb-2013-speaker-recognition-mobio\" rel=\"nofollow\">Speaker Recognition</a> and <a href=\"http://www.beat-eu.org/evaluations/icb-2013-face-recognition-mobio\" rel=\"nofollow\">Face Recognition</a> evaluations on the <a href=\"http://www.idiap.ch/dataset/mobio\" rel=\"nofollow\">MOBIO</a> database.\nWith the generous permission of the participants, we re-arranged and unified their score files and made them publicly available.\nUsing these score files, in our paper we show that the uni-modal fusion of scores from several face or speaker recognition algorithms can moderately improve verification performance, whereas the bi-modal fusion of face and speaker recognition systems enormously improves performance.</p>\n<p>With this package we provide the source code to re-compute and re-evaluate the score fusions that were generated for the paper.\nAlso, code for re-producing the Figures and Tables in the paper is provided.</p>\n<div id=\"errata\">\n<h2>Errata</h2>\n<p>The DET plots in the published Paper were plotted with exchanged FAR and FRR axes.\nThe plots have been corrected in the <a href=\"http://publications.idiap.ch/index.php/publications/show/2688\" rel=\"nofollow\">Paper</a>, and in version 1.0.1 of this package.</p>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>To be able to re-run the experiments of the paper, two external sources are required:</p>\n<div id=\"bob\">\n<h3>Bob</h3>\n<p>All software given in this paper relies on the free signal-processing and machine learning toolbox <a href=\"http://www.idiap.ch/software/bob\" rel=\"nofollow\">Bob</a> originally developed by the Biometrics Group at <a href=\"http://www.idiap.ch\" rel=\"nofollow\">Idiap</a>.\nPlease visit <a href=\"http://www.idiap.ch/software/bob\" rel=\"nofollow\">Bob</a>\u2019s web page for installation instructions.\nThis package is tested with Bob in version 1.2.0, but in principle it should also work with later versions.</p>\n<p>Be aware that currently there is no MS Windows port for <a href=\"http://www.idiap.ch/software/bob\" rel=\"nofollow\">Bob</a>.\nIf you are using Windows, please consider installing <a href=\"http://www.virtualbox.org\" rel=\"nofollow\">VirtualBox</a> and download our pre-packaged <a href=\"http://www.idiap.ch/software/bob/images/xubuntu_raring_i386--with_bob_1.2.0.ova\" rel=\"nofollow\">Ubuntu VDI</a> (right click, save target as), which has the size of 1.1 GB.</p>\n</div>\n<div id=\"mobio\">\n<h3>MOBIO</h3>\n<p>The score files from the participants of the competitions are stored in the <a href=\"http://www.idiap.ch/dataset/mobio\" rel=\"nofollow\">MOBIO</a> database.\nTo be granted access to the database, you need to sign the license agreement.\nAfterwards, please download the file <strong>ICB_2013_system_scores.tar.gz</strong> and extract it to a directory of your choice.\nThis directory will be called <tt>[SCORE_DIRECTORY]</tt> in the following.</p>\n</div>\n<div id=\"this-package\">\n<h3>This package</h3>\n<p>Finally, this package can be easily downloaded and extracted using the download link on the <a href=\"http://pypi.python.org/pypi/xbob.paper.BTFS2013\" rel=\"nofollow\">PyPI</a> page.\nAfterwards, please go to the command line, change to the extracted directory and call:</p>\n<pre>$ python bootstrap.py\n$ bin/buildout\n</pre>\n<p>This sequence will automatically download detect the installation directory of <a href=\"http://www.idiap.ch/software/bob\" rel=\"nofollow\">Bob</a> and locally download any software that is not installed globally on your system.\nAlso, it will create a <strong>bin</strong> directory that will contain all executable scripts that will allow you to re-run the experiments, and which are explained in more detail below.</p>\n<div>\n<p>Note</p>\n<p>When you have installed <a href=\"http://www.idiap.ch/software/bob\" rel=\"nofollow\">Bob</a> in a non-standard directory, please open the <strong>buildout.cfg</strong> file in the root directory of the package and change the <tt>prefixes</tt> accordingly.</p>\n</div>\n</div>\n</div>\n<div id=\"running-the-experiments\">\n<h2>Running the experiments</h2>\n<p>The set of experiments are divided into four groups.\nFor each group of experiments, one script in the <strong>bin</strong> directory exists.\nMost of these scripts have a common set of command line options, which have a long (starting with <tt><span class=\"pre\">--</span></tt>) and a short (starting with a single <tt>-</tt>) name.\nHere, we will only introduce the long one, please use the <tt><span class=\"pre\">--help</span></tt> option to see the shortcuts:</p>\n<ul>\n<li><tt><span class=\"pre\">--data-directory</span></tt> (required): The <tt>[SCORE_DIRECTORY]</tt> where the original score files can be found.</li>\n<li><tt><span class=\"pre\">--modality</span></tt>: The modalities that should be evaluated, i.e., <tt>face</tt>, <tt>speaker</tt> or both (the default).</li>\n<li><tt><span class=\"pre\">--protocol</span></tt>: The protocol of the MOBIO database that should be evaluated, i.e., <tt>male</tt>, <tt>female</tt> or both (the default).</li>\n<li><tt><span class=\"pre\">--verbose</span></tt>: The verbosity level of the script. Use this option several times to increase the verbosity from 0 (ERROR) to 1 (WARNING), 2 (INFO) and 3 (DEBUG). A verbosity level of 2 (e.g. using <tt><span class=\"pre\">-vv</span></tt>) is recommended.</li>\n<li><tt><span class=\"pre\">--fused-directory</span></tt>: The directory in which the fused score files will be written (not available in all scripts). The default values will fit in most cases.</li>\n<li><tt><span class=\"pre\">--force</span></tt>: By default, already computed fused score files will not be re-generated if they already exist. Use the <tt><span class=\"pre\">--force</span></tt> option to always generate the fused score files (not available in all scripts).</li>\n</ul>\n<div id=\"evaluation-of-raw-score-files\">\n<h3>Evaluation of raw score files</h3>\n<p>The first group of experiments will simply re-evaluate the original score files and re-produce Table 2 of the <a href=\"http://publications.idiap.ch/index.php/publications/show/2688\" rel=\"nofollow\">Paper</a>.\nSimply call:</p>\n<pre>$ bin/single.py --data-directory [SCORE_DIRECTORY] --latex-directory [LATEX] -vv\n</pre>\n<p>where <tt>[LATEX]</tt> is a directory of your choice, which will be automatically created if it does not exist yet.\nThis experiment will create two LaTeX-compatible files <strong>[LATEX]/single_face.tex</strong> and <strong>[LATEX]/single_speaker.tex</strong> containing the system shortcut, the long system name, the EER (male), the HTER (male), the EER (female) and the HTER (female).\nThese files can be imported into LaTeX tables, e.g., by defining <tt><span class=\"pre\">\\newcommand{\\Result}[6]{#1</span> &amp; #3 &amp; #4 &amp; #5 &amp; <span class=\"pre\">#6\\\\}</span></tt> outside and calling <tt><span class=\"pre\">\\input{[LATEX]/single_face}</span></tt> inside a <tt>tabular</tt> with five columns.</p>\n</div>\n<div id=\"uni-modal-fusion\">\n<h3>Uni-modal fusion</h3>\n<p>The second group of experiments perform a uni-modal fusion of face or speaker recognition systems.\nIt iterates through the algorithms and fuses the <tt>N</tt> best algorithms for each modality, with <tt>N</tt> varying from 1 to 9 (face) and 12 (speaker).\nFinally, the Table 3 of the <a href=\"http://publications.idiap.ch/index.php/publications/show/2688\" rel=\"nofollow\">Paper</a> is regenerated by:</p>\n<pre>$ bin/unimodal.py --data-directory [SCORE_DIRECTORY] --latex-directory [LATEX] -vv\n</pre>\n<p>Again, the generated LaTeX-compatible files <strong>[LATEX]/fusion_face.tex</strong> and <strong>[LATEX]/fusion_speaker.tex</strong> can be imported to LaTeX tables similarly as described above.</p>\n<div>\n<p>Note</p>\n<p>The script above might run for several minutes to compute all uni-modal fusions.</p>\n</div>\n</div>\n<div id=\"bi-modal-fusion\">\n<h3>Bi-modal fusion</h3>\n<p>The third experiment will perform a bi-modal fusion in a more greedy way.\nIt loads the score files for all face and all speaker recognition systems and searches for the best combination of fusing algorithms.\nStarting with the best system, which is <tt><span class=\"pre\">F-1</span></tt> in both the male and the female protocol, it searches for the algorithm that decreases the EER best and add that to the set of fused systems.\nOne after another, all systems will be added, and finally the Figure 3 is generated as a multi-page PDF file <tt>[BI_MODAL_PDF_FILE]</tt> by the call:</p>\n<pre>$ bin/bimodal.py --data-directory [SCORE_DIRECTORY] --plot-file [BI_MODAL_PDF_FILE] -vv\n</pre>\n<div>\n<p>Warning</p>\n<p>Due to the nature of the experiment, the execution of this script might take several hours (even days) to be computed.</p>\n</div>\n</div>\n<div id=\"comparison-plots\">\n<h3>Comparison plots</h3>\n<p>The final script will generate the remaining Figures 1 and 2 of the <a href=\"http://publications.idiap.ch/index.php/publications/show/2688\" rel=\"nofollow\">Paper</a>, together in one PDF file <tt>[COMPARISON_PDF_FILE]</tt>.\nThis script can only be run after the uni-modal and bi-modal fusion scrips <tt>bin/unimodal.py</tt> and <tt>bin/bimodal.py</tt> have finished.\nPlease call:</p>\n<pre>$ bin/plots.py --data-directory [SCORE_DIRECTORY] --plot-file [COMPARISON_PDF_FILE]\n</pre>\n<p>If you have specified other <tt><span class=\"pre\">--fused-directory</span></tt> parameters in the calls to <tt>bin/unimodal.py</tt> or <tt>bin/bimodal.py</tt>, please set the <tt><span class=\"pre\">--unimodal-directory</span></tt> or <tt><span class=\"pre\">--bimodal-directory</span></tt> option accordingly.</p>\n</div>\n</div>\n<div id=\"getting-help\">\n<h2>Getting Help</h2>\n<p>In case you need help or you found some mistake, please feel free to file a bug report under <a href=\"http://gitlab.idiap.ch/manuel.guenther/xbob-paper-btfs2013\" rel=\"nofollow\">http://gitlab.idiap.ch/manuel.guenther/xbob-paper-btfs2013</a> or send an <a href=\"mailto:manuel.guenther%40idiap.ch\">email</a>.</p>\n</div>\n<div id=\"helping-us\">\n<h2>Helping us</h2>\n<p>In case you liked <a href=\"http://www.idiap.ch/software/bob\" rel=\"nofollow\">Bob</a>, our <a href=\"http://publications.idiap.ch/index.php/publications/show/2688\" rel=\"nofollow\">Paper</a> and/or our package, please cite:</p>\n<pre>@INPROCEEDINGS{Khoury_BTFS_2013,\n         author = {Khoury, Elie and G{\\\"{u}}nther, Manuel and El Shafey, Laurent and Marcel, S{\\'{e}}bastien},\n          month = oct,\n          title = {On the Improvements of Uni-modal and Bi-modal Fusions of Speaker and Face Recognition for Mobile Biometrics},\n      booktitle = {Biometric Technologies in Forensic Science},\n           year = {2013},\n       location = {Nijmegen, The Netherlands},\n       abstract = {The MOBIO database provides a challenging test-bed for speaker and face recognition systems because it includes voice and face samples as they would appear in forensic scenarios.\n                  In this paper, we investigate uni-modal and bi-modal multi-algorithm fusion using logistic regression.\n                  The source speaker and face recognition systems were taken from the 2013 speaker and face recognition evaluations that were held in the context of the last International Conference on Biometrics (ICB-2013).\n                  Using the unbiased MOBIO protocols, the employed evaluation measures are the equal error rate (EER), the half-total error rate (HTER) and the detection error trade-off (DET).\n                  The results show that by uni-modal algorithm fusion, the HTER's of the speaker recognition system are reduced by around 35\\%, and of the face recognition system by between 15\\% and 20\\%.\n                  Bi-modal fusion drastically boosts recognition by a relative gain of 65\\% - 70\\% of performance compared to the best uni-modal system.},\n            pdf = {http://publications.idiap.ch/downloads/papers/2013/Khoury_BTFS_2013.pdf}\n}\n</pre>\n<p>TODO:</p>\n<pre>Improve the BibTeX entry as soon as the paper is published.\n</pre>\n</div>\n\n          </div>"}, "last_serial": 937562, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "83dc6e88818053a919559e0c2251b1e3", "sha256": "0acea96b912fae34614721582f1e622278461b1ccaea3ab7858734723604ffa2"}, "downloads": -1, "filename": "xbob.paper.BTFS2013-1.0.0.zip", "has_sig": false, "md5_digest": "83dc6e88818053a919559e0c2251b1e3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 47498, "upload_time": "2013-10-07T15:22:29", "upload_time_iso_8601": "2013-10-07T15:22:29.053255Z", "url": "https://files.pythonhosted.org/packages/08/74/93dc5034609c522742d7ce93ec725e803c54746b540a10fa8dc69a52fbcb/xbob.paper.BTFS2013-1.0.0.zip", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "5e1305db906921e8748e08e5e3c6800f", "sha256": "32241bd6f65d3944f0424d9e1f58f9646421738b4cb631c4583805e3f436d090"}, "downloads": -1, "filename": "xbob.paper.BTFS2013-1.0.1.zip", "has_sig": false, "md5_digest": "5e1305db906921e8748e08e5e3c6800f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 47728, "upload_time": "2013-12-06T13:04:02", "upload_time_iso_8601": "2013-12-06T13:04:02.676931Z", "url": "https://files.pythonhosted.org/packages/b5/e4/ade18f7339e05fb0f3e5646024f15f090b23fc9e84812583502bd1789d9e/xbob.paper.BTFS2013-1.0.1.zip", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "5e1305db906921e8748e08e5e3c6800f", "sha256": "32241bd6f65d3944f0424d9e1f58f9646421738b4cb631c4583805e3f436d090"}, "downloads": -1, "filename": "xbob.paper.BTFS2013-1.0.1.zip", "has_sig": false, "md5_digest": "5e1305db906921e8748e08e5e3c6800f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 47728, "upload_time": "2013-12-06T13:04:02", "upload_time_iso_8601": "2013-12-06T13:04:02.676931Z", "url": "https://files.pythonhosted.org/packages/b5/e4/ade18f7339e05fb0f3e5646024f15f090b23fc9e84812583502bd1789d9e/xbob.paper.BTFS2013-1.0.1.zip", "yanked": false}], "timestamp": "Fri May  8 03:26:10 2020"}