{"info": {"author": "Guilherme Routar", "author_email": "groutar@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Twikenizer\n\nThis repository hosts the code for a tokenizer of tweets. It's main purpose is to identify subtle profanity, so it should\nobtain better performance on data containing hidden profanity (e.g. 'f*ck').\n\nDisclaimer: The following paragraphs may contain profanity.\n\n## Description\n\nPython offers a set of sentence tokenizers for different purposes: nltk's word tokenizer, spacy's, scikit-learn's default and \nTweetTokenizer, among others. All but TweetTokenizer disregard hashtags and mentions by separating the symbols from the rest of the token(s).\nAlthough TweetTokenizer considers the Twitter *dialect*, it fails to tokenize subtle hidden profanity.\n\nFor the word ```f*ck```,the tokens considered are ```[f, *, ck]```. The word ```g@y``` is tokenized as ```[g, @y]```, considering \na single token ```g``` and a wrongly identified mention ```@y```. While the hashtag ```#hash_tag``` is correctly tokenized as \n```[#hash_tag]```, *regular* tokens are not underscore separated: ```love_twitter``` is tokenized as ```['love_twitter']``` instead of ```['love', '_', 'twitter']```.\n\nTwikenizer was created in order to enable a proper identification of hidden profane words, considering the features detailed above. Applying distance related features, i.e. levenshtein distance to slang words should output better results using this tokenizer.\n\n## Installation\n\n**Using pip**\n\npip install twikenizer\n\n**Clone repository**\n\ngit clone https://github.com/Guilherme-Routar/Twikenizer.git\n\n## Usage\n\n```python\n> import twikenizer as twk\n> twk = twk.Twikenizer()\n> tweet = 'This is an #hashtag'\n> twk.tokenize(tweet)\n['This', 'is', 'an', '#hashtag']\n```\n\nTwikenizer has a built-in function ```examplify``` which demonstrates how it tokenizes different kind of words/tokens.\n\n```python\n> twk.examplify()\nGenerated tweet\n###############\nTw33t # @dude_really #hash_tag $hit (g@y) retard#d @dude. \ud83d\ude00\ud83d\ude00 !\ud83d\ude00abc %\ud83d\ude00lol #hateit #hate.it $%&/ f*ck-\n\nGenerated tokens\n################\n['Tw33t', '#', '@dude_really', '#hash_tag', '$hit', '(', 'g', '@', 'y', ')', 'retard#d', '@dude', '.', '\ud83d\ude00', '\ud83d\ude00', '!', '\ud83d\ude00', 'abc', '%', '\ud83d\ude00', 'lol', '#hateit', '#hate', '.', 'it', '$', '%', '&', '/', 'f*ck', '-']\n\u00b4\u00b4\u00b4", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Guilherme-Routar/Twikenizer", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "twikenizer", "package_url": "https://pypi.org/project/twikenizer/", "platform": "", "project_url": "https://pypi.org/project/twikenizer/", "project_urls": {"Homepage": "https://github.com/Guilherme-Routar/Twikenizer"}, "release_url": "https://pypi.org/project/twikenizer/1.0/", "requires_dist": null, "requires_python": "", "summary": "Tokenizer for Twitter comments (tweets)", "version": "1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Twikenizer</h1>\n<p>This repository hosts the code for a tokenizer of tweets. It's main purpose is to identify subtle profanity, so it should\nobtain better performance on data containing hidden profanity (e.g. 'f*ck').</p>\n<p>Disclaimer: The following paragraphs may contain profanity.</p>\n<h2>Description</h2>\n<p>Python offers a set of sentence tokenizers for different purposes: nltk's word tokenizer, spacy's, scikit-learn's default and\nTweetTokenizer, among others. All but TweetTokenizer disregard hashtags and mentions by separating the symbols from the rest of the token(s).\nAlthough TweetTokenizer considers the Twitter <em>dialect</em>, it fails to tokenize subtle hidden profanity.</p>\n<p>For the word <code>f*ck</code>,the tokens considered are <code>[f, *, ck]</code>. The word <code>g@y</code> is tokenized as <code>[g, @y]</code>, considering\na single token <code>g</code> and a wrongly identified mention <code>@y</code>. While the hashtag <code>#hash_tag</code> is correctly tokenized as\n<code>[#hash_tag]</code>, <em>regular</em> tokens are not underscore separated: <code>love_twitter</code> is tokenized as <code>['love_twitter']</code> instead of <code>['love', '_', 'twitter']</code>.</p>\n<p>Twikenizer was created in order to enable a proper identification of hidden profane words, considering the features detailed above. Applying distance related features, i.e. levenshtein distance to slang words should output better results using this tokenizer.</p>\n<h2>Installation</h2>\n<p><strong>Using pip</strong></p>\n<p>pip install twikenizer</p>\n<p><strong>Clone repository</strong></p>\n<p>git clone <a href=\"https://github.com/Guilherme-Routar/Twikenizer.git\" rel=\"nofollow\">https://github.com/Guilherme-Routar/Twikenizer.git</a></p>\n<h2>Usage</h2>\n<pre><span class=\"o\">&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">twikenizer</span> <span class=\"k\">as</span> <span class=\"nn\">twk</span>\n<span class=\"o\">&gt;</span> <span class=\"n\">twk</span> <span class=\"o\">=</span> <span class=\"n\">twk</span><span class=\"o\">.</span><span class=\"n\">Twikenizer</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;</span> <span class=\"n\">tweet</span> <span class=\"o\">=</span> <span class=\"s1\">'This is an #hashtag'</span>\n<span class=\"o\">&gt;</span> <span class=\"n\">twk</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"n\">tweet</span><span class=\"p\">)</span>\n<span class=\"p\">[</span><span class=\"s1\">'This'</span><span class=\"p\">,</span> <span class=\"s1\">'is'</span><span class=\"p\">,</span> <span class=\"s1\">'an'</span><span class=\"p\">,</span> <span class=\"s1\">'#hashtag'</span><span class=\"p\">]</span>\n</pre>\n<p>Twikenizer has a built-in function <code>examplify</code> which demonstrates how it tokenizes different kind of words/tokens.</p>\n<pre><span class=\"o\">&gt;</span> <span class=\"n\">twk</span><span class=\"o\">.</span><span class=\"n\">examplify</span><span class=\"p\">()</span>\n<span class=\"n\">Generated</span> <span class=\"n\">tweet</span>\n<span class=\"c1\">###############</span>\n<span class=\"n\">Tw33t</span> <span class=\"c1\"># @dude_really #hash_tag $hit (g@y) retard#d @dude. \ud83d\ude00\ud83d\ude00 !\ud83d\ude00abc %\ud83d\ude00lol #hateit #hate.it $%&amp;/ f*ck-</span>\n\n<span class=\"n\">Generated</span> <span class=\"n\">tokens</span>\n<span class=\"c1\">################</span>\n<span class=\"p\">[</span><span class=\"s1\">'Tw33t'</span><span class=\"p\">,</span> <span class=\"s1\">'#'</span><span class=\"p\">,</span> <span class=\"s1\">'@dude_really'</span><span class=\"p\">,</span> <span class=\"s1\">'#hash_tag'</span><span class=\"p\">,</span> <span class=\"s1\">'$hit'</span><span class=\"p\">,</span> <span class=\"s1\">'('</span><span class=\"p\">,</span> <span class=\"s1\">'g'</span><span class=\"p\">,</span> <span class=\"s1\">'@'</span><span class=\"p\">,</span> <span class=\"s1\">'y'</span><span class=\"p\">,</span> <span class=\"s1\">')'</span><span class=\"p\">,</span> <span class=\"s1\">'retard#d'</span><span class=\"p\">,</span> <span class=\"s1\">'@dude'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">,</span> <span class=\"s1\">'\ud83d\ude00'</span><span class=\"p\">,</span> <span class=\"s1\">'\ud83d\ude00'</span><span class=\"p\">,</span> <span class=\"s1\">'!'</span><span class=\"p\">,</span> <span class=\"s1\">'\ud83d\ude00'</span><span class=\"p\">,</span> <span class=\"s1\">'abc'</span><span class=\"p\">,</span> <span class=\"s1\">'%'</span><span class=\"p\">,</span> <span class=\"s1\">'\ud83d\ude00'</span><span class=\"p\">,</span> <span class=\"s1\">'lol'</span><span class=\"p\">,</span> <span class=\"s1\">'#hateit'</span><span class=\"p\">,</span> <span class=\"s1\">'#hate'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">,</span> <span class=\"s1\">'it'</span><span class=\"p\">,</span> <span class=\"s1\">'$'</span><span class=\"p\">,</span> <span class=\"s1\">'%'</span><span class=\"p\">,</span> <span class=\"s1\">'&amp;'</span><span class=\"p\">,</span> <span class=\"s1\">'/'</span><span class=\"p\">,</span> <span class=\"s1\">'f*ck'</span><span class=\"p\">,</span> <span class=\"s1\">'-'</span><span class=\"p\">]</span>\n<span class=\"err\">\u00b4\u00b4\u00b4</span>\n</pre>\n\n          </div>"}, "last_serial": 4666819, "releases": {"1.0": [{"comment_text": "", "digests": {"md5": "62806ede5e47dcac792aedd4fd321a9c", "sha256": "678d7fc2adef86f6e9e2693c8710ef31b76a342923558a37d87ec09f8f97a33f"}, "downloads": -1, "filename": "twikenizer-1.0.tar.gz", "has_sig": false, "md5_digest": "62806ede5e47dcac792aedd4fd321a9c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4434, "upload_time": "2019-01-06T23:22:07", "upload_time_iso_8601": "2019-01-06T23:22:07.501024Z", "url": "https://files.pythonhosted.org/packages/d2/51/7aee33630b948f0716efae7a96c4fd8f859b348694058c380fd899a4227e/twikenizer-1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "62806ede5e47dcac792aedd4fd321a9c", "sha256": "678d7fc2adef86f6e9e2693c8710ef31b76a342923558a37d87ec09f8f97a33f"}, "downloads": -1, "filename": "twikenizer-1.0.tar.gz", "has_sig": false, "md5_digest": "62806ede5e47dcac792aedd4fd321a9c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4434, "upload_time": "2019-01-06T23:22:07", "upload_time_iso_8601": "2019-01-06T23:22:07.501024Z", "url": "https://files.pythonhosted.org/packages/d2/51/7aee33630b948f0716efae7a96c4fd8f859b348694058c380fd899a4227e/twikenizer-1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:43:59 2020"}