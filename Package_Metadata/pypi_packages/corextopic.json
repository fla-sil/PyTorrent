{"info": {"author": "Greg Ver Steeg/Ryan J. Gallagher", "author_email": "gregv@isi.edu", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3"], "description": "# Anchored CorEx: Hierarchical Topic Modeling with Minimal Domain Knowledge\n\nTopic modeling by way of **Cor**relation **Ex**planation (CorEx) yields rich topics that are maximally informative about a set of data. This project optimizes the CorEx framework for sparse binary data, allowing for topic modeling over large corpora. In addition, this code supports hierarchical topic modeling, and provides a mechanism for integrating domain knowledge via anchor words and the information bottleneck. This semi-supervised anchoring is flexible and allows the user to anchor words through creative strategies that promote topic representation, separability, and aspects.\n\nUnlike LDA, the CorEx topic model and its hierarchical and semi-supervised extensions make no assumptions on how documents are generated and, yet, they still find coherent, meaningful topics as measured across a variety of metrics. Our TACL paper makes detailed comparisons to unsupervised and semi-supervised variants of LDA:  \n\nGallagher, Ryan J., Kyle Reing, David Kale, and Greg Ver Steeg. \"[Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge](https://www.transacl.org/ojs/index.php/tacl/article/view/1244).\" *Transactions of the Association for Computational Linguistics (TACL)*, 2017.\n\nThis code can be used for any sparse binary dataset. In principle, continuous values in the range zero to one can also be used as inputs but the effect of this is not well tested.\n\n## Getting Started\n\n### Install\n\nPython code for the CorEx topic model can be installed via pip:\n\n```\npip install corextopic\n```\n\n### Running the CorEx Topic Model\n\nGiven a doc-word matrix, the CorEx topic model is easy to train. The code follows the scikit-learn fit/transform conventions.\n\n```python\nimport numpy as np\nimport scipy.sparse as ss\nfrom corextopic import corextopic as ct\n\n# Define a matrix where rows are samples (docs) and columns are features (words)\nX = np.array([[0,0,0,1,1],\n              [1,1,1,0,0],\n              [1,1,1,1,1]], dtype=int)\n# Sparse matrices are also supported\nX = ss.csr_matrix(X)\n# Word labels for each column can be provided to the model\nwords = ['dog', 'cat', 'fish', 'apple', 'orange']\n# Document labels for each row can be provided\ndocs = ['fruit doc', 'animal doc', 'mixed doc']\n\n# Train the CorEx topic model\ntopic_model = ct.Corex(n_hidden=2)  # Define the number of latent (hidden) topics to use.\ntopic_model.fit(X, words=words, docs=docs)\n```\n\nOnce the model is trained, the topics can be accessed through the ```get_topics()``` function.\n\n```python\ntopics = topic_model.get_topics()\nfor topic_n,topic in enumerate(topics):\n    words,mis = zip(*topic)\n    topic_str = str(topic_n+1)+': '+','.join(words)\n    print(topic_str)\n```\n\nSimilarly, the most probable documents for each topic can be accessed through the ``get_top_docs()`` function.\n\n```python\ntop_docs = topic_model.get_top_docs()\nfor topic_n, topic_docs in enumerate(top_docs):\n    docs,probs = zip(*topic_docs)\n    topic_str = str(topic_n+1)+': '+','.join(docs)\n    print(topic_str)\n```\n\nSummary files and visualizations can be outputted from ```vis_topic.py```.\n\n```python\nfrom corextopic import vis_topic as vt\nvt.vis_rep(topic_model, column_label=words, prefix='topic-model-example')\n```\n\nThe visualizations utilize ```seaborn```, and [```graphviz```](http://www.graphviz.org) is used for plotting hierarchical topic models. Graphviz should be compiled with the triangulation library for the best visual results.\n\nFull details on how to retrieve and interpret output from the CorEx topic model are given in the [example notebook](https://github.com/gregversteeg/corex_topic/blob/master/corextopic/example/corex_topic_example.ipynb).\n\n\n## Hierarchical Topic Modeling\n\n### Building a Hierarchical Topic Model\n\nFor the CorEx topic model, topics are latent factors that can be expressed or not in each document. We can use these binary topic expressions as input for another layer of the CorEx topic model, yielding a hierarchical representation.\n\n```python\n# Train the first layer\ntopic_model = ct.Corex(n_hidden=100)\ntopic_model.fit(X)\n\n# Train successive layers\ntm_layer2 = ct.Corex(n_hidden=10)\ntm_layer2.fit(topic_model.labels)\n\ntm_layer3 = ct.Corex(n_hidden=1)\ntm_layer3.fit(tm_layer2.labels)\n```\n\nVisualizations of the hierarchical topic model can be accessed through ```vis_topic.py```.\n\n```python\nvt.vis_hierarchy([topic_model, tm_layer2, tm_layer3], column_label=words, max_edges=300, prefix='topic-model-example')\n```\n\n### Choosing the Number of Topics\n\nThere is a principled way for choosing the number of topics within each layer of the topic model. Each topic explains a certain portion of the *total correlation* (TC). These topic TCs can be accessed through the ```tcs``` attribute, and the overall TC (the sum of the topic TCs) can be accessed through ```tc```. To assess how many topics to choose at each layer, you may look at the distribution of ```tcs``` for each layer. As a rule of thumb, additional latent topics should be added until additional topics contribute little to the overall TC.\n\nTo get better topic results, you can restart the CorEx topic model several times from different initializations, and choose the topic model that has the highest TC (explains the most information about the documents).\n\n\n## Semi-Supervised Topic Modeling\n\n### Using Anchor Words\n\nAnchored CorEx allows a user to anchor words to topics in a semi-supervised fashion to uncover otherwise elusive topics. If ```words``` is initialized, anchoring is straightforward:\n\n```python\ntopic_model.fit(X, words=words, anchors=[['dog','cat'], 'apple'], anchor_strength=2)\n```\n\nThis anchors \"dog\" and \"cat\" to the first topic, and \"apple\" to the second topic. As a rule of thumb ```anchor_strength``` should always be set above 1, where setting ```anchor_strength``` between 1 and 3 gently nudges a topic towards the anchor words, and setting it above 5 more strongly encourages the topic towards the anchor words. We encourage users to experiment with ```anchor_strength``` for their own purposes.  \n\nIf ```words``` is not initialized, you may anchor by specifying the integer column feature indices that you wish to anchor on. For example,\n\n```python\ntopic_model.fit(X, anchors=[[0, 2], 1], anchor_strength=2)\n```\n\nanchors the features of columns 0 and 2 to the first topic, and feature 1 to the second topic.\n\n### Anchoring Strategies\n\nIn our TACL paper, we explore several anchoring strategies:\n\n1. *Anchoring a single set of words to a single topic*. This can help promote a topic that did not naturally emerge when running an unsupervised instance of the CorEx topic model. For example, one might anchor words like \"snow,\" \"cold,\" and \"avalanche\" to a topic if one suspects there should be a snow avalanche topic within a set of disaster relief articles.\n\n```python\ntopic_model.fit(X, words=words, anchors=[['snow', 'cold', 'avalanche']], anchor_strength=4)\n```\n\n2. *Anchoring single sets of words to multiple topics*. This can help find different aspects of a topic that may be discussed in several different contexts. For example, one might anchor \"protest\" to three topics and \"riot\" to three other topics to understand different framings that arise from tweets about political protests.\n\n```python\ntopic_model.fit(X, words=words, anchors=['protest', 'protest', 'protest', 'riot', 'riot', 'riot'], anchor_strength=2)\n```\n\n3. *Anchoring different sets of words to multiple topics.* This can help enforce topic separability if there appear to be chimera topics. For example, one might anchor \"mountain,\" \"Bernese,\" and \"dog\" to one topic and \"mountain,\" \"rocky,\" and \"colorado\" to another topic to help separate topics that merge discussion of Bernese Mountain Dogs and the Rocky Mountains.\n\n```python\ntopic_model.fit(X, words=words, anchors=[['bernese', 'mountain', 'dog'], ['mountain', 'rocky', 'colorado'], anchor_strength=2)\n```\n\nThe [example notebook](https://github.com/gregversteeg/corex_topic/blob/master/corextopic/example/corex_topic_example.ipynb) details other examples of using anchored CorEx. We encourage domain experts to experiment with other anchoring strategies that suit their needs.\n\nNote, when running unsupervised CorEx, the topics are returned and sorted according to how much total correlation they each explain. When running anchored CorEx, the topics are not sorted by total correlation, and the first *n* topics will correspond to the *n* anchored topics in the order given by the model input.\n\n\n## Technical notes\n\n### Binarization of Documents\n\nFor speed reasons, this version of the CorEx topic model works only on binary data and produces binary latent factors. Despite this limitation, our work demonstrates CorEx produces coherent topics that are as good as or better than those produced by LDA for short to medium length documents. However, you may wish to consider additional preprocessing for working with longer documents. We have several strategies for handling text data.\n\n0. Naive binarization. This will be good for documents of similar length and especially short- to medium-length documents.\n\n1. Average binary bag of words. We split documents into chunks, compute the binary bag of words for each documents and then average. This implicitly weights all documents equally.\n\n2. All binary bag of words. Split documents into chunks and consider each chunk as its own binary bag of words documents.\n This changes the number of documents so it may take some work to match the ids back, if desired. Implicitly, this\n will weight longer documents more heavily. Generally this seems like the most theoretically justified method. Ideally, you could aggregate the latent factors over sub-documents to get 'counts' of latent factors at the higher layers.\n\n 3. Fractional counts. This converts counts into a fraction of the background rate, with 1 as the max. Short documents tend to stay binary and words in long documents are weighted according to their frequency with respect to background in the corpus. This seems to work Ok on tests. It requires no preprocessing of count data and it uses the full range of possible inputs. However, this approach is not very rigorous or well tested.\n\nFor the python API, for 1 and 2, you can use the functions in ```vis_topic``` to process data or do the same yourself. Naive binarization is specified through the python api with count='binarize' and fractional counts with count='fraction'. While fractional counts may be work theoretically, their usage in the CorEx topic model has not be adequately tested.\n\n### Single Membership of Words in Topics\n\nAlso for speed reasons, the CorEx topic model enforces single membership of words in topics. If a user anchors a word to multiple topics, the single membership can be overriden. Going forward, we plan to develop a multi-membership extension of the CorEx topic model that retains the computational efficiency.\n\n\n## Underlying Theory and Motivation of CorEx\n[*Discovering Structure in High-Dimensional Data Through Correlation Explanation*](http://arxiv.org/abs/1406.1222), Ver Steeg and Galstyan, NIPS 2014. <br>\n[*Maximally Informative Hierarchical Representions of High-Dimensional Data*](http://arxiv.org/abs/1410.7404), Ver Steeg and Galstyan, AISTATS 2015.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/gregversteeg/corex_topic", "keywords": "topic model,corex,anchored corex,LDA,semi-supervised,hierarchical topic model,information theory", "license": "", "maintainer": "", "maintainer_email": "", "name": "corextopic", "package_url": "https://pypi.org/project/corextopic/", "platform": "", "project_url": "https://pypi.org/project/corextopic/", "project_urls": {"Homepage": "https://github.com/gregversteeg/corex_topic"}, "release_url": "https://pypi.org/project/corextopic/1.0.5/", "requires_dist": null, "requires_python": "", "summary": "Hierarchical and semi-supervised topic modeling with minimal domain knowledge through Anchored Correlation Explanation", "version": "1.0.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Anchored CorEx: Hierarchical Topic Modeling with Minimal Domain Knowledge</h1>\n<p>Topic modeling by way of <strong>Cor</strong>relation <strong>Ex</strong>planation (CorEx) yields rich topics that are maximally informative about a set of data. This project optimizes the CorEx framework for sparse binary data, allowing for topic modeling over large corpora. In addition, this code supports hierarchical topic modeling, and provides a mechanism for integrating domain knowledge via anchor words and the information bottleneck. This semi-supervised anchoring is flexible and allows the user to anchor words through creative strategies that promote topic representation, separability, and aspects.</p>\n<p>Unlike LDA, the CorEx topic model and its hierarchical and semi-supervised extensions make no assumptions on how documents are generated and, yet, they still find coherent, meaningful topics as measured across a variety of metrics. Our TACL paper makes detailed comparisons to unsupervised and semi-supervised variants of LDA:</p>\n<p>Gallagher, Ryan J., Kyle Reing, David Kale, and Greg Ver Steeg. \"<a href=\"https://www.transacl.org/ojs/index.php/tacl/article/view/1244\" rel=\"nofollow\">Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge</a>.\" <em>Transactions of the Association for Computational Linguistics (TACL)</em>, 2017.</p>\n<p>This code can be used for any sparse binary dataset. In principle, continuous values in the range zero to one can also be used as inputs but the effect of this is not well tested.</p>\n<h2>Getting Started</h2>\n<h3>Install</h3>\n<p>Python code for the CorEx topic model can be installed via pip:</p>\n<pre><code>pip install corextopic\n</code></pre>\n<h3>Running the CorEx Topic Model</h3>\n<p>Given a doc-word matrix, the CorEx topic model is easy to train. The code follows the scikit-learn fit/transform conventions.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">scipy.sparse</span> <span class=\"k\">as</span> <span class=\"nn\">ss</span>\n<span class=\"kn\">from</span> <span class=\"nn\">corextopic</span> <span class=\"kn\">import</span> <span class=\"n\">corextopic</span> <span class=\"k\">as</span> <span class=\"n\">ct</span>\n\n<span class=\"c1\"># Define a matrix where rows are samples (docs) and columns are features (words)</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],</span>\n              <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">],</span>\n              <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">)</span>\n<span class=\"c1\"># Sparse matrices are also supported</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">ss</span><span class=\"o\">.</span><span class=\"n\">csr_matrix</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n<span class=\"c1\"># Word labels for each column can be provided to the model</span>\n<span class=\"n\">words</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'dog'</span><span class=\"p\">,</span> <span class=\"s1\">'cat'</span><span class=\"p\">,</span> <span class=\"s1\">'fish'</span><span class=\"p\">,</span> <span class=\"s1\">'apple'</span><span class=\"p\">,</span> <span class=\"s1\">'orange'</span><span class=\"p\">]</span>\n<span class=\"c1\"># Document labels for each row can be provided</span>\n<span class=\"n\">docs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'fruit doc'</span><span class=\"p\">,</span> <span class=\"s1\">'animal doc'</span><span class=\"p\">,</span> <span class=\"s1\">'mixed doc'</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># Train the CorEx topic model</span>\n<span class=\"n\">topic_model</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">Corex</span><span class=\"p\">(</span><span class=\"n\">n_hidden</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>  <span class=\"c1\"># Define the number of latent (hidden) topics to use.</span>\n<span class=\"n\">topic_model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">words</span><span class=\"o\">=</span><span class=\"n\">words</span><span class=\"p\">,</span> <span class=\"n\">docs</span><span class=\"o\">=</span><span class=\"n\">docs</span><span class=\"p\">)</span>\n</pre>\n<p>Once the model is trained, the topics can be accessed through the <code>get_topics()</code> function.</p>\n<pre><span class=\"n\">topics</span> <span class=\"o\">=</span> <span class=\"n\">topic_model</span><span class=\"o\">.</span><span class=\"n\">get_topics</span><span class=\"p\">()</span>\n<span class=\"k\">for</span> <span class=\"n\">topic_n</span><span class=\"p\">,</span><span class=\"n\">topic</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">topics</span><span class=\"p\">):</span>\n    <span class=\"n\">words</span><span class=\"p\">,</span><span class=\"n\">mis</span> <span class=\"o\">=</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">topic</span><span class=\"p\">)</span>\n    <span class=\"n\">topic_str</span> <span class=\"o\">=</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">topic_n</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">+</span><span class=\"s1\">': '</span><span class=\"o\">+</span><span class=\"s1\">','</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">words</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">topic_str</span><span class=\"p\">)</span>\n</pre>\n<p>Similarly, the most probable documents for each topic can be accessed through the <code>get_top_docs()</code> function.</p>\n<pre><span class=\"n\">top_docs</span> <span class=\"o\">=</span> <span class=\"n\">topic_model</span><span class=\"o\">.</span><span class=\"n\">get_top_docs</span><span class=\"p\">()</span>\n<span class=\"k\">for</span> <span class=\"n\">topic_n</span><span class=\"p\">,</span> <span class=\"n\">topic_docs</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">top_docs</span><span class=\"p\">):</span>\n    <span class=\"n\">docs</span><span class=\"p\">,</span><span class=\"n\">probs</span> <span class=\"o\">=</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">topic_docs</span><span class=\"p\">)</span>\n    <span class=\"n\">topic_str</span> <span class=\"o\">=</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">topic_n</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">+</span><span class=\"s1\">': '</span><span class=\"o\">+</span><span class=\"s1\">','</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">docs</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">topic_str</span><span class=\"p\">)</span>\n</pre>\n<p>Summary files and visualizations can be outputted from <code>vis_topic.py</code>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">corextopic</span> <span class=\"kn\">import</span> <span class=\"n\">vis_topic</span> <span class=\"k\">as</span> <span class=\"n\">vt</span>\n<span class=\"n\">vt</span><span class=\"o\">.</span><span class=\"n\">vis_rep</span><span class=\"p\">(</span><span class=\"n\">topic_model</span><span class=\"p\">,</span> <span class=\"n\">column_label</span><span class=\"o\">=</span><span class=\"n\">words</span><span class=\"p\">,</span> <span class=\"n\">prefix</span><span class=\"o\">=</span><span class=\"s1\">'topic-model-example'</span><span class=\"p\">)</span>\n</pre>\n<p>The visualizations utilize <code>seaborn</code>, and <a href=\"http://www.graphviz.org\" rel=\"nofollow\"><code>graphviz</code></a> is used for plotting hierarchical topic models. Graphviz should be compiled with the triangulation library for the best visual results.</p>\n<p>Full details on how to retrieve and interpret output from the CorEx topic model are given in the <a href=\"https://github.com/gregversteeg/corex_topic/blob/master/corextopic/example/corex_topic_example.ipynb\" rel=\"nofollow\">example notebook</a>.</p>\n<h2>Hierarchical Topic Modeling</h2>\n<h3>Building a Hierarchical Topic Model</h3>\n<p>For the CorEx topic model, topics are latent factors that can be expressed or not in each document. We can use these binary topic expressions as input for another layer of the CorEx topic model, yielding a hierarchical representation.</p>\n<pre><span class=\"c1\"># Train the first layer</span>\n<span class=\"n\">topic_model</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">Corex</span><span class=\"p\">(</span><span class=\"n\">n_hidden</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"n\">topic_model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Train successive layers</span>\n<span class=\"n\">tm_layer2</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">Corex</span><span class=\"p\">(</span><span class=\"n\">n_hidden</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"n\">tm_layer2</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">topic_model</span><span class=\"o\">.</span><span class=\"n\">labels</span><span class=\"p\">)</span>\n\n<span class=\"n\">tm_layer3</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"o\">.</span><span class=\"n\">Corex</span><span class=\"p\">(</span><span class=\"n\">n_hidden</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">tm_layer3</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">tm_layer2</span><span class=\"o\">.</span><span class=\"n\">labels</span><span class=\"p\">)</span>\n</pre>\n<p>Visualizations of the hierarchical topic model can be accessed through <code>vis_topic.py</code>.</p>\n<pre><span class=\"n\">vt</span><span class=\"o\">.</span><span class=\"n\">vis_hierarchy</span><span class=\"p\">([</span><span class=\"n\">topic_model</span><span class=\"p\">,</span> <span class=\"n\">tm_layer2</span><span class=\"p\">,</span> <span class=\"n\">tm_layer3</span><span class=\"p\">],</span> <span class=\"n\">column_label</span><span class=\"o\">=</span><span class=\"n\">words</span><span class=\"p\">,</span> <span class=\"n\">max_edges</span><span class=\"o\">=</span><span class=\"mi\">300</span><span class=\"p\">,</span> <span class=\"n\">prefix</span><span class=\"o\">=</span><span class=\"s1\">'topic-model-example'</span><span class=\"p\">)</span>\n</pre>\n<h3>Choosing the Number of Topics</h3>\n<p>There is a principled way for choosing the number of topics within each layer of the topic model. Each topic explains a certain portion of the <em>total correlation</em> (TC). These topic TCs can be accessed through the <code>tcs</code> attribute, and the overall TC (the sum of the topic TCs) can be accessed through <code>tc</code>. To assess how many topics to choose at each layer, you may look at the distribution of <code>tcs</code> for each layer. As a rule of thumb, additional latent topics should be added until additional topics contribute little to the overall TC.</p>\n<p>To get better topic results, you can restart the CorEx topic model several times from different initializations, and choose the topic model that has the highest TC (explains the most information about the documents).</p>\n<h2>Semi-Supervised Topic Modeling</h2>\n<h3>Using Anchor Words</h3>\n<p>Anchored CorEx allows a user to anchor words to topics in a semi-supervised fashion to uncover otherwise elusive topics. If <code>words</code> is initialized, anchoring is straightforward:</p>\n<pre><span class=\"n\">topic_model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">words</span><span class=\"o\">=</span><span class=\"n\">words</span><span class=\"p\">,</span> <span class=\"n\">anchors</span><span class=\"o\">=</span><span class=\"p\">[[</span><span class=\"s1\">'dog'</span><span class=\"p\">,</span><span class=\"s1\">'cat'</span><span class=\"p\">],</span> <span class=\"s1\">'apple'</span><span class=\"p\">],</span> <span class=\"n\">anchor_strength</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n</pre>\n<p>This anchors \"dog\" and \"cat\" to the first topic, and \"apple\" to the second topic. As a rule of thumb <code>anchor_strength</code> should always be set above 1, where setting <code>anchor_strength</code> between 1 and 3 gently nudges a topic towards the anchor words, and setting it above 5 more strongly encourages the topic towards the anchor words. We encourage users to experiment with <code>anchor_strength</code> for their own purposes.</p>\n<p>If <code>words</code> is not initialized, you may anchor by specifying the integer column feature indices that you wish to anchor on. For example,</p>\n<pre><span class=\"n\">topic_model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">anchors</span><span class=\"o\">=</span><span class=\"p\">[[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">anchor_strength</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n</pre>\n<p>anchors the features of columns 0 and 2 to the first topic, and feature 1 to the second topic.</p>\n<h3>Anchoring Strategies</h3>\n<p>In our TACL paper, we explore several anchoring strategies:</p>\n<ol>\n<li><em>Anchoring a single set of words to a single topic</em>. This can help promote a topic that did not naturally emerge when running an unsupervised instance of the CorEx topic model. For example, one might anchor words like \"snow,\" \"cold,\" and \"avalanche\" to a topic if one suspects there should be a snow avalanche topic within a set of disaster relief articles.</li>\n</ol>\n<pre><span class=\"n\">topic_model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">words</span><span class=\"o\">=</span><span class=\"n\">words</span><span class=\"p\">,</span> <span class=\"n\">anchors</span><span class=\"o\">=</span><span class=\"p\">[[</span><span class=\"s1\">'snow'</span><span class=\"p\">,</span> <span class=\"s1\">'cold'</span><span class=\"p\">,</span> <span class=\"s1\">'avalanche'</span><span class=\"p\">]],</span> <span class=\"n\">anchor_strength</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n</pre>\n<ol>\n<li><em>Anchoring single sets of words to multiple topics</em>. This can help find different aspects of a topic that may be discussed in several different contexts. For example, one might anchor \"protest\" to three topics and \"riot\" to three other topics to understand different framings that arise from tweets about political protests.</li>\n</ol>\n<pre><span class=\"n\">topic_model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">words</span><span class=\"o\">=</span><span class=\"n\">words</span><span class=\"p\">,</span> <span class=\"n\">anchors</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'protest'</span><span class=\"p\">,</span> <span class=\"s1\">'protest'</span><span class=\"p\">,</span> <span class=\"s1\">'protest'</span><span class=\"p\">,</span> <span class=\"s1\">'riot'</span><span class=\"p\">,</span> <span class=\"s1\">'riot'</span><span class=\"p\">,</span> <span class=\"s1\">'riot'</span><span class=\"p\">],</span> <span class=\"n\">anchor_strength</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n</pre>\n<ol>\n<li><em>Anchoring different sets of words to multiple topics.</em> This can help enforce topic separability if there appear to be chimera topics. For example, one might anchor \"mountain,\" \"Bernese,\" and \"dog\" to one topic and \"mountain,\" \"rocky,\" and \"colorado\" to another topic to help separate topics that merge discussion of Bernese Mountain Dogs and the Rocky Mountains.</li>\n</ol>\n<pre><span class=\"n\">topic_model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">words</span><span class=\"o\">=</span><span class=\"n\">words</span><span class=\"p\">,</span> <span class=\"n\">anchors</span><span class=\"o\">=</span><span class=\"p\">[[</span><span class=\"s1\">'bernese'</span><span class=\"p\">,</span> <span class=\"s1\">'mountain'</span><span class=\"p\">,</span> <span class=\"s1\">'dog'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'mountain'</span><span class=\"p\">,</span> <span class=\"s1\">'rocky'</span><span class=\"p\">,</span> <span class=\"s1\">'colorado'</span><span class=\"p\">],</span> <span class=\"n\">anchor_strength</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n</pre>\n<p>The <a href=\"https://github.com/gregversteeg/corex_topic/blob/master/corextopic/example/corex_topic_example.ipynb\" rel=\"nofollow\">example notebook</a> details other examples of using anchored CorEx. We encourage domain experts to experiment with other anchoring strategies that suit their needs.</p>\n<p>Note, when running unsupervised CorEx, the topics are returned and sorted according to how much total correlation they each explain. When running anchored CorEx, the topics are not sorted by total correlation, and the first <em>n</em> topics will correspond to the <em>n</em> anchored topics in the order given by the model input.</p>\n<h2>Technical notes</h2>\n<h3>Binarization of Documents</h3>\n<p>For speed reasons, this version of the CorEx topic model works only on binary data and produces binary latent factors. Despite this limitation, our work demonstrates CorEx produces coherent topics that are as good as or better than those produced by LDA for short to medium length documents. However, you may wish to consider additional preprocessing for working with longer documents. We have several strategies for handling text data.</p>\n<ol>\n<li>\n<p>Naive binarization. This will be good for documents of similar length and especially short- to medium-length documents.</p>\n</li>\n<li>\n<p>Average binary bag of words. We split documents into chunks, compute the binary bag of words for each documents and then average. This implicitly weights all documents equally.</p>\n</li>\n<li>\n<p>All binary bag of words. Split documents into chunks and consider each chunk as its own binary bag of words documents.\nThis changes the number of documents so it may take some work to match the ids back, if desired. Implicitly, this\nwill weight longer documents more heavily. Generally this seems like the most theoretically justified method. Ideally, you could aggregate the latent factors over sub-documents to get 'counts' of latent factors at the higher layers.</p>\n</li>\n<li>\n<p>Fractional counts. This converts counts into a fraction of the background rate, with 1 as the max. Short documents tend to stay binary and words in long documents are weighted according to their frequency with respect to background in the corpus. This seems to work Ok on tests. It requires no preprocessing of count data and it uses the full range of possible inputs. However, this approach is not very rigorous or well tested.</p>\n</li>\n</ol>\n<p>For the python API, for 1 and 2, you can use the functions in <code>vis_topic</code> to process data or do the same yourself. Naive binarization is specified through the python api with count='binarize' and fractional counts with count='fraction'. While fractional counts may be work theoretically, their usage in the CorEx topic model has not be adequately tested.</p>\n<h3>Single Membership of Words in Topics</h3>\n<p>Also for speed reasons, the CorEx topic model enforces single membership of words in topics. If a user anchors a word to multiple topics, the single membership can be overriden. Going forward, we plan to develop a multi-membership extension of the CorEx topic model that retains the computational efficiency.</p>\n<h2>Underlying Theory and Motivation of CorEx</h2>\n<p><a href=\"http://arxiv.org/abs/1406.1222\" rel=\"nofollow\"><em>Discovering Structure in High-Dimensional Data Through Correlation Explanation</em></a>, Ver Steeg and Galstyan, NIPS 2014. <br>\n<a href=\"http://arxiv.org/abs/1410.7404\" rel=\"nofollow\"><em>Maximally Informative Hierarchical Representions of High-Dimensional Data</em></a>, Ver Steeg and Galstyan, AISTATS 2015.</p>\n\n          </div>"}, "last_serial": 6010205, "releases": {"1.0.1": [{"comment_text": "", "digests": {"md5": "d79fe2c136ab297cf874f3f0eea20f27", "sha256": "cc3e7e2df6200c3f697b4534308e64fb90a2d803ea03cea5a8de700e60efebef"}, "downloads": -1, "filename": "corextopic-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "d79fe2c136ab297cf874f3f0eea20f27", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 22159, "upload_time": "2018-05-29T02:46:16", "upload_time_iso_8601": "2018-05-29T02:46:16.828207Z", "url": "https://files.pythonhosted.org/packages/1d/31/997f1c45a540a3c8245bf71b47813c9d0c3a19866afe4d4830908dcdbad1/corextopic-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e6bb28146feeacea3c492f92966b5de4", "sha256": "33b1dd8bd6d903257b70110f247e8dc595fa25d37580e4e5025b8525c4ff3183"}, "downloads": -1, "filename": "corextopic-1.0.1.tar.gz", "has_sig": false, "md5_digest": "e6bb28146feeacea3c492f92966b5de4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24916, "upload_time": "2018-05-29T02:46:18", "upload_time_iso_8601": "2018-05-29T02:46:18.132896Z", "url": "https://files.pythonhosted.org/packages/60/2c/6a6322ee3bcd013285657fe1957dc4298a258405ac0fcca8c8f0bed09830/corextopic-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "0cf79630d3613e4248cbb6027a71a246", "sha256": "2a2e2b7a56254ca164741edda0e4ca358456228b51b5af62a71a22e2f3a82c00"}, "downloads": -1, "filename": "corextopic-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "0cf79630d3613e4248cbb6027a71a246", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 22041, "upload_time": "2018-05-29T02:49:26", "upload_time_iso_8601": "2018-05-29T02:49:26.707760Z", "url": "https://files.pythonhosted.org/packages/b6/6c/2f5ee2b93d110f7f3fed8102535924fc3d34afd6bedc3b05ccfecea24605/corextopic-1.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ccdbced792219e92c8ab3a22fc6a8620", "sha256": "b7d65963791b5d0776fc141fa700e2742ce9e03b7555fb03b31cec8343f90794"}, "downloads": -1, "filename": "corextopic-1.0.2.tar.gz", "has_sig": false, "md5_digest": "ccdbced792219e92c8ab3a22fc6a8620", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24632, "upload_time": "2018-05-29T02:49:28", "upload_time_iso_8601": "2018-05-29T02:49:28.664190Z", "url": "https://files.pythonhosted.org/packages/0c/67/06378177d2d811f309d500d2ef9ce9edff1bcff6514614a738ef745d801c/corextopic-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "fbb09784c18160128be6df4aaedda0aa", "sha256": "c79a5f0778e234e38458e8a0824bfc3c5b0bfe7a8bfcd93754096dbe1e147df4"}, "downloads": -1, "filename": "corextopic-1.0.3.tar.gz", "has_sig": false, "md5_digest": "fbb09784c18160128be6df4aaedda0aa", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24933, "upload_time": "2019-06-12T02:06:49", "upload_time_iso_8601": "2019-06-12T02:06:49.117359Z", "url": "https://files.pythonhosted.org/packages/43/a0/44772d08d22d622e063a9b9e6d890cb0e96932178028bde5bafd61a5f2fa/corextopic-1.0.3.tar.gz", "yanked": false}], "1.0.4": [{"comment_text": "", "digests": {"md5": "87d36117f2363f27fafb45e82ad60953", "sha256": "2d9637974301f08ed18cecd81dc8d55c4d7295758feddb471e3f64b0629b61b4"}, "downloads": -1, "filename": "corextopic-1.0.4.tar.gz", "has_sig": false, "md5_digest": "87d36117f2363f27fafb45e82ad60953", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24952, "upload_time": "2019-06-12T02:31:43", "upload_time_iso_8601": "2019-06-12T02:31:43.199133Z", "url": "https://files.pythonhosted.org/packages/17/b3/5068b3f0f285260313362389d3eb84590be7f8ea2a8aa606e47b1800f471/corextopic-1.0.4.tar.gz", "yanked": false}], "1.0.5": [{"comment_text": "", "digests": {"md5": "b23624d66fc8880effcb67ca63523a79", "sha256": "e42f69492b879a9f16be99754cc83a4f935bda19639dae1b42ac3e5003ee9da4"}, "downloads": -1, "filename": "corextopic-1.0.5.tar.gz", "has_sig": false, "md5_digest": "b23624d66fc8880effcb67ca63523a79", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24980, "upload_time": "2019-10-22T02:41:36", "upload_time_iso_8601": "2019-10-22T02:41:36.617213Z", "url": "https://files.pythonhosted.org/packages/e6/da/c68f01915c86061b55c8cbe0bc092999a5444eef5b10abf69a0d1e06b6c3/corextopic-1.0.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b23624d66fc8880effcb67ca63523a79", "sha256": "e42f69492b879a9f16be99754cc83a4f935bda19639dae1b42ac3e5003ee9da4"}, "downloads": -1, "filename": "corextopic-1.0.5.tar.gz", "has_sig": false, "md5_digest": "b23624d66fc8880effcb67ca63523a79", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24980, "upload_time": "2019-10-22T02:41:36", "upload_time_iso_8601": "2019-10-22T02:41:36.617213Z", "url": "https://files.pythonhosted.org/packages/e6/da/c68f01915c86061b55c8cbe0bc092999a5444eef5b10abf69a0d1e06b6c3/corextopic-1.0.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:10 2020"}