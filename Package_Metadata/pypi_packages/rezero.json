{"info": {"author": "Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W. Cottrell, Julian McAuley", "author_email": "henry@calclavia.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# ReZero for Deep Neural Networks\n\n[**ReZero is All You Need: Fast Convergence at Large Depth**](https://arxiv.org/abs/2003.04887); *ArXiv, March 2020*.\n\nThomas Bachlechner*, Bodhisattwa Prasad Majumder*, Huanru Henry Mao*, Garrison W. Cottrell, Julian McAuley (* denotes equal contributions)\n\nThis repository contains the ReZero-Transformer implementation from the paper. It matches Pytorch's Transformer and can be easily used as a drop-in replacement. See sections below for installation and usage.\n\n# Abstract\n\nDeep networks have enabled significant performance gains across domains, but they often suffer from vanishing/exploding gradients. This is especially true for Transformer architectures where depth beyond 12 layers is difficult to train without large datasets and computational budgets. In general, we find that inefficient signal propagation impedes learning in deep networks. In Transformers, multi-head self-attention is the main cause of this poor signal propagation. To facilitate deep signal propagation, we propose **ReZero**, a simple change to the architecture that initializes an arbitrary layer as the identity map, using a single additional learned parameter per layer. We apply this technique to language modeling and find that we can easily train ReZero-Transformer networks over a hundred layers. When applied to 12 layer Transformers, ReZero converges 56% faster on enwiki8. ReZero applies beyond Transformers to other residual networks, enabling 1,500% faster convergence for deep fully connected networks and 32% faster convergence for a ResNet-56 trained on CIFAR 10.\n\n# Installation\nSimply install from pip:\n\n```\npip install rezero\n```\n\nPytorch 1.4 or greater is required.\n\n# Usage\nWe provide custom ReZero Transformer layers (RZTX).\n\nFor example, this will create a Transformer encoder:\n```py\nimport torch\nimport torch.nn as nn\nfrom rezero.transformer import RZTXEncoderLayer\n\nencoder_layer = RZTXEncoderLayer(d_model=512, nhead=8)\ntransformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\nsrc = torch.rand(10, 32, 512)\nout = transformer_encoder(src)\n```\n\nThis will create a Transformer decoder:\n```py\nimport torch\nimport torch.nn as nn\nfrom rezero.transformer import RZTXDecoderLayer\n\ndecoder_layer = RZTXDecoderLayer(d_model=512, nhead=8)\ntransformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\nmemory = torch.rand(10, 32, 512)\ntgt = torch.rand(20, 32, 512)\nout = transformer_decoder(tgt, memory)\n```\n\nMake sure `norm` argument is left as `None` as to not use `LayerNorm` in the Transformer.\n\nSee https://pytorch.org/docs/master/nn.html#torch.nn.Transformer for details on how to integrate customer Transformer layers to Pytorch.\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/majumderb/rezero", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "rezero", "package_url": "https://pypi.org/project/rezero/", "platform": "", "project_url": "https://pypi.org/project/rezero/", "project_urls": {"Homepage": "https://github.com/majumderb/rezero"}, "release_url": "https://pypi.org/project/rezero/0.1.0/", "requires_dist": null, "requires_python": ">=3.6", "summary": "ReZero networks", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>ReZero for Deep Neural Networks</h1>\n<p><a href=\"https://arxiv.org/abs/2003.04887\" rel=\"nofollow\"><strong>ReZero is All You Need: Fast Convergence at Large Depth</strong></a>; <em>ArXiv, March 2020</em>.</p>\n<p>Thomas Bachlechner*, Bodhisattwa Prasad Majumder*, Huanru Henry Mao*, Garrison W. Cottrell, Julian McAuley (* denotes equal contributions)</p>\n<p>This repository contains the ReZero-Transformer implementation from the paper. It matches Pytorch's Transformer and can be easily used as a drop-in replacement. See sections below for installation and usage.</p>\n<h1>Abstract</h1>\n<p>Deep networks have enabled significant performance gains across domains, but they often suffer from vanishing/exploding gradients. This is especially true for Transformer architectures where depth beyond 12 layers is difficult to train without large datasets and computational budgets. In general, we find that inefficient signal propagation impedes learning in deep networks. In Transformers, multi-head self-attention is the main cause of this poor signal propagation. To facilitate deep signal propagation, we propose <strong>ReZero</strong>, a simple change to the architecture that initializes an arbitrary layer as the identity map, using a single additional learned parameter per layer. We apply this technique to language modeling and find that we can easily train ReZero-Transformer networks over a hundred layers. When applied to 12 layer Transformers, ReZero converges 56% faster on enwiki8. ReZero applies beyond Transformers to other residual networks, enabling 1,500% faster convergence for deep fully connected networks and 32% faster convergence for a ResNet-56 trained on CIFAR 10.</p>\n<h1>Installation</h1>\n<p>Simply install from pip:</p>\n<pre><code>pip install rezero\n</code></pre>\n<p>Pytorch 1.4 or greater is required.</p>\n<h1>Usage</h1>\n<p>We provide custom ReZero Transformer layers (RZTX).</p>\n<p>For example, this will create a Transformer encoder:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"nn\">nn</span>\n<span class=\"kn\">from</span> <span class=\"nn\">rezero.transformer</span> <span class=\"kn\">import</span> <span class=\"n\">RZTXEncoderLayer</span>\n\n<span class=\"n\">encoder_layer</span> <span class=\"o\">=</span> <span class=\"n\">RZTXEncoderLayer</span><span class=\"p\">(</span><span class=\"n\">d_model</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">nhead</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span>\n<span class=\"n\">transformer_encoder</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">TransformerEncoder</span><span class=\"p\">(</span><span class=\"n\">encoder_layer</span><span class=\"p\">,</span> <span class=\"n\">num_layers</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">)</span>\n<span class=\"n\">src</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">)</span>\n<span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"n\">transformer_encoder</span><span class=\"p\">(</span><span class=\"n\">src</span><span class=\"p\">)</span>\n</pre>\n<p>This will create a Transformer decoder:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"nn\">nn</span>\n<span class=\"kn\">from</span> <span class=\"nn\">rezero.transformer</span> <span class=\"kn\">import</span> <span class=\"n\">RZTXDecoderLayer</span>\n\n<span class=\"n\">decoder_layer</span> <span class=\"o\">=</span> <span class=\"n\">RZTXDecoderLayer</span><span class=\"p\">(</span><span class=\"n\">d_model</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">nhead</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span>\n<span class=\"n\">transformer_decoder</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">TransformerDecoder</span><span class=\"p\">(</span><span class=\"n\">decoder_layer</span><span class=\"p\">,</span> <span class=\"n\">num_layers</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">)</span>\n<span class=\"n\">memory</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">)</span>\n<span class=\"n\">tgt</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">)</span>\n<span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"n\">transformer_decoder</span><span class=\"p\">(</span><span class=\"n\">tgt</span><span class=\"p\">,</span> <span class=\"n\">memory</span><span class=\"p\">)</span>\n</pre>\n<p>Make sure <code>norm</code> argument is left as <code>None</code> as to not use <code>LayerNorm</code> in the Transformer.</p>\n<p>See <a href=\"https://pytorch.org/docs/master/nn.html#torch.nn.Transformer\" rel=\"nofollow\">https://pytorch.org/docs/master/nn.html#torch.nn.Transformer</a> for details on how to integrate customer Transformer layers to Pytorch.</p>\n\n          </div>"}, "last_serial": 6797456, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "29b8e9c2198cdaa312c9f8543ab0f3e4", "sha256": "edc9816fae53f928f4d187c8378e371ae98f854f19eb9aa75ec3aeaef4081a78"}, "downloads": -1, "filename": "rezero-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "29b8e9c2198cdaa312c9f8543ab0f3e4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 5356, "upload_time": "2020-03-12T06:24:41", "upload_time_iso_8601": "2020-03-12T06:24:41.659739Z", "url": "https://files.pythonhosted.org/packages/ec/d2/4751a70110fc219e6abe056f3cfe261bef8dfa03cbc20a7c0adaf25aef78/rezero-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "70749a045feacc83803114961bf44c8e", "sha256": "d89916e9ba26688b6e105caaa017569c8d29e6d5004810e680cc3f7cdc2ce7f2"}, "downloads": -1, "filename": "rezero-0.1.0.tar.gz", "has_sig": false, "md5_digest": "70749a045feacc83803114961bf44c8e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 3877, "upload_time": "2020-03-12T06:24:43", "upload_time_iso_8601": "2020-03-12T06:24:43.890138Z", "url": "https://files.pythonhosted.org/packages/25/a8/a997ffb4e407727f88679c554f8121b0cfa484a02b4d6852a9679faa685d/rezero-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "29b8e9c2198cdaa312c9f8543ab0f3e4", "sha256": "edc9816fae53f928f4d187c8378e371ae98f854f19eb9aa75ec3aeaef4081a78"}, "downloads": -1, "filename": "rezero-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "29b8e9c2198cdaa312c9f8543ab0f3e4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 5356, "upload_time": "2020-03-12T06:24:41", "upload_time_iso_8601": "2020-03-12T06:24:41.659739Z", "url": "https://files.pythonhosted.org/packages/ec/d2/4751a70110fc219e6abe056f3cfe261bef8dfa03cbc20a7c0adaf25aef78/rezero-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "70749a045feacc83803114961bf44c8e", "sha256": "d89916e9ba26688b6e105caaa017569c8d29e6d5004810e680cc3f7cdc2ce7f2"}, "downloads": -1, "filename": "rezero-0.1.0.tar.gz", "has_sig": false, "md5_digest": "70749a045feacc83803114961bf44c8e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 3877, "upload_time": "2020-03-12T06:24:43", "upload_time_iso_8601": "2020-03-12T06:24:43.890138Z", "url": "https://files.pythonhosted.org/packages/25/a8/a997ffb4e407727f88679c554f8121b0cfa484a02b4d6852a9679faa685d/rezero-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:03:12 2020"}