{"info": {"author": "Welliton de Souza, Benilton Carvalho", "author_email": "well309@gmail.com, benilton@unicamp.br", "bugtrack_url": null, "classifiers": [], "description": "# Automated and reproducible tool for identifying genomic variants at scale\n\nProcessing high-throughput sequencing data was the biggest challenge in the past, dealing with small datasets of large sample data.\nGenomics projects with thousands of samples have became popular due to decreasing in sequencing costs.\nNow we are facing a new challenge: how to process and store large-scale datasets efficiently.\n\nThe [Sequence Alignment/Map Format Specification (SAM)](https://samtools.github.io/hts-specs/SAMv1.pdf) and its binary version (BAM) is the most popular and recommended file format to store sequence alignment data. However, due to the increasing throughput of sequencing platforms, BAM files have lost their efficiency, as we are dealing more frequently with deeper whole-genome sequencing (WGS), whole-exome sequencing (WES) and other large-scale genomics data.\n\nThe [CRAM file format](https://samtools.github.io/hts-specs/CRAMv3.pdf) is an  improved version of SAM format. Instead of storing DNA sequences aligned to a reference genome, it stores only the _differences_ between the sample and the genome sequence. With this feature, it [reduces dramatically the file sizes](http://www.htslib.org/benchmarks/CRAM.html). To use this new format, common tools, like [Samtools](http://samtools.github.io), require the original genome file (normally in FASTA format).\n\nOn another front, different computational tools have been developed to address different problems and they are often chained together in a pipeline to solve a bigger problem. For example, a basic variant caller workflow will:\n\n1. map the reads from a FASTQ file to a reference genome;\n2. use the allelic counts at each site to call a genotype.\n\nIt is often the case that each of the aforementioned steps will be performed by a different tool (like BWA + GATK). To assist on this matter (combining different tools, linking the output of one step to the input of the following), languages that are agnostic to the choice of platform were developed. These languages allow us to *describe* _tasks_ and combine them into _workflows_, using a dialect closer to the research domain and distant from the computer science domain. [Common Workflow Language (CWL)](https://www.commonwl.org), [Nextflow](https://www.nextflow.io) and [Workflow Description Language (WDL)](http://www.openwdl.org) are the most popular languages to define workflows for processing genomics data. It is important to highlight that these languages only describe what needs to be done.\n\nAs an actual user, one expects that the aforementioned tasks/workflows are *executed* to produce results of interest. For this reason, researchers developed _workflow execution systems_, which combine workflow files (`.cwl`, `.nf` or `.wdl`) with parameter files (`.json` of `.yaml`) and input data to generate a _dependency tree_ and actually _execute the processing tasks_.\nThe most popular workflow executors are: [Toil](https://toil.readthedocs.io/en/latest/) and [Cromwell](http://cromwell.readthedocs.io).\nThese tools are flexible enough to support different _computing backends_ such as local, cluster, custom environments and cloud.\n\nDistributing bioinformatics tools and keeping track of their versions between several workflows and computing nodes have become a big problem for both system administrators (system updates breaks user tools) and users (updated version o tool outputs _different results_, compromising the reproducibility of the research -- for more information see [FAIR Guiding Principles](https://www.nature.com/articles/sdata201618)).\nTo solve this problem the research community adopted container technologies, mainly [Singularity](https://sylabs.io/docs/) and [Docker](https://www.docker.com). Projects like [BioContainers](https://biocontainers.pro) provide thousands of container images of bioinformatics tools.\nAlso, [Docker Hub](https://hub.docker.com) is the main repository for popular open-source projects.\n\nBack to workflows... the workflow file + inputs file have been working well for small datasets. With our [wftools](https://github.com/labbcb/wftools) software, we can submit workflows (and their inputs) to execution services, such as Cromwell in server mode, check workflow executions status and logs, and collect output files copying them to desirable directory. The input JSON file must contain workflow-specific parameter to work, for example, a workflow that require FASTQ files as input we have to provide a _list of absolute paths to FASTQ files_ (two list if paired-end), other workflows may require several resource files that can be specific for different genome versions. It is important to note that Cromwell checks the file existence immediately prior to its use within the task, and this is associated with several downstream errors that force the system to abort the execution of the workflow. Additionally, the input JSON files are manually produced.\n\nEspresso-Caller is a tool that automates execution of workflows for identification of genomic variants from whole-exome and whole-genome datasets.\nIt collects raw sequencing paired-end FASTQ or raw gVCF files, together with resources files (__b37__ or __hg38__ versions), assessing their existence, to generate the JSON file that is used as input of workflows for processing WES/WGS data.\nOur software takes some conventions to automatically find the required files and to define sample metadata.\nInternally it submits WDL files to Cromwell server through [Cromwell API](https://cromwell.readthedocs.io/en/stable/api/RESTAPI/).\nThe next section introduces the workflows, next section provides examples of command lines and explanation of arguments, conventions and outputs.\nThe last section shows advanced uses of _espresso_\nThis document ends with the actual usage help.\n\n## Workflows\n\n_espresso_ provides three workflows: __hc__ (HaplotypeCalling) takes raw FASTQ files and their metadata as input plus resources files and generates per-sample raw gVCF alongside unmapped BAM and analysis-ready CRAM files. __joint__ (JointDiscovery) takes raw gVCF files and merge into a single unified VCF; __all__  executes all data processing steps: from raw FASTQs to unified VCF.\n\nOur tool bundles [in-house workflows](https://github.com/labbcb/workflows) and workflows defined by the Broad Institute as [GATK Best Practices](https://software.broadinstitute.org/gatk/) defined in WDL format.\nThe workflow files are stored inside the tool package.\nThe list below presents each workflow in the order that is executed by this tool.\n\n[HaplotypeCalling](espresso/workflows/haplotype-calling.wdl) runs the following workflows:\n\n1. [Convert raw paired-end FASTQ files to unmapped BAM format (uBAM)](espresso/workflows/paired-fastq-to-unmapped-bam.wdl)\n2. [Map sequences to reference genome, recalibrate alignments and generate analysis-ready BAM files](espresso/workflows/processing-for-variant-discovery-gatk4.wdl)\n3. [Produce sample-specific sufficient statistics to call variants](espresso/workflows/haplotypecaller-gvcf-gatk4.wdl)\n4. [Validate BAM files](espresso/workflows/validate-bam.wdl)\n5. [Convert BAM files to CRAM format](espresso/workflows/bam-to-cram.wdl)\n\n[JointDiscovery](espresso/workflows/joint-discovery-gatk4-local.wdl), we combine the sample-specific sufficient statistics to produce final genotype calls. For this step, we use the original WDL file produced by the Broad Institute.\n\nEspresso-Caller follows some _convention over configuration_ (where configuration is the inputs JSON file).\nYour data files must files the following conventions to work:\n\nFor __all__ and __hc__:\n\n- Partial FASTQ files were previously merged into a single one\n- Each sample is paired-end sequencing data divided in two FASTQ files by strand: forward and reverse  \n- FASTQ files are located at the same directory, one directory for each library name/batch\n- FASTQ file names matches this pattern: `(sample_name)_R?[12].fastq(.gz)?`\n- FASTQ sequence headers match this pattern: `@_:_:(sample_id):(flowcell):_:_:_:_:_:(primer)` which is merged as `sample_id.flowcell.primer`\n- Resource files, including reference genome files, are in the same directory, one directory for each version\n- Resource files have the same name from download URL\n- Destination directory does not exist or is empty, any conflicting file __will be overwritten__ silently\n- Relative paths are allowed in command arguments\n\nResource files can be downloaded using the following scripts.\nDownloaded files will have the right names to work with _espresso_.\n\n- [b37](download_resources_b37_files.sh)\n- [hg38](download_resources_hg38_files.sh)\n\nTo download resource files run.\nIt is important to inform the _absolute path_ to destination directory.\n\n```bash\nbash download_resources_b37_files.sh /home/data/ref/b37\nbash download_resources_hg38_files.sh /home/data/ref/hg38\n```\n\nThe following command will run _espresso_ with two directories containing raw FASTQ files.\nSamples files can be separated by directories to use different metadata for each group of samples (or batch).\nIf two `--fastq <fastq directory>` argument is defined you have to inform the following arguments twice.\n\n- `--library <library name>` Library (`LB` SAM tag)\n- `--date` Sequencing run date(`DT` SAM tag) - _Must be ISO8601 format (YYYY-MM-DD)!_\n- `--platform` Sequencing platform (`PL` SAM tag)\n- `--center` Sequencing center (`CN` SAM tag)\n\nAlso, sample name (`SM`), platform unit (`PU`) are extracted from FASTQ automatically by using predefined conventions.\n\nNext we have to inform the path to resources files (`--reference <resources directory>`) and the reference genome version (only __b37__ and __hg38__ are supported), `--version <genome version>`.\n\nSome computing environments do not support container technology.\nIn this case we have to inform the _absolute paths_ to this software.\nSee [installing required software script](install_software.sh).\n\n- `--gatk_path_override` path to __GATK version 4__, it must point to `gatk` wrapper script (not the Jar file).\n- `--gotc_path_override` path to directory containing all softwares (`bwa`, `picard.jar`, etc)\n- `--samtools_path_override` path to `samtools`.\n\nThe `--dont_run` flag does _espresso_ to __not submit workflows to Cromwell server__.\nIn this mode, the tool will check all required files, copy required workflow files and generate inputs JSON file writing both to destination directory.\nIt is very recommend that you run our tool in this mode and check JSON file _before_ running in production.\nAlso, it is useful when there are some change to do in the default JSON file and then submit workflow using _espresso_ instead.\n\nThe optional `--move` flag will tell _espresso_ to _move_ output files from Cromwell execution directory to destination directory.\nIt is useful for processing large-scale genomics datasets avoiding file duplication.\n\nThe last two arguments are required: callset name and destination directory to write all files.\n\n```bash\nespresso all \\\n\t--fastq ~/raw/batch1 \\\n\t--fastq ~/raw/batch2 \\\n\t--library Batch1 \\\n\t--library Batch2 \\\n\t--date 2018-10-19 \\\n\t--date 2019-02-07 \\\n\t--platform ILLUMINA \\\n\t--platform ILLUMINA \\\n\t--center MyCenter \\\n\t--center MyCenter \\\n\t--reference ~/ref/b37 \\\n\t--version b37 \\\n\t--gatk_path_override ~/bin/gatk-4.1.0.0/gatk \\\n\t--gotc_path_override ~/bin \\\n\t--samtools_path_override ~/bin/samtools-1.9/samtools \\\n\t--move \\\n\t--dont_run \\\n\tMyDataset.b37 \\\n\t~/res/my_dataset\n```\n\n\tStarting the haplotype-calling workflow with reference genome version b37\n\tWorkflow file: /home/data/res/my_dataset/haplotype-calling.wdl\n\tInputs JSON file: /home/data/res/my_dataset/haplotype-calling.b37.inputs.json\n\tWorkflow will not be submitted to Cromwell. See workflow files in /home/data/res/my_dataset\n\nIf you run without `--dont_run` argument you will see the text below.\nAs you can see, after workflow is submitted no output is presented until execution is finished.\nThen the tool will print the collected output files and exit.\n\n> In this version __all__ run __hc__ first then __joint__. TODO: write WDL file that do both\n\n\tStarting haplotype-calling workflow with reference genome version b37\n\tWorkflow file: /home/data/res/my_dataset/haplotype-calling.wdl\n\tInputs JSON file: /home/data/res/my_dataset/haplotype-calling.b37.inputs.json\n\tWorkflow submitted to Cromwell Server (http://localhost:8000)\n\tWorkflow id: 9977f400-d1b6-41ff-ab92-7ebbbf7a30a8\n\n## Expected outputs\n\nAt the end of execution _espresso_ will _collect output files_ copying them to destination directory.\nThese are the file you expect to see.\n\n- `{callset}.vcf.gz` is the _final unified VCF file_  where `callset` was previously defined (index `{callset}.vcf.gz.tbi`)\n- `haplotype-calling.wdl` is __hc__ workflow in WDL\n- `joint-discovery-gatk4-local.wdl` is __joint__ workflow in WDL\n- `haplotype-calling.{version}.inputs.json` is inputs for __hc__ where `version` is __b37__ or __hg38__\n- `joint-discovery.{version}.inputs.json` is inputs for __joint__\n- `out.intervals` is an intervals files used by __joint__ to call variants on all samples\n- `{callset}.variant_calling_detail_metrics` is the variant call metrics files\n- `{callset}.variant_calling_summary_metrics` summary variant calling metrics file\n\nFor each sample it should have.\n\n- `{sample}.{version}.g.vcf.gz` is raw gVCF file where `sample` was extracted from FASTQ file name (index `{sample}.{version}.g.vcf.gz.tbi`)\n- `{sample}.{version}.cram` is the analysis-ready CRAM file (contains both aligned sequences and sequences that weren't mapped at the end of file)\n- `{sample}.{version}.duplicate_metrics` is the duplication metrics file\n- `{sample}.{version}.validation_.txt` is the BAM validation output file\n- `{sample}.unmapped.bam` is the unmapped sequences in BAM format (exactly the same sequences from FASTQ but with recalibrated qualities)\n- `{sample}.{version}.recal_data.csv` is the file used to recalibrate PHRED qualities\n\n### Running __hc__ or __joint__ individually\n\nIt is possible to run only __hc__ by `espresso hc ...` and __joint__ by `espresso joint ...`.\nRunning __hc__ is useful when we don't want to generate the unified VCF for theses FASTQ files.\nThe __joint__ is useful when we already have raw gVCF files and we want to merge to or more data from batches or studies.\nHowever there are some conventions that your VCF files must follow (only if they weren't generated by _espresso_).\n\n- VCF file names must match this pattern: `(sample_name)(.version)?.g.vcf(.gz)?` (it must have `.g.vcf` extension to skip unified VCFs that may exist in the same directory).\n- Index files with the same name plus `.tbi` extension in the same directory.\n\n\n### Reproduce data processing\n\nWith WDL and JSON files written in the _result directory_ it is possible to re-execute data processing _without_ espresso.\nTo do that we need the Cromwell binary file.\nIt should also works on different workflow engine such as [miniwdl](https://github.com/chanzuckerberg/miniwdl) or workflow submission toolm such as [wf](https://github.com/labbcb/wf).\n\nRun workflow Cromwell in Local mode\n\n```bash\njava -jar cromwell.jar run \\\n\t-i /home/data/res/my_dataset/haplotype-calling.b37.inputs.json \\\n\t/home/data/res/my_dataset/haplotype-calling.wdl\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/labbcb/espresso-caller", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "espresso-caller", "package_url": "https://pypi.org/project/espresso-caller/", "platform": "", "project_url": "https://pypi.org/project/espresso-caller/", "project_urls": {"Homepage": "https://github.com/labbcb/espresso-caller"}, "release_url": "https://pypi.org/project/espresso-caller/1.2.2/", "requires_dist": null, "requires_python": "", "summary": "Automated and reproducible tool for identifying genomic variations at scale", "version": "1.2.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Automated and reproducible tool for identifying genomic variants at scale</h1>\n<p>Processing high-throughput sequencing data was the biggest challenge in the past, dealing with small datasets of large sample data.\nGenomics projects with thousands of samples have became popular due to decreasing in sequencing costs.\nNow we are facing a new challenge: how to process and store large-scale datasets efficiently.</p>\n<p>The <a href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\" rel=\"nofollow\">Sequence Alignment/Map Format Specification (SAM)</a> and its binary version (BAM) is the most popular and recommended file format to store sequence alignment data. However, due to the increasing throughput of sequencing platforms, BAM files have lost their efficiency, as we are dealing more frequently with deeper whole-genome sequencing (WGS), whole-exome sequencing (WES) and other large-scale genomics data.</p>\n<p>The <a href=\"https://samtools.github.io/hts-specs/CRAMv3.pdf\" rel=\"nofollow\">CRAM file format</a> is an  improved version of SAM format. Instead of storing DNA sequences aligned to a reference genome, it stores only the <em>differences</em> between the sample and the genome sequence. With this feature, it <a href=\"http://www.htslib.org/benchmarks/CRAM.html\" rel=\"nofollow\">reduces dramatically the file sizes</a>. To use this new format, common tools, like <a href=\"http://samtools.github.io\" rel=\"nofollow\">Samtools</a>, require the original genome file (normally in FASTA format).</p>\n<p>On another front, different computational tools have been developed to address different problems and they are often chained together in a pipeline to solve a bigger problem. For example, a basic variant caller workflow will:</p>\n<ol>\n<li>map the reads from a FASTQ file to a reference genome;</li>\n<li>use the allelic counts at each site to call a genotype.</li>\n</ol>\n<p>It is often the case that each of the aforementioned steps will be performed by a different tool (like BWA + GATK). To assist on this matter (combining different tools, linking the output of one step to the input of the following), languages that are agnostic to the choice of platform were developed. These languages allow us to <em>describe</em> <em>tasks</em> and combine them into <em>workflows</em>, using a dialect closer to the research domain and distant from the computer science domain. <a href=\"https://www.commonwl.org\" rel=\"nofollow\">Common Workflow Language (CWL)</a>, <a href=\"https://www.nextflow.io\" rel=\"nofollow\">Nextflow</a> and <a href=\"http://www.openwdl.org\" rel=\"nofollow\">Workflow Description Language (WDL)</a> are the most popular languages to define workflows for processing genomics data. It is important to highlight that these languages only describe what needs to be done.</p>\n<p>As an actual user, one expects that the aforementioned tasks/workflows are <em>executed</em> to produce results of interest. For this reason, researchers developed <em>workflow execution systems</em>, which combine workflow files (<code>.cwl</code>, <code>.nf</code> or <code>.wdl</code>) with parameter files (<code>.json</code> of <code>.yaml</code>) and input data to generate a <em>dependency tree</em> and actually <em>execute the processing tasks</em>.\nThe most popular workflow executors are: <a href=\"https://toil.readthedocs.io/en/latest/\" rel=\"nofollow\">Toil</a> and <a href=\"http://cromwell.readthedocs.io\" rel=\"nofollow\">Cromwell</a>.\nThese tools are flexible enough to support different <em>computing backends</em> such as local, cluster, custom environments and cloud.</p>\n<p>Distributing bioinformatics tools and keeping track of their versions between several workflows and computing nodes have become a big problem for both system administrators (system updates breaks user tools) and users (updated version o tool outputs <em>different results</em>, compromising the reproducibility of the research -- for more information see <a href=\"https://www.nature.com/articles/sdata201618\" rel=\"nofollow\">FAIR Guiding Principles</a>).\nTo solve this problem the research community adopted container technologies, mainly <a href=\"https://sylabs.io/docs/\" rel=\"nofollow\">Singularity</a> and <a href=\"https://www.docker.com\" rel=\"nofollow\">Docker</a>. Projects like <a href=\"https://biocontainers.pro\" rel=\"nofollow\">BioContainers</a> provide thousands of container images of bioinformatics tools.\nAlso, <a href=\"https://hub.docker.com\" rel=\"nofollow\">Docker Hub</a> is the main repository for popular open-source projects.</p>\n<p>Back to workflows... the workflow file + inputs file have been working well for small datasets. With our <a href=\"https://github.com/labbcb/wftools\" rel=\"nofollow\">wftools</a> software, we can submit workflows (and their inputs) to execution services, such as Cromwell in server mode, check workflow executions status and logs, and collect output files copying them to desirable directory. The input JSON file must contain workflow-specific parameter to work, for example, a workflow that require FASTQ files as input we have to provide a <em>list of absolute paths to FASTQ files</em> (two list if paired-end), other workflows may require several resource files that can be specific for different genome versions. It is important to note that Cromwell checks the file existence immediately prior to its use within the task, and this is associated with several downstream errors that force the system to abort the execution of the workflow. Additionally, the input JSON files are manually produced.</p>\n<p>Espresso-Caller is a tool that automates execution of workflows for identification of genomic variants from whole-exome and whole-genome datasets.\nIt collects raw sequencing paired-end FASTQ or raw gVCF files, together with resources files (<strong>b37</strong> or <strong>hg38</strong> versions), assessing their existence, to generate the JSON file that is used as input of workflows for processing WES/WGS data.\nOur software takes some conventions to automatically find the required files and to define sample metadata.\nInternally it submits WDL files to Cromwell server through <a href=\"https://cromwell.readthedocs.io/en/stable/api/RESTAPI/\" rel=\"nofollow\">Cromwell API</a>.\nThe next section introduces the workflows, next section provides examples of command lines and explanation of arguments, conventions and outputs.\nThe last section shows advanced uses of <em>espresso</em>\nThis document ends with the actual usage help.</p>\n<h2>Workflows</h2>\n<p><em>espresso</em> provides three workflows: <strong>hc</strong> (HaplotypeCalling) takes raw FASTQ files and their metadata as input plus resources files and generates per-sample raw gVCF alongside unmapped BAM and analysis-ready CRAM files. <strong>joint</strong> (JointDiscovery) takes raw gVCF files and merge into a single unified VCF; <strong>all</strong>  executes all data processing steps: from raw FASTQs to unified VCF.</p>\n<p>Our tool bundles <a href=\"https://github.com/labbcb/workflows\" rel=\"nofollow\">in-house workflows</a> and workflows defined by the Broad Institute as <a href=\"https://software.broadinstitute.org/gatk/\" rel=\"nofollow\">GATK Best Practices</a> defined in WDL format.\nThe workflow files are stored inside the tool package.\nThe list below presents each workflow in the order that is executed by this tool.</p>\n<p><a href=\"espresso/workflows/haplotype-calling.wdl\" rel=\"nofollow\">HaplotypeCalling</a> runs the following workflows:</p>\n<ol>\n<li><a href=\"espresso/workflows/paired-fastq-to-unmapped-bam.wdl\" rel=\"nofollow\">Convert raw paired-end FASTQ files to unmapped BAM format (uBAM)</a></li>\n<li><a href=\"espresso/workflows/processing-for-variant-discovery-gatk4.wdl\" rel=\"nofollow\">Map sequences to reference genome, recalibrate alignments and generate analysis-ready BAM files</a></li>\n<li><a href=\"espresso/workflows/haplotypecaller-gvcf-gatk4.wdl\" rel=\"nofollow\">Produce sample-specific sufficient statistics to call variants</a></li>\n<li><a href=\"espresso/workflows/validate-bam.wdl\" rel=\"nofollow\">Validate BAM files</a></li>\n<li><a href=\"espresso/workflows/bam-to-cram.wdl\" rel=\"nofollow\">Convert BAM files to CRAM format</a></li>\n</ol>\n<p><a href=\"espresso/workflows/joint-discovery-gatk4-local.wdl\" rel=\"nofollow\">JointDiscovery</a>, we combine the sample-specific sufficient statistics to produce final genotype calls. For this step, we use the original WDL file produced by the Broad Institute.</p>\n<p>Espresso-Caller follows some <em>convention over configuration</em> (where configuration is the inputs JSON file).\nYour data files must files the following conventions to work:</p>\n<p>For <strong>all</strong> and <strong>hc</strong>:</p>\n<ul>\n<li>Partial FASTQ files were previously merged into a single one</li>\n<li>Each sample is paired-end sequencing data divided in two FASTQ files by strand: forward and reverse</li>\n<li>FASTQ files are located at the same directory, one directory for each library name/batch</li>\n<li>FASTQ file names matches this pattern: <code>(sample_name)_R?[12].fastq(.gz)?</code></li>\n<li>FASTQ sequence headers match this pattern: <code>@_:_:(sample_id):(flowcell):_:_:_:_:_:(primer)</code> which is merged as <code>sample_id.flowcell.primer</code></li>\n<li>Resource files, including reference genome files, are in the same directory, one directory for each version</li>\n<li>Resource files have the same name from download URL</li>\n<li>Destination directory does not exist or is empty, any conflicting file <strong>will be overwritten</strong> silently</li>\n<li>Relative paths are allowed in command arguments</li>\n</ul>\n<p>Resource files can be downloaded using the following scripts.\nDownloaded files will have the right names to work with <em>espresso</em>.</p>\n<ul>\n<li><a href=\"download_resources_b37_files.sh\" rel=\"nofollow\">b37</a></li>\n<li><a href=\"download_resources_hg38_files.sh\" rel=\"nofollow\">hg38</a></li>\n</ul>\n<p>To download resource files run.\nIt is important to inform the <em>absolute path</em> to destination directory.</p>\n<pre>bash download_resources_b37_files.sh /home/data/ref/b37\nbash download_resources_hg38_files.sh /home/data/ref/hg38\n</pre>\n<p>The following command will run <em>espresso</em> with two directories containing raw FASTQ files.\nSamples files can be separated by directories to use different metadata for each group of samples (or batch).\nIf two <code>--fastq &lt;fastq directory&gt;</code> argument is defined you have to inform the following arguments twice.</p>\n<ul>\n<li><code>--library &lt;library name&gt;</code> Library (<code>LB</code> SAM tag)</li>\n<li><code>--date</code> Sequencing run date(<code>DT</code> SAM tag) - <em>Must be ISO8601 format (YYYY-MM-DD)!</em></li>\n<li><code>--platform</code> Sequencing platform (<code>PL</code> SAM tag)</li>\n<li><code>--center</code> Sequencing center (<code>CN</code> SAM tag)</li>\n</ul>\n<p>Also, sample name (<code>SM</code>), platform unit (<code>PU</code>) are extracted from FASTQ automatically by using predefined conventions.</p>\n<p>Next we have to inform the path to resources files (<code>--reference &lt;resources directory&gt;</code>) and the reference genome version (only <strong>b37</strong> and <strong>hg38</strong> are supported), <code>--version &lt;genome version&gt;</code>.</p>\n<p>Some computing environments do not support container technology.\nIn this case we have to inform the <em>absolute paths</em> to this software.\nSee <a href=\"install_software.sh\" rel=\"nofollow\">installing required software script</a>.</p>\n<ul>\n<li><code>--gatk_path_override</code> path to <strong>GATK version 4</strong>, it must point to <code>gatk</code> wrapper script (not the Jar file).</li>\n<li><code>--gotc_path_override</code> path to directory containing all softwares (<code>bwa</code>, <code>picard.jar</code>, etc)</li>\n<li><code>--samtools_path_override</code> path to <code>samtools</code>.</li>\n</ul>\n<p>The <code>--dont_run</code> flag does <em>espresso</em> to <strong>not submit workflows to Cromwell server</strong>.\nIn this mode, the tool will check all required files, copy required workflow files and generate inputs JSON file writing both to destination directory.\nIt is very recommend that you run our tool in this mode and check JSON file <em>before</em> running in production.\nAlso, it is useful when there are some change to do in the default JSON file and then submit workflow using <em>espresso</em> instead.</p>\n<p>The optional <code>--move</code> flag will tell <em>espresso</em> to <em>move</em> output files from Cromwell execution directory to destination directory.\nIt is useful for processing large-scale genomics datasets avoiding file duplication.</p>\n<p>The last two arguments are required: callset name and destination directory to write all files.</p>\n<pre>espresso all <span class=\"se\">\\</span>\n\t--fastq ~/raw/batch1 <span class=\"se\">\\</span>\n\t--fastq ~/raw/batch2 <span class=\"se\">\\</span>\n\t--library Batch1 <span class=\"se\">\\</span>\n\t--library Batch2 <span class=\"se\">\\</span>\n\t--date <span class=\"m\">2018</span>-10-19 <span class=\"se\">\\</span>\n\t--date <span class=\"m\">2019</span>-02-07 <span class=\"se\">\\</span>\n\t--platform ILLUMINA <span class=\"se\">\\</span>\n\t--platform ILLUMINA <span class=\"se\">\\</span>\n\t--center MyCenter <span class=\"se\">\\</span>\n\t--center MyCenter <span class=\"se\">\\</span>\n\t--reference ~/ref/b37 <span class=\"se\">\\</span>\n\t--version b37 <span class=\"se\">\\</span>\n\t--gatk_path_override ~/bin/gatk-4.1.0.0/gatk <span class=\"se\">\\</span>\n\t--gotc_path_override ~/bin <span class=\"se\">\\</span>\n\t--samtools_path_override ~/bin/samtools-1.9/samtools <span class=\"se\">\\</span>\n\t--move <span class=\"se\">\\</span>\n\t--dont_run <span class=\"se\">\\</span>\n\tMyDataset.b37 <span class=\"se\">\\</span>\n\t~/res/my_dataset\n</pre>\n<pre><code>Starting the haplotype-calling workflow with reference genome version b37\nWorkflow file: /home/data/res/my_dataset/haplotype-calling.wdl\nInputs JSON file: /home/data/res/my_dataset/haplotype-calling.b37.inputs.json\nWorkflow will not be submitted to Cromwell. See workflow files in /home/data/res/my_dataset\n</code></pre>\n<p>If you run without <code>--dont_run</code> argument you will see the text below.\nAs you can see, after workflow is submitted no output is presented until execution is finished.\nThen the tool will print the collected output files and exit.</p>\n<blockquote>\n<p>In this version <strong>all</strong> run <strong>hc</strong> first then <strong>joint</strong>. TODO: write WDL file that do both</p>\n</blockquote>\n<pre><code>Starting haplotype-calling workflow with reference genome version b37\nWorkflow file: /home/data/res/my_dataset/haplotype-calling.wdl\nInputs JSON file: /home/data/res/my_dataset/haplotype-calling.b37.inputs.json\nWorkflow submitted to Cromwell Server (http://localhost:8000)\nWorkflow id: 9977f400-d1b6-41ff-ab92-7ebbbf7a30a8\n</code></pre>\n<h2>Expected outputs</h2>\n<p>At the end of execution <em>espresso</em> will <em>collect output files</em> copying them to destination directory.\nThese are the file you expect to see.</p>\n<ul>\n<li><code>{callset}.vcf.gz</code> is the <em>final unified VCF file</em>  where <code>callset</code> was previously defined (index <code>{callset}.vcf.gz.tbi</code>)</li>\n<li><code>haplotype-calling.wdl</code> is <strong>hc</strong> workflow in WDL</li>\n<li><code>joint-discovery-gatk4-local.wdl</code> is <strong>joint</strong> workflow in WDL</li>\n<li><code>haplotype-calling.{version}.inputs.json</code> is inputs for <strong>hc</strong> where <code>version</code> is <strong>b37</strong> or <strong>hg38</strong></li>\n<li><code>joint-discovery.{version}.inputs.json</code> is inputs for <strong>joint</strong></li>\n<li><code>out.intervals</code> is an intervals files used by <strong>joint</strong> to call variants on all samples</li>\n<li><code>{callset}.variant_calling_detail_metrics</code> is the variant call metrics files</li>\n<li><code>{callset}.variant_calling_summary_metrics</code> summary variant calling metrics file</li>\n</ul>\n<p>For each sample it should have.</p>\n<ul>\n<li><code>{sample}.{version}.g.vcf.gz</code> is raw gVCF file where <code>sample</code> was extracted from FASTQ file name (index <code>{sample}.{version}.g.vcf.gz.tbi</code>)</li>\n<li><code>{sample}.{version}.cram</code> is the analysis-ready CRAM file (contains both aligned sequences and sequences that weren't mapped at the end of file)</li>\n<li><code>{sample}.{version}.duplicate_metrics</code> is the duplication metrics file</li>\n<li><code>{sample}.{version}.validation_.txt</code> is the BAM validation output file</li>\n<li><code>{sample}.unmapped.bam</code> is the unmapped sequences in BAM format (exactly the same sequences from FASTQ but with recalibrated qualities)</li>\n<li><code>{sample}.{version}.recal_data.csv</code> is the file used to recalibrate PHRED qualities</li>\n</ul>\n<h3>Running <strong>hc</strong> or <strong>joint</strong> individually</h3>\n<p>It is possible to run only <strong>hc</strong> by <code>espresso hc ...</code> and <strong>joint</strong> by <code>espresso joint ...</code>.\nRunning <strong>hc</strong> is useful when we don't want to generate the unified VCF for theses FASTQ files.\nThe <strong>joint</strong> is useful when we already have raw gVCF files and we want to merge to or more data from batches or studies.\nHowever there are some conventions that your VCF files must follow (only if they weren't generated by <em>espresso</em>).</p>\n<ul>\n<li>VCF file names must match this pattern: <code>(sample_name)(.version)?.g.vcf(.gz)?</code> (it must have <code>.g.vcf</code> extension to skip unified VCFs that may exist in the same directory).</li>\n<li>Index files with the same name plus <code>.tbi</code> extension in the same directory.</li>\n</ul>\n<h3>Reproduce data processing</h3>\n<p>With WDL and JSON files written in the <em>result directory</em> it is possible to re-execute data processing <em>without</em> espresso.\nTo do that we need the Cromwell binary file.\nIt should also works on different workflow engine such as <a href=\"https://github.com/chanzuckerberg/miniwdl\" rel=\"nofollow\">miniwdl</a> or workflow submission toolm such as <a href=\"https://github.com/labbcb/wf\" rel=\"nofollow\">wf</a>.</p>\n<p>Run workflow Cromwell in Local mode</p>\n<pre>java -jar cromwell.jar run <span class=\"se\">\\</span>\n\t-i /home/data/res/my_dataset/haplotype-calling.b37.inputs.json <span class=\"se\">\\</span>\n\t/home/data/res/my_dataset/haplotype-calling.wdl\n</pre>\n\n          </div>"}, "last_serial": 6649205, "releases": {"1.2.2": [{"comment_text": "", "digests": {"md5": "2c9c2b8733792c3edaa7674e5018e57c", "sha256": "6fb9451fef4b95fe825067b836c9e8c858b97a8f9591ec9d1aeb6c89f85f5cae"}, "downloads": -1, "filename": "espresso_caller-1.2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "2c9c2b8733792c3edaa7674e5018e57c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 45698, "upload_time": "2020-02-17T22:09:34", "upload_time_iso_8601": "2020-02-17T22:09:34.720360Z", "url": "https://files.pythonhosted.org/packages/c4/35/180b76627ed5cd65797cd005c04939c4fc6d2c1225a9870f0ff0c5e40f52/espresso_caller-1.2.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fe063fbeeef2fe73efc98750f5c925a2", "sha256": "348af1da2ecf4a289c455a45f9ea426221509464d758c8a1a3a2b82abd0cca58"}, "downloads": -1, "filename": "espresso-caller-1.2.2.tar.gz", "has_sig": false, "md5_digest": "fe063fbeeef2fe73efc98750f5c925a2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 38530, "upload_time": "2020-02-17T22:09:38", "upload_time_iso_8601": "2020-02-17T22:09:38.534903Z", "url": "https://files.pythonhosted.org/packages/e2/4a/6ad5295a4c1588ebe933fae14cd47e4bf027a7d50ae8376bde1f3bbdf2d8/espresso-caller-1.2.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2c9c2b8733792c3edaa7674e5018e57c", "sha256": "6fb9451fef4b95fe825067b836c9e8c858b97a8f9591ec9d1aeb6c89f85f5cae"}, "downloads": -1, "filename": "espresso_caller-1.2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "2c9c2b8733792c3edaa7674e5018e57c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 45698, "upload_time": "2020-02-17T22:09:34", "upload_time_iso_8601": "2020-02-17T22:09:34.720360Z", "url": "https://files.pythonhosted.org/packages/c4/35/180b76627ed5cd65797cd005c04939c4fc6d2c1225a9870f0ff0c5e40f52/espresso_caller-1.2.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fe063fbeeef2fe73efc98750f5c925a2", "sha256": "348af1da2ecf4a289c455a45f9ea426221509464d758c8a1a3a2b82abd0cca58"}, "downloads": -1, "filename": "espresso-caller-1.2.2.tar.gz", "has_sig": false, "md5_digest": "fe063fbeeef2fe73efc98750f5c925a2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 38530, "upload_time": "2020-02-17T22:09:38", "upload_time_iso_8601": "2020-02-17T22:09:38.534903Z", "url": "https://files.pythonhosted.org/packages/e2/4a/6ad5295a4c1588ebe933fae14cd47e4bf027a7d50ae8376bde1f3bbdf2d8/espresso-caller-1.2.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:45:37 2020"}