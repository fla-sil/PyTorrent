{"info": {"author": "Fishtown Analytics", "author_email": "info@fishtownanalytics.com", "bugtrack_url": null, "classifiers": [], "description": "## dbt-spark\n\n### Documentation\nFor more information on using Spark with dbt, consult the dbt documentation:\n- [Spark profile](https://docs.getdbt.com/docs/profile-spark)\n- [Spark specific configs](https://docs.getdbt.com/docs/spark-configs)\n\n### Installation\nThis plugin can be installed via pip:\n\n```\n# Install dbt-spark from PyPi:\n$ pip install dbt-spark\n```\n\n### Configuring your profile\n\n**Connection Method**\n\nConnections can be made to Spark in two different modes. The `http` mode is used when connecting to a managed service such as Databricks, which provides an HTTP endpoint; the `thrift` mode is used to connect directly to the master node of a cluster (either on-premise or in the cloud).\n\nA dbt profile can be configured to run against Spark using the following configuration:\n\n| Option  | Description                                        | Required?               | Example                  |\n|---------|----------------------------------------------------|-------------------------|--------------------------|\n| method    | Specify the connection method (`thrift` or `http`)   | Required   | `http`   |\n| schema  | Specify the schema (database) to build models into | Required                | `analytics`              |\n| host    | The hostname to connect to                         | Required                | `yourorg.sparkhost.com`  |\n| port    | The port to connect to the host on                 | Optional (default: 443 for `http`, 10001 for `thrift`) | `443`                    |\n| token   | The token to use for authenticating to the cluster | Required for `http`                | `abc123`                 |\n| organization | The id of the Azure Databricks workspace being used; only for  Azure Databricks | See Databricks Note | `1234567891234567` |\n| cluster | The name of the cluster to connect to              | Required for `http`               | `01234-23423-coffeetime` |\n| user    | The username to use to connect to the cluster  | Optional  | `hadoop`  |\n| connect_timeout | The number of seconds to wait before retrying to connect to a Pending Spark cluster | Optional (default: 10) | `60` |\n| connect_retries | The number of times to try connecting to a Pending Spark cluster before giving up   | Optional (default: 0)  | `5` |\n\n**Databricks Note**\n\nAWS and Azure Databricks have differences in their connections, likely due to differences in how their URLs are generated between the two services.\n\n**Organization:** To connect to an Azure Databricks cluster, you will need to obtain your organization ID, which is a unique ID Azure Databricks generates for each customer workspace.  To find the organization ID, see https://docs.microsoft.com/en-us/azure/databricks/dev-tools/databricks-connect#step-2-configure-connection-properties. This is a string field; if there is a leading zero, be sure to include it.\n\n**Port:** Please ignore all references to port 15001 in the databricks-connect docs as that is specific to that tool; port 443 is used for dbt-spark's https connection.\n\n**Host:** The host field for Databricks can be found at the start of your workspace or cluster url: `region.azuredatabricks.net` for Azure, or `account.cloud.databricks.com` for AWS. Do not include `https://`.\n\n**Usage with Amazon EMR**\n\nTo connect to Spark running on an Amazon EMR cluster, you will need to run `sudo /usr/lib/spark/sbin/start-thriftserver.sh` on the master node of the cluster to start the Thrift server (see https://aws.amazon.com/premiumsupport/knowledge-center/jdbc-connection-emr/ for further context). You will also need to connect to port `10001`, which will connect to the Spark backend Thrift server; port `10000` will instead connect to a Hive backend, which will not work correctly with dbt.\n\n\n**Example profiles.yml entries:**\n\n**http, e.g. Databricks**\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      method: http\n      type: spark\n      schema: analytics\n      host: yourorg.sparkhost.com\n      organization: 1234567891234567    # Azure Databricks ONLY\n      port: 443\n      token: abc123\n      cluster: 01234-23423-coffeetime\n      connect_retries: 5\n      connect_timeout: 60\n```\n\n**Thrift connection**\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      method: thrift\n      type: spark\n      schema: analytics\n      host: 127.0.0.1\n      port: 10001\n      user: hadoop\n      connect_retries: 5\n      connect_timeout: 60\n```\n\n\n\n### Usage Notes\n\n**Model Configuration**\n\nThe following configurations can be supplied to models run with the dbt-spark plugin:\n\n\n| Option  | Description                                        | Required?               | Example                  |\n|---------|----------------------------------------------------|-------------------------|--------------------------|\n| file_format | The file format to use when creating tables (`parquet`, `delta`, `csv`, `json`, `text`, `jdbc`, `orc`, `hive` or `libsvm`). | Optional | `parquet`|\n| location_root  | The created table uses the specified directory to store its data. The table alias is appended to it. | Optional                | `/mnt/root`              |\n| partition_by  | Partition the created table by the specified columns. A directory is created for each partition. | Optional                | `partition_1`              |\n| clustered_by  | Each partition in the created table will be split into a fixed number of buckets by the specified columns. | Optional               | `cluster_1`              |\n| buckets  | The number of buckets to create while clustering | Required if `clustered_by` is specified                | `8`              |\n| incremental_strategy | The strategy to use for incremental models (`insert_overwrite` or `merge`). Note `merge` requires `file_format` = `delta` and `unique_key` to be specified. | Optional (default: `insert_overwrite`) | `merge` |\n| persist_docs | Whether dbt should include the model description as a table `comment` | Optional | `{'relation': true}` |\n\n\n**Incremental Models**\n\nTo use incremental models, specify a `partition_by` clause in your model config. The default incremental strategy used is `insert_overwrite`, which will overwrite the partitions included in your query. Be sure to re-select _all_ of the relevant\ndata for a partition when using the `insert_overwrite` strategy.\n\n```\n{{ config(\n    materialized='incremental',\n    partition_by=['date_day'],\n    file_format='parquet'\n) }}\n\n/*\n  Every partition returned by this query will be overwritten\n  when this model runs\n*/\n\nselect\n    date_day,\n    count(*) as users\n\nfrom {{ ref('events') }}\nwhere date_day::date >= '2019-01-01'\ngroup by 1\n```\n\nThe `merge` strategy is only supported when using file_format `delta` (supported in Databricks). It also requires you to specify a `unique key` to match existing records.\n\n```\n{{ config(\n    materialized='incremental',\n    incremental_strategy='merge',\n    partition_by=['date_day'],\n    file_format='delta'\n) }}\n\nselect *\nfrom {{ ref('events') }}\n{% if is_incremental() %}\n  where date_day > (select max(date_day) from {{ this }})\n{% endif %}\n```\n\n### Running locally\n\nA `docker-compose` environment starts a Spark Thrift server and a Postgres database as a Hive Metastore backend.\n\n```\ndocker-compose up\n```\n\nYour profile should look like this:\n\n```\nyour_profile_name:\n  target: local\n  outputs:\n    local:\n      method: thrift\n      type: spark\n      schema: analytics\n      host: 127.0.0.1\n      port: 10000\n      user: dbt\n      connect_retries: 5\n      connect_timeout: 60\n```\n\nConnecting to the local spark instance:\n\n* The Spark UI should be available at [http://localhost:4040/sqlserver/](http://localhost:4040/sqlserver/)\n* The endpoint for SQL-based testing is at `http://localhost:10000` and can be referenced with the Hive or Spark JDBC drivers using connection string `jdbc:hive2://localhost:10000` and default credentials `dbt`:`dbt`\n\nNote that the Hive metastore data is persisted under `./.hive-metastore/`, and the Spark-produced data under `./.spark-warehouse/`. To completely reset you environment run the following:\n\n```\ndocker-compose down\nrm -rf ./.hive-metastore/\nrm -rf ./.spark-warehouse/\n```\n\n### Reporting bugs and contributing code\n\n-   Want to report a bug or request a feature? Let us know on [Slack](http://slack.getdbt.com/), or open [an issue](https://github.com/fishtown-analytics/dbt-spark/issues/new).\n\n## Code of Conduct\n\nEveryone interacting in the dbt project's codebases, issue trackers, chat rooms, and mailing lists is expected to follow the [PyPA Code of Conduct](https://www.pypa.io/en/latest/code-of-conduct/).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/fishtown-analytics/dbt-spark", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "dbt-spark", "package_url": "https://pypi.org/project/dbt-spark/", "platform": "", "project_url": "https://pypi.org/project/dbt-spark/", "project_urls": {"Homepage": "https://github.com/fishtown-analytics/dbt-spark"}, "release_url": "https://pypi.org/project/dbt-spark/0.16.1/", "requires_dist": ["dbt-core (==0.16.1)", "PyHive[hive] (<0.7.0,>=0.6.0)", "thrift (<0.12.0,>=0.11.0)"], "requires_python": "", "summary": "The SparkSQL plugin for dbt (data build tool)", "version": "0.16.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h2>dbt-spark</h2>\n<h3>Documentation</h3>\n<p>For more information on using Spark with dbt, consult the dbt documentation:</p>\n<ul>\n<li><a href=\"https://docs.getdbt.com/docs/profile-spark\" rel=\"nofollow\">Spark profile</a></li>\n<li><a href=\"https://docs.getdbt.com/docs/spark-configs\" rel=\"nofollow\">Spark specific configs</a></li>\n</ul>\n<h3>Installation</h3>\n<p>This plugin can be installed via pip:</p>\n<pre><code># Install dbt-spark from PyPi:\n$ pip install dbt-spark\n</code></pre>\n<h3>Configuring your profile</h3>\n<p><strong>Connection Method</strong></p>\n<p>Connections can be made to Spark in two different modes. The <code>http</code> mode is used when connecting to a managed service such as Databricks, which provides an HTTP endpoint; the <code>thrift</code> mode is used to connect directly to the master node of a cluster (either on-premise or in the cloud).</p>\n<p>A dbt profile can be configured to run against Spark using the following configuration:</p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Description</th>\n<th>Required?</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>method</td>\n<td>Specify the connection method (<code>thrift</code> or <code>http</code>)</td>\n<td>Required</td>\n<td><code>http</code></td>\n</tr>\n<tr>\n<td>schema</td>\n<td>Specify the schema (database) to build models into</td>\n<td>Required</td>\n<td><code>analytics</code></td>\n</tr>\n<tr>\n<td>host</td>\n<td>The hostname to connect to</td>\n<td>Required</td>\n<td><code>yourorg.sparkhost.com</code></td>\n</tr>\n<tr>\n<td>port</td>\n<td>The port to connect to the host on</td>\n<td>Optional (default: 443 for <code>http</code>, 10001 for <code>thrift</code>)</td>\n<td><code>443</code></td>\n</tr>\n<tr>\n<td>token</td>\n<td>The token to use for authenticating to the cluster</td>\n<td>Required for <code>http</code></td>\n<td><code>abc123</code></td>\n</tr>\n<tr>\n<td>organization</td>\n<td>The id of the Azure Databricks workspace being used; only for  Azure Databricks</td>\n<td>See Databricks Note</td>\n<td><code>1234567891234567</code></td>\n</tr>\n<tr>\n<td>cluster</td>\n<td>The name of the cluster to connect to</td>\n<td>Required for <code>http</code></td>\n<td><code>01234-23423-coffeetime</code></td>\n</tr>\n<tr>\n<td>user</td>\n<td>The username to use to connect to the cluster</td>\n<td>Optional</td>\n<td><code>hadoop</code></td>\n</tr>\n<tr>\n<td>connect_timeout</td>\n<td>The number of seconds to wait before retrying to connect to a Pending Spark cluster</td>\n<td>Optional (default: 10)</td>\n<td><code>60</code></td>\n</tr>\n<tr>\n<td>connect_retries</td>\n<td>The number of times to try connecting to a Pending Spark cluster before giving up</td>\n<td>Optional (default: 0)</td>\n<td><code>5</code></td>\n</tr></tbody></table>\n<p><strong>Databricks Note</strong></p>\n<p>AWS and Azure Databricks have differences in their connections, likely due to differences in how their URLs are generated between the two services.</p>\n<p><strong>Organization:</strong> To connect to an Azure Databricks cluster, you will need to obtain your organization ID, which is a unique ID Azure Databricks generates for each customer workspace.  To find the organization ID, see <a href=\"https://docs.microsoft.com/en-us/azure/databricks/dev-tools/databricks-connect#step-2-configure-connection-properties\" rel=\"nofollow\">https://docs.microsoft.com/en-us/azure/databricks/dev-tools/databricks-connect#step-2-configure-connection-properties</a>. This is a string field; if there is a leading zero, be sure to include it.</p>\n<p><strong>Port:</strong> Please ignore all references to port 15001 in the databricks-connect docs as that is specific to that tool; port 443 is used for dbt-spark's https connection.</p>\n<p><strong>Host:</strong> The host field for Databricks can be found at the start of your workspace or cluster url: <code>region.azuredatabricks.net</code> for Azure, or <code>account.cloud.databricks.com</code> for AWS. Do not include <code>https://</code>.</p>\n<p><strong>Usage with Amazon EMR</strong></p>\n<p>To connect to Spark running on an Amazon EMR cluster, you will need to run <code>sudo /usr/lib/spark/sbin/start-thriftserver.sh</code> on the master node of the cluster to start the Thrift server (see <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/jdbc-connection-emr/\" rel=\"nofollow\">https://aws.amazon.com/premiumsupport/knowledge-center/jdbc-connection-emr/</a> for further context). You will also need to connect to port <code>10001</code>, which will connect to the Spark backend Thrift server; port <code>10000</code> will instead connect to a Hive backend, which will not work correctly with dbt.</p>\n<p><strong>Example profiles.yml entries:</strong></p>\n<p><strong>http, e.g. Databricks</strong></p>\n<pre><code>your_profile_name:\n  target: dev\n  outputs:\n    dev:\n      method: http\n      type: spark\n      schema: analytics\n      host: yourorg.sparkhost.com\n      organization: 1234567891234567    # Azure Databricks ONLY\n      port: 443\n      token: abc123\n      cluster: 01234-23423-coffeetime\n      connect_retries: 5\n      connect_timeout: 60\n</code></pre>\n<p><strong>Thrift connection</strong></p>\n<pre><code>your_profile_name:\n  target: dev\n  outputs:\n    dev:\n      method: thrift\n      type: spark\n      schema: analytics\n      host: 127.0.0.1\n      port: 10001\n      user: hadoop\n      connect_retries: 5\n      connect_timeout: 60\n</code></pre>\n<h3>Usage Notes</h3>\n<p><strong>Model Configuration</strong></p>\n<p>The following configurations can be supplied to models run with the dbt-spark plugin:</p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Description</th>\n<th>Required?</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>file_format</td>\n<td>The file format to use when creating tables (<code>parquet</code>, <code>delta</code>, <code>csv</code>, <code>json</code>, <code>text</code>, <code>jdbc</code>, <code>orc</code>, <code>hive</code> or <code>libsvm</code>).</td>\n<td>Optional</td>\n<td><code>parquet</code></td>\n</tr>\n<tr>\n<td>location_root</td>\n<td>The created table uses the specified directory to store its data. The table alias is appended to it.</td>\n<td>Optional</td>\n<td><code>/mnt/root</code></td>\n</tr>\n<tr>\n<td>partition_by</td>\n<td>Partition the created table by the specified columns. A directory is created for each partition.</td>\n<td>Optional</td>\n<td><code>partition_1</code></td>\n</tr>\n<tr>\n<td>clustered_by</td>\n<td>Each partition in the created table will be split into a fixed number of buckets by the specified columns.</td>\n<td>Optional</td>\n<td><code>cluster_1</code></td>\n</tr>\n<tr>\n<td>buckets</td>\n<td>The number of buckets to create while clustering</td>\n<td>Required if <code>clustered_by</code> is specified</td>\n<td><code>8</code></td>\n</tr>\n<tr>\n<td>incremental_strategy</td>\n<td>The strategy to use for incremental models (<code>insert_overwrite</code> or <code>merge</code>). Note <code>merge</code> requires <code>file_format</code> = <code>delta</code> and <code>unique_key</code> to be specified.</td>\n<td>Optional (default: <code>insert_overwrite</code>)</td>\n<td><code>merge</code></td>\n</tr>\n<tr>\n<td>persist_docs</td>\n<td>Whether dbt should include the model description as a table <code>comment</code></td>\n<td>Optional</td>\n<td><code>{'relation': true}</code></td>\n</tr></tbody></table>\n<p><strong>Incremental Models</strong></p>\n<p>To use incremental models, specify a <code>partition_by</code> clause in your model config. The default incremental strategy used is <code>insert_overwrite</code>, which will overwrite the partitions included in your query. Be sure to re-select <em>all</em> of the relevant\ndata for a partition when using the <code>insert_overwrite</code> strategy.</p>\n<pre><code>{{ config(\n    materialized='incremental',\n    partition_by=['date_day'],\n    file_format='parquet'\n) }}\n\n/*\n  Every partition returned by this query will be overwritten\n  when this model runs\n*/\n\nselect\n    date_day,\n    count(*) as users\n\nfrom {{ ref('events') }}\nwhere date_day::date &gt;= '2019-01-01'\ngroup by 1\n</code></pre>\n<p>The <code>merge</code> strategy is only supported when using file_format <code>delta</code> (supported in Databricks). It also requires you to specify a <code>unique key</code> to match existing records.</p>\n<pre><code>{{ config(\n    materialized='incremental',\n    incremental_strategy='merge',\n    partition_by=['date_day'],\n    file_format='delta'\n) }}\n\nselect *\nfrom {{ ref('events') }}\n{% if is_incremental() %}\n  where date_day &gt; (select max(date_day) from {{ this }})\n{% endif %}\n</code></pre>\n<h3>Running locally</h3>\n<p>A <code>docker-compose</code> environment starts a Spark Thrift server and a Postgres database as a Hive Metastore backend.</p>\n<pre><code>docker-compose up\n</code></pre>\n<p>Your profile should look like this:</p>\n<pre><code>your_profile_name:\n  target: local\n  outputs:\n    local:\n      method: thrift\n      type: spark\n      schema: analytics\n      host: 127.0.0.1\n      port: 10000\n      user: dbt\n      connect_retries: 5\n      connect_timeout: 60\n</code></pre>\n<p>Connecting to the local spark instance:</p>\n<ul>\n<li>The Spark UI should be available at <a href=\"http://localhost:4040/sqlserver/\" rel=\"nofollow\">http://localhost:4040/sqlserver/</a></li>\n<li>The endpoint for SQL-based testing is at <code>http://localhost:10000</code> and can be referenced with the Hive or Spark JDBC drivers using connection string <code>jdbc:hive2://localhost:10000</code> and default credentials <code>dbt</code>:<code>dbt</code></li>\n</ul>\n<p>Note that the Hive metastore data is persisted under <code>./.hive-metastore/</code>, and the Spark-produced data under <code>./.spark-warehouse/</code>. To completely reset you environment run the following:</p>\n<pre><code>docker-compose down\nrm -rf ./.hive-metastore/\nrm -rf ./.spark-warehouse/\n</code></pre>\n<h3>Reporting bugs and contributing code</h3>\n<ul>\n<li>Want to report a bug or request a feature? Let us know on <a href=\"http://slack.getdbt.com/\" rel=\"nofollow\">Slack</a>, or open <a href=\"https://github.com/fishtown-analytics/dbt-spark/issues/new\" rel=\"nofollow\">an issue</a>.</li>\n</ul>\n<h2>Code of Conduct</h2>\n<p>Everyone interacting in the dbt project's codebases, issue trackers, chat rooms, and mailing lists is expected to follow the <a href=\"https://www.pypa.io/en/latest/code-of-conduct/\" rel=\"nofollow\">PyPA Code of Conduct</a>.</p>\n\n          </div>"}, "last_serial": 7115404, "releases": {"0.13.0": [{"comment_text": "", "digests": {"md5": "b4985cce5174703043df23a701f7cce3", "sha256": "65d8d9ccfd5185cfaba1652bb732d69e25eda12dbafcdb67943615d3255e6242"}, "downloads": -1, "filename": "dbt_spark-0.13.0-py3.7.egg", "has_sig": false, "md5_digest": "b4985cce5174703043df23a701f7cce3", "packagetype": "bdist_egg", "python_version": "3.7", "requires_python": null, "size": 26236, "upload_time": "2019-07-03T17:12:07", "upload_time_iso_8601": "2019-07-03T17:12:07.076459Z", "url": "https://files.pythonhosted.org/packages/6a/79/686f13b7bfa55ff80abc40c3db0a61f59fafba6c17e9b8fcebb153eed6bf/dbt_spark-0.13.0-py3.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "aba7d7199a6f4f76fcc8c0933cbc5a4d", "sha256": "d0c3255edadec5a2d423ca7fd20a4d2b0ba45c75fc0b73b554121a98f74c72c6"}, "downloads": -1, "filename": "dbt-spark-0.13.0.tar.gz", "has_sig": false, "md5_digest": "aba7d7199a6f4f76fcc8c0933cbc5a4d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12799, "upload_time": "2019-07-03T17:12:04", "upload_time_iso_8601": "2019-07-03T17:12:04.935194Z", "url": "https://files.pythonhosted.org/packages/bb/37/fe34166ef27c5d71022ae27ec2445c8c0227b3f17bd5999e5893e6012ca8/dbt-spark-0.13.0.tar.gz", "yanked": false}], "0.14.3": [{"comment_text": "", "digests": {"md5": "2086bf50c5cfd9f85a84f1e4b4eb0aa4", "sha256": "019ee2f1d81831f74d7c78ff151811811ac897b277cf8b7617fd6c7edc290bd2"}, "downloads": -1, "filename": "dbt_spark-0.14.3-py3-none-any.whl", "has_sig": false, "md5_digest": "2086bf50c5cfd9f85a84f1e4b4eb0aa4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 15931, "upload_time": "2020-03-23T16:28:06", "upload_time_iso_8601": "2020-03-23T16:28:06.536918Z", "url": "https://files.pythonhosted.org/packages/6f/37/f2e676ca33c19f9881252ac1e83960d1125035d8521d1076f0f6041e22f4/dbt_spark-0.14.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ef1baa39f359458aa1aaa6058f643ea2", "sha256": "e7d84d06a64872476617ba774e3fecde108ea25a3ace54384f50202a8bf7d5f4"}, "downloads": -1, "filename": "dbt-spark-0.14.3.tar.gz", "has_sig": false, "md5_digest": "ef1baa39f359458aa1aaa6058f643ea2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13254, "upload_time": "2020-03-23T16:28:07", "upload_time_iso_8601": "2020-03-23T16:28:07.983001Z", "url": "https://files.pythonhosted.org/packages/27/f6/ab35352293399dfbcbf96a7d4b93cbc801ef2e0d1527081d84a2be089d35/dbt-spark-0.14.3.tar.gz", "yanked": false}], "0.15.3": [{"comment_text": "", "digests": {"md5": "d2a7714ca10068aa27656de146128924", "sha256": "86968568b8abbd845843d70cc38d09992628cf350099b3f5ad05e30d02e8da45"}, "downloads": -1, "filename": "dbt_spark-0.15.3-py3-none-any.whl", "has_sig": false, "md5_digest": "d2a7714ca10068aa27656de146128924", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 22946, "upload_time": "2020-03-23T16:38:40", "upload_time_iso_8601": "2020-03-23T16:38:40.054443Z", "url": "https://files.pythonhosted.org/packages/32/90/8868bcd053e9369a5ed28099a2e879708b756227ba14b66b062480fe5314/dbt_spark-0.15.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "eda7c505105890445fd4961b91a6fa08", "sha256": "5782468bf7a54d7dd8fde753a16ff3196b0d36d1ff6b01b58a0830992bf96c27"}, "downloads": -1, "filename": "dbt-spark-0.15.3.tar.gz", "has_sig": false, "md5_digest": "eda7c505105890445fd4961b91a6fa08", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22617, "upload_time": "2020-03-23T16:38:41", "upload_time_iso_8601": "2020-03-23T16:38:41.088757Z", "url": "https://files.pythonhosted.org/packages/75/76/38456629b791664f3bbad23e2043a7258ba0af2d9f293ebfee3bb6920a99/dbt-spark-0.15.3.tar.gz", "yanked": false}], "0.16.0": [{"comment_text": "", "digests": {"md5": "0e7d20563e9606f3fbfcf2f3f3b13e14", "sha256": "660056ca84e93b2a077852979ef77d21f25d14f25769889399d984419e17d585"}, "downloads": -1, "filename": "dbt_spark-0.16.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0e7d20563e9606f3fbfcf2f3f3b13e14", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23459, "upload_time": "2020-04-08T16:53:46", "upload_time_iso_8601": "2020-04-08T16:53:46.208850Z", "url": "https://files.pythonhosted.org/packages/5c/4c/3712c5842bb6504f067749eea4e9da4b9fea59dd36e7aeb26d78bc716504/dbt_spark-0.16.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c6361a56ba829bc282429ad77c2180df", "sha256": "8636fbc5e10cc75cc34378b982d708ee4aeeb17bae96f1b869bb75dddc79121a"}, "downloads": -1, "filename": "dbt-spark-0.16.0.tar.gz", "has_sig": false, "md5_digest": "c6361a56ba829bc282429ad77c2180df", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22969, "upload_time": "2020-04-08T16:53:47", "upload_time_iso_8601": "2020-04-08T16:53:47.575198Z", "url": "https://files.pythonhosted.org/packages/b3/a8/cccf7554fc6213658b15157c1567af0ad67d9a253b76921a4b55a098cc00/dbt-spark-0.16.0.tar.gz", "yanked": false}], "0.16.1": [{"comment_text": "", "digests": {"md5": "ef03a92c14e5d03fd21ed04a9e397a0c", "sha256": "d4b33bfb691326bb85488823ca8b2bda2431f5c7522588d800036d58f23583e7"}, "downloads": -1, "filename": "dbt_spark-0.16.1-py3-none-any.whl", "has_sig": false, "md5_digest": "ef03a92c14e5d03fd21ed04a9e397a0c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23728, "upload_time": "2020-04-27T21:34:38", "upload_time_iso_8601": "2020-04-27T21:34:38.654789Z", "url": "https://files.pythonhosted.org/packages/b8/6c/2f0b14450594bde3b60e846084fad813af591e7fa5f3a95db1b880f9c942/dbt_spark-0.16.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e1164814d1a287205bbe2fd291708bd3", "sha256": "0fec09f6b1af060366ef23c7b6bba026b12795adb138dc3a996b9dc22684c751"}, "downloads": -1, "filename": "dbt-spark-0.16.1.tar.gz", "has_sig": false, "md5_digest": "e1164814d1a287205bbe2fd291708bd3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23231, "upload_time": "2020-04-27T21:34:40", "upload_time_iso_8601": "2020-04-27T21:34:40.000256Z", "url": "https://files.pythonhosted.org/packages/37/21/eb4ffea506abab4182fd72a43f272c6e6eabace9b0bf78884eff4206ee6f/dbt-spark-0.16.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ef03a92c14e5d03fd21ed04a9e397a0c", "sha256": "d4b33bfb691326bb85488823ca8b2bda2431f5c7522588d800036d58f23583e7"}, "downloads": -1, "filename": "dbt_spark-0.16.1-py3-none-any.whl", "has_sig": false, "md5_digest": "ef03a92c14e5d03fd21ed04a9e397a0c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23728, "upload_time": "2020-04-27T21:34:38", "upload_time_iso_8601": "2020-04-27T21:34:38.654789Z", "url": "https://files.pythonhosted.org/packages/b8/6c/2f0b14450594bde3b60e846084fad813af591e7fa5f3a95db1b880f9c942/dbt_spark-0.16.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e1164814d1a287205bbe2fd291708bd3", "sha256": "0fec09f6b1af060366ef23c7b6bba026b12795adb138dc3a996b9dc22684c751"}, "downloads": -1, "filename": "dbt-spark-0.16.1.tar.gz", "has_sig": false, "md5_digest": "e1164814d1a287205bbe2fd291708bd3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23231, "upload_time": "2020-04-27T21:34:40", "upload_time_iso_8601": "2020-04-27T21:34:40.000256Z", "url": "https://files.pythonhosted.org/packages/37/21/eb4ffea506abab4182fd72a43f272c6e6eabace9b0bf78884eff4206ee6f/dbt-spark-0.16.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:39:51 2020"}