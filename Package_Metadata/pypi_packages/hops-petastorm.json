{"info": {"author": "Uber Technologies, Inc., Kim Hammar", "author_email": "kim@logicalclocks.com", "bugtrack_url": null, "classifiers": ["Environment :: Console", "Environment :: Web Environment", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6"], "description": "Petastorm\n=========\n\n.. image:: https://travis-ci.com/uber/petastorm.svg?branch=master\n   :target: https://travis-ci.com/uber/petastorm\n   :alt: Build Status (Travis CI)\n\n.. image:: https://codecov.io/gh/uber/petastorm/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/uber/petastorm/branch/master\n   :alt: Code coverage\n\n.. image:: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :target: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :alt: License\n\n.. image:: https://badge.fury.io/py/petastorm.svg\n   :target: https://pypi.org/project/petastorm\n   :alt: Latest Version\n\n.. inclusion-marker-start-do-not-remove\n\n.. contents::\n\n\nPetastorm is an open source data access library developed at Uber ATG. This library enables single machine or\ndistributed training and evaluation of deep learning models directly from datasets in Apache Parquet\nformat. Petastorm supports popular Python-based machine learning (ML) frameworks such as\n`Tensorflow <http://www.tensorflow.org/>`_, `PyTorch <https://pytorch.org/>`_, and\n`PySpark <http://spark.apache.org/docs/latest/api/python/pyspark.html>`_. It can also be used from pure Python code.\n\nDocumentation web site: `<https://petastorm.readthedocs.io>`_\n\n\n\nInstallation\n------------\n\n.. code-block:: bash\n\n    pip install petastorm\n\n\nThere are several extra dependencies that are defined by the ``petastorm`` package that are not installed automatically.\nThe extras are: ``tf``, ``tf_gpu``, ``torch``, ``opencv``, ``docs``, ``test``.\n\nFor example to trigger installation of GPU version of tensorflow and opencv, use the following pip command:\n\n.. code-block:: bash\n\n    pip install petastorm[opencv,tf_gpu]\n\n\n\nGenerating a dataset\n--------------------\n\nA dataset created using Petastorm is stored in `Apache Parquet <https://parquet.apache.org/>`_ format.\nOn top of a Parquet schema, petastorm also stores higher-level schema information that makes multidimensional arrays into a native part of a petastorm dataset. \n\nPetastorm supports extensible data codecs. These enable a user to use one of the standard data compressions (jpeg, png) or implement her own.\n\nGenerating a dataset is done using PySpark.\nPySpark natively supports Parquet format, making it easy to run on a single machine or on a Spark compute cluster.\nHere is a minimalistic example writing out a table with some random data.\n\n\n.. code-block:: python\n\n    HelloWorldSchema = Unischema('HelloWorldSchema', [\n       UnischemaField('id', np.int32, (), ScalarCodec(IntegerType()), False),\n       UnischemaField('image1', np.uint8, (128, 256, 3), CompressedImageCodec('png'), False),\n       UnischemaField('other_data', np.uint8, (None, 128, 30, None), NdarrayCodec(), False),\n    ])\n\n\n    def row_generator(x):\n       \"\"\"Returns a single entry in the generated dataset. Return a bunch of random values as an example.\"\"\"\n       return {'id': x,\n               'image1': np.random.randint(0, 255, dtype=np.uint8, size=(128, 256, 3)),\n               'other_data': np.random.randint(0, 255, dtype=np.uint8, size=(4, 128, 30, 3))}\n\n\n    def generate_hello_world_dataset(output_url='file:///tmp/hello_world_dataset'):\n       rows_count = 10\n       rowgroup_size_mb = 256\n\n       spark = SparkSession.builder.config('spark.driver.memory', '2g').master('local[2]').getOrCreate()\n       sc = spark.sparkContext\n\n       # Wrap dataset materialization portion. Will take care of setting up spark environment variables as\n       # well as save petastorm specific metadata\n       with materialize_dataset(spark, output_url, HelloWorldSchema, rowgroup_size_mb):\n\n           rows_rdd = sc.parallelize(range(rows_count))\\\n               .map(row_generator)\\\n               .map(lambda x: dict_to_spark_row(HelloWorldSchema, x))\n\n           spark.createDataFrame(rows_rdd, HelloWorldSchema.as_spark_schema()) \\\n               .coalesce(10) \\\n               .write \\\n               .mode('overwrite') \\\n               .parquet(output_url)\n\n- ``HelloWorldSchema`` is an instance of a ``Unischema`` object.\n  ``Unischema`` is capable of rendering types of its fields into different\n  framework specific formats, such as: Spark ``StructType``, Tensorflow\n  ``tf.DType`` and numpy ``numpy.dtype``.\n- To define a dataset field, you need to specify a ``type``, ``shape``, a\n  ``codec`` instance and whether the field is nullable for each field of the\n  ``Unischema``.\n- We use PySpark for writing output Parquet files. In this example, we launch\n  PySpark on a local box (``.master('local[2]')``). Of course for a larger\n  scale dataset generation we would need a real compute cluster.\n- We wrap spark dataset generation code with the ``materialize_dataset``\n  context manager.  The context manager is responsible for configuring row\n  group size at the beginning and write out petastorm specific metadata at the\n  end.\n- The row generating code is expected to return a Python dictionary indexed by\n  a field name. We use ``row_generator`` function for that. \n- ``dict_to_spark_row`` converts the dictionary into a ``pyspark.Row``\n  object while ensuring schema ``HelloWorldSchema`` compliance (shape,\n  type and is-nullable condition are tested).\n- Once we have a ``pyspark.DataFrame`` we write it out to a parquet\n  storage. The parquet schema is automatically derived from\n  ``HelloWorldSchema``.\n\nPlain Python API\n----------------\nThe ``petastorm.reader.Reader`` class is the main entry point for user\ncode that accesses the data from an ML framework such as Tensorflow or Pytorch.\nThe reader has multiple features such as:\n\n- Selective column readout\n- Multiple parallelism strategies: thread, process, single-threaded (for debug)\n- N-grams readout support\n- Row filtering (row predicates)\n- Shuffling\n- Partitioning for multi-GPU training\n- Local caching\n\nReading a dataset is simple using the ``petastorm.reader.Reader`` class which can be created using the\n``petastorm.make_reader`` factory method:\n\n.. code-block:: python\n\n   from petastorm import make_reader\n\n    with make_reader('hdfs://myhadoop/some_dataset') as reader:\n       for row in reader:\n           print(row)\n\n``hdfs://...`` and ``file://...`` are supported URL protocols.\n\nOnce a ``Reader`` is instantiated, you can use it as an iterator.\n\nTensorflow API\n--------------\n\nTo hookup the reader into a tensorflow graph, you can use the ``tf_tensors``\nfunction:\n\n.. code-block:: python\n\n    with make_reader('file:///some/localpath/a_dataset') as reader:\n       row_tensors = tf_tensors(reader)\n       with tf.Session() as session:\n           for _ in range(3):\n               print(session.run(row_tensors))\n\nAlternatively, you can use new ``tf.data.Dataset`` API;\n\n.. code-block:: python\n\n    with make_reader('file:///some/localpath/a_dataset') as reader:\n        dataset = make_petastorm_dataset(reader)\n        iterator = dataset.make_one_shot_iterator()\n        tensor = iterator.get_next()\n        with tf.Session() as sess:\n            sample = sess.run(tensor)\n            print(sample.id)\n\nPytorch API\n-----------\n\nAs illustrated in\n`pytorch_example.py <https://github.com/uber/petastorm/blob/master/examples/mnist/pytorch_example.py>`_,\nreading a petastorm dataset from pytorch\ncan be done via the adapter class ``petastorm.pytorch.DataLoader``,\nwhich allows custom pytorch collating function and transforms to be supplied.\n\nBe sure you have ``torch`` and ``torchvision`` installed:\n\n.. code-block:: bash\n\n    pip install torchvision\n\nThe minimalist example below assumes the definition of a ``Net`` class and\n``train`` and ``test`` functions, included in ``pytorch_example``:\n\n.. code-block:: python\n\n    import torch\n    from petastorm.pytorch import DataLoader\n\n    torch.manual_seed(1)\n    device = torch.device('cpu')\n    model = Net().to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n    def _transform_row(mnist_row):\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n        return (transform(mnist_row['image']), mnist_row['digit'])\n\n\n    transform = TransformSpec(_transform_row, removed_fields=['idx'])\n\n    with DataLoader(make_reader('file:///localpath/mnist/train', num_epochs=10,\n                                transform_spec=transform), batch_size=64) as train_loader:\n        train(model, device, train_loader, 10, optimizer, 1)\n    with DataLoader(make_reader('file:///localpath/mnist/test', num_epochs=10,\n                                transform_spec=transform), batch_size=1000) as test_loader:\n        test(model, device, test_loader)\n\nPySpark and SQL\n---------------\n\nUsing the Parquet data format, which is natively supported by Spark, makes it possible to use a wide range of Spark\ntools to analyze and manipulate the dataset. The example below shows how to read a Petastorm dataset\nas a Spark RDD object:\n\n.. code-block:: python\n\n   # Create a dataframe object from a parquet file\n   dataframe = spark.read.parquet(dataset_url)\n\n   # Show a schema\n   dataframe.printSchema()\n\n   # Count all\n   dataframe.count()\n\n   # Show a single column\n   dataframe.select('id').show()\n\nSQL can be used to query a Petastorm dataset:\n\n.. code-block:: python\n\n   spark.sql(\n      'SELECT count(id) '\n      'from parquet.`file:///tmp/hello_world_dataset`').collect()\n\nYou can find a full code sample here: `pyspark_hello_world.py <https://github.com/uber/petastorm/blob/master/examples/hello_world/petastorm_dataset/pyspark_hello_world.py>`_,\n\nNon Petastorm Parquet Stores\n----------------------------\nPetastorm can also be used to read data directly from Apache Parquet stores. To achieve that, use\n``make_batch_reader`` (and not ``make_reader``). The following table summarizes the differences\n``make_batch_reader`` and ``make_reader`` functions.\n\n\n==================================================================  =====================================================\n``make_reader``                                                     ``make_batch_reader``\n==================================================================  =====================================================\nOnly Petastorm datasets (created using materializes_dataset)        Any Parquet store (some native Parquet column types\n                                                                    are not supported yet.\n------------------------------------------------------------------  -----------------------------------------------------\nThe reader returns one record at a time.                            The reader returns batches of records. The size of the\n                                                                    batch is not fixed and defined by Parquet row-group\n                                                                    size.\n------------------------------------------------------------------  -----------------------------------------------------\nPredicates passed to ``make_reader`` are evaluated per single row.  Predicates passed to ``make_batch_reader`` are evaluated per batch.\n==================================================================  =====================================================\n\n\nTroubleshooting\n---------------\n\nSee the Troubleshooting_ page and please submit a ticket_ if you can't find an\nanswer.\n\n\nPublications\n------------\n\n1. Gruener, R., Cheng, O., and Litvin, Y. (2018) *Introducing Petastorm: Uber ATG's Data Access Library for Deep Learning*. URL: https://eng.uber.com/petastorm/\n\n\n.. _Troubleshooting: docs/troubleshoot.rst\n.. _ticket: https://github.com/uber/petastorm/issues/new\n.. _Development: docs/development.rst\n\nHow to Contribute\n=================\n\nWe prefer to receive contributions in the form of GitHub pull requests. Please send pull requests against the ``github.com/uber/petastorm`` repository.\n\n- If you are looking for some ideas on what to contribute, check out `github issues <https://github.com/uber/petastorm/issues>`_ and comment on the issue.\n- If you have an idea for an improvement, or you'd like to report a bug but don't have time to fix it please a `create a github issue <https://github.com/uber/petastorm/issues/new>`_.\n\nTo contribute a patch:\n\n- Break your work into small, single-purpose patches if possible. It's much harder to merge in a large change with a lot of disjoint features.\n- Submit the patch as a GitHub pull request against the master branch. For a tutorial, see the GitHub guides on forking a repo and sending a pull request.\n- Include a detailed describtion of the proposed change in the pull request.\n- Make sure that your code passes the unit tests. You can find instructions how to run the unit tests `here <https://github.com/uber/petastorm/blob/master/docs/development.rst>`_.\n- Add new unit tests for your code.\n\nThank you in advance for your contributions!\n\n\nSee the Development_ for development related information.\n\n\n.. inclusion-marker-end-do-not-remove\n   Place contents above here if they should also appear in read-the-docs.\n   Contents below are already part of the read-the-docs table of contents.", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/logicalclocks/petastorm", "keywords": "", "license": "Apache License, Version 2.0", "maintainer": "", "maintainer_email": "", "name": "hops-petastorm", "package_url": "https://pypi.org/project/hops-petastorm/", "platform": "", "project_url": "https://pypi.org/project/hops-petastorm/", "project_urls": {"Homepage": "https://github.com/logicalclocks/petastorm"}, "release_url": "https://pypi.org/project/hops-petastorm/0.7.6/", "requires_dist": null, "requires_python": "", "summary": "Petastorm is a library enabling the use of Parquet storage from Tensorflow, Pytorch, and other Python-based ML training frameworks. This is a fork of Petastorm that is compatible with Hops installations", "version": "0.7.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"petastorm\">\n<h2><a href=\"#id2\" rel=\"nofollow\">Petastorm</a></h2>\n<a href=\"https://travis-ci.com/uber/petastorm\" rel=\"nofollow\"><img alt=\"Build Status (Travis CI)\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3bbc078ba4e9812d4bb8b925438f66790e466ed5/68747470733a2f2f7472617669732d63692e636f6d2f756265722f7065746173746f726d2e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://codecov.io/gh/uber/petastorm/branch/master\" rel=\"nofollow\"><img alt=\"Code coverage\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/78bd59b852ef3c8b9d4279b6edecb830bfab8ccf/68747470733a2f2f636f6465636f762e696f2f67682f756265722f7065746173746f726d2f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a>\n<a href=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b97ca76cf5d8fd16c7bc4731270e0bbe53df7aa1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667\"></a>\n<a href=\"https://pypi.org/project/petastorm\" rel=\"nofollow\"><img alt=\"Latest Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/30ffd73717f4022f12f98f515f71b6f20415998b/68747470733a2f2f62616467652e667572792e696f2f70792f7065746173746f726d2e737667\"></a>\n<div id=\"contents\">\n<p>Contents</p>\n<ul>\n<li><a href=\"#petastorm\" id=\"id2\" rel=\"nofollow\">Petastorm</a><ul>\n<li><a href=\"#installation\" id=\"id3\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#generating-a-dataset\" id=\"id4\" rel=\"nofollow\">Generating a dataset</a></li>\n<li><a href=\"#plain-python-api\" id=\"id5\" rel=\"nofollow\">Plain Python API</a></li>\n<li><a href=\"#tensorflow-api\" id=\"id6\" rel=\"nofollow\">Tensorflow API</a></li>\n<li><a href=\"#pytorch-api\" id=\"id7\" rel=\"nofollow\">Pytorch API</a></li>\n<li><a href=\"#pyspark-and-sql\" id=\"id8\" rel=\"nofollow\">PySpark and SQL</a></li>\n<li><a href=\"#non-petastorm-parquet-stores\" id=\"id9\" rel=\"nofollow\">Non Petastorm Parquet Stores</a></li>\n<li><a href=\"#troubleshooting\" id=\"id10\" rel=\"nofollow\">Troubleshooting</a></li>\n<li><a href=\"#publications\" id=\"id11\" rel=\"nofollow\">Publications</a></li>\n</ul>\n</li>\n<li><a href=\"#how-to-contribute\" id=\"id12\" rel=\"nofollow\">How to Contribute</a></li>\n</ul>\n</div>\n<p>Petastorm is an open source data access library developed at Uber ATG. This library enables single machine or\ndistributed training and evaluation of deep learning models directly from datasets in Apache Parquet\nformat. Petastorm supports popular Python-based machine learning (ML) frameworks such as\n<a href=\"http://www.tensorflow.org/\" rel=\"nofollow\">Tensorflow</a>, <a href=\"https://pytorch.org/\" rel=\"nofollow\">PyTorch</a>, and\n<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html\" rel=\"nofollow\">PySpark</a>. It can also be used from pure Python code.</p>\n<p>Documentation web site: <a href=\"https://petastorm.readthedocs.io\" rel=\"nofollow\">https://petastorm.readthedocs.io</a></p>\n<div id=\"installation\">\n<h3><a href=\"#id3\" rel=\"nofollow\">Installation</a></h3>\n<pre>pip install petastorm\n</pre>\n<p>There are several extra dependencies that are defined by the <tt>petastorm</tt> package that are not installed automatically.\nThe extras are: <tt>tf</tt>, <tt>tf_gpu</tt>, <tt>torch</tt>, <tt>opencv</tt>, <tt>docs</tt>, <tt>test</tt>.</p>\n<p>For example to trigger installation of GPU version of tensorflow and opencv, use the following pip command:</p>\n<pre>pip install petastorm<span class=\"o\">[</span>opencv,tf_gpu<span class=\"o\">]</span>\n</pre>\n</div>\n<div id=\"generating-a-dataset\">\n<h3><a href=\"#id4\" rel=\"nofollow\">Generating a dataset</a></h3>\n<p>A dataset created using Petastorm is stored in <a href=\"https://parquet.apache.org/\" rel=\"nofollow\">Apache Parquet</a> format.\nOn top of a Parquet schema, petastorm also stores higher-level schema information that makes multidimensional arrays into a native part of a petastorm dataset.</p>\n<p>Petastorm supports extensible data codecs. These enable a user to use one of the standard data compressions (jpeg, png) or implement her own.</p>\n<p>Generating a dataset is done using PySpark.\nPySpark natively supports Parquet format, making it easy to run on a single machine or on a Spark compute cluster.\nHere is a minimalistic example writing out a table with some random data.</p>\n<pre><span class=\"n\">HelloWorldSchema</span> <span class=\"o\">=</span> <span class=\"n\">Unischema</span><span class=\"p\">(</span><span class=\"s1\">'HelloWorldSchema'</span><span class=\"p\">,</span> <span class=\"p\">[</span>\n   <span class=\"n\">UnischemaField</span><span class=\"p\">(</span><span class=\"s1\">'id'</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">,</span> <span class=\"p\">(),</span> <span class=\"n\">ScalarCodec</span><span class=\"p\">(</span><span class=\"n\">IntegerType</span><span class=\"p\">()),</span> <span class=\"kc\">False</span><span class=\"p\">),</span>\n   <span class=\"n\">UnischemaField</span><span class=\"p\">(</span><span class=\"s1\">'image1'</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"n\">CompressedImageCodec</span><span class=\"p\">(</span><span class=\"s1\">'png'</span><span class=\"p\">),</span> <span class=\"kc\">False</span><span class=\"p\">),</span>\n   <span class=\"n\">UnischemaField</span><span class=\"p\">(</span><span class=\"s1\">'other_data'</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">),</span> <span class=\"n\">NdarrayCodec</span><span class=\"p\">(),</span> <span class=\"kc\">False</span><span class=\"p\">),</span>\n<span class=\"p\">])</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">row_generator</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n   <span class=\"sd\">\"\"\"Returns a single entry in the generated dataset. Return a bunch of random values as an example.\"\"\"</span>\n   <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"n\">x</span><span class=\"p\">,</span>\n           <span class=\"s1\">'image1'</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">255</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)),</span>\n           <span class=\"s1\">'other_data'</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">255</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">))}</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">generate_hello_world_dataset</span><span class=\"p\">(</span><span class=\"n\">output_url</span><span class=\"o\">=</span><span class=\"s1\">'file:///tmp/hello_world_dataset'</span><span class=\"p\">):</span>\n   <span class=\"n\">rows_count</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n   <span class=\"n\">rowgroup_size_mb</span> <span class=\"o\">=</span> <span class=\"mi\">256</span>\n\n   <span class=\"n\">spark</span> <span class=\"o\">=</span> <span class=\"n\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"p\">(</span><span class=\"s1\">'spark.driver.memory'</span><span class=\"p\">,</span> <span class=\"s1\">'2g'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">master</span><span class=\"p\">(</span><span class=\"s1\">'local[2]'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">getOrCreate</span><span class=\"p\">()</span>\n   <span class=\"n\">sc</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sparkContext</span>\n\n   <span class=\"c1\"># Wrap dataset materialization portion. Will take care of setting up spark environment variables as</span>\n   <span class=\"c1\"># well as save petastorm specific metadata</span>\n   <span class=\"k\">with</span> <span class=\"n\">materialize_dataset</span><span class=\"p\">(</span><span class=\"n\">spark</span><span class=\"p\">,</span> <span class=\"n\">output_url</span><span class=\"p\">,</span> <span class=\"n\">HelloWorldSchema</span><span class=\"p\">,</span> <span class=\"n\">rowgroup_size_mb</span><span class=\"p\">):</span>\n\n       <span class=\"n\">rows_rdd</span> <span class=\"o\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">parallelize</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">rows_count</span><span class=\"p\">))</span>\\\n           <span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span><span class=\"n\">row_generator</span><span class=\"p\">)</span>\\\n           <span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">dict_to_spark_row</span><span class=\"p\">(</span><span class=\"n\">HelloWorldSchema</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">))</span>\n\n       <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">rows_rdd</span><span class=\"p\">,</span> <span class=\"n\">HelloWorldSchema</span><span class=\"o\">.</span><span class=\"n\">as_spark_schema</span><span class=\"p\">())</span> \\\n           <span class=\"o\">.</span><span class=\"n\">coalesce</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)</span> \\\n           <span class=\"o\">.</span><span class=\"n\">write</span> \\\n           <span class=\"o\">.</span><span class=\"n\">mode</span><span class=\"p\">(</span><span class=\"s1\">'overwrite'</span><span class=\"p\">)</span> \\\n           <span class=\"o\">.</span><span class=\"n\">parquet</span><span class=\"p\">(</span><span class=\"n\">output_url</span><span class=\"p\">)</span>\n</pre>\n<ul>\n<li><tt>HelloWorldSchema</tt> is an instance of a <tt>Unischema</tt> object.\n<tt>Unischema</tt> is capable of rendering types of its fields into different\nframework specific formats, such as: Spark <tt>StructType</tt>, Tensorflow\n<tt>tf.DType</tt> and numpy <tt>numpy.dtype</tt>.</li>\n<li>To define a dataset field, you need to specify a <tt>type</tt>, <tt>shape</tt>, a\n<tt>codec</tt> instance and whether the field is nullable for each field of the\n<tt>Unischema</tt>.</li>\n<li>We use PySpark for writing output Parquet files. In this example, we launch\nPySpark on a local box (<tt><span class=\"pre\">.master('local[2]')</span></tt>). Of course for a larger\nscale dataset generation we would need a real compute cluster.</li>\n<li>We wrap spark dataset generation code with the <tt>materialize_dataset</tt>\ncontext manager.  The context manager is responsible for configuring row\ngroup size at the beginning and write out petastorm specific metadata at the\nend.</li>\n<li>The row generating code is expected to return a Python dictionary indexed by\na field name. We use <tt>row_generator</tt> function for that.</li>\n<li><tt>dict_to_spark_row</tt> converts the dictionary into a <tt>pyspark.Row</tt>\nobject while ensuring schema <tt>HelloWorldSchema</tt> compliance (shape,\ntype and is-nullable condition are tested).</li>\n<li>Once we have a <tt>pyspark.DataFrame</tt> we write it out to a parquet\nstorage. The parquet schema is automatically derived from\n<tt>HelloWorldSchema</tt>.</li>\n</ul>\n</div>\n<div id=\"plain-python-api\">\n<h3><a href=\"#id5\" rel=\"nofollow\">Plain Python API</a></h3>\n<p>The <tt>petastorm.reader.Reader</tt> class is the main entry point for user\ncode that accesses the data from an ML framework such as Tensorflow or Pytorch.\nThe reader has multiple features such as:</p>\n<ul>\n<li>Selective column readout</li>\n<li>Multiple parallelism strategies: thread, process, single-threaded (for debug)</li>\n<li>N-grams readout support</li>\n<li>Row filtering (row predicates)</li>\n<li>Shuffling</li>\n<li>Partitioning for multi-GPU training</li>\n<li>Local caching</li>\n</ul>\n<p>Reading a dataset is simple using the <tt>petastorm.reader.Reader</tt> class which can be created using the\n<tt>petastorm.make_reader</tt> factory method:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">petastorm</span> <span class=\"kn\">import</span> <span class=\"n\">make_reader</span>\n\n <span class=\"k\">with</span> <span class=\"n\">make_reader</span><span class=\"p\">(</span><span class=\"s1\">'hdfs://myhadoop/some_dataset'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">reader</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">reader</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">row</span><span class=\"p\">)</span>\n</pre>\n<p><tt><span class=\"pre\">hdfs://...</span></tt> and <tt><span class=\"pre\">file://...</span></tt> are supported URL protocols.</p>\n<p>Once a <tt>Reader</tt> is instantiated, you can use it as an iterator.</p>\n</div>\n<div id=\"tensorflow-api\">\n<h3><a href=\"#id6\" rel=\"nofollow\">Tensorflow API</a></h3>\n<p>To hookup the reader into a tensorflow graph, you can use the <tt>tf_tensors</tt>\nfunction:</p>\n<pre><span class=\"k\">with</span> <span class=\"n\">make_reader</span><span class=\"p\">(</span><span class=\"s1\">'file:///some/localpath/a_dataset'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">reader</span><span class=\"p\">:</span>\n   <span class=\"n\">row_tensors</span> <span class=\"o\">=</span> <span class=\"n\">tf_tensors</span><span class=\"p\">(</span><span class=\"n\">reader</span><span class=\"p\">)</span>\n   <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">session</span><span class=\"p\">:</span>\n       <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">):</span>\n           <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">row_tensors</span><span class=\"p\">))</span>\n</pre>\n<p>Alternatively, you can use new <tt>tf.data.Dataset</tt> API;</p>\n<pre><span class=\"k\">with</span> <span class=\"n\">make_reader</span><span class=\"p\">(</span><span class=\"s1\">'file:///some/localpath/a_dataset'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">reader</span><span class=\"p\">:</span>\n    <span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">make_petastorm_dataset</span><span class=\"p\">(</span><span class=\"n\">reader</span><span class=\"p\">)</span>\n    <span class=\"n\">iterator</span> <span class=\"o\">=</span> <span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">make_one_shot_iterator</span><span class=\"p\">()</span>\n    <span class=\"n\">tensor</span> <span class=\"o\">=</span> <span class=\"n\">iterator</span><span class=\"o\">.</span><span class=\"n\">get_next</span><span class=\"p\">()</span>\n    <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\n        <span class=\"n\">sample</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">tensor</span><span class=\"p\">)</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">sample</span><span class=\"o\">.</span><span class=\"n\">id</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"pytorch-api\">\n<h3><a href=\"#id7\" rel=\"nofollow\">Pytorch API</a></h3>\n<p>As illustrated in\n<a href=\"https://github.com/uber/petastorm/blob/master/examples/mnist/pytorch_example.py\" rel=\"nofollow\">pytorch_example.py</a>,\nreading a petastorm dataset from pytorch\ncan be done via the adapter class <tt>petastorm.pytorch.DataLoader</tt>,\nwhich allows custom pytorch collating function and transforms to be supplied.</p>\n<p>Be sure you have <tt>torch</tt> and <tt>torchvision</tt> installed:</p>\n<pre>pip install torchvision\n</pre>\n<p>The minimalist example below assumes the definition of a <tt>Net</tt> class and\n<tt>train</tt> and <tt>test</tt> functions, included in <tt>pytorch_example</tt>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">petastorm.pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n\n<span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s1\">'cpu'</span><span class=\"p\">)</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Net</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">SGD</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">_transform_row</span><span class=\"p\">(</span><span class=\"n\">mnist_row</span><span class=\"p\">):</span>\n    <span class=\"n\">transform</span> <span class=\"o\">=</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">([</span>\n        <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">(),</span>\n        <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Normalize</span><span class=\"p\">((</span><span class=\"mf\">0.1307</span><span class=\"p\">,),</span> <span class=\"p\">(</span><span class=\"mf\">0.3081</span><span class=\"p\">,))</span>\n    <span class=\"p\">])</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">mnist_row</span><span class=\"p\">[</span><span class=\"s1\">'image'</span><span class=\"p\">]),</span> <span class=\"n\">mnist_row</span><span class=\"p\">[</span><span class=\"s1\">'digit'</span><span class=\"p\">])</span>\n\n\n<span class=\"n\">transform</span> <span class=\"o\">=</span> <span class=\"n\">TransformSpec</span><span class=\"p\">(</span><span class=\"n\">_transform_row</span><span class=\"p\">,</span> <span class=\"n\">removed_fields</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'idx'</span><span class=\"p\">])</span>\n\n<span class=\"k\">with</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">make_reader</span><span class=\"p\">(</span><span class=\"s1\">'file:///localpath/mnist/train'</span><span class=\"p\">,</span> <span class=\"n\">num_epochs</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n                            <span class=\"n\">transform_spec</span><span class=\"o\">=</span><span class=\"n\">transform</span><span class=\"p\">),</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">train_loader</span><span class=\"p\">:</span>\n    <span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">train_loader</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"k\">with</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">make_reader</span><span class=\"p\">(</span><span class=\"s1\">'file:///localpath/mnist/test'</span><span class=\"p\">,</span> <span class=\"n\">num_epochs</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n                            <span class=\"n\">transform_spec</span><span class=\"o\">=</span><span class=\"n\">transform</span><span class=\"p\">),</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">test_loader</span><span class=\"p\">:</span>\n    <span class=\"n\">test</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">test_loader</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"pyspark-and-sql\">\n<h3><a href=\"#id8\" rel=\"nofollow\">PySpark and SQL</a></h3>\n<p>Using the Parquet data format, which is natively supported by Spark, makes it possible to use a wide range of Spark\ntools to analyze and manipulate the dataset. The example below shows how to read a Petastorm dataset\nas a Spark RDD object:</p>\n<pre><span class=\"c1\"># Create a dataframe object from a parquet file</span>\n<span class=\"n\">dataframe</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">parquet</span><span class=\"p\">(</span><span class=\"n\">dataset_url</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Show a schema</span>\n<span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">printSchema</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Count all</span>\n<span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Show a single column</span>\n<span class=\"n\">dataframe</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"s1\">'id'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</pre>\n<p>SQL can be used to query a Petastorm dataset:</p>\n<pre><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"p\">(</span>\n   <span class=\"s1\">'SELECT count(id) '</span>\n   <span class=\"s1\">'from parquet.`file:///tmp/hello_world_dataset`'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span>\n</pre>\n<p>You can find a full code sample here: <a href=\"https://github.com/uber/petastorm/blob/master/examples/hello_world/petastorm_dataset/pyspark_hello_world.py\" rel=\"nofollow\">pyspark_hello_world.py</a>,</p>\n</div>\n<div id=\"non-petastorm-parquet-stores\">\n<h3><a href=\"#id9\" rel=\"nofollow\">Non Petastorm Parquet Stores</a></h3>\n<p>Petastorm can also be used to read data directly from Apache Parquet stores. To achieve that, use\n<tt>make_batch_reader</tt> (and not <tt>make_reader</tt>). The following table summarizes the differences\n<tt>make_batch_reader</tt> and <tt>make_reader</tt> functions.</p>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th><tt>make_reader</tt></th>\n<th><tt>make_batch_reader</tt></th>\n</tr>\n</thead>\n<tbody>\n<tr><td>Only Petastorm datasets (created using materializes_dataset)</td>\n<td>Any Parquet store (some native Parquet column types\nare not supported yet.</td>\n</tr>\n<tr><td>The reader returns one record at a time.</td>\n<td>The reader returns batches of records. The size of the\nbatch is not fixed and defined by Parquet row-group\nsize.</td>\n</tr>\n<tr><td>Predicates passed to <tt>make_reader</tt> are evaluated per single row.</td>\n<td>Predicates passed to <tt>make_batch_reader</tt> are evaluated per batch.</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div id=\"troubleshooting\">\n<h3><a href=\"#id10\" rel=\"nofollow\">Troubleshooting</a></h3>\n<p>See the <a href=\"docs/troubleshoot.rst\" rel=\"nofollow\">Troubleshooting</a> page and please submit a <a href=\"https://github.com/uber/petastorm/issues/new\" rel=\"nofollow\">ticket</a> if you can\u2019t find an\nanswer.</p>\n</div>\n<div id=\"publications\">\n<h3><a href=\"#id11\" rel=\"nofollow\">Publications</a></h3>\n<ol>\n<li>Gruener, R., Cheng, O., and Litvin, Y. (2018) <em>Introducing Petastorm: Uber ATG\u2019s Data Access Library for Deep Learning</em>. URL: <a href=\"https://eng.uber.com/petastorm/\" rel=\"nofollow\">https://eng.uber.com/petastorm/</a></li>\n</ol>\n</div>\n</div>\n<div id=\"how-to-contribute\">\n<h2><a href=\"#id12\" rel=\"nofollow\">How to Contribute</a></h2>\n<p>We prefer to receive contributions in the form of GitHub pull requests. Please send pull requests against the <tt>github.com/uber/petastorm</tt> repository.</p>\n<ul>\n<li>If you are looking for some ideas on what to contribute, check out <a href=\"https://github.com/uber/petastorm/issues\" rel=\"nofollow\">github issues</a> and comment on the issue.</li>\n<li>If you have an idea for an improvement, or you\u2019d like to report a bug but don\u2019t have time to fix it please a <a href=\"https://github.com/uber/petastorm/issues/new\" rel=\"nofollow\">create a github issue</a>.</li>\n</ul>\n<p>To contribute a patch:</p>\n<ul>\n<li>Break your work into small, single-purpose patches if possible. It\u2019s much harder to merge in a large change with a lot of disjoint features.</li>\n<li>Submit the patch as a GitHub pull request against the master branch. For a tutorial, see the GitHub guides on forking a repo and sending a pull request.</li>\n<li>Include a detailed describtion of the proposed change in the pull request.</li>\n<li>Make sure that your code passes the unit tests. You can find instructions how to run the unit tests <a href=\"https://github.com/uber/petastorm/blob/master/docs/development.rst\" rel=\"nofollow\">here</a>.</li>\n<li>Add new unit tests for your code.</li>\n</ul>\n<p>Thank you in advance for your contributions!</p>\n<p>See the <a href=\"docs/development.rst\" rel=\"nofollow\">Development</a> for development related information.</p>\n</div>\n\n          </div>"}, "last_serial": 5792124, "releases": {"0.6.0": [{"comment_text": "", "digests": {"md5": "9e37cf62eb1608e630f46fac0c2a9bd9", "sha256": "f355bac1aa7eb937af5485ebc745af5074fc8db360a8ace7a1960d3b33cc3885"}, "downloads": -1, "filename": "hops-petastorm-0.6.0.tar.gz", "has_sig": false, "md5_digest": "9e37cf62eb1608e630f46fac0c2a9bd9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 153803, "upload_time": "2019-03-05T21:37:34", "upload_time_iso_8601": "2019-03-05T21:37:34.185790Z", "url": "https://files.pythonhosted.org/packages/d8/90/0d693897fc8cc057c4da47fe1041a2750b6f308c8ddc00d57bb31b451248/hops-petastorm-0.6.0.tar.gz", "yanked": false}], "0.7.0": [{"comment_text": "", "digests": {"md5": "8e9ddf633c20a977dc0496b6dd6b6d11", "sha256": "bb2ab686b4f657589c50a339d6fc5f7966a3c0f1b6bdac22b03d9a017d5718d9"}, "downloads": -1, "filename": "hops-petastorm-0.7.0.tar.gz", "has_sig": false, "md5_digest": "8e9ddf633c20a977dc0496b6dd6b6d11", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 155618, "upload_time": "2019-04-01T14:50:31", "upload_time_iso_8601": "2019-04-01T14:50:31.683043Z", "url": "https://files.pythonhosted.org/packages/30/4d/cad0dc30493799e65e2d755d6e0d37104caeb2c083074058b2fb8e7d3345/hops-petastorm-0.7.0.tar.gz", "yanked": false}], "0.7.2": [{"comment_text": "", "digests": {"md5": "59e3b200f08c1f66d4f42b3f47563b95", "sha256": "aac25fecb40fef786733cf92b62a28b3787521f56d78af06f5fd5f8685513172"}, "downloads": -1, "filename": "hops-petastorm-0.7.2.tar.gz", "has_sig": false, "md5_digest": "59e3b200f08c1f66d4f42b3f47563b95", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 160703, "upload_time": "2019-04-27T16:46:51", "upload_time_iso_8601": "2019-04-27T16:46:51.874779Z", "url": "https://files.pythonhosted.org/packages/3c/fd/3dfc5e6eb3eba2e0a6ea4b0f8ab062c4009d106d17a42a7b6328cbf87c1f/hops-petastorm-0.7.2.tar.gz", "yanked": false}], "0.7.4": [{"comment_text": "", "digests": {"md5": "c7fdf638aa400f9106441e9c555cd5c3", "sha256": "b7aab9a1ef01e93dcb79d96fabaec353f939aa973731f42effe180fe3af3b86e"}, "downloads": -1, "filename": "hops_petastorm-0.7.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c7fdf638aa400f9106441e9c555cd5c3", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 251279, "upload_time": "2019-06-04T11:27:20", "upload_time_iso_8601": "2019-06-04T11:27:20.312953Z", "url": "https://files.pythonhosted.org/packages/de/05/1d27dd7cf6f3f2a2ca9e462469bd869a6b2308cecd357c1acad8194a564f/hops_petastorm-0.7.4-py2.py3-none-any.whl", "yanked": false}], "0.7.6": [{"comment_text": "", "digests": {"md5": "898c8aafdb6c493dd60dd014a4fbb794", "sha256": "a670dba461201048041276daad514e6d6ed1ca836c8506fcd20ba02b29c8a952"}, "downloads": -1, "filename": "hops-petastorm-0.7.6.tar.gz", "has_sig": false, "md5_digest": "898c8aafdb6c493dd60dd014a4fbb794", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 153537, "upload_time": "2019-09-06T12:58:58", "upload_time_iso_8601": "2019-09-06T12:58:58.523155Z", "url": "https://files.pythonhosted.org/packages/f1/a5/6cd312f4745a86f30ea75db75866c6e3fc73b0a729dd6a81b0043e2f6220/hops-petastorm-0.7.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "898c8aafdb6c493dd60dd014a4fbb794", "sha256": "a670dba461201048041276daad514e6d6ed1ca836c8506fcd20ba02b29c8a952"}, "downloads": -1, "filename": "hops-petastorm-0.7.6.tar.gz", "has_sig": false, "md5_digest": "898c8aafdb6c493dd60dd014a4fbb794", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 153537, "upload_time": "2019-09-06T12:58:58", "upload_time_iso_8601": "2019-09-06T12:58:58.523155Z", "url": "https://files.pythonhosted.org/packages/f1/a5/6cd312f4745a86f30ea75db75866c6e3fc73b0a729dd6a81b0043e2f6220/hops-petastorm-0.7.6.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:38 2020"}