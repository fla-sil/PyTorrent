{"info": {"author": "Edmund Martin", "author_email": "", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Operating System :: MacOS :: MacOS X", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "# searchit\nSearchit is a library for async scraping of search engines. The library supports multiple search engines \n(currently Google, Yandex, and Bing) with support for other search engines to come.\n\n# Install\n```\npip install searchit\n```\nCan be installed using pip, by running the above command.\n\n# Using Searchit\n```python\nimport asyncio\n\nfrom searchit import GoogleScraper, YandexScraper, BingScraper\nfrom searchit import ScrapeRequest\n\nrequest = ScrapeRequest(\"watch movies online\", 30)\ngoogle = GoogleScraper(max_results_per_page=10) # max_results = Number of results per page\nyandex = YandexScraper(max_results_per_page=10)\n\nloop = asyncio.get_event_loop()\n\nresults = loop.run_until_complete(google.scrape(request))\nresults = loop.run_until_complete(yandex.scrape(request))\n```\nTo use Searchit users first create a ScrapeRequest object, with term and number of results as required fields. \nThis object can then be passed to multiple different search engines and scraped asynchronously.\n\n## Scrape Request - Object\n```\nterm - Required str - the term to be searched for\ncount - Required int - the total number of results\ndomain - Optional[str] - the domain to search i.e. .com or .com\nsleep - Optional[int] - time to wait betweeen paginating pages - important to prevent getting blocked\nproxy - Optional[str] - proxy to be used to make request - default none\nlanguage - Optional[str] - language to conduct search in (only Google atm)\ngeo - Optional[str] - Geo location to conduct search from Yandex, and Qwant\n```\n\n## Roadmap\n* Add additional search engines\n* Tests\n* Blocking non-async scrape method\n* Add support for page rendering (Selenium and Puppeteer)\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/EdmundMartin/search_it", "keywords": "", "license": "", "maintainer": "Edmund Martin <edmartin101@gmail.com>", "maintainer_email": "edmartin101@gmail.com", "name": "searchit", "package_url": "https://pypi.org/project/searchit/", "platform": "", "project_url": "https://pypi.org/project/searchit/", "project_urls": {"Homepage": "https://github.com/EdmundMartin/search_it"}, "release_url": "https://pypi.org/project/searchit/2019.12.30.2/", "requires_dist": ["aiohttp (>=3.6.2)", "beautifulsoup4 (>=4.8.2)"], "requires_python": ">=3.6", "summary": "Aysncio search engine scraping package", "version": "2019.12.30.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>searchit</h1>\n<p>Searchit is a library for async scraping of search engines. The library supports multiple search engines\n(currently Google, Yandex, and Bing) with support for other search engines to come.</p>\n<h1>Install</h1>\n<pre><code>pip install searchit\n</code></pre>\n<p>Can be installed using pip, by running the above command.</p>\n<h1>Using Searchit</h1>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">asyncio</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">searchit</span> <span class=\"kn\">import</span> <span class=\"n\">GoogleScraper</span><span class=\"p\">,</span> <span class=\"n\">YandexScraper</span><span class=\"p\">,</span> <span class=\"n\">BingScraper</span>\n<span class=\"kn\">from</span> <span class=\"nn\">searchit</span> <span class=\"kn\">import</span> <span class=\"n\">ScrapeRequest</span>\n\n<span class=\"n\">request</span> <span class=\"o\">=</span> <span class=\"n\">ScrapeRequest</span><span class=\"p\">(</span><span class=\"s2\">\"watch movies online\"</span><span class=\"p\">,</span> <span class=\"mi\">30</span><span class=\"p\">)</span>\n<span class=\"n\">google</span> <span class=\"o\">=</span> <span class=\"n\">GoogleScraper</span><span class=\"p\">(</span><span class=\"n\">max_results_per_page</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span> <span class=\"c1\"># max_results = Number of results per page</span>\n<span class=\"n\">yandex</span> <span class=\"o\">=</span> <span class=\"n\">YandexScraper</span><span class=\"p\">(</span><span class=\"n\">max_results_per_page</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n<span class=\"n\">loop</span> <span class=\"o\">=</span> <span class=\"n\">asyncio</span><span class=\"o\">.</span><span class=\"n\">get_event_loop</span><span class=\"p\">()</span>\n\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">loop</span><span class=\"o\">.</span><span class=\"n\">run_until_complete</span><span class=\"p\">(</span><span class=\"n\">google</span><span class=\"o\">.</span><span class=\"n\">scrape</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"p\">))</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">loop</span><span class=\"o\">.</span><span class=\"n\">run_until_complete</span><span class=\"p\">(</span><span class=\"n\">yandex</span><span class=\"o\">.</span><span class=\"n\">scrape</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"p\">))</span>\n</pre>\n<p>To use Searchit users first create a ScrapeRequest object, with term and number of results as required fields.\nThis object can then be passed to multiple different search engines and scraped asynchronously.</p>\n<h2>Scrape Request - Object</h2>\n<pre><code>term - Required str - the term to be searched for\ncount - Required int - the total number of results\ndomain - Optional[str] - the domain to search i.e. .com or .com\nsleep - Optional[int] - time to wait betweeen paginating pages - important to prevent getting blocked\nproxy - Optional[str] - proxy to be used to make request - default none\nlanguage - Optional[str] - language to conduct search in (only Google atm)\ngeo - Optional[str] - Geo location to conduct search from Yandex, and Qwant\n</code></pre>\n<h2>Roadmap</h2>\n<ul>\n<li>Add additional search engines</li>\n<li>Tests</li>\n<li>Blocking non-async scrape method</li>\n<li>Add support for page rendering (Selenium and Puppeteer)</li>\n</ul>\n\n          </div>"}, "last_serial": 6376565, "releases": {"2019.12.29.0": [{"comment_text": "", "digests": {"md5": "eecef218b8dd8d825b1d72e1beeb9a21", "sha256": "deda15a22ff89a29537ac6f5ca4508870e5f3e77c8dea136673b62bc781b10f7"}, "downloads": -1, "filename": "searchit-2019.12.29.0-py3-none-any.whl", "has_sig": false, "md5_digest": "eecef218b8dd8d825b1d72e1beeb9a21", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 5524, "upload_time": "2019-12-29T14:49:02", "upload_time_iso_8601": "2019-12-29T14:49:02.390787Z", "url": "https://files.pythonhosted.org/packages/58/f0/b3b6e60d133a406c5ee29e20db2b78530112dc359d15eecfa74cb0d1ac3a/searchit-2019.12.29.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "46281575522b0c95721e37e492be1221", "sha256": "50a4e094a91c92000804f0a24121acee0133d676243bf0b09041fa4a648e1781"}, "downloads": -1, "filename": "searchit-2019.12.29.0.tar.gz", "has_sig": false, "md5_digest": "46281575522b0c95721e37e492be1221", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 3554, "upload_time": "2019-12-29T14:49:04", "upload_time_iso_8601": "2019-12-29T14:49:04.432353Z", "url": "https://files.pythonhosted.org/packages/c7/54/fce4e2c48928d54141a864b4d030b9054a64ade23b2f4868de365e50a7ba/searchit-2019.12.29.0.tar.gz", "yanked": false}], "2019.12.29.1": [{"comment_text": "", "digests": {"md5": "fa19f053e21b8956bde113d5043c9706", "sha256": "47115c413a12221de08ed542ac1bb24d0855676388b0330d699f080fc57c8afe"}, "downloads": -1, "filename": "searchit-2019.12.29.1-py3-none-any.whl", "has_sig": false, "md5_digest": "fa19f053e21b8956bde113d5043c9706", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 7541, "upload_time": "2019-12-29T20:10:58", "upload_time_iso_8601": "2019-12-29T20:10:58.868554Z", "url": "https://files.pythonhosted.org/packages/02/fd/51cbdb75ecd0a6a0f8c81867a6c32b6669f6528fdf8520166597f71b21b7/searchit-2019.12.29.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c2b2ec980fc846a8f7dae7276bdf8a20", "sha256": "754d72a7c74aefde2a39f24454141d11069a81cf67241af9661eac0e1030604f"}, "downloads": -1, "filename": "searchit-2019.12.29.1.tar.gz", "has_sig": false, "md5_digest": "c2b2ec980fc846a8f7dae7276bdf8a20", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 4855, "upload_time": "2019-12-29T20:11:01", "upload_time_iso_8601": "2019-12-29T20:11:01.347562Z", "url": "https://files.pythonhosted.org/packages/20/f2/fdf443f34618a4a6f4752e67e3030098cf80bc85666e6b27f663c586eb5e/searchit-2019.12.29.1.tar.gz", "yanked": false}], "2019.12.30.1": [{"comment_text": "", "digests": {"md5": "8226563be88bd26044278249b08317f8", "sha256": "1e76d15eff0bfe4ff4f2b22706e314e4bca02e3ca6947be5e2c182b408a03163"}, "downloads": -1, "filename": "searchit-2019.12.30.1-py3-none-any.whl", "has_sig": false, "md5_digest": "8226563be88bd26044278249b08317f8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 20157, "upload_time": "2019-12-30T16:32:05", "upload_time_iso_8601": "2019-12-30T16:32:05.478708Z", "url": "https://files.pythonhosted.org/packages/77/29/9d780437018d774a5ac43e13682fefd00fa4483b0942631c874e95058b52/searchit-2019.12.30.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6e032c055bf1abb8a59739910f89a92d", "sha256": "61c0de8b60e80fdc730f74012fe72a64ecbab97efa963d9dce77a589f0a42742"}, "downloads": -1, "filename": "searchit-2019.12.30.1.tar.gz", "has_sig": false, "md5_digest": "6e032c055bf1abb8a59739910f89a92d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 5021, "upload_time": "2019-12-30T16:32:09", "upload_time_iso_8601": "2019-12-30T16:32:09.009349Z", "url": "https://files.pythonhosted.org/packages/0c/5e/310449bf8fdfa55108bd8c4c89965fa287b17a97e9ccb1a4a66ee32acdad/searchit-2019.12.30.1.tar.gz", "yanked": false}], "2019.12.30.2": [{"comment_text": "", "digests": {"md5": "2fd4df05d806781f7a07d9ff2f80de07", "sha256": "37d12a45783e417bdda27ec699c3dd33525b4887bd6b9bb8264fdc418102618e"}, "downloads": -1, "filename": "searchit-2019.12.30.2-py3-none-any.whl", "has_sig": false, "md5_digest": "2fd4df05d806781f7a07d9ff2f80de07", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 21380, "upload_time": "2019-12-30T19:50:49", "upload_time_iso_8601": "2019-12-30T19:50:49.633570Z", "url": "https://files.pythonhosted.org/packages/71/0c/20ce035bfbf9a015cb9d2e25c394d479b0efa0e594930c742af1212db028/searchit-2019.12.30.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e823312414638b329c3b491f0f5854c6", "sha256": "6f10b39fb73851ad7b9d2c73ea508d98e0cba0e30a84da1ae30852fc6616b0a6"}, "downloads": -1, "filename": "searchit-2019.12.30.2.tar.gz", "has_sig": false, "md5_digest": "e823312414638b329c3b491f0f5854c6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 5437, "upload_time": "2019-12-30T19:50:54", "upload_time_iso_8601": "2019-12-30T19:50:54.086665Z", "url": "https://files.pythonhosted.org/packages/48/c2/4df79da90f2e1dbb7d888e72e4ba0e653c02a9f305935091fd658d1ef379/searchit-2019.12.30.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2fd4df05d806781f7a07d9ff2f80de07", "sha256": "37d12a45783e417bdda27ec699c3dd33525b4887bd6b9bb8264fdc418102618e"}, "downloads": -1, "filename": "searchit-2019.12.30.2-py3-none-any.whl", "has_sig": false, "md5_digest": "2fd4df05d806781f7a07d9ff2f80de07", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 21380, "upload_time": "2019-12-30T19:50:49", "upload_time_iso_8601": "2019-12-30T19:50:49.633570Z", "url": "https://files.pythonhosted.org/packages/71/0c/20ce035bfbf9a015cb9d2e25c394d479b0efa0e594930c742af1212db028/searchit-2019.12.30.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e823312414638b329c3b491f0f5854c6", "sha256": "6f10b39fb73851ad7b9d2c73ea508d98e0cba0e30a84da1ae30852fc6616b0a6"}, "downloads": -1, "filename": "searchit-2019.12.30.2.tar.gz", "has_sig": false, "md5_digest": "e823312414638b329c3b491f0f5854c6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 5437, "upload_time": "2019-12-30T19:50:54", "upload_time_iso_8601": "2019-12-30T19:50:54.086665Z", "url": "https://files.pythonhosted.org/packages/48/c2/4df79da90f2e1dbb7d888e72e4ba0e653c02a9f305935091fd658d1ef379/searchit-2019.12.30.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:05 2020"}