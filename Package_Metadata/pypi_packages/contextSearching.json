{"info": {"author": "Xiao Ma", "author_email": "Marshalma0923@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Software Development :: Build Tools"], "description": "# Introduction\n\nGiven a word token and a corpus where this word appears, this package helps you find and analyze the context in which the word appears. It can be easily leveraged to improve your bag-of-words based analysis.\n\n# Installation\n\n```\npip install contextSearching\n```\n\n# Usage\n\nAs an example to illustrate the usage, we choose the term \"break\" and the Amazon review corpus for Nintendo Switch where people used the term \"break\". \n\nFrom a simple bag-of-words analysis, we know that whenever people mention \"break\", the product is likely to receive a low star rating. But we do not know what breaks or any other context around \"break.\" \n\n### Preparation\n\n\n```python\n\"\"\"\nPreparation\n\"\"\"\nimport pandas as pd\nimport numpy as np\n# read in corpus\ncorpus = pd.read_csv(\"data/switch_w_break.csv\")\n# define the target token\ntarget = \"break\"\n\ncorpus.head()\n```\n\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stars</th>\n      <th>titles</th>\n      <th>reviews</th>\n      <th>dates</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>Already broken parts\\n</td>\n      <td>Only 3 months later and parts are breaking. Th...</td>\n      <td>September 13, 2019</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>Dock is broken\\n</td>\n      <td>Hey. This was supposed to work. Dock is broken...</td>\n      <td>September 11, 2019</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.0</td>\n      <td>Dependable seller\\n</td>\n      <td>Arrived on time,  well packed for the trip.  N...</td>\n      <td>August 10, 2019</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>Nintendo Does Not Honor Warranty\\n</td>\n      <td>My son used this unit for 7 months.  At which ...</td>\n      <td>August 8, 2019</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>Great product, Joycons need work.\\n</td>\n      <td>Everyone knows the switch is great. I waited a...</td>\n      <td>August 5, 2019</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n### Loading the package and initialize the class\n\n\n\n```python\n\"\"\"\nLoading the package and initialize the class\n\"\"\"\nfrom contextSearching import context_searching\n\ncs = context_searching(target_token=target,doc=corpus['reviews'],left_window=5,right_window=5,padding_token=\"_empty_\")\n```\n\nIn addition to the target token and the corpus, the class requires three more inputs: left/right window and padding token.\n\nThe algorithm takes in the target token and aggressively collect all the words within the specified window. \n\nFor example, when left_window is set to 10, it will find the target token within each document of the corpus, then collect all the ten words to the left of the target, recording the relative position. If there are less than 10 words to the left, the algorithm will append the word list with the padding token.\n\n### Get the Context Probing Matrix\n\n\n\n```python\n\"\"\"\nGet the Context Probing Matrix\n\"\"\"\n# Get a list of stopwords\nfrom gensim.parsing.preprocessing import STOPWORDS\nstopwords = list(STOPWORDS)\ncontextPMat = cs.get_context_prob_matrix(stop_words = stopwords,lemmatize=True, stem = False)\n```\n\nAssuming we have N documents in the corpus, and left_window and right window are set to 5. The Context Probing Matrix (CPM) is an N by 11 matrix like below:\n\n\n```python\n# We can examine the actual CPM like this:\ncpm_df = pd.DataFrame(np.array(contextPMat.context_prob_matrix))\ncpm_df.columns = [str(x) for x in contextPMat.position_idx]\ncpm_df.head()\n```\n\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>-5</th>\n      <th>-4</th>\n      <th>-3</th>\n      <th>-2</th>\n      <th>-1</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>_empty_</td>\n      <td>_empty_</td>\n      <td>_empty_</td>\n      <td>month</td>\n      <td>later</td>\n      <td>break</td>\n      <td>joy</td>\n      <td>button</td>\n      <td>work</td>\n      <td>replace</td>\n      <td>_empty_</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>_empty_</td>\n      <td>hey</td>\n      <td>suppose</td>\n      <td>work</td>\n      <td>dock</td>\n      <td>break</td>\n      <td>replace</td>\n      <td>dock</td>\n      <td>pretty</td>\n      <td>angry</td>\n      <td>_empty_</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>_empty_</td>\n      <td>arrive</td>\n      <td>time</td>\n      <td>pack</td>\n      <td>trip</td>\n      <td>break</td>\n      <td>work</td>\n      <td>great</td>\n      <td>_empty_</td>\n      <td>_empty_</td>\n      <td>_empty_</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gadget</td>\n      <td>year</td>\n      <td>expensive</td>\n      <td>relative</td>\n      <td>function</td>\n      <td>break</td>\n      <td>quickly</td>\n      <td>support</td>\n      <td>manufacturer</td>\n      <td>beware</td>\n      <td>nintendo</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>issue</td>\n      <td>real</td>\n      <td>button</td>\n      <td>leave</td>\n      <td>joycon</td>\n      <td>break</td>\n      <td>month</td>\n      <td>fortunately</td>\n      <td>nintendo</td>\n      <td>replace</td>\n      <td>warranty</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nThe column index indicates the relative position. For example, in the first document, the word \"button\" appears two words to the right of the target term \"break\".\n\n### Get the vocabs dictionary\n\n\n\n```python\n\"\"\"\nGet the vocabs dictionary\n\"\"\"\ncontextPMat.vocabs['joycon']\n```\n\n\n\n\n    [-1, -1, -4, -2, -2, -1, -2, 1]\n\n\n\nThe .vocabs is a dictionary whose keys are unique tokens collected in constructing the CPM, and the values are lists of recorded relative positions to the target token.\n\nIn the output above, we see the term \"joycon\" appears 8 times in total within the +- 5 window of the target term. It most often appears on the left side of the target term.\n\n### Get the statistics table for each term\n\n\n\n```python\n\"\"\"\nGet the statistics table for each term\n\"\"\"\ncpm_stats = contextPMat.get_cpm_stats_tb()\ncpm_stats.cpm_stats_tb.head()\n```\n\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tokens</th>\n      <th>mean</th>\n      <th>variance</th>\n      <th>abs_mean</th>\n      <th>count</th>\n      <th>median</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>month</td>\n      <td>0.142857</td>\n      <td>3.979592</td>\n      <td>1.857143</td>\n      <td>14</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>later</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.333333</td>\n      <td>3</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>break</td>\n      <td>0.186441</td>\n      <td>0.643206</td>\n      <td>0.186441</td>\n      <td>118</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>joy</td>\n      <td>-1.444444</td>\n      <td>6.024691</td>\n      <td>2.555556</td>\n      <td>9</td>\n      <td>-2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>button</td>\n      <td>-0.500000</td>\n      <td>2.583333</td>\n      <td>1.500000</td>\n      <td>6</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nTo understand the context, we can look at the statistics of relative positions for each term collected above. \n\nFor example,\n\nWhen the occurrence of a term is high, we know that it always appears around the target token;\n\nWhen the variance of a term's relative position is low, we know that it always appears at the same relative location;\n\n### Infer potential N-grams containing the target term\n\n\n\n```python\n\"\"\"\nInfer potential N-grams containing the term\n\"\"\"\ncpm_stats.guess_ngram(n = 5)\n```\n\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ngram_candidates</th>\n      <th>total_scores</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>look expensive joycon controller break</td>\n      <td>0.327352</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>expensive joycon controller break month</td>\n      <td>0.330129</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>joycon controller break month handle</td>\n      <td>0.337807</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>controller break month handle inside</td>\n      <td>0.379144</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>break month handle inside item</td>\n      <td>0.447479</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nbased on the statistics table, the algorithm can infer most likely n-grams containing the target term\n\nFor example, when we want to infer what is most likely the word appears to the left of \"break\" (i.e. with relative location = -1), we go through the following steps\n\n0. start with a word collected in the CPM constructing process above (e.g. \"controller\")\n1. for the word, take the mean of the observed relative positions, minus the mean by -1 and take the absolute value\n2. for the word, take the median of the observed relative positions, minus the median by -1 and take the absolute value\n3. Calculate 1/count\n4. Calculate the variance of the relative positions of the word\n5. Repeat the above on all the collected words and acquire 4 lists of metrics above (abs median difference, abs mean difference, 1/count, variance)\n6. normalize the 4 lists\n7. for each collected word, multiply its 4 metrics with user-defined weights and take the sum to get a final score\n\nThe best candidate words at location -1 will have the smallest final score.\n\nWhen we want to find the most likely tri-grams, the algorithm considers a trigram with the target token in each possible location. Thus in the example output above, the target term \"break\" appears as the 5th, 4th, 3rd, 2nd and 1st term on the n-gram respectively.\n\nNow we get more context around \"break\": \n\n**Expensive Joycon Controller breaks in months** seem to be the problem.\n\n## Some Notes\n\n1. Currently, when the target term appears more than once in a single document, the CPM only takes the first one into consideration. I will try to improve this in the near future\n\n2. This method works better when we have more documents while each document is short. It will not work well on, for example, a collection of News articles.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/ALaughingHorse/ContextSearching/archive/v_04_2.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ALaughingHorse/ContextSearching", "keywords": "NLP", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "contextSearching", "package_url": "https://pypi.org/project/contextSearching/", "platform": "", "project_url": "https://pypi.org/project/contextSearching/", "project_urls": {"Download": "https://github.com/ALaughingHorse/ContextSearching/archive/v_04_2.tar.gz", "Homepage": "https://github.com/ALaughingHorse/ContextSearching"}, "release_url": "https://pypi.org/project/contextSearching/0.4.2/", "requires_dist": null, "requires_python": "", "summary": "Search the context where a token appears", "version": "0.4.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Introduction</h1>\n<p>Given a word token and a corpus where this word appears, this package helps you find and analyze the context in which the word appears. It can be easily leveraged to improve your bag-of-words based analysis.</p>\n<h1>Installation</h1>\n<pre><code>pip install contextSearching\n</code></pre>\n<h1>Usage</h1>\n<p>As an example to illustrate the usage, we choose the term \"break\" and the Amazon review corpus for Nintendo Switch where people used the term \"break\".</p>\n<p>From a simple bag-of-words analysis, we know that whenever people mention \"break\", the product is likely to receive a low star rating. But we do not know what breaks or any other context around \"break.\"</p>\n<h3>Preparation</h3>\n<pre><span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">Preparation</span>\n<span class=\"sd\">\"\"\"</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"c1\"># read in corpus</span>\n<span class=\"n\">corpus</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s2\">\"data/switch_w_break.csv\"</span><span class=\"p\">)</span>\n<span class=\"c1\"># define the target token</span>\n<span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"s2\">\"break\"</span>\n\n<span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">()</span>\n</pre>\n<div>\n<table>\n  <thead>\n    <tr>\n      <th></th>\n      <th>stars</th>\n      <th>titles</th>\n      <th>reviews</th>\n      <th>dates</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>Already broken parts\\n</td>\n      <td>Only 3 months later and parts are breaking. Th...</td>\n      <td>September 13, 2019</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>Dock is broken\\n</td>\n      <td>Hey. This was supposed to work. Dock is broken...</td>\n      <td>September 11, 2019</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.0</td>\n      <td>Dependable seller\\n</td>\n      <td>Arrived on time,  well packed for the trip.  N...</td>\n      <td>August 10, 2019</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>Nintendo Does Not Honor Warranty\\n</td>\n      <td>My son used this unit for 7 months.  At which ...</td>\n      <td>August 8, 2019</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>Great product, Joycons need work.\\n</td>\n      <td>Everyone knows the switch is great. I waited a...</td>\n      <td>August 5, 2019</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<h3>Loading the package and initialize the class</h3>\n<pre><span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">Loading the package and initialize the class</span>\n<span class=\"sd\">\"\"\"</span>\n<span class=\"kn\">from</span> <span class=\"nn\">contextSearching</span> <span class=\"kn\">import</span> <span class=\"n\">context_searching</span>\n\n<span class=\"n\">cs</span> <span class=\"o\">=</span> <span class=\"n\">context_searching</span><span class=\"p\">(</span><span class=\"n\">target_token</span><span class=\"o\">=</span><span class=\"n\">target</span><span class=\"p\">,</span><span class=\"n\">doc</span><span class=\"o\">=</span><span class=\"n\">corpus</span><span class=\"p\">[</span><span class=\"s1\">'reviews'</span><span class=\"p\">],</span><span class=\"n\">left_window</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"n\">right_window</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"n\">padding_token</span><span class=\"o\">=</span><span class=\"s2\">\"_empty_\"</span><span class=\"p\">)</span>\n</pre>\n<p>In addition to the target token and the corpus, the class requires three more inputs: left/right window and padding token.</p>\n<p>The algorithm takes in the target token and aggressively collect all the words within the specified window.</p>\n<p>For example, when left_window is set to 10, it will find the target token within each document of the corpus, then collect all the ten words to the left of the target, recording the relative position. If there are less than 10 words to the left, the algorithm will append the word list with the padding token.</p>\n<h3>Get the Context Probing Matrix</h3>\n<pre><span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">Get the Context Probing Matrix</span>\n<span class=\"sd\">\"\"\"</span>\n<span class=\"c1\"># Get a list of stopwords</span>\n<span class=\"kn\">from</span> <span class=\"nn\">gensim.parsing.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">STOPWORDS</span>\n<span class=\"n\">stopwords</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">STOPWORDS</span><span class=\"p\">)</span>\n<span class=\"n\">contextPMat</span> <span class=\"o\">=</span> <span class=\"n\">cs</span><span class=\"o\">.</span><span class=\"n\">get_context_prob_matrix</span><span class=\"p\">(</span><span class=\"n\">stop_words</span> <span class=\"o\">=</span> <span class=\"n\">stopwords</span><span class=\"p\">,</span><span class=\"n\">lemmatize</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">stem</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n</pre>\n<p>Assuming we have N documents in the corpus, and left_window and right window are set to 5. The Context Probing Matrix (CPM) is an N by 11 matrix like below:</p>\n<pre><span class=\"c1\"># We can examine the actual CPM like this:</span>\n<span class=\"n\">cpm_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">contextPMat</span><span class=\"o\">.</span><span class=\"n\">context_prob_matrix</span><span class=\"p\">))</span>\n<span class=\"n\">cpm_df</span><span class=\"o\">.</span><span class=\"n\">columns</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">contextPMat</span><span class=\"o\">.</span><span class=\"n\">position_idx</span><span class=\"p\">]</span>\n<span class=\"n\">cpm_df</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">()</span>\n</pre>\n<div>\n<table>\n  <thead>\n    <tr>\n      <th></th>\n      <th>-5</th>\n      <th>-4</th>\n      <th>-3</th>\n      <th>-2</th>\n      <th>-1</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>_empty_</td>\n      <td>_empty_</td>\n      <td>_empty_</td>\n      <td>month</td>\n      <td>later</td>\n      <td>break</td>\n      <td>joy</td>\n      <td>button</td>\n      <td>work</td>\n      <td>replace</td>\n      <td>_empty_</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>_empty_</td>\n      <td>hey</td>\n      <td>suppose</td>\n      <td>work</td>\n      <td>dock</td>\n      <td>break</td>\n      <td>replace</td>\n      <td>dock</td>\n      <td>pretty</td>\n      <td>angry</td>\n      <td>_empty_</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>_empty_</td>\n      <td>arrive</td>\n      <td>time</td>\n      <td>pack</td>\n      <td>trip</td>\n      <td>break</td>\n      <td>work</td>\n      <td>great</td>\n      <td>_empty_</td>\n      <td>_empty_</td>\n      <td>_empty_</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gadget</td>\n      <td>year</td>\n      <td>expensive</td>\n      <td>relative</td>\n      <td>function</td>\n      <td>break</td>\n      <td>quickly</td>\n      <td>support</td>\n      <td>manufacturer</td>\n      <td>beware</td>\n      <td>nintendo</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>issue</td>\n      <td>real</td>\n      <td>button</td>\n      <td>leave</td>\n      <td>joycon</td>\n      <td>break</td>\n      <td>month</td>\n      <td>fortunately</td>\n      <td>nintendo</td>\n      <td>replace</td>\n      <td>warranty</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>The column index indicates the relative position. For example, in the first document, the word \"button\" appears two words to the right of the target term \"break\".</p>\n<h3>Get the vocabs dictionary</h3>\n<pre><span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">Get the vocabs dictionary</span>\n<span class=\"sd\">\"\"\"</span>\n<span class=\"n\">contextPMat</span><span class=\"o\">.</span><span class=\"n\">vocabs</span><span class=\"p\">[</span><span class=\"s1\">'joycon'</span><span class=\"p\">]</span>\n</pre>\n<pre><code>[-1, -1, -4, -2, -2, -1, -2, 1]\n</code></pre>\n<p>The .vocabs is a dictionary whose keys are unique tokens collected in constructing the CPM, and the values are lists of recorded relative positions to the target token.</p>\n<p>In the output above, we see the term \"joycon\" appears 8 times in total within the +- 5 window of the target term. It most often appears on the left side of the target term.</p>\n<h3>Get the statistics table for each term</h3>\n<pre><span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">Get the statistics table for each term</span>\n<span class=\"sd\">\"\"\"</span>\n<span class=\"n\">cpm_stats</span> <span class=\"o\">=</span> <span class=\"n\">contextPMat</span><span class=\"o\">.</span><span class=\"n\">get_cpm_stats_tb</span><span class=\"p\">()</span>\n<span class=\"n\">cpm_stats</span><span class=\"o\">.</span><span class=\"n\">cpm_stats_tb</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">()</span>\n</pre>\n<div>\n<table>\n  <thead>\n    <tr>\n      <th></th>\n      <th>tokens</th>\n      <th>mean</th>\n      <th>variance</th>\n      <th>abs_mean</th>\n      <th>count</th>\n      <th>median</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>month</td>\n      <td>0.142857</td>\n      <td>3.979592</td>\n      <td>1.857143</td>\n      <td>14</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>later</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.333333</td>\n      <td>3</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>break</td>\n      <td>0.186441</td>\n      <td>0.643206</td>\n      <td>0.186441</td>\n      <td>118</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>joy</td>\n      <td>-1.444444</td>\n      <td>6.024691</td>\n      <td>2.555556</td>\n      <td>9</td>\n      <td>-2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>button</td>\n      <td>-0.500000</td>\n      <td>2.583333</td>\n      <td>1.500000</td>\n      <td>6</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>To understand the context, we can look at the statistics of relative positions for each term collected above.</p>\n<p>For example,</p>\n<p>When the occurrence of a term is high, we know that it always appears around the target token;</p>\n<p>When the variance of a term's relative position is low, we know that it always appears at the same relative location;</p>\n<h3>Infer potential N-grams containing the target term</h3>\n<pre><span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">Infer potential N-grams containing the term</span>\n<span class=\"sd\">\"\"\"</span>\n<span class=\"n\">cpm_stats</span><span class=\"o\">.</span><span class=\"n\">guess_ngram</span><span class=\"p\">(</span><span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n</pre>\n<div>\n<table>\n  <thead>\n    <tr>\n      <th></th>\n      <th>ngram_candidates</th>\n      <th>total_scores</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>look expensive joycon controller break</td>\n      <td>0.327352</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>expensive joycon controller break month</td>\n      <td>0.330129</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>joycon controller break month handle</td>\n      <td>0.337807</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>controller break month handle inside</td>\n      <td>0.379144</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>break month handle inside item</td>\n      <td>0.447479</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>based on the statistics table, the algorithm can infer most likely n-grams containing the target term</p>\n<p>For example, when we want to infer what is most likely the word appears to the left of \"break\" (i.e. with relative location = -1), we go through the following steps</p>\n<ol>\n<li>start with a word collected in the CPM constructing process above (e.g. \"controller\")</li>\n<li>for the word, take the mean of the observed relative positions, minus the mean by -1 and take the absolute value</li>\n<li>for the word, take the median of the observed relative positions, minus the median by -1 and take the absolute value</li>\n<li>Calculate 1/count</li>\n<li>Calculate the variance of the relative positions of the word</li>\n<li>Repeat the above on all the collected words and acquire 4 lists of metrics above (abs median difference, abs mean difference, 1/count, variance)</li>\n<li>normalize the 4 lists</li>\n<li>for each collected word, multiply its 4 metrics with user-defined weights and take the sum to get a final score</li>\n</ol>\n<p>The best candidate words at location -1 will have the smallest final score.</p>\n<p>When we want to find the most likely tri-grams, the algorithm considers a trigram with the target token in each possible location. Thus in the example output above, the target term \"break\" appears as the 5th, 4th, 3rd, 2nd and 1st term on the n-gram respectively.</p>\n<p>Now we get more context around \"break\":</p>\n<p><strong>Expensive Joycon Controller breaks in months</strong> seem to be the problem.</p>\n<h2>Some Notes</h2>\n<ol>\n<li>\n<p>Currently, when the target term appears more than once in a single document, the CPM only takes the first one into consideration. I will try to improve this in the near future</p>\n</li>\n<li>\n<p>This method works better when we have more documents while each document is short. It will not work well on, for example, a collection of News articles.</p>\n</li>\n</ol>\n\n          </div>"}, "last_serial": 7193401, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "02848e76dea3c31b62f17ed6ad1e3059", "sha256": "e88209d1bce675e8f3ed4a3518ccb2d20826df9b4d248efd1e686d970fde82e3"}, "downloads": -1, "filename": "contextSearching-0.1.tar.gz", "has_sig": false, "md5_digest": "02848e76dea3c31b62f17ed6ad1e3059", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3830, "upload_time": "2019-09-21T18:59:40", "upload_time_iso_8601": "2019-09-21T18:59:40.352803Z", "url": "https://files.pythonhosted.org/packages/28/15/a6786df576d45b5d760b67e4c7a5bd62772ab0579dbfa52a50a9c8e7de08/contextSearching-0.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "5ea4cf37549687a68de756f95eb70b8a", "sha256": "59a4c55c516875f11e0cb816b8ff86ee91ac78f042e28544eff90fd598767874"}, "downloads": -1, "filename": "contextSearching-0.2.tar.gz", "has_sig": false, "md5_digest": "5ea4cf37549687a68de756f95eb70b8a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4281, "upload_time": "2019-09-26T18:56:36", "upload_time_iso_8601": "2019-09-26T18:56:36.593579Z", "url": "https://files.pythonhosted.org/packages/a4/be/9fd3e84e66c9b0cea096eeb4935593bba6c05588dd60de34a90c17263d7b/contextSearching-0.2.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "d4402d3e9fb5f162fe02e20a78b9773e", "sha256": "e8cba7c1b6eb68b43758f6c45a38a61ffae4a899e96c7987969ec97ae5a19238"}, "downloads": -1, "filename": "contextSearching-0.3.tar.gz", "has_sig": false, "md5_digest": "d4402d3e9fb5f162fe02e20a78b9773e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10011, "upload_time": "2019-12-09T23:14:07", "upload_time_iso_8601": "2019-12-09T23:14:07.490443Z", "url": "https://files.pythonhosted.org/packages/56/19/d0ba76a458331b4341fb5b83f3ed4e318e840c3386a3d5b4af3cb460e287/contextSearching-0.3.tar.gz", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "6fdf11a400c3797cc44fbbe8016e4004", "sha256": "bbb7455863bbe2d112672a909f9d7a1d6b9b1769abef06b1883da4b73f2ee65c"}, "downloads": -1, "filename": "contextSearching-0.4.tar.gz", "has_sig": false, "md5_digest": "6fdf11a400c3797cc44fbbe8016e4004", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10016, "upload_time": "2019-12-09T23:19:00", "upload_time_iso_8601": "2019-12-09T23:19:00.959387Z", "url": "https://files.pythonhosted.org/packages/e1/4a/064f809775e3ea7c88e82a3271da7ffefc32b910d0aa504d754be2122b65/contextSearching-0.4.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "1fd0387e1fa3b67c680bec75ddb9d08b", "sha256": "b7928d103e92d70968779f7fcda45a0112edd93082d70418aac989b179ad7c95"}, "downloads": -1, "filename": "contextSearching-0.4.1.tar.gz", "has_sig": false, "md5_digest": "1fd0387e1fa3b67c680bec75ddb9d08b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10037, "upload_time": "2019-12-10T00:19:05", "upload_time_iso_8601": "2019-12-10T00:19:05.033336Z", "url": "https://files.pythonhosted.org/packages/58/4e/bd819d83a8375a9e5cd1e1d5c350894e6358617dfbd99daf40e95e7ba516/contextSearching-0.4.1.tar.gz", "yanked": false}], "0.4.2": [{"comment_text": "", "digests": {"md5": "a3ac7e748618414ee7ee93ad39b6fc5b", "sha256": "a3bd735a51c5dbb2b7db7f8cb335fd5c5536a8a72a1b8d79d074eaa0bc136d15"}, "downloads": -1, "filename": "contextSearching-0.4.2.tar.gz", "has_sig": false, "md5_digest": "a3ac7e748618414ee7ee93ad39b6fc5b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9808, "upload_time": "2020-05-08T01:06:38", "upload_time_iso_8601": "2020-05-08T01:06:38.087132Z", "url": "https://files.pythonhosted.org/packages/57/63/c94985cdbe5326031e29d960968fc59cedbc38aabba178a11d7c1f1f671b/contextSearching-0.4.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a3ac7e748618414ee7ee93ad39b6fc5b", "sha256": "a3bd735a51c5dbb2b7db7f8cb335fd5c5536a8a72a1b8d79d074eaa0bc136d15"}, "downloads": -1, "filename": "contextSearching-0.4.2.tar.gz", "has_sig": false, "md5_digest": "a3ac7e748618414ee7ee93ad39b6fc5b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9808, "upload_time": "2020-05-08T01:06:38", "upload_time_iso_8601": "2020-05-08T01:06:38.087132Z", "url": "https://files.pythonhosted.org/packages/57/63/c94985cdbe5326031e29d960968fc59cedbc38aabba178a11d7c1f1f671b/contextSearching-0.4.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:32 2020"}