{"info": {"author": "Reza Sherafat", "author_email": "sherafat.us@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Tic Tac Toe Game in OpenAI Gym\nThe 3D version of Tic Tac Toe is implemented as an OpenAI's Gym environment. The [`learning`](./learning) folder includes several Jupyter notebooks for deep neural network models used to implement a computer-based player.\n\n## Complexity\nThe traditional (2D) Tic Tac Toe has a very small game space (9^3). In comparison, the 3D version in this repo has a much larger space which is in the order of 81^3. This makes computer-based players using search and pruning techniques of the game space prohibitively expensive.\n\nRather, the current learning models are based on policy gradient and deep Q-learning. The [DQN model](learning/TicTacToe-RL-DQN-TF-v2.ipynb) has produced very promising results. Feel free to experience on your own and contribute if interested. The [PG-based model](learning/TicTacToe-RL-PG-TF.ipynb) needs more work :)\n\n## Contributions\nThe repo is also open for pull requests and collaborations both in game development as well as learning.\n\n## Dependencies\n- Base dependency: `gym`.\n- Plot-rendering dependencies: `numpy`, `matplotlib`.\n- DQN learning dependencies: `tensorflow`, `numpy`.\n\n## Installation\nTo install run:\n```console\n# In your virtual environment\npip install gym-tictactoe\n```\n\n## Usage\nCurrently 2 types of environments with different rendering modes are supported.\n\n### Textual rendering\nTo use textual rendering create environment as `tictactoe-v0` like so:\n```python\nimport gym\nimport gym_tictactoe\n\ndef play_game(actions, step_fn=input):\n  env = gym.make('tictactoe-v0')\n  env.reset()\n\n  # Play actions in action profile\n  for action in actions:\n    print(env.step(action))\n    env.render()\n    if step_fn:\n      step_fn()\n  return env\n\nactions = ['1021', '2111', '1221', '2222', '1121']\n_ = play_game(actions, None)\n```\nThe output produced is:\n\n```\nStep 1:\n- - -    - - -    - - -    \n- - x    - - -    - - -    \n- - -    - - -    - - -    \n\nStep 2:\n- - -    - - -    - - -    \n- - x    - o -    - - -    \n- - -    - - -    - - -    \n\nStep 3:\n- - -    - - -    - - -    \n- - x    - o -    - - x    \n- - -    - - -    - - -    \n\nStep 4:\n- - -    - - -    - - -    \n- - x    - o -    - - x    \n- - -    - - -    - - o    \n\nStep 5:\n- - -    - - -    - - -    \n- - X    - o X    - - X    \n- - -    - - -    - - o   \n```\nThe winning sequence after gameplay: `(0,2,1), (1,2,1), (2,2,1)`.\n\n### Plotted rendering\nTo use textual rendering create environment as `tictactoe-plt-v0` like so:\n```python\nimport gym\nimport gym_tictactoe\n\ndef play_game(actions, step_fn=input):\n  env = gym.make('tictactoe-plt-v0')\n  env.reset()\n\n  # Play actions in action profile\n  for action in actions:\n    print(env.step(action))\n    env.render()\n    if step_fn:\n      step_fn()\n  return env\n\nactions = ['1021', '2111', '1221', '2222', '1121']\n_ = play_game(actions, None)\n```\nThis produces the following gameplay:\n\nStep 1:\n<p style='text-align:center' >\n  <img src='./media/game-play1-1.png'></img>\n</p>\nStep 2:\n<p style='text-align:center' >\n  <img src='./media/game-play1-2.png'></img>\n</p>\nStep 3:\n<p style='text-align:center' >\n  <img src='./media/game-play1-3.png'></img>\n</p>\nStep 4:\n<p style='text-align:center' >\n  <img src='./media/game-play1-4.png'></img>\n</p>\nStep 5:\n<p style='text-align:center' >\n  <img src='./media/game-play1-5.png'></img>\n</p>\n\n\n## DQN Learning\nThe current models are under [`learning`](./learning) folder. See [Jupyter notebook](./learning/TicTacToe-RL-DQN-TF-v2-eval.ipynb) for a DQN learning with a 2-layer neural network and using actor-critic technique.\n\nSample game plays produced by the trained model (the winning sequence is `(0,0,0), (1,0,0), (2,0,0)`):\n<p style='text-align:center' >\n  <img src='./media/game-play-1.png'></img>\n</p>\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/shkreza/gym-tictactoe", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "gym-tictactoe", "package_url": "https://pypi.org/project/gym-tictactoe/", "platform": "", "project_url": "https://pypi.org/project/gym-tictactoe/", "project_urls": {"Homepage": "https://github.com/shkreza/gym-tictactoe"}, "release_url": "https://pypi.org/project/gym-tictactoe/0.30/", "requires_dist": ["gym", "matplotlib", "numpy"], "requires_python": "", "summary": "Tic-Tac-Toe environment in OpenAI gym", "version": "0.30", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Tic Tac Toe Game in OpenAI Gym</h1>\n<p>The 3D version of Tic Tac Toe is implemented as an OpenAI's Gym environment. The <a href=\"./learning\" rel=\"nofollow\"><code>learning</code></a> folder includes several Jupyter notebooks for deep neural network models used to implement a computer-based player.</p>\n<h2>Complexity</h2>\n<p>The traditional (2D) Tic Tac Toe has a very small game space (9^3). In comparison, the 3D version in this repo has a much larger space which is in the order of 81^3. This makes computer-based players using search and pruning techniques of the game space prohibitively expensive.</p>\n<p>Rather, the current learning models are based on policy gradient and deep Q-learning. The <a href=\"learning/TicTacToe-RL-DQN-TF-v2.ipynb\" rel=\"nofollow\">DQN model</a> has produced very promising results. Feel free to experience on your own and contribute if interested. The <a href=\"learning/TicTacToe-RL-PG-TF.ipynb\" rel=\"nofollow\">PG-based model</a> needs more work :)</p>\n<h2>Contributions</h2>\n<p>The repo is also open for pull requests and collaborations both in game development as well as learning.</p>\n<h2>Dependencies</h2>\n<ul>\n<li>Base dependency: <code>gym</code>.</li>\n<li>Plot-rendering dependencies: <code>numpy</code>, <code>matplotlib</code>.</li>\n<li>DQN learning dependencies: <code>tensorflow</code>, <code>numpy</code>.</li>\n</ul>\n<h2>Installation</h2>\n<p>To install run:</p>\n<pre><span class=\"gp\">#</span> In your virtual environment\n<span class=\"go\">pip install gym-tictactoe</span>\n</pre>\n<h2>Usage</h2>\n<p>Currently 2 types of environments with different rendering modes are supported.</p>\n<h3>Textual rendering</h3>\n<p>To use textual rendering create environment as <code>tictactoe-v0</code> like so:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">gym</span>\n<span class=\"kn\">import</span> <span class=\"nn\">gym_tictactoe</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">play_game</span><span class=\"p\">(</span><span class=\"n\">actions</span><span class=\"p\">,</span> <span class=\"n\">step_fn</span><span class=\"o\">=</span><span class=\"nb\">input</span><span class=\"p\">):</span>\n  <span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s1\">'tictactoe-v0'</span><span class=\"p\">)</span>\n  <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n\n  <span class=\"c1\"># Play actions in action profile</span>\n  <span class=\"k\">for</span> <span class=\"n\">action</span> <span class=\"ow\">in</span> <span class=\"n\">actions</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">action</span><span class=\"p\">))</span>\n    <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">render</span><span class=\"p\">()</span>\n    <span class=\"k\">if</span> <span class=\"n\">step_fn</span><span class=\"p\">:</span>\n      <span class=\"n\">step_fn</span><span class=\"p\">()</span>\n  <span class=\"k\">return</span> <span class=\"n\">env</span>\n\n<span class=\"n\">actions</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'1021'</span><span class=\"p\">,</span> <span class=\"s1\">'2111'</span><span class=\"p\">,</span> <span class=\"s1\">'1221'</span><span class=\"p\">,</span> <span class=\"s1\">'2222'</span><span class=\"p\">,</span> <span class=\"s1\">'1121'</span><span class=\"p\">]</span>\n<span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">play_game</span><span class=\"p\">(</span><span class=\"n\">actions</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>\n</pre>\n<p>The output produced is:</p>\n<pre><code>Step 1:\n- - -    - - -    - - -    \n- - x    - - -    - - -    \n- - -    - - -    - - -    \n\nStep 2:\n- - -    - - -    - - -    \n- - x    - o -    - - -    \n- - -    - - -    - - -    \n\nStep 3:\n- - -    - - -    - - -    \n- - x    - o -    - - x    \n- - -    - - -    - - -    \n\nStep 4:\n- - -    - - -    - - -    \n- - x    - o -    - - x    \n- - -    - - -    - - o    \n\nStep 5:\n- - -    - - -    - - -    \n- - X    - o X    - - X    \n- - -    - - -    - - o   \n</code></pre>\n<p>The winning sequence after gameplay: <code>(0,2,1), (1,2,1), (2,2,1)</code>.</p>\n<h3>Plotted rendering</h3>\n<p>To use textual rendering create environment as <code>tictactoe-plt-v0</code> like so:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">gym</span>\n<span class=\"kn\">import</span> <span class=\"nn\">gym_tictactoe</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">play_game</span><span class=\"p\">(</span><span class=\"n\">actions</span><span class=\"p\">,</span> <span class=\"n\">step_fn</span><span class=\"o\">=</span><span class=\"nb\">input</span><span class=\"p\">):</span>\n  <span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s1\">'tictactoe-plt-v0'</span><span class=\"p\">)</span>\n  <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n\n  <span class=\"c1\"># Play actions in action profile</span>\n  <span class=\"k\">for</span> <span class=\"n\">action</span> <span class=\"ow\">in</span> <span class=\"n\">actions</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">action</span><span class=\"p\">))</span>\n    <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">render</span><span class=\"p\">()</span>\n    <span class=\"k\">if</span> <span class=\"n\">step_fn</span><span class=\"p\">:</span>\n      <span class=\"n\">step_fn</span><span class=\"p\">()</span>\n  <span class=\"k\">return</span> <span class=\"n\">env</span>\n\n<span class=\"n\">actions</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'1021'</span><span class=\"p\">,</span> <span class=\"s1\">'2111'</span><span class=\"p\">,</span> <span class=\"s1\">'1221'</span><span class=\"p\">,</span> <span class=\"s1\">'2222'</span><span class=\"p\">,</span> <span class=\"s1\">'1121'</span><span class=\"p\">]</span>\n<span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">play_game</span><span class=\"p\">(</span><span class=\"n\">actions</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>\n</pre>\n<p>This produces the following gameplay:</p>\n<p>Step 1:</p>\n<p>\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/94519569793f5e3c1b750812acead0c1344ce12a/2e2f6d656469612f67616d652d706c6179312d312e706e67\">\n</p>\nStep 2:\n<p>\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/368949608810a9b3111db0a68fcc47aa5ef6612a/2e2f6d656469612f67616d652d706c6179312d322e706e67\">\n</p>\nStep 3:\n<p>\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8e97a7e151c2b5c85b54a9f6a78ddb22420e2eb8/2e2f6d656469612f67616d652d706c6179312d332e706e67\">\n</p>\nStep 4:\n<p>\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3b6e0baf8eb6f3aebb8aeaaf64ecc1c1a501a47c/2e2f6d656469612f67616d652d706c6179312d342e706e67\">\n</p>\nStep 5:\n<p>\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/15df2dad6e7b95573a6c1072451290ab6a80fd6b/2e2f6d656469612f67616d652d706c6179312d352e706e67\">\n</p>\n<h2>DQN Learning</h2>\n<p>The current models are under <a href=\"./learning\" rel=\"nofollow\"><code>learning</code></a> folder. See <a href=\"./learning/TicTacToe-RL-DQN-TF-v2-eval.ipynb\" rel=\"nofollow\">Jupyter notebook</a> for a DQN learning with a 2-layer neural network and using actor-critic technique.</p>\n<p>Sample game plays produced by the trained model (the winning sequence is <code>(0,0,0), (1,0,0), (2,0,0)</code>):</p>\n<p>\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bd9dc07ac61f8939dc7911c0163001f72e78a6ff/2e2f6d656469612f67616d652d706c61792d312e706e67\">\n</p>\n\n          </div>"}, "last_serial": 4667123, "releases": {"0.30": [{"comment_text": "", "digests": {"md5": "7ef844626c8d534fc9ab349744a8f527", "sha256": "389e9078990a1f1419dfe09f3881e75d0d8405cbc79f34ab7e470d3bdca91c0c"}, "downloads": -1, "filename": "gym_tictactoe-0.30-py3-none-any.whl", "has_sig": false, "md5_digest": "7ef844626c8d534fc9ab349744a8f527", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5800, "upload_time": "2019-01-06T19:16:10", "upload_time_iso_8601": "2019-01-06T19:16:10.733857Z", "url": "https://files.pythonhosted.org/packages/6e/29/368a5dc8abc95ced695c458fa5bf8175f5941ed8404b2f0337bc331506d2/gym_tictactoe-0.30-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "7ef844626c8d534fc9ab349744a8f527", "sha256": "389e9078990a1f1419dfe09f3881e75d0d8405cbc79f34ab7e470d3bdca91c0c"}, "downloads": -1, "filename": "gym_tictactoe-0.30-py3-none-any.whl", "has_sig": false, "md5_digest": "7ef844626c8d534fc9ab349744a8f527", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5800, "upload_time": "2019-01-06T19:16:10", "upload_time_iso_8601": "2019-01-06T19:16:10.733857Z", "url": "https://files.pythonhosted.org/packages/6e/29/368a5dc8abc95ced695c458fa5bf8175f5941ed8404b2f0337bc331506d2/gym_tictactoe-0.30-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 00:52:59 2020"}