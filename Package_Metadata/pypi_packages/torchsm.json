{"info": {"author": "Federico A. Galatolo", "author_email": "federico.galatolo@ing.unipi.it", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Science/Research", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# torchsm\npytorch implementation of the **Stigmergic Memory** as presented in the paper [Using stigmergy as a computational memory in the design of recurrent neural networks]().\n\nYou can use this package to **easly** integrate our model into existing ones\n\nYou can safely **mix** native pytorch Modules with ours.  \n\nBut **do not forget** to `reset()` them before starting every new time sequence\n\nImplementing our [proposed architecture to solve MNIST]() becomes as easy as:\n```python\nimport torch\nimport torchsm\n\nnet = torchsm.Sequential(\n    torchsm.RecurrentStigmergicMemoryLayer(28, 15, hidden_layers=1, hidden_dim=20),\n    torch.nn.Linear(15, 10),\n    torch.nn.PReLU(),\n    torch.nn.Linear(10, 10),\n    torch.nn.PReLU()\n)\n```\n\nYou can train the time-unfolded model by computing the loss function on the desired temporal output\n\n```python\noptimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\nloss_fn = torch.nn.MSELoss()\n\nfor i in range(0,N):\n    for X, Y in zip(dataset_X, dataset_Y):\n        net.reset()\n        out = None\n        for i in range(0, X.shape[1]):\n            out = net(torch.tensor(X[:,i], dtype=torch.float32))\n        \n        loss = loss_fn(out, Y)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```\n\n### Does it support batch inputs?\n\nYes! The inputs have to be batched\n\n```python\nfor t in range(0, num_ticks):\n    batch_out[0], batch_out[1], ... = net(torch.tensor([batch_in[0][t], batch_in[1][t], ...]))\n```\n### Can it run on CUDA?\n\nYes and as you will expect from a pytorch Module!  \nYou just need to call the `to(device)` method on a model to move it in the GPU memory\n\n```python\ndevice = torch.device(\"cuda\")\n\nnet = net.to(device)\n\nnet(torch.tensor(..., device=device))\n```\n## Documentation\n\n### torchsm.Sequential\n\nWrapper of `torch.nn.Sequential` that adds the `reset()` method and forward the call to each `torchsm.BaseLayer` child.\n\nIf you want to use a `SequentialContaier` to build your models with one or more torchsm's layers you have to use `torchsm.Sequential` instead of `torch.nn.Sequential` in order to be able to `reset()` them.\n\n### torchsm.StigmergicMemoryLayer\n\nThis layer has two hidden ANNs with the layer's inputs as inputs and which outputs respectively determine the marks and ticks of a multi-monodimensional stigmergic space.\n\n![Imgur](https://i.imgur.com/yS4M4nA.png)\n\n### torchsm.RecurrentStigmergicMemoryLayer\n\nThis layer is a StigmergicMemoryLayer which output is normalized by a linear layer and recurrently forwarded as input to the two hidden ANNs \n\n![Imgur](https://i.imgur.com/JWQF6ft.png)\n\n## Citing\n\nWe can't wait to see what you will build with torchsm!  \nWhen you will publish your work you can use this BibTex to cite us :)\n\n```\n@article{galatolo_snn\n,\tauthor\t= {Galatolo, Federico A and Cimino, Mario GCA and Vaglini, Gigliola}\n,\ttitle\t= {Using stigmergy as a computational memory in the design of recurrent neural networks}\n,\tjournal\t= {ICPRAM 2019}\n,\tyear\t= {2019}\n,\tpages\t= {}\n}\n```\n\n## Contributing\n\nThis code is released under GNU/GPLv3 so feel free to fork it and submit your changes, every PR helps.  \nIf you need help using it or for any question please reach me at [federico.galatolo@ing.unipi.it](mailto:galatolo.federico@gmail.com) or on Telegram  [@galatolo](https://t.me/galatolo)", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/galatolofederico/torchsm", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "torchsm", "package_url": "https://pypi.org/project/torchsm/", "platform": "", "project_url": "https://pypi.org/project/torchsm/", "project_urls": {"Homepage": "https://github.com/galatolofederico/torchsm"}, "release_url": "https://pypi.org/project/torchsm/0.1.0/", "requires_dist": null, "requires_python": "", "summary": "Pytorch implementation of the Stigmergic Memory", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>torchsm</h1>\n<p>pytorch implementation of the <strong>Stigmergic Memory</strong> as presented in the paper <a href=\"\" rel=\"nofollow\">Using stigmergy as a computational memory in the design of recurrent neural networks</a>.</p>\n<p>You can use this package to <strong>easly</strong> integrate our model into existing ones</p>\n<p>You can safely <strong>mix</strong> native pytorch Modules with ours.</p>\n<p>But <strong>do not forget</strong> to <code>reset()</code> them before starting every new time sequence</p>\n<p>Implementing our <a href=\"\" rel=\"nofollow\">proposed architecture to solve MNIST</a> becomes as easy as:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchsm</span>\n\n<span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">torchsm</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n    <span class=\"n\">torchsm</span><span class=\"o\">.</span><span class=\"n\">RecurrentStigmergicMemoryLayer</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"n\">hidden_layers</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">hidden_dim</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">),</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">PReLU</span><span class=\"p\">(),</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">PReLU</span><span class=\"p\">()</span>\n<span class=\"p\">)</span>\n</pre>\n<p>You can train the time-unfolded model by computing the loss function on the desired temporal output</p>\n<pre><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span><span class=\"p\">)</span>\n<span class=\"n\">loss_fn</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">MSELoss</span><span class=\"p\">()</span>\n\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"n\">N</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">dataset_X</span><span class=\"p\">,</span> <span class=\"n\">dataset_Y</span><span class=\"p\">):</span>\n        <span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n        <span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]):</span>\n            <span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[:,</span><span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">))</span>\n        \n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">out</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n        \n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<h3>Does it support batch inputs?</h3>\n<p>Yes! The inputs have to be batched</p>\n<pre><span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_ticks</span><span class=\"p\">):</span>\n    <span class=\"n\">batch_out</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">batch_out</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"o\">...</span> <span class=\"o\">=</span> <span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"n\">batch_in</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"n\">t</span><span class=\"p\">],</span> <span class=\"n\">batch_in</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">][</span><span class=\"n\">t</span><span class=\"p\">],</span> <span class=\"o\">...</span><span class=\"p\">]))</span>\n</pre>\n<h3>Can it run on CUDA?</h3>\n<p>Yes and as you will expect from a pytorch Module!<br>\nYou just need to call the <code>to(device)</code> method on a model to move it in the GPU memory</p>\n<pre><span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">\"cuda\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n\n<span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">))</span>\n</pre>\n<h2>Documentation</h2>\n<h3>torchsm.Sequential</h3>\n<p>Wrapper of <code>torch.nn.Sequential</code> that adds the <code>reset()</code> method and forward the call to each <code>torchsm.BaseLayer</code> child.</p>\n<p>If you want to use a <code>SequentialContaier</code> to build your models with one or more torchsm's layers you have to use <code>torchsm.Sequential</code> instead of <code>torch.nn.Sequential</code> in order to be able to <code>reset()</code> them.</p>\n<h3>torchsm.StigmergicMemoryLayer</h3>\n<p>This layer has two hidden ANNs with the layer's inputs as inputs and which outputs respectively determine the marks and ticks of a multi-monodimensional stigmergic space.</p>\n<p><img alt=\"Imgur\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ea92719b373544f93db0bfe4d14bbae357dd0163/68747470733a2f2f692e696d6775722e636f6d2f7953344d346e412e706e67\"></p>\n<h3>torchsm.RecurrentStigmergicMemoryLayer</h3>\n<p>This layer is a StigmergicMemoryLayer which output is normalized by a linear layer and recurrently forwarded as input to the two hidden ANNs</p>\n<p><img alt=\"Imgur\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7c0f3c2c2d4281ce9729efff828a87bb8a8fcfcb/68747470733a2f2f692e696d6775722e636f6d2f4a5751463666742e706e67\"></p>\n<h2>Citing</h2>\n<p>We can't wait to see what you will build with torchsm!<br>\nWhen you will publish your work you can use this BibTex to cite us :)</p>\n<pre><code>@article{galatolo_snn\n,\tauthor\t= {Galatolo, Federico A and Cimino, Mario GCA and Vaglini, Gigliola}\n,\ttitle\t= {Using stigmergy as a computational memory in the design of recurrent neural networks}\n,\tjournal\t= {ICPRAM 2019}\n,\tyear\t= {2019}\n,\tpages\t= {}\n}\n</code></pre>\n<h2>Contributing</h2>\n<p>This code is released under GNU/GPLv3 so feel free to fork it and submit your changes, every PR helps.<br>\nIf you need help using it or for any question please reach me at <a href=\"mailto:galatolo.federico@gmail.com\">federico.galatolo@ing.unipi.it</a> or on Telegram  <a href=\"https://t.me/galatolo\" rel=\"nofollow\">@galatolo</a></p>\n\n          </div>"}, "last_serial": 4695316, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "88957dba8b3e68739b617de94e58b46b", "sha256": "e83698a788ab6c2145be331636f0a1822d87cb0e4054a78b767ee3ae15c669d3"}, "downloads": -1, "filename": "torchsm-0.1.0.tar.gz", "has_sig": false, "md5_digest": "88957dba8b3e68739b617de94e58b46b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4372, "upload_time": "2019-01-14T18:19:45", "upload_time_iso_8601": "2019-01-14T18:19:45.099910Z", "url": "https://files.pythonhosted.org/packages/2f/ff/707cdcb31a93a47c88b680e44bc87273674c2018bb178d39a96dd5f6d9dc/torchsm-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "88957dba8b3e68739b617de94e58b46b", "sha256": "e83698a788ab6c2145be331636f0a1822d87cb0e4054a78b767ee3ae15c669d3"}, "downloads": -1, "filename": "torchsm-0.1.0.tar.gz", "has_sig": false, "md5_digest": "88957dba8b3e68739b617de94e58b46b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4372, "upload_time": "2019-01-14T18:19:45", "upload_time_iso_8601": "2019-01-14T18:19:45.099910Z", "url": "https://files.pythonhosted.org/packages/2f/ff/707cdcb31a93a47c88b680e44bc87273674c2018bb178d39a96dd5f6d9dc/torchsm-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:10 2020"}