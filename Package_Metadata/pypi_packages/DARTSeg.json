{"info": {"author": "Razavian Lab", "author_email": "ark576@nyu.edu", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# DenseUnet-based Automatic Rapid brain Segmentation (DARTS)\n\n## Paper Associated with the project\nHere is the paper describing the project and experiments in detail (Link to the paper to be updated).\n\n## Deep learning models for brain MR segmentation\nWe pretrain our Dense Unet model using the Freesurfer segmentations of 1113 subjects available in the [Human Connectome Project](https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release) dataset and fine-tuned the model using 101 manually labeled brain scans from [Mindboggle](https://mindboggle.info/data.html) dataset.\n\nThe model is able to perform the segmentation of complete brain **within a minute** (on a machine with single GPU). The model labels 102 regions in the brain making it the first model to segment more than 100 brain regions within a minute. The details of 102 regions can be found below.\n\n\n\n## Results on the Mindboggle held out data\nThe box plot compares the dice scores of different ROIs for Dense U-Net and U-Net. The Dense U-Net consistently outperforms U-Net and achieves good dice scores for most of the ROIs.\n\n<img src=\"plots/compare_dice_plot_aparc_manual_fd_part_1_dn_v_unet.png\" width=\"800\" /> \n<img src=\"plots/compare_dice_plot_aparc_manual_fd_part_2_dn_v_unet.png\" width=\"800\" /> \n\n\n## Using Pretrained models for performing complete brain segmentation\nThe users can use the pre-trained models to perform a complete brain MR segmentation. For using the **coronally** pre-trained models, the user will have to execute the [`perform_pred.py`](https://github.com/NYUMedML/BrainSeg/blob/master/perform_pred.py) script. An illustration can be seen in [`predicting_segmentation_illustration.ipynb`](https://github.com/NYUMedML/BrainSeg/blob/master/predicting_segmentation_illustration.ipynb) notebook.\n\nThe following code block could be used to perform the prediction:\n```\nusage: perform_pred.py [-h] [--input_image_path INPUT_IMAGE_PATH]\n                       [--segmentation_dir_path SEGMENTATION_DIR_PATH]\n                       [--file_name FILE_NAME] [--model_type MODEL_TYPE]\n                       [--model_wts_path MODEL_WTS_PATH] [--is_mgz [IS_MGZ]]\n                       [--save_prob [SAVE_PROB]] [--use_gpu [USE_GPU]]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --input_image_path INPUT_IMAGE_PATH\n                        Path to input image (can be of .mgz or .nii.gz\n                        format)(required)\n  --segmentation_dir_path SEGMENTATION_DIR_PATH\n                        Directory path to save the output segmentation\n                        (required)\n  --file_name FILE_NAME\n                        Name of the segmentation file (required)\n  --model_type MODEL_TYPE\n                        Model types: \"dense-unet\", \"unet\" (default: \"dense-\n                        unet\")\n  --model_wts_path MODEL_WTS_PATH\n                        Path for model wts to be used (default='./saved_model_\n                        wts/dense_unet_back2front_finetuned.pth')\n  --is_mgz [IS_MGZ]     Is the image in .mgz format (default=False, default\n                        format is .nii.gz)\n  --save_prob [SAVE_PROB]\n                        Should the softmax prob values for each voxel be saved\n                        ? (default: False)\n  --use_gpu [USE_GPU]   Use GPU for inference? (default: True)\n\n```\nAn example could look something like this:\n```\npython3 perform_pred.py --input_image_path './../../../data_orig/199251/mri/T1.mgz' \\\n--segmentation_dir_path './sample_pred/' \\\n--file_name '199251' \\\n--is_mgz True \\\n--model_wts_path './saved_model_wts/dense_unet_back2front_non_finetuned.pth' \\\n--save_prob False \\\n--use_gpu True \\\n--save_prob False\n```\n\n## Pretrained model wts\nPretrained model wts can be downloaded from [here](https://drive.google.com/file/d/1-reUDvwBhSOUqOa48W9Vgh_LN3F5ZRjQ/view?usp=sharing). \n\nThere are two model architectures: Dense U-Net and U-Net. Each of the model is trained using 2D slices extracted coronally, sagittally,or axially. The name of the model will contain the orientation and model architecture information. \n\n## Output segmentation\nThe output segmentation has 103 labeled segments with the last one being the **None** class. The labels of the segmentation closely resembles the aseg+aparc segmentation protocol of Freesurfer. \n\nWe exclude 4 brain regions that are not common to a normal brain: White matter and non-white matter hypointentisites, left and right frontal and temporal poles. We also excluded left and right 'unknown' segments. We also exclude left and right bankssts as there is no common definition for these segments that is widely accepted by the neuroradiology community.\n\n\nThe complete list of class number and the corresponding segment name can be found [here](https://github.com/NYUMedML/BrainSeg/blob/master/name_class_mapping.p).\n\n## Sample Predicitons\n### Insula\nHere we can clearly see that Freesurfer (FS) incorrectly predicts the right insula segment, the model trained only using FS segmentations also learns a wrong prediction. Our proposed model which is finetuned on manually annotated dataset correctly captures the region. Moreover, the segment looks biologically natural unlike FS's segmentation which is grainy, noisy and with non-smooth boundaries.\n<img src=\"plots/rt_insula_aparc_with_man_3.png\" width=\"800\" /> \n\n### Putamen\nHere again, we see that FS segmentation is of low quality but our proposed fine-tuned model performs well and produces more natural looking segmentation.\n<img src=\"plots/Faulty_seg_Putamen.png\" width=\"800\" /> \n\n### Pallidum\nFS segmentation for pallidum also of low quality, but the proposed model performs well.\n<img src=\"plots/Faulty_seg_Pallidum.png\" width=\"800\" /> \n\n### More predicitons\nSome sample predictions for [Putamen](https://github.com/NYUMedML/BrainSeg/blob/master/plots/Left-Putamen_627549_143_0_1_2.pdf), [Caudate](https://github.com/NYUMedML/BrainSeg/blob/master/plots/Right-Caudate_194443_137_0_1_2.pdf), [Hippocampus](https://github.com/NYUMedML/BrainSeg/blob/master/plots/Right-Hippocampus_894774_108_0_1_2.pdf) and [Insula](https://github.com/NYUMedML/BrainSeg/blob/master/plots/ctx-lh-insula_147030_138_0_1_2.pdf) can be seen here. In all the images, prediction 1 = Freesurfer, Prediction 2 = Non-Finetuned Dense Unet, Prediction 3 = Finetuned Dense Unet. \n\nIt could be seen that Freesurfer often make errors in determining the accurate boundaries whereas the deep learning based models have natural looking ROIs with accurate boundaries.\n\n## Contact\nIf you have any questions regarding the code, please contact ark576[at]nyu.edu or raise an issue on the github repo.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/NYUMedML/DARTS", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "DARTSeg", "package_url": "https://pypi.org/project/DARTSeg/", "platform": "", "project_url": "https://pypi.org/project/DARTSeg/", "project_urls": {"Homepage": "https://github.com/NYUMedML/DARTS"}, "release_url": "https://pypi.org/project/DARTSeg/1.0.0/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Package for Brain Segmentation", "version": "1.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>DenseUnet-based Automatic Rapid brain Segmentation (DARTS)</h1>\n<h2>Paper Associated with the project</h2>\n<p>Here is the paper describing the project and experiments in detail (Link to the paper to be updated).</p>\n<h2>Deep learning models for brain MR segmentation</h2>\n<p>We pretrain our Dense Unet model using the Freesurfer segmentations of 1113 subjects available in the <a href=\"https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release\" rel=\"nofollow\">Human Connectome Project</a> dataset and fine-tuned the model using 101 manually labeled brain scans from <a href=\"https://mindboggle.info/data.html\" rel=\"nofollow\">Mindboggle</a> dataset.</p>\n<p>The model is able to perform the segmentation of complete brain <strong>within a minute</strong> (on a machine with single GPU). The model labels 102 regions in the brain making it the first model to segment more than 100 brain regions within a minute. The details of 102 regions can be found below.</p>\n<h2>Results on the Mindboggle held out data</h2>\n<p>The box plot compares the dice scores of different ROIs for Dense U-Net and U-Net. The Dense U-Net consistently outperforms U-Net and achieves good dice scores for most of the ROIs.</p>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/aeb0b8d5d63eb0aedcb008ccf872de0a674221d9/706c6f74732f636f6d706172655f646963655f706c6f745f61706172635f6d616e75616c5f66645f706172745f315f646e5f765f756e65742e706e67\" width=\"800\"> \n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/207eeb22efd0419fb47d54fa8c79e9801d285fe5/706c6f74732f636f6d706172655f646963655f706c6f745f61706172635f6d616e75616c5f66645f706172745f325f646e5f765f756e65742e706e67\" width=\"800\"> \n<h2>Using Pretrained models for performing complete brain segmentation</h2>\n<p>The users can use the pre-trained models to perform a complete brain MR segmentation. For using the <strong>coronally</strong> pre-trained models, the user will have to execute the <a href=\"https://github.com/NYUMedML/BrainSeg/blob/master/perform_pred.py\" rel=\"nofollow\"><code>perform_pred.py</code></a> script. An illustration can be seen in <a href=\"https://github.com/NYUMedML/BrainSeg/blob/master/predicting_segmentation_illustration.ipynb\" rel=\"nofollow\"><code>predicting_segmentation_illustration.ipynb</code></a> notebook.</p>\n<p>The following code block could be used to perform the prediction:</p>\n<pre><code>usage: perform_pred.py [-h] [--input_image_path INPUT_IMAGE_PATH]\n                       [--segmentation_dir_path SEGMENTATION_DIR_PATH]\n                       [--file_name FILE_NAME] [--model_type MODEL_TYPE]\n                       [--model_wts_path MODEL_WTS_PATH] [--is_mgz [IS_MGZ]]\n                       [--save_prob [SAVE_PROB]] [--use_gpu [USE_GPU]]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --input_image_path INPUT_IMAGE_PATH\n                        Path to input image (can be of .mgz or .nii.gz\n                        format)(required)\n  --segmentation_dir_path SEGMENTATION_DIR_PATH\n                        Directory path to save the output segmentation\n                        (required)\n  --file_name FILE_NAME\n                        Name of the segmentation file (required)\n  --model_type MODEL_TYPE\n                        Model types: \"dense-unet\", \"unet\" (default: \"dense-\n                        unet\")\n  --model_wts_path MODEL_WTS_PATH\n                        Path for model wts to be used (default='./saved_model_\n                        wts/dense_unet_back2front_finetuned.pth')\n  --is_mgz [IS_MGZ]     Is the image in .mgz format (default=False, default\n                        format is .nii.gz)\n  --save_prob [SAVE_PROB]\n                        Should the softmax prob values for each voxel be saved\n                        ? (default: False)\n  --use_gpu [USE_GPU]   Use GPU for inference? (default: True)\n\n</code></pre>\n<p>An example could look something like this:</p>\n<pre><code>python3 perform_pred.py --input_image_path './../../../data_orig/199251/mri/T1.mgz' \\\n--segmentation_dir_path './sample_pred/' \\\n--file_name '199251' \\\n--is_mgz True \\\n--model_wts_path './saved_model_wts/dense_unet_back2front_non_finetuned.pth' \\\n--save_prob False \\\n--use_gpu True \\\n--save_prob False\n</code></pre>\n<h2>Pretrained model wts</h2>\n<p>Pretrained model wts can be downloaded from <a href=\"https://drive.google.com/file/d/1-reUDvwBhSOUqOa48W9Vgh_LN3F5ZRjQ/view?usp=sharing\" rel=\"nofollow\">here</a>.</p>\n<p>There are two model architectures: Dense U-Net and U-Net. Each of the model is trained using 2D slices extracted coronally, sagittally,or axially. The name of the model will contain the orientation and model architecture information.</p>\n<h2>Output segmentation</h2>\n<p>The output segmentation has 103 labeled segments with the last one being the <strong>None</strong> class. The labels of the segmentation closely resembles the aseg+aparc segmentation protocol of Freesurfer.</p>\n<p>We exclude 4 brain regions that are not common to a normal brain: White matter and non-white matter hypointentisites, left and right frontal and temporal poles. We also excluded left and right 'unknown' segments. We also exclude left and right bankssts as there is no common definition for these segments that is widely accepted by the neuroradiology community.</p>\n<p>The complete list of class number and the corresponding segment name can be found <a href=\"https://github.com/NYUMedML/BrainSeg/blob/master/name_class_mapping.p\" rel=\"nofollow\">here</a>.</p>\n<h2>Sample Predicitons</h2>\n<h3>Insula</h3>\n<p>Here we can clearly see that Freesurfer (FS) incorrectly predicts the right insula segment, the model trained only using FS segmentations also learns a wrong prediction. Our proposed model which is finetuned on manually annotated dataset correctly captures the region. Moreover, the segment looks biologically natural unlike FS's segmentation which is grainy, noisy and with non-smooth boundaries.\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/367e9100eed05309c23ac487e15618dbddb2c9f0/706c6f74732f72745f696e73756c615f61706172635f776974685f6d616e5f332e706e67\" width=\"800\"></p>\n<h3>Putamen</h3>\n<p>Here again, we see that FS segmentation is of low quality but our proposed fine-tuned model performs well and produces more natural looking segmentation.\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0c8e0a1a6ce8ef10bf1aea0abf944b36374280d0/706c6f74732f4661756c74795f7365675f507574616d656e2e706e67\" width=\"800\"></p>\n<h3>Pallidum</h3>\n<p>FS segmentation for pallidum also of low quality, but the proposed model performs well.\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/06c4381c41f0566fec048312e7ca8dba773a725d/706c6f74732f4661756c74795f7365675f50616c6c6964756d2e706e67\" width=\"800\"></p>\n<h3>More predicitons</h3>\n<p>Some sample predictions for <a href=\"https://github.com/NYUMedML/BrainSeg/blob/master/plots/Left-Putamen_627549_143_0_1_2.pdf\" rel=\"nofollow\">Putamen</a>, <a href=\"https://github.com/NYUMedML/BrainSeg/blob/master/plots/Right-Caudate_194443_137_0_1_2.pdf\" rel=\"nofollow\">Caudate</a>, <a href=\"https://github.com/NYUMedML/BrainSeg/blob/master/plots/Right-Hippocampus_894774_108_0_1_2.pdf\" rel=\"nofollow\">Hippocampus</a> and <a href=\"https://github.com/NYUMedML/BrainSeg/blob/master/plots/ctx-lh-insula_147030_138_0_1_2.pdf\" rel=\"nofollow\">Insula</a> can be seen here. In all the images, prediction 1 = Freesurfer, Prediction 2 = Non-Finetuned Dense Unet, Prediction 3 = Finetuned Dense Unet.</p>\n<p>It could be seen that Freesurfer often make errors in determining the accurate boundaries whereas the deep learning based models have natural looking ROIs with accurate boundaries.</p>\n<h2>Contact</h2>\n<p>If you have any questions regarding the code, please contact ark576[at]nyu.edu or raise an issue on the github repo.</p>\n\n          </div>"}, "last_serial": 6153640, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "ce10410af37d4fe80c165eb2520d49bb", "sha256": "773c7dfd578337dd8c3b2b232ef1a60175bbaec9fdee5257fb3f9170c631ab2a"}, "downloads": -1, "filename": "DARTSeg-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "ce10410af37d4fe80c165eb2520d49bb", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 26688, "upload_time": "2019-11-18T04:22:43", "upload_time_iso_8601": "2019-11-18T04:22:43.804726Z", "url": "https://files.pythonhosted.org/packages/a4/f3/842e373df55ad9a8948234749217bdf015e885194bba5c3a3d7cb7e6d208/DARTSeg-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4433824fd7f96c79febd47fe6649357a", "sha256": "361254643848a371c567db8765af04433d6505dfeba28496b80b5506cbab09ca"}, "downloads": -1, "filename": "DARTSeg-1.0.0.tar.gz", "has_sig": false, "md5_digest": "4433824fd7f96c79febd47fe6649357a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 11481, "upload_time": "2019-11-18T04:22:45", "upload_time_iso_8601": "2019-11-18T04:22:45.779148Z", "url": "https://files.pythonhosted.org/packages/c1/c6/c8416d4919b79bbfd477bc8fa8ccf3e20150ea99ef3c23b3d817d2d55fe8/DARTSeg-1.0.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ce10410af37d4fe80c165eb2520d49bb", "sha256": "773c7dfd578337dd8c3b2b232ef1a60175bbaec9fdee5257fb3f9170c631ab2a"}, "downloads": -1, "filename": "DARTSeg-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "ce10410af37d4fe80c165eb2520d49bb", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 26688, "upload_time": "2019-11-18T04:22:43", "upload_time_iso_8601": "2019-11-18T04:22:43.804726Z", "url": "https://files.pythonhosted.org/packages/a4/f3/842e373df55ad9a8948234749217bdf015e885194bba5c3a3d7cb7e6d208/DARTSeg-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4433824fd7f96c79febd47fe6649357a", "sha256": "361254643848a371c567db8765af04433d6505dfeba28496b80b5506cbab09ca"}, "downloads": -1, "filename": "DARTSeg-1.0.0.tar.gz", "has_sig": false, "md5_digest": "4433824fd7f96c79febd47fe6649357a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 11481, "upload_time": "2019-11-18T04:22:45", "upload_time_iso_8601": "2019-11-18T04:22:45.779148Z", "url": "https://files.pythonhosted.org/packages/c1/c6/c8416d4919b79bbfd477bc8fa8ccf3e20150ea99ef3c23b3d817d2d55fe8/DARTSeg-1.0.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:40:38 2020"}