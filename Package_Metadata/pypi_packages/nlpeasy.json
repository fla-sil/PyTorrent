{"info": {"author": "Philipp Thomann", "author_email": "philipp.thomann@d-one.ai", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Text Processing :: Markup :: HTML"], "description": "[![Travis Build Status](<https://img.shields.io/travis/d-one/nlpeasy/master.svg?style=flat-square&logo=travis-ci&logoColor=white&label=build>)](https://travis-ci.org/d-one/nlpeasy)\n[![pypi Version](https://img.shields.io/pypi/v/nlpeasy.svg?style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/nlpeasy/)\n\nNLPeasy\n=======\n\nBuild NLP pipelines the easy way\n\n> **Disclaimer:** This is in Alpha stage, lot of things can go wrong.\n> It could possibly change your Elasticsearch Data the API is not fixed yet\n> and even the name NLPeasy might change.\n\n* Free software: Apache Software License 2.0\n\n\nUsage\n-----\n\nFor this example to completely work you need to have Python at least in Version 3.6 installed.\nAlso you need to have install and start either\n\n- **Docker** <https://www.docker.com/get-started>, direct download links for\n    [Mac (DMG)](https://download.docker.com/mac/stable/Docker.dmg) and\n    [Windows (exe)](https://download.docker.com/win/stable/Docker%20for%20Windows%20Installer.exe).\n- **Elasticsearch** and **Kibana**:\n    <https://www.elastic.co/downloads/> or\n    <https://www.elastic.co/downloads/elasticsearch-oss> (pure Apache licensed version)\n\nThen on the terminal issue:\n```bash\npython -m venv venv\nsource venv/bin/activate\npip install nlpeasy scikit-learn\npython -m spacy download en_core_web_md\n```\nThe package `scikit-learn` is just used in this example to get the newsgroups data and preprocess it.\nThe last command downloads a spacy model for the english language -\nfor the following you need to have at least it's `md` (=medium) version which has wordvectors.\n\n```python\nimport pandas as pd\nimport nlpeasy as ne\nfrom sklearn.datasets import fetch_20newsgroups\n\n# connect to running elastic or else start an Open Source stack on your docker\nelk = ne.connect_elastic(docker_prefix='nlp', elk_version='7.4.0', mount_volume_prefix=None)\n# If it is started on docker it will on the first time pull the images (1.3GB)!\n# Setting mountVolumePrefix=\"./elastic-data/\" would keep the data of elastic in your\n# filesystems and then the data survives container restarts\n\n# read data as Pandas data frame\nnews_raw = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\nnews_groups = [news_raw['target_names'][i] for i in news_raw['target']]\nnews = pd.DataFrame({'newsgroup': news_groups, 'message': news_raw['data']})\n\n# setup NLPeasy pipeline with name for the elastic index and set the text column\npipeline = ne.Pipeline(index='news', text_cols=['message'], tag_cols=['newsgroup'], elk=elk)\n\npipeline += ne.VaderSentiment('message', 'sentiment')\npipeline += ne.SpacyEnrichment(nlp='en_core_web_md', cols=['message'], vec=True)\n\n# do the pipeline - just for first 100, the whole thing would take 10 minutes\nnews_enriched = pipeline.process(news.head(10000000), write_elastic=True)\n\n# Create Kibana Dashboard of all the columns\npipeline.create_kibana_dashboard()\n\n# open Kibana in webbrowser\nelk.show_kibana()\n```\n\nLet's have some fun outside of Elastic/Kibana - but this needs `pip install matplotlib`\n```python\nimport numpy as np\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\ngrouped = news_enriched.loc[~news_enriched.message_vec.isna()].groupby('newsgroup')\ngroup_vec = grouped.apply(lambda z: np.stack(z.message_vec.values).mean(axis=0))\nclust = linkage(np.stack(group_vec), 'ward')\n# calculate full dendrogram\nplt.figure(figsize=(10, 10))\nplt.title('Hierarchical Clustering Dendrogram Newsgroups')\nplt.xlabel('sample index')\nplt.ylabel('distance')\ndendrogram(\n    clust,\n    leaf_rotation=0.,  # rotates the x axis labels\n    leaf_font_size=8.,  # font size for the x axis labels\n    labels=group_vec.index,\n    orientation='left'\n)\nplt.show()\n```\n\nInstallation\n------------\n\nPrerequisites:\n\n- Python 3 (we use Python 3.7)\n- Elastic: Several possibilities\n    - Have Docker installed - needs to have the docker package installed (see below).\n    - Install and start Elasticsearch and Kibana:\n        <https://www.elastic.co/downloads/> or\n        <https://www.elastic.co/downloads/elasticsearch-oss> (pure Apache licensed version)\n    - Use any running Elasticsearch and Kibana (on premise or cloud)...\n- Pretrained Models: See below for Spacy Language Models and WordVectors\n\nIt is recommended to use a virtual environment:\n```bash\ncd $PROJECT_DIR\npython -m venv venv\nsource venv/bin/activate\n```\nThe source statement has to be repeated whenever you open a new terminal.\n\nThen install\n```bash\npip install nlpeasy\n```\nOr the development version from GitHub:\n```bash\npip install --upgrade git+https://github.com/d-one/nlpeasy\n```\n\nIf you want to use spaCy language models download them (90-200 MB), e.g.\n```bash\npython -m spacy download en_core_web_md\n# and/or\npython -m spacy download de_core_news_md\n```\nIf you want to use pretrained [FastText-Wordvectors](https://fasttext.cc/docs/en/pretrained-vectors.html) (each ~7GB):\n```bash\ncurl -O https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\ncurl -O https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.de.zip\n```\n\nIf you want to use Jupyter, install it to the virtual environment:\n```bash\npip install jupyterlab\n```\n\n### Development\nTo install this module in Dev-mode, i.e. change files and reload module:\n```bash\ngit clone https://github.com/d-one/nlpeasy\ncd nlpeasy\n```\n\nIt is recommended to use a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate\n```\n\nInstall the version in edit mode:\n```bash\npip install -e .\n```\n\nIn Jupyter you can have reloaded code when you change the files as in:\n```python\n%load_ext autoreload\n%autoreload 2\n```\n\nFeatures\n--------\n\n* Pandas based pipeline\n* Support for any extensions - now includes some for Regex, spaCy, VaderSentiment\n* Write results to ElasticSearch\n* Automatic Kibana dashboard generation\n* Have Elastic started in Docker if it is not installed locally or remotely\n* Apache License 2.0\n\nCredits\n-------\n\nThis package was created with [Cookiecutter](<https://github.com/audreyr/cookiecutter>) and the [`audreyr/cookiecutter-pypackage`]<https://github.com/audreyr/cookiecutter-pypackage> project template.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/d-one/nlpeasy", "keywords": "nlpeasy", "license": "Apache Software License 2.0", "maintainer": "", "maintainer_email": "", "name": "nlpeasy", "package_url": "https://pypi.org/project/nlpeasy/", "platform": "", "project_url": "https://pypi.org/project/nlpeasy/", "project_urls": {"Homepage": "https://github.com/d-one/nlpeasy"}, "release_url": "https://pypi.org/project/nlpeasy/0.7.0/", "requires_dist": null, "requires_python": "", "summary": "Easy Peasy Language Squeezy", "version": "0.7.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://travis-ci.org/d-one/nlpeasy\" rel=\"nofollow\"><img alt=\"Travis Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/19b3ef46aaee89acb186cfb0ca5564b92bc3c691/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f642d6f6e652f6e6c70656173792f6d61737465722e7376673f7374796c653d666c61742d737175617265266c6f676f3d7472617669732d6369266c6f676f436f6c6f723d7768697465266c6162656c3d6275696c64\"></a>\n<a href=\"https://pypi.org/project/nlpeasy/\" rel=\"nofollow\"><img alt=\"pypi Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3d0ee3089b36da4ad29d4e199c73021c1f018167/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6e6c70656173792e7376673f7374796c653d666c61742d737175617265266c6f676f3d70797069266c6f676f436f6c6f723d7768697465\"></a></p>\n<h1>NLPeasy</h1>\n<p>Build NLP pipelines the easy way</p>\n<blockquote>\n<p><strong>Disclaimer:</strong> This is in Alpha stage, lot of things can go wrong.\nIt could possibly change your Elasticsearch Data the API is not fixed yet\nand even the name NLPeasy might change.</p>\n</blockquote>\n<ul>\n<li>Free software: Apache Software License 2.0</li>\n</ul>\n<h2>Usage</h2>\n<p>For this example to completely work you need to have Python at least in Version 3.6 installed.\nAlso you need to have install and start either</p>\n<ul>\n<li><strong>Docker</strong> <a href=\"https://www.docker.com/get-started\" rel=\"nofollow\">https://www.docker.com/get-started</a>, direct download links for\n<a href=\"https://download.docker.com/mac/stable/Docker.dmg\" rel=\"nofollow\">Mac (DMG)</a> and\n<a href=\"https://download.docker.com/win/stable/Docker%20for%20Windows%20Installer.exe\" rel=\"nofollow\">Windows (exe)</a>.</li>\n<li><strong>Elasticsearch</strong> and <strong>Kibana</strong>:\n<a href=\"https://www.elastic.co/downloads/\" rel=\"nofollow\">https://www.elastic.co/downloads/</a> or\n<a href=\"https://www.elastic.co/downloads/elasticsearch-oss\" rel=\"nofollow\">https://www.elastic.co/downloads/elasticsearch-oss</a> (pure Apache licensed version)</li>\n</ul>\n<p>Then on the terminal issue:</p>\n<pre>python -m venv venv\n<span class=\"nb\">source</span> venv/bin/activate\npip install nlpeasy scikit-learn\npython -m spacy download en_core_web_md\n</pre>\n<p>The package <code>scikit-learn</code> is just used in this example to get the newsgroups data and preprocess it.\nThe last command downloads a spacy model for the english language -\nfor the following you need to have at least it's <code>md</code> (=medium) version which has wordvectors.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">nlpeasy</span> <span class=\"k\">as</span> <span class=\"nn\">ne</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">fetch_20newsgroups</span>\n\n<span class=\"c1\"># connect to running elastic or else start an Open Source stack on your docker</span>\n<span class=\"n\">elk</span> <span class=\"o\">=</span> <span class=\"n\">ne</span><span class=\"o\">.</span><span class=\"n\">connect_elastic</span><span class=\"p\">(</span><span class=\"n\">docker_prefix</span><span class=\"o\">=</span><span class=\"s1\">'nlp'</span><span class=\"p\">,</span> <span class=\"n\">elk_version</span><span class=\"o\">=</span><span class=\"s1\">'7.4.0'</span><span class=\"p\">,</span> <span class=\"n\">mount_volume_prefix</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">)</span>\n<span class=\"c1\"># If it is started on docker it will on the first time pull the images (1.3GB)!</span>\n<span class=\"c1\"># Setting mountVolumePrefix=\"./elastic-data/\" would keep the data of elastic in your</span>\n<span class=\"c1\"># filesystems and then the data survives container restarts</span>\n\n<span class=\"c1\"># read data as Pandas data frame</span>\n<span class=\"n\">news_raw</span> <span class=\"o\">=</span> <span class=\"n\">fetch_20newsgroups</span><span class=\"p\">(</span><span class=\"n\">remove</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"s1\">'headers'</span><span class=\"p\">,</span> <span class=\"s1\">'footers'</span><span class=\"p\">,</span> <span class=\"s1\">'quotes'</span><span class=\"p\">))</span>\n<span class=\"n\">news_groups</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">news_raw</span><span class=\"p\">[</span><span class=\"s1\">'target_names'</span><span class=\"p\">][</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">news_raw</span><span class=\"p\">[</span><span class=\"s1\">'target'</span><span class=\"p\">]]</span>\n<span class=\"n\">news</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">'newsgroup'</span><span class=\"p\">:</span> <span class=\"n\">news_groups</span><span class=\"p\">,</span> <span class=\"s1\">'message'</span><span class=\"p\">:</span> <span class=\"n\">news_raw</span><span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">]})</span>\n\n<span class=\"c1\"># setup NLPeasy pipeline with name for the elastic index and set the text column</span>\n<span class=\"n\">pipeline</span> <span class=\"o\">=</span> <span class=\"n\">ne</span><span class=\"o\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">index</span><span class=\"o\">=</span><span class=\"s1\">'news'</span><span class=\"p\">,</span> <span class=\"n\">text_cols</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'message'</span><span class=\"p\">],</span> <span class=\"n\">tag_cols</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'newsgroup'</span><span class=\"p\">],</span> <span class=\"n\">elk</span><span class=\"o\">=</span><span class=\"n\">elk</span><span class=\"p\">)</span>\n\n<span class=\"n\">pipeline</span> <span class=\"o\">+=</span> <span class=\"n\">ne</span><span class=\"o\">.</span><span class=\"n\">VaderSentiment</span><span class=\"p\">(</span><span class=\"s1\">'message'</span><span class=\"p\">,</span> <span class=\"s1\">'sentiment'</span><span class=\"p\">)</span>\n<span class=\"n\">pipeline</span> <span class=\"o\">+=</span> <span class=\"n\">ne</span><span class=\"o\">.</span><span class=\"n\">SpacyEnrichment</span><span class=\"p\">(</span><span class=\"n\">nlp</span><span class=\"o\">=</span><span class=\"s1\">'en_core_web_md'</span><span class=\"p\">,</span> <span class=\"n\">cols</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'message'</span><span class=\"p\">],</span> <span class=\"n\">vec</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># do the pipeline - just for first 100, the whole thing would take 10 minutes</span>\n<span class=\"n\">news_enriched</span> <span class=\"o\">=</span> <span class=\"n\">pipeline</span><span class=\"o\">.</span><span class=\"n\">process</span><span class=\"p\">(</span><span class=\"n\">news</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"mi\">10000000</span><span class=\"p\">),</span> <span class=\"n\">write_elastic</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create Kibana Dashboard of all the columns</span>\n<span class=\"n\">pipeline</span><span class=\"o\">.</span><span class=\"n\">create_kibana_dashboard</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># open Kibana in webbrowser</span>\n<span class=\"n\">elk</span><span class=\"o\">.</span><span class=\"n\">show_kibana</span><span class=\"p\">()</span>\n</pre>\n<p>Let's have some fun outside of Elastic/Kibana - but this needs <code>pip install matplotlib</code></p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">scipy.cluster.hierarchy</span> <span class=\"kn\">import</span> <span class=\"n\">dendrogram</span><span class=\"p\">,</span> <span class=\"n\">linkage</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n<span class=\"n\">grouped</span> <span class=\"o\">=</span> <span class=\"n\">news_enriched</span><span class=\"o\">.</span><span class=\"n\">loc</span><span class=\"p\">[</span><span class=\"o\">~</span><span class=\"n\">news_enriched</span><span class=\"o\">.</span><span class=\"n\">message_vec</span><span class=\"o\">.</span><span class=\"n\">isna</span><span class=\"p\">()]</span><span class=\"o\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"s1\">'newsgroup'</span><span class=\"p\">)</span>\n<span class=\"n\">group_vec</span> <span class=\"o\">=</span> <span class=\"n\">grouped</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">z</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">(</span><span class=\"n\">z</span><span class=\"o\">.</span><span class=\"n\">message_vec</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n<span class=\"n\">clust</span> <span class=\"o\">=</span> <span class=\"n\">linkage</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">(</span><span class=\"n\">group_vec</span><span class=\"p\">),</span> <span class=\"s1\">'ward'</span><span class=\"p\">)</span>\n<span class=\"c1\"># calculate full dendrogram</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s1\">'Hierarchical Clustering Dendrogram Newsgroups'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">'sample index'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">'distance'</span><span class=\"p\">)</span>\n<span class=\"n\">dendrogram</span><span class=\"p\">(</span>\n    <span class=\"n\">clust</span><span class=\"p\">,</span>\n    <span class=\"n\">leaf_rotation</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">,</span>  <span class=\"c1\"># rotates the x axis labels</span>\n    <span class=\"n\">leaf_font_size</span><span class=\"o\">=</span><span class=\"mf\">8.</span><span class=\"p\">,</span>  <span class=\"c1\"># font size for the x axis labels</span>\n    <span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">group_vec</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">,</span>\n    <span class=\"n\">orientation</span><span class=\"o\">=</span><span class=\"s1\">'left'</span>\n<span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</pre>\n<h2>Installation</h2>\n<p>Prerequisites:</p>\n<ul>\n<li>Python 3 (we use Python 3.7)</li>\n<li>Elastic: Several possibilities\n<ul>\n<li>Have Docker installed - needs to have the docker package installed (see below).</li>\n<li>Install and start Elasticsearch and Kibana:\n<a href=\"https://www.elastic.co/downloads/\" rel=\"nofollow\">https://www.elastic.co/downloads/</a> or\n<a href=\"https://www.elastic.co/downloads/elasticsearch-oss\" rel=\"nofollow\">https://www.elastic.co/downloads/elasticsearch-oss</a> (pure Apache licensed version)</li>\n<li>Use any running Elasticsearch and Kibana (on premise or cloud)...</li>\n</ul>\n</li>\n<li>Pretrained Models: See below for Spacy Language Models and WordVectors</li>\n</ul>\n<p>It is recommended to use a virtual environment:</p>\n<pre><span class=\"nb\">cd</span> <span class=\"nv\">$PROJECT_DIR</span>\npython -m venv venv\n<span class=\"nb\">source</span> venv/bin/activate\n</pre>\n<p>The source statement has to be repeated whenever you open a new terminal.</p>\n<p>Then install</p>\n<pre>pip install nlpeasy\n</pre>\n<p>Or the development version from GitHub:</p>\n<pre>pip install --upgrade git+https://github.com/d-one/nlpeasy\n</pre>\n<p>If you want to use spaCy language models download them (90-200 MB), e.g.</p>\n<pre>python -m spacy download en_core_web_md\n<span class=\"c1\"># and/or</span>\npython -m spacy download de_core_news_md\n</pre>\n<p>If you want to use pretrained <a href=\"https://fasttext.cc/docs/en/pretrained-vectors.html\" rel=\"nofollow\">FastText-Wordvectors</a> (each ~7GB):</p>\n<pre>curl -O https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\ncurl -O https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.de.zip\n</pre>\n<p>If you want to use Jupyter, install it to the virtual environment:</p>\n<pre>pip install jupyterlab\n</pre>\n<h3>Development</h3>\n<p>To install this module in Dev-mode, i.e. change files and reload module:</p>\n<pre>git clone https://github.com/d-one/nlpeasy\n<span class=\"nb\">cd</span> nlpeasy\n</pre>\n<p>It is recommended to use a virtual environment:</p>\n<pre>python -m venv venv\n<span class=\"nb\">source</span> venv/bin/activate\n</pre>\n<p>Install the version in edit mode:</p>\n<pre>pip install -e .\n</pre>\n<p>In Jupyter you can have reloaded code when you change the files as in:</p>\n<pre><span class=\"o\">%</span><span class=\"n\">load_ext</span> <span class=\"n\">autoreload</span>\n<span class=\"o\">%</span><span class=\"n\">autoreload</span> <span class=\"mi\">2</span>\n</pre>\n<h2>Features</h2>\n<ul>\n<li>Pandas based pipeline</li>\n<li>Support for any extensions - now includes some for Regex, spaCy, VaderSentiment</li>\n<li>Write results to ElasticSearch</li>\n<li>Automatic Kibana dashboard generation</li>\n<li>Have Elastic started in Docker if it is not installed locally or remotely</li>\n<li>Apache License 2.0</li>\n</ul>\n<h2>Credits</h2>\n<p>This package was created with <a href=\"https://github.com/audreyr/cookiecutter\" rel=\"nofollow\">Cookiecutter</a> and the [<code>audreyr/cookiecutter-pypackage</code>]<a href=\"https://github.com/audreyr/cookiecutter-pypackage\" rel=\"nofollow\">https://github.com/audreyr/cookiecutter-pypackage</a> project template.</p>\n\n          </div>"}, "last_serial": 6188413, "releases": {"0.6.0": [{"comment_text": "", "digests": {"md5": "f1e438643afad349ef148f260d54e07e", "sha256": "040d7a662177b4805914e76adc247ec7ccef2e8b0208a01978b52fb4b06dcd9c"}, "downloads": -1, "filename": "nlpeasy-0.6.0.tar.gz", "has_sig": false, "md5_digest": "f1e438643afad349ef148f260d54e07e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23745, "upload_time": "2019-10-10T14:34:00", "upload_time_iso_8601": "2019-10-10T14:34:00.581421Z", "url": "https://files.pythonhosted.org/packages/0c/0c/920e87b91d66ef3ee7afef7fe13026f2f9ba47081f5fa81eb906c5f82b03/nlpeasy-0.6.0.tar.gz", "yanked": false}], "0.6.1": [{"comment_text": "", "digests": {"md5": "ccad599cc5ff48bb736e184bb18678a1", "sha256": "23843df46d03e7e480676e3c872515d68d93a693cb57e3cbd7ea6b339bdb4442"}, "downloads": -1, "filename": "nlpeasy-0.6.1.tar.gz", "has_sig": false, "md5_digest": "ccad599cc5ff48bb736e184bb18678a1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22454, "upload_time": "2019-10-11T01:49:31", "upload_time_iso_8601": "2019-10-11T01:49:31.161298Z", "url": "https://files.pythonhosted.org/packages/66/28/dc0af2361fbe3fa6df92075b754affd5c4db3ab8f7b73eb841f5a2de4f6f/nlpeasy-0.6.1.tar.gz", "yanked": false}], "0.6.2": [{"comment_text": "", "digests": {"md5": "19290e9358382f0aef09ab4771ba94ba", "sha256": "90356e863628155dff4bcecd2646df20e52adb56efb4aafdc44d7f5ad80b87bb"}, "downloads": -1, "filename": "nlpeasy-0.6.2.tar.gz", "has_sig": false, "md5_digest": "19290e9358382f0aef09ab4771ba94ba", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24263, "upload_time": "2019-10-13T22:58:41", "upload_time_iso_8601": "2019-10-13T22:58:41.887201Z", "url": "https://files.pythonhosted.org/packages/91/b6/0891b6aa3e834ca2e1884711d4d887870580985339b2674e4e71d791cc80/nlpeasy-0.6.2.tar.gz", "yanked": false}], "0.7.0": [{"comment_text": "", "digests": {"md5": "506aee9a724cb04db9a0f1056d3a29c9", "sha256": "4cddc5e6ccd4ffdf624677ac7b4568b34e9fc18373bad293709ef9f4faffe72c"}, "downloads": -1, "filename": "nlpeasy-0.7.0.tar.gz", "has_sig": false, "md5_digest": "506aee9a724cb04db9a0f1056d3a29c9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28095, "upload_time": "2019-11-24T00:25:59", "upload_time_iso_8601": "2019-11-24T00:25:59.289631Z", "url": "https://files.pythonhosted.org/packages/36/38/e5fc83bda1c71a25e76b32a2f8edc151dd30b90c36ad59b973c463828669/nlpeasy-0.7.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "506aee9a724cb04db9a0f1056d3a29c9", "sha256": "4cddc5e6ccd4ffdf624677ac7b4568b34e9fc18373bad293709ef9f4faffe72c"}, "downloads": -1, "filename": "nlpeasy-0.7.0.tar.gz", "has_sig": false, "md5_digest": "506aee9a724cb04db9a0f1056d3a29c9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28095, "upload_time": "2019-11-24T00:25:59", "upload_time_iso_8601": "2019-11-24T00:25:59.289631Z", "url": "https://files.pythonhosted.org/packages/36/38/e5fc83bda1c71a25e76b32a2f8edc151dd30b90c36ad59b973c463828669/nlpeasy-0.7.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:45:12 2020"}