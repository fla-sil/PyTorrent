{"info": {"author": "Erik Rose", "author_email": "erikrose@grinchcentral.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering :: Information Analysis", "Topic :: Software Development :: Libraries", "Topic :: Text Processing :: General"], "description": "============\nParsimonious\n============\n\nParsimonious aims to be the fastest arbitrary-lookahead parser written in pure\nPython\u2014and the most usable. It's based on parsing expression grammars (PEGs),\nwhich means you feed it a simplified sort of EBNF notation. Parsimonious was\ndesigned to undergird a MediaWiki parser that wouldn't take 5 seconds or a GB\nof RAM to do one page, but it's applicable to all sorts of languages.\n\n\nGoals\n=====\n\n* Speed\n* Frugal RAM use\n* Minimalistic, understandable, idiomatic Python code\n* Readable grammars\n* Extensible grammars\n* Complete test coverage\n* Separation of concerns. Some Python parsing kits mix recognition with\n  instructions about how to turn the resulting tree into some kind of other\n  representation. This is limiting when you want to do several different things\n  with a tree: for example, render wiki markup to HTML *or* to text.\n* Good error reporting. I want the parser to work *with* me as I develop a\n  grammar.\n\n\nExample Usage\n=============\n\nHere's how to build a simple grammar:\n\n.. code:: python\n\n    >>> from parsimonious.grammar import Grammar\n    >>> grammar = Grammar(\n    ...     \"\"\"\n    ...     bold_text  = bold_open text bold_close\n    ...     text       = ~\"[A-Z 0-9]*\"i\n    ...     bold_open  = \"((\"\n    ...     bold_close = \"))\"\n    ...     \"\"\")\n\nYou can have forward references and even right recursion; it's all taken care\nof by the grammar compiler. The first rule is taken to be the default start\nsymbol, but you can override that.\n\nNext, let's parse something and get an abstract syntax tree:\n\n.. code:: python\n\n    >>> print grammar.parse('((bold stuff))')\n    <Node called \"bold_text\" matching \"((bold stuff))\">\n        <Node called \"bold_open\" matching \"((\">\n        <RegexNode called \"text\" matching \"bold stuff\">\n        <Node called \"bold_close\" matching \"))\">\n\nYou'd typically then use a ``nodes.NodeVisitor`` subclass (see below) to walk\nthe tree and do something useful with it.\n\n\nStatus\n======\n\n* Everything that exists works. Test coverage is good.\n* I don't plan on making any backward-incompatible changes to the rule syntax\n  in the future, so you can write grammars with confidence.\n* It may be slow and use a lot of RAM; I haven't measured either yet. However,\n  I have yet to begin optimizing in earnest.\n* Error reporting is now in place. ``repr`` methods of expressions, grammars,\n  and nodes are clear and helpful as well. The ``Grammar`` ones are\n  even round-trippable!\n* The grammar extensibility story is underdeveloped at the moment. You should\n  be able to extend a grammar by simply concatening more rules onto the\n  existing ones; later rules of the same name should override previous ones.\n  However, this is untested and may not be the final story.\n* Sphinx docs are coming, but the docstrings are quite useful now.\n* Note that there may be API changes until we get to 1.0, so be sure to pin to\n  the version you're using.\n\nComing Soon\n-----------\n\n* Optimizations to make Parsimonious worthy of its name\n* Tighter RAM use\n* Better-thought-out grammar extensibility story\n* Amazing grammar debugging\n\n\nA Little About PEG Parsers\n==========================\n\nPEG parsers don't draw a distinction between lexing and parsing; everything is\ndone at once. As a result, there is no lookahead limit, as there is with, for\ninstance, Yacc. And, due to both of these properties, PEG grammars are easier\nto write: they're basically just a more practical dialect of EBNF. With\ncaching, they take O(grammar size * text length) memory (though I plan to do\nbetter), but they run in O(text length) time.\n\nMore Technically\n----------------\n\nPEGs can describe a superset of *LL(k)* languages, any deterministic *LR(k)*\nlanguage, and many others\u2014including some that aren't context-free\n(http://www.brynosaurus.com/pub/lang/peg.pdf). They can also deal with what\nwould be ambiguous languages if described in canonical EBNF. They do this by\ntrading the ``|`` alternation operator for the ``/`` operator, which works the\nsame except that it makes priority explicit: ``a / b / c`` first tries matching\n``a``. If that fails, it tries ``b``, and, failing that, moves on to ``c``.\nThus, ambiguity is resolved by always yielding the first successful recognition.\n\n\nWriting Grammars\n================\n\nGrammars are defined by a series of rules. The syntax should be familiar to\nanyone who uses regexes or reads programming language manuals. An example will\nserve best:\n\n.. code:: python\n\n    my_grammar = Grammar(r\"\"\"\n        styled_text = bold_text / italic_text\n        bold_text   = \"((\" text \"))\"\n        italic_text = \"''\" text \"''\"\n        text        = ~\"[A-Z 0-9]*\"i\n        \"\"\")\n\nYou can wrap a rule across multiple lines if you like; the syntax is very\nforgiving.\n\n\nSyntax Reference\n----------------\n\n====================    ========================================================\n``\"some literal\"``      Used to quote literals. Backslash escaping and Python\n                        conventions for \"raw\" and Unicode strings help support\n                        fiddly characters.\n\n[space]                 Sequences are made out of space- or tab-delimited\n                        things. ``a b c`` matches spots where those 3\n                        terms appear in that order.\n\n``a / b / c``           Alternatives. The first to succeed of ``a / b / c``\n                        wins.\n\n``thing?``              An optional expression. This is greedy, always consuming\n                        ``thing`` if it exists.\n\n``&thing``              A lookahead assertion. Ensures ``thing`` matches at the\n                        current position but does not consume it.\n\n``!thing``              A negative lookahead assertion. Matches if ``thing``\n                        isn't found here. Doesn't consume any text.\n\n``things*``             Zero or more things. This is greedy, always consuming as\n                        many repetitions as it can.\n\n``things+``             One or more things. This is greedy, always consuming as\n                        many repetitions as it can.\n\n``~r\"regex\"ilmsux``     Regexes have ``~`` in front and are quoted like\n                        literals. Any flags follow the end quotes as single\n                        chars. Regexes are good for representing character\n                        classes (``[a-z0-9]``) and optimizing for speed. The\n                        downside is that they won't be able to take advantage\n                        of our fancy debugging, once we get that working.\n                        Ultimately, I'd like to deprecate explicit regexes and\n                        instead have Parsimonious dynamically build them out of\n                        simpler primitives.\n\n``(things)``            Parentheses are used for grouping, like in every other\n                        language.\n====================    ========================================================\n\n\nOptimizing Grammars\n===================\n\nDon't Repeat Expressions\n------------------------\n\nIf you need a ``~\"[a-z0-9]\"i`` at two points in your grammar, don't type it\ntwice. Make it a rule of its own, and reference it from wherever you need it. \nYou'll get the most out of the caching this way, since cache lookups are by \nexpression object identity (for speed). \n\nEven if you have an expression that's very simple, not repeating it will \nsave RAM, as there can, at worst, be a cached int for every char in the text \nyou're parsing. In the future, we may identify repeated subexpressions \nautomatically and factor them up while building the grammar.\n\nHow much should you shove into one regex, versus how much should you break them\nup to not repeat yourself? That's a fine balance and worthy of benchmarking.\nMore stuff jammed into a regex will execute faster, because it doesn't have to\nrun any Python between pieces, but a broken-up one will give better cache\nperformance if the individual pieces are re-used elsewhere. If the pieces of a\nregex aren't used anywhere else, by all means keep the whole thing together.\n\n\nQuantifiers\n-----------\n\nBring your ``?`` and ``*`` quantifiers up to the highest level you\ncan. Otherwise, lower-level patterns could succeed but be empty and put a bunch\nof useless nodes in your tree that didn't really match anything.\n\n\nProcessing Parse Trees\n======================\n\nA parse tree has a node for each expression matched, even if it matched a\nzero-length string, like ``\"thing\"?`` might.\n\nThe ``NodeVisitor`` class provides an inversion-of-control framework for\nwalking a tree and returning a new construct (tree, string, or whatever) based\non it. For now, have a look at its docstrings for more detail. There's also a\ngood example in ``grammar.RuleVisitor``. Notice how we take advantage of nodes'\niterability by using tuple unpacks in the formal parameter lists:\n\n.. code:: python\n\n    def visit_or_term(self, or_term, (slash, _, term)):\n        ...\n\nFor reference, here is the production the above unpacks::\n\n    or_term = \"/\" _ term\n\nWhen something goes wrong in your visitor, you get a nice error like this::\n\n    [normal traceback here...]\n    VisitationException: 'Node' object has no attribute 'foo'\n\n    Parse tree:\n    <Node called \"rules\" matching \"number = ~\"[0-9]+\"\">  <-- *** We were here. ***\n        <Node matching \"number = ~\"[0-9]+\"\">\n            <Node called \"rule\" matching \"number = ~\"[0-9]+\"\">\n                <Node matching \"\">\n                <Node called \"label\" matching \"number\">\n                <Node matching \" \">\n                    <Node called \"_\" matching \" \">\n                <Node matching \"=\">\n                <Node matching \" \">\n                    <Node called \"_\" matching \" \">\n                <Node called \"rhs\" matching \"~\"[0-9]+\"\">\n                    <Node called \"term\" matching \"~\"[0-9]+\"\">\n                        <Node called \"atom\" matching \"~\"[0-9]+\"\">\n                            <Node called \"regex\" matching \"~\"[0-9]+\"\">\n                                <Node matching \"~\">\n                                <Node called \"literal\" matching \"\"[0-9]+\"\">\n                                <Node matching \"\">\n                <Node matching \"\">\n                <Node called \"eol\" matching \"\n                \">\n        <Node matching \"\">\n\nThe parse tree is tacked onto the exception, and the node whose visitor method\nraised the error is pointed out.\n\nWhy No Streaming Tree Processing?\n---------------------------------\n\nSome have asked why we don't process the tree as we go, SAX-style. There are\ntwo main reasons:\n\n1. It wouldn't work. With a PEG parser, no parsing decision is final until the\n   whole text is parsed. If we had to change a decision, we'd have to backtrack\n   and redo the SAX-style interpretation as well, which would involve\n   reconstituting part of the AST and quite possibly scuttling whatever you\n   were doing with the streaming output. (Note that some bursty SAX-style\n   processing may be possible in the future if we use cuts.)\n\n2. It interferes with the ability to derive multiple representations from the\n   AST: for example, turning wiki markup into first HTML and then text.\n\n\nFuture Directions\n=================\n\nRule Syntax Changes\n-------------------\n\n* Maybe support left-recursive rules like PyMeta, if anybody cares.\n* Ultimately, I'd like to get rid of explicit regexes and break them into more\n  atomic things like character classes. Then we can dynamically compile bits\n  of the grammar into regexes as necessary to boost speed.\n\nOptimizations\n-------------\n\n* Make RAM use almost constant by automatically inserting \"cuts\", as described\n  in\n  http://ialab.cs.tsukuba.ac.jp/~mizusima/publications/paste513-mizushima.pdf.\n  This would also improve error reporting, as we wouldn't backtrack out of\n  everything informative before finally failing.\n* Find all the distinct subexpressions, and unify duplicates for a better cache\n  hit ratio.\n* Think about having the user (optionally) provide some representative input\n  along with a grammar. We can then profile against it, see which expressions\n  are worth caching, and annotate the grammar. Perhaps there will even be\n  positions at which a given expression is more worth caching. Or we could keep\n  a count of how many times each cache entry has been used and evict the most\n  useless ones as RAM use grows.\n* We could possibly compile the grammar into VM instructions, like in \"A\n  parsing machine for PEGs\" by Medeiros.\n* If the recursion gets too deep in practice, use trampolining to dodge it.\n\nNiceties\n--------\n\n* Pijnu has a raft of tree manipulators. I don't think I want all of them, but\n  a judicious subset might be nice. Don't get into mixing formatting with tree\n  manipulation.\n  https://github.com/erikrose/pijnu/blob/master/library/node.py#L333. PyPy's\n  parsing lib exposes a sane subset:\n  http://doc.pypy.org/en/latest/rlib.html#tree-transformations.\n\n\nVersion History\n===============\n\n0.8.1\n  * Switch to a function-style ``print`` in the benchmark tests so we work\n    cleanly as a dependency on Python 3. (Edward Betts)\n\n0.8.0\n  * Make Grammar iteration ordered, making the ``__repr__`` more like the\n    original input. (Lucas Wiman)\n  * Improve text representation and error messages for anonymous\n    subexpressions. (Lucas Wiman)\n  * Expose BadGrammar and VisitationError as top-level imports.\n  * No longer crash when you try to compare a Node to an instance of a\n    different class. (Esben Sonne)\n  * Pin ``six`` at 1.9.0 to ensure we have ``python_2_unicode_compatible``.\n    (Sam Raker)\n  * Drop Python 2.6 support.\n\n0.7.0\n  * Add experimental token-based parsing, via TokenGrammar class, for those\n    operating on pre-lexed streams of tokens. This can, for example, help parse\n    indentation-sensitive languages that use the \"off-side rule\", like Python.\n    (Erik Rose)\n  * Common codebase for Python 2 and 3: no more 2to3 translation step (Mattias\n    Urlichs, Lucas Wiman)\n  * Drop Python 3.1 and 3.2 support.\n  * Fix a bug in ``Grammar.__repr__`` which fails to work on Python 3 since the\n    string_escape codec is gone in Python 3. (Lucas Wiman)\n  * Don't lose parentheses when printing representations of expressions.\n    (Michael Kelly)\n  * Make Grammar an immutable mapping (until we add automatic recompilation).\n    (Michael Kelly)\n\n0.6.2\n  * Make grammar compilation 100x faster. Thanks to dmoisset for the initial\n    patch.\n\n0.6.1\n  * Fix bug which made the default rule of a grammar invalid when it\n    contained a forward reference.\n\n0.6\n  .. warning::\n\n      This release makes backward-incompatible changes:\n\n      * The ``default_rule`` arg to Grammar's constructor has been replaced\n        with a method, ``some_grammar.default('rule_name')``, which returns a\n        new grammar just like the old except with its default rule changed.\n        This is to free up the constructor kwargs for custom rules.\n      * ``UndefinedLabel`` is no longer a subclass of ``VisitationError``. This\n        matters only in the unlikely case that you were catching\n        ``VisitationError`` exceptions and expecting to thus also catch\n        ``UndefinedLabel``.\n\n  * Add support for \"custom rules\" in Grammars. These provide a hook for simple\n    custom parsing hooks spelled as Python lambdas. For heavy-duty needs,\n    you can put in Compound Expressions with LazyReferences as subexpressions,\n    and the Grammar will hook them up for optimal efficiency--no calling\n    ``__getitem__`` on Grammar at parse time.\n  * Allow grammars without a default rule (in cases where there are no string\n    rules), which leads to also allowing empty grammars. Perhaps someone\n    building up grammars dynamically will find that useful.\n  * Add ``@rule`` decorator, allowing grammars to be constructed out of\n    notations on ``NodeVisitor`` methods. This saves looking back and forth\n    between the visitor and the grammar when there is only one visitor per\n    grammar.\n  * Add ``parse()`` and ``match()`` convenience methods to ``NodeVisitor``.\n    This makes the common case of parsing a string and applying exactly one\n    visitor to the AST shorter and simpler.\n  * Improve exception message when you forget to declare a visitor method.\n  * Add ``unwrapped_exceptions`` attribute to ``NodeVisitor``, letting you\n    name certain exceptions which propagate out of visitors without being\n    wrapped by ``VisitationError`` exceptions.\n  * Expose much more of the library in ``__init__``, making your imports\n    shorter.\n  * Drastically simplify reference resolution machinery. (Vladimir Keleshev)\n\n0.5\n  .. warning::\n\n      This release makes some backward-incompatible changes. See below.\n\n  * Add alpha-quality error reporting. Now, rather than returning ``None``,\n    ``parse()`` and ``match()`` raise ``ParseError`` if they don't succeed.\n    This makes more sense, since you'd rarely attempt to parse something and\n    not care if it succeeds. It was too easy before to forget to check for a\n    ``None`` result. ``ParseError`` gives you a human-readable unicode\n    representation as well as some attributes that let you construct your own\n    custom presentation.\n  * Grammar construction now raises ``ParseError`` rather than ``BadGrammar``\n    if it can't parse your rules.\n  * ``parse()`` now takes an optional ``pos`` argument, like ``match()``.\n  * Make the ``_str__()`` method of ``UndefinedLabel`` return the right type.\n  * Support splitting rules across multiple lines, interleaving comments,\n    putting multiple rules on one line (but don't do that) and all sorts of\n    other horrific behavior.\n  * Tolerate whitespace after opening parens.\n  * Add support for single-quoted literals.\n\n0.4\n  * Support Python 3.\n  * Fix ``import *`` for ``parsimonious.expressions``.\n  * Rewrite grammar compiler so right-recursive rules can be compiled and\n    parsing no longer fails in some cases with forward rule references.\n\n0.3\n  * Support comments, the ``!`` (\"not\") operator, and parentheses in grammar\n    definition syntax.\n  * Change the ``&`` operator to a prefix operator to conform to the original\n    PEG syntax. The version in Parsing Techniques was infix, and that's what I\n    used as a reference. However, the unary version is more convenient, as it\n    lets you spell ``AB & A`` as simply ``A &B``.\n  * Take the ``print`` statements out of the benchmark tests.\n  * Give Node an evaluate-able ``__repr__``.\n\n0.2\n  * Support matching of prefixes and other not-to-the-end slices of strings by\n    making ``match()`` public and able to initialize a new cache. Add\n    ``match()`` callthrough method to ``Grammar``.\n  * Report a ``BadGrammar`` exception (rather than crashing) when there are\n    mistakes in a grammar definition.\n  * Simplify grammar compilation internals: get rid of superfluous visitor\n    methods and factor up repetitive ones. Simplify rule grammar as well.\n  * Add ``NodeVisitor.lift_child`` convenience method.\n  * Rename ``VisitationException`` to ``VisitationError`` for consistency with\n    the standard Python exception hierarchy.\n  * Rework ``repr`` and ``str`` values for grammars and expressions. Now they\n    both look like rule syntax. Grammars are even round-trippable! This fixes a\n    unicode encoding error when printing nodes that had parsed unicode text.\n  * Add tox for testing. Stop advertising Python 2.5 support, which never\n    worked (and won't unless somebody cares a lot, since it makes Python 3\n    support harder).\n  * Settle (hopefully) on the term \"rule\" to mean \"the string representation of\n    a production\". Get rid of the vague, mysterious \"DSL\".\n\n0.1\n  * A rough but useable preview release\n\nThanks to Wiki Loves Monuments Panama for showing their support with a generous\ngift.", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/erikrose/parsimonious", "keywords": "parse,parser,parsing,peg,packrat,grammar,language", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "parsimonious", "package_url": "https://pypi.org/project/parsimonious/", "platform": "", "project_url": "https://pypi.org/project/parsimonious/", "project_urls": {"Homepage": "https://github.com/erikrose/parsimonious"}, "release_url": "https://pypi.org/project/parsimonious/0.8.1/", "requires_dist": null, "requires_python": "", "summary": "(Soon to be) the fastest pure-Python PEG parser I could muster", "version": "0.8.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Parsimonious aims to be the fastest arbitrary-lookahead parser written in pure\nPython\u2014and the most usable. It\u2019s based on parsing expression grammars (PEGs),\nwhich means you feed it a simplified sort of EBNF notation. Parsimonious was\ndesigned to undergird a MediaWiki parser that wouldn\u2019t take 5 seconds or a GB\nof RAM to do one page, but it\u2019s applicable to all sorts of languages.</p>\n<div id=\"goals\">\n<h2>Goals</h2>\n<ul>\n<li>Speed</li>\n<li>Frugal RAM use</li>\n<li>Minimalistic, understandable, idiomatic Python code</li>\n<li>Readable grammars</li>\n<li>Extensible grammars</li>\n<li>Complete test coverage</li>\n<li>Separation of concerns. Some Python parsing kits mix recognition with\ninstructions about how to turn the resulting tree into some kind of other\nrepresentation. This is limiting when you want to do several different things\nwith a tree: for example, render wiki markup to HTML <em>or</em> to text.</li>\n<li>Good error reporting. I want the parser to work <em>with</em> me as I develop a\ngrammar.</li>\n</ul>\n</div>\n<div id=\"example-usage\">\n<h2>Example Usage</h2>\n<p>Here\u2019s how to build a simple grammar:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">parsimonious.grammar</span> <span class=\"kn\">import</span> <span class=\"n\">Grammar</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">grammar</span> <span class=\"o\">=</span> <span class=\"n\">Grammar</span><span class=\"p\">(</span>\n<span class=\"o\">...</span>     <span class=\"s2\">\"\"\"\n...     bold_text  = bold_open text bold_close\n...     text       = ~\"[A-Z 0-9]*\"i\n...     bold_open  = \"((\"\n...     bold_close = \"))\"\n...     \"\"\"</span><span class=\"p\">)</span>\n</pre>\n<p>You can have forward references and even right recursion; it\u2019s all taken care\nof by the grammar compiler. The first rule is taken to be the default start\nsymbol, but you can override that.</p>\n<p>Next, let\u2019s parse something and get an abstract syntax tree:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">print</span> <span class=\"n\">grammar</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"p\">(</span><span class=\"s1\">'((bold stuff))'</span><span class=\"p\">)</span>\n<span class=\"o\">&lt;</span><span class=\"n\">Node</span> <span class=\"n\">called</span> <span class=\"s2\">\"bold_text\"</span> <span class=\"n\">matching</span> <span class=\"s2\">\"((bold stuff))\"</span><span class=\"o\">&gt;</span>\n    <span class=\"o\">&lt;</span><span class=\"n\">Node</span> <span class=\"n\">called</span> <span class=\"s2\">\"bold_open\"</span> <span class=\"n\">matching</span> <span class=\"s2\">\"((\"</span><span class=\"o\">&gt;</span>\n    <span class=\"o\">&lt;</span><span class=\"n\">RegexNode</span> <span class=\"n\">called</span> <span class=\"s2\">\"text\"</span> <span class=\"n\">matching</span> <span class=\"s2\">\"bold stuff\"</span><span class=\"o\">&gt;</span>\n    <span class=\"o\">&lt;</span><span class=\"n\">Node</span> <span class=\"n\">called</span> <span class=\"s2\">\"bold_close\"</span> <span class=\"n\">matching</span> <span class=\"s2\">\"))\"</span><span class=\"o\">&gt;</span>\n</pre>\n<p>You\u2019d typically then use a <tt>nodes.NodeVisitor</tt> subclass (see below) to walk\nthe tree and do something useful with it.</p>\n</div>\n<div id=\"status\">\n<h2>Status</h2>\n<ul>\n<li>Everything that exists works. Test coverage is good.</li>\n<li>I don\u2019t plan on making any backward-incompatible changes to the rule syntax\nin the future, so you can write grammars with confidence.</li>\n<li>It may be slow and use a lot of RAM; I haven\u2019t measured either yet. However,\nI have yet to begin optimizing in earnest.</li>\n<li>Error reporting is now in place. <tt>repr</tt> methods of expressions, grammars,\nand nodes are clear and helpful as well. The <tt>Grammar</tt> ones are\neven round-trippable!</li>\n<li>The grammar extensibility story is underdeveloped at the moment. You should\nbe able to extend a grammar by simply concatening more rules onto the\nexisting ones; later rules of the same name should override previous ones.\nHowever, this is untested and may not be the final story.</li>\n<li>Sphinx docs are coming, but the docstrings are quite useful now.</li>\n<li>Note that there may be API changes until we get to 1.0, so be sure to pin to\nthe version you\u2019re using.</li>\n</ul>\n<div id=\"coming-soon\">\n<h3>Coming Soon</h3>\n<ul>\n<li>Optimizations to make Parsimonious worthy of its name</li>\n<li>Tighter RAM use</li>\n<li>Better-thought-out grammar extensibility story</li>\n<li>Amazing grammar debugging</li>\n</ul>\n</div>\n</div>\n<div id=\"a-little-about-peg-parsers\">\n<h2>A Little About PEG Parsers</h2>\n<p>PEG parsers don\u2019t draw a distinction between lexing and parsing; everything is\ndone at once. As a result, there is no lookahead limit, as there is with, for\ninstance, Yacc. And, due to both of these properties, PEG grammars are easier\nto write: they\u2019re basically just a more practical dialect of EBNF. With\ncaching, they take O(grammar size * text length) memory (though I plan to do\nbetter), but they run in O(text length) time.</p>\n<div id=\"more-technically\">\n<h3>More Technically</h3>\n<p>PEGs can describe a superset of <em>LL(k)</em> languages, any deterministic <em>LR(k)</em>\nlanguage, and many others\u2014including some that aren\u2019t context-free\n(<a href=\"http://www.brynosaurus.com/pub/lang/peg.pdf\" rel=\"nofollow\">http://www.brynosaurus.com/pub/lang/peg.pdf</a>). They can also deal with what\nwould be ambiguous languages if described in canonical EBNF. They do this by\ntrading the <tt>|</tt> alternation operator for the <tt>/</tt> operator, which works the\nsame except that it makes priority explicit: <tt>a / b / c</tt> first tries matching\n<tt>a</tt>. If that fails, it tries <tt>b</tt>, and, failing that, moves on to <tt>c</tt>.\nThus, ambiguity is resolved by always yielding the first successful recognition.</p>\n</div>\n</div>\n<div id=\"writing-grammars\">\n<h2>Writing Grammars</h2>\n<p>Grammars are defined by a series of rules. The syntax should be familiar to\nanyone who uses regexes or reads programming language manuals. An example will\nserve best:</p>\n<pre><span class=\"n\">my_grammar</span> <span class=\"o\">=</span> <span class=\"n\">Grammar</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s2\">\"\"\"\n    styled_text = bold_text / italic_text\n    bold_text   = \"((\" text \"))\"\n    italic_text = \"''\" text \"''\"\n    text        = ~\"[A-Z 0-9]*\"i\n    \"\"\"</span><span class=\"p\">)</span>\n</pre>\n<p>You can wrap a rule across multiple lines if you like; the syntax is very\nforgiving.</p>\n<div id=\"syntax-reference\">\n<h3>Syntax Reference</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><tt>\"some literal\"</tt></td>\n<td>Used to quote literals. Backslash escaping and Python\nconventions for \u201craw\u201d and Unicode strings help support\nfiddly characters.</td>\n</tr>\n<tr><td>[space]</td>\n<td>Sequences are made out of space- or tab-delimited\nthings. <tt>a b c</tt> matches spots where those 3\nterms appear in that order.</td>\n</tr>\n<tr><td><tt>a / b / c</tt></td>\n<td>Alternatives. The first to succeed of <tt>a / b / c</tt>\nwins.</td>\n</tr>\n<tr><td><tt>thing?</tt></td>\n<td>An optional expression. This is greedy, always consuming\n<tt>thing</tt> if it exists.</td>\n</tr>\n<tr><td><tt>&amp;thing</tt></td>\n<td>A lookahead assertion. Ensures <tt>thing</tt> matches at the\ncurrent position but does not consume it.</td>\n</tr>\n<tr><td><tt>!thing</tt></td>\n<td>A negative lookahead assertion. Matches if <tt>thing</tt>\nisn\u2019t found here. Doesn\u2019t consume any text.</td>\n</tr>\n<tr><td><tt>things*</tt></td>\n<td>Zero or more things. This is greedy, always consuming as\nmany repetitions as it can.</td>\n</tr>\n<tr><td><tt>things+</tt></td>\n<td>One or more things. This is greedy, always consuming as\nmany repetitions as it can.</td>\n</tr>\n<tr><td><tt>~r\"regex\"ilmsux</tt></td>\n<td>Regexes have <tt>~</tt> in front and are quoted like\nliterals. Any flags follow the end quotes as single\nchars. Regexes are good for representing character\nclasses (<tt><span class=\"pre\">[a-z0-9]</span></tt>) and optimizing for speed. The\ndownside is that they won\u2019t be able to take advantage\nof our fancy debugging, once we get that working.\nUltimately, I\u2019d like to deprecate explicit regexes and\ninstead have Parsimonious dynamically build them out of\nsimpler primitives.</td>\n</tr>\n<tr><td><tt>(things)</tt></td>\n<td>Parentheses are used for grouping, like in every other\nlanguage.</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<div id=\"optimizing-grammars\">\n<h2>Optimizing Grammars</h2>\n<div id=\"don-t-repeat-expressions\">\n<h3>Don\u2019t Repeat Expressions</h3>\n<p>If you need a <tt><span class=\"pre\">~\"[a-z0-9]\"i</span></tt> at two points in your grammar, don\u2019t type it\ntwice. Make it a rule of its own, and reference it from wherever you need it.\nYou\u2019ll get the most out of the caching this way, since cache lookups are by\nexpression object identity (for speed).</p>\n<p>Even if you have an expression that\u2019s very simple, not repeating it will\nsave RAM, as there can, at worst, be a cached int for every char in the text\nyou\u2019re parsing. In the future, we may identify repeated subexpressions\nautomatically and factor them up while building the grammar.</p>\n<p>How much should you shove into one regex, versus how much should you break them\nup to not repeat yourself? That\u2019s a fine balance and worthy of benchmarking.\nMore stuff jammed into a regex will execute faster, because it doesn\u2019t have to\nrun any Python between pieces, but a broken-up one will give better cache\nperformance if the individual pieces are re-used elsewhere. If the pieces of a\nregex aren\u2019t used anywhere else, by all means keep the whole thing together.</p>\n</div>\n<div id=\"quantifiers\">\n<h3>Quantifiers</h3>\n<p>Bring your <tt>?</tt> and <tt>*</tt> quantifiers up to the highest level you\ncan. Otherwise, lower-level patterns could succeed but be empty and put a bunch\nof useless nodes in your tree that didn\u2019t really match anything.</p>\n</div>\n</div>\n<div id=\"processing-parse-trees\">\n<h2>Processing Parse Trees</h2>\n<p>A parse tree has a node for each expression matched, even if it matched a\nzero-length string, like <tt>\"thing\"?</tt> might.</p>\n<p>The <tt>NodeVisitor</tt> class provides an inversion-of-control framework for\nwalking a tree and returning a new construct (tree, string, or whatever) based\non it. For now, have a look at its docstrings for more detail. There\u2019s also a\ngood example in <tt>grammar.RuleVisitor</tt>. Notice how we take advantage of nodes\u2019\niterability by using tuple unpacks in the formal parameter lists:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">visit_or_term</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">or_term</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">slash</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">term</span><span class=\"p\">)):</span>\n    <span class=\"o\">...</span>\n</pre>\n<p>For reference, here is the production the above unpacks:</p>\n<pre>or_term = \"/\" _ term\n</pre>\n<p>When something goes wrong in your visitor, you get a nice error like this:</p>\n<pre>[normal traceback here...]\nVisitationException: 'Node' object has no attribute 'foo'\n\nParse tree:\n&lt;Node called \"rules\" matching \"number = ~\"[0-9]+\"\"&gt;  &lt;-- *** We were here. ***\n    &lt;Node matching \"number = ~\"[0-9]+\"\"&gt;\n        &lt;Node called \"rule\" matching \"number = ~\"[0-9]+\"\"&gt;\n            &lt;Node matching \"\"&gt;\n            &lt;Node called \"label\" matching \"number\"&gt;\n            &lt;Node matching \" \"&gt;\n                &lt;Node called \"_\" matching \" \"&gt;\n            &lt;Node matching \"=\"&gt;\n            &lt;Node matching \" \"&gt;\n                &lt;Node called \"_\" matching \" \"&gt;\n            &lt;Node called \"rhs\" matching \"~\"[0-9]+\"\"&gt;\n                &lt;Node called \"term\" matching \"~\"[0-9]+\"\"&gt;\n                    &lt;Node called \"atom\" matching \"~\"[0-9]+\"\"&gt;\n                        &lt;Node called \"regex\" matching \"~\"[0-9]+\"\"&gt;\n                            &lt;Node matching \"~\"&gt;\n                            &lt;Node called \"literal\" matching \"\"[0-9]+\"\"&gt;\n                            &lt;Node matching \"\"&gt;\n            &lt;Node matching \"\"&gt;\n            &lt;Node called \"eol\" matching \"\n            \"&gt;\n    &lt;Node matching \"\"&gt;\n</pre>\n<p>The parse tree is tacked onto the exception, and the node whose visitor method\nraised the error is pointed out.</p>\n<div id=\"why-no-streaming-tree-processing\">\n<h3>Why No Streaming Tree Processing?</h3>\n<p>Some have asked why we don\u2019t process the tree as we go, SAX-style. There are\ntwo main reasons:</p>\n<ol>\n<li>It wouldn\u2019t work. With a PEG parser, no parsing decision is final until the\nwhole text is parsed. If we had to change a decision, we\u2019d have to backtrack\nand redo the SAX-style interpretation as well, which would involve\nreconstituting part of the AST and quite possibly scuttling whatever you\nwere doing with the streaming output. (Note that some bursty SAX-style\nprocessing may be possible in the future if we use cuts.)</li>\n<li>It interferes with the ability to derive multiple representations from the\nAST: for example, turning wiki markup into first HTML and then text.</li>\n</ol>\n</div>\n</div>\n<div id=\"future-directions\">\n<h2>Future Directions</h2>\n<div id=\"rule-syntax-changes\">\n<h3>Rule Syntax Changes</h3>\n<ul>\n<li>Maybe support left-recursive rules like PyMeta, if anybody cares.</li>\n<li>Ultimately, I\u2019d like to get rid of explicit regexes and break them into more\natomic things like character classes. Then we can dynamically compile bits\nof the grammar into regexes as necessary to boost speed.</li>\n</ul>\n</div>\n<div id=\"optimizations\">\n<h3>Optimizations</h3>\n<ul>\n<li>Make RAM use almost constant by automatically inserting \u201ccuts\u201d, as described\nin\n<a href=\"http://ialab.cs.tsukuba.ac.jp/~mizusima/publications/paste513-mizushima.pdf\" rel=\"nofollow\">http://ialab.cs.tsukuba.ac.jp/~mizusima/publications/paste513-mizushima.pdf</a>.\nThis would also improve error reporting, as we wouldn\u2019t backtrack out of\neverything informative before finally failing.</li>\n<li>Find all the distinct subexpressions, and unify duplicates for a better cache\nhit ratio.</li>\n<li>Think about having the user (optionally) provide some representative input\nalong with a grammar. We can then profile against it, see which expressions\nare worth caching, and annotate the grammar. Perhaps there will even be\npositions at which a given expression is more worth caching. Or we could keep\na count of how many times each cache entry has been used and evict the most\nuseless ones as RAM use grows.</li>\n<li>We could possibly compile the grammar into VM instructions, like in \u201cA\nparsing machine for PEGs\u201d by Medeiros.</li>\n<li>If the recursion gets too deep in practice, use trampolining to dodge it.</li>\n</ul>\n</div>\n<div id=\"niceties\">\n<h3>Niceties</h3>\n<ul>\n<li>Pijnu has a raft of tree manipulators. I don\u2019t think I want all of them, but\na judicious subset might be nice. Don\u2019t get into mixing formatting with tree\nmanipulation.\n<a href=\"https://github.com/erikrose/pijnu/blob/master/library/node.py#L333\" rel=\"nofollow\">https://github.com/erikrose/pijnu/blob/master/library/node.py#L333</a>. PyPy\u2019s\nparsing lib exposes a sane subset:\n<a href=\"http://doc.pypy.org/en/latest/rlib.html#tree-transformations\" rel=\"nofollow\">http://doc.pypy.org/en/latest/rlib.html#tree-transformations</a>.</li>\n</ul>\n</div>\n</div>\n<div id=\"version-history\">\n<h2>Version History</h2>\n<dl>\n<dt>0.8.1</dt>\n<dd><ul>\n<li>Switch to a function-style <tt>print</tt> in the benchmark tests so we work\ncleanly as a dependency on Python 3. (Edward Betts)</li>\n</ul>\n</dd>\n<dt>0.8.0</dt>\n<dd><ul>\n<li>Make Grammar iteration ordered, making the <tt>__repr__</tt> more like the\noriginal input. (Lucas Wiman)</li>\n<li>Improve text representation and error messages for anonymous\nsubexpressions. (Lucas Wiman)</li>\n<li>Expose BadGrammar and VisitationError as top-level imports.</li>\n<li>No longer crash when you try to compare a Node to an instance of a\ndifferent class. (Esben Sonne)</li>\n<li>Pin <tt>six</tt> at 1.9.0 to ensure we have <tt>python_2_unicode_compatible</tt>.\n(Sam Raker)</li>\n<li>Drop Python 2.6 support.</li>\n</ul>\n</dd>\n<dt>0.7.0</dt>\n<dd><ul>\n<li>Add experimental token-based parsing, via TokenGrammar class, for those\noperating on pre-lexed streams of tokens. This can, for example, help parse\nindentation-sensitive languages that use the \u201coff-side rule\u201d, like Python.\n(Erik Rose)</li>\n<li>Common codebase for Python 2 and 3: no more 2to3 translation step (Mattias\nUrlichs, Lucas Wiman)</li>\n<li>Drop Python 3.1 and 3.2 support.</li>\n<li>Fix a bug in <tt>Grammar.__repr__</tt> which fails to work on Python 3 since the\nstring_escape codec is gone in Python 3. (Lucas Wiman)</li>\n<li>Don\u2019t lose parentheses when printing representations of expressions.\n(Michael Kelly)</li>\n<li>Make Grammar an immutable mapping (until we add automatic recompilation).\n(Michael Kelly)</li>\n</ul>\n</dd>\n<dt>0.6.2</dt>\n<dd><ul>\n<li>Make grammar compilation 100x faster. Thanks to dmoisset for the initial\npatch.</li>\n</ul>\n</dd>\n<dt>0.6.1</dt>\n<dd><ul>\n<li>Fix bug which made the default rule of a grammar invalid when it\ncontained a forward reference.</li>\n</ul>\n</dd>\n<dt>0.6</dt>\n<dd><div>\n<p>Warning</p>\n<p>This release makes backward-incompatible changes:</p>\n<ul>\n<li>The <tt>default_rule</tt> arg to Grammar\u2019s constructor has been replaced\nwith a method, <tt><span class=\"pre\">some_grammar.default('rule_name')</span></tt>, which returns a\nnew grammar just like the old except with its default rule changed.\nThis is to free up the constructor kwargs for custom rules.</li>\n<li><tt>UndefinedLabel</tt> is no longer a subclass of <tt>VisitationError</tt>. This\nmatters only in the unlikely case that you were catching\n<tt>VisitationError</tt> exceptions and expecting to thus also catch\n<tt>UndefinedLabel</tt>.</li>\n</ul>\n</div>\n<ul>\n<li>Add support for \u201ccustom rules\u201d in Grammars. These provide a hook for simple\ncustom parsing hooks spelled as Python lambdas. For heavy-duty needs,\nyou can put in Compound Expressions with LazyReferences as subexpressions,\nand the Grammar will hook them up for optimal efficiency\u2013no calling\n<tt>__getitem__</tt> on Grammar at parse time.</li>\n<li>Allow grammars without a default rule (in cases where there are no string\nrules), which leads to also allowing empty grammars. Perhaps someone\nbuilding up grammars dynamically will find that useful.</li>\n<li>Add <tt>@rule</tt> decorator, allowing grammars to be constructed out of\nnotations on <tt>NodeVisitor</tt> methods. This saves looking back and forth\nbetween the visitor and the grammar when there is only one visitor per\ngrammar.</li>\n<li>Add <tt>parse()</tt> and <tt>match()</tt> convenience methods to <tt>NodeVisitor</tt>.\nThis makes the common case of parsing a string and applying exactly one\nvisitor to the AST shorter and simpler.</li>\n<li>Improve exception message when you forget to declare a visitor method.</li>\n<li>Add <tt>unwrapped_exceptions</tt> attribute to <tt>NodeVisitor</tt>, letting you\nname certain exceptions which propagate out of visitors without being\nwrapped by <tt>VisitationError</tt> exceptions.</li>\n<li>Expose much more of the library in <tt>__init__</tt>, making your imports\nshorter.</li>\n<li>Drastically simplify reference resolution machinery. (Vladimir Keleshev)</li>\n</ul>\n</dd>\n<dt>0.5</dt>\n<dd><div>\n<p>Warning</p>\n<p>This release makes some backward-incompatible changes. See below.</p>\n</div>\n<ul>\n<li>Add alpha-quality error reporting. Now, rather than returning <tt>None</tt>,\n<tt>parse()</tt> and <tt>match()</tt> raise <tt>ParseError</tt> if they don\u2019t succeed.\nThis makes more sense, since you\u2019d rarely attempt to parse something and\nnot care if it succeeds. It was too easy before to forget to check for a\n<tt>None</tt> result. <tt>ParseError</tt> gives you a human-readable unicode\nrepresentation as well as some attributes that let you construct your own\ncustom presentation.</li>\n<li>Grammar construction now raises <tt>ParseError</tt> rather than <tt>BadGrammar</tt>\nif it can\u2019t parse your rules.</li>\n<li><tt>parse()</tt> now takes an optional <tt>pos</tt> argument, like <tt>match()</tt>.</li>\n<li>Make the <tt>_str__()</tt> method of <tt>UndefinedLabel</tt> return the right type.</li>\n<li>Support splitting rules across multiple lines, interleaving comments,\nputting multiple rules on one line (but don\u2019t do that) and all sorts of\nother horrific behavior.</li>\n<li>Tolerate whitespace after opening parens.</li>\n<li>Add support for single-quoted literals.</li>\n</ul>\n</dd>\n<dt>0.4</dt>\n<dd><ul>\n<li>Support Python 3.</li>\n<li>Fix <tt>import *</tt> for <tt>parsimonious.expressions</tt>.</li>\n<li>Rewrite grammar compiler so right-recursive rules can be compiled and\nparsing no longer fails in some cases with forward rule references.</li>\n</ul>\n</dd>\n<dt>0.3</dt>\n<dd><ul>\n<li>Support comments, the <tt>!</tt> (\u201cnot\u201d) operator, and parentheses in grammar\ndefinition syntax.</li>\n<li>Change the <tt>&amp;</tt> operator to a prefix operator to conform to the original\nPEG syntax. The version in Parsing Techniques was infix, and that\u2019s what I\nused as a reference. However, the unary version is more convenient, as it\nlets you spell <tt>AB &amp; A</tt> as simply <tt>A &amp;B</tt>.</li>\n<li>Take the <tt>print</tt> statements out of the benchmark tests.</li>\n<li>Give Node an evaluate-able <tt>__repr__</tt>.</li>\n</ul>\n</dd>\n<dt>0.2</dt>\n<dd><ul>\n<li>Support matching of prefixes and other not-to-the-end slices of strings by\nmaking <tt>match()</tt> public and able to initialize a new cache. Add\n<tt>match()</tt> callthrough method to <tt>Grammar</tt>.</li>\n<li>Report a <tt>BadGrammar</tt> exception (rather than crashing) when there are\nmistakes in a grammar definition.</li>\n<li>Simplify grammar compilation internals: get rid of superfluous visitor\nmethods and factor up repetitive ones. Simplify rule grammar as well.</li>\n<li>Add <tt>NodeVisitor.lift_child</tt> convenience method.</li>\n<li>Rename <tt>VisitationException</tt> to <tt>VisitationError</tt> for consistency with\nthe standard Python exception hierarchy.</li>\n<li>Rework <tt>repr</tt> and <tt>str</tt> values for grammars and expressions. Now they\nboth look like rule syntax. Grammars are even round-trippable! This fixes a\nunicode encoding error when printing nodes that had parsed unicode text.</li>\n<li>Add tox for testing. Stop advertising Python 2.5 support, which never\nworked (and won\u2019t unless somebody cares a lot, since it makes Python 3\nsupport harder).</li>\n<li>Settle (hopefully) on the term \u201crule\u201d to mean \u201cthe string representation of\na production\u201d. Get rid of the vague, mysterious \u201cDSL\u201d.</li>\n</ul>\n</dd>\n<dt>0.1</dt>\n<dd><ul>\n<li>A rough but useable preview release</li>\n</ul>\n</dd>\n</dl>\n<p>Thanks to Wiki Loves Monuments Panama for showing their support with a generous\ngift.</p>\n</div>\n\n          </div>"}, "last_serial": 3983512, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "0d20529608681e9014046ae805e0d495", "sha256": "38befd276f1b9df8aad3f9ec73d8845c76d9072b3fb2fee8d636abc9f0cba0d4"}, "downloads": -1, "filename": "parsimonious-0.1.tar.gz", "has_sig": false, "md5_digest": "0d20529608681e9014046ae805e0d495", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23496, "upload_time": "2012-12-01T11:05:12", "upload_time_iso_8601": "2012-12-01T11:05:12.120682Z", "url": "https://files.pythonhosted.org/packages/89/ab/72d5399c76e2428b3393b991603b7b542fd311e94773d415bb60baf7cf13/parsimonious-0.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "77a9383470627f9803c471a41799cc78", "sha256": "03d0f1f948ad29bd60d48100d6cd0631b0fe7e5e8dfad483f0764cd8bba644c3"}, "downloads": -1, "filename": "parsimonious-0.2.tar.gz", "has_sig": false, "md5_digest": "77a9383470627f9803c471a41799cc78", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23946, "upload_time": "2012-12-12T04:43:03", "upload_time_iso_8601": "2012-12-12T04:43:03.948989Z", "url": "https://files.pythonhosted.org/packages/15/eb/f1c3885a1fda8e75eab64cb7a9d755b502492b221a15a97bc15c5bec1fe8/parsimonious-0.2.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "b9de92de3c5d7155aec9e8c15ba25ed5", "sha256": "83613ac6d7226071331ba45277c59c8ea9e8d3c5b1896b8a68cd511b2236c034"}, "downloads": -1, "filename": "parsimonious-0.3.tar.gz", "has_sig": false, "md5_digest": "b9de92de3c5d7155aec9e8c15ba25ed5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25423, "upload_time": "2013-01-05T08:23:12", "upload_time_iso_8601": "2013-01-05T08:23:12.985403Z", "url": "https://files.pythonhosted.org/packages/bc/25/f2268d92c24dea6315e0d6f9f2d18e5459c94069d5457c598e2ae0d5718d/parsimonious-0.3.tar.gz", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "2533787e4b05c2ec4971c27b4953ff87", "sha256": "ae6bdc8c669f6b83557b3c0b5a2d6a0e1326cf933b301842bba09fb546d5e6fd"}, "downloads": -1, "filename": "parsimonious-0.4.tar.gz", "has_sig": false, "md5_digest": "2533787e4b05c2ec4971c27b4953ff87", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26968, "upload_time": "2013-04-17T06:27:02", "upload_time_iso_8601": "2013-04-17T06:27:02.611099Z", "url": "https://files.pythonhosted.org/packages/04/ed/0a1a92a97eb03ae37dc92718ce86402f107655bfc8cfa1f791df32163833/parsimonious-0.4.tar.gz", "yanked": false}], "0.5": [{"comment_text": "", "digests": {"md5": "09fde0f5d8a8c0e77173a82ee5d16f88", "sha256": "d43832ba25ea4ce747b0aab675c07ea4638417a2cc9b9c5c0cb8ef68294b12c0"}, "downloads": -1, "filename": "parsimonious-0.5.tar.gz", "has_sig": false, "md5_digest": "09fde0f5d8a8c0e77173a82ee5d16f88", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 30389, "upload_time": "2013-05-21T23:39:00", "upload_time_iso_8601": "2013-05-21T23:39:00.984211Z", "url": "https://files.pythonhosted.org/packages/4d/55/831dd0d5e552f2d7bc68f5cf5dbdc02d3f3cabf096a4dbb0bc43c870eae2/parsimonious-0.5.tar.gz", "yanked": false}], "0.6": [{"comment_text": "", "digests": {"md5": "0cb92a858670074864bf1819f952337c", "sha256": "44f2e29cde9ef7bf348c8754b26139a608690d9f023dbc849ead0246a0c95d0a"}, "downloads": -1, "filename": "parsimonious-0.6.tar.gz", "has_sig": false, "md5_digest": "0cb92a858670074864bf1819f952337c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 35236, "upload_time": "2014-09-09T19:18:39", "upload_time_iso_8601": "2014-09-09T19:18:39.571744Z", "url": "https://files.pythonhosted.org/packages/cf/dc/2e6813aaefa826a08848b0667e92313942429e19e63349f14241b239e485/parsimonious-0.6.tar.gz", "yanked": false}], "0.6.1": [{"comment_text": "", "digests": {"md5": "0ff5dfa13fd62e682708f68665be5218", "sha256": "51542176aec3a1146086318dfccde31b38cd2477a25f987e84770f3388e89bc5"}, "downloads": -1, "filename": "parsimonious-0.6.1.tar.gz", "has_sig": false, "md5_digest": "0ff5dfa13fd62e682708f68665be5218", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 35424, "upload_time": "2014-09-24T17:42:34", "upload_time_iso_8601": "2014-09-24T17:42:34.328314Z", "url": "https://files.pythonhosted.org/packages/1c/69/9c5beb595ebafc91f703c1ce3f69708275ddf5ba1beeea87c38cc8535433/parsimonious-0.6.1.tar.gz", "yanked": false}], "0.6.2": [{"comment_text": "", "digests": {"md5": "7a32835027927522be5ae2b523862a75", "sha256": "423ae2e16061504418ab7abf0a740e26a781f9bc7674a6cf5e2f11edb4ae8029"}, "downloads": -1, "filename": "parsimonious-0.6.2.tar.gz", "has_sig": false, "md5_digest": "7a32835027927522be5ae2b523862a75", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 35529, "upload_time": "2014-10-20T04:48:25", "upload_time_iso_8601": "2014-10-20T04:48:25.304373Z", "url": "https://files.pythonhosted.org/packages/ae/37/b58d4c02513b4831db14e9759d886075bcc95a2c4f13a68c28ddfca33268/parsimonious-0.6.2.tar.gz", "yanked": false}], "0.7.0": [{"comment_text": "", "digests": {"md5": "7fce85c276a07066337990c344b3ba85", "sha256": "396d424f64f834f9463e81ba79a331661507a21f1ed7b644f7f6a744006fd938"}, "downloads": -1, "filename": "parsimonious-0.7.0.tar.gz", "has_sig": false, "md5_digest": "7fce85c276a07066337990c344b3ba85", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 37650, "upload_time": "2016-09-09T17:11:12", "upload_time_iso_8601": "2016-09-09T17:11:12.000273Z", "url": "https://files.pythonhosted.org/packages/11/db/06a1d0a41b4d236cd84fb27fced4479645b1fb6100501e03accde2804e51/parsimonious-0.7.0.tar.gz", "yanked": false}], "0.8.0": [{"comment_text": "", "digests": {"md5": "e9b5af1a3ccc04a0f1321e1084b553be", "sha256": "ae0869d72a6e57703f24313a5f5748e73ebff836e6fe8b3ddf34ea0dc00d086b"}, "downloads": -1, "filename": "parsimonious-0.8.0.tar.gz", "has_sig": false, "md5_digest": "e9b5af1a3ccc04a0f1321e1084b553be", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 38730, "upload_time": "2017-08-05T01:07:19", "upload_time_iso_8601": "2017-08-05T01:07:19.656733Z", "url": "https://files.pythonhosted.org/packages/4a/89/32c55944cd30dff856f16859ee325b13c83c260d0c56c0eed511e8063c87/parsimonious-0.8.0.tar.gz", "yanked": false}], "0.8.1": [{"comment_text": "", "digests": {"md5": "6cd62ccdb5f4a6e2f8d2886a08a36209", "sha256": "3add338892d580e0cb3b1a39e4a1b427ff9f687858fdd61097053742391a9f6b"}, "downloads": -1, "filename": "parsimonious-0.8.1.tar.gz", "has_sig": false, "md5_digest": "6cd62ccdb5f4a6e2f8d2886a08a36209", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 45057, "upload_time": "2018-06-21T01:50:51", "upload_time_iso_8601": "2018-06-21T01:50:51.298957Z", "url": "https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "6cd62ccdb5f4a6e2f8d2886a08a36209", "sha256": "3add338892d580e0cb3b1a39e4a1b427ff9f687858fdd61097053742391a9f6b"}, "downloads": -1, "filename": "parsimonious-0.8.1.tar.gz", "has_sig": false, "md5_digest": "6cd62ccdb5f4a6e2f8d2886a08a36209", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 45057, "upload_time": "2018-06-21T01:50:51", "upload_time_iso_8601": "2018-06-21T01:50:51.298957Z", "url": "https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:58:16 2020"}