{"info": {"author": "Tom Charnock", "author_email": "charnock@iap.fr", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.7"], "description": "\n\n```python\n%load_ext autoreload\n%autoreload 2\n```\n\n# Information maximiser\n\nUsing neural networks, sufficient statistics can be obtained from data by maximising the Fisher information.\n\nThe neural network takes some data ${\\bf d}$ and maps it to a compressed summary $\\mathscr{f}:{\\bf d}\\to{\\bf x}$ where ${\\bf x}$ can have the same size as the dimensionality of the parameter space, rather than the data space.\n\nTo train the neural network a batch of simulations ${\\bf d}_{\\sf sim}^{\\sf fid}$ created at a fiducial parameter value $\\boldsymbol{\\theta}^{\\rm fid}$ are compressed by the neural network to obtain ${\\bf x}_{\\sf sim}^{\\sf fid}$. From this we can calculate the covariance ${\\bf C_\\mathscr{f}}$ of the compressed summaries. We learn about model parameter distributions using the derivative of the simulation. This can be provided analytically or numercially using ${\\bf d}_{\\sf sim}^{\\sf fid+}$ created above the fiducial parameter value $\\boldsymbol{\\theta}^{\\sf fid+}$ and ${\\bf d}_{\\sf sim}^{\\sf fid-}$ created below the fiducial parameter value $\\boldsymbol{\\theta}^{\\sf fid-}$. The simulations are compressed using the network and used to find mean of the summaries\n$$\\frac{\\partial\\boldsymbol{\\mu}_\\mathscr{f}}{\\partial\\theta_\\alpha}=\\frac{1}{n_\\textrm{derivs}}\\sum_{i=1}^{n_\\textrm{derivs}}\\frac{{\\bf x}_{\\sf sim}^{\\sf fid+}-{\\bf x}_{\\sf sim}^{\\sf fid-}}{\\boldsymbol{\\theta}^{\\sf fid+}-\\boldsymbol{\\theta}^{\\sf fid-}}.$$\nIf the derivative of the simulations with respect to the parameters can be calculated analytically (or via autograd, etc.) then that can be used directly using the chain rule since the derivative of the network outputs with respect to the network input can be calculated easily\n$$\\frac{\\partial\\mu_\\mathscr{f}}{\\partial\\theta_\\alpha} = \\frac{1}{n_{\\textrm{sims}}}\\sum_{i=1}^{n_{\\textrm{sims}}}\\frac{\\partial{\\bf x}_i}{\\partial{\\bf d}_j}\\frac{\\partial{\\bf d}_j}{\\partial\\theta_\\alpha}\\delta_{ij}.$$\nWe then use ${\\bf C}_\\mathscr{f}$ and $\\boldsymbol{\\mu}_\\mathscr{f},_\\alpha$ to calculate the Fisher information\n$${\\bf F}_{\\alpha\\beta} = \\boldsymbol{\\mu}_\\mathscr{f},^T_{\\alpha}{\\bf C}^{-1}_\\mathscr{f}\\boldsymbol{\\mu}_\\mathscr{f},_{\\beta}.$$\nWe want to maximise the Fisher information, and we want the summaries to be orthogonal so to train the network we minimise the loss function\n$$\\Lambda = -\\ln|{\\bf F}_{\\alpha\\beta}|+r(\\Lambda_2)\\Lambda_2$$\nwhere\n$$\\Lambda_2 = ||{\\bf C}_\\mathscr{f}-\\mathbb{1}||_2 + ||{\\bf C}_\\mathscr{f}^{-1}-\\mathbb{1}||_2$$\nis a regularisation term whose strength is dictated by\n$$r(\\Lambda_2) = \\frac{\\lambda\\Lambda_2}{\\Lambda_2 + \\exp(-\\alpha\\Lambda_2)}$$\nwhere $\\lambda$ is a strength and $\\alpha$ is a rate parameter which can be determined from a closeness condition on the Frobenius norm of the difference between the convariance (and inverse covariance) from the identity matrix.\n\nWhen using this code please cite <a href=\"https://arxiv.org/abs/1802.03537\">arXiv:1802.03537</a>.<br><br>\nThe code in the paper can be downloaded as v1 or v1.1 of the code kept on zenodo:<br><br>\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1175196.svg)](https://doi.org/10.5281/zenodo.1175196)\n<br>\n\nThis code is run using<br>\n>`python v3.7.4`\n\n>`tensorflow==2.0.0`\n\n>`numpy==1.16.2`\n\n>`tqdm==4.31.1`\n\nAlthough these precise versions may not be necessary, I have put them here to avoid possible conflicts.\n\n## Load modules\n\n\n```python\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom IMNN import IMNN\nfrom IMNN.ABC import ABC, priors\n```\n\n# Summarising the mean and the variance\n\nFor this example we are going to use $n_{\\bf d}=10$ data points of a 1D field of Gaussian noise with unknown mean and variance to see if the network can learn to summarise them.<br><br>\n\nThe likelihood is given by\n$$\\mathcal{L} = \\prod_i^{n_{\\bf d}}\\frac{1}{\\sqrt{2\\pi|\\Sigma|}}\\exp\\left[-\\frac{1}{2}\\frac{(d_i-\\mu)^2}{\\Sigma}\\right]$$\n\nWe can solve this problem analytically, so it is useful to check how well the network does. There is a single sufficient statistic which describes each the mean and the variance, which can be found by finding the maximum of the probability. We find that\n$$\\sum_i^{n_{\\bf d}}d_i = \\mu\\textrm{ and }\\sum_i^{n_{\\bf d}}(d_i-\\mu)^2=n_{\\bf d}\\Sigma$$\n\nWe can calculate the Fisher information by taking the negative of second derivative of the likelihood taking the expectation by inserting the above relations at examining at some fiducial parameter values\n$${\\bf F}_{\\alpha\\beta} = -\\left.\\left(\\begin{array}{cc}\\displaystyle-\\frac{n_{\\bf d}}{\\Sigma}&0\\\\0&\\displaystyle-\\frac{n_{\\bf d}}{2\\Sigma^2}\\end{array}\\right)\\right|_{\\textrm{fiducial}}.$$\nIf we choose a fiducial mean of $\\mu^{\\textrm{fid}}=0$ and variance of $\\Sigma^{\\textrm{fid}} = 1$ then we obtain a Fisher information matrix of\n\n\n\n\n\n```python\ninput_shape = (10,)\n\nn_params = 2\n\u03b8_fid = np.array([0, 1.])\n```\n\n\n```python\nexact_fisher = -np.array([[-input_shape[0] / \u03b8_fid[1], 0.], [0. , - 0.5 * input_shape[0] / \u03b8_fid[1]**2.]])\ndeterminant_exact_fisher = np.linalg.det(exact_fisher)\nprint(\"determinant of the Fisher information\", determinant_exact_fisher)\nplt.imshow(np.linalg.inv(exact_fisher))\nplt.title(\"Inverse Fisher matrix\")\nplt.xticks([0, 1], [r\"$\\mu$\", r\"$\\Sigma$\"])\nplt.yticks([0, 1], [r\"$\\mu$\", r\"$\\Sigma$\"])\nplt.colorbar();\n```\n\n    determinant of the Fisher information 50.000000000000014\n\n\n\n![png](figures/output_9_1.png)\n\n\nLet us observe our _real_ data which happens to have true parameters $\\mu=3$ and $\\Sigma=2$\n\n\n```python\nreal_data = np.random.normal(3., np.sqrt(2.), size = (1,) + input_shape)\n```\n\n\n```python\nfig, ax = plt.subplots(1, 1, figsize = (10, 6))\nax.plot(real_data[0], label = \"observed data\")\nax.legend(frameon = False)\nax.set_xlim([0, 9])\nax.set_xticks([])\nax.set_ylabel(\"Data amplitude\");\n```\n\n\n![png](figures/output_12_0.png)\n\n\nThe posterior distribution for this data (normalised to integrate to 1) is\n\n\n```python\n\u03bc_array = np.linspace(-10, 10, 1000)\n\u03a3_array = np.linspace(0.001, 10, 1000)\n\nparameter_grid = np.array(np.meshgrid(\u03bc_array, \u03a3_array, indexing = \"ij\"))\ndx = (\u03bc_array[1] - \u03bc_array[0]) * (\u03a3_array[1] - \u03a3_array[0])\n\nanalytic_posterior = np.exp(-0.5 * (np.sum((real_data[0][:, np.newaxis] - parameter_grid[0, :, 0][np.newaxis, :])**2., axis = 0)[:, np.newaxis] / parameter_grid[1, 0, :][np.newaxis, :] + real_data.shape[1] * np.log(2. * np.pi * parameter_grid[1, 0, :][np.newaxis, :])))\nanalytic_posterior = analytic_posterior.T / np.sum(analytic_posterior * dx)\n```\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize = (16, 10))\nplt.subplots_adjust(wspace = 0, hspace = 0)\nax[0, 0].plot(parameter_grid[0, :, 0], np.sum(analytic_posterior, axis = 0), linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[0, 0].legend(frameon = False)\nax[0, 0].set_xlim([-10, 10])\nax[0, 0].set_ylabel('$\\\\mathcal{P}(\\\\mu|{\\\\bf d})$')\nax[0, 0].set_yticks([])\nax[0, 0].set_xticks([])\nax[1, 0].set_xlabel('$\\mu$');\nax[1, 0].set_ylim([0, 10])\nax[1, 0].set_ylabel('$\\Sigma$')\nax[1, 0].set_xlim([-10, 10])\nax[1, 0].contour(parameter_grid[0, :, 0], parameter_grid[1, 0, :], analytic_posterior, colors = \"C2\")\nax[1, 1].plot(np.sum(analytic_posterior, axis = 1), parameter_grid[1, 0, :], linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[1, 1].legend(frameon = False)\nax[1, 1].set_ylim([0, 10])\nax[1, 1].set_xlabel('$\\\\mathcal{P}(\\\\Sigma|{\\\\bf d})$')\nax[1, 1].set_xticks([])\nax[1, 1].set_yticks([])\nax[0, 1].axis(\"off\");\n```\n\n\n![png](figures/output_15_0.png)\n\n\nNow lets see how the information maximising neural network can recover this posterior.\n\n## Generate data\n\nWe start by defining a function to generate the data with the correct shape. The shape must be\n```\ndata_shape = (None,) + input_shape\n```\n\nIt is useful to define the generating function so that it only takes in the value of the parameter as its input since the function can then be used for ABC later.<br><br>\nThe data needs to be generated at a fiducial parameter value and at perturbed values just below and above the fiducial parameter for the numerical derivative.\n\n\n```python\n\u03b4\u03b8 = np.array([0.1, 0.1])\n```\n\nThe training and validation data should be generated at the perturbed values with shape\n```\nperturbed_data_shape = (None, 2, n_params) + input_shape\n```\nif using numerical derivatives, where `[:, 0, ...]` is the parameter at `\u0394\u03b8pm` below `\u03b8_fid` and `[:, 1, ...]` is the parameter at `\u0394\u03b8pm` above `\u03b8_fid`. This is done for each parameter keeping ever non-perturbed parameter at its fiducial parameter.\n\nIf the true derivative of the simulations with respect to the parameters is available then this can be calculated for each fiducial simulation in the dataset with shape\n```\nderivative_data_shape = (None, n_params) + input_shape\n```\n\nThe generating function is defined so that the fiducial parameter is passed as a list so that many simulations can be made at once. This is very useful for the ABC function later.\n\n\n```python\ndef simulator(\u03b8, seed, simulator_args):\n    if seed is not None:\n        np.random.seed(seed)\n    if len(\u03b8.shape) > 1:\n        \u03bc = \u03b8[:, 0]\n        \u03a3 = \u03b8[:, 1]\n    else:\n        \u03bc = 0.\n        \u03a3 = \u03b8\n    return np.moveaxis(np.random.normal(\u03bc, np.sqrt(\u03a3), simulator_args[\"input shape\"] + (\u03b8.shape[0],)), -1, 0)\n```\n\n### Training data\nEnough data needs to be made to approximate the covariance matrix of the output summaries. The number of simulations needed to approximate the covariance is `n_s`. We can make stochastic updates by making `n_train` more simulations than are needed to approximate the covariance. `n_train` should be an integer number.\n\n\n```python\nn_s = 1000\nn_train = 1\nseed = np.random.randint(1e6)\n```\n\nThe training data can now be made\n\n\n```python\nd = simulator(\u03b8 = np.tile(\u03b8_fid, [n_train * n_s, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})\n```\n\nIdeally we would be able to take the derivative of our simulations with respect to the model parameters. We can indeed do that in this case, but since this is possibly a rare occurance I will show an example where the derivatives are calculated numerically. By suppressing the sample variance between the simulations created at some lower and upper varied parameter values, far fewer simulations are needed. We should choose the extra `n_train` simulations for the stochastic updates to be the same as for the fiducial simulations.\n\n\n```python\nn_d = 1000\n```\n\nThe sample variance is supressed by choosing the same initial seed when creating the upper and lower simulations.\n\n\n```python\ndd_m_d\u03b8 = simulator(\u03b8 = np.tile(\u03b8_fid - np.array([0.1, 0.]), [n_train * n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})\ndd_p_d\u03b8 = simulator(\u03b8 = np.tile(\u03b8_fid + np.array([0.1, 0.]), [n_train * n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})\ndd_m_d\u03b8 = np.stack([dd_m_d\u03b8, simulator(\u03b8 = np.tile(\u03b8_fid - np.array([0., 0.1]), [n_train * n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})], axis = 1)\ndd_p_d\u03b8 = np.stack([dd_p_d\u03b8, simulator(\u03b8 = np.tile(\u03b8_fid + np.array([0., 0.1]), [n_train * n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})], axis = 1)\ndd_d\u03b8_num = np.concatenate([dd_m_d\u03b8[:, np.newaxis, ...], dd_p_d\u03b8[:, np.newaxis, ...]], axis=1)\ndd_d\u03b8 = (dd_p_d\u03b8 - dd_m_d\u03b8) / (2. * \u03b4\u03b8)[np.newaxis, :, np.newaxis]\n```\n\n### Test data\nWe should also make some test data, but here we will use only one combination. This needs adding to the dictionary\n\n\n```python\nseed = np.random.randint(1e6)\ntd = simulator(\u03b8 = np.tile(\u03b8_fid, [n_s, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})\ntdd_m_d\u03b8 = simulator(\u03b8 = np.tile(\u03b8_fid - np.array([0.1, 0.]), [n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})\ntdd_p_d\u03b8 = simulator(\u03b8 = np.tile(\u03b8_fid + np.array([0.1, 0.]), [n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})\ntdd_m_d\u03b8 = np.stack([tdd_m_d\u03b8, simulator(\u03b8 = np.tile(\u03b8_fid - np.array([0., 0.1]), [n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})], axis = 1)\ntdd_p_d\u03b8 = np.stack([tdd_p_d\u03b8, simulator(\u03b8 = np.tile(\u03b8_fid + np.array([0., 0.1]), [n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})], axis = 1)\ntdd_d\u03b8_num = np.concatenate([tdd_m_d\u03b8[:, np.newaxis, ...], tdd_p_d\u03b8[:, np.newaxis, ...]], axis=1)\ntdd_d\u03b8 = (tdd_p_d\u03b8 - tdd_m_d\u03b8) / (2. * \u03b4\u03b8)[np.newaxis, :, np.newaxis]\n```\n\n### Data visualisation\nWe can plot the data to see what it looks like.\n\n\n```python\nfig, ax = plt.subplots(1, 1, figsize = (10, 6))\nax.plot(d[np.random.randint(n_train * n_s)], label = \"training data\")\nax.plot(td[np.random.randint(n_s)], label = \"test data\")\nax.legend(frameon = False)\nax.set_xlim([0, 9])\nax.set_xticks([])\nax.set_ylabel(\"Data amplitude\");\n```\n\n\n![png](figures/output_33_0.png)\n\n\nIt is also very useful to plot the upper and lower derivatives to check that the sample variance is actually supressed since the network learns extremely slowly if this isn't done properly.\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize = (20, 10))\nplt.subplots_adjust(hspace = 0)\ntraining_index = np.random.randint(n_train * n_d)\ntest_index = np.random.randint(n_d)\n\nax[0, 0].plot(dd_m_d\u03b8[training_index, 0], label = \"lower training data\", color = \"C0\", linestyle = \"dashed\")\nax[0, 0].plot(dd_p_d\u03b8[training_index, 0], label = \"upper training data\", color = \"C0\")\nax[0, 0].plot(tdd_m_d\u03b8[test_index, 0], label = \"lower validation data\", color = \"C1\", linestyle = \"dashed\")\nax[0, 0].plot(tdd_p_d\u03b8[test_index, 0], label = \"upper validation data\", color = \"C1\")\nax[0, 0].legend(frameon = False)\nax[0, 0].set_xlim([0, 9])\nax[0, 0].set_xticks([])\nax[0, 0].set_ylabel(\"Data amplitude with varied mean\")\nax[1, 0].plot(dd_d\u03b8[training_index, 0], label = \"derivative training data\", color = \"C0\")\nax[1, 0].plot(tdd_d\u03b8[test_index, 0], label = \"derivative validation data\", color = \"C1\")\nax[1, 0].set_xlim([0, 9])\nax[1, 0].set_xticks([])\nax[1, 0].legend(frameon = False)\nax[1, 0].set_ylabel(\"Amplitude of the derivative of the data\\nwith respect to the mean\");\n\nax[0, 1].plot(dd_m_d\u03b8[training_index, 1], label = \"lower training data\", color = \"C0\", linestyle = \"dashed\")\nax[0, 1].plot(dd_p_d\u03b8[training_index, 1], label = \"upper training data\", color = \"C0\")\nax[0, 1].plot(tdd_m_d\u03b8[test_index, 1], label = \"lower validation data\", color = \"C1\", linestyle = \"dashed\")\nax[0, 1].plot(tdd_p_d\u03b8[test_index, 1], label = \"upper validation data\", color = \"C1\")\nax[0, 1].legend(frameon = False)\nax[0, 1].set_xlim([0, 9])\nax[0, 1].set_xticks([])\nax[0, 1].set_ylabel(\"Data amplitude with varied covariance\")\nax[1, 1].plot(dd_d\u03b8[training_index, 1], label = \"derivative training data\", color = \"C0\")\nax[1, 1].plot(tdd_d\u03b8[test_index, 1], label = \"derivative validation data\", color = \"C1\")\nax[1, 1].set_xlim([0, 9])\nax[1, 1].set_xticks([])\nax[1, 1].legend(frameon = False)\nax[1, 1].set_ylabel(\"Amplitude of the derivative of the data\\nwith respect to covariance\");\n```\n\n\n![png](figures/output_35_0.png)\n\n\n## Initiliase the neural network\n\nWe will choose a fairly generic network as an example here. These networks can be written in TensorFlow or keras. It must be able to take in `(None,) + input_shape` and `(None, n_params) + input_shape` or `(None, 2, n_params) + input_shape` and output a flat array with `n_summaries` outputs, i.e. `(None, n_summaries` and `(None, n_params, n_summaries)` or `(None, 2, n_params, n_summaries)`.\n\nIn principle `n_summaries` can be any number, but information loss is guaranteed if `n_summaries < n_params` and we overparameterise the summaries if we use `n_summaries>n_params`. Therefore, in principle, we should use `n_summaries=n_params`. Note that this isn't necessarily true if external informative summaries are included in the training.\n\n\n```python\nn_summaries = n_params\n```\n\nIn keras we shall define a network with two hidden layers with 128 hidden nodes in each and every layer apart from the output activated using `tanh`. We shall optimise the network using the `Adam` optimiser on its default settings\n\n\n```python\nmodel = tf.keras.Sequential(\n    [tf.keras.Input(shape=input_shape),\n     tf.keras.layers.Dense(128),\n     tf.keras.layers.Activation(\"tanh\"),\n     tf.keras.layers.Dense(128),\n     tf.keras.layers.Activation(\"tanh\"),\n     tf.keras.layers.Dense(n_summaries),\n    ])\nopt = tf.keras.optimizers.Adam()\n```\n\n## Initialise the IMNN module\n\nThe IMNN only needs to be provided with the number of parameters in the model, the number of summaries output by the network and the number of simulations needed to approximate the covariance of the summaries and to approximate the mean of the derivative of the summaries with respect to the parameters. Optionally we can also choose whether to use 32 or 64 bit floats (and ints) in TensorFlow and choose whether or not to get verbose size checking whilst loading the model and the data.\n\n\n```python\nimnn = IMNN.IMNN(n_params=n_params, n_summaries=n_summaries, n_covariance_sims=n_s, n_derivative_sims=n_d, dtype=tf.float32, verbose=True)\n```\n\n    Using single dataset\n\n\nThe network is then passed to the module using\n\n\n```python\nimnn.set_model(model=model, optimiser=opt)\n```\n\n    Checking is not currently done on the model. Make sure that its output has shape (None, 2) for the fiducial values a nd (None, 2, 2, 2) for the derivative values.\n\n\n## Load the data\n\nThe data is passed as a TensorFlow data set for extremely quick and efficient transfer to GPUs, etc. For most reasonable sized datasets (GPU/CPU memory dependent of course) we can load this using\n\n\n```python\nimnn.setup_dataset(\u03b8_fid=\u03b8_fid, d=d, dd_d\u03b8=dd_d\u03b8_num, \u03b4\u03b8=\u03b4\u03b8, external_summaries=None, external_derivatives=None)\n```\n\nSince we might already know some very informative summaries of the data, we can include these. To do so the external summaries can be passed to the above function as numpy arrays. The summaries must have the same structure as the data (and aligned with the data) where `input_shape` is replaced with a flat array with `n_external` elements.\n\nThe dataset can be setup externally by setting the data attributes\n```python\nimnn.dataset = tf.data.Dataset.from_tensor_slices(data)\nimnn.dataset = imnn.dataset.batch(imnn.n_s)\nimnn.dataset = imnn.dataset.shuffle(n_train * imnn.n_s)\n```\nwhere `data=(fiducial_data, derivative_data)` is a tuple containing the fiducial and the derivative simulations. If the number of parameters for the mean of the derivatives is different to the number needed for the covariance then `data` should not be a tuple and instead just contain the fiducial simulations and instead the derivatives can be passed as\n```python\nimnn.derivative_dataset = tf.data.Dataset.from_tensor_slices(derivative_data)\nimnn.dataset = imnn.dataset.batch(imnn.n_d)\nimnn.dataset = imnn.dataset.shuffle(n_train * imnn.n_d)\n```\nIf the derivative is done numerically then `imnn.numerical=True` must be set, and likewise `imnn.numerical=False` must be used otherwise. The fiducial parameters should be set using\n```python\nimnn.\u03b8_fid = tf.Variable(\u03b8_fid, dtype=imnn.dtype)\n```\nand if using the numerical derivative we also need to set\n```python\nimnn.\u03b4\u03b8 = tf.Variable(1. / (2. * \u03b4\u03b8), dtype=imnn.dtype)\n```\nIf external informative summaries are going to be used then the when the number of simulations for the derivative of the mean of the summaries and the number of simulations for the covariance of the summaries is the same the data tuple should be `data=(fiducial_data, derivative_data, external_summaries, external_derivatives)` and when then number is different the tuples should be different `data=(fiducial_data, external_summaries`) and `derivatives=(derivative_data, external_derivatives)`.\n\nAlthough there is little point in doing the above externally, this could be extremely useful for large datasets which are written as TFRecords.\n\nThe validation data can be passed using\n\n\n```python\nimnn.setup_dataset(\u03b8_fid=\u03b8_fid, d=td, dd_d\u03b8=tdd_d\u03b8_num, \u03b4\u03b8=\u03b4\u03b8, external_summaries=None, external_derivatives=None, train=False)\n```\n\nThe same external setup can be be performed exactly as above but with `test_` appended before each attribute name.\n\n## Train the network\n\nSince we need to constrain the scale of the summaries (remember, every linear rescaling of a summary is also a summary) we choose to constrain the summaries to have a covariance close to the identity matrix. This makes the summaries somewhat (and ideally exactly) orthogonal. To enforce this scale we have a regulariser which is the Frobenius norm of the difference between the covariance matrix and the identity matrix and also the difference betwee the inverse covariance matrix and the identity matrix. How close the covariance gets to the identity is controlled by a strength `\u03bb` parameter and a closeness `\u03f5` parameter. These determine a rate of convergence \u03b1 which determines how sharp the loss function is (if it's too sharp then training can be unstable). We choose an `\u03f5=0.01` and a strength of `\u03bb=10`\n\n\n```python\nimnn.set_regularisation_strength(\u03f5=0.01, \u03bb=10.)\n```\n\nWe can now train the network. We just need to choose a number iterations of training updates (equivalent to full passes through the dataset). To validate we just pass the `validate=True` option.\n\n\n```python\nimnn.fit(n_iterations=1000, validate=True)\n```\n\n\n    HBox(children=(IntProgress(value=0, description='Iterations', max=1000, style=ProgressStyle(description_width=\u2026\n\n\nTraining can be continued simply by running the fit again.\n\nThe network can also be reinitialised before training if something goes awry by running\n```python\nn.fit(n_iterations=1000, validate=True, reset=True)\n```\n\nNote that it is normal for the network to initially run slow and quite quickly speed up as the data flow from the dataset to the CPU is properly filled.\n\nA history (imnn.history) dictionary is collecting diagnostics. It contains\n- `det_F` : determinant of the Fisher information\n- `det_C` : determinant of the covariance of the summaries\n- `det_Cinv` : determinant of the inverse covariance of the summaries\n- `det_d\u03bc_d\u03b8` : derivative of the mean of the summaries with respect to the parameters\n- `reg` : value of the regulariser\n- `r` : value of the regulariser strength\n\nand the same for the validation set\n\n- `val_det_F` : determinant of the Fisher information for the validation set\n- `val_det_C` : determinant of the covariance of the summaries from the validation set\n- `val_det_Cinv` : determinant of the inverse covariance of the summaries from the validation set\n- `val_det_d\u03bc_d\u03b8` : derivative of the mean of the summaries from the validation set with respect to the parameters\n- `val_reg` : value of the regulariser for the validation set\n- `val_r` : value of the regulariser strength for the validation set\n\n\n```python\nfig, ax = plt.subplots(4, 1, sharex = True, figsize = (10, 20))\nplt.subplots_adjust(hspace = 0)\nepochs = np.arange(1, len(imnn.history[\"det_F\"]) + 1)\nax[0].plot(epochs, imnn.history[\"det_F\"], color=\"C0\",\n           label=r'$|{\\bf F}_{\\alpha\\beta}|$ from training data')\nax[0].plot(epochs, imnn.history[\"val_det_F\"], color=\"C1\",\n           label=r'$|{\\bf F}_{\\alpha\\beta}|$ from validation data')\nax[0].axhline(determinant_exact_fisher, color=\"black\", linestyle=\"dashed\")\nax[0].legend(frameon=False)\nax[0].set_xlim([1, epochs[-1]])\nax[0].set_ylabel(r\"$|{\\bf F}_{\\alpha\\beta}|$\")\nax[1].plot(epochs, imnn.history[\"det_C\"], color=\"C0\",\n           label=r'$|{\\bf C}|$ from training data')\nax[1].plot(epochs, imnn.history[\"val_det_C\"], color=\"C1\",\n           label=r'$|{\\bf C}|$ from validation data')\nax[1].plot(epochs, imnn.history[\"det_Cinv\"], color=\"C0\", linestyle=\"dashed\",\n           label=r'$|{\\bf C}^{-1}|$ from training data')\nax[1].plot(epochs, imnn.history[\"val_det_Cinv\"], color=\"C1\", linestyle=\"dashed\",\n           label=r'$|{\\bf C}^{-1}|$ from validation data')\nax[1].axhline(1., color=\"black\", linestyle=\"dashed\")\nax[1].legend(frameon=False, loc=\"best\")\nax[1].set_ylabel(r\"$|{\\bf C}|$ and $|{\\bf C}^{-1}|$\")\nax[1].set_xlim([1, epochs[-1]])\nax[1].set_yscale(\"log\")\nax[2].plot(epochs, imnn.history[\"reg\"],\n           label=r'$\\Lambda_2$ from training data')\nax[2].plot(epochs, imnn.history[\"val_reg\"],\n           label=r'$\\Lambda_2$ from validation data')\nax[2].legend(frameon=False)\nax[2].set_ylabel(r\"$\\Lambda_2$\")\nax[2].set_xlim([1, epochs[-1]])\nax[3].plot(epochs, imnn.history[\"r\"],\n           label=r'$r$ from training data')\nax[3].plot(epochs, imnn.history[\"val_r\"],\n           label=r'$r$ from validation data')\nax[3].legend(frameon=False)\nax[3].set_ylabel(r\"$r$\")\nax[3].set_xlim([1, epochs[-1]])\nax[3].set_xlabel(\"Number of epochs\");\n```\n\n\n![png](figures/output_58_0.png)\n\n\n## Maximum likelihood estimates\n\nThe IMNN can provide maximum likelihood estimates of model parameters by initialising\n\n\n```python\nimnn.setup_MLE()\n```\n\nNote this only works when a validation set is already loaded. It uses the validation set to calculate the Fisher information, covariance, derivative of the mean summaries and therefore the compression and transformation into MLE space. If a different dataset is to be used then it can be provided by setting `dataset=False` and providing all the necessary extra data\n```python\nimnn.setup_MLE(dataset=True, \u03b8_fid=None, d=None, dd_d\u03b8=None, \u03b4\u03b8=None, s=None, ds_d\u03b8=None)\n```\n\nThe maxmimum likelihood estimate is then obtained by running `imnn.get_MLE(d)` on data `d`\n\n\n```python\nprint(\"The maximum likelihood estimate of the real data is \" + str(imnn.get_MLE(real_data)[0].numpy()))\n```\n\n    The maximum likelihood estimate of the real data is [1.8213251 4.3077555]\n\n\n## Approximate Bayesian computation\n\nWe can now do ABC (or PMC-ABC) with our calculated summary. From the samples we create simulations at each parameter value and feed each simulation through the network to get summaries. The summaries are compared to the summary of the real data to find the distances which can be used to accept or reject points.\n\nWe start by defining our prior as a truncated Gaussian (uniform is also available). The uniform function is taken from delfi by Justin Alsing. At some point in the near future (for a given value of \"near\") this whole module will upgraded to TensorFlow Probability and complete TensorFlow 2 implementation.\n\nWe are going to choose the mean value of the variance to be 1 with a variance of the variance of 10 cut at 0 and 10.\n\n\n```python\nprior = priors.TruncatedGaussian(np.array([0., 1.]), np.array([[10., 0.], [0., 10.]]), np.array([-10., 0.]), np.array([10., 10.]))\n```\n\nThe ABC module takes in the _observed_ data, the prior and the function for obtaining the MLE from the IMNN as well as the Fisher information matrix used for obtaining the MLE. It also takes in the simulator and its arguments.\n\n\n```python\nabc = ABC(real_data=real_data, prior=prior, F=imnn.MLE_F, get_MLE=imnn.get_MLE, simulator=simulator, seed=None, simulator_args={\"input shape\": input_shape})\n```\n\n## Gaussian approximation\nBefore running all the simulations need for approximate Bayesian computation, we can get the Gaussian approximation of the posterior from the MLE and the inverse Fisher information.\n\n\n```python\nprint(\"maximum likelihood estimate\", abc.MLE[0])\nprint(\"determinant of the Fisher information\", np.linalg.det(abc.F))\nplt.imshow(np.linalg.inv(abc.Finv))\nplt.title(\"Inverse Fisher matrix\")\nplt.xticks([0, 1], [r\"$\\mu$\", r\"$\\Sigma$\"])\nplt.yticks([0, 1], [r\"$\\mu$\", r\"$\\Sigma$\"])\nplt.colorbar();\n```\n\n    maximum likelihood estimate [1.8213251 4.3077555]\n    determinant of the Fisher information 67.50628\n\n\n\n![png](figures/output_68_1.png)\n\n\n\n```python\ngaussian_approximation, grid = abc.gaussian_approximation(gridsize = 100)\n```\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize = (16, 10))\nplt.subplots_adjust(wspace = 0, hspace = 0)\nax[0, 0].plot(parameter_grid[0, :, 0], np.sum(analytic_posterior * (parameter_grid[0, 1, 0] - parameter_grid[0, 0, 0]), axis = 0), linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[0, 0].plot(grid[0, :, 0], np.sum(gaussian_approximation * (grid[0, 1, 0] - grid[0, 0, 0]), axis = 0), color = \"C1\", label = \"Gaussian approximation\")\nax[0, 0].axvline(abc.MLE[0, 0], linestyle = \"dashed\", color = \"black\", label = \"Maximum likelihood estimate of mean\")\nax[0, 0].legend(frameon = False)\nax[0, 0].set_xlim([-10, 10])\nax[0, 0].set_ylabel('$\\\\mathcal{P}(\\\\mu|{\\\\bf d})$')\nax[0, 0].set_yticks([])\nax[0, 0].set_xticks([])\nax[1, 0].set_xlabel('$\\mu$');\nax[1, 0].set_ylim([0, 10])\nax[1, 0].set_ylabel('$\\Sigma$')\nax[1, 0].set_xlim([-10, 10])\nax[1, 0].contour(parameter_grid[0, :, 0], parameter_grid[1, 0, :], analytic_posterior, colors = \"C2\")\nax[1, 0].contour(grid[0, :, 0], grid[1, 0, :], gaussian_approximation, colors = \"C1\")\nax[1, 0].axvline(abc.MLE[0, 0], linestyle = \"dashed\", color = \"black\", label = \"Maximum likelihood estimate of mean\")\nax[1, 0].axhline(abc.MLE[0, 1], linestyle = \"dotted\", color = \"black\", label = \"Maximum likelihood estimate of covariance\")\nax[1, 1].plot(np.sum(analytic_posterior * (parameter_grid[1, 0, 1] - parameter_grid[1, 0, 0]), axis = 1), parameter_grid[1, 0, :], linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[1, 1].plot(np.sum(gaussian_approximation * (grid[1, 0, 1] - grid[1, 0, 0]), axis = 1), grid[1, 0, :], color = \"C1\", label = \"Gaussian approximation\")\nax[1, 1].axhline(abc.MLE[0, 1], linestyle = \"dotted\", color = \"black\", label = \"Maximum likelihood estimate of covariance\")\nax[1, 1].legend(frameon = False)\nax[1, 1].set_ylim([0, 10])\nax[1, 1].set_xlabel('$\\\\mathcal{P}(\\\\Sigma|{\\\\bf d})$')\nax[1, 1].set_xticks([])\nax[1, 1].set_yticks([])\nax[0, 1].axis(\"off\");\n```\n\n\n![png](figures/output_70_0.png)\n\n\nWe can see that the maximum likelihood estimate for the mean is almost perfect whilst it is incorrect for the variance. However, we can now see the ABC does in its place.\n\n### ABC\nThe most simple ABC takes the number of draws and a switch to state whether to run all the simulations in parallel or sequentially. The full simulations can also be saved by passing a file name. The draws are stored in the class attribute `ABC_dict`.\n\n\n```python\nabc.ABC(draws=100000, at_once=True, save_sims=None)\n```\n\nIn ABC, draws are accepted if the distance between the simulation summary and the simulation of the real data are \"close\", i.e. smaller than some \u03f5 value, which is chosen somewhat arbitrarily.\n\n\n```python\n\u03f5 = .5\naccept_indices = np.argwhere(abc.ABC_dict[\"distances\"] < \u03f5)[:, 0]\nreject_indices = np.argwhere(abc.ABC_dict[\"distances\"] >= \u03f5)[:, 0]\nprint(\"Number of accepted samples = \", accept_indices.shape[0])\n```\n\n    Number of accepted samples =  2319\n\n\n### Plot samples\nWe can plot the output samples and the histogram of the accepted samples, which should peak around `\u03b8 = 1` (where we generated the real data). The monotonic function of all the output samples shows that the network has learned how to summarise the data.\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize = (14, 10))\nplt.subplots_adjust(hspace = 0, wspace = 0.2)\nax[0, 0].scatter(abc.ABC_dict[\"parameters\"][reject_indices, 0], abc.ABC_dict[\"MLE\"][reject_indices, 0], s = 1, alpha = 0.1, label = \"Rejected samples\", color = \"C3\")\nax[0, 0].scatter(abc.ABC_dict[\"parameters\"][accept_indices, 0] , abc.ABC_dict[\"MLE\"][accept_indices, 0], s = 1, label = \"Accepted samples\", color = \"C6\", alpha = 0.5)\nax[0, 0].axhline(abc.MLE[0, 0], color = 'black', linestyle = 'dashed', label = \"$\\hat{\\mu}_{obs}$\")\nax[0, 0].legend(frameon=False)\nax[0, 0].set_ylabel('$\\hat{\\mu}$', labelpad = 0)\nax[0, 0].set_xlim([-10, 10])\nax[0, 0].set_xticks([])\nax[1, 0].scatter(abc.ABC_dict[\"parameters\"][reject_indices, 0], abc.ABC_dict[\"MLE\"][reject_indices, 1], s = 1, alpha = 0.1, label = \"Rejected samples\", color = \"C3\")\nax[1, 0].scatter(abc.ABC_dict[\"parameters\"][accept_indices, 0] , abc.ABC_dict[\"MLE\"][accept_indices, 1], s = 1, label = \"Accepted samples\", color = \"C6\", alpha = 0.5)\nax[1, 0].axhline(abc.MLE[0, 1], color = 'black', linestyle = 'dashed', label = \"$\\hat{\\Sigma}_{obs}$\")\nax[1, 0].legend(frameon=False)\nax[1, 0].set_ylabel('$\\hat{\\Sigma}$', labelpad = 0)\nax[1, 0].set_xlim([-10, 10])\nax[1, 0].set_xlabel(\"$\\mu$\")\nax[0, 1].scatter(abc.ABC_dict[\"parameters\"][reject_indices, 1], abc.ABC_dict[\"MLE\"][reject_indices, 0], s = 1, alpha = 0.1, label = \"Rejected samples\", color = \"C3\")\nax[0, 1].scatter(abc.ABC_dict[\"parameters\"][accept_indices, 1] , abc.ABC_dict[\"MLE\"][accept_indices, 0], s = 1, label = \"Accepted samples\", color = \"C6\", alpha = 0.5)\nax[0, 1].axhline(abc.MLE[0, 0], color = 'black', linestyle = 'dashed', label = \"$\\hat{\\mu}_{obs}$\")\nax[0, 1].legend(frameon=False)\nax[0, 1].set_ylabel('$\\hat{\\mu}$', labelpad = 0)\nax[0, 1].set_xlim([0, 10])\nax[0, 1].set_xticks([])\nax[1, 1].scatter(abc.ABC_dict[\"parameters\"][reject_indices, 1], abc.ABC_dict[\"MLE\"][reject_indices, 1], s = 1, alpha = 0.1, label = \"Rejected samples\", color = \"C3\")\nax[1, 1].scatter(abc.ABC_dict[\"parameters\"][accept_indices, 1] , abc.ABC_dict[\"MLE\"][accept_indices, 1], s = 1, label = \"Accepted samples\", color = \"C6\", alpha = 0.5)\nax[1, 1].axhline(abc.MLE[0, 1], color = 'black', linestyle = 'dashed', label = \"$\\hat{\\Sigma}_{obs}$\")\nax[1, 1].legend(frameon=False)\nax[1, 1].set_ylabel('$\\hat{\\Sigma}$', labelpad = 0)\nax[1, 1].set_xlim([0, 10])\nax[1, 1].set_xlabel(\"$\\Sigma$\");\n```\n\n\n![png](figures/output_77_0.png)\n\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize = (16, 10))\nplt.subplots_adjust(wspace = 0, hspace = 0)\nax[0, 0].plot(parameter_grid[0, :, 0], np.sum(analytic_posterior * (parameter_grid[0, 1, 0] - parameter_grid[0, 0, 0]), axis = 0), linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[0, 0].plot(grid[0, :, 0], np.sum(gaussian_approximation * (grid[0, 1, 0] - grid[0, 0, 0]), axis = 0), color = \"C1\", label = \"Gaussian approximation\")\nax[0, 0].hist(abc.ABC_dict[\"parameters\"][accept_indices, 0], np.linspace(-10, 10, 100), histtype = u'step', density = True, linewidth = 1.5, color = \"C6\", label = \"ABC posterior\");\nax[0, 0].axvline(abc.MLE[0, 0], linestyle = \"dashed\", color = \"black\", label = \"Maximum likelihood estimate of mean\")\nax[0, 0].legend(frameon = False)\nax[0, 0].set_xlim([-10, 10])\nax[0, 0].set_ylabel('$\\\\mathcal{P}(\\\\mu|{\\\\bf d})$')\nax[0, 0].set_yticks([])\nax[0, 0].set_xticks([])\nax[1, 0].set_xlabel('$\\mu$');\nax[1, 0].set_ylim([0, 10])\nax[1, 0].set_ylabel('$\\Sigma$')\nax[1, 0].set_xlim([-10, 10])\nax[1, 0].scatter(abc.ABC_dict[\"parameters\"][accept_indices, 0], abc.ABC_dict[\"parameters\"][accept_indices, 1], color = \"C6\", s = 1, alpha = 0.5)\nax[1, 0].scatter(abc.ABC_dict[\"parameters\"][reject_indices, 0], abc.ABC_dict[\"parameters\"][reject_indices, 1], color = \"C3\", s = 1, alpha = 0.01)\nax[1, 0].contour(parameter_grid[0, :, 0], parameter_grid[1, 0, :], analytic_posterior, colors = \"C2\")\nax[1, 0].contour(grid[0, :, 0], grid[1, 0, :], gaussian_approximation, colors = \"C1\")\nax[1, 0].axvline(abc.MLE[0, 0], linestyle = \"dashed\", color = \"black\", label = \"Maximum likelihood estimate of mean\")\nax[1, 0].axhline(abc.MLE[0, 1], linestyle = \"dotted\", color = \"black\", label = \"Maximum likelihood estimate of covariance\")\nax[1, 1].hist(abc.ABC_dict[\"parameters\"][accept_indices, 1], np.linspace(0, 10, 100), histtype = u'step', orientation=\"horizontal\", density = True, linewidth = 1.5, color = \"C6\", label = \"ABC posterior\");\nax[1, 1].plot(np.sum(analytic_posterior * (parameter_grid[1, 0, 1] - parameter_grid[1, 0, 0]), axis = 1), parameter_grid[1, 0, :], linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[1, 1].plot(np.sum(gaussian_approximation * (grid[1, 0, 1] - grid[1, 0, 0]), axis = 1), grid[1, 0, :], color = \"C1\", label = \"Gaussian approximation\")\nax[1, 1].axhline(abc.MLE[0, 1], linestyle = \"dotted\", color = \"black\", label = \"Maximum likelihood estimate of covariance\")\nax[1, 1].legend(frameon = False)\nax[1, 1].set_ylim([0, 10])\nax[1, 1].set_xlabel('$\\\\mathcal{P}(\\\\Sigma|{\\\\bf d})$')\nax[1, 1].set_xticks([])\nax[1, 1].set_yticks([])\nax[0, 1].axis(\"off\");\n```\n\n\n![png](figures/output_78_0.png)\n\n\nWe now get samples from the posterior disrtibution which is not too far from the analytic posterior, and is at least unbiased. However, many samples are rejected to achieve this, and the rejection is defined somewhat arbitrarily, making it very computationally heavy and uncertain. We can improve on this using a PMC.\n\n## PMC-ABC\nPopulation Monte Carlo ABC is a way of reducing the number of draws by first sampling from a prior, accepting the closest 75% of the samples and weighting all the rest of the samples to create a new proposal distribution. The furthest 25% of the original samples are redrawn from the new proposal distribution. The furthest 25% of the simulation summaries are continually rejected and the proposal distribution updated until the number of draws needed accept all the 25% of the samples is much greater than this number of samples. This ratio is called the criterion.\n\nIf we want 1000 samples from the approximate distribution at the end of the PMC we need to set `posterior = 1000`. The initial random draw (as in ABC above) initialises with `draws`, the larger this is the better proposal distribution will be on the first iteration.\n\nThe `PMC` can be continued by running again with a smaller criterion.\n\n\n```python\nabc.PMC(draws=2000, posterior=2000, criterion=0.01, at_once=True, save_sims=None)\n```\n\n    iteration = 28, current criterion = 0.009747776288534179, total draws = 920480, \u03f5 = 0.08228597976267338.\n\nTo restart the PMC from scratch then one can run\n```python\nabc.PMC(draws=1000, posterior=1000, criterion=0.01, at_once=True, save_sims=None, restart=True)\n```\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize=(14, 10))\nplt.subplots_adjust(hspace=0, wspace=0.2)\nax[0, 0].scatter(abc.PMC_dict[\"parameters\"][:, 0] , abc.PMC_dict[\"MLE\"][:, 0], s=1, label=\"Accepted samples\", color=\"C4\", alpha=0.5)\nax[0, 0].axhline(abc.MLE[0, 0], color='black', linestyle='dashed', label=\"$\\hat{\\mu}_{obs}$\")\nax[0, 0].legend(frameon=False)\nax[0, 0].set_ylabel('$\\hat{\\mu}$', labelpad=0)\nax[0, 0].set_xlim([-10, 10])\nax[0, 0].set_xticks([])\nax[1, 0].scatter(abc.PMC_dict[\"parameters\"][:, 0], abc.PMC_dict[\"MLE\"][:, 1], s=1, alpha=0.5, label=\"Accepted samples\", color=\"C4\")\nax[1, 0].axhline(abc.MLE[0, 1], color='black', linestyle='dashed', label=\"$\\hat{\\Sigma}_{obs}$\")\nax[1, 0].legend(frameon=False)\nax[1, 0].set_ylabel('$\\hat{\\Sigma}$', labelpad=0)\nax[1, 0].set_xlim([-10, 10])\nax[1, 0].set_xlabel(\"$\\mu$\")\nax[0, 1].scatter(abc.PMC_dict[\"parameters\"][:, 1], abc.PMC_dict[\"MLE\"][:, 0], s=1, alpha=0.5, label=\"Accepted samples\", color=\"C4\")\nax[0, 1].axhline(abc.MLE[0, 0], color='black', linestyle='dashed', label=\"$\\hat{\\mu}_{obs}$\")\nax[0, 1].legend(frameon=False)\nax[0, 1].set_ylabel('$\\hat{\\mu}$', labelpad=0)\nax[0, 1].set_xlim([0, 10])\nax[0, 1].set_xticks([])\nax[1, 1].scatter(abc.PMC_dict[\"parameters\"][:, 1], abc.PMC_dict[\"MLE\"][:, 1], s=1, alpha=0.5, label=\"Accepted samples\", color=\"C4\")\nax[1, 1].axhline(abc.MLE[0, 1], color='black', linestyle='dashed', label=\"\\hat{\\Sigma}_{obs}\")\nax[1, 1].legend(frameon=False)\nax[1, 1].set_ylabel('$\\hat{\\Sigma}$', labelpad=0)\nax[1, 1].set_xlim([0, 10])\nax[1, 1].set_xlabel(\"$\\Sigma$\");\n```\n\n\n![png](figures/output_83_0.png)\n\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize=(16, 10))\nplt.subplots_adjust(wspace = 0, hspace = 0)\nax[0, 0].plot(parameter_grid[0, :, 0], np.sum(analytic_posterior * (parameter_grid[0, 1, 0] - parameter_grid[0, 0, 0]), axis = 0), linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[0, 0].plot(grid[0, :, 0], np.sum(gaussian_approximation * (grid[0, 1, 0] - grid[0, 0, 0]), axis = 0), color = \"C1\", label = \"Gaussian approximation\")\nax[0, 0].hist(abc.ABC_dict[\"parameters\"][accept_indices, 0], np.linspace(-10, 10, 100), histtype = u'step', density = True, linewidth = 1.5, color = \"C6\", alpha = 0.3, label = \"ABC posterior\");\nax[0, 0].hist(abc.PMC_dict[\"parameters\"][:, 0], np.linspace(-10, 10, 100), histtype = u'step', density = True, linewidth = 1.5, color = \"C4\", label = \"PMC posterior\");\nax[0, 0].axvline(abc.MLE[0, 0], linestyle = \"dashed\", color = \"black\", label = \"Maximum likelihood estimate of mean\")\nax[0, 0].legend(frameon = False)\nax[0, 0].set_xlim([-10, 10])\nax[0, 0].set_ylabel('$\\\\mathcal{P}(\\\\mu|{\\\\bf d})$')\nax[0, 0].set_yticks([])\nax[0, 0].set_xticks([])\nax[1, 0].set_xlabel('$\\mu$');\nax[1, 0].set_ylim([0, 10])\nax[1, 0].set_ylabel('$\\Sigma$')\nax[1, 0].set_xlim([-10, 10])\nax[1, 0].scatter(abc.ABC_dict[\"parameters\"][accept_indices, 0], abc.ABC_dict[\"parameters\"][accept_indices, 1], color = \"C6\", s = 1, alpha = 0.2)\nax[1, 0].scatter(abc.PMC_dict[\"parameters\"][:, 0], abc.PMC_dict[\"parameters\"][:, 1], color = \"C4\", s = 1, alpha = 0.7)\nax[1, 0].contour(parameter_grid[0, :, 0], parameter_grid[1, 0, :], analytic_posterior, colors = \"C2\")\nax[1, 0].contour(grid[0, :, 0], grid[1, 0, :], gaussian_approximation, colors = \"C1\")\nax[1, 0].axvline(abc.MLE[0, 0], linestyle = \"dashed\", color = \"black\", label = \"Maximum likelihood estimate of mean\")\nax[1, 0].axhline(abc.MLE[0, 1], linestyle = \"dotted\", color = \"black\", label = \"Maximum likelihood estimate of covariance\")\nax[1, 1].hist(abc.ABC_dict[\"parameters\"][accept_indices, 1], np.linspace(0, 10, 100), histtype = u'step', orientation=\"horizontal\", density = True, linewidth = 1.5, color = \"C6\", alpha = 0.3, label = \"ABC posterior\");\nax[1, 1].plot(np.sum(analytic_posterior * (parameter_grid[1, 0, 1] - parameter_grid[1, 0, 0]), axis = 1), parameter_grid[1, 0, :], linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[1, 1].plot(np.sum(gaussian_approximation * (grid[1, 0, 1] - grid[1, 0, 0]), axis = 1), grid[1, 0, :], color = \"C1\", label = \"Gaussian approximation\")\nax[1, 1].hist(abc.PMC_dict[\"parameters\"][:, 1], np.linspace(0, 10, 100), histtype = u'step', orientation=\"horizontal\", density = True, linewidth = 1.5, color = \"C4\", label = \"PMC posterior\");\nax[1, 1].axhline(abc.MLE[0, 1], linestyle = \"dotted\", color = \"black\", label = \"Maximum likelihood estimate of covariance\")\nax[1, 1].legend(frameon = False)\nax[1, 1].set_ylim([0, 10])\nax[1, 1].set_xlabel('$\\\\mathcal{P}(\\\\Sigma|{\\\\bf d})$')\nax[1, 1].set_xticks([])\nax[1, 1].set_yticks([])\nax[0, 1].axis(\"off\");\n```\n\n\n![png](figures/output_84_0.png)\n\n\nWe can see that the IMNN can recover great posteriors even when the data is extremely far from the fiducial parameter value at which the network was trained! Woohoo - give yourself a pat on the back!\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/tomcharnock/information_maximiser.git", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "IMNN", "package_url": "https://pypi.org/project/IMNN/", "platform": "", "project_url": "https://pypi.org/project/IMNN/", "project_urls": {"Homepage": "https://github.com/tomcharnock/information_maximiser.git"}, "release_url": "https://pypi.org/project/IMNN/0.2a2/", "requires_dist": ["tensorflow (>=2.0.0)", "tqdm (>=4.31.0)", "numpy (>=1.16.0)"], "requires_python": ">=3", "summary": "Using neural networks to extract sufficient statistics from         data by maximising the Fisher information", "version": "0.2a2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <pre><span class=\"o\">%</span><span class=\"n\">load_ext</span> <span class=\"n\">autoreload</span>\n<span class=\"o\">%</span><span class=\"n\">autoreload</span> <span class=\"mi\">2</span>\n</pre>\n<h1>Information maximiser</h1>\n<p>Using neural networks, sufficient statistics can be obtained from data by maximising the Fisher information.</p>\n<p>The neural network takes some data ${\\bf d}$ and maps it to a compressed summary $\\mathscr{f}:{\\bf d}\\to{\\bf x}$ where ${\\bf x}$ can have the same size as the dimensionality of the parameter space, rather than the data space.</p>\n<p>To train the neural network a batch of simulations ${\\bf d}<em>{\\sf sim}^{\\sf fid}$ created at a fiducial parameter value $\\boldsymbol{\\theta}^{\\rm fid}$ are compressed by the neural network to obtain ${\\bf x}</em>{\\sf sim}^{\\sf fid}$. From this we can calculate the covariance ${\\bf C_\\mathscr{f}}$ of the compressed summaries. We learn about model parameter distributions using the derivative of the simulation. This can be provided analytically or numercially using ${\\bf d}<em>{\\sf sim}^{\\sf fid+}$ created above the fiducial parameter value $\\boldsymbol{\\theta}^{\\sf fid+}$ and ${\\bf d}</em>{\\sf sim}^{\\sf fid-}$ created below the fiducial parameter value $\\boldsymbol{\\theta}^{\\sf fid-}$. The simulations are compressed using the network and used to find mean of the summaries\n$$\\frac{\\partial\\boldsymbol{\\mu}<em>\\mathscr{f}}{\\partial\\theta</em>\\alpha}=\\frac{1}{n_\\textrm{derivs}}\\sum_{i=1}^{n_\\textrm{derivs}}\\frac{{\\bf x}<em>{\\sf sim}^{\\sf fid+}-{\\bf x}</em>{\\sf sim}^{\\sf fid-}}{\\boldsymbol{\\theta}^{\\sf fid+}-\\boldsymbol{\\theta}^{\\sf fid-}}.$$\nIf the derivative of the simulations with respect to the parameters can be calculated analytically (or via autograd, etc.) then that can be used directly using the chain rule since the derivative of the network outputs with respect to the network input can be calculated easily\n$$\\frac{\\partial\\mu_\\mathscr{f}}{\\partial\\theta_\\alpha} = \\frac{1}{n_{\\textrm{sims}}}\\sum_{i=1}^{n_{\\textrm{sims}}}\\frac{\\partial{\\bf x}<em>i}{\\partial{\\bf d}<em>j}\\frac{\\partial{\\bf d}<em>j}{\\partial\\theta</em>\\alpha}\\delta</em>{ij}.$$\nWe then use ${\\bf C}</em>\\mathscr{f}$ and $\\boldsymbol{\\mu}<em>\\mathscr{f},</em>\\alpha$ to calculate the Fisher information\n$${\\bf F}<em>{\\alpha\\beta} = \\boldsymbol{\\mu}</em>\\mathscr{f},^T_{\\alpha}{\\bf C}^{-1}<em>\\mathscr{f}\\boldsymbol{\\mu}</em>\\mathscr{f},<em>{\\beta}.$$\nWe want to maximise the Fisher information, and we want the summaries to be orthogonal so to train the network we minimise the loss function\n$$\\Lambda = -\\ln|{\\bf F}</em>{\\alpha\\beta}|+r(\\Lambda_2)\\Lambda_2$$\nwhere\n$$\\Lambda_2 = ||{\\bf C}_\\mathscr{f}-\\mathbb{1}||<em>2 + ||{\\bf C}</em>\\mathscr{f}^{-1}-\\mathbb{1}||_2$$\nis a regularisation term whose strength is dictated by\n$$r(\\Lambda_2) = \\frac{\\lambda\\Lambda_2}{\\Lambda_2 + \\exp(-\\alpha\\Lambda_2)}$$\nwhere $\\lambda$ is a strength and $\\alpha$ is a rate parameter which can be determined from a closeness condition on the Frobenius norm of the difference between the convariance (and inverse covariance) from the identity matrix.</p>\n<p>When using this code please cite <a href=\"https://arxiv.org/abs/1802.03537\" rel=\"nofollow\">arXiv:1802.03537</a>.<br><br>\nThe code in the paper can be downloaded as v1 or v1.1 of the code kept on zenodo:<br><br>\n<a href=\"https://doi.org/10.5281/zenodo.1175196\" rel=\"nofollow\"><img alt=\"DOI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4d8fa01e5ca0d49a35e624359a45e54531b263dc/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e313137353139362e737667\"></a>\n<br></p>\n<p>This code is run using<br></p>\n<blockquote>\n<p><code>python v3.7.4</code></p>\n</blockquote>\n<blockquote>\n<p><code>tensorflow==2.0.0</code></p>\n</blockquote>\n<blockquote>\n<p><code>numpy==1.16.2</code></p>\n</blockquote>\n<blockquote>\n<p><code>tqdm==4.31.1</code></p>\n</blockquote>\n<p>Although these precise versions may not be necessary, I have put them here to avoid possible conflicts.</p>\n<h2>Load modules</h2>\n<pre><span class=\"o\">%</span><span class=\"n\">matplotlib</span> <span class=\"n\">inline</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"nn\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">IMNN</span> <span class=\"kn\">import</span> <span class=\"n\">IMNN</span>\n<span class=\"kn\">from</span> <span class=\"nn\">IMNN.ABC</span> <span class=\"kn\">import</span> <span class=\"n\">ABC</span><span class=\"p\">,</span> <span class=\"n\">priors</span>\n</pre>\n<h1>Summarising the mean and the variance</h1>\n<p>For this example we are going to use $n_{\\bf d}=10$ data points of a 1D field of Gaussian noise with unknown mean and variance to see if the network can learn to summarise them.<br><br></p>\n<p>The likelihood is given by\n$$\\mathcal{L} = \\prod_i^{n_{\\bf d}}\\frac{1}{\\sqrt{2\\pi|\\Sigma|}}\\exp\\left[-\\frac{1}{2}\\frac{(d_i-\\mu)^2}{\\Sigma}\\right]$$</p>\n<p>We can solve this problem analytically, so it is useful to check how well the network does. There is a single sufficient statistic which describes each the mean and the variance, which can be found by finding the maximum of the probability. We find that\n$$\\sum_i^{n_{\\bf d}}d_i = \\mu\\textrm{ and }\\sum_i^{n_{\\bf d}}(d_i-\\mu)^2=n_{\\bf d}\\Sigma$$</p>\n<p>We can calculate the Fisher information by taking the negative of second derivative of the likelihood taking the expectation by inserting the above relations at examining at some fiducial parameter values\n$${\\bf F}<em>{\\alpha\\beta} = -\\left.\\left(\\begin{array}{cc}\\displaystyle-\\frac{n</em>{\\bf d}}{\\Sigma}&amp;0\\0&amp;\\displaystyle-\\frac{n_{\\bf d}}{2\\Sigma^2}\\end{array}\\right)\\right|_{\\textrm{fiducial}}.$$\nIf we choose a fiducial mean of $\\mu^{\\textrm{fid}}=0$ and variance of $\\Sigma^{\\textrm{fid}} = 1$ then we obtain a Fisher information matrix of</p>\n<pre><span class=\"n\">input_shape</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,)</span>\n\n<span class=\"n\">n_params</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n<span class=\"n\">\u03b8_fid</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mf\">1.</span><span class=\"p\">])</span>\n</pre>\n<pre><span class=\"n\">exact_fisher</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"o\">-</span><span class=\"n\">input_shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">/</span> <span class=\"n\">\u03b8_fid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"mf\">0.</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">0.</span> <span class=\"p\">,</span> <span class=\"o\">-</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"n\">input_shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">/</span> <span class=\"n\">\u03b8_fid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">**</span><span class=\"mf\">2.</span><span class=\"p\">]])</span>\n<span class=\"n\">determinant_exact_fisher</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">det</span><span class=\"p\">(</span><span class=\"n\">exact_fisher</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"determinant of the Fisher information\"</span><span class=\"p\">,</span> <span class=\"n\">determinant_exact_fisher</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">exact_fisher</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">\"Inverse Fisher matrix\"</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xticks</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"sa\">r</span><span class=\"s2\">\"$\\mu$\"</span><span class=\"p\">,</span> <span class=\"sa\">r</span><span class=\"s2\">\"$\\Sigma$\"</span><span class=\"p\">])</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">yticks</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"sa\">r</span><span class=\"s2\">\"$\\mu$\"</span><span class=\"p\">,</span> <span class=\"sa\">r</span><span class=\"s2\">\"$\\Sigma$\"</span><span class=\"p\">])</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">colorbar</span><span class=\"p\">();</span>\n</pre>\n<pre><code>determinant of the Fisher information 50.000000000000014\n</code></pre>\n<p><img alt=\"png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2d45a03ad1e9ae5780e73c54ab898e34bb72e39c/666967757265732f6f75747075745f395f312e706e67\"></p>\n<p>Let us observe our <em>real</em> data which happens to have true parameters $\\mu=3$ and $\\Sigma=2$</p>\n<pre><span class=\"n\">real_data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"mf\">3.</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"mf\">2.</span><span class=\"p\">),</span> <span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,)</span> <span class=\"o\">+</span> <span class=\"n\">input_shape</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">figsize</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">))</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">real_data</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"observed data\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s2\">\"Data amplitude\"</span><span class=\"p\">);</span>\n</pre>\n<p><img alt=\"png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fd680b80e097db8400efc798a31c11826dd114cf/666967757265732f6f75747075745f31325f302e706e67\"></p>\n<p>The posterior distribution for this data (normalised to integrate to 1) is</p>\n<pre><span class=\"n\">\u03bc_array</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">)</span>\n<span class=\"n\">\u03a3_array</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"mf\">0.001</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">)</span>\n\n<span class=\"n\">parameter_grid</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">meshgrid</span><span class=\"p\">(</span><span class=\"n\">\u03bc_array</span><span class=\"p\">,</span> <span class=\"n\">\u03a3_array</span><span class=\"p\">,</span> <span class=\"n\">indexing</span> <span class=\"o\">=</span> <span class=\"s2\">\"ij\"</span><span class=\"p\">))</span>\n<span class=\"n\">dx</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">\u03bc_array</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">\u03bc_array</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">\u03a3_array</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">\u03a3_array</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n\n<span class=\"n\">analytic_posterior</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">((</span><span class=\"n\">real_data</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][:,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">newaxis</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">newaxis</span><span class=\"p\">,</span> <span class=\"p\">:])</span><span class=\"o\">**</span><span class=\"mf\">2.</span><span class=\"p\">,</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)[:,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">newaxis</span><span class=\"p\">]</span> <span class=\"o\">/</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:][</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">newaxis</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">+</span> <span class=\"n\">real_data</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"mf\">2.</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">pi</span> <span class=\"o\">*</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:][</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">newaxis</span><span class=\"p\">,</span> <span class=\"p\">:])))</span>\n<span class=\"n\">analytic_posterior</span> <span class=\"o\">=</span> <span class=\"n\">analytic_posterior</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">analytic_posterior</span> <span class=\"o\">*</span> <span class=\"n\">dx</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">figsize</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots_adjust</span><span class=\"p\">(</span><span class=\"n\">wspace</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">hspace</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">analytic_posterior</span><span class=\"p\">,</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s1\">'C2'</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Analytic marginalised posterior\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$</span><span class=\"se\">\\\\</span><span class=\"s1\">mathcal</span><span class=\"si\">{P}</span><span class=\"s1\">(</span><span class=\"se\">\\\\</span><span class=\"s1\">mu|{</span><span class=\"se\">\\\\</span><span class=\"s1\">bf d})$'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_yticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\mu$'</span><span class=\"p\">);</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\Sigma$'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">analytic_posterior</span><span class=\"p\">,</span> <span class=\"n\">colors</span> <span class=\"o\">=</span> <span class=\"s2\">\"C2\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">analytic_posterior</span><span class=\"p\">,</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s1\">'C2'</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Analytic marginalised posterior\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">'$</span><span class=\"se\">\\\\</span><span class=\"s1\">mathcal</span><span class=\"si\">{P}</span><span class=\"s1\">(</span><span class=\"se\">\\\\</span><span class=\"s1\">Sigma|{</span><span class=\"se\">\\\\</span><span class=\"s1\">bf d})$'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_yticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"p\">(</span><span class=\"s2\">\"off\"</span><span class=\"p\">);</span>\n</pre>\n<p><img alt=\"png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6af18d29a98bb82619e4d9bbc6da6271a55acace/666967757265732f6f75747075745f31355f302e706e67\"></p>\n<p>Now lets see how the information maximising neural network can recover this posterior.</p>\n<h2>Generate data</h2>\n<p>We start by defining a function to generate the data with the correct shape. The shape must be</p>\n<pre><code>data_shape = (None,) + input_shape\n</code></pre>\n<p>It is useful to define the generating function so that it only takes in the value of the parameter as its input since the function can then be used for ABC later.<br><br>\nThe data needs to be generated at a fiducial parameter value and at perturbed values just below and above the fiducial parameter for the numerical derivative.</p>\n<pre><span class=\"n\">\u03b4\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">])</span>\n</pre>\n<p>The training and validation data should be generated at the perturbed values with shape</p>\n<pre><code>perturbed_data_shape = (None, 2, n_params) + input_shape\n</code></pre>\n<p>if using numerical derivatives, where <code>[:, 0, ...]</code> is the parameter at <code>\u0394\u03b8pm</code> below <code>\u03b8_fid</code> and <code>[:, 1, ...]</code> is the parameter at <code>\u0394\u03b8pm</code> above <code>\u03b8_fid</code>. This is done for each parameter keeping ever non-perturbed parameter at its fiducial parameter.</p>\n<p>If the true derivative of the simulations with respect to the parameters is available then this can be calculated for each fiducial simulation in the dataset with shape</p>\n<pre><code>derivative_data_shape = (None, n_params) + input_shape\n</code></pre>\n<p>The generating function is defined so that the fiducial parameter is passed as a list so that many simulations can be made at once. This is very useful for the ABC function later.</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">simulator</span><span class=\"p\">(</span><span class=\"n\">\u03b8</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">simulator_args</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">seed</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">\u03b8</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n        <span class=\"n\">\u03bc</span> <span class=\"o\">=</span> <span class=\"n\">\u03b8</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">]</span>\n        <span class=\"n\">\u03a3</span> <span class=\"o\">=</span> <span class=\"n\">\u03b8</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">\u03bc</span> <span class=\"o\">=</span> <span class=\"mf\">0.</span>\n        <span class=\"n\">\u03a3</span> <span class=\"o\">=</span> <span class=\"n\">\u03b8</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">moveaxis</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"n\">\u03bc</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">\u03a3</span><span class=\"p\">),</span> <span class=\"n\">simulator_args</span><span class=\"p\">[</span><span class=\"s2\">\"input shape\"</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">\u03b8</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],)),</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n</pre>\n<h3>Training data</h3>\n<p>Enough data needs to be made to approximate the covariance matrix of the output summaries. The number of simulations needed to approximate the covariance is <code>n_s</code>. We can make stochastic updates by making <code>n_train</code> more simulations than are needed to approximate the covariance. <code>n_train</code> should be an integer number.</p>\n<pre><span class=\"n\">n_s</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span>\n<span class=\"n\">n_train</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n<span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mf\">1e6</span><span class=\"p\">)</span>\n</pre>\n<p>The training data can now be made</p>\n<pre><span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">simulator</span><span class=\"p\">(</span><span class=\"n\">\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">tile</span><span class=\"p\">(</span><span class=\"n\">\u03b8_fid</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"n\">n_train</span> <span class=\"o\">*</span> <span class=\"n\">n_s</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">simulator_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"input shape\"</span><span class=\"p\">:</span> <span class=\"n\">input_shape</span><span class=\"p\">})</span>\n</pre>\n<p>Ideally we would be able to take the derivative of our simulations with respect to the model parameters. We can indeed do that in this case, but since this is possibly a rare occurance I will show an example where the derivatives are calculated numerically. By suppressing the sample variance between the simulations created at some lower and upper varied parameter values, far fewer simulations are needed. We should choose the extra <code>n_train</code> simulations for the stochastic updates to be the same as for the fiducial simulations.</p>\n<pre><span class=\"n\">n_d</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span>\n</pre>\n<p>The sample variance is supressed by choosing the same initial seed when creating the upper and lower simulations.</p>\n<pre><span class=\"n\">dd_m_d\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">simulator</span><span class=\"p\">(</span><span class=\"n\">\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">tile</span><span class=\"p\">(</span><span class=\"n\">\u03b8_fid</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">]),</span> <span class=\"p\">[</span><span class=\"n\">n_train</span> <span class=\"o\">*</span> <span class=\"n\">n_d</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">simulator_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"input shape\"</span><span class=\"p\">:</span> <span class=\"n\">input_shape</span><span class=\"p\">})</span>\n<span class=\"n\">dd_p_d\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">simulator</span><span class=\"p\">(</span><span class=\"n\">\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">tile</span><span class=\"p\">(</span><span class=\"n\">\u03b8_fid</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">]),</span> <span class=\"p\">[</span><span class=\"n\">n_train</span> <span class=\"o\">*</span> <span class=\"n\">n_d</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">simulator_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"input shape\"</span><span class=\"p\">:</span> <span class=\"n\">input_shape</span><span class=\"p\">})</span>\n<span class=\"n\">dd_m_d\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">([</span><span class=\"n\">dd_m_d\u03b8</span><span class=\"p\">,</span> <span class=\"n\">simulator</span><span class=\"p\">(</span><span class=\"n\">\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">tile</span><span class=\"p\">(</span><span class=\"n\">\u03b8_fid</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">]),</span> <span class=\"p\">[</span><span class=\"n\">n_train</span> <span class=\"o\">*</span> <span class=\"n\">n_d</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">simulator_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"input shape\"</span><span class=\"p\">:</span> <span class=\"n\">input_shape</span><span class=\"p\">})],</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">dd_p_d\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">([</span><span class=\"n\">dd_p_d\u03b8</span><span class=\"p\">,</span> <span class=\"n\">simulator</span><span class=\"p\">(</span><span class=\"n\">\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">tile</span><span class=\"p\">(</span><span class=\"n\">\u03b8_fid</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">]),</span> <span class=\"p\">[</span><span class=\"n\">n_train</span> <span class=\"o\">*</span> <span class=\"n\">n_d</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">simulator_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"input shape\"</span><span class=\"p\">:</span> <span class=\"n\">input_shape</span><span class=\"p\">})],</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">dd_d\u03b8_num</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">concatenate</span><span class=\"p\">([</span><span class=\"n\">dd_m_d\u03b8</span><span class=\"p\">[:,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">newaxis</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">dd_p_d\u03b8</span><span class=\"p\">[:,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">newaxis</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]],</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">dd_d\u03b8</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">dd_p_d\u03b8</span> <span class=\"o\">-</span> <span class=\"n\">dd_m_d\u03b8</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mf\">2.</span> <span class=\"o\">*</span> <span class=\"n\">\u03b4\u03b8</span><span class=\"p\">)[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">newaxis</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">newaxis</span><span class=\"p\">]</span>\n</pre>\n<h3>Test data</h3>\n<p>We should also make some test data, but here we will use only one combination. This needs adding to the dictionary</p>\n<pre><span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mf\">1e6</span><span class=\"p\">)</span>\n<span class=\"n\">td</span> <span class=\"o\">=</span> <span class=\"n\">simulator</span><span class=\"p\">(</span><span class=\"n\">\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">tile</span><span class=\"p\">(</span><span class=\"n\">\u03b8_fid</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"n\">n_s</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">simulator_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"input shape\"</span><span class=\"p\">:</span> <span class=\"n\">input_shape</span><span class=\"p\">})</span>\n<span class=\"n\">tdd_m_d\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">simulator</span><span class=\"p\">(</span><span class=\"n\">\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">tile</span><span class=\"p\">(</span><span class=\"n\">\u03b8_fid</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">]),</span> <span class=\"p\">[</span><span class=\"n\">n_d</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">simulator_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"input shape\"</span><span class=\"p\">:</span> <span class=\"n\">input_shape</span><span class=\"p\">})</span>\n<span class=\"n\">tdd_p_d\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">simulator</span><span class=\"p\">(</span><span class=\"n\">\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">tile</span><span class=\"p\">(</span><span class=\"n\">\u03b8_fid</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">]),</span> <span class=\"p\">[</span><span class=\"n\">n_d</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">simulator_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"input shape\"</span><span class=\"p\">:</span> <span class=\"n\">input_shape</span><span class=\"p\">})</span>\n<span class=\"n\">tdd_m_d\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">([</span><span class=\"n\">tdd_m_d\u03b8</span><span class=\"p\">,</span> <span class=\"n\">simulator</span><span class=\"p\">(</span><span class=\"n\">\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">tile</span><span class=\"p\">(</span><span class=\"n\">\u03b8_fid</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">]),</span> <span class=\"p\">[</span><span class=\"n\">n_d</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">simulator_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"input shape\"</span><span class=\"p\">:</span> <span class=\"n\">input_shape</span><span class=\"p\">})],</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">tdd_p_d\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">([</span><span class=\"n\">tdd_p_d\u03b8</span><span class=\"p\">,</span> <span class=\"n\">simulator</span><span class=\"p\">(</span><span class=\"n\">\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">tile</span><span class=\"p\">(</span><span class=\"n\">\u03b8_fid</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">]),</span> <span class=\"p\">[</span><span class=\"n\">n_d</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]),</span> <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">simulator_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"input shape\"</span><span class=\"p\">:</span> <span class=\"n\">input_shape</span><span class=\"p\">})],</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">tdd_d\u03b8_num</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">concatenate</span><span class=\"p\">([</span><span class=\"n\">tdd_m_d\u03b8</span><span class=\"p\">[:,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">newaxis</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">],</span> <span class=\"n\">tdd_p_d\u03b8</span><span class=\"p\">[:,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">newaxis</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]],</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">tdd_d\u03b8</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">tdd_p_d\u03b8</span> <span class=\"o\">-</span> <span class=\"n\">tdd_m_d\u03b8</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mf\">2.</span> <span class=\"o\">*</span> <span class=\"n\">\u03b4\u03b8</span><span class=\"p\">)[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">newaxis</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">newaxis</span><span class=\"p\">]</span>\n</pre>\n<h3>Data visualisation</h3>\n<p>We can plot the data to see what it looks like.</p>\n<pre><span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">figsize</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">))</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"n\">n_train</span> <span class=\"o\">*</span> <span class=\"n\">n_s</span><span class=\"p\">)],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"training data\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">td</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"n\">n_s</span><span class=\"p\">)],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"test data\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s2\">\"Data amplitude\"</span><span class=\"p\">);</span>\n</pre>\n<p><img alt=\"png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c662f48c877c1893bc051e544a84eca6b851309f/666967757265732f6f75747075745f33335f302e706e67\"></p>\n<p>It is also very useful to plot the upper and lower derivatives to check that the sample variance is actually supressed since the network learns extremely slowly if this isn't done properly.</p>\n<pre><span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">figsize</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots_adjust</span><span class=\"p\">(</span><span class=\"n\">hspace</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">training_index</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"n\">n_train</span> <span class=\"o\">*</span> <span class=\"n\">n_d</span><span class=\"p\">)</span>\n<span class=\"n\">test_index</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"n\">n_d</span><span class=\"p\">)</span>\n\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">dd_m_d\u03b8</span><span class=\"p\">[</span><span class=\"n\">training_index</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"lower training data\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C0\"</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dashed\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">dd_p_d\u03b8</span><span class=\"p\">[</span><span class=\"n\">training_index</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"upper training data\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C0\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">tdd_m_d\u03b8</span><span class=\"p\">[</span><span class=\"n\">test_index</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"lower validation data\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dashed\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">tdd_p_d\u03b8</span><span class=\"p\">[</span><span class=\"n\">test_index</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"upper validation data\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s2\">\"Data amplitude with varied mean\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">dd_d\u03b8</span><span class=\"p\">[</span><span class=\"n\">training_index</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"derivative training data\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C0\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">tdd_d\u03b8</span><span class=\"p\">[</span><span class=\"n\">test_index</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"derivative validation data\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s2\">\"Amplitude of the derivative of the data</span><span class=\"se\">\\n</span><span class=\"s2\">with respect to the mean\"</span><span class=\"p\">);</span>\n\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">dd_m_d\u03b8</span><span class=\"p\">[</span><span class=\"n\">training_index</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"lower training data\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C0\"</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dashed\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">dd_p_d\u03b8</span><span class=\"p\">[</span><span class=\"n\">training_index</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"upper training data\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C0\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">tdd_m_d\u03b8</span><span class=\"p\">[</span><span class=\"n\">test_index</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"lower validation data\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dashed\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">tdd_p_d\u03b8</span><span class=\"p\">[</span><span class=\"n\">test_index</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"upper validation data\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s2\">\"Data amplitude with varied covariance\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">dd_d\u03b8</span><span class=\"p\">[</span><span class=\"n\">training_index</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"derivative training data\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C0\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">tdd_d\u03b8</span><span class=\"p\">[</span><span class=\"n\">test_index</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"derivative validation data\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s2\">\"Amplitude of the derivative of the data</span><span class=\"se\">\\n</span><span class=\"s2\">with respect to covariance\"</span><span class=\"p\">);</span>\n</pre>\n<p><img alt=\"png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ae2f8ae7104a0766c1f8d55389923a60c1ed5c6f/666967757265732f6f75747075745f33355f302e706e67\"></p>\n<h2>Initiliase the neural network</h2>\n<p>We will choose a fairly generic network as an example here. These networks can be written in TensorFlow or keras. It must be able to take in <code>(None,) + input_shape</code> and <code>(None, n_params) + input_shape</code> or <code>(None, 2, n_params) + input_shape</code> and output a flat array with <code>n_summaries</code> outputs, i.e. <code>(None, n_summaries</code> and <code>(None, n_params, n_summaries)</code> or <code>(None, 2, n_params, n_summaries)</code>.</p>\n<p>In principle <code>n_summaries</code> can be any number, but information loss is guaranteed if <code>n_summaries &lt; n_params</code> and we overparameterise the summaries if we use <code>n_summaries&gt;n_params</code>. Therefore, in principle, we should use <code>n_summaries=n_params</code>. Note that this isn't necessarily true if external informative summaries are included in the training.</p>\n<pre><span class=\"n\">n_summaries</span> <span class=\"o\">=</span> <span class=\"n\">n_params</span>\n</pre>\n<p>In keras we shall define a network with two hidden layers with 128 hidden nodes in each and every layer apart from the output activated using <code>tanh</code>. We shall optimise the network using the <code>Adam</code> optimiser on its default settings</p>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n    <span class=\"p\">[</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"n\">input_shape</span><span class=\"p\">),</span>\n     <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">),</span>\n     <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Activation</span><span class=\"p\">(</span><span class=\"s2\">\"tanh\"</span><span class=\"p\">),</span>\n     <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">),</span>\n     <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Activation</span><span class=\"p\">(</span><span class=\"s2\">\"tanh\"</span><span class=\"p\">),</span>\n     <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">n_summaries</span><span class=\"p\">),</span>\n    <span class=\"p\">])</span>\n<span class=\"n\">opt</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">()</span>\n</pre>\n<h2>Initialise the IMNN module</h2>\n<p>The IMNN only needs to be provided with the number of parameters in the model, the number of summaries output by the network and the number of simulations needed to approximate the covariance of the summaries and to approximate the mean of the derivative of the summaries with respect to the parameters. Optionally we can also choose whether to use 32 or 64 bit floats (and ints) in TensorFlow and choose whether or not to get verbose size checking whilst loading the model and the data.</p>\n<pre><span class=\"n\">imnn</span> <span class=\"o\">=</span> <span class=\"n\">IMNN</span><span class=\"o\">.</span><span class=\"n\">IMNN</span><span class=\"p\">(</span><span class=\"n\">n_params</span><span class=\"o\">=</span><span class=\"n\">n_params</span><span class=\"p\">,</span> <span class=\"n\">n_summaries</span><span class=\"o\">=</span><span class=\"n\">n_summaries</span><span class=\"p\">,</span> <span class=\"n\">n_covariance_sims</span><span class=\"o\">=</span><span class=\"n\">n_s</span><span class=\"p\">,</span> <span class=\"n\">n_derivative_sims</span><span class=\"o\">=</span><span class=\"n\">n_d</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<pre><code>Using single dataset\n</code></pre>\n<p>The network is then passed to the module using</p>\n<pre><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">set_model</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">optimiser</span><span class=\"o\">=</span><span class=\"n\">opt</span><span class=\"p\">)</span>\n</pre>\n<pre><code>Checking is not currently done on the model. Make sure that its output has shape (None, 2) for the fiducial values a nd (None, 2, 2, 2) for the derivative values.\n</code></pre>\n<h2>Load the data</h2>\n<p>The data is passed as a TensorFlow data set for extremely quick and efficient transfer to GPUs, etc. For most reasonable sized datasets (GPU/CPU memory dependent of course) we can load this using</p>\n<pre><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">setup_dataset</span><span class=\"p\">(</span><span class=\"n\">\u03b8_fid</span><span class=\"o\">=</span><span class=\"n\">\u03b8_fid</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"o\">=</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">dd_d\u03b8</span><span class=\"o\">=</span><span class=\"n\">dd_d\u03b8_num</span><span class=\"p\">,</span> <span class=\"n\">\u03b4\u03b8</span><span class=\"o\">=</span><span class=\"n\">\u03b4\u03b8</span><span class=\"p\">,</span> <span class=\"n\">external_summaries</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">external_derivatives</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">)</span>\n</pre>\n<p>Since we might already know some very informative summaries of the data, we can include these. To do so the external summaries can be passed to the above function as numpy arrays. The summaries must have the same structure as the data (and aligned with the data) where <code>input_shape</code> is replaced with a flat array with <code>n_external</code> elements.</p>\n<p>The dataset can be setup externally by setting the data attributes</p>\n<pre><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"o\">.</span><span class=\"n\">from_tensor_slices</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"p\">(</span><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">n_s</span><span class=\"p\">)</span>\n<span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"n\">n_train</span> <span class=\"o\">*</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">n_s</span><span class=\"p\">)</span>\n</pre>\n<p>where <code>data=(fiducial_data, derivative_data)</code> is a tuple containing the fiducial and the derivative simulations. If the number of parameters for the mean of the derivatives is different to the number needed for the covariance then <code>data</code> should not be a tuple and instead just contain the fiducial simulations and instead the derivatives can be passed as</p>\n<pre><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">derivative_dataset</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"o\">.</span><span class=\"n\">from_tensor_slices</span><span class=\"p\">(</span><span class=\"n\">derivative_data</span><span class=\"p\">)</span>\n<span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"p\">(</span><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">n_d</span><span class=\"p\">)</span>\n<span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"n\">n_train</span> <span class=\"o\">*</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">n_d</span><span class=\"p\">)</span>\n</pre>\n<p>If the derivative is done numerically then <code>imnn.numerical=True</code> must be set, and likewise <code>imnn.numerical=False</code> must be used otherwise. The fiducial parameters should be set using</p>\n<pre><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">\u03b8_fid</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">\u03b8_fid</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n</pre>\n<p>and if using the numerical derivative we also need to set</p>\n<pre><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">\u03b4\u03b8</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"mf\">1.</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mf\">2.</span> <span class=\"o\">*</span> <span class=\"n\">\u03b4\u03b8</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n</pre>\n<p>If external informative summaries are going to be used then the when the number of simulations for the derivative of the mean of the summaries and the number of simulations for the covariance of the summaries is the same the data tuple should be <code>data=(fiducial_data, derivative_data, external_summaries, external_derivatives)</code> and when then number is different the tuples should be different <code>data=(fiducial_data, external_summaries</code>) and <code>derivatives=(derivative_data, external_derivatives)</code>.</p>\n<p>Although there is little point in doing the above externally, this could be extremely useful for large datasets which are written as TFRecords.</p>\n<p>The validation data can be passed using</p>\n<pre><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">setup_dataset</span><span class=\"p\">(</span><span class=\"n\">\u03b8_fid</span><span class=\"o\">=</span><span class=\"n\">\u03b8_fid</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"o\">=</span><span class=\"n\">td</span><span class=\"p\">,</span> <span class=\"n\">dd_d\u03b8</span><span class=\"o\">=</span><span class=\"n\">tdd_d\u03b8_num</span><span class=\"p\">,</span> <span class=\"n\">\u03b4\u03b8</span><span class=\"o\">=</span><span class=\"n\">\u03b4\u03b8</span><span class=\"p\">,</span> <span class=\"n\">external_summaries</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">external_derivatives</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n</pre>\n<p>The same external setup can be be performed exactly as above but with <code>test_</code> appended before each attribute name.</p>\n<h2>Train the network</h2>\n<p>Since we need to constrain the scale of the summaries (remember, every linear rescaling of a summary is also a summary) we choose to constrain the summaries to have a covariance close to the identity matrix. This makes the summaries somewhat (and ideally exactly) orthogonal. To enforce this scale we have a regulariser which is the Frobenius norm of the difference between the covariance matrix and the identity matrix and also the difference betwee the inverse covariance matrix and the identity matrix. How close the covariance gets to the identity is controlled by a strength <code>\u03bb</code> parameter and a closeness <code>\u03f5</code> parameter. These determine a rate of convergence \u03b1 which determines how sharp the loss function is (if it's too sharp then training can be unstable). We choose an <code>\u03f5=0.01</code> and a strength of <code>\u03bb=10</code></p>\n<pre><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">set_regularisation_strength</span><span class=\"p\">(</span><span class=\"n\">\u03f5</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"n\">\u03bb</span><span class=\"o\">=</span><span class=\"mf\">10.</span><span class=\"p\">)</span>\n</pre>\n<p>We can now train the network. We just need to choose a number iterations of training updates (equivalent to full passes through the dataset). To validate we just pass the <code>validate=True</code> option.</p>\n<pre><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">n_iterations</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"n\">validate</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<pre><code>HBox(children=(IntProgress(value=0, description='Iterations', max=1000, style=ProgressStyle(description_width=\u2026\n</code></pre>\n<p>Training can be continued simply by running the fit again.</p>\n<p>The network can also be reinitialised before training if something goes awry by running</p>\n<pre><span class=\"n\">n</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">n_iterations</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"n\">validate</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">reset</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<p>Note that it is normal for the network to initially run slow and quite quickly speed up as the data flow from the dataset to the CPU is properly filled.</p>\n<p>A history (imnn.history) dictionary is collecting diagnostics. It contains</p>\n<ul>\n<li><code>det_F</code> : determinant of the Fisher information</li>\n<li><code>det_C</code> : determinant of the covariance of the summaries</li>\n<li><code>det_Cinv</code> : determinant of the inverse covariance of the summaries</li>\n<li><code>det_d\u03bc_d\u03b8</code> : derivative of the mean of the summaries with respect to the parameters</li>\n<li><code>reg</code> : value of the regulariser</li>\n<li><code>r</code> : value of the regulariser strength</li>\n</ul>\n<p>and the same for the validation set</p>\n<ul>\n<li><code>val_det_F</code> : determinant of the Fisher information for the validation set</li>\n<li><code>val_det_C</code> : determinant of the covariance of the summaries from the validation set</li>\n<li><code>val_det_Cinv</code> : determinant of the inverse covariance of the summaries from the validation set</li>\n<li><code>val_det_d\u03bc_d\u03b8</code> : derivative of the mean of the summaries from the validation set with respect to the parameters</li>\n<li><code>val_reg</code> : value of the regulariser for the validation set</li>\n<li><code>val_r</code> : value of the regulariser strength for the validation set</li>\n</ul>\n<pre><span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">sharex</span> <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">figsize</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots_adjust</span><span class=\"p\">(</span><span class=\"n\">hspace</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">history</span><span class=\"p\">[</span><span class=\"s2\">\"det_F\"</span><span class=\"p\">])</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">history</span><span class=\"p\">[</span><span class=\"s2\">\"det_F\"</span><span class=\"p\">],</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s2\">\"C0\"</span><span class=\"p\">,</span>\n           <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sa\">r</span><span class=\"s1\">'$|{\\bf F}_{\\alpha\\beta}|$ from training data'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">history</span><span class=\"p\">[</span><span class=\"s2\">\"val_det_F\"</span><span class=\"p\">],</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s2\">\"C1\"</span><span class=\"p\">,</span>\n           <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sa\">r</span><span class=\"s1\">'$|{\\bf F}_{\\alpha\\beta}|$ from validation data'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">determinant_exact_fisher</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span><span class=\"o\">=</span><span class=\"s2\">\"dashed\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s2\">\"$|{\\bf F}_{\\alpha\\beta}|$\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">history</span><span class=\"p\">[</span><span class=\"s2\">\"det_C\"</span><span class=\"p\">],</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s2\">\"C0\"</span><span class=\"p\">,</span>\n           <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sa\">r</span><span class=\"s1\">'$|{\\bf C}|$ from training data'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">history</span><span class=\"p\">[</span><span class=\"s2\">\"val_det_C\"</span><span class=\"p\">],</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s2\">\"C1\"</span><span class=\"p\">,</span>\n           <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sa\">r</span><span class=\"s1\">'$|{\\bf C}|$ from validation data'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">history</span><span class=\"p\">[</span><span class=\"s2\">\"det_Cinv\"</span><span class=\"p\">],</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s2\">\"C0\"</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span><span class=\"o\">=</span><span class=\"s2\">\"dashed\"</span><span class=\"p\">,</span>\n           <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sa\">r</span><span class=\"s1\">'$|{\\bf C}^{-1}|$ from training data'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">history</span><span class=\"p\">[</span><span class=\"s2\">\"val_det_Cinv\"</span><span class=\"p\">],</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s2\">\"C1\"</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span><span class=\"o\">=</span><span class=\"s2\">\"dashed\"</span><span class=\"p\">,</span>\n           <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sa\">r</span><span class=\"s1\">'$|{\\bf C}^{-1}|$ from validation data'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span><span class=\"o\">=</span><span class=\"s2\">\"dashed\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">loc</span><span class=\"o\">=</span><span class=\"s2\">\"best\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s2\">\"$|{\\bf C}|$ and $|{\\bf C}^{-1}|$\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_yscale</span><span class=\"p\">(</span><span class=\"s2\">\"log\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">history</span><span class=\"p\">[</span><span class=\"s2\">\"reg\"</span><span class=\"p\">],</span>\n           <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sa\">r</span><span class=\"s1\">'$\\Lambda_2$ from training data'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">history</span><span class=\"p\">[</span><span class=\"s2\">\"val_reg\"</span><span class=\"p\">],</span>\n           <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sa\">r</span><span class=\"s1\">'$\\Lambda_2$ from validation data'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s2\">\"$\\Lambda_2$\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">history</span><span class=\"p\">[</span><span class=\"s2\">\"r\"</span><span class=\"p\">],</span>\n           <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sa\">r</span><span class=\"s1\">'$r$ from training data'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">history</span><span class=\"p\">[</span><span class=\"s2\">\"val_r\"</span><span class=\"p\">],</span>\n           <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sa\">r</span><span class=\"s1\">'$r$ from validation data'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s2\">\"$r$\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s2\">\"Number of epochs\"</span><span class=\"p\">);</span>\n</pre>\n<p><img alt=\"png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4f66394a6d23c08bc7c3476fa736fc1aeb6e3bb0/666967757265732f6f75747075745f35385f302e706e67\"></p>\n<h2>Maximum likelihood estimates</h2>\n<p>The IMNN can provide maximum likelihood estimates of model parameters by initialising</p>\n<pre><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">setup_MLE</span><span class=\"p\">()</span>\n</pre>\n<p>Note this only works when a validation set is already loaded. It uses the validation set to calculate the Fisher information, covariance, derivative of the mean summaries and therefore the compression and transformation into MLE space. If a different dataset is to be used then it can be provided by setting <code>dataset=False</code> and providing all the necessary extra data</p>\n<pre><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">setup_MLE</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">\u03b8_fid</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">dd_d\u03b8</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">\u03b4\u03b8</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">ds_d\u03b8</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">)</span>\n</pre>\n<p>The maxmimum likelihood estimate is then obtained by running <code>imnn.get_MLE(d)</code> on data <code>d</code></p>\n<pre><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"The maximum likelihood estimate of the real data is \"</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">get_MLE</span><span class=\"p\">(</span><span class=\"n\">real_data</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">()))</span>\n</pre>\n<pre><code>The maximum likelihood estimate of the real data is [1.8213251 4.3077555]\n</code></pre>\n<h2>Approximate Bayesian computation</h2>\n<p>We can now do ABC (or PMC-ABC) with our calculated summary. From the samples we create simulations at each parameter value and feed each simulation through the network to get summaries. The summaries are compared to the summary of the real data to find the distances which can be used to accept or reject points.</p>\n<p>We start by defining our prior as a truncated Gaussian (uniform is also available). The uniform function is taken from delfi by Justin Alsing. At some point in the near future (for a given value of \"near\") this whole module will upgraded to TensorFlow Probability and complete TensorFlow 2 implementation.</p>\n<p>We are going to choose the mean value of the variance to be 1 with a variance of the variance of 10 cut at 0 and 10.</p>\n<pre><span class=\"n\">prior</span> <span class=\"o\">=</span> <span class=\"n\">priors</span><span class=\"o\">.</span><span class=\"n\">TruncatedGaussian</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"mf\">1.</span><span class=\"p\">]),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">10.</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"mf\">10.</span><span class=\"p\">]]),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mf\">10.</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">]),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">10.</span><span class=\"p\">,</span> <span class=\"mf\">10.</span><span class=\"p\">]))</span>\n</pre>\n<p>The ABC module takes in the <em>observed</em> data, the prior and the function for obtaining the MLE from the IMNN as well as the Fisher information matrix used for obtaining the MLE. It also takes in the simulator and its arguments.</p>\n<pre><span class=\"n\">abc</span> <span class=\"o\">=</span> <span class=\"n\">ABC</span><span class=\"p\">(</span><span class=\"n\">real_data</span><span class=\"o\">=</span><span class=\"n\">real_data</span><span class=\"p\">,</span> <span class=\"n\">prior</span><span class=\"o\">=</span><span class=\"n\">prior</span><span class=\"p\">,</span> <span class=\"n\">F</span><span class=\"o\">=</span><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">MLE_F</span><span class=\"p\">,</span> <span class=\"n\">get_MLE</span><span class=\"o\">=</span><span class=\"n\">imnn</span><span class=\"o\">.</span><span class=\"n\">get_MLE</span><span class=\"p\">,</span> <span class=\"n\">simulator</span><span class=\"o\">=</span><span class=\"n\">simulator</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">simulator_args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"input shape\"</span><span class=\"p\">:</span> <span class=\"n\">input_shape</span><span class=\"p\">})</span>\n</pre>\n<h2>Gaussian approximation</h2>\n<p>Before running all the simulations need for approximate Bayesian computation, we can get the Gaussian approximation of the posterior from the MLE and the inverse Fisher information.</p>\n<pre><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"maximum likelihood estimate\"</span><span class=\"p\">,</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"determinant of the Fisher information\"</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">det</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">F</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">Finv</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">\"Inverse Fisher matrix\"</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xticks</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"sa\">r</span><span class=\"s2\">\"$\\mu$\"</span><span class=\"p\">,</span> <span class=\"sa\">r</span><span class=\"s2\">\"$\\Sigma$\"</span><span class=\"p\">])</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">yticks</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"sa\">r</span><span class=\"s2\">\"$\\mu$\"</span><span class=\"p\">,</span> <span class=\"sa\">r</span><span class=\"s2\">\"$\\Sigma$\"</span><span class=\"p\">])</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">colorbar</span><span class=\"p\">();</span>\n</pre>\n<pre><code>maximum likelihood estimate [1.8213251 4.3077555]\ndeterminant of the Fisher information 67.50628\n</code></pre>\n<p><img alt=\"png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/84cc9768b66a63f7414ee762253666ab388a83d5/666967757265732f6f75747075745f36385f312e706e67\"></p>\n<pre><span class=\"n\">gaussian_approximation</span><span class=\"p\">,</span> <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">gaussian_approximation</span><span class=\"p\">(</span><span class=\"n\">gridsize</span> <span class=\"o\">=</span> <span class=\"mi\">100</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">figsize</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots_adjust</span><span class=\"p\">(</span><span class=\"n\">wspace</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">hspace</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">analytic_posterior</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s1\">'C2'</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Analytic marginalised posterior\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">gaussian_approximation</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Gaussian approximation\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axvline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dashed\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Maximum likelihood estimate of mean\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$</span><span class=\"se\">\\\\</span><span class=\"s1\">mathcal</span><span class=\"si\">{P}</span><span class=\"s1\">(</span><span class=\"se\">\\\\</span><span class=\"s1\">mu|{</span><span class=\"se\">\\\\</span><span class=\"s1\">bf d})$'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_yticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\mu$'</span><span class=\"p\">);</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\Sigma$'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">analytic_posterior</span><span class=\"p\">,</span> <span class=\"n\">colors</span> <span class=\"o\">=</span> <span class=\"s2\">\"C2\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">gaussian_approximation</span><span class=\"p\">,</span> <span class=\"n\">colors</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axvline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dashed\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Maximum likelihood estimate of mean\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dotted\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Maximum likelihood estimate of covariance\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">analytic_posterior</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s1\">'C2'</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Analytic marginalised posterior\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">gaussian_approximation</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Gaussian approximation\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dotted\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Maximum likelihood estimate of covariance\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">'$</span><span class=\"se\">\\\\</span><span class=\"s1\">mathcal</span><span class=\"si\">{P}</span><span class=\"s1\">(</span><span class=\"se\">\\\\</span><span class=\"s1\">Sigma|{</span><span class=\"se\">\\\\</span><span class=\"s1\">bf d})$'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_yticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"p\">(</span><span class=\"s2\">\"off\"</span><span class=\"p\">);</span>\n</pre>\n<p><img alt=\"png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d3be03c062f71d31e6c69874dacf9a34767fcb94/666967757265732f6f75747075745f37305f302e706e67\"></p>\n<p>We can see that the maximum likelihood estimate for the mean is almost perfect whilst it is incorrect for the variance. However, we can now see the ABC does in its place.</p>\n<h3>ABC</h3>\n<p>The most simple ABC takes the number of draws and a switch to state whether to run all the simulations in parallel or sequentially. The full simulations can also be saved by passing a file name. The draws are stored in the class attribute <code>ABC_dict</code>.</p>\n<pre><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC</span><span class=\"p\">(</span><span class=\"n\">draws</span><span class=\"o\">=</span><span class=\"mi\">100000</span><span class=\"p\">,</span> <span class=\"n\">at_once</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">save_sims</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">)</span>\n</pre>\n<p>In ABC, draws are accepted if the distance between the simulation summary and the simulation of the real data are \"close\", i.e. smaller than some \u03f5 value, which is chosen somewhat arbitrarily.</p>\n<pre><span class=\"n\">\u03f5</span> <span class=\"o\">=</span> <span class=\"o\">.</span><span class=\"mi\">5</span>\n<span class=\"n\">accept_indices</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argwhere</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"distances\"</span><span class=\"p\">]</span> <span class=\"o\">&lt;</span> <span class=\"n\">\u03f5</span><span class=\"p\">)[:,</span> <span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">reject_indices</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argwhere</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"distances\"</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"n\">\u03f5</span><span class=\"p\">)[:,</span> <span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Number of accepted samples = \"</span><span class=\"p\">,</span> <span class=\"n\">accept_indices</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n</pre>\n<pre><code>Number of accepted samples =  2319\n</code></pre>\n<h3>Plot samples</h3>\n<p>We can plot the output samples and the histogram of the accepted samples, which should peak around <code>\u03b8 = 1</code> (where we generated the real data). The monotonic function of all the output samples shows that the network has learned how to summarise the data.</p>\n<pre><span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">figsize</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">14</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots_adjust</span><span class=\"p\">(</span><span class=\"n\">hspace</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">wspace</span> <span class=\"o\">=</span> <span class=\"mf\">0.2</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">reject_indices</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"MLE\"</span><span class=\"p\">][</span><span class=\"n\">reject_indices</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Rejected samples\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C3\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"p\">,</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"MLE\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Accepted samples\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C6\"</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s1\">'black'</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s1\">'dashed'</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"$\\hat{\\mu}_</span><span class=\"si\">{obs}</span><span class=\"s2\">$\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\hat{\\mu}$'</span><span class=\"p\">,</span> <span class=\"n\">labelpad</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">reject_indices</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"MLE\"</span><span class=\"p\">][</span><span class=\"n\">reject_indices</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Rejected samples\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C3\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"p\">,</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"MLE\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Accepted samples\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C6\"</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s1\">'black'</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s1\">'dashed'</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"$\\hat{\\Sigma}_</span><span class=\"si\">{obs}</span><span class=\"s2\">$\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\hat{\\Sigma}$'</span><span class=\"p\">,</span> <span class=\"n\">labelpad</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s2\">\"$\\mu$\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">reject_indices</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"MLE\"</span><span class=\"p\">][</span><span class=\"n\">reject_indices</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Rejected samples\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C3\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"p\">,</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"MLE\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Accepted samples\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C6\"</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s1\">'black'</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s1\">'dashed'</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"$\\hat{\\mu}_</span><span class=\"si\">{obs}</span><span class=\"s2\">$\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\hat{\\mu}$'</span><span class=\"p\">,</span> <span class=\"n\">labelpad</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">reject_indices</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"MLE\"</span><span class=\"p\">][</span><span class=\"n\">reject_indices</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Rejected samples\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C3\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"p\">,</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"MLE\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Accepted samples\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C6\"</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s1\">'black'</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s1\">'dashed'</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"$\\hat{\\Sigma}_</span><span class=\"si\">{obs}</span><span class=\"s2\">$\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\hat{\\Sigma}$'</span><span class=\"p\">,</span> <span class=\"n\">labelpad</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s2\">\"$\\Sigma$\"</span><span class=\"p\">);</span>\n</pre>\n<p><img alt=\"png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0af7745f0155b54551d2ea7aa52d9ff03deb9cd9/666967757265732f6f75747075745f37375f302e706e67\"></p>\n<pre><span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">figsize</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots_adjust</span><span class=\"p\">(</span><span class=\"n\">wspace</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">hspace</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">analytic_posterior</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s1\">'C2'</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Analytic marginalised posterior\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">gaussian_approximation</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Gaussian approximation\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">hist</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">),</span> <span class=\"n\">histtype</span> <span class=\"o\">=</span> <span class=\"sa\">u</span><span class=\"s1\">'step'</span><span class=\"p\">,</span> <span class=\"n\">density</span> <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C6\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"ABC posterior\"</span><span class=\"p\">);</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axvline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dashed\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Maximum likelihood estimate of mean\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$</span><span class=\"se\">\\\\</span><span class=\"s1\">mathcal</span><span class=\"si\">{P}</span><span class=\"s1\">(</span><span class=\"se\">\\\\</span><span class=\"s1\">mu|{</span><span class=\"se\">\\\\</span><span class=\"s1\">bf d})$'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_yticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\mu$'</span><span class=\"p\">);</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\Sigma$'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C6\"</span><span class=\"p\">,</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">reject_indices</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">reject_indices</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C3\"</span><span class=\"p\">,</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">analytic_posterior</span><span class=\"p\">,</span> <span class=\"n\">colors</span> <span class=\"o\">=</span> <span class=\"s2\">\"C2\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">gaussian_approximation</span><span class=\"p\">,</span> <span class=\"n\">colors</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axvline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dashed\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Maximum likelihood estimate of mean\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dotted\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Maximum likelihood estimate of covariance\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">hist</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">),</span> <span class=\"n\">histtype</span> <span class=\"o\">=</span> <span class=\"sa\">u</span><span class=\"s1\">'step'</span><span class=\"p\">,</span> <span class=\"n\">orientation</span><span class=\"o\">=</span><span class=\"s2\">\"horizontal\"</span><span class=\"p\">,</span> <span class=\"n\">density</span> <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C6\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"ABC posterior\"</span><span class=\"p\">);</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">analytic_posterior</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s1\">'C2'</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Analytic marginalised posterior\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">gaussian_approximation</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Gaussian approximation\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dotted\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Maximum likelihood estimate of covariance\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">'$</span><span class=\"se\">\\\\</span><span class=\"s1\">mathcal</span><span class=\"si\">{P}</span><span class=\"s1\">(</span><span class=\"se\">\\\\</span><span class=\"s1\">Sigma|{</span><span class=\"se\">\\\\</span><span class=\"s1\">bf d})$'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_yticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"p\">(</span><span class=\"s2\">\"off\"</span><span class=\"p\">);</span>\n</pre>\n<p><img alt=\"png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2106ee7309d2d883516174def84a89f0ae920810/666967757265732f6f75747075745f37385f302e706e67\"></p>\n<p>We now get samples from the posterior disrtibution which is not too far from the analytic posterior, and is at least unbiased. However, many samples are rejected to achieve this, and the rejection is defined somewhat arbitrarily, making it very computationally heavy and uncertain. We can improve on this using a PMC.</p>\n<h2>PMC-ABC</h2>\n<p>Population Monte Carlo ABC is a way of reducing the number of draws by first sampling from a prior, accepting the closest 75% of the samples and weighting all the rest of the samples to create a new proposal distribution. The furthest 25% of the original samples are redrawn from the new proposal distribution. The furthest 25% of the simulation summaries are continually rejected and the proposal distribution updated until the number of draws needed accept all the 25% of the samples is much greater than this number of samples. This ratio is called the criterion.</p>\n<p>If we want 1000 samples from the approximate distribution at the end of the PMC we need to set <code>posterior = 1000</code>. The initial random draw (as in ABC above) initialises with <code>draws</code>, the larger this is the better proposal distribution will be on the first iteration.</p>\n<p>The <code>PMC</code> can be continued by running again with a smaller criterion.</p>\n<pre><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC</span><span class=\"p\">(</span><span class=\"n\">draws</span><span class=\"o\">=</span><span class=\"mi\">2000</span><span class=\"p\">,</span> <span class=\"n\">posterior</span><span class=\"o\">=</span><span class=\"mi\">2000</span><span class=\"p\">,</span> <span class=\"n\">criterion</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"n\">at_once</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">save_sims</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">)</span>\n</pre>\n<pre><code>iteration = 28, current criterion = 0.009747776288534179, total draws = 920480, \u03f5 = 0.08228597976267338.\n</code></pre>\n<p>To restart the PMC from scratch then one can run</p>\n<pre><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC</span><span class=\"p\">(</span><span class=\"n\">draws</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"n\">posterior</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"n\">criterion</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"n\">at_once</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">save_sims</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">restart</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">14</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots_adjust</span><span class=\"p\">(</span><span class=\"n\">hspace</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">wspace</span><span class=\"o\">=</span><span class=\"mf\">0.2</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][:,</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"p\">,</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"MLE\"</span><span class=\"p\">][:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">s</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s2\">\"Accepted samples\"</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s2\">\"C4\"</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">'black'</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span><span class=\"o\">=</span><span class=\"s1\">'dashed'</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s2\">\"$\\hat{\\mu}_</span><span class=\"si\">{obs}</span><span class=\"s2\">$\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\hat{\\mu}$'</span><span class=\"p\">,</span> <span class=\"n\">labelpad</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"MLE\"</span><span class=\"p\">][:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">s</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s2\">\"Accepted samples\"</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s2\">\"C4\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">'black'</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span><span class=\"o\">=</span><span class=\"s1\">'dashed'</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s2\">\"$\\hat{\\Sigma}_</span><span class=\"si\">{obs}</span><span class=\"s2\">$\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\hat{\\Sigma}$'</span><span class=\"p\">,</span> <span class=\"n\">labelpad</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s2\">\"$\\mu$\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"MLE\"</span><span class=\"p\">][:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">s</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s2\">\"Accepted samples\"</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s2\">\"C4\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">'black'</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span><span class=\"o\">=</span><span class=\"s1\">'dashed'</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s2\">\"$\\hat{\\mu}_</span><span class=\"si\">{obs}</span><span class=\"s2\">$\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\hat{\\mu}$'</span><span class=\"p\">,</span> <span class=\"n\">labelpad</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"MLE\"</span><span class=\"p\">][:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">s</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s2\">\"Accepted samples\"</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s2\">\"C4\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">'black'</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span><span class=\"o\">=</span><span class=\"s1\">'dashed'</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s2\">\"\\hat{\\Sigma}_</span><span class=\"si\">{obs}</span><span class=\"s2\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\hat{\\Sigma}$'</span><span class=\"p\">,</span> <span class=\"n\">labelpad</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s2\">\"$\\Sigma$\"</span><span class=\"p\">);</span>\n</pre>\n<p><img alt=\"png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/43d92498c7bed1491951aaf0f929912928d7196f/666967757265732f6f75747075745f38335f302e706e67\"></p>\n<pre><span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots_adjust</span><span class=\"p\">(</span><span class=\"n\">wspace</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">hspace</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">analytic_posterior</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s1\">'C2'</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Analytic marginalised posterior\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">gaussian_approximation</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Gaussian approximation\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">hist</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">),</span> <span class=\"n\">histtype</span> <span class=\"o\">=</span> <span class=\"sa\">u</span><span class=\"s1\">'step'</span><span class=\"p\">,</span> <span class=\"n\">density</span> <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C6\"</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"ABC posterior\"</span><span class=\"p\">);</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">hist</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">),</span> <span class=\"n\">histtype</span> <span class=\"o\">=</span> <span class=\"sa\">u</span><span class=\"s1\">'step'</span><span class=\"p\">,</span> <span class=\"n\">density</span> <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C4\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"PMC posterior\"</span><span class=\"p\">);</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axvline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dashed\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Maximum likelihood estimate of mean\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$</span><span class=\"se\">\\\\</span><span class=\"s1\">mathcal</span><span class=\"si\">{P}</span><span class=\"s1\">(</span><span class=\"se\">\\\\</span><span class=\"s1\">mu|{</span><span class=\"se\">\\\\</span><span class=\"s1\">bf d})$'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_yticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\mu$'</span><span class=\"p\">);</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">'$\\Sigma$'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlim</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C6\"</span><span class=\"p\">,</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.2</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C4\"</span><span class=\"p\">,</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">analytic_posterior</span><span class=\"p\">,</span> <span class=\"n\">colors</span> <span class=\"o\">=</span> <span class=\"s2\">\"C2\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">contour</span><span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">gaussian_approximation</span><span class=\"p\">,</span> <span class=\"n\">colors</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axvline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dashed\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Maximum likelihood estimate of mean\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dotted\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Maximum likelihood estimate of covariance\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">hist</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">ABC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][</span><span class=\"n\">accept_indices</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">),</span> <span class=\"n\">histtype</span> <span class=\"o\">=</span> <span class=\"sa\">u</span><span class=\"s1\">'step'</span><span class=\"p\">,</span> <span class=\"n\">orientation</span><span class=\"o\">=</span><span class=\"s2\">\"horizontal\"</span><span class=\"p\">,</span> <span class=\"n\">density</span> <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C6\"</span><span class=\"p\">,</span> <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"ABC posterior\"</span><span class=\"p\">);</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">analytic_posterior</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">parameter_grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s1\">'C2'</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Analytic marginalised posterior\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">gaussian_approximation</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C1\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Gaussian approximation\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">hist</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">PMC_dict</span><span class=\"p\">[</span><span class=\"s2\">\"parameters\"</span><span class=\"p\">][:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">),</span> <span class=\"n\">histtype</span> <span class=\"o\">=</span> <span class=\"sa\">u</span><span class=\"s1\">'step'</span><span class=\"p\">,</span> <span class=\"n\">orientation</span><span class=\"o\">=</span><span class=\"s2\">\"horizontal\"</span><span class=\"p\">,</span> <span class=\"n\">density</span> <span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mf\">1.5</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"C4\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"PMC posterior\"</span><span class=\"p\">);</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axhline</span><span class=\"p\">(</span><span class=\"n\">abc</span><span class=\"o\">.</span><span class=\"n\">MLE</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">linestyle</span> <span class=\"o\">=</span> <span class=\"s2\">\"dotted\"</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s2\">\"black\"</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"Maximum likelihood estimate of covariance\"</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">frameon</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_ylim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">'$</span><span class=\"se\">\\\\</span><span class=\"s1\">mathcal</span><span class=\"si\">{P}</span><span class=\"s1\">(</span><span class=\"se\">\\\\</span><span class=\"s1\">Sigma|{</span><span class=\"se\">\\\\</span><span class=\"s1\">bf d})$'</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_xticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_yticks</span><span class=\"p\">([])</span>\n<span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"p\">(</span><span class=\"s2\">\"off\"</span><span class=\"p\">);</span>\n</pre>\n<p><img alt=\"png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/dca88e69133e23819456e2f2af675189a5b15e78/666967757265732f6f75747075745f38345f302e706e67\"></p>\n<p>We can see that the IMNN can recover great posteriors even when the data is extremely far from the fiducial parameter value at which the network was trained! Woohoo - give yourself a pat on the back!</p>\n\n          </div>"}, "last_serial": 6208296, "releases": {"0.1.dev0": [{"comment_text": "", "digests": {"md5": "3e1977e6bac0f3329c48ca0f15ccf10b", "sha256": "2503e8aeb9bf4413fb4c8fbe7ce87c6ab86544afa1685ffcec3ae2128d63add0"}, "downloads": -1, "filename": "IMNN-0.1.dev0-py3-none-any.whl", "has_sig": false, "md5_digest": "3e1977e6bac0f3329c48ca0f15ccf10b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 35250, "upload_time": "2019-01-23T17:36:04", "upload_time_iso_8601": "2019-01-23T17:36:04.982909Z", "url": "https://files.pythonhosted.org/packages/d6/4d/92e3b83a60d39127f8af8f5597448057de8060f39822d78a352001a306e8/IMNN-0.1.dev0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "be51168f81db7066ac4b0356b000ba74", "sha256": "e718b51b8f67365ea0069240091040807259d42fca158a3f073572d22461c573"}, "downloads": -1, "filename": "IMNN-0.1.dev0.tar.gz", "has_sig": false, "md5_digest": "be51168f81db7066ac4b0356b000ba74", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 49339, "upload_time": "2019-01-23T17:36:06", "upload_time_iso_8601": "2019-01-23T17:36:06.898708Z", "url": "https://files.pythonhosted.org/packages/0d/ad/ea5a670be10cd97bf81eb4033bdd8ca963c4656f1f06322d4a4094dd119a/IMNN-0.1.dev0.tar.gz", "yanked": false}], "0.1.dev2": [{"comment_text": "", "digests": {"md5": "2fe58c400d0a3de14f7f16ed0f04f6ee", "sha256": "4eeb4ed8d347b9eedf72356be589101d9e857eaa4a57030fba528e0ba022a76a"}, "downloads": -1, "filename": "IMNN-0.1.dev2-py3-none-any.whl", "has_sig": false, "md5_digest": "2fe58c400d0a3de14f7f16ed0f04f6ee", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 34996, "upload_time": "2019-01-28T11:07:11", "upload_time_iso_8601": "2019-01-28T11:07:11.004882Z", "url": "https://files.pythonhosted.org/packages/d4/27/d7e51b66acf3c7b8b3a2f8077f7e012d83cd9809281f4220cd794bb66131/IMNN-0.1.dev2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2c0008b8d90d369680cb1094e126fdfd", "sha256": "f962aa006ba1776a317ddae0b5fe836e5840590b5cfbce4ef89742e2b33988c0"}, "downloads": -1, "filename": "IMNN-0.1.dev2.tar.gz", "has_sig": false, "md5_digest": "2c0008b8d90d369680cb1094e126fdfd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 48651, "upload_time": "2019-01-28T11:07:13", "upload_time_iso_8601": "2019-01-28T11:07:13.194089Z", "url": "https://files.pythonhosted.org/packages/bf/13/bf5345e3b85e425735949638c8e75c2b13c0c746cb04543008aaebf21533/IMNN-0.1.dev2.tar.gz", "yanked": false}], "0.1.dev3": [{"comment_text": "", "digests": {"md5": "a51c6ead7513f57e0722cdf02e268e01", "sha256": "5a1dc1d06db2e7809904a3926b457cf9d7669a4e3bea40d551d4d1dadfc7d032"}, "downloads": -1, "filename": "IMNN-0.1.dev3-py3-none-any.whl", "has_sig": false, "md5_digest": "a51c6ead7513f57e0722cdf02e268e01", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 35003, "upload_time": "2019-01-28T11:58:49", "upload_time_iso_8601": "2019-01-28T11:58:49.574781Z", "url": "https://files.pythonhosted.org/packages/2f/97/ad16b748782bb56040583ed3c4e27a12c8241c4e8c2abbecaad0c5557c38/IMNN-0.1.dev3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d9aae80c21d9b6902d2d5e95060ae409", "sha256": "4499e43651c2585e76a9fc5eed5e1ee55b3c606eed3c59e7fb35a2d9f76795d9"}, "downloads": -1, "filename": "IMNN-0.1.dev3.tar.gz", "has_sig": false, "md5_digest": "d9aae80c21d9b6902d2d5e95060ae409", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 48653, "upload_time": "2019-01-28T11:58:51", "upload_time_iso_8601": "2019-01-28T11:58:51.261890Z", "url": "https://files.pythonhosted.org/packages/2e/fe/6f6e666d7a444d755250bf45c035bc6c2e2959a36feb6218ea58c013e22e/IMNN-0.1.dev3.tar.gz", "yanked": false}], "0.1.dev4": [{"comment_text": "", "digests": {"md5": "76ecccc76a96f13e18e47b83729c4b7a", "sha256": "ff814a7096f97c3a5aa71d87492cc88c610a5707830e44a941954eba1e4f4b0d"}, "downloads": -1, "filename": "IMNN-0.1.dev4-py3-none-any.whl", "has_sig": false, "md5_digest": "76ecccc76a96f13e18e47b83729c4b7a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 35002, "upload_time": "2019-01-28T12:13:11", "upload_time_iso_8601": "2019-01-28T12:13:11.866826Z", "url": "https://files.pythonhosted.org/packages/ab/36/c67ad160b987c50110e0960d48b688bad0445e76d90a8849ceb5c5d6bdcb/IMNN-0.1.dev4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "39f419fc52499f8ebe0e03ba1208f1e2", "sha256": "4b87c285b6a7bc54283d20fe8d3cfdcd8ab8c39274163c08cf8ddc1b546304fc"}, "downloads": -1, "filename": "IMNN-0.1.dev4.tar.gz", "has_sig": false, "md5_digest": "39f419fc52499f8ebe0e03ba1208f1e2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 48653, "upload_time": "2019-01-28T12:13:13", "upload_time_iso_8601": "2019-01-28T12:13:13.456697Z", "url": "https://files.pythonhosted.org/packages/16/41/50978445be50fae20ccaacccbb77e3188cf6cd9f39853b45f38e098ebdb8/IMNN-0.1.dev4.tar.gz", "yanked": false}], "0.1.dev5": [{"comment_text": "", "digests": {"md5": "18296fe144d015772ca5576c1a9a3e3c", "sha256": "9957217601cd6d7c2818cd12d6735c957e9a892f45b8b10830baf27c129485ba"}, "downloads": -1, "filename": "IMNN-0.1.dev5-py3-none-any.whl", "has_sig": false, "md5_digest": "18296fe144d015772ca5576c1a9a3e3c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 35001, "upload_time": "2019-01-28T12:17:08", "upload_time_iso_8601": "2019-01-28T12:17:08.815098Z", "url": "https://files.pythonhosted.org/packages/ef/96/6bba42cac447e1623a64d3a79d6f78c298e0784a18b9a7e3e1e2693bae9a/IMNN-0.1.dev5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3022e29661f800bf3312ef81ffce0cb1", "sha256": "6132855954e3d0ee4599c7c41076cb02f8bc061c41226e3630f0467a1f4374d3"}, "downloads": -1, "filename": "IMNN-0.1.dev5.tar.gz", "has_sig": false, "md5_digest": "3022e29661f800bf3312ef81ffce0cb1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 48648, "upload_time": "2019-01-28T12:17:10", "upload_time_iso_8601": "2019-01-28T12:17:10.398221Z", "url": "https://files.pythonhosted.org/packages/07/38/92cc32817d3452f2c9a3faccb85107509e02bd3e25a197aaf09f83431998/IMNN-0.1.dev5.tar.gz", "yanked": false}], "0.1.dev6": [{"comment_text": "", "digests": {"md5": "5e918e9e534ae7b7be2d08a536ccc9b8", "sha256": "f3b048958ac811e33de46d12317b0f2099da72670e5b50b6bc0fd5b883134444"}, "downloads": -1, "filename": "IMNN-0.1.dev6-py3-none-any.whl", "has_sig": false, "md5_digest": "5e918e9e534ae7b7be2d08a536ccc9b8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 34946, "upload_time": "2019-01-28T18:25:49", "upload_time_iso_8601": "2019-01-28T18:25:49.491243Z", "url": "https://files.pythonhosted.org/packages/9f/65/01c081cb92464740725b698bf884210bb29936312f87c2a9bb19a4cbc07e/IMNN-0.1.dev6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d8a17a51a224a45f05ad4394eb65131c", "sha256": "76c8e412a9bfa0a79d4fc55690fa98aa7762759a1e89d72d4671d51ede8ece40"}, "downloads": -1, "filename": "IMNN-0.1.dev6.tar.gz", "has_sig": false, "md5_digest": "d8a17a51a224a45f05ad4394eb65131c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 48604, "upload_time": "2019-01-28T18:25:53", "upload_time_iso_8601": "2019-01-28T18:25:53.261825Z", "url": "https://files.pythonhosted.org/packages/45/d1/97929f102dedc740ae0f985ee5cd40b4b8dc4cbe2d08040468429d01901c/IMNN-0.1.dev6.tar.gz", "yanked": false}], "0.1.dev8": [{"comment_text": "", "digests": {"md5": "b7ce6e5aa451b9219500449934154656", "sha256": "4da6851163337e0a355e399ed7482f571c3ee6b864e95113e959b4e785cad79f"}, "downloads": -1, "filename": "IMNN-0.1.dev8-py3-none-any.whl", "has_sig": false, "md5_digest": "b7ce6e5aa451b9219500449934154656", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 38707, "upload_time": "2019-03-07T15:25:54", "upload_time_iso_8601": "2019-03-07T15:25:54.283004Z", "url": "https://files.pythonhosted.org/packages/66/7a/9974dd11345297ccf8848bcb0fe23b87dba1c779da41cfa44fe785663e6c/IMNN-0.1.dev8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e2b45deca9f2265d0ed5d54f9923fece", "sha256": "ee7add025a03cdaf48308c3f5d341771d4c751729970a283034998a073b887a5"}, "downloads": -1, "filename": "IMNN-0.1.dev8.tar.gz", "has_sig": false, "md5_digest": "e2b45deca9f2265d0ed5d54f9923fece", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 54410, "upload_time": "2019-03-07T15:25:56", "upload_time_iso_8601": "2019-03-07T15:25:56.074474Z", "url": "https://files.pythonhosted.org/packages/63/c4/6a1254c4766211f4225b7999102c835fb79813594f6e1629dee963fabfc8/IMNN-0.1.dev8.tar.gz", "yanked": false}], "0.1rc1": [{"comment_text": "", "digests": {"md5": "6fbf225f0f3fb80b5004503f9242a754", "sha256": "4c2271a2d7d7d4c563bc15a98bafd3221486ceaf50ea28b25a1093498d60315d"}, "downloads": -1, "filename": "IMNN-0.1rc1-py3-none-any.whl", "has_sig": false, "md5_digest": "6fbf225f0f3fb80b5004503f9242a754", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 37879, "upload_time": "2019-02-01T18:11:40", "upload_time_iso_8601": "2019-02-01T18:11:40.230801Z", "url": "https://files.pythonhosted.org/packages/6e/45/fcef3827bbec6acf00af9c2e1dd77223ba03c0f6d7e673a83e7ec3162a07/IMNN-0.1rc1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9bc2f2f3d09085b862335c9d6d388cca", "sha256": "8d810c18e7f4f5104f0968ace045712253b3b1abfdfe35c3858d9b67667bde63"}, "downloads": -1, "filename": "IMNN-0.1rc1.tar.gz", "has_sig": false, "md5_digest": "9bc2f2f3d09085b862335c9d6d388cca", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 54298, "upload_time": "2019-02-01T18:11:43", "upload_time_iso_8601": "2019-02-01T18:11:43.168104Z", "url": "https://files.pythonhosted.org/packages/e7/71/31bb47c26cbf7bef9d91a8a0c2a03f687ece2f422334070beeaff0eb21de/IMNN-0.1rc1.tar.gz", "yanked": false}], "0.2a1": [{"comment_text": "", "digests": {"md5": "940a6fd908c2bd0dfcd3112aab0dbcfd", "sha256": "7593bbc6816b707074b2276af5e7e082465e70af622dc694428a5bb29e3f8c7a"}, "downloads": -1, "filename": "IMNN-0.2a1-py3-none-any.whl", "has_sig": false, "md5_digest": "940a6fd908c2bd0dfcd3112aab0dbcfd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 32276, "upload_time": "2019-11-26T15:19:31", "upload_time_iso_8601": "2019-11-26T15:19:31.286790Z", "url": "https://files.pythonhosted.org/packages/d7/57/3d270f9d2e7141552bc5d9413b0cce0a87c192fd4373604405fa4fcc7c83/IMNN-0.2a1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "526daba1ebd6f63f60bb964dd1a52ba4", "sha256": "55f1d635693df3690371c21ef563c5fa4f8819a9bd1ac26086c25facffa810d5"}, "downloads": -1, "filename": "IMNN-0.2a1.tar.gz", "has_sig": false, "md5_digest": "526daba1ebd6f63f60bb964dd1a52ba4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 48047, "upload_time": "2019-11-26T15:19:36", "upload_time_iso_8601": "2019-11-26T15:19:36.032897Z", "url": "https://files.pythonhosted.org/packages/83/5c/6391c3042068e62730140616d1f736a7a74da67de31492a7ddb166139514/IMNN-0.2a1.tar.gz", "yanked": false}], "0.2a2": [{"comment_text": "", "digests": {"md5": "766bfcc197ac7e41e0ca8388e8ce56bf", "sha256": "e1a4a5faf42622af9d04391b596ec210a28c1ca6c1f58e797e625c8743c973f4"}, "downloads": -1, "filename": "IMNN-0.2a2-py3-none-any.whl", "has_sig": false, "md5_digest": "766bfcc197ac7e41e0ca8388e8ce56bf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 32261, "upload_time": "2019-11-27T14:28:37", "upload_time_iso_8601": "2019-11-27T14:28:37.046727Z", "url": "https://files.pythonhosted.org/packages/f6/2f/47385598b4b6911b554a11259504950a8cadeac92bc890deb438a8da43be/IMNN-0.2a2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c535b6e474bc8902dd62637227046b3f", "sha256": "0ddbe7b4de6934163aaeed5d97fb319c815085e83247a945afdfc49b11f63e4b"}, "downloads": -1, "filename": "IMNN-0.2a2.tar.gz", "has_sig": false, "md5_digest": "c535b6e474bc8902dd62637227046b3f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 48033, "upload_time": "2019-11-27T14:28:39", "upload_time_iso_8601": "2019-11-27T14:28:39.938806Z", "url": "https://files.pythonhosted.org/packages/62/2c/3e3350a7478e36546219aada011929adf66e793c61f8416ae680cf8b608b/IMNN-0.2a2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "766bfcc197ac7e41e0ca8388e8ce56bf", "sha256": "e1a4a5faf42622af9d04391b596ec210a28c1ca6c1f58e797e625c8743c973f4"}, "downloads": -1, "filename": "IMNN-0.2a2-py3-none-any.whl", "has_sig": false, "md5_digest": "766bfcc197ac7e41e0ca8388e8ce56bf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 32261, "upload_time": "2019-11-27T14:28:37", "upload_time_iso_8601": "2019-11-27T14:28:37.046727Z", "url": "https://files.pythonhosted.org/packages/f6/2f/47385598b4b6911b554a11259504950a8cadeac92bc890deb438a8da43be/IMNN-0.2a2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c535b6e474bc8902dd62637227046b3f", "sha256": "0ddbe7b4de6934163aaeed5d97fb319c815085e83247a945afdfc49b11f63e4b"}, "downloads": -1, "filename": "IMNN-0.2a2.tar.gz", "has_sig": false, "md5_digest": "c535b6e474bc8902dd62637227046b3f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 48033, "upload_time": "2019-11-27T14:28:39", "upload_time_iso_8601": "2019-11-27T14:28:39.938806Z", "url": "https://files.pythonhosted.org/packages/62/2c/3e3350a7478e36546219aada011929adf66e793c61f8416ae680cf8b608b/IMNN-0.2a2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:56:36 2020"}