{"info": {"author": "Maxim Podkolzine", "author_email": "maxim.podkolzine@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "============================================\nHyper-parameters Tuning for Machine Learning\n============================================\n\n- `Overview <#overview>`__\n    - `About <#about>`__\n    - `Installation <#installation>`__\n    - `How to use <#how-to-use>`__\n- `Features <#features>`__\n    - `Straight-forward specification <#straight-forward-specification>`__\n    - `Exploration-exploitation trade-off <#exploration-exploitation-trade-off>`__\n    - `Learning Curve Estimation <#learning-curve-estimation>`__\n- `Bayesian Optimization <#bayesian-optimization>`__\n\n--------\nOverview\n--------\n\nAbout\n=====\n\n*Hyper-Engine* is a toolbox for `model selection and hyper-parameters tuning <https://en.wikipedia.org/wiki/Hyperparameter_optimization>`__.\nIt aims to provide most state-of-the-art techniques via intuitive API and with minimum dependencies.\n*Hyper-Engine* is **not a framework**, which means it doesn't enforce any structure or design to the main code,\nthus making integration local and non-intrusive.\n\nInstallation\n============\n\n.. code-block:: shell\n\n    pip install git+https://github.com/maxim5/hyper-engine.git@master \n\nDependencies:\n\n-  Six, NumPy, SciPy\n-  TensorFlow (optional)\n-  PyPlot (optional, only for development)\n\nCompatibility:\n\n.. image:: https://travis-ci.org/maxim5/hyper-engine.svg?branch=master\n    :target: https://travis-ci.org/maxim5/hyper-engine\n\n-  Python 2.7, 3.5, 3.6\n\nLicense:\n\n- `Apache 2.0 <LICENSE>`__\n\n*Hyper-Engine* is designed to be ML-platform agnostic, but currently provides only simple `TensorFlow <https://github.com/tensorflow/tensorflow>`__ binding.\n\nHow to use\n==========\n\nAdapting your code to *Hyper-Engine* usually boils down to migrating hard-coded hyper-parameters to a dictionary (or an object)\nand giving names to particular tensors.\n\n**Before:**\n\n.. code-block:: python\n\n    def my_model():\n      x = tf.placeholder(...)\n      y = tf.placeholder(...)\n      ...\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n      ...\n\n**After:**\n\n.. code-block:: python\n\n    def my_model(params):\n      x = tf.placeholder(..., name='input')\n      y = tf.placeholder(..., name='label')\n      ...\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=params['learning_rate'])\n      ...\n\n    # Now can run the model with any set of hyper-parameters\n\n\nThe rest of the integration code is isolated and can be placed in the ``main`` script.\nSee the examples of hyper-parameter tuning in `examples <hyperengine/examples>`__ package.\n\n--------\nFeatures\n--------\n\nStraight-forward specification\n==============================\n\nThe crucial part of hyper-parameter tuning is the definition of a *domain*\nover which the engine is going to optimize the model. Some variables are continuous (e.g., the learning rate),\nsome variables are integer values in a certain range (e.g., the number of hidden units), some variables are categorical\nand represent architecture knobs (e.g., the choice of non-linearity).\n\nYou can define all these variables and their ranges in ``numpy``-like fashion:\n\n.. code-block:: python\n\n    hyper_params_spec = {\n      'optimizer': {\n        'learning_rate': 10**spec.uniform(-3, -1),          # makes the continuous range [0.1, 0.001]\n        'epsilon': 1e-8,                                    # constants work too\n      },\n      'conv': {\n        'filters': [[3, 3, spec.choice(range(32, 48))],     # an integer between [32, 48]\n                    [3, 3, spec.choice(range(64, 96))],     # an integer between [64, 96]\n                    [3, 3, spec.choice(range(128, 192))]],  # an integer between [128, 192]\n        'activation': spec.choice(['relu','prelu','elu']),  # a categorical range: 1 of 3 activations\n        'down_sample': {\n          'size': [2, 2],\n          'pooling': spec.choice(['max_pool', 'avg_pool'])  # a categorical range: 1 of 2 pooling methods\n        },\n        'residual': spec.random_bool(),                     # either True or False\n        'dropout': spec.uniform(0.75, 1.0),                 # a uniform continuous range\n      },\n    }\n\nNote that ``10**spec.uniform(-3, -1)`` is not the same *distribution* as ``spec.uniform(0.001, 0.1)``\n(though they both define the same *range* of values).\nIn the first case, the whole logarithmic spectrum ``(-3, -1)`` is equally probable, while in\nthe second case, small values around ``0.001`` are much less likely than the values around the mean ``0.0495``.\nSpecifying the following domain range for the learning rate - ``spec.uniform(0.001, 0.1)`` - will likely skew the results\ntowards higher learning rates. This outlines the importance of random variable transformations and arithmetic operations.\n\nExploration-exploitation trade-off\n==================================\n\nMachine learning model selection is expensive.\nEach model evaluation requires full training from scratch and may take minutes to hours to days, \ndepending on the problem complexity and available computational resources.\n*Hyper-Engine* provides the algorithm to explore the space of parameters efficiently, focus on the most promising areas,\nthus converge to the maximum as fast as possible.\n\n**Example 1**: the true function is 1-dimensional, ``f(x) = x * sin(x)`` (black curve) on [-10, 10] interval.\nRed dots represent each trial, red curve is the `Gaussian Process <https://en.wikipedia.org/wiki/Gaussian_process>`__ mean,\nblue curve is the mean plus or minus one standard deviation.\nThe optimizer randomly chose the negative mode as more promising.\n\n.. image:: /.images/figure_1.png\n    :width: 80%\n    :alt: 1D Bayesian Optimization\n    :align: center\n\n**Example 2**: the 2-dimensional function ``f(x, y) = (x + y) / ((x - 1) ** 2 - sin(y) + 2)`` (black surface) on [0,9]x[0,9] square.\nRed dots represent each trial, the Gaussian Process mean and standard deviations are not shown for simplicity.\nNote that to achieve the maximum both variables must be picked accurately.\n\n.. image:: /.images/figure_2-1.png\n   :width: 100%\n   :alt: 2D Bayesian Optimization\n   :align: center\n\n.. image:: /.images/figure_2-2.png\n   :width: 100%\n   :alt: 2D Bayesian Optimization\n   :align: center\n\nThe code for these and others examples is `here <https://github.com/maxim5/hyper-engine/blob/master/hyperengine/tests/strategy_test.py>`__.\n\nLearning Curve Estimation\n=========================\n\n*Hyper-Engine* can monitor the model performance during the training and stop early if it's learning too slowly.\nThis is done via *learning curve prediction*. Note that this technique is compatible with Bayesian Optimization, since\nit estimates the model accuracy after full training - this value can be safely used to update Gaussian Process parameters.\n\nExample code:\n\n.. code-block:: python\n\n    curve_params = {\n      'burn_in': 30,                # burn-in period: 30 models \n      'min_input_size': 5,          # start predicting after 5 epochs\n      'value_limit': 0.80,          # stop if the estimate is less than 80% with high probability\n    }\n    curve_predictor = LinearCurvePredictor(**curve_params)\n\nCurrently there is only one implementation of the predictor, ``LinearCurvePredictor``, \nwhich is very efficient, but requires relatively large burn-in period to predict model accuracy without flaws.\n\nNote that learning curves can be reused between different models and works quite well for the burn-in,\nso it's recommended to serialize and load curve data via ``io_save_dir`` and ``io_load_dir`` parameters.\n\nSee also the following paper:\n`Speeding up Automatic Hyperparameter Optimization of Deep Neural Networks\nby Extrapolation of Learning Curves <http://aad.informatik.uni-freiburg.de/papers/15-IJCAI-Extrapolation_of_Learning_Curves.pdf>`__\n\n---------------------\nBayesian Optimization\n---------------------\n\nImplements the following `methods <https://en.wikipedia.org/wiki/Bayesian_optimization>`__:\n\n-  Probability of improvement (See H. J. Kushner. A new method of locating the maximum of an arbitrary multipeak curve in the presence of noise. J. Basic Engineering, 86:97\u2013106, 1964.)\n-  Expected Improvement (See J. Mockus, V. Tiesis, and A. Zilinskas. Toward Global Optimization, volume 2, chapter The Application of Bayesian Methods for Seeking the Extremum, pages 117\u2013128. Elsevier, 1978)\n-  `Upper Confidence Bound <http://www.jmlr.org/papers/volume3/auer02a/auer02a.pdf>`__\n-  `Mixed / Portfolio strategy <http://mlg.eng.cam.ac.uk/hoffmanm/papers/hoffman:2011.pdf>`__\n-  Naive random search.\n\nPI method prefers exploitation to exploration, UCB is the opposite. One of the best strategies we've seen is a mixed one:\nstart with high probability of UCB and gradually decrease it, increasing PI probability.\n\nDefault kernel function used is `RBF kernel <https://en.wikipedia.org/wiki/Radial_basis_function_kernel>`__, but it is extensible.", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/maxim5/hyper-engine", "keywords": "machine learning,hyper-parameters,model selection,bayesian optimization", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "hyperengine", "package_url": "https://pypi.org/project/hyperengine/", "platform": "", "project_url": "https://pypi.org/project/hyperengine/", "project_urls": {"Homepage": "https://github.com/maxim5/hyper-engine"}, "release_url": "https://pypi.org/project/hyperengine/0.1.1/", "requires_dist": null, "requires_python": "", "summary": "Python library for Bayesian hyper-parameters optimization", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <ul>\n<li><dl>\n<dt><a href=\"#overview\" rel=\"nofollow\">Overview</a></dt>\n<dd><ul>\n<li><a href=\"#about\" rel=\"nofollow\">About</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#how-to-use\" rel=\"nofollow\">How to use</a></li>\n</ul>\n</dd>\n</dl>\n</li>\n<li><dl>\n<dt><a href=\"#features\" rel=\"nofollow\">Features</a></dt>\n<dd><ul>\n<li><a href=\"#straight-forward-specification\" rel=\"nofollow\">Straight-forward specification</a></li>\n<li><a href=\"#exploration-exploitation-trade-off\" rel=\"nofollow\">Exploration-exploitation trade-off</a></li>\n<li><a href=\"#learning-curve-estimation\" rel=\"nofollow\">Learning Curve Estimation</a></li>\n</ul>\n</dd>\n</dl>\n</li>\n<li><a href=\"#bayesian-optimization\" rel=\"nofollow\">Bayesian Optimization</a></li>\n</ul>\n<div id=\"overview\">\n<h2>Overview</h2>\n<div id=\"about\">\n<h3>About</h3>\n<p><em>Hyper-Engine</em> is a toolbox for <a href=\"https://en.wikipedia.org/wiki/Hyperparameter_optimization\" rel=\"nofollow\">model selection and hyper-parameters tuning</a>.\nIt aims to provide most state-of-the-art techniques via intuitive API and with minimum dependencies.\n<em>Hyper-Engine</em> is <strong>not a framework</strong>, which means it doesn\u2019t enforce any structure or design to the main code,\nthus making integration local and non-intrusive.</p>\n</div>\n<div id=\"installation\">\n<h3>Installation</h3>\n<pre>pip install git+https://github.com/maxim5/hyper-engine.git@master\n</pre>\n<p>Dependencies:</p>\n<ul>\n<li>Six, NumPy, SciPy</li>\n<li>TensorFlow (optional)</li>\n<li>PyPlot (optional, only for development)</li>\n</ul>\n<p>Compatibility:</p>\n<a href=\"https://travis-ci.org/maxim5/hyper-engine\" rel=\"nofollow\"><img alt=\"https://travis-ci.org/maxim5/hyper-engine.svg?branch=master\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/eb3a0e507cac725b0a97fd971c9d92dcdf5abc27/68747470733a2f2f7472617669732d63692e6f72672f6d6178696d352f68797065722d656e67696e652e7376673f6272616e63683d6d6173746572\"></a>\n<ul>\n<li>Python 2.7, 3.5, 3.6</li>\n</ul>\n<p>License:</p>\n<ul>\n<li><a href=\"LICENSE\" rel=\"nofollow\">Apache 2.0</a></li>\n</ul>\n<p><em>Hyper-Engine</em> is designed to be ML-platform agnostic, but currently provides only simple <a href=\"https://github.com/tensorflow/tensorflow\" rel=\"nofollow\">TensorFlow</a> binding.</p>\n</div>\n<div id=\"how-to-use\">\n<h3>How to use</h3>\n<p>Adapting your code to <em>Hyper-Engine</em> usually boils down to migrating hard-coded hyper-parameters to a dictionary (or an object)\nand giving names to particular tensors.</p>\n<p><strong>Before:</strong></p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">my_model</span><span class=\"p\">():</span>\n  <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">)</span>\n  <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">)</span>\n  <span class=\"o\">...</span>\n  <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">GradientDescentOptimizer</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">)</span>\n  <span class=\"o\">...</span>\n</pre>\n<p><strong>After:</strong></p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">my_model</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">):</span>\n  <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'input'</span><span class=\"p\">)</span>\n  <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'label'</span><span class=\"p\">)</span>\n  <span class=\"o\">...</span>\n  <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">GradientDescentOptimizer</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"n\">params</span><span class=\"p\">[</span><span class=\"s1\">'learning_rate'</span><span class=\"p\">])</span>\n  <span class=\"o\">...</span>\n\n<span class=\"c1\"># Now can run the model with any set of hyper-parameters</span>\n</pre>\n<p>The rest of the integration code is isolated and can be placed in the <tt>main</tt> script.\nSee the examples of hyper-parameter tuning in <a href=\"hyperengine/examples\" rel=\"nofollow\">examples</a> package.</p>\n</div>\n</div>\n<div id=\"features\">\n<h2>Features</h2>\n<div id=\"straight-forward-specification\">\n<h3>Straight-forward specification</h3>\n<p>The crucial part of hyper-parameter tuning is the definition of a <em>domain</em>\nover which the engine is going to optimize the model. Some variables are continuous (e.g., the learning rate),\nsome variables are integer values in a certain range (e.g., the number of hidden units), some variables are categorical\nand represent architecture knobs (e.g., the choice of non-linearity).</p>\n<p>You can define all these variables and their ranges in <tt>numpy</tt>-like fashion:</p>\n<pre><span class=\"n\">hyper_params_spec</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"s1\">'optimizer'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n    <span class=\"s1\">'learning_rate'</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"o\">**</span><span class=\"n\">spec</span><span class=\"o\">.</span><span class=\"n\">uniform</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">),</span>          <span class=\"c1\"># makes the continuous range [0.1, 0.001]</span>\n    <span class=\"s1\">'epsilon'</span><span class=\"p\">:</span> <span class=\"mf\">1e-8</span><span class=\"p\">,</span>                                    <span class=\"c1\"># constants work too</span>\n  <span class=\"p\">},</span>\n  <span class=\"s1\">'conv'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n    <span class=\"s1\">'filters'</span><span class=\"p\">:</span> <span class=\"p\">[[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">spec</span><span class=\"o\">.</span><span class=\"n\">choice</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">48</span><span class=\"p\">))],</span>     <span class=\"c1\"># an integer between [32, 48]</span>\n                <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">spec</span><span class=\"o\">.</span><span class=\"n\">choice</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">96</span><span class=\"p\">))],</span>     <span class=\"c1\"># an integer between [64, 96]</span>\n                <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">spec</span><span class=\"o\">.</span><span class=\"n\">choice</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">192</span><span class=\"p\">))]],</span>  <span class=\"c1\"># an integer between [128, 192]</span>\n    <span class=\"s1\">'activation'</span><span class=\"p\">:</span> <span class=\"n\">spec</span><span class=\"o\">.</span><span class=\"n\">choice</span><span class=\"p\">([</span><span class=\"s1\">'relu'</span><span class=\"p\">,</span><span class=\"s1\">'prelu'</span><span class=\"p\">,</span><span class=\"s1\">'elu'</span><span class=\"p\">]),</span>  <span class=\"c1\"># a categorical range: 1 of 3 activations</span>\n    <span class=\"s1\">'down_sample'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n      <span class=\"s1\">'size'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span>\n      <span class=\"s1\">'pooling'</span><span class=\"p\">:</span> <span class=\"n\">spec</span><span class=\"o\">.</span><span class=\"n\">choice</span><span class=\"p\">([</span><span class=\"s1\">'max_pool'</span><span class=\"p\">,</span> <span class=\"s1\">'avg_pool'</span><span class=\"p\">])</span>  <span class=\"c1\"># a categorical range: 1 of 2 pooling methods</span>\n    <span class=\"p\">},</span>\n    <span class=\"s1\">'residual'</span><span class=\"p\">:</span> <span class=\"n\">spec</span><span class=\"o\">.</span><span class=\"n\">random_bool</span><span class=\"p\">(),</span>                     <span class=\"c1\"># either True or False</span>\n    <span class=\"s1\">'dropout'</span><span class=\"p\">:</span> <span class=\"n\">spec</span><span class=\"o\">.</span><span class=\"n\">uniform</span><span class=\"p\">(</span><span class=\"mf\">0.75</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">),</span>                 <span class=\"c1\"># a uniform continuous range</span>\n  <span class=\"p\">},</span>\n<span class=\"p\">}</span>\n</pre>\n<p>Note that <tt><span class=\"pre\">10**spec.uniform(-3,</span> <span class=\"pre\">-1)</span></tt> is not the same <em>distribution</em> as <tt>spec.uniform(0.001, 0.1)</tt>\n(though they both define the same <em>range</em> of values).\nIn the first case, the whole logarithmic spectrum <tt><span class=\"pre\">(-3,</span> <span class=\"pre\">-1)</span></tt> is equally probable, while in\nthe second case, small values around <tt>0.001</tt> are much less likely than the values around the mean <tt>0.0495</tt>.\nSpecifying the following domain range for the learning rate - <tt>spec.uniform(0.001, 0.1)</tt> - will likely skew the results\ntowards higher learning rates. This outlines the importance of random variable transformations and arithmetic operations.</p>\n</div>\n<div id=\"exploration-exploitation-trade-off\">\n<h3>Exploration-exploitation trade-off</h3>\n<p>Machine learning model selection is expensive.\nEach model evaluation requires full training from scratch and may take minutes to hours to days,\ndepending on the problem complexity and available computational resources.\n<em>Hyper-Engine</em> provides the algorithm to explore the space of parameters efficiently, focus on the most promising areas,\nthus converge to the maximum as fast as possible.</p>\n<p><strong>Example 1</strong>: the true function is 1-dimensional, <tt>f(x) = x * sin(x)</tt> (black curve) on [-10, 10] interval.\nRed dots represent each trial, red curve is the <a href=\"https://en.wikipedia.org/wiki/Gaussian_process\" rel=\"nofollow\">Gaussian Process</a> mean,\nblue curve is the mean plus or minus one standard deviation.\nThe optimizer randomly chose the negative mode as more promising.</p>\n<img alt=\"1D Bayesian Optimization\" class=\"align-center\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6ae038a8921268d5ca3a6bf929d7608220802e7e/2f2e696d616765732f6669677572655f312e706e67\" width=\"80%\">\n<p><strong>Example 2</strong>: the 2-dimensional function <tt>f(x, y) = (x + y) / ((x - 1) ** 2 - sin(y) + 2)</tt> (black surface) on [0,9]x[0,9] square.\nRed dots represent each trial, the Gaussian Process mean and standard deviations are not shown for simplicity.\nNote that to achieve the maximum both variables must be picked accurately.</p>\n<img alt=\"2D Bayesian Optimization\" class=\"align-center\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ec1ad0fae74f34503243fcb3c148b8e82f05a673/2f2e696d616765732f6669677572655f322d312e706e67\" width=\"100%\">\n<img alt=\"2D Bayesian Optimization\" class=\"align-center\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/553b9590f61be43dfc7004474742b1df7eca09d8/2f2e696d616765732f6669677572655f322d322e706e67\" width=\"100%\">\n<p>The code for these and others examples is <a href=\"https://github.com/maxim5/hyper-engine/blob/master/hyperengine/tests/strategy_test.py\" rel=\"nofollow\">here</a>.</p>\n</div>\n<div id=\"learning-curve-estimation\">\n<h3>Learning Curve Estimation</h3>\n<p><em>Hyper-Engine</em> can monitor the model performance during the training and stop early if it\u2019s learning too slowly.\nThis is done via <em>learning curve prediction</em>. Note that this technique is compatible with Bayesian Optimization, since\nit estimates the model accuracy after full training - this value can be safely used to update Gaussian Process parameters.</p>\n<p>Example code:</p>\n<pre><span class=\"n\">curve_params</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"s1\">'burn_in'</span><span class=\"p\">:</span> <span class=\"mi\">30</span><span class=\"p\">,</span>                <span class=\"c1\"># burn-in period: 30 models</span>\n  <span class=\"s1\">'min_input_size'</span><span class=\"p\">:</span> <span class=\"mi\">5</span><span class=\"p\">,</span>          <span class=\"c1\"># start predicting after 5 epochs</span>\n  <span class=\"s1\">'value_limit'</span><span class=\"p\">:</span> <span class=\"mf\">0.80</span><span class=\"p\">,</span>          <span class=\"c1\"># stop if the estimate is less than 80% with high probability</span>\n<span class=\"p\">}</span>\n<span class=\"n\">curve_predictor</span> <span class=\"o\">=</span> <span class=\"n\">LinearCurvePredictor</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">curve_params</span><span class=\"p\">)</span>\n</pre>\n<p>Currently there is only one implementation of the predictor, <tt>LinearCurvePredictor</tt>,\nwhich is very efficient, but requires relatively large burn-in period to predict model accuracy without flaws.</p>\n<p>Note that learning curves can be reused between different models and works quite well for the burn-in,\nso it\u2019s recommended to serialize and load curve data via <tt>io_save_dir</tt> and <tt>io_load_dir</tt> parameters.</p>\n<p>See also the following paper:\n<a href=\"http://aad.informatik.uni-freiburg.de/papers/15-IJCAI-Extrapolation_of_Learning_Curves.pdf\" rel=\"nofollow\">Speeding up Automatic Hyperparameter Optimization of Deep Neural Networks\nby Extrapolation of Learning Curves</a></p>\n</div>\n</div>\n<div id=\"bayesian-optimization\">\n<h2>Bayesian Optimization</h2>\n<p>Implements the following <a href=\"https://en.wikipedia.org/wiki/Bayesian_optimization\" rel=\"nofollow\">methods</a>:</p>\n<ul>\n<li>Probability of improvement (See H. J. Kushner. A new method of locating the maximum of an arbitrary multipeak curve in the presence of noise. J. Basic Engineering, 86:97\u2013106, 1964.)</li>\n<li>Expected Improvement (See J. Mockus, V. Tiesis, and A. Zilinskas. Toward Global Optimization, volume 2, chapter The Application of Bayesian Methods for Seeking the Extremum, pages 117\u2013128. Elsevier, 1978)</li>\n<li><a href=\"http://www.jmlr.org/papers/volume3/auer02a/auer02a.pdf\" rel=\"nofollow\">Upper Confidence Bound</a></li>\n<li><a href=\"http://mlg.eng.cam.ac.uk/hoffmanm/papers/hoffman:2011.pdf\" rel=\"nofollow\">Mixed / Portfolio strategy</a></li>\n<li>Naive random search.</li>\n</ul>\n<p>PI method prefers exploitation to exploration, UCB is the opposite. One of the best strategies we\u2019ve seen is a mixed one:\nstart with high probability of UCB and gradually decrease it, increasing PI probability.</p>\n<p>Default kernel function used is <a href=\"https://en.wikipedia.org/wiki/Radial_basis_function_kernel\" rel=\"nofollow\">RBF kernel</a>, but it is extensible.</p>\n</div>\n\n          </div>"}, "last_serial": 3569838, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "05c3de48ece3ffc00c21ac639bdbdfe9", "sha256": "1230857839fcaa94f8781966e2f10b1eb549a30977c7483a2933f315570d141e"}, "downloads": -1, "filename": "hyperengine-0.1.1.tar.gz", "has_sig": false, "md5_digest": "05c3de48ece3ffc00c21ac639bdbdfe9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31237, "upload_time": "2018-02-10T12:56:52", "upload_time_iso_8601": "2018-02-10T12:56:52.077559Z", "url": "https://files.pythonhosted.org/packages/d7/de/cc05d99e18ddb74012bf5d5ec8f7932fd5d667a5373c576d26dfad6f598a/hyperengine-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "05c3de48ece3ffc00c21ac639bdbdfe9", "sha256": "1230857839fcaa94f8781966e2f10b1eb549a30977c7483a2933f315570d141e"}, "downloads": -1, "filename": "hyperengine-0.1.1.tar.gz", "has_sig": false, "md5_digest": "05c3de48ece3ffc00c21ac639bdbdfe9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31237, "upload_time": "2018-02-10T12:56:52", "upload_time_iso_8601": "2018-02-10T12:56:52.077559Z", "url": "https://files.pythonhosted.org/packages/d7/de/cc05d99e18ddb74012bf5d5ec8f7932fd5d667a5373c576d26dfad6f598a/hyperengine-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:49:33 2020"}