{"info": {"author": "\"Joakim Rishaug\"", "author_email": "\"joakimrishaug@notmyrealemail.com\"", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "<!--\n\n#################################################\n### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###\n#################################################\n# file to edit: index.ipynb\n# command to build the docs after a change: nbdev_build_docs\n\n-->\n\n# WARP-Pytorch\n\n> An implementation of WARP loss which uses matrixes and stays on the GPU in PyTorch.\n\n\nAn implementation of WARP loss which uses matrixes and stays on the GPU in PyTorch.\n\nThis means instead of using a for-loop to find the first offending negative sample that ranks above our positive,\nwe compute all of them at once. Only later do we find which sample is the first offender, and compute the loss with\nrespect to this sample.\n\nThe advantage is that it can use the speedups that comes with GPU-usage. \n\n## When is WARP loss advantageous?\nIf you're ranking items or making models for recommendations, it's often advantageous to let your loss function directly\noptimize for this case. WARP loss looks at 1 explicit positive up against the implicit negative items that a user never sampled,\nand allows us to adjust weights of the network accordingly.\n\n\n## Install\n\n`pip install warp_loss`\n\n## How to use\n\nThe loss function requires scores for both positive examples, and negative examples to be supplied, such as in the example below.\n<div class=\"codecell\" markdown=\"1\">\n<div class=\"input_area\" markdown=\"1\">\n\n```\nfrom torch import nn\nimport torch\n\nclass OurModel(nn.Module):\n    def __init__(self, num_labels, emb_dim=10):\n        super(OurModel, self).__init__()\n        self.emb = nn.Embedding(num_labels, emb_dim)\n        self.user_embs = nn.Embedding(1, emb_dim)\n\n    def forward(self, pos, neg):\n        batch_size = neg.size(0)\n        one_user_vector = self.user_embs(torch.zeros(1).long())\n        repeated_user_vector = one_user_vector.repeat((batch_size, 1)).view(batch_size, -1, 1)\n        pos_res = torch.bmm(self.emb(pos), repeated_user_vector).squeeze(2)\n        neg_res = torch.bmm(self.emb(neg), repeated_user_vector).squeeze(2)\n\n        return pos_res, neg_res\n\nnum_labels = 100\nmodel = OurModel(num_labels)\n```\n\n</div>\n\n</div>\n<div class=\"codecell\" markdown=\"1\">\n<div class=\"input_area\" markdown=\"1\">\n\n```\npos_labels = torch.randint(high=num_labels, size=(3,1)) # our five labels\nneg_labels = torch.randint(high=num_labels, size=(3,2)) # a few random negatives per positive\n\npos_res, neg_res = model(pos_labels, neg_labels)\nprint('Positive Labels:', pos_labels)\nprint('Negative Labels:', neg_labels)\nprint('Model positive scores:', pos_res)\nprint('Model negative scores:', neg_res)\nloss = warp_loss(pos_res, neg_res, num_labels=num_labels, device=torch.device('cpu'))\nprint('Loss:', loss)\nloss.backward()\n```\n\n</div>\n<div class=\"output_area\" markdown=\"1\">\n\n    Positive Labels: tensor([[65],\n            [94],\n            [21]])\n    Negative Labels: tensor([[ 8, 45],\n            [37, 93],\n            [88, 84]])\n    Model positive scores: tensor([[-3.7806],\n            [-1.9974],\n            [-4.1741]], grad_fn=<SqueezeBackward1>)\n    Model negative scores: tensor([[-1.5696, -4.4905],\n            [-1.9300, -0.3826],\n            [ 2.4564, -2.1741]], grad_fn=<SqueezeBackward1>)\n    Loss: tensor(54.7226, grad_fn=<SumBackward0>)\n\n\n</div>\n\n</div>\n<div class=\"codecell\" markdown=\"1\">\n<div class=\"input_area\" markdown=\"1\">\n\n```\nprint('We can also see that the gradient is only active for 2x the number of positive labels:', (model.emb.weight.grad.sum(1) != 0).sum().item())\nprint('Meaning we correctly discard the gradients for all other than the offending negative label.')\n```\n\n</div>\n<div class=\"output_area\" markdown=\"1\">\n\n    We can also see that the gradient is only active for 2x the number of positive labels: 6\n    Meaning we correctly discard the gradients for all other than the offending negative label.\n\n\n</div>\n\n</div>\n\n## Assumptions\nThe loss function assumes you have already sampled your negatives randomly.\n\nAs an example this could be done in your dataloader:\n\n1. Assume we have a total dataset of 100 items\n2. Select a positive sample with index 8\n2. Your negatives should be a random selection from 0-100 excluding 8.\n\nEx input to loss function: model scores for pos: [8] neg: [88, 3, 99, 7]\n\nCurrently only tested on PyTorch v0.4\n\n### References\n* [WSABIE: Scaling Up To Large Vocabulary Image Annotation](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37180.pdf)\n* [Intro to WARP loss - Automatic differentiation and PyTorch](https://medium.com/@gabrieltseng/intro-to-warp-loss-automatic-differentiation-and-pytorch-b6aa5083187a)\n* [LightFM](https://github.com/lyst/lightfm) as a reference implementaiton\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/NegatioN/WARP-Pytorch", "keywords": "pytorch loss WARP WSABIE", "license": "Apache Software License 2.0", "maintainer": "", "maintainer_email": "", "name": "warp-loss", "package_url": "https://pypi.org/project/warp-loss/", "platform": "", "project_url": "https://pypi.org/project/warp-loss/", "project_urls": {"Homepage": "https://github.com/NegatioN/WARP-Pytorch"}, "release_url": "https://pypi.org/project/warp-loss/0.0.1/", "requires_dist": ["numpy (>=1.15.4)", "torch (>=0.4.1)"], "requires_python": ">=3.6", "summary": "\"WARP loss for Pytorch. WSABIE\"", "version": "0.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>WARP-Pytorch</h1>\n<blockquote>\n<p>An implementation of WARP loss which uses matrixes and stays on the GPU in PyTorch.</p>\n</blockquote>\n<p>An implementation of WARP loss which uses matrixes and stays on the GPU in PyTorch.</p>\n<p>This means instead of using a for-loop to find the first offending negative sample that ranks above our positive,\nwe compute all of them at once. Only later do we find which sample is the first offender, and compute the loss with\nrespect to this sample.</p>\n<p>The advantage is that it can use the speedups that comes with GPU-usage.</p>\n<h2>When is WARP loss advantageous?</h2>\n<p>If you're ranking items or making models for recommendations, it's often advantageous to let your loss function directly\noptimize for this case. WARP loss looks at 1 explicit positive up against the implicit negative items that a user never sampled,\nand allows us to adjust weights of the network accordingly.</p>\n<h2>Install</h2>\n<p><code>pip install warp_loss</code></p>\n<h2>How to use</h2>\n<p>The loss function requires scores for both positive examples, and negative examples to be supplied, such as in the example below.</p>\n<div>\n<div>\n<pre><code>from torch import nn\nimport torch\n\nclass OurModel(nn.Module):\n    def __init__(self, num_labels, emb_dim=10):\n        super(OurModel, self).__init__()\n        self.emb = nn.Embedding(num_labels, emb_dim)\n        self.user_embs = nn.Embedding(1, emb_dim)\n\n    def forward(self, pos, neg):\n        batch_size = neg.size(0)\n        one_user_vector = self.user_embs(torch.zeros(1).long())\n        repeated_user_vector = one_user_vector.repeat((batch_size, 1)).view(batch_size, -1, 1)\n        pos_res = torch.bmm(self.emb(pos), repeated_user_vector).squeeze(2)\n        neg_res = torch.bmm(self.emb(neg), repeated_user_vector).squeeze(2)\n\n        return pos_res, neg_res\n\nnum_labels = 100\nmodel = OurModel(num_labels)\n</code></pre>\n</div>\n</div>\n<div>\n<div>\n<pre><code>pos_labels = torch.randint(high=num_labels, size=(3,1)) # our five labels\nneg_labels = torch.randint(high=num_labels, size=(3,2)) # a few random negatives per positive\n\npos_res, neg_res = model(pos_labels, neg_labels)\nprint('Positive Labels:', pos_labels)\nprint('Negative Labels:', neg_labels)\nprint('Model positive scores:', pos_res)\nprint('Model negative scores:', neg_res)\nloss = warp_loss(pos_res, neg_res, num_labels=num_labels, device=torch.device('cpu'))\nprint('Loss:', loss)\nloss.backward()\n</code></pre>\n</div>\n<div>\n<pre><code>Positive Labels: tensor([[65],\n        [94],\n        [21]])\nNegative Labels: tensor([[ 8, 45],\n        [37, 93],\n        [88, 84]])\nModel positive scores: tensor([[-3.7806],\n        [-1.9974],\n        [-4.1741]], grad_fn=&lt;SqueezeBackward1&gt;)\nModel negative scores: tensor([[-1.5696, -4.4905],\n        [-1.9300, -0.3826],\n        [ 2.4564, -2.1741]], grad_fn=&lt;SqueezeBackward1&gt;)\nLoss: tensor(54.7226, grad_fn=&lt;SumBackward0&gt;)\n</code></pre>\n</div>\n</div>\n<div>\n<div>\n<pre><code>print('We can also see that the gradient is only active for 2x the number of positive labels:', (model.emb.weight.grad.sum(1) != 0).sum().item())\nprint('Meaning we correctly discard the gradients for all other than the offending negative label.')\n</code></pre>\n</div>\n<div>\n<pre><code>We can also see that the gradient is only active for 2x the number of positive labels: 6\nMeaning we correctly discard the gradients for all other than the offending negative label.\n</code></pre>\n</div>\n</div>\n<h2>Assumptions</h2>\n<p>The loss function assumes you have already sampled your negatives randomly.</p>\n<p>As an example this could be done in your dataloader:</p>\n<ol>\n<li>Assume we have a total dataset of 100 items</li>\n<li>Select a positive sample with index 8</li>\n<li>Your negatives should be a random selection from 0-100 excluding 8.</li>\n</ol>\n<p>Ex input to loss function: model scores for pos: [8] neg: [88, 3, 99, 7]</p>\n<p>Currently only tested on PyTorch v0.4</p>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37180.pdf\" rel=\"nofollow\">WSABIE: Scaling Up To Large Vocabulary Image Annotation</a></li>\n<li><a href=\"https://medium.com/@gabrieltseng/intro-to-warp-loss-automatic-differentiation-and-pytorch-b6aa5083187a\" rel=\"nofollow\">Intro to WARP loss - Automatic differentiation and PyTorch</a></li>\n<li><a href=\"https://github.com/lyst/lightfm\" rel=\"nofollow\">LightFM</a> as a reference implementaiton</li>\n</ul>\n\n          </div>"}, "last_serial": 6496326, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "160ca9bb60dc172ed8f8a23908043cfe", "sha256": "b99efa5ebbde617b41a9a4a6cb4917782202c27876099593d9701bf9f0117703"}, "downloads": -1, "filename": "warp_loss-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "160ca9bb60dc172ed8f8a23908043cfe", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 5990, "upload_time": "2020-01-21T21:05:37", "upload_time_iso_8601": "2020-01-21T21:05:37.485723Z", "url": "https://files.pythonhosted.org/packages/66/77/105ff3d78ca5f07568613393695ed8e75e61fd153bed61f23d350d721233/warp_loss-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0ee058ee5c450ffacd8924aa9b4f7f01", "sha256": "789ed5fc6abacbca45f5528a3031cc290eed8aea731c3a61e6f26333257b49a6"}, "downloads": -1, "filename": "warp_loss-0.0.1.tar.gz", "has_sig": false, "md5_digest": "0ee058ee5c450ffacd8924aa9b4f7f01", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 5228, "upload_time": "2020-01-21T21:05:40", "upload_time_iso_8601": "2020-01-21T21:05:40.129875Z", "url": "https://files.pythonhosted.org/packages/d1/d6/18b09de85c9bea4a70c3987866986bbab27f4e9ff758f77fb6441b0b0ff0/warp_loss-0.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "160ca9bb60dc172ed8f8a23908043cfe", "sha256": "b99efa5ebbde617b41a9a4a6cb4917782202c27876099593d9701bf9f0117703"}, "downloads": -1, "filename": "warp_loss-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "160ca9bb60dc172ed8f8a23908043cfe", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 5990, "upload_time": "2020-01-21T21:05:37", "upload_time_iso_8601": "2020-01-21T21:05:37.485723Z", "url": "https://files.pythonhosted.org/packages/66/77/105ff3d78ca5f07568613393695ed8e75e61fd153bed61f23d350d721233/warp_loss-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0ee058ee5c450ffacd8924aa9b4f7f01", "sha256": "789ed5fc6abacbca45f5528a3031cc290eed8aea731c3a61e6f26333257b49a6"}, "downloads": -1, "filename": "warp_loss-0.0.1.tar.gz", "has_sig": false, "md5_digest": "0ee058ee5c450ffacd8924aa9b4f7f01", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 5228, "upload_time": "2020-01-21T21:05:40", "upload_time_iso_8601": "2020-01-21T21:05:40.129875Z", "url": "https://files.pythonhosted.org/packages/d1/d6/18b09de85c9bea4a70c3987866986bbab27f4e9ff758f77fb6441b0b0ff0/warp_loss-0.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:32:22 2020"}