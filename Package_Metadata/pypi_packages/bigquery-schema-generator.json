{"info": {"author": "Brian T. Park", "author_email": "brian@xparks.net", "bugtrack_url": null, "classifiers": [], "description": "BigQuery Schema Generator\n=========================\n\nThis script generates the BigQuery schema from the newline-delimited\ndata records on the STDIN. The records can be in JSON format or CSV\nformat. The BigQuery data importer (``bq load``) uses only the first 100\nlines when the schema auto-detection feature is enabled. In contrast,\nthis script uses all data records to generate the schema.\n\nUsage:\n\n::\n\n    $ generate-schema < file.data.json > file.schema.json\n    $ generate-schema --input_format csv < file.data.csv > file.schema.json\n\nVersion: 1.0 (2020-04-04)\n\nBackground\n----------\n\nData can be imported into\n`BigQuery <https://cloud.google.com/bigquery/>`__ using the\n`bq <https://cloud.google.com/bigquery/bq-command-line-tool>`__ command\nline tool. It accepts a number of data formats including CSV or\nnewline-delimited JSON. The data can be loaded into an existing table or\na new table can be created during the loading process. The structure of\nthe table is defined by its\n`schema <https://cloud.google.com/bigquery/docs/schemas>`__. The table's\nschema can be defined manually or the schema can be\n`auto-detected <https://cloud.google.com/bigquery/docs/schema-detect#auto-detect>`__.\n\nWhen the auto-detect feature is used, the BigQuery data importer\nexamines only the `first 100\nrecords <https://cloud.google.com/bigquery/docs/schema-detect>`__ of the\ninput data. In many cases, this is sufficient because the data records\nwere dumped from another database and the exact schema of the source\ntable was known. However, for data extracted from a service (e.g. using\na REST API) the record fields could have been organically added at later\ndates. In this case, the first 100 records do not contain fields which\nare present in later records. The **bq load** auto-detection fails and\nthe data fails to load.\n\nThe **bq load** tool does not support the ability to process the entire\ndataset to determine a more accurate schema. This script fills in that\ngap. It processes the entire dataset given in the STDIN and outputs the\nBigQuery schema in JSON format on the STDOUT. This schema file can be\nfed back into the **bq load** tool to create a table that is more\ncompatible with the data fields in the input dataset.\n\nInstallation\n------------\n\nInstall from `PyPI <https://pypi.python.org/pypi>`__ repository using\n``pip3``. There are too many ways to install packages in Python. The\nfollowing are in order highest to lowest recommendation:\n\n1) If you are using a virtual environment (such as\n   `venv <https://docs.python.org/3/library/venv.html>`__), then use:\n\n   ::\n\n       $ pip3 install bigquery_schema_generator\n\n2) If you aren't using a virtual environment you can install into your\n   local Python directory:\n\n::\n\n    $ pip3 install --user bigquery_schema_generator\n\n3) If you want to install the package for your entire system globally,\n   use\n\n   ::\n\n       $ sudo -H pip3 install bigquery_schema_generator\n\n   but realize that you will be running code from PyPI as ``root`` so\n   this has security implications.\n\nSometimes, your Python environment gets into a complete mess and the\n``pip3`` command won't work. Try typing ``python3 -m pip`` instead.\n\nA successful install should print out something like the following (the\nversion number may be different):\n\n::\n\n    Collecting bigquery-schema-generator\n    Installing collected packages: bigquery-schema-generator\n    Successfully installed bigquery-schema-generator-0.3.2\n\nThe shell script ``generate-schema`` is installed in the same directory\nas ``pip3``.\n\nUbuntu Linux\n~~~~~~~~~~~~\n\nUnder Ubuntu Linux, you should find the ``generate-schema`` script at\n``/usr/local/bin/generate-schema``.\n\nMacOS\n~~~~~\n\nIf you installed Python from `Python Releases for Mac OS\nX <https://www.python.org/downloads/mac-osx/>`__, then\n``/usr/local/bin/pip3`` is a symlink to\n``/Library/Frameworks/Python.framework/Versions/3.6/bin/pip3``. So\n``generate-schema`` is installed at\n``/Library/Frameworks/Python.framework/Versions/3.6/bin/generate-schema``.\n\nThe Python installer updates ``$HOME/.bash_profile`` to add\n``/Library/Frameworks/Python.framework/Versions/3.6/bin`` to the\n``$PATH`` environment variable. So you should be able to run the\n``generate-schema`` command without typing in the full path.\n\nUsage\n-----\n\nThe ``generate_schema.py`` script accepts a newline-delimited JSON or\nCSV data file on the STDIN. JSON input format has been tested\nextensively. CSV input format was added more recently (in v0.4) using\nthe ``--input_format csv`` flag. The support is not as robust as JSON\nfile. For example, CSV format supports only the comma-separator, and\ndoes not support the pipe (``|``) or tab (``\\t``) character.\n\nUnlike ``bq load``, the ``generate_schema.py`` script reads every record\nin the input data file to deduce the table's schema. It prints the JSON\nformatted schema file on the STDOUT.\n\nThere are at least 3 ways to run this script:\n\n**1) Shell script**\n\nIf you installed using ``pip3``, then it should have installed a small\nhelper script named ``generate-schema`` in your local ``./bin``\ndirectory of your current environment (depending on whether you are\nusing a virtual environment).\n\n::\n\n    $ generate-schema < file.data.json > file.schema.json\n\n**2) Python module**\n\nYou can invoke the module directly using:\n\n::\n\n    $ python3 -m bigquery_schema_generator.generate_schema < file.data.json > file.schema.json\n\nThis is essentially what the ``generate-schema`` command does.\n\n**3) Python script**\n\nIf you retrieved this code from its `GitHub\nrepository <https://github.com/bxparks/bigquery-schema-generator>`__,\nthen you can invoke the Python script directly:\n\n::\n\n    $ ./generate_schema.py < file.data.json > file.schema.json\n\nUsing the Schema Output\n~~~~~~~~~~~~~~~~~~~~~~~\n\nThe resulting schema file can be given to the **bq load** command using\nthe ``--schema`` flag:\n\n::\n\n\n    $ bq load --source_format NEWLINE_DELIMITED_JSON \\\n        --ignore_unknown_values \\\n        --schema file.schema.json \\\n        mydataset.mytable \\\n        file.data.json\n\nwhere ``mydataset.mytable`` is the target table in BigQuery.\n\nFor debugging purposes, here is the equivalent ``bq load`` command using\nschema autodetection:\n\n::\n\n    $ bq load --source_format NEWLINE_DELIMITED_JSON \\\n        --autodetect \\\n        mydataset.mytable \\\n        file.data.json\n\nIf the input file is in CSV format, the first line will be the header\nline which enumerates the names of the columns. But this header line\nmust be skipped when importing the file into the BigQuery table. We\naccomplish this using ``--skip_leading_rows`` flag:\n\n::\n\n    $ bq load --source_format CSV \\\n        --schema file.schema.json \\\n        --skip_leading_rows 1 \\\n        mydataset.mytable \\\n        file.data.csv\n\nHere is the equivalent ``bq load`` command for CSV files using\nautodetection:\n\n::\n\n    $ bq load --source_format CSV \\\n        --autodetect \\\n        mydataset.mytable \\\n        file.data.csv\n\nA useful flag for ``bq load``, particularly for JSON files, is\n``--ignore_unknown_values``, which causes ``bq load`` to ignore fields\nin the input data which are not defined in the schema. When\n``generate_schema.py`` detects an inconsistency in the definition of a\nparticular field in the input data, it removes the field from the schema\ndefinition. Without the ``--ignore_unknown_values``, the ``bq load``\nfails when the inconsistent data record is read.\n\nAnother useful flag during development and debugging is ``--replace``\nwhich replaces any existing BigQuery table.\n\nAfter the BigQuery table is loaded, the schema can be retrieved using:\n\n::\n\n    $ bq show --schema mydataset.mytable | python3 -m json.tool\n\n(The ``python -m json.tool`` command will pretty-print the JSON\nformatted schema file. An alternative is the `jq\ncommand <https://stedolan.github.io/jq/>`__.) The resulting schema file\nshould be identical to ``file.schema.json``.\n\nFlag Options\n~~~~~~~~~~~~\n\nThe ``generate_schema.py`` script supports a handful of command line\nflags as shown by the ``--help`` flag below.\n\nHelp (``--help``)\n^^^^^^^^^^^^^^^^^\n\nPrint the built-in help strings:\n\n::\n\n    $ generate-schema --help\n    usage: generate_schema.py [-h] [--input_format INPUT_FORMAT] [--keep_nulls]\n                              [--quoted_values_are_strings] [--infer_mode]\n                              [--debugging_interval DEBUGGING_INTERVAL]\n                              [--debugging_map] [--sanitize_names]\n\n    Generate BigQuery schema from JSON or CSV file.\n\n    optional arguments:\n      -h, --help            show this help message and exit\n      --input_format INPUT_FORMAT\n                            Specify an alternative input format ('csv', 'json')\n      --keep_nulls          Print the schema for null values, empty arrays or\n                            empty records\n      --quoted_values_are_strings\n                            Quoted values should be interpreted as strings\n      --infer_mode          Determine if mode can be 'NULLABLE' or 'REQUIRED'\n      --debugging_interval DEBUGGING_INTERVAL\n                            Number of lines between heartbeat debugging messages\n      --debugging_map       Print the metadata schema_map instead of the schema\n      --sanitize_names      Forces schema name to comply with BigQuery naming\n                            standard\n\nInput Format (``--input_format``)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSpecifies the format of the input file, either ``json`` (default) or\n``csv``.\n\nIf ``csv`` file is specified, the ``--keep_nulls`` flag is automatically\nactivated. This is required because CSV columns are defined\npositionally, so the schema file must contain all the columns specified\nby the CSV file, in the same order, even if the column contains an empty\nvalue for every record.\n\nSee `Issue\n#26 <https://github.com/bxparks/bigquery-schema-generator/issues/26>`__\nfor implementation details.\n\nKeep Nulls (``--keep_nulls``)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nNormally when the input data file contains a field which has a null,\nempty array or empty record as its value, the field is suppressed in the\nschema file. This flag enables this field to be included in the schema\nfile.\n\nIn other words, using a data file containing just nulls and empty\nvalues:\n\n::\n\n    $ generate_schema\n    { \"s\": null, \"a\": [], \"m\": {} }\n    ^D\n    INFO:root:Processed 1 lines\n    []\n\nWith the ``keep_nulls`` flag, we get:\n\n::\n\n    $ generate-schema --keep_nulls\n    { \"s\": null, \"a\": [], \"m\": {} }\n    ^D\n    INFO:root:Processed 1 lines\n    [\n      {\n        \"mode\": \"REPEATED\",\n        \"type\": \"STRING\",\n        \"name\": \"a\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"fields\": [\n          {\n            \"mode\": \"NULLABLE\",\n            \"type\": \"STRING\",\n            \"name\": \"__unknown__\"\n          }\n        ],\n        \"type\": \"RECORD\",\n        \"name\": \"d\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"type\": \"STRING\",\n        \"name\": \"s\"\n      }\n    ]\n\nQuoted Values Are Strings (``--quoted_values_are_strings``)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBy default, quoted values are inspected to determine if they can be\ninterpreted as ``DATE``, ``TIME``, ``TIMESTAMP``, ``BOOLEAN``,\n``INTEGER`` or ``FLOAT``. This is consistent with the algorithm used by\n``bq load``. However, for the ``BOOLEAN``, ``INTEGER``, or ``FLOAT``\ntypes, it is sometimes more useful to interpret those as normal strings\ninstead. This flag disables type inference for ``BOOLEAN``, ``INTEGER``\nand ``FLOAT`` types inside quoted strings.\n\n::\n\n    $ generate-schema\n    { \"name\": \"1\" }\n    ^D\n    [\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"name\",\n        \"type\": \"INTEGER\"\n      }\n    ]\n\n    $ generate-schema --quoted_values_are_strings\n    { \"name\": \"1\" }\n    ^D\n    [\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"name\",\n        \"type\": \"STRING\"\n      }\n    ]\n\nInfer Mode (``--infer_mode``)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSet the schema ``mode`` of a field to ``REQUIRED`` instead of the\ndefault ``NULLABLE`` if the field contains a non-null or non-empty value\nfor every data record in the input file. This option is available only\nfor CSV (``--input_format csv``) files. It is theoretically possible to\nimplement this feature for JSON files, but too difficult to implement in\npractice because fields are often completely missing from a given JSON\nrecord (instead of explicitly being defined to be ``null``).\n\nSee `Issue\n#28 <https://github.com/bxparks/bigquery-schema-generator/issues/28>`__\nfor implementation details.\n\nDebugging Interval (``--debugging_interval``)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBy default, the ``generate_schema.py`` script prints a short progress\nmessage every 1000 lines of input data. This interval can be changed\nusing the ``--debugging_interval`` flag.\n\n::\n\n    $ generate-schema --debugging_interval 50 < file.data.json > file.schema.json\n\nDebugging Map (``--debugging_map``)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nInstead of printing out the BigQuery schema, the ``--debugging_map``\nprints out the bookkeeping metadata map which is used internally to keep\ntrack of the various fields and their types that were inferred using the\ndata file. This flag is intended to be used for debugging.\n\n::\n\n    $ generate-schema --debugging_map < file.data.json > file.schema.json\n\nSanitize Names (``--sanitize_names``)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBigQuery column names are restricted to certain characters and length.\nWith this flag, column names are sanitizes so that any character outside\nof ASCII letters, numbers and underscore (``[a-zA-Z0-9_]``) are\nconverted to an underscore. (For example \"go&2#there!\" is converted to\n\"go\\_2\\_there\\_\".) Names longer than 128 characters are truncated to\n128.\n\nSchema Types\n------------\n\nSupported Types\n~~~~~~~~~~~~~~~\n\nThe **bq show --schema** command produces a JSON schema file that uses\nthe older `Legacy SQL date\ntypes <https://cloud.google.com/bigquery/data-types>`__. For\ncompatibility, **generate-schema** script will also generate a schema\nfile using the legacy data types.\n\nThe supported types are:\n\n-  ``BOOLEAN``\n-  ``INTEGER``\n-  ``FLOAT``\n-  ``STRING``\n-  ``TIMESTAMP``\n-  ``DATE``\n-  ``TIME``\n-  ``RECORD``\n\nThe ``generate-schema`` script supports both ``NULLABLE`` and\n``REPEATED`` modes of all of the above types.\n\nThe supported format of ``TIMESTAMP`` is as close as practical to the\n`bq load\nformat <https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#timestamp-type>`__:\n\n::\n\n    YYYY-[M]M-[D]D[( |T)[H]H:[M]M:[S]S[.DDDDDD]][time zone]\n\nwhich appears to be an extension of the `ISO 8601\nformat <https://en.wikipedia.org/wiki/ISO_8601>`__. The difference from\n``bq load`` is that the ``[time zone]`` component can be only \\* ``Z``\n\\* ``UTC`` (same as ``Z``) \\* ``(+|-)H[H][:M[M]]``\n\nNote that BigQuery supports up to 6 decimal places after the integer\n'second' component. ``generate-schema`` follows the same restriction for\ncompatibility. If your input file contains more than 6 decimal places,\nyou need to write a data cleansing filter to fix this.\n\nThe suffix ``UTC`` is not standard ISO 8601 nor `documented by\nGoogle <https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#time-zones>`__\nbut the ``UTC`` suffix is used by ``bq extract`` and the web interface.\n(See `Issue\n19 <https://github.com/bxparks/bigquery-schema-generator/issues/19>`__.)\n\nTimezone names from the `tz database <http://www.iana.org/time-zones>`__\n(e.g. \"America/Los\\_Angeles\") are *not* supported by\n``generate-schema``.\n\nThe following types are *not* supported at all:\n\n-  ``BYTES``\n-  ``DATETIME`` (unable to distinguish from ``TIMESTAMP``)\n\nType Inferrence Rules\n~~~~~~~~~~~~~~~~~~~~~\n\nThe ``generate-schema`` script attempts to emulate the various type\nconversion and compatibility rules implemented by **bq load**:\n\n-  ``INTEGER`` can upgrade to ``FLOAT``\n\n   -  if a field in an early record is an ``INTEGER``, but a subsequent\n      record shows this field to have a ``FLOAT`` value, the type of the\n      field will be upgraded to a ``FLOAT``\n   -  the reverse does not happen, once a field is a ``FLOAT``, it will\n      remain a ``FLOAT``\n\n-  conflicting ``TIME``, ``DATE``, ``TIMESTAMP`` types upgrades to\n   ``STRING``\n\n   -  if a field is determined to have one type of \"time\" in one record,\n      then subsequently a different \"time\" type, then the field will be\n      assigned a ``STRING`` type\n\n-  ``NULLABLE RECORD`` can upgrade to a ``REPEATED RECORD``\n\n   -  a field may be defined as ``RECORD`` (aka \"Struct\") type with\n      ``{ ... }``\n   -  if the field is subsequently read as an array with a\n      ``[{ ... }]``, the field is upgraded to a ``REPEATED RECORD``\n\n-  a primitive type (``FLOAT``, ``INTEGER``, ``STRING``) cannot upgrade\n   to a ``REPEATED`` primitive type\n\n   -  there's no technical reason why this cannot be allowed, but **bq\n      load** does not support it, so we follow its behavior\n\n-  a ``DATETIME`` field is always inferred to be a ``TIMESTAMP``\n\n   -  the format of these two fields is identical (in the absence of\n      timezone)\n   -  we follow the same logic as **bq load** and always infer these as\n      ``TIMESTAMP``\n\n-  ``BOOLEAN``, ``INTEGER``, and ``FLOAT`` can appear inside quoted\n   strings\n\n   -  In other words, ``\"true\"`` (or ``\"True\"`` or ``\"false\"``, etc) is\n      considered a BOOLEAN type, ``\"1\"`` is considered an INTEGER type,\n      and ``\"2.1\"`` is considered a FLOAT type. Luigi Mori\n      (jtschichold@) added additional logic to replicate the type\n      conversion logic used by ``bq load`` for these strings.\n   -  This type inference inside quoted strings can be disabled using\n      the ``--quoted_values_are_strings`` flag\n   -  (See `Issue\n      #22 <https://github.com/bxparks/bigquery-schema-generator/issues/22>`__\n      for more details.)\n\n-  ``INTEGER`` values overflowing a 64-bit signed integer upgrade to\n   ``FLOAT``\n\n   -  integers greater than ``2^63-1`` (9223372036854775807)\n   -  integers less than ``-2^63`` (-9223372036854775808)\n   -  (See `Issue\n      #18 <https://github.com/bxparks/bigquery-schema-generator/issues/18>`__\n      for more details)\n\nExamples\n--------\n\nHere is an example of a single JSON data record on the STDIN (the ``^D``\nbelow means typing Control-D, which indicates \"end of file\" under Linux\nand MacOS):\n\n::\n\n    $ generate-schema\n    { \"s\": \"string\", \"b\": true, \"i\": 1, \"x\": 3.1, \"t\": \"2017-05-22T17:10:00-07:00\" }\n    ^D\n    INFO:root:Processed 1 lines\n    [\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"b\",\n        \"type\": \"BOOLEAN\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"i\",\n        \"type\": \"INTEGER\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"s\",\n        \"type\": \"STRING\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"t\",\n        \"type\": \"TIMESTAMP\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"x\",\n        \"type\": \"FLOAT\"\n      }\n    ]\n\nIn most cases, the data file will be stored in a file:\n\n::\n\n    $ cat > file.data.json\n    { \"a\": [1, 2] }\n    { \"i\": 3 }\n    ^D\n\n    $ generate-schema < file.data.json > file.schema.json\n    INFO:root:Processed 2 lines\n\n    $ cat file.schema.json\n    [\n      {\n        \"mode\": \"REPEATED\",\n        \"name\": \"a\",\n        \"type\": \"INTEGER\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"i\",\n        \"type\": \"INTEGER\"\n      }\n    ]\n\nHere is the schema generated from a CSV input file. The first line is\nthe header containing the names of the columns, and the schema lists the\ncolumns in the same order as the header:\n\n::\n\n    $ generate-schema --input_format csv\n    e,b,c,d,a\n    1,x,true,,2.0\n    2,x,,,4\n    3,,,,\n    ^D\n    INFO:root:Processed 3 lines\n    [\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"e\",\n        \"type\": \"INTEGER\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"b\",\n        \"type\": \"STRING\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"c\",\n        \"type\": \"BOOLEAN\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"d\",\n        \"type\": \"STRING\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"a\",\n        \"type\": \"FLOAT\"\n      }\n    ]\n\nHere is an example of the schema generated with the ``--infer_mode``\nflag:\n\n::\n\n    $ generate-schema --input_format csv --infer_mode\n    name,surname,age\n    John\n    Michael,,\n    Maria,Smith,30\n    Joanna,Anders,21\n    ^D\n    INFO:root:Processed 4 lines\n    [\n      {\n        \"mode\": \"REQUIRED\",\n        \"name\": \"name\",\n        \"type\": \"STRING\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"surname\",\n        \"type\": \"STRING\"\n      },\n      {\n        \"mode\": \"NULLABLE\",\n        \"name\": \"age\",\n        \"type\": \"INTEGER\"\n      }\n    ]\n\nUsing As a Library\n------------------\n\nThe ``bigquery_schema_generator`` module can be used as a library by an\nexternal Python client code by creating an instance of\n``SchemaGenerator`` and calling the ``run(input, output)`` method:\n\n.. code:: python\n\n    from bigquery_schema_generator.generate_schema import SchemaGenerator\n\n    generator = SchemaGenerator(\n        input_format=input_format,\n        infer_mode=infer_mode,\n        keep_nulls=keep_nulls,\n        quoted_values_are_strings=quoted_values_are_strings,\n        debugging_interval=debugging_interval,\n        debugging_map=debugging_map)\n    generator.run(input_file, output_file)\n\nIf you need to process the generated schema programmatically, use the\n``deduce_schema()`` method and process the resulting ``schema_map`` and\n``error_log`` data structures like this:\n\n.. code:: python\n\n    from bigquery_schema_generator.generate_schema import SchemaGenerator\n    ...\n    schema_map, error_logs = generator.deduce_schema(input_file)\n\n    for error in error_logs:\n        logging.info(\"Problem on line %s: %s\", error['line'], error['msg'])\n\n    schema = generator.flatten_schema(schema_map)\n    json.dump(schema, output_file, indent=2)\n\nBenchmarks\n----------\n\nI wrote the ``bigquery_schema_generator/anonymize.py`` script to create\nan anonymized data file ``tests/testdata/anon1.data.json.gz``:\n\n::\n\n    $ ./bigquery_schema_generator/anonymize.py < original.data.json \\\n        > anon1.data.json\n    $ gzip anon1.data.json\n\nThis data file is 290MB (5.6MB compressed) with 103080 data records.\n\nGenerating the schema using\n\n::\n\n    $ bigquery_schema_generator/generate_schema.py < anon1.data.json \\\n        > anon1.schema.json\n\ntook 67s on a Dell Precision M4700 laptop with an Intel Core i7-3840QM\nCPU @ 2.80GHz, 32GB of RAM, Ubuntu Linux 18.04, Python 3.6.7.\n\nSystem Requirements\n-------------------\n\nThis project was initially developed on Ubuntu 17.04 using Python 3.5.3,\nbut it now requires Python 3.6 or higher, I think mostly due to the use\nof f-strings.\n\nI have tested it on:\n\n-  Ubuntu 18.04, Python 3.7.7\n-  Ubuntu 18.04, Python 3.6.7\n-  Ubuntu 17.10, Python 3.6.3\n-  MacOS 10.14.2, `Python\n   3.6.4 <https://www.python.org/downloads/release/python-364/>`__\n-  MacOS 10.13.2, `Python\n   3.6.4 <https://www.python.org/downloads/release/python-364/>`__\n\nThe GitHub Actions continuous integration pipeline validates on Python\n3.6, 3.7 and 3.8.\n\nChangelog\n---------\n\nSee `CHANGELOG.md <CHANGELOG.md>`__.\n\nAuthors\n-------\n\n-  Created by Brian T. Park (brian@xparks.net).\n-  Type inference inside quoted strings by Luigi Mori (jtschichold@).\n-  Flag to disable type inference inside quoted strings by Daniel Ecer\n   (de-code@).\n-  Support for CSV files and detection of ``REQUIRED`` fields by Sandor\n   Korotkevics (korotkevics@).\n-  Better support for using ``bigquery_schema_generator`` as a library\n   from an external Python code by StefanoG\\_ITA (StefanoGITA@).\n-  Sanitizing of column names to valid BigQuery characters and length by\n   Jon Warghed (jonwarghed@).\n\nLicense\n-------\n\nApache License 2.0", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/bxparks/bigquery-schema-generator", "keywords": "", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "bigquery-schema-generator", "package_url": "https://pypi.org/project/bigquery-schema-generator/", "platform": "", "project_url": "https://pypi.org/project/bigquery-schema-generator/", "project_urls": {"Homepage": "https://github.com/bxparks/bigquery-schema-generator"}, "release_url": "https://pypi.org/project/bigquery-schema-generator/1.0/", "requires_dist": null, "requires_python": "~=3.6", "summary": "BigQuery schema generator from JSON or CSV data", "version": "1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>This script generates the BigQuery schema from the newline-delimited\ndata records on the STDIN. The records can be in JSON format or CSV\nformat. The BigQuery data importer (<tt>bq load</tt>) uses only the first 100\nlines when the schema auto-detection feature is enabled. In contrast,\nthis script uses all data records to generate the schema.</p>\n<p>Usage:</p>\n<pre>$ generate-schema &lt; file.data.json &gt; file.schema.json\n$ generate-schema --input_format csv &lt; file.data.csv &gt; file.schema.json\n</pre>\n<p>Version: 1.0 (2020-04-04)</p>\n<div id=\"background\">\n<h2>Background</h2>\n<p>Data can be imported into\n<a href=\"https://cloud.google.com/bigquery/\" rel=\"nofollow\">BigQuery</a> using the\n<a href=\"https://cloud.google.com/bigquery/bq-command-line-tool\" rel=\"nofollow\">bq</a> command\nline tool. It accepts a number of data formats including CSV or\nnewline-delimited JSON. The data can be loaded into an existing table or\na new table can be created during the loading process. The structure of\nthe table is defined by its\n<a href=\"https://cloud.google.com/bigquery/docs/schemas\" rel=\"nofollow\">schema</a>. The table\u2019s\nschema can be defined manually or the schema can be\n<a href=\"https://cloud.google.com/bigquery/docs/schema-detect#auto-detect\" rel=\"nofollow\">auto-detected</a>.</p>\n<p>When the auto-detect feature is used, the BigQuery data importer\nexamines only the <a href=\"https://cloud.google.com/bigquery/docs/schema-detect\" rel=\"nofollow\">first 100\nrecords</a> of the\ninput data. In many cases, this is sufficient because the data records\nwere dumped from another database and the exact schema of the source\ntable was known. However, for data extracted from a service (e.g. using\na REST API) the record fields could have been organically added at later\ndates. In this case, the first 100 records do not contain fields which\nare present in later records. The <strong>bq load</strong> auto-detection fails and\nthe data fails to load.</p>\n<p>The <strong>bq load</strong> tool does not support the ability to process the entire\ndataset to determine a more accurate schema. This script fills in that\ngap. It processes the entire dataset given in the STDIN and outputs the\nBigQuery schema in JSON format on the STDOUT. This schema file can be\nfed back into the <strong>bq load</strong> tool to create a table that is more\ncompatible with the data fields in the input dataset.</p>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>Install from <a href=\"https://pypi.python.org/pypi\" rel=\"nofollow\">PyPI</a> repository using\n<tt>pip3</tt>. There are too many ways to install packages in Python. The\nfollowing are in order highest to lowest recommendation:</p>\n<ol>\n<li><p>If you are using a virtual environment (such as\n<a href=\"https://docs.python.org/3/library/venv.html\" rel=\"nofollow\">venv</a>), then use:</p>\n<pre>$ pip3 install bigquery_schema_generator\n</pre>\n</li>\n<li><p>If you aren\u2019t using a virtual environment you can install into your\nlocal Python directory:</p>\n</li>\n</ol>\n<pre>$ pip3 install --user bigquery_schema_generator\n</pre>\n<ol>\n<li><p>If you want to install the package for your entire system globally,\nuse</p>\n<pre>$ sudo -H pip3 install bigquery_schema_generator\n</pre>\n<p>but realize that you will be running code from PyPI as <tt>root</tt> so\nthis has security implications.</p>\n</li>\n</ol>\n<p>Sometimes, your Python environment gets into a complete mess and the\n<tt>pip3</tt> command won\u2019t work. Try typing <tt>python3 <span class=\"pre\">-m</span> pip</tt> instead.</p>\n<p>A successful install should print out something like the following (the\nversion number may be different):</p>\n<pre>Collecting bigquery-schema-generator\nInstalling collected packages: bigquery-schema-generator\nSuccessfully installed bigquery-schema-generator-0.3.2\n</pre>\n<p>The shell script <tt><span class=\"pre\">generate-schema</span></tt> is installed in the same directory\nas <tt>pip3</tt>.</p>\n<div id=\"ubuntu-linux\">\n<h3>Ubuntu Linux</h3>\n<p>Under Ubuntu Linux, you should find the <tt><span class=\"pre\">generate-schema</span></tt> script at\n<tt><span class=\"pre\">/usr/local/bin/generate-schema</span></tt>.</p>\n</div>\n<div id=\"macos\">\n<h3>MacOS</h3>\n<p>If you installed Python from <a href=\"https://www.python.org/downloads/mac-osx/\" rel=\"nofollow\">Python Releases for Mac OS\nX</a>, then\n<tt>/usr/local/bin/pip3</tt> is a symlink to\n<tt>/Library/Frameworks/Python.framework/Versions/3.6/bin/pip3</tt>. So\n<tt><span class=\"pre\">generate-schema</span></tt> is installed at\n<tt><span class=\"pre\">/Library/Frameworks/Python.framework/Versions/3.6/bin/generate-schema</span></tt>.</p>\n<p>The Python installer updates <tt><span class=\"pre\">$HOME/.bash_profile</span></tt> to add\n<tt>/Library/Frameworks/Python.framework/Versions/3.6/bin</tt> to the\n<tt>$PATH</tt> environment variable. So you should be able to run the\n<tt><span class=\"pre\">generate-schema</span></tt> command without typing in the full path.</p>\n</div>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>The <tt>generate_schema.py</tt> script accepts a newline-delimited JSON or\nCSV data file on the STDIN. JSON input format has been tested\nextensively. CSV input format was added more recently (in v0.4) using\nthe <tt><span class=\"pre\">--input_format</span> csv</tt> flag. The support is not as robust as JSON\nfile. For example, CSV format supports only the comma-separator, and\ndoes not support the pipe (<tt>|</tt>) or tab (<tt>\\t</tt>) character.</p>\n<p>Unlike <tt>bq load</tt>, the <tt>generate_schema.py</tt> script reads every record\nin the input data file to deduce the table\u2019s schema. It prints the JSON\nformatted schema file on the STDOUT.</p>\n<p>There are at least 3 ways to run this script:</p>\n<p><strong>1) Shell script</strong></p>\n<p>If you installed using <tt>pip3</tt>, then it should have installed a small\nhelper script named <tt><span class=\"pre\">generate-schema</span></tt> in your local <tt>./bin</tt>\ndirectory of your current environment (depending on whether you are\nusing a virtual environment).</p>\n<pre>$ generate-schema &lt; file.data.json &gt; file.schema.json\n</pre>\n<p><strong>2) Python module</strong></p>\n<p>You can invoke the module directly using:</p>\n<pre>$ python3 -m bigquery_schema_generator.generate_schema &lt; file.data.json &gt; file.schema.json\n</pre>\n<p>This is essentially what the <tt><span class=\"pre\">generate-schema</span></tt> command does.</p>\n<p><strong>3) Python script</strong></p>\n<p>If you retrieved this code from its <a href=\"https://github.com/bxparks/bigquery-schema-generator\" rel=\"nofollow\">GitHub\nrepository</a>,\nthen you can invoke the Python script directly:</p>\n<pre>$ ./generate_schema.py &lt; file.data.json &gt; file.schema.json\n</pre>\n<div id=\"using-the-schema-output\">\n<h3>Using the Schema Output</h3>\n<p>The resulting schema file can be given to the <strong>bq load</strong> command using\nthe <tt><span class=\"pre\">--schema</span></tt> flag:</p>\n<pre>$ bq load --source_format NEWLINE_DELIMITED_JSON \\\n    --ignore_unknown_values \\\n    --schema file.schema.json \\\n    mydataset.mytable \\\n    file.data.json\n</pre>\n<p>where <tt>mydataset.mytable</tt> is the target table in BigQuery.</p>\n<p>For debugging purposes, here is the equivalent <tt>bq load</tt> command using\nschema autodetection:</p>\n<pre>$ bq load --source_format NEWLINE_DELIMITED_JSON \\\n    --autodetect \\\n    mydataset.mytable \\\n    file.data.json\n</pre>\n<p>If the input file is in CSV format, the first line will be the header\nline which enumerates the names of the columns. But this header line\nmust be skipped when importing the file into the BigQuery table. We\naccomplish this using <tt><span class=\"pre\">--skip_leading_rows</span></tt> flag:</p>\n<pre>$ bq load --source_format CSV \\\n    --schema file.schema.json \\\n    --skip_leading_rows 1 \\\n    mydataset.mytable \\\n    file.data.csv\n</pre>\n<p>Here is the equivalent <tt>bq load</tt> command for CSV files using\nautodetection:</p>\n<pre>$ bq load --source_format CSV \\\n    --autodetect \\\n    mydataset.mytable \\\n    file.data.csv\n</pre>\n<p>A useful flag for <tt>bq load</tt>, particularly for JSON files, is\n<tt><span class=\"pre\">--ignore_unknown_values</span></tt>, which causes <tt>bq load</tt> to ignore fields\nin the input data which are not defined in the schema. When\n<tt>generate_schema.py</tt> detects an inconsistency in the definition of a\nparticular field in the input data, it removes the field from the schema\ndefinition. Without the <tt><span class=\"pre\">--ignore_unknown_values</span></tt>, the <tt>bq load</tt>\nfails when the inconsistent data record is read.</p>\n<p>Another useful flag during development and debugging is <tt><span class=\"pre\">--replace</span></tt>\nwhich replaces any existing BigQuery table.</p>\n<p>After the BigQuery table is loaded, the schema can be retrieved using:</p>\n<pre>$ bq show --schema mydataset.mytable | python3 -m json.tool\n</pre>\n<p>(The <tt>python <span class=\"pre\">-m</span> json.tool</tt> command will pretty-print the JSON\nformatted schema file. An alternative is the <a href=\"https://stedolan.github.io/jq/\" rel=\"nofollow\">jq\ncommand</a>.) The resulting schema file\nshould be identical to <tt>file.schema.json</tt>.</p>\n</div>\n<div id=\"flag-options\">\n<h3>Flag Options</h3>\n<p>The <tt>generate_schema.py</tt> script supports a handful of command line\nflags as shown by the <tt><span class=\"pre\">--help</span></tt> flag below.</p>\n<div id=\"help-help\">\n<h4>Help (<tt><span class=\"pre\">--help</span></tt>)</h4>\n<p>Print the built-in help strings:</p>\n<pre>$ generate-schema --help\nusage: generate_schema.py [-h] [--input_format INPUT_FORMAT] [--keep_nulls]\n                          [--quoted_values_are_strings] [--infer_mode]\n                          [--debugging_interval DEBUGGING_INTERVAL]\n                          [--debugging_map] [--sanitize_names]\n\nGenerate BigQuery schema from JSON or CSV file.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --input_format INPUT_FORMAT\n                        Specify an alternative input format ('csv', 'json')\n  --keep_nulls          Print the schema for null values, empty arrays or\n                        empty records\n  --quoted_values_are_strings\n                        Quoted values should be interpreted as strings\n  --infer_mode          Determine if mode can be 'NULLABLE' or 'REQUIRED'\n  --debugging_interval DEBUGGING_INTERVAL\n                        Number of lines between heartbeat debugging messages\n  --debugging_map       Print the metadata schema_map instead of the schema\n  --sanitize_names      Forces schema name to comply with BigQuery naming\n                        standard\n</pre>\n</div>\n<div id=\"input-format-input-format\">\n<h4>Input Format (<tt><span class=\"pre\">--input_format</span></tt>)</h4>\n<p>Specifies the format of the input file, either <tt>json</tt> (default) or\n<tt>csv</tt>.</p>\n<p>If <tt>csv</tt> file is specified, the <tt><span class=\"pre\">--keep_nulls</span></tt> flag is automatically\nactivated. This is required because CSV columns are defined\npositionally, so the schema file must contain all the columns specified\nby the CSV file, in the same order, even if the column contains an empty\nvalue for every record.</p>\n<p>See <a href=\"https://github.com/bxparks/bigquery-schema-generator/issues/26\" rel=\"nofollow\">Issue\n#26</a>\nfor implementation details.</p>\n</div>\n<div id=\"keep-nulls-keep-nulls\">\n<h4>Keep Nulls (<tt><span class=\"pre\">--keep_nulls</span></tt>)</h4>\n<p>Normally when the input data file contains a field which has a null,\nempty array or empty record as its value, the field is suppressed in the\nschema file. This flag enables this field to be included in the schema\nfile.</p>\n<p>In other words, using a data file containing just nulls and empty\nvalues:</p>\n<pre>$ generate_schema\n{ \"s\": null, \"a\": [], \"m\": {} }\n^D\nINFO:root:Processed 1 lines\n[]\n</pre>\n<p>With the <tt>keep_nulls</tt> flag, we get:</p>\n<pre>$ generate-schema --keep_nulls\n{ \"s\": null, \"a\": [], \"m\": {} }\n^D\nINFO:root:Processed 1 lines\n[\n  {\n    \"mode\": \"REPEATED\",\n    \"type\": \"STRING\",\n    \"name\": \"a\"\n  },\n  {\n    \"mode\": \"NULLABLE\",\n    \"fields\": [\n      {\n        \"mode\": \"NULLABLE\",\n        \"type\": \"STRING\",\n        \"name\": \"__unknown__\"\n      }\n    ],\n    \"type\": \"RECORD\",\n    \"name\": \"d\"\n  },\n  {\n    \"mode\": \"NULLABLE\",\n    \"type\": \"STRING\",\n    \"name\": \"s\"\n  }\n]\n</pre>\n</div>\n<div id=\"quoted-values-are-strings-quoted-values-are-strings\">\n<h4>Quoted Values Are Strings (<tt><span class=\"pre\">--quoted_values_are_strings</span></tt>)</h4>\n<p>By default, quoted values are inspected to determine if they can be\ninterpreted as <tt>DATE</tt>, <tt>TIME</tt>, <tt>TIMESTAMP</tt>, <tt>BOOLEAN</tt>,\n<tt>INTEGER</tt> or <tt>FLOAT</tt>. This is consistent with the algorithm used by\n<tt>bq load</tt>. However, for the <tt>BOOLEAN</tt>, <tt>INTEGER</tt>, or <tt>FLOAT</tt>\ntypes, it is sometimes more useful to interpret those as normal strings\ninstead. This flag disables type inference for <tt>BOOLEAN</tt>, <tt>INTEGER</tt>\nand <tt>FLOAT</tt> types inside quoted strings.</p>\n<pre>$ generate-schema\n{ \"name\": \"1\" }\n^D\n[\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"name\",\n    \"type\": \"INTEGER\"\n  }\n]\n\n$ generate-schema --quoted_values_are_strings\n{ \"name\": \"1\" }\n^D\n[\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"name\",\n    \"type\": \"STRING\"\n  }\n]\n</pre>\n</div>\n<div id=\"infer-mode-infer-mode\">\n<h4>Infer Mode (<tt><span class=\"pre\">--infer_mode</span></tt>)</h4>\n<p>Set the schema <tt>mode</tt> of a field to <tt>REQUIRED</tt> instead of the\ndefault <tt>NULLABLE</tt> if the field contains a non-null or non-empty value\nfor every data record in the input file. This option is available only\nfor CSV (<tt><span class=\"pre\">--input_format</span> csv</tt>) files. It is theoretically possible to\nimplement this feature for JSON files, but too difficult to implement in\npractice because fields are often completely missing from a given JSON\nrecord (instead of explicitly being defined to be <tt>null</tt>).</p>\n<p>See <a href=\"https://github.com/bxparks/bigquery-schema-generator/issues/28\" rel=\"nofollow\">Issue\n#28</a>\nfor implementation details.</p>\n</div>\n<div id=\"debugging-interval-debugging-interval\">\n<h4>Debugging Interval (<tt><span class=\"pre\">--debugging_interval</span></tt>)</h4>\n<p>By default, the <tt>generate_schema.py</tt> script prints a short progress\nmessage every 1000 lines of input data. This interval can be changed\nusing the <tt><span class=\"pre\">--debugging_interval</span></tt> flag.</p>\n<pre>$ generate-schema --debugging_interval 50 &lt; file.data.json &gt; file.schema.json\n</pre>\n</div>\n<div id=\"debugging-map-debugging-map\">\n<h4>Debugging Map (<tt><span class=\"pre\">--debugging_map</span></tt>)</h4>\n<p>Instead of printing out the BigQuery schema, the <tt><span class=\"pre\">--debugging_map</span></tt>\nprints out the bookkeeping metadata map which is used internally to keep\ntrack of the various fields and their types that were inferred using the\ndata file. This flag is intended to be used for debugging.</p>\n<pre>$ generate-schema --debugging_map &lt; file.data.json &gt; file.schema.json\n</pre>\n</div>\n<div id=\"sanitize-names-sanitize-names\">\n<h4>Sanitize Names (<tt><span class=\"pre\">--sanitize_names</span></tt>)</h4>\n<p>BigQuery column names are restricted to certain characters and length.\nWith this flag, column names are sanitizes so that any character outside\nof ASCII letters, numbers and underscore (<tt><span class=\"pre\">[a-zA-Z0-9_]</span></tt>) are\nconverted to an underscore. (For example \u201cgo&amp;2#there!\u201d is converted to\n\u201cgo_2_there_\u201d.) Names longer than 128 characters are truncated to\n128.</p>\n</div>\n</div>\n</div>\n<div id=\"schema-types\">\n<h2>Schema Types</h2>\n<div id=\"supported-types\">\n<h3>Supported Types</h3>\n<p>The <strong>bq show \u2013schema</strong> command produces a JSON schema file that uses\nthe older <a href=\"https://cloud.google.com/bigquery/data-types\" rel=\"nofollow\">Legacy SQL date\ntypes</a>. For\ncompatibility, <strong>generate-schema</strong> script will also generate a schema\nfile using the legacy data types.</p>\n<p>The supported types are:</p>\n<ul>\n<li><tt>BOOLEAN</tt></li>\n<li><tt>INTEGER</tt></li>\n<li><tt>FLOAT</tt></li>\n<li><tt>STRING</tt></li>\n<li><tt>TIMESTAMP</tt></li>\n<li><tt>DATE</tt></li>\n<li><tt>TIME</tt></li>\n<li><tt>RECORD</tt></li>\n</ul>\n<p>The <tt><span class=\"pre\">generate-schema</span></tt> script supports both <tt>NULLABLE</tt> and\n<tt>REPEATED</tt> modes of all of the above types.</p>\n<p>The supported format of <tt>TIMESTAMP</tt> is as close as practical to the\n<a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#timestamp-type\" rel=\"nofollow\">bq load\nformat</a>:</p>\n<pre>YYYY-[M]M-[D]D[( |T)[H]H:[M]M:[S]S[.DDDDDD]][time zone]\n</pre>\n<p>which appears to be an extension of the <a href=\"https://en.wikipedia.org/wiki/ISO_8601\" rel=\"nofollow\">ISO 8601\nformat</a>. The difference from\n<tt>bq load</tt> is that the <tt>[time zone]</tt> component can be only * <tt>Z</tt>\n* <tt>UTC</tt> (same as <tt>Z</tt>) * <tt><span class=\"pre\">(+|-)H[H][:M[M]]</span></tt></p>\n<p>Note that BigQuery supports up to 6 decimal places after the integer\n\u2018second\u2019 component. <tt><span class=\"pre\">generate-schema</span></tt> follows the same restriction for\ncompatibility. If your input file contains more than 6 decimal places,\nyou need to write a data cleansing filter to fix this.</p>\n<p>The suffix <tt>UTC</tt> is not standard ISO 8601 nor <a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#time-zones\" rel=\"nofollow\">documented by\nGoogle</a>\nbut the <tt>UTC</tt> suffix is used by <tt>bq extract</tt> and the web interface.\n(See <a href=\"https://github.com/bxparks/bigquery-schema-generator/issues/19\" rel=\"nofollow\">Issue\n19</a>.)</p>\n<p>Timezone names from the <a href=\"http://www.iana.org/time-zones\" rel=\"nofollow\">tz database</a>\n(e.g. \u201cAmerica/Los_Angeles\u201d) are <em>not</em> supported by\n<tt><span class=\"pre\">generate-schema</span></tt>.</p>\n<p>The following types are <em>not</em> supported at all:</p>\n<ul>\n<li><tt>BYTES</tt></li>\n<li><tt>DATETIME</tt> (unable to distinguish from <tt>TIMESTAMP</tt>)</li>\n</ul>\n</div>\n<div id=\"type-inferrence-rules\">\n<h3>Type Inferrence Rules</h3>\n<p>The <tt><span class=\"pre\">generate-schema</span></tt> script attempts to emulate the various type\nconversion and compatibility rules implemented by <strong>bq load</strong>:</p>\n<ul>\n<li><tt>INTEGER</tt> can upgrade to <tt>FLOAT</tt><ul>\n<li>if a field in an early record is an <tt>INTEGER</tt>, but a subsequent\nrecord shows this field to have a <tt>FLOAT</tt> value, the type of the\nfield will be upgraded to a <tt>FLOAT</tt></li>\n<li>the reverse does not happen, once a field is a <tt>FLOAT</tt>, it will\nremain a <tt>FLOAT</tt></li>\n</ul>\n</li>\n<li>conflicting <tt>TIME</tt>, <tt>DATE</tt>, <tt>TIMESTAMP</tt> types upgrades to\n<tt>STRING</tt><ul>\n<li>if a field is determined to have one type of \u201ctime\u201d in one record,\nthen subsequently a different \u201ctime\u201d type, then the field will be\nassigned a <tt>STRING</tt> type</li>\n</ul>\n</li>\n<li><tt>NULLABLE RECORD</tt> can upgrade to a <tt>REPEATED RECORD</tt><ul>\n<li>a field may be defined as <tt>RECORD</tt> (aka \u201cStruct\u201d) type with\n<tt>{ ... }</tt></li>\n<li>if the field is subsequently read as an array with a\n<tt>[{ ... }]</tt>, the field is upgraded to a <tt>REPEATED RECORD</tt></li>\n</ul>\n</li>\n<li>a primitive type (<tt>FLOAT</tt>, <tt>INTEGER</tt>, <tt>STRING</tt>) cannot upgrade\nto a <tt>REPEATED</tt> primitive type<ul>\n<li>there\u2019s no technical reason why this cannot be allowed, but <strong>bq\nload</strong> does not support it, so we follow its behavior</li>\n</ul>\n</li>\n<li>a <tt>DATETIME</tt> field is always inferred to be a <tt>TIMESTAMP</tt><ul>\n<li>the format of these two fields is identical (in the absence of\ntimezone)</li>\n<li>we follow the same logic as <strong>bq load</strong> and always infer these as\n<tt>TIMESTAMP</tt></li>\n</ul>\n</li>\n<li><tt>BOOLEAN</tt>, <tt>INTEGER</tt>, and <tt>FLOAT</tt> can appear inside quoted\nstrings<ul>\n<li>In other words, <tt>\"true\"</tt> (or <tt>\"True\"</tt> or <tt>\"false\"</tt>, etc) is\nconsidered a BOOLEAN type, <tt>\"1\"</tt> is considered an INTEGER type,\nand <tt>\"2.1\"</tt> is considered a FLOAT type. Luigi Mori\n(jtschichold@) added additional logic to replicate the type\nconversion logic used by <tt>bq load</tt> for these strings.</li>\n<li>This type inference inside quoted strings can be disabled using\nthe <tt><span class=\"pre\">--quoted_values_are_strings</span></tt> flag</li>\n<li>(See <a href=\"https://github.com/bxparks/bigquery-schema-generator/issues/22\" rel=\"nofollow\">Issue\n#22</a>\nfor more details.)</li>\n</ul>\n</li>\n<li><tt>INTEGER</tt> values overflowing a 64-bit signed integer upgrade to\n<tt>FLOAT</tt><ul>\n<li>integers greater than <tt><span class=\"pre\">2^63-1</span></tt> (9223372036854775807)</li>\n<li>integers less than <tt><span class=\"pre\">-2^63</span></tt> (-9223372036854775808)</li>\n<li>(See <a href=\"https://github.com/bxparks/bigquery-schema-generator/issues/18\" rel=\"nofollow\">Issue\n#18</a>\nfor more details)</li>\n</ul>\n</li>\n</ul>\n</div>\n</div>\n<div id=\"examples\">\n<h2>Examples</h2>\n<p>Here is an example of a single JSON data record on the STDIN (the <tt>^D</tt>\nbelow means typing Control-D, which indicates \u201cend of file\u201d under Linux\nand MacOS):</p>\n<pre>$ generate-schema\n{ \"s\": \"string\", \"b\": true, \"i\": 1, \"x\": 3.1, \"t\": \"2017-05-22T17:10:00-07:00\" }\n^D\nINFO:root:Processed 1 lines\n[\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"b\",\n    \"type\": \"BOOLEAN\"\n  },\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"i\",\n    \"type\": \"INTEGER\"\n  },\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"s\",\n    \"type\": \"STRING\"\n  },\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"t\",\n    \"type\": \"TIMESTAMP\"\n  },\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"x\",\n    \"type\": \"FLOAT\"\n  }\n]\n</pre>\n<p>In most cases, the data file will be stored in a file:</p>\n<pre>$ cat &gt; file.data.json\n{ \"a\": [1, 2] }\n{ \"i\": 3 }\n^D\n\n$ generate-schema &lt; file.data.json &gt; file.schema.json\nINFO:root:Processed 2 lines\n\n$ cat file.schema.json\n[\n  {\n    \"mode\": \"REPEATED\",\n    \"name\": \"a\",\n    \"type\": \"INTEGER\"\n  },\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"i\",\n    \"type\": \"INTEGER\"\n  }\n]\n</pre>\n<p>Here is the schema generated from a CSV input file. The first line is\nthe header containing the names of the columns, and the schema lists the\ncolumns in the same order as the header:</p>\n<pre>$ generate-schema --input_format csv\ne,b,c,d,a\n1,x,true,,2.0\n2,x,,,4\n3,,,,\n^D\nINFO:root:Processed 3 lines\n[\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"e\",\n    \"type\": \"INTEGER\"\n  },\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"b\",\n    \"type\": \"STRING\"\n  },\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"c\",\n    \"type\": \"BOOLEAN\"\n  },\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"d\",\n    \"type\": \"STRING\"\n  },\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"a\",\n    \"type\": \"FLOAT\"\n  }\n]\n</pre>\n<p>Here is an example of the schema generated with the <tt><span class=\"pre\">--infer_mode</span></tt>\nflag:</p>\n<pre>$ generate-schema --input_format csv --infer_mode\nname,surname,age\nJohn\nMichael,,\nMaria,Smith,30\nJoanna,Anders,21\n^D\nINFO:root:Processed 4 lines\n[\n  {\n    \"mode\": \"REQUIRED\",\n    \"name\": \"name\",\n    \"type\": \"STRING\"\n  },\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"surname\",\n    \"type\": \"STRING\"\n  },\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"age\",\n    \"type\": \"INTEGER\"\n  }\n]\n</pre>\n</div>\n<div id=\"using-as-a-library\">\n<h2>Using As a Library</h2>\n<p>The <tt>bigquery_schema_generator</tt> module can be used as a library by an\nexternal Python client code by creating an instance of\n<tt>SchemaGenerator</tt> and calling the <tt>run(input, output)</tt> method:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">bigquery_schema_generator.generate_schema</span> <span class=\"kn\">import</span> <span class=\"n\">SchemaGenerator</span>\n\n<span class=\"n\">generator</span> <span class=\"o\">=</span> <span class=\"n\">SchemaGenerator</span><span class=\"p\">(</span>\n    <span class=\"n\">input_format</span><span class=\"o\">=</span><span class=\"n\">input_format</span><span class=\"p\">,</span>\n    <span class=\"n\">infer_mode</span><span class=\"o\">=</span><span class=\"n\">infer_mode</span><span class=\"p\">,</span>\n    <span class=\"n\">keep_nulls</span><span class=\"o\">=</span><span class=\"n\">keep_nulls</span><span class=\"p\">,</span>\n    <span class=\"n\">quoted_values_are_strings</span><span class=\"o\">=</span><span class=\"n\">quoted_values_are_strings</span><span class=\"p\">,</span>\n    <span class=\"n\">debugging_interval</span><span class=\"o\">=</span><span class=\"n\">debugging_interval</span><span class=\"p\">,</span>\n    <span class=\"n\">debugging_map</span><span class=\"o\">=</span><span class=\"n\">debugging_map</span><span class=\"p\">)</span>\n<span class=\"n\">generator</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">input_file</span><span class=\"p\">,</span> <span class=\"n\">output_file</span><span class=\"p\">)</span>\n</pre>\n<p>If you need to process the generated schema programmatically, use the\n<tt>deduce_schema()</tt> method and process the resulting <tt>schema_map</tt> and\n<tt>error_log</tt> data structures like this:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">bigquery_schema_generator.generate_schema</span> <span class=\"kn\">import</span> <span class=\"n\">SchemaGenerator</span>\n<span class=\"o\">...</span>\n<span class=\"n\">schema_map</span><span class=\"p\">,</span> <span class=\"n\">error_logs</span> <span class=\"o\">=</span> <span class=\"n\">generator</span><span class=\"o\">.</span><span class=\"n\">deduce_schema</span><span class=\"p\">(</span><span class=\"n\">input_file</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">error</span> <span class=\"ow\">in</span> <span class=\"n\">error_logs</span><span class=\"p\">:</span>\n    <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s2\">\"Problem on line </span><span class=\"si\">%s</span><span class=\"s2\">: </span><span class=\"si\">%s</span><span class=\"s2\">\"</span><span class=\"p\">,</span> <span class=\"n\">error</span><span class=\"p\">[</span><span class=\"s1\">'line'</span><span class=\"p\">],</span> <span class=\"n\">error</span><span class=\"p\">[</span><span class=\"s1\">'msg'</span><span class=\"p\">])</span>\n\n<span class=\"n\">schema</span> <span class=\"o\">=</span> <span class=\"n\">generator</span><span class=\"o\">.</span><span class=\"n\">flatten_schema</span><span class=\"p\">(</span><span class=\"n\">schema_map</span><span class=\"p\">)</span>\n<span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dump</span><span class=\"p\">(</span><span class=\"n\">schema</span><span class=\"p\">,</span> <span class=\"n\">output_file</span><span class=\"p\">,</span> <span class=\"n\">indent</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"benchmarks\">\n<h2>Benchmarks</h2>\n<p>I wrote the <tt>bigquery_schema_generator/anonymize.py</tt> script to create\nan anonymized data file <tt>tests/testdata/anon1.data.json.gz</tt>:</p>\n<pre>$ ./bigquery_schema_generator/anonymize.py &lt; original.data.json \\\n    &gt; anon1.data.json\n$ gzip anon1.data.json\n</pre>\n<p>This data file is 290MB (5.6MB compressed) with 103080 data records.</p>\n<p>Generating the schema using</p>\n<pre>$ bigquery_schema_generator/generate_schema.py &lt; anon1.data.json \\\n    &gt; anon1.schema.json\n</pre>\n<p>took 67s on a Dell Precision M4700 laptop with an Intel Core i7-3840QM\nCPU @ 2.80GHz, 32GB of RAM, Ubuntu Linux 18.04, Python 3.6.7.</p>\n</div>\n<div id=\"system-requirements\">\n<h2>System Requirements</h2>\n<p>This project was initially developed on Ubuntu 17.04 using Python 3.5.3,\nbut it now requires Python 3.6 or higher, I think mostly due to the use\nof f-strings.</p>\n<p>I have tested it on:</p>\n<ul>\n<li>Ubuntu 18.04, Python 3.7.7</li>\n<li>Ubuntu 18.04, Python 3.6.7</li>\n<li>Ubuntu 17.10, Python 3.6.3</li>\n<li>MacOS 10.14.2, <a href=\"https://www.python.org/downloads/release/python-364/\" rel=\"nofollow\">Python\n3.6.4</a></li>\n<li>MacOS 10.13.2, <a href=\"https://www.python.org/downloads/release/python-364/\" rel=\"nofollow\">Python\n3.6.4</a></li>\n</ul>\n<p>The GitHub Actions continuous integration pipeline validates on Python\n3.6, 3.7 and 3.8.</p>\n</div>\n<div id=\"changelog\">\n<h2>Changelog</h2>\n<p>See <a href=\"CHANGELOG.md\" rel=\"nofollow\">CHANGELOG.md</a>.</p>\n</div>\n<div id=\"authors\">\n<h2>Authors</h2>\n<ul>\n<li>Created by Brian T. Park (<a href=\"mailto:brian%40xparks.net\">brian<span>@</span>xparks<span>.</span>net</a>).</li>\n<li>Type inference inside quoted strings by Luigi Mori (jtschichold@).</li>\n<li>Flag to disable type inference inside quoted strings by Daniel Ecer\n(de-code@).</li>\n<li>Support for CSV files and detection of <tt>REQUIRED</tt> fields by Sandor\nKorotkevics (korotkevics@).</li>\n<li>Better support for using <tt>bigquery_schema_generator</tt> as a library\nfrom an external Python code by StefanoG_ITA (StefanoGITA@).</li>\n<li>Sanitizing of column names to valid BigQuery characters and length by\nJon Warghed (jonwarghed@).</li>\n</ul>\n</div>\n<div id=\"license\">\n<h2>License</h2>\n<p>Apache License 2.0</p>\n</div>\n\n          </div>"}, "last_serial": 6952391, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "24cf9cc9d0c5f9f043cb304612367bc2", "sha256": "f72de9d303d89997b7d83f1ecc046ff7d089a76f8ce864e55d80336c1c296eba"}, "downloads": -1, "filename": "bigquery-schema-generator-0.1.tar.gz", "has_sig": false, "md5_digest": "24cf9cc9d0c5f9f043cb304612367bc2", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 8142, "upload_time": "2018-01-02T21:23:28", "upload_time_iso_8601": "2018-01-02T21:23:28.093428Z", "url": "https://files.pythonhosted.org/packages/e1/db/d9a03a7e36a708489a655effd59a3cb5fcc13f8dd2aba74a319d65d7e03a/bigquery-schema-generator-0.1.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "4c1ab0d67aa70ab53cbe32fc757113fc", "sha256": "12174eac7b354136d0aede4845b557116c4a7d6d64d4276f1377219a085ccd0c"}, "downloads": -1, "filename": "bigquery-schema-generator-0.1.1.tar.gz", "has_sig": false, "md5_digest": "4c1ab0d67aa70ab53cbe32fc757113fc", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 8614, "upload_time": "2018-01-03T22:39:39", "upload_time_iso_8601": "2018-01-03T22:39:39.494487Z", "url": "https://files.pythonhosted.org/packages/34/1b/cae35741da8bbd894f78cef64157b091be6477475cf8d78785031f5e1a20/bigquery-schema-generator-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "0ce0f074da4741cb365a3c3dba2bc4a0", "sha256": "4def9de61d384654948a31d80a757fea205038933a36575e773b9fa4b30a34ec"}, "downloads": -1, "filename": "bigquery-schema-generator-0.1.2.tar.gz", "has_sig": false, "md5_digest": "0ce0f074da4741cb365a3c3dba2bc4a0", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 8955, "upload_time": "2018-01-04T22:34:23", "upload_time_iso_8601": "2018-01-04T22:34:23.173963Z", "url": "https://files.pythonhosted.org/packages/d7/82/976a46999464e60371ba72cdb89bd8609e071104208e9196d6e2ae9a4b09/bigquery-schema-generator-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "c7358bd3388833e629ac9270e391df71", "sha256": "6928113821d15f91e30e6c066085fd9afbe39760d6f7100d7427aeabe72b6052"}, "downloads": -1, "filename": "bigquery-schema-generator-0.1.3.tar.gz", "has_sig": false, "md5_digest": "c7358bd3388833e629ac9270e391df71", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 9029, "upload_time": "2018-01-23T20:59:07", "upload_time_iso_8601": "2018-01-23T20:59:07.746303Z", "url": "https://files.pythonhosted.org/packages/6f/73/882c4267dc6052e5268a2e5e90cac7ddc27c3090e5ebccf55b9b6d5c3c0c/bigquery-schema-generator-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "4b6fb6845b185e45fabb29d0e044fa7f", "sha256": "3039e4aa860f1e1d8ffa98f5816ec53aa8885354f5f8e06263ee0d0a9f59d275"}, "downloads": -1, "filename": "bigquery-schema-generator-0.1.4.tar.gz", "has_sig": false, "md5_digest": "4b6fb6845b185e45fabb29d0e044fa7f", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 9074, "upload_time": "2018-01-23T21:21:24", "upload_time_iso_8601": "2018-01-23T21:21:24.977283Z", "url": "https://files.pythonhosted.org/packages/ab/70/e8a09f5c5f293e05d862b99884a8037c29c66ac4852dea2fc33b732f747b/bigquery-schema-generator-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "c7ef625216d89078b4f2507709df2494", "sha256": "98ebe2499d3b46ad9ae375ba78061a0b7fd689c3d8e18e897e92982b7ddfa149"}, "downloads": -1, "filename": "bigquery-schema-generator-0.1.5.tar.gz", "has_sig": false, "md5_digest": "c7ef625216d89078b4f2507709df2494", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 9608, "upload_time": "2018-01-26T07:10:43", "upload_time_iso_8601": "2018-01-26T07:10:43.113456Z", "url": "https://files.pythonhosted.org/packages/4d/07/b11cdcbafde5778bc2e9546487bd4ef83ac625e109c5d2c28a35667efbaf/bigquery-schema-generator-0.1.5.tar.gz", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "a7df0a0ee7e3656ff4e5e85a976bbc40", "sha256": "57e1b1cdea34436c3c48d962fdd3f7d511637b82948aba20914bf4f55239c4e7"}, "downloads": -1, "filename": "bigquery-schema-generator-0.1.6.tar.gz", "has_sig": false, "md5_digest": "a7df0a0ee7e3656ff4e5e85a976bbc40", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 9740, "upload_time": "2018-01-26T16:42:40", "upload_time_iso_8601": "2018-01-26T16:42:40.160952Z", "url": "https://files.pythonhosted.org/packages/dd/aa/477a39a6c5a2bb65b8e71a4c5346be7bb66e115dcb56f55eb35dd258d58b/bigquery-schema-generator-0.1.6.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "e3db9557e604f82eeee46d8fcc431c7a", "sha256": "ef08aecaef20a744f48e032bc051ccaa26b4af9d15922bfb653ca10dbec02e6d"}, "downloads": -1, "filename": "bigquery-schema-generator-0.2.0.tar.gz", "has_sig": false, "md5_digest": "e3db9557e604f82eeee46d8fcc431c7a", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 11141, "upload_time": "2018-02-10T21:33:00", "upload_time_iso_8601": "2018-02-10T21:33:00.257736Z", "url": "https://files.pythonhosted.org/packages/7c/ad/274590eb8dd3b6722be664f2f20228b690c680e4526ca2d6921d608d36af/bigquery-schema-generator-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "1bcb0bde44cf776ad8dac21ba9c5d437", "sha256": "4117cd053d8cf298514ec09280988fc026ff7db97171bba4d21a85f9003df109"}, "downloads": -1, "filename": "bigquery-schema-generator-0.2.1.tar.gz", "has_sig": false, "md5_digest": "1bcb0bde44cf776ad8dac21ba9c5d437", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 13175, "upload_time": "2018-07-18T20:51:30", "upload_time_iso_8601": "2018-07-18T20:51:30.648756Z", "url": "https://files.pythonhosted.org/packages/75/4d/a27f70a0509efe5e6370535aca308c0ec3aaf186962a20976a8b940b9adf/bigquery-schema-generator-0.2.1.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "0cb9d1a6ddaa34cde7c5fdb055de7750", "sha256": "b35505b1fb4e836fb3a4499152f1423fbe6382cd2c9ed1250e7e9222eba7618b"}, "downloads": -1, "filename": "bigquery-schema-generator-0.3.tar.gz", "has_sig": false, "md5_digest": "0cb9d1a6ddaa34cde7c5fdb055de7750", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 15572, "upload_time": "2018-12-17T19:07:59", "upload_time_iso_8601": "2018-12-17T19:07:59.577559Z", "url": "https://files.pythonhosted.org/packages/d2/26/189e153d127ad3fc56c6805514141aea23616719e77d44d59a854087bc46/bigquery-schema-generator-0.3.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "b2c396332181206508282cef0031995d", "sha256": "a4693f19147d0fc1f2ccc2e8b35f5b86444175c8ae3c790a394a394dbb3a59d3"}, "downloads": -1, "filename": "bigquery-schema-generator-0.3.1.tar.gz", "has_sig": false, "md5_digest": "b2c396332181206508282cef0031995d", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 19262, "upload_time": "2019-01-18T17:39:32", "upload_time_iso_8601": "2019-01-18T17:39:32.152508Z", "url": "https://files.pythonhosted.org/packages/74/9b/0dab5d57fd13cbe955d3519f312f822712df36c14b40add9e88d8643cc7c/bigquery-schema-generator-0.3.1.tar.gz", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "e5b5a4bbbafa81e0ca4ce0b09f6b6d0d", "sha256": "1ca6a7f85eb7757b900b6766ad4a2ca915fad9cbba66f6ce48819ba33df61028"}, "downloads": -1, "filename": "bigquery-schema-generator-0.3.2.tar.gz", "has_sig": false, "md5_digest": "e5b5a4bbbafa81e0ca4ce0b09f6b6d0d", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 17222, "upload_time": "2019-02-24T22:55:51", "upload_time_iso_8601": "2019-02-24T22:55:51.394686Z", "url": "https://files.pythonhosted.org/packages/04/c3/9a14e42b03c9b397c364275355de61c550b399c6a38348c6c956076410ac/bigquery-schema-generator-0.3.2.tar.gz", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "078e8f64be76d8acf17461111d67e5b6", "sha256": "b0ac6b9c7c2d24e927e908b2f9c42da71bd6434a38f004069e59427e05bf7a3d"}, "downloads": -1, "filename": "bigquery-schema-generator-0.4.tar.gz", "has_sig": false, "md5_digest": "078e8f64be76d8acf17461111d67e5b6", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 25802, "upload_time": "2019-03-06T19:00:31", "upload_time_iso_8601": "2019-03-06T19:00:31.313155Z", "url": "https://files.pythonhosted.org/packages/7a/f7/bb78c092db1e1c6f061d99e9e23195c4090ca1f451fee3445478ce562899/bigquery-schema-generator-0.4.tar.gz", "yanked": false}], "0.5": [{"comment_text": "", "digests": {"md5": "cdfb14eea068bb0b99d0ae624475634a", "sha256": "7025f9b32ec93e6a8be4351cc5229a785c9fd275ea4c46a8a8cb6fb2d82a313a"}, "downloads": -1, "filename": "bigquery-schema-generator-0.5.tar.gz", "has_sig": false, "md5_digest": "cdfb14eea068bb0b99d0ae624475634a", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 24566, "upload_time": "2019-06-06T18:23:33", "upload_time_iso_8601": "2019-06-06T18:23:33.975318Z", "url": "https://files.pythonhosted.org/packages/97/f7/157fa13d44fce29b8c727afc35a19cbc0b2b5d2c5afe34f55f8901d6be29/bigquery-schema-generator-0.5.tar.gz", "yanked": false}], "0.5.1": [{"comment_text": "", "digests": {"md5": "1022895b5c8a4150e0e743ff420f434e", "sha256": "bf02e747e7dfa7a393d1c5e981a3258c36dbda21998bbc4fc6020219190f1fca"}, "downloads": -1, "filename": "bigquery-schema-generator-0.5.1.tar.gz", "has_sig": false, "md5_digest": "1022895b5c8a4150e0e743ff420f434e", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.5", "size": 25356, "upload_time": "2019-06-17T15:06:05", "upload_time_iso_8601": "2019-06-17T15:06:05.935685Z", "url": "https://files.pythonhosted.org/packages/55/ae/a9b8a6ecc0c8224536b7ac3538522f1848c017268e79f5e3206026c64775/bigquery-schema-generator-0.5.1.tar.gz", "yanked": false}], "1.0": [{"comment_text": "", "digests": {"md5": "d2aa7108151e3abc65f207834337ecbf", "sha256": "68e1566a7181484270faf83f1fc839c4ee46272252e6a5f010de75b4f240e927"}, "downloads": -1, "filename": "bigquery-schema-generator-1.0.tar.gz", "has_sig": false, "md5_digest": "d2aa7108151e3abc65f207834337ecbf", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.6", "size": 28749, "upload_time": "2020-04-04T19:35:51", "upload_time_iso_8601": "2020-04-04T19:35:51.014367Z", "url": "https://files.pythonhosted.org/packages/45/3e/5b3364aa3c8766cd1ffe36638157480c1f1f2603ab9abe82252bef343387/bigquery-schema-generator-1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d2aa7108151e3abc65f207834337ecbf", "sha256": "68e1566a7181484270faf83f1fc839c4ee46272252e6a5f010de75b4f240e927"}, "downloads": -1, "filename": "bigquery-schema-generator-1.0.tar.gz", "has_sig": false, "md5_digest": "d2aa7108151e3abc65f207834337ecbf", "packagetype": "sdist", "python_version": "source", "requires_python": "~=3.6", "size": 28749, "upload_time": "2020-04-04T19:35:51", "upload_time_iso_8601": "2020-04-04T19:35:51.014367Z", "url": "https://files.pythonhosted.org/packages/45/3e/5b3364aa3c8766cd1ffe36638157480c1f1f2603ab9abe82252bef343387/bigquery-schema-generator-1.0.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:37:33 2020"}