{"info": {"author": "Alexandre Gattiker", "author_email": "algattik@microsoft.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# databricks_test\n\n## About\n\nAn experimental unit test framework for Databricks notebooks.\n\n_This open-source project is not developed by nor affiliated with Databricks._\n\n## Installing\n\n```\npip install databricks_test\n```\n\n## Usage\n\nAdd a cell at the beginning of your Databricks notebook:\n\n```python\n# Instrument for unit tests. This is only executed in local unit tests, not in Databricks.\nif 'dbutils' not in locals():\n    import databricks_test\n    databricks_test.inject_variables()\n```\n\nThe `if` clause causes the inner code to be skipped when run in Databricks.\nTherefore there is no need to install the `databricks_test` module on your Databricks environment.\n\nAdd your notebook into a code project, for example using [GitHub version control in Azure Databricks](https://docs.microsoft.com/en-us/azure/databricks/notebooks/azure-devops-services-version-control).\n\nSet up pytest in your code project (outside of Databricks).\n\nCreate a test case with the following structure:\n\n```python\nimport databricks_test\n\ndef test_method():\n    with databricks_test.session() as dbrickstest:\n\n        # Set up mocks on dbrickstest\n        # ...\n\n        # Run notebook\n        dbrickstest.run_notebook(\"notebook_dir\", \"notebook_name_without_py_suffix\")\n\n        # Test assertions\n        # ...\n```\n\nYou can set up [mocks](https://docs.python.org/dev/library/unittest.mock.html) on `dbrickstest`, for example:\n\n```python\ndbrickstest.dbutils.widgets.get.return_value = \"myvalue\"\n```\n\nSee samples below for more examples.\n\n## Supported features\n\n* Spark context injected into Databricks notebooks: `spark`, `table`, `sql` etc.\n* PySpark with all Spark features including reading and writing to disk, UDFs and Pandas UDFs\n* Databricks Utilities (`dbutils`, `display`) with user-configurable mocks\n* Mocking connectors such as Azure Storage, S3 and SQL Data Warehouse\n\n## Unsupported features\n\n* Notebook formats other than `.py` (`.ipynb`, `.dbc`) are not supported\n* Non-python cells such as `%scala` and `%sql` (those cells are skipped, as they are stored in `.py` notebooks as comments)\n* Writing directly to `/dbfs` mount on local filesystem:\n  write to a local temporary file instead and use dbutils.fs.cp() to copy to DBFS, which you can intercept with a mock\n* Databricks extensions to Spark such as `spark.read.format(\"binaryFile\")`\n\n## Sample test\n\nSample test case for an ETL notebook reading CSV and writing Parquet.\n\n```python\nimport pandas as pd\nimport databricks_test\nfrom tempfile import TemporaryDirectory\n\nfrom pandas.testing import assert_frame_equal\n\ndef test_etl():\n    with databricks_test.session() as dbrickstest:\n        with TemporaryDirectory() as tmp_dir:\n            out_dir = f\"{tmp_dir}/out\"\n\n            # Provide input and output location as widgets to notebook\n            switch = {\n                \"input\": \"tests/etl_input.csv\",\n                \"output\": out_dir,\n            }\n            dbrickstest.dbutils.widgets.get.side_effect = lambda x: switch.get(\n                x, \"\")\n\n            # Run notebook\n            dbrickstest.run_notebook(\".\", \"etl_notebook\")\n\n            # Notebook produces a Parquet file (directory)\n            resultDF = pd.read_parquet(out_dir)\n\n        # Compare produced Parquet file and expected CSV file\n        expectedDF = pd.read_csv(\"tests/etl_expected.csv\")\n        assert_frame_equal(expectedDF, resultDF, check_dtype=False)\n```\n\nIn the notebook, we pass parameters using widgets.\nThis makes it easy to pass\na local file location in tests, and a remote URL (such as Azure Storage or S3)\nin production.\n\n```python\n# Databricks notebook source\n# This notebook processed the training dataset (imported by Data Factory)\n# and computes a cleaned dataset with additional features such as city.\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DoubleType, IntegerType\nfrom pyspark.sql.functions import col, pandas_udf, PandasUDFType\n\n# COMMAND ----------\n\n# Instrument for unit tests. This is only executed in local unit tests, not in Databricks.\nif 'dbutils' not in locals():\n    import databricks_test\n    databricks_test.inject_variables()\n\n# COMMAND ----------\n\n# Widgets for interactive development.\ndbutils.widgets.text(\"input\", \"\")\ndbutils.widgets.text(\"output\", \"\")\ndbutils.widgets.text(\"secretscope\", \"\")\ndbutils.widgets.text(\"secretname\", \"\")\ndbutils.widgets.text(\"keyname\", \"\")\n\n# COMMAND ----------\n\n# Set up storage credentials\n\nspark.conf.set(\n    dbutils.widgets.get(\"keyname\"),\n    dbutils.secrets.get(\n        scope=dbutils.widgets.get(\"secretscope\"),\n        key=dbutils.widgets.get(\"secretname\")\n    ),\n)\n\n# COMMAND ----------\n\n# Import CSV files\nschema = StructType(\n    [\n        StructField(\"aDouble\", DoubleType(), nullable=False),\n        StructField(\"anInteger\", IntegerType(), nullable=False),\n    ]\n)\n\ndf = (\n    spark.read.format(\"csv\")\n    .options(header=\"true\", mode=\"FAILFAST\")\n    .schema(schema)\n    .load(dbutils.widgets.get('input'))\n)\ndisplay(df)\n\n# COMMAND ----------\n\ndf.count()\n\n# COMMAND ----------\n\n# Inputs and output are pandas.Series of doubles\n@pandas_udf('integer', PandasUDFType.SCALAR)\ndef square(x):\n    return x * x\n\n\n# COMMAND ----------\n\n# Write out Parquet data\n(df\n    .withColumn(\"aSquaredInteger\", square(col(\"anInteger\")))\n    .write\n    .parquet(dbutils.widgets.get('output'))\n )\n```\n\n\n## Advanced mocking\n\nSample test case mocking PySpark classes for a notebook connecting to Azure SQL Data Warehouse.\n\n```python\nimport databricks_test\nimport pyspark\nimport pyspark.sql.functions as F\nfrom tempfile import TemporaryDirectory\nfrom pandas.testing import assert_frame_equal\nimport pandas as pd\n\n\ndef test_sqldw(monkeypatch):\n    with databricks_test.session() as dbrickstest, TemporaryDirectory() as tmp:\n\n        out_dir = f\"{tmp}/out\"\n\n        # Mock SQL DW loader, creating a Spark DataFrame instead\n        def mock_load(reader):\n            return (\n                dbrickstest.spark\n                .range(10)\n                .withColumn(\"age\", F.col(\"id\") * 6)\n                .withColumn(\"salary\", F.col(\"id\") * 10000)\n            )\n\n        monkeypatch.setattr(\n            pyspark.sql.readwriter.DataFrameReader, \"load\", mock_load)\n\n        # Mock SQL DW writer, writing to a local Parquet file instead\n        def mock_save(writer):\n            monkeypatch.undo()\n            writer.format(\"parquet\")\n            writer.save(out_dir)\n\n        monkeypatch.setattr(\n            pyspark.sql.readwriter.DataFrameWriter, \"save\", mock_save)\n\n        # Run notebook\n        dbrickstest.run_notebook(\".\", \"sqldw_notebook\")\n\n        # Notebook produces a Parquet file (directory)\n        resultDF = pd.read_parquet(out_dir)\n\n        # Compare produced Parquet file and expected CSV file\n        expectedDF = pd.read_csv(\"tests/sqldw_expected.csv\")\n        assert_frame_equal(expectedDF, resultDF, check_dtype=False)\n```\n\n## Issues\n\nPlease report issues at [https://github.com/microsoft/DataOps/issues](https://github.com/microsoft/DataOps/issues).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/microsoft/DataOps", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "databricks-test", "package_url": "https://pypi.org/project/databricks-test/", "platform": "", "project_url": "https://pypi.org/project/databricks-test/", "project_urls": {"Homepage": "https://github.com/microsoft/DataOps"}, "release_url": "https://pypi.org/project/databricks-test/0.0.4/", "requires_dist": null, "requires_python": "", "summary": "Unit testing and mocking for Databricks", "version": "0.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>databricks_test</h1>\n<h2>About</h2>\n<p>An experimental unit test framework for Databricks notebooks.</p>\n<p><em>This open-source project is not developed by nor affiliated with Databricks.</em></p>\n<h2>Installing</h2>\n<pre><code>pip install databricks_test\n</code></pre>\n<h2>Usage</h2>\n<p>Add a cell at the beginning of your Databricks notebook:</p>\n<pre><span class=\"c1\"># Instrument for unit tests. This is only executed in local unit tests, not in Databricks.</span>\n<span class=\"k\">if</span> <span class=\"s1\">'dbutils'</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"nb\">locals</span><span class=\"p\">():</span>\n    <span class=\"kn\">import</span> <span class=\"nn\">databricks_test</span>\n    <span class=\"n\">databricks_test</span><span class=\"o\">.</span><span class=\"n\">inject_variables</span><span class=\"p\">()</span>\n</pre>\n<p>The <code>if</code> clause causes the inner code to be skipped when run in Databricks.\nTherefore there is no need to install the <code>databricks_test</code> module on your Databricks environment.</p>\n<p>Add your notebook into a code project, for example using <a href=\"https://docs.microsoft.com/en-us/azure/databricks/notebooks/azure-devops-services-version-control\" rel=\"nofollow\">GitHub version control in Azure Databricks</a>.</p>\n<p>Set up pytest in your code project (outside of Databricks).</p>\n<p>Create a test case with the following structure:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">databricks_test</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">test_method</span><span class=\"p\">():</span>\n    <span class=\"k\">with</span> <span class=\"n\">databricks_test</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">dbrickstest</span><span class=\"p\">:</span>\n\n        <span class=\"c1\"># Set up mocks on dbrickstest</span>\n        <span class=\"c1\"># ...</span>\n\n        <span class=\"c1\"># Run notebook</span>\n        <span class=\"n\">dbrickstest</span><span class=\"o\">.</span><span class=\"n\">run_notebook</span><span class=\"p\">(</span><span class=\"s2\">\"notebook_dir\"</span><span class=\"p\">,</span> <span class=\"s2\">\"notebook_name_without_py_suffix\"</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Test assertions</span>\n        <span class=\"c1\"># ...</span>\n</pre>\n<p>You can set up <a href=\"https://docs.python.org/dev/library/unittest.mock.html\" rel=\"nofollow\">mocks</a> on <code>dbrickstest</code>, for example:</p>\n<pre><span class=\"n\">dbrickstest</span><span class=\"o\">.</span><span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">widgets</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"o\">.</span><span class=\"n\">return_value</span> <span class=\"o\">=</span> <span class=\"s2\">\"myvalue\"</span>\n</pre>\n<p>See samples below for more examples.</p>\n<h2>Supported features</h2>\n<ul>\n<li>Spark context injected into Databricks notebooks: <code>spark</code>, <code>table</code>, <code>sql</code> etc.</li>\n<li>PySpark with all Spark features including reading and writing to disk, UDFs and Pandas UDFs</li>\n<li>Databricks Utilities (<code>dbutils</code>, <code>display</code>) with user-configurable mocks</li>\n<li>Mocking connectors such as Azure Storage, S3 and SQL Data Warehouse</li>\n</ul>\n<h2>Unsupported features</h2>\n<ul>\n<li>Notebook formats other than <code>.py</code> (<code>.ipynb</code>, <code>.dbc</code>) are not supported</li>\n<li>Non-python cells such as <code>%scala</code> and <code>%sql</code> (those cells are skipped, as they are stored in <code>.py</code> notebooks as comments)</li>\n<li>Writing directly to <code>/dbfs</code> mount on local filesystem:\nwrite to a local temporary file instead and use dbutils.fs.cp() to copy to DBFS, which you can intercept with a mock</li>\n<li>Databricks extensions to Spark such as <code>spark.read.format(\"binaryFile\")</code></li>\n</ul>\n<h2>Sample test</h2>\n<p>Sample test case for an ETL notebook reading CSV and writing Parquet.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">databricks_test</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tempfile</span> <span class=\"kn\">import</span> <span class=\"n\">TemporaryDirectory</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">pandas.testing</span> <span class=\"kn\">import</span> <span class=\"n\">assert_frame_equal</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">test_etl</span><span class=\"p\">():</span>\n    <span class=\"k\">with</span> <span class=\"n\">databricks_test</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">dbrickstest</span><span class=\"p\">:</span>\n        <span class=\"k\">with</span> <span class=\"n\">TemporaryDirectory</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">tmp_dir</span><span class=\"p\">:</span>\n            <span class=\"n\">out_dir</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s2\">\"</span><span class=\"si\">{</span><span class=\"n\">tmp_dir</span><span class=\"si\">}</span><span class=\"s2\">/out\"</span>\n\n            <span class=\"c1\"># Provide input and output location as widgets to notebook</span>\n            <span class=\"n\">switch</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n                <span class=\"s2\">\"input\"</span><span class=\"p\">:</span> <span class=\"s2\">\"tests/etl_input.csv\"</span><span class=\"p\">,</span>\n                <span class=\"s2\">\"output\"</span><span class=\"p\">:</span> <span class=\"n\">out_dir</span><span class=\"p\">,</span>\n            <span class=\"p\">}</span>\n            <span class=\"n\">dbrickstest</span><span class=\"o\">.</span><span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">widgets</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"o\">.</span><span class=\"n\">side_effect</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">switch</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span>\n                <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s2\">\"\"</span><span class=\"p\">)</span>\n\n            <span class=\"c1\"># Run notebook</span>\n            <span class=\"n\">dbrickstest</span><span class=\"o\">.</span><span class=\"n\">run_notebook</span><span class=\"p\">(</span><span class=\"s2\">\".\"</span><span class=\"p\">,</span> <span class=\"s2\">\"etl_notebook\"</span><span class=\"p\">)</span>\n\n            <span class=\"c1\"># Notebook produces a Parquet file (directory)</span>\n            <span class=\"n\">resultDF</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_parquet</span><span class=\"p\">(</span><span class=\"n\">out_dir</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Compare produced Parquet file and expected CSV file</span>\n        <span class=\"n\">expectedDF</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s2\">\"tests/etl_expected.csv\"</span><span class=\"p\">)</span>\n        <span class=\"n\">assert_frame_equal</span><span class=\"p\">(</span><span class=\"n\">expectedDF</span><span class=\"p\">,</span> <span class=\"n\">resultDF</span><span class=\"p\">,</span> <span class=\"n\">check_dtype</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n</pre>\n<p>In the notebook, we pass parameters using widgets.\nThis makes it easy to pass\na local file location in tests, and a remote URL (such as Azure Storage or S3)\nin production.</p>\n<pre><span class=\"c1\"># Databricks notebook source</span>\n<span class=\"c1\"># This notebook processed the training dataset (imported by Data Factory)</span>\n<span class=\"c1\"># and computes a cleaned dataset with additional features such as city.</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.types</span> <span class=\"kn\">import</span> <span class=\"n\">StructType</span><span class=\"p\">,</span> <span class=\"n\">StructField</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.types</span> <span class=\"kn\">import</span> <span class=\"n\">DoubleType</span><span class=\"p\">,</span> <span class=\"n\">IntegerType</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.functions</span> <span class=\"kn\">import</span> <span class=\"n\">col</span><span class=\"p\">,</span> <span class=\"n\">pandas_udf</span><span class=\"p\">,</span> <span class=\"n\">PandasUDFType</span>\n\n<span class=\"c1\"># COMMAND ----------</span>\n\n<span class=\"c1\"># Instrument for unit tests. This is only executed in local unit tests, not in Databricks.</span>\n<span class=\"k\">if</span> <span class=\"s1\">'dbutils'</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"nb\">locals</span><span class=\"p\">():</span>\n    <span class=\"kn\">import</span> <span class=\"nn\">databricks_test</span>\n    <span class=\"n\">databricks_test</span><span class=\"o\">.</span><span class=\"n\">inject_variables</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># COMMAND ----------</span>\n\n<span class=\"c1\"># Widgets for interactive development.</span>\n<span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">widgets</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">(</span><span class=\"s2\">\"input\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\"</span><span class=\"p\">)</span>\n<span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">widgets</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">(</span><span class=\"s2\">\"output\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\"</span><span class=\"p\">)</span>\n<span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">widgets</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">(</span><span class=\"s2\">\"secretscope\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\"</span><span class=\"p\">)</span>\n<span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">widgets</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">(</span><span class=\"s2\">\"secretname\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\"</span><span class=\"p\">)</span>\n<span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">widgets</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">(</span><span class=\"s2\">\"keyname\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># COMMAND ----------</span>\n\n<span class=\"c1\"># Set up storage credentials</span>\n\n<span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">conf</span><span class=\"o\">.</span><span class=\"n\">set</span><span class=\"p\">(</span>\n    <span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">widgets</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">\"keyname\"</span><span class=\"p\">),</span>\n    <span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">secrets</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span>\n        <span class=\"n\">scope</span><span class=\"o\">=</span><span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">widgets</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">\"secretscope\"</span><span class=\"p\">),</span>\n        <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">widgets</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">\"secretname\"</span><span class=\"p\">)</span>\n    <span class=\"p\">),</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># COMMAND ----------</span>\n\n<span class=\"c1\"># Import CSV files</span>\n<span class=\"n\">schema</span> <span class=\"o\">=</span> <span class=\"n\">StructType</span><span class=\"p\">(</span>\n    <span class=\"p\">[</span>\n        <span class=\"n\">StructField</span><span class=\"p\">(</span><span class=\"s2\">\"aDouble\"</span><span class=\"p\">,</span> <span class=\"n\">DoubleType</span><span class=\"p\">(),</span> <span class=\"n\">nullable</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">),</span>\n        <span class=\"n\">StructField</span><span class=\"p\">(</span><span class=\"s2\">\"anInteger\"</span><span class=\"p\">,</span> <span class=\"n\">IntegerType</span><span class=\"p\">(),</span> <span class=\"n\">nullable</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">),</span>\n    <span class=\"p\">]</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"s2\">\"csv\"</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">options</span><span class=\"p\">(</span><span class=\"n\">header</span><span class=\"o\">=</span><span class=\"s2\">\"true\"</span><span class=\"p\">,</span> <span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s2\">\"FAILFAST\"</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"p\">(</span><span class=\"n\">schema</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">widgets</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'input'</span><span class=\"p\">))</span>\n<span class=\"p\">)</span>\n<span class=\"n\">display</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># COMMAND ----------</span>\n\n<span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># COMMAND ----------</span>\n\n<span class=\"c1\"># Inputs and output are pandas.Series of doubles</span>\n<span class=\"nd\">@pandas_udf</span><span class=\"p\">(</span><span class=\"s1\">'integer'</span><span class=\"p\">,</span> <span class=\"n\">PandasUDFType</span><span class=\"o\">.</span><span class=\"n\">SCALAR</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">square</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">x</span> <span class=\"o\">*</span> <span class=\"n\">x</span>\n\n\n<span class=\"c1\"># COMMAND ----------</span>\n\n<span class=\"c1\"># Write out Parquet data</span>\n<span class=\"p\">(</span><span class=\"n\">df</span>\n    <span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"p\">(</span><span class=\"s2\">\"aSquaredInteger\"</span><span class=\"p\">,</span> <span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">col</span><span class=\"p\">(</span><span class=\"s2\">\"anInteger\"</span><span class=\"p\">)))</span>\n    <span class=\"o\">.</span><span class=\"n\">write</span>\n    <span class=\"o\">.</span><span class=\"n\">parquet</span><span class=\"p\">(</span><span class=\"n\">dbutils</span><span class=\"o\">.</span><span class=\"n\">widgets</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'output'</span><span class=\"p\">))</span>\n <span class=\"p\">)</span>\n</pre>\n<h2>Advanced mocking</h2>\n<p>Sample test case mocking PySpark classes for a notebook connecting to Azure SQL Data Warehouse.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">databricks_test</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pyspark</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pyspark.sql.functions</span> <span class=\"k\">as</span> <span class=\"nn\">F</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tempfile</span> <span class=\"kn\">import</span> <span class=\"n\">TemporaryDirectory</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pandas.testing</span> <span class=\"kn\">import</span> <span class=\"n\">assert_frame_equal</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">test_sqldw</span><span class=\"p\">(</span><span class=\"n\">monkeypatch</span><span class=\"p\">):</span>\n    <span class=\"k\">with</span> <span class=\"n\">databricks_test</span><span class=\"o\">.</span><span class=\"n\">session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">dbrickstest</span><span class=\"p\">,</span> <span class=\"n\">TemporaryDirectory</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">tmp</span><span class=\"p\">:</span>\n\n        <span class=\"n\">out_dir</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s2\">\"</span><span class=\"si\">{</span><span class=\"n\">tmp</span><span class=\"si\">}</span><span class=\"s2\">/out\"</span>\n\n        <span class=\"c1\"># Mock SQL DW loader, creating a Spark DataFrame instead</span>\n        <span class=\"k\">def</span> <span class=\"nf\">mock_load</span><span class=\"p\">(</span><span class=\"n\">reader</span><span class=\"p\">):</span>\n            <span class=\"k\">return</span> <span class=\"p\">(</span>\n                <span class=\"n\">dbrickstest</span><span class=\"o\">.</span><span class=\"n\">spark</span>\n                <span class=\"o\">.</span><span class=\"n\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n                <span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"p\">(</span><span class=\"s2\">\"age\"</span><span class=\"p\">,</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">col</span><span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">6</span><span class=\"p\">)</span>\n                <span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"p\">(</span><span class=\"s2\">\"salary\"</span><span class=\"p\">,</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">col</span><span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">10000</span><span class=\"p\">)</span>\n            <span class=\"p\">)</span>\n\n        <span class=\"n\">monkeypatch</span><span class=\"o\">.</span><span class=\"n\">setattr</span><span class=\"p\">(</span>\n            <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">readwriter</span><span class=\"o\">.</span><span class=\"n\">DataFrameReader</span><span class=\"p\">,</span> <span class=\"s2\">\"load\"</span><span class=\"p\">,</span> <span class=\"n\">mock_load</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Mock SQL DW writer, writing to a local Parquet file instead</span>\n        <span class=\"k\">def</span> <span class=\"nf\">mock_save</span><span class=\"p\">(</span><span class=\"n\">writer</span><span class=\"p\">):</span>\n            <span class=\"n\">monkeypatch</span><span class=\"o\">.</span><span class=\"n\">undo</span><span class=\"p\">()</span>\n            <span class=\"n\">writer</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"s2\">\"parquet\"</span><span class=\"p\">)</span>\n            <span class=\"n\">writer</span><span class=\"o\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"n\">out_dir</span><span class=\"p\">)</span>\n\n        <span class=\"n\">monkeypatch</span><span class=\"o\">.</span><span class=\"n\">setattr</span><span class=\"p\">(</span>\n            <span class=\"n\">pyspark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"n\">readwriter</span><span class=\"o\">.</span><span class=\"n\">DataFrameWriter</span><span class=\"p\">,</span> <span class=\"s2\">\"save\"</span><span class=\"p\">,</span> <span class=\"n\">mock_save</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Run notebook</span>\n        <span class=\"n\">dbrickstest</span><span class=\"o\">.</span><span class=\"n\">run_notebook</span><span class=\"p\">(</span><span class=\"s2\">\".\"</span><span class=\"p\">,</span> <span class=\"s2\">\"sqldw_notebook\"</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Notebook produces a Parquet file (directory)</span>\n        <span class=\"n\">resultDF</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_parquet</span><span class=\"p\">(</span><span class=\"n\">out_dir</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Compare produced Parquet file and expected CSV file</span>\n        <span class=\"n\">expectedDF</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s2\">\"tests/sqldw_expected.csv\"</span><span class=\"p\">)</span>\n        <span class=\"n\">assert_frame_equal</span><span class=\"p\">(</span><span class=\"n\">expectedDF</span><span class=\"p\">,</span> <span class=\"n\">resultDF</span><span class=\"p\">,</span> <span class=\"n\">check_dtype</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n</pre>\n<h2>Issues</h2>\n<p>Please report issues at <a href=\"https://github.com/microsoft/DataOps/issues\" rel=\"nofollow\">https://github.com/microsoft/DataOps/issues</a>.</p>\n\n          </div>"}, "last_serial": 6747622, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "71d66d614b9166a4b51b9f3f885f057e", "sha256": "56071a5d5cb5f5bd61aba893946b09bec8e3c1f7104fcb1bf9a926e530266da4"}, "downloads": -1, "filename": "databricks_test-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "71d66d614b9166a4b51b9f3f885f057e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7000, "upload_time": "2020-01-18T20:34:32", "upload_time_iso_8601": "2020-01-18T20:34:32.002766Z", "url": "https://files.pythonhosted.org/packages/c4/2b/dd8ed56b57571ccc28f4639f21dcefe085d20e581c0b545dcadde8306c26/databricks_test-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c1d7fba2f450aab1b592857782c07564", "sha256": "998b4ad0f8d47e10c1c6d97e11dbc1eb4ace25cdae1c26335759b0daffd3d235"}, "downloads": -1, "filename": "databricks_test-0.0.1.tar.gz", "has_sig": false, "md5_digest": "c1d7fba2f450aab1b592857782c07564", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5158, "upload_time": "2020-01-18T20:36:09", "upload_time_iso_8601": "2020-01-18T20:36:09.837057Z", "url": "https://files.pythonhosted.org/packages/1b/d7/8f402a028f099d333c3539ff414c7f76393aa48f65ff5cbb3e9d1befe883/databricks_test-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "cce003a8540b094202da62f2845231ed", "sha256": "bd1f9d8b560acd77375028de219f2b0bb3959ab334f63956ad280e1eeb6d5fb7"}, "downloads": -1, "filename": "databricks_test-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "cce003a8540b094202da62f2845231ed", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5127, "upload_time": "2020-02-25T17:38:25", "upload_time_iso_8601": "2020-02-25T17:38:25.861586Z", "url": "https://files.pythonhosted.org/packages/c1/09/abbde28e44790c7375dd998b770a445f4e99dab23004f446500c25901594/databricks_test-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6e7099e5be934942b996f1f47a39112a", "sha256": "0cecd589f8806ea33953660d48d9d6b521e26e6fb557bbcf813cbd9cb0c7194d"}, "downloads": -1, "filename": "databricks_test-0.0.2.tar.gz", "has_sig": false, "md5_digest": "6e7099e5be934942b996f1f47a39112a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5368, "upload_time": "2020-02-25T17:38:27", "upload_time_iso_8601": "2020-02-25T17:38:27.560934Z", "url": "https://files.pythonhosted.org/packages/86/0c/f4c33f81418d9a5f8c90841fc3b62d67ec2c03044959449890d8c95e33cd/databricks_test-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "827dc4c0348b99ce5436076f402adfe5", "sha256": "f98cdc47811da8400eee9d8068cfaa0f5596c9e3498a7bac9b0a2c54ed34afbb"}, "downloads": -1, "filename": "databricks_test-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "827dc4c0348b99ce5436076f402adfe5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5142, "upload_time": "2020-02-26T12:44:34", "upload_time_iso_8601": "2020-02-26T12:44:34.768926Z", "url": "https://files.pythonhosted.org/packages/af/ee/ad40c7f5f4caab83df0bf1094d69dd0891a4f5f66d83a789a2a4e97160b8/databricks_test-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e9d3214b473820d25d3adef747d12053", "sha256": "c1e03eaaf88736254d2b4f86e5bb9a5d0dee960ce4ee8df5b6dc18147a11c3f7"}, "downloads": -1, "filename": "databricks_test-0.0.3.tar.gz", "has_sig": false, "md5_digest": "e9d3214b473820d25d3adef747d12053", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5377, "upload_time": "2020-02-26T12:44:36", "upload_time_iso_8601": "2020-02-26T12:44:36.883090Z", "url": "https://files.pythonhosted.org/packages/52/d0/be2bcc6b368bff7909fa03593894238d7a9a1cd0ac7fe86d2bff8b1f62a7/databricks_test-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "e360238d171a136d645904afe94cfd4a", "sha256": "b05120d883c5cb113367e39ce648c05fe2362878b442566897e56c4f3b5c4967"}, "downloads": -1, "filename": "databricks_test-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "e360238d171a136d645904afe94cfd4a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5158, "upload_time": "2020-03-04T12:50:29", "upload_time_iso_8601": "2020-03-04T12:50:29.020963Z", "url": "https://files.pythonhosted.org/packages/dd/c9/52a96a597f107bb53aa467f9391424764518664a712476981b4417b104f6/databricks_test-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2764975fb61273728e654d0db7687fb2", "sha256": "0b40c9e94c07811aaf1a87ae592718f2e84f6ff388b645156479a4e6dcb9cd63"}, "downloads": -1, "filename": "databricks_test-0.0.4.tar.gz", "has_sig": false, "md5_digest": "2764975fb61273728e654d0db7687fb2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5402, "upload_time": "2020-03-04T12:50:32", "upload_time_iso_8601": "2020-03-04T12:50:32.696047Z", "url": "https://files.pythonhosted.org/packages/bf/18/590f8c5d0160d6afed83ec7e59b352b694228e9ecfbddf2fddc021df7fbe/databricks_test-0.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e360238d171a136d645904afe94cfd4a", "sha256": "b05120d883c5cb113367e39ce648c05fe2362878b442566897e56c4f3b5c4967"}, "downloads": -1, "filename": "databricks_test-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "e360238d171a136d645904afe94cfd4a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5158, "upload_time": "2020-03-04T12:50:29", "upload_time_iso_8601": "2020-03-04T12:50:29.020963Z", "url": "https://files.pythonhosted.org/packages/dd/c9/52a96a597f107bb53aa467f9391424764518664a712476981b4417b104f6/databricks_test-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2764975fb61273728e654d0db7687fb2", "sha256": "0b40c9e94c07811aaf1a87ae592718f2e84f6ff388b645156479a4e6dcb9cd63"}, "downloads": -1, "filename": "databricks_test-0.0.4.tar.gz", "has_sig": false, "md5_digest": "2764975fb61273728e654d0db7687fb2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5402, "upload_time": "2020-03-04T12:50:32", "upload_time_iso_8601": "2020-03-04T12:50:32.696047Z", "url": "https://files.pythonhosted.org/packages/bf/18/590f8c5d0160d6afed83ec7e59b352b694228e9ecfbddf2fddc021df7fbe/databricks_test-0.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:40:29 2020"}