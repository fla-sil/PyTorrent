{"info": {"author": "Lars G.B. Johnsen", "author_email": "yoonsen@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Modules\nThis repository only contains python modules for accessing text and pictures at the National Library of Norway.\n\n# nbtext.py\n\n\n\n# Metadata and URNs\n\n### `totals(top=200)`\nReturns a dictionary of the top \u00c2\u00b4top\u00c2\u00b4 words in the digital collection\n\n\n### `urn_from_text(T)`\nReturn a list of URNs as 13 digit serial numbers. \n\n### `metadata(urn=\"\"\"text\"\"\")`\nReturns a list of metada entries for given URN.\n\n### `pure_urn(data)`\nConvert URN-lists with extra data into list of serial numbers. Used to convert different ways of presenting URNs into a list of serial decimal digits. Designed to work with book URNs, and will not work for newspaper URNs.\n```    \n    Args:\n        data: May be a list of URNs, a list of lists with URNs as their\n            initial element, or a string of raw texts containing URNs\n    Returns:\n        List[str]: A list of URNs. Empty list if input is on the wrong\n            format or contains no URNs\n```\n\n\n### `col_agg(df, col='sum')`\nAggregate columns of a panda dataframe.\n\n### `row_agg(df, col='sum')`\nAggregate rows of a panda dataframe.\n\n\n# Access texts as frequency lists\n\n### `get_freq(urn, top=50, cutoff=3)`\nGet frequency list of words for a given URN (as a serial number).\n\n### `get_urn(metadata=None)`\nGet URNSs from metadata specified as a dictionary. Keys specified in quotes are:\n* \"corpus\": \"avis\" or \"bok\"\n* \"author\": wildcard match using % as wildcard.\n* \"title\": wildcard match using %. For newspapers this corresponds to name of paper.\n* \"year\": starting year as number or number as string.\n* \"next\": the next number of years starting from \u00c2\u00b4year\u00c2\u00b4\n*  \"ddk\": Dewy decimal number as wildcard match e.g. \"64%\" \n*   \"gender\": value is \"m\" for male or \"f\" for female\n*  \"subject\": keywords used to annotate text in the national bibliography.\n\n### `get_corpus_text(urns, top = 10000, cutoff=5)`\nFrom a corpus as a list of URNs, get the top `top` words that have a frequency above `cutoff`. Builds on top of `get_freq`.\nReturns a dataframe with URNs as row headers and words as indices.\n``\n    k = dict()\n    for u in urns:\n        k[u] = get_freq(u, top = top, cutoff = cutoff)\n    return pd.DataFrame(k)\n``\n### `get_papers(top=5, cutoff=5, navn='%', yearfrom=1800, yearto=2020, samplesize=100)`\nGet newspapers as frequency lists. Parameter `top` asks for the top ranked words, `cutoff` indicates the lower frequency limit, while `navn` indicates newspaper name as wildcard string.\n\n\n# Collocations and clusters\n\n### `make_a_collocation(word, period=(1990, 2000), before=5, after=5, corpus='avis', samplesize=100, limit=2000)`\nReturn a collocation as dataframe.\n\n\n### `compute_assoc(coll_frame, column, exponent=1.1, refcolumn = 'reference_corpus')`\nCompute an association using PMI.\n``    return pd.DataFrame(coll_frame[column]**exponent/coll_frame.mean(axis=1))``\n\n### `urn_coll(word, urns=[], after=5, before=5, limit=1000)`\nlimit is max number of occurences of word pr. urn. \nFind collocations for word in a set of book URNs. Only books at the moment\n\n### `collocation(word, yearfrom=2010, yearto=2018, before=3, after=3, limit=1000, corpus='avis')`\nCompute a collocation for a given word within indicated period.\n`before` is the number of preceeding words, `after` number of words following, `limit`. \n``\ndata =  requests.get(\n        \"https://api.nb.no/ngram/collocation\", \n        params={\n            'word':word,\n            'corpus':corpus, \n            'yearfrom':yearfrom, \n            'before':before,\n            'after':after,\n            'limit':limit,\n            'yearto':yearto}).json()\n    return pd.DataFrame.from_dict(data['freq'], orient='index')\n``\n\n\n\n### `normalize_corpus_dataframe(df)`\nNormalized all values in corpus `df` as a dataframe. Changes `df` in situ, and returns `True`.\n\n### `show_korpus(korpus, start=0, size=4, vstart=0, vsize=20, sortby = '')`\nShow part of a dataframe `korpus`, slicing along columns starting from `start`and numbers by `size` and slicing rows by `vstart`and `vsize`. Sorts by first column by default.\n\n### `aggregate(korpus)`\nMake an aggregated sum of all documents across the corpus, here we use average\n``    return pd.DataFrame(korpus.fillna(0).mean(axis=1))``\n\n### `convert_list_of_freqs_to_dataframe(referanse)`\nThe function get_papers() returns a list of frequencies - convert it and normalize.\n\n### `get_corpus(top=5, cutoff=5, navn='%', corpus='avis', yearfrom=1800, yearto=2020, samplesize=10)`\nFirst version of collecting a corpus using parameters described above for `get_papers` (for newspapers) and `get_corpus` (for books).\n\n\n# Classes\n\n### `class Cluster`\n    def __init__(self, word = '', filename = '', period = (1950,1960) , before = 5, after = 5, corpus='avis', reference = 200, \n                 word_samples=1000):\nSee clustering notebook for example and closer description.\n\n### `class Corpus`\nSee `Corpus` notebook for examples and explanation.\n\n### `Corpus_urn`\nSee example notebook\n\n# Graphs and network analysis\n\n### `make_newspaper_network(key, wordbag, titel='%', yearfrom='1980', yearto='1990', limit=500)`\nSeems not to work at the moment.\n\n### `make_network(urn, wordbag, cutoff=0)`\nMake a graph as `networkx` object from `wordbag` and `urn`. Two words are connected if they occur within same paragraph.\n\n### `make_network_graph(urn, wordbag, cutoff=0)`\nMake a graph as `networkx` object from `wordbag` and `urn`. Two words are connected if they occur within same paragraph.\n\n### `draw_graph_centrality(G, h=15, v=10, fontsize=20, k=0.2, arrows=False, font_color='black', threshold=0.01)` \nDraw a graph using force atlas.\n\n\n# Wordclouds\n\n### `make_cloud(json_text, top=100, background='white', stretch=lambda x: 2**(10*x), width=500, height=500, font_path=None)`*\nCreate a word cloud from a frequency list. First line of code: ``pairs0 = Counter(json_text).most_common(top)``\n\n### `draw_cloud(sky, width=20, height=20, fil='')`\nDraw a word cloud produces by `make_cloud` \n\n### `cloud(pd, column='', top=200, width=1000, height=1000, background='black', file='', stretch=10, font_path=None)`\nMake and draw a cloud from  a pandas dataframe, using `make_cloud` and `draw_cloud`.\n\n# Growth diagrams (sentiment analysis)\n\n### `vekstdiagram(urn, params=None)`\nMake a growth diagram for a given book using a set of words:\nParameters \n\n``\n'words': list of words \n'window': chunk size in the book\n'pr': how many words are skipped before next chunk\n``\n\n### `plot_sammen_vekst(urn, ordlister, window=5000, pr = 100)`\nFor ploting more than one growth diagram. Have a look at example notebook.\n\n# Word relations and n-grams\n\n### `difference(first, second, rf, rs, years=(1980, 2000),smooth=1, corpus='bok')`\nCompute difference of difference (first/second)/(rf/rs) for ngrams.\n\n\n### `relaterte_ord(word, number = 20, score=False)`\nFind related words using eigenvector centrality from networkx. Related words are taken from [NB Ngram](https://www.nb.no/sp_tjenester/beta/ngram_1/galaxies). Note: Works for english and german - add parameter!!\n\n\n### `nb_ngram(terms, corpus='bok', smooth=3, years=(1810, 2010), mode='relative')`\nCollect an ngram as json object from [NB Ngram](https://www.nb.no/sp_tjenester/beta/ngram_1/trends#ngram/query?terms=norway). Terms is string of comma separated ngrams (single words up to trigrams).\n\n### `ngram_conv(ngrams, smooth=1, years=(1810,2013), mode='relative')`\nConvert ngrams to a dataframe.\n\n\n### `make_graph(word)`\nGet graph like in [NB Ngram](https://www.nb.no/sp_tjenester/beta/ngram_1/galaxies)\n\n\n# Concordances\n\n### `get_konk(word, params=None, kind='html')`\nGet a concordance for given word. Params are like `get_urn`. Value is either an HTML-page, a json structure, or a dataframe. Specify `kind` as 'html', 'json' or '' respectively.\n\n### `get_urnkonk(word, params=None, html=True)`\nSame as `get_konk` but from a list of URNs.\n\n\n# Character Analysis and graphs\n\n### `central_characters(graph, n=10)`\nwrapper around `networkx`\n``    res = Counter(nx.degree_centrality(graph)).most_common(n)\n    return res\n``\n### `central_betweenness_characters(graph, n=10)`\nwrapper around `networkx`\n``    res = Counter(nx.betweenness_centrality(graph)).most_common(n)\n    return res\n  ``  \n\n### `check_words(urn, ordbag)`\nFind frequency of words in `ordbag` within a book given by `urn`.\n\n\n# Text complexity\n\n### `sttr(urn, chunk=5000)`\nCompute a standardized type/token-ratio for text identified with urn. The function expects the serial number of a URN for a book. Returns a number.\n\n### `navn(urn)`\nReturns a dictionary of frequency of possible names from URN as serial number.    \n\n# Utilities\n\n### `heatmap(df, color='green')`\nA wrapper for heatmap of df as a Pandas dataframe, like this:\n``return df.fillna(0).style.background_gradient(cmap=sns.light_palette(color, as_cmap=True))``\n\n### `df_combine(array_df)`\nCombine one column dataframes into one dataframe.\n\n### `wildcardsearch(params=None)`\nDefault values: ``params = {'word': '', 'freq_lim': 50, 'limit': 50, 'factor': 2}``\nReturns a dataframe containing matches for `word`. See examples in notebook `wildcardsearch`.\n\n### `sorted_wildcardsearch(params)`\nSame as `wildcardsearch` with results sorted on frequency.\n\n### `combine(clusters)`\nMake new collocation analyses from data in clusters\n\n### `cluster_join(cluster)`\nUsed with serial clusters. Join them together in one dataframe. See example in cluster notebook.\n\n### `serie_cluster(word, start\u00c3\u00a5r, slutt\u00c3\u00a5r, inkrement, before=5, after=5, reference=150, word_samples=500)`\nMake a series of clusters.\n\n#### `save_serie_cluster(tidscluster)`\nSave series to files.\n\n#### `les_serie_cluster(word, start\u00c3\u00a5r, slutt\u00c3\u00a5r, inkrement)`\nRead them\n### `frame(something, name)`\nCreate a dataframe of `something`\n\n\n### `get_urns_from_docx(document)`\nA file in `docx` format may contain a list of URNs.\n\n### `get_urns_from_text(document)`\nURNs from a `.txt`-document.\n\n### `get_urns_from_files(mappe, file_type='txt')`\nExtract URNs from a folder with `.txt` and `.docs` files. Returns a dictionary with filenames as keys, each with a list of URNs.\n\n\n### `check_vals(korpus, vals)`\nA wrapper for dataframes: ``\n    return korpus[korpus.index.isin(vals)].sort_values(by=0, ascending=False)``\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Yoonsen/Modules", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "dhlab", "package_url": "https://pypi.org/project/dhlab/", "platform": "", "project_url": "https://pypi.org/project/dhlab/", "project_urls": {"Homepage": "https://github.com/Yoonsen/Modules"}, "release_url": "https://pypi.org/project/dhlab/0.7.5/", "requires_dist": ["msgpack", "wordcloud", "python-louvain", "beautifulsoup4"], "requires_python": ">=3.2", "summary": "API for National Library of Norway", "version": "0.7.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Modules</h1>\n<p>This repository only contains python modules for accessing text and pictures at the National Library of Norway.</p>\n<h1>nbtext.py</h1>\n<h1>Metadata and URNs</h1>\n<h3><code>totals(top=200)</code></h3>\n<p>Returns a dictionary of the top \u00c2\u00b4top\u00c2\u00b4 words in the digital collection</p>\n<h3><code>urn_from_text(T)</code></h3>\n<p>Return a list of URNs as 13 digit serial numbers.</p>\n<h3><code>metadata(urn=\"\"\"text\"\"\")</code></h3>\n<p>Returns a list of metada entries for given URN.</p>\n<h3><code>pure_urn(data)</code></h3>\n<p>Convert URN-lists with extra data into list of serial numbers. Used to convert different ways of presenting URNs into a list of serial decimal digits. Designed to work with book URNs, and will not work for newspaper URNs.</p>\n<pre><code>    Args:\n        data: May be a list of URNs, a list of lists with URNs as their\n            initial element, or a string of raw texts containing URNs\n    Returns:\n        List[str]: A list of URNs. Empty list if input is on the wrong\n            format or contains no URNs\n</code></pre>\n<h3><code>col_agg(df, col='sum')</code></h3>\n<p>Aggregate columns of a panda dataframe.</p>\n<h3><code>row_agg(df, col='sum')</code></h3>\n<p>Aggregate rows of a panda dataframe.</p>\n<h1>Access texts as frequency lists</h1>\n<h3><code>get_freq(urn, top=50, cutoff=3)</code></h3>\n<p>Get frequency list of words for a given URN (as a serial number).</p>\n<h3><code>get_urn(metadata=None)</code></h3>\n<p>Get URNSs from metadata specified as a dictionary. Keys specified in quotes are:</p>\n<ul>\n<li>\"corpus\": \"avis\" or \"bok\"</li>\n<li>\"author\": wildcard match using % as wildcard.</li>\n<li>\"title\": wildcard match using %. For newspapers this corresponds to name of paper.</li>\n<li>\"year\": starting year as number or number as string.</li>\n<li>\"next\": the next number of years starting from \u00c2\u00b4year\u00c2\u00b4</li>\n<li>\"ddk\": Dewy decimal number as wildcard match e.g. \"64%\"</li>\n<li>\"gender\": value is \"m\" for male or \"f\" for female</li>\n<li>\"subject\": keywords used to annotate text in the national bibliography.</li>\n</ul>\n<h3><code>get_corpus_text(urns, top = 10000, cutoff=5)</code></h3>\n<p>From a corpus as a list of URNs, get the top <code>top</code> words that have a frequency above <code>cutoff</code>. Builds on top of <code>get_freq</code>.\nReturns a dataframe with URNs as row headers and words as indices.\n<code>k = dict() for u in urns: k[u] = get_freq(u, top = top, cutoff = cutoff) return pd.DataFrame(k)</code></p>\n<h3><code>get_papers(top=5, cutoff=5, navn='%', yearfrom=1800, yearto=2020, samplesize=100)</code></h3>\n<p>Get newspapers as frequency lists. Parameter <code>top</code> asks for the top ranked words, <code>cutoff</code> indicates the lower frequency limit, while <code>navn</code> indicates newspaper name as wildcard string.</p>\n<h1>Collocations and clusters</h1>\n<h3><code>make_a_collocation(word, period=(1990, 2000), before=5, after=5, corpus='avis', samplesize=100, limit=2000)</code></h3>\n<p>Return a collocation as dataframe.</p>\n<h3><code>compute_assoc(coll_frame, column, exponent=1.1, refcolumn = 'reference_corpus')</code></h3>\n<p>Compute an association using PMI.\n<code>return pd.DataFrame(coll_frame[column]**exponent/coll_frame.mean(axis=1))</code></p>\n<h3><code>urn_coll(word, urns=[], after=5, before=5, limit=1000)</code></h3>\n<p>limit is max number of occurences of word pr. urn.\nFind collocations for word in a set of book URNs. Only books at the moment</p>\n<h3><code>collocation(word, yearfrom=2010, yearto=2018, before=3, after=3, limit=1000, corpus='avis')</code></h3>\n<p>Compute a collocation for a given word within indicated period.\n<code>before</code> is the number of preceeding words, <code>after</code> number of words following, <code>limit</code>.\n<code>data = requests.get( \"https://api.nb.no/ngram/collocation\", params={ 'word':word, 'corpus':corpus, 'yearfrom':yearfrom, 'before':before, 'after':after, 'limit':limit, 'yearto':yearto}).json() return pd.DataFrame.from_dict(data['freq'], orient='index')</code></p>\n<h3><code>normalize_corpus_dataframe(df)</code></h3>\n<p>Normalized all values in corpus <code>df</code> as a dataframe. Changes <code>df</code> in situ, and returns <code>True</code>.</p>\n<h3><code>show_korpus(korpus, start=0, size=4, vstart=0, vsize=20, sortby = '')</code></h3>\n<p>Show part of a dataframe <code>korpus</code>, slicing along columns starting from <code>start</code>and numbers by <code>size</code> and slicing rows by <code>vstart</code>and <code>vsize</code>. Sorts by first column by default.</p>\n<h3><code>aggregate(korpus)</code></h3>\n<p>Make an aggregated sum of all documents across the corpus, here we use average\n<code>return pd.DataFrame(korpus.fillna(0).mean(axis=1))</code></p>\n<h3><code>convert_list_of_freqs_to_dataframe(referanse)</code></h3>\n<p>The function get_papers() returns a list of frequencies - convert it and normalize.</p>\n<h3><code>get_corpus(top=5, cutoff=5, navn='%', corpus='avis', yearfrom=1800, yearto=2020, samplesize=10)</code></h3>\n<p>First version of collecting a corpus using parameters described above for <code>get_papers</code> (for newspapers) and <code>get_corpus</code> (for books).</p>\n<h1>Classes</h1>\n<h3><code>class Cluster</code></h3>\n<pre><code>def __init__(self, word = '', filename = '', period = (1950,1960) , before = 5, after = 5, corpus='avis', reference = 200, \n             word_samples=1000):\n</code></pre>\n<p>See clustering notebook for example and closer description.</p>\n<h3><code>class Corpus</code></h3>\n<p>See <code>Corpus</code> notebook for examples and explanation.</p>\n<h3><code>Corpus_urn</code></h3>\n<p>See example notebook</p>\n<h1>Graphs and network analysis</h1>\n<h3><code>make_newspaper_network(key, wordbag, titel='%', yearfrom='1980', yearto='1990', limit=500)</code></h3>\n<p>Seems not to work at the moment.</p>\n<h3><code>make_network(urn, wordbag, cutoff=0)</code></h3>\n<p>Make a graph as <code>networkx</code> object from <code>wordbag</code> and <code>urn</code>. Two words are connected if they occur within same paragraph.</p>\n<h3><code>make_network_graph(urn, wordbag, cutoff=0)</code></h3>\n<p>Make a graph as <code>networkx</code> object from <code>wordbag</code> and <code>urn</code>. Two words are connected if they occur within same paragraph.</p>\n<h3><code>draw_graph_centrality(G, h=15, v=10, fontsize=20, k=0.2, arrows=False, font_color='black', threshold=0.01)</code></h3>\n<p>Draw a graph using force atlas.</p>\n<h1>Wordclouds</h1>\n<h3><code>make_cloud(json_text, top=100, background='white', stretch=lambda x: 2**(10*x), width=500, height=500, font_path=None)</code>*</h3>\n<p>Create a word cloud from a frequency list. First line of code: <code>pairs0 = Counter(json_text).most_common(top)</code></p>\n<h3><code>draw_cloud(sky, width=20, height=20, fil='')</code></h3>\n<p>Draw a word cloud produces by <code>make_cloud</code></p>\n<h3><code>cloud(pd, column='', top=200, width=1000, height=1000, background='black', file='', stretch=10, font_path=None)</code></h3>\n<p>Make and draw a cloud from  a pandas dataframe, using <code>make_cloud</code> and <code>draw_cloud</code>.</p>\n<h1>Growth diagrams (sentiment analysis)</h1>\n<h3><code>vekstdiagram(urn, params=None)</code></h3>\n<p>Make a growth diagram for a given book using a set of words:\nParameters</p>\n<p><code>'words': list of words 'window': chunk size in the book 'pr': how many words are skipped before next chunk</code></p>\n<h3><code>plot_sammen_vekst(urn, ordlister, window=5000, pr = 100)</code></h3>\n<p>For ploting more than one growth diagram. Have a look at example notebook.</p>\n<h1>Word relations and n-grams</h1>\n<h3><code>difference(first, second, rf, rs, years=(1980, 2000),smooth=1, corpus='bok')</code></h3>\n<p>Compute difference of difference (first/second)/(rf/rs) for ngrams.</p>\n<h3><code>relaterte_ord(word, number = 20, score=False)</code></h3>\n<p>Find related words using eigenvector centrality from networkx. Related words are taken from <a href=\"https://www.nb.no/sp_tjenester/beta/ngram_1/galaxies\" rel=\"nofollow\">NB Ngram</a>. Note: Works for english and german - add parameter!!</p>\n<h3><code>nb_ngram(terms, corpus='bok', smooth=3, years=(1810, 2010), mode='relative')</code></h3>\n<p>Collect an ngram as json object from <a href=\"https://www.nb.no/sp_tjenester/beta/ngram_1/trends#ngram/query?terms=norway\" rel=\"nofollow\">NB Ngram</a>. Terms is string of comma separated ngrams (single words up to trigrams).</p>\n<h3><code>ngram_conv(ngrams, smooth=1, years=(1810,2013), mode='relative')</code></h3>\n<p>Convert ngrams to a dataframe.</p>\n<h3><code>make_graph(word)</code></h3>\n<p>Get graph like in <a href=\"https://www.nb.no/sp_tjenester/beta/ngram_1/galaxies\" rel=\"nofollow\">NB Ngram</a></p>\n<h1>Concordances</h1>\n<h3><code>get_konk(word, params=None, kind='html')</code></h3>\n<p>Get a concordance for given word. Params are like <code>get_urn</code>. Value is either an HTML-page, a json structure, or a dataframe. Specify <code>kind</code> as 'html', 'json' or '' respectively.</p>\n<h3><code>get_urnkonk(word, params=None, html=True)</code></h3>\n<p>Same as <code>get_konk</code> but from a list of URNs.</p>\n<h1>Character Analysis and graphs</h1>\n<h3><code>central_characters(graph, n=10)</code></h3>\n<p>wrapper around <code>networkx</code>\n<code>res = Counter(nx.degree_centrality(graph)).most_common(n) return res</code></p>\n<h3><code>central_betweenness_characters(graph, n=10)</code></h3>\n<p>wrapper around <code>networkx</code>\n<code>res = Counter(nx.betweenness_centrality(graph)).most_common(n) return res</code></p>\n<h3><code>check_words(urn, ordbag)</code></h3>\n<p>Find frequency of words in <code>ordbag</code> within a book given by <code>urn</code>.</p>\n<h1>Text complexity</h1>\n<h3><code>sttr(urn, chunk=5000)</code></h3>\n<p>Compute a standardized type/token-ratio for text identified with urn. The function expects the serial number of a URN for a book. Returns a number.</p>\n<h3><code>navn(urn)</code></h3>\n<p>Returns a dictionary of frequency of possible names from URN as serial number.</p>\n<h1>Utilities</h1>\n<h3><code>heatmap(df, color='green')</code></h3>\n<p>A wrapper for heatmap of df as a Pandas dataframe, like this:\n<code>return df.fillna(0).style.background_gradient(cmap=sns.light_palette(color, as_cmap=True))</code></p>\n<h3><code>df_combine(array_df)</code></h3>\n<p>Combine one column dataframes into one dataframe.</p>\n<h3><code>wildcardsearch(params=None)</code></h3>\n<p>Default values: <code>params = {'word': '', 'freq_lim': 50, 'limit': 50, 'factor': 2}</code>\nReturns a dataframe containing matches for <code>word</code>. See examples in notebook <code>wildcardsearch</code>.</p>\n<h3><code>sorted_wildcardsearch(params)</code></h3>\n<p>Same as <code>wildcardsearch</code> with results sorted on frequency.</p>\n<h3><code>combine(clusters)</code></h3>\n<p>Make new collocation analyses from data in clusters</p>\n<h3><code>cluster_join(cluster)</code></h3>\n<p>Used with serial clusters. Join them together in one dataframe. See example in cluster notebook.</p>\n<h3><code>serie_cluster(word, start\u00c3\u00a5r, slutt\u00c3\u00a5r, inkrement, before=5, after=5, reference=150, word_samples=500)</code></h3>\n<p>Make a series of clusters.</p>\n<h4><code>save_serie_cluster(tidscluster)</code></h4>\n<p>Save series to files.</p>\n<h4><code>les_serie_cluster(word, start\u00c3\u00a5r, slutt\u00c3\u00a5r, inkrement)</code></h4>\n<p>Read them</p>\n<h3><code>frame(something, name)</code></h3>\n<p>Create a dataframe of <code>something</code></p>\n<h3><code>get_urns_from_docx(document)</code></h3>\n<p>A file in <code>docx</code> format may contain a list of URNs.</p>\n<h3><code>get_urns_from_text(document)</code></h3>\n<p>URNs from a <code>.txt</code>-document.</p>\n<h3><code>get_urns_from_files(mappe, file_type='txt')</code></h3>\n<p>Extract URNs from a folder with <code>.txt</code> and <code>.docs</code> files. Returns a dictionary with filenames as keys, each with a list of URNs.</p>\n<h3><code>check_vals(korpus, vals)</code></h3>\n<p>A wrapper for dataframes: <code>return korpus[korpus.index.isin(vals)].sort_values(by=0, ascending=False)</code></p>\n\n          </div>"}, "last_serial": 5800900, "releases": {"0.2": [{"comment_text": "", "digests": {"md5": "095eb2be29d3567ee15823a6ba228c59", "sha256": "c8c0d290f781cd0fb02a7e49afa4426aea7089d320944c90da5b8618e0ba6da3"}, "downloads": -1, "filename": "dhlab-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "095eb2be29d3567ee15823a6ba228c59", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 5980, "upload_time": "2019-09-08T20:13:29", "upload_time_iso_8601": "2019-09-08T20:13:29.188919Z", "url": "https://files.pythonhosted.org/packages/fc/5c/eee77e904bf2a20a0edeb2c7fd26fba494d106742f0e0f06d1d0cbf615b1/dhlab-0.2-py3-none-any.whl", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "b50cf7149b030b6498106af209a9bf88", "sha256": "77f4a00c5358ccf784a6933e84eb3774113b69901320e120d27d4fdcbe0dd02b"}, "downloads": -1, "filename": "dhlab-0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "b50cf7149b030b6498106af209a9bf88", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.2", "size": 35006, "upload_time": "2019-09-08T20:38:25", "upload_time_iso_8601": "2019-09-08T20:38:25.209716Z", "url": "https://files.pythonhosted.org/packages/05/d6/8d79d4058893395f48faf484b3ed4739b5de686fa3deb631f3cac97162dc/dhlab-0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7aa8c8567d7ea8b4d75d2dfa2c0286a2", "sha256": "7f51b44fea24a0782e343f9ede632f4c99b6c749c2dff0c66e807f5e6d00770b"}, "downloads": -1, "filename": "dhlab-0.3.tar.gz", "has_sig": false, "md5_digest": "7aa8c8567d7ea8b4d75d2dfa2c0286a2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.2", "size": 32426, "upload_time": "2019-09-08T21:04:49", "upload_time_iso_8601": "2019-09-08T21:04:49.960391Z", "url": "https://files.pythonhosted.org/packages/bb/c6/4cba7330f65f66d78a63128aa7f02cd76e3bb44bd29f4ad73c4224fa5743/dhlab-0.3.tar.gz", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "51145ee083cf2fe12f94bde3e98e3f64", "sha256": "e0b5e9d11d6e0af2950ce53fb7d286a9e15a5cd51092b4d620ad98e92d331e0d"}, "downloads": -1, "filename": "dhlab-0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "51145ee083cf2fe12f94bde3e98e3f64", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.2", "size": 35009, "upload_time": "2019-09-08T21:04:48", "upload_time_iso_8601": "2019-09-08T21:04:48.246798Z", "url": "https://files.pythonhosted.org/packages/20/6e/48eee2a58f35d4de53b26de3cac0610735d2572182acc11c8982a1b1c4a2/dhlab-0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "43cc7e01a189a46a3b0aee8c455dfc6b", "sha256": "22f5859eadd1b192c1a20b86b195a262a2f4f0755cf4bc626a2de97e7ebdfd16"}, "downloads": -1, "filename": "dhlab-0.4.tar.gz", "has_sig": false, "md5_digest": "43cc7e01a189a46a3b0aee8c455dfc6b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.2", "size": 32430, "upload_time": "2019-09-08T21:04:51", "upload_time_iso_8601": "2019-09-08T21:04:51.254611Z", "url": "https://files.pythonhosted.org/packages/f0/52/a1ec542b3cd85e89799800ae6f710e3919c4a26699d15cf71f4ad38852f1/dhlab-0.4.tar.gz", "yanked": false}], "0.5": [{"comment_text": "", "digests": {"md5": "2578b9ce740e3325fe04a3fc1d75efa7", "sha256": "15659e2fc21641725aa638f24dfc82021145cdc309edc72927c74c09dd34339f"}, "downloads": -1, "filename": "dhlab-0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "2578b9ce740e3325fe04a3fc1d75efa7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.2", "size": 35004, "upload_time": "2019-09-08T21:19:21", "upload_time_iso_8601": "2019-09-08T21:19:21.865333Z", "url": "https://files.pythonhosted.org/packages/44/a7/160323a604399d3031554808af69cd3e36ebfa9ec1b6700fd68922df1917/dhlab-0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d9549f5fb9223f7f8aabc1ee1f79974a", "sha256": "2975b41894dbb257f6a1f469eaab482dc138ef9a52193a79b9b0c81cbdebdbf7"}, "downloads": -1, "filename": "dhlab-0.5.tar.gz", "has_sig": false, "md5_digest": "d9549f5fb9223f7f8aabc1ee1f79974a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.2", "size": 32426, "upload_time": "2019-09-08T21:19:23", "upload_time_iso_8601": "2019-09-08T21:19:23.552143Z", "url": "https://files.pythonhosted.org/packages/02/c4/3d533ff596e65dd7631bcb3b7beb2f8f6f17c7deb8d738bc0c9d7606f6a8/dhlab-0.5.tar.gz", "yanked": false}], "0.6": [{"comment_text": "", "digests": {"md5": "8dc7c21d1961ea9933038ed1097e1ee9", "sha256": "c9232945e13964518c15fe4772d98b3362999b2abcbef4e76d93c9a5e87bd7ed"}, "downloads": -1, "filename": "dhlab-0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "8dc7c21d1961ea9933038ed1097e1ee9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.2", "size": 35234, "upload_time": "2019-09-08T21:44:29", "upload_time_iso_8601": "2019-09-08T21:44:29.949315Z", "url": "https://files.pythonhosted.org/packages/b6/ef/56e5472d7dcaf036e90cde38866c2f7b45e92673fff384a94a36da78598f/dhlab-0.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1fbd5d3a65f0c84c68c305e0b1208f98", "sha256": "54d8a3df2eff4b2141af836a4249aec987a519852c41e6d1c36d370a257b0640"}, "downloads": -1, "filename": "dhlab-0.6.tar.gz", "has_sig": false, "md5_digest": "1fbd5d3a65f0c84c68c305e0b1208f98", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.2", "size": 36516, "upload_time": "2019-09-08T21:44:31", "upload_time_iso_8601": "2019-09-08T21:44:31.262045Z", "url": "https://files.pythonhosted.org/packages/a5/ab/838576179ee761a344fdb2abdf574c31541439c4971c27aa151d8fce042e/dhlab-0.6.tar.gz", "yanked": false}], "0.7": [{"comment_text": "", "digests": {"md5": "17dae12f6e70207c411e0b8348bd5f82", "sha256": "fb48875d80e8dfb54aec3d8f91b525c5d08138d42ef53e1b6a76b35058079254"}, "downloads": -1, "filename": "dhlab-0.7-py3-none-any.whl", "has_sig": false, "md5_digest": "17dae12f6e70207c411e0b8348bd5f82", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.2", "size": 63613, "upload_time": "2019-09-08T22:02:01", "upload_time_iso_8601": "2019-09-08T22:02:01.175662Z", "url": "https://files.pythonhosted.org/packages/bc/4a/4717137785b8e84a90043673508b86f78d8964a0f27e0164cc9c00d669e9/dhlab-0.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a945a9fbd089379cdb4176873b8e06c2", "sha256": "e9e23b6cad3b9cffde37e379236850d16d59c2f7db538673f13145192dededd8"}, "downloads": -1, "filename": "dhlab-0.7.tar.gz", "has_sig": false, "md5_digest": "a945a9fbd089379cdb4176873b8e06c2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.2", "size": 36392, "upload_time": "2019-09-08T22:02:02", "upload_time_iso_8601": "2019-09-08T22:02:02.643329Z", "url": "https://files.pythonhosted.org/packages/e2/b1/e803402b8cea4f960070966c37bdaa5cb022f33b364403814749aba2e5b0/dhlab-0.7.tar.gz", "yanked": false}], "0.7.1": [{"comment_text": "", "digests": {"md5": "9beefdd7ae3c305758a4ec9887a453b3", "sha256": "82b8a6c67acf0db9eca5019cc996e86ce627aa13d3b996f673a61da46c4415cd"}, "downloads": -1, "filename": "dhlab-0.7.1-py3-none-any.whl", "has_sig": false, "md5_digest": "9beefdd7ae3c305758a4ec9887a453b3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.2", "size": 63636, "upload_time": "2019-09-08T22:03:49", "upload_time_iso_8601": "2019-09-08T22:03:49.762690Z", "url": "https://files.pythonhosted.org/packages/d2/f9/f5b37e819d00146711da8a7fd5c50c513f409e815a281a0acc939a89779d/dhlab-0.7.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0b318962a5da9dfbf1a4f5850a065466", "sha256": "8696b11dd6ab0ed478349f5457c48d32ab8faed0c38d5c391512886a99ecfca9"}, "downloads": -1, "filename": "dhlab-0.7.1.tar.gz", "has_sig": false, "md5_digest": "0b318962a5da9dfbf1a4f5850a065466", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.2", "size": 36388, "upload_time": "2019-09-08T22:03:51", "upload_time_iso_8601": "2019-09-08T22:03:51.488779Z", "url": "https://files.pythonhosted.org/packages/ad/54/dfde12fe9038adf68acdd7ff49a934db8e887db74d48daf1c3a32368e207/dhlab-0.7.1.tar.gz", "yanked": false}], "0.7.2": [{"comment_text": "", "digests": {"md5": "fd4a2fac881ddbc8896237c56b711db4", "sha256": "49bf81fb2fb5e4d265a288943f1bf9cbc4dede293be802d28d27f79a78e7ee87"}, "downloads": -1, "filename": "dhlab-0.7.2-py3-none-any.whl", "has_sig": false, "md5_digest": "fd4a2fac881ddbc8896237c56b711db4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.2", "size": 63644, "upload_time": "2019-09-08T22:09:11", "upload_time_iso_8601": "2019-09-08T22:09:11.645848Z", "url": "https://files.pythonhosted.org/packages/22/a4/a766de3a3656d31a2601d09f9de646cf79fde56d5eaada908524413fcb4d/dhlab-0.7.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "990c4905e6bb97534265170e6da0ac40", "sha256": "2d273261a11db830c7a0e1bde325475be4161ff4d3167bfd6f8ab31335826e18"}, "downloads": -1, "filename": "dhlab-0.7.2.tar.gz", "has_sig": false, "md5_digest": "990c4905e6bb97534265170e6da0ac40", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.2", "size": 36394, "upload_time": "2019-09-08T22:09:14", "upload_time_iso_8601": "2019-09-08T22:09:14.650848Z", "url": "https://files.pythonhosted.org/packages/f0/42/74c3fe2393e2ece78175f906a95cce2811c2f11172b984d19c55fd6a8475/dhlab-0.7.2.tar.gz", "yanked": false}], "0.7.3": [{"comment_text": "", "digests": {"md5": "261af9fa942a3af7a265258e9a90eea8", "sha256": "bb01c7f3d547006a8ec809efe89461b17f34f47c0a1014b54b0850c838eb94ff"}, "downloads": -1, "filename": "dhlab-0.7.3-py3-none-any.whl", "has_sig": false, "md5_digest": "261af9fa942a3af7a265258e9a90eea8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.2", "size": 63731, "upload_time": "2019-09-08T22:09:13", "upload_time_iso_8601": "2019-09-08T22:09:13.268197Z", "url": "https://files.pythonhosted.org/packages/51/ae/40010aee27618f5172566ceb986a7decabe0a0ebd475f5448d137e7ab885/dhlab-0.7.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bda627c86fc338b7dbbc949d571a66a3", "sha256": "78ecb7f342acf3ffb5a9fcd2ab618a7e0df8d78e6943cc0ed21cb74e65b72df4"}, "downloads": -1, "filename": "dhlab-0.7.3.tar.gz", "has_sig": false, "md5_digest": "bda627c86fc338b7dbbc949d571a66a3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.2", "size": 36408, "upload_time": "2019-09-08T22:09:15", "upload_time_iso_8601": "2019-09-08T22:09:15.902715Z", "url": "https://files.pythonhosted.org/packages/be/55/5ee87f63d71f63f55e6d96fab7eb7159524832101bae2d92db597fdf22f8/dhlab-0.7.3.tar.gz", "yanked": false}], "0.7.4": [{"comment_text": "", "digests": {"md5": "96a13941a79877126ddb00c9b5c54cb4", "sha256": "0f67367e915151155150ca0d8ef6f78ebe8eaa1a88c7b66e9399401ae469bd2d"}, "downloads": -1, "filename": "dhlab-0.7.4-py3-none-any.whl", "has_sig": false, "md5_digest": "96a13941a79877126ddb00c9b5c54cb4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.2", "size": 63731, "upload_time": "2019-09-08T22:11:18", "upload_time_iso_8601": "2019-09-08T22:11:18.800932Z", "url": "https://files.pythonhosted.org/packages/91/18/7efd718759cd4e2a7e23fcd00dee2a4aff135a51c7aefb7752b97857343d/dhlab-0.7.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fd31c908ac655752d76940a92b8c0de0", "sha256": "c5539a70efe30ee9cae91d35d538fcefbbe3ea1e33765b184652102b66c0f921"}, "downloads": -1, "filename": "dhlab-0.7.4.tar.gz", "has_sig": false, "md5_digest": "fd31c908ac655752d76940a92b8c0de0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.2", "size": 36406, "upload_time": "2019-09-08T22:11:20", "upload_time_iso_8601": "2019-09-08T22:11:20.227980Z", "url": "https://files.pythonhosted.org/packages/95/25/d0ab83a8c92d29748075546b97196c349bf153ebaafdae3277ec1bfb99e5/dhlab-0.7.4.tar.gz", "yanked": false}], "0.7.5": [{"comment_text": "", "digests": {"md5": "295cf830ce360295aed48df3f6364490", "sha256": "af1ffa5332d09f3c440731e9fb37492acf7da87d39b3fe61c9b6f0ff8c8de693"}, "downloads": -1, "filename": "dhlab-0.7.5-py3-none-any.whl", "has_sig": false, "md5_digest": "295cf830ce360295aed48df3f6364490", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.2", "size": 63607, "upload_time": "2019-09-08T22:18:14", "upload_time_iso_8601": "2019-09-08T22:18:14.953649Z", "url": "https://files.pythonhosted.org/packages/a8/01/f279806c8c2da1b99aca74d85b50ea6f28134270b960e3d6fdc58802afcf/dhlab-0.7.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1f4d33a0394bb1f2e67583911ee51bf2", "sha256": "1ff6f8f666102ba2e886ae63cfb5a1eb0707006d18f000d3f032fcfc1445cec6"}, "downloads": -1, "filename": "dhlab-0.7.5.tar.gz", "has_sig": false, "md5_digest": "1f4d33a0394bb1f2e67583911ee51bf2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.2", "size": 36381, "upload_time": "2019-09-08T22:18:16", "upload_time_iso_8601": "2019-09-08T22:18:16.678256Z", "url": "https://files.pythonhosted.org/packages/a8/52/006959d72f40682cc76ed86fffb5335306d553923a01206dd3f711d24e7e/dhlab-0.7.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "295cf830ce360295aed48df3f6364490", "sha256": "af1ffa5332d09f3c440731e9fb37492acf7da87d39b3fe61c9b6f0ff8c8de693"}, "downloads": -1, "filename": "dhlab-0.7.5-py3-none-any.whl", "has_sig": false, "md5_digest": "295cf830ce360295aed48df3f6364490", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.2", "size": 63607, "upload_time": "2019-09-08T22:18:14", "upload_time_iso_8601": "2019-09-08T22:18:14.953649Z", "url": "https://files.pythonhosted.org/packages/a8/01/f279806c8c2da1b99aca74d85b50ea6f28134270b960e3d6fdc58802afcf/dhlab-0.7.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1f4d33a0394bb1f2e67583911ee51bf2", "sha256": "1ff6f8f666102ba2e886ae63cfb5a1eb0707006d18f000d3f032fcfc1445cec6"}, "downloads": -1, "filename": "dhlab-0.7.5.tar.gz", "has_sig": false, "md5_digest": "1f4d33a0394bb1f2e67583911ee51bf2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.2", "size": 36381, "upload_time": "2019-09-08T22:18:16", "upload_time_iso_8601": "2019-09-08T22:18:16.678256Z", "url": "https://files.pythonhosted.org/packages/a8/52/006959d72f40682cc76ed86fffb5335306d553923a01206dd3f711d24e7e/dhlab-0.7.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:38:39 2020"}