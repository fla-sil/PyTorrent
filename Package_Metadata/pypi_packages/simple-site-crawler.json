{"info": {"author": "Pawe\u0142 Adamczak", "author_email": "pawel.ad@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Intended Audience :: End Users/Desktop", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Utilities"], "description": "simple-site-crawler\n===================\n\n|Build status| |Test coverage| |PyPI version| |Python versions|\n|License|\n\nSimple website crawler that asynchronously crawls a website and all\nsubpages that it can find, along with static content that they rely on.\nYou can either use it as a library, inside your Python project or check\nout the provided CLI that can currently show you the crawled data\n(links, images, CSS and Javascript files) for each found site and create\na ``sitemap.xml`` file.\n\nCreated primarily to play with ``asyncio``, ``aiohttp`` and the new\n``async/await`` syntax, so:\n\n-  it requires Python 3.5 or higher\n-  new features are not planned at the moment; feel free to suggest them\n   though, as I'm happy to implement them if someone will actually use\n   them ; -)\n\nFull disclosure - halfway through the project I found\n`this <http://aosabook.org/en/500L/a-web-crawler-with-asyncio-coroutines.html>`__\narticle (and code) which does pretty much exactly what I wanted and is\nco-written by the BDFL himself. Oh well. I still finished the project\nand didn't copy anything explicitly but it did influence some of my\nchoices. After all, if it's good enough for the creator of the language\nI'm using, it's probably good enough for me.\n\nInstallation\n------------\n\n>From PyPI:\n\n::\n\n    $ pip3 install simple-site-crawler\n\nWith git clone:\n\n::\n\n    $ git clone https://github.com/pawelad/simple-site-crawler\n    $ pip3 install -r simple-site-crawler/requirements.txt\n    $ cd simple-site-crawler/bin\n\nUsage\n-----\n\n::\n\n    $ simple-site-crawler --help                      \n    Usage: simple-site-crawler [OPTIONS] URL\n\n      Simple website crawler that generates its sitemap and can either print it\n      (and its static content) or export it to standard XML format.\n\n      See https://github.com/pawelad/simple-site-crawler for more info.\n\n    Options:\n      -t, --max-tasks INTEGER  Maximum allowed number of async tasks.\n      -e, --export-to-xml      Export sitemap to XML file.\n      -s, --suppress           Suppress printing output to stdout.\n      --help                   Show this message and exit.\n\nAPI\n---\n\nThere's no proper documentation as of now, but the code is commented and\n*should* be pretty straightforward to use.\n\nThat said - feel free to ask me either via\n`email <mailto:pawel.ad@gmail.com>`__ or `GitHub\nissues <https://github.com/pawelad/simple-site-crawler/issues/new>`__ if\nanything is unclear.\n\nTests\n-----\n\nPackage was tested with the help of ``py.test`` and ``tox`` on Python\n3.5 and 3.6 (see ``tox.ini``).\n\nCode coverage is available at\n`Coveralls <https://coveralls.io/github/pawelad/simple-site-crawler>`__.\n\nTo run tests yourself you need to run ``tox`` inside the repository:\n\n.. code:: shell\n\n    $ pip install -r requirements/dev.txt\n    $ tox\n\nContributions\n-------------\n\nPackage source code is available at\n`GitHub <https://github.com/pawelad/simple-site-crawler>`__.\n\nFeel free to use, ask, fork, star, report bugs, fix them, suggest\nenhancements, add functionality and point out any mistakes. Thanks!\n\nAuthors\n-------\n\nDeveloped and maintained by `Pawe\u0142\nAdamczak <https://github.com/pawelad>`__.\n\nReleased under `MIT\nLicense <https://github.com/pawelad/simple-site-crawler/blob/master/LICENSE>`__.\n\n.. |Build status| image:: https://img.shields.io/travis/pawelad/simple-site-crawler.svg\n   :target: https://travis-ci.org/pawelad/simple-site-crawler\n.. |Test coverage| image:: https://img.shields.io/coveralls/pawelad/simple-site-crawler.svg\n   :target: https://coveralls.io/github/pawelad/simple-site-crawler\n.. |PyPI version| image:: https://img.shields.io/pypi/v/simple-site-crawler.svg\n   :target: https://pypi.python.org/pypi/simple-site-crawler\n.. |Python versions| image:: https://img.shields.io/pypi/pyversions/simple-site-crawler.svg\n   :target: https://pypi.python.org/pypi/simple-site-crawler\n.. |License| image:: https://img.shields.io/github/license/pawelad/simple-site-crawler.svg\n   :target: https://github.com/pawelad/simple-site-crawler/blob/master/LICENSE\n\n\n", "description_content_type": null, "docs_url": null, "download_url": "https://github.com/pawelad/simple-site-crawler/releases/latest", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/pawelad/simple-site-crawler", "keywords": "website crawler sitemap", "license": "MIT License", "maintainer": "", "maintainer_email": "", "name": "simple-site-crawler", "package_url": "https://pypi.org/project/simple-site-crawler/", "platform": "", "project_url": "https://pypi.org/project/simple-site-crawler/", "project_urls": {"Download": "https://github.com/pawelad/simple-site-crawler/releases/latest", "Homepage": "https://github.com/pawelad/simple-site-crawler"}, "release_url": "https://pypi.org/project/simple-site-crawler/0.1.1/", "requires_dist": ["aiodns (>=1.1.1)", "aiohttp (>=1.2.0)", "beautifulsoup4 (>=4.5.3)", "cchardet (>=1.1.2)", "click (>=6.7)", "html5lib (>=1.0b10)", "pytest; extra == 'testing'"], "requires_python": "", "summary": "Simple website crawler that asynchronously crawls a website and all subpages that it can find, along with static content that they rely on.", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://travis-ci.org/pawelad/simple-site-crawler\" rel=\"nofollow\"><img alt=\"Build status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f2c25936bcf52aebd3e63ff4ce557a0367a1c0c4/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f706177656c61642f73696d706c652d736974652d637261776c65722e737667\"></a> <a href=\"https://coveralls.io/github/pawelad/simple-site-crawler\" rel=\"nofollow\"><img alt=\"Test coverage\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fd66f69a6da25607cbbf09be6513ea3fdf7a8d88/68747470733a2f2f696d672e736869656c64732e696f2f636f766572616c6c732f706177656c61642f73696d706c652d736974652d637261776c65722e737667\"></a> <a href=\"https://pypi.python.org/pypi/simple-site-crawler\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/250944053979140a4d90b8d724c8fd46eab7bbf1/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f73696d706c652d736974652d637261776c65722e737667\"></a> <a href=\"https://pypi.python.org/pypi/simple-site-crawler\" rel=\"nofollow\"><img alt=\"Python versions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/12717888d8024d4d9a6c7f2b24eb2253d68d0580/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f73696d706c652d736974652d637261776c65722e737667\"></a>\n<a href=\"https://github.com/pawelad/simple-site-crawler/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e82a193cce4920b610586d4cffa52461efa8ed79/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f706177656c61642f73696d706c652d736974652d637261776c65722e737667\"></a></p>\n<p>Simple website crawler that asynchronously crawls a website and all\nsubpages that it can find, along with static content that they rely on.\nYou can either use it as a library, inside your Python project or check\nout the provided CLI that can currently show you the crawled data\n(links, images, CSS and Javascript files) for each found site and create\na <tt>sitemap.xml</tt> file.</p>\n<p>Created primarily to play with <tt>asyncio</tt>, <tt>aiohttp</tt> and the new\n<tt>async/await</tt> syntax, so:</p>\n<ul>\n<li>it requires Python 3.5 or higher</li>\n<li>new features are not planned at the moment; feel free to suggest them\nthough, as I\u2019m happy to implement them if someone will actually use\nthem ; -)</li>\n</ul>\n<p>Full disclosure - halfway through the project I found\n<a href=\"http://aosabook.org/en/500L/a-web-crawler-with-asyncio-coroutines.html\" rel=\"nofollow\">this</a>\narticle (and code) which does pretty much exactly what I wanted and is\nco-written by the BDFL himself. Oh well. I still finished the project\nand didn\u2019t copy anything explicitly but it did influence some of my\nchoices. After all, if it\u2019s good enough for the creator of the language\nI\u2019m using, it\u2019s probably good enough for me.</p>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>&gt;From PyPI:</p>\n<pre>$ pip3 install simple-site-crawler\n</pre>\n<p>With git clone:</p>\n<pre>$ git clone https://github.com/pawelad/simple-site-crawler\n$ pip3 install -r simple-site-crawler/requirements.txt\n$ cd simple-site-crawler/bin\n</pre>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<pre>$ simple-site-crawler --help\nUsage: simple-site-crawler [OPTIONS] URL\n\n  Simple website crawler that generates its sitemap and can either print it\n  (and its static content) or export it to standard XML format.\n\n  See https://github.com/pawelad/simple-site-crawler for more info.\n\nOptions:\n  -t, --max-tasks INTEGER  Maximum allowed number of async tasks.\n  -e, --export-to-xml      Export sitemap to XML file.\n  -s, --suppress           Suppress printing output to stdout.\n  --help                   Show this message and exit.\n</pre>\n</div>\n<div id=\"api\">\n<h2>API</h2>\n<p>There\u2019s no proper documentation as of now, but the code is commented and\n<em>should</em> be pretty straightforward to use.</p>\n<p>That said - feel free to ask me either via\n<a href=\"mailto:pawel.ad%40gmail.com\">email</a> or <a href=\"https://github.com/pawelad/simple-site-crawler/issues/new\" rel=\"nofollow\">GitHub\nissues</a> if\nanything is unclear.</p>\n</div>\n<div id=\"tests\">\n<h2>Tests</h2>\n<p>Package was tested with the help of <tt>py.test</tt> and <tt>tox</tt> on Python\n3.5 and 3.6 (see <tt>tox.ini</tt>).</p>\n<p>Code coverage is available at\n<a href=\"https://coveralls.io/github/pawelad/simple-site-crawler\" rel=\"nofollow\">Coveralls</a>.</p>\n<p>To run tests yourself you need to run <tt>tox</tt> inside the repository:</p>\n<pre>$ pip install -r requirements/dev.txt\n$ tox\n</pre>\n</div>\n<div id=\"contributions\">\n<h2>Contributions</h2>\n<p>Package source code is available at\n<a href=\"https://github.com/pawelad/simple-site-crawler\" rel=\"nofollow\">GitHub</a>.</p>\n<p>Feel free to use, ask, fork, star, report bugs, fix them, suggest\nenhancements, add functionality and point out any mistakes. Thanks!</p>\n</div>\n<div id=\"authors\">\n<h2>Authors</h2>\n<p>Developed and maintained by <a href=\"https://github.com/pawelad\" rel=\"nofollow\">Pawe\u0142\nAdamczak</a>.</p>\n<p>Released under <a href=\"https://github.com/pawelad/simple-site-crawler/blob/master/LICENSE\" rel=\"nofollow\">MIT\nLicense</a>.</p>\n</div>\n\n          </div>"}, "last_serial": 2607621, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "37152544d3a07b9279db4e63ab2548bb", "sha256": "7e4317fa3d85675560d56c05083904b361a0c92b00e3b7ca3c2ad036a5943b44"}, "downloads": -1, "filename": "simple_site_crawler-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "37152544d3a07b9279db4e63ab2548bb", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 14505, "upload_time": "2017-01-18T23:41:01", "upload_time_iso_8601": "2017-01-18T23:41:01.165167Z", "url": "https://files.pythonhosted.org/packages/e1/62/0eafac1c281d00feeb3cca3c48263571694da2f5bf11ca622b03c59651e5/simple_site_crawler-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "96c39c1f7e9892df66607cbb4b946098", "sha256": "3009f6de271528996f34b9421c2ef41fae2c1c584967bd8665cbee52425005f4"}, "downloads": -1, "filename": "simple-site-crawler-0.1.0.tar.gz", "has_sig": false, "md5_digest": "96c39c1f7e9892df66607cbb4b946098", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12069, "upload_time": "2017-01-18T23:41:02", "upload_time_iso_8601": "2017-01-18T23:41:02.999814Z", "url": "https://files.pythonhosted.org/packages/af/63/8a33754eb08318edceccd2d0d03f39fb70f38b942ca3e917bf3603e8f889/simple-site-crawler-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "eba58b782dbcc23c1283ac1207df50d8", "sha256": "523ac3b8ff62bf695864e8c7cebbddc9fdf868c56d33c4dfd621010397afb33d"}, "downloads": -1, "filename": "simple_site_crawler-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "eba58b782dbcc23c1283ac1207df50d8", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 14858, "upload_time": "2017-01-30T21:04:36", "upload_time_iso_8601": "2017-01-30T21:04:36.930409Z", "url": "https://files.pythonhosted.org/packages/4f/37/5d1f6af51327ba580468c23600a00af01b59b51ff652e05b984547a7353b/simple_site_crawler-0.1.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fea01c46736e64371de1a0a1b4d9e5f1", "sha256": "28089a9c70d322e289a8a6d8added7c0c65cd2faee124731bb421be0c8fc9255"}, "downloads": -1, "filename": "simple-site-crawler-0.1.1.tar.gz", "has_sig": false, "md5_digest": "fea01c46736e64371de1a0a1b4d9e5f1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12227, "upload_time": "2017-01-30T21:04:38", "upload_time_iso_8601": "2017-01-30T21:04:38.325965Z", "url": "https://files.pythonhosted.org/packages/cc/06/eb99ecea4e45d52761a525ee84b75282fb18d109b3442465185bc0c7d407/simple-site-crawler-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "eba58b782dbcc23c1283ac1207df50d8", "sha256": "523ac3b8ff62bf695864e8c7cebbddc9fdf868c56d33c4dfd621010397afb33d"}, "downloads": -1, "filename": "simple_site_crawler-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "eba58b782dbcc23c1283ac1207df50d8", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 14858, "upload_time": "2017-01-30T21:04:36", "upload_time_iso_8601": "2017-01-30T21:04:36.930409Z", "url": "https://files.pythonhosted.org/packages/4f/37/5d1f6af51327ba580468c23600a00af01b59b51ff652e05b984547a7353b/simple_site_crawler-0.1.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fea01c46736e64371de1a0a1b4d9e5f1", "sha256": "28089a9c70d322e289a8a6d8added7c0c65cd2faee124731bb421be0c8fc9255"}, "downloads": -1, "filename": "simple-site-crawler-0.1.1.tar.gz", "has_sig": false, "md5_digest": "fea01c46736e64371de1a0a1b4d9e5f1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12227, "upload_time": "2017-01-30T21:04:38", "upload_time_iso_8601": "2017-01-30T21:04:38.325965Z", "url": "https://files.pythonhosted.org/packages/cc/06/eb99ecea4e45d52761a525ee84b75282fb18d109b3442465185bc0c7d407/simple-site-crawler-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:10:08 2020"}