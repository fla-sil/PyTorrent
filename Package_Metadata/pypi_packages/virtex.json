{"info": {"author": "Chris Larson", "author_email": "chris7larson@gmail.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "## Virtex\nVirtex is an asyncronous Python serving framework for computational workloads. \n\n![release](https://github.com/chrislarson1/virtex/workflows/release/badge.svg?branch=master)\n\n### Contents\n|                     Section                      |               Description             |\n|:------------------------------------------------:|:-------------------------------------:|\n| [Design Principles](#design-principles)          | Philosopy & implementation |\n| [Features](#features)                            | Feature overview                      |\n| [Installation](#installation)                    | How to install the package            |\n| [Framework](#framework)                          | Virtex overview                       |\n| [Example](#example)                              | Resnet 50 v2 example                  |\n| [Performance](#performance)                      | Performance comparison                |\n| [Documentation][(1.0.0)](http://virtex.ai/docs)  | Full API documentation and more       |\n\n### Design principles\n\n##### Philosophy \n- Flexibility: no restrictions on application, model, implementation, deep learning framework etc.\n- Shared-nothing: single thread, single event loop, offload expensive compute to an accelerator.\n- Scaling: your autoscaling/load balancing solution of choice; eliminate expensive interprocess communcation.\n\n##### Implementation\n- Coroutines and futures.\n- Server side computation is partitioned into three execution blocks (implemented as callbacks): \n    1. Request deserialization, preprocessing, and batching\n    2. Inference\n    3. Postprocessing & response serialization \n- Callbacks can be written in blocking or async Python code.\n- HTTP transport layer abstracted away as much as possible.\n\n### Features\n- Functional API.\n- High performance.\n- Execution pattern that is entirely agnostic to your model and implementation.\n- Simple callback template/pattern that takes minutes to refactor your code into.\n- A unified data structure for defining and executing your SLA.\n- Built-in (de)serialization for transporting commonly used data structures:\n    - Numpy arrays\n    - Pandas Dataframes\n    - PIL Images\n    - OpenCV Images\n    - Byte arrays (``bytearray`` and ``bytes``)\n- Support for user-specified data serialization.\n- Serving metrics dashboards (Prometheus, TensorboardX)\n\n### Installation\nVirtex is Python 3.6.5+ compatible.\n\n#### With pip\n```bash\npip install virtex\n```\n\n#### From source\n```bash\ngit clone https://github.com/chrislarson1/virtex.git && cd virtex\npip install -r requirements.txt --user\npip install . --user\n```\n\n### Framework\n\nSee <a href=\"https://virtex.ai/docs/virtex/types\">API documentation</a> for full details.\n\n#### Service Contract\nVirtex service contracts are defined using the `Message` class. Internally messages take the form:\n\n ``{ data: [ x1, ..., xn ] }``\n \nwhere `xi` is the *ith* data element of the message. Importantly, `Message.data` always has type `list[Any]`; this \nprovides a consistent format to send both serial and batch requests. The `Message` class has built-in methods to support\njson serialization, `Message.encode(cb_f)` and `Message.decode(cb_f)`, where `cb_f` represents a callback function that \nencodes or decodes each element in `data` (element-wise). Below is an example of how to construct a message with two \nnumpy array data elements, serialize it into a json string, and then deserialize the json string back into the original \nmessage.\n\n```python\nimport json\nimport numpy as np\nfrom virtex.types import Message\nfrom virtex.serial import encode_np, decode_np\n\n# Request data\nreq1 = np.array([[0.3, 0.1],\n                 [1.0, 0.5]], dtype=np.float32)\nreq2 = np.array([[0.0, 0.4], \n                 [0.0, 0.2]], dtype=np.float32)\n\n# Request message\nmsg = Message(data=[req1, req2])\n\n# Encode numpy array to bytestring\nmsg.encode(encode_np)\n\n# Validate that the message is serializable\nmsg.validate()\n\n# Get json string\nmsg_str = msg.json\n\n# Recover original message\nmsg = Message(**json.loads(msg_str))\n\n# Recover numpy arrays\nmsg.decode(decode_np)\n```\n\n#### Server\nYou can spin up a Virtex `Server` as follows:\n\n```python \nfrom virtex.http import Server\n\nServer(\n    name='your_service_name',\n    input_cb=your_input_cb,\n    inference_cb=your_inference_cb,\n    output_cb=your_output_cb,\n    max_batch_size=1024,\n    max_time_on_queue=10\n).start()\n```\n\nwhere `your_input_cb`,`your_inference_cb`, and `your_output_cb` are callback functions that execute your computation. \nThe server manages a FSM that consists of an input and an output queue. Incoming requests get deserialzed into a\n`Message`, and the `data` elements in that message get unpacked onto the input queue. A coroutine continously polls the \ninput queue; its behavior is controlled through the `max_batch_size` and `max_time_on_queue` flags, which specify the \nmaximum queue size and the maximum time (in milliseconds) inbetween model executions. The server will accumulate items \non the queue until one of these conditions is met, and then proceed to execute the user callback functions.\n\n\n##### Input Callback\nWhen triggered, the server will remove items from the input queue (up to `max_batch_size`) and pass them to the input \ncallback function. The input callback always accepts a list of data elements, and returns a batched input. The purpose \nof this function is to perform any deserialization, preprocessing, and batching tasks necessary to convert the list of \ndata elements into a batched model input.\n\n```python \ndef input_cb(items:list[Any]) -> batch:Any\n```\n\n##### Inference Callback\nThe inference callback is typically a one-liner, something akin to `model.predict(batch)`. This function should consist \nof model execution code, and little if not nothing more. Importantly, `output` must satisfy two conditions: (1) \nit must be ordered with respect to the input batch, and (2) it must be an iterable object with length equal to that of\nthe items passed into the function. Keep in mind that Numpy arrays and Pandas Dataframes are iterable objects.\n\n```python \ndef inference_cb(batch:Any) -> output:Any\n```\n\n##### Output Callback\nThe server takes the batched output of the inference callback, iterates through it along the batch dimension, passing\neach output to the output callback, which performs post-processing and serialization necessary to form each response \ndata element.\n\n```python \ndef output_cb(result:Any) -> data_elem:Any\n```\n\n#### Client\nThe Virtex `Client` is very simple to use. Let's assume that our server model accepts PIL images and returns a numpy \narray. The flow might look something like this:\n\n```python \nimport numpy as np\nfrom PIL import Image\nfrom virtex.http import Client\nfrom virtex.serial import encode_pil, decode_np\n\nimg = Image.load_img(\"path/to/image_file\")\nmsg = Message(data=[img])\nmsg.encode(encode_pil)\n\nclient = Client()\nresp = client.post(msg)\nresp.decode(decode_np)\n\n# The response data elements are here\ndata = resp.data\n\n# Do stuff ...\n```\n\nThe response data and request data are bijective with order preserved.\n\n### Example\nVirtex is extremely flexible and easy to use. Below is an example that covers the key concepts that will allow you to \nstart using Virtex in your application. We're going to create a ResNet50-V2 image classification service. The source \ncode for this and other examples can be found here <a href=\"https://github.com/chrislarson1/virtex/examples\">here</a>.\n\n##### Server Code\n```python\nimport numpy as np\nfrom PIL import Image\nfrom keras.applications import resnet_v2\nfrom keras.applications import resnet50\nfrom keras_preprocessing import image\nfrom virtex.types import Message\nfrom virtex.http import Server\nfrom virtex.serial import (decode_image, \n                           encode_bytes,\n                           encode_np)\n\n# ResNetV2 Model\nmodel = resnet_v2.ResNet50V2(weights='imagenet')\n\n# Define your computation in the form of callbacks\ndef resnet_input_cb(items:list):\n    for i, item in enumerate(items):\n        img = decode_image(item)\n        img = img.convert('RGB')\n        img = img.resize((224, 224), Image.NEAREST)\n        items[i] = resnet50.preprocess_input(image.img_to_array(img))\n    return np.stack(items, axis=0)\n\nresnet_inference_cb = model.predict\n\nresnet_output_cb = encode_np\n\n# Instantiate server, pass in callbacks\nserver = Server(\n    name='resnet_50_v2_image_classification_service',\n    host='127.0.0.1',\n    port=8081,\n    input_cb=resnet_input_cb,\n    inference_cb=resnet_inference_cb,\n    output_cb=resnet_output_cb,\n    max_batch_size=1024,\n    max_time_on_queue=10\n)\n\n# Validate the server using client's message type and run it\nimg = open('path/to/image-file', 'rb').read()\nmessage = Message(data=[img])\nmessage.encode(encode_bytes)\nif server.validate(message):\n    server.start()\n```\n\n##### Client Code\n```python\nfrom virtex.types import Message\nfrom virtex.http import Client\nfrom virtex.serial import encode_bytes, decode_np\n\n# Server endpoint\nurl = \"http://127.0.0.1:8081\"\n\n# Load your image\nimage = open('path/to/image-file', 'rb').read()\n\n# Create your message\nmessage = Message(data=[image])\n\n# Encode your message\nmessage.encode(encode_bytes)\n\n# Validate your message\nmessage.validate()\n\n# Create your client\nclient = Client()\n\n# Post your request, get response\nresponse = client.post(url, message)\n\n# Decode your response\nresponse.decode(decode_np)\n```\n\n### Performance\nVirtex is a performant machine learning model server, implementing dynamic batching in a similar fasion as other \ninference engines such as tensorflow serving. Virtex was designed with an emphasis on flexibility for on-line inference \napplications, in which requests are not being aggregated (batched) on the client side but rather sent serially. Below \nare two plots showing the performance of virtex relative to other serving engines (TF-serving and BERT-serving) for two \nfairly large machine learning models, Resnet50V2 and BERT base respectively. Unlike Virtex, these engines are optimized \nfor batched requests. \n\n![Resnet_Perf](https://raw.githubusercontent.com/chrislarson1/virtex/master/docs/resnet_perf_tps.png)\n![BERT_Perf](https://raw.githubusercontent.com/chrislarson1/virtex/master/docs/resnet_perf_latency.png)\n\n![Resnet_Perf](https://raw.githubusercontent.com/chrislarson1/virtex/master/docs/bert_perf_tps.png)\n![BERT_Perf](https://raw.githubusercontent.com/chrislarson1/virtex/master/docs/bert_perf_latency.png)\n\nFor the BERT-serving benchmark grid search was performed to obtain semi-optimal parameters: ``n_workers=4``, \n``gpu_frac=0.25``, ``pooling=-1``, ``max_bs=32``, ``-xla``, and ``max_seq_len=64``. The Virtex benchmark was obtained\nfrom a single core (i.e., ``n_workers=1``) and the same ``max_seq_len=64``. All experiments were run on an Intel 6850K\nCPU (6-core, 32GB memory) and a single Nvidia GTX 1080ti GPU (12GB memory).\n\nNote: These are not true benchmarks, don't interpret them as such. Performance testing code can be found \n<a href=\"https://github.com/chrislarson1/virtex/tree/master/benchmark\">here</a>.\n\n### Citation\nIf you use Virtex in your research, feel free to cite it!\n```bibtex\n@misc{Larson2020,\n  author = {Larson, Chris},\n  title = {Virtex},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/chrislarson1/virtex}},\n  commit = {}\n}\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/chrislarson1/virtex.git", "keywords": "machine deep learning ai serving asyncronous microservice", "license": "Apache Version 2.0", "maintainer": "", "maintainer_email": "", "name": "virtex", "package_url": "https://pypi.org/project/virtex/", "platform": "", "project_url": "https://pypi.org/project/virtex/", "project_urls": {"Homepage": "https://github.com/chrislarson1/virtex.git"}, "release_url": "https://pypi.org/project/virtex/1.0.0/", "requires_dist": null, "requires_python": "", "summary": "Serving for computational workloads", "version": "1.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h2>Virtex</h2>\n<p>Virtex is an asyncronous Python serving framework for computational workloads.</p>\n<p><img alt=\"release\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/faf96c3f39b2b161d5d1e5142a282266f5078610/68747470733a2f2f6769746875622e636f6d2f63687269736c6172736f6e312f7669727465782f776f726b666c6f77732f72656c656173652f62616467652e7376673f6272616e63683d6d6173746572\"></p>\n<h3>Contents</h3>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Section</th>\n<th align=\"center\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\"><a href=\"#design-principles\" rel=\"nofollow\">Design Principles</a></td>\n<td align=\"center\">Philosopy &amp; implementation</td>\n</tr>\n<tr>\n<td align=\"center\"><a href=\"#features\" rel=\"nofollow\">Features</a></td>\n<td align=\"center\">Feature overview</td>\n</tr>\n<tr>\n<td align=\"center\"><a href=\"#installation\" rel=\"nofollow\">Installation</a></td>\n<td align=\"center\">How to install the package</td>\n</tr>\n<tr>\n<td align=\"center\"><a href=\"#framework\" rel=\"nofollow\">Framework</a></td>\n<td align=\"center\">Virtex overview</td>\n</tr>\n<tr>\n<td align=\"center\"><a href=\"#example\" rel=\"nofollow\">Example</a></td>\n<td align=\"center\">Resnet 50 v2 example</td>\n</tr>\n<tr>\n<td align=\"center\"><a href=\"#performance\" rel=\"nofollow\">Performance</a></td>\n<td align=\"center\">Performance comparison</td>\n</tr>\n<tr>\n<td align=\"center\">[Documentation]<a href=\"http://virtex.ai/docs\" rel=\"nofollow\">(1.0.0)</a></td>\n<td align=\"center\">Full API documentation and more</td>\n</tr></tbody></table>\n<h3>Design principles</h3>\n<h5>Philosophy</h5>\n<ul>\n<li>Flexibility: no restrictions on application, model, implementation, deep learning framework etc.</li>\n<li>Shared-nothing: single thread, single event loop, offload expensive compute to an accelerator.</li>\n<li>Scaling: your autoscaling/load balancing solution of choice; eliminate expensive interprocess communcation.</li>\n</ul>\n<h5>Implementation</h5>\n<ul>\n<li>Coroutines and futures.</li>\n<li>Server side computation is partitioned into three execution blocks (implemented as callbacks):\n<ol>\n<li>Request deserialization, preprocessing, and batching</li>\n<li>Inference</li>\n<li>Postprocessing &amp; response serialization</li>\n</ol>\n</li>\n<li>Callbacks can be written in blocking or async Python code.</li>\n<li>HTTP transport layer abstracted away as much as possible.</li>\n</ul>\n<h3>Features</h3>\n<ul>\n<li>Functional API.</li>\n<li>High performance.</li>\n<li>Execution pattern that is entirely agnostic to your model and implementation.</li>\n<li>Simple callback template/pattern that takes minutes to refactor your code into.</li>\n<li>A unified data structure for defining and executing your SLA.</li>\n<li>Built-in (de)serialization for transporting commonly used data structures:\n<ul>\n<li>Numpy arrays</li>\n<li>Pandas Dataframes</li>\n<li>PIL Images</li>\n<li>OpenCV Images</li>\n<li>Byte arrays (<code>bytearray</code> and <code>bytes</code>)</li>\n</ul>\n</li>\n<li>Support for user-specified data serialization.</li>\n<li>Serving metrics dashboards (Prometheus, TensorboardX)</li>\n</ul>\n<h3>Installation</h3>\n<p>Virtex is Python 3.6.5+ compatible.</p>\n<h4>With pip</h4>\n<pre>pip install virtex\n</pre>\n<h4>From source</h4>\n<pre>git clone https://github.com/chrislarson1/virtex.git <span class=\"o\">&amp;&amp;</span> <span class=\"nb\">cd</span> virtex\npip install -r requirements.txt --user\npip install . --user\n</pre>\n<h3>Framework</h3>\n<p>See <a href=\"https://virtex.ai/docs/virtex/types\" rel=\"nofollow\">API documentation</a> for full details.</p>\n<h4>Service Contract</h4>\n<p>Virtex service contracts are defined using the <code>Message</code> class. Internally messages take the form:</p>\n<p><code>{ data: [ x1, ..., xn ] }</code></p>\n<p>where <code>xi</code> is the <em>ith</em> data element of the message. Importantly, <code>Message.data</code> always has type <code>list[Any]</code>; this\nprovides a consistent format to send both serial and batch requests. The <code>Message</code> class has built-in methods to support\njson serialization, <code>Message.encode(cb_f)</code> and <code>Message.decode(cb_f)</code>, where <code>cb_f</code> represents a callback function that\nencodes or decodes each element in <code>data</code> (element-wise). Below is an example of how to construct a message with two\nnumpy array data elements, serialize it into a json string, and then deserialize the json string back into the original\nmessage.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">virtex.types</span> <span class=\"kn\">import</span> <span class=\"n\">Message</span>\n<span class=\"kn\">from</span> <span class=\"nn\">virtex.serial</span> <span class=\"kn\">import</span> <span class=\"n\">encode_np</span><span class=\"p\">,</span> <span class=\"n\">decode_np</span>\n\n<span class=\"c1\"># Request data</span>\n<span class=\"n\">req1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">],</span>\n                 <span class=\"p\">[</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n<span class=\"n\">req2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.4</span><span class=\"p\">],</span> \n                 <span class=\"p\">[</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Request message</span>\n<span class=\"n\">msg</span> <span class=\"o\">=</span> <span class=\"n\">Message</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">req1</span><span class=\"p\">,</span> <span class=\"n\">req2</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Encode numpy array to bytestring</span>\n<span class=\"n\">msg</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">encode_np</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Validate that the message is serializable</span>\n<span class=\"n\">msg</span><span class=\"o\">.</span><span class=\"n\">validate</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Get json string</span>\n<span class=\"n\">msg_str</span> <span class=\"o\">=</span> <span class=\"n\">msg</span><span class=\"o\">.</span><span class=\"n\">json</span>\n\n<span class=\"c1\"># Recover original message</span>\n<span class=\"n\">msg</span> <span class=\"o\">=</span> <span class=\"n\">Message</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">msg_str</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Recover numpy arrays</span>\n<span class=\"n\">msg</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"n\">decode_np</span><span class=\"p\">)</span>\n</pre>\n<h4>Server</h4>\n<p>You can spin up a Virtex <code>Server</code> as follows:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">virtex.http</span> <span class=\"kn\">import</span> <span class=\"n\">Server</span>\n\n<span class=\"n\">Server</span><span class=\"p\">(</span>\n    <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'your_service_name'</span><span class=\"p\">,</span>\n    <span class=\"n\">input_cb</span><span class=\"o\">=</span><span class=\"n\">your_input_cb</span><span class=\"p\">,</span>\n    <span class=\"n\">inference_cb</span><span class=\"o\">=</span><span class=\"n\">your_inference_cb</span><span class=\"p\">,</span>\n    <span class=\"n\">output_cb</span><span class=\"o\">=</span><span class=\"n\">your_output_cb</span><span class=\"p\">,</span>\n    <span class=\"n\">max_batch_size</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">,</span>\n    <span class=\"n\">max_time_on_queue</span><span class=\"o\">=</span><span class=\"mi\">10</span>\n<span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"p\">()</span>\n</pre>\n<p>where <code>your_input_cb</code>,<code>your_inference_cb</code>, and <code>your_output_cb</code> are callback functions that execute your computation.\nThe server manages a FSM that consists of an input and an output queue. Incoming requests get deserialzed into a\n<code>Message</code>, and the <code>data</code> elements in that message get unpacked onto the input queue. A coroutine continously polls the\ninput queue; its behavior is controlled through the <code>max_batch_size</code> and <code>max_time_on_queue</code> flags, which specify the\nmaximum queue size and the maximum time (in milliseconds) inbetween model executions. The server will accumulate items\non the queue until one of these conditions is met, and then proceed to execute the user callback functions.</p>\n<h5>Input Callback</h5>\n<p>When triggered, the server will remove items from the input queue (up to <code>max_batch_size</code>) and pass them to the input\ncallback function. The input callback always accepts a list of data elements, and returns a batched input. The purpose\nof this function is to perform any deserialization, preprocessing, and batching tasks necessary to convert the list of\ndata elements into a batched model input.</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">input_cb</span><span class=\"p\">(</span><span class=\"n\">items</span><span class=\"p\">:</span><span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">])</span> <span class=\"o\">-&gt;</span> <span class=\"n\">batch</span><span class=\"p\">:</span><span class=\"n\">Any</span>\n</pre>\n<h5>Inference Callback</h5>\n<p>The inference callback is typically a one-liner, something akin to <code>model.predict(batch)</code>. This function should consist\nof model execution code, and little if not nothing more. Importantly, <code>output</code> must satisfy two conditions: (1)\nit must be ordered with respect to the input batch, and (2) it must be an iterable object with length equal to that of\nthe items passed into the function. Keep in mind that Numpy arrays and Pandas Dataframes are iterable objects.</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">inference_cb</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">:</span><span class=\"n\">Any</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">output</span><span class=\"p\">:</span><span class=\"n\">Any</span>\n</pre>\n<h5>Output Callback</h5>\n<p>The server takes the batched output of the inference callback, iterates through it along the batch dimension, passing\neach output to the output callback, which performs post-processing and serialization necessary to form each response\ndata element.</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">output_cb</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">:</span><span class=\"n\">Any</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">data_elem</span><span class=\"p\">:</span><span class=\"n\">Any</span>\n</pre>\n<h4>Client</h4>\n<p>The Virtex <code>Client</code> is very simple to use. Let's assume that our server model accepts PIL images and returns a numpy\narray. The flow might look something like this:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span>\n<span class=\"kn\">from</span> <span class=\"nn\">virtex.http</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span>\n<span class=\"kn\">from</span> <span class=\"nn\">virtex.serial</span> <span class=\"kn\">import</span> <span class=\"n\">encode_pil</span><span class=\"p\">,</span> <span class=\"n\">decode_np</span>\n\n<span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"o\">.</span><span class=\"n\">load_img</span><span class=\"p\">(</span><span class=\"s2\">\"path/to/image_file\"</span><span class=\"p\">)</span>\n<span class=\"n\">msg</span> <span class=\"o\">=</span> <span class=\"n\">Message</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">img</span><span class=\"p\">])</span>\n<span class=\"n\">msg</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">encode_pil</span><span class=\"p\">)</span>\n\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">()</span>\n<span class=\"n\">resp</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"n\">msg</span><span class=\"p\">)</span>\n<span class=\"n\">resp</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"n\">decode_np</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># The response data elements are here</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">resp</span><span class=\"o\">.</span><span class=\"n\">data</span>\n\n<span class=\"c1\"># Do stuff ...</span>\n</pre>\n<p>The response data and request data are bijective with order preserved.</p>\n<h3>Example</h3>\n<p>Virtex is extremely flexible and easy to use. Below is an example that covers the key concepts that will allow you to\nstart using Virtex in your application. We're going to create a ResNet50-V2 image classification service. The source\ncode for this and other examples can be found here <a href=\"https://github.com/chrislarson1/virtex/examples\" rel=\"nofollow\">here</a>.</p>\n<h5>Server Code</h5>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.applications</span> <span class=\"kn\">import</span> <span class=\"n\">resnet_v2</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.applications</span> <span class=\"kn\">import</span> <span class=\"n\">resnet50</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras_preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">image</span>\n<span class=\"kn\">from</span> <span class=\"nn\">virtex.types</span> <span class=\"kn\">import</span> <span class=\"n\">Message</span>\n<span class=\"kn\">from</span> <span class=\"nn\">virtex.http</span> <span class=\"kn\">import</span> <span class=\"n\">Server</span>\n<span class=\"kn\">from</span> <span class=\"nn\">virtex.serial</span> <span class=\"kn\">import</span> <span class=\"p\">(</span><span class=\"n\">decode_image</span><span class=\"p\">,</span> \n                           <span class=\"n\">encode_bytes</span><span class=\"p\">,</span>\n                           <span class=\"n\">encode_np</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># ResNetV2 Model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">resnet_v2</span><span class=\"o\">.</span><span class=\"n\">ResNet50V2</span><span class=\"p\">(</span><span class=\"n\">weights</span><span class=\"o\">=</span><span class=\"s1\">'imagenet'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Define your computation in the form of callbacks</span>\n<span class=\"k\">def</span> <span class=\"nf\">resnet_input_cb</span><span class=\"p\">(</span><span class=\"n\">items</span><span class=\"p\">:</span><span class=\"nb\">list</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">item</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">items</span><span class=\"p\">):</span>\n        <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">decode_image</span><span class=\"p\">(</span><span class=\"n\">item</span><span class=\"p\">)</span>\n        <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">img</span><span class=\"o\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"s1\">'RGB'</span><span class=\"p\">)</span>\n        <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">img</span><span class=\"o\">.</span><span class=\"n\">resize</span><span class=\"p\">((</span><span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">),</span> <span class=\"n\">Image</span><span class=\"o\">.</span><span class=\"n\">NEAREST</span><span class=\"p\">)</span>\n        <span class=\"n\">items</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">resnet50</span><span class=\"o\">.</span><span class=\"n\">preprocess_input</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">img_to_array</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">(</span><span class=\"n\">items</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n<span class=\"n\">resnet_inference_cb</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span>\n\n<span class=\"n\">resnet_output_cb</span> <span class=\"o\">=</span> <span class=\"n\">encode_np</span>\n\n<span class=\"c1\"># Instantiate server, pass in callbacks</span>\n<span class=\"n\">server</span> <span class=\"o\">=</span> <span class=\"n\">Server</span><span class=\"p\">(</span>\n    <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'resnet_50_v2_image_classification_service'</span><span class=\"p\">,</span>\n    <span class=\"n\">host</span><span class=\"o\">=</span><span class=\"s1\">'127.0.0.1'</span><span class=\"p\">,</span>\n    <span class=\"n\">port</span><span class=\"o\">=</span><span class=\"mi\">8081</span><span class=\"p\">,</span>\n    <span class=\"n\">input_cb</span><span class=\"o\">=</span><span class=\"n\">resnet_input_cb</span><span class=\"p\">,</span>\n    <span class=\"n\">inference_cb</span><span class=\"o\">=</span><span class=\"n\">resnet_inference_cb</span><span class=\"p\">,</span>\n    <span class=\"n\">output_cb</span><span class=\"o\">=</span><span class=\"n\">resnet_output_cb</span><span class=\"p\">,</span>\n    <span class=\"n\">max_batch_size</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">,</span>\n    <span class=\"n\">max_time_on_queue</span><span class=\"o\">=</span><span class=\"mi\">10</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Validate the server using client's message type and run it</span>\n<span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">'path/to/image-file'</span><span class=\"p\">,</span> <span class=\"s1\">'rb'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">()</span>\n<span class=\"n\">message</span> <span class=\"o\">=</span> <span class=\"n\">Message</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">img</span><span class=\"p\">])</span>\n<span class=\"n\">message</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">encode_bytes</span><span class=\"p\">)</span>\n<span class=\"k\">if</span> <span class=\"n\">server</span><span class=\"o\">.</span><span class=\"n\">validate</span><span class=\"p\">(</span><span class=\"n\">message</span><span class=\"p\">):</span>\n    <span class=\"n\">server</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"p\">()</span>\n</pre>\n<h5>Client Code</h5>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">virtex.types</span> <span class=\"kn\">import</span> <span class=\"n\">Message</span>\n<span class=\"kn\">from</span> <span class=\"nn\">virtex.http</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span>\n<span class=\"kn\">from</span> <span class=\"nn\">virtex.serial</span> <span class=\"kn\">import</span> <span class=\"n\">encode_bytes</span><span class=\"p\">,</span> <span class=\"n\">decode_np</span>\n\n<span class=\"c1\"># Server endpoint</span>\n<span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s2\">\"http://127.0.0.1:8081\"</span>\n\n<span class=\"c1\"># Load your image</span>\n<span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">'path/to/image-file'</span><span class=\"p\">,</span> <span class=\"s1\">'rb'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Create your message</span>\n<span class=\"n\">message</span> <span class=\"o\">=</span> <span class=\"n\">Message</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">image</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Encode your message</span>\n<span class=\"n\">message</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">encode_bytes</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Validate your message</span>\n<span class=\"n\">message</span><span class=\"o\">.</span><span class=\"n\">validate</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Create your client</span>\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Post your request, get response</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Decode your response</span>\n<span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"n\">decode_np</span><span class=\"p\">)</span>\n</pre>\n<h3>Performance</h3>\n<p>Virtex is a performant machine learning model server, implementing dynamic batching in a similar fasion as other\ninference engines such as tensorflow serving. Virtex was designed with an emphasis on flexibility for on-line inference\napplications, in which requests are not being aggregated (batched) on the client side but rather sent serially. Below\nare two plots showing the performance of virtex relative to other serving engines (TF-serving and BERT-serving) for two\nfairly large machine learning models, Resnet50V2 and BERT base respectively. Unlike Virtex, these engines are optimized\nfor batched requests.</p>\n<p><img alt=\"Resnet_Perf\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9ce9ada53b7f07cc63cadbe3cb6dac4d760721a1/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f63687269736c6172736f6e312f7669727465782f6d61737465722f646f63732f7265736e65745f706572665f7470732e706e67\">\n<img alt=\"BERT_Perf\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/513bca480935da7a3305e1db20b222ebe9ba534e/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f63687269736c6172736f6e312f7669727465782f6d61737465722f646f63732f7265736e65745f706572665f6c6174656e63792e706e67\"></p>\n<p><img alt=\"Resnet_Perf\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/eb39443c4f8992c40ceef000c1731d5860e91d84/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f63687269736c6172736f6e312f7669727465782f6d61737465722f646f63732f626572745f706572665f7470732e706e67\">\n<img alt=\"BERT_Perf\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ca0c2276576b6cb0950bb7485a6f2a3a5adb8be7/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f63687269736c6172736f6e312f7669727465782f6d61737465722f646f63732f626572745f706572665f6c6174656e63792e706e67\"></p>\n<p>For the BERT-serving benchmark grid search was performed to obtain semi-optimal parameters: <code>n_workers=4</code>,\n<code>gpu_frac=0.25</code>, <code>pooling=-1</code>, <code>max_bs=32</code>, <code>-xla</code>, and <code>max_seq_len=64</code>. The Virtex benchmark was obtained\nfrom a single core (i.e., <code>n_workers=1</code>) and the same <code>max_seq_len=64</code>. All experiments were run on an Intel 6850K\nCPU (6-core, 32GB memory) and a single Nvidia GTX 1080ti GPU (12GB memory).</p>\n<p>Note: These are not true benchmarks, don't interpret them as such. Performance testing code can be found\n<a href=\"https://github.com/chrislarson1/virtex/tree/master/benchmark\" rel=\"nofollow\">here</a>.</p>\n<h3>Citation</h3>\n<p>If you use Virtex in your research, feel free to cite it!</p>\n<pre><span class=\"nc\">@misc</span><span class=\"p\">{</span><span class=\"nl\">Larson2020</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Larson, Chris}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{Virtex}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2020}</span><span class=\"p\">,</span>\n  <span class=\"na\">publisher</span> <span class=\"p\">=</span> <span class=\"s\">{GitHub}</span><span class=\"p\">,</span>\n  <span class=\"na\">journal</span> <span class=\"p\">=</span> <span class=\"s\">{GitHub repository}</span><span class=\"p\">,</span>\n  <span class=\"na\">howpublished</span> <span class=\"p\">=</span> <span class=\"s\">{\\url{https://github.com/chrislarson1/virtex}}</span><span class=\"p\">,</span>\n  <span class=\"na\">commit</span> <span class=\"p\">=</span> <span class=\"s\">{}</span>\n<span class=\"p\">}</span>\n</pre>\n\n          </div>"}, "last_serial": 7035314, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "e5b0784d429099e8880eb5a22cecdbb4", "sha256": "5101bad2fad61cc1ddb747e8c7540466e04cbc8eaf6c537e78ecaa944301abae"}, "downloads": -1, "filename": "virtex-1.0.0.tar.gz", "has_sig": false, "md5_digest": "e5b0784d429099e8880eb5a22cecdbb4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16140, "upload_time": "2020-04-16T20:19:58", "upload_time_iso_8601": "2020-04-16T20:19:58.322614Z", "url": "https://files.pythonhosted.org/packages/25/86/111829809bd32938596ec6ff8701d0fb3ab1d50809a2970905502e7bc0ca/virtex-1.0.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e5b0784d429099e8880eb5a22cecdbb4", "sha256": "5101bad2fad61cc1ddb747e8c7540466e04cbc8eaf6c537e78ecaa944301abae"}, "downloads": -1, "filename": "virtex-1.0.0.tar.gz", "has_sig": false, "md5_digest": "e5b0784d429099e8880eb5a22cecdbb4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16140, "upload_time": "2020-04-16T20:19:58", "upload_time_iso_8601": "2020-04-16T20:19:58.322614Z", "url": "https://files.pythonhosted.org/packages/25/86/111829809bd32938596ec6ff8701d0fb3ab1d50809a2970905502e7bc0ca/virtex-1.0.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:35:32 2020"}