{"info": {"author": "Gareth Jones", "author_email": "author@example.com", "bugtrack_url": null, "classifiers": [], "description": "# Incremental trees v0.3.3\n![The overcomplicated tests are...](https://github.com/garethjns/IncrementalTrees/workflows/The%20overcomplicated%20tests%20are.../badge.svg)\n\nAdds partial fit method to sklearn's forest estimators (currently RandomForestClassifier/Regressor and ExtraTreesClassifier/Regressor) to allow [incremental training](https://scikit-learn.org/0.15/modules/scaling_strategies.html) without being limited to a linear model. Works with or without [Dask-ml's Incremental](http://ml.dask.org/incremental.html).\n\nThese methods don't try and implement partial fitting for decision trees, rather they remove requirement that individual decision trees within forests are trained with the same data (or equally sized bootstraps). This reduces memory burden, training time, and variance. This is at the cost of generally increasing the number of weak learners will probably be required. \n\nThe resulting forests are not \"true\" online learners, as batch size affects performance. However, they should have similar (possibly better) performance as their standard versions after seeing an equivalent number of training rows.\n\n## Installing package\n\nQuick start:\n\n1) Clone repo and build pip installable package.\n   ````bash\n   git clone https://github.com/garethjns/IncrementalTrees.git\n   python -m pip install --upgrade pip setuptools wheel\n   cd IncrementalTrees\n   pip install .\n   ````\n\n\n## Usage Examples\nCurrently implemented:\n - Streaming versions of RandomForestClassifier (StreamingRFC) and ExtraTreesClassifer (StreamingEXTC). They work should work for binary and multi-class classification, but not multi-output yet.\n - Streaming versions of RandomForestRegressor (StreamingRFR) and ExtraTreesRegressor (StreamingEXTR). \n\nSee;\n- Below for example of using different mechanisms to feed .partial_fit() and different paraemeter set ups.  \n- [notes/PerformanceComparisons.ipynb](https://github.com/garethjns/IncrementalTrees/blob/master/notes/PerformanceComparisons.ipynb) and  [notes/PerformanceComparisonsDask.ipynb](https://github.com/garethjns/IncrementalTrees/blob/master/notes/PerformanceComparisonsDask.ipynb) for more examples and performance comparisons against RandomForest. Also there are some (unfinished) performance comparisons in tests/.\n\n\n### Data feeding mechanisms\n\n#### Fitting with .fit()\nFeeds .partial_fit() with randomly samples rows.\n\n\n````python\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom incremental_trees.trees import StreamingRFC\n\n# Generate some data in memory\nx, y = make_blobs(n_samples=int(2e5), random_state=0, n_features=40,\n                  centers=2, cluster_std=100)\n\nsrfc = StreamingRFC(n_estimators_per_chunk=3,\n                    max_n_estimators=np.inf,\n                    spf_n_fits=30,  # Number of calls to .partial_fit()\n                    spf_sample_prop=0.3)  # Number of rows to sample each on .partial_fit()\n\nsrfc.fit(x, y)\n# Should be n_estimators_per_chunk * spf_n_fits\nprint(len(srfc.estimators_))\nprint(srfc.score(x, y))\n````\n\n#### Fitting with .fit() and Dask\nCall .fit() directly, let dask handle sending data to .partial_fit()\n\n````python\nimport numpy as np\nimport dask_ml.datasets\nfrom dask_ml.wrappers import Incremental\nfrom dask.distributed import Client, LocalCluster\nfrom dask import delayed\nfrom incremental_trees.trees import StreamingRFC\n\n# Generate some data out-of-core\nx, y = dask_ml.datasets.make_blobs(n_samples=2e5, chunks=1e4, random_state=0,\n                                   n_features=40, centers=2, cluster_std=100)\n\n# Create throwaway cluster and client to run on                                  \nwith LocalCluster(processes=False, n_workers=2, \n                  threads_per_worker=2) as cluster, Client(cluster) as client:\n\n    # Wrap model with Dask Incremental\n    srfc = Incremental(StreamingRFC(dask_feeding=True,  # Turn dask on\n                                    n_estimators_per_chunk=10,\n                                    max_n_estimators=np.inf,\n                                    n_jobs=4))\n    \n    # Call fit directly, specifying the expected classes\n    srfc.fit(x, y,\n             classes=delayed(np.unique)(y).compute())\n             \n    print(len(srfc.estimators_))\n    print(srfc.score(x, y))\n````\n\n#### Feeding .partial_fit() manually \n.partial_fit can be called directly and fed data manually.\n\nFor example, this can be used to feed .partial_fit() sequentially (although below example selects random rows, which is similar to non-dask example above).\n\n````python\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom incremental_trees.trees import StreamingRFC\n\nsrfc = StreamingRFC(n_estimators_per_chunk=20,\n                    max_n_estimators=np.inf,\n                    n_jobs=4)\n\n# Generate some data in memory\nx, y = make_blobs(n_samples=int(2e5), random_state=0, n_features=40,\n                  centers=2, cluster_std=100)\n\n# Feed .partial_fit() with random samples of the data\nn_chunks = 30\nchunk_size = int(2e3)\nfor i in range(n_chunks):\n    sample_idx = np.random.randint(0, x.shape[0], chunk_size)\n    # Call .partial_fit(), specifying expected classes\n    srfc.partial_fit(x[sample_idx, :], y[sample_idx],\n                     classes=np.unique(y))\n           \n# Should be n_chunks * n_estimators_per_chunk             \nprint(len(srfc.estimators_))\nprint(srfc.score(x, y))\n````\n\n### Possible model set ups\nThere are a couple of different model setups worth considering. No idea which works best. \n\n#### \"Incremental forest\"\nFor the number of chunks/fits, sample rows from X, then fit a number of single trees (with different column subsets), eg.\n````python\nsrfc = StreamingRFC(n_estimators_per_chunk=10,\n                    max_features='sqrt')    \n````\n#### \"Incremental decision trees\"\nSingle (or few) decision trees per data subset, with all features. \n````python\nsrfc = StreamingRFC(n_estimators_per_chunk=1,\n                    max_features=x.shape[1])\n````\n\n# Version history\n## v0.3.1-3\n  - Update Dask versions\n## v0.3\n  - Updated unit tests\n  - Added performance benchmark tests for classifiers, not finished.\n  - Added regressor versions of RandomForest (StreamingRFR) and ExtaTrees (StreamingEXTR, also renamed StreamingEXT to StreamingEXTC).\n  - .fit() overload to handle feeding .partial_fit() with random row samples, without using Dask. Adds compatibility with sklearn SearchCV objects.\n\n## v0.2\n  - Add ExtraTreesClassifier (StreamingEXT)\n\n## v0.1\n  - .partial_fit() for RandomForestClassifier (StreamingRFC)\n  - .predict_proba() for RandomforestClassifier", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/garethjns/IncrementalTrees", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "incremental-trees", "package_url": "https://pypi.org/project/incremental-trees/", "platform": "", "project_url": "https://pypi.org/project/incremental-trees/", "project_urls": {"Homepage": "https://github.com/garethjns/IncrementalTrees"}, "release_url": "https://pypi.org/project/incremental-trees/0.3.3/", "requires_dist": null, "requires_python": "", "summary": "Sklearn forests with partial fits", "version": "0.3.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Incremental trees v0.3.3</h1>\n<p><img alt=\"The overcomplicated tests are...\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/81a5a9cbcea7a6d0d23d16081f255394e171be8b/68747470733a2f2f6769746875622e636f6d2f6761726574686a6e732f496e6372656d656e74616c54726565732f776f726b666c6f77732f5468652532306f766572636f6d706c69636174656425323074657374732532306172652e2e2e2f62616467652e737667\"></p>\n<p>Adds partial fit method to sklearn's forest estimators (currently RandomForestClassifier/Regressor and ExtraTreesClassifier/Regressor) to allow <a href=\"https://scikit-learn.org/0.15/modules/scaling_strategies.html\" rel=\"nofollow\">incremental training</a> without being limited to a linear model. Works with or without <a href=\"http://ml.dask.org/incremental.html\" rel=\"nofollow\">Dask-ml's Incremental</a>.</p>\n<p>These methods don't try and implement partial fitting for decision trees, rather they remove requirement that individual decision trees within forests are trained with the same data (or equally sized bootstraps). This reduces memory burden, training time, and variance. This is at the cost of generally increasing the number of weak learners will probably be required.</p>\n<p>The resulting forests are not \"true\" online learners, as batch size affects performance. However, they should have similar (possibly better) performance as their standard versions after seeing an equivalent number of training rows.</p>\n<h2>Installing package</h2>\n<p>Quick start:</p>\n<ol>\n<li>Clone repo and build pip installable package.\n<pre>git clone https://github.com/garethjns/IncrementalTrees.git\npython -m pip install --upgrade pip setuptools wheel\n<span class=\"nb\">cd</span> IncrementalTrees\npip install .\n</pre>\n</li>\n</ol>\n<h2>Usage Examples</h2>\n<p>Currently implemented:</p>\n<ul>\n<li>Streaming versions of RandomForestClassifier (StreamingRFC) and ExtraTreesClassifer (StreamingEXTC). They work should work for binary and multi-class classification, but not multi-output yet.</li>\n<li>Streaming versions of RandomForestRegressor (StreamingRFR) and ExtraTreesRegressor (StreamingEXTR).</li>\n</ul>\n<p>See;</p>\n<ul>\n<li>Below for example of using different mechanisms to feed .partial_fit() and different paraemeter set ups.</li>\n<li><a href=\"https://github.com/garethjns/IncrementalTrees/blob/master/notes/PerformanceComparisons.ipynb\" rel=\"nofollow\">notes/PerformanceComparisons.ipynb</a> and  <a href=\"https://github.com/garethjns/IncrementalTrees/blob/master/notes/PerformanceComparisonsDask.ipynb\" rel=\"nofollow\">notes/PerformanceComparisonsDask.ipynb</a> for more examples and performance comparisons against RandomForest. Also there are some (unfinished) performance comparisons in tests/.</li>\n</ul>\n<h3>Data feeding mechanisms</h3>\n<h4>Fitting with .fit()</h4>\n<p>Feeds .partial_fit() with randomly samples rows.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">make_blobs</span>\n<span class=\"kn\">from</span> <span class=\"nn\">incremental_trees.trees</span> <span class=\"kn\">import</span> <span class=\"n\">StreamingRFC</span>\n\n<span class=\"c1\"># Generate some data in memory</span>\n<span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">make_blobs</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mf\">2e5</span><span class=\"p\">),</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">40</span><span class=\"p\">,</span>\n                  <span class=\"n\">centers</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">cluster_std</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n\n<span class=\"n\">srfc</span> <span class=\"o\">=</span> <span class=\"n\">StreamingRFC</span><span class=\"p\">(</span><span class=\"n\">n_estimators_per_chunk</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span>\n                    <span class=\"n\">max_n_estimators</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">inf</span><span class=\"p\">,</span>\n                    <span class=\"n\">spf_n_fits</span><span class=\"o\">=</span><span class=\"mi\">30</span><span class=\"p\">,</span>  <span class=\"c1\"># Number of calls to .partial_fit()</span>\n                    <span class=\"n\">spf_sample_prop</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">)</span>  <span class=\"c1\"># Number of rows to sample each on .partial_fit()</span>\n\n<span class=\"n\">srfc</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"c1\"># Should be n_estimators_per_chunk * spf_n_fits</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">srfc</span><span class=\"o\">.</span><span class=\"n\">estimators_</span><span class=\"p\">))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">srfc</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">))</span>\n</pre>\n<h4>Fitting with .fit() and Dask</h4>\n<p>Call .fit() directly, let dask handle sending data to .partial_fit()</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">dask_ml.datasets</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dask_ml.wrappers</span> <span class=\"kn\">import</span> <span class=\"n\">Incremental</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dask.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span><span class=\"p\">,</span> <span class=\"n\">LocalCluster</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dask</span> <span class=\"kn\">import</span> <span class=\"n\">delayed</span>\n<span class=\"kn\">from</span> <span class=\"nn\">incremental_trees.trees</span> <span class=\"kn\">import</span> <span class=\"n\">StreamingRFC</span>\n\n<span class=\"c1\"># Generate some data out-of-core</span>\n<span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">dask_ml</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">make_blobs</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mf\">2e5</span><span class=\"p\">,</span> <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"mf\">1e4</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n                                   <span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">40</span><span class=\"p\">,</span> <span class=\"n\">centers</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">cluster_std</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create throwaway cluster and client to run on                                  </span>\n<span class=\"k\">with</span> <span class=\"n\">LocalCluster</span><span class=\"p\">(</span><span class=\"n\">processes</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">n_workers</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> \n                  <span class=\"n\">threads_per_worker</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">cluster</span><span class=\"p\">,</span> <span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"n\">cluster</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">client</span><span class=\"p\">:</span>\n\n    <span class=\"c1\"># Wrap model with Dask Incremental</span>\n    <span class=\"n\">srfc</span> <span class=\"o\">=</span> <span class=\"n\">Incremental</span><span class=\"p\">(</span><span class=\"n\">StreamingRFC</span><span class=\"p\">(</span><span class=\"n\">dask_feeding</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>  <span class=\"c1\"># Turn dask on</span>\n                                    <span class=\"n\">n_estimators_per_chunk</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n                                    <span class=\"n\">max_n_estimators</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">inf</span><span class=\"p\">,</span>\n                                    <span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">))</span>\n    \n    <span class=\"c1\"># Call fit directly, specifying the expected classes</span>\n    <span class=\"n\">srfc</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span>\n             <span class=\"n\">classes</span><span class=\"o\">=</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">unique</span><span class=\"p\">)(</span><span class=\"n\">y</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">compute</span><span class=\"p\">())</span>\n             \n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">srfc</span><span class=\"o\">.</span><span class=\"n\">estimators_</span><span class=\"p\">))</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">srfc</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">))</span>\n</pre>\n<h4>Feeding .partial_fit() manually</h4>\n<p>.partial_fit can be called directly and fed data manually.</p>\n<p>For example, this can be used to feed .partial_fit() sequentially (although below example selects random rows, which is similar to non-dask example above).</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">make_blobs</span>\n<span class=\"kn\">from</span> <span class=\"nn\">incremental_trees.trees</span> <span class=\"kn\">import</span> <span class=\"n\">StreamingRFC</span>\n\n<span class=\"n\">srfc</span> <span class=\"o\">=</span> <span class=\"n\">StreamingRFC</span><span class=\"p\">(</span><span class=\"n\">n_estimators_per_chunk</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span>\n                    <span class=\"n\">max_n_estimators</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">inf</span><span class=\"p\">,</span>\n                    <span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Generate some data in memory</span>\n<span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">make_blobs</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mf\">2e5</span><span class=\"p\">),</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">40</span><span class=\"p\">,</span>\n                  <span class=\"n\">centers</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">cluster_std</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Feed .partial_fit() with random samples of the data</span>\n<span class=\"n\">n_chunks</span> <span class=\"o\">=</span> <span class=\"mi\">30</span>\n<span class=\"n\">chunk_size</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mf\">2e3</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_chunks</span><span class=\"p\">):</span>\n    <span class=\"n\">sample_idx</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">chunk_size</span><span class=\"p\">)</span>\n    <span class=\"c1\"># Call .partial_fit(), specifying expected classes</span>\n    <span class=\"n\">srfc</span><span class=\"o\">.</span><span class=\"n\">partial_fit</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"n\">sample_idx</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">sample_idx</span><span class=\"p\">],</span>\n                     <span class=\"n\">classes</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">unique</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">))</span>\n           \n<span class=\"c1\"># Should be n_chunks * n_estimators_per_chunk             </span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">srfc</span><span class=\"o\">.</span><span class=\"n\">estimators_</span><span class=\"p\">))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">srfc</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">))</span>\n</pre>\n<h3>Possible model set ups</h3>\n<p>There are a couple of different model setups worth considering. No idea which works best.</p>\n<h4>\"Incremental forest\"</h4>\n<p>For the number of chunks/fits, sample rows from X, then fit a number of single trees (with different column subsets), eg.</p>\n<pre><span class=\"n\">srfc</span> <span class=\"o\">=</span> <span class=\"n\">StreamingRFC</span><span class=\"p\">(</span><span class=\"n\">n_estimators_per_chunk</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n                    <span class=\"n\">max_features</span><span class=\"o\">=</span><span class=\"s1\">'sqrt'</span><span class=\"p\">)</span>    \n</pre>\n<h4>\"Incremental decision trees\"</h4>\n<p>Single (or few) decision trees per data subset, with all features.</p>\n<pre><span class=\"n\">srfc</span> <span class=\"o\">=</span> <span class=\"n\">StreamingRFC</span><span class=\"p\">(</span><span class=\"n\">n_estimators_per_chunk</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n                    <span class=\"n\">max_features</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n</pre>\n<h1>Version history</h1>\n<h2>v0.3.1-3</h2>\n<ul>\n<li>Update Dask versions</li>\n</ul>\n<h2>v0.3</h2>\n<ul>\n<li>Updated unit tests</li>\n<li>Added performance benchmark tests for classifiers, not finished.</li>\n<li>Added regressor versions of RandomForest (StreamingRFR) and ExtaTrees (StreamingEXTR, also renamed StreamingEXT to StreamingEXTC).</li>\n<li>.fit() overload to handle feeding .partial_fit() with random row samples, without using Dask. Adds compatibility with sklearn SearchCV objects.</li>\n</ul>\n<h2>v0.2</h2>\n<ul>\n<li>Add ExtraTreesClassifier (StreamingEXT)</li>\n</ul>\n<h2>v0.1</h2>\n<ul>\n<li>.partial_fit() for RandomForestClassifier (StreamingRFC)</li>\n<li>.predict_proba() for RandomforestClassifier</li>\n</ul>\n\n          </div>"}, "last_serial": 6788625, "releases": {"0.2": [{"comment_text": "", "digests": {"md5": "cf99d81fdd94f95f33a961ccc641c6ec", "sha256": "836d44e2657b80ee61a6a554532d86b5e11e4fcf017b29fbc75c5e2afb9cd73d"}, "downloads": -1, "filename": "incremental_trees-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "cf99d81fdd94f95f33a961ccc641c6ec", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5298, "upload_time": "2020-03-10T23:20:39", "upload_time_iso_8601": "2020-03-10T23:20:39.843920Z", "url": "https://files.pythonhosted.org/packages/2f/59/0a17a3b190cf5e1868f3c434a771d4f733c201523ecc748cd8c0f8eb91ab/incremental_trees-0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "647fcba0a249d0c257bd8ee5a4459b8e", "sha256": "b7eef67e81c5680e356ff2d84726ac6384fe633a127381ec4e8bc769abe6e4f0"}, "downloads": -1, "filename": "incremental_trees-0.2.tar.gz", "has_sig": false, "md5_digest": "647fcba0a249d0c257bd8ee5a4459b8e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5965, "upload_time": "2020-03-10T23:20:43", "upload_time_iso_8601": "2020-03-10T23:20:43.059014Z", "url": "https://files.pythonhosted.org/packages/bf/74/c5d77788d2a34071f9a6e501d02daa5df48dbff2fcc31668bf6e45632da3/incremental_trees-0.2.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "143db1e64dcaa63ae6ac64e47bd09f82", "sha256": "fdee5cca9f62448593054e6f361e2baa0da98c4ec6c225b748b39f0ff65fe5fb"}, "downloads": -1, "filename": "incremental_trees-0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "143db1e64dcaa63ae6ac64e47bd09f82", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 6303, "upload_time": "2020-03-10T23:20:41", "upload_time_iso_8601": "2020-03-10T23:20:41.920769Z", "url": "https://files.pythonhosted.org/packages/44/27/5792d339bf00aee0c52f3a336d64eaf6b4a02837f4f35054733c4fc6f84b/incremental_trees-0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c6cb38642246a610cd5ae790d8257271", "sha256": "0f6a964152df283d3836b9a4fe06af5218e88731545b0e88804228c5fb079f3d"}, "downloads": -1, "filename": "incremental_trees-0.3.tar.gz", "has_sig": false, "md5_digest": "c6cb38642246a610cd5ae790d8257271", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7830, "upload_time": "2020-03-10T23:20:47", "upload_time_iso_8601": "2020-03-10T23:20:47.468015Z", "url": "https://files.pythonhosted.org/packages/16/55/55ef024381b4c03b68ab6b554c984ac4d607282da835d2e82ad7f753aa55/incremental_trees-0.3.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "1dd87be22e002796692126a134def360", "sha256": "d6368c45079fc17155ce06e3483014fe81b264cab6a3da7632f06dc5ca09123a"}, "downloads": -1, "filename": "incremental_trees-0.3.1.tar.gz", "has_sig": false, "md5_digest": "1dd87be22e002796692126a134def360", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10323, "upload_time": "2020-03-10T23:20:44", "upload_time_iso_8601": "2020-03-10T23:20:44.381089Z", "url": "https://files.pythonhosted.org/packages/db/b0/03bf337106c918e0d59737c10e638843f32cf4415691afa33b455cd9dc4d/incremental_trees-0.3.1.tar.gz", "yanked": false}], "0.3.3": [{"comment_text": "", "digests": {"md5": "e8dfc2286daf14204d3e52aa03db51f7", "sha256": "b9fdc0a2e3fe722bb86b1b07265593d525db8777c80b58897c4c1840573e1c04"}, "downloads": -1, "filename": "incremental_trees-0.3.3.tar.gz", "has_sig": false, "md5_digest": "e8dfc2286daf14204d3e52aa03db51f7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18274, "upload_time": "2020-03-10T23:20:45", "upload_time_iso_8601": "2020-03-10T23:20:45.886225Z", "url": "https://files.pythonhosted.org/packages/5e/32/23496264da87ff0fb475e9c9b6a1f0a9fd6906894eb29968d4f9ac8449b1/incremental_trees-0.3.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e8dfc2286daf14204d3e52aa03db51f7", "sha256": "b9fdc0a2e3fe722bb86b1b07265593d525db8777c80b58897c4c1840573e1c04"}, "downloads": -1, "filename": "incremental_trees-0.3.3.tar.gz", "has_sig": false, "md5_digest": "e8dfc2286daf14204d3e52aa03db51f7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18274, "upload_time": "2020-03-10T23:20:45", "upload_time_iso_8601": "2020-03-10T23:20:45.886225Z", "url": "https://files.pythonhosted.org/packages/5e/32/23496264da87ff0fb475e9c9b6a1f0a9fd6906894eb29968d4f9ac8449b1/incremental_trees-0.3.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:56:24 2020"}