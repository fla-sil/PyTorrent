{"info": {"author": "Explosion", "author_email": "contact@explosion.ai", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "<a href=\"https://explosion.ai\"><img src=\"https://explosion.ai/assets/img/logo.svg\" width=\"125\" height=\"125\" align=\"right\" /></a>\n\n# spaCy wrapper for PyTorch Transformers\n\nThis package provides [spaCy](https://github.com/explosion/spaCy) model\npipelines that wrap\n[Hugging Face's `pytorch-transformers`](https://github.com/huggingface/pytorch-transformers)\npackage, so you can use them in spaCy. The result is convenient access to\nstate-of-the-art transformer architectures, such as BERT, GPT-2, XLNet, etc. For\nmore details and background, check out\n[our blog post](https://explosion.ai/blog/spacy-pytorch-transformers).\n\n[![Azure Pipelines](https://img.shields.io/azure-devops/build/explosion-ai/public/11/master.svg?logo=azure-devops&style=flat-square)](https://dev.azure.com/explosion-ai/public/_build?definitionId=11)\n[![PyPi](https://img.shields.io/pypi/v/spacy-pytorch-transformers.svg?style=flat-square)](https://pypi.python.org/pypi/spacy-pytorch-transformers)\n[![GitHub](https://img.shields.io/github/release/explosion/spacy-pytorch-transformers/all.svg?style=flat-square)](https://github.com/explosion/spacy-pytorch-transformers/releases)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/ambv/black)\n[![Open demo in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/explosion/spacy-pytorch-transformers/blob/master/examples/Spacy_Transformers_Demo.ipynb)\n\n## Features\n\n-   Use **BERT**, **RoBERTa**, **XLNet** and **GPT-2** directly in your spaCy\n    pipeline.\n-   **Fine-tune** pretrained transformer models on your task using spaCy's API.\n-   Custom component for **text classification** using transformer features.\n-   Automatic **alignment** of wordpieces and outputs to linguistic tokens.\n-   Process multi-sentence documents with intelligent **per-sentence\n    prediction**.\n-   Built-in hooks for **context-sensitive vectors** and similarity.\n-   Out-of-the-box serialization and model packaging.\n\n## \ud83d\ude80 Quickstart\n\nInstalling the package from pip will automatically install all dependencies,\nincluding PyTorch and spaCy. Make sure you install this package **before** you\ninstall the models. Also note that this package requires **Python 3.6+** and the\nlatest version of spaCy,\n[v2.1.7](https://github.com/explosion/spaCy/releases/tag/v2.1.7) or above.\n\n```bash\npip install spacy-pytorch-transformers\n```\n\nFor GPU installation, find your CUDA version using `nvcc --version` and add the\n[version in brackets](https://spacy.io/usage/#gpu), e.g.\n`spacy-pytorch-transformers[cuda92]` for CUDA9.2 or\n`spacy-pytorch-transformers[cuda100]` for CUDA10.0.\n\nWe've also pre-packaged some of the pretrained models as spaCy model packages.\nYou can either use the `spacy download` command or download the packages from\nthe [model releases](https://github.com/explosion/spacy-models/releases).\n\n| Package name                       | Pretrained model          | Language | Author                                                                      |  Size |                                               Release                                                |\n| ---------------------------------- | ------------------------- | -------- | --------------------------------------------------------------------------- | ----: | :--------------------------------------------------------------------------------------------------: |\n| `en_pytt_bertbaseuncased_lg`       | `bert-base-uncased`       | English  | [Google Research](https://github.com/google-research/bert)                  | 406MB |    [\ud83d\udce6\ufe0f](https://github.com/explosion/spacy-models/releases/tag/en_pytt_bertbaseuncased_lg-2.1.1)    |\n| `de_pytt_bertbasecased_lg`         | `bert-base-german-cased`  | German   | [deepset](https://deepset.ai/german-bert)                                   | 406MB |     [\ud83d\udce6\ufe0f](https://github.com/explosion/spacy-models/releases/tag/de_pytt_bertbasecased_lg-2.1.1)     |\n| `en_pytt_xlnetbasecased_lg`        | `xlnet-base-cased`        | English  | [CMU/Google Brain](https://github.com/zihangdai/xlnet/)                     | 434MB |    [\ud83d\udce6\ufe0f](https://github.com/explosion/spacy-models/releases/tag/en_pytt_xlnetbasecased_lg-2.1.1)     |\n| `en_pytt_robertabase_lg`           | `roberta-base`            | English  | [Facebook](https://github.com/pytorch/fairseq/tree/master/examples/roberta) | 292MB |      [\ud83d\udce6\ufe0f](https://github.com/explosion/spacy-models/releases/tag/en_pytt_robertabase_lg-2.1.0)      |\n| `en_pytt_distilbertbaseuncased_lg` | `distilbert-base-uncased` | English  | [Hugging Face](https://medium.com/huggingface/distilbert-8cf3380435b5)      | 245MB | [\ud83d\udce6\ufe0f](https://github.com/explosion/spacy-models/releases/tag/en_pytt_distilbertbaseuncased_lg-2.1.0) |\n\n```bash\npython -m spacy download en_pytt_bertbaseuncased_lg\npython -m spacy download de_pytt_bertbasecased_lg\npython -m spacy download en_pytt_xlnetbasecased_lg\npython -m spacy download en_pytt_robertabase_lg\npython -m spacy download en_pytt_distilbertbaseuncased_lg\n```\n\nOnce the model is installed, you can load it in spaCy like any other model\npackage.\n\n```python\nimport spacy\n\nnlp = spacy.load(\"en_pytt_bertbaseuncased_lg\")\ndoc = nlp(\"Apple shares rose on the news. Apple pie is delicious.\")\nprint(doc[0].similarity(doc[7]))\nprint(doc._.pytt_last_hidden_state.shape)\n```\n\n> \ud83d\udca1 If you're seeing an error like `No module named 'spacy.lang.pytt'`,\n> double-check that `spacy-pytorch-transformers` is installed. It needs to be\n> available so it can register its language entry points. Also make sure that\n> you're running spaCy v2.1.7 or higher.\n\n## \ud83d\udcd6 Usage\n\n### Transfer learning\n\nThe main use case for pretrained transformer models is transfer learning. You\nload in a large generic model pretrained on lots of text, and start training on\nyour smaller dataset with labels specific to your problem. This package has\ncustom pipeline components that make this especially easy. We provide an example\ncomponent for text categorization. Development of analogous components for other\ntasks should be quite straight-forward.\n\nThe `pytt_textcat` component is based on spaCy's built-in\n[`TextCategorizer`](https://spacy.io/api/textcategorizer) and supports using the\nfeatures assigned by the PyTorch-Transformers models, via the `pytt_tok2vec`\ncomponent. This lets you use a model like BERT to predict contextual token\nrepresentations, and then learn a text categorizer on top as a task-specific\n\"head\". The API is the same as any other spaCy pipeline:\n\n```python\nTRAIN_DATA = [\n    (\"text1\", {\"cats\": {\"POSITIVE\": 1.0, \"NEGATIVE\": 0.0}})\n]\n```\n\n```python\nimport spacy\nfrom spacy.util import minibatch\nimport random\nimport torch\n\nis_using_gpu = spacy.prefer_gpu()\nif is_using_gpu:\n    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n\nnlp = spacy.load(\"en_pytt_bertbaseuncased_lg\")\nprint(nlp.pipe_names) # [\"sentencizer\", \"pytt_wordpiecer\", \"pytt_tok2vec\"]\ntextcat = nlp.create_pipe(\"pytt_textcat\", config={\"exclusive_classes\": True})\nfor label in (\"POSITIVE\", \"NEGATIVE\"):\n    textcat.add_label(label)\nnlp.add_pipe(textcat)\n\noptimizer = nlp.resume_training()\nfor i in range(10):\n    random.shuffle(TRAIN_DATA)\n    losses = {}\n    for batch in minibatch(TRAIN_DATA, size=8):\n        texts, cats = zip(*batch)\n        nlp.update(texts, cats, sgd=optimizer, losses=losses)\n    print(i, losses)\nnlp.to_disk(\"/bert-textcat\")\n```\n\nFor a full example, see the\n[`examples/train_textcat.py` script](examples/train_textcat.py).\n\n### Vectors and similarity\n\nThe `PyTT_TokenVectorEncoder` component of the model sets custom hooks that\noverride the default behaviour of the `.vector` attribute and `.similarity`\nmethod of the `Token`, `Span` and `Doc` objects. By default, these usually refer\nto the word vectors table at `nlp.vocab.vectors`. Naturally, in the transformer\nmodels we'd rather use the `doc.tensor` attribute, since it holds a much more\ninformative context-sensitive representation.\n\n```python\napple1 = nlp(\"Apple shares rose on the news.\")\napple2 = nlp(\"Apple sold fewer iPhones this quarter.\")\napple3 = nlp(\"Apple pie is delicious.\")\nprint(apple1[0].similarity(apple2[0]))\nprint(apple1[0].similarity(apple3[0]))\n```\n\n### Serialization\n\nSaving and loading pretrained transformer models and packaging them as spaCy\nmodels \u2728just works \u2728 (at least, it should). The wrapper and components follow\nspaCy's API, so when you save and load the `nlp` object, it...\n\n-   Writes the pretrained weights to disk / bytes and loads them back in.\n-   Adds `\"lang_factory\": \"pytt\"` in the `meta.json` so spaCy knows how to\n    initialize the `Language` class when you load the model.\n-   Adds this package and its version to the `\"requirements\"` in the\n    `meta.json`, so when you run\n    [`spacy package`](https://spacy.io/api/cli#package) to create an installable\n    Python package it's automatically added to the setup's `install_requires`.\n\nFor example, if you've trained your own text classifier, you can package it like\nthis:\n\n```bash\npython -m spacy package /bert-textcat /output\ncd /output/en_pytt_bertbaseuncased_lg-1.0.0\npython setup.py sdist\npip install dist/en_pytt_bertbaseuncased_lg-1.0.0.tar.gz\n```\n\n### Extension attributes\n\nThis wrapper sets the following\n[custom extension attributes](https://spacy.io/usage/processing-pipelines#custom-components-attributes)\non the `Doc`, `Span` and `Token` objects:\n\n| Name                          | Type              | Description                                                                                                                                                   |\n| ----------------------------- | ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `._.pytt_alignment`           | `List[List[int]]` | Alignment between wordpieces and spaCy tokens. Contains lists of wordpiece token indices (one per spaCy token) or a list of indices (if called on a `Token`). |\n| `._.pytt_word_pieces`         | `List[int]`       | The wordpiece IDs.                                                                                                                                            |\n| `._.pytt_word_pieces_`        | `List[str]`       | The string forms of the wordpiece IDs.                                                                                                                        |\n| `._.pytt_last_hidden_state`   | `ndarray`         | The `last_hidden_state` output from the PyTorch-Transformers model.                                                                                           |\n| `._.pytt_pooler_output`       | `List[ndarray]`   | The `pooler_output` output from the PyTorch-Transformers model.                                                                                               |\n| `._.pytt_all_hidden_states`   | `List[ndarray]`   | The `all_hidden_states` output from the PyTorch-Transformers model.                                                                                           |\n| `._.all_attentions`           | `List[ndarray]`   | The `all_attentions` output from the PyTorch-Transformers model.                                                                                              |\n| `._.pytt_d_last_hidden_state` | `ndarray`         | The gradient of the `last_hidden_state` output from the PyTorch-Transformers model.                                                                           |\n| `._.pytt_d_pooler_output`     | `List[ndarray]`   | The gradient of the `pooler_output` output from the PyTorch-Transformers model.                                                                               |\n| `._.pytt_d_all_hidden_states` | `List[ndarray]`   | The gradient of the `all_hidden_states` output from the PyTorch-Transformers model.                                                                           |\n| `._.pytt_d_all_attentions`    | `List[ndarray]`   | The gradient of the `all_attentions` output from the PyTorch-Transformers model.                                                                              |\n\nThe values can be accessed via the `._` attribute. For example:\n\n```python\ndoc = nlp(\"This is a text.\")\nprint(doc._.pytt_word_pieces_)\n```\n\n### Setting up the pipeline\n\nIn order to run, the `nlp` object created using `PyTT_Language` requires a few\ncomponents to run in order: a component that assigns sentence boundaries (e.g.\nspaCy's built-in\n[`Sentencizer`](https://spacy.io/usage/linguistic-features#sbd-component)), the\n`PyTT_WordPiecer`, which assigns the wordpiece tokens and the\n`PyTT_TokenVectorEncoder`, which assigns the token vectors. The `pytt_name`\nargument defines the name of the pretrained model to use. The `from_pretrained`\nmethods load the pretrained model via `pytorch-transformers`.\n\n```python\nfrom spacy_pytorch_transformers import PyTT_Language, PyTT_WordPiecer, PyTT_TokenVectorEncoder\n\nname = \"bert-base-uncased\"\nnlp = PyTT_Language(pytt_name=name, meta={\"lang\": \"en\"})\nnlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\nnlp.add_pipe(PyTT_WordPiecer.from_pretrained(nlp.vocab, name))\nnlp.add_pipe(PyTT_TokenVectorEncoder.from_pretrained(nlp.vocab, name))\nprint(nlp.pipe_names)  # ['sentencizer', 'pytt_wordpiecer', 'pytt_tok2vec']\n```\n\nYou can also use the [`init_model.py`](examples/init_model.py) script in the\nexamples.\n\n#### Loading models from a path\n\nPytorch-Transformers models can also be loaded from a file path instead of just\na name. For instance, let's say you want to use Allen AI's\n[`scibert`](https://github.com/allenai/scibert). First, download the PyTorch\nmodel files, unpack them them, unpack the `weights.tar`, rename the\n`bert_config.json` to `config.json` and put everything into one directory. Your\ndirectory should now have a `pytorch_model.bin`, `vocab.txt` and `config.json`.\nAlso make sure that your path **includes the name of the model**. You can then\ninitialize the `nlp` object like this:\n\n```python\nfrom spacy_pytorch_transformers import PyTT_Language, PyTT_WordPiecer, PyTT_TokenVectorEncoder\n\nname = \"scibert-scivocab-uncased\"\npath = \"/path/to/scibert-scivocab-uncased\"\n\nnlp = PyTT_Language(pytt_name=name, meta={\"lang\": \"en\"})\nnlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\nnlp.add_pipe(PyTT_WordPiecer.from_pretrained(nlp.vocab, path))\nnlp.add_pipe(PyTT_TokenVectorEncoder.from_pretrained(nlp.vocab, path))\n```\n\n### Tokenization alignment\n\nTransformer models are usually trained on text preprocessed with the \"wordpiece\"\nalgorithm, which limits the number of distinct token-types the model needs to\nconsider. Wordpiece is convenient for training neural networks, but it doesn't\nproduce segmentations that match up to any linguistic notion of a \"word\". Most\nrare words will map to multiple wordpiece tokens, and occasionally the alignment\nwill be many-to-many. `spacy-pytorch-transformers` calculates this alignment,\nwhich you can access at `doc._.pytt_alignment`. It's a list of length equal to\nthe number of spaCy tokens. Each value in the list is a list of consecutive\nintegers, which are indexes into the wordpieces list.\n\nIf you can work on representations that aren't aligned to actual words, it's\nbest to use the raw outputs of the transformer, which can be accessed at\n`doc._.pytt_last_hidden_state`. This variable gives you a tensor with one row\nper wordpiece token.\n\nIf you're working on token-level tasks such as part-of-speech tagging or\nspelling correction, you'll want to work on the token-aligned features, which\nare stored in the `doc.tensor` variable.\n\nWe've taken care to calculate the aligned `doc.tensor` representation as\nfaithfully as possible, with priority given to avoid information loss. The\nalignment has been calculated such that\n`doc.tensor.sum(axis=1) == doc._.pytt_last_hidden_state.sum(axis=1)`. To make\nthis work, each row of the `doc.tensor` (which corresponds to a spaCy token) is\nset to a weighted sum of the rows of the `last_hidden_state` tensor that the\ntoken is aligned to, where the weighting is proportional to the number of other\nspaCy tokens aligned to that row. To include the information from the (often\nimportant --- see Clark et al., 2019) boundary tokens, we imagine that these are\nalso \"aligned\" to all of the tokens in the sentence.\n\n### Batching, padding and per-sentence processing\n\nTransformer models have cubic runtime and memory complexity with respect to\nsequence length. This means that longer texts need to be divided into sentences\nin order to achieve reasonable efficiency.\n\n`spacy-pytorch-transformers` handles this internally, and requires that sort of\nsentence-boundary detection component has been added to the pipeline. We\nrecommend:\n\n```python\nsentencizer = nlp.create_pipe(\"sentencizer\")\nnlp.add_pipe(sentencizer, first=True)\n```\n\nInternally, the transformer model will predict over sentences, and the resulting\ntensor features will be reconstructed to produce document-level annotations.\n\nIn order to further improve efficiency and reduce memory requirements,\n`spacy-pytorch-transformers` also performs length-based subbatching internally.\nThe subbatching regroups the batched sentences by sequence length, to minimise\nthe amount of padding required. The configuration option `words_per_batch`\ncontrols this behaviour. You can set it to 0 to disable the subbatching, or set\nit to an integer to require a maximum limit on the number of words (including\npadding) per subbatch. The default value of 3000 words works reasonably well on\na Tesla V100.\n\nMany of the pretrained transformer models have a maximum sequence length. If a\nsentence is longer than the maximum, it is truncated and the affected ending\ntokens will receive zeroed vectors.\n\n## \ud83c\udf9b API\n\n### <kbd>class</kbd> `PyTT_Language`\n\nA subclass of [`Language`](https://spacy.io/api/language) that holds a\nPyTorch-Transformer (PyTT) pipeline. PyTT pipelines work only slightly\ndifferently from spaCy's default pipelines. Specifically, we introduce a new\npipeline component at the start of the pipeline, `PyTT_TokenVectorEncoder`. We\nthen modify the [`nlp.update`](https://spacy.io/api/language#update) function to\nrun the `PyTT_TokenVectorEncoder` before the other pipeline components, and\nbackprop it after the other components are done.\n\n#### <kbd>staticmethod</kbd> `PyTT_Language.install_extensions`\n\nRegister the\n[custom extension attributes](https://spacy.io/usage/processing-pipelines#custom-components-attributes)\non the `Doc`, `Span` and `Token` objects. If the extensions have already been\nregistered, spaCy will raise an error. [See here](#extension-attributes) for the\nextension attributes that will be set. You shouldn't have to call this method\nyourself \u2013 it already runs when you import the package.\n\n#### <kbd>method</kbd> `PyTT_Language.__init__`\n\nSee [`Language.__init__`](https://spacy.io/api/language#init). Expects either a\n`pytt_name` setting in the `meta` or as a keyword argument, specifying the\npretrained model name. This is used to set up the model-specific tokenizer.\n\n#### <kbd>method</kbd> `PyTT_Language.update`\n\nUpdate the models in the pipeline.\n\n| Name            | Type     | Description                                                                                                                                |\n| --------------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------ |\n| `docs`          | iterable | A batch of `Doc` objects or unicode. If unicode, a `Doc` object will be created from the text.                                             |\n| `golds`         | iterable | A batch of `GoldParse` objects or dictionaries. Dictionaries will be used to create [`GoldParse`](https://spacy.io/api/goldparse) objects. |\n| `drop`          | float    | The dropout rate.                                                                                                                          |\n| `sgd`           | callable | An optimizer.                                                                                                                              |\n| `losses`        | dict     | Dictionary to update with the loss, keyed by pipeline component.                                                                           |\n| `component_cfg` | dict     | Config parameters for specific pipeline components, keyed by component name.                                                               |\n\n### <kbd>class</kbd> `PyTT_WordPiecer`\n\nspaCy pipeline component to assign PyTorch-Transformers wordpiece tokenization\nto the Doc, which can then be used by the token vector encoder. Note that this\ncomponent doesn't modify spaCy's tokenization. It only sets extension attributes\n`pytt_word_pieces_`, `pytt_word_pieces` and `pytt_alignment` (alignment between\nwordpiece tokens and spaCy tokens).\n\nThe component is available as `pytt_wordpiecer` and registered via an entry\npoint, so it can also be created using\n[`nlp.create_pipe`](https://spacy.io/api/language#create_pipe):\n\n```python\nwordpiecer = nlp.create_pipe(\"pytt_wordpiecer\")\n```\n\n#### Config\n\nThe component can be configured with the following settings, usually passed in\nas the `**cfg`.\n\n| Name        | Type    | Description                                           |\n| ----------- | ------- | ----------------------------------------------------- |\n| `pytt_name` | unicode | Name of pretrained model, e.g. `\"bert-base-uncased\"`. |\n\n#### <kbd>classmethod</kbd> `PyTT_WordPiecer.from_nlp`\n\nFactory to add to `Language.factories` via entry point.\n\n| Name        | Type                      | Description                                     |\n| ----------- | ------------------------- | ----------------------------------------------- |\n| `nlp`       | `spacy.language.Language` | The `nlp` object the component is created with. |\n| `**cfg`     | -                         | Optional config parameters.                     |\n| **RETURNS** | `PyTT_WordPiecer`         | The wordpiecer.                                 |\n\n#### <kbd>method</kbd> `PyTT_WordPiecer.__init__`\n\nInitialize the component.\n\n| Name        | Type                | Description                                           |\n| ----------- | ------------------- | ----------------------------------------------------- |\n| `vocab`     | `spacy.vocab.Vocab` | The spaCy vocab to use.                               |\n| `name`      | unicode             | Name of pretrained model, e.g. `\"bert-base-uncased\"`. |\n| `**cfg`     | -                   | Optional config parameters.                           |\n| **RETURNS** | `PyTT_WordPiecer`   | The wordpiecer.                                       |\n\n#### <kbd>method</kbd> `PyTT_WordPiecer.predict`\n\nRun the wordpiece tokenizer on a batch of docs and return the extracted strings.\n\n| Name        | Type     | Description                                                                      |\n| ----------- | -------- | -------------------------------------------------------------------------------- |\n| `docs`      | iterable | A batch of `Doc`s to process.                                                    |\n| **RETURNS** | tuple    | A `(strings, None)` tuple. The strings are lists of strings, one list per `Doc`. |\n\n#### <kbd>method</kbd> `PyTT_WordPiecer.set_annotations`\n\nAssign the extracted tokens and IDs to the `Doc` objects.\n\n| Name      | Type     | Description               |\n| --------- | -------- | ------------------------- |\n| `docs`    | iterable | A batch of `Doc` objects. |\n| `outputs` | iterable | A batch of outputs.       |\n\n### <kbd>class</kbd> `PyTT_TokenVectorEncoder`\n\nspaCy pipeline component to use PyTorch-Transformers models. The component\nassigns the output of the transformer to extension attributes. We also calculate\nan alignment between the wordpiece tokens and the spaCy tokenization, so that we\ncan use the last hidden states to set the `doc.tensor` attribute. When multiple\nwordpiece tokens align to the same spaCy token, the spaCy token receives the sum\nof their values.\n\nThe component is available as `pytt_tok2vec` and registered via an entry point,\nso it can also be created using\n[`nlp.create_pipe`](https://spacy.io/api/language#create_pipe):\n\n```python\ntok2vec = nlp.create_pipe(\"pytt_tok2vec\")\n```\n\n#### Config\n\nThe component can be configured with the following settings, usually passed in\nas the `**cfg`.\n\n| Name              | Type    | Description                                                                                                                                                                                                                 |\n| ----------------- | ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `pytt_name`       | unicode | Name of pretrained model, e.g. `\"bert-base-uncased\"`.                                                                                                                                                                       |\n| `words_per_batch` | int     | Group sentences into subbatches of max `words_per_batch` in size. For instance, a batch with one 100 word sentence and one 10 word sentence will have size 200 (due to padding). Set to `0` to disable. Defaults to `2000`. |\n\n#### <kbd>classmethod</kbd> `PyTT_TokenVectorEncoder.from_nlp`\n\nFactory to add to `Language.factories` via entry point.\n\n| Name        | Type                      | Description                                     |\n| ----------- | ------------------------- | ----------------------------------------------- |\n| `nlp`       | `spacy.language.Language` | The `nlp` object the component is created with. |\n| `**cfg`     | -                         | Optional config parameters.                     |\n| **RETURNS** | `PyTT_TokenVectorEncoder` | The token vector encoder.                       |\n\n#### <kbd>classmethod</kbd> `PyTT_TokenVectorEncoder.from_pretrained`\n\nCreate a `PyTT_TokenVectorEncoder` instance using pretrained weights from a\nPyTorch-Transformers model, even if it's not installed as a spaCy package.\n\n```python\nfrom spacy_pytorch_transformers import PyTT_TokenVectorEncoder\nfrom spacy.tokens import Vocab\ntok2vec = PyTT_TokenVectorEncoder.from_pretrained(Vocab(), \"bert-base-uncased\")\n```\n\n| Name        | Type                      | Description                                           |\n| ----------- | ------------------------- | ----------------------------------------------------- |\n| `vocab`     | `spacy.vocab.Vocab`       | The spaCy vocab to use.                               |\n| `name`      | unicode                   | Name of pretrained model, e.g. `\"bert-base-uncased\"`. |\n| `**cfg`     | -                         | Optional config parameters.                           |\n| **RETURNS** | `PyTT_TokenVectorEncoder` | The token vector encoder.                             |\n\n#### <kbd>classmethod</kbd> `PyTT_TokenVectorEncoder.Model`\n\nCreate an instance of `PyTT_Wrapper`, which holds the PyTorch-Transformers\nmodel.\n\n| Name        | Type                 | Description                                           |\n| ----------- | -------------------- | ----------------------------------------------------- |\n| `name`      | unicode              | Name of pretrained model, e.g. `\"bert-base-uncased\"`. |\n| `**cfg`     | -                    | Optional config parameters.                           |\n| **RETURNS** | `thinc.neural.Model` | The wrapped model.                                    |\n\n#### <kbd>method</kbd> `PyTT_TokenVectorEncoder.__init__`\n\nInitialize the component.\n\n| Name        | Type                          | Description                                             |\n| ----------- | ----------------------------- | ------------------------------------------------------- |\n| `vocab`     | `spacy.vocab.Vocab`           | The spaCy vocab to use.                                 |\n| `model`     | `thinc.neural.Model` / `True` | The component's model or `True` if not initialized yet. |\n| `**cfg`     | -                             | Optional config parameters.                             |\n| **RETURNS** | `PyTT_TokenVectorEncoder`     | The token vector encoder.                               |\n\n#### <kbd>method</kbd> `PyTT_TokenVectorEncoder.__call__`\n\nProcess a `Doc` and assign the extracted features.\n\n| Name        | Type               | Description           |\n| ----------- | ------------------ | --------------------- |\n| `doc`       | `spacy.tokens.Doc` | The `Doc` to process. |\n| **RETURNS** | `spacy.tokens.Doc` | The processed `Doc`.  |\n\n#### <kbd>method</kbd> `PyTT_TokenVectorEncoder.pipe`\n\nProcess `Doc` objects as a stream and assign the extracted features.\n\n| Name         | Type               | Description                                       |\n| ------------ | ------------------ | ------------------------------------------------- |\n| `stream`     | iterable           | A stream of `Doc` objects.                        |\n| `batch_size` | int                | The number of texts to buffer. Defaults to `128`. |\n| **YIELDS**   | `spacy.tokens.Doc` | Processed `Doc`s in order.                        |\n\n#### <kbd>method</kbd> `PyTT_TokenVectorEncoder.predict`\n\nRun the transformer model on a batch of docs and return the extracted features.\n\n| Name        | Type         | Description                         |\n| ----------- | ------------ | ----------------------------------- |\n| `docs`      | iterable     | A batch of `Doc`s to process.       |\n| **RETURNS** | `namedtuple` | Named tuple containing the outputs. |\n\n#### <kbd>method</kbd> `PyTT_TokenVectorEncoder.set_annotations`\n\nAssign the extracted features to the Doc objects and overwrite the vector and\nsimilarity hooks.\n\n| Name      | Type     | Description               |\n| --------- | -------- | ------------------------- |\n| `docs`    | iterable | A batch of `Doc` objects. |\n| `outputs` | iterable | A batch of outputs.       |\n\n### <kbd>class</kbd> `PyTT_TextCategorizer`\n\nSubclass of spaCy's built-in\n[`TextCategorizer`](https://spacy.io/api/textcategorizer) component that\nsupports using the features assigned by the PyTorch-Transformers models via the\ntoken vector encoder. It requires the `PyTT_TokenVectorEncoder` to run before it\nin the pipeline.\n\nThe component is available as `pytt_textcat` and registered via an entry point,\nso it can also be created using\n[`nlp.create_pipe`](https://spacy.io/api/language#create_pipe):\n\n```python\ntextcat = nlp.create_pipe(\"pytt_textcat\")\n```\n\n#### <kbd>classmethod</kbd> `PyTT_TextCategorizer.from_nlp`\n\nFactory to add to `Language.factories` via entry point.\n\n| Name        | Type                      | Description                                     |\n| ----------- | ------------------------- | ----------------------------------------------- |\n| `nlp`       | `spacy.language.Language` | The `nlp` object the component is created with. |\n| `**cfg`     | -                         | Optional config parameters.                     |\n| **RETURNS** | `PyTT_TextCategorizer`    | The text categorizer.                           |\n\n#### <kbd>classmethod</kbd> `PyTT_TextCategorizer.Model`\n\nCreate a text classification model using a PyTorch-Transformers model for token\nvector encoding.\n\n| Name                | Type                 | Description                                              |\n| ------------------- | -------------------- | -------------------------------------------------------- |\n| `nr_class`          | int                  | Number of classes.                                       |\n| `width`             | int                  | The width of the tensors being assigned.                 |\n| `exclusive_classes` | bool                 | Make categories mutually exclusive. Defaults to `False`. |\n| `**cfg`             | -                    | Optional config parameters.                              |\n| **RETURNS**         | `thinc.neural.Model` | The model.                                               |\n\n### <kbd>dataclass</kbd> `Activations`\n\nDataclass to hold the features produced by PyTorch-Transformers.\n\n| Attribute           | Type   | Description |\n| ------------------- | ------ | ----------- |\n| `last_hidden_state` | object |             |\n| `pooler_output`     | object |             |\n| `all_hidden_states` | object |             |\n| `all_attentions`    | object |             |\n| `is_grad`           | bool   |             |\n\n### Entry points\n\nThis package exposes several\n[entry points](https://spacy.io/usage/saving-loading#entry-points) that tell\nspaCy how to initialize its components. If `spacy-pytorch-transformers` and\nspaCy are installed in the same environment, you'll be able to run the following\nand it'll work as expected:\n\n```python\ntok2vec = nlp.create_pipe(\"pytt_tok2vec\")\n```\n\nThis also means that your custom models can ship a `pytt_tok2vec` component and\ndefine `\"pytt_tok2vec\"` in their pipelines, and spaCy will know how to create\nthose components when you deserialize the model. The following entry points are\nset:\n\n| Name              | Target                    | Type              | Description                      |\n| ----------------- | ------------------------- | ----------------- | -------------------------------- |\n| `pytt_wordpiecer` | `PyTT_WordPiecer`         | `spacy_factories` | Factory to create the component. |\n| `pytt_tok2vec`    | `PyTT_TokenVectorEncoder` | `spacy_factories` | Factory to create the component. |\n| `pytt_textcat`    | `PyTT_TextCategorizer`    | `spacy_factories` | Factory to create the component. |\n| `pytt`            | `PyTT_Language`           | `spacy_languages` | Custom `Language` subclass.      |", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://explosion.ai", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "spacy-pytorch-transformers", "package_url": "https://pypi.org/project/spacy-pytorch-transformers/", "platform": "", "project_url": "https://pypi.org/project/spacy-pytorch-transformers/", "project_urls": {"Homepage": "https://explosion.ai"}, "release_url": "https://pypi.org/project/spacy-pytorch-transformers/0.4.0/", "requires_dist": null, "requires_python": ">=3.6", "summary": "spaCy pipelines for pre-trained BERT and other transformers", "version": "0.4.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://explosion.ai\" rel=\"nofollow\"><img align=\"right\" height=\"125\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/657dbf4006ad44b146bd59acc7f61d67177af1a5/68747470733a2f2f6578706c6f73696f6e2e61692f6173736574732f696d672f6c6f676f2e737667\" width=\"125\"></a></p>\n<h1>spaCy wrapper for PyTorch Transformers</h1>\n<p>This package provides <a href=\"https://github.com/explosion/spaCy\" rel=\"nofollow\">spaCy</a> model\npipelines that wrap\n<a href=\"https://github.com/huggingface/pytorch-transformers\" rel=\"nofollow\">Hugging Face's <code>pytorch-transformers</code></a>\npackage, so you can use them in spaCy. The result is convenient access to\nstate-of-the-art transformer architectures, such as BERT, GPT-2, XLNet, etc. For\nmore details and background, check out\n<a href=\"https://explosion.ai/blog/spacy-pytorch-transformers\" rel=\"nofollow\">our blog post</a>.</p>\n<p><a href=\"https://dev.azure.com/explosion-ai/public/_build?definitionId=11\" rel=\"nofollow\"><img alt=\"Azure Pipelines\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b3ec46dd3d7e5f4b464abfc297740e0a0086aa84/68747470733a2f2f696d672e736869656c64732e696f2f617a7572652d6465766f70732f6275696c642f6578706c6f73696f6e2d61692f7075626c69632f31312f6d61737465722e7376673f6c6f676f3d617a7572652d6465766f7073267374796c653d666c61742d737175617265\"></a>\n<a href=\"https://pypi.python.org/pypi/spacy-pytorch-transformers\" rel=\"nofollow\"><img alt=\"PyPi\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9d9e6377b3bee8b179feb389a8a1be872a8abe0f/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f73706163792d7079746f7263682d7472616e73666f726d6572732e7376673f7374796c653d666c61742d737175617265\"></a>\n<a href=\"https://github.com/explosion/spacy-pytorch-transformers/releases\" rel=\"nofollow\"><img alt=\"GitHub\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e6b74c8888c606697876d441aaee26f717e23011/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6578706c6f73696f6e2f73706163792d7079746f7263682d7472616e73666f726d6572732f616c6c2e7376673f7374796c653d666c61742d737175617265\"></a>\n<a href=\"https://github.com/ambv/black\" rel=\"nofollow\"><img alt=\"Code style: black\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1c326c58e924b9f3508f32a8ac6b3ee91f40b090/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e7376673f7374796c653d666c61742d737175617265\"></a>\n<a href=\"https://colab.research.google.com/github/explosion/spacy-pytorch-transformers/blob/master/examples/Spacy_Transformers_Demo.ipynb\" rel=\"nofollow\"><img alt=\"Open demo in Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></p>\n<h2>Features</h2>\n<ul>\n<li>Use <strong>BERT</strong>, <strong>RoBERTa</strong>, <strong>XLNet</strong> and <strong>GPT-2</strong> directly in your spaCy\npipeline.</li>\n<li><strong>Fine-tune</strong> pretrained transformer models on your task using spaCy's API.</li>\n<li>Custom component for <strong>text classification</strong> using transformer features.</li>\n<li>Automatic <strong>alignment</strong> of wordpieces and outputs to linguistic tokens.</li>\n<li>Process multi-sentence documents with intelligent <strong>per-sentence\nprediction</strong>.</li>\n<li>Built-in hooks for <strong>context-sensitive vectors</strong> and similarity.</li>\n<li>Out-of-the-box serialization and model packaging.</li>\n</ul>\n<h2>\ud83d\ude80 Quickstart</h2>\n<p>Installing the package from pip will automatically install all dependencies,\nincluding PyTorch and spaCy. Make sure you install this package <strong>before</strong> you\ninstall the models. Also note that this package requires <strong>Python 3.6+</strong> and the\nlatest version of spaCy,\n<a href=\"https://github.com/explosion/spaCy/releases/tag/v2.1.7\" rel=\"nofollow\">v2.1.7</a> or above.</p>\n<pre>pip install spacy-pytorch-transformers\n</pre>\n<p>For GPU installation, find your CUDA version using <code>nvcc --version</code> and add the\n<a href=\"https://spacy.io/usage/#gpu\" rel=\"nofollow\">version in brackets</a>, e.g.\n<code>spacy-pytorch-transformers[cuda92]</code> for CUDA9.2 or\n<code>spacy-pytorch-transformers[cuda100]</code> for CUDA10.0.</p>\n<p>We've also pre-packaged some of the pretrained models as spaCy model packages.\nYou can either use the <code>spacy download</code> command or download the packages from\nthe <a href=\"https://github.com/explosion/spacy-models/releases\" rel=\"nofollow\">model releases</a>.</p>\n<table>\n<thead>\n<tr>\n<th>Package name</th>\n<th>Pretrained model</th>\n<th>Language</th>\n<th>Author</th>\n<th align=\"right\">Size</th>\n<th align=\"center\">Release</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>en_pytt_bertbaseuncased_lg</code></td>\n<td><code>bert-base-uncased</code></td>\n<td>English</td>\n<td><a href=\"https://github.com/google-research/bert\" rel=\"nofollow\">Google Research</a></td>\n<td align=\"right\">406MB</td>\n<td align=\"center\"><a href=\"https://github.com/explosion/spacy-models/releases/tag/en_pytt_bertbaseuncased_lg-2.1.1\" rel=\"nofollow\">\ud83d\udce6\ufe0f</a></td>\n</tr>\n<tr>\n<td><code>de_pytt_bertbasecased_lg</code></td>\n<td><code>bert-base-german-cased</code></td>\n<td>German</td>\n<td><a href=\"https://deepset.ai/german-bert\" rel=\"nofollow\">deepset</a></td>\n<td align=\"right\">406MB</td>\n<td align=\"center\"><a href=\"https://github.com/explosion/spacy-models/releases/tag/de_pytt_bertbasecased_lg-2.1.1\" rel=\"nofollow\">\ud83d\udce6\ufe0f</a></td>\n</tr>\n<tr>\n<td><code>en_pytt_xlnetbasecased_lg</code></td>\n<td><code>xlnet-base-cased</code></td>\n<td>English</td>\n<td><a href=\"https://github.com/zihangdai/xlnet/\" rel=\"nofollow\">CMU/Google Brain</a></td>\n<td align=\"right\">434MB</td>\n<td align=\"center\"><a href=\"https://github.com/explosion/spacy-models/releases/tag/en_pytt_xlnetbasecased_lg-2.1.1\" rel=\"nofollow\">\ud83d\udce6\ufe0f</a></td>\n</tr>\n<tr>\n<td><code>en_pytt_robertabase_lg</code></td>\n<td><code>roberta-base</code></td>\n<td>English</td>\n<td><a href=\"https://github.com/pytorch/fairseq/tree/master/examples/roberta\" rel=\"nofollow\">Facebook</a></td>\n<td align=\"right\">292MB</td>\n<td align=\"center\"><a href=\"https://github.com/explosion/spacy-models/releases/tag/en_pytt_robertabase_lg-2.1.0\" rel=\"nofollow\">\ud83d\udce6\ufe0f</a></td>\n</tr>\n<tr>\n<td><code>en_pytt_distilbertbaseuncased_lg</code></td>\n<td><code>distilbert-base-uncased</code></td>\n<td>English</td>\n<td><a href=\"https://medium.com/huggingface/distilbert-8cf3380435b5\" rel=\"nofollow\">Hugging Face</a></td>\n<td align=\"right\">245MB</td>\n<td align=\"center\"><a href=\"https://github.com/explosion/spacy-models/releases/tag/en_pytt_distilbertbaseuncased_lg-2.1.0\" rel=\"nofollow\">\ud83d\udce6\ufe0f</a></td>\n</tr></tbody></table>\n<pre>python -m spacy download en_pytt_bertbaseuncased_lg\npython -m spacy download de_pytt_bertbasecased_lg\npython -m spacy download en_pytt_xlnetbasecased_lg\npython -m spacy download en_pytt_robertabase_lg\npython -m spacy download en_pytt_distilbertbaseuncased_lg\n</pre>\n<p>Once the model is installed, you can load it in spaCy like any other model\npackage.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">spacy</span>\n\n<span class=\"n\">nlp</span> <span class=\"o\">=</span> <span class=\"n\">spacy</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s2\">\"en_pytt_bertbaseuncased_lg\"</span><span class=\"p\">)</span>\n<span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"p\">(</span><span class=\"s2\">\"Apple shares rose on the news. Apple pie is delicious.\"</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">similarity</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">[</span><span class=\"mi\">7</span><span class=\"p\">]))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">_</span><span class=\"o\">.</span><span class=\"n\">pytt_last_hidden_state</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n</pre>\n<blockquote>\n<p>\ud83d\udca1 If you're seeing an error like <code>No module named 'spacy.lang.pytt'</code>,\ndouble-check that <code>spacy-pytorch-transformers</code> is installed. It needs to be\navailable so it can register its language entry points. Also make sure that\nyou're running spaCy v2.1.7 or higher.</p>\n</blockquote>\n<h2>\ud83d\udcd6 Usage</h2>\n<h3>Transfer learning</h3>\n<p>The main use case for pretrained transformer models is transfer learning. You\nload in a large generic model pretrained on lots of text, and start training on\nyour smaller dataset with labels specific to your problem. This package has\ncustom pipeline components that make this especially easy. We provide an example\ncomponent for text categorization. Development of analogous components for other\ntasks should be quite straight-forward.</p>\n<p>The <code>pytt_textcat</code> component is based on spaCy's built-in\n<a href=\"https://spacy.io/api/textcategorizer\" rel=\"nofollow\"><code>TextCategorizer</code></a> and supports using the\nfeatures assigned by the PyTorch-Transformers models, via the <code>pytt_tok2vec</code>\ncomponent. This lets you use a model like BERT to predict contextual token\nrepresentations, and then learn a text categorizer on top as a task-specific\n\"head\". The API is the same as any other spaCy pipeline:</p>\n<pre><span class=\"n\">TRAIN_DATA</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">(</span><span class=\"s2\">\"text1\"</span><span class=\"p\">,</span> <span class=\"p\">{</span><span class=\"s2\">\"cats\"</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">\"POSITIVE\"</span><span class=\"p\">:</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"s2\">\"NEGATIVE\"</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">}})</span>\n<span class=\"p\">]</span>\n</pre>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">spacy</span>\n<span class=\"kn\">from</span> <span class=\"nn\">spacy.util</span> <span class=\"kn\">import</span> <span class=\"n\">minibatch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">random</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n<span class=\"n\">is_using_gpu</span> <span class=\"o\">=</span> <span class=\"n\">spacy</span><span class=\"o\">.</span><span class=\"n\">prefer_gpu</span><span class=\"p\">()</span>\n<span class=\"k\">if</span> <span class=\"n\">is_using_gpu</span><span class=\"p\">:</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">set_default_tensor_type</span><span class=\"p\">(</span><span class=\"s2\">\"torch.cuda.FloatTensor\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">nlp</span> <span class=\"o\">=</span> <span class=\"n\">spacy</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s2\">\"en_pytt_bertbaseuncased_lg\"</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">pipe_names</span><span class=\"p\">)</span> <span class=\"c1\"># [\"sentencizer\", \"pytt_wordpiecer\", \"pytt_tok2vec\"]</span>\n<span class=\"n\">textcat</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">create_pipe</span><span class=\"p\">(</span><span class=\"s2\">\"pytt_textcat\"</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"exclusive_classes\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">})</span>\n<span class=\"k\">for</span> <span class=\"n\">label</span> <span class=\"ow\">in</span> <span class=\"p\">(</span><span class=\"s2\">\"POSITIVE\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NEGATIVE\"</span><span class=\"p\">):</span>\n    <span class=\"n\">textcat</span><span class=\"o\">.</span><span class=\"n\">add_label</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">)</span>\n<span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">add_pipe</span><span class=\"p\">(</span><span class=\"n\">textcat</span><span class=\"p\">)</span>\n\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">resume_training</span><span class=\"p\">()</span>\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">):</span>\n    <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"n\">TRAIN_DATA</span><span class=\"p\">)</span>\n    <span class=\"n\">losses</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">minibatch</span><span class=\"p\">(</span><span class=\"n\">TRAIN_DATA</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">):</span>\n        <span class=\"n\">texts</span><span class=\"p\">,</span> <span class=\"n\">cats</span> <span class=\"o\">=</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n        <span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">(</span><span class=\"n\">texts</span><span class=\"p\">,</span> <span class=\"n\">cats</span><span class=\"p\">,</span> <span class=\"n\">sgd</span><span class=\"o\">=</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">losses</span><span class=\"o\">=</span><span class=\"n\">losses</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">losses</span><span class=\"p\">)</span>\n<span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">to_disk</span><span class=\"p\">(</span><span class=\"s2\">\"/bert-textcat\"</span><span class=\"p\">)</span>\n</pre>\n<p>For a full example, see the\n<a href=\"examples/train_textcat.py\" rel=\"nofollow\"><code>examples/train_textcat.py</code> script</a>.</p>\n<h3>Vectors and similarity</h3>\n<p>The <code>PyTT_TokenVectorEncoder</code> component of the model sets custom hooks that\noverride the default behaviour of the <code>.vector</code> attribute and <code>.similarity</code>\nmethod of the <code>Token</code>, <code>Span</code> and <code>Doc</code> objects. By default, these usually refer\nto the word vectors table at <code>nlp.vocab.vectors</code>. Naturally, in the transformer\nmodels we'd rather use the <code>doc.tensor</code> attribute, since it holds a much more\ninformative context-sensitive representation.</p>\n<pre><span class=\"n\">apple1</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"p\">(</span><span class=\"s2\">\"Apple shares rose on the news.\"</span><span class=\"p\">)</span>\n<span class=\"n\">apple2</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"p\">(</span><span class=\"s2\">\"Apple sold fewer iPhones this quarter.\"</span><span class=\"p\">)</span>\n<span class=\"n\">apple3</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"p\">(</span><span class=\"s2\">\"Apple pie is delicious.\"</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">apple1</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">similarity</span><span class=\"p\">(</span><span class=\"n\">apple2</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">apple1</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">similarity</span><span class=\"p\">(</span><span class=\"n\">apple3</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]))</span>\n</pre>\n<h3>Serialization</h3>\n<p>Saving and loading pretrained transformer models and packaging them as spaCy\nmodels \u2728just works \u2728 (at least, it should). The wrapper and components follow\nspaCy's API, so when you save and load the <code>nlp</code> object, it...</p>\n<ul>\n<li>Writes the pretrained weights to disk / bytes and loads them back in.</li>\n<li>Adds <code>\"lang_factory\": \"pytt\"</code> in the <code>meta.json</code> so spaCy knows how to\ninitialize the <code>Language</code> class when you load the model.</li>\n<li>Adds this package and its version to the <code>\"requirements\"</code> in the\n<code>meta.json</code>, so when you run\n<a href=\"https://spacy.io/api/cli#package\" rel=\"nofollow\"><code>spacy package</code></a> to create an installable\nPython package it's automatically added to the setup's <code>install_requires</code>.</li>\n</ul>\n<p>For example, if you've trained your own text classifier, you can package it like\nthis:</p>\n<pre>python -m spacy package /bert-textcat /output\n<span class=\"nb\">cd</span> /output/en_pytt_bertbaseuncased_lg-1.0.0\npython setup.py sdist\npip install dist/en_pytt_bertbaseuncased_lg-1.0.0.tar.gz\n</pre>\n<h3>Extension attributes</h3>\n<p>This wrapper sets the following\n<a href=\"https://spacy.io/usage/processing-pipelines#custom-components-attributes\" rel=\"nofollow\">custom extension attributes</a>\non the <code>Doc</code>, <code>Span</code> and <code>Token</code> objects:</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>._.pytt_alignment</code></td>\n<td><code>List[List[int]]</code></td>\n<td>Alignment between wordpieces and spaCy tokens. Contains lists of wordpiece token indices (one per spaCy token) or a list of indices (if called on a <code>Token</code>).</td>\n</tr>\n<tr>\n<td><code>._.pytt_word_pieces</code></td>\n<td><code>List[int]</code></td>\n<td>The wordpiece IDs.</td>\n</tr>\n<tr>\n<td><code>._.pytt_word_pieces_</code></td>\n<td><code>List[str]</code></td>\n<td>The string forms of the wordpiece IDs.</td>\n</tr>\n<tr>\n<td><code>._.pytt_last_hidden_state</code></td>\n<td><code>ndarray</code></td>\n<td>The <code>last_hidden_state</code> output from the PyTorch-Transformers model.</td>\n</tr>\n<tr>\n<td><code>._.pytt_pooler_output</code></td>\n<td><code>List[ndarray]</code></td>\n<td>The <code>pooler_output</code> output from the PyTorch-Transformers model.</td>\n</tr>\n<tr>\n<td><code>._.pytt_all_hidden_states</code></td>\n<td><code>List[ndarray]</code></td>\n<td>The <code>all_hidden_states</code> output from the PyTorch-Transformers model.</td>\n</tr>\n<tr>\n<td><code>._.all_attentions</code></td>\n<td><code>List[ndarray]</code></td>\n<td>The <code>all_attentions</code> output from the PyTorch-Transformers model.</td>\n</tr>\n<tr>\n<td><code>._.pytt_d_last_hidden_state</code></td>\n<td><code>ndarray</code></td>\n<td>The gradient of the <code>last_hidden_state</code> output from the PyTorch-Transformers model.</td>\n</tr>\n<tr>\n<td><code>._.pytt_d_pooler_output</code></td>\n<td><code>List[ndarray]</code></td>\n<td>The gradient of the <code>pooler_output</code> output from the PyTorch-Transformers model.</td>\n</tr>\n<tr>\n<td><code>._.pytt_d_all_hidden_states</code></td>\n<td><code>List[ndarray]</code></td>\n<td>The gradient of the <code>all_hidden_states</code> output from the PyTorch-Transformers model.</td>\n</tr>\n<tr>\n<td><code>._.pytt_d_all_attentions</code></td>\n<td><code>List[ndarray]</code></td>\n<td>The gradient of the <code>all_attentions</code> output from the PyTorch-Transformers model.</td>\n</tr></tbody></table>\n<p>The values can be accessed via the <code>._</code> attribute. For example:</p>\n<pre><span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"p\">(</span><span class=\"s2\">\"This is a text.\"</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">_</span><span class=\"o\">.</span><span class=\"n\">pytt_word_pieces_</span><span class=\"p\">)</span>\n</pre>\n<h3>Setting up the pipeline</h3>\n<p>In order to run, the <code>nlp</code> object created using <code>PyTT_Language</code> requires a few\ncomponents to run in order: a component that assigns sentence boundaries (e.g.\nspaCy's built-in\n<a href=\"https://spacy.io/usage/linguistic-features#sbd-component\" rel=\"nofollow\"><code>Sentencizer</code></a>), the\n<code>PyTT_WordPiecer</code>, which assigns the wordpiece tokens and the\n<code>PyTT_TokenVectorEncoder</code>, which assigns the token vectors. The <code>pytt_name</code>\nargument defines the name of the pretrained model to use. The <code>from_pretrained</code>\nmethods load the pretrained model via <code>pytorch-transformers</code>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">spacy_pytorch_transformers</span> <span class=\"kn\">import</span> <span class=\"n\">PyTT_Language</span><span class=\"p\">,</span> <span class=\"n\">PyTT_WordPiecer</span><span class=\"p\">,</span> <span class=\"n\">PyTT_TokenVectorEncoder</span>\n\n<span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s2\">\"bert-base-uncased\"</span>\n<span class=\"n\">nlp</span> <span class=\"o\">=</span> <span class=\"n\">PyTT_Language</span><span class=\"p\">(</span><span class=\"n\">pytt_name</span><span class=\"o\">=</span><span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"lang\"</span><span class=\"p\">:</span> <span class=\"s2\">\"en\"</span><span class=\"p\">})</span>\n<span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">add_pipe</span><span class=\"p\">(</span><span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">create_pipe</span><span class=\"p\">(</span><span class=\"s2\">\"sentencizer\"</span><span class=\"p\">))</span>\n<span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">add_pipe</span><span class=\"p\">(</span><span class=\"n\">PyTT_WordPiecer</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"p\">))</span>\n<span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">add_pipe</span><span class=\"p\">(</span><span class=\"n\">PyTT_TokenVectorEncoder</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"p\">))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">pipe_names</span><span class=\"p\">)</span>  <span class=\"c1\"># ['sentencizer', 'pytt_wordpiecer', 'pytt_tok2vec']</span>\n</pre>\n<p>You can also use the <a href=\"examples/init_model.py\" rel=\"nofollow\"><code>init_model.py</code></a> script in the\nexamples.</p>\n<h4>Loading models from a path</h4>\n<p>Pytorch-Transformers models can also be loaded from a file path instead of just\na name. For instance, let's say you want to use Allen AI's\n<a href=\"https://github.com/allenai/scibert\" rel=\"nofollow\"><code>scibert</code></a>. First, download the PyTorch\nmodel files, unpack them them, unpack the <code>weights.tar</code>, rename the\n<code>bert_config.json</code> to <code>config.json</code> and put everything into one directory. Your\ndirectory should now have a <code>pytorch_model.bin</code>, <code>vocab.txt</code> and <code>config.json</code>.\nAlso make sure that your path <strong>includes the name of the model</strong>. You can then\ninitialize the <code>nlp</code> object like this:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">spacy_pytorch_transformers</span> <span class=\"kn\">import</span> <span class=\"n\">PyTT_Language</span><span class=\"p\">,</span> <span class=\"n\">PyTT_WordPiecer</span><span class=\"p\">,</span> <span class=\"n\">PyTT_TokenVectorEncoder</span>\n\n<span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s2\">\"scibert-scivocab-uncased\"</span>\n<span class=\"n\">path</span> <span class=\"o\">=</span> <span class=\"s2\">\"/path/to/scibert-scivocab-uncased\"</span>\n\n<span class=\"n\">nlp</span> <span class=\"o\">=</span> <span class=\"n\">PyTT_Language</span><span class=\"p\">(</span><span class=\"n\">pytt_name</span><span class=\"o\">=</span><span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"lang\"</span><span class=\"p\">:</span> <span class=\"s2\">\"en\"</span><span class=\"p\">})</span>\n<span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">add_pipe</span><span class=\"p\">(</span><span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">create_pipe</span><span class=\"p\">(</span><span class=\"s2\">\"sentencizer\"</span><span class=\"p\">))</span>\n<span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">add_pipe</span><span class=\"p\">(</span><span class=\"n\">PyTT_WordPiecer</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">,</span> <span class=\"n\">path</span><span class=\"p\">))</span>\n<span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">add_pipe</span><span class=\"p\">(</span><span class=\"n\">PyTT_TokenVectorEncoder</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">,</span> <span class=\"n\">path</span><span class=\"p\">))</span>\n</pre>\n<h3>Tokenization alignment</h3>\n<p>Transformer models are usually trained on text preprocessed with the \"wordpiece\"\nalgorithm, which limits the number of distinct token-types the model needs to\nconsider. Wordpiece is convenient for training neural networks, but it doesn't\nproduce segmentations that match up to any linguistic notion of a \"word\". Most\nrare words will map to multiple wordpiece tokens, and occasionally the alignment\nwill be many-to-many. <code>spacy-pytorch-transformers</code> calculates this alignment,\nwhich you can access at <code>doc._.pytt_alignment</code>. It's a list of length equal to\nthe number of spaCy tokens. Each value in the list is a list of consecutive\nintegers, which are indexes into the wordpieces list.</p>\n<p>If you can work on representations that aren't aligned to actual words, it's\nbest to use the raw outputs of the transformer, which can be accessed at\n<code>doc._.pytt_last_hidden_state</code>. This variable gives you a tensor with one row\nper wordpiece token.</p>\n<p>If you're working on token-level tasks such as part-of-speech tagging or\nspelling correction, you'll want to work on the token-aligned features, which\nare stored in the <code>doc.tensor</code> variable.</p>\n<p>We've taken care to calculate the aligned <code>doc.tensor</code> representation as\nfaithfully as possible, with priority given to avoid information loss. The\nalignment has been calculated such that\n<code>doc.tensor.sum(axis=1) == doc._.pytt_last_hidden_state.sum(axis=1)</code>. To make\nthis work, each row of the <code>doc.tensor</code> (which corresponds to a spaCy token) is\nset to a weighted sum of the rows of the <code>last_hidden_state</code> tensor that the\ntoken is aligned to, where the weighting is proportional to the number of other\nspaCy tokens aligned to that row. To include the information from the (often\nimportant --- see Clark et al., 2019) boundary tokens, we imagine that these are\nalso \"aligned\" to all of the tokens in the sentence.</p>\n<h3>Batching, padding and per-sentence processing</h3>\n<p>Transformer models have cubic runtime and memory complexity with respect to\nsequence length. This means that longer texts need to be divided into sentences\nin order to achieve reasonable efficiency.</p>\n<p><code>spacy-pytorch-transformers</code> handles this internally, and requires that sort of\nsentence-boundary detection component has been added to the pipeline. We\nrecommend:</p>\n<pre><span class=\"n\">sentencizer</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">create_pipe</span><span class=\"p\">(</span><span class=\"s2\">\"sentencizer\"</span><span class=\"p\">)</span>\n<span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">add_pipe</span><span class=\"p\">(</span><span class=\"n\">sentencizer</span><span class=\"p\">,</span> <span class=\"n\">first</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<p>Internally, the transformer model will predict over sentences, and the resulting\ntensor features will be reconstructed to produce document-level annotations.</p>\n<p>In order to further improve efficiency and reduce memory requirements,\n<code>spacy-pytorch-transformers</code> also performs length-based subbatching internally.\nThe subbatching regroups the batched sentences by sequence length, to minimise\nthe amount of padding required. The configuration option <code>words_per_batch</code>\ncontrols this behaviour. You can set it to 0 to disable the subbatching, or set\nit to an integer to require a maximum limit on the number of words (including\npadding) per subbatch. The default value of 3000 words works reasonably well on\na Tesla V100.</p>\n<p>Many of the pretrained transformer models have a maximum sequence length. If a\nsentence is longer than the maximum, it is truncated and the affected ending\ntokens will receive zeroed vectors.</p>\n<h2>\ud83c\udf9b API</h2>\n<h3><kbd>class</kbd> <code>PyTT_Language</code></h3>\n<p>A subclass of <a href=\"https://spacy.io/api/language\" rel=\"nofollow\"><code>Language</code></a> that holds a\nPyTorch-Transformer (PyTT) pipeline. PyTT pipelines work only slightly\ndifferently from spaCy's default pipelines. Specifically, we introduce a new\npipeline component at the start of the pipeline, <code>PyTT_TokenVectorEncoder</code>. We\nthen modify the <a href=\"https://spacy.io/api/language#update\" rel=\"nofollow\"><code>nlp.update</code></a> function to\nrun the <code>PyTT_TokenVectorEncoder</code> before the other pipeline components, and\nbackprop it after the other components are done.</p>\n<h4><kbd>staticmethod</kbd> <code>PyTT_Language.install_extensions</code></h4>\n<p>Register the\n<a href=\"https://spacy.io/usage/processing-pipelines#custom-components-attributes\" rel=\"nofollow\">custom extension attributes</a>\non the <code>Doc</code>, <code>Span</code> and <code>Token</code> objects. If the extensions have already been\nregistered, spaCy will raise an error. <a href=\"#extension-attributes\" rel=\"nofollow\">See here</a> for the\nextension attributes that will be set. You shouldn't have to call this method\nyourself \u2013 it already runs when you import the package.</p>\n<h4><kbd>method</kbd> <code>PyTT_Language.__init__</code></h4>\n<p>See <a href=\"https://spacy.io/api/language#init\" rel=\"nofollow\"><code>Language.__init__</code></a>. Expects either a\n<code>pytt_name</code> setting in the <code>meta</code> or as a keyword argument, specifying the\npretrained model name. This is used to set up the model-specific tokenizer.</p>\n<h4><kbd>method</kbd> <code>PyTT_Language.update</code></h4>\n<p>Update the models in the pipeline.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>docs</code></td>\n<td>iterable</td>\n<td>A batch of <code>Doc</code> objects or unicode. If unicode, a <code>Doc</code> object will be created from the text.</td>\n</tr>\n<tr>\n<td><code>golds</code></td>\n<td>iterable</td>\n<td>A batch of <code>GoldParse</code> objects or dictionaries. Dictionaries will be used to create <a href=\"https://spacy.io/api/goldparse\" rel=\"nofollow\"><code>GoldParse</code></a> objects.</td>\n</tr>\n<tr>\n<td><code>drop</code></td>\n<td>float</td>\n<td>The dropout rate.</td>\n</tr>\n<tr>\n<td><code>sgd</code></td>\n<td>callable</td>\n<td>An optimizer.</td>\n</tr>\n<tr>\n<td><code>losses</code></td>\n<td>dict</td>\n<td>Dictionary to update with the loss, keyed by pipeline component.</td>\n</tr>\n<tr>\n<td><code>component_cfg</code></td>\n<td>dict</td>\n<td>Config parameters for specific pipeline components, keyed by component name.</td>\n</tr></tbody></table>\n<h3><kbd>class</kbd> <code>PyTT_WordPiecer</code></h3>\n<p>spaCy pipeline component to assign PyTorch-Transformers wordpiece tokenization\nto the Doc, which can then be used by the token vector encoder. Note that this\ncomponent doesn't modify spaCy's tokenization. It only sets extension attributes\n<code>pytt_word_pieces_</code>, <code>pytt_word_pieces</code> and <code>pytt_alignment</code> (alignment between\nwordpiece tokens and spaCy tokens).</p>\n<p>The component is available as <code>pytt_wordpiecer</code> and registered via an entry\npoint, so it can also be created using\n<a href=\"https://spacy.io/api/language#create_pipe\" rel=\"nofollow\"><code>nlp.create_pipe</code></a>:</p>\n<pre><span class=\"n\">wordpiecer</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">create_pipe</span><span class=\"p\">(</span><span class=\"s2\">\"pytt_wordpiecer\"</span><span class=\"p\">)</span>\n</pre>\n<h4>Config</h4>\n<p>The component can be configured with the following settings, usually passed in\nas the <code>**cfg</code>.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>pytt_name</code></td>\n<td>unicode</td>\n<td>Name of pretrained model, e.g. <code>\"bert-base-uncased\"</code>.</td>\n</tr></tbody></table>\n<h4><kbd>classmethod</kbd> <code>PyTT_WordPiecer.from_nlp</code></h4>\n<p>Factory to add to <code>Language.factories</code> via entry point.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>nlp</code></td>\n<td><code>spacy.language.Language</code></td>\n<td>The <code>nlp</code> object the component is created with.</td>\n</tr>\n<tr>\n<td><code>**cfg</code></td>\n<td>-</td>\n<td>Optional config parameters.</td>\n</tr>\n<tr>\n<td><strong>RETURNS</strong></td>\n<td><code>PyTT_WordPiecer</code></td>\n<td>The wordpiecer.</td>\n</tr></tbody></table>\n<h4><kbd>method</kbd> <code>PyTT_WordPiecer.__init__</code></h4>\n<p>Initialize the component.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>vocab</code></td>\n<td><code>spacy.vocab.Vocab</code></td>\n<td>The spaCy vocab to use.</td>\n</tr>\n<tr>\n<td><code>name</code></td>\n<td>unicode</td>\n<td>Name of pretrained model, e.g. <code>\"bert-base-uncased\"</code>.</td>\n</tr>\n<tr>\n<td><code>**cfg</code></td>\n<td>-</td>\n<td>Optional config parameters.</td>\n</tr>\n<tr>\n<td><strong>RETURNS</strong></td>\n<td><code>PyTT_WordPiecer</code></td>\n<td>The wordpiecer.</td>\n</tr></tbody></table>\n<h4><kbd>method</kbd> <code>PyTT_WordPiecer.predict</code></h4>\n<p>Run the wordpiece tokenizer on a batch of docs and return the extracted strings.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>docs</code></td>\n<td>iterable</td>\n<td>A batch of <code>Doc</code>s to process.</td>\n</tr>\n<tr>\n<td><strong>RETURNS</strong></td>\n<td>tuple</td>\n<td>A <code>(strings, None)</code> tuple. The strings are lists of strings, one list per <code>Doc</code>.</td>\n</tr></tbody></table>\n<h4><kbd>method</kbd> <code>PyTT_WordPiecer.set_annotations</code></h4>\n<p>Assign the extracted tokens and IDs to the <code>Doc</code> objects.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>docs</code></td>\n<td>iterable</td>\n<td>A batch of <code>Doc</code> objects.</td>\n</tr>\n<tr>\n<td><code>outputs</code></td>\n<td>iterable</td>\n<td>A batch of outputs.</td>\n</tr></tbody></table>\n<h3><kbd>class</kbd> <code>PyTT_TokenVectorEncoder</code></h3>\n<p>spaCy pipeline component to use PyTorch-Transformers models. The component\nassigns the output of the transformer to extension attributes. We also calculate\nan alignment between the wordpiece tokens and the spaCy tokenization, so that we\ncan use the last hidden states to set the <code>doc.tensor</code> attribute. When multiple\nwordpiece tokens align to the same spaCy token, the spaCy token receives the sum\nof their values.</p>\n<p>The component is available as <code>pytt_tok2vec</code> and registered via an entry point,\nso it can also be created using\n<a href=\"https://spacy.io/api/language#create_pipe\" rel=\"nofollow\"><code>nlp.create_pipe</code></a>:</p>\n<pre><span class=\"n\">tok2vec</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">create_pipe</span><span class=\"p\">(</span><span class=\"s2\">\"pytt_tok2vec\"</span><span class=\"p\">)</span>\n</pre>\n<h4>Config</h4>\n<p>The component can be configured with the following settings, usually passed in\nas the <code>**cfg</code>.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>pytt_name</code></td>\n<td>unicode</td>\n<td>Name of pretrained model, e.g. <code>\"bert-base-uncased\"</code>.</td>\n</tr>\n<tr>\n<td><code>words_per_batch</code></td>\n<td>int</td>\n<td>Group sentences into subbatches of max <code>words_per_batch</code> in size. For instance, a batch with one 100 word sentence and one 10 word sentence will have size 200 (due to padding). Set to <code>0</code> to disable. Defaults to <code>2000</code>.</td>\n</tr></tbody></table>\n<h4><kbd>classmethod</kbd> <code>PyTT_TokenVectorEncoder.from_nlp</code></h4>\n<p>Factory to add to <code>Language.factories</code> via entry point.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>nlp</code></td>\n<td><code>spacy.language.Language</code></td>\n<td>The <code>nlp</code> object the component is created with.</td>\n</tr>\n<tr>\n<td><code>**cfg</code></td>\n<td>-</td>\n<td>Optional config parameters.</td>\n</tr>\n<tr>\n<td><strong>RETURNS</strong></td>\n<td><code>PyTT_TokenVectorEncoder</code></td>\n<td>The token vector encoder.</td>\n</tr></tbody></table>\n<h4><kbd>classmethod</kbd> <code>PyTT_TokenVectorEncoder.from_pretrained</code></h4>\n<p>Create a <code>PyTT_TokenVectorEncoder</code> instance using pretrained weights from a\nPyTorch-Transformers model, even if it's not installed as a spaCy package.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">spacy_pytorch_transformers</span> <span class=\"kn\">import</span> <span class=\"n\">PyTT_TokenVectorEncoder</span>\n<span class=\"kn\">from</span> <span class=\"nn\">spacy.tokens</span> <span class=\"kn\">import</span> <span class=\"n\">Vocab</span>\n<span class=\"n\">tok2vec</span> <span class=\"o\">=</span> <span class=\"n\">PyTT_TokenVectorEncoder</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"n\">Vocab</span><span class=\"p\">(),</span> <span class=\"s2\">\"bert-base-uncased\"</span><span class=\"p\">)</span>\n</pre>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>vocab</code></td>\n<td><code>spacy.vocab.Vocab</code></td>\n<td>The spaCy vocab to use.</td>\n</tr>\n<tr>\n<td><code>name</code></td>\n<td>unicode</td>\n<td>Name of pretrained model, e.g. <code>\"bert-base-uncased\"</code>.</td>\n</tr>\n<tr>\n<td><code>**cfg</code></td>\n<td>-</td>\n<td>Optional config parameters.</td>\n</tr>\n<tr>\n<td><strong>RETURNS</strong></td>\n<td><code>PyTT_TokenVectorEncoder</code></td>\n<td>The token vector encoder.</td>\n</tr></tbody></table>\n<h4><kbd>classmethod</kbd> <code>PyTT_TokenVectorEncoder.Model</code></h4>\n<p>Create an instance of <code>PyTT_Wrapper</code>, which holds the PyTorch-Transformers\nmodel.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>name</code></td>\n<td>unicode</td>\n<td>Name of pretrained model, e.g. <code>\"bert-base-uncased\"</code>.</td>\n</tr>\n<tr>\n<td><code>**cfg</code></td>\n<td>-</td>\n<td>Optional config parameters.</td>\n</tr>\n<tr>\n<td><strong>RETURNS</strong></td>\n<td><code>thinc.neural.Model</code></td>\n<td>The wrapped model.</td>\n</tr></tbody></table>\n<h4><kbd>method</kbd> <code>PyTT_TokenVectorEncoder.__init__</code></h4>\n<p>Initialize the component.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>vocab</code></td>\n<td><code>spacy.vocab.Vocab</code></td>\n<td>The spaCy vocab to use.</td>\n</tr>\n<tr>\n<td><code>model</code></td>\n<td><code>thinc.neural.Model</code> / <code>True</code></td>\n<td>The component's model or <code>True</code> if not initialized yet.</td>\n</tr>\n<tr>\n<td><code>**cfg</code></td>\n<td>-</td>\n<td>Optional config parameters.</td>\n</tr>\n<tr>\n<td><strong>RETURNS</strong></td>\n<td><code>PyTT_TokenVectorEncoder</code></td>\n<td>The token vector encoder.</td>\n</tr></tbody></table>\n<h4><kbd>method</kbd> <code>PyTT_TokenVectorEncoder.__call__</code></h4>\n<p>Process a <code>Doc</code> and assign the extracted features.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>doc</code></td>\n<td><code>spacy.tokens.Doc</code></td>\n<td>The <code>Doc</code> to process.</td>\n</tr>\n<tr>\n<td><strong>RETURNS</strong></td>\n<td><code>spacy.tokens.Doc</code></td>\n<td>The processed <code>Doc</code>.</td>\n</tr></tbody></table>\n<h4><kbd>method</kbd> <code>PyTT_TokenVectorEncoder.pipe</code></h4>\n<p>Process <code>Doc</code> objects as a stream and assign the extracted features.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>stream</code></td>\n<td>iterable</td>\n<td>A stream of <code>Doc</code> objects.</td>\n</tr>\n<tr>\n<td><code>batch_size</code></td>\n<td>int</td>\n<td>The number of texts to buffer. Defaults to <code>128</code>.</td>\n</tr>\n<tr>\n<td><strong>YIELDS</strong></td>\n<td><code>spacy.tokens.Doc</code></td>\n<td>Processed <code>Doc</code>s in order.</td>\n</tr></tbody></table>\n<h4><kbd>method</kbd> <code>PyTT_TokenVectorEncoder.predict</code></h4>\n<p>Run the transformer model on a batch of docs and return the extracted features.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>docs</code></td>\n<td>iterable</td>\n<td>A batch of <code>Doc</code>s to process.</td>\n</tr>\n<tr>\n<td><strong>RETURNS</strong></td>\n<td><code>namedtuple</code></td>\n<td>Named tuple containing the outputs.</td>\n</tr></tbody></table>\n<h4><kbd>method</kbd> <code>PyTT_TokenVectorEncoder.set_annotations</code></h4>\n<p>Assign the extracted features to the Doc objects and overwrite the vector and\nsimilarity hooks.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>docs</code></td>\n<td>iterable</td>\n<td>A batch of <code>Doc</code> objects.</td>\n</tr>\n<tr>\n<td><code>outputs</code></td>\n<td>iterable</td>\n<td>A batch of outputs.</td>\n</tr></tbody></table>\n<h3><kbd>class</kbd> <code>PyTT_TextCategorizer</code></h3>\n<p>Subclass of spaCy's built-in\n<a href=\"https://spacy.io/api/textcategorizer\" rel=\"nofollow\"><code>TextCategorizer</code></a> component that\nsupports using the features assigned by the PyTorch-Transformers models via the\ntoken vector encoder. It requires the <code>PyTT_TokenVectorEncoder</code> to run before it\nin the pipeline.</p>\n<p>The component is available as <code>pytt_textcat</code> and registered via an entry point,\nso it can also be created using\n<a href=\"https://spacy.io/api/language#create_pipe\" rel=\"nofollow\"><code>nlp.create_pipe</code></a>:</p>\n<pre><span class=\"n\">textcat</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">create_pipe</span><span class=\"p\">(</span><span class=\"s2\">\"pytt_textcat\"</span><span class=\"p\">)</span>\n</pre>\n<h4><kbd>classmethod</kbd> <code>PyTT_TextCategorizer.from_nlp</code></h4>\n<p>Factory to add to <code>Language.factories</code> via entry point.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>nlp</code></td>\n<td><code>spacy.language.Language</code></td>\n<td>The <code>nlp</code> object the component is created with.</td>\n</tr>\n<tr>\n<td><code>**cfg</code></td>\n<td>-</td>\n<td>Optional config parameters.</td>\n</tr>\n<tr>\n<td><strong>RETURNS</strong></td>\n<td><code>PyTT_TextCategorizer</code></td>\n<td>The text categorizer.</td>\n</tr></tbody></table>\n<h4><kbd>classmethod</kbd> <code>PyTT_TextCategorizer.Model</code></h4>\n<p>Create a text classification model using a PyTorch-Transformers model for token\nvector encoding.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>nr_class</code></td>\n<td>int</td>\n<td>Number of classes.</td>\n</tr>\n<tr>\n<td><code>width</code></td>\n<td>int</td>\n<td>The width of the tensors being assigned.</td>\n</tr>\n<tr>\n<td><code>exclusive_classes</code></td>\n<td>bool</td>\n<td>Make categories mutually exclusive. Defaults to <code>False</code>.</td>\n</tr>\n<tr>\n<td><code>**cfg</code></td>\n<td>-</td>\n<td>Optional config parameters.</td>\n</tr>\n<tr>\n<td><strong>RETURNS</strong></td>\n<td><code>thinc.neural.Model</code></td>\n<td>The model.</td>\n</tr></tbody></table>\n<h3><kbd>dataclass</kbd> <code>Activations</code></h3>\n<p>Dataclass to hold the features produced by PyTorch-Transformers.</p>\n<table>\n<thead>\n<tr>\n<th>Attribute</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>last_hidden_state</code></td>\n<td>object</td>\n<td></td>\n</tr>\n<tr>\n<td><code>pooler_output</code></td>\n<td>object</td>\n<td></td>\n</tr>\n<tr>\n<td><code>all_hidden_states</code></td>\n<td>object</td>\n<td></td>\n</tr>\n<tr>\n<td><code>all_attentions</code></td>\n<td>object</td>\n<td></td>\n</tr>\n<tr>\n<td><code>is_grad</code></td>\n<td>bool</td>\n<td></td>\n</tr></tbody></table>\n<h3>Entry points</h3>\n<p>This package exposes several\n<a href=\"https://spacy.io/usage/saving-loading#entry-points\" rel=\"nofollow\">entry points</a> that tell\nspaCy how to initialize its components. If <code>spacy-pytorch-transformers</code> and\nspaCy are installed in the same environment, you'll be able to run the following\nand it'll work as expected:</p>\n<pre><span class=\"n\">tok2vec</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">create_pipe</span><span class=\"p\">(</span><span class=\"s2\">\"pytt_tok2vec\"</span><span class=\"p\">)</span>\n</pre>\n<p>This also means that your custom models can ship a <code>pytt_tok2vec</code> component and\ndefine <code>\"pytt_tok2vec\"</code> in their pipelines, and spaCy will know how to create\nthose components when you deserialize the model. The following entry points are\nset:</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Target</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>pytt_wordpiecer</code></td>\n<td><code>PyTT_WordPiecer</code></td>\n<td><code>spacy_factories</code></td>\n<td>Factory to create the component.</td>\n</tr>\n<tr>\n<td><code>pytt_tok2vec</code></td>\n<td><code>PyTT_TokenVectorEncoder</code></td>\n<td><code>spacy_factories</code></td>\n<td>Factory to create the component.</td>\n</tr>\n<tr>\n<td><code>pytt_textcat</code></td>\n<td><code>PyTT_TextCategorizer</code></td>\n<td><code>spacy_factories</code></td>\n<td>Factory to create the component.</td>\n</tr>\n<tr>\n<td><code>pytt</code></td>\n<td><code>PyTT_Language</code></td>\n<td><code>spacy_languages</code></td>\n<td>Custom <code>Language</code> subclass.</td>\n</tr></tbody></table>\n\n          </div>"}, "last_serial": 5852998, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "730cdfd62c294a97e8eadc11d7b73466", "sha256": "b7319154416f51cbe7ada9ec3f1e7777ce7cac83bba21f629be7800af220379f"}, "downloads": -1, "filename": "spacy_pytorch_transformers-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "730cdfd62c294a97e8eadc11d7b73466", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 36407, "upload_time": "2019-08-02T19:37:12", "upload_time_iso_8601": "2019-08-02T19:37:12.741024Z", "url": "https://files.pythonhosted.org/packages/46/a9/c39c72db9f2bb9f47355afa39beda3bbce46c4eb0f9f4347289fdb20344e/spacy_pytorch_transformers-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "47e0984fc3435643339bc4fba8571235", "sha256": "b7e4f9d057a11a4291e26a83f1f7acb0bab9a919038eaf5c70600a6b8b5f81cd"}, "downloads": -1, "filename": "spacy-pytorch-transformers-0.0.1.tar.gz", "has_sig": false, "md5_digest": "47e0984fc3435643339bc4fba8571235", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 43017, "upload_time": "2019-08-02T19:36:58", "upload_time_iso_8601": "2019-08-02T19:36:58.554569Z", "url": "https://files.pythonhosted.org/packages/21/18/ce2c35584c91a0127c0f45ac093426b7143564a8553dc7b176aef8a92442/spacy-pytorch-transformers-0.0.1.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "ca53cd19d48dcd962d9dd462d283623c", "sha256": "49b09791d73f3d355361c2d70f0f58908ac60602fe84d55ffd2642f3c7bea2f0"}, "downloads": -1, "filename": "spacy_pytorch_transformers-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "ca53cd19d48dcd962d9dd462d283623c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 54546, "upload_time": "2019-08-10T09:28:42", "upload_time_iso_8601": "2019-08-10T09:28:42.618053Z", "url": "https://files.pythonhosted.org/packages/3a/14/d167f0e41813f3a61f4dadeaddbf6eda291932aac3431b526fbeb424526d/spacy_pytorch_transformers-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5d7d8f32d40e7f9dec96ed1ced7ac310", "sha256": "5e7b82002615bf31a68a6357ee9c18ddb05a412e0e502f0d510f767311c1b88b"}, "downloads": -1, "filename": "spacy-pytorch-transformers-0.1.0.tar.gz", "has_sig": false, "md5_digest": "5d7d8f32d40e7f9dec96ed1ced7ac310", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 51196, "upload_time": "2019-08-10T09:28:28", "upload_time_iso_8601": "2019-08-10T09:28:28.289905Z", "url": "https://files.pythonhosted.org/packages/17/52/c676c2663bb8b53ffe69ad38b6790bfc98c6e15561e5e7169dc2d8d3330f/spacy-pytorch-transformers-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "99ac4d9f07ef00205964447424926b40", "sha256": "eb8caf7e10d945dcb87059d9103f3ff8b7fd78b3da4bc955b25ee3fb5729d44d"}, "downloads": -1, "filename": "spacy_pytorch_transformers-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "99ac4d9f07ef00205964447424926b40", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 54580, "upload_time": "2019-08-10T12:12:36", "upload_time_iso_8601": "2019-08-10T12:12:36.063071Z", "url": "https://files.pythonhosted.org/packages/a2/a0/c003c654c172b9e7b127c371af6792ffa87a60c9e576976ef10572caf939/spacy_pytorch_transformers-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9b58f0a4fab27a6097790e2014ab813e", "sha256": "9577559b49678c3ba60371d0d85979f1795cfa0e7cc98ec8f3a2f43089bfa2da"}, "downloads": -1, "filename": "spacy-pytorch-transformers-0.1.1.tar.gz", "has_sig": false, "md5_digest": "9b58f0a4fab27a6097790e2014ab813e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 51240, "upload_time": "2019-08-10T12:12:22", "upload_time_iso_8601": "2019-08-10T12:12:22.090616Z", "url": "https://files.pythonhosted.org/packages/3f/53/31d7fea43f9177a7b59805edef0b78102c937cb03730d83c9f039967b509/spacy-pytorch-transformers-0.1.1.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "df0b83e8fdc4da7220eedcca91cecbbd", "sha256": "e937cfcdb2d612cc6ebc81cbc3ecf5b58af7a3f3b0e8863aca6a440a61c3852a"}, "downloads": -1, "filename": "spacy_pytorch_transformers-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "df0b83e8fdc4da7220eedcca91cecbbd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 57934, "upload_time": "2019-08-12T11:24:12", "upload_time_iso_8601": "2019-08-12T11:24:12.470135Z", "url": "https://files.pythonhosted.org/packages/b4/a5/45618feff3774b96b046eaafd0d5980c8671159da0bac8ac308dc387532f/spacy_pytorch_transformers-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "71833d04a7c2ef0ba4f950f034a284d2", "sha256": "ffd17828ba073c4fa7bd593fd41f8969f45c52c78f9847a7a8f847d07d67474a"}, "downloads": -1, "filename": "spacy-pytorch-transformers-0.2.0.tar.gz", "has_sig": false, "md5_digest": "71833d04a7c2ef0ba4f950f034a284d2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 52123, "upload_time": "2019-08-12T11:24:00", "upload_time_iso_8601": "2019-08-12T11:24:00.337230Z", "url": "https://files.pythonhosted.org/packages/79/94/15d79aed3d0822ed98a8593592de73e53cac1708e9a5500d4a2c0c53c0e9/spacy-pytorch-transformers-0.2.0.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "7cc7220083fb1eaca5513345f842ff23", "sha256": "9b2f943e19d01dd35b3689fac5f99fcc9becd6816cac7cef08f4c5822926f37e"}, "downloads": -1, "filename": "spacy_pytorch_transformers-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "7cc7220083fb1eaca5513345f842ff23", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 61921, "upload_time": "2019-08-27T10:11:07", "upload_time_iso_8601": "2019-08-27T10:11:07.702131Z", "url": "https://files.pythonhosted.org/packages/52/4e/c1e18b58eeb7d1bcee19aca284daf1e8658005b8f47c05443a94be377ee8/spacy_pytorch_transformers-0.3.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "726238a81713300c6f6dafc47cd99eac", "sha256": "6ceac4c6db0f507ffd6aeee1620ebed7b07ab97eff4edb72c3a3e2aa3f40d788"}, "downloads": -1, "filename": "spacy-pytorch-transformers-0.3.0.tar.gz", "has_sig": false, "md5_digest": "726238a81713300c6f6dafc47cd99eac", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 56125, "upload_time": "2019-08-27T10:10:54", "upload_time_iso_8601": "2019-08-27T10:10:54.313408Z", "url": "https://files.pythonhosted.org/packages/2e/50/478af9def719ca887734bde66775f51bab0f775cacbe039bad65eb484b77/spacy-pytorch-transformers-0.3.0.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "943c07c260ba7f3da26880cac4b0ab34", "sha256": "77ed625fb02001f73b55d62ff5388b33ad23995465e3a48334742317eac7a10f"}, "downloads": -1, "filename": "spacy_pytorch_transformers-0.4.0-py3-none-any.whl", "has_sig": false, "md5_digest": "943c07c260ba7f3da26880cac4b0ab34", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 62135, "upload_time": "2019-09-04T15:13:11", "upload_time_iso_8601": "2019-09-04T15:13:11.407061Z", "url": "https://files.pythonhosted.org/packages/fd/46/3271586944ee5e0bd493df03b1ad189eb9ccdad1d2476aeb843b0d2f1b47/spacy_pytorch_transformers-0.4.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "54d1a0d200054b33bd2013cac20c9638", "sha256": "7ff4be62212d636c126b21ac54367c0c210ebdabfb67837b904cde8e55123cde"}, "downloads": -1, "filename": "spacy-pytorch-transformers-0.4.0.tar.gz", "has_sig": false, "md5_digest": "54d1a0d200054b33bd2013cac20c9638", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 56516, "upload_time": "2019-09-04T15:12:58", "upload_time_iso_8601": "2019-09-04T15:12:58.420454Z", "url": "https://files.pythonhosted.org/packages/46/65/85f4cc13b6b2fe93b9eb646d422b9a049e2ac4372e86ab081b1ba5e996f4/spacy-pytorch-transformers-0.4.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "943c07c260ba7f3da26880cac4b0ab34", "sha256": "77ed625fb02001f73b55d62ff5388b33ad23995465e3a48334742317eac7a10f"}, "downloads": -1, "filename": "spacy_pytorch_transformers-0.4.0-py3-none-any.whl", "has_sig": false, "md5_digest": "943c07c260ba7f3da26880cac4b0ab34", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 62135, "upload_time": "2019-09-04T15:13:11", "upload_time_iso_8601": "2019-09-04T15:13:11.407061Z", "url": "https://files.pythonhosted.org/packages/fd/46/3271586944ee5e0bd493df03b1ad189eb9ccdad1d2476aeb843b0d2f1b47/spacy_pytorch_transformers-0.4.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "54d1a0d200054b33bd2013cac20c9638", "sha256": "7ff4be62212d636c126b21ac54367c0c210ebdabfb67837b904cde8e55123cde"}, "downloads": -1, "filename": "spacy-pytorch-transformers-0.4.0.tar.gz", "has_sig": false, "md5_digest": "54d1a0d200054b33bd2013cac20c9638", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 56516, "upload_time": "2019-09-04T15:12:58", "upload_time_iso_8601": "2019-09-04T15:12:58.420454Z", "url": "https://files.pythonhosted.org/packages/46/65/85f4cc13b6b2fe93b9eb646d422b9a049e2ac4372e86ab081b1ba5e996f4/spacy-pytorch-transformers-0.4.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:06:03 2020"}