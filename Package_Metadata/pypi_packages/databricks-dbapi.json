{"info": {"author": "Christopher Flynn", "author_email": "crf204@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: Implementation :: CPython", "Topic :: Database", "Topic :: Database :: Front-Ends"], "description": "databricks-dbapi\n================\n\n|pypi| |pyversions|\n\n.. |pypi| image:: https://img.shields.io/pypi/v/databricks-dbapi.svg\n    :target: https://pypi.python.org/pypi/databricks-dbapi\n\n.. |pyversions| image:: https://img.shields.io/pypi/pyversions/databricks-dbapi.svg\n    :target: https://pypi.python.org/pypi/databricks-dbapi\n\nA thin wrapper around `pyhive <https://github.com/dropbox/PyHive>`_ for creating a `DBAPI <https://www.python.org/dev/peps/pep-0249/>`_ connection to an interactive Databricks cluster.\n\nAlso provides a SQLAlchemy Dialect for Databricks interactive clusters.\n\nInstallation\n------------\n\nInstall using pip:\n\n.. code-block:: bash\n\n    pip install databricks-dbapi\n\n\nFor SQLAlchemy support install with:\n\n.. code-block:: bash\n\n    pip install databricks-dbapi[sqlalchemy]\n\nUsage\n-----\n\nThe ``connect()`` function returns a ``pyhive`` Hive connection object, which internally wraps a ``thrift`` connection.\n\nUsing a Databricks API token (recommended):\n\n.. code-block:: python\n\n    import os\n\n    from databricks_dbapi import databricks\n\n\n    token = os.environ[\"DATABRICKS_TOKEN\"]\n    host = os.environ[\"DATABRICKS_HOST\"]\n    cluster = os.environ[\"DATABRICKS_CLUSTER\"]\n\n\n    connection = databricks.connect(\n        host=host,\n        cluster=cluster,\n        token=token,\n    )\n    cursor = connection.cursor()\n\n    cursor.execute(\"SELECT * FROM some_table LIMIT 100\")\n\n    print(cursor.fetchone())\n    print(cursor.fetchall())\n\n\nUsing your username and password (not recommended):\n\n.. code-block:: python\n\n    import os\n\n    from databricks_dbapi import databricks\n\n\n    user = os.environ[\"DATABRICKS_USER\"]\n    password = os.environ[\"DATABRICKS_PASSWORD\"]\n    host = os.environ[\"DATABRICKS_HOST\"]\n    cluster = os.environ[\"DATABRICKS_CLUSTER\"]\n\n\n    connection = databricks.connect(\n        host=host,\n        cluster=cluster,\n        user=user,\n        password=password\n    )\n    cursor = connection.cursor()\n\n    cursor.execute(\"SELECT * FROM some_table LIMIT 100\")\n\n    print(cursor.fetchone())\n    print(cursor.fetchall())\n\n\nConnecting on Azure platform, or with ``http_path``:\n\n.. code-block:: python\n\n    import os\n\n    from databricks_dbapi import databricks\n\n\n    token = os.environ[\"DATABRICKS_TOKEN\"]\n    host = os.environ[\"DATABRICKS_HOST\"]\n    http_path = os.environ[\"DATABRICKS_HTTP_PATH\"]\n\n\n    connection = databricks.connect(\n        host=host,\n        http_path=http_path,\n        token=token,\n    )\n    cursor = connection.cursor()\n\n    cursor.execute(\"SELECT * FROM some_table LIMIT 100\")\n\n    print(cursor.fetchone())\n    print(cursor.fetchall())\n\n\nThe ``pyhive`` connection also provides async functionality:\n\n.. code-block:: python\n\n    import os\n\n    from databricks_dbapi import databricks\n    from TCLIService.ttypes import TOperationState\n\n\n    token = os.environ[\"DATABRICKS_TOKEN\"]\n    host = os.environ[\"DATABRICKS_HOST\"]\n    cluster = os.environ[\"DATABRICKS_CLUSTER\"]\n\n\n    connection = databricks.connect(\n        host=host,\n        cluster=cluster,\n        token=token,\n    )\n    cursor = connection.cursor()\n\n    cursor.execute(\"SELECT * FROM some_table LIMIT 100\", async_=True)\n\n    status = cursor.poll().operationState\n    while status in (TOperationState.INITIALIZED_STATE, TOperationState.RUNNING_STATE):\n        logs = cursor.fetch_logs()\n        for message in logs:\n            print(message)\n\n        # If needed, an asynchronous query can be cancelled at any time with:\n        # cursor.cancel()\n\n        status = cursor.poll().operationState\n\n    print(cursor.fetchall())\n\n\n\nSQLAlchemy\n----------\n\nOnce the ``databricks-dbapi`` package is installed, the ``databricks+pyhive`` dialect/driver will be registered to SQLAlchemy. Fill in the required information when passing the engine URL.\n\n.. code-block:: python\n\n    from sqlalchemy import *\n    from sqlalchemy.engine import create_engine\n    from sqlalchemy.schema import *\n\n\n    # Standard Databricks with user + password\n    # provide user, password, company name for url, database name, cluster name\n    engine = create_engine(\n        \"databricks+pyhive://<user>:<password>@<companyname>.cloud.databricks.com:443/<database>\",\n        connect_args={\"cluster\": \"<cluster>\"}\n    )\n\n    # Standard Databricks with token\n    # provide token, company name for url, database name, cluster name\n    engine = create_engine(\n        \"databricks+pyhive://token:<databricks_token>@<companyname>.cloud.databricks.com:443/<database>\",\n        connect_args={\"cluster\": \"<cluster>\"}\n    )\n\n    # Azure Databricks with user + password\n    # provide user, password, region for url, database name, http_path (with cluster name)\n    engine = create_engine(\n        \"databricks+pyhive://<user>:<password>@<region>.azuredatabricks.net:443/<database>\",\n        connect_args={\"http_path\": \"<azure_databricks_http_path>\"}\n    )\n\n    # Azure Databricks with token\n    # provide token, region for url, database name, http_path (with cluster name)\n    engine = create_engine(\n        \"databricks+pyhive://token:<databrickstoken>@<region>.azuredatabricks.net:443/<database>\",\n        connect_args={\"http_path\": \"<azure_databricks_http_path>\"}\n    )\n\n\n    logs = Table(\"my_table\", MetaData(bind=engine), autoload=True)\n    print(select([func.count(\"*\")], from_obj=logs).scalar())\n\n\nRefer to the following documentation for more details on hostname, cluster name, and http path:\n\n* `Databricks <https://docs.databricks.com/user-guide/bi/jdbc-odbc-bi.html>`_\n* `Azure Databricks <https://docs.azuredatabricks.net/user-guide/bi/jdbc-odbc-bi.html>`_\n\n\nRelated\n-------\n\n* `pyhive <https://github.com/dropbox/PyHive>`_\n* `thrift <https://github.com/apache/thrift/tree/master/lib/py>`_\n\n", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/crflynn/databricks-dbapi", "keywords": "databricks,hive,dbapi,sqlalchemy,dialect", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "databricks-dbapi", "package_url": "https://pypi.org/project/databricks-dbapi/", "platform": "", "project_url": "https://pypi.org/project/databricks-dbapi/", "project_urls": {"Documentation": "https://github.com/crflynn/databricks-dbapi", "Homepage": "https://github.com/crflynn/databricks-dbapi", "Repository": "https://github.com/crflynn/databricks-dbapi"}, "release_url": "https://pypi.org/project/databricks-dbapi/0.3.0/", "requires_dist": ["pyhive[hive] (>=0.6.1,<0.7.0)", "sqlalchemy (>=1.3,<2.0); extra == \"sqlalchemy\""], "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*", "summary": "A DBAPI 2.0 interface and SQLAlchemy dialect for Databricks interactive clusters.", "version": "0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://pypi.python.org/pypi/databricks-dbapi\" rel=\"nofollow\"><img alt=\"pypi\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/df2501ffe54dea9f31da4b9fb2720e29ca6a60a4/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f64617461627269636b732d64626170692e737667\"></a> <a href=\"https://pypi.python.org/pypi/databricks-dbapi\" rel=\"nofollow\"><img alt=\"pyversions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/29a78e3f065d8a6df9e2cc99adad0ad4c4414d02/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f64617461627269636b732d64626170692e737667\"></a></p>\n<p>A thin wrapper around <a href=\"https://github.com/dropbox/PyHive\" rel=\"nofollow\">pyhive</a> for creating a <a href=\"https://www.python.org/dev/peps/pep-0249/\" rel=\"nofollow\">DBAPI</a> connection to an interactive Databricks cluster.</p>\n<p>Also provides a SQLAlchemy Dialect for Databricks interactive clusters.</p>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>Install using pip:</p>\n<pre>pip install databricks-dbapi\n</pre>\n<p>For SQLAlchemy support install with:</p>\n<pre>pip install databricks-dbapi<span class=\"o\">[</span>sqlalchemy<span class=\"o\">]</span>\n</pre>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>The <tt>connect()</tt> function returns a <tt>pyhive</tt> Hive connection object, which internally wraps a <tt>thrift</tt> connection.</p>\n<p>Using a Databricks API token (recommended):</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">databricks_dbapi</span> <span class=\"kn\">import</span> <span class=\"n\">databricks</span>\n\n\n<span class=\"n\">token</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"DATABRICKS_TOKEN\"</span><span class=\"p\">]</span>\n<span class=\"n\">host</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"DATABRICKS_HOST\"</span><span class=\"p\">]</span>\n<span class=\"n\">cluster</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"DATABRICKS_CLUSTER\"</span><span class=\"p\">]</span>\n\n\n<span class=\"n\">connection</span> <span class=\"o\">=</span> <span class=\"n\">databricks</span><span class=\"o\">.</span><span class=\"n\">connect</span><span class=\"p\">(</span>\n    <span class=\"n\">host</span><span class=\"o\">=</span><span class=\"n\">host</span><span class=\"p\">,</span>\n    <span class=\"n\">cluster</span><span class=\"o\">=</span><span class=\"n\">cluster</span><span class=\"p\">,</span>\n    <span class=\"n\">token</span><span class=\"o\">=</span><span class=\"n\">token</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">cursor</span> <span class=\"o\">=</span> <span class=\"n\">connection</span><span class=\"o\">.</span><span class=\"n\">cursor</span><span class=\"p\">()</span>\n\n<span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s2\">\"SELECT * FROM some_table LIMIT 100\"</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">fetchone</span><span class=\"p\">())</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">fetchall</span><span class=\"p\">())</span>\n</pre>\n<p>Using your username and password (not recommended):</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">databricks_dbapi</span> <span class=\"kn\">import</span> <span class=\"n\">databricks</span>\n\n\n<span class=\"n\">user</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"DATABRICKS_USER\"</span><span class=\"p\">]</span>\n<span class=\"n\">password</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"DATABRICKS_PASSWORD\"</span><span class=\"p\">]</span>\n<span class=\"n\">host</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"DATABRICKS_HOST\"</span><span class=\"p\">]</span>\n<span class=\"n\">cluster</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"DATABRICKS_CLUSTER\"</span><span class=\"p\">]</span>\n\n\n<span class=\"n\">connection</span> <span class=\"o\">=</span> <span class=\"n\">databricks</span><span class=\"o\">.</span><span class=\"n\">connect</span><span class=\"p\">(</span>\n    <span class=\"n\">host</span><span class=\"o\">=</span><span class=\"n\">host</span><span class=\"p\">,</span>\n    <span class=\"n\">cluster</span><span class=\"o\">=</span><span class=\"n\">cluster</span><span class=\"p\">,</span>\n    <span class=\"n\">user</span><span class=\"o\">=</span><span class=\"n\">user</span><span class=\"p\">,</span>\n    <span class=\"n\">password</span><span class=\"o\">=</span><span class=\"n\">password</span>\n<span class=\"p\">)</span>\n<span class=\"n\">cursor</span> <span class=\"o\">=</span> <span class=\"n\">connection</span><span class=\"o\">.</span><span class=\"n\">cursor</span><span class=\"p\">()</span>\n\n<span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s2\">\"SELECT * FROM some_table LIMIT 100\"</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">fetchone</span><span class=\"p\">())</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">fetchall</span><span class=\"p\">())</span>\n</pre>\n<p>Connecting on Azure platform, or with <tt>http_path</tt>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">databricks_dbapi</span> <span class=\"kn\">import</span> <span class=\"n\">databricks</span>\n\n\n<span class=\"n\">token</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"DATABRICKS_TOKEN\"</span><span class=\"p\">]</span>\n<span class=\"n\">host</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"DATABRICKS_HOST\"</span><span class=\"p\">]</span>\n<span class=\"n\">http_path</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"DATABRICKS_HTTP_PATH\"</span><span class=\"p\">]</span>\n\n\n<span class=\"n\">connection</span> <span class=\"o\">=</span> <span class=\"n\">databricks</span><span class=\"o\">.</span><span class=\"n\">connect</span><span class=\"p\">(</span>\n    <span class=\"n\">host</span><span class=\"o\">=</span><span class=\"n\">host</span><span class=\"p\">,</span>\n    <span class=\"n\">http_path</span><span class=\"o\">=</span><span class=\"n\">http_path</span><span class=\"p\">,</span>\n    <span class=\"n\">token</span><span class=\"o\">=</span><span class=\"n\">token</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">cursor</span> <span class=\"o\">=</span> <span class=\"n\">connection</span><span class=\"o\">.</span><span class=\"n\">cursor</span><span class=\"p\">()</span>\n\n<span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s2\">\"SELECT * FROM some_table LIMIT 100\"</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">fetchone</span><span class=\"p\">())</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">fetchall</span><span class=\"p\">())</span>\n</pre>\n<p>The <tt>pyhive</tt> connection also provides async functionality:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">databricks_dbapi</span> <span class=\"kn\">import</span> <span class=\"n\">databricks</span>\n<span class=\"kn\">from</span> <span class=\"nn\">TCLIService.ttypes</span> <span class=\"kn\">import</span> <span class=\"n\">TOperationState</span>\n\n\n<span class=\"n\">token</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"DATABRICKS_TOKEN\"</span><span class=\"p\">]</span>\n<span class=\"n\">host</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"DATABRICKS_HOST\"</span><span class=\"p\">]</span>\n<span class=\"n\">cluster</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"DATABRICKS_CLUSTER\"</span><span class=\"p\">]</span>\n\n\n<span class=\"n\">connection</span> <span class=\"o\">=</span> <span class=\"n\">databricks</span><span class=\"o\">.</span><span class=\"n\">connect</span><span class=\"p\">(</span>\n    <span class=\"n\">host</span><span class=\"o\">=</span><span class=\"n\">host</span><span class=\"p\">,</span>\n    <span class=\"n\">cluster</span><span class=\"o\">=</span><span class=\"n\">cluster</span><span class=\"p\">,</span>\n    <span class=\"n\">token</span><span class=\"o\">=</span><span class=\"n\">token</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">cursor</span> <span class=\"o\">=</span> <span class=\"n\">connection</span><span class=\"o\">.</span><span class=\"n\">cursor</span><span class=\"p\">()</span>\n\n<span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s2\">\"SELECT * FROM some_table LIMIT 100\"</span><span class=\"p\">,</span> <span class=\"n\">async_</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"n\">status</span> <span class=\"o\">=</span> <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">poll</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">operationState</span>\n<span class=\"k\">while</span> <span class=\"n\">status</span> <span class=\"ow\">in</span> <span class=\"p\">(</span><span class=\"n\">TOperationState</span><span class=\"o\">.</span><span class=\"n\">INITIALIZED_STATE</span><span class=\"p\">,</span> <span class=\"n\">TOperationState</span><span class=\"o\">.</span><span class=\"n\">RUNNING_STATE</span><span class=\"p\">):</span>\n    <span class=\"n\">logs</span> <span class=\"o\">=</span> <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">fetch_logs</span><span class=\"p\">()</span>\n    <span class=\"k\">for</span> <span class=\"n\">message</span> <span class=\"ow\">in</span> <span class=\"n\">logs</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">message</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># If needed, an asynchronous query can be cancelled at any time with:</span>\n    <span class=\"c1\"># cursor.cancel()</span>\n\n    <span class=\"n\">status</span> <span class=\"o\">=</span> <span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">poll</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">operationState</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">cursor</span><span class=\"o\">.</span><span class=\"n\">fetchall</span><span class=\"p\">())</span>\n</pre>\n</div>\n<div id=\"sqlalchemy\">\n<h2>SQLAlchemy</h2>\n<p>Once the <tt><span class=\"pre\">databricks-dbapi</span></tt> package is installed, the <tt>databricks+pyhive</tt> dialect/driver will be registered to SQLAlchemy. Fill in the required information when passing the engine URL.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sqlalchemy</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sqlalchemy.engine</span> <span class=\"kn\">import</span> <span class=\"n\">create_engine</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sqlalchemy.schema</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n\n\n<span class=\"c1\"># Standard Databricks with user + password</span>\n<span class=\"c1\"># provide user, password, company name for url, database name, cluster name</span>\n<span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">create_engine</span><span class=\"p\">(</span>\n    <span class=\"s2\">\"databricks+pyhive://&lt;user&gt;:&lt;password&gt;@&lt;companyname&gt;.cloud.databricks.com:443/&lt;database&gt;\"</span><span class=\"p\">,</span>\n    <span class=\"n\">connect_args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"cluster\"</span><span class=\"p\">:</span> <span class=\"s2\">\"&lt;cluster&gt;\"</span><span class=\"p\">}</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Standard Databricks with token</span>\n<span class=\"c1\"># provide token, company name for url, database name, cluster name</span>\n<span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">create_engine</span><span class=\"p\">(</span>\n    <span class=\"s2\">\"databricks+pyhive://token:&lt;databricks_token&gt;@&lt;companyname&gt;.cloud.databricks.com:443/&lt;database&gt;\"</span><span class=\"p\">,</span>\n    <span class=\"n\">connect_args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"cluster\"</span><span class=\"p\">:</span> <span class=\"s2\">\"&lt;cluster&gt;\"</span><span class=\"p\">}</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Azure Databricks with user + password</span>\n<span class=\"c1\"># provide user, password, region for url, database name, http_path (with cluster name)</span>\n<span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">create_engine</span><span class=\"p\">(</span>\n    <span class=\"s2\">\"databricks+pyhive://&lt;user&gt;:&lt;password&gt;@&lt;region&gt;.azuredatabricks.net:443/&lt;database&gt;\"</span><span class=\"p\">,</span>\n    <span class=\"n\">connect_args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"http_path\"</span><span class=\"p\">:</span> <span class=\"s2\">\"&lt;azure_databricks_http_path&gt;\"</span><span class=\"p\">}</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Azure Databricks with token</span>\n<span class=\"c1\"># provide token, region for url, database name, http_path (with cluster name)</span>\n<span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">create_engine</span><span class=\"p\">(</span>\n    <span class=\"s2\">\"databricks+pyhive://token:&lt;databrickstoken&gt;@&lt;region&gt;.azuredatabricks.net:443/&lt;database&gt;\"</span><span class=\"p\">,</span>\n    <span class=\"n\">connect_args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"http_path\"</span><span class=\"p\">:</span> <span class=\"s2\">\"&lt;azure_databricks_http_path&gt;\"</span><span class=\"p\">}</span>\n<span class=\"p\">)</span>\n\n\n<span class=\"n\">logs</span> <span class=\"o\">=</span> <span class=\"n\">Table</span><span class=\"p\">(</span><span class=\"s2\">\"my_table\"</span><span class=\"p\">,</span> <span class=\"n\">MetaData</span><span class=\"p\">(</span><span class=\"n\">bind</span><span class=\"o\">=</span><span class=\"n\">engine</span><span class=\"p\">),</span> <span class=\"n\">autoload</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">select</span><span class=\"p\">([</span><span class=\"n\">func</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"p\">(</span><span class=\"s2\">\"*\"</span><span class=\"p\">)],</span> <span class=\"n\">from_obj</span><span class=\"o\">=</span><span class=\"n\">logs</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">scalar</span><span class=\"p\">())</span>\n</pre>\n<p>Refer to the following documentation for more details on hostname, cluster name, and http path:</p>\n<ul>\n<li><a href=\"https://docs.databricks.com/user-guide/bi/jdbc-odbc-bi.html\" rel=\"nofollow\">Databricks</a></li>\n<li><a href=\"https://docs.azuredatabricks.net/user-guide/bi/jdbc-odbc-bi.html\" rel=\"nofollow\">Azure Databricks</a></li>\n</ul>\n</div>\n<div id=\"related\">\n<h2>Related</h2>\n<ul>\n<li><a href=\"https://github.com/dropbox/PyHive\" rel=\"nofollow\">pyhive</a></li>\n<li><a href=\"https://github.com/apache/thrift/tree/master/lib/py\" rel=\"nofollow\">thrift</a></li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 5674807, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "70a8b72f48e4de74359e3408784fb1d2", "sha256": "0539bd1613b4e988d80a1107fe41f2ef82092ea9588cf7b0337e7a9c293eaea9"}, "downloads": -1, "filename": "databricks_dbapi-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "70a8b72f48e4de74359e3408784fb1d2", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 3260, "upload_time": "2018-09-11T01:45:44", "upload_time_iso_8601": "2018-09-11T01:45:44.393108Z", "url": "https://files.pythonhosted.org/packages/59/5b/9184078aa1cc3eeb59178667493399acb0b5cefcbae256ccbd85b81dd6bb/databricks_dbapi-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "17c4d0bfafbfd23eba6862f778b7428f", "sha256": "fe0be07c78b23793cecac9cfc321686bbb78c27433479d7b0b37ab8510b4c109"}, "downloads": -1, "filename": "databricks_dbapi-0.1.0.tar.gz", "has_sig": false, "md5_digest": "17c4d0bfafbfd23eba6862f778b7428f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3682, "upload_time": "2018-09-11T01:45:45", "upload_time_iso_8601": "2018-09-11T01:45:45.985536Z", "url": "https://files.pythonhosted.org/packages/26/14/a2bd4e1a6d4a78d89c8bc893967f5495265244b24c228bf1c4960f8352c3/databricks_dbapi-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "8c1d8d08bf865998da4c0dbc9a118559", "sha256": "e8cd6d4f265dc1934bf19caec35c8f8361512de5edb2abe625603f8b58790166"}, "downloads": -1, "filename": "databricks_dbapi-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "8c1d8d08bf865998da4c0dbc9a118559", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 3291, "upload_time": "2018-10-11T01:29:35", "upload_time_iso_8601": "2018-10-11T01:29:35.203235Z", "url": "https://files.pythonhosted.org/packages/4c/32/2a7e255371509a4aea6ababb3f95b725b47716fea2df1e2963dba0b72845/databricks_dbapi-0.1.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0f9387b97115998e003c50829ba3273e", "sha256": "20fbe2ed1eb95179de8543273dba596c58cd5505c2494993b0df6b5ad6b83688"}, "downloads": -1, "filename": "databricks_dbapi-0.1.1.tar.gz", "has_sig": false, "md5_digest": "0f9387b97115998e003c50829ba3273e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3710, "upload_time": "2018-10-11T01:29:36", "upload_time_iso_8601": "2018-10-11T01:29:36.665582Z", "url": "https://files.pythonhosted.org/packages/74/56/e345f69133c40235f081c7d89f54bece617e740d5ea7ca5bfc1ce366aea0/databricks_dbapi-0.1.1.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "c618a052cdbb074204256f5fc2a8cc21", "sha256": "59ab8930f308cf13e7f5645b7deeb432f8ed2cd374127a86497defc2c49714f5"}, "downloads": -1, "filename": "databricks_dbapi-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c618a052cdbb074204256f5fc2a8cc21", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 3767, "upload_time": "2018-12-13T23:16:15", "upload_time_iso_8601": "2018-12-13T23:16:15.419079Z", "url": "https://files.pythonhosted.org/packages/75/66/38cccdc14866d22ad4d25207e85d99feb17e20cbe500ba1245d5f1c0ba6d/databricks_dbapi-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bba23dc0007c1d6d65902196e16dce3c", "sha256": "f2ac2e9c8765a6d13200264016c5511fb542308bc15bf2cc9d6d8f58cb7b7f1f"}, "downloads": -1, "filename": "databricks_dbapi-0.2.0.tar.gz", "has_sig": false, "md5_digest": "bba23dc0007c1d6d65902196e16dce3c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4208, "upload_time": "2018-12-13T23:16:17", "upload_time_iso_8601": "2018-12-13T23:16:17.108981Z", "url": "https://files.pythonhosted.org/packages/07/7f/4aadd858938962ff02b2109d3a52f631afca5beb011333a9a8c5ee6c2400/databricks_dbapi-0.2.0.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "5091afe383a7e7428b1650413b4af115", "sha256": "1a6567fe5235a233ec05024fc537a557ea294c4ceee8d058497a06a4e2a07f4d"}, "downloads": -1, "filename": "databricks_dbapi-0.3.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "5091afe383a7e7428b1650413b4af115", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*", "size": 6915, "upload_time": "2019-08-14T05:05:17", "upload_time_iso_8601": "2019-08-14T05:05:17.432248Z", "url": "https://files.pythonhosted.org/packages/32/38/4d4271b3bd8cf704468dd728fdbae0e4114494d41a9a60ec9a6bfbe38efb/databricks_dbapi-0.3.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bc131363ac50d83b62c880a4478c0a8f", "sha256": "3c7e529cdeca4004962342d339119e631df1a934f4e7d7da168370f13b1a48c4"}, "downloads": -1, "filename": "databricks_dbapi-0.3.0.tar.gz", "has_sig": false, "md5_digest": "bc131363ac50d83b62c880a4478c0a8f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*", "size": 6157, "upload_time": "2019-08-14T05:05:19", "upload_time_iso_8601": "2019-08-14T05:05:19.344253Z", "url": "https://files.pythonhosted.org/packages/6f/b1/b96f723083f02d201ee680d92fb38877ffa928b7733b5906bedf19e44bd4/databricks_dbapi-0.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "5091afe383a7e7428b1650413b4af115", "sha256": "1a6567fe5235a233ec05024fc537a557ea294c4ceee8d058497a06a4e2a07f4d"}, "downloads": -1, "filename": "databricks_dbapi-0.3.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "5091afe383a7e7428b1650413b4af115", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*", "size": 6915, "upload_time": "2019-08-14T05:05:17", "upload_time_iso_8601": "2019-08-14T05:05:17.432248Z", "url": "https://files.pythonhosted.org/packages/32/38/4d4271b3bd8cf704468dd728fdbae0e4114494d41a9a60ec9a6bfbe38efb/databricks_dbapi-0.3.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bc131363ac50d83b62c880a4478c0a8f", "sha256": "3c7e529cdeca4004962342d339119e631df1a934f4e7d7da168370f13b1a48c4"}, "downloads": -1, "filename": "databricks_dbapi-0.3.0.tar.gz", "has_sig": false, "md5_digest": "bc131363ac50d83b62c880a4478c0a8f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*", "size": 6157, "upload_time": "2019-08-14T05:05:19", "upload_time_iso_8601": "2019-08-14T05:05:19.344253Z", "url": "https://files.pythonhosted.org/packages/6f/b1/b96f723083f02d201ee680d92fb38877ffa928b7733b5906bedf19e44bd4/databricks_dbapi-0.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:40:30 2020"}