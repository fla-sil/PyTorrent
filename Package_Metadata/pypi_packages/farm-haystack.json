{"info": {"author": "Malte Pietsch, Timo Moeller, Branden Chan, Tanay Soni", "author_email": "malte.pietsch@deepset.ai", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "*******************************************************\nHaystack \u2014 Neural Question Answering At Scale\n*******************************************************\n.. image:: https://travis-ci.org/deepset-ai/haystack.svg?branch=master\n\t:target: https://travis-ci.org/deepset-ai/haystack\n\t:alt: Build\n\n.. image:: https://img.shields.io/github/release/deepset-ai/haystack\n\t:target: https://github.com/deepset-ai/haystack/releases\n\t:alt: Release\n\n.. image:: https://img.shields.io/github/license/deepset-ai/haystack\n\t:target: https://github.com/deepset-ai/haystack/blob/master/LICENSE\n\t:alt: License\n\n.. image:: https://img.shields.io/github/last-commit/deepset-ai/haystack\n\t:target: https://github.com/deepset-ai/haystack/commits/master\n\t:alt: Last Commit\n\n\nIntroduction\n============\n\nThe performance of **modern Question Answering Models** (BERT, ALBERT ...) has seen drastic improvements within the last year enabling many new opportunities for accessing information more efficiently. However, those models are designed to find answers within rather small text passages. **Haystack lets you scale QA models** to large collections of documents!\nWhile QA is the focussed use case for haystack, we will soon support additional options to boost search (re-ranking, most-similar search ...).\n\nHaystack is designed in a modular way and lets you use any models trained with  `FARM <https://github.com/deepset-ai/FARM>`_ or `Transformers <https://github.com/huggingface/transformers>`_.\n\n\n\nCore Features\n=============\n- **Powerful ML models**: Utilize all latest transformer based models (BERT, ALBERT, RoBERTa ...)\n- **Modular & future-proof**: Easily switch to newer models once they get published.\n- **Developer friendly**: Easy to debug, extend and modify.\n- **Scalable**: Production-ready deployments via Elasticsearch backend & REST API\n- **Customizable**: Fine-tune models to your own domain & improve them continuously via user feedback\n\n\nComponents\n==========\n\n1. **DocumentStore**: Database storing the documents for our search. We recommend Elasticsearch, but have also more light-weight options for fast prototyping (SQL or In-Memory).\n\n2. **Retriever**:  Fast, simple algorithm that identifies candidate passages from a large collection of documents. Algorithms include TF-IDF or BM25, custom Elasticsearch queries, and embedding-based approaches. The Retriever helps to narrow down the scope for Reader to smaller units of text where a given question could be answered.\n\n3. **Reader**: Powerful neural model that reads through texts in detail to find an answer. Use diverse models like BERT, RoBERTa or XLNet trained via `FARM <https://github.com/deepset-ai/FARM>`_ or `Transformers <https://github.com/huggingface/transformers>`_ on SQuAD like tasks. The Reader takes multiple passages of text as input and returns top-n answers with corresponding confidence scores. You can just load a pretrained model from  `Hugging Face's model hub <https://huggingface.co/models>`_ or fine-tune it to your own domain data.\n\n4. **Finder**: Glues together a Reader and a Retriever as a pipeline to provide an easy-to-use question answering interface.\n\n5. **REST API**: Exposes a simple API for running QA search, collecting feedback and monitoring requests\n\n6. **Labeling Tool**: `Hosted version <https://annotate.deepset.ai/login>`_  (Beta), Docker images (coming soon)\n\n\nResources\n=========\n- Tutorial 1  - Basic QA Pipeline: `Jupyter notebook  <https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb>`__  or `Colab <https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb>`__\n- Tutorial 2  - Fine-tuning a model on own data: `Jupyter notebook <https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb>`__ or `Colab <https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb>`__\n- Tutorial 3  - Basic QA Pipeline without Elasticsearch: `Jupyter notebook <https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb>`__ or `Colab <https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb>`__\n\nQuick Start\n===========\n\nInstallation\n------------\n\nRecommended (because of active development)::\n\n    git clone https://github.com/deepset-ai/haystack.git\n    cd haystack\n    pip install --editable .\n\nTo update your installation, just do a git pull. The --editable flag will update changes immediately.\n\nFrom PyPi::\n\n    pip install farm-haystack\n\nUsage\n-----\n.. image:: https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/img/code_snippet_usage.png\n\n\nQuick Tour\n==========\n\n\n1) DocumentStores\n---------------------\n\nHaystack has an extensible DocumentStore-Layer, which is storing the documents for our search. We recommend Elasticsearch, but have also more light-weight options for fast prototyping.\n\nElasticsearch (Recommended)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n:code:`haystack.database.elasticsearch.ElasticsearchDocumentStore`\n\n* Keeps all the logic to store and query documents from Elastic, incl. mapping of fields, adding filters or boosts to your queries, and storing embeddings\n* You can either use an existing Elasticsearch index or create a new one via haystack\n* Retrievers operate on top of this DocumentStore to find the relevant documents for a query\n* Documents can optionally be chunked into smaller units (e.g. paragraphs) before indexing to make the results returned by the Retriever more granular and accurate.\n\nYou can get started by running a single Elasticsearch node using docker::\n\n     docker run -d -p 9200:9200 -e \"discovery.type=single-node\" elasticsearch:7.6.1\n\nOr if docker is not possible for you::\n\n     wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n     tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n     chown -R daemon:daemon elasticsearch-7.6.2\n     elasticsearch-7.0.0/bin/elasticsearch\n\nSee Tutorial 1 on how to go on with indexing your docs.\n\n\nSQL / InMemory (Alternative)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n:code:`haystack.database.sql.SQLDocumentStore` & :code:`haystack.database.memory.InMemoryDocumentStore`\n\nThese DocumentStores are mainly intended to simplify the first development steps or test a prototype on an existing SQL Database containing your texts. The SQLDocumentStore initializes by default a local file-based SQLite database.\nHowever, you can easily configure it for PostgreSQL or MySQL since our implementation is based on SQLAlchemy.\nLimitations: Retrieval (e.g. via TfidfRetriever) happens in-memory here and will therefore only work efficiently on small datasets\n\n2) Retrievers\n---------------------\nElasticsearchRetriever\n^^^^^^^^^^^^^^^^^^^^^^\nScoring text similarity via sparse Bag-of-words representations are strong and well-established baselines in Information Retrieval.\nThe default `ElasticsearchRetriever` uses Elasticsearch's native scoring (BM25), but can be extended easily with custom queries or filtering.\n\nExample::\n\n    retriever = ElasticsearchRetriever(document_store=document_store, custom_query=None)\n    retriever.retrieve(query=\"Why did the revenue increase?\", filters={\"years\": [\"2019\"], \"company\": [\"Q1\", \"Q2\"]})\n    # returns: [Document, Document]\n\nEmbeddingRetriever\n^^^^^^^^^^^^^^^^^^^^^^\nUsing dense embeddings (i.e. vector representations) of texts is a powerful alternative to score similarity of texts.\nThis retriever allows you to transform your query into an embedding using a model (e.g. Sentence-BERT) and find similar texts by using cosine similarity.\n\nExample::\n\n    retriever = EmbeddingRetriever(document_store=document_store,\n                                   embedding_model=\"deepset/sentence-bert\",\n                                   model_format=\"farm\")\n    retriever.retrieve(query=\"Why did the revenue increase?\", filters={\"years\": [\"2019\"], \"company\": [\"Q1\", \"Q2\"]})\n    # returns: [Document, Document]\n\nWe are working on extending this category of retrievers a lot as there's a lot of exciting work in research indicating substantial performance improvements (e.g. `DPR <https://arxiv.org/abs/2004.04906>`_ , `REALM <https://arxiv.org/abs/2002.08909>`_  )\n\nTfidfRetriever\n^^^^^^^^^^^^^^^^^^^^^^\nBasic in-memory retriever getting texts from the DocumentStore, creating TF-IDF representations in-memory and allowing to query them.\n\n3) Readers\n---------------------\nNeural networks (i.e. mostly Transformer-based) that read through texts in detail to find an answer. Use diverse models like BERT, RoBERTa or XLNet trained via `FARM <https://github.com/deepset-ai/FARM>`_ or  on SQuAD like tasks. The Reader takes multiple passages of text as input and returns top-n answers with corresponding confidence scores.\nBoth readers can load either a local model or any public model from  `Hugging Face's model hub <https://huggingface.co/models>`_\n\nFARMReader\n^^^^^^^^^^\nImplementing various QA models via the `FARM <https://github.com/deepset-ai/FARM>`_ Framework.\nExample::\n\n    reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\",\n                    use_gpu=False, no_ans_boost=-10, context_window_size=500,\n                    top_k_per_candidate=3, top_k_per_sample=1,\n                    num_processes=8, max_seq_len=256, doc_stride=128)\n\n    # Optional: Training & eval\n    reader.train(...)\n    reader.eval(...)\n\n    # Predict\n    reader.predict(question=\"Who is the father of Arya Starck?\", documents=documents, top_k=3)\n\nThis Reader comes with:\n* quite many configuration options\n* using multiple processes for preprocessing\n* option to train\n* option to evaluate\n\nTransformersReader\n^^^^^^^^^^^^^^^^^^\nImplementing various QA models via the :code:`pipeline` class of `Transformers <https://github.com/huggingface/transformers>`_ Framework.\n\nExample::\n\n    reader = TransformersReader(model=\"distilbert-base-uncased-distilled-squad\",\n                                tokenizer=\"distilbert-base-uncased\",\n                                context_window_size=500,\n                                use_gpu=-1)\n\n    reader.predict(question=\"Who is the father of Arya Starck?\", documents=documents, top_k=3)\n\n\n5. REST API\n---------------------\nA simple REST API based on `FastAPI <https://fastapi.tiangolo.com/>`_ is provided to:\n\n*  search answers in texts (`extractive QA  <https://github.com/deepset-ai/haystack/blob/master/haystack/api/controller/search.py>`_)\n*  search answers by comparing user question to existing questions (`FAQ-style QA  <https://github.com/deepset-ai/haystack/blob/master/haystack/api/controller/search.py>`_)\n*  collect & export user feedback on answers to gain domain-specific training data (`feedback  <https://github.com/deepset-ai/haystack/blob/master/haystack/api/controller/feedback.py>`_)\n*  allow basic monitoring of requests (currently via APM in Kibana)\n\nTo serve the API, run::\n\n    gunicorn haystack.api.application:app -b 0.0.0.0:80 -k uvicorn.workers.UvicornWorker`\n\nYou will find the Swagger API documentation at http://127.0.0.1:80/docs\n\n\n6. Labeling Tool\n---------------------\n* Use the `hosted version <https://annotate.deepset.ai/login>`_  (Beta) or deploy it yourself via Docker images (coming soon)  \n* Create labels with different techniques: Come up with questions (+ answers) while reading passages (SQuAD style) or have a set of predefined questions and look for answers in the document (~ Natural Questions).\n* Structure your work via organizations, projects, users \n* Upload your documents or import labels from an existing SQuAD-style dataset\n* Coming soon: more file formats for document upload, metrics for label quality ...\n\n.. image:: https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/img/annotation_tool.png", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "https://github.com/deepset-ai/haystack/archive/0.2.1.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/deepset-ai/haystack", "keywords": "QA Question-Answering Reader Retriever BERT roberta albert squad mrc transfer-learning language-model transformer", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "farm-haystack", "package_url": "https://pypi.org/project/farm-haystack/", "platform": "", "project_url": "https://pypi.org/project/farm-haystack/", "project_urls": {"Download": "https://github.com/deepset-ai/haystack/archive/0.2.1.tar.gz", "Homepage": "https://github.com/deepset-ai/haystack"}, "release_url": "https://pypi.org/project/farm-haystack/0.2.1/", "requires_dist": null, "requires_python": ">=3.6.0", "summary": "Neural Question Answering at Scale. Use modern transformer based models like BERT to find answers in large document collections", "version": "0.2.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://travis-ci.org/deepset-ai/haystack\" rel=\"nofollow\"><img alt=\"Build\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/096e32f33c9f697d5421c3ecaaa9703443564ff8/68747470733a2f2f7472617669732d63692e6f72672f646565707365742d61692f686179737461636b2e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://github.com/deepset-ai/haystack/releases\" rel=\"nofollow\"><img alt=\"Release\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b514fac86b3e0c21d9907571a7be46db997347c0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f646565707365742d61692f686179737461636b\"></a>\n<a href=\"https://github.com/deepset-ai/haystack/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a724db6fe821165744d9223dd8c0c8507656824a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f646565707365742d61692f686179737461636b\"></a>\n<a href=\"https://github.com/deepset-ai/haystack/commits/master\" rel=\"nofollow\"><img alt=\"Last Commit\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e2cf708ae90759b9164d4aa380f92283c44cc5dd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f646565707365742d61692f686179737461636b\"></a>\n<div id=\"introduction\">\n<h2>Introduction</h2>\n<p>The performance of <strong>modern Question Answering Models</strong> (BERT, ALBERT \u2026) has seen drastic improvements within the last year enabling many new opportunities for accessing information more efficiently. However, those models are designed to find answers within rather small text passages. <strong>Haystack lets you scale QA models</strong> to large collections of documents!\nWhile QA is the focussed use case for haystack, we will soon support additional options to boost search (re-ranking, most-similar search \u2026).</p>\n<p>Haystack is designed in a modular way and lets you use any models trained with  <a href=\"https://github.com/deepset-ai/FARM\" rel=\"nofollow\">FARM</a> or <a href=\"https://github.com/huggingface/transformers\" rel=\"nofollow\">Transformers</a>.</p>\n</div>\n<div id=\"core-features\">\n<h2>Core Features</h2>\n<ul>\n<li><strong>Powerful ML models</strong>: Utilize all latest transformer based models (BERT, ALBERT, RoBERTa \u2026)</li>\n<li><strong>Modular &amp; future-proof</strong>: Easily switch to newer models once they get published.</li>\n<li><strong>Developer friendly</strong>: Easy to debug, extend and modify.</li>\n<li><strong>Scalable</strong>: Production-ready deployments via Elasticsearch backend &amp; REST API</li>\n<li><strong>Customizable</strong>: Fine-tune models to your own domain &amp; improve them continuously via user feedback</li>\n</ul>\n</div>\n<div id=\"components\">\n<h2>Components</h2>\n<ol>\n<li><strong>DocumentStore</strong>: Database storing the documents for our search. We recommend Elasticsearch, but have also more light-weight options for fast prototyping (SQL or In-Memory).</li>\n<li><strong>Retriever</strong>:  Fast, simple algorithm that identifies candidate passages from a large collection of documents. Algorithms include TF-IDF or BM25, custom Elasticsearch queries, and embedding-based approaches. The Retriever helps to narrow down the scope for Reader to smaller units of text where a given question could be answered.</li>\n<li><strong>Reader</strong>: Powerful neural model that reads through texts in detail to find an answer. Use diverse models like BERT, RoBERTa or XLNet trained via <a href=\"https://github.com/deepset-ai/FARM\" rel=\"nofollow\">FARM</a> or <a href=\"https://github.com/huggingface/transformers\" rel=\"nofollow\">Transformers</a> on SQuAD like tasks. The Reader takes multiple passages of text as input and returns top-n answers with corresponding confidence scores. You can just load a pretrained model from  <a href=\"https://huggingface.co/models\" rel=\"nofollow\">Hugging Face\u2019s model hub</a> or fine-tune it to your own domain data.</li>\n<li><strong>Finder</strong>: Glues together a Reader and a Retriever as a pipeline to provide an easy-to-use question answering interface.</li>\n<li><strong>REST API</strong>: Exposes a simple API for running QA search, collecting feedback and monitoring requests</li>\n<li><strong>Labeling Tool</strong>: <a href=\"https://annotate.deepset.ai/login\" rel=\"nofollow\">Hosted version</a>  (Beta), Docker images (coming soon)</li>\n</ol>\n</div>\n<div id=\"resources\">\n<h2>Resources</h2>\n<ul>\n<li>Tutorial 1  - Basic QA Pipeline: <a href=\"https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb\" rel=\"nofollow\">Jupyter notebook</a>  or <a href=\"https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb\" rel=\"nofollow\">Colab</a></li>\n<li>Tutorial 2  - Fine-tuning a model on own data: <a href=\"https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb\" rel=\"nofollow\">Jupyter notebook</a> or <a href=\"https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb\" rel=\"nofollow\">Colab</a></li>\n<li>Tutorial 3  - Basic QA Pipeline without Elasticsearch: <a href=\"https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb\" rel=\"nofollow\">Jupyter notebook</a> or <a href=\"https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb\" rel=\"nofollow\">Colab</a></li>\n</ul>\n</div>\n<div id=\"quick-start\">\n<h2>Quick Start</h2>\n<div id=\"installation\">\n<h3>Installation</h3>\n<p>Recommended (because of active development):</p>\n<pre>git clone https://github.com/deepset-ai/haystack.git\ncd haystack\npip install --editable .\n</pre>\n<p>To update your installation, just do a git pull. The \u2013editable flag will update changes immediately.</p>\n<p>From PyPi:</p>\n<pre>pip install farm-haystack\n</pre>\n</div>\n<div id=\"usage\">\n<h3>Usage</h3>\n<img alt=\"https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/img/code_snippet_usage.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8b7d472b7defb25ea34c85eac391fdf1c825c04a/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f646565707365742d61692f686179737461636b2f6d61737465722f646f63732f696d672f636f64655f736e69707065745f75736167652e706e67\">\n</div>\n</div>\n<div id=\"quick-tour\">\n<h2>Quick Tour</h2>\n<div id=\"documentstores\">\n<h3>1) DocumentStores</h3>\n<p>Haystack has an extensible DocumentStore-Layer, which is storing the documents for our search. We recommend Elasticsearch, but have also more light-weight options for fast prototyping.</p>\n<div id=\"elasticsearch-recommended\">\n<h4>Elasticsearch (Recommended)</h4>\n<p><code>haystack.database.elasticsearch.ElasticsearchDocumentStore</code></p>\n<ul>\n<li>Keeps all the logic to store and query documents from Elastic, incl. mapping of fields, adding filters or boosts to your queries, and storing embeddings</li>\n<li>You can either use an existing Elasticsearch index or create a new one via haystack</li>\n<li>Retrievers operate on top of this DocumentStore to find the relevant documents for a query</li>\n<li>Documents can optionally be chunked into smaller units (e.g. paragraphs) before indexing to make the results returned by the Retriever more granular and accurate.</li>\n</ul>\n<p>You can get started by running a single Elasticsearch node using docker:</p>\n<pre>docker run -d -p 9200:9200 -e \"discovery.type=single-node\" elasticsearch:7.6.1\n</pre>\n<p>Or if docker is not possible for you:</p>\n<pre>wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\ntar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\nchown -R daemon:daemon elasticsearch-7.6.2\nelasticsearch-7.0.0/bin/elasticsearch\n</pre>\n<p>See Tutorial 1 on how to go on with indexing your docs.</p>\n</div>\n<div id=\"sql-inmemory-alternative\">\n<h4>SQL / InMemory (Alternative)</h4>\n<p><code>haystack.database.sql.SQLDocumentStore</code> &amp; <code>haystack.database.memory.InMemoryDocumentStore</code></p>\n<p>These DocumentStores are mainly intended to simplify the first development steps or test a prototype on an existing SQL Database containing your texts. The SQLDocumentStore initializes by default a local file-based SQLite database.\nHowever, you can easily configure it for PostgreSQL or MySQL since our implementation is based on SQLAlchemy.\nLimitations: Retrieval (e.g. via TfidfRetriever) happens in-memory here and will therefore only work efficiently on small datasets</p>\n</div>\n</div>\n<div id=\"retrievers\">\n<h3>2) Retrievers</h3>\n<div id=\"elasticsearchretriever\">\n<h4>ElasticsearchRetriever</h4>\n<p>Scoring text similarity via sparse Bag-of-words representations are strong and well-established baselines in Information Retrieval.\nThe default <cite>ElasticsearchRetriever</cite> uses Elasticsearch\u2019s native scoring (BM25), but can be extended easily with custom queries or filtering.</p>\n<p>Example:</p>\n<pre>retriever = ElasticsearchRetriever(document_store=document_store, custom_query=None)\nretriever.retrieve(query=\"Why did the revenue increase?\", filters={\"years\": [\"2019\"], \"company\": [\"Q1\", \"Q2\"]})\n# returns: [Document, Document]\n</pre>\n</div>\n<div id=\"embeddingretriever\">\n<h4>EmbeddingRetriever</h4>\n<p>Using dense embeddings (i.e. vector representations) of texts is a powerful alternative to score similarity of texts.\nThis retriever allows you to transform your query into an embedding using a model (e.g. Sentence-BERT) and find similar texts by using cosine similarity.</p>\n<p>Example:</p>\n<pre>retriever = EmbeddingRetriever(document_store=document_store,\n                               embedding_model=\"deepset/sentence-bert\",\n                               model_format=\"farm\")\nretriever.retrieve(query=\"Why did the revenue increase?\", filters={\"years\": [\"2019\"], \"company\": [\"Q1\", \"Q2\"]})\n# returns: [Document, Document]\n</pre>\n<p>We are working on extending this category of retrievers a lot as there\u2019s a lot of exciting work in research indicating substantial performance improvements (e.g. <a href=\"https://arxiv.org/abs/2004.04906\" rel=\"nofollow\">DPR</a> , <a href=\"https://arxiv.org/abs/2002.08909\" rel=\"nofollow\">REALM</a>  )</p>\n</div>\n<div id=\"tfidfretriever\">\n<h4>TfidfRetriever</h4>\n<p>Basic in-memory retriever getting texts from the DocumentStore, creating TF-IDF representations in-memory and allowing to query them.</p>\n</div>\n</div>\n<div id=\"readers\">\n<h3>3) Readers</h3>\n<p>Neural networks (i.e. mostly Transformer-based) that read through texts in detail to find an answer. Use diverse models like BERT, RoBERTa or XLNet trained via <a href=\"https://github.com/deepset-ai/FARM\" rel=\"nofollow\">FARM</a> or  on SQuAD like tasks. The Reader takes multiple passages of text as input and returns top-n answers with corresponding confidence scores.\nBoth readers can load either a local model or any public model from  <a href=\"https://huggingface.co/models\" rel=\"nofollow\">Hugging Face\u2019s model hub</a></p>\n<div id=\"farmreader\">\n<h4>FARMReader</h4>\n<p>Implementing various QA models via the <a href=\"https://github.com/deepset-ai/FARM\" rel=\"nofollow\">FARM</a> Framework.\nExample:</p>\n<pre>reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\",\n                use_gpu=False, no_ans_boost=-10, context_window_size=500,\n                top_k_per_candidate=3, top_k_per_sample=1,\n                num_processes=8, max_seq_len=256, doc_stride=128)\n\n# Optional: Training &amp; eval\nreader.train(...)\nreader.eval(...)\n\n# Predict\nreader.predict(question=\"Who is the father of Arya Starck?\", documents=documents, top_k=3)\n</pre>\n<p>This Reader comes with:\n* quite many configuration options\n* using multiple processes for preprocessing\n* option to train\n* option to evaluate</p>\n</div>\n<div id=\"transformersreader\">\n<h4>TransformersReader</h4>\n<p>Implementing various QA models via the <code>pipeline</code> class of <a href=\"https://github.com/huggingface/transformers\" rel=\"nofollow\">Transformers</a> Framework.</p>\n<p>Example:</p>\n<pre>reader = TransformersReader(model=\"distilbert-base-uncased-distilled-squad\",\n                            tokenizer=\"distilbert-base-uncased\",\n                            context_window_size=500,\n                            use_gpu=-1)\n\nreader.predict(question=\"Who is the father of Arya Starck?\", documents=documents, top_k=3)\n</pre>\n</div>\n</div>\n<div id=\"rest-api\">\n<h3>5. REST API</h3>\n<p>A simple REST API based on <a href=\"https://fastapi.tiangolo.com/\" rel=\"nofollow\">FastAPI</a> is provided to:</p>\n<ul>\n<li>search answers in texts (<a href=\"https://github.com/deepset-ai/haystack/blob/master/haystack/api/controller/search.py\" rel=\"nofollow\">extractive QA</a>)</li>\n<li>search answers by comparing user question to existing questions (<a href=\"https://github.com/deepset-ai/haystack/blob/master/haystack/api/controller/search.py\" rel=\"nofollow\">FAQ-style QA</a>)</li>\n<li>collect &amp; export user feedback on answers to gain domain-specific training data (<a href=\"https://github.com/deepset-ai/haystack/blob/master/haystack/api/controller/feedback.py\" rel=\"nofollow\">feedback</a>)</li>\n<li>allow basic monitoring of requests (currently via APM in Kibana)</li>\n</ul>\n<p>To serve the API, run:</p>\n<pre>gunicorn haystack.api.application:app -b 0.0.0.0:80 -k uvicorn.workers.UvicornWorker`\n</pre>\n<p>You will find the Swagger API documentation at <a href=\"http://127.0.0.1:80/docs\" rel=\"nofollow\">http://127.0.0.1:80/docs</a></p>\n</div>\n<div id=\"labeling-tool\">\n<h3>6. Labeling Tool</h3>\n<ul>\n<li>Use the <a href=\"https://annotate.deepset.ai/login\" rel=\"nofollow\">hosted version</a>  (Beta) or deploy it yourself via Docker images (coming soon)</li>\n<li>Create labels with different techniques: Come up with questions (+ answers) while reading passages (SQuAD style) or have a set of predefined questions and look for answers in the document (~ Natural Questions).</li>\n<li>Structure your work via organizations, projects, users</li>\n<li>Upload your documents or import labels from an existing SQuAD-style dataset</li>\n<li>Coming soon: more file formats for document upload, metrics for label quality \u2026</li>\n</ul>\n<img alt=\"https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/img/annotation_tool.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3d156354b698e4041739763d293120785f5e89b7/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f646565707365742d61692f686179737461636b2f6d61737465722f646f63732f696d672f616e6e6f746174696f6e5f746f6f6c2e706e67\">\n</div>\n</div>\n\n          </div>"}, "last_serial": 7171885, "releases": {"0.1.0.post2": [{"comment_text": "", "digests": {"md5": "320e4b03c371d37cb14094c2971727fc", "sha256": "813f00e2bef7407448fccb1e486b16c8078f52cc7dca31b4bdf6f3c6d3d928f2"}, "downloads": -1, "filename": "farm-haystack-0.1.0.post2.tar.gz", "has_sig": false, "md5_digest": "320e4b03c371d37cb14094c2971727fc", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 14725, "upload_time": "2019-11-28T10:12:50", "upload_time_iso_8601": "2019-11-28T10:12:50.491190Z", "url": "https://files.pythonhosted.org/packages/4f/9c/e43877fda5183cc2adc9539ff6a05ad525051893de3391b5058628e675b4/farm-haystack-0.1.0.post2.tar.gz", "yanked": false}], "0.2.0.post1": [{"comment_text": "", "digests": {"md5": "966249db8deb1e4dc4035368b3d2d71e", "sha256": "106a9f26a11a6d31c623f1c7c26cc607a8ec1eddd68bca9f16935b36e473fffb"}, "downloads": -1, "filename": "farm-haystack-0.2.0.post1.tar.gz", "has_sig": false, "md5_digest": "966249db8deb1e4dc4035368b3d2d71e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 30507, "upload_time": "2020-05-05T12:43:38", "upload_time_iso_8601": "2020-05-05T12:43:38.780235Z", "url": "https://files.pythonhosted.org/packages/aa/9f/08a01deaad47a813765a19d7205a2169f48767dcbf151fcc52586127a96b/farm-haystack-0.2.0.post1.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "d67c7a0be99cdb6f5d886b8679121d7b", "sha256": "c588e29cab7aeccdced9b6f0127051ce4fb2c1b318762d69e66baef6414b80ff"}, "downloads": -1, "filename": "farm-haystack-0.2.1.tar.gz", "has_sig": false, "md5_digest": "d67c7a0be99cdb6f5d886b8679121d7b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 30488, "upload_time": "2020-05-05T13:09:38", "upload_time_iso_8601": "2020-05-05T13:09:38.446581Z", "url": "https://files.pythonhosted.org/packages/f2/fa/4a519d222deb09c3a1ffdfaaef55175fa0bade71a316e7f0ed9ac082052b/farm-haystack-0.2.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d67c7a0be99cdb6f5d886b8679121d7b", "sha256": "c588e29cab7aeccdced9b6f0127051ce4fb2c1b318762d69e66baef6414b80ff"}, "downloads": -1, "filename": "farm-haystack-0.2.1.tar.gz", "has_sig": false, "md5_digest": "d67c7a0be99cdb6f5d886b8679121d7b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 30488, "upload_time": "2020-05-05T13:09:38", "upload_time_iso_8601": "2020-05-05T13:09:38.446581Z", "url": "https://files.pythonhosted.org/packages/f2/fa/4a519d222deb09c3a1ffdfaaef55175fa0bade71a316e7f0ed9ac082052b/farm-haystack-0.2.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:42 2020"}