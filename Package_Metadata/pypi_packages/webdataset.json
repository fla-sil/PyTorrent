{"info": {"author": "Thomas Breuel", "author_email": "tmbdev+removeme@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "License :: OSI Approved :: BSD License", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "[![Test](https://github.com/tmbdev/webdataset/workflows/Test/badge.svg)](https://github.com/tmbdev/webdataset/actions?query=workflow%3ATest)\n[![DeepSource](https://static.deepsource.io/deepsource-badge-light-mini.svg)](https://deepsource.io/gh/tmbdev/webdataset/?ref=repository-badge)\n\n# WebDataset\n\nWebDataset is a PyTorch Dataset (IterableDataset) implementation providing\nefficient access to datasets stored in POSIX tar archives.\n\nStoring data in POSIX tar archives greatly speeds up I/O operations on\nrotational storage and on networked file systems because it permits all\nI/O operations to operate as large sequential reads and writes.\n\nWebDataset fulfills a similar function to Tensorflow's TFRecord/tf.Example\nclasses, but it is much easier to adopt because it does not actually\nrequire any kind of data conversion: data is stored in exactly the same\nformat inside tar files as it is on disk, and all preprocessing and data\naugmentation code remains unchanged.\n\n# Installation and Documentation\n\n```Bash\n    $ pip install webdataset\n```\n\nFor the Github version:\n\n```Bash\n    $ pip install git+https://github.com/tmbdev/webdataset.git\n```\n\nDocumentation: [ReadTheDocs](http://webdataset.readthedocs.io)\n\n# Using WebDataset\n\nWebDataset reads dataset that are stored as tar files, with the simple convention that files that belong together and make up a training sample share the same basename. WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores.\n\n\n```bash\n%%bash\ncurl -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar | tar tf - | sed 10q\n```\n\n    e39871fd9fd74f55.jpg\n    e39871fd9fd74f55.json\n    f18b91585c4d3f3e.jpg\n    f18b91585c4d3f3e.json\n    ede6e66b2fb59aab.jpg\n    ede6e66b2fb59aab.json\n    ed600d57fcee4f94.jpg\n    ed600d57fcee4f94.json\n    ff47e649b23f446d.jpg\n    ff47e649b23f446d.json\n\n\n\n```python\n%pylab inline\n\nimport torch\nfrom torchvision import transforms\nimport webdataset as wds\nfrom itertools import islice\n\nurl = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\"\nurl = f\"pipe:curl -L -s {url} || true\"\n```\n\n    Populating the interactive namespace from numpy and matplotlib\n\n\nWebDatasets are an implementation of PyTorch `IterableDataset` and fully compatible with PyTorch input pipelines. By default, WebDataset just iterates through the files in a tar file without decoding anything, returning related files in each sample.\n\n\n```python\ndataset = wds.Dataset(url)\n\nfor sample in islice(dataset, 0, 3):\n    for key, value in sample.items():\n        print(key, repr(value)[:50])\n    print()\n```\n\n    __key__ 'e39871fd9fd74f55'\n    jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01\n    json b'[{\"ImageID\": \"e39871fd9fd74f55\", \"Source\": \"xcli\n\n    __key__ 'f18b91585c4d3f3e'\n    jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\n    json b'[{\"ImageID\": \"f18b91585c4d3f3e\", \"Source\": \"acti\n\n    __key__ 'ede6e66b2fb59aab'\n    jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\n    json b'[{\"ImageID\": \"ede6e66b2fb59aab\", \"Source\": \"acti\n\n\n\n\n```python\ndataset = (\n    wds.Dataset(url)\n    .shuffle(100)\n    .decode()\n    .rename(image=\"jpg;png\", data=\"json\")\n    .to_tuple(\"image\", \"data\")\n)\n\nfor image, data in islice(dataset, 0, 3):\n    print(image.shape, image.dtype, type(data))\n```\n\n    (768, 1024, 3) float32 <class 'list'>\n    (768, 1024, 3) float32 <class 'list'>\n    (683, 1024, 3) float32 <class 'list'>\n\n\n\n```python\ndataset = (\n    wds.Dataset(url)\n    .shuffle(100)\n    .decode()\n    .rename(image=\"jpg;png\", data=\"json\")\n    .to_tuple(\"image\", \"data\")\n)\n\nfor image, data in islice(dataset, 0, 3):\n    print(image.shape, image.dtype, type(data))\n```\n\n    (1024, 771, 3) float32 <class 'list'>\n    (575, 1024, 3) float32 <class 'list'>\n    (683, 1024, 3) float32 <class 'list'>\n\n\nThere are common processing stages you can add to a dataset to make it a drop-in replacement for any existing dataset. For convenience, common operations are available through a \"fluent\" interface (as chained method calls).\n\n\n```python\ndataset = (\n    wds.Dataset(url)\n    .shuffle(100)\n    .decode()\n    .rename(image=\"jpg;png\", data=\"json\")\n    .to_tuple(\"image\", \"data\")\n)\n\nfor image, data in islice(dataset, 0, 3):\n    print(image.shape, image.dtype, type(data))\n```\n\n    (699, 1024, 3) float32 <class 'list'>\n    (683, 1024, 3) float32 <class 'list'>\n    (768, 1024, 3) float32 <class 'list'>\n\n\nCommon operations:\n\n- `shuffle(n)`: shuffle the dataset with a buffer of size `n`; also shuffles shards (see below)\n- `decode([type])`: automatically decode files; the `type` determines desired outputs for images, video, and audio: `pil`, `rgb`, `rgb8`, `rgbtorch`, etc.\n- `rename(new=\"old1;old2\", ...)`: rename fields\n- `map(f)`: apply `f` to each sample\n- `map_dict(key=f, ...)`: apply `f` to its corresponding key\n- `map_tuple(f, g, ...)`: apply `f`, `g`, etc. to their corresponding values in the tuple\n- `pipe(f)`: `f` should be a function that takes an iterator and returns a new iterator\n\nStages commonly take a `handler=` argument, which is a function that gets called when there is an exception; you can write whatever function you want, but common functions are:\n\n- `webdataset.ignore_and_stop`\n- `webdataset.ignore_and_continue`\n- `webdataset.warn_and_stop`\n- `webdataset.warn_and_continue`\n- `webdataset.reraise_exception`\n\n\nHere is an example that uses `torchvision` data augmentation the same way you might use it with a `FileDataset`.\n\n\n```python\nnormalize = transforms.Normalize(\n    mean=[0.485, 0.456, 0.406],\n    std=[0.229, 0.224, 0.225])\n\npreproc = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    normalize,\n])\n\ndataset = (\n    wds.Dataset(url)\n    .shuffle(100)\n    .decode(\"pil\")\n    .rename(image=\"jpg;png\", data=\"json\")\n    .map_dict(image=preproc)\n    .to_tuple(\"image\", \"data\")\n)\n\nfor image, data in islice(dataset, 0, 3):\n    print(image.shape, image.dtype, type(data))\n```\n\n    torch.Size([3, 224, 224]) torch.float32 <class 'list'>\n    torch.Size([3, 224, 224]) torch.float32 <class 'list'>\n    torch.Size([3, 224, 224]) torch.float32 <class 'list'>\n\n\n# Sharding and Parallel I/O\n\nIn order to be able to shuffle data better and to process and load data in parallel, it is a good idea to shard it; that is, to split up the dataset into several `.tar` files.\n\nWebDataset uses standard UNIX brace notation for sharded dataset. For example, the OpenImages dataset consists of 554 shards, each containing about 1 Gbyte of images. You can open the entire dataset as follows.\n\n\n```python\nurl = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-{000000..000554}.tar\"\nurl = f\"pipe:curl -L -s {url} || true\"\ndataset = (\n    wds.Dataset(url)\n    .shuffle(100)\n    .decode(\"pil\")\n    .rename(image=\"jpg;png\", data=\"json\")\n    .map_dict(image=preproc)\n    .to_tuple(\"image\", \"data\")\n)\n```\n\nWhen used with a standard Torch `DataLoader`, this will now perform parallel I/O and preprocessing.\n\n\n```python\ndataloader = torch.utils.data.DataLoader(dataset, num_workers=4, batch_size=16)\nimages, targets = next(iter(dataloader))\nimages.shape\n```\n\n\n\n\n    torch.Size([16, 3, 224, 224])\n\n\n\n# Data Sources\n\nWhen creating a dataset with `webdataset.Dataset(url)`, the URL can be:\n\n- the string \"-\", referring to stdin\n- a UNIX path, opened as a regular file\n- a URL-like string with the schema \"pipe:\"; such URLs are opened with `subprocess.Popen`. For example:\n    - `pipe:curl -s -L http://server/file` accesses a file via HTTP\n    - `pipe:gsutil cat gs://bucket/file` accesses a file on GCS\n    - `pipe:az cp --container bucket --name file --file /dev/stdout` accesses a file on Azure\n    - `pipe:ssh host cat file` accesses a file via `ssh`\n- any other URL-like string with another schema; such URLs are passed to the `objectio` libraries if it is installed\n\nIt might seem at first glance to be \"more efficient\" to use built-in Python libraries for accessing object stores rather than subprocesses, but efficient object store access from Python really requires spawning a separate process anyway, so this approach to accessing object stores is not only convenient, it also is as efficient as we can make it in Python.\n\n# Creating a WebDataset\n\nSince WebDatasets are just regular tar files, you can usually create them by just using the `tar` command. All you have to do is to arrange for any files that should be in the same sample to share the same basename. Many datasets already come that way. For those, you can simply create a WebDataset with\n\n```Bash\n$ tar --sort=name -cf dataset.tar dataset/\n```\n\nIf your dataset has some other directory layout, you can either rearrange the files on disk, or you can use `tar --transform` to get the right kinds of names in your tar file.\n\nYou can also create a WebDataset with library functions in this library:\n\n- `webdataset.TarWriter` takes dictionaries containing key value pairs and writes them to disk\n- `webdataset.ShardWriter` takes dictionaries containing key value pairs and writes them to disk as a series of shards\n\nHere is how you can use `TarWriter` for writing a dataset:\n\n```Python\nsink = wds.TarWriter(\"dest.tar\", encoder=False)\nfor basename in basenames:\n    with open(f\"{basename}.png\", \"rb\") as stream):\n        image = stream.read()\n    cls = lookup_cls(basename)\n    sample = {\n        \"__key__\": basename,\n        \"png\": image,\n        \"cls\": cls\n    }\n    sink.write(sample)\nsink.close()\n```\n\n# Writing Filters and Offline Augmentation\n\nWebdataset can be used for filters and offline augmentation of datasets. Here is a complete example that pre-augments a shard and extracts class labels.\n\n\n```python\ndef extract_class(data):\n    # mock implementation\n    return 0\n\ndef augment_wds(input, output, maxcount=999999999):\n    src = (\n        wds.Dataset(input)\n        .decode(\"pil\")\n        .rename(key=\"__key__\", image=\"jpg;png\", data=\"json\")\n        .map_dict(image=preproc)\n        .to_tuple(\"key\", \"image\", \"data\")\n    )\n    with wds.TarWriter(output) as dst:\n        for key, image, data in islice(src, 0, maxcount):\n            print(key)\n            image = image.numpy().transpose(1, 2, 0)\n            image -= amin(image)\n            image /= amax(image)\n            sample = {\n                \"__key__\": key,\n                \"png\": image,\n                \"cls\": extract_class(data)\n            }\n            dst.write(sample)\n```\n\n\n```python\nurl = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\"\nurl = f\"pipe:curl -L -s {url} || true\"\naugment_wds(url, \"_temp.tar\", maxcount=5)\n```\n\n    e39871fd9fd74f55\n    f18b91585c4d3f3e\n    ede6e66b2fb59aab\n    ed600d57fcee4f94\n    ff47e649b23f446d\n\n\n\n```bash\n%%bash\ntar tf _temp.tar\n```\n\n    e39871fd9fd74f55.cls\n    e39871fd9fd74f55.png\n    f18b91585c4d3f3e.cls\n    f18b91585c4d3f3e.png\n    ede6e66b2fb59aab.cls\n    ede6e66b2fb59aab.png\n    ed600d57fcee4f94.cls\n    ed600d57fcee4f94.png\n    ff47e649b23f446d.cls\n    ff47e649b23f446d.png\n\n\nIf you want to preprocess the entire OpenImages dataset with a process like this, you can use your favorite job queueing or worflow system.\n\nFor example, using Dask, you could process all 554 shards in parallel using code like this:\n\n```Python\nshards = braceexpand.braceexpand(\"{000000..000554}\")\ninputs = [f\"gs://bucket/openimages-{shard}.tar\" for shard in shards]\noutputs = [f\"gs://bucket2/openimages-augmented-{shard}.tar\" for shard in shards]\nresults = [dask.delayed(augment_wds)(args) for args in zip(inputs, outputs)]\ndask.compute(*results)\n```\n\nNote that the data is streaming from and to Google Cloud Storage buckets, so very little local storage is required on each worker.\n\nFor very large scale processing, it's easiest to submit separate jobs to a Kubernetes cluster using the Kubernetes `Job` template, or using a workflow engine like Argo.\n\n# Related Libraries and Software\n\nThe [AIStore](http://github.com/NVIDIA/aistore) server provides an efficient backend for WebDataset; it functions like a combination of web server, content distribution network, P2P network, and distributed file system. Together, AIStore and WebDataset can serve input data from rotational drives distributed across many servers at the speed of local SSDs to many GPUs, at a fraction of the cost. We can easily achieve hundreds of MBytes/s of I/O per GPU even in large, distributed training jobs.\n\nThe [tarp](http://github.com/tmbdev/tarp) utilities provide command line manipulation and processing of webdatasets and other tar files, including splitting, concatenation, and `xargs`-like functionality. (There is a Python prototype called [tarproc](http://github.com/tmbdev/tarproc) available as well.)\n\nFor large scale ETL and preprocessing jobs, you can simply schedule Kubernetes Pods or Jobs to process shards in parallel. Workflow systems like Argo may be useful for that, or you may find [qupods](http://github.com/tmbdev/qupods) useful, since it will pace job submission and keep track of logs for you.\n\nThe [tensorcom](http://github.com/tmbdev/tensorcom/) library provides fast three-tiered I/O; it can be inserted between [AIStore](http://github.com/NVIDIA/aistore) and [WebDataset](http://github.com/tmbdev/webdataset) to permit distributed data augmentation and I/O. It is particularly useful when data augmentation requires more CPU than the GPU server has available.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/tmbdev/webdataset", "keywords": "object store,client,deep learning", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "webdataset", "package_url": "https://pypi.org/project/webdataset/", "platform": "", "project_url": "https://pypi.org/project/webdataset/", "project_urls": {"Homepage": "http://github.com/tmbdev/webdataset"}, "release_url": "https://pypi.org/project/webdataset/0.1.21/", "requires_dist": ["Pillow", "simplejson", "braceexpand", "msgpack", "pyyaml", "numpy", "torch", "objectio"], "requires_python": ">=3.6", "summary": "Record sequential storage for deep learning.", "version": "0.1.21", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://github.com/tmbdev/webdataset/actions?query=workflow%3ATest\" rel=\"nofollow\"><img alt=\"Test\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c6bd7fa9c910ed52875dec50cd9902ce8b568267/68747470733a2f2f6769746875622e636f6d2f746d626465762f776562646174617365742f776f726b666c6f77732f546573742f62616467652e737667\"></a>\n<a href=\"https://deepsource.io/gh/tmbdev/webdataset/?ref=repository-badge\" rel=\"nofollow\"><img alt=\"DeepSource\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6837f002aa8f2f99f307c9febf81cee78d869751/68747470733a2f2f7374617469632e64656570736f757263652e696f2f64656570736f757263652d62616467652d6c696768742d6d696e692e737667\"></a></p>\n<h1>WebDataset</h1>\n<p>WebDataset is a PyTorch Dataset (IterableDataset) implementation providing\nefficient access to datasets stored in POSIX tar archives.</p>\n<p>Storing data in POSIX tar archives greatly speeds up I/O operations on\nrotational storage and on networked file systems because it permits all\nI/O operations to operate as large sequential reads and writes.</p>\n<p>WebDataset fulfills a similar function to Tensorflow's TFRecord/tf.Example\nclasses, but it is much easier to adopt because it does not actually\nrequire any kind of data conversion: data is stored in exactly the same\nformat inside tar files as it is on disk, and all preprocessing and data\naugmentation code remains unchanged.</p>\n<h1>Installation and Documentation</h1>\n<pre>    $ pip install webdataset\n</pre>\n<p>For the Github version:</p>\n<pre>    $ pip install git+https://github.com/tmbdev/webdataset.git\n</pre>\n<p>Documentation: <a href=\"http://webdataset.readthedocs.io\" rel=\"nofollow\">ReadTheDocs</a></p>\n<h1>Using WebDataset</h1>\n<p>WebDataset reads dataset that are stored as tar files, with the simple convention that files that belong together and make up a training sample share the same basename. WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores.</p>\n<pre>%%bash\ncurl -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar <span class=\"p\">|</span> tar tf - <span class=\"p\">|</span> sed 10q\n</pre>\n<pre><code>e39871fd9fd74f55.jpg\ne39871fd9fd74f55.json\nf18b91585c4d3f3e.jpg\nf18b91585c4d3f3e.json\nede6e66b2fb59aab.jpg\nede6e66b2fb59aab.json\ned600d57fcee4f94.jpg\ned600d57fcee4f94.json\nff47e649b23f446d.jpg\nff47e649b23f446d.json\n</code></pre>\n<pre><span class=\"o\">%</span><span class=\"n\">pylab</span> <span class=\"n\">inline</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torchvision</span> <span class=\"kn\">import</span> <span class=\"n\">transforms</span>\n<span class=\"kn\">import</span> <span class=\"nn\">webdataset</span> <span class=\"k\">as</span> <span class=\"nn\">wds</span>\n<span class=\"kn\">from</span> <span class=\"nn\">itertools</span> <span class=\"kn\">import</span> <span class=\"n\">islice</span>\n\n<span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s2\">\"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\"</span>\n<span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s2\">\"pipe:curl -L -s </span><span class=\"si\">{</span><span class=\"n\">url</span><span class=\"si\">}</span><span class=\"s2\"> || true\"</span>\n</pre>\n<pre><code>Populating the interactive namespace from numpy and matplotlib\n</code></pre>\n<p>WebDatasets are an implementation of PyTorch <code>IterableDataset</code> and fully compatible with PyTorch input pipelines. By default, WebDataset just iterates through the files in a tar file without decoding anything, returning related files in each sample.</p>\n<pre><span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">wds</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">sample</span> <span class=\"ow\">in</span> <span class=\"n\">islice</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">value</span> <span class=\"ow\">in</span> <span class=\"n\">sample</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">():</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"nb\">repr</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">)[:</span><span class=\"mi\">50</span><span class=\"p\">])</span>\n    <span class=\"nb\">print</span><span class=\"p\">()</span>\n</pre>\n<pre><code>__key__ 'e39871fd9fd74f55'\njpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01\njson b'[{\"ImageID\": \"e39871fd9fd74f55\", \"Source\": \"xcli\n\n__key__ 'f18b91585c4d3f3e'\njpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\njson b'[{\"ImageID\": \"f18b91585c4d3f3e\", \"Source\": \"acti\n\n__key__ 'ede6e66b2fb59aab'\njpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\njson b'[{\"ImageID\": \"ede6e66b2fb59aab\", \"Source\": \"acti\n</code></pre>\n<pre><span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"n\">wds</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">()</span>\n    <span class=\"o\">.</span><span class=\"n\">rename</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">=</span><span class=\"s2\">\"jpg;png\"</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"s2\">\"json\"</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">to_tuple</span><span class=\"p\">(</span><span class=\"s2\">\"image\"</span><span class=\"p\">,</span> <span class=\"s2\">\"data\"</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">data</span> <span class=\"ow\">in</span> <span class=\"n\">islice</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">))</span>\n</pre>\n<pre><code>(768, 1024, 3) float32 &lt;class 'list'&gt;\n(768, 1024, 3) float32 &lt;class 'list'&gt;\n(683, 1024, 3) float32 &lt;class 'list'&gt;\n</code></pre>\n<pre><span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"n\">wds</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">()</span>\n    <span class=\"o\">.</span><span class=\"n\">rename</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">=</span><span class=\"s2\">\"jpg;png\"</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"s2\">\"json\"</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">to_tuple</span><span class=\"p\">(</span><span class=\"s2\">\"image\"</span><span class=\"p\">,</span> <span class=\"s2\">\"data\"</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">data</span> <span class=\"ow\">in</span> <span class=\"n\">islice</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">))</span>\n</pre>\n<pre><code>(1024, 771, 3) float32 &lt;class 'list'&gt;\n(575, 1024, 3) float32 &lt;class 'list'&gt;\n(683, 1024, 3) float32 &lt;class 'list'&gt;\n</code></pre>\n<p>There are common processing stages you can add to a dataset to make it a drop-in replacement for any existing dataset. For convenience, common operations are available through a \"fluent\" interface (as chained method calls).</p>\n<pre><span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"n\">wds</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">()</span>\n    <span class=\"o\">.</span><span class=\"n\">rename</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">=</span><span class=\"s2\">\"jpg;png\"</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"s2\">\"json\"</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">to_tuple</span><span class=\"p\">(</span><span class=\"s2\">\"image\"</span><span class=\"p\">,</span> <span class=\"s2\">\"data\"</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">data</span> <span class=\"ow\">in</span> <span class=\"n\">islice</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">))</span>\n</pre>\n<pre><code>(699, 1024, 3) float32 &lt;class 'list'&gt;\n(683, 1024, 3) float32 &lt;class 'list'&gt;\n(768, 1024, 3) float32 &lt;class 'list'&gt;\n</code></pre>\n<p>Common operations:</p>\n<ul>\n<li><code>shuffle(n)</code>: shuffle the dataset with a buffer of size <code>n</code>; also shuffles shards (see below)</li>\n<li><code>decode([type])</code>: automatically decode files; the <code>type</code> determines desired outputs for images, video, and audio: <code>pil</code>, <code>rgb</code>, <code>rgb8</code>, <code>rgbtorch</code>, etc.</li>\n<li><code>rename(new=\"old1;old2\", ...)</code>: rename fields</li>\n<li><code>map(f)</code>: apply <code>f</code> to each sample</li>\n<li><code>map_dict(key=f, ...)</code>: apply <code>f</code> to its corresponding key</li>\n<li><code>map_tuple(f, g, ...)</code>: apply <code>f</code>, <code>g</code>, etc. to their corresponding values in the tuple</li>\n<li><code>pipe(f)</code>: <code>f</code> should be a function that takes an iterator and returns a new iterator</li>\n</ul>\n<p>Stages commonly take a <code>handler=</code> argument, which is a function that gets called when there is an exception; you can write whatever function you want, but common functions are:</p>\n<ul>\n<li><code>webdataset.ignore_and_stop</code></li>\n<li><code>webdataset.ignore_and_continue</code></li>\n<li><code>webdataset.warn_and_stop</code></li>\n<li><code>webdataset.warn_and_continue</code></li>\n<li><code>webdataset.reraise_exception</code></li>\n</ul>\n<p>Here is an example that uses <code>torchvision</code> data augmentation the same way you might use it with a <code>FileDataset</code>.</p>\n<pre><span class=\"n\">normalize</span> <span class=\"o\">=</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Normalize</span><span class=\"p\">(</span>\n    <span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.485</span><span class=\"p\">,</span> <span class=\"mf\">0.456</span><span class=\"p\">,</span> <span class=\"mf\">0.406</span><span class=\"p\">],</span>\n    <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.229</span><span class=\"p\">,</span> <span class=\"mf\">0.224</span><span class=\"p\">,</span> <span class=\"mf\">0.225</span><span class=\"p\">])</span>\n\n<span class=\"n\">preproc</span> <span class=\"o\">=</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">([</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">RandomResizedCrop</span><span class=\"p\">(</span><span class=\"mi\">224</span><span class=\"p\">),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">RandomHorizontalFlip</span><span class=\"p\">(),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">(),</span>\n    <span class=\"n\">normalize</span><span class=\"p\">,</span>\n<span class=\"p\">])</span>\n\n<span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"n\">wds</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s2\">\"pil\"</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">rename</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">=</span><span class=\"s2\">\"jpg;png\"</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"s2\">\"json\"</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">map_dict</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">=</span><span class=\"n\">preproc</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">to_tuple</span><span class=\"p\">(</span><span class=\"s2\">\"image\"</span><span class=\"p\">,</span> <span class=\"s2\">\"data\"</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">data</span> <span class=\"ow\">in</span> <span class=\"n\">islice</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">))</span>\n</pre>\n<pre><code>torch.Size([3, 224, 224]) torch.float32 &lt;class 'list'&gt;\ntorch.Size([3, 224, 224]) torch.float32 &lt;class 'list'&gt;\ntorch.Size([3, 224, 224]) torch.float32 &lt;class 'list'&gt;\n</code></pre>\n<h1>Sharding and Parallel I/O</h1>\n<p>In order to be able to shuffle data better and to process and load data in parallel, it is a good idea to shard it; that is, to split up the dataset into several <code>.tar</code> files.</p>\n<p>WebDataset uses standard UNIX brace notation for sharded dataset. For example, the OpenImages dataset consists of 554 shards, each containing about 1 Gbyte of images. You can open the entire dataset as follows.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s2\">\"http://storage.googleapis.com/nvdata-openimages/openimages-train-{000000..000554}.tar\"</span>\n<span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s2\">\"pipe:curl -L -s </span><span class=\"si\">{</span><span class=\"n\">url</span><span class=\"si\">}</span><span class=\"s2\"> || true\"</span>\n<span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"n\">wds</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s2\">\"pil\"</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">rename</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">=</span><span class=\"s2\">\"jpg;png\"</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"s2\">\"json\"</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">map_dict</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">=</span><span class=\"n\">preproc</span><span class=\"p\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">to_tuple</span><span class=\"p\">(</span><span class=\"s2\">\"image\"</span><span class=\"p\">,</span> <span class=\"s2\">\"data\"</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n</pre>\n<p>When used with a standard Torch <code>DataLoader</code>, this will now perform parallel I/O and preprocessing.</p>\n<pre><span class=\"n\">dataloader</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">,</span> <span class=\"n\">num_workers</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">)</span>\n<span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">targets</span> <span class=\"o\">=</span> <span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"nb\">iter</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"p\">))</span>\n<span class=\"n\">images</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n</pre>\n<pre><code>torch.Size([16, 3, 224, 224])\n</code></pre>\n<h1>Data Sources</h1>\n<p>When creating a dataset with <code>webdataset.Dataset(url)</code>, the URL can be:</p>\n<ul>\n<li>the string \"-\", referring to stdin</li>\n<li>a UNIX path, opened as a regular file</li>\n<li>a URL-like string with the schema \"pipe:\"; such URLs are opened with <code>subprocess.Popen</code>. For example:\n<ul>\n<li><code>pipe:curl -s -L http://server/file</code> accesses a file via HTTP</li>\n<li><code>pipe:gsutil cat gs://bucket/file</code> accesses a file on GCS</li>\n<li><code>pipe:az cp --container bucket --name file --file /dev/stdout</code> accesses a file on Azure</li>\n<li><code>pipe:ssh host cat file</code> accesses a file via <code>ssh</code></li>\n</ul>\n</li>\n<li>any other URL-like string with another schema; such URLs are passed to the <code>objectio</code> libraries if it is installed</li>\n</ul>\n<p>It might seem at first glance to be \"more efficient\" to use built-in Python libraries for accessing object stores rather than subprocesses, but efficient object store access from Python really requires spawning a separate process anyway, so this approach to accessing object stores is not only convenient, it also is as efficient as we can make it in Python.</p>\n<h1>Creating a WebDataset</h1>\n<p>Since WebDatasets are just regular tar files, you can usually create them by just using the <code>tar</code> command. All you have to do is to arrange for any files that should be in the same sample to share the same basename. Many datasets already come that way. For those, you can simply create a WebDataset with</p>\n<pre>$ tar --sort<span class=\"o\">=</span>name -cf dataset.tar dataset/\n</pre>\n<p>If your dataset has some other directory layout, you can either rearrange the files on disk, or you can use <code>tar --transform</code> to get the right kinds of names in your tar file.</p>\n<p>You can also create a WebDataset with library functions in this library:</p>\n<ul>\n<li><code>webdataset.TarWriter</code> takes dictionaries containing key value pairs and writes them to disk</li>\n<li><code>webdataset.ShardWriter</code> takes dictionaries containing key value pairs and writes them to disk as a series of shards</li>\n</ul>\n<p>Here is how you can use <code>TarWriter</code> for writing a dataset:</p>\n<pre><span class=\"n\">sink</span> <span class=\"o\">=</span> <span class=\"n\">wds</span><span class=\"o\">.</span><span class=\"n\">TarWriter</span><span class=\"p\">(</span><span class=\"s2\">\"dest.tar\"</span><span class=\"p\">,</span> <span class=\"n\">encoder</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">basename</span> <span class=\"ow\">in</span> <span class=\"n\">basenames</span><span class=\"p\">:</span>\n    <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"</span><span class=\"si\">{</span><span class=\"n\">basename</span><span class=\"si\">}</span><span class=\"s2\">.png\"</span><span class=\"p\">,</span> <span class=\"s2\">\"rb\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">stream</span><span class=\"p\">):</span>\n        <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">stream</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">()</span>\n    <span class=\"bp\">cls</span> <span class=\"o\">=</span> <span class=\"n\">lookup_cls</span><span class=\"p\">(</span><span class=\"n\">basename</span><span class=\"p\">)</span>\n    <span class=\"n\">sample</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n        <span class=\"s2\">\"__key__\"</span><span class=\"p\">:</span> <span class=\"n\">basename</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"png\"</span><span class=\"p\">:</span> <span class=\"n\">image</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"cls\"</span><span class=\"p\">:</span> <span class=\"bp\">cls</span>\n    <span class=\"p\">}</span>\n    <span class=\"n\">sink</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">sample</span><span class=\"p\">)</span>\n<span class=\"n\">sink</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n</pre>\n<h1>Writing Filters and Offline Augmentation</h1>\n<p>Webdataset can be used for filters and offline augmentation of datasets. Here is a complete example that pre-augments a shard and extracts class labels.</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">extract_class</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">):</span>\n    <span class=\"c1\"># mock implementation</span>\n    <span class=\"k\">return</span> <span class=\"mi\">0</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">augment_wds</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">maxcount</span><span class=\"o\">=</span><span class=\"mi\">999999999</span><span class=\"p\">):</span>\n    <span class=\"n\">src</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n        <span class=\"n\">wds</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s2\">\"pil\"</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">rename</span><span class=\"p\">(</span><span class=\"n\">key</span><span class=\"o\">=</span><span class=\"s2\">\"__key__\"</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"o\">=</span><span class=\"s2\">\"jpg;png\"</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"s2\">\"json\"</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">map_dict</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">=</span><span class=\"n\">preproc</span><span class=\"p\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">to_tuple</span><span class=\"p\">(</span><span class=\"s2\">\"key\"</span><span class=\"p\">,</span> <span class=\"s2\">\"image\"</span><span class=\"p\">,</span> <span class=\"s2\">\"data\"</span><span class=\"p\">)</span>\n    <span class=\"p\">)</span>\n    <span class=\"k\">with</span> <span class=\"n\">wds</span><span class=\"o\">.</span><span class=\"n\">TarWriter</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">dst</span><span class=\"p\">:</span>\n        <span class=\"k\">for</span> <span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">data</span> <span class=\"ow\">in</span> <span class=\"n\">islice</span><span class=\"p\">(</span><span class=\"n\">src</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">maxcount</span><span class=\"p\">):</span>\n            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">)</span>\n            <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n            <span class=\"n\">image</span> <span class=\"o\">-=</span> <span class=\"n\">amin</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>\n            <span class=\"n\">image</span> <span class=\"o\">/=</span> <span class=\"n\">amax</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>\n            <span class=\"n\">sample</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n                <span class=\"s2\">\"__key__\"</span><span class=\"p\">:</span> <span class=\"n\">key</span><span class=\"p\">,</span>\n                <span class=\"s2\">\"png\"</span><span class=\"p\">:</span> <span class=\"n\">image</span><span class=\"p\">,</span>\n                <span class=\"s2\">\"cls\"</span><span class=\"p\">:</span> <span class=\"n\">extract_class</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n            <span class=\"p\">}</span>\n            <span class=\"n\">dst</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">sample</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s2\">\"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\"</span>\n<span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s2\">\"pipe:curl -L -s </span><span class=\"si\">{</span><span class=\"n\">url</span><span class=\"si\">}</span><span class=\"s2\"> || true\"</span>\n<span class=\"n\">augment_wds</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"s2\">\"_temp.tar\"</span><span class=\"p\">,</span> <span class=\"n\">maxcount</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n</pre>\n<pre><code>e39871fd9fd74f55\nf18b91585c4d3f3e\nede6e66b2fb59aab\ned600d57fcee4f94\nff47e649b23f446d\n</code></pre>\n<pre>%%bash\ntar tf _temp.tar\n</pre>\n<pre><code>e39871fd9fd74f55.cls\ne39871fd9fd74f55.png\nf18b91585c4d3f3e.cls\nf18b91585c4d3f3e.png\nede6e66b2fb59aab.cls\nede6e66b2fb59aab.png\ned600d57fcee4f94.cls\ned600d57fcee4f94.png\nff47e649b23f446d.cls\nff47e649b23f446d.png\n</code></pre>\n<p>If you want to preprocess the entire OpenImages dataset with a process like this, you can use your favorite job queueing or worflow system.</p>\n<p>For example, using Dask, you could process all 554 shards in parallel using code like this:</p>\n<pre><span class=\"n\">shards</span> <span class=\"o\">=</span> <span class=\"n\">braceexpand</span><span class=\"o\">.</span><span class=\"n\">braceexpand</span><span class=\"p\">(</span><span class=\"s2\">\"{000000..000554}\"</span><span class=\"p\">)</span>\n<span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"sa\">f</span><span class=\"s2\">\"gs://bucket/openimages-</span><span class=\"si\">{</span><span class=\"n\">shard</span><span class=\"si\">}</span><span class=\"s2\">.tar\"</span> <span class=\"k\">for</span> <span class=\"n\">shard</span> <span class=\"ow\">in</span> <span class=\"n\">shards</span><span class=\"p\">]</span>\n<span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"sa\">f</span><span class=\"s2\">\"gs://bucket2/openimages-augmented-</span><span class=\"si\">{</span><span class=\"n\">shard</span><span class=\"si\">}</span><span class=\"s2\">.tar\"</span> <span class=\"k\">for</span> <span class=\"n\">shard</span> <span class=\"ow\">in</span> <span class=\"n\">shards</span><span class=\"p\">]</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">dask</span><span class=\"o\">.</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">augment_wds</span><span class=\"p\">)(</span><span class=\"n\">args</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">args</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"p\">)]</span>\n<span class=\"n\">dask</span><span class=\"o\">.</span><span class=\"n\">compute</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">results</span><span class=\"p\">)</span>\n</pre>\n<p>Note that the data is streaming from and to Google Cloud Storage buckets, so very little local storage is required on each worker.</p>\n<p>For very large scale processing, it's easiest to submit separate jobs to a Kubernetes cluster using the Kubernetes <code>Job</code> template, or using a workflow engine like Argo.</p>\n<h1>Related Libraries and Software</h1>\n<p>The <a href=\"http://github.com/NVIDIA/aistore\" rel=\"nofollow\">AIStore</a> server provides an efficient backend for WebDataset; it functions like a combination of web server, content distribution network, P2P network, and distributed file system. Together, AIStore and WebDataset can serve input data from rotational drives distributed across many servers at the speed of local SSDs to many GPUs, at a fraction of the cost. We can easily achieve hundreds of MBytes/s of I/O per GPU even in large, distributed training jobs.</p>\n<p>The <a href=\"http://github.com/tmbdev/tarp\" rel=\"nofollow\">tarp</a> utilities provide command line manipulation and processing of webdatasets and other tar files, including splitting, concatenation, and <code>xargs</code>-like functionality. (There is a Python prototype called <a href=\"http://github.com/tmbdev/tarproc\" rel=\"nofollow\">tarproc</a> available as well.)</p>\n<p>For large scale ETL and preprocessing jobs, you can simply schedule Kubernetes Pods or Jobs to process shards in parallel. Workflow systems like Argo may be useful for that, or you may find <a href=\"http://github.com/tmbdev/qupods\" rel=\"nofollow\">qupods</a> useful, since it will pace job submission and keep track of logs for you.</p>\n<p>The <a href=\"http://github.com/tmbdev/tensorcom/\" rel=\"nofollow\">tensorcom</a> library provides fast three-tiered I/O; it can be inserted between <a href=\"http://github.com/NVIDIA/aistore\" rel=\"nofollow\">AIStore</a> and <a href=\"http://github.com/tmbdev/webdataset\" rel=\"nofollow\">WebDataset</a> to permit distributed data augmentation and I/O. It is particularly useful when data augmentation requires more CPU than the GPU server has available.</p>\n\n          </div>"}, "last_serial": 7190520, "releases": {"0.0.0": [{"comment_text": "", "digests": {"md5": "15c1f287985e0c4c1cb795ba7b842f30", "sha256": "0507c7f17938971ddab1f43e69d47080c947de86aa836e880f5bbbf8d2b3414c"}, "downloads": -1, "filename": "webdataset-0.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "15c1f287985e0c4c1cb795ba7b842f30", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18260, "upload_time": "2020-02-20T08:53:20", "upload_time_iso_8601": "2020-02-20T08:53:20.270605Z", "url": "https://files.pythonhosted.org/packages/31/bc/5767af9c744ee659cdb0f5e6b13b809d5343dd4d4027049318c7a9d28b9f/webdataset-0.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a3081cd468a7e53c0a0c6d8ace958349", "sha256": "76b20ae5c40709704655db8767c937bbe9aa2b4534dc82707be862902edd6429"}, "downloads": -1, "filename": "webdataset-0.0.0.tar.gz", "has_sig": false, "md5_digest": "a3081cd468a7e53c0a0c6d8ace958349", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17979, "upload_time": "2020-02-20T08:53:22", "upload_time_iso_8601": "2020-02-20T08:53:22.308827Z", "url": "https://files.pythonhosted.org/packages/76/96/b3142953f0d03b098f49cb1e65515c917bf9060ac374c5372a8b0fb43b85/webdataset-0.0.0.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "d4454ddd062f1dbbfbd81eb7164c5334", "sha256": "f0dbf505b064eea087f5ca20918c7f953fba5f1ab9a5fd0f40dacbc583d7b273"}, "downloads": -1, "filename": "webdataset-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "d4454ddd062f1dbbfbd81eb7164c5334", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 19250, "upload_time": "2020-02-20T23:18:08", "upload_time_iso_8601": "2020-02-20T23:18:08.104108Z", "url": "https://files.pythonhosted.org/packages/7c/15/ca190c820fb6bc9c692a6f776ae40989738049165f6566daee168b7cb645/webdataset-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ba82b9a198efaf48e39b3e30c725784d", "sha256": "83d7c6e6d5311bcf6d9b659ee0b7fbe266551c358e68c203e497dcba37faa704"}, "downloads": -1, "filename": "webdataset-0.1.0.tar.gz", "has_sig": false, "md5_digest": "ba82b9a198efaf48e39b3e30c725784d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 18809, "upload_time": "2020-02-20T23:18:09", "upload_time_iso_8601": "2020-02-20T23:18:09.627387Z", "url": "https://files.pythonhosted.org/packages/4d/64/da6fffb21169a3600869ca3ac6a744bd3feb940453fd52a30d2b324491bb/webdataset-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "f6a84b8852f3b2b79c4af5698e19466a", "sha256": "85c13c8919ffcbeb959f8fc7a32ce155ca93eeba9674ae8ac4cd00b3531d9349"}, "downloads": -1, "filename": "webdataset-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "f6a84b8852f3b2b79c4af5698e19466a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18858, "upload_time": "2020-02-25T00:19:50", "upload_time_iso_8601": "2020-02-25T00:19:50.021094Z", "url": "https://files.pythonhosted.org/packages/5a/c6/5fe036187f5f2147e843d6fff18a0eb5222c22fcee132ad574ef69129227/webdataset-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e979c7047bd135aab4c8de66cc8d17be", "sha256": "3bb3370f7082f9436cd4eb2044dd340261717a6753ea54d05a1c138474ecf5b9"}, "downloads": -1, "filename": "webdataset-0.1.1.tar.gz", "has_sig": false, "md5_digest": "e979c7047bd135aab4c8de66cc8d17be", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 18655, "upload_time": "2020-02-25T00:19:51", "upload_time_iso_8601": "2020-02-25T00:19:51.414780Z", "url": "https://files.pythonhosted.org/packages/38/5e/8cf815dc9e67f57ae15ebeb07533f8b67822e53d3d22e6b8164fdd551ace/webdataset-0.1.1.tar.gz", "yanked": false}], "0.1.13": [{"comment_text": "", "digests": {"md5": "9ae33506b7fa12a05a2ac655b389d409", "sha256": "4aa6c8733136dbb417f40005a0518a25129f7ea2d10fa931bc6829899ddad61d"}, "downloads": -1, "filename": "webdataset-0.1.13-py3-none-any.whl", "has_sig": false, "md5_digest": "9ae33506b7fa12a05a2ac655b389d409", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 25129, "upload_time": "2020-03-17T06:53:57", "upload_time_iso_8601": "2020-03-17T06:53:57.992618Z", "url": "https://files.pythonhosted.org/packages/09/ab/352f97f449d3dd21fb87e28e0d356e3152cebe69623ccae76b1e994a5daf/webdataset-0.1.13-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "adf776dfc9f97e3314116f5555ba78d5", "sha256": "b742d58e10e40446d82aec12fd0ca5a0691aaf4393a0857b34d100e93833d38d"}, "downloads": -1, "filename": "webdataset-0.1.13.tar.gz", "has_sig": false, "md5_digest": "adf776dfc9f97e3314116f5555ba78d5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 21113, "upload_time": "2020-03-17T06:53:59", "upload_time_iso_8601": "2020-03-17T06:53:59.078150Z", "url": "https://files.pythonhosted.org/packages/f8/1a/25fbdbe7af43851bbc4f4aac7d8a86d4f82f3cd73137eed3ad497128d1f3/webdataset-0.1.13.tar.gz", "yanked": false}], "0.1.14": [{"comment_text": "", "digests": {"md5": "ed747c4593c727128f9f8e8f6d68c8f8", "sha256": "f1d1c481cac5fc9becc1580978a2189454b06a96320311042b223a3846692b57"}, "downloads": -1, "filename": "webdataset-0.1.14-py3-none-any.whl", "has_sig": false, "md5_digest": "ed747c4593c727128f9f8e8f6d68c8f8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 24424, "upload_time": "2020-03-17T23:34:05", "upload_time_iso_8601": "2020-03-17T23:34:05.850316Z", "url": "https://files.pythonhosted.org/packages/7a/ef/845582ae7ff95675fe28277870cd5b22438fd76d5551957cf3e5f2d1985d/webdataset-0.1.14-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ee883e6416d76096523d8dfa60cf0b9e", "sha256": "c4a29789e6999b466d4f1c1b89f794ab03277b108290833aa2ef33db663999c0"}, "downloads": -1, "filename": "webdataset-0.1.14.tar.gz", "has_sig": false, "md5_digest": "ee883e6416d76096523d8dfa60cf0b9e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 20485, "upload_time": "2020-03-17T23:34:07", "upload_time_iso_8601": "2020-03-17T23:34:07.114556Z", "url": "https://files.pythonhosted.org/packages/2c/0b/8de51ce24062f324011ddf194424b7410081a03fa1ca31461a8a5ef995a6/webdataset-0.1.14.tar.gz", "yanked": false}], "0.1.15": [{"comment_text": "", "digests": {"md5": "7cedf7f2368ee1c259b849dfcf0f41c2", "sha256": "3080430d6833b38db5bb6ea46827446932ef1853cea5b15e25748a67e0f14b1c"}, "downloads": -1, "filename": "webdataset-0.1.15-py3-none-any.whl", "has_sig": false, "md5_digest": "7cedf7f2368ee1c259b849dfcf0f41c2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 28068, "upload_time": "2020-03-19T04:15:37", "upload_time_iso_8601": "2020-03-19T04:15:37.622371Z", "url": "https://files.pythonhosted.org/packages/f7/ed/7d53c856347dc8d0f7f703745cc6a97570cea73636b3cc72b9ba5b05e4d2/webdataset-0.1.15-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3cb7be3fc5bb892c9162150cc5f30644", "sha256": "0af672e97ba6b55a963f4133762a8dd632b846e866e1df2dd1ec90315266368a"}, "downloads": -1, "filename": "webdataset-0.1.15.tar.gz", "has_sig": false, "md5_digest": "3cb7be3fc5bb892c9162150cc5f30644", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 27671, "upload_time": "2020-03-19T04:15:38", "upload_time_iso_8601": "2020-03-19T04:15:38.884738Z", "url": "https://files.pythonhosted.org/packages/b0/ec/9d95e089dc52ed53524bcc36c8e8e0e8d579318f567ebc10e4c5e45bec93/webdataset-0.1.15.tar.gz", "yanked": false}], "0.1.16": [{"comment_text": "", "digests": {"md5": "cd5a12ac023cdd70e22dfcde964c8f83", "sha256": "f943410f8ea3096607eb6d28d17f5ddebf4760d1d44d2c9c9c7daee215eea192"}, "downloads": -1, "filename": "webdataset-0.1.16-py3-none-any.whl", "has_sig": false, "md5_digest": "cd5a12ac023cdd70e22dfcde964c8f83", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 28077, "upload_time": "2020-03-19T04:44:43", "upload_time_iso_8601": "2020-03-19T04:44:43.189996Z", "url": "https://files.pythonhosted.org/packages/62/03/153962b55c96bf42c559d085e7f5ab97b6b35d6cfac83dd2ddc20740e0b6/webdataset-0.1.16-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "08f5d07e7dd92f0d1d9b3f04bfb07be5", "sha256": "e797087e056db6867235f0956ac6422aed82c8ffa87fd589e9fbf4c45a17f0ce"}, "downloads": -1, "filename": "webdataset-0.1.16.tar.gz", "has_sig": false, "md5_digest": "08f5d07e7dd92f0d1d9b3f04bfb07be5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 27707, "upload_time": "2020-03-19T04:44:44", "upload_time_iso_8601": "2020-03-19T04:44:44.237340Z", "url": "https://files.pythonhosted.org/packages/9b/d2/df5e70f17a34282e17442e9ca2cbf10cdf34d4a7abefebe2f7ab9cb81b32/webdataset-0.1.16.tar.gz", "yanked": false}], "0.1.17": [{"comment_text": "", "digests": {"md5": "3b1a73b3b670b230c63c664c0b3f13c1", "sha256": "254b31216c50f2958406b9cbb6f86932437909286eae389c5dada5460d219719"}, "downloads": -1, "filename": "webdataset-0.1.17-py3-none-any.whl", "has_sig": false, "md5_digest": "3b1a73b3b670b230c63c664c0b3f13c1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 28177, "upload_time": "2020-03-20T03:35:33", "upload_time_iso_8601": "2020-03-20T03:35:33.781906Z", "url": "https://files.pythonhosted.org/packages/9f/cc/6e186453dd982349b9997d51ef758692654579dab87f95681aa2f29d6e16/webdataset-0.1.17-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0665a40777d3b21fae9331c4f1a2965b", "sha256": "41c0cdb25890c3b71fb2d576e0dbbbe6ed053f181b4880c9e5db4857aca7d8c7"}, "downloads": -1, "filename": "webdataset-0.1.17.tar.gz", "has_sig": false, "md5_digest": "0665a40777d3b21fae9331c4f1a2965b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 27804, "upload_time": "2020-03-20T03:35:35", "upload_time_iso_8601": "2020-03-20T03:35:35.280546Z", "url": "https://files.pythonhosted.org/packages/87/8d/ec7a4ddb8ffe14a6630ee262703ba9ba9402b0d4d544a932652d8d22473d/webdataset-0.1.17.tar.gz", "yanked": false}], "0.1.18": [{"comment_text": "", "digests": {"md5": "effea864fa2a5d368be20fc5b7f646b5", "sha256": "da245f42289b0e56286ac5656a4b20cfc270f8ae84b901ee3a1abe299e755c32"}, "downloads": -1, "filename": "webdataset-0.1.18-py3-none-any.whl", "has_sig": false, "md5_digest": "effea864fa2a5d368be20fc5b7f646b5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 28157, "upload_time": "2020-03-20T08:34:06", "upload_time_iso_8601": "2020-03-20T08:34:06.951476Z", "url": "https://files.pythonhosted.org/packages/2f/10/698e1064a5056db98893e18b2f84e8f473fb9b2d4652a6291cb2929e7c52/webdataset-0.1.18-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d0b14f5dc00935804847c44f4dfd975c", "sha256": "fad85bed7de4ba7066c497a543629001531b7272a68c5a11501291d0727f92ac"}, "downloads": -1, "filename": "webdataset-0.1.18.tar.gz", "has_sig": false, "md5_digest": "d0b14f5dc00935804847c44f4dfd975c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 27786, "upload_time": "2020-03-20T08:34:08", "upload_time_iso_8601": "2020-03-20T08:34:08.204299Z", "url": "https://files.pythonhosted.org/packages/86/0e/3fd89a768efc6dede53602ae31cb958a32bbe9bae19b17131f6947582cb5/webdataset-0.1.18.tar.gz", "yanked": false}], "0.1.19": [{"comment_text": "", "digests": {"md5": "dd33874c804748641410165b1effdd8c", "sha256": "a7c2a519b0ad8332fa33026ac6220d559d84322b030d8e3cbcb4f5c814c2d050"}, "downloads": -1, "filename": "webdataset-0.1.19-py3-none-any.whl", "has_sig": false, "md5_digest": "dd33874c804748641410165b1effdd8c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 28695, "upload_time": "2020-05-06T04:49:27", "upload_time_iso_8601": "2020-05-06T04:49:27.471641Z", "url": "https://files.pythonhosted.org/packages/0d/2a/1eb6737378f0282c8c4955b8c1ad66102320540f511e6601d4bb5d4434dc/webdataset-0.1.19-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5b30ccda0e8fa37d9e817f6d71503842", "sha256": "a7d99d770df34d19a3ffbfe017071fd24c17eab0004ff57bf2f8d76c35caad3f"}, "downloads": -1, "filename": "webdataset-0.1.19.tar.gz", "has_sig": false, "md5_digest": "5b30ccda0e8fa37d9e817f6d71503842", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 28445, "upload_time": "2020-05-06T04:49:28", "upload_time_iso_8601": "2020-05-06T04:49:28.789996Z", "url": "https://files.pythonhosted.org/packages/57/c1/a77d198e7d0624c1c13542ebfcefb4ab1528caa301853cd989a478341420/webdataset-0.1.19.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "87463feccfc4dc98bf76d92655970046", "sha256": "aaa4a8567b2905faabe1f0b5331c8da954239c682dac2728b71639cdaa00bf3d"}, "downloads": -1, "filename": "webdataset-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "87463feccfc4dc98bf76d92655970046", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18862, "upload_time": "2020-02-25T17:57:06", "upload_time_iso_8601": "2020-02-25T17:57:06.071164Z", "url": "https://files.pythonhosted.org/packages/3d/d4/a603fe8ecea32265a19a926fccd26b3ddccebabb52a9657c2555fdb4eb14/webdataset-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "097542c7ffb39a9fb3436c864f4ea477", "sha256": "9cb669794559089fda6bbde701473a60de3180c0623bf56c5559d9702a330fe6"}, "downloads": -1, "filename": "webdataset-0.1.2.tar.gz", "has_sig": false, "md5_digest": "097542c7ffb39a9fb3436c864f4ea477", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 18767, "upload_time": "2020-02-25T17:57:07", "upload_time_iso_8601": "2020-02-25T17:57:07.281168Z", "url": "https://files.pythonhosted.org/packages/e7/6e/89a09620082ec90789ba4a9a45a4fd25ec7ad7a56ef633e47a52032df28b/webdataset-0.1.2.tar.gz", "yanked": false}], "0.1.20": [{"comment_text": "", "digests": {"md5": "dc6bb633d87d6cda79079ad58ea5141c", "sha256": "47ba8af40a7a035f13acce0b557c0574c41574f2f903942b6df6fd1365f80e3a"}, "downloads": -1, "filename": "webdataset-0.1.20-py3-none-any.whl", "has_sig": false, "md5_digest": "dc6bb633d87d6cda79079ad58ea5141c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 28747, "upload_time": "2020-05-06T22:15:49", "upload_time_iso_8601": "2020-05-06T22:15:49.433386Z", "url": "https://files.pythonhosted.org/packages/e8/b4/d516c4493f1db9c9b9b6f6a735650f693894497f6961fdac7e01fd97fcab/webdataset-0.1.20-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3674273f9f7d00abb6a4607ae1cacb3d", "sha256": "03d321e90115f7a93f76b24ec43ad07a263cfba2b3781b104820121d7a560082"}, "downloads": -1, "filename": "webdataset-0.1.20.tar.gz", "has_sig": false, "md5_digest": "3674273f9f7d00abb6a4607ae1cacb3d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 28457, "upload_time": "2020-05-06T22:15:50", "upload_time_iso_8601": "2020-05-06T22:15:50.714679Z", "url": "https://files.pythonhosted.org/packages/51/98/9b36491b570a93b33c574b73c7424727eb461c9dfba7a6e21ff21799a321/webdataset-0.1.20.tar.gz", "yanked": false}], "0.1.21": [{"comment_text": "", "digests": {"md5": "c0c47191f7f6e12ac46f7f7fdd981681", "sha256": "0d8ac5cb678f2f7ba33beb80dbd24b4b0596033c188bfafe4d064f90fe61cb5b"}, "downloads": -1, "filename": "webdataset-0.1.21-py3-none-any.whl", "has_sig": false, "md5_digest": "c0c47191f7f6e12ac46f7f7fdd981681", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 28793, "upload_time": "2020-05-07T17:50:29", "upload_time_iso_8601": "2020-05-07T17:50:29.954717Z", "url": "https://files.pythonhosted.org/packages/5c/81/07e072bad8df4b7453136d9b6ab8d2ef3787ea554954e1596c47492259eb/webdataset-0.1.21-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f19381c43a6f361cde65919c8de5183f", "sha256": "8fe54b0d69f838605fa0a39a75e069943e4e2973c34e750cd8d85b7f0783aa5c"}, "downloads": -1, "filename": "webdataset-0.1.21.tar.gz", "has_sig": false, "md5_digest": "f19381c43a6f361cde65919c8de5183f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 28512, "upload_time": "2020-05-07T17:50:31", "upload_time_iso_8601": "2020-05-07T17:50:31.394146Z", "url": "https://files.pythonhosted.org/packages/c5/ed/3b820eae429b822643e8b90f068067f98b085d313b27d9f45e29f06f616e/webdataset-0.1.21.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "a71ff16e790024ea10dfa9fbc29d775a", "sha256": "550fa1bcf19061328f40ccc138e9572004a3391bf453ff7606353a1a18dc2899"}, "downloads": -1, "filename": "webdataset-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "a71ff16e790024ea10dfa9fbc29d775a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 19070, "upload_time": "2020-02-28T18:58:30", "upload_time_iso_8601": "2020-02-28T18:58:30.099406Z", "url": "https://files.pythonhosted.org/packages/60/2b/33c01088fc329a7a5fcdbd1ac348e2429903c87f08868dfcee4223cd163b/webdataset-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ad331c1815d2bf192289e7300e929504", "sha256": "cb5f7e6c22950763f03365351b780b48f0965f1c5d8e19fcdd876fb08583be49"}, "downloads": -1, "filename": "webdataset-0.1.3.tar.gz", "has_sig": false, "md5_digest": "ad331c1815d2bf192289e7300e929504", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 18995, "upload_time": "2020-02-28T18:58:31", "upload_time_iso_8601": "2020-02-28T18:58:31.424845Z", "url": "https://files.pythonhosted.org/packages/97/6f/c58500f763c4661a70f5396a8a502efe86537c0ea2dbedd97bd08d18bbfe/webdataset-0.1.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c0c47191f7f6e12ac46f7f7fdd981681", "sha256": "0d8ac5cb678f2f7ba33beb80dbd24b4b0596033c188bfafe4d064f90fe61cb5b"}, "downloads": -1, "filename": "webdataset-0.1.21-py3-none-any.whl", "has_sig": false, "md5_digest": "c0c47191f7f6e12ac46f7f7fdd981681", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 28793, "upload_time": "2020-05-07T17:50:29", "upload_time_iso_8601": "2020-05-07T17:50:29.954717Z", "url": "https://files.pythonhosted.org/packages/5c/81/07e072bad8df4b7453136d9b6ab8d2ef3787ea554954e1596c47492259eb/webdataset-0.1.21-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f19381c43a6f361cde65919c8de5183f", "sha256": "8fe54b0d69f838605fa0a39a75e069943e4e2973c34e750cd8d85b7f0783aa5c"}, "downloads": -1, "filename": "webdataset-0.1.21.tar.gz", "has_sig": false, "md5_digest": "f19381c43a6f361cde65919c8de5183f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 28512, "upload_time": "2020-05-07T17:50:31", "upload_time_iso_8601": "2020-05-07T17:50:31.394146Z", "url": "https://files.pythonhosted.org/packages/c5/ed/3b820eae429b822643e8b90f068067f98b085d313b27d9f45e29f06f616e/webdataset-0.1.21.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:31:20 2020"}