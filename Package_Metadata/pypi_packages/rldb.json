{"info": {"author": "Seungjae (Ryan) Lee", "author_email": "seungjaeryanlee@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "<img src=\"docs/rldb.png\" align=\"right\" width=\"20%\"/>\n\n# rldb\n\n[![Build Status](https://travis-ci.com/seungjaeryanlee/rldb.svg?branch=master)](https://travis-ci.com/seungjaeryanlee/rldb)\n\n![Environments tracked in rldb](https://img.shields.io/badge/environments-114-blue.svg)\n![Papers tracked in rldb](https://img.shields.io/badge/papers-22-blue.svg)\n![Repos tracked in rldb](https://img.shields.io/badge/repos-2-blue.svg)\n![Algorithms tracked in rldb](https://img.shields.io/badge/algorithms-59-blue.svg)\n![Entries tracked in rldb](https://img.shields.io/badge/entries-3266-blue.svg)\n\nDatabase of RL algorithms\n\n| Atari Space Invaders Scores | MuJoCo Walker2d Scores |\n|:-:|:-:|\n| ![Atari Space Invaders Scores](/docs/atari-space-invaders.png) | ![MuJoCo Walker2d Scores](/docs/mujoco-walker2d.png) |\n\n## Examples\n\nYou can use `rldb.find_all({})` to retrieve all existing entries in `rldb`.\n\n```python\nimport rldb\n\n\nall_entries = rldb.find_all({})\n```\n\nYou can also filter entries by specifying key-value pairs that the entry must match:\n\n```python\nimport rldb\n\n\ndqn_entries = rldb.find_all({'algo-nickname': 'DQN'})\nbreakout_noop_entries = rldb.find_all({\n    'env-title': 'atari-breakout',\n    'env-variant': 'No-op start',\n})\n```\n\nYou can also use `rldbl.find_one(filter_dict)` to find one entry that matches the key-value pair specified in `filter_dict`:\n\n```python\nimport rldb\nimport pprint\n\n\nentry = rldb.find_one({\n    'env-title': 'atari-pong',\n    'algo-title': 'Human',\n})\npprint.pprint(entry)\n```\n\n\n<details><summary>Output</summary>\n<p>\n\n```python\n{\n    'algo-nickname': 'Human',\n    'algo-title': 'Human',\n    'env-title': 'atari-pong',\n    'env-variant': 'No-op start',\n    'score': 14.6,\n    'source-arxiv-id': '1511.06581',\n    'source-arxiv-version': 3,\n    'source-authors': [   'Ziyu Wang',\n                          'Tom Schaul',\n                          'Matteo Hessel',\n                          'Hado van Hasselt',\n                          'Marc Lanctot',\n                          'Nando de Freitas'],\n    'source-bibtex': '@article{DBLP:journals/corr/WangFL15,\\n'\n                     '    author    = {Ziyu Wang and\\n'\n                     '                 Nando de Freitas and\\n'\n                     '                 Marc Lanctot},\\n'\n                     '    title     = {Dueling Network Architectures for Deep '\n                     'Reinforcement Learning},\\n'\n                     '    journal   = {CoRR},\\n'\n                     '    volume    = {abs/1511.06581},\\n'\n                     '    year      = {2015},\\n'\n                     '    url       = {http://arxiv.org/abs/1511.06581},\\n'\n                     '    archivePrefix = {arXiv},\\n'\n                     '    eprint    = {1511.06581},\\n'\n                     '    timestamp = {Mon, 13 Aug 2018 16:48:17 +0200},\\n'\n                     '    biburl    = '\n                     '{https://dblp.org/rec/bib/journals/corr/WangFL15},\\n'\n                     '    bibsource = {dblp computer science bibliography, '\n                     'https://dblp.org}\\n'\n                     '}',\n    'source-nickname': 'DuDQN',\n    'source-title': 'Dueling Network Architectures for Deep Reinforcement '\n                    'Learning'\n}\n```\n\n</p>\n</details>\n\n## Entry Structure\n\nHere is the format of every entry:\n\n```python3\n{\n    # BASICS\n    \"source-title\": \"\",\n    \"source-nickname\": \"\",\n    \"source-authors\": [],\n\n    # MISC.\n    \"source-bibtex\": \"\",\n\n    # ALGORITHM\n    \"algo-title\": \"\",\n    \"algo-nickname\": \"\",\n    \"algo-source-title\": \"\",\n\n    # SCORE\n    \"env-title\": \"\",\n    \"score\": 0,\n}\n```\n\n- `source-title` is the full title of the source of the score: it can be the title of the paper or GitHub repository title. `source-nickname` is a popular nickname or acronym for that title if it exists, otherwise it is the same as `source-title`. \n- `source-authors` are a list of authors or contributors.\n- `source-bibtex` is a BibTeX-format citation.\n- `algo-title` is the full title of the algorithm used. `algo-nickname` is the nickname or acronym for that algorithm if it exists, otherwise it is the same as `algo-nickname`.\n- `algo-source-title` is the title of the source of the **algorithm**. It can and often is different from `source-title`.\n\nFor example, the **Space Invaders** score of **Asynchronous Advantage Actor Critic (A3C)** algorithm in the **Noisy Networks for Exploration (NoisyNet)** paper is represented by the following entry:\n\n```python3\n{\n    #  BASICS\n    \"source-title\": \"Noisy Networks for Exploration\",\n    \"source-nickname\": \"NoisyNet\",\n    \"source-authors\": [\n        \"Meire Fortunato\",\n        \"Mohammad Gheshlaghi Azar\",\n        \"Bilal Piot\",\n        \"Jacob Menick\",\n        \"Ian Osband\",\n        \"Alex Graves\",\n        \"Vlad Mnih\",\n        \"Remi Munos\",\n        \"Demis Hassabis\",\n        \"Olivier Pietquin\",\n        \"Charles Blundell\",\n        \"Shane Legg\",\n    ],\n\n    #  ARXIV\n    \"source-arxiv-id\": \"1706.10295\",\n    \"source-arxiv-version\": 2,\n\n    #  MISC.\n    \"source-bibtex\": \"\"\"\n@article{DBLP:journals/corr/FortunatoAPMOGM17,\n    author    = {Meire Fortunato and\n                 Mohammad Gheshlaghi Azar and\n                 Bilal Piot and\n                 Jacob Menick and\n                 Ian Osband and\n                 Alex Graves and\n                 Vlad Mnih and\n                 R{\\'{e}}mi Munos and\n                 Demis Hassabis and\n                 Olivier Pietquin and\n                 Charles Blundell and\n                 Shane Legg},\n    title     = {Noisy Networks for Exploration},\n    journal   = {CoRR},\n    volume    = {abs/1706.10295},\n    year      = {2017},\n    url       = {http://arxiv.org/abs/1706.10295},\n    archivePrefix = {arXiv},\n    eprint    = {1706.10295},\n    timestamp = {Mon, 13 Aug 2018 16:46:11 +0200},\n    biburl    = {https://dblp.org/rec/bib/journals/corr/FortunatoAPMOGM17},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\"\"\",\n\n    # ALGORITHM\n    \"algo-title\": \"Asynchronous Advantage Actor Critic\",\n    \"algo-nickname\": \"A3C\",\n    \"algo-source-title\": \"Asynchronous Methods for Deep Reinforcement Learning\",\n\n    # HYPERPARAMETERS\n    \"algo-frames\": 320 * 1000 * 1000,  # Number of frames\n\n    # SCORE\n    \"env-title\": \"atari-space-invaders\",\n    \"env-variant\": \"No-op start\",\n    \"score\": 1034,\n    \"stddev\": 49,\n}\n```\n\nNote that, as shown here, the entry can contain additional information.\n\n## Sources\n\n### Papers\n\n#### Deep Q-Networks\n\n- [x] [Playing Atari with Deep Reinforcement Learning (Mnih et al., 2013)](https://arxiv.org/abs/1312.5602)\n- [x] [Human-level control through deep reinforcement learning (Mnih et al., 2015)](https://deepmind.com/research/dqn/)\n- [x] [Deep Recurrent Q-Learning for Partially Observable MDPs (Hausknecht and Stone, 2015)](https://arxiv.org/abs/1507.06527)\n- [x] [Massively Parallel Methods for Deep Reinforcement Learning (Nair et al., 2015)](https://arxiv.org/abs/1507.04296)\n- [x] [Deep Reinforcement Learning with Double Q-learning (Hasselt et al., 2015)](https://arxiv.org/abs/1509.06461)\n- [x] [Prioritized Experience Replay (Schaul et al., 2015)](https://arxiv.org/abs/1511.05952)\n- [x] [Dueling Network Architectures for Deep Reinforcement Learning (Wang et al., 2015)](https://arxiv.org/abs/1511.06581)\n- [x] [Noisy Networks for Exploration (Fortunato et al., 2017)](https://arxiv.org/abs/1706.10295)\n- [x] [A Distributional Perspective on Reinforcement Learning (Bellemare et al., 2017)](https://arxiv.org/abs/1707.06887)\n- [x] [Rainbow: Combining Improvements in Deep Reinforcement Learning (Hessel et al., 2017)](https://arxiv.org/abs/1710.02298)\n- [x] [Distributional Reinforcement Learning with Quantile Regression (Dabney et al., 2017)](https://arxiv.org/abs/1710.10044)\n- [x] [Implicit Quantile Networks for Distributional Reinforcement Learning (Dabney et al., 2018)](https://arxiv.org/abs/1806.06923)\n\n#### Policy Gradients\n\n- [x] [Asynchronous Methods for Deep Reinforcement Learning (Mnih et al., 2016)](https://arxiv.org/abs/1602.01783)\n- [x] [Trust Region Policy Optimization (Schulman et al., 2015)](https://arxiv.org/abs/1502.05477)\n- [x] [Proximal Policy Optimization Algorithms (Schulman et al., 2017)](https://arxiv.org/abs/1707.06347)\n- [x] [Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (Wu et al., 2017)](https://arxiv.org/abs/1708.05144)\n- [x] [Addressing Function Approximation Error in Actor-Critic Methods (Fujimoto et al., 2018)](https://arxiv.org/abs/1802.09477)\n- [x] [IMPALA: Importance Weighted Actor-Learner Architectures (Espeholt et al., 2018)](https://arxiv.org/abs/1802.01561)\n- [x] [The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning (Gruslys et al., 2017)](https://arxiv.org/abs/1704.04651)\n\n#### Exploration\n\n- [x] [Exploration by Random Network Distillation (Burda et al., 2018)](https://arxiv.org/abs/1810.12894)\n\n#### Misc.\n\n- [x] [Trust-PCL: An Off-Policy Trust Region Method for Continuous Control (Nachum et al., 2017)](https://arxiv.org/abs/1707.01891)\n\n### Repositories\n\n- [x] [OpenAI Baselines](https://github.com/openai/baselines)\n- [x] [RL Baselines Zoo](https://github.com/araffin/rl-baselines-zoo)\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/seungjaeryanlee/rldb", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "rldb", "package_url": "https://pypi.org/project/rldb/", "platform": "", "project_url": "https://pypi.org/project/rldb/", "project_urls": {"Homepage": "https://github.com/seungjaeryanlee/rldb"}, "release_url": "https://pypi.org/project/rldb/0.0.0/", "requires_dist": null, "requires_python": "", "summary": "Performances of Reinforcement Learning Agents", "version": "0.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <img align=\"right\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d0515511578eafcd8d16356565c8ce6ab9b59536/646f63732f726c64622e706e67\" width=\"20%\">\n<h1>rldb</h1>\n<p><a href=\"https://travis-ci.com/seungjaeryanlee/rldb\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3c029828555e4e8e6736af40869ac20ee3d7d64e/68747470733a2f2f7472617669732d63692e636f6d2f7365756e676a61657279616e6c65652f726c64622e7376673f6272616e63683d6d6173746572\"></a></p>\n<p><img alt=\"Environments tracked in rldb\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/26df6960b0f7a0426a95be76843f8c6cf3f9d465/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f656e7669726f6e6d656e74732d3131342d626c75652e737667\">\n<img alt=\"Papers tracked in rldb\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e41ef2de25908cb052293b09cad379b5364800c8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7061706572732d32322d626c75652e737667\">\n<img alt=\"Repos tracked in rldb\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/eefa133bd092ec74eb0a674d1f01f3b1ddde131d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7265706f732d322d626c75652e737667\">\n<img alt=\"Algorithms tracked in rldb\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5cedea00cdcff3fe8ae241f66f34850617e4511c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f616c676f726974686d732d35392d626c75652e737667\">\n<img alt=\"Entries tracked in rldb\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5c2a1ba544d63c48422fd2c6e31e1ceff1956bcb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f656e74726965732d333236362d626c75652e737667\"></p>\n<p>Database of RL algorithms</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Atari Space Invaders Scores</th>\n<th align=\"center\">MuJoCo Walker2d Scores</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\"><img alt=\"Atari Space Invaders Scores\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b9b9a86ba8bd7447041375aba6274534a75dde72/2f646f63732f61746172692d73706163652d696e7661646572732e706e67\"></td>\n<td align=\"center\"><img alt=\"MuJoCo Walker2d Scores\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/985e4d40261f3c589d4acddab24448f78ab45884/2f646f63732f6d756a6f636f2d77616c6b657232642e706e67\"></td>\n</tr></tbody></table>\n<h2>Examples</h2>\n<p>You can use <code>rldb.find_all({})</code> to retrieve all existing entries in <code>rldb</code>.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">rldb</span>\n\n\n<span class=\"n\">all_entries</span> <span class=\"o\">=</span> <span class=\"n\">rldb</span><span class=\"o\">.</span><span class=\"n\">find_all</span><span class=\"p\">({})</span>\n</pre>\n<p>You can also filter entries by specifying key-value pairs that the entry must match:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">rldb</span>\n\n\n<span class=\"n\">dqn_entries</span> <span class=\"o\">=</span> <span class=\"n\">rldb</span><span class=\"o\">.</span><span class=\"n\">find_all</span><span class=\"p\">({</span><span class=\"s1\">'algo-nickname'</span><span class=\"p\">:</span> <span class=\"s1\">'DQN'</span><span class=\"p\">})</span>\n<span class=\"n\">breakout_noop_entries</span> <span class=\"o\">=</span> <span class=\"n\">rldb</span><span class=\"o\">.</span><span class=\"n\">find_all</span><span class=\"p\">({</span>\n    <span class=\"s1\">'env-title'</span><span class=\"p\">:</span> <span class=\"s1\">'atari-breakout'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'env-variant'</span><span class=\"p\">:</span> <span class=\"s1\">'No-op start'</span><span class=\"p\">,</span>\n<span class=\"p\">})</span>\n</pre>\n<p>You can also use <code>rldbl.find_one(filter_dict)</code> to find one entry that matches the key-value pair specified in <code>filter_dict</code>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">rldb</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pprint</span>\n\n\n<span class=\"n\">entry</span> <span class=\"o\">=</span> <span class=\"n\">rldb</span><span class=\"o\">.</span><span class=\"n\">find_one</span><span class=\"p\">({</span>\n    <span class=\"s1\">'env-title'</span><span class=\"p\">:</span> <span class=\"s1\">'atari-pong'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'algo-title'</span><span class=\"p\">:</span> <span class=\"s1\">'Human'</span><span class=\"p\">,</span>\n<span class=\"p\">})</span>\n<span class=\"n\">pprint</span><span class=\"o\">.</span><span class=\"n\">pprint</span><span class=\"p\">(</span><span class=\"n\">entry</span><span class=\"p\">)</span>\n</pre>\n<details><summary>Output</summary>\n<p>\n</p><pre><span class=\"p\">{</span>\n    <span class=\"s1\">'algo-nickname'</span><span class=\"p\">:</span> <span class=\"s1\">'Human'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'algo-title'</span><span class=\"p\">:</span> <span class=\"s1\">'Human'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'env-title'</span><span class=\"p\">:</span> <span class=\"s1\">'atari-pong'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'env-variant'</span><span class=\"p\">:</span> <span class=\"s1\">'No-op start'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'score'</span><span class=\"p\">:</span> <span class=\"mf\">14.6</span><span class=\"p\">,</span>\n    <span class=\"s1\">'source-arxiv-id'</span><span class=\"p\">:</span> <span class=\"s1\">'1511.06581'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'source-arxiv-version'</span><span class=\"p\">:</span> <span class=\"mi\">3</span><span class=\"p\">,</span>\n    <span class=\"s1\">'source-authors'</span><span class=\"p\">:</span> <span class=\"p\">[</span>   <span class=\"s1\">'Ziyu Wang'</span><span class=\"p\">,</span>\n                          <span class=\"s1\">'Tom Schaul'</span><span class=\"p\">,</span>\n                          <span class=\"s1\">'Matteo Hessel'</span><span class=\"p\">,</span>\n                          <span class=\"s1\">'Hado van Hasselt'</span><span class=\"p\">,</span>\n                          <span class=\"s1\">'Marc Lanctot'</span><span class=\"p\">,</span>\n                          <span class=\"s1\">'Nando de Freitas'</span><span class=\"p\">],</span>\n    <span class=\"s1\">'source-bibtex'</span><span class=\"p\">:</span> <span class=\"s1\">'@article{DBLP:journals/corr/WangFL15,</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'    author    = {Ziyu Wang and</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'                 Nando de Freitas and</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'                 Marc Lanctot},</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'    title     = {Dueling Network Architectures for Deep '</span>\n                     <span class=\"s1\">'Reinforcement Learning},</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'    journal   = </span><span class=\"si\">{CoRR}</span><span class=\"s1\">,</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'    volume    = {abs/1511.06581},</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'    year      = </span><span class=\"si\">{2015}</span><span class=\"s1\">,</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'    url       = {http://arxiv.org/abs/1511.06581},</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'    archivePrefix = </span><span class=\"si\">{arXiv}</span><span class=\"s1\">,</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'    eprint    = </span><span class=\"si\">{1511.06581}</span><span class=\"s1\">,</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'    timestamp = {Mon, 13 Aug 2018 16:48:17 +0200},</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'    biburl    = '</span>\n                     <span class=\"s1\">'{https://dblp.org/rec/bib/journals/corr/WangFL15},</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'    bibsource = {dblp computer science bibliography, '</span>\n                     <span class=\"s1\">'https://dblp.org}</span><span class=\"se\">\\n</span><span class=\"s1\">'</span>\n                     <span class=\"s1\">'}'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'source-nickname'</span><span class=\"p\">:</span> <span class=\"s1\">'DuDQN'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'source-title'</span><span class=\"p\">:</span> <span class=\"s1\">'Dueling Network Architectures for Deep Reinforcement '</span>\n                    <span class=\"s1\">'Learning'</span>\n<span class=\"p\">}</span>\n</pre>\n<p></p>\n</details>\n<h2>Entry Structure</h2>\n<p>Here is the format of every entry:</p>\n<pre><span class=\"p\">{</span>\n    <span class=\"c1\"># BASICS</span>\n    <span class=\"s2\">\"source-title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"source-nickname\"</span><span class=\"p\">:</span> <span class=\"s2\">\"\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"source-authors\"</span><span class=\"p\">:</span> <span class=\"p\">[],</span>\n\n    <span class=\"c1\"># MISC.</span>\n    <span class=\"s2\">\"source-bibtex\"</span><span class=\"p\">:</span> <span class=\"s2\">\"\"</span><span class=\"p\">,</span>\n\n    <span class=\"c1\"># ALGORITHM</span>\n    <span class=\"s2\">\"algo-title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"algo-nickname\"</span><span class=\"p\">:</span> <span class=\"s2\">\"\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"algo-source-title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"\"</span><span class=\"p\">,</span>\n\n    <span class=\"c1\"># SCORE</span>\n    <span class=\"s2\">\"env-title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"score\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n</pre>\n<ul>\n<li><code>source-title</code> is the full title of the source of the score: it can be the title of the paper or GitHub repository title. <code>source-nickname</code> is a popular nickname or acronym for that title if it exists, otherwise it is the same as <code>source-title</code>.</li>\n<li><code>source-authors</code> are a list of authors or contributors.</li>\n<li><code>source-bibtex</code> is a BibTeX-format citation.</li>\n<li><code>algo-title</code> is the full title of the algorithm used. <code>algo-nickname</code> is the nickname or acronym for that algorithm if it exists, otherwise it is the same as <code>algo-nickname</code>.</li>\n<li><code>algo-source-title</code> is the title of the source of the <strong>algorithm</strong>. It can and often is different from <code>source-title</code>.</li>\n</ul>\n<p>For example, the <strong>Space Invaders</strong> score of <strong>Asynchronous Advantage Actor Critic (A3C)</strong> algorithm in the <strong>Noisy Networks for Exploration (NoisyNet)</strong> paper is represented by the following entry:</p>\n<pre><span class=\"p\">{</span>\n    <span class=\"c1\">#  BASICS</span>\n    <span class=\"s2\">\"source-title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Noisy Networks for Exploration\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"source-nickname\"</span><span class=\"p\">:</span> <span class=\"s2\">\"NoisyNet\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"source-authors\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"Meire Fortunato\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"Mohammad Gheshlaghi Azar\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"Bilal Piot\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"Jacob Menick\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"Ian Osband\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"Alex Graves\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"Vlad Mnih\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"Remi Munos\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"Demis Hassabis\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"Olivier Pietquin\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"Charles Blundell\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"Shane Legg\"</span><span class=\"p\">,</span>\n    <span class=\"p\">],</span>\n\n    <span class=\"c1\">#  ARXIV</span>\n    <span class=\"s2\">\"source-arxiv-id\"</span><span class=\"p\">:</span> <span class=\"s2\">\"1706.10295\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"source-arxiv-version\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n\n    <span class=\"c1\">#  MISC.</span>\n    <span class=\"s2\">\"source-bibtex\"</span><span class=\"p\">:</span> <span class=\"s2\">\"\"\"</span>\n<span class=\"s2\">@article{DBLP:journals/corr/FortunatoAPMOGM17,</span>\n<span class=\"s2\">    author    = {Meire Fortunato and</span>\n<span class=\"s2\">                 Mohammad Gheshlaghi Azar and</span>\n<span class=\"s2\">                 Bilal Piot and</span>\n<span class=\"s2\">                 Jacob Menick and</span>\n<span class=\"s2\">                 Ian Osband and</span>\n<span class=\"s2\">                 Alex Graves and</span>\n<span class=\"s2\">                 Vlad Mnih and</span>\n<span class=\"s2\">                 R{</span><span class=\"se\">\\'</span><span class=\"si\">{e}</span><span class=\"s2\">}mi Munos and</span>\n<span class=\"s2\">                 Demis Hassabis and</span>\n<span class=\"s2\">                 Olivier Pietquin and</span>\n<span class=\"s2\">                 Charles Blundell and</span>\n<span class=\"s2\">                 Shane Legg},</span>\n<span class=\"s2\">    title     = {Noisy Networks for Exploration},</span>\n<span class=\"s2\">    journal   = </span><span class=\"si\">{CoRR}</span><span class=\"s2\">,</span>\n<span class=\"s2\">    volume    = {abs/1706.10295},</span>\n<span class=\"s2\">    year      = </span><span class=\"si\">{2017}</span><span class=\"s2\">,</span>\n<span class=\"s2\">    url       = {http://arxiv.org/abs/1706.10295},</span>\n<span class=\"s2\">    archivePrefix = </span><span class=\"si\">{arXiv}</span><span class=\"s2\">,</span>\n<span class=\"s2\">    eprint    = </span><span class=\"si\">{1706.10295}</span><span class=\"s2\">,</span>\n<span class=\"s2\">    timestamp = {Mon, 13 Aug 2018 16:46:11 +0200},</span>\n<span class=\"s2\">    biburl    = {https://dblp.org/rec/bib/journals/corr/FortunatoAPMOGM17},</span>\n<span class=\"s2\">    bibsource = {dblp computer science bibliography, https://dblp.org}</span>\n<span class=\"s2\">}\"\"\"</span><span class=\"p\">,</span>\n\n    <span class=\"c1\"># ALGORITHM</span>\n    <span class=\"s2\">\"algo-title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Asynchronous Advantage Actor Critic\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"algo-nickname\"</span><span class=\"p\">:</span> <span class=\"s2\">\"A3C\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"algo-source-title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Asynchronous Methods for Deep Reinforcement Learning\"</span><span class=\"p\">,</span>\n\n    <span class=\"c1\"># HYPERPARAMETERS</span>\n    <span class=\"s2\">\"algo-frames\"</span><span class=\"p\">:</span> <span class=\"mi\">320</span> <span class=\"o\">*</span> <span class=\"mi\">1000</span> <span class=\"o\">*</span> <span class=\"mi\">1000</span><span class=\"p\">,</span>  <span class=\"c1\"># Number of frames</span>\n\n    <span class=\"c1\"># SCORE</span>\n    <span class=\"s2\">\"env-title\"</span><span class=\"p\">:</span> <span class=\"s2\">\"atari-space-invaders\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"env-variant\"</span><span class=\"p\">:</span> <span class=\"s2\">\"No-op start\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"score\"</span><span class=\"p\">:</span> <span class=\"mi\">1034</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"stddev\"</span><span class=\"p\">:</span> <span class=\"mi\">49</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n</pre>\n<p>Note that, as shown here, the entry can contain additional information.</p>\n<h2>Sources</h2>\n<h3>Papers</h3>\n<h4>Deep Q-Networks</h4>\n<ul>\n<li>[x] <a href=\"https://arxiv.org/abs/1312.5602\" rel=\"nofollow\">Playing Atari with Deep Reinforcement Learning (Mnih et al., 2013)</a></li>\n<li>[x] <a href=\"https://deepmind.com/research/dqn/\" rel=\"nofollow\">Human-level control through deep reinforcement learning (Mnih et al., 2015)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1507.06527\" rel=\"nofollow\">Deep Recurrent Q-Learning for Partially Observable MDPs (Hausknecht and Stone, 2015)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1507.04296\" rel=\"nofollow\">Massively Parallel Methods for Deep Reinforcement Learning (Nair et al., 2015)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1509.06461\" rel=\"nofollow\">Deep Reinforcement Learning with Double Q-learning (Hasselt et al., 2015)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1511.05952\" rel=\"nofollow\">Prioritized Experience Replay (Schaul et al., 2015)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1511.06581\" rel=\"nofollow\">Dueling Network Architectures for Deep Reinforcement Learning (Wang et al., 2015)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1706.10295\" rel=\"nofollow\">Noisy Networks for Exploration (Fortunato et al., 2017)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1707.06887\" rel=\"nofollow\">A Distributional Perspective on Reinforcement Learning (Bellemare et al., 2017)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1710.02298\" rel=\"nofollow\">Rainbow: Combining Improvements in Deep Reinforcement Learning (Hessel et al., 2017)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1710.10044\" rel=\"nofollow\">Distributional Reinforcement Learning with Quantile Regression (Dabney et al., 2017)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1806.06923\" rel=\"nofollow\">Implicit Quantile Networks for Distributional Reinforcement Learning (Dabney et al., 2018)</a></li>\n</ul>\n<h4>Policy Gradients</h4>\n<ul>\n<li>[x] <a href=\"https://arxiv.org/abs/1602.01783\" rel=\"nofollow\">Asynchronous Methods for Deep Reinforcement Learning (Mnih et al., 2016)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1502.05477\" rel=\"nofollow\">Trust Region Policy Optimization (Schulman et al., 2015)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1707.06347\" rel=\"nofollow\">Proximal Policy Optimization Algorithms (Schulman et al., 2017)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1708.05144\" rel=\"nofollow\">Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (Wu et al., 2017)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1802.09477\" rel=\"nofollow\">Addressing Function Approximation Error in Actor-Critic Methods (Fujimoto et al., 2018)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1802.01561\" rel=\"nofollow\">IMPALA: Importance Weighted Actor-Learner Architectures (Espeholt et al., 2018)</a></li>\n<li>[x] <a href=\"https://arxiv.org/abs/1704.04651\" rel=\"nofollow\">The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning (Gruslys et al., 2017)</a></li>\n</ul>\n<h4>Exploration</h4>\n<ul>\n<li>[x] <a href=\"https://arxiv.org/abs/1810.12894\" rel=\"nofollow\">Exploration by Random Network Distillation (Burda et al., 2018)</a></li>\n</ul>\n<h4>Misc.</h4>\n<ul>\n<li>[x] <a href=\"https://arxiv.org/abs/1707.01891\" rel=\"nofollow\">Trust-PCL: An Off-Policy Trust Region Method for Continuous Control (Nachum et al., 2017)</a></li>\n</ul>\n<h3>Repositories</h3>\n<ul>\n<li>[x] <a href=\"https://github.com/openai/baselines\" rel=\"nofollow\">OpenAI Baselines</a></li>\n<li>[x] <a href=\"https://github.com/araffin/rl-baselines-zoo\" rel=\"nofollow\">RL Baselines Zoo</a></li>\n</ul>\n\n          </div>"}, "last_serial": 5275040, "releases": {"0.0.0": [{"comment_text": "", "digests": {"md5": "3c33243225adc13b22171d58b7a51343", "sha256": "ec2ebf98140759edf515c7f2274b802ea4c12483dd109f4a814ba04aa9a51cfb"}, "downloads": -1, "filename": "rldb-0.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3c33243225adc13b22171d58b7a51343", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 146571, "upload_time": "2019-05-16T02:06:01", "upload_time_iso_8601": "2019-05-16T02:06:01.485841Z", "url": "https://files.pythonhosted.org/packages/c7/ab/95db1d80c06b53b232569953cf12baa4ba3f3f60666124ae98583b713be2/rldb-0.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f662d95c7f10bf62184972bd8acfbbb7", "sha256": "39a7499a04882800c2421a30af2efebb2854c69b75629ff5058084c4766d0817"}, "downloads": -1, "filename": "rldb-0.0.0.tar.gz", "has_sig": false, "md5_digest": "f662d95c7f10bf62184972bd8acfbbb7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59185, "upload_time": "2019-05-16T02:06:05", "upload_time_iso_8601": "2019-05-16T02:06:05.061615Z", "url": "https://files.pythonhosted.org/packages/6d/59/ece9626bc9e433562490fefe956c09f097bd4b142b37475c21e7af3dd85e/rldb-0.0.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3c33243225adc13b22171d58b7a51343", "sha256": "ec2ebf98140759edf515c7f2274b802ea4c12483dd109f4a814ba04aa9a51cfb"}, "downloads": -1, "filename": "rldb-0.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3c33243225adc13b22171d58b7a51343", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 146571, "upload_time": "2019-05-16T02:06:01", "upload_time_iso_8601": "2019-05-16T02:06:01.485841Z", "url": "https://files.pythonhosted.org/packages/c7/ab/95db1d80c06b53b232569953cf12baa4ba3f3f60666124ae98583b713be2/rldb-0.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f662d95c7f10bf62184972bd8acfbbb7", "sha256": "39a7499a04882800c2421a30af2efebb2854c69b75629ff5058084c4766d0817"}, "downloads": -1, "filename": "rldb-0.0.0.tar.gz", "has_sig": false, "md5_digest": "f662d95c7f10bf62184972bd8acfbbb7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59185, "upload_time": "2019-05-16T02:06:05", "upload_time_iso_8601": "2019-05-16T02:06:05.061615Z", "url": "https://files.pythonhosted.org/packages/6d/59/ece9626bc9e433562490fefe956c09f097bd4b142b37475c21e7af3dd85e/rldb-0.0.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:02:18 2020"}