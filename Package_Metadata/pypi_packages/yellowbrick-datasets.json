{"info": {"author": "Rebecca Bilbro, Benjamin Bengfort", "author_email": "yellowbrick@googlegroups.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3.6", "Topic :: Software Development", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# Yellowbrick Datasets\n**Yellowbrick datasets management and deployment scripts.**\n\nYellowbrick datasets are hosted in an S3 drive in the cloud to allow easy access to the data for examples. This repository manages those datasets, their data structure, and interactions with the cloud.\n\n## Getting Started\n\nThe `ybdata` script is installed as an entry point in `setup.py`. You can install the package and the script using `pip install yellowbrick-data`. If you've downloaded the source code from GitHub you can install the app using editable mode with pip. In the current working directory of the project, use:\n\n```\n$ pip install -e .\n```\n\nAt this point you should have a `ybdata` command on your `$PATH`. Like git, this utility has many subcommands for various data related management tasks. To see a list of the commands and their descriptions:\n\n```\n$ ybdata --help\n```\n\n## Datasets Basics\n\nAll datasets must have the following properties:\n\n- a unique name that identifies the dataset to a user (e.g. \"bikeshare\")\n- a README.md that describes the provenance and contents of the data\n- one or more data files that can be read by the yellowbrick library\n- an optional citation.bib file to cite the source of the data\n\nDatasets are stored in the `fixtures/` directory in a subdirectory with the same name as the dataset. This subdirectory contains both the data and metadata files that make up the data package structure. The `uploads/` directory contains the most recent version of the compressed datasets found in the `fixtures/` directory and is the content that is uploaded to S3 for use in Yellowbrick.\n\nCurrently there are two kinds of datasets:\n\n1. Standard: A single data table containing both features and targets.\n2. Corpus: A text corpus for natural language processing.\n\nBoth kinds of datasets have their own specific package structures as defined in the following sections. Note that the `ybdata validate` command can be used to check if a dataset is ready to be uploaded.\n\n### Standard Datasets\n\nA standard dataset is composed of a single data table that can be loaded into a data frame or numpy array for machine learning with scikit-learn. In addition to the files mentioned in dataset basics, the data and metadata files that make up the standard dataset package are as follows (where \"name\" is the unique dataset name):\n\n- `fixtures/name/name.csv.gz`: The gzip compressed CSV file _with header row_ to be loaded with `pd.read_csv` (no index column).\n- `fixtures/name/name.npz`: The compressed numpy matrix representation of `X` and `y` to be loaded with `np.load`.\n- `fixtures/name/meta.json`: A metadata file that identifies the features and the target column names of the data in the CSV file.\n\nConsider the following example CSV file:\n\n```csv\ndatetime,temperature,relative humidity,light,CO2,humidity,occupancy\n2015-02-04 17:51:00,23.18,27.272,426,721.25,0.00479298817650529,1\n2015-02-04 17:51:59,23.15,27.2675,429.5,714,0.00478344094931065,1\n2015-02-04 17:53:00,23.15,27.245,426,713.5,0.00477946352442199,1\n2015-02-04 17:54:00,23.15,27.2,426,708.25,0.00477150882608175,1\n2015-02-04 17:55:00,23.1,27.2,426,704.5,0.00475699293331518,1\n2015-02-04 17:55:59,23.1,27.2,419,701,0.00475699293331518,1\n2015-02-04 17:57:00,23.1,27.2,419,701.666666666667,0.00475699293331518,1\n2015-02-04 17:57:59,23.1,27.2,419,699,0.00475699293331518,1\n2015-02-04 17:58:59,23.1,27.2,419,689.333333333333,0.00475699293331518,1\n```\n\nAn example `meta.json` for this file would be as follows:\n\n```json\n{\n  \"features\": [\n      \"temperature\",\n      \"relative humidity\",\n      \"light\",\n      \"CO2\",\n      \"humidity\",\n  ],\n  \"target\": \"occupancy\",\n  \"labels\": {\n    \"occupied\": 1,\n    \"not occupied\": 0\n  }\n}\n```\n\nThis will ensure that the dataset `X` is a `pd.DataFrame` with columns corresponding to the features list and that `y` is a `pd.Series` from the column described in the `target` key. The `labels` key is used to transform numerically encoded categorical variables for a classification target.\n\n### Corpus Datasets\n\nA corpus dataset contains plain text files stored in subdirectories of the dataset directory that correspond to the class or category the plain text files belong to. Note that these text files should be only one level deep and that each document should be stored in its own file.\n\nAt the momement, individual corpus files should be uncompressed (the directory as a whole is compressed). The text corpus is read similarly to the following:\n\n```python\nimport os\nimport glob\n\npaths = os.path.join(data_dir, \"*\", \"*.txt\")\ndocuments = glob.glob(paths)\nlabels = [os.path.basename(os.path.dirname(path)) for path in documents]\n```\n\nDocuments and labels can then be directly passed to scikit-learn text feature extraction transformers for analysis.\n\n## Creating and Uploading Datasets\n\nThis section outlines how to create and upload a dataset for use in Yellowbrick examples and testing. More detailed steps follow, but in brief here is a sketch required for the actions to take to package a dataset:\n\n1. Create a dataset in `fixtures/`\n2. Convert dataset to all appropriate types using `ybdata convert`\n3. Validate the dataset is ready using `ybdata validate`\n4. Package the dataset using `ybdata package`\n5. Upload the dataset using `ybdata upload`\n6. Update `yellowbrick.datasets` with `uploads/manifest.json`\n\nMost of the datasets in this repository are from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). A basic methodology for creating a repository is to use the unique name of the UCIML Repository as the unique name of the data set to store in `fixtures/`. Wrangle the data so that it exists as a pandas-readable CSV file with a header row (usually by joining the target with the features or extracting data from a TSV, etc). Make sure that the CSV is gzip-compressed when done!\n\nOnce the pandas CSV file is created, create the README.md, meta.json, and citation.bib files manually. It is usually also fairly simple to copy and paste the README.md from the UCIML page description (wrangling it where necessary to include as many details as possible).  The citation.bib file can be found by searching with Google Scholar and selecting \"cite as bibtex\". The meta.json usually has to be manually written. Once done, you can convert the CSV into the `.npz` objects using `ybdata convert` as follows:\n\n```\n$ ybdata convert fixtures/mydata/mydata.csv.gz fixtures/mydata/mydata.npz\n```\n\nNote that you can go from .npz to csv.gz asa well, but it is usually easier to go in the reverse direction when wrangling.\n\nOnce done, validate that the dataset is ready to be packaged using:\n\n```\n$ ybdata validate fixtures/mydata\n```\n\nThis should print out a table of both required and optional items for validation, and the validation status should be listed at the bottom. Once validated, convert the dataset into a package:\n\n```\n$ ybdata convert fixtures/mydata\n```\n\nBy default this will create a package in `uploads/mydata.zip` and update the `uploads/manifest.json` with the package and signature information. Note if you're updating a previously created dataset, you can use the `-f` flag to overwrite the old data and create a new package.\n\nFinally upload the datasets to our S3 storage in the cloud. You will need proper AWS access keys to do this (see the environment or aws-configure options). If you would like to upload the datasets elsewhere, use the `--bucket` flag.\n\n```\n$ ybdata upload --pending v1.0\n```\n\nThe upload process also updates the `uploads/manifold.json` with the final download URL and in a format that can be added to the Yellowbrick library. Make sure the yellowbrick library is updated in the correct Yellowbrick version, otherwise YB downloads will fail!\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/districtdatalabs/yellowbrick-datasets/tarball/v1.0", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/districtdatalabs/yellowbrick-datasets", "keywords": "datasets,machine learning,scikit-learn,matplotlib,data science", "license": "Apache 2", "maintainer": "Benjamin Bengfort", "maintainer_email": "yellowbrick@googlegroups.com", "name": "yellowbrick-datasets", "package_url": "https://pypi.org/project/yellowbrick-datasets/", "platform": "", "project_url": "https://pypi.org/project/yellowbrick-datasets/", "project_urls": {"Download": "https://github.com/districtdatalabs/yellowbrick-datasets/tarball/v1.0", "Homepage": "https://github.com/districtdatalabs/yellowbrick-datasets"}, "release_url": "https://pypi.org/project/yellowbrick-datasets/1.0/", "requires_dist": ["boto3 (==1.9.71)", "commis (==0.4)", "numpy (==1.15.4)", "pandas (==0.23.4)", "tabulate (==0.8.2)", "tqdm (==4.28.1)"], "requires_python": ">=3.4", "summary": "Yellowbrick datasets management and deployment scripts.", "version": "1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Yellowbrick Datasets</h1>\n<p><strong>Yellowbrick datasets management and deployment scripts.</strong></p>\n<p>Yellowbrick datasets are hosted in an S3 drive in the cloud to allow easy access to the data for examples. This repository manages those datasets, their data structure, and interactions with the cloud.</p>\n<h2>Getting Started</h2>\n<p>The <code>ybdata</code> script is installed as an entry point in <code>setup.py</code>. You can install the package and the script using <code>pip install yellowbrick-data</code>. If you've downloaded the source code from GitHub you can install the app using editable mode with pip. In the current working directory of the project, use:</p>\n<pre><code>$ pip install -e .\n</code></pre>\n<p>At this point you should have a <code>ybdata</code> command on your <code>$PATH</code>. Like git, this utility has many subcommands for various data related management tasks. To see a list of the commands and their descriptions:</p>\n<pre><code>$ ybdata --help\n</code></pre>\n<h2>Datasets Basics</h2>\n<p>All datasets must have the following properties:</p>\n<ul>\n<li>a unique name that identifies the dataset to a user (e.g. \"bikeshare\")</li>\n<li>a README.md that describes the provenance and contents of the data</li>\n<li>one or more data files that can be read by the yellowbrick library</li>\n<li>an optional citation.bib file to cite the source of the data</li>\n</ul>\n<p>Datasets are stored in the <code>fixtures/</code> directory in a subdirectory with the same name as the dataset. This subdirectory contains both the data and metadata files that make up the data package structure. The <code>uploads/</code> directory contains the most recent version of the compressed datasets found in the <code>fixtures/</code> directory and is the content that is uploaded to S3 for use in Yellowbrick.</p>\n<p>Currently there are two kinds of datasets:</p>\n<ol>\n<li>Standard: A single data table containing both features and targets.</li>\n<li>Corpus: A text corpus for natural language processing.</li>\n</ol>\n<p>Both kinds of datasets have their own specific package structures as defined in the following sections. Note that the <code>ybdata validate</code> command can be used to check if a dataset is ready to be uploaded.</p>\n<h3>Standard Datasets</h3>\n<p>A standard dataset is composed of a single data table that can be loaded into a data frame or numpy array for machine learning with scikit-learn. In addition to the files mentioned in dataset basics, the data and metadata files that make up the standard dataset package are as follows (where \"name\" is the unique dataset name):</p>\n<ul>\n<li><code>fixtures/name/name.csv.gz</code>: The gzip compressed CSV file <em>with header row</em> to be loaded with <code>pd.read_csv</code> (no index column).</li>\n<li><code>fixtures/name/name.npz</code>: The compressed numpy matrix representation of <code>X</code> and <code>y</code> to be loaded with <code>np.load</code>.</li>\n<li><code>fixtures/name/meta.json</code>: A metadata file that identifies the features and the target column names of the data in the CSV file.</li>\n</ul>\n<p>Consider the following example CSV file:</p>\n<pre>datetime,temperature,relative humidity,light,CO2,humidity,occupancy\n2015-02-04 17:51:00,23.18,27.272,426,721.25,0.00479298817650529,1\n2015-02-04 17:51:59,23.15,27.2675,429.5,714,0.00478344094931065,1\n2015-02-04 17:53:00,23.15,27.245,426,713.5,0.00477946352442199,1\n2015-02-04 17:54:00,23.15,27.2,426,708.25,0.00477150882608175,1\n2015-02-04 17:55:00,23.1,27.2,426,704.5,0.00475699293331518,1\n2015-02-04 17:55:59,23.1,27.2,419,701,0.00475699293331518,1\n2015-02-04 17:57:00,23.1,27.2,419,701.666666666667,0.00475699293331518,1\n2015-02-04 17:57:59,23.1,27.2,419,699,0.00475699293331518,1\n2015-02-04 17:58:59,23.1,27.2,419,689.333333333333,0.00475699293331518,1\n</pre>\n<p>An example <code>meta.json</code> for this file would be as follows:</p>\n<pre><span class=\"p\">{</span>\n  <span class=\"nt\">\"features\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n      <span class=\"s2\">\"temperature\"</span><span class=\"p\">,</span>\n      <span class=\"s2\">\"relative humidity\"</span><span class=\"p\">,</span>\n      <span class=\"s2\">\"light\"</span><span class=\"p\">,</span>\n      <span class=\"s2\">\"CO2\"</span><span class=\"p\">,</span>\n      <span class=\"s2\">\"humidity\"</span><span class=\"p\">,</span>\n  <span class=\"p\">],</span>\n  <span class=\"nt\">\"target\"</span><span class=\"p\">:</span> <span class=\"s2\">\"occupancy\"</span><span class=\"p\">,</span>\n  <span class=\"nt\">\"labels\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n    <span class=\"nt\">\"occupied\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"not occupied\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span>\n  <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</pre>\n<p>This will ensure that the dataset <code>X</code> is a <code>pd.DataFrame</code> with columns corresponding to the features list and that <code>y</code> is a <code>pd.Series</code> from the column described in the <code>target</code> key. The <code>labels</code> key is used to transform numerically encoded categorical variables for a classification target.</p>\n<h3>Corpus Datasets</h3>\n<p>A corpus dataset contains plain text files stored in subdirectories of the dataset directory that correspond to the class or category the plain text files belong to. Note that these text files should be only one level deep and that each document should be stored in its own file.</p>\n<p>At the momement, individual corpus files should be uncompressed (the directory as a whole is compressed). The text corpus is read similarly to the following:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"kn\">import</span> <span class=\"nn\">glob</span>\n\n<span class=\"n\">paths</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">data_dir</span><span class=\"p\">,</span> <span class=\"s2\">\"*\"</span><span class=\"p\">,</span> <span class=\"s2\">\"*.txt\"</span><span class=\"p\">)</span>\n<span class=\"n\">documents</span> <span class=\"o\">=</span> <span class=\"n\">glob</span><span class=\"o\">.</span><span class=\"n\">glob</span><span class=\"p\">(</span><span class=\"n\">paths</span><span class=\"p\">)</span>\n<span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">basename</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">dirname</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">))</span> <span class=\"k\">for</span> <span class=\"n\">path</span> <span class=\"ow\">in</span> <span class=\"n\">documents</span><span class=\"p\">]</span>\n</pre>\n<p>Documents and labels can then be directly passed to scikit-learn text feature extraction transformers for analysis.</p>\n<h2>Creating and Uploading Datasets</h2>\n<p>This section outlines how to create and upload a dataset for use in Yellowbrick examples and testing. More detailed steps follow, but in brief here is a sketch required for the actions to take to package a dataset:</p>\n<ol>\n<li>Create a dataset in <code>fixtures/</code></li>\n<li>Convert dataset to all appropriate types using <code>ybdata convert</code></li>\n<li>Validate the dataset is ready using <code>ybdata validate</code></li>\n<li>Package the dataset using <code>ybdata package</code></li>\n<li>Upload the dataset using <code>ybdata upload</code></li>\n<li>Update <code>yellowbrick.datasets</code> with <code>uploads/manifest.json</code></li>\n</ol>\n<p>Most of the datasets in this repository are from the <a href=\"https://archive.ics.uci.edu/ml/index.php\" rel=\"nofollow\">UCI Machine Learning Repository</a>. A basic methodology for creating a repository is to use the unique name of the UCIML Repository as the unique name of the data set to store in <code>fixtures/</code>. Wrangle the data so that it exists as a pandas-readable CSV file with a header row (usually by joining the target with the features or extracting data from a TSV, etc). Make sure that the CSV is gzip-compressed when done!</p>\n<p>Once the pandas CSV file is created, create the README.md, meta.json, and citation.bib files manually. It is usually also fairly simple to copy and paste the README.md from the UCIML page description (wrangling it where necessary to include as many details as possible).  The citation.bib file can be found by searching with Google Scholar and selecting \"cite as bibtex\". The meta.json usually has to be manually written. Once done, you can convert the CSV into the <code>.npz</code> objects using <code>ybdata convert</code> as follows:</p>\n<pre><code>$ ybdata convert fixtures/mydata/mydata.csv.gz fixtures/mydata/mydata.npz\n</code></pre>\n<p>Note that you can go from .npz to csv.gz asa well, but it is usually easier to go in the reverse direction when wrangling.</p>\n<p>Once done, validate that the dataset is ready to be packaged using:</p>\n<pre><code>$ ybdata validate fixtures/mydata\n</code></pre>\n<p>This should print out a table of both required and optional items for validation, and the validation status should be listed at the bottom. Once validated, convert the dataset into a package:</p>\n<pre><code>$ ybdata convert fixtures/mydata\n</code></pre>\n<p>By default this will create a package in <code>uploads/mydata.zip</code> and update the <code>uploads/manifest.json</code> with the package and signature information. Note if you're updating a previously created dataset, you can use the <code>-f</code> flag to overwrite the old data and create a new package.</p>\n<p>Finally upload the datasets to our S3 storage in the cloud. You will need proper AWS access keys to do this (see the environment or aws-configure options). If you would like to upload the datasets elsewhere, use the <code>--bucket</code> flag.</p>\n<pre><code>$ ybdata upload --pending v1.0\n</code></pre>\n<p>The upload process also updates the <code>uploads/manifold.json</code> with the final download URL and in a format that can be added to the Yellowbrick library. Make sure the yellowbrick library is updated in the correct Yellowbrick version, otherwise YB downloads will fail!</p>\n\n          </div>"}, "last_serial": 4650591, "releases": {"1.0": [{"comment_text": "", "digests": {"md5": "f8458da72e42a640ded274af90912091", "sha256": "ede8ad77026465023bcb6a38dfaacecd86c6bcc0847d72d336f4a0e68afa1c57"}, "downloads": -1, "filename": "yellowbrick_datasets-1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f8458da72e42a640ded274af90912091", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4", "size": 21145, "upload_time": "2019-01-01T17:17:36", "upload_time_iso_8601": "2019-01-01T17:17:36.273010Z", "url": "https://files.pythonhosted.org/packages/0b/9c/4014f6c0d88ee9be55a03c9ccaa084b0405c97c720e01d53443a08bcc3cf/yellowbrick_datasets-1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "76ec933f7b3e68480760d07b266e71b7", "sha256": "e809a222811e1de8345a8017f3191db0d167b916f7a180d96bbf0a8462dcd7a9"}, "downloads": -1, "filename": "yellowbrick-datasets-1.0.tar.gz", "has_sig": false, "md5_digest": "76ec933f7b3e68480760d07b266e71b7", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 13110820, "upload_time": "2019-01-01T17:17:39", "upload_time_iso_8601": "2019-01-01T17:17:39.304652Z", "url": "https://files.pythonhosted.org/packages/50/5b/551fb4b1321fe0ed51560ebeb8d9fce4a5505d41eec15398bc3a160181c5/yellowbrick-datasets-1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f8458da72e42a640ded274af90912091", "sha256": "ede8ad77026465023bcb6a38dfaacecd86c6bcc0847d72d336f4a0e68afa1c57"}, "downloads": -1, "filename": "yellowbrick_datasets-1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f8458da72e42a640ded274af90912091", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4", "size": 21145, "upload_time": "2019-01-01T17:17:36", "upload_time_iso_8601": "2019-01-01T17:17:36.273010Z", "url": "https://files.pythonhosted.org/packages/0b/9c/4014f6c0d88ee9be55a03c9ccaa084b0405c97c720e01d53443a08bcc3cf/yellowbrick_datasets-1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "76ec933f7b3e68480760d07b266e71b7", "sha256": "e809a222811e1de8345a8017f3191db0d167b916f7a180d96bbf0a8462dcd7a9"}, "downloads": -1, "filename": "yellowbrick-datasets-1.0.tar.gz", "has_sig": false, "md5_digest": "76ec933f7b3e68480760d07b266e71b7", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 13110820, "upload_time": "2019-01-01T17:17:39", "upload_time_iso_8601": "2019-01-01T17:17:39.304652Z", "url": "https://files.pythonhosted.org/packages/50/5b/551fb4b1321fe0ed51560ebeb8d9fce4a5505d41eec15398bc3a160181c5/yellowbrick-datasets-1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:22:25 2020"}