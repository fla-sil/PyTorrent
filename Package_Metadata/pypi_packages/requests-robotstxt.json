{"info": {"author": "\u0141ukasz Langa", "author_email": "lukasz@langa.pl", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.2", "Programming Language :: Python :: 3.3", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "==================\nrequests-robotstxt\n==================\n\n.. image:: https://secure.travis-ci.org/ambv/requests-robotstxt.png\n  :target: https://secure.travis-ci.org/ambv/requests-robotstxt\n\nCurrently just a proof of concept, the module strives to be an extension to\n`requests <http://pypi.python.org/pypi/requests>`_ that brings automatic\nsupport for robots.txt.\n\nHow to use\n----------\n\nSimply use ``RobotsAwareSession`` instead of the built-in ``requests.Session``.\nIf a resource is not allowed, a ``RobotsTxtDisallowed`` exception is raised.\n\nHow do I run the tests?\n-----------------------\n\nThe easiest way would be to extract the source tarball and run::\n\n  $ python test/test_robotstxt.py\n\nChange Log\n----------\n\n0.1.0\n~~~~~\n\n* initial published version\n\nAuthors\n-------\n\nGlued together by `\u0141ukasz Langa <mailto:lukasz@langa.pl>`_.", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ambv/requests-robotstxt", "keywords": "", "license": "MIT", "maintainer": null, "maintainer_email": null, "name": "requests-robotstxt", "package_url": "https://pypi.org/project/requests-robotstxt/", "platform": "any", "project_url": "https://pypi.org/project/requests-robotstxt/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/ambv/requests-robotstxt"}, "release_url": "https://pypi.org/project/requests-robotstxt/0.1.0/", "requires_dist": null, "requires_python": null, "summary": "Brings automatic support for robots.txt files in requests.", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://secure.travis-ci.org/ambv/requests-robotstxt\" rel=\"nofollow\"><img alt=\"https://secure.travis-ci.org/ambv/requests-robotstxt.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/34046cda27cb5bb9661161a0cb30332cbe37016a/68747470733a2f2f7365637572652e7472617669732d63692e6f72672f616d62762f72657175657374732d726f626f74737478742e706e67\"></a>\n<p>Currently just a proof of concept, the module strives to be an extension to\n<a href=\"http://pypi.python.org/pypi/requests\" rel=\"nofollow\">requests</a> that brings automatic\nsupport for robots.txt.</p>\n<div id=\"how-to-use\">\n<h2>How to use</h2>\n<p>Simply use <tt>RobotsAwareSession</tt> instead of the built-in <tt>requests.Session</tt>.\nIf a resource is not allowed, a <tt>RobotsTxtDisallowed</tt> exception is raised.</p>\n</div>\n<div id=\"how-do-i-run-the-tests\">\n<h2>How do I run the tests?</h2>\n<p>The easiest way would be to extract the source tarball and run:</p>\n<pre>$ python test/test_robotstxt.py\n</pre>\n</div>\n<div id=\"change-log\">\n<h2>Change Log</h2>\n<h2 id=\"id1\"><span class=\"section-subtitle\">0.1.0</span></h2>\n<ul>\n<li>initial published version</li>\n</ul>\n</div>\n<div id=\"authors\">\n<h2>Authors</h2>\n<p>Glued together by <a href=\"mailto:lukasz%40langa.pl\">\u0141ukasz Langa</a>.</p>\n</div>\n\n          </div>"}, "last_serial": 649227, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "620945ddd2fa5d81a09d0d6ac81bd5d8", "sha256": "502292aac0e7c2ef7de34921d4e59ff38a82b4412255f551f8f99d1718213563"}, "downloads": -1, "filename": "requests-robotstxt-0.1.0.tar.gz", "has_sig": false, "md5_digest": "620945ddd2fa5d81a09d0d6ac81bd5d8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3919, "upload_time": "2013-04-18T20:17:27", "upload_time_iso_8601": "2013-04-18T20:17:27.720860Z", "url": "https://files.pythonhosted.org/packages/d2/96/2c5d5d1420370b8f6193f562573623095245cc09e4820385b4e7d43540a5/requests-robotstxt-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "620945ddd2fa5d81a09d0d6ac81bd5d8", "sha256": "502292aac0e7c2ef7de34921d4e59ff38a82b4412255f551f8f99d1718213563"}, "downloads": -1, "filename": "requests-robotstxt-0.1.0.tar.gz", "has_sig": false, "md5_digest": "620945ddd2fa5d81a09d0d6ac81bd5d8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3919, "upload_time": "2013-04-18T20:17:27", "upload_time_iso_8601": "2013-04-18T20:17:27.720860Z", "url": "https://files.pythonhosted.org/packages/d2/96/2c5d5d1420370b8f6193f562573623095245cc09e4820385b4e7d43540a5/requests-robotstxt-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:04:01 2020"}