{"info": {"author": "Kensho Technologies, LLC.", "author_email": "bubsr@kensho.com", "bugtrack_url": null, "classifiers": [], "description": "# Bubs\n\nBubs is a Keras/TensorFlow reimplementation of the Flair Contextualized Embeddings (https://alanakbik.github.io/papers/coling2018.pdf). It was developed as a building block for use in Keras/TensorFlow natural language models by [Yuliya Dovzhenko](https://github.com/ydovzhenko) and the [Kensho Technologies AI Research Team](https://www.kensho.com/) ([full contributor list](https://github.com/kensho-technologies/bubs/blob/master/AUTHORS.md)).\n\nPlease check out our [blog post](https://blog.kensho.com/bubs-14c71be43fe9) announcing Bubs!\n\n----------------------------------------------\n\nBubs implements two types of Flair embeddings:  `news-forward-fast` and `news-backward-fast`.\n\nBubs consists of two parts:\n* ContextualizedEmbedding: a Keras custom layer, which computes the contextualized embeddings. It has two outputs corresponding to the `news-forward-fast` and `news-backward-fast` embeddings.\n* InputEncoder: an object for constructing inputs to the ContextualizedEmbedding layer.\n\n\n#### ContextualizedEmbedding\nThis layer consists of:\n* two character-level embedding layers (one to be used as input to the forward LSTM, one to be used as an input to the backward LSTM)\n* two character-level LSTM layers (one going forward, one going backward along the sentence) \n* two indexing layers for selecting character-level LSTM outputs at the locations where tokens end/begin (resulting in two output vectors per token).\n* two masking layers to make sure the outputs at padded locations are set to zeros. This is necessary because sentences will have different numbers of tokens and the outputs will be padded to max_token_sequence_length.\n\nThe following inputs to the ContextualizedEmbedding layer are required:\n* `forward_input`: padded array of character codes corresponding to each sentence with special begin/end characters\n* `backward_input`: padded array of character codes in reverse order with special begin/end characters\n* `forward_index_input`: padded array of locations of token outputs in forward_input\n* `backward_index_input`: padded array of locations of token outputs in backward_input\n* `forward_mask_input`: mask of same shape as forward_index_input, with 0's where padded and 1's where real tokens\n* `backward_mask_input`:mask of same shape as back_index_input, with 0's where padded and 1's where real tokens\n\n#### InputEncoder\n\nThis class provides two methods for preparing inputs to the ContextualizedEmbedding layer:\n\n* `input_batches_from_raw_text()` will accept a raw text string, split it into sentences and tokens, enforce character and token limits by breaking longer sentences into parts. It will then translate characters into numeric codes from the dictionary in `char_to_int.py`, pad sentences to the same length, and compute indices of token-level outputs from the character-level LSTMs.\n\n* `prepare_inputs_from_pretokenized()` will accept a list of lists of tokens and output model inputs . Use at your own risk: this function will not enforce character or token limits and will assume that all sentences fit into one batch. Make sure you split all your sentences into batches before calling this function. Otherwise the indices in `forward_index_input` and `backward_index_input` will be incorrect.\n\n### The model weights\n\nThe weights of the ContextualizedEmbedding layer were copied from the corresponding weights inside flair's `news-forward-fast` and `news-backward-fast` embeddings (see `scripts/extracting_model_weights.py` for a code snippet that was used to extract the weights).\n\n### The name Bubs\n\nBubs is named after the author's cat, Bubs (short for Bubbles).\n\n### A minimal example model\nBelow we define a very simple example that outputs contextualized embeddings for the following text: \"Bubs is a cat. Bubs is cute.\".\n\n```python\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\nfrom bubs import ContextualizedEmbedding, load_weights_from_npz\nfrom bubs.helpers import InputEncoder\n\n\nMAX_TOKEN_SEQUENCE_LEN = 125\nMAX_CHAR_SEQUENCE_LEN = 2500\n\n\"\"\"Load the default weights (provided with this package). If you would like to provide your own \nweights, you may pass a path to the weights npz file to the load_weights_from_npz() function.\n\"\"\"\nweights = load_weights_from_npz()\ncontext_embedding_layer = ContextualizedEmbedding(MAX_TOKEN_SEQUENCE_LEN, weights)\n\n\"\"\"Required: define inputs to the ContextualizedEmbedding layer\"\"\"\nforward_input = Input(shape=(None,), name=\"forward_input\", dtype=\"int16\")\nbackward_input = Input(shape=(None,), name=\"backward_input\", dtype=\"int16\")\nforward_index_input = Input(\n    batch_shape=(None, MAX_TOKEN_SEQUENCE_LEN, 2), name=\"forward_index_input\", dtype=\"int32\"\n)\nforward_mask_input = Input(\n    batch_shape=(None, MAX_TOKEN_SEQUENCE_LEN), name=\"forward_mask_input\", dtype=\"float32\"\n)\nbackward_index_input = Input(\n    batch_shape=(None, MAX_TOKEN_SEQUENCE_LEN, 2), name=\"backward_index_input\", dtype=\"int32\"\n)\nbackward_mask_input = Input(\n    batch_shape=(None, MAX_TOKEN_SEQUENCE_LEN), name=\"backward_mask_input\", dtype=\"float32\"\n)\n\nall_inputs = [\n    forward_input,\n    backward_input,\n    forward_index_input,\n    backward_index_input,\n    forward_mask_input,\n    backward_mask_input,\n]\n\nforward_embeddings, backward_embeddings = context_embedding_layer(all_inputs)\n\nmodel = Model(inputs=all_inputs, outputs=[forward_embeddings, backward_embeddings])\nmodel.compile(optimizer=Adam(), loss=\"categorical_crossentropy\")\n```\nNow, let's get contextualized embeddings for each token in a couple of sentences.\n```python\n# Initialize an InputEncoder for creating model inputs from raw text sentences\ninput_encoder = InputEncoder(MAX_TOKEN_SEQUENCE_LEN, MAX_CHAR_SEQUENCE_LEN)\n\n# Embed a couple of test sentences\nraw_text = \"Bubs is a cat. Bubs is cute.\"\n\n(\n    generator,\n    num_batches,\n    document_index_batches\n) = input_encoder.input_batches_from_raw_text(raw_text, batch_size=128)\n\n# Only one batch, so we use the generator once\nforward_embedding, backward_embedding = model.predict_on_batch(next(generator))\n```\n\nThe shape of each output will be (2, 125, 1024) for:\n* 2 sentences\n* 125 words in a padded sentence = `MAX_TOKEN_SEQUENCE_LEN`\n* 1024: dimension of the embedding for each word\n\nNote that the outputs are padded with zeros from the left. For example, to get the forward and backward embedding of the word 'Bubs' in the first sentence, you would need to index the following locations in the model outputs:\n`forward_embedding[0, -5]` \n`backward_embedding[0, -5]`\n\nThe embeddings for the word 'cat' are: `forward_embedding[0, -2]` and \n`backward_embedding[0, -2]`\n\n### Installation\n\nFirst, install tensorflow or tensorflow-gpu, depending on your system:\n\n```pip install tensorflow-gpu==1.7.1```\nor\n```pip install tensorflow==1.7.1```\n\nTensorflow versions `1.7.1`, `1.10`, and `1.13.1` pass the tests.\n\nThen, install Bubs:\n\n```pip install bubs```\n\n# License\n\nLicensed under the Apache 2.0 License. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n\nCopyright 2019 Kensho Technologies, Inc.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/kensho-technologies/bubs", "keywords": "", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "bubs", "package_url": "https://pypi.org/project/bubs/", "platform": "", "project_url": "https://pypi.org/project/bubs/", "project_urls": {"Homepage": "https://github.com/kensho-technologies/bubs"}, "release_url": "https://pypi.org/project/bubs/0.0.2/", "requires_dist": ["funcy (>=1.10)", "Keras (==2.2.4)", "numpy (>=1.10.0)", "segtok (>=1.5.7)"], "requires_python": "", "summary": "Keras Implementation of Flair's Contextualized Embeddings", "version": "0.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Bubs</h1>\n<p>Bubs is a Keras/TensorFlow reimplementation of the Flair Contextualized Embeddings (<a href=\"https://alanakbik.github.io/papers/coling2018.pdf\" rel=\"nofollow\">https://alanakbik.github.io/papers/coling2018.pdf</a>). It was developed as a building block for use in Keras/TensorFlow natural language models by <a href=\"https://github.com/ydovzhenko\" rel=\"nofollow\">Yuliya Dovzhenko</a> and the <a href=\"https://www.kensho.com/\" rel=\"nofollow\">Kensho Technologies AI Research Team</a> (<a href=\"https://github.com/kensho-technologies/bubs/blob/master/AUTHORS.md\" rel=\"nofollow\">full contributor list</a>).</p>\n<p>Please check out our <a href=\"https://blog.kensho.com/bubs-14c71be43fe9\" rel=\"nofollow\">blog post</a> announcing Bubs!</p>\n<hr>\n<p>Bubs implements two types of Flair embeddings:  <code>news-forward-fast</code> and <code>news-backward-fast</code>.</p>\n<p>Bubs consists of two parts:</p>\n<ul>\n<li>ContextualizedEmbedding: a Keras custom layer, which computes the contextualized embeddings. It has two outputs corresponding to the <code>news-forward-fast</code> and <code>news-backward-fast</code> embeddings.</li>\n<li>InputEncoder: an object for constructing inputs to the ContextualizedEmbedding layer.</li>\n</ul>\n<h4>ContextualizedEmbedding</h4>\n<p>This layer consists of:</p>\n<ul>\n<li>two character-level embedding layers (one to be used as input to the forward LSTM, one to be used as an input to the backward LSTM)</li>\n<li>two character-level LSTM layers (one going forward, one going backward along the sentence)</li>\n<li>two indexing layers for selecting character-level LSTM outputs at the locations where tokens end/begin (resulting in two output vectors per token).</li>\n<li>two masking layers to make sure the outputs at padded locations are set to zeros. This is necessary because sentences will have different numbers of tokens and the outputs will be padded to max_token_sequence_length.</li>\n</ul>\n<p>The following inputs to the ContextualizedEmbedding layer are required:</p>\n<ul>\n<li><code>forward_input</code>: padded array of character codes corresponding to each sentence with special begin/end characters</li>\n<li><code>backward_input</code>: padded array of character codes in reverse order with special begin/end characters</li>\n<li><code>forward_index_input</code>: padded array of locations of token outputs in forward_input</li>\n<li><code>backward_index_input</code>: padded array of locations of token outputs in backward_input</li>\n<li><code>forward_mask_input</code>: mask of same shape as forward_index_input, with 0's where padded and 1's where real tokens</li>\n<li><code>backward_mask_input</code>:mask of same shape as back_index_input, with 0's where padded and 1's where real tokens</li>\n</ul>\n<h4>InputEncoder</h4>\n<p>This class provides two methods for preparing inputs to the ContextualizedEmbedding layer:</p>\n<ul>\n<li>\n<p><code>input_batches_from_raw_text()</code> will accept a raw text string, split it into sentences and tokens, enforce character and token limits by breaking longer sentences into parts. It will then translate characters into numeric codes from the dictionary in <code>char_to_int.py</code>, pad sentences to the same length, and compute indices of token-level outputs from the character-level LSTMs.</p>\n</li>\n<li>\n<p><code>prepare_inputs_from_pretokenized()</code> will accept a list of lists of tokens and output model inputs . Use at your own risk: this function will not enforce character or token limits and will assume that all sentences fit into one batch. Make sure you split all your sentences into batches before calling this function. Otherwise the indices in <code>forward_index_input</code> and <code>backward_index_input</code> will be incorrect.</p>\n</li>\n</ul>\n<h3>The model weights</h3>\n<p>The weights of the ContextualizedEmbedding layer were copied from the corresponding weights inside flair's <code>news-forward-fast</code> and <code>news-backward-fast</code> embeddings (see <code>scripts/extracting_model_weights.py</code> for a code snippet that was used to extract the weights).</p>\n<h3>The name Bubs</h3>\n<p>Bubs is named after the author's cat, Bubs (short for Bubbles).</p>\n<h3>A minimal example model</h3>\n<p>Below we define a very simple example that outputs contextualized embeddings for the following text: \"Bubs is a cat. Bubs is cute.\".</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Input</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">Model</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.optimizers</span> <span class=\"kn\">import</span> <span class=\"n\">Adam</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">bubs</span> <span class=\"kn\">import</span> <span class=\"n\">ContextualizedEmbedding</span><span class=\"p\">,</span> <span class=\"n\">load_weights_from_npz</span>\n<span class=\"kn\">from</span> <span class=\"nn\">bubs.helpers</span> <span class=\"kn\">import</span> <span class=\"n\">InputEncoder</span>\n\n\n<span class=\"n\">MAX_TOKEN_SEQUENCE_LEN</span> <span class=\"o\">=</span> <span class=\"mi\">125</span>\n<span class=\"n\">MAX_CHAR_SEQUENCE_LEN</span> <span class=\"o\">=</span> <span class=\"mi\">2500</span>\n\n<span class=\"sd\">\"\"\"Load the default weights (provided with this package). If you would like to provide your own </span>\n<span class=\"sd\">weights, you may pass a path to the weights npz file to the load_weights_from_npz() function.</span>\n<span class=\"sd\">\"\"\"</span>\n<span class=\"n\">weights</span> <span class=\"o\">=</span> <span class=\"n\">load_weights_from_npz</span><span class=\"p\">()</span>\n<span class=\"n\">context_embedding_layer</span> <span class=\"o\">=</span> <span class=\"n\">ContextualizedEmbedding</span><span class=\"p\">(</span><span class=\"n\">MAX_TOKEN_SEQUENCE_LEN</span><span class=\"p\">,</span> <span class=\"n\">weights</span><span class=\"p\">)</span>\n\n<span class=\"sd\">\"\"\"Required: define inputs to the ContextualizedEmbedding layer\"\"\"</span>\n<span class=\"n\">forward_input</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"forward_input\"</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s2\">\"int16\"</span><span class=\"p\">)</span>\n<span class=\"n\">backward_input</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"backward_input\"</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s2\">\"int16\"</span><span class=\"p\">)</span>\n<span class=\"n\">forward_index_input</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">(</span>\n    <span class=\"n\">batch_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">MAX_TOKEN_SEQUENCE_LEN</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"forward_index_input\"</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s2\">\"int32\"</span>\n<span class=\"p\">)</span>\n<span class=\"n\">forward_mask_input</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">(</span>\n    <span class=\"n\">batch_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">MAX_TOKEN_SEQUENCE_LEN</span><span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"forward_mask_input\"</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s2\">\"float32\"</span>\n<span class=\"p\">)</span>\n<span class=\"n\">backward_index_input</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">(</span>\n    <span class=\"n\">batch_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">MAX_TOKEN_SEQUENCE_LEN</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"backward_index_input\"</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s2\">\"int32\"</span>\n<span class=\"p\">)</span>\n<span class=\"n\">backward_mask_input</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">(</span>\n    <span class=\"n\">batch_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">MAX_TOKEN_SEQUENCE_LEN</span><span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"backward_mask_input\"</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s2\">\"float32\"</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">all_inputs</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"n\">forward_input</span><span class=\"p\">,</span>\n    <span class=\"n\">backward_input</span><span class=\"p\">,</span>\n    <span class=\"n\">forward_index_input</span><span class=\"p\">,</span>\n    <span class=\"n\">backward_index_input</span><span class=\"p\">,</span>\n    <span class=\"n\">forward_mask_input</span><span class=\"p\">,</span>\n    <span class=\"n\">backward_mask_input</span><span class=\"p\">,</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">forward_embeddings</span><span class=\"p\">,</span> <span class=\"n\">backward_embeddings</span> <span class=\"o\">=</span> <span class=\"n\">context_embedding_layer</span><span class=\"p\">(</span><span class=\"n\">all_inputs</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"n\">all_inputs</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">forward_embeddings</span><span class=\"p\">,</span> <span class=\"n\">backward_embeddings</span><span class=\"p\">])</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"n\">Adam</span><span class=\"p\">(),</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s2\">\"categorical_crossentropy\"</span><span class=\"p\">)</span>\n</pre>\n<p>Now, let's get contextualized embeddings for each token in a couple of sentences.</p>\n<pre><span class=\"c1\"># Initialize an InputEncoder for creating model inputs from raw text sentences</span>\n<span class=\"n\">input_encoder</span> <span class=\"o\">=</span> <span class=\"n\">InputEncoder</span><span class=\"p\">(</span><span class=\"n\">MAX_TOKEN_SEQUENCE_LEN</span><span class=\"p\">,</span> <span class=\"n\">MAX_CHAR_SEQUENCE_LEN</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Embed a couple of test sentences</span>\n<span class=\"n\">raw_text</span> <span class=\"o\">=</span> <span class=\"s2\">\"Bubs is a cat. Bubs is cute.\"</span>\n\n<span class=\"p\">(</span>\n    <span class=\"n\">generator</span><span class=\"p\">,</span>\n    <span class=\"n\">num_batches</span><span class=\"p\">,</span>\n    <span class=\"n\">document_index_batches</span>\n<span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">input_encoder</span><span class=\"o\">.</span><span class=\"n\">input_batches_from_raw_text</span><span class=\"p\">(</span><span class=\"n\">raw_text</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">128</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Only one batch, so we use the generator once</span>\n<span class=\"n\">forward_embedding</span><span class=\"p\">,</span> <span class=\"n\">backward_embedding</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict_on_batch</span><span class=\"p\">(</span><span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"n\">generator</span><span class=\"p\">))</span>\n</pre>\n<p>The shape of each output will be (2, 125, 1024) for:</p>\n<ul>\n<li>2 sentences</li>\n<li>125 words in a padded sentence = <code>MAX_TOKEN_SEQUENCE_LEN</code></li>\n<li>1024: dimension of the embedding for each word</li>\n</ul>\n<p>Note that the outputs are padded with zeros from the left. For example, to get the forward and backward embedding of the word 'Bubs' in the first sentence, you would need to index the following locations in the model outputs:\n<code>forward_embedding[0, -5]</code>\n<code>backward_embedding[0, -5]</code></p>\n<p>The embeddings for the word 'cat' are: <code>forward_embedding[0, -2]</code> and\n<code>backward_embedding[0, -2]</code></p>\n<h3>Installation</h3>\n<p>First, install tensorflow or tensorflow-gpu, depending on your system:</p>\n<p><code>pip install tensorflow-gpu==1.7.1</code>\nor\n<code>pip install tensorflow==1.7.1</code></p>\n<p>Tensorflow versions <code>1.7.1</code>, <code>1.10</code>, and <code>1.13.1</code> pass the tests.</p>\n<p>Then, install Bubs:</p>\n<p><code>pip install bubs</code></p>\n<h1>License</h1>\n<p>Licensed under the Apache 2.0 License. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>\n<p>Copyright 2019 Kensho Technologies, Inc.</p>\n\n          </div>"}, "last_serial": 6274249, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "a1b989ae9e5f0c331aa4010726f2ff62", "sha256": "6671592ddffa7362096dc881db4f17a606df4c78ad918c56e05d1a397f43928e"}, "downloads": -1, "filename": "bubs-0.0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "a1b989ae9e5f0c331aa4010726f2ff62", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 25775, "upload_time": "2019-11-12T14:37:11", "upload_time_iso_8601": "2019-11-12T14:37:11.503519Z", "url": "https://files.pythonhosted.org/packages/e1/fd/4c1850b98aa54516713077a36c5c72637fcfff55d53b4a3562494d4c7690/bubs-0.0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "856e8eb38adaa3bea140b17af855d1ec", "sha256": "fe4bba6f3591691f17dafacacc1c8c9cf67b291bb0e9a494a4757af270af39c0"}, "downloads": -1, "filename": "bubs-0.0.1.tar.gz", "has_sig": false, "md5_digest": "856e8eb38adaa3bea140b17af855d1ec", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24162, "upload_time": "2019-11-12T14:37:14", "upload_time_iso_8601": "2019-11-12T14:37:14.370266Z", "url": "https://files.pythonhosted.org/packages/0a/76/129b721f574209037053e3ac9488416d67efd996ecd37ecf5097c09d343d/bubs-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "ae4e809e0c47d66052fbd8e5c1af5b9e", "sha256": "a75fdd0de71ce6527b9ecdbad7b264fc282fdf37f4889e781d0857683e085894"}, "downloads": -1, "filename": "bubs-0.0.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "ae4e809e0c47d66052fbd8e5c1af5b9e", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 30506, "upload_time": "2019-12-10T18:21:16", "upload_time_iso_8601": "2019-12-10T18:21:16.043875Z", "url": "https://files.pythonhosted.org/packages/06/00/3659464560a4d91df64d4f1815e59bb1ce774ccfc28c780d433596470ceb/bubs-0.0.2-py2.py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ae4e809e0c47d66052fbd8e5c1af5b9e", "sha256": "a75fdd0de71ce6527b9ecdbad7b264fc282fdf37f4889e781d0857683e085894"}, "downloads": -1, "filename": "bubs-0.0.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "ae4e809e0c47d66052fbd8e5c1af5b9e", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 30506, "upload_time": "2019-12-10T18:21:16", "upload_time_iso_8601": "2019-12-10T18:21:16.043875Z", "url": "https://files.pythonhosted.org/packages/06/00/3659464560a4d91df64d4f1815e59bb1ce774ccfc28c780d433596470ceb/bubs-0.0.2-py2.py3-none-any.whl", "yanked": false}], "timestamp": "Thu May  7 22:36:07 2020"}