{"info": {"author": "Dan Gillick", "author_email": "dgillick@cs.berkeley.edu", "bugtrack_url": null, "classifiers": [], "description": "Improved Sentence Boundary Detection\r\n====================================\r\n\r\nOverview\r\n--------\r\n\r\nConsider the following text:\r\n\r\n\"On Jan. 20, former Sen. Barack Obama became the 44th President of the\r\nU.S. Millions attended the Inauguration.\"\r\n\r\nThe periods are potentially ambiguous, signifying either the end of a\r\nsentence, an abbreviation, or both. The sentence boundary detection\r\n(SBD) task involves disambiguating the periods, and in particular,\r\nclassifying each period as end-of-sentence () or not. In the example,\r\nonly the period at the end of U.S. should be classified as :\r\n\r\n\"On Jan. 20, former Sen. Barack Obama became the 44th President of the\r\nU.S. Millions attended the Inauguration.\"\r\n\r\nChances are, if you are using some SBD system, it has an error rate of\r\n1%-3% on English newswire text. The system described here achieves the\r\nbest known error rate on the Wall Street Journal corpus: 0.25% and\r\ncomparable error rates on the Brown corpus (mixed genre) and other test\r\ncorpora.\r\n\r\nBackground\r\n----------\r\n\r\nSBD is fundamental to many natural language processing problems, but\r\nonly a few papers describe solutions. A variety of rule-based systems\r\nare floating around, and a few semi-statistical systems are available if\r\nyou know where to look. The most widely cited are:\r\n\r\n-  Alembic (Aberdeen, et al. 1995): Abbreviation list and ~100\r\n   hand-crafted regular expressions.\r\n-  Satz (Palmer & Hearst at Berkeley, 1997): Part of speech features and\r\n   abbreviation lists as input to a classifier (neural nets and decision\r\n   trees have similar performance).\r\n-  mxTerminator (Reynar & Ratnaparkhi, 1997): Maximum entropy\r\n   classification with simple lexical features.\r\n-  Mikheev (Mikheev, 2002): Observes that perfect labels for\r\n   abbreviations and names gives almost perfect SBD results. Creates\r\n   heuristics for marking these, unsupervised, from held-out data.\r\n-  Punkt (Strunk and Kiss, 2006): Unsupervised method uses heuristics to\r\n   identify abbreviations and sentence starters.\r\n\r\nI have not been able to find publicly available copies of any of these\r\nsystems, with the exception of Punkt, which ships with NLTK.\r\nNonetheless, here are some error rates reported on what I believe to be\r\nthe same subset of the WSJ corpus (sections 03-16).\r\n\r\n-  Alembic: 0.9%\r\n-  Satz: 1.5%; 1.0% with extra hand-written lists of abbreviations and\r\n   non-names.\r\n-  mxTerminator: 2.0%; 1.2% with extra abbreviation list.\r\n-  Mikheev: 1.4%; 0.45% with abbreviation list (assembled automatically\r\n   but carefully tuned; test-set-dependent parameters are a concern)\r\n-  Punkt: 1.65% (Though if you use the model that ships with NLTK,\r\n   you'll get over 3%)\r\n\r\nAll of these systems use lists of abbreviations in some capacity, which\r\nI think is a mistake. Some abbreviations almost never end a sentence\r\n(Mr.), which makes list-building appealing. But many abbreviations are\r\nmore ambiguous (U.S., U.N.), which complicates the decision.\r\n\r\n--------------\r\n\r\nWhile 1%-3% is a low error rate, this is often not good enough. In\r\nautomatic document summarization, for example, including a sentence\r\nfragment usually renders the resulting summary unintelligible. With\r\n10-sentence summaries, 1 in 10 is ruined by an SBD system with 99%\r\naccuracy. Improving the accuracy to 99.75%, only 1 in 40 is ruined.\r\nImproved sentence boundary detection is also likely to help with\r\nlanguage modeling and text alignment.\r\n\r\n--------------\r\n\r\nI built a supervised system that classifies sentence boundaries without\r\nany heuristics or hand-generated lists. It uses the same training data\r\nas mxTerminator, and allows for Naive Bayes or SVM models (SVM Light).\r\n\r\n+-------------------------------------+---------+---------------+\r\n| Corpus                              | SVM     | Naive Bayes   |\r\n+=====================================+=========+===============+\r\n| WSJ                                 | 0.25%   | 0.35%         |\r\n+-------------------------------------+---------+---------------+\r\n| Brown                               | 0.36%   | 0.45%         |\r\n+-------------------------------------+---------+---------------+\r\n| Complete Works of Edgar Allen Poe   | 0.52%   | 0.44          |\r\n+-------------------------------------+---------+---------------+\r\n\r\nI've packaged this code, written in Python, for general use. Word-level\r\ntokenization, which is particularly important for good sentence boundary\r\ndetection, is included.\r\n\r\nNote that the included models use all of the labeled data listed here,\r\nmeaning that the expected results are somewhat better than the numbers\r\nreported above. Including the Brown data as training improves the WSJ\r\nresult to 0.22% and the Poe result to 0.37 (using the SVM).\r\n\r\nPerformance\r\n-----------\r\n\r\nA few other notes on performance. The standard WSJ test corpus includes\r\n26977 possible sentence boundaries. About 70% are in fact sentence\r\nboundaries. Classification with the included SVM model will give 59\r\nerrors. Of these, 24 (41%) involve the word \"U.S.\", a particularly\r\ninteresting case. In training, \"U.S.\" appears 2029 times, and 90 of\r\nthese are sentence boundaries. Further complicating the situation,\r\n\"U.S.\" often appears in a context like \"U.S. Security Council\" or \"U.S.\r\nGovernment\", and either \"Security\" or \"Government\" are viable sentence\r\nstarters.\r\n\r\nOther confusing cases include \"U.N.\", \"U.K.\", and state abbreviations\r\nlike \"N.Y.\" which have similar characteristics as \"U.S.\" but appear\r\nsomewhat less frequently.\r\n\r\nSetup\r\n-----\r\n\r\nSee SETUP.md for setup instructions and notes.", "description_content_type": null, "docs_url": null, "download_url": "https://github.com/hinstitute/splitta/tarball/0.1.0", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/hinstitute/splitta", "keywords": "splitta,setence bounadry detection,sbd", "license": "(c) 2009 Dan Gillick", "maintainer": "", "maintainer_email": "", "name": "splitta", "package_url": "https://pypi.org/project/splitta/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/splitta/", "project_urls": {"Download": "https://github.com/hinstitute/splitta/tarball/0.1.0", "Homepage": "https://github.com/hinstitute/splitta"}, "release_url": "https://pypi.org/project/splitta/0.1.0/", "requires_dist": null, "requires_python": null, "summary": "Sentence boundary detection", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"overview\">\n<h2>Overview</h2>\n<p>Consider the following text:</p>\n<p>\u201cOn Jan. 20, former Sen. Barack Obama became the 44th President of the\nU.S. Millions attended the Inauguration.\u201d</p>\n<p>The periods are potentially ambiguous, signifying either the end of a\nsentence, an abbreviation, or both. The sentence boundary detection\n(SBD) task involves disambiguating the periods, and in particular,\nclassifying each period as end-of-sentence () or not. In the example,\nonly the period at the end of U.S. should be classified as :</p>\n<p>\u201cOn Jan. 20, former Sen. Barack Obama became the 44th President of the\nU.S. Millions attended the Inauguration.\u201d</p>\n<p>Chances are, if you are using some SBD system, it has an error rate of\n1%-3% on English newswire text. The system described here achieves the\nbest known error rate on the Wall Street Journal corpus: 0.25% and\ncomparable error rates on the Brown corpus (mixed genre) and other test\ncorpora.</p>\n</div>\n<div id=\"background\">\n<h2>Background</h2>\n<p>SBD is fundamental to many natural language processing problems, but\nonly a few papers describe solutions. A variety of rule-based systems\nare floating around, and a few semi-statistical systems are available if\nyou know where to look. The most widely cited are:</p>\n<ul>\n<li>Alembic (Aberdeen, et al. 1995): Abbreviation list and ~100\nhand-crafted regular expressions.</li>\n<li>Satz (Palmer &amp; Hearst at Berkeley, 1997): Part of speech features and\nabbreviation lists as input to a classifier (neural nets and decision\ntrees have similar performance).</li>\n<li>mxTerminator (Reynar &amp; Ratnaparkhi, 1997): Maximum entropy\nclassification with simple lexical features.</li>\n<li>Mikheev (Mikheev, 2002): Observes that perfect labels for\nabbreviations and names gives almost perfect SBD results. Creates\nheuristics for marking these, unsupervised, from held-out data.</li>\n<li>Punkt (Strunk and Kiss, 2006): Unsupervised method uses heuristics to\nidentify abbreviations and sentence starters.</li>\n</ul>\n<p>I have not been able to find publicly available copies of any of these\nsystems, with the exception of Punkt, which ships with NLTK.\nNonetheless, here are some error rates reported on what I believe to be\nthe same subset of the WSJ corpus (sections 03-16).</p>\n<ul>\n<li>Alembic: 0.9%</li>\n<li>Satz: 1.5%; 1.0% with extra hand-written lists of abbreviations and\nnon-names.</li>\n<li>mxTerminator: 2.0%; 1.2% with extra abbreviation list.</li>\n<li>Mikheev: 1.4%; 0.45% with abbreviation list (assembled automatically\nbut carefully tuned; test-set-dependent parameters are a concern)</li>\n<li>Punkt: 1.65% (Though if you use the model that ships with NLTK,\nyou\u2019ll get over 3%)</li>\n</ul>\n<p>All of these systems use lists of abbreviations in some capacity, which\nI think is a mistake. Some abbreviations almost never end a sentence\n(Mr.), which makes list-building appealing. But many abbreviations are\nmore ambiguous (U.S., U.N.), which complicates the decision.</p>\n<hr class=\"docutils\">\n<p>While 1%-3% is a low error rate, this is often not good enough. In\nautomatic document summarization, for example, including a sentence\nfragment usually renders the resulting summary unintelligible. With\n10-sentence summaries, 1 in 10 is ruined by an SBD system with 99%\naccuracy. Improving the accuracy to 99.75%, only 1 in 40 is ruined.\nImproved sentence boundary detection is also likely to help with\nlanguage modeling and text alignment.</p>\n<hr class=\"docutils\">\n<p>I built a supervised system that classifies sentence boundaries without\nany heuristics or hand-generated lists. It uses the same training data\nas mxTerminator, and allows for Naive Bayes or SVM models (SVM Light).</p>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>Corpus</th>\n<th>SVM</th>\n<th>Naive Bayes</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>WSJ</td>\n<td>0.25%</td>\n<td>0.35%</td>\n</tr>\n<tr><td>Brown</td>\n<td>0.36%</td>\n<td>0.45%</td>\n</tr>\n<tr><td>Complete Works of Edgar Allen Poe</td>\n<td>0.52%</td>\n<td>0.44</td>\n</tr>\n</tbody>\n</table>\n<p>I\u2019ve packaged this code, written in Python, for general use. Word-level\ntokenization, which is particularly important for good sentence boundary\ndetection, is included.</p>\n<p>Note that the included models use all of the labeled data listed here,\nmeaning that the expected results are somewhat better than the numbers\nreported above. Including the Brown data as training improves the WSJ\nresult to 0.22% and the Poe result to 0.37 (using the SVM).</p>\n</div>\n<div id=\"performance\">\n<h2>Performance</h2>\n<p>A few other notes on performance. The standard WSJ test corpus includes\n26977 possible sentence boundaries. About 70% are in fact sentence\nboundaries. Classification with the included SVM model will give 59\nerrors. Of these, 24 (41%) involve the word \u201cU.S.\u201d, a particularly\ninteresting case. In training, \u201cU.S.\u201d appears 2029 times, and 90 of\nthese are sentence boundaries. Further complicating the situation,\n\u201cU.S.\u201d often appears in a context like \u201cU.S. Security Council\u201d or \u201cU.S.\nGovernment\u201d, and either \u201cSecurity\u201d or \u201cGovernment\u201d are viable sentence\nstarters.</p>\n<p>Other confusing cases include \u201cU.N.\u201d, \u201cU.K.\u201d, and state abbreviations\nlike \u201cN.Y.\u201d which have similar characteristics as \u201cU.S.\u201d but appear\nsomewhat less frequently.</p>\n</div>\n<div id=\"setup\">\n<h2>Setup</h2>\n<p>See SETUP.md for setup instructions and notes.</p>\n</div>\n\n          </div>"}, "last_serial": 1349492, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "219e00ebfe0d946c53cb0bfaf7e36490", "sha256": "578c601fca7a93ddbbf07a06298c44057248dbd786e2479ed3c9e6a71548503a"}, "downloads": -1, "filename": "splitta-0.1.0.tar.gz", "has_sig": false, "md5_digest": "219e00ebfe0d946c53cb0bfaf7e36490", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6943603, "upload_time": "2014-12-18T16:41:41", "upload_time_iso_8601": "2014-12-18T16:41:41.515545Z", "url": "https://files.pythonhosted.org/packages/cf/d2/9771eb65f1dc3925dbcfc7c4b2adaefa38e1549e4e4e75409df316f8c453/splitta-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "219e00ebfe0d946c53cb0bfaf7e36490", "sha256": "578c601fca7a93ddbbf07a06298c44057248dbd786e2479ed3c9e6a71548503a"}, "downloads": -1, "filename": "splitta-0.1.0.tar.gz", "has_sig": false, "md5_digest": "219e00ebfe0d946c53cb0bfaf7e36490", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6943603, "upload_time": "2014-12-18T16:41:41", "upload_time_iso_8601": "2014-12-18T16:41:41.515545Z", "url": "https://files.pythonhosted.org/packages/cf/d2/9771eb65f1dc3925dbcfc7c4b2adaefa38e1549e4e4e75409df316f8c453/splitta-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:04:41 2020"}