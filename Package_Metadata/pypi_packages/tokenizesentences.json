{"info": {"author": "Carlos A. Planch\u00f3n", "author_email": "bubbledoloresuruguay2@gmail.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Software Development :: Build Tools"], "description": "# tokenizesentences\n*Python3 module to tokenize english sentences.*\nBased on the answer of D Greenberg in StackOverflow:\nhttps://stackoverflow.com/questions/4576077/python-split-text-on-sentences\n\n## Installation\n### Install with pip\n```\npip3 install -U tokenizesentences\n```\n\n## Usage\n```\nIn [1]: import tokenizesentences\n\nIn [2]: m = tokenizesentences.SplitIntoSentences()\n\nIn [3]: m.split_into_sentences(\n    \"Mr. John Johnson Jr. was born in the U.S.A but earned his Ph.D. in Israel before joining Nike Inc. as an engineer. He also worked at craigslist.org as a business analyst.\"\n    )\n\nOut[3]: \n[\n    'Mr. John Johnson Jr. was born in the U.S.A but earned his Ph.D. in Israel before joining Nike Inc. as an engineer.',\n    'He also worked at craigslist.org as a business analyst.'\n]\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/carlosplanchon/tokenizesentences/archive/v0.2.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/carlosplanchon/tokenizesentences", "keywords": "tokenize,english,sentences", "license": "GPL3", "maintainer": "", "maintainer_email": "", "name": "tokenizesentences", "package_url": "https://pypi.org/project/tokenizesentences/", "platform": "", "project_url": "https://pypi.org/project/tokenizesentences/", "project_urls": {"Download": "https://github.com/carlosplanchon/tokenizesentences/archive/v0.2.tar.gz", "Homepage": "https://github.com/carlosplanchon/tokenizesentences"}, "release_url": "https://pypi.org/project/tokenizesentences/0.2/", "requires_dist": null, "requires_python": "", "summary": "Python3 module to tokenize english sentences.", "version": "0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>tokenizesentences</h1>\n<p><em>Python3 module to tokenize english sentences.</em>\nBased on the answer of D Greenberg in StackOverflow:\n<a href=\"https://stackoverflow.com/questions/4576077/python-split-text-on-sentences\" rel=\"nofollow\">https://stackoverflow.com/questions/4576077/python-split-text-on-sentences</a></p>\n<h2>Installation</h2>\n<h3>Install with pip</h3>\n<pre><code>pip3 install -U tokenizesentences\n</code></pre>\n<h2>Usage</h2>\n<pre><code>In [1]: import tokenizesentences\n\nIn [2]: m = tokenizesentences.SplitIntoSentences()\n\nIn [3]: m.split_into_sentences(\n    \"Mr. John Johnson Jr. was born in the U.S.A but earned his Ph.D. in Israel before joining Nike Inc. as an engineer. He also worked at craigslist.org as a business analyst.\"\n    )\n\nOut[3]: \n[\n    'Mr. John Johnson Jr. was born in the U.S.A but earned his Ph.D. in Israel before joining Nike Inc. as an engineer.',\n    'He also worked at craigslist.org as a business analyst.'\n]\n</code></pre>\n\n          </div>"}, "last_serial": 5180288, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "36de88c1a2c1c2267eb4d80b10c1b5a6", "sha256": "7484a3068e86a9736fffad1dd212d241385aa4fe72139a88f369e277ebc6a8b6"}, "downloads": -1, "filename": "tokenizesentences-0.1.tar.gz", "has_sig": false, "md5_digest": "36de88c1a2c1c2267eb4d80b10c1b5a6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2408, "upload_time": "2019-04-22T19:52:01", "upload_time_iso_8601": "2019-04-22T19:52:01.483990Z", "url": "https://files.pythonhosted.org/packages/63/5b/61d586db39da9c12545c4c0a64a4370d306d40ae38fd9d895e0f759aede5/tokenizesentences-0.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "b5c7a6e82db5fa034d5ec02d90a832dc", "sha256": "a7863c7244782825a2f48145c634fea633806b703a6b9d3c78ed095bf88a5742"}, "downloads": -1, "filename": "tokenizesentences-0.2.tar.gz", "has_sig": false, "md5_digest": "b5c7a6e82db5fa034d5ec02d90a832dc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2438, "upload_time": "2019-04-24T04:38:48", "upload_time_iso_8601": "2019-04-24T04:38:48.508143Z", "url": "https://files.pythonhosted.org/packages/60/77/9464717d1efd6441bf0d4978506cb164c71e5d0e13bd0e3d1f36bb50c6fc/tokenizesentences-0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b5c7a6e82db5fa034d5ec02d90a832dc", "sha256": "a7863c7244782825a2f48145c634fea633806b703a6b9d3c78ed095bf88a5742"}, "downloads": -1, "filename": "tokenizesentences-0.2.tar.gz", "has_sig": false, "md5_digest": "b5c7a6e82db5fa034d5ec02d90a832dc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2438, "upload_time": "2019-04-24T04:38:48", "upload_time_iso_8601": "2019-04-24T04:38:48.508143Z", "url": "https://files.pythonhosted.org/packages/60/77/9464717d1efd6441bf0d4978506cb164c71e5d0e13bd0e3d1f36bb50c6fc/tokenizesentences-0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:51:24 2020"}