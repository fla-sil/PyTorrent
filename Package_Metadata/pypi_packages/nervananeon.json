{"info": {"author": "Intel Deep Learning System", "author_email": "intelnervana@intel.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Environment :: Console", "Environment :: Console :: Curses", "Environment :: Web Environment", "Intended Audience :: Developers", "Intended Audience :: End Users/Desktop", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Operating System :: MacOS :: MacOS X", "Operating System :: POSIX", "Programming Language :: Python :: 2", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Information Analysis", "Topic :: System :: Distributed Computing"], "description": "neon\n====\n\n`neon <https://github.com/NervanaSystems/neon>`__ is Intel's reference\ndeep learning framework committed to `best\nperformance <https://github.com/soumith/convnet-benchmarks>`__ on all\nhardware. Designed for ease-of-use and extensibility.\n\n-  `Tutorials <http://neon.nervanasys.com/docs/latest/tutorials.html>`__\n   and `iPython notebooks <https://github.com/NervanaSystems/meetup>`__\n   to get users started with using neon for deep learning.\n-  Support for commonly used layers: convolution, RNN, LSTM, GRU,\n   BatchNorm, and more.\n-  `Model Zoo <https://github.com/NervanaSystems/ModelZoo>`__ contains\n   pre-trained weights and example scripts for start-of-the-art models,\n   including:\n   `VGG <https://github.com/NervanaSystems/ModelZoo/tree/master/ImageClassification/ILSVRC2012/VGG>`__,\n   `Reinforcement\n   learning <https://github.com/NervanaSystems/ModelZoo/tree/master/DeepReinforcement>`__,\n   `Deep Residual\n   Networks <https://github.com/NervanaSystems/ModelZoo/tree/master/SceneClassification/DeepResNet>`__,\n   `Image\n   Captioning <https://github.com/NervanaSystems/ModelZoo/tree/master/ImageCaptioning>`__,\n   `Sentiment\n   analysis <https://github.com/NervanaSystems/ModelZoo/tree/master/NLP/SentimentClassification/IMDB>`__,\n   and `more <http://neon.nervanasys.com/docs/latest/model_zoo.html>`__.\n-  Swappable hardware backends: write code once and then deploy on CPUs,\n   GPUs, or Nervana hardware\n\nFor fast iteration and model exploration, neon has the fastest\nperformance among deep learning libraries (2x speed of cuDNNv4, see\n`benchmarks <https://github.com/soumith/convnet-benchmarks>`__). \\*\n2.5s/macrobatch (3072 images) on AlexNet on Titan X (Full run on 1 GPU ~\n26 hrs) \\* Training VGG with 16-bit floating point on 1 Titan X takes\n~10 days (original paper: 4 GPUs for 2-3 weeks)\n\nWe use neon internally at Intel Nervana to solve our customers' problems\nacross many `domains <http://www.nervanasys.com/solutions/>`__. We are\nhiring across several roles. Apply\n`here <http://www.nervanasys.com/careers/>`__!\n\nSee the `new\nfeatures <https://github.com/NervanaSystems/neon/blob/master/ChangeLog>`__\nin our latest release. We want to highlight that neon v2.0.0+ has been\noptimized for much better performance on CPUs by enabling Intel Math\nKernel Library (MKL). The DNN (Deep Neural Networks) component of MKL\nthat is used by neon is provided free of charge and downloaded\nautomatically as part of the neon installation.\n\nQuick Install\n-------------\n\n-  `Local install and\n   dependencies <http://neon.nervanasys.com/docs/latest/installation.html>`__\n\nOn a Mac OSX or Linux machine, enter the following to download and\ninstall neon (conda users see the\n`guide <http://neon.nervanasys.com/docs/latest/installation.html>`__),\nand use it to train your first multi-layer perceptron. To force a\npython2 or python3 install, replace ``make`` below with either\n``make python2`` or ``make python3``.\n\n.. code:: bash\n\n        git clone https://github.com/NervanaSystems/neon.git\n        cd neon\n        make\n        . .venv/bin/activate\n\nStarting after neon v2.2.0, the master branch of neon will be updated\nweekly with work-in-progress toward the next release. Check out a\nrelease tag (e.g., \"git checkout v2.2.0\") for a stable release. Or\nsimply check out the \"latest\" release tag to get the latest stable\nrelease (i.e., \"git checkout latest\")\n\n-  `Install via pypi <https://pypi.python.org/pypi/nervananeon>`__\n\nFrom version 2.4.0, we re-enabled pip install. Neon can be installed\nusing package name nervananeon.\n\n.. code:: bash\n\n        pip install nervananeon\n\nIt is noted that\n`aeon <https://aeon.nervanasys.com/index.html/getting_started.html>`__\nneeds to be installed separately. The latest release v2.6.0 uses aeon\nv1.3.0.\n\n**Warning**\n\n    Between neon v2.1.0 and v2.2.0, the aeon manifest file format has\n    been changed. When updating from neon < v2.2.0 manifests have to be\n    recreated using ingest scripts (in examples folder) or updated using\n    `this <neon/data/convert_manifest.py>`__ script.\n\nUse a script to run an example\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: bash\n\n        python examples/mnist_mlp.py \n\nSelecting a backend engine from the command line\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe gpu backend is selected by default, so the above command is\nequivalent to if a compatible GPU resource is found on the system:\n\n.. code:: bash\n\n        python examples/mnist_mlp.py -b gpu\n\nWhen no GPU is available, the **optimized** CPU (MKL) backend is now\nselected by default as of neon v2.1.0, which means the above command is\nnow equivalent to:\n\n.. code:: bash\n\n        python examples/mnist_mlp.py -b mkl\n\nIf you are interested in comparing the default mkl backend with the\nnon-optimized CPU backend, use the following command:\n\n.. code:: bash\n\n        python examples/mnist_mlp.py -b cpu\n\nUse a yaml file to run an example\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAlternatively, a yaml file may be used run an example.\n\n.. code:: bash\n\n        neon examples/mnist_mlp.yaml\n\nTo select a specific backend in a yaml file, add or modify a line that\ncontains ``backend: mkl`` to enable mkl backend, or ``backend: cpu`` to\nenable cpu backend. The gpu backend is selected by default if a GPU is\navailable.\n\nRecommended Settings for neon with MKL on Intel Architectures\n-------------------------------------------------------------\n\nThe Intel Math Kernel Library takes advantages of the parallelization\nand vectorization capabilities of Intel Xeon and Xeon Phi systems. When\nhyperthreading is enabled on the system, we recommend the following\nKMP\\_AFFINITY setting to make sure parallel threads are 1:1 mapped to\nthe available physical cores.\n\n.. code:: bash\n\n        export OMP_NUM_THREADS=<Number of Physical Cores>\n        export KMP_AFFINITY=compact,1,0,granularity=fine  \n\nor\n\n.. code:: bash\n\n        export OMP_NUM_THREADS=<Number of Physical Cores>\n        export KMP_AFFINITY=verbose,granularity=fine,proclist=[0-<Number of Physical Cores>],explicit\n\nFor more information about KMP\\_AFFINITY, please check\n`here <https://software.intel.com/en-us/node/522691>`__. We encourage\nusers to set out trying and establishing their own best performance\nsettings.\n\nDocumentation\n-------------\n\nThe complete documentation for neon is available\n`here <http://neon.nervanasys.com/docs/latest>`__. Some useful starting\npoints are:\n\n-  `Tutorials <http://neon.nervanasys.com/docs/latest/tutorials.html>`__\n   for neon\n-  `Overview <http://neon.nervanasys.com/docs/latest/overview.html>`__\n   of the neon workflow\n-  `API <http://neon.nervanasys.com/docs/latest/api.html>`__\n   documentation\n-  `Resources <http://neon.nervanasys.com/docs/latest/resources.html>`__\n   for neon and deep learning\n\nSupport\n-------\n\nFor any bugs or feature requests please:\n\n1. Search the open and closed `issues\n   list <https://github.com/NervanaSystems/neon/issues>`__ to see if\n   we're already working on what you have uncovered.\n2. Check that your issue/request hasn't already been addressed in our\n   `Frequently Asked Questions\n   (FAQ) <http://neon.nervanasys.com/docs/latest/faq.html>`__ or\n   `neon-users <https://groups.google.com/forum/#!forum/neon-users>`__\n   Google group.\n3. File a new `issue <https://github.com/NervanaSystems/neon/issues>`__\n   or submit a new `pull\n   request <https://github.com/NervanaSystems/neon/pulls>`__ if you have\n   some code you'd like to contribute\n\nFor other questions and discussions please post a message to the\n`neon-users <https://groups.google.com/forum/?hl=en#!forum/neon-users>`__\nGoogle group\n\nLicense\n-------\n\nWe are releasing `neon <https://github.com/NervanaSystems/neon>`__ under\nan open source `Apache\n2.0 <https://www.apache.org/licenses/LICENSE-2.0>`__ License. We welcome\nyou to `contact us <mailto:info@nervanasys.com>`__ with your use cases.\n\n\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://www.intelnervana.com", "keywords": "", "license": "License :: OSI Approved :: Apache Software License", "maintainer": "", "maintainer_email": "", "name": "nervananeon", "package_url": "https://pypi.org/project/nervananeon/", "platform": "", "project_url": "https://pypi.org/project/nervananeon/", "project_urls": {"Homepage": "http://www.intelnervana.com"}, "release_url": "https://pypi.org/project/nervananeon/2.6.0/", "requires_dist": ["pytest-mock", "configargparse", "numpy", "pyyaml", "pep8", "flake8", "pytest", "pytest-cov", "posix-ipc", "pillow", "pylint", "sphinx", "h5py", "appdirs", "future", "tqdm", "cffi", "filelock", "py-cpuinfo", "pypandoc", "pandoc"], "requires_python": "", "summary": "Intel's deep learning framework", "version": "2.6.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://github.com/NervanaSystems/neon\" rel=\"nofollow\">neon</a> is Intel\u2019s reference\ndeep learning framework committed to <a href=\"https://github.com/soumith/convnet-benchmarks\" rel=\"nofollow\">best\nperformance</a> on all\nhardware. Designed for ease-of-use and extensibility.</p>\n<ul>\n<li><a href=\"http://neon.nervanasys.com/docs/latest/tutorials.html\" rel=\"nofollow\">Tutorials</a>\nand <a href=\"https://github.com/NervanaSystems/meetup\" rel=\"nofollow\">iPython notebooks</a>\nto get users started with using neon for deep learning.</li>\n<li>Support for commonly used layers: convolution, RNN, LSTM, GRU,\nBatchNorm, and more.</li>\n<li><a href=\"https://github.com/NervanaSystems/ModelZoo\" rel=\"nofollow\">Model Zoo</a> contains\npre-trained weights and example scripts for start-of-the-art models,\nincluding:\n<a href=\"https://github.com/NervanaSystems/ModelZoo/tree/master/ImageClassification/ILSVRC2012/VGG\" rel=\"nofollow\">VGG</a>,\n<a href=\"https://github.com/NervanaSystems/ModelZoo/tree/master/DeepReinforcement\" rel=\"nofollow\">Reinforcement\nlearning</a>,\n<a href=\"https://github.com/NervanaSystems/ModelZoo/tree/master/SceneClassification/DeepResNet\" rel=\"nofollow\">Deep Residual\nNetworks</a>,\n<a href=\"https://github.com/NervanaSystems/ModelZoo/tree/master/ImageCaptioning\" rel=\"nofollow\">Image\nCaptioning</a>,\n<a href=\"https://github.com/NervanaSystems/ModelZoo/tree/master/NLP/SentimentClassification/IMDB\" rel=\"nofollow\">Sentiment\nanalysis</a>,\nand <a href=\"http://neon.nervanasys.com/docs/latest/model_zoo.html\" rel=\"nofollow\">more</a>.</li>\n<li>Swappable hardware backends: write code once and then deploy on CPUs,\nGPUs, or Nervana hardware</li>\n</ul>\n<p>For fast iteration and model exploration, neon has the fastest\nperformance among deep learning libraries (2x speed of cuDNNv4, see\n<a href=\"https://github.com/soumith/convnet-benchmarks\" rel=\"nofollow\">benchmarks</a>). *\n2.5s/macrobatch (3072 images) on AlexNet on Titan X (Full run on 1 GPU ~\n26 hrs) * Training VGG with 16-bit floating point on 1 Titan X takes\n~10 days (original paper: 4 GPUs for 2-3 weeks)</p>\n<p>We use neon internally at Intel Nervana to solve our customers\u2019 problems\nacross many <a href=\"http://www.nervanasys.com/solutions/\" rel=\"nofollow\">domains</a>. We are\nhiring across several roles. Apply\n<a href=\"http://www.nervanasys.com/careers/\" rel=\"nofollow\">here</a>!</p>\n<p>See the <a href=\"https://github.com/NervanaSystems/neon/blob/master/ChangeLog\" rel=\"nofollow\">new\nfeatures</a>\nin our latest release. We want to highlight that neon v2.0.0+ has been\noptimized for much better performance on CPUs by enabling Intel Math\nKernel Library (MKL). The DNN (Deep Neural Networks) component of MKL\nthat is used by neon is provided free of charge and downloaded\nautomatically as part of the neon installation.</p>\n<div id=\"quick-install\">\n<h2>Quick Install</h2>\n<ul>\n<li><a href=\"http://neon.nervanasys.com/docs/latest/installation.html\" rel=\"nofollow\">Local install and\ndependencies</a></li>\n</ul>\n<p>On a Mac OSX or Linux machine, enter the following to download and\ninstall neon (conda users see the\n<a href=\"http://neon.nervanasys.com/docs/latest/installation.html\" rel=\"nofollow\">guide</a>),\nand use it to train your first multi-layer perceptron. To force a\npython2 or python3 install, replace <tt>make</tt> below with either\n<tt>make python2</tt> or <tt>make python3</tt>.</p>\n<pre>git clone https://github.com/NervanaSystems/neon.git\n<span class=\"nb\">cd</span> neon\nmake\n. .venv/bin/activate\n</pre>\n<p>Starting after neon v2.2.0, the master branch of neon will be updated\nweekly with work-in-progress toward the next release. Check out a\nrelease tag (e.g., \u201cgit checkout v2.2.0\u201d) for a stable release. Or\nsimply check out the \u201clatest\u201d release tag to get the latest stable\nrelease (i.e., \u201cgit checkout latest\u201d)</p>\n<ul>\n<li><a href=\"https://pypi.python.org/pypi/nervananeon\" rel=\"nofollow\">Install via pypi</a></li>\n</ul>\n<p>From version 2.4.0, we re-enabled pip install. Neon can be installed\nusing package name nervananeon.</p>\n<pre>pip install nervananeon\n</pre>\n<p>It is noted that\n<a href=\"https://aeon.nervanasys.com/index.html/getting_started.html\" rel=\"nofollow\">aeon</a>\nneeds to be installed separately. The latest release v2.6.0 uses aeon\nv1.3.0.</p>\n<p><strong>Warning</strong></p>\n<blockquote>\nBetween neon v2.1.0 and v2.2.0, the aeon manifest file format has\nbeen changed. When updating from neon &lt; v2.2.0 manifests have to be\nrecreated using ingest scripts (in examples folder) or updated using\n<a href=\"neon/data/convert_manifest.py\" rel=\"nofollow\">this</a> script.</blockquote>\n<div id=\"use-a-script-to-run-an-example\">\n<h3>Use a script to run an example</h3>\n<pre>python examples/mnist_mlp.py\n</pre>\n<div id=\"selecting-a-backend-engine-from-the-command-line\">\n<h4>Selecting a backend engine from the command line</h4>\n<p>The gpu backend is selected by default, so the above command is\nequivalent to if a compatible GPU resource is found on the system:</p>\n<pre>python examples/mnist_mlp.py -b gpu\n</pre>\n<p>When no GPU is available, the <strong>optimized</strong> CPU (MKL) backend is now\nselected by default as of neon v2.1.0, which means the above command is\nnow equivalent to:</p>\n<pre>python examples/mnist_mlp.py -b mkl\n</pre>\n<p>If you are interested in comparing the default mkl backend with the\nnon-optimized CPU backend, use the following command:</p>\n<pre>python examples/mnist_mlp.py -b cpu\n</pre>\n</div>\n</div>\n<div id=\"use-a-yaml-file-to-run-an-example\">\n<h3>Use a yaml file to run an example</h3>\n<p>Alternatively, a yaml file may be used run an example.</p>\n<pre>neon examples/mnist_mlp.yaml\n</pre>\n<p>To select a specific backend in a yaml file, add or modify a line that\ncontains <tt>backend: mkl</tt> to enable mkl backend, or <tt>backend: cpu</tt> to\nenable cpu backend. The gpu backend is selected by default if a GPU is\navailable.</p>\n</div>\n</div>\n<div id=\"recommended-settings-for-neon-with-mkl-on-intel-architectures\">\n<h2>Recommended Settings for neon with MKL on Intel Architectures</h2>\n<p>The Intel Math Kernel Library takes advantages of the parallelization\nand vectorization capabilities of Intel Xeon and Xeon Phi systems. When\nhyperthreading is enabled on the system, we recommend the following\nKMP_AFFINITY setting to make sure parallel threads are 1:1 mapped to\nthe available physical cores.</p>\n<pre><span class=\"nb\">export</span> <span class=\"nv\">OMP_NUM_THREADS</span><span class=\"o\">=</span>&lt;Number of Physical Cores&gt;\n<span class=\"nb\">export</span> <span class=\"nv\">KMP_AFFINITY</span><span class=\"o\">=</span>compact,1,0,granularity<span class=\"o\">=</span>fine\n</pre>\n<p>or</p>\n<pre><span class=\"nb\">export</span> <span class=\"nv\">OMP_NUM_THREADS</span><span class=\"o\">=</span>&lt;Number of Physical Cores&gt;\n<span class=\"nb\">export</span> <span class=\"nv\">KMP_AFFINITY</span><span class=\"o\">=</span>verbose,granularity<span class=\"o\">=</span>fine,proclist<span class=\"o\">=[</span><span class=\"m\">0</span>-&lt;Number of Physical Cores&gt;<span class=\"o\">]</span>,explicit\n</pre>\n<p>For more information about KMP_AFFINITY, please check\n<a href=\"https://software.intel.com/en-us/node/522691\" rel=\"nofollow\">here</a>. We encourage\nusers to set out trying and establishing their own best performance\nsettings.</p>\n</div>\n<div id=\"documentation\">\n<h2>Documentation</h2>\n<p>The complete documentation for neon is available\n<a href=\"http://neon.nervanasys.com/docs/latest\" rel=\"nofollow\">here</a>. Some useful starting\npoints are:</p>\n<ul>\n<li><a href=\"http://neon.nervanasys.com/docs/latest/tutorials.html\" rel=\"nofollow\">Tutorials</a>\nfor neon</li>\n<li><a href=\"http://neon.nervanasys.com/docs/latest/overview.html\" rel=\"nofollow\">Overview</a>\nof the neon workflow</li>\n<li><a href=\"http://neon.nervanasys.com/docs/latest/api.html\" rel=\"nofollow\">API</a>\ndocumentation</li>\n<li><a href=\"http://neon.nervanasys.com/docs/latest/resources.html\" rel=\"nofollow\">Resources</a>\nfor neon and deep learning</li>\n</ul>\n</div>\n<div id=\"support\">\n<h2>Support</h2>\n<p>For any bugs or feature requests please:</p>\n<ol>\n<li>Search the open and closed <a href=\"https://github.com/NervanaSystems/neon/issues\" rel=\"nofollow\">issues\nlist</a> to see if\nwe\u2019re already working on what you have uncovered.</li>\n<li>Check that your issue/request hasn\u2019t already been addressed in our\n<a href=\"http://neon.nervanasys.com/docs/latest/faq.html\" rel=\"nofollow\">Frequently Asked Questions\n(FAQ)</a> or\n<a href=\"https://groups.google.com/forum/#!forum/neon-users\" rel=\"nofollow\">neon-users</a>\nGoogle group.</li>\n<li>File a new <a href=\"https://github.com/NervanaSystems/neon/issues\" rel=\"nofollow\">issue</a>\nor submit a new <a href=\"https://github.com/NervanaSystems/neon/pulls\" rel=\"nofollow\">pull\nrequest</a> if you have\nsome code you\u2019d like to contribute</li>\n</ol>\n<p>For other questions and discussions please post a message to the\n<a href=\"https://groups.google.com/forum/?hl=en#!forum/neon-users\" rel=\"nofollow\">neon-users</a>\nGoogle group</p>\n</div>\n<div id=\"license\">\n<h2>License</h2>\n<p>We are releasing <a href=\"https://github.com/NervanaSystems/neon\" rel=\"nofollow\">neon</a> under\nan open source <a href=\"https://www.apache.org/licenses/LICENSE-2.0\" rel=\"nofollow\">Apache\n2.0</a> License. We welcome\nyou to <a href=\"mailto:info%40nervanasys.com\">contact us</a> with your use cases.</p>\n</div>\n\n          </div>"}, "last_serial": 3466141, "releases": {"0.8.1": [{"comment_text": "", "digests": {"md5": "8d5f1bc7ab42d36c05ba6d9a98b2818a", "sha256": "310052781db4fa49daeb1f940b9c7d72dec7668b87df5536ce8f310d5d229559"}, "downloads": -1, "filename": "nervananeon-0.8.1.tar.gz", "has_sig": false, "md5_digest": "8d5f1bc7ab42d36c05ba6d9a98b2818a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 334150, "upload_time": "2015-05-04T22:59:25", "upload_time_iso_8601": "2015-05-04T22:59:25.192656Z", "url": "https://files.pythonhosted.org/packages/08/78/2d87a9c14b22caa2e53b6f6db42ec5d1f7b660279f01e4519b27535d7b30/nervananeon-0.8.1.tar.gz", "yanked": false}], "2.4.0": [{"comment_text": "", "digests": {"md5": "4845952b535c245d9eec2d537e050ebe", "sha256": "820bd4e26c86b0cfc7f7513a1f7c3cc810a2e3422f5d0696dddc8cc672ced650"}, "downloads": -1, "filename": "nervananeon-2.4.0-py2-none-any.whl", "has_sig": false, "md5_digest": "4845952b535c245d9eec2d537e050ebe", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 78422222, "upload_time": "2017-11-27T23:17:10", "upload_time_iso_8601": "2017-11-27T23:17:10.956923Z", "url": "https://files.pythonhosted.org/packages/c8/89/6bcdf004374d920635494fc25ee7db64d5b065b98674cf92852f4428a2c4/nervananeon-2.4.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "64407393235d9a6c44aa2a6142780133", "sha256": "f57d8d2928853ff6f7be6cb950a51b9aef5096e841c129f02fcf8f543bb53248"}, "downloads": -1, "filename": "nervananeon-2.4.0-py3-none-any.whl", "has_sig": false, "md5_digest": "64407393235d9a6c44aa2a6142780133", "packagetype": "bdist_wheel", "python_version": "3.5", "requires_python": null, "size": 78422166, "upload_time": "2017-11-27T23:26:50", "upload_time_iso_8601": "2017-11-27T23:26:50.119162Z", "url": "https://files.pythonhosted.org/packages/b7/4d/07964b639c3b04f6a2b4a2967722af0f76dd11ccdbf9cf25613d217b44f9/nervananeon-2.4.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2aa70073887d32c1726f290f80df76d7", "sha256": "a7eb96af0ce7cda14128c9b0ff850c827061840968d4ca8164cb469b513204ea"}, "downloads": -1, "filename": "nervananeon-2.4.0.tar.gz", "has_sig": false, "md5_digest": "2aa70073887d32c1726f290f80df76d7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 77527857, "upload_time": "2017-11-27T23:10:41", "upload_time_iso_8601": "2017-11-27T23:10:41.255840Z", "url": "https://files.pythonhosted.org/packages/0a/ca/7d310d4568f9ce6a2a609b707b9128976507f8d18fe90f8bf95ba345410f/nervananeon-2.4.0.tar.gz", "yanked": false}], "2.5.0": [{"comment_text": "", "digests": {"md5": "de9cebd04c3059b5a9c6c3fcc9d2bcb7", "sha256": "63a0c4bcc1268bfe5f99f9f9a6f065bd5d56113d8a0c52c2eb45fd5b0934b894"}, "downloads": -1, "filename": "nervananeon-2.5.0-py2-none-any.whl", "has_sig": false, "md5_digest": "de9cebd04c3059b5a9c6c3fcc9d2bcb7", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 78439915, "upload_time": "2017-12-21T18:44:53", "upload_time_iso_8601": "2017-12-21T18:44:53.482345Z", "url": "https://files.pythonhosted.org/packages/bc/8d/480f7440bd1cbdbc58b89a9cb8a19296832c3012c5a3481b87942a4d7f1d/nervananeon-2.5.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "67ae7f18f016d75d92f0ee97e7ac452d", "sha256": "0ddb8d00ef961192ac9009c7b93d6c40566f2e0c57a9360224598a87cc0f65b3"}, "downloads": -1, "filename": "nervananeon-2.5.0-py3-none-any.whl", "has_sig": false, "md5_digest": "67ae7f18f016d75d92f0ee97e7ac452d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 78439879, "upload_time": "2017-12-21T18:53:45", "upload_time_iso_8601": "2017-12-21T18:53:45.197679Z", "url": "https://files.pythonhosted.org/packages/f7/66/577548de6baa3435f0c717b05bce2c7e39cb431d61528a8cf649642ed535/nervananeon-2.5.0-py3-none-any.whl", "yanked": false}], "2.6.0": [{"comment_text": "", "digests": {"md5": "888477434d2aac5b235ccb3a11eab981", "sha256": "a61f80d557430a548107cf3c0062855ff6c399949c0b03eb07c3bb31f97f858f"}, "downloads": -1, "filename": "nervananeon-2.6.0-py2-none-any.whl", "has_sig": false, "md5_digest": "888477434d2aac5b235ccb3a11eab981", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 78531451, "upload_time": "2018-01-05T23:25:04", "upload_time_iso_8601": "2018-01-05T23:25:04.426670Z", "url": "https://files.pythonhosted.org/packages/68/6a/8dde6aa8ecef2ef7260d74c2cf75c0e6ed7d4262f43f2b62d579b5a778b5/nervananeon-2.6.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5d6a35291c9c3475a537d55266e25f14", "sha256": "e21fc8e9d713f8b3b43096ecf7717a7efe28f962ce26475ee1b2b3960ea54645"}, "downloads": -1, "filename": "nervananeon-2.6.0-py3-none-any.whl", "has_sig": false, "md5_digest": "5d6a35291c9c3475a537d55266e25f14", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 78531422, "upload_time": "2018-01-05T23:36:13", "upload_time_iso_8601": "2018-01-05T23:36:13.129832Z", "url": "https://files.pythonhosted.org/packages/c6/71/4e8f2c1ff5318d21952a80b83d8eeb6bf48c7c3a7f41ceac97d6e8b8e537/nervananeon-2.6.0-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "888477434d2aac5b235ccb3a11eab981", "sha256": "a61f80d557430a548107cf3c0062855ff6c399949c0b03eb07c3bb31f97f858f"}, "downloads": -1, "filename": "nervananeon-2.6.0-py2-none-any.whl", "has_sig": false, "md5_digest": "888477434d2aac5b235ccb3a11eab981", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 78531451, "upload_time": "2018-01-05T23:25:04", "upload_time_iso_8601": "2018-01-05T23:25:04.426670Z", "url": "https://files.pythonhosted.org/packages/68/6a/8dde6aa8ecef2ef7260d74c2cf75c0e6ed7d4262f43f2b62d579b5a778b5/nervananeon-2.6.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5d6a35291c9c3475a537d55266e25f14", "sha256": "e21fc8e9d713f8b3b43096ecf7717a7efe28f962ce26475ee1b2b3960ea54645"}, "downloads": -1, "filename": "nervananeon-2.6.0-py3-none-any.whl", "has_sig": false, "md5_digest": "5d6a35291c9c3475a537d55266e25f14", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 78531422, "upload_time": "2018-01-05T23:36:13", "upload_time_iso_8601": "2018-01-05T23:36:13.129832Z", "url": "https://files.pythonhosted.org/packages/c6/71/4e8f2c1ff5318d21952a80b83d8eeb6bf48c7c3a7f41ceac97d6e8b8e537/nervananeon-2.6.0-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 00:46:25 2020"}