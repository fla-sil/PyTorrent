{"info": {"author": "Wesley Baugh", "author_email": "wesley@bwbaugh.com", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Science/Research", "Natural Language :: English", "Programming Language :: Python", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: Implementation :: PyPy", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Information Analysis", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Text Processing :: Linguistic"], "description": "infer\r\n=====\r\n\r\nA machine learning toolkit for classification and assisted\r\nexperimentation.\r\n\r\n|Build Status|\r\n\r\nFeatures\r\n--------\r\n\r\nAssisted experimentation\r\n~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\nWe provide an experimental foundation for assisted experimentation by\r\nallowing the user to modularize the tasks that are common, but unique in\r\nimplementation, to every experiment, including knowing how to:\r\n\r\n-  Perform any required initial setup to get the model ready.\r\n-  Parse the training dataset.\r\n-  Parse the testing dataset.\r\n-  Train the model given a training instance.\r\n-  Make a prediction for a test instance using the current model.\r\n\r\nOnce the task specific implementations are known, we then coordinate the\r\nuse of those functions to perform the experiment in an automated fashion\r\nby automatically performing tasks that can be common to any experiment:\r\n\r\n-  Run an experiment given an implementation of all the modular tasks.\r\n-  Incrementally train the model and get the current model's\r\n   performance.\r\n-  Get the test instances that were incorrectly labeled using the\r\n   current model.\r\n\r\nFeature extraction\r\n~~~~~~~~~~~~~~~~~~\r\n\r\nA feature extractor takes a document and extracts features to be used\r\nwith a machine learning classifier (during training and prediction).\r\n\r\nWe wanted to provide a way to easily perform the common task of\r\nextracting textual features from documents, while at the same time\r\nmaking it easy for the researcher to experiment with the kinds of\r\nfeatures that are extracted. The researcher can currently specify:\r\n\r\n-  The function used to tokenize the raw document string.\r\n-  The range of n-grams to extract from the text\r\n\r\nClassification\r\n~~~~~~~~~~~~~~\r\n\r\nPerformance metrics\r\n^^^^^^^^^^^^^^^^^^^\r\n\r\nWe provide a means of measuring the performance of your classifier by\r\nproviding standard performance metrics, expanded to allow for\r\nmultinomial classifiers, including:\r\n\r\n-  Confusion matrix\r\n-  Accuracy\r\n-  Recall (average, weighted average, and per class)\r\n-  Precision (average, weighted average, and per class)\r\n-  F-measure with a selectable *beta* parameter (average, weighted\r\n   average, and per class)\r\n\r\n(Multinomial) Naive Bayes\r\n^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nSome of the existing implementations of Naive Bayes that are available\r\nin various libraries we have found to be very memory inefficient.\r\nBecause of this, we decided to write our own implementation that can\r\nhopefully be better optimized.\r\n\r\nIn addition, there are lots of ways you can experiment with using and\r\noptimizing the performance of Naive Bayes that we wanted to be able to\r\neasily experiment with.\r\n\r\nFeature selection\r\n~~~~~~~~~~~~~~~~~\r\n\r\nFeature selection is another tool that the researcher should be able to\r\nexperiment with.\r\n\r\nDictionary trimming\r\n^^^^^^^^^^^^^^^^^^^\r\n\r\nCurrently, we provide a form of feature selection that is similar to\r\ndictionary trimming, by having the classifier ignore all but the top-k\r\nmost frequent features. This often gives us 90% of the benefit of\r\nfeature selection without the work of computing more complex metrics.\r\n\r\nDictionary trimming normally involves making a pass over your dataset in\r\norder to extract the (feature) vocabulary. However, this is infeasible\r\nwhen you are attempting to learn in an online (streaming) setting, such\r\nas when your documents are continuously coming in, like tweets off of a\r\nstream from Twitter. To handle this case, we created a data structure\r\nthat *efficiently* keeps track of the top-k most common terms that is\r\nupdated while training. This allows O(1) checking if a feature is in the\r\ntop-k features *without* having to make a pass over the vocabulary to\r\nmake the check.\r\n\r\n.. |Build Status| image:: https://travis-ci.org/bwbaugh/infer.png?branch=master\r\n   :target: https://travis-ci.org/bwbaugh/infer", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://www.github.com/bwbaugh/infer", "keywords": "", "license": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License", "maintainer": "", "maintainer_email": "", "name": "infer", "package_url": "https://pypi.org/project/infer/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/infer/", "project_urls": {"Download": "UNKNOWN", "Homepage": "http://www.github.com/bwbaugh/infer"}, "release_url": "https://pypi.org/project/infer/0.1/", "requires_dist": null, "requires_python": null, "summary": "A machine learning toolkit for classification and assisted experimentation.", "version": "0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>A machine learning toolkit for classification and assisted\nexperimentation.</p>\n<p><a href=\"https://travis-ci.org/bwbaugh/infer\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/59b677f7db9aa8788cd0b46b75de46cae60f2a95/68747470733a2f2f7472617669732d63692e6f72672f627762617567682f696e6665722e706e673f6272616e63683d6d6173746572\"></a></p>\n<div id=\"features\">\n<h2>Features</h2>\n<div id=\"assisted-experimentation\">\n<h3>Assisted experimentation</h3>\n<p>We provide an experimental foundation for assisted experimentation by\nallowing the user to modularize the tasks that are common, but unique in\nimplementation, to every experiment, including knowing how to:</p>\n<ul>\n<li>Perform any required initial setup to get the model ready.</li>\n<li>Parse the training dataset.</li>\n<li>Parse the testing dataset.</li>\n<li>Train the model given a training instance.</li>\n<li>Make a prediction for a test instance using the current model.</li>\n</ul>\n<p>Once the task specific implementations are known, we then coordinate the\nuse of those functions to perform the experiment in an automated fashion\nby automatically performing tasks that can be common to any experiment:</p>\n<ul>\n<li>Run an experiment given an implementation of all the modular tasks.</li>\n<li>Incrementally train the model and get the current model\u2019s\nperformance.</li>\n<li>Get the test instances that were incorrectly labeled using the\ncurrent model.</li>\n</ul>\n</div>\n<div id=\"feature-extraction\">\n<h3>Feature extraction</h3>\n<p>A feature extractor takes a document and extracts features to be used\nwith a machine learning classifier (during training and prediction).</p>\n<p>We wanted to provide a way to easily perform the common task of\nextracting textual features from documents, while at the same time\nmaking it easy for the researcher to experiment with the kinds of\nfeatures that are extracted. The researcher can currently specify:</p>\n<ul>\n<li>The function used to tokenize the raw document string.</li>\n<li>The range of n-grams to extract from the text</li>\n</ul>\n</div>\n<div id=\"classification\">\n<h3>Classification</h3>\n<div id=\"performance-metrics\">\n<h4>Performance metrics</h4>\n<p>We provide a means of measuring the performance of your classifier by\nproviding standard performance metrics, expanded to allow for\nmultinomial classifiers, including:</p>\n<ul>\n<li>Confusion matrix</li>\n<li>Accuracy</li>\n<li>Recall (average, weighted average, and per class)</li>\n<li>Precision (average, weighted average, and per class)</li>\n<li>F-measure with a selectable <em>beta</em> parameter (average, weighted\naverage, and per class)</li>\n</ul>\n</div>\n<div id=\"multinomial-naive-bayes\">\n<h4>(Multinomial) Naive Bayes</h4>\n<p>Some of the existing implementations of Naive Bayes that are available\nin various libraries we have found to be very memory inefficient.\nBecause of this, we decided to write our own implementation that can\nhopefully be better optimized.</p>\n<p>In addition, there are lots of ways you can experiment with using and\noptimizing the performance of Naive Bayes that we wanted to be able to\neasily experiment with.</p>\n</div>\n</div>\n<div id=\"feature-selection\">\n<h3>Feature selection</h3>\n<p>Feature selection is another tool that the researcher should be able to\nexperiment with.</p>\n<div id=\"dictionary-trimming\">\n<h4>Dictionary trimming</h4>\n<p>Currently, we provide a form of feature selection that is similar to\ndictionary trimming, by having the classifier ignore all but the top-k\nmost frequent features. This often gives us 90% of the benefit of\nfeature selection without the work of computing more complex metrics.</p>\n<p>Dictionary trimming normally involves making a pass over your dataset in\norder to extract the (feature) vocabulary. However, this is infeasible\nwhen you are attempting to learn in an online (streaming) setting, such\nas when your documents are continuously coming in, like tweets off of a\nstream from Twitter. To handle this case, we created a data structure\nthat <em>efficiently</em> keeps track of the top-k most common terms that is\nupdated while training. This allows O(1) checking if a feature is in the\ntop-k features <em>without</em> having to make a pass over the vocabulary to\nmake the check.</p>\n</div>\n</div>\n</div>\n\n          </div>"}, "last_serial": 793269, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "dbb7eea841302ecf562a987608f53b68", "sha256": "af81d0f0e40b3db138d496ee4af46d117eef55bff594a890cc808379359ac5db"}, "downloads": -1, "filename": "infer-0.1.zip", "has_sig": false, "md5_digest": "dbb7eea841302ecf562a987608f53b68", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23364, "upload_time": "2013-03-22T06:32:27", "upload_time_iso_8601": "2013-03-22T06:32:27.315220Z", "url": "https://files.pythonhosted.org/packages/c1/e8/88d216f7abec7d439165bdd49dd5008e51cf885ee47be568784c00b8e1ba/infer-0.1.zip", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "dbb7eea841302ecf562a987608f53b68", "sha256": "af81d0f0e40b3db138d496ee4af46d117eef55bff594a890cc808379359ac5db"}, "downloads": -1, "filename": "infer-0.1.zip", "has_sig": false, "md5_digest": "dbb7eea841302ecf562a987608f53b68", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23364, "upload_time": "2013-03-22T06:32:27", "upload_time_iso_8601": "2013-03-22T06:32:27.315220Z", "url": "https://files.pythonhosted.org/packages/c1/e8/88d216f7abec7d439165bdd49dd5008e51cf885ee47be568784c00b8e1ba/infer-0.1.zip", "yanked": false}], "timestamp": "Fri May  8 00:56:13 2020"}