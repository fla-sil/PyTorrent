{"info": {"author": "", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Operating System :: OS Independent", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: Implementation :: CPython"], "description": "<h1 align=\"center\">Service Streamer</h1>\n\n<p align=\"center\">\nBoosting your Web Services of Deep Learning Applications. \n<a href=\"./README_zh.md\">\u4e2d\u6587README</a>\n</p>\n\n<p align=\"center\">\n</p>\n\n<p align=\"center\">\n  <a href=\"#what-is-service-streamer-\">What is Service Streamer ?</a> \u2022\n  <a href=\"#highlights\">Highlights</a> \u2022\n  <a href=\"#installation\">Installation</a> \u2022\n  <a href=\"#develop-bert-service-in-5-minutes\">Develop BERT Service in 5 Minutes</a> \u2022\n  <a href=\"#api\">API</a> \u2022\n  <a href=\"#benchmark\">Benchmark</a> \u2022\n  <a href=\"#faq\">FAQ</a> \u2022\n</p>\n\n<h6 align=\"center\">\n    <a href=\"https://travis-ci.org/ShannonAI/service-streamer\">\n        <img src=\"https://travis-ci.org/ShannonAI/service-streamer.svg?branch=master\" alt=\"Build status\">\n    </a>\n \u2022 Made by ShannonAI \u2022 :globe_with_meridians: <a href=\"http://www.shannonai.com/\">http://www.shannonai.com/</a>\n</h6>\n\n\n<h2 align=\"center\">What is Service Streamer ?</h2>\n\nA mini-batch collects data samples and is usually used in deep learning models. In this way, models can utilize the parallel computing capability of GPUs. However, requests from users for web services are usually discrete. If using conventional loop server or threaded server, GPUs will be idle dealing with one request at a time. And the latency time will be linearly increasing when there are concurrent user requests. \n\nServiceStreamer is a middleware for web service of machine learning applications. Queue requests from users are sampled into mini-batches. ServiceStreamer can significantly enhance the overall performance of the system by improving GPU utilization. \n\n<h2 align=\"center\">Highlights</h2>\n\n- :hatching_chick: **Easy to use**: Minor changes can speed up the model ten times. \n- :zap: **Fast processing speed**: Low latency for online inference of machine learning models. \n- :octopus: **Good expandability**: Easy to be applied to multi-GPU scenarios for handling enormous requests.\n- :crossed_swords: **Applicability**: Used with any web frameworks and/or deep learning frameworks. \n\n\n<h2 align=\"center\">Installation</h2>\n\nInstall ServiceStream by using `pip`\uff0crequires **Python >= 3.5** :\n```bash\npip install service_streamer \n```\n\n<h2 align=\"center\">Develop BERT Service in 5 Minutes</h2>\n\nWe provide a step-by-step tutorial for you to bring BERT online in 5 minutes. The service processes 1400 sentences per second.  \n\n``Text Infilling`` is a task in natural language processing: given a sentence with several words randomly removed, the model predicts those words removed through the given context. \n\n``BERT`` has attracted a lot of attention in these two years and it achieves State-Of-The-Art results across many nlp tasks. BERT utilizes \"Masked Language Model (MLM)\" as one of the pre-training objectives. MLM models randomly mask some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based on its context. MLM has similarities with text infilling. It is natural to introduce BERT to text infilling task. \n\n\n1. First, we define a model for text filling task [bert_model.py](./example/bert_model.py). The `predict` function accepts a batch of sentences and returns predicted position results of the `[MASK]` token. \n\n    ```python\n    class TextInfillingModel(object):\n        ...\n\n\n    batch = [\"twinkle twinkle [MASK] star.\",\n             \"Happy birthday to [MASK].\",\n             'the answer to life, the [MASK], and everything.']\n    model = TextInfillingModel()\n    outputs = model.predict(batch)\n    print(outputs)\n    # ['little', 'you', 'universe']\n    ```\n    **Note**: Please download pre-trained BERT model at first. \n\n\n2. Second, utilize [Flask](https://github.com/pallets/flask) to pack predicting interfaces to Web service. [flask_example.py](./example/flask_example.py)\n\n\n    ```python\n    model = TextInfillingModel()\n    @app.route(\"/naive\", methods=[\"GET\", \"POST\"])\n    def naive_predict():\n        if request.method == \"GET\":\n            inputs = request.args.getlist(\"s\")\n        else:\n            inputs = request.form.getlist(\"s\")\n        outputs = model.predict(inputs)\n        return jsonify(outputs)\n     \n    app.run(port=5005)\n    ```\n    \n    Please run [flask_example.py](./example/flask_example.py), then you will get a vanilla Web server. \n\n    ```bash\n    curl -X POST http://localhost:5005/naive -d 's=Happy birthday to [MASK].' \n    [\"you\"]\n    ```\n\n    At this time, your web server can only serve 12 requests per second. Please see [benchmark](#benchmark) for more details.\n\n\n3. Third, encapsulate model functions through `service_streamer`. Three lines of code make the prediction speed of BERT service reach 200+ sentences per second (16x faster).\n\n    ```python\n    from service_streamer import ThreadedStreamer\n    streamer = ThreadedStreamer(model.predict, batch_size=64, max_latency=0.1)\n\n    @app.route(\"/stream\", methods=[\"POST\"])\n    def stream_predict():\n        inputs = request.form.getlist(\"s\")\n        outputs = streamer.predict(inputs)\n        return jsonify(outputs)\n\n    app.run(port=5005, debug=False)\n    ```\n\n    Run [flask_example.py](./example/flask_example.py) and test the performance with [wrk](https://github.com/wg/wrk). \n\n    ```bash\n    ./wrk -t 2 -c 128 -d 20s --timeout=10s -s example/benchmark.lua http://127.0.0.1:5005/stream\n    ...\n    Requests/sec:    200.31\n    ```\n\n4. Finally, encapsulate models through ``Streamer`` and start service workers on multiple GPUs. ``Streamer`` further accelerates inference speed and achieves 1000+ sentences per second (80x faster). \n\n\n\n    ```python\n    from service_streamer import ManagedModel, Streamer\n\n    class ManagedBertModel(ManagedModel):\n\n        def init_model(self):\n            self.model = TextInfillingModel()\n\n        def predict(self, batch):\n            return self.model.predict(batch)\n\n    streamer = Streamer(ManagedBertModel, batch_size=64, max_latency=0.1, worker_num=8, cuda_devices=(0, 1, 2, 3))\n    app.run(port=5005, debug=False)\n    ```\n\n    8 gpu workers can be started and evenly distributed on 4 GPUs.\n\n\n<h2 align=\"center\">API</h2>\n\n#### Quick Start\n\nIn general, the inference speed will be faster by utilizing parallel computing.\n\n```python\noutputs = model.predict(batch_inputs)\n```\n\n**ServiceStreamer** is a middleware for web service of machine learning applications. Queue requests from users are scheduled into mini-batches and forward into GPU workers. ServiceStreamer sacrifices a certain delay (default maximum is 0.1s) and enhance the overall performance by improving the ratio of GPU utilization. \n\n\n```python\nfrom service_streamer import ThreadedStreamer\n\n# Encapsulate batch_predict function with Streamer\n\nstreamer = ThreadedStreamer(model.predict, batch_size=64, max_latency=0.1)\n\n# Replace model.predict with streamer.predict\n\noutputs = streamer.predict(batch_inputs)\n```\n\nStart web server on multi-threading (or coordination). Your server can usually achieve 10x (```batch_size/batch_per_request```) times faster by adding a few lines of code.\n\n\n#### Distributed GPU worker\n\nThe performance of web server (QPS) in practice is much higher than that of GPU model. We also support one web server with multiple GPU worker processes.\n\n```python\nfrom service_streamer import Streamer\n\n# Spawn releases 4 gpu worker processes\nstreamer = Streamer(model.predict, 64, 0.1, worker_num=4)\noutputs = streamer.predict(batch)\n```\n\n\n``Streamer`` uses ``spawn`` subprocesses to run gpu workers by default. ``Streamer`` uses interprocess queues to communicate and queue. It can distribute a large number of requests to multiple workers for processing.\n\nThen the prediction results of the model are returned to the corresponding web server in batches. And results are forwarded to the corresponding http response.\n\n```\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 390.116                Driver Version: 390.116                   |\n|-------------------------------+----------------------+----------------------+\n...\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      7574      C   /home/liuxin/nlp/venv/bin/python            1889MiB |\n|    1      7575      C   /home/liuxin/nlp/venv/bin/python            1889MiB |\n|    2      7576      C   /home/liuxin/nlp/venv/bin/python            1889MiB |\n|    3      7577      C   /home/liuxin/nlp/venv/bin/python            1889MiB |\n+-----------------------------------------------------------------------------+\n\n```\n\nThe above method is simple to define, but the main process initialization model takes up an extra portion of memory. And the model can only run on the same GPU.\nTherefore, we have provided the ```ManagedModel``` class to facilitate model lazy initialization and migration while supporting multiple GPUs.\n\n```python\nfrom service_streamer import ManagedModel\n\nclass ManagedBertModel(ManagedModel):\n\n    def init_model(self):\n        self.model = Model()\n\n    def predict(self, batch):\n        return self.model.predict(batch)\n\n\n# Spawn produces 4 gpu worker processes, which are evenly distributed on 0/1/2/3 GPU\nstreamer = Streamer(ManagedBertModel, 64, 0.1, worker_num=4, cuda_devices=(0, 1, 2, 3))\noutputs = streamer.predict(batch)\n```\n\n#### Distributed Web Server\n\nSome cpu-intensive calculations, such as image and text preprocessing, need to be done first in web server. The preprocessed data is then forward into GPU worker for predictions. CPU resources often become performance bottlenecks in practice. Therefore, we also provide the mode of multi-web servers matching (single or multiple) gpu workers.\n\n\nUse ```RedisStream``` to specify a unique Redis address for all web servers and gpu workers. \n\n\n```python\n# default parameters can be omitted and localhost:6379 is used.\nstreamer = RedisStreamer(redis_broker=\"172.22.22.22:6379\")\n```\n\nWe make use of ``gunicorn`` or ``uwsgi`` to implement reverse proxy and load balancing.\n\n```bash\ncd example\ngunicorn -c redis_streamer_gunicorn.py flask_example:app\n```\n\nEach request will be load balanced to each web server for cpu preprocessing, and then evenly distributed to gpu worker for model prediction.\n\n\n### Future API\n\nYou might be familiar with `future` if you have used any concurrent library. \nYou can use the Future API directly if you want to use ``service_streamer`` for queueing requests or distributed GPU computing and using scenario is not web service. \n\n\n```python\nfrom service_streamer import ThreadedStreamer\nstreamer = ThreadedStreamer(model.predict, 64, 0.1)\n\nxs = []\nfor i in range(200):\n    future = streamer.submit([[\"How\", \"are\", \"you\", \"?\"], [\"Fine\", \".\"], [\"Thank\", \"you\", \".\"]])\n    xs.append(future)\n\n\n# Get all instances of future object and wait for asynchronous responses. \nfor future in xs:\n    outputs = future.result()\n    print(outputs)\n```\n\n<h2 align=\"center\">Benchmark</h2>\n\n### Benchmark\n\nWe utilize [wrk](https://github.com/wg/wrk) to conduct benchmark test.\n\nTest examples and scripts can be found in [example](./example).\n\n\n### Environment\n\n*   gpu: Titan Xp\n*   cuda: 9.0\n*   pytorch: 1.1   \n\n### Single GPU process\n\n```bash\n# start flask threaded server\npython example/flask_example.py\n\n# benchmark naive api without service_streamer\n./wrk -t 4 -c 128 -d 20s --timeout=10s -s scripts/streamer.lua http://127.0.0.1:5005/naive\n# benchmark stream api with service_streamer\n./wrk -t 4 -c 128 -d 20s --timeout=10s -s scripts/streamer.lua http://127.0.0.1:5005/stream\n```\n\n| |Naive|ThreaedStreamer|Streamer|RedisStreamer\n|-|-|-|-|-|\n| qps | 12.78 | 207.59 | 321.70 | 372.45 |\n| latency  | 8440ms | 603.35ms | 392.66ms | 340.74ms |\n\n### Multiple GPU processes\n\nThe performance loss of the communications and load balancing mechanism of multi-gpu workers are verified compared with a single web server process.\n\nWe adopt gevent server because multi-threaded Flask server has become a performance bottleneck. Please refer to the [flask_multigpu_example.py](example/flask_multigpu_example.py)\n\n\n```bash\n./wrk -t 8 -c 512 -d 20s --timeout=10s -s scripts/streamer.lua http://127.0.0.1:5005/stream\n```\n\n| gpu_worker_num | Naive | ThreadedStreamer |Streamer|RedisStreamer\n|-|-|-|-|-|\n|1|11.62|211.02|362.69|365.80|\n|2|N/A|N/A|488.40|609.63|\n|4|N/A|N/A|494.20|1034.57|\n\n\n* ``Threaded Streamer`` Due to the limitation of Python GIL, multi-worker is meaningless. We conduct comparison studies using single GPU worker. \n\n* ``Streamer`` Performance improvement is not linear when it is greater than 2 gpu worker.\nThe utilization rate of CPU reaches 100. The bottleneck is CPU at this time and the performance issue of flask is the obstacle.  \n\n\n### Utilize Future API to start multiple GPU processes\n\nWe adopt [Future API](#future-api) to conduct multi-GPU benchmeark test locally in order to reduce the performance influence of web server. Please refer to code example in [future_example.py](example/future_example.py)\n\n\n| gpu_worker_num | Batched | ThreadedStreamer |Streamer|RedisStreamer\n|-|-|-|-|-|\n|1|422.883|401.01|399.26|384.79|\n|2|N/A|N/A|742.16|714.781|\n|4|N/A|N/A|1400.12|1356.47|\n\nIt can be seen that the performance of ``service_streamer`` is almost linearly related to the number of gpu workers. Communications of inter-process in ``service_streamer`` is more efficient than redis. \n\n<h2 align=\"center\">FAQ</h2>\n\n**Q:** using a model trained from [allennlp](https://github.com/allenai/allennlp),set ``worker_num=4`` of [Streamer](./service_streamer/service_streamer.py) during inference, what's the reason that 16-core cpu is full and speed is slower than [Streamer](./service_streamer/service_streamer.py) with ``worker_num=1``?\n\n**A:** for multi-process inference, if the model process data using numpy with multi-thread, it may cause cpu overheads, resulting in a multi-core computing speed that slower than a single core. This kind of problem may occur when using third-party libraries such as alennlp, spacy, etc. It could be solved by setting ``numpy threads``environment variables.\n   ```python\n   import os\n   os.environ[\"MKL_NUM_THREADS\"] = \"1\"  # export MKL_NUM_THREADS=1 \n   os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"  # export NUMEXPR_NUM_THREADS=1 \n   os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # export OMP_NUM_THREADS=1\n   import numpy\n   ```\n   make sure putting environment variables before ``import numpy``.\n\n**Q:** When using RedisStreamer, if there are only one redis broker and more than one model, the input batches may have different structure. How to deal with such situation?  \n\n**A:** Specify the prefix when initializing worker and streamer, each streamer will use a unique channel.  \n\nexample of initialiazing workers:  \n    \n```python\nfrom service_streamer import run_redis_workers_forever\nfrom bert_model import ManagedBertModel\n\nif __name__ == \"__main__\":\n    from multiprocessing import freeze_support\n    freeze_support()\n    run_redis_workers_forever(ManagedBertModel, 64, prefix='channel_1')\n    run_redis_workers_forever(ManagedBertModel, 64, prefix='channel_2')\n```\n\nexample of using streamer to have result:  \n    \n```python\nfrom service_streamer import RedisStreamer\n\nstreamer_1 = RedisStreaemr(prefix='channel_1')\nstreamer_2 = RedisStreaemr(prefix='channel_2')\n\n# predict\noutput_1 = streamer_1.predict(batch)\noutput_2 = streamer_2.predict(batch)\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/shannonAI", "keywords": "service_streamer", "license": "", "maintainer": "", "maintainer_email": "", "name": "service-streamer", "package_url": "https://pypi.org/project/service-streamer/", "platform": "", "project_url": "https://pypi.org/project/service-streamer/", "project_urls": {"Homepage": "https://github.com/shannonAI"}, "release_url": "https://pypi.org/project/service-streamer/0.1.2/", "requires_dist": null, "requires_python": ">=3.5", "summary": "Boosting your web service of deep learning applications", "version": "0.1.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Service Streamer</h1>\n<p align=\"center\">\nBoosting your Web Services of Deep Learning Applications. \n<a href=\"./README_zh.md\" rel=\"nofollow\">\u4e2d\u6587README</a>\n</p>\n<p align=\"center\">\n</p>\n<p align=\"center\">\n  <a href=\"#what-is-service-streamer-\" rel=\"nofollow\">What is Service Streamer ?</a> \u2022\n  <a href=\"#highlights\" rel=\"nofollow\">Highlights</a> \u2022\n  <a href=\"#installation\" rel=\"nofollow\">Installation</a> \u2022\n  <a href=\"#develop-bert-service-in-5-minutes\" rel=\"nofollow\">Develop BERT Service in 5 Minutes</a> \u2022\n  <a href=\"#api\" rel=\"nofollow\">API</a> \u2022\n  <a href=\"#benchmark\" rel=\"nofollow\">Benchmark</a> \u2022\n  <a href=\"#faq\" rel=\"nofollow\">FAQ</a> \u2022\n</p>\n<h6>\n    <a href=\"https://travis-ci.org/ShannonAI/service-streamer\" rel=\"nofollow\">\n        <img alt=\"Build status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d9236cb0c7a9d468bd98acb6eb4d42fad282aec0/68747470733a2f2f7472617669732d63692e6f72672f5368616e6e6f6e41492f736572766963652d73747265616d65722e7376673f6272616e63683d6d6173746572\">\n    </a>\n \u2022 Made by ShannonAI \u2022 :globe_with_meridians: <a href=\"http://www.shannonai.com/\" rel=\"nofollow\">http://www.shannonai.com/</a>\n</h6>\n<h2>What is Service Streamer ?</h2>\n<p>A mini-batch collects data samples and is usually used in deep learning models. In this way, models can utilize the parallel computing capability of GPUs. However, requests from users for web services are usually discrete. If using conventional loop server or threaded server, GPUs will be idle dealing with one request at a time. And the latency time will be linearly increasing when there are concurrent user requests.</p>\n<p>ServiceStreamer is a middleware for web service of machine learning applications. Queue requests from users are sampled into mini-batches. ServiceStreamer can significantly enhance the overall performance of the system by improving GPU utilization.</p>\n<h2>Highlights</h2>\n<ul>\n<li>:hatching_chick: <strong>Easy to use</strong>: Minor changes can speed up the model ten times.</li>\n<li>:zap: <strong>Fast processing speed</strong>: Low latency for online inference of machine learning models.</li>\n<li>:octopus: <strong>Good expandability</strong>: Easy to be applied to multi-GPU scenarios for handling enormous requests.</li>\n<li>:crossed_swords: <strong>Applicability</strong>: Used with any web frameworks and/or deep learning frameworks.</li>\n</ul>\n<h2>Installation</h2>\n<p>Install ServiceStream by using <code>pip</code>\uff0crequires <strong>Python &gt;= 3.5</strong> :</p>\n<pre>pip install service_streamer \n</pre>\n<h2>Develop BERT Service in 5 Minutes</h2>\n<p>We provide a step-by-step tutorial for you to bring BERT online in 5 minutes. The service processes 1400 sentences per second.</p>\n<p><code>Text Infilling</code> is a task in natural language processing: given a sentence with several words randomly removed, the model predicts those words removed through the given context.</p>\n<p><code>BERT</code> has attracted a lot of attention in these two years and it achieves State-Of-The-Art results across many nlp tasks. BERT utilizes \"Masked Language Model (MLM)\" as one of the pre-training objectives. MLM models randomly mask some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based on its context. MLM has similarities with text infilling. It is natural to introduce BERT to text infilling task.</p>\n<ol>\n<li>\n<p>First, we define a model for text filling task <a href=\"./example/bert_model.py\" rel=\"nofollow\">bert_model.py</a>. The <code>predict</code> function accepts a batch of sentences and returns predicted position results of the <code>[MASK]</code> token.</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">TextInfillingModel</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n    <span class=\"o\">...</span>\n\n\n<span class=\"n\">batch</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"twinkle twinkle [MASK] star.\"</span><span class=\"p\">,</span>\n         <span class=\"s2\">\"Happy birthday to [MASK].\"</span><span class=\"p\">,</span>\n         <span class=\"s1\">'the answer to life, the [MASK], and everything.'</span><span class=\"p\">]</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">TextInfillingModel</span><span class=\"p\">()</span>\n<span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">outputs</span><span class=\"p\">)</span>\n<span class=\"c1\"># ['little', 'you', 'universe']</span>\n</pre>\n<p><strong>Note</strong>: Please download pre-trained BERT model at first.</p>\n</li>\n<li>\n<p>Second, utilize <a href=\"https://github.com/pallets/flask\" rel=\"nofollow\">Flask</a> to pack predicting interfaces to Web service. <a href=\"./example/flask_example.py\" rel=\"nofollow\">flask_example.py</a></p>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">TextInfillingModel</span><span class=\"p\">()</span>\n<span class=\"nd\">@app</span><span class=\"o\">.</span><span class=\"n\">route</span><span class=\"p\">(</span><span class=\"s2\">\"/naive\"</span><span class=\"p\">,</span> <span class=\"n\">methods</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"GET\"</span><span class=\"p\">,</span> <span class=\"s2\">\"POST\"</span><span class=\"p\">])</span>\n<span class=\"k\">def</span> <span class=\"nf\">naive_predict</span><span class=\"p\">():</span>\n    <span class=\"k\">if</span> <span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">method</span> <span class=\"o\">==</span> <span class=\"s2\">\"GET\"</span><span class=\"p\">:</span>\n        <span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">getlist</span><span class=\"p\">(</span><span class=\"s2\">\"s\"</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">form</span><span class=\"o\">.</span><span class=\"n\">getlist</span><span class=\"p\">(</span><span class=\"s2\">\"s\"</span><span class=\"p\">)</span>\n    <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">jsonify</span><span class=\"p\">(</span><span class=\"n\">outputs</span><span class=\"p\">)</span>\n \n<span class=\"n\">app</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">port</span><span class=\"o\">=</span><span class=\"mi\">5005</span><span class=\"p\">)</span>\n</pre>\n<p>Please run <a href=\"./example/flask_example.py\" rel=\"nofollow\">flask_example.py</a>, then you will get a vanilla Web server.</p>\n<pre>curl -X POST http://localhost:5005/naive -d <span class=\"s1\">'s=Happy birthday to [MASK].'</span> \n<span class=\"o\">[</span><span class=\"s2\">\"you\"</span><span class=\"o\">]</span>\n</pre>\n<p>At this time, your web server can only serve 12 requests per second. Please see <a href=\"#benchmark\" rel=\"nofollow\">benchmark</a> for more details.</p>\n</li>\n<li>\n<p>Third, encapsulate model functions through <code>service_streamer</code>. Three lines of code make the prediction speed of BERT service reach 200+ sentences per second (16x faster).</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">service_streamer</span> <span class=\"kn\">import</span> <span class=\"n\">ThreadedStreamer</span>\n<span class=\"n\">streamer</span> <span class=\"o\">=</span> <span class=\"n\">ThreadedStreamer</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">max_latency</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n\n<span class=\"nd\">@app</span><span class=\"o\">.</span><span class=\"n\">route</span><span class=\"p\">(</span><span class=\"s2\">\"/stream\"</span><span class=\"p\">,</span> <span class=\"n\">methods</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"POST\"</span><span class=\"p\">])</span>\n<span class=\"k\">def</span> <span class=\"nf\">stream_predict</span><span class=\"p\">():</span>\n    <span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">form</span><span class=\"o\">.</span><span class=\"n\">getlist</span><span class=\"p\">(</span><span class=\"s2\">\"s\"</span><span class=\"p\">)</span>\n    <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">streamer</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">jsonify</span><span class=\"p\">(</span><span class=\"n\">outputs</span><span class=\"p\">)</span>\n\n<span class=\"n\">app</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">port</span><span class=\"o\">=</span><span class=\"mi\">5005</span><span class=\"p\">,</span> <span class=\"n\">debug</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n</pre>\n<p>Run <a href=\"./example/flask_example.py\" rel=\"nofollow\">flask_example.py</a> and test the performance with <a href=\"https://github.com/wg/wrk\" rel=\"nofollow\">wrk</a>.</p>\n<pre>./wrk -t <span class=\"m\">2</span> -c <span class=\"m\">128</span> -d 20s --timeout<span class=\"o\">=</span>10s -s example/benchmark.lua http://127.0.0.1:5005/stream\n...\nRequests/sec:    <span class=\"m\">200</span>.31\n</pre>\n</li>\n<li>\n<p>Finally, encapsulate models through <code>Streamer</code> and start service workers on multiple GPUs. <code>Streamer</code> further accelerates inference speed and achieves 1000+ sentences per second (80x faster).</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">service_streamer</span> <span class=\"kn\">import</span> <span class=\"n\">ManagedModel</span><span class=\"p\">,</span> <span class=\"n\">Streamer</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">ManagedBertModel</span><span class=\"p\">(</span><span class=\"n\">ManagedModel</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">init_model</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">TextInfillingModel</span><span class=\"p\">()</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n\n<span class=\"n\">streamer</span> <span class=\"o\">=</span> <span class=\"n\">Streamer</span><span class=\"p\">(</span><span class=\"n\">ManagedBertModel</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">max_latency</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">worker_num</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"n\">cuda_devices</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">))</span>\n<span class=\"n\">app</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">port</span><span class=\"o\">=</span><span class=\"mi\">5005</span><span class=\"p\">,</span> <span class=\"n\">debug</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n</pre>\n<p>8 gpu workers can be started and evenly distributed on 4 GPUs.</p>\n</li>\n</ol>\n<h2>API</h2>\n<h4>Quick Start</h4>\n<p>In general, the inference speed will be faster by utilizing parallel computing.</p>\n<pre><span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">batch_inputs</span><span class=\"p\">)</span>\n</pre>\n<p><strong>ServiceStreamer</strong> is a middleware for web service of machine learning applications. Queue requests from users are scheduled into mini-batches and forward into GPU workers. ServiceStreamer sacrifices a certain delay (default maximum is 0.1s) and enhance the overall performance by improving the ratio of GPU utilization.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">service_streamer</span> <span class=\"kn\">import</span> <span class=\"n\">ThreadedStreamer</span>\n\n<span class=\"c1\"># Encapsulate batch_predict function with Streamer</span>\n\n<span class=\"n\">streamer</span> <span class=\"o\">=</span> <span class=\"n\">ThreadedStreamer</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">max_latency</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Replace model.predict with streamer.predict</span>\n\n<span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">streamer</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">batch_inputs</span><span class=\"p\">)</span>\n</pre>\n<p>Start web server on multi-threading (or coordination). Your server can usually achieve 10x (<code>batch_size/batch_per_request</code>) times faster by adding a few lines of code.</p>\n<h4>Distributed GPU worker</h4>\n<p>The performance of web server (QPS) in practice is much higher than that of GPU model. We also support one web server with multiple GPU worker processes.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">service_streamer</span> <span class=\"kn\">import</span> <span class=\"n\">Streamer</span>\n\n<span class=\"c1\"># Spawn releases 4 gpu worker processes</span>\n<span class=\"n\">streamer</span> <span class=\"o\">=</span> <span class=\"n\">Streamer</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">worker_num</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">streamer</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n</pre>\n<p><code>Streamer</code> uses <code>spawn</code> subprocesses to run gpu workers by default. <code>Streamer</code> uses interprocess queues to communicate and queue. It can distribute a large number of requests to multiple workers for processing.</p>\n<p>Then the prediction results of the model are returned to the corresponding web server in batches. And results are forwarded to the corresponding http response.</p>\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 390.116                Driver Version: 390.116                   |\n|-------------------------------+----------------------+----------------------+\n...\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      7574      C   /home/liuxin/nlp/venv/bin/python            1889MiB |\n|    1      7575      C   /home/liuxin/nlp/venv/bin/python            1889MiB |\n|    2      7576      C   /home/liuxin/nlp/venv/bin/python            1889MiB |\n|    3      7577      C   /home/liuxin/nlp/venv/bin/python            1889MiB |\n+-----------------------------------------------------------------------------+\n\n</code></pre>\n<p>The above method is simple to define, but the main process initialization model takes up an extra portion of memory. And the model can only run on the same GPU.\nTherefore, we have provided the <code>ManagedModel</code> class to facilitate model lazy initialization and migration while supporting multiple GPUs.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">service_streamer</span> <span class=\"kn\">import</span> <span class=\"n\">ManagedModel</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">ManagedBertModel</span><span class=\"p\">(</span><span class=\"n\">ManagedModel</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">init_model</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">()</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n\n\n<span class=\"c1\"># Spawn produces 4 gpu worker processes, which are evenly distributed on 0/1/2/3 GPU</span>\n<span class=\"n\">streamer</span> <span class=\"o\">=</span> <span class=\"n\">Streamer</span><span class=\"p\">(</span><span class=\"n\">ManagedBertModel</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">worker_num</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">cuda_devices</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">))</span>\n<span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">streamer</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n</pre>\n<h4>Distributed Web Server</h4>\n<p>Some cpu-intensive calculations, such as image and text preprocessing, need to be done first in web server. The preprocessed data is then forward into GPU worker for predictions. CPU resources often become performance bottlenecks in practice. Therefore, we also provide the mode of multi-web servers matching (single or multiple) gpu workers.</p>\n<p>Use <code>RedisStream</code> to specify a unique Redis address for all web servers and gpu workers.</p>\n<pre><span class=\"c1\"># default parameters can be omitted and localhost:6379 is used.</span>\n<span class=\"n\">streamer</span> <span class=\"o\">=</span> <span class=\"n\">RedisStreamer</span><span class=\"p\">(</span><span class=\"n\">redis_broker</span><span class=\"o\">=</span><span class=\"s2\">\"172.22.22.22:6379\"</span><span class=\"p\">)</span>\n</pre>\n<p>We make use of <code>gunicorn</code> or <code>uwsgi</code> to implement reverse proxy and load balancing.</p>\n<pre><span class=\"nb\">cd</span> example\ngunicorn -c redis_streamer_gunicorn.py flask_example:app\n</pre>\n<p>Each request will be load balanced to each web server for cpu preprocessing, and then evenly distributed to gpu worker for model prediction.</p>\n<h3>Future API</h3>\n<p>You might be familiar with <code>future</code> if you have used any concurrent library.\nYou can use the Future API directly if you want to use <code>service_streamer</code> for queueing requests or distributed GPU computing and using scenario is not web service.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">service_streamer</span> <span class=\"kn\">import</span> <span class=\"n\">ThreadedStreamer</span>\n<span class=\"n\">streamer</span> <span class=\"o\">=</span> <span class=\"n\">ThreadedStreamer</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">)</span>\n\n<span class=\"n\">xs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">200</span><span class=\"p\">):</span>\n    <span class=\"n\">future</span> <span class=\"o\">=</span> <span class=\"n\">streamer</span><span class=\"o\">.</span><span class=\"n\">submit</span><span class=\"p\">([[</span><span class=\"s2\">\"How\"</span><span class=\"p\">,</span> <span class=\"s2\">\"are\"</span><span class=\"p\">,</span> <span class=\"s2\">\"you\"</span><span class=\"p\">,</span> <span class=\"s2\">\"?\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Fine\"</span><span class=\"p\">,</span> <span class=\"s2\">\".\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"Thank\"</span><span class=\"p\">,</span> <span class=\"s2\">\"you\"</span><span class=\"p\">,</span> <span class=\"s2\">\".\"</span><span class=\"p\">]])</span>\n    <span class=\"n\">xs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">future</span><span class=\"p\">)</span>\n\n\n<span class=\"c1\"># Get all instances of future object and wait for asynchronous responses. </span>\n<span class=\"k\">for</span> <span class=\"n\">future</span> <span class=\"ow\">in</span> <span class=\"n\">xs</span><span class=\"p\">:</span>\n    <span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">future</span><span class=\"o\">.</span><span class=\"n\">result</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">outputs</span><span class=\"p\">)</span>\n</pre>\n<h2>Benchmark</h2>\n<h3>Benchmark</h3>\n<p>We utilize <a href=\"https://github.com/wg/wrk\" rel=\"nofollow\">wrk</a> to conduct benchmark test.</p>\n<p>Test examples and scripts can be found in <a href=\"./example\" rel=\"nofollow\">example</a>.</p>\n<h3>Environment</h3>\n<ul>\n<li>gpu: Titan Xp</li>\n<li>cuda: 9.0</li>\n<li>pytorch: 1.1</li>\n</ul>\n<h3>Single GPU process</h3>\n<pre><span class=\"c1\"># start flask threaded server</span>\npython example/flask_example.py\n\n<span class=\"c1\"># benchmark naive api without service_streamer</span>\n./wrk -t <span class=\"m\">4</span> -c <span class=\"m\">128</span> -d 20s --timeout<span class=\"o\">=</span>10s -s scripts/streamer.lua http://127.0.0.1:5005/naive\n<span class=\"c1\"># benchmark stream api with service_streamer</span>\n./wrk -t <span class=\"m\">4</span> -c <span class=\"m\">128</span> -d 20s --timeout<span class=\"o\">=</span>10s -s scripts/streamer.lua http://127.0.0.1:5005/stream\n</pre>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Naive</th>\n<th>ThreaedStreamer</th>\n<th>Streamer</th>\n<th>RedisStreamer</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>qps</td>\n<td>12.78</td>\n<td>207.59</td>\n<td>321.70</td>\n<td>372.45</td>\n</tr>\n<tr>\n<td>latency</td>\n<td>8440ms</td>\n<td>603.35ms</td>\n<td>392.66ms</td>\n<td>340.74ms</td>\n</tr></tbody></table>\n<h3>Multiple GPU processes</h3>\n<p>The performance loss of the communications and load balancing mechanism of multi-gpu workers are verified compared with a single web server process.</p>\n<p>We adopt gevent server because multi-threaded Flask server has become a performance bottleneck. Please refer to the <a href=\"example/flask_multigpu_example.py\" rel=\"nofollow\">flask_multigpu_example.py</a></p>\n<pre>./wrk -t <span class=\"m\">8</span> -c <span class=\"m\">512</span> -d 20s --timeout<span class=\"o\">=</span>10s -s scripts/streamer.lua http://127.0.0.1:5005/stream\n</pre>\n<table>\n<thead>\n<tr>\n<th>gpu_worker_num</th>\n<th>Naive</th>\n<th>ThreadedStreamer</th>\n<th>Streamer</th>\n<th>RedisStreamer</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>11.62</td>\n<td>211.02</td>\n<td>362.69</td>\n<td>365.80</td>\n</tr>\n<tr>\n<td>2</td>\n<td>N/A</td>\n<td>N/A</td>\n<td>488.40</td>\n<td>609.63</td>\n</tr>\n<tr>\n<td>4</td>\n<td>N/A</td>\n<td>N/A</td>\n<td>494.20</td>\n<td>1034.57</td>\n</tr></tbody></table>\n<ul>\n<li>\n<p><code>Threaded Streamer</code> Due to the limitation of Python GIL, multi-worker is meaningless. We conduct comparison studies using single GPU worker.</p>\n</li>\n<li>\n<p><code>Streamer</code> Performance improvement is not linear when it is greater than 2 gpu worker.\nThe utilization rate of CPU reaches 100. The bottleneck is CPU at this time and the performance issue of flask is the obstacle.</p>\n</li>\n</ul>\n<h3>Utilize Future API to start multiple GPU processes</h3>\n<p>We adopt <a href=\"#future-api\" rel=\"nofollow\">Future API</a> to conduct multi-GPU benchmeark test locally in order to reduce the performance influence of web server. Please refer to code example in <a href=\"example/future_example.py\" rel=\"nofollow\">future_example.py</a></p>\n<table>\n<thead>\n<tr>\n<th>gpu_worker_num</th>\n<th>Batched</th>\n<th>ThreadedStreamer</th>\n<th>Streamer</th>\n<th>RedisStreamer</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>422.883</td>\n<td>401.01</td>\n<td>399.26</td>\n<td>384.79</td>\n</tr>\n<tr>\n<td>2</td>\n<td>N/A</td>\n<td>N/A</td>\n<td>742.16</td>\n<td>714.781</td>\n</tr>\n<tr>\n<td>4</td>\n<td>N/A</td>\n<td>N/A</td>\n<td>1400.12</td>\n<td>1356.47</td>\n</tr></tbody></table>\n<p>It can be seen that the performance of <code>service_streamer</code> is almost linearly related to the number of gpu workers. Communications of inter-process in <code>service_streamer</code> is more efficient than redis.</p>\n<h2>FAQ</h2>\n<p><strong>Q:</strong> using a model trained from <a href=\"https://github.com/allenai/allennlp\" rel=\"nofollow\">allennlp</a>,set <code>worker_num=4</code> of <a href=\"./service_streamer/service_streamer.py\" rel=\"nofollow\">Streamer</a> during inference, what's the reason that 16-core cpu is full and speed is slower than <a href=\"./service_streamer/service_streamer.py\" rel=\"nofollow\">Streamer</a> with <code>worker_num=1</code>?</p>\n<p><strong>A:</strong> for multi-process inference, if the model process data using numpy with multi-thread, it may cause cpu overheads, resulting in a multi-core computing speed that slower than a single core. This kind of problem may occur when using third-party libraries such as alennlp, spacy, etc. It could be solved by setting <code>numpy threads</code>environment variables.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"MKL_NUM_THREADS\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"1\"</span>  <span class=\"c1\"># export MKL_NUM_THREADS=1 </span>\n<span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"NUMEXPR_NUM_THREADS\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"1\"</span>  <span class=\"c1\"># export NUMEXPR_NUM_THREADS=1 </span>\n<span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s2\">\"OMP_NUM_THREADS\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">\"1\"</span>  <span class=\"c1\"># export OMP_NUM_THREADS=1</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span>\n</pre>\n<p>make sure putting environment variables before <code>import numpy</code>.</p>\n<p><strong>Q:</strong> When using RedisStreamer, if there are only one redis broker and more than one model, the input batches may have different structure. How to deal with such situation?</p>\n<p><strong>A:</strong> Specify the prefix when initializing worker and streamer, each streamer will use a unique channel.</p>\n<p>example of initialiazing workers:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">service_streamer</span> <span class=\"kn\">import</span> <span class=\"n\">run_redis_workers_forever</span>\n<span class=\"kn\">from</span> <span class=\"nn\">bert_model</span> <span class=\"kn\">import</span> <span class=\"n\">ManagedBertModel</span>\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">\"__main__\"</span><span class=\"p\">:</span>\n    <span class=\"kn\">from</span> <span class=\"nn\">multiprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">freeze_support</span>\n    <span class=\"n\">freeze_support</span><span class=\"p\">()</span>\n    <span class=\"n\">run_redis_workers_forever</span><span class=\"p\">(</span><span class=\"n\">ManagedBertModel</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">prefix</span><span class=\"o\">=</span><span class=\"s1\">'channel_1'</span><span class=\"p\">)</span>\n    <span class=\"n\">run_redis_workers_forever</span><span class=\"p\">(</span><span class=\"n\">ManagedBertModel</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">prefix</span><span class=\"o\">=</span><span class=\"s1\">'channel_2'</span><span class=\"p\">)</span>\n</pre>\n<p>example of using streamer to have result:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">service_streamer</span> <span class=\"kn\">import</span> <span class=\"n\">RedisStreamer</span>\n\n<span class=\"n\">streamer_1</span> <span class=\"o\">=</span> <span class=\"n\">RedisStreaemr</span><span class=\"p\">(</span><span class=\"n\">prefix</span><span class=\"o\">=</span><span class=\"s1\">'channel_1'</span><span class=\"p\">)</span>\n<span class=\"n\">streamer_2</span> <span class=\"o\">=</span> <span class=\"n\">RedisStreaemr</span><span class=\"p\">(</span><span class=\"n\">prefix</span><span class=\"o\">=</span><span class=\"s1\">'channel_2'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># predict</span>\n<span class=\"n\">output_1</span> <span class=\"o\">=</span> <span class=\"n\">streamer_1</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n<span class=\"n\">output_2</span> <span class=\"o\">=</span> <span class=\"n\">streamer_2</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n</pre>\n\n          </div>"}, "last_serial": 5790739, "releases": {"0.0.2": [{"comment_text": "", "digests": {"md5": "696dd80458bd72da1758b51137b64f92", "sha256": "cf396742d3718f57d04056f18976dd174af0e93f6df21451ab0376db62b98395"}, "downloads": -1, "filename": "service_streamer-0.0.2.tar.gz", "has_sig": false, "md5_digest": "696dd80458bd72da1758b51137b64f92", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10577, "upload_time": "2019-08-03T09:54:23", "upload_time_iso_8601": "2019-08-03T09:54:23.304676Z", "url": "https://files.pythonhosted.org/packages/99/45/51262de6418dea1765fa6592fb210d6184fbc4282fba70b9840b6d6ff155/service_streamer-0.0.2.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "00e35d8e566f630fe597a133dbe4e204", "sha256": "d1fa674017e6f9befe222940f75ce5286e93a0aa1e8f2d48d527d0a8f81913c2"}, "downloads": -1, "filename": "service_streamer-0.1.0.tar.gz", "has_sig": false, "md5_digest": "00e35d8e566f630fe597a133dbe4e204", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10436, "upload_time": "2019-08-20T07:33:22", "upload_time_iso_8601": "2019-08-20T07:33:22.055562Z", "url": "https://files.pythonhosted.org/packages/bb/40/ab5c9c616b4ced2e76f0692536252360a0fde1e8bf093c339889649e587a/service_streamer-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "11761f8b1fa0bd288d20bf4f5101b3e0", "sha256": "f385a54f77673619686383d99cd025ce892ded692ea55e81e3813418cdb0bcfd"}, "downloads": -1, "filename": "service_streamer-0.1.1.tar.gz", "has_sig": false, "md5_digest": "11761f8b1fa0bd288d20bf4f5101b3e0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 11337, "upload_time": "2019-08-30T02:06:50", "upload_time_iso_8601": "2019-08-30T02:06:50.040135Z", "url": "https://files.pythonhosted.org/packages/06/18/07d330c794abf537cce71defc85e2c46794d6652c9c24479d63f4381a6b5/service_streamer-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "e0af6c517b50206ae1b0ec01f16041d3", "sha256": "097ec1a139e8d7478804326644d681c9b2d635fd14c938d9ce084731c74a8cfd"}, "downloads": -1, "filename": "service_streamer-0.1.2.tar.gz", "has_sig": false, "md5_digest": "e0af6c517b50206ae1b0ec01f16041d3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 11544, "upload_time": "2019-09-06T08:08:32", "upload_time_iso_8601": "2019-09-06T08:08:32.150833Z", "url": "https://files.pythonhosted.org/packages/0d/b8/847f04820858bf8f0abb3c68b0f14216aed18b86d2ce7051fe85c14984f7/service_streamer-0.1.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e0af6c517b50206ae1b0ec01f16041d3", "sha256": "097ec1a139e8d7478804326644d681c9b2d635fd14c938d9ce084731c74a8cfd"}, "downloads": -1, "filename": "service_streamer-0.1.2.tar.gz", "has_sig": false, "md5_digest": "e0af6c517b50206ae1b0ec01f16041d3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 11544, "upload_time": "2019-09-06T08:08:32", "upload_time_iso_8601": "2019-09-06T08:08:32.150833Z", "url": "https://files.pythonhosted.org/packages/0d/b8/847f04820858bf8f0abb3c68b0f14216aed18b86d2ce7051fe85c14984f7/service_streamer-0.1.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:14:53 2020"}