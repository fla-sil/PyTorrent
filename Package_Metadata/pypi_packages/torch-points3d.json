{"info": {"author": "Thomas Chaton", "author_email": "thomas.chaton.ai@gmail.com", "bugtrack_url": null, "classifiers": ["Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "<p align=\"center\">\n  <img width=\"40%\" src=\"https://raw.githubusercontent.com/nicolas-chaulet/torch-points3d/master/docs/logo.png\" />\n</p>\n\n[![codecov](https://codecov.io/gh/nicolas-chaulet/torch-points3d/branch/master/graph/badge.svg)](https://codecov.io/gh/nicolas-chaulet/torch-points3d) [![Actions Status](https://github.com/nicolas-chaulet/torch-points3d/workflows/unittest/badge.svg)](https://github.com/nicolas-chaulet/torch-points3d/actions) [![Documentation Status](https://readthedocs.org/projects/torch-points3d/badge/?version=latest)](https://torch-points3d.readthedocs.io/en/latest/?badge=latest)\n\nThis is a framework for running common deep learning models for point cloud analysis tasks against classic benchmark. It heavily relies on [Pytorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/notes/resources.html) and [Facebook Hydra](https://hydra.cc/).\n\nThe framework allows lean and yet complex model to be built with minimum effort and great reproducibility.\n\n## Project structure\n\n```bash\n\u251c\u2500 benchmark               # Output from various benchmark runs\n\u251c\u2500 conf                    # All configurations for training nad evaluation leave there\n\u251c\u2500 dashboard               # A collection of notebooks that allow result exploration and network debugging\n\u251c\u2500 docker                  # Docker image that can be used for inference or training\n\u251c\u2500 docs                    # All the doc\n\u251c\u2500 eval.py                 # Eval script\n\u251c\u2500 find_neighbour_dist.py  # Script to find optimal #neighbours within neighbour search operations\n\u251c\u2500 forward_scripts         # Script that runs a forward pass on possibly non annotated data\n\u251c\u2500 outputs                 # All outputs from your runs sorted by date\n\u251c\u2500 scripts                 # Some scripts to help manage the project\n\u251c\u2500 torch_points3d\n    \u251c\u2500 core                # Core components\n    \u251c\u2500 datasets            # All code related to datasets\n    \u251c\u2500 metrics             # All metrics and trackers\n    \u251c\u2500 models              # All models\n    \u251c\u2500 modules             # Basic modules that can be used in a modular way\n    \u251c\u2500 utils               # Various utils\n    \u2514\u2500 visualization       # Visualization\n\u251c\u2500 test\n\u2514\u2500 train.py                # Main script to launch a training\n```\n\nAs a general philosophy we have split datasets and models by task. For example, datasets has three subfolders:\n\n- segmentation\n- classification\n- registration\n\nwhere each folder contains the dataset related to each task.\n\n## Methods currently implemented\n\n- **[PointNet](https://github.com/nicolas-chaulet/torch-points3d/blob/master/torch_points3d/modules/PointNet/modules.py#L54)** from Charles R. Qi _et al._: [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593) (CVPR 2017)\n- **[PointNet++](https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/modules/pointnet2)** from Charles from Charles R. Qi _et al._: [PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](https://arxiv.org/abs/1706.02413)\n- **[RSConv](https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/modules/RSConv)** from Yongcheng Liu _et al._: [Relation-Shape Convolutional Neural Network for Point Cloud Analysis](https://arxiv.org/abs/1904.07601) (CVPR 2019)\n- **[RandLA-Net](https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/modules/RandLANet)** from Qingyong Hu _et al._: [RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds](https://arxiv.org/abs/1911.11236)\n- **[PointCNN](https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/modules/PointCNN)** from Yangyan Li _et al._: [PointCNN: Convolution On X-Transformed Points](https://arxiv.org/abs/1801.07791) (NIPS 2018)\n- **[KPConv](https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/modules/KPConv)** from Hugues Thomas _et al._: [KPConv: Flexible and Deformable Convolution for Point Clouds](https://arxiv.org/abs/1801.07791) (ICCV 2019)\n- **[MinkowskiEngine](https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/modules/MinkowskiEngine)** from Christopher Choy _et al._: [4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks](https://arxiv.org/abs/1904.08755) (CVPR'19)\n\n## Available datasets\n\n### Segmentation\n\n- **[Scannet](https://github.com/ScanNet/ScanNet)** from Angela Dai _et al._: [ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes](https://arxiv.org/abs/1702.04405)\n\n- **[S3DIS](http://buildingparser.stanford.edu/dataset.html)** from Iro Armeni _et al._: [Joint 2D-3D-Semantic Data for Indoor Scene Understanding](https://arxiv.org/abs/1702.01105)\n\n```\n* S3DIS 1x1\n* S3DIS Room\n* S3DIS Fused\n```\n\n- **[Shapenet](https://www.shapenet.org/)** from Iro Armeni _et al._: [ShapeNet: An Information-Rich 3D Model Repository](https://arxiv.org/abs/1512.03012)\n\n### Registration\n\n- **[3DMatch](http://3dmatch.cs.princeton.edu)** from Andy Zeng _et al._: [3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions](https://arxiv.org/abs/1603.08182)\n\n### Classification\n\n- **[ModelNet](https://modelnet.cs.princeton.edu)** from Zhirong Wu _et al._: [3D ShapeNets: A Deep Representation for Volumetric Shapes](https://people.csail.mit.edu/khosla/papers/cvpr2015_wu.pdf)\n\n## Getting started\n\n### Requirements:\n\n- CUDA > 10\n- Python 3 + headers (python-dev)\n- [Poetry](https://poetry.eustace.io/) (Optional but highly recommended)\n\n### Setup repo\n\nClone the repo to your local machine\n\nRun the following command from the root of the repo\n\n```\npoetry install --no-root\n```\n\nThis will install all required dependencies in a new virtual environment.\n\nActivate it\n\n```bash\npoetry shell\n```\n\nYou can check that the install has been successful by running\n\n```bash\npython -m unittest -v\n```\n\nor from pypi\n\n```bash\npip install torch_points3d\n```\n\n#### [Minkowski Engine](https://github.com/StanfordVL/MinkowskiEngine)\n\nThe repository is supporting [Minkowski Engine](https://github.com/StanfordVL/MinkowskiEngine) which requires `openblas-dev` and `nvcc` if you have a CUDA device on your machine. First install `openblas`\n\n```bash\nsudo apt install libopenblas-dev\n```\n\nthen make sure that `nvcc` is in your path:\n\n```bash\nnvcc -V\n```\n\nIf it's not then locate it (`locate nvcc`) and add its location to your `PATH` variable. On my machine:\n\n```bash\nexport PATH=\"/usr/local/cuda-10.2/bin:$PATH\"\n```\n\nYou are now in a position to install MinkowskiEngine with GPU support:\n\n```bash\npoetry install -E MinkowskiEngine --no-root\n```\n\n#### Pycuda\n\n```bash\npip install pycuda\n```\n\n## Train pointnet++ on part segmentation task for dataset shapenet\n\n```bash\npoetry run python train.py task=segmentation model_type=pointnet2 model_name=pointnet2_charlesssg dataset=shapenet-fixed\n```\n\nAnd you should see something like that\n\n![logging](https://raw.githubusercontent.com/nicolas-chaulet/torch-points3d/master/docs/imgs/logging.png)\n\nThe [config](https://raw.githubusercontent.com/nicolas-chaulet/torch-points3d/master/conf/models/segmentation/pointnet2.yaml) for pointnet++ is a good example of how to define a model and is as follow:\n\n```yaml\n# PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (https://arxiv.org/abs/1706.02413)\n# Credit Charles R. Qi: https://github.com/charlesq34/pointnet2/blob/master/models/pointnet2_part_seg_msg_one_hot.py\n\npointnet2_onehot:\n  architecture: pointnet2.PointNet2_D\n  conv_type: 'DENSE'\n  use_category: True\n  down_conv:\n    module_name: PointNetMSGDown\n    npoint: [1024, 256, 64, 16]\n    radii: [[0.05, 0.1], [0.1, 0.2], [0.2, 0.4], [0.4, 0.8]]\n    nsamples: [[16, 32], [16, 32], [16, 32], [16, 32]]\n    down_conv_nn:\n      [\n        [[FEAT, 16, 16, 32], [FEAT, 32, 32, 64]],\n        [[32 + 64, 64, 64, 128], [32 + 64, 64, 96, 128]],\n        [[128 + 128, 128, 196, 256], [128 + 128, 128, 196, 256]],\n        [[256 + 256, 256, 256, 512], [256 + 256, 256, 384, 512]],\n      ]\n  up_conv:\n    module_name: DenseFPModule\n    up_conv_nn:\n      [\n        [512 + 512 + 256 + 256, 512, 512],\n        [512 + 128 + 128, 512, 512],\n        [512 + 64 + 32, 256, 256],\n        [256 + FEAT, 128, 128],\n      ]\n    skip: True\n  mlp_cls:\n    nn: [128, 128]\n    dropout: 0.5\n```\n\n# Benchmark\n\n## S3DIS 1x1\n\n| Model Name                                                           | # params  | Speed Train / Test      | Cross Entropy | OAcc  | mIou  | mAcc  |\n| -------------------------------------------------------------------- | --------- | ----------------------- | ------------- | ----- | ----- | ----- |\n| [`pointnet2_original`](https://github.com/nicolas-chaulet/torch-points3d/blob/master/benchmark/s3dis_fold5/Pointnet2_original.md) | 3,026,829 | 04:29 / 01:07(RTX 2060) | 0.0512        | 85.26 | 45.58 | 73.11 |\n\n## Shapenet part segmentation\n\nThe data reported below correspond to the part segmentation problem for Shapenet for all categories. We report against mean instance IoU and mean class IoU (average of the mean instance IoU per class)\n\n| Model Name                                                            | Use Normals | # params  | Speed Train / Test      | Cross Entropy | CmIou  | ImIou |\n| --------------------------------------------------------------------- | ----------- | --------- | ----------------------- | ------------- | ------ | ----- |\n| [`pointnet2_charlesmsg`](https://github.com/nicolas-chaulet/torch-points3d/blob/master/benchmark/shapenet/pointnet2_charlesmsg.md) | Yes         | 1,733,946 | 15:07 / 01:20 (K80)     | 0.089         | 82.1   | 85.1  |\n| [`RSCNN_MSG`](https://github.com/nicolas-chaulet/torch-points3d/blob/master/benchmark/shapenet/rscnn_original.md)                  | No          | 3,488,417 | 05:40 / 0:24 (RTX 2060) | 0.04          | 82.811 | 85.3  |\n\n## Explore your experiments\n\nWe provide a [notebook](https://github.com/nicolas-chaulet/torch-points3d/blob/master/dashboard/dashboard.ipynb) based [pyvista](https://docs.pyvista.org/) and [panel](https://panel.holoviz.org/) that allows you to explore your past experiments visually. When using jupyter lab you will have to install an extension:\n\n```\njupyter labextension install @pyviz/jupyterlab_pyviz\n```\n\nRun through the notebook and you should see a dashboard starting that looks like the following:\n\n![dashboard](https://raw.githubusercontent.com/nicolas-chaulet/torch-points3d/master/docs/imgs/Dashboard_demo.gif)\n\n## Inference\n\n### Inference script\n\nWe provide a script for running a given pre trained model on custom data that may not be annotated. You will find an [example](https://github.com/nicolas-chaulet/torch-points3d/blob/master/forward_scripts/forward.py) of this for the part segmentation task on Shapenet. Just like for the rest of the codebase most of the customization happens through config files and the provided example can be extended to other datasets. You can also easily create your own from there. Going back to the part segmentation task, say you have a folder full of point clouds that you know are Airplanes, and you have the checkpoint of a model trained on Airplanes and potentially other classes, simply edit the [config.yaml](https://github.com/nicolas-chaulet/torch-points3d/blob/master/forward_scripts/conf/config.yaml) and [shapenet.yaml](https://github.com/nicolas-chaulet/torch-points3d/blob/master/forward_scripts/conf/dataset/shapenet.yaml) and run the following command:\n\n```bash\npython forward_scripts/forward.py\n```\n\nThe result of the forward run will be placed in the specified `output_folder` and you can use the [notebook](https://github.com/nicolas-chaulet/torch-points3d/blob/master/forward_scripts/notebooks/viz_shapenet.ipynb) provided to explore the results. Below is an example of the outcome of using a model trained on caps only to find the parts of airplanes and caps.\n\n![resexplore](https://raw.githubusercontent.com/nicolas-chaulet/torch-points3d/master/docs/imgs/inference_demo.gif)\n\n### Containerize your model with Docker\n\nFinally, for people interested in deploying their models to production environments, we provide a [Dockerfile](https://github.com/nicolas-chaulet/torch-points3d/blob/master/docker/Dockerfile) as well as a [build script](https://github.com/nicolas-chaulet/torch-points3d/blob/master/docker/build.sh). Say you have trained a network for semantic segmentation that gave the weight `<outputfolder/weights.pt>`, the following command will build a docker image for you:\n\n```bash\ncd docker\n./build.sh outputfolder/weights.pt\n```\n\nYou can then use it to run a forward pass on a all the point clouds in `input_path` and generate the results in `output_path`\n\n```bash\ndocker run -v /test_data:/in -v /test_data/out:/out pointnet2_charlesssg:latest python3 forward_scripts/forward.py dataset=shapenet data.forward_category=Cap input_path=\"/in\" output_path=\"/out\"\n```\n\nThe `-v` option mounts a local directory to the container's file system. For example in the command line above, `/test_data/out` will be mounted at the location `/out`. As a consequence, all files written in `/out` will be available in the folder `/test_data/out` on your machine.\n\n## Profiling\n\nWe advice to use [`snakeviz`](https://jiffyclub.github.io/snakeviz/) and [`cProfile`](https://docs.python.org/2/library/profile.html)\n\nUse cProfile to profile your code\n\n```\npoetry run python -m cProfile -o {your_name}.prof train.py ... debugging.profiling=True\n```\n\nAnd visualize results using snakeviz.\n\n```\nsnakeviz {your_name}.prof\n```\n\nIt is also possible to use [`torch.utils.bottleneck`](https://pytorch.org/docs/stable/bottleneck.html)\n\n```\npython -m torch.utils.bottleneck /path/to/source/script.py [args]\n```\n\n## Troubleshooting\n\n#### Undefined symbol / Updating Pytorch\n\nWhen we update the version of Pytorch that is used, the compiled packages need to be reinstalled, otherwise you will run into an error that looks like this:\n\n```\n... scatter_cpu.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN3c1012CUDATensorIdEv\n```\n\nThis can happen for the following libraries:\n\n- torch-points\n- torch-scatter\n- torch-cluster\n- torch-sparse\n\nAn easy way to fix this is to run the following command with the virtual env activated:\n\n```\npip uninstall torch-scatter torch-sparse torch-cluster torch-points-kernels -y\nrm -rf ~/.cache/pip\npoetry install\n```\n\n## Contributing\n\nContributions are welcome! The only asks are that you stick to the styling and that you add tests as you add more features!\n\nFor styling you can use [pre-commit hooks](https://ljvmiranda921.github.io/notebook/2018/06/21/precommits-using-black-and-flake8/) to help you:\n\n```\npre-commit install\n```\n\nA sequence of checks will be run for you and you may have to add the fixed files again to the stashed files.\n\nWhen it comes to docstrings we use [numpy style](https://numpydoc.readthedocs.io/en/latest/format.html) docstrings, for those who use\nVisual Studio Code, there is a great [extension](https://github.com/NilsJPWerner/autoDocstring) that can help with that. Install it and set the format to numpy and you should be good to go!\n\nFinaly, if you want to have a direct chat with us feel free to join our slack, just shoot us an email and we'll add you.\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "torch-points3d", "package_url": "https://pypi.org/project/torch-points3d/", "platform": "", "project_url": "https://pypi.org/project/torch-points3d/", "project_urls": {"Documentation": "https://torch-points3d.readthedocs.io/en/latest/"}, "release_url": "https://pypi.org/project/torch-points3d/0.1.2/", "requires_dist": ["matplotlib (>=3.1,<4.0)", "hydra-core (>=0.11.2,<0.12.0)", "wandb (>=0.8.18,<0.9.0)", "tqdm (>=4.40,<5.0)", "torchnet (>=0.0.4,<0.0.5)", "tensorboard (>=2.1,<3.0)", "torch (==1.3.1)", "torch-scatter (==1.4.0)", "torch-sparse (==0.4.3)", "torch-cluster (==1.4.5)", "torch-geometric (==1.3.2)", "pytorch_metric_learning (>=0.9.75,<0.10.0)", "MinkowskiEngine (>=0.4.2,<0.5.0); extra == \"MinkowskiEngine\"", "torch-points-kernels (==0.5.2)", "numpy (>=1.18.2,<2.0.0)", "scikit-image (>=0.16.2,<0.17.0)", "numba (>=0.49.0,<0.50.0)"], "requires_python": ">=3.6,<4.0", "summary": "Point Cloud Deep Learning Extension Library for PyTorch", "version": "0.1.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p align=\"center\">\n  <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fd299a209915b77c139f7903ec7a59cb5ead5a71/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6e69636f6c61732d636861756c65742f746f7263682d706f696e747333642f6d61737465722f646f63732f6c6f676f2e706e67\" width=\"40%\">\n</p>\n<p><a href=\"https://codecov.io/gh/nicolas-chaulet/torch-points3d\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1ae1f8baf261b0109b2d887479c889b1b638307f/68747470733a2f2f636f6465636f762e696f2f67682f6e69636f6c61732d636861756c65742f746f7263682d706f696e747333642f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a> <a href=\"https://github.com/nicolas-chaulet/torch-points3d/actions\" rel=\"nofollow\"><img alt=\"Actions Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/757d7d67a964725825b4fbedc889715340ed3d50/68747470733a2f2f6769746875622e636f6d2f6e69636f6c61732d636861756c65742f746f7263682d706f696e747333642f776f726b666c6f77732f756e6974746573742f62616467652e737667\"></a> <a href=\"https://torch-points3d.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2c10789955b629bc84be73a762751cdc5cfba475/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f746f7263682d706f696e747333642f62616467652f3f76657273696f6e3d6c6174657374\"></a></p>\n<p>This is a framework for running common deep learning models for point cloud analysis tasks against classic benchmark. It heavily relies on <a href=\"https://pytorch-geometric.readthedocs.io/en/latest/notes/resources.html\" rel=\"nofollow\">Pytorch Geometric</a> and <a href=\"https://hydra.cc/\" rel=\"nofollow\">Facebook Hydra</a>.</p>\n<p>The framework allows lean and yet complex model to be built with minimum effort and great reproducibility.</p>\n<h2>Project structure</h2>\n<pre>\u251c\u2500 benchmark               <span class=\"c1\"># Output from various benchmark runs</span>\n\u251c\u2500 conf                    <span class=\"c1\"># All configurations for training nad evaluation leave there</span>\n\u251c\u2500 dashboard               <span class=\"c1\"># A collection of notebooks that allow result exploration and network debugging</span>\n\u251c\u2500 docker                  <span class=\"c1\"># Docker image that can be used for inference or training</span>\n\u251c\u2500 docs                    <span class=\"c1\"># All the doc</span>\n\u251c\u2500 eval.py                 <span class=\"c1\"># Eval script</span>\n\u251c\u2500 find_neighbour_dist.py  <span class=\"c1\"># Script to find optimal #neighbours within neighbour search operations</span>\n\u251c\u2500 forward_scripts         <span class=\"c1\"># Script that runs a forward pass on possibly non annotated data</span>\n\u251c\u2500 outputs                 <span class=\"c1\"># All outputs from your runs sorted by date</span>\n\u251c\u2500 scripts                 <span class=\"c1\"># Some scripts to help manage the project</span>\n\u251c\u2500 torch_points3d\n    \u251c\u2500 core                <span class=\"c1\"># Core components</span>\n    \u251c\u2500 datasets            <span class=\"c1\"># All code related to datasets</span>\n    \u251c\u2500 metrics             <span class=\"c1\"># All metrics and trackers</span>\n    \u251c\u2500 models              <span class=\"c1\"># All models</span>\n    \u251c\u2500 modules             <span class=\"c1\"># Basic modules that can be used in a modular way</span>\n    \u251c\u2500 utils               <span class=\"c1\"># Various utils</span>\n    \u2514\u2500 visualization       <span class=\"c1\"># Visualization</span>\n\u251c\u2500 <span class=\"nb\">test</span>\n\u2514\u2500 train.py                <span class=\"c1\"># Main script to launch a training</span>\n</pre>\n<p>As a general philosophy we have split datasets and models by task. For example, datasets has three subfolders:</p>\n<ul>\n<li>segmentation</li>\n<li>classification</li>\n<li>registration</li>\n</ul>\n<p>where each folder contains the dataset related to each task.</p>\n<h2>Methods currently implemented</h2>\n<ul>\n<li><strong><a href=\"https://github.com/nicolas-chaulet/torch-points3d/blob/master/torch_points3d/modules/PointNet/modules.py#L54\" rel=\"nofollow\">PointNet</a></strong> from Charles R. Qi <em>et al.</em>: <a href=\"https://arxiv.org/abs/1612.00593\" rel=\"nofollow\">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</a> (CVPR 2017)</li>\n<li><strong><a href=\"https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/modules/pointnet2\" rel=\"nofollow\">PointNet++</a></strong> from Charles from Charles R. Qi <em>et al.</em>: <a href=\"https://arxiv.org/abs/1706.02413\" rel=\"nofollow\">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a></li>\n<li><strong><a href=\"https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/modules/RSConv\" rel=\"nofollow\">RSConv</a></strong> from Yongcheng Liu <em>et al.</em>: <a href=\"https://arxiv.org/abs/1904.07601\" rel=\"nofollow\">Relation-Shape Convolutional Neural Network for Point Cloud Analysis</a> (CVPR 2019)</li>\n<li><strong><a href=\"https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/modules/RandLANet\" rel=\"nofollow\">RandLA-Net</a></strong> from Qingyong Hu <em>et al.</em>: <a href=\"https://arxiv.org/abs/1911.11236\" rel=\"nofollow\">RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds</a></li>\n<li><strong><a href=\"https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/modules/PointCNN\" rel=\"nofollow\">PointCNN</a></strong> from Yangyan Li <em>et al.</em>: <a href=\"https://arxiv.org/abs/1801.07791\" rel=\"nofollow\">PointCNN: Convolution On X-Transformed Points</a> (NIPS 2018)</li>\n<li><strong><a href=\"https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/modules/KPConv\" rel=\"nofollow\">KPConv</a></strong> from Hugues Thomas <em>et al.</em>: <a href=\"https://arxiv.org/abs/1801.07791\" rel=\"nofollow\">KPConv: Flexible and Deformable Convolution for Point Clouds</a> (ICCV 2019)</li>\n<li><strong><a href=\"https://github.com/nicolas-chaulet/torch-points3d/tree/master/torch_points3d/modules/MinkowskiEngine\" rel=\"nofollow\">MinkowskiEngine</a></strong> from Christopher Choy <em>et al.</em>: <a href=\"https://arxiv.org/abs/1904.08755\" rel=\"nofollow\">4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks</a> (CVPR'19)</li>\n</ul>\n<h2>Available datasets</h2>\n<h3>Segmentation</h3>\n<ul>\n<li>\n<p><strong><a href=\"https://github.com/ScanNet/ScanNet\" rel=\"nofollow\">Scannet</a></strong> from Angela Dai <em>et al.</em>: <a href=\"https://arxiv.org/abs/1702.04405\" rel=\"nofollow\">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</a></p>\n</li>\n<li>\n<p><strong><a href=\"http://buildingparser.stanford.edu/dataset.html\" rel=\"nofollow\">S3DIS</a></strong> from Iro Armeni <em>et al.</em>: <a href=\"https://arxiv.org/abs/1702.01105\" rel=\"nofollow\">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</a></p>\n</li>\n</ul>\n<pre><code>* S3DIS 1x1\n* S3DIS Room\n* S3DIS Fused\n</code></pre>\n<ul>\n<li><strong><a href=\"https://www.shapenet.org/\" rel=\"nofollow\">Shapenet</a></strong> from Iro Armeni <em>et al.</em>: <a href=\"https://arxiv.org/abs/1512.03012\" rel=\"nofollow\">ShapeNet: An Information-Rich 3D Model Repository</a></li>\n</ul>\n<h3>Registration</h3>\n<ul>\n<li><strong><a href=\"http://3dmatch.cs.princeton.edu\" rel=\"nofollow\">3DMatch</a></strong> from Andy Zeng <em>et al.</em>: <a href=\"https://arxiv.org/abs/1603.08182\" rel=\"nofollow\">3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions</a></li>\n</ul>\n<h3>Classification</h3>\n<ul>\n<li><strong><a href=\"https://modelnet.cs.princeton.edu\" rel=\"nofollow\">ModelNet</a></strong> from Zhirong Wu <em>et al.</em>: <a href=\"https://people.csail.mit.edu/khosla/papers/cvpr2015_wu.pdf\" rel=\"nofollow\">3D ShapeNets: A Deep Representation for Volumetric Shapes</a></li>\n</ul>\n<h2>Getting started</h2>\n<h3>Requirements:</h3>\n<ul>\n<li>CUDA &gt; 10</li>\n<li>Python 3 + headers (python-dev)</li>\n<li><a href=\"https://poetry.eustace.io/\" rel=\"nofollow\">Poetry</a> (Optional but highly recommended)</li>\n</ul>\n<h3>Setup repo</h3>\n<p>Clone the repo to your local machine</p>\n<p>Run the following command from the root of the repo</p>\n<pre><code>poetry install --no-root\n</code></pre>\n<p>This will install all required dependencies in a new virtual environment.</p>\n<p>Activate it</p>\n<pre>poetry shell\n</pre>\n<p>You can check that the install has been successful by running</p>\n<pre>python -m unittest -v\n</pre>\n<p>or from pypi</p>\n<pre>pip install torch_points3d\n</pre>\n<h4><a href=\"https://github.com/StanfordVL/MinkowskiEngine\" rel=\"nofollow\">Minkowski Engine</a></h4>\n<p>The repository is supporting <a href=\"https://github.com/StanfordVL/MinkowskiEngine\" rel=\"nofollow\">Minkowski Engine</a> which requires <code>openblas-dev</code> and <code>nvcc</code> if you have a CUDA device on your machine. First install <code>openblas</code></p>\n<pre>sudo apt install libopenblas-dev\n</pre>\n<p>then make sure that <code>nvcc</code> is in your path:</p>\n<pre>nvcc -V\n</pre>\n<p>If it's not then locate it (<code>locate nvcc</code>) and add its location to your <code>PATH</code> variable. On my machine:</p>\n<pre><span class=\"nb\">export</span> <span class=\"nv\">PATH</span><span class=\"o\">=</span><span class=\"s2\">\"/usr/local/cuda-10.2/bin:</span><span class=\"nv\">$PATH</span><span class=\"s2\">\"</span>\n</pre>\n<p>You are now in a position to install MinkowskiEngine with GPU support:</p>\n<pre>poetry install -E MinkowskiEngine --no-root\n</pre>\n<h4>Pycuda</h4>\n<pre>pip install pycuda\n</pre>\n<h2>Train pointnet++ on part segmentation task for dataset shapenet</h2>\n<pre>poetry run python train.py <span class=\"nv\">task</span><span class=\"o\">=</span>segmentation <span class=\"nv\">model_type</span><span class=\"o\">=</span>pointnet2 <span class=\"nv\">model_name</span><span class=\"o\">=</span>pointnet2_charlesssg <span class=\"nv\">dataset</span><span class=\"o\">=</span>shapenet-fixed\n</pre>\n<p>And you should see something like that</p>\n<p><img alt=\"logging\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/943dfa1b6ecc502d4b8104590186e87e1a6295a9/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6e69636f6c61732d636861756c65742f746f7263682d706f696e747333642f6d61737465722f646f63732f696d67732f6c6f6767696e672e706e67\"></p>\n<p>The <a href=\"https://raw.githubusercontent.com/nicolas-chaulet/torch-points3d/master/conf/models/segmentation/pointnet2.yaml\" rel=\"nofollow\">config</a> for pointnet++ is a good example of how to define a model and is as follow:</p>\n<pre><span class=\"c1\"># PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (https://arxiv.org/abs/1706.02413)</span>\n<span class=\"c1\"># Credit Charles R. Qi: https://github.com/charlesq34/pointnet2/blob/master/models/pointnet2_part_seg_msg_one_hot.py</span>\n\n<span class=\"nt\">pointnet2_onehot</span><span class=\"p\">:</span>\n  <span class=\"nt\">architecture</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">pointnet2.PointNet2_D</span>\n  <span class=\"nt\">conv_type</span><span class=\"p\">:</span> <span class=\"s\">'DENSE'</span>\n  <span class=\"nt\">use_category</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">True</span>\n  <span class=\"nt\">down_conv</span><span class=\"p\">:</span>\n    <span class=\"nt\">module_name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">PointNetMSGDown</span>\n    <span class=\"nt\">npoint</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">[</span><span class=\"nv\">1024</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">256</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">64</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">16</span><span class=\"p p-Indicator\">]</span>\n    <span class=\"nt\">radii</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">[[</span><span class=\"nv\">0.05</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">0.1</span><span class=\"p p-Indicator\">],</span> <span class=\"p p-Indicator\">[</span><span class=\"nv\">0.1</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">0.2</span><span class=\"p p-Indicator\">],</span> <span class=\"p p-Indicator\">[</span><span class=\"nv\">0.2</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">0.4</span><span class=\"p p-Indicator\">],</span> <span class=\"p p-Indicator\">[</span><span class=\"nv\">0.4</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">0.8</span><span class=\"p p-Indicator\">]]</span>\n    <span class=\"nt\">nsamples</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">[[</span><span class=\"nv\">16</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">32</span><span class=\"p p-Indicator\">],</span> <span class=\"p p-Indicator\">[</span><span class=\"nv\">16</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">32</span><span class=\"p p-Indicator\">],</span> <span class=\"p p-Indicator\">[</span><span class=\"nv\">16</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">32</span><span class=\"p p-Indicator\">],</span> <span class=\"p p-Indicator\">[</span><span class=\"nv\">16</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">32</span><span class=\"p p-Indicator\">]]</span>\n    <span class=\"nt\">down_conv_nn</span><span class=\"p\">:</span>\n      <span class=\"p p-Indicator\">[</span>\n        <span class=\"p p-Indicator\">[[</span><span class=\"nv\">FEAT</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">16</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">16</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">32</span><span class=\"p p-Indicator\">],</span> <span class=\"p p-Indicator\">[</span><span class=\"nv\">FEAT</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">32</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">32</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">64</span><span class=\"p p-Indicator\">]],</span>\n        <span class=\"p p-Indicator\">[[</span><span class=\"nv\">32 + 64</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">64</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">64</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">128</span><span class=\"p p-Indicator\">],</span> <span class=\"p p-Indicator\">[</span><span class=\"nv\">32 + 64</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">64</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">96</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">128</span><span class=\"p p-Indicator\">]],</span>\n        <span class=\"p p-Indicator\">[[</span><span class=\"nv\">128 + 128</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">128</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">196</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">256</span><span class=\"p p-Indicator\">],</span> <span class=\"p p-Indicator\">[</span><span class=\"nv\">128 + 128</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">128</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">196</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">256</span><span class=\"p p-Indicator\">]],</span>\n        <span class=\"p p-Indicator\">[[</span><span class=\"nv\">256 + 256</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">256</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">256</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">512</span><span class=\"p p-Indicator\">],</span> <span class=\"p p-Indicator\">[</span><span class=\"nv\">256 + 256</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">256</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">384</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">512</span><span class=\"p p-Indicator\">]],</span>\n      <span class=\"p p-Indicator\">]</span>\n  <span class=\"nt\">up_conv</span><span class=\"p\">:</span>\n    <span class=\"nt\">module_name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">DenseFPModule</span>\n    <span class=\"nt\">up_conv_nn</span><span class=\"p\">:</span>\n      <span class=\"p p-Indicator\">[</span>\n        <span class=\"p p-Indicator\">[</span><span class=\"nv\">512 + 512 + 256 + 256</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">512</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">512</span><span class=\"p p-Indicator\">],</span>\n        <span class=\"p p-Indicator\">[</span><span class=\"nv\">512 + 128 + 128</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">512</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">512</span><span class=\"p p-Indicator\">],</span>\n        <span class=\"p p-Indicator\">[</span><span class=\"nv\">512 + 64 + 32</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">256</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">256</span><span class=\"p p-Indicator\">],</span>\n        <span class=\"p p-Indicator\">[</span><span class=\"nv\">256 + FEAT</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">128</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">128</span><span class=\"p p-Indicator\">],</span>\n      <span class=\"p p-Indicator\">]</span>\n    <span class=\"nt\">skip</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">True</span>\n  <span class=\"nt\">mlp_cls</span><span class=\"p\">:</span>\n    <span class=\"nt\">nn</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">[</span><span class=\"nv\">128</span><span class=\"p p-Indicator\">,</span> <span class=\"nv\">128</span><span class=\"p p-Indicator\">]</span>\n    <span class=\"nt\">dropout</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">0.5</span>\n</pre>\n<h1>Benchmark</h1>\n<h2>S3DIS 1x1</h2>\n<table>\n<thead>\n<tr>\n<th>Model Name</th>\n<th># params</th>\n<th>Speed Train / Test</th>\n<th>Cross Entropy</th>\n<th>OAcc</th>\n<th>mIou</th>\n<th>mAcc</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/nicolas-chaulet/torch-points3d/blob/master/benchmark/s3dis_fold5/Pointnet2_original.md\" rel=\"nofollow\"><code>pointnet2_original</code></a></td>\n<td>3,026,829</td>\n<td>04:29 / 01:07(RTX 2060)</td>\n<td>0.0512</td>\n<td>85.26</td>\n<td>45.58</td>\n<td>73.11</td>\n</tr></tbody></table>\n<h2>Shapenet part segmentation</h2>\n<p>The data reported below correspond to the part segmentation problem for Shapenet for all categories. We report against mean instance IoU and mean class IoU (average of the mean instance IoU per class)</p>\n<table>\n<thead>\n<tr>\n<th>Model Name</th>\n<th>Use Normals</th>\n<th># params</th>\n<th>Speed Train / Test</th>\n<th>Cross Entropy</th>\n<th>CmIou</th>\n<th>ImIou</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/nicolas-chaulet/torch-points3d/blob/master/benchmark/shapenet/pointnet2_charlesmsg.md\" rel=\"nofollow\"><code>pointnet2_charlesmsg</code></a></td>\n<td>Yes</td>\n<td>1,733,946</td>\n<td>15:07 / 01:20 (K80)</td>\n<td>0.089</td>\n<td>82.1</td>\n<td>85.1</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/nicolas-chaulet/torch-points3d/blob/master/benchmark/shapenet/rscnn_original.md\" rel=\"nofollow\"><code>RSCNN_MSG</code></a></td>\n<td>No</td>\n<td>3,488,417</td>\n<td>05:40 / 0:24 (RTX 2060)</td>\n<td>0.04</td>\n<td>82.811</td>\n<td>85.3</td>\n</tr></tbody></table>\n<h2>Explore your experiments</h2>\n<p>We provide a <a href=\"https://github.com/nicolas-chaulet/torch-points3d/blob/master/dashboard/dashboard.ipynb\" rel=\"nofollow\">notebook</a> based <a href=\"https://docs.pyvista.org/\" rel=\"nofollow\">pyvista</a> and <a href=\"https://panel.holoviz.org/\" rel=\"nofollow\">panel</a> that allows you to explore your past experiments visually. When using jupyter lab you will have to install an extension:</p>\n<pre><code>jupyter labextension install @pyviz/jupyterlab_pyviz\n</code></pre>\n<p>Run through the notebook and you should see a dashboard starting that looks like the following:</p>\n<p><img alt=\"dashboard\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5df0eda0289bb661c89e114352a7e449f01dc28a/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6e69636f6c61732d636861756c65742f746f7263682d706f696e747333642f6d61737465722f646f63732f696d67732f44617368626f6172645f64656d6f2e676966\"></p>\n<h2>Inference</h2>\n<h3>Inference script</h3>\n<p>We provide a script for running a given pre trained model on custom data that may not be annotated. You will find an <a href=\"https://github.com/nicolas-chaulet/torch-points3d/blob/master/forward_scripts/forward.py\" rel=\"nofollow\">example</a> of this for the part segmentation task on Shapenet. Just like for the rest of the codebase most of the customization happens through config files and the provided example can be extended to other datasets. You can also easily create your own from there. Going back to the part segmentation task, say you have a folder full of point clouds that you know are Airplanes, and you have the checkpoint of a model trained on Airplanes and potentially other classes, simply edit the <a href=\"https://github.com/nicolas-chaulet/torch-points3d/blob/master/forward_scripts/conf/config.yaml\" rel=\"nofollow\">config.yaml</a> and <a href=\"https://github.com/nicolas-chaulet/torch-points3d/blob/master/forward_scripts/conf/dataset/shapenet.yaml\" rel=\"nofollow\">shapenet.yaml</a> and run the following command:</p>\n<pre>python forward_scripts/forward.py\n</pre>\n<p>The result of the forward run will be placed in the specified <code>output_folder</code> and you can use the <a href=\"https://github.com/nicolas-chaulet/torch-points3d/blob/master/forward_scripts/notebooks/viz_shapenet.ipynb\" rel=\"nofollow\">notebook</a> provided to explore the results. Below is an example of the outcome of using a model trained on caps only to find the parts of airplanes and caps.</p>\n<p><img alt=\"resexplore\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3e93c65df5cc1f0d2f5b5d69cdacb5e73e0def13/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6e69636f6c61732d636861756c65742f746f7263682d706f696e747333642f6d61737465722f646f63732f696d67732f696e666572656e63655f64656d6f2e676966\"></p>\n<h3>Containerize your model with Docker</h3>\n<p>Finally, for people interested in deploying their models to production environments, we provide a <a href=\"https://github.com/nicolas-chaulet/torch-points3d/blob/master/docker/Dockerfile\" rel=\"nofollow\">Dockerfile</a> as well as a <a href=\"https://github.com/nicolas-chaulet/torch-points3d/blob/master/docker/build.sh\" rel=\"nofollow\">build script</a>. Say you have trained a network for semantic segmentation that gave the weight <code>&lt;outputfolder/weights.pt&gt;</code>, the following command will build a docker image for you:</p>\n<pre><span class=\"nb\">cd</span> docker\n./build.sh outputfolder/weights.pt\n</pre>\n<p>You can then use it to run a forward pass on a all the point clouds in <code>input_path</code> and generate the results in <code>output_path</code></p>\n<pre>docker run -v /test_data:/in -v /test_data/out:/out pointnet2_charlesssg:latest python3 forward_scripts/forward.py <span class=\"nv\">dataset</span><span class=\"o\">=</span>shapenet data.forward_category<span class=\"o\">=</span>Cap <span class=\"nv\">input_path</span><span class=\"o\">=</span><span class=\"s2\">\"/in\"</span> <span class=\"nv\">output_path</span><span class=\"o\">=</span><span class=\"s2\">\"/out\"</span>\n</pre>\n<p>The <code>-v</code> option mounts a local directory to the container's file system. For example in the command line above, <code>/test_data/out</code> will be mounted at the location <code>/out</code>. As a consequence, all files written in <code>/out</code> will be available in the folder <code>/test_data/out</code> on your machine.</p>\n<h2>Profiling</h2>\n<p>We advice to use <a href=\"https://jiffyclub.github.io/snakeviz/\" rel=\"nofollow\"><code>snakeviz</code></a> and <a href=\"https://docs.python.org/2/library/profile.html\" rel=\"nofollow\"><code>cProfile</code></a></p>\n<p>Use cProfile to profile your code</p>\n<pre><code>poetry run python -m cProfile -o {your_name}.prof train.py ... debugging.profiling=True\n</code></pre>\n<p>And visualize results using snakeviz.</p>\n<pre><code>snakeviz {your_name}.prof\n</code></pre>\n<p>It is also possible to use <a href=\"https://pytorch.org/docs/stable/bottleneck.html\" rel=\"nofollow\"><code>torch.utils.bottleneck</code></a></p>\n<pre><code>python -m torch.utils.bottleneck /path/to/source/script.py [args]\n</code></pre>\n<h2>Troubleshooting</h2>\n<h4>Undefined symbol / Updating Pytorch</h4>\n<p>When we update the version of Pytorch that is used, the compiled packages need to be reinstalled, otherwise you will run into an error that looks like this:</p>\n<pre><code>... scatter_cpu.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN3c1012CUDATensorIdEv\n</code></pre>\n<p>This can happen for the following libraries:</p>\n<ul>\n<li>torch-points</li>\n<li>torch-scatter</li>\n<li>torch-cluster</li>\n<li>torch-sparse</li>\n</ul>\n<p>An easy way to fix this is to run the following command with the virtual env activated:</p>\n<pre><code>pip uninstall torch-scatter torch-sparse torch-cluster torch-points-kernels -y\nrm -rf ~/.cache/pip\npoetry install\n</code></pre>\n<h2>Contributing</h2>\n<p>Contributions are welcome! The only asks are that you stick to the styling and that you add tests as you add more features!</p>\n<p>For styling you can use <a href=\"https://ljvmiranda921.github.io/notebook/2018/06/21/precommits-using-black-and-flake8/\" rel=\"nofollow\">pre-commit hooks</a> to help you:</p>\n<pre><code>pre-commit install\n</code></pre>\n<p>A sequence of checks will be run for you and you may have to add the fixed files again to the stashed files.</p>\n<p>When it comes to docstrings we use <a href=\"https://numpydoc.readthedocs.io/en/latest/format.html\" rel=\"nofollow\">numpy style</a> docstrings, for those who use\nVisual Studio Code, there is a great <a href=\"https://github.com/NilsJPWerner/autoDocstring\" rel=\"nofollow\">extension</a> that can help with that. Install it and set the format to numpy and you should be good to go!</p>\n<p>Finaly, if you want to have a direct chat with us feel free to join our slack, just shoot us an email and we'll add you.</p>\n\n          </div>"}, "last_serial": 7170636, "releases": {"0.1.0.dev1": [{"comment_text": "", "digests": {"md5": "d683f5c31b0b5fb545158debab3c87ee", "sha256": "9aba00d2f22c39c5da4d31034675761cdd7a00ec6107c5f462f0f6f26629899f"}, "downloads": -1, "filename": "torch_points3d-0.1.0.dev1-py3-none-any.whl", "has_sig": false, "md5_digest": "d683f5c31b0b5fb545158debab3c87ee", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4.0", "size": 217342, "upload_time": "2020-05-05T08:50:34", "upload_time_iso_8601": "2020-05-05T08:50:34.857956Z", "url": "https://files.pythonhosted.org/packages/61/62/aec96187672b8cdbd50b549fcd62e3c87dc512c343b4a1f79b36ae1788e0/torch_points3d-0.1.0.dev1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "01ebcb8dacabaee138e42f6ff29835a6", "sha256": "d84425bc73866dc84481b28000f1de68c7b2654c79f3bf8afcbd7cb2f8cbb4c0"}, "downloads": -1, "filename": "torch_points3d-0.1.0.dev1.tar.gz", "has_sig": false, "md5_digest": "01ebcb8dacabaee138e42f6ff29835a6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4.0", "size": 157974, "upload_time": "2020-05-05T08:50:36", "upload_time_iso_8601": "2020-05-05T08:50:36.149304Z", "url": "https://files.pythonhosted.org/packages/a2/a4/668d602b568560c8b670cc6be494b2af12bd86a9351438348862ff7b22ea/torch_points3d-0.1.0.dev1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "8031280d77192be91080e99a9beeace1", "sha256": "5bde5a58404fe429a8a1fa11ad19bbea367c6471197804966f1c4ff0b4781942"}, "downloads": -1, "filename": "torch_points3d-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "8031280d77192be91080e99a9beeace1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4.0", "size": 217290, "upload_time": "2020-05-04T08:54:16", "upload_time_iso_8601": "2020-05-04T08:54:16.184672Z", "url": "https://files.pythonhosted.org/packages/5e/c2/5c7cb7f7e44bc99f9b2ab7375daab819805beebb6e724a4effb59ad1a8e3/torch_points3d-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7f250dc17575fa9cb36fc87e99cb7727", "sha256": "0e84707ab9813abd7df3d1213114e6a4eb8559ad43708972fa7a5f3e4c691ffb"}, "downloads": -1, "filename": "torch_points3d-0.1.2.tar.gz", "has_sig": false, "md5_digest": "7f250dc17575fa9cb36fc87e99cb7727", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4.0", "size": 157996, "upload_time": "2020-05-04T08:54:17", "upload_time_iso_8601": "2020-05-04T08:54:17.421110Z", "url": "https://files.pythonhosted.org/packages/81/6e/8f90638f1560fde0e5809030e388063d46d84b191398f3564d8d8646061e/torch_points3d-0.1.2.tar.gz", "yanked": false}], "0.2.0.dev0": [{"comment_text": "", "digests": {"md5": "d1ccef15b304304b60d537dc6243bc87", "sha256": "2ca58c795167ba12409ae6d4985b3d74bf7ceb934994223bcc9d194848a42a9a"}, "downloads": -1, "filename": "torch_points3d-0.2.0.dev0-py3-none-any.whl", "has_sig": false, "md5_digest": "d1ccef15b304304b60d537dc6243bc87", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4.0", "size": 217341, "upload_time": "2020-05-05T09:28:07", "upload_time_iso_8601": "2020-05-05T09:28:07.317678Z", "url": "https://files.pythonhosted.org/packages/98/69/6de0dc997f7691f53cd16fcd41a3c019224b21871a024b16fcf1e6f0c861/torch_points3d-0.2.0.dev0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "992c9c3a93ce729ad63923b2a7486504", "sha256": "b371479b1ecaf3b737db98b5b85997673e2e0dea03a90840c38293ee7485a645"}, "downloads": -1, "filename": "torch_points3d-0.2.0.dev0.tar.gz", "has_sig": false, "md5_digest": "992c9c3a93ce729ad63923b2a7486504", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4.0", "size": 157988, "upload_time": "2020-05-05T09:28:08", "upload_time_iso_8601": "2020-05-05T09:28:08.301410Z", "url": "https://files.pythonhosted.org/packages/7e/39/578e84d730e18eaf136e28a145b8a850920a18afba30a70a8a10817ff280/torch_points3d-0.2.0.dev0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "8031280d77192be91080e99a9beeace1", "sha256": "5bde5a58404fe429a8a1fa11ad19bbea367c6471197804966f1c4ff0b4781942"}, "downloads": -1, "filename": "torch_points3d-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "8031280d77192be91080e99a9beeace1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4.0", "size": 217290, "upload_time": "2020-05-04T08:54:16", "upload_time_iso_8601": "2020-05-04T08:54:16.184672Z", "url": "https://files.pythonhosted.org/packages/5e/c2/5c7cb7f7e44bc99f9b2ab7375daab819805beebb6e724a4effb59ad1a8e3/torch_points3d-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7f250dc17575fa9cb36fc87e99cb7727", "sha256": "0e84707ab9813abd7df3d1213114e6a4eb8559ad43708972fa7a5f3e4c691ffb"}, "downloads": -1, "filename": "torch_points3d-0.1.2.tar.gz", "has_sig": false, "md5_digest": "7f250dc17575fa9cb36fc87e99cb7727", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4.0", "size": 157996, "upload_time": "2020-05-04T08:54:17", "upload_time_iso_8601": "2020-05-04T08:54:17.421110Z", "url": "https://files.pythonhosted.org/packages/81/6e/8f90638f1560fde0e5809030e388063d46d84b191398f3564d8d8646061e/torch_points3d-0.1.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:14 2020"}