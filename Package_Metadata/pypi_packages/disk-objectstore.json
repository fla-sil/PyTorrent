{"info": {"author": "Giovanni Pizzi", "author_email": "", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: MacOS :: MacOS X", "Operating System :: Microsoft :: Windows", "Operating System :: OS Independent", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# disk-objectstore\n\nAn implementation of an efficient object store that writes directly on disk\nand does not require a server running.\n\n|    | |\n|-----|----------------------------------------------------------------------------|\n|Latest release| [![PyPI version](https://badge.fury.io/py/disk-objectstore.svg)](https://badge.fury.io/py/disk-objectstore) [![PyPI pyversions](https://img.shields.io/pypi/pyversions/disk-objectstore.svg)](https://pypi.python.org/pypi/disk-objectstore/) |\n|Build status| [![Build Status](https://github.com/giovannipizzi/disk-objectstore/workflows/Continuous%20integration/badge.svg)](https://github.com/giovannipizzi/disk-bjectstore/actions) [![Coverage Status](https://codecov.io/gh/giovannipizzi/disk-objectstore/branch/develop/graph/badge.svg)](https://codecov.io/gh/giovannipizzi/disk-objectstore) |\n\n\n## Goal\n\nThe goal of this project is to have a very efficient implementation of an \"object store\"\nthat works directly on a disk folder, does not require a server to run, and addresses\na number of performance issues, discussed also below.\n\nThis project originated from the requirements needed by an efficient repository\nimplementation in [AiiDA](http://www.aiida.net) (note, however, that this\npackage is completely independent of AiiDA).\n\n## How to install\nTo install, run:\n```\npip install disk-objectstore\n```\n\nTo install in development mode, run, after checking out, in a (python 3) virtual environment:\n```\npip install -e .[dev]\n```\n\n## Implementation considerations\n\nThis implementation, in particular, addresses the following aspects:\n\n- objects are written, by default, in loose format. They are also uncompressed.\n  This gives maximum performance when writing a file, and ensures that many writers\n  can write at the same time without data corruption.\n\n- loose objects are stored in a one-level sharding format: aa/bbccddeeff00...\n  Current experience (with AiiDA) shows that it's actually not so good to use two\n  levels of nesting.\n  And anyway when there are too many loose objects, the idea\n  is that we will pack them in few files (see below).\n  The number of characters in the first part can be chosen, but a good compromise is\n  2 (default, also used by git)\n\n- for maximum performance, loose objects are simply written as they are,\n  without compression, hashing, ...\n  They are actually written first to a sandbox folder (in the same filesystem),\n  and then moved in place with the correct UUID only when the file is closed.\n  This should prevent having leftover objects if the process dies, and\n  the move operation should be hopefully a fast atomic operation on most filesystems.\n\n- When the user wants, loose objects are repacked in a few pack files. Indeed,\n  just the fact of storing too many files is quite expensive\n  (e.g. storing 65536 empty files in the same folder took over 3 minutes to write\n  and over 4 minutes to delete on a Mac SSD).\n\n- packing can be triggered by the user periodically.\n  It is even possible (to be stress tested, though) to pack while the object store\n  is in use (this might temporarily impact read performance, though).\n  This operation takes all loose objects and puts them in a controllable number\n  of packs. The name of the packs is given by the first few letters of the UUID\n  (by default: 2, so 256 packs in total; configurable). A value of 2 is a good balance\n  between the size of each pack (on average, ~4GB/pack for a 1TB repository) and\n  the number of packs (having many packs means that, even when performing bulk access,\n  many different files need to be open, which slows down performance).\n\n- pack files are just concatenation of bytes of the packed objects. Any new object\n  is appended to the pack (thanks to the efficiency of opening a file for appending).\n  The information for the offset and length of each pack is kept in a single SQLite\n  database.\n\n- For each object, the SQLite database contains: the `uuid`, the `offset` (starting\n  position of the bytestream in the file), the `length` (number of bytes to read),\n  a boolean `compressed` flag, meaning if the bytestream has been zlib-compressed,\n  and the `size` of the actual data (equal to `length` if `compressed` is false,\n  otherwise the size of the uncompressed stream, useful for statistics).\n\n- Note that compression is on a per-object level. This allows much greater flexibility\n  (the API still does not allow for this, but this is very easy to implement).\n  One could also think to clever logic to try to compress a file, but then store it\n  uncompressed if it turns out that the compression ratio is not worth the time\n  needed to further uncompress it later.\n\n- the repository configuration is kept in a top-level json (number of nesting levels\n  for loose objects and for packs, ...)\n\n- API exists both to get and write a single object, but also to write directly\n  to pack files (this cannot be done by multiple processes at the same time, though),\n  and to read in bulk a given number of objects.\n  This is particularly convenient when using the object store for bulk import and\n  export, and very fast. Also, it is useful when getting all files of a given node.\n\n  In normal operation, however, we expect the client to write loose objects,\n  to be repacked  periodically (e.g. once a week).\n\n  Some reference results for bulk operations:\n  Storing 100'000 small objects directly to the packs takes about 10s.\n  The time to retrieve all of them is ~2.2s when using a single bulk call,\n  compared to ~44.5s when using 100'000 independent calls (still probably acceptable).\n  Moreover, even getting, in 10 bulk calls, 10 random chunks of the objects (eventually\n  covering the whole set of 100'000 objects) only takes ~3.4s. This should demonstrate\n  that exporting a subset of the graph should be efficient (and the object store format\n  could be used also inside the export file).\n\n- All operations internally (storing to a loose object, storing to a pack, reading\n  from a loose object or from a pack, compression) are all happening via streaming.\n  So, even when dealing with huge files, these never fill the RAM (e.g. when reading\n  or writing a multi-GB file, the memory usage has been tested to be capped at ~150MB).\n  Convenience methods are available, anyway, to get directly an object content, if\n  the user wants.\n\n## Further design choices\n\nIn addition, the following design choices have been made:\n\n- Each given object will get a random UUID (its generation cost is negligible, about\n  4 microseconds per UUID).\n  It's up to the caller to track this into a filename or a folder structure.\n  The UUID is generated by the implementation and cannot be passed from the outside.\n  This guarantees random distribution of objects in packs, and avoids to have to\n  check for files already existing.\n\n- Pack naming and strategy is not determined by the user. Anyway it would be difficult\n  to provide easy options to the user to customize the behavior, while implementing\n  a packing strategy that is efficient. Moreover, with the current packing strategy,\n  it is immediate to know in which pack to check without having to keep also an index\n  of the packs (this, however, would be possible in case we want to extend the behavior,\n  since anyway we have an index). But at the moment it does not seem necessary.\n\n- A single index file is used. Having one pack index per file, while reducing a bit\n  the size of the index (one could skip storing the first part of the UUID, determined\n  by the pack naming) turns out not to be very effective. Either one would keep all\n  indexes open (but then one quickly hits the maximum number of open files, that e.g.\n  on Mac OS is of the order of ~256), or open the index, at every request, that risks to\n  be quite inefficient (not only to open, but also to load the DB, perform the query,\n  return the results, and close again the file). Also for bulk requests, anyway, this\n  would prevent making very few DB requests (unless you keep all files open, that\n  again is not feasible).\n\n- I tried a different way of storing the UUID on the DB (two long long ints rather than\n  1 UUID string). I put a combined index on the two columns.\n  I hoped in a speed up, using ints rather than strings, but (beside making the logic\n  much more cumbersome and error prone) the performance actually decreased.\n  So I reverted to a UUID indexed string column.\n\n- Deletion (not implemented yet), can just occur as a deletion of the loose object or\n  a removal from the index file. Later repacking of the packs can be used to recover\n  the disk space still occupied in the pack files.\n\n- The current packing format is `rsync`-friendly. `rsync` has an algorithm to just\n  send the new part of a file, when appending. Actually, `rsync` has a clever rolling\n  algorithm that can also detect if the same block is in the file, even if at a\n  different position. Therefore, even if a pack is \"repacked\" (e.g. reordering\n  objects inside it, or removing deleted objects) does not prevent efficient\n  rsync transfer.\n\n  Some results: Let's considering a 1GB file that took ~4.5 mins to transfer fully\n  the first time  over my network.\n  After transferring this 1GB file, `rsync` only takes 14 seconds\n  to check the difference and transfer the additional 10MB appended to the 1GB file\n  (and it indeed transfers only ~10MB).\n\n  In addition,  if the contents are randomly reshuffled, the second time the `rsync`\n  process took only 14 seconds, transferring only ~32MB, with a speedup of ~30x\n  (in this test, I divided the file in 1021 chunks of random size, uniformly\n  distributed between 0 bytes and 2MB, so with a total size of ~1GB, and in the\n  second `rsync` run I randomly reshuffled the chunks).\n\n- Appending files to a single file does not prevent the Linux disk cache to work.\n  To test this, I created a ~3GB file, composed of a ~1GB file (of which I know the MD5)\n  and of a ~2GB file (of which I know the MD5).\n  They are concatenated on a single file on disk.\n  File sizes are not multiples of a power of 2 to avoid alignment with block size.\n\n  After flushing the caches, if one reads only the second half, 2GB are added to the\n  kernel memory cache.\n\n  After re-flushing the caches, if one reads only the first half, only 1GB is added\n  to the memory cache.\n  Without further flushing the caches, if one reads also the first half,\n  2 more GBs are added to the memory cache (totalling 3GB more).\n\n  Therefore, caches are per blocks/pages in linux, not per file.\n  Concatenating files does not impact performance on cache efficiency.\n\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/giovannipizzi/disk-objectstore", "keywords": "object store,repository,file store", "license": "The MIT license", "maintainer": "", "maintainer_email": "", "name": "disk-objectstore", "package_url": "https://pypi.org/project/disk-objectstore/", "platform": "", "project_url": "https://pypi.org/project/disk-objectstore/", "project_urls": {"Homepage": "http://github.com/giovannipizzi/disk-objectstore"}, "release_url": "https://pypi.org/project/disk-objectstore/0.2.1/", "requires_dist": ["sqlalchemy", "profilehooks; extra == 'dev'", "psutil; extra == 'dev'", "click; extra == 'dev'", "pre-commit; extra == 'dev'", "yapf; extra == 'dev'", "prospector; extra == 'dev'", "pylint; extra == 'dev'", "pytest; extra == 'dev'", "pytest-cov; extra == 'dev'"], "requires_python": "", "summary": "An implementation of an efficient object store writing directly into a disk folder", "version": "0.2.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>disk-objectstore</h1>\n<p>An implementation of an efficient object store that writes directly on disk\nand does not require a server running.</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Latest release</td>\n<td><a href=\"https://badge.fury.io/py/disk-objectstore\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f4c2457bdf843c48a7859eb14b6847a771f4eb9d/68747470733a2f2f62616467652e667572792e696f2f70792f6469736b2d6f626a65637473746f72652e737667\"></a> <a href=\"https://pypi.python.org/pypi/disk-objectstore/\" rel=\"nofollow\"><img alt=\"PyPI pyversions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/54a4107994b4c3b43ff84c9d58c1ee55e56567ce/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6469736b2d6f626a65637473746f72652e737667\"></a></td>\n</tr>\n<tr>\n<td>Build status</td>\n<td><a href=\"https://github.com/giovannipizzi/disk-bjectstore/actions\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/dbb8ebaa2a07bc592cfa0d06c820c9bffe12afec/68747470733a2f2f6769746875622e636f6d2f67696f76616e6e6970697a7a692f6469736b2d6f626a65637473746f72652f776f726b666c6f77732f436f6e74696e756f7573253230696e746567726174696f6e2f62616467652e737667\"></a> <a href=\"https://codecov.io/gh/giovannipizzi/disk-objectstore\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9ebad67337c8568644bfdeb40d933f708f0391a4/68747470733a2f2f636f6465636f762e696f2f67682f67696f76616e6e6970697a7a692f6469736b2d6f626a65637473746f72652f6272616e63682f646576656c6f702f67726170682f62616467652e737667\"></a></td>\n</tr></tbody></table>\n<h2>Goal</h2>\n<p>The goal of this project is to have a very efficient implementation of an \"object store\"\nthat works directly on a disk folder, does not require a server to run, and addresses\na number of performance issues, discussed also below.</p>\n<p>This project originated from the requirements needed by an efficient repository\nimplementation in <a href=\"http://www.aiida.net\" rel=\"nofollow\">AiiDA</a> (note, however, that this\npackage is completely independent of AiiDA).</p>\n<h2>How to install</h2>\n<p>To install, run:</p>\n<pre><code>pip install disk-objectstore\n</code></pre>\n<p>To install in development mode, run, after checking out, in a (python 3) virtual environment:</p>\n<pre><code>pip install -e .[dev]\n</code></pre>\n<h2>Implementation considerations</h2>\n<p>This implementation, in particular, addresses the following aspects:</p>\n<ul>\n<li>\n<p>objects are written, by default, in loose format. They are also uncompressed.\nThis gives maximum performance when writing a file, and ensures that many writers\ncan write at the same time without data corruption.</p>\n</li>\n<li>\n<p>loose objects are stored in a one-level sharding format: aa/bbccddeeff00...\nCurrent experience (with AiiDA) shows that it's actually not so good to use two\nlevels of nesting.\nAnd anyway when there are too many loose objects, the idea\nis that we will pack them in few files (see below).\nThe number of characters in the first part can be chosen, but a good compromise is\n2 (default, also used by git)</p>\n</li>\n<li>\n<p>for maximum performance, loose objects are simply written as they are,\nwithout compression, hashing, ...\nThey are actually written first to a sandbox folder (in the same filesystem),\nand then moved in place with the correct UUID only when the file is closed.\nThis should prevent having leftover objects if the process dies, and\nthe move operation should be hopefully a fast atomic operation on most filesystems.</p>\n</li>\n<li>\n<p>When the user wants, loose objects are repacked in a few pack files. Indeed,\njust the fact of storing too many files is quite expensive\n(e.g. storing 65536 empty files in the same folder took over 3 minutes to write\nand over 4 minutes to delete on a Mac SSD).</p>\n</li>\n<li>\n<p>packing can be triggered by the user periodically.\nIt is even possible (to be stress tested, though) to pack while the object store\nis in use (this might temporarily impact read performance, though).\nThis operation takes all loose objects and puts them in a controllable number\nof packs. The name of the packs is given by the first few letters of the UUID\n(by default: 2, so 256 packs in total; configurable). A value of 2 is a good balance\nbetween the size of each pack (on average, ~4GB/pack for a 1TB repository) and\nthe number of packs (having many packs means that, even when performing bulk access,\nmany different files need to be open, which slows down performance).</p>\n</li>\n<li>\n<p>pack files are just concatenation of bytes of the packed objects. Any new object\nis appended to the pack (thanks to the efficiency of opening a file for appending).\nThe information for the offset and length of each pack is kept in a single SQLite\ndatabase.</p>\n</li>\n<li>\n<p>For each object, the SQLite database contains: the <code>uuid</code>, the <code>offset</code> (starting\nposition of the bytestream in the file), the <code>length</code> (number of bytes to read),\na boolean <code>compressed</code> flag, meaning if the bytestream has been zlib-compressed,\nand the <code>size</code> of the actual data (equal to <code>length</code> if <code>compressed</code> is false,\notherwise the size of the uncompressed stream, useful for statistics).</p>\n</li>\n<li>\n<p>Note that compression is on a per-object level. This allows much greater flexibility\n(the API still does not allow for this, but this is very easy to implement).\nOne could also think to clever logic to try to compress a file, but then store it\nuncompressed if it turns out that the compression ratio is not worth the time\nneeded to further uncompress it later.</p>\n</li>\n<li>\n<p>the repository configuration is kept in a top-level json (number of nesting levels\nfor loose objects and for packs, ...)</p>\n</li>\n<li>\n<p>API exists both to get and write a single object, but also to write directly\nto pack files (this cannot be done by multiple processes at the same time, though),\nand to read in bulk a given number of objects.\nThis is particularly convenient when using the object store for bulk import and\nexport, and very fast. Also, it is useful when getting all files of a given node.</p>\n<p>In normal operation, however, we expect the client to write loose objects,\nto be repacked  periodically (e.g. once a week).</p>\n<p>Some reference results for bulk operations:\nStoring 100'000 small objects directly to the packs takes about 10s.\nThe time to retrieve all of them is ~2.2s when using a single bulk call,\ncompared to ~44.5s when using 100'000 independent calls (still probably acceptable).\nMoreover, even getting, in 10 bulk calls, 10 random chunks of the objects (eventually\ncovering the whole set of 100'000 objects) only takes ~3.4s. This should demonstrate\nthat exporting a subset of the graph should be efficient (and the object store format\ncould be used also inside the export file).</p>\n</li>\n<li>\n<p>All operations internally (storing to a loose object, storing to a pack, reading\nfrom a loose object or from a pack, compression) are all happening via streaming.\nSo, even when dealing with huge files, these never fill the RAM (e.g. when reading\nor writing a multi-GB file, the memory usage has been tested to be capped at ~150MB).\nConvenience methods are available, anyway, to get directly an object content, if\nthe user wants.</p>\n</li>\n</ul>\n<h2>Further design choices</h2>\n<p>In addition, the following design choices have been made:</p>\n<ul>\n<li>\n<p>Each given object will get a random UUID (its generation cost is negligible, about\n4 microseconds per UUID).\nIt's up to the caller to track this into a filename or a folder structure.\nThe UUID is generated by the implementation and cannot be passed from the outside.\nThis guarantees random distribution of objects in packs, and avoids to have to\ncheck for files already existing.</p>\n</li>\n<li>\n<p>Pack naming and strategy is not determined by the user. Anyway it would be difficult\nto provide easy options to the user to customize the behavior, while implementing\na packing strategy that is efficient. Moreover, with the current packing strategy,\nit is immediate to know in which pack to check without having to keep also an index\nof the packs (this, however, would be possible in case we want to extend the behavior,\nsince anyway we have an index). But at the moment it does not seem necessary.</p>\n</li>\n<li>\n<p>A single index file is used. Having one pack index per file, while reducing a bit\nthe size of the index (one could skip storing the first part of the UUID, determined\nby the pack naming) turns out not to be very effective. Either one would keep all\nindexes open (but then one quickly hits the maximum number of open files, that e.g.\non Mac OS is of the order of ~256), or open the index, at every request, that risks to\nbe quite inefficient (not only to open, but also to load the DB, perform the query,\nreturn the results, and close again the file). Also for bulk requests, anyway, this\nwould prevent making very few DB requests (unless you keep all files open, that\nagain is not feasible).</p>\n</li>\n<li>\n<p>I tried a different way of storing the UUID on the DB (two long long ints rather than\n1 UUID string). I put a combined index on the two columns.\nI hoped in a speed up, using ints rather than strings, but (beside making the logic\nmuch more cumbersome and error prone) the performance actually decreased.\nSo I reverted to a UUID indexed string column.</p>\n</li>\n<li>\n<p>Deletion (not implemented yet), can just occur as a deletion of the loose object or\na removal from the index file. Later repacking of the packs can be used to recover\nthe disk space still occupied in the pack files.</p>\n</li>\n<li>\n<p>The current packing format is <code>rsync</code>-friendly. <code>rsync</code> has an algorithm to just\nsend the new part of a file, when appending. Actually, <code>rsync</code> has a clever rolling\nalgorithm that can also detect if the same block is in the file, even if at a\ndifferent position. Therefore, even if a pack is \"repacked\" (e.g. reordering\nobjects inside it, or removing deleted objects) does not prevent efficient\nrsync transfer.</p>\n<p>Some results: Let's considering a 1GB file that took ~4.5 mins to transfer fully\nthe first time  over my network.\nAfter transferring this 1GB file, <code>rsync</code> only takes 14 seconds\nto check the difference and transfer the additional 10MB appended to the 1GB file\n(and it indeed transfers only ~10MB).</p>\n<p>In addition,  if the contents are randomly reshuffled, the second time the <code>rsync</code>\nprocess took only 14 seconds, transferring only ~32MB, with a speedup of ~30x\n(in this test, I divided the file in 1021 chunks of random size, uniformly\ndistributed between 0 bytes and 2MB, so with a total size of ~1GB, and in the\nsecond <code>rsync</code> run I randomly reshuffled the chunks).</p>\n</li>\n<li>\n<p>Appending files to a single file does not prevent the Linux disk cache to work.\nTo test this, I created a ~3GB file, composed of a ~1GB file (of which I know the MD5)\nand of a ~2GB file (of which I know the MD5).\nThey are concatenated on a single file on disk.\nFile sizes are not multiples of a power of 2 to avoid alignment with block size.</p>\n<p>After flushing the caches, if one reads only the second half, 2GB are added to the\nkernel memory cache.</p>\n<p>After re-flushing the caches, if one reads only the first half, only 1GB is added\nto the memory cache.\nWithout further flushing the caches, if one reads also the first half,\n2 more GBs are added to the memory cache (totalling 3GB more).</p>\n<p>Therefore, caches are per blocks/pages in linux, not per file.\nConcatenating files does not impact performance on cache efficiency.</p>\n</li>\n</ul>\n\n          </div>"}, "last_serial": 7012149, "releases": {"0.2.0": [{"comment_text": "", "digests": {"md5": "62e8336c76f3825a6903e7a90a7ff9b3", "sha256": "883948de1e5151a398795b3f3b54ebd3f7a3f5673e59090e3c883e08c8d4e57f"}, "downloads": -1, "filename": "disk_objectstore-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "62e8336c76f3825a6903e7a90a7ff9b3", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 21845, "upload_time": "2020-04-11T14:12:28", "upload_time_iso_8601": "2020-04-11T14:12:28.421257Z", "url": "https://files.pythonhosted.org/packages/12/ea/8ca673054d9fd7dbc9e17cc164aea9c304225a9fb4f5f5e40eea729d8c04/disk_objectstore-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9175e4bdde130388dea079b18c4891ba", "sha256": "c76daa62f3c256446b432078332618598895749abbcb6d10c2741466c8527f53"}, "downloads": -1, "filename": "disk_objectstore-0.2.0.tar.gz", "has_sig": false, "md5_digest": "9175e4bdde130388dea079b18c4891ba", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24242, "upload_time": "2020-04-11T14:12:30", "upload_time_iso_8601": "2020-04-11T14:12:30.698185Z", "url": "https://files.pythonhosted.org/packages/6a/7d/df427b2503d8fc80ebaec06301bbb9315c2388df1717357aa029d92984a3/disk_objectstore-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "5a0527125e9166d4f4291e2c15d0a42b", "sha256": "0a8665d27760460a64286f862c5f5bb73a725a90e93ab8260cf31fa765b46124"}, "downloads": -1, "filename": "disk_objectstore-0.2.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "5a0527125e9166d4f4291e2c15d0a42b", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 36485, "upload_time": "2020-04-13T19:06:02", "upload_time_iso_8601": "2020-04-13T19:06:02.824033Z", "url": "https://files.pythonhosted.org/packages/19/db/cfb4ff509ea4afbdf31c6242576a745623e95d3ccec6ae78fc89f9f3b633/disk_objectstore-0.2.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "52624c4b79edfd5bf527478048d737f7", "sha256": "fc05aa446f8fa14edc88c843ab98ef80751c8748056113922119f589709f8390"}, "downloads": -1, "filename": "disk_objectstore-0.2.1.tar.gz", "has_sig": false, "md5_digest": "52624c4b79edfd5bf527478048d737f7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39834, "upload_time": "2020-04-13T19:06:04", "upload_time_iso_8601": "2020-04-13T19:06:04.212075Z", "url": "https://files.pythonhosted.org/packages/5d/24/a98cbcb9bf83f9e8790e118a261282fafd11d78cb36c6149f88a9fb798fa/disk_objectstore-0.2.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "5a0527125e9166d4f4291e2c15d0a42b", "sha256": "0a8665d27760460a64286f862c5f5bb73a725a90e93ab8260cf31fa765b46124"}, "downloads": -1, "filename": "disk_objectstore-0.2.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "5a0527125e9166d4f4291e2c15d0a42b", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 36485, "upload_time": "2020-04-13T19:06:02", "upload_time_iso_8601": "2020-04-13T19:06:02.824033Z", "url": "https://files.pythonhosted.org/packages/19/db/cfb4ff509ea4afbdf31c6242576a745623e95d3ccec6ae78fc89f9f3b633/disk_objectstore-0.2.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "52624c4b79edfd5bf527478048d737f7", "sha256": "fc05aa446f8fa14edc88c843ab98ef80751c8748056113922119f589709f8390"}, "downloads": -1, "filename": "disk_objectstore-0.2.1.tar.gz", "has_sig": false, "md5_digest": "52624c4b79edfd5bf527478048d737f7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39834, "upload_time": "2020-04-13T19:06:04", "upload_time_iso_8601": "2020-04-13T19:06:04.212075Z", "url": "https://files.pythonhosted.org/packages/5d/24/a98cbcb9bf83f9e8790e118a261282fafd11d78cb36c6149f88a9fb798fa/disk_objectstore-0.2.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:38:08 2020"}