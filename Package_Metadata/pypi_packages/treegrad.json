{"info": {"author": "Chapman Siu", "author_email": "chpmn.siu@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# TreeGrad\n\n`TreeGrad` implements a naive approach to converting a Gradient Boosted Tree Model to an Online trainable model. It does this by creating differentiable tree models which can be learned via auto-differentiable frameworks. `TreeGrad` is in essence an implementation of Kontschieder, Peter, et al. \"Deep neural decision forests.\" with extensions.\n\nTo install\n\n```\npython setup.py install\n```\n\nor alternatively from pypi\n\n\n```\npip install treegrad\n```\n\nRun tests:\n\n```\npython -m nose2\n```\n\n```\n@inproceedings{siu2019transferring,\n  title={Transferring Tree Ensembles to Neural Networks},\n  author={Siu, Chapman},\n  booktitle={International Conference on Neural Information Processing},\n  pages={471--480},\n  year={2019},\n  organization={Springer}\n}\n```\n\nLink: https://arxiv.org/abs/1904.11132\n\n\n# Usage\n\n```py\nfrom sklearn.\nimport treegrad as tgd\n\nmod = tgd.TGDClassifier(num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, autograd_config={'refit_splits':False})\nmod.fit(X, y)\nmod.partial_fit(X, y)\n```\n\n# Requirments\n\nThe requirements for this package are:\n\n*  lightgbm\n*  scikit-learn\n*  autograd\n\nFuture plans:\n\n*  Add implementation for Neural Architecture search for decision boundary splits (requires a bit of clean up - TBA)\n   *  Implementation can be done quite trivially using objects residing in `tree_utils.py` - Challenge is getting this working in a sane manner with `scikit-learn` interface.\n*  GPU enabled auto differentiation framework - see `notebooks/` for progress off Colab for Tensorflow 2.0 port\n*  support xgboost/lightgbm additional features such as monotone constraints\n*  Support `RegressorMixin`\n\n# Results\n\nWhen decision splits are reset and subsequently re-learned, TreeGrad can be competitive in performance with popular implementations (albeit an order of magnitude slower). Below is a table showing accuracy on test dataset on UCI benchmark datasets for Boosted Ensemble models (100 trees)\n\n\n| Dataset  | TreeGrad  | LightGBM  | Scikit-Learn (Gradient Boosting Classifier) |\n| ---------| --------- | --------- | ------------------------------------------- |\n| adult    | 0.860     | 0.873     | **0.874**                                   |\n| covtype  | 0.832     | **0.835** | 0.826                                       |\n| dna      | **0.950** | 0.949     | 0.946                                       |\n| glass    | 0.766     | **0.813** | 0.719                                       |\n| mandelon | **0.882** | 0.881     | 0.866                                       |\n| soybean  | **0.936** | **0.936** | 0.917                                       |\n| yeast    | **0.591** | 0.573     | 0.542                                       |\n\n\n# Implementation\n\n<!-- insert link to arxiv paper -->\n\nTo understand the implementation of `TreeGrad`, we interpret a decision tree algorithm to be a three layer neural network, where the layers are as follows:\n\n1.  Node layer, which determines the decision boundaries\n2.  Routing layer, which determines which nodes are used to route to the final leaf nodes\n3.  Leaf layer, the layer which determines the final predictions\n\nIn the node layer, the decision boundaries can be interpreted as _axis-parallel_ decision boundaries from your typical Linear Classifier; i.e. a fully connected dense layer\n\nThe routing layer requires a binary routing matrix to which essentially the global product routing is applied\n\nThe leaf layer is your typical fully connected dense layer.\n\nThis approach is the same as the one taken by Kontschieder, Peter, et al. \"Deep neural decision forests.\"\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/chappers/TreeGrad", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "treegrad", "package_url": "https://pypi.org/project/treegrad/", "platform": "", "project_url": "https://pypi.org/project/treegrad/", "project_urls": {"Homepage": "http://github.com/chappers/TreeGrad"}, "release_url": "https://pypi.org/project/treegrad/1.0.1/", "requires_dist": ["autograd", "sklearn", "lightgbm", "awscli (>=1.15.26); extra == 'complete'", "coverage (>=4.5.1); extra == 'complete'", "flake8 (>=3.5.0); extra == 'complete'", "m2r; extra == 'complete'", "nbsphinx (>=0.3.3); extra == 'complete'", "nose2 (>=0.7.4); extra == 'complete'", "nose2-html-report (>=0.6.0); extra == 'complete'", "pandoc (>=1.0.2); extra == 'complete'", "sphinx (>=1.6.6); extra == 'complete'", "sphinxcontrib-napoleon (>=0.6.1); extra == 'complete'", "sphinx (>=1.6.6); extra == 'development'", "sphinxcontrib-napoleon (>=0.6.1); extra == 'development'", "pandoc (>=1.0.2); extra == 'development'", "nbsphinx (>=0.3.3); extra == 'development'", "nose2 (>=0.7.4); extra == 'development'", "nose2-html-report (>=0.6.0); extra == 'development'", "coverage (>=4.5.1); extra == 'development'", "awscli (>=1.15.26); extra == 'development'", "flake8 (>=3.5.0); extra == 'development'", "m2r; extra == 'development'"], "requires_python": ">=3.5", "summary": "transfer parameters from lightgbm to differentiable decision trees!", "version": "1.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>TreeGrad</h1>\n<p><code>TreeGrad</code> implements a naive approach to converting a Gradient Boosted Tree Model to an Online trainable model. It does this by creating differentiable tree models which can be learned via auto-differentiable frameworks. <code>TreeGrad</code> is in essence an implementation of Kontschieder, Peter, et al. \"Deep neural decision forests.\" with extensions.</p>\n<p>To install</p>\n<pre><code>python setup.py install\n</code></pre>\n<p>or alternatively from pypi</p>\n<pre><code>pip install treegrad\n</code></pre>\n<p>Run tests:</p>\n<pre><code>python -m nose2\n</code></pre>\n<pre><code>@inproceedings{siu2019transferring,\n  title={Transferring Tree Ensembles to Neural Networks},\n  author={Siu, Chapman},\n  booktitle={International Conference on Neural Information Processing},\n  pages={471--480},\n  year={2019},\n  organization={Springer}\n}\n</code></pre>\n<p>Link: <a href=\"https://arxiv.org/abs/1904.11132\" rel=\"nofollow\">https://arxiv.org/abs/1904.11132</a></p>\n<h1>Usage</h1>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sklearn.</span>\n<span class=\"kn\">import</span> <span class=\"n\">treegrad</span> <span class=\"k\">as</span> <span class=\"n\">tgd</span>\n\n<span class=\"n\">mod</span> <span class=\"o\">=</span> <span class=\"n\">tgd</span><span class=\"o\">.</span><span class=\"n\">TGDClassifier</span><span class=\"p\">(</span><span class=\"n\">num_leaves</span><span class=\"o\">=</span><span class=\"mi\">31</span><span class=\"p\">,</span> <span class=\"n\">max_depth</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">autograd_config</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'refit_splits'</span><span class=\"p\">:</span><span class=\"kc\">False</span><span class=\"p\">})</span>\n<span class=\"n\">mod</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">mod</span><span class=\"o\">.</span><span class=\"n\">partial_fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n</pre>\n<h1>Requirments</h1>\n<p>The requirements for this package are:</p>\n<ul>\n<li>lightgbm</li>\n<li>scikit-learn</li>\n<li>autograd</li>\n</ul>\n<p>Future plans:</p>\n<ul>\n<li>Add implementation for Neural Architecture search for decision boundary splits (requires a bit of clean up - TBA)\n<ul>\n<li>Implementation can be done quite trivially using objects residing in <code>tree_utils.py</code> - Challenge is getting this working in a sane manner with <code>scikit-learn</code> interface.</li>\n</ul>\n</li>\n<li>GPU enabled auto differentiation framework - see <code>notebooks/</code> for progress off Colab for Tensorflow 2.0 port</li>\n<li>support xgboost/lightgbm additional features such as monotone constraints</li>\n<li>Support <code>RegressorMixin</code></li>\n</ul>\n<h1>Results</h1>\n<p>When decision splits are reset and subsequently re-learned, TreeGrad can be competitive in performance with popular implementations (albeit an order of magnitude slower). Below is a table showing accuracy on test dataset on UCI benchmark datasets for Boosted Ensemble models (100 trees)</p>\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>TreeGrad</th>\n<th>LightGBM</th>\n<th>Scikit-Learn (Gradient Boosting Classifier)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>adult</td>\n<td>0.860</td>\n<td>0.873</td>\n<td><strong>0.874</strong></td>\n</tr>\n<tr>\n<td>covtype</td>\n<td>0.832</td>\n<td><strong>0.835</strong></td>\n<td>0.826</td>\n</tr>\n<tr>\n<td>dna</td>\n<td><strong>0.950</strong></td>\n<td>0.949</td>\n<td>0.946</td>\n</tr>\n<tr>\n<td>glass</td>\n<td>0.766</td>\n<td><strong>0.813</strong></td>\n<td>0.719</td>\n</tr>\n<tr>\n<td>mandelon</td>\n<td><strong>0.882</strong></td>\n<td>0.881</td>\n<td>0.866</td>\n</tr>\n<tr>\n<td>soybean</td>\n<td><strong>0.936</strong></td>\n<td><strong>0.936</strong></td>\n<td>0.917</td>\n</tr>\n<tr>\n<td>yeast</td>\n<td><strong>0.591</strong></td>\n<td>0.573</td>\n<td>0.542</td>\n</tr></tbody></table>\n<h1>Implementation</h1>\n\n<p>To understand the implementation of <code>TreeGrad</code>, we interpret a decision tree algorithm to be a three layer neural network, where the layers are as follows:</p>\n<ol>\n<li>Node layer, which determines the decision boundaries</li>\n<li>Routing layer, which determines which nodes are used to route to the final leaf nodes</li>\n<li>Leaf layer, the layer which determines the final predictions</li>\n</ol>\n<p>In the node layer, the decision boundaries can be interpreted as <em>axis-parallel</em> decision boundaries from your typical Linear Classifier; i.e. a fully connected dense layer</p>\n<p>The routing layer requires a binary routing matrix to which essentially the global product routing is applied</p>\n<p>The leaf layer is your typical fully connected dense layer.</p>\n<p>This approach is the same as the one taken by Kontschieder, Peter, et al. \"Deep neural decision forests.\"</p>\n\n          </div>"}, "last_serial": 6572541, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "8cf3bef4f1d8e81d1c0800873df601a3", "sha256": "b58ba89e0ef2bb58b4a97db5b228bccc3591dff7c35347a18057596270c4ab8d"}, "downloads": -1, "filename": "treegrad-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "8cf3bef4f1d8e81d1c0800873df601a3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11728, "upload_time": "2020-02-04T22:35:11", "upload_time_iso_8601": "2020-02-04T22:35:11.096355Z", "url": "https://files.pythonhosted.org/packages/4c/2c/178b32e93adfd55c2f9488ddb31d843e7607e3fd803ef1ed8204c80a3f63/treegrad-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5709fd528f6af855f81a8562f3a287a8", "sha256": "0ec91349f6c411e8516e27775f02929d0b15feee63184d10e9944a968d33ef21"}, "downloads": -1, "filename": "treegrad-1.0.0.tar.gz", "has_sig": false, "md5_digest": "5709fd528f6af855f81a8562f3a287a8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 12301, "upload_time": "2019-04-25T02:57:21", "upload_time_iso_8601": "2019-04-25T02:57:21.494783Z", "url": "https://files.pythonhosted.org/packages/18/aa/bbca431e01080457213a2d8960cc781b4fa7b91fb8aa9b4a5c36bcb072a0/treegrad-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "a1f075086e047a0e0ae1f3326250388f", "sha256": "213a617c38cfa7e2af8c6f821f6f0cbf06ba08246b78fb5a3611b92efdc09eea"}, "downloads": -1, "filename": "treegrad-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "a1f075086e047a0e0ae1f3326250388f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11729, "upload_time": "2020-02-04T22:36:17", "upload_time_iso_8601": "2020-02-04T22:36:17.655186Z", "url": "https://files.pythonhosted.org/packages/32/44/d6ae0a7731b4bb7df2ade6ac748b0cab89c498a7c642372166b84391db2c/treegrad-1.0.1-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a1f075086e047a0e0ae1f3326250388f", "sha256": "213a617c38cfa7e2af8c6f821f6f0cbf06ba08246b78fb5a3611b92efdc09eea"}, "downloads": -1, "filename": "treegrad-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "a1f075086e047a0e0ae1f3326250388f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11729, "upload_time": "2020-02-04T22:36:17", "upload_time_iso_8601": "2020-02-04T22:36:17.655186Z", "url": "https://files.pythonhosted.org/packages/32/44/d6ae0a7731b4bb7df2ade6ac748b0cab89c498a7c642372166b84391db2c/treegrad-1.0.1-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 03:47:41 2020"}