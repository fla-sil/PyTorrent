{"info": {"author": "Rinesh P R", "author_email": "rineshpr90@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.5"], "description": "Solution Overview:\netl_pipeline is a standalone module implemented in standard python 3.5.4 environment using standard libraries for performing data cleansing, preparation and enrichment before feeding it to the machine learning model.\nThis module contains a class etl_pipeline in which all functionalities are implemented. The class contains two public methods for performing ETL operations on input data:\nload(): This method extracts data from input file and stores it as a python list in a private variable of the class for further processing in transform() method. The size of the list depends on the list of columns that are required in the output. This can be controlled using the parameters available in load() method.\ntransform():This method performs data transformation on the data stored in the list variable created by load method. The mapping rule can be passed as a parameter to transform() method.\nBesides the two methods described above, there are other support methods (private) available in the class:\n_get_delimiters: Returns delimiter and lineterminator of input file.\n_format_rows: Returns row text as list of columns from the input file using delimiter and lineterminator to split.\n_get_index: Generate a list of integers indicating the position/index of the values specified in select_list parameter of load() method with respect to the header(column names from file).\nload_to_csv: Stores transformed data in a csv file in specified by input parameter. The file will be saved in default working directory in output.csv if no filename is specified.\nThe below list of class variables are available for debug and testing:\ndelimiter: Column delimiter in input file\nlineterminator: Row delimiter used in input file\nrejected_rows: Provides the list of rows that are excluded from dataset due to bad data. In case of any error while performing data transformation in transform() method, last entry in this list variable gives more details on the location and type of error. \ninput_rows: Provides the list of rows that are used for data transform in the original format.\nheader:  Header row from the input file\nselect_list: List of columns that are present in output dataset.\nload():\nThis method accepts following parameters:\n1.\tdataFile: it is a mandatory parameter to pass the input file name.\n2.\theader: it is an optional parameter to pass the header information. If it is not provided first row in the file will be used as header. Default value is None.\n3.\tselect_list: it is an optional parameter containing the list of columns to be shown in the output. If it is not provided during function call, all columns in header will be available in output dataset. Default value is None.\n4.\tskip_rows_with: it is an optional parameter containing a single value or list of values used for identifying bad records. Default value \u00e2\u20ac\u02dc-\u00e2\u20ac\u02dc.\nExample 1: \u00e2\u20ac\u02dc-\u00e2\u20ac\u2122 excludes the rows containing \u00e2\u20ac\u02dc-\u00e2\u20ac\u2122 in any column\nExample 2: [\u00e2\u20ac\u02dc-\u00e2\u20ac\u2122,\u00e2\u20ac\u2122NA\u00e2\u20ac\u2122] excludes the rows containing \u00e2\u20ac\u02dc-\u00e2\u20ac\u2122 or \u00e2\u20ac\u02dcNA\u00e2\u20ac\u2122.\n5.\tdelimiter: it is an optional parameter containing the column delimiter. If it is not provided during function call, _get_delimiter method will be used to get the value. Default value is None.\n6.\tlineterminator: it is an optional parameter containing the row delimiter. If it is not provided during function call, _get_delimiter method will be used to get the value. Default value is None.\n\n\ntransform(): \nThis method accepts one optional parameter mapping_rule. No transformation will be applied on the data if a value is not provided.\nIf a mapping rule is passed as a dictionary, the output dataset will be transformed as defined.\n\nWe can use a dictionary to pass mapping details of the columns which need data transformation in transform method. The column name is used as the dictionary key and the value corresponds to a list of function(s) to be applied on the column values. You can also pass the output data type for each column as last value in the list. \nThis function returns a list of list containing the output dataset. The first list will be the header and remaining lists will be the values in row format. \n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "etl-pipeline", "package_url": "https://pypi.org/project/etl-pipeline/", "platform": "", "project_url": "https://pypi.org/project/etl-pipeline/", "project_urls": null, "release_url": "https://pypi.org/project/etl-pipeline/0.0.2/", "requires_dist": null, "requires_python": "", "summary": "package for performing etl processing on csv files", "version": "0.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Solution Overview:\netl_pipeline is a standalone module implemented in standard python 3.5.4 environment using standard libraries for performing data cleansing, preparation and enrichment before feeding it to the machine learning model.\nThis module contains a class etl_pipeline in which all functionalities are implemented. The class contains two public methods for performing ETL operations on input data:\nload(): This method extracts data from input file and stores it as a python list in a private variable of the class for further processing in transform() method. The size of the list depends on the list of columns that are required in the output. This can be controlled using the parameters available in load() method.\ntransform():This method performs data transformation on the data stored in the list variable created by load method. The mapping rule can be passed as a parameter to transform() method.\nBesides the two methods described above, there are other support methods (private) available in the class:\n_get_delimiters: Returns delimiter and lineterminator of input file.\n_format_rows: Returns row text as list of columns from the input file using delimiter and lineterminator to split.\n_get_index: Generate a list of integers indicating the position/index of the values specified in select_list parameter of load() method with respect to the header(column names from file).\nload_to_csv: Stores transformed data in a csv file in specified by input parameter. The file will be saved in default working directory in output.csv if no filename is specified.\nThe below list of class variables are available for debug and testing:\ndelimiter: Column delimiter in input file\nlineterminator: Row delimiter used in input file\nrejected_rows: Provides the list of rows that are excluded from dataset due to bad data. In case of any error while performing data transformation in transform() method, last entry in this list variable gives more details on the location and type of error.\ninput_rows: Provides the list of rows that are used for data transform in the original format.\nheader:  Header row from the input file\nselect_list: List of columns that are present in output dataset.\nload():\nThis method accepts following parameters:</p>\n<ol>\n<li>dataFile: it is a mandatory parameter to pass the input file name.</li>\n<li>header: it is an optional parameter to pass the header information. If it is not provided first row in the file will be used as header. Default value is None.</li>\n<li>select_list: it is an optional parameter containing the list of columns to be shown in the output. If it is not provided during function call, all columns in header will be available in output dataset. Default value is None.</li>\n<li>skip_rows_with: it is an optional parameter containing a single value or list of values used for identifying bad records. Default value \u00e2\u20ac\u02dc-\u00e2\u20ac\u02dc.\nExample 1: \u00e2\u20ac\u02dc-\u00e2\u20ac\u2122 excludes the rows containing \u00e2\u20ac\u02dc-\u00e2\u20ac\u2122 in any column\nExample 2: [\u00e2\u20ac\u02dc-\u00e2\u20ac\u2122,\u00e2\u20ac\u2122NA\u00e2\u20ac\u2122] excludes the rows containing \u00e2\u20ac\u02dc-\u00e2\u20ac\u2122 or \u00e2\u20ac\u02dcNA\u00e2\u20ac\u2122.</li>\n<li>delimiter: it is an optional parameter containing the column delimiter. If it is not provided during function call, _get_delimiter method will be used to get the value. Default value is None.</li>\n<li>lineterminator: it is an optional parameter containing the row delimiter. If it is not provided during function call, _get_delimiter method will be used to get the value. Default value is None.</li>\n</ol>\n<p>transform():\nThis method accepts one optional parameter mapping_rule. No transformation will be applied on the data if a value is not provided.\nIf a mapping rule is passed as a dictionary, the output dataset will be transformed as defined.</p>\n<p>We can use a dictionary to pass mapping details of the columns which need data transformation in transform method. The column name is used as the dictionary key and the value corresponds to a list of function(s) to be applied on the column values. You can also pass the output data type for each column as last value in the list.\nThis function returns a list of list containing the output dataset. The first list will be the header and remaining lists will be the values in row format.</p>\n\n          </div>"}, "last_serial": 4508270, "releases": {"0.0.2": [{"comment_text": "", "digests": {"md5": "0fe4193395998fd59d57f6d9810d4044", "sha256": "9c210d49900169ac65a64f5ea48ad3d2a8fd077a04bc4fcf04511ed2e63c748f"}, "downloads": -1, "filename": "etl_pipeline-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "0fe4193395998fd59d57f6d9810d4044", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5756, "upload_time": "2018-11-20T16:12:52", "upload_time_iso_8601": "2018-11-20T16:12:52.579029Z", "url": "https://files.pythonhosted.org/packages/d6/59/1e823b041ab9a704041430d4b1419ad408ee90414e638b1625658ba9f679/etl_pipeline-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "dd938a2ffd1f55308ff1cad85dd0fb69", "sha256": "06f6797569cd7663ffbea712f2bf69fd8b1b8f4c13336e07285726912589d5eb"}, "downloads": -1, "filename": "etl_pipeline-0.0.2.tar.gz", "has_sig": false, "md5_digest": "dd938a2ffd1f55308ff1cad85dd0fb69", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4401, "upload_time": "2018-11-20T16:12:55", "upload_time_iso_8601": "2018-11-20T16:12:55.273231Z", "url": "https://files.pythonhosted.org/packages/8d/0e/cc263a4c986b63c99dd9520bc58c24b8fe942c26a0c69dc8413ebc4da8c1/etl_pipeline-0.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "0fe4193395998fd59d57f6d9810d4044", "sha256": "9c210d49900169ac65a64f5ea48ad3d2a8fd077a04bc4fcf04511ed2e63c748f"}, "downloads": -1, "filename": "etl_pipeline-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "0fe4193395998fd59d57f6d9810d4044", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5756, "upload_time": "2018-11-20T16:12:52", "upload_time_iso_8601": "2018-11-20T16:12:52.579029Z", "url": "https://files.pythonhosted.org/packages/d6/59/1e823b041ab9a704041430d4b1419ad408ee90414e638b1625658ba9f679/etl_pipeline-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "dd938a2ffd1f55308ff1cad85dd0fb69", "sha256": "06f6797569cd7663ffbea712f2bf69fd8b1b8f4c13336e07285726912589d5eb"}, "downloads": -1, "filename": "etl_pipeline-0.0.2.tar.gz", "has_sig": false, "md5_digest": "dd938a2ffd1f55308ff1cad85dd0fb69", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4401, "upload_time": "2018-11-20T16:12:55", "upload_time_iso_8601": "2018-11-20T16:12:55.273231Z", "url": "https://files.pythonhosted.org/packages/8d/0e/cc263a4c986b63c99dd9520bc58c24b8fe942c26a0c69dc8413ebc4da8c1/etl_pipeline-0.0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:45:24 2020"}