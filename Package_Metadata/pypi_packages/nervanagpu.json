{"info": {"author": "Nervana Systems", "author_email": "info@nervanasys.com", "bugtrack_url": null, "classifiers": [], "description": "NervanaGPU library\r\n==================\r\n\r\nIntroduction\r\n------------\r\n\r\n**nervanagpu** is a Python module for deep learning. It includes,\r\n\r\n-  matrix-multiply (GEMM), convolution, and pooling kernels optimized\r\n   using a custom\r\n   `assembler <https://github.com/NervanaSystems/maxas>`__,\r\n-  element-wise and broadcast operations that automatically compound\r\n   into efficient kernels,\r\n-  a simple but powerful array class, leveraging and with code partially\r\n   borrowed from pycuda\\ `1 <#refs>`__\\ ,\r\n-  layer classes for building networks for benchmarking,\r\n-  full assembler source to encourage contributions from the community.\r\n\r\nDesign goals\r\n^^^^^^^^^^^^\r\n\r\n**nervanagpu** grew out of a tool Nervana uses for internal hardware\r\nefforts. It's been repackaged for use by the community. The goals for\r\n**nervanagpu** are to provide,\r\n\r\n-  near **theoretical peak performance**,\r\n-  numpy functionality for **ease-of-use**,\r\n-  convolution kernel features and arguments identical to\r\n   cuDNN\\ `2 <#refs>`__\\ ,\r\n-  integration into `neon <https://github.com/NervanaSystems/neon>`__,\r\n   Nervana's full-featured deep learning library,\r\n-  a tool for algorithmic explorations using alternative numerical\r\n   formats,\r\n-  a seemless transition path to Nervana hardware,\r\n-  ease of integration into other deep learning frameworks.\r\n\r\nOnly NVIDIA Maxwell and future architectures are supported. Older\r\narchitectures are not well-suited for assembler level optimizations used\r\nhere.\r\n\r\nNumerical formats\r\n^^^^^^^^^^^^^^^^^\r\n\r\nSupported numerical formats currently include,\r\n\r\n-  **fp32**: standard 32-bit floating point,\r\n-  **fp16**: 16-bit floating point memory format with underlying\r\n   operations in 32 bits.\r\n-  **int8** and **uint8**: in elementwise and as input to the first\r\n   convolutional layer.\r\n\r\nwith more to come (eg. like\r\n`this <https://github.com/NervanaSystems/nervana-lib-gpu-performance-preview>`__).\r\n\r\nExtra features\r\n^^^^^^^^^^^^^^\r\n\r\nOur kernels have some additional useful features:\r\n\r\n-  3D convolutions and 4D pooling (including output feature map dim)\r\n-  optional ReLu is builtin to GEMM and convolution operations,\r\n-  stochastic rounding support for **fp16**\\ \\ `3 <#refs>`__\\ ,\r\n-  instrumented to return statistics useful for avoiding numerical\r\n   issues (coming soon),\r\n-  support for matrix sizes common in deep learning, significantly out\r\n   performing cuBLAS\r\n\r\nSmall optimizations like these can result in significant speed and\r\nperformance improvements.\r\n\r\nUsage\r\n-----\r\n\r\n**nervanagpu** includes a factory class ``NervanaGPU`` and a numpy-like\r\narray class ``GPUTensor``. Memory layout for tensors and gemm ops is\r\n**row-ordered**. Below are examples on how they are used.\r\n\r\nMatrix multiplication example\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nHere is full example of doing a basic GEMM operation using 16-bit float:\r\n\r\n.. code:: python\r\n\r\n    import numpy as np\r\n    import pycuda.autoinit\r\n    from nervanagpu import NervanaGPU\r\n\r\n    # initialize factory class\r\n    ng = NervanaGPU(stochastic_round=False)\r\n\r\n    m, n, k  = 10, 20, 10\r\n    dtype = np.float16\r\n\r\n    # create matrices on host\r\n    cpuA = np.random.randn(k,m)\r\n    cpuB = np.random.randn(k,n)\r\n\r\n    # transfer to device\r\n    devA = ng.array(cpuA, dtype=dtype)\r\n    devB = ng.array(cpuB, dtype=dtype)\r\n    devC = ng.empty((m,n), dtype=dtype)\r\n\r\n    # do GEMM operation\r\n    ng.dot(devA.T, devB, devC, relu=False)\r\n\r\n    # get from device\r\n    cpuC = devC.get()\r\n\r\nElement-wise operations\r\n~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\n**nervanagpu** compiles tensor arithmetic expressions into efficient\r\nCUDA kernels which are lazily evaluated upon assignment. For example,\r\ncomputing variance along an axis consists of a set of element-wise,\r\nreduction and broadcast operations that compiles to a single kernel:\r\n\r\n.. code:: python\r\n\r\n    # import and initialize NervanaGPU, transfer matrix from cpu to dev as above\r\n\r\n    devC[:] = ng.mean(ng.square(devA - ng.mean(devA, axis=1)), axis=1)\r\n\r\nBatch normalization can be done by computing mean and variance across\r\nthe batch (n) dimension and automatically taking advantage of\r\nbroadcasting to subtract and divide the original data.\r\n\r\n.. code:: python\r\n\r\n    # import and initialize NervanaGPU as above\r\n\r\n    eps  = .001\r\n    A    = ng.empty((128, 32), dtype=np.float16)\r\n    A[:] = ng.rand() # generate uniform random on device between 0 and 1\r\n\r\n    # Normalize batch data by batch mean and variance, \r\n    A[:] = ng.reciprocal(ng.sqrt(ng.var(A, axis=1) + eps)) * (A - ng.mean(A, axis=1)) \r\n\r\nThe last expression above is automatically collapsed into a single gpu\r\nkernel. The two mean(A,axis=1) expressions are automatically simplified\r\ninto one. To be able to broadcast a reduction to a subsequent operation\r\nthe reduction op must appear prior to the broadcast op in the\r\n`*postfix* <http://en.wikipedia.org/wiki/Reverse_Polish_notation>`__\r\nversion of the expression. Hence, the reciprocal operation instead of\r\ndivision.\r\n\r\nBuilding\r\n--------\r\n\r\n**nervanagpu** comes with full assembler code for kernels. To build the\r\nkernels, install\r\n`**maxas** <https://github.com/NervanaSystems/maxas>`__, Nervana's\r\nassembler for NVIDIA Maxwell. The module can then be built by running:\r\n\r\n::\r\n\r\n    make kernels      # build the kernels\r\n    make python       # build python bindings\r\n    make test         # run nosetests\r\n    make doc          # build sphinx docs\r\n\r\nA simple ``make`` will perform the first two steps.\r\n\r\nDocumentation and tests are currently sparse. Please contribute.\r\n\r\nPerformance\r\n-----------\r\n\r\n**nervanagpu** comes with a set of benchmark scripts under\r\n``nervanagpu/benchmarks``. Also included are scripts to validate kernel\r\nresults against cuBLAS and cuDNN.\r\n\r\nHere is a sample run of ``benchmarks/convnet-benchmarks.py`` using the\r\nnetworks listed on Soumith Chintala's `benchmarking\r\npage <https://github.com/soumith/convnet-benchmarks>`__. Run on a single\r\nTitanX with default clocks and power limit:\r\n\r\n::\r\n\r\n    ---------------------------------------------\r\n    Alexnet (dtype=float16, N=128)  Results:\r\n    ---------------------------------------------\r\n    Avg(10) fprop:   29.498 msecs 6043.698 gflops\r\n    Avg(10) bprop:   66.562 msecs 5356.689 gflops\r\n    Avg(10) total:   96.059 msecs 5567.654 gflops\r\n    ---------------------------------------------\r\n    Alexnet (dtype=float32, N=128)  Results:\r\n    ---------------------------------------------\r\n    Avg(10) fprop:   31.251 msecs 5704.698 gflops\r\n    Avg(10) bprop:   77.567 msecs 4596.660 gflops\r\n    Avg(10) total:  108.818 msecs 4914.869 gflops\r\n\r\n    ---------------------------------------------\r\n    Overfeat (dtype=float16, N=128)  Results:\r\n    ---------------------------------------------\r\n    Avg(10) fprop:  116.723 msecs 6134.994 gflops\r\n    Avg(10) bprop:  242.084 msecs 5916.054 gflops\r\n    Avg(10) total:  358.807 msecs 5987.277 gflops\r\n    ---------------------------------------------\r\n    Overfeat (dtype=float32, N=128)  Results:\r\n    ---------------------------------------------\r\n    Avg(10) fprop:  124.569 msecs 5748.559 gflops\r\n    Avg(10) bprop:  278.408 msecs 5144.192 gflops\r\n    Avg(10) total:  402.977 msecs 5331.015 gflops\r\n\r\n    ---------------------------------------------\r\n    VGG (dtype=float16, N=64)  Results:\r\n    ---------------------------------------------\r\n    Avg(10) fprop:  162.186 msecs 5978.348 gflops\r\n    Avg(10) bprop:  357.850 msecs 5419.051 gflops\r\n    Avg(10) total:  520.036 msecs 5593.481 gflops\r\n    ---------------------------------------------\r\n    VGG (dtype=float32, N=64)  Results:\r\n    ---------------------------------------------\r\n    Avg(10) fprop:  170.822 msecs 5676.112 gflops\r\n    Avg(10) bprop:  438.031 msecs 4427.108 gflops\r\n    Avg(10) total:  608.853 msecs 4777.533 gflops\r\n\r\nAcknowledgements\r\n^^^^^^^^^^^^^^^^\r\n\r\nThanks to Erich Elsen and Bryan Catanzaro of Baidu, Matthieu Courbariaux\r\nand Fr\u00e9d\u00e9ric Bastien of the Bengio lab, Vincent Vanhoucke of Google, and\r\nSoumith Chintala of Facebook for feedback on early versions of this\r\nlibrary. We'd also like to thank NVIDIA for generously providing us with\r\nseveral TitanXs for benchmarking.\r\n\r\nReferences \r\n^^^^^^^^^^^\r\n\r\n1. Andreas Kl\u00f6ckner, Nicolas Pinto, Yunsup Lee, Bryan Catanzaro, Paul\r\n   Ivanov, Ahmed Fasih. `*PyCUDA and PyOpenCL: A scripting-based\r\n   approach to GPU run-time code\r\n   generation* <http://arxiv.org/abs/0911.3456>`__ Parallel Computing,\r\n   Volume 38, Issue 3, March 2012, Pages 157-174.\r\n\r\n2. Chetlur, Sharan, Cliff Woolley, Philippe Vandermersch, Jonathan\r\n   Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. `*cuDNN:\r\n   Efficient primitives for deep\r\n   learning.* <http://arxiv.org/abs/1410.0759>`__ arXiv preprint\r\n   arXiv:1410.0759 (2014).\r\n\r\n3. Gupta, Suyog, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish\r\n   Narayanan. `*Deep Learning with Limited Numerical\r\n   Precision.* <http://arxiv.org/abs/1502.02551>`__ arXiv preprint\r\n   arXiv:1502.02551 (2015).", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/nervanasys/nervanagpu", "keywords": "", "license": "(see LICENSE document)", "maintainer": "", "maintainer_email": "", "name": "nervanagpu", "package_url": "https://pypi.org/project/nervanagpu/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/nervanagpu/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/nervanasys/nervanagpu"}, "release_url": "https://pypi.org/project/nervanagpu/0.3.1/", "requires_dist": null, "requires_python": null, "summary": "Python bindings for Nervana GPU kernels", "version": "0.3.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"introduction\">\n<h2>Introduction</h2>\n<p><strong>nervanagpu</strong> is a Python module for deep learning. It includes,</p>\n<ul>\n<li>matrix-multiply (GEMM), convolution, and pooling kernels optimized\nusing a custom\n<a href=\"https://github.com/NervanaSystems/maxas\" rel=\"nofollow\">assembler</a>,</li>\n<li>element-wise and broadcast operations that automatically compound\ninto efficient kernels,</li>\n<li>a simple but powerful array class, leveraging and with code partially\nborrowed from pycuda<a href=\"#refs\" rel=\"nofollow\">1</a>,</li>\n<li>layer classes for building networks for benchmarking,</li>\n<li>full assembler source to encourage contributions from the community.</li>\n</ul>\n<div id=\"design-goals\">\n<h3>Design goals</h3>\n<p><strong>nervanagpu</strong> grew out of a tool Nervana uses for internal hardware\nefforts. It\u2019s been repackaged for use by the community. The goals for\n<strong>nervanagpu</strong> are to provide,</p>\n<ul>\n<li>near <strong>theoretical peak performance</strong>,</li>\n<li>numpy functionality for <strong>ease-of-use</strong>,</li>\n<li>convolution kernel features and arguments identical to\ncuDNN<a href=\"#refs\" rel=\"nofollow\">2</a>,</li>\n<li>integration into <a href=\"https://github.com/NervanaSystems/neon\" rel=\"nofollow\">neon</a>,\nNervana\u2019s full-featured deep learning library,</li>\n<li>a tool for algorithmic explorations using alternative numerical\nformats,</li>\n<li>a seemless transition path to Nervana hardware,</li>\n<li>ease of integration into other deep learning frameworks.</li>\n</ul>\n<p>Only NVIDIA Maxwell and future architectures are supported. Older\narchitectures are not well-suited for assembler level optimizations used\nhere.</p>\n</div>\n<div id=\"numerical-formats\">\n<h3>Numerical formats</h3>\n<p>Supported numerical formats currently include,</p>\n<ul>\n<li><strong>fp32</strong>: standard 32-bit floating point,</li>\n<li><strong>fp16</strong>: 16-bit floating point memory format with underlying\noperations in 32 bits.</li>\n<li><strong>int8</strong> and <strong>uint8</strong>: in elementwise and as input to the first\nconvolutional layer.</li>\n</ul>\n<p>with more to come (eg. like\n<a href=\"https://github.com/NervanaSystems/nervana-lib-gpu-performance-preview\" rel=\"nofollow\">this</a>).</p>\n</div>\n<div id=\"extra-features\">\n<h3>Extra features</h3>\n<p>Our kernels have some additional useful features:</p>\n<ul>\n<li>3D convolutions and 4D pooling (including output feature map dim)</li>\n<li>optional ReLu is builtin to GEMM and convolution operations,</li>\n<li>stochastic rounding support for <strong>fp16</strong><a href=\"#refs\" rel=\"nofollow\">3</a>,</li>\n<li>instrumented to return statistics useful for avoiding numerical\nissues (coming soon),</li>\n<li>support for matrix sizes common in deep learning, significantly out\nperforming cuBLAS</li>\n</ul>\n<p>Small optimizations like these can result in significant speed and\nperformance improvements.</p>\n</div>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p><strong>nervanagpu</strong> includes a factory class <tt>NervanaGPU</tt> and a numpy-like\narray class <tt>GPUTensor</tt>. Memory layout for tensors and gemm ops is\n<strong>row-ordered</strong>. Below are examples on how they are used.</p>\n<div id=\"matrix-multiplication-example\">\n<h3>Matrix multiplication example</h3>\n<p>Here is full example of doing a basic GEMM operation using 16-bit float:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pycuda.autoinit</span>\n<span class=\"kn\">from</span> <span class=\"nn\">nervanagpu</span> <span class=\"kn\">import</span> <span class=\"n\">NervanaGPU</span>\n\n<span class=\"c1\"># initialize factory class</span>\n<span class=\"n\">ng</span> <span class=\"o\">=</span> <span class=\"n\">NervanaGPU</span><span class=\"p\">(</span><span class=\"n\">stochastic_round</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n\n<span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">k</span>  <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">10</span>\n<span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float16</span>\n\n<span class=\"c1\"># create matrices on host</span>\n<span class=\"n\">cpuA</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">m</span><span class=\"p\">)</span>\n<span class=\"n\">cpuB</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">n</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># transfer to device</span>\n<span class=\"n\">devA</span> <span class=\"o\">=</span> <span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">cpuA</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n<span class=\"n\">devB</span> <span class=\"o\">=</span> <span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">cpuB</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n<span class=\"n\">devC</span> <span class=\"o\">=</span> <span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">m</span><span class=\"p\">,</span><span class=\"n\">n</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># do GEMM operation</span>\n<span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">devA</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">devB</span><span class=\"p\">,</span> <span class=\"n\">devC</span><span class=\"p\">,</span> <span class=\"n\">relu</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># get from device</span>\n<span class=\"n\">cpuC</span> <span class=\"o\">=</span> <span class=\"n\">devC</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">()</span>\n</pre>\n<div id=\"element-wise-operations\">\n<h4>Element-wise operations</h4>\n<p><strong>nervanagpu</strong> compiles tensor arithmetic expressions into efficient\nCUDA kernels which are lazily evaluated upon assignment. For example,\ncomputing variance along an axis consists of a set of element-wise,\nreduction and broadcast operations that compiles to a single kernel:</p>\n<pre><span class=\"c1\"># import and initialize NervanaGPU, transfer matrix from cpu to dev as above</span>\n\n<span class=\"n\">devC</span><span class=\"p\">[:]</span> <span class=\"o\">=</span> <span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">devA</span> <span class=\"o\">-</span> <span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">devA</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)),</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n</pre>\n<p>Batch normalization can be done by computing mean and variance across\nthe batch (n) dimension and automatically taking advantage of\nbroadcasting to subtract and divide the original data.</p>\n<pre><span class=\"c1\"># import and initialize NervanaGPU as above</span>\n\n<span class=\"n\">eps</span>  <span class=\"o\">=</span> <span class=\"o\">.</span><span class=\"mi\">001</span>\n<span class=\"n\">A</span>    <span class=\"o\">=</span> <span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n<span class=\"n\">A</span><span class=\"p\">[:]</span> <span class=\"o\">=</span> <span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">()</span> <span class=\"c1\"># generate uniform random on device between 0 and 1</span>\n\n<span class=\"c1\"># Normalize batch data by batch mean and variance,</span>\n<span class=\"n\">A</span><span class=\"p\">[:]</span> <span class=\"o\">=</span> <span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">reciprocal</span><span class=\"p\">(</span><span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">var</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">eps</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">A</span> <span class=\"o\">-</span> <span class=\"n\">ng</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n</pre>\n<p>The last expression above is automatically collapsed into a single gpu\nkernel. The two mean(A,axis=1) expressions are automatically simplified\ninto one. To be able to broadcast a reduction to a subsequent operation\nthe reduction op must appear prior to the broadcast op in the\n<a href=\"http://en.wikipedia.org/wiki/Reverse_Polish_notation\" rel=\"nofollow\">*postfix*</a>\nversion of the expression. Hence, the reciprocal operation instead of\ndivision.</p>\n</div>\n</div>\n</div>\n<div id=\"building\">\n<h2>Building</h2>\n<p><strong>nervanagpu</strong> comes with full assembler code for kernels. To build the\nkernels, install\n<a href=\"https://github.com/NervanaSystems/maxas\" rel=\"nofollow\">**maxas**</a>, Nervana\u2019s\nassembler for NVIDIA Maxwell. The module can then be built by running:</p>\n<pre>make kernels      # build the kernels\nmake python       # build python bindings\nmake test         # run nosetests\nmake doc          # build sphinx docs\n</pre>\n<p>A simple <tt>make</tt> will perform the first two steps.</p>\n<p>Documentation and tests are currently sparse. Please contribute.</p>\n</div>\n<div id=\"performance\">\n<h2>Performance</h2>\n<p><strong>nervanagpu</strong> comes with a set of benchmark scripts under\n<tt>nervanagpu/benchmarks</tt>. Also included are scripts to validate kernel\nresults against cuBLAS and cuDNN.</p>\n<p>Here is a sample run of <tt><span class=\"pre\">benchmarks/convnet-benchmarks.py</span></tt> using the\nnetworks listed on Soumith Chintala\u2019s <a href=\"https://github.com/soumith/convnet-benchmarks\" rel=\"nofollow\">benchmarking\npage</a>. Run on a single\nTitanX with default clocks and power limit:</p>\n<pre>---------------------------------------------\nAlexnet (dtype=float16, N=128)  Results:\n---------------------------------------------\nAvg(10) fprop:   29.498 msecs 6043.698 gflops\nAvg(10) bprop:   66.562 msecs 5356.689 gflops\nAvg(10) total:   96.059 msecs 5567.654 gflops\n---------------------------------------------\nAlexnet (dtype=float32, N=128)  Results:\n---------------------------------------------\nAvg(10) fprop:   31.251 msecs 5704.698 gflops\nAvg(10) bprop:   77.567 msecs 4596.660 gflops\nAvg(10) total:  108.818 msecs 4914.869 gflops\n\n---------------------------------------------\nOverfeat (dtype=float16, N=128)  Results:\n---------------------------------------------\nAvg(10) fprop:  116.723 msecs 6134.994 gflops\nAvg(10) bprop:  242.084 msecs 5916.054 gflops\nAvg(10) total:  358.807 msecs 5987.277 gflops\n---------------------------------------------\nOverfeat (dtype=float32, N=128)  Results:\n---------------------------------------------\nAvg(10) fprop:  124.569 msecs 5748.559 gflops\nAvg(10) bprop:  278.408 msecs 5144.192 gflops\nAvg(10) total:  402.977 msecs 5331.015 gflops\n\n---------------------------------------------\nVGG (dtype=float16, N=64)  Results:\n---------------------------------------------\nAvg(10) fprop:  162.186 msecs 5978.348 gflops\nAvg(10) bprop:  357.850 msecs 5419.051 gflops\nAvg(10) total:  520.036 msecs 5593.481 gflops\n---------------------------------------------\nVGG (dtype=float32, N=64)  Results:\n---------------------------------------------\nAvg(10) fprop:  170.822 msecs 5676.112 gflops\nAvg(10) bprop:  438.031 msecs 4427.108 gflops\nAvg(10) total:  608.853 msecs 4777.533 gflops\n</pre>\n<div id=\"acknowledgements\">\n<h3>Acknowledgements</h3>\n<p>Thanks to Erich Elsen and Bryan Catanzaro of Baidu, Matthieu Courbariaux\nand Fr\u00e9d\u00e9ric Bastien of the Bengio lab, Vincent Vanhoucke of Google, and\nSoumith Chintala of Facebook for feedback on early versions of this\nlibrary. We\u2019d also like to thank NVIDIA for generously providing us with\nseveral TitanXs for benchmarking.</p>\n</div>\n<div id=\"references\">\n<h3>References</h3>\n<ol>\n<li>Andreas Kl\u00f6ckner, Nicolas Pinto, Yunsup Lee, Bryan Catanzaro, Paul\nIvanov, Ahmed Fasih. <a href=\"http://arxiv.org/abs/0911.3456\" rel=\"nofollow\">*PyCUDA and PyOpenCL: A scripting-based\napproach to GPU run-time code\ngeneration*</a> Parallel Computing,\nVolume 38, Issue 3, March 2012, Pages 157-174.</li>\n<li>Chetlur, Sharan, Cliff Woolley, Philippe Vandermersch, Jonathan\nCohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. <a href=\"http://arxiv.org/abs/1410.0759\" rel=\"nofollow\">*cuDNN:\nEfficient primitives for deep\nlearning.*</a> arXiv preprint\narXiv:1410.0759 (2014).</li>\n<li>Gupta, Suyog, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish\nNarayanan. <a href=\"http://arxiv.org/abs/1502.02551\" rel=\"nofollow\">*Deep Learning with Limited Numerical\nPrecision.*</a> arXiv preprint\narXiv:1502.02551 (2015).</li>\n</ol>\n</div>\n</div>\n\n          </div>"}, "last_serial": 1534832, "releases": {"0.3.1": [{"comment_text": "", "digests": {"md5": "769ab14b1a2f2c94d95f68b1770e48a9", "sha256": "7329813f1bf44a00a4b028dfb345386224ea5ba48b057e4a615b501db25a155b"}, "downloads": -1, "filename": "nervanagpu-0.3.1.tar.gz", "has_sig": false, "md5_digest": "769ab14b1a2f2c94d95f68b1770e48a9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 103690, "upload_time": "2015-05-04T22:59:56", "upload_time_iso_8601": "2015-05-04T22:59:56.853607Z", "url": "https://files.pythonhosted.org/packages/73/1b/6d170c5c42733cc94db34e0e4bae6c51cf4f49510b512eccaa4d0ad53f85/nervanagpu-0.3.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "769ab14b1a2f2c94d95f68b1770e48a9", "sha256": "7329813f1bf44a00a4b028dfb345386224ea5ba48b057e4a615b501db25a155b"}, "downloads": -1, "filename": "nervanagpu-0.3.1.tar.gz", "has_sig": false, "md5_digest": "769ab14b1a2f2c94d95f68b1770e48a9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 103690, "upload_time": "2015-05-04T22:59:56", "upload_time_iso_8601": "2015-05-04T22:59:56.853607Z", "url": "https://files.pythonhosted.org/packages/73/1b/6d170c5c42733cc94db34e0e4bae6c51cf4f49510b512eccaa4d0ad53f85/nervanagpu-0.3.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:46:25 2020"}