{"info": {"author": "Wu Zhen Zhou", "author_email": "hyciswu@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "[![Build Status](https://travis-ci.org/hycis/TensorGraphX.svg?branch=master)](https://travis-ci.org/hycis/TensorGraphX)\n\n# TensorGraphX - Simplicity is Beauty\nTensorGraphX is a simple, lean, and clean framework on TensorFlow for building any imaginable models.\n\nAs deep learning becomes more and more common and the architectures becoming more\nand more complicated, it seems that we need some easy to use framework to quickly\nbuild these models and that's what TensorGraphX is designed for. It's a very simple\nframework that adds a very thin layer above tensorflow. It is for more advanced\nusers who want to have more control and flexibility over his model building and\nwho wants efficiency at the same time.\n\n-----\n### Target Audience\nTensorGraphX is targeted more at intermediate to advance users who feel keras or\nother packages is having too much restrictions and too much black box on model\nbuilding, and someone who don't want to rewrite the standard layers in tensorflow\nconstantly. Also for enterprise users who want to share deep learning models\neasily between teams.\n\n-----\n### Install\n\nFirst you need to install [tensorflow](https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html)\n\nTo install tensorgraphx simply do via pip\n```bash\nsudo pip install tensorgraphx\n```\nor for bleeding edge version do\n```bash\nsudo pip install --upgrade git+https://github.com/hycis/TensorGraphX.git@master\n```\nor simply clone and add to `PYTHONPATH`.\n```bash\ngit clone https://github.com/hycis/TensorGraphX.git\nexport PYTHONPATH=/path/to/TensorGraphX:$PYTHONPATH\n```\nin order for the install to persist via export `PYTHONPATH`. Add `PYTHONPATH=/path/to/TensorGraphX:$PYTHONPATH` to your `.bashrc` for linux or\n`.bash_profile` for mac. While this method works, you will have to ensure that\nall the dependencies in [setup.py](setup.py) are installed.\n\n-----\n### Everything in TensorGraphX is about Layers\nEverything in TensorGraphX is about layers. A model such as VGG or Resnet can be a layer. An identity block from Resnet or a dense block from Densenet can be a layer as well. Building models in TensorGraphX is same as building a toy with lego. For example you can create a new model (layer) by subclass the `BaseModel` layer and use `DenseBlock` layer inside your `ModelA` layer.\n\n```python\nfrom tensorgraphx.layers import DenseBlock, BaseModel, Flatten, Linear, Softmax\nimport tensorgraphx as tg\n\nclass ModelA(BaseModel):\n    @BaseModel.init_name_scope\n    def __init__(self):\n        layers = []\n        layers.append(DenseBlock())\n        layers.append(Flatten())\n        layers.append(Linear())\n        layers.append(Softmax())\n        self.startnode = tg.StartNode(input_vars=[None])\n        hn = tg.HiddenNode(prev=[self.startnode], layers=layers)\n        self.endnode = tg.EndNode(prev=[hn])\n```\n\nif someone wants to use your `ModelA` in his `ModelB`, he can easily do this\n```python\nclass ModelB(BaseModel):\n    @BaseModel.init_name_scope\n    def __int__(self):\n        layers = []\n        layers.append(ModelA())\n        layers.append(Linear())\n        layers.append(Softmax())\n        self.startnode = tg.StartNode(input_vars=[None])\n        hn = tg.HiddenNode(prev=[self.startnode], layers=layers)\n        self.endnode = tg.EndNode(prev=[hn])\n```\n\ncreating a layer only created all the `Variables`. To connect the `Variables` into a graph, you can do a `train_fprop(X)` or `test_fprop(X)` to create the tensorflow graph. By abstracting `Variable` creation away from linking the `Variable` nodes into graph prevent the problem of certain tensorflow layers that always reinitialise its weights when it's called, example the [`tf.nn.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization) layer. Also having a separate channel for training and testing is to cater to layers with different training and testing behaviours such as batchnorm and dropout.\n\n```python\nmodelb = ModelB()\nX_ph = tf.placeholder()\ny_train = modelb.train_fprop(X_ph)\ny_test = modelb.test_fprop(X_ph)\n```\n\ncheckout some well known models in TensorGraphX\n1. [VGG16 code](tensorgraphx/layers/backbones.py#L37) and [VGG19 code](tensorgraphx/layers/backbones.py#L125) - [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n2. [DenseNet code](tensorgraphx/layers/backbones.py#L477) - [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)\n3. [ResNet code](tensorgraphx/layers/backbones.py#L225) - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n4. [Unet code](tensorgraphx/layers/backbones.py#L531) - [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n\n-----\n### TensorGraphX on Multiple GPUS\nTo use tensorgraphx on multiple gpus, you can easily integrate it with [horovod](https://github.com/uber/horovod).\n\n```python\nimport horovod.tensorflow as hvd\nfrom tensorflow.python.framework import ops\nimport tensorflow as tf\nhvd.init()\n\n# tensorgraphx model derived previously\nmodelb = ModelB()\nX_ph = tf.placeholder()\ny_ph = tf.placeholder()\ny_train = modelb.train_fprop(X_ph)\ny_test = modelb.test_fprop(X_ph)\n\ntrain_cost = mse(y_train, y_ph)\ntest_cost = mse(y_test, y_ph)\n\nopt = tf.train.RMSPropOptimizer(0.001)\nopt = hvd.DistributedOptimizer(opt)\n\n# required for BatchNormalization layer\nupdate_ops = ops.get_collection(ops.GraphKeys.UPDATE_OPS)\nwith ops.control_dependencies(update_ops):\n    train_op = opt.minimize(train_cost)\n\ninit_op = tf.group(tf.global_variables_initializer(),\n                   tf.local_variables_initializer())\nbcast = hvd.broadcast_global_variables(0)\n\n# Pin GPU to be used to process local rank (one GPU per process)\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list = str(hvd.local_rank())\n\nwith tf.Session(graph=graph, config=config) as sess:\n    sess.run(init_op)\n    bcast.run()\n\n    # training model\n    for epoch in range(100):\n        for X,y in train_data:\n            _, loss_train = sess.run([train_op, train_cost], feed_dict={X_ph:X, y_ph:y})\n```\n\nfor a full example on [tensorgraphx on horovod](./examples/multi_gpus_horovod.py)\n\n-----\n### How TensorGraphX Works?\nIn TensorGraphX, we defined three types of nodes\n\n1. StartNode : for inputs to the graph\n2. HiddenNode : for putting sequential layers inside\n3. EndNode : for getting outputs from the model\n\nWe put all the sequential layers into a `HiddenNode`, and connect the hidden nodes\ntogether to build the architecture that you want. The graph always\nstarts with `StartNode` and ends with `EndNode`. The `StartNode` is where you place\nyour starting point, it can be a `placeholder`, a symbolic output from another graph,\nor data output from `tfrecords`. `EndNode` is where you want to get an output from\nthe graph, where the output can be used to calculate loss or simply just a peek at the\noutputs at that particular layer. Below shows an\n[example](examples/example.py) of building a tensor graph.\n\n-----\n### Graph Example\n\n<img src=\"draw/graph.png\" height=\"250\">\n\nFirst define the `StartNode` for putting the input placeholder\n```python\ny1_dim = 50\ny2_dim = 100\nbatchsize = 32\nlearning_rate = 0.01\n\ny1 = tf.placeholder('float32', [None, y1_dim])\ny2 = tf.placeholder('float32', [None, y2_dim])\ns1 = StartNode(input_vars=[y1])\ns2 = StartNode(input_vars=[y2])\n```\nThen define the `HiddenNode` for putting the sequential layers in each `HiddenNode`\n```python\nh1 = HiddenNode(prev=[s1, s2],\n                input_merge_mode=Concat(),\n                layers=[Linear(y1_dim+y2_dim, y2_dim), RELU()])\nh2 = HiddenNode(prev=[s2],\n                layers=[Linear(y2_dim, y2_dim), RELU()])\nh3 = HiddenNode(prev=[h1, h2],\n                input_merge_mode=Sum(),\n                layers=[Linear(y2_dim, y1_dim), RELU()])\n```\nThen define the `EndNode`. `EndNode` is used to back-trace the graph to connect\nthe nodes together.\n```python\ne1 = EndNode(prev=[h3])\ne2 = EndNode(prev=[h2])\n```\nFinally build the graph by putting `StartNodes` and `EndNodes` into `Graph`\n```python\ngraph = Graph(start=[s1, s2], end=[e1, e2])\n```\nRun train forward propagation to get symbolic output from train mode. The number\nof outputs from `graph.train_fprop` is the same as the number of `EndNodes` put\ninto `Graph`\n```python\no1, o2 = graph.train_fprop()\n```\nFinally build an optimizer to optimize the objective function\n```python\no1_mse = tf.reduce_mean((y1 - o1)**2)\no2_mse = tf.reduce_mean((y2 - o2)**2)\nmse = o1_mse + o2_mse\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse)\n```\n-----\n### Hierachical Softmax Example\nBelow is another example for building a more powerful [hierachical softmax](examples/hierachical_softmax.py)\nwhereby the lower hierachical softmax layer can be conditioned on all the upper\nhierachical softmax layers.\n\n<img src=\"draw/hsoftmax.png\" height=\"250\">\n\n```python\n## params\nx_dim = 50\ncomponent_dim = 100\nbatchsize = 32\nlearning_rate = 0.01\n\n\nx_ph = tf.placeholder('float32', [None, x_dim])\n# the three hierachical level\ny1_ph = tf.placeholder('float32', [None, component_dim])\ny2_ph = tf.placeholder('float32', [None, component_dim])\ny3_ph = tf.placeholder('float32', [None, component_dim])\n\n# define the graph model structure\nstart = StartNode(input_vars=[x_ph])\n\nh1 = HiddenNode(prev=[start], layers=[Linear(x_dim, component_dim), Softmax()])\nh2 = HiddenNode(prev=[h1], layers=[Linear(component_dim, component_dim), Softmax()])\nh3 = HiddenNode(prev=[h2], layers=[Linear(component_dim, component_dim), Softmax()])\n\n\ne1 = EndNode(prev=[h1], input_merge_mode=Sum())\ne2 = EndNode(prev=[h1, h2], input_merge_mode=Sum())\ne3 = EndNode(prev=[h1, h2, h3], input_merge_mode=Sum())\n\ngraph = Graph(start=[start], end=[e1, e2, e3])\n\no1, o2, o3 = graph.train_fprop()\n\no1_mse = tf.reduce_mean((y1_ph - o1)**2)\no2_mse = tf.reduce_mean((y2_ph - o2)**2)\no3_mse = tf.reduce_mean((y3_ph - o3)**2)\nmse = o1_mse + o2_mse + o3_mse\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse)\n```\n\n-----\n### Transfer Learning Example\nBelow is an example on transfer learning with bi-modality inputs and merge at\nthe middle layer with shared representation, in fact, TensorGraphX can be used\nto build any number of modalities for transfer learning.\n\n<img src=\"draw/transferlearn.png\" height=\"250\">\n\n```python\n## params\nx1_dim = 50\nx2_dim = 100\nshared_dim = 200\ny_dim = 100\nbatchsize = 32\nlearning_rate = 0.01\n\n\nx1_ph = tf.placeholder('float32', [None, x1_dim])\nx2_ph = tf.placeholder('float32', [None, x2_dim])\ny_ph = tf.placeholder('float32', [None, y_dim])\n\n# define the graph model structure\ns1 = StartNode(input_vars=[x1_ph])\ns2 = StartNode(input_vars=[x2_ph])\n\nh1 = HiddenNode(prev=[s1], layers=[Linear(x1_dim, shared_dim), RELU()])\nh2 = HiddenNode(prev=[s2], layers=[Linear(x2_dim, shared_dim), RELU()])\nh3 = HiddenNode(prev=[h1,h2], input_merge_mode=Sum(),\n                layers=[Linear(shared_dim, y_dim), Softmax()])\n\ne1 = EndNode(prev=[h3])\n\ngraph = Graph(start=[s1, s2], end=[e1])\no1, = graph.train_fprop()\n\nmse = tf.reduce_mean((y_ph - o1)**2)\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse)\n```\n", "description_content_type": "", "docs_url": null, "download_url": "https://github.com/hycis/TensorGraphX/tarball/6.0.0", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/hycis/TensorGraphX", "keywords": "", "license": "Apache 2.0, see LICENCE", "maintainer": "", "maintainer_email": "", "name": "tensorgraphx", "package_url": "https://pypi.org/project/tensorgraphx/", "platform": "", "project_url": "https://pypi.org/project/tensorgraphx/", "project_urls": {"Download": "https://github.com/hycis/TensorGraphX/tarball/6.0.0", "Homepage": "https://github.com/hycis/TensorGraphX"}, "release_url": "https://pypi.org/project/tensorgraphx/6.0.0/", "requires_dist": null, "requires_python": "", "summary": "A high level tensorflow library for building deep learning models", "version": "6.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            [![Build Status](https://travis-ci.org/hycis/TensorGraphX.svg?branch=master)](https://travis-ci.org/hycis/TensorGraphX)<br><br># TensorGraphX - Simplicity is Beauty<br>TensorGraphX is a simple, lean, and clean framework on TensorFlow for building any imaginable models.<br><br>As deep learning becomes more and more common and the architectures becoming more<br>and more complicated, it seems that we need some easy to use framework to quickly<br>build these models and that's what TensorGraphX is designed for. It's a very simple<br>framework that adds a very thin layer above tensorflow. It is for more advanced<br>users who want to have more control and flexibility over his model building and<br>who wants efficiency at the same time.<br><br>-----<br>### Target Audience<br>TensorGraphX is targeted more at intermediate to advance users who feel keras or<br>other packages is having too much restrictions and too much black box on model<br>building, and someone who don't want to rewrite the standard layers in tensorflow<br>constantly. Also for enterprise users who want to share deep learning models<br>easily between teams.<br><br>-----<br>### Install<br><br>First you need to install [tensorflow](https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html)<br><br>To install tensorgraphx simply do via pip<br>```bash<br>sudo pip install tensorgraphx<br>```<br>or for bleeding edge version do<br>```bash<br>sudo pip install --upgrade git+https://github.com/hycis/TensorGraphX.git@master<br>```<br>or simply clone and add to `PYTHONPATH`.<br>```bash<br>git clone https://github.com/hycis/TensorGraphX.git<br>export PYTHONPATH=/path/to/TensorGraphX:$PYTHONPATH<br>```<br>in order for the install to persist via export `PYTHONPATH`. Add `PYTHONPATH=/path/to/TensorGraphX:$PYTHONPATH` to your `.bashrc` for linux or<br>`.bash_profile` for mac. While this method works, you will have to ensure that<br>all the dependencies in [setup.py](setup.py) are installed.<br><br>-----<br>### Everything in TensorGraphX is about Layers<br>Everything in TensorGraphX is about layers. A model such as VGG or Resnet can be a layer. An identity block from Resnet or a dense block from Densenet can be a layer as well. Building models in TensorGraphX is same as building a toy with lego. For example you can create a new model (layer) by subclass the `BaseModel` layer and use `DenseBlock` layer inside your `ModelA` layer.<br><br>```python<br>from tensorgraphx.layers import DenseBlock, BaseModel, Flatten, Linear, Softmax<br>import tensorgraphx as tg<br><br>class ModelA(BaseModel):<br>    @BaseModel.init_name_scope<br>    def __init__(self):<br>        layers = []<br>        layers.append(DenseBlock())<br>        layers.append(Flatten())<br>        layers.append(Linear())<br>        layers.append(Softmax())<br>        self.startnode = tg.StartNode(input_vars=[None])<br>        hn = tg.HiddenNode(prev=[self.startnode], layers=layers)<br>        self.endnode = tg.EndNode(prev=[hn])<br>```<br><br>if someone wants to use your `ModelA` in his `ModelB`, he can easily do this<br>```python<br>class ModelB(BaseModel):<br>    @BaseModel.init_name_scope<br>    def __int__(self):<br>        layers = []<br>        layers.append(ModelA())<br>        layers.append(Linear())<br>        layers.append(Softmax())<br>        self.startnode = tg.StartNode(input_vars=[None])<br>        hn = tg.HiddenNode(prev=[self.startnode], layers=layers)<br>        self.endnode = tg.EndNode(prev=[hn])<br>```<br><br>creating a layer only created all the `Variables`. To connect the `Variables` into a graph, you can do a `train_fprop(X)` or `test_fprop(X)` to create the tensorflow graph. By abstracting `Variable` creation away from linking the `Variable` nodes into graph prevent the problem of certain tensorflow layers that always reinitialise its weights when it's called, example the [`tf.nn.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization) layer. Also having a separate channel for training and testing is to cater to layers with different training and testing behaviours such as batchnorm and dropout.<br><br>```python<br>modelb = ModelB()<br>X_ph = tf.placeholder()<br>y_train = modelb.train_fprop(X_ph)<br>y_test = modelb.test_fprop(X_ph)<br>```<br><br>checkout some well known models in TensorGraphX<br>1. [VGG16 code](tensorgraphx/layers/backbones.py#L37) and [VGG19 code](tensorgraphx/layers/backbones.py#L125) - [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)<br>2. [DenseNet code](tensorgraphx/layers/backbones.py#L477) - [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)<br>3. [ResNet code](tensorgraphx/layers/backbones.py#L225) - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)<br>4. [Unet code](tensorgraphx/layers/backbones.py#L531) - [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)<br><br>-----<br>### TensorGraphX on Multiple GPUS<br>To use tensorgraphx on multiple gpus, you can easily integrate it with [horovod](https://github.com/uber/horovod).<br><br>```python<br>import horovod.tensorflow as hvd<br>from tensorflow.python.framework import ops<br>import tensorflow as tf<br>hvd.init()<br><br># tensorgraphx model derived previously<br>modelb = ModelB()<br>X_ph = tf.placeholder()<br>y_ph = tf.placeholder()<br>y_train = modelb.train_fprop(X_ph)<br>y_test = modelb.test_fprop(X_ph)<br><br>train_cost = mse(y_train, y_ph)<br>test_cost = mse(y_test, y_ph)<br><br>opt = tf.train.RMSPropOptimizer(0.001)<br>opt = hvd.DistributedOptimizer(opt)<br><br># required for BatchNormalization layer<br>update_ops = ops.get_collection(ops.GraphKeys.UPDATE_OPS)<br>with ops.control_dependencies(update_ops):<br>    train_op = opt.minimize(train_cost)<br><br>init_op = tf.group(tf.global_variables_initializer(),<br>                   tf.local_variables_initializer())<br>bcast = hvd.broadcast_global_variables(0)<br><br># Pin GPU to be used to process local rank (one GPU per process)<br>config = tf.ConfigProto()<br>config.gpu_options.allow_growth = True<br>config.gpu_options.visible_device_list = str(hvd.local_rank())<br><br>with tf.Session(graph=graph, config=config) as sess:<br>    sess.run(init_op)<br>    bcast.run()<br><br>    # training model<br>    for epoch in range(100):<br>        for X,y in train_data:<br>            _, loss_train = sess.run([train_op, train_cost], feed_dict={X_ph:X, y_ph:y})<br>```<br><br>for a full example on [tensorgraphx on horovod](./examples/multi_gpus_horovod.py)<br><br>-----<br>### How TensorGraphX Works?<br>In TensorGraphX, we defined three types of nodes<br><br>1. StartNode : for inputs to the graph<br>2. HiddenNode : for putting sequential layers inside<br>3. EndNode : for getting outputs from the model<br><br>We put all the sequential layers into a `HiddenNode`, and connect the hidden nodes<br>together to build the architecture that you want. The graph always<br>starts with `StartNode` and ends with `EndNode`. The `StartNode` is where you place<br>your starting point, it can be a `placeholder`, a symbolic output from another graph,<br>or data output from `tfrecords`. `EndNode` is where you want to get an output from<br>the graph, where the output can be used to calculate loss or simply just a peek at the<br>outputs at that particular layer. Below shows an<br>[example](examples/example.py) of building a tensor graph.<br><br>-----<br>### Graph Example<br><br>&lt;img src=\"draw/graph.png\" height=\"250\"&gt;<br><br>First define the `StartNode` for putting the input placeholder<br>```python<br>y1_dim = 50<br>y2_dim = 100<br>batchsize = 32<br>learning_rate = 0.01<br><br>y1 = tf.placeholder('float32', [None, y1_dim])<br>y2 = tf.placeholder('float32', [None, y2_dim])<br>s1 = StartNode(input_vars=[y1])<br>s2 = StartNode(input_vars=[y2])<br>```<br>Then define the `HiddenNode` for putting the sequential layers in each `HiddenNode`<br>```python<br>h1 = HiddenNode(prev=[s1, s2],<br>                input_merge_mode=Concat(),<br>                layers=[Linear(y1_dim+y2_dim, y2_dim), RELU()])<br>h2 = HiddenNode(prev=[s2],<br>                layers=[Linear(y2_dim, y2_dim), RELU()])<br>h3 = HiddenNode(prev=[h1, h2],<br>                input_merge_mode=Sum(),<br>                layers=[Linear(y2_dim, y1_dim), RELU()])<br>```<br>Then define the `EndNode`. `EndNode` is used to back-trace the graph to connect<br>the nodes together.<br>```python<br>e1 = EndNode(prev=[h3])<br>e2 = EndNode(prev=[h2])<br>```<br>Finally build the graph by putting `StartNodes` and `EndNodes` into `Graph`<br>```python<br>graph = Graph(start=[s1, s2], end=[e1, e2])<br>```<br>Run train forward propagation to get symbolic output from train mode. The number<br>of outputs from `graph.train_fprop` is the same as the number of `EndNodes` put<br>into `Graph`<br>```python<br>o1, o2 = graph.train_fprop()<br>```<br>Finally build an optimizer to optimize the objective function<br>```python<br>o1_mse = tf.reduce_mean((y1 - o1)**2)<br>o2_mse = tf.reduce_mean((y2 - o2)**2)<br>mse = o1_mse + o2_mse<br>optimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse)<br>```<br>-----<br>### Hierachical Softmax Example<br>Below is another example for building a more powerful [hierachical softmax](examples/hierachical_softmax.py)<br>whereby the lower hierachical softmax layer can be conditioned on all the upper<br>hierachical softmax layers.<br><br>&lt;img src=\"draw/hsoftmax.png\" height=\"250\"&gt;<br><br>```python<br>## params<br>x_dim = 50<br>component_dim = 100<br>batchsize = 32<br>learning_rate = 0.01<br><br><br>x_ph = tf.placeholder('float32', [None, x_dim])<br># the three hierachical level<br>y1_ph = tf.placeholder('float32', [None, component_dim])<br>y2_ph = tf.placeholder('float32', [None, component_dim])<br>y3_ph = tf.placeholder('float32', [None, component_dim])<br><br># define the graph model structure<br>start = StartNode(input_vars=[x_ph])<br><br>h1 = HiddenNode(prev=[start], layers=[Linear(x_dim, component_dim), Softmax()])<br>h2 = HiddenNode(prev=[h1], layers=[Linear(component_dim, component_dim), Softmax()])<br>h3 = HiddenNode(prev=[h2], layers=[Linear(component_dim, component_dim), Softmax()])<br><br><br>e1 = EndNode(prev=[h1], input_merge_mode=Sum())<br>e2 = EndNode(prev=[h1, h2], input_merge_mode=Sum())<br>e3 = EndNode(prev=[h1, h2, h3], input_merge_mode=Sum())<br><br>graph = Graph(start=[start], end=[e1, e2, e3])<br><br>o1, o2, o3 = graph.train_fprop()<br><br>o1_mse = tf.reduce_mean((y1_ph - o1)**2)<br>o2_mse = tf.reduce_mean((y2_ph - o2)**2)<br>o3_mse = tf.reduce_mean((y3_ph - o3)**2)<br>mse = o1_mse + o2_mse + o3_mse<br>optimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse)<br>```<br><br>-----<br>### Transfer Learning Example<br>Below is an example on transfer learning with bi-modality inputs and merge at<br>the middle layer with shared representation, in fact, TensorGraphX can be used<br>to build any number of modalities for transfer learning.<br><br>&lt;img src=\"draw/transferlearn.png\" height=\"250\"&gt;<br><br>```python<br>## params<br>x1_dim = 50<br>x2_dim = 100<br>shared_dim = 200<br>y_dim = 100<br>batchsize = 32<br>learning_rate = 0.01<br><br><br>x1_ph = tf.placeholder('float32', [None, x1_dim])<br>x2_ph = tf.placeholder('float32', [None, x2_dim])<br>y_ph = tf.placeholder('float32', [None, y_dim])<br><br># define the graph model structure<br>s1 = StartNode(input_vars=[x1_ph])<br>s2 = StartNode(input_vars=[x2_ph])<br><br>h1 = HiddenNode(prev=[s1], layers=[Linear(x1_dim, shared_dim), RELU()])<br>h2 = HiddenNode(prev=[s2], layers=[Linear(x2_dim, shared_dim), RELU()])<br>h3 = HiddenNode(prev=[h1,h2], input_merge_mode=Sum(),<br>                layers=[Linear(shared_dim, y_dim), Softmax()])<br><br>e1 = EndNode(prev=[h3])<br><br>graph = Graph(start=[s1, s2], end=[e1])<br>o1, = graph.train_fprop()<br><br>mse = tf.reduce_mean((y_ph - o1)**2)<br>optimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse)<br>```<br>\n          </div>"}, "last_serial": 3780521, "releases": {"6.0.0": [{"comment_text": "", "digests": {"md5": "43d343843380e9063eeba5100a257119", "sha256": "f19a6b728238c8bcc0227e8e9c7fc0bf67ec3387057ef5be9476aa6e77eefeac"}, "downloads": -1, "filename": "tensorgraphx-6.0.0.tar.gz", "has_sig": false, "md5_digest": "43d343843380e9063eeba5100a257119", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 43460, "upload_time": "2018-04-19T12:53:06", "upload_time_iso_8601": "2018-04-19T12:53:06.050869Z", "url": "https://files.pythonhosted.org/packages/13/53/a6b6156aa066937325e0edf2f93da417de0f73079e96a0fc1ebf94c7c596/tensorgraphx-6.0.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "43d343843380e9063eeba5100a257119", "sha256": "f19a6b728238c8bcc0227e8e9c7fc0bf67ec3387057ef5be9476aa6e77eefeac"}, "downloads": -1, "filename": "tensorgraphx-6.0.0.tar.gz", "has_sig": false, "md5_digest": "43d343843380e9063eeba5100a257119", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 43460, "upload_time": "2018-04-19T12:53:06", "upload_time_iso_8601": "2018-04-19T12:53:06.050869Z", "url": "https://files.pythonhosted.org/packages/13/53/a6b6156aa066937325e0edf2f93da417de0f73079e96a0fc1ebf94c7c596/tensorgraphx-6.0.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:05 2020"}