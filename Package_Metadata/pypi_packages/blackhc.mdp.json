{"info": {"author": "Andreas Kirsch", "author_email": "blackhc+mdp@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# MDP environments for the OpenAI Gym\n\nThis Python framework makes it very easy to specify simple MDPs.\n\n[![Build Status](https://travis-ci.org/BlackHC/mdp.svg?branch=master)](https://travis-ci.org/BlackHC/mdp)\n\n## Installation\n\nTo install using pip, use:\n\n```\npip install blackhc.mdp\n```\n\nTo run the tests, use:\n\n```\npython setup.py test\n```\n\n## Whitepaper\n\nA whitepaper is available at [docs/whitepaper.pdf](docs/whitepaper.pdf). Here is a BibTeX entry that you can use in publications (or download [CITE_ME.bib](CITE_ME.bib)):\n```\n@techreport{blackhc.mdp,\n    Author = {Andreas Kirsch},\n    Title = {MDP environments for the OpenAI Gym},\n    Year = {2017},\n    Url = {http://github.com/BlackHC/mdp/raw/master/docs/whitepaper.pdf}\n}\n```\n\n\n## Introduction\n\nIn reinforcement learning, agents learn to maximize accumulated rewards from an environment that they can interact with by observing and taking actions. Usually, these environments satisfy a Markov property and are treated as *Markov Decision Processes* (*MDPs*).\n\nThe OpenAI Gym is a standardized and open framework that provides many different environments to train agents against through a simple API.\n\nEven the simplest of these environments already has a level of complexity that is interesting for research but can make it hard to track down bugs. However, the gym provides four very simple environments that are useful for testing. The `gym.envs.debugging` package contains a one-round environment with deterministic rewards and one with non-deterministic rewards, and a two-round environment with deterministic rewards and another one with non-deterministic rewards.\nThe author has found these environments very useful for smoke-testing code changes.\n\nThis Python framework makes it very easy to specify simple MDPs like the ones described above in an extensible way. With it, one can validate that agents converge correctly as well as examine other properties.\n\n## Specification of MDPs\n\nMDPs are Markov processes that are augmented with a reward function and discount factor. An MDP can be fully specified by a tuple of:\n\n* a finite set of states,\n* a finite set of actions,\n* a matrix that specifies probabilities of transitions to a new state for a given a state and action,\n* a reward function that specifies the reward for a given action taken in a certain state, and\n* a discount rate.\n\nThe reward function can be either deterministic, or it can be a probability distribution.\n\nWithin this framework, MDPs can be specified in Python using a simple *domain-specific language* (*DSL*).\nFor example, the one-round deterministic environment defined in `gym.envs.debugging.one_round_deterministic_reward` could be specified as follows:\n\n```python\nfrom blackhc.mdp import dsl\n\nstart = dsl.state()\nend = dsl.terminal_state()\n\naction_0 = dsl.action()\naction_1 = dsl.action()\n\nstart & (action_0 | action_1) > end\nstart & action_1 > dsl.reward(1.)\n```\n\nThe DSL is based on the following grammar (using EBNF[@ebnf]): \n\n    TRANSITION ::= STATE '&' ACTION '>' OUTCOME\n    OUTCOME ::= (REWARD | STATE) ['*' WEIGHT]\n    ALTERNATIVES ::= ALTERNATIVE ('|' ALTERNATIVE)* \n\nFor a given state and action, outcomes can be specified. Outcomes are state transitions or rewards.\nIf multiple state transitions or rewards are specified for the same state and action, the MDP is non-deterministic and the state transition (or reward) are determined using a categorical distribution. By default, each outcome is weighted uniformly, except if specified otherwise by either having duplicate transitions or by using an explicit weight factor. \n\nFor example, to specify that a state receives a reward of +1 or -1 with equal probability and does not change states with probability 3/4 and only transitions to the next state with probability 1/4, we could write:\n\n```python\nstate & action > dsl.reward(-1.) | dsl.reward(1.)\nstate & action > state * 3 | next_state\n```\n\nAlternatives are distributive with respect to both conjunctions (`&`) and outcome mappings (`>`), so:\n\n    (a | b) & (c | d) > (e | f) ===\n    (a & c > e) | (a & c > f) | (a & d > e) | \n    (a & d > f) | (b & c > e) | ... \n\nAlternatives can consist of states, actions, outcomes, conjunctions or partial transitions. For example, the following are valid alternatives:\n\n    stateA & actionA | stateB & actionB\n    (actionA > stateC) | (actionB > stateD)\n\nAs the DSL is implemented within Python, operator overloading is used to implement the semantics. Operator precedence is favorable as `*` has higher precedence than `&`, which has higher precedence than `|`, which has higher precedence than `>`. This allows for a natural formulation of transitions.\n\n## Conventional API\n\nThe framework also supports specifying an MDP using a conventional API as DSLs are not always preferred.\n\n```python\nfrom blackhc import mdp\n\nspec = mdp.MDPSpec()\nstart = spec.state('start')\nend = spec.state('end', terminal_state=True)\naction_0 = spec.action()\naction_1 = spec.action()\n\nspec.transition(start, action_0, mdp.NextState(end))\nspec.transition(start, action_1, mdp.NextState(end))\nspec.transition(start, action_1, mdp.Reward(1))\n```\n\n## Visualization\n\nTo make debugging easier, MDPs can be converted to `networkx` graphs and rendered using `pydotplus` and `GraphViz`.\n\n```python\nfrom blackhc import mdp\nfrom blackhc.mdp import example\n\nspec = example.ONE_ROUND_DMDP\n\nspec_graph = spec.to_graph()\nspec_png = mdp.graph_to_png(spec_graph)\n\nmdp.display_mdp(spec)\n```\n\n<div>\n<img src=\"docs/one_round_dmdp.png\" alt=\"One round deterministic MDP\" width=\"96\" />\n</div>\n<b>Figure 1: One round deterministic MDP</b>\n\n## Optimal values\n\nThe framework also contains a small module that can compute the optimal value functions using linear programming.\n\n```python\nfrom blackhc.mdp import lp\nfrom blackhc.mdp import example\n\nsolver = lp.LinearProgramming(example.ONE_ROUND_DMDP)\nprint(solver.compute_q_table())\nprint(solver.compute_v_vector())\n```\n\n## Gym environment\n\nAn environment that is compatible with the OpenAI Gym can be created easily by using the `to_env()` method. It supports rendering into Jupyter notebooks, as RGB array for storing videos, and as png byte data.\n\n```python\nfrom blackhc import mdp\nfrom blackhc.mdp import example\n\nenv = example.MULTI_ROUND_NMDP.to_env()\n\nenv.reset()\nenv.render()\n\nis_done = False\nwhile not is_done:\n    state, reward, is_done, _ = env.step(env.action_space.sample())\n    env.render()\n```\n\n<div>\n<img src=\"docs/multi_round_nmdp_render.png\" alt=\"env.render() of `example.MULTI_ROUND_NMDP`\" width=\"192\" />\n</div>\n<b>Figure 2: env.render() of `example.MULTI_ROUND_NMDP`</b>\n\n# Examples\n\nThe `blackhc.mdp.example` package provides 5 MDPs. Four of them match the ones in `gym.envs.debugging`, and the fifth one is depicted in figure 2. \n\n\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/blackhc/mdp", "keywords": "mdp rl", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "blackhc.mdp", "package_url": "https://pypi.org/project/blackhc.mdp/", "platform": "", "project_url": "https://pypi.org/project/blackhc.mdp/", "project_urls": {"Homepage": "https://github.com/blackhc/mdp"}, "release_url": "https://pypi.org/project/blackhc.mdp/1.0.6/", "requires_dist": ["gym (>=0.9.2)", "ipython (>=6.1.0)", "ipywidgets", "matplotlib", "networkx (<2.0.0,>=1.11.0)", "numpy", "pydotplus", "typing", "check-manifest; extra == 'dev'", "coverage; extra == 'test'", "pytest; extra == 'test'"], "requires_python": "", "summary": "MDP framework for the OpenAI Gym", "version": "1.0.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            # MDP environments for the OpenAI Gym<br><br>This Python framework makes it very easy to specify simple MDPs.<br><br>[![Build Status](https://travis-ci.org/BlackHC/mdp.svg?branch=master)](https://travis-ci.org/BlackHC/mdp)<br><br>## Installation<br><br>To install using pip, use:<br><br>```<br>pip install blackhc.mdp<br>```<br><br>To run the tests, use:<br><br>```<br>python setup.py test<br>```<br><br>## Whitepaper<br><br>A whitepaper is available at [docs/whitepaper.pdf](docs/whitepaper.pdf). Here is a BibTeX entry that you can use in publications (or download [CITE_ME.bib](CITE_ME.bib)):<br>```<br>@techreport{blackhc.mdp,<br>    Author = {Andreas Kirsch},<br>    Title = {MDP environments for the OpenAI Gym},<br>    Year = {2017},<br>    Url = {http://github.com/BlackHC/mdp/raw/master/docs/whitepaper.pdf}<br>}<br>```<br><br><br>## Introduction<br><br>In reinforcement learning, agents learn to maximize accumulated rewards from an environment that they can interact with by observing and taking actions. Usually, these environments satisfy a Markov property and are treated as *Markov Decision Processes* (*MDPs*).<br><br>The OpenAI Gym is a standardized and open framework that provides many different environments to train agents against through a simple API.<br><br>Even the simplest of these environments already has a level of complexity that is interesting for research but can make it hard to track down bugs. However, the gym provides four very simple environments that are useful for testing. The `gym.envs.debugging` package contains a one-round environment with deterministic rewards and one with non-deterministic rewards, and a two-round environment with deterministic rewards and another one with non-deterministic rewards.<br>The author has found these environments very useful for smoke-testing code changes.<br><br>This Python framework makes it very easy to specify simple MDPs like the ones described above in an extensible way. With it, one can validate that agents converge correctly as well as examine other properties.<br><br>## Specification of MDPs<br><br>MDPs are Markov processes that are augmented with a reward function and discount factor. An MDP can be fully specified by a tuple of:<br><br>* a finite set of states,<br>* a finite set of actions,<br>* a matrix that specifies probabilities of transitions to a new state for a given a state and action,<br>* a reward function that specifies the reward for a given action taken in a certain state, and<br>* a discount rate.<br><br>The reward function can be either deterministic, or it can be a probability distribution.<br><br>Within this framework, MDPs can be specified in Python using a simple *domain-specific language* (*DSL*).<br>For example, the one-round deterministic environment defined in `gym.envs.debugging.one_round_deterministic_reward` could be specified as follows:<br><br>```python<br>from blackhc.mdp import dsl<br><br>start = dsl.state()<br>end = dsl.terminal_state()<br><br>action_0 = dsl.action()<br>action_1 = dsl.action()<br><br>start &amp; (action_0 | action_1) &gt; end<br>start &amp; action_1 &gt; dsl.reward(1.)<br>```<br><br>The DSL is based on the following grammar (using EBNF[@ebnf]): <br><br>    TRANSITION ::= STATE '&amp;' ACTION '&gt;' OUTCOME<br>    OUTCOME ::= (REWARD | STATE) ['*' WEIGHT]<br>    ALTERNATIVES ::= ALTERNATIVE ('|' ALTERNATIVE)* <br><br>For a given state and action, outcomes can be specified. Outcomes are state transitions or rewards.<br>If multiple state transitions or rewards are specified for the same state and action, the MDP is non-deterministic and the state transition (or reward) are determined using a categorical distribution. By default, each outcome is weighted uniformly, except if specified otherwise by either having duplicate transitions or by using an explicit weight factor. <br><br>For example, to specify that a state receives a reward of +1 or -1 with equal probability and does not change states with probability 3/4 and only transitions to the next state with probability 1/4, we could write:<br><br>```python<br>state &amp; action &gt; dsl.reward(-1.) | dsl.reward(1.)<br>state &amp; action &gt; state * 3 | next_state<br>```<br><br>Alternatives are distributive with respect to both conjunctions (`&amp;`) and outcome mappings (`&gt;`), so:<br><br>    (a | b) &amp; (c | d) &gt; (e | f) ===<br>    (a &amp; c &gt; e) | (a &amp; c &gt; f) | (a &amp; d &gt; e) | <br>    (a &amp; d &gt; f) | (b &amp; c &gt; e) | ... <br><br>Alternatives can consist of states, actions, outcomes, conjunctions or partial transitions. For example, the following are valid alternatives:<br><br>    stateA &amp; actionA | stateB &amp; actionB<br>    (actionA &gt; stateC) | (actionB &gt; stateD)<br><br>As the DSL is implemented within Python, operator overloading is used to implement the semantics. Operator precedence is favorable as `*` has higher precedence than `&amp;`, which has higher precedence than `|`, which has higher precedence than `&gt;`. This allows for a natural formulation of transitions.<br><br>## Conventional API<br><br>The framework also supports specifying an MDP using a conventional API as DSLs are not always preferred.<br><br>```python<br>from blackhc import mdp<br><br>spec = mdp.MDPSpec()<br>start = spec.state('start')<br>end = spec.state('end', terminal_state=True)<br>action_0 = spec.action()<br>action_1 = spec.action()<br><br>spec.transition(start, action_0, mdp.NextState(end))<br>spec.transition(start, action_1, mdp.NextState(end))<br>spec.transition(start, action_1, mdp.Reward(1))<br>```<br><br>## Visualization<br><br>To make debugging easier, MDPs can be converted to `networkx` graphs and rendered using `pydotplus` and `GraphViz`.<br><br>```python<br>from blackhc import mdp<br>from blackhc.mdp import example<br><br>spec = example.ONE_ROUND_DMDP<br><br>spec_graph = spec.to_graph()<br>spec_png = mdp.graph_to_png(spec_graph)<br><br>mdp.display_mdp(spec)<br>```<br><br>&lt;div&gt;<br>&lt;img src=\"docs/one_round_dmdp.png\" alt=\"One round deterministic MDP\" width=\"96\" /&gt;<br>&lt;/div&gt;<br>&lt;b&gt;Figure 1: One round deterministic MDP&lt;/b&gt;<br><br>## Optimal values<br><br>The framework also contains a small module that can compute the optimal value functions using linear programming.<br><br>```python<br>from blackhc.mdp import lp<br>from blackhc.mdp import example<br><br>solver = lp.LinearProgramming(example.ONE_ROUND_DMDP)<br>print(solver.compute_q_table())<br>print(solver.compute_v_vector())<br>```<br><br>## Gym environment<br><br>An environment that is compatible with the OpenAI Gym can be created easily by using the `to_env()` method. It supports rendering into Jupyter notebooks, as RGB array for storing videos, and as png byte data.<br><br>```python<br>from blackhc import mdp<br>from blackhc.mdp import example<br><br>env = example.MULTI_ROUND_NMDP.to_env()<br><br>env.reset()<br>env.render()<br><br>is_done = False<br>while not is_done:<br>    state, reward, is_done, _ = env.step(env.action_space.sample())<br>    env.render()<br>```<br><br>&lt;div&gt;<br>&lt;img src=\"docs/multi_round_nmdp_render.png\" alt=\"env.render() of `example.MULTI_ROUND_NMDP`\" width=\"192\" /&gt;<br>&lt;/div&gt;<br>&lt;b&gt;Figure 2: env.render() of `example.MULTI_ROUND_NMDP`&lt;/b&gt;<br><br># Examples<br><br>The `blackhc.mdp.example` package provides 5 MDPs. Four of them match the ones in `gym.envs.debugging`, and the fifth one is depicted in figure 2. <br><br><br>\n          </div>"}, "last_serial": 3204431, "releases": {"1.0.2": [{"comment_text": "", "digests": {"md5": "ed5e75b90be68811754aecf4390bc5db", "sha256": "85747cb5ff02db9abeb1479e52aebf8ca8afd73889160631a02eaaabdfe5e69a"}, "downloads": -1, "filename": "blackhc.mdp-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "ed5e75b90be68811754aecf4390bc5db", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 2151, "upload_time": "2017-09-18T19:33:54", "upload_time_iso_8601": "2017-09-18T19:33:54.483256Z", "url": "https://files.pythonhosted.org/packages/69/25/32367b94cde58eb90f0e996f63f254f14bf71f8ec4860235589c9c50b575/blackhc.mdp-1.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7126ecadddc15461d32c141aa3e7ace8", "sha256": "ecae54b278183c99dddf14771927a26de64a08039add51598395d36077165c6e"}, "downloads": -1, "filename": "blackhc.mdp-1.0.2.tar.gz", "has_sig": false, "md5_digest": "7126ecadddc15461d32c141aa3e7ace8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5659, "upload_time": "2017-09-18T19:33:56", "upload_time_iso_8601": "2017-09-18T19:33:56.419075Z", "url": "https://files.pythonhosted.org/packages/d0/b9/359ec04cb37dffadb3ea871b0f21607f04010d96cedf8c8fa8949ebbeaf0/blackhc.mdp-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "51acba3408d7e1bfbc7584bb231ff5fd", "sha256": "2316aa698e13ea9781b578632cb5949bb096a3385ddc6a19b91a65e9979171c2"}, "downloads": -1, "filename": "blackhc.mdp-1.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "51acba3408d7e1bfbc7584bb231ff5fd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 12409, "upload_time": "2017-09-18T20:12:30", "upload_time_iso_8601": "2017-09-18T20:12:30.525560Z", "url": "https://files.pythonhosted.org/packages/62/6e/922a415176dfe3b496c041b165e920fb8a784e47d7cacb358b8cc52910bf/blackhc.mdp-1.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b2bb96b6b63c352419bbf94a2125ba9e", "sha256": "338ca9d7e5001061046da31c5f40305696c5387b485915fb3d27ca8d760421ee"}, "downloads": -1, "filename": "blackhc.mdp-1.0.3.tar.gz", "has_sig": false, "md5_digest": "b2bb96b6b63c352419bbf94a2125ba9e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12308, "upload_time": "2017-09-18T20:12:31", "upload_time_iso_8601": "2017-09-18T20:12:31.840703Z", "url": "https://files.pythonhosted.org/packages/21/48/7a22d63d8ef2432092749361f2741b9edf3fd035b1435c3c7cec14b63c8f/blackhc.mdp-1.0.3.tar.gz", "yanked": false}], "1.0.4": [{"comment_text": "", "digests": {"md5": "be774d7ed50a5c801289c815d6a8bc47", "sha256": "1633eabae9e8b92240685c650c6f576b6d58e66cb50380e11fa14207964f85b8"}, "downloads": -1, "filename": "blackhc.mdp-1.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "be774d7ed50a5c801289c815d6a8bc47", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 12408, "upload_time": "2017-09-18T20:32:13", "upload_time_iso_8601": "2017-09-18T20:32:13.377010Z", "url": "https://files.pythonhosted.org/packages/b7/ed/3429963f228f1786f61648eed086b764e74c10d3f7e2951b230e47b33ee4/blackhc.mdp-1.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "60702550048d89f41cf69fb10e6e6ee2", "sha256": "8cf00181ac6088d1eb0220230b30e269af0a235b5e10490e07c694c6f7a456a2"}, "downloads": -1, "filename": "blackhc.mdp-1.0.4.tar.gz", "has_sig": false, "md5_digest": "60702550048d89f41cf69fb10e6e6ee2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12312, "upload_time": "2017-09-18T20:32:15", "upload_time_iso_8601": "2017-09-18T20:32:15.162224Z", "url": "https://files.pythonhosted.org/packages/2d/9b/b99f5cf3e9fe667a2e87a5cc9fa1de947d682fcbd662bb5cb0c36749a69a/blackhc.mdp-1.0.4.tar.gz", "yanked": false}], "1.0.5": [{"comment_text": "", "digests": {"md5": "c723d6a46b13d5de64f46eaa271219a5", "sha256": "d068d350cf98ddb7e7af458857567e084212fdb75d7e8f3d04022983cd365ee2"}, "downloads": -1, "filename": "blackhc.mdp-1.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "c723d6a46b13d5de64f46eaa271219a5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18175, "upload_time": "2017-09-26T15:50:01", "upload_time_iso_8601": "2017-09-26T15:50:01.164337Z", "url": "https://files.pythonhosted.org/packages/c5/af/477eb6fcba1d1e0b9a067b14c06561baaacf6f8ef3d71764344a7d553236/blackhc.mdp-1.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "eb32f53cd9abf8ee9e8da1992f174c00", "sha256": "86e1d20cb07784d5ce119d21507ddd350d84d28e28340a40a4b5380e982044fe"}, "downloads": -1, "filename": "blackhc.mdp-1.0.5.tar.gz", "has_sig": false, "md5_digest": "eb32f53cd9abf8ee9e8da1992f174c00", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12438, "upload_time": "2017-09-26T15:50:05", "upload_time_iso_8601": "2017-09-26T15:50:05.049050Z", "url": "https://files.pythonhosted.org/packages/a7/c0/a3de42e9f9d2a9136b185e5f06f03215097fde1f8231b85dc4a5a8ea9380/blackhc.mdp-1.0.5.tar.gz", "yanked": false}], "1.0.6": [{"comment_text": "", "digests": {"md5": "b39e887b9898af7ee1e477584767031a", "sha256": "a164b3d9cd32652dcf1dbaaaed21eb387ceb35c3e0ebe505506e19ccb68ff580"}, "downloads": -1, "filename": "blackhc.mdp-1.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "b39e887b9898af7ee1e477584767031a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18179, "upload_time": "2017-09-26T15:50:02", "upload_time_iso_8601": "2017-09-26T15:50:02.405676Z", "url": "https://files.pythonhosted.org/packages/5f/d8/a6c4e3c1ac85662a0250f681b85f6d7f0efd7a36327396865059e325ba70/blackhc.mdp-1.0.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3d33b203b0a3ba5685a0e15f3cbcaead", "sha256": "228ba342f5766f2aa250f0ec752a046252c91d24018ee80e3c9c08b8b20bd732"}, "downloads": -1, "filename": "blackhc.mdp-1.0.6.tar.gz", "has_sig": false, "md5_digest": "3d33b203b0a3ba5685a0e15f3cbcaead", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12431, "upload_time": "2017-09-26T15:50:06", "upload_time_iso_8601": "2017-09-26T15:50:06.849274Z", "url": "https://files.pythonhosted.org/packages/69/aa/c7334285dfdb6b2f8320d2a1c9a836fd1b41099567fdea25fccc63ac6e89/blackhc.mdp-1.0.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b39e887b9898af7ee1e477584767031a", "sha256": "a164b3d9cd32652dcf1dbaaaed21eb387ceb35c3e0ebe505506e19ccb68ff580"}, "downloads": -1, "filename": "blackhc.mdp-1.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "b39e887b9898af7ee1e477584767031a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18179, "upload_time": "2017-09-26T15:50:02", "upload_time_iso_8601": "2017-09-26T15:50:02.405676Z", "url": "https://files.pythonhosted.org/packages/5f/d8/a6c4e3c1ac85662a0250f681b85f6d7f0efd7a36327396865059e325ba70/blackhc.mdp-1.0.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3d33b203b0a3ba5685a0e15f3cbcaead", "sha256": "228ba342f5766f2aa250f0ec752a046252c91d24018ee80e3c9c08b8b20bd732"}, "downloads": -1, "filename": "blackhc.mdp-1.0.6.tar.gz", "has_sig": false, "md5_digest": "3d33b203b0a3ba5685a0e15f3cbcaead", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12431, "upload_time": "2017-09-26T15:50:06", "upload_time_iso_8601": "2017-09-26T15:50:06.849274Z", "url": "https://files.pythonhosted.org/packages/69/aa/c7334285dfdb6b2f8320d2a1c9a836fd1b41099567fdea25fccc63ac6e89/blackhc.mdp-1.0.6.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:37:10 2020"}