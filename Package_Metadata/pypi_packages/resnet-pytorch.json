{"info": {"author": "Liu Changyu", "author_email": "liuchangyu1111@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: Apache Software License", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "\n# ResNet-PyTorch\n\n### Update (Feb 20, 2020)\n\nThe update is for ease of use and deployment.\n\n * [Example: Export to ONNX](#example-export-to-onnx)\n * [Example: Extract features](#example-feature-extraction)\n * [Example: Visual](#example-visual)\n\nIt is also now incredibly simple to load a pretrained model with a new number of classes for transfer learning:\n\n```python\nfrom resnet_pytorch import ResNet \nmodel = ResNet.from_pretrained('resnet18', num_classes=10)\n```\n\n### Update (February 2, 2020)\n\nThis update allows you to use NVIDIA's Apex tool for accelerated training. By default choice `hybrid training precision` + `dynamic loss amplified` version, if you need to learn more and details about `apex` tools, please visit https://github.com/NVIDIA/apex.\n\n### Overview\nThis repository contains an op-for-op PyTorch reimplementation of [Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385).\n\nThe goal of this implementation is to be simple, highly extensible, and easy to integrate into your own projects. This implementation is a work in progress -- new features are currently being implemented.  \n\nAt the moment, you can easily:  \n * Load pretrained ResNet models \n * Use ResNet models for classification or feature extraction \n\n_Upcoming features_: In the next few days, you will be able to:\n * Quickly finetune an ResNet on your own dataset\n * Export ResNet models for production\n\n### Table of contents\n1. [About ResNet](#about-resnet)\n2. [Installation](#installation)\n3. [Usage](#usage)\n    * [Load pretrained models](#loading-pretrained-models)\n    * [Example: Classify](#example-classification)\n    * [Example: Extract features](#example-feature-extraction)\n    * [Example: Export to ONNX](#example-export-to-onnx)\n    * [Example: Visual](#example-visual)\n4. [Contributing](#contributing) \n\n### About ResNet\n\nIf you're new to ResNets, here is an explanation straight from the official PyTorch implementation: \n\nResnet models were proposed in \"Deep Residual Learning for Image Recognition\". Here we have the 5 versions of resnet models, \nwhich contains 5, 34, 50, 101, 152 layers respectively. Detailed model architectures can be found in Table 1. \n\n### Installation\n\nInstall from pypi:\n```bash\n$ pip3 install resnet_pytorch\n```\n\nInstall from source:\n```bash\n$ git clone https://github.com/Lornatang/ResNet-PyTorch.git\n$ cd ResNet-PyTorch\n$ pip3 install -e .\n``` \n\n### Usage\n\n#### Loading pretrained models\n\nLoad an resnet18 network:\n```python\nfrom resnet_pytorch import ResNet\nmodel = ResNet.from_name(\"resnet18\")\n```\n\nLoad a pretrained resnet18: \n```python\nfrom resnet_pytorch import ResNet\nmodel = ResNet.from_pretrained(\"resnet18\")\n```\n\nTheir 1-crop error rates on imagenet dataset with pretrained models are listed below.\n\n| Model structure | Top-1 error | Top-5 error |\n| --------------- | ----------- | ----------- |\n|  resnet18       | 30.24       | 10.92       |\n|  resnet34       | 26.70       | 8.58        |\n|  resnet50       | 23.85       | 7.13        |\n|  resnet101      | 22.63       | 6.44        |\n|  resnet152      | 21.69       | 5.94        |\n\n*Option B of resnet-18/34/50/101/152 only uses projections to increase dimensions.*\n\nFor results extending to the cifar10 dataset, see `examples/cifar`\n\n#### Example: Classification\n\nWe assume that in your current directory, there is a `img.jpg` file and a `labels_map.txt` file (ImageNet class names). These are both included in `examples/simple`. \n\nAll pre-trained models expect input images normalized in the same way,\ni.e. mini-batches of 3-channel RGB images of shape `(3 x H x W)`, where `H` and `W` are expected to be at least `224`.\nThe images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\nand `std = [0.229, 0.224, 0.225]`.\n\nHere's a sample execution.\n\n```python\nimport json\n\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nfrom resnet_pytorch import ResNet \n\n# Open image\ninput_image = Image.open(\"img.jpg\")\n\n# Preprocess image\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n\n# Load class names\nlabels_map = json.load(open(\"labels_map.txt\"))\nlabels_map = [labels_map[str(i)] for i in range(1000)]\n\n# Classify with ResNet18\nmodel = ResNet.from_pretrained(\"resnet18\")\nmodel.eval()\n\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n    input_batch = input_batch.to(\"cuda\")\n    model.to(\"cuda\")\n\nwith torch.no_grad():\n    logits = model(input_batch)\npreds = torch.topk(logits, k=5).indices.squeeze(0).tolist()\n\nprint(\"-----\")\nfor idx in preds:\n    label = labels_map[idx]\n    prob = torch.softmax(logits, dim=1)[0, idx].item()\n    print(f\"{label:<75} ({prob * 100:.2f}%)\")\n```\n\n#### Example: Feature Extraction \n\nYou can easily extract features with `model.extract_features`:\n```python\nimport torch\nfrom resnet_pytorch import ResNet \nmodel = ResNet.from_pretrained('resnet18')\n\n# ... image preprocessing as in the classification example ...\ninputs = torch.randn(1, 3, 224, 224)\nprint(inputs.shape) # torch.Size([1, 3, 224, 224])\n\nfeatures = model.extract_features(inputs)\nprint(features.shape) # torch.Size([1, 512, 1, 1])\n```\n\n#### Example: Export to ONNX  \n\nExporting to ONNX for deploying to production is now simple: \n```python\nimport torch \nfrom resnet_pytorch import ResNet \n\nmodel = ResNet.from_pretrained('resnet18')\ndummy_input = torch.randn(16, 3, 224, 224)\n\ntorch.onnx.export(model, dummy_input, \"demo.onnx\", verbose=True)\n```\n\n#### Example: Visual\n\n```text\ncd $REPO$/framework\nsh start.sh\n```\n\nThen open the browser and type in the browser address [http://127.0.0.1:10004/](http://127.0.0.1:10004/).\n\nEnjoy it.\n\n#### ImageNet\n\nSee `examples/imagenet` for details about evaluating on ImageNet.\n\n### Credit\n\n#### Deep Residual Learning for Image Recognition\n\n*Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun*\n\n##### Abstract\n\nDeeper neural networks are more difficult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. On the ImageNet dataset we\nevaluate residual nets with a depth of up to 152 layers\u20148\u00d7\ndeeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep\nresidual nets are foundations of our submissions to ILSVRC\n& COCO 2015 competitions1\n, where we also won the 1st\nplaces on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\n\n[paper](http://arxiv.org/abs/1512.03385) [code](https://github.com/KaimingHe/deep-residual-networks)\n\n```text\n@article{He2015,\n\tauthor = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n\ttitle = {Deep Residual Learning for Image Recognition},\n\tjournal = {arXiv preprint arXiv:1512.03385},\n\tyear = {2015}\n}\n```\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Lornatang/ResNet-PyTorch", "keywords": "", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "resnet-pytorch", "package_url": "https://pypi.org/project/resnet-pytorch/", "platform": "", "project_url": "https://pypi.org/project/resnet-pytorch/", "project_urls": {"Homepage": "https://github.com/Lornatang/ResNet-PyTorch"}, "release_url": "https://pypi.org/project/resnet-pytorch/0.2.0/", "requires_dist": ["torch"], "requires_python": ">=3.6.0", "summary": "An ResNet implements of PyTorch.", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>ResNet-PyTorch</h1>\n<h3>Update (Feb 20, 2020)</h3>\n<p>The update is for ease of use and deployment.</p>\n<ul>\n<li><a href=\"#example-export-to-onnx\" rel=\"nofollow\">Example: Export to ONNX</a></li>\n<li><a href=\"#example-feature-extraction\" rel=\"nofollow\">Example: Extract features</a></li>\n<li><a href=\"#example-visual\" rel=\"nofollow\">Example: Visual</a></li>\n</ul>\n<p>It is also now incredibly simple to load a pretrained model with a new number of classes for transfer learning:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">resnet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">ResNet</span> \n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ResNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'resnet18'</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n</pre>\n<h3>Update (February 2, 2020)</h3>\n<p>This update allows you to use NVIDIA's Apex tool for accelerated training. By default choice <code>hybrid training precision</code> + <code>dynamic loss amplified</code> version, if you need to learn more and details about <code>apex</code> tools, please visit <a href=\"https://github.com/NVIDIA/apex\" rel=\"nofollow\">https://github.com/NVIDIA/apex</a>.</p>\n<h3>Overview</h3>\n<p>This repository contains an op-for-op PyTorch reimplementation of <a href=\"http://arxiv.org/abs/1512.03385\" rel=\"nofollow\">Deep Residual Learning for Image Recognition</a>.</p>\n<p>The goal of this implementation is to be simple, highly extensible, and easy to integrate into your own projects. This implementation is a work in progress -- new features are currently being implemented.</p>\n<p>At the moment, you can easily:</p>\n<ul>\n<li>Load pretrained ResNet models</li>\n<li>Use ResNet models for classification or feature extraction</li>\n</ul>\n<p><em>Upcoming features</em>: In the next few days, you will be able to:</p>\n<ul>\n<li>Quickly finetune an ResNet on your own dataset</li>\n<li>Export ResNet models for production</li>\n</ul>\n<h3>Table of contents</h3>\n<ol>\n<li><a href=\"#about-resnet\" rel=\"nofollow\">About ResNet</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#usage\" rel=\"nofollow\">Usage</a>\n<ul>\n<li><a href=\"#loading-pretrained-models\" rel=\"nofollow\">Load pretrained models</a></li>\n<li><a href=\"#example-classification\" rel=\"nofollow\">Example: Classify</a></li>\n<li><a href=\"#example-feature-extraction\" rel=\"nofollow\">Example: Extract features</a></li>\n<li><a href=\"#example-export-to-onnx\" rel=\"nofollow\">Example: Export to ONNX</a></li>\n<li><a href=\"#example-visual\" rel=\"nofollow\">Example: Visual</a></li>\n</ul>\n</li>\n<li><a href=\"#contributing\" rel=\"nofollow\">Contributing</a></li>\n</ol>\n<h3>About ResNet</h3>\n<p>If you're new to ResNets, here is an explanation straight from the official PyTorch implementation:</p>\n<p>Resnet models were proposed in \"Deep Residual Learning for Image Recognition\". Here we have the 5 versions of resnet models,\nwhich contains 5, 34, 50, 101, 152 layers respectively. Detailed model architectures can be found in Table 1.</p>\n<h3>Installation</h3>\n<p>Install from pypi:</p>\n<pre>$ pip3 install resnet_pytorch\n</pre>\n<p>Install from source:</p>\n<pre>$ git clone https://github.com/Lornatang/ResNet-PyTorch.git\n$ <span class=\"nb\">cd</span> ResNet-PyTorch\n$ pip3 install -e .\n</pre>\n<h3>Usage</h3>\n<h4>Loading pretrained models</h4>\n<p>Load an resnet18 network:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">resnet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">ResNet</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ResNet</span><span class=\"o\">.</span><span class=\"n\">from_name</span><span class=\"p\">(</span><span class=\"s2\">\"resnet18\"</span><span class=\"p\">)</span>\n</pre>\n<p>Load a pretrained resnet18:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">resnet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">ResNet</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ResNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s2\">\"resnet18\"</span><span class=\"p\">)</span>\n</pre>\n<p>Their 1-crop error rates on imagenet dataset with pretrained models are listed below.</p>\n<table>\n<thead>\n<tr>\n<th>Model structure</th>\n<th>Top-1 error</th>\n<th>Top-5 error</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>resnet18</td>\n<td>30.24</td>\n<td>10.92</td>\n</tr>\n<tr>\n<td>resnet34</td>\n<td>26.70</td>\n<td>8.58</td>\n</tr>\n<tr>\n<td>resnet50</td>\n<td>23.85</td>\n<td>7.13</td>\n</tr>\n<tr>\n<td>resnet101</td>\n<td>22.63</td>\n<td>6.44</td>\n</tr>\n<tr>\n<td>resnet152</td>\n<td>21.69</td>\n<td>5.94</td>\n</tr></tbody></table>\n<p><em>Option B of resnet-18/34/50/101/152 only uses projections to increase dimensions.</em></p>\n<p>For results extending to the cifar10 dataset, see <code>examples/cifar</code></p>\n<h4>Example: Classification</h4>\n<p>We assume that in your current directory, there is a <code>img.jpg</code> file and a <code>labels_map.txt</code> file (ImageNet class names). These are both included in <code>examples/simple</code>.</p>\n<p>All pre-trained models expect input images normalized in the same way,\ni.e. mini-batches of 3-channel RGB images of shape <code>(3 x H x W)</code>, where <code>H</code> and <code>W</code> are expected to be at least <code>224</code>.\nThe images have to be loaded in to a range of <code>[0, 1]</code> and then normalized using <code>mean = [0.485, 0.456, 0.406]</code>\nand <code>std = [0.229, 0.224, 0.225]</code>.</p>\n<p>Here's a sample execution.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">json</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision.transforms</span> <span class=\"k\">as</span> <span class=\"nn\">transforms</span>\n<span class=\"kn\">from</span> <span class=\"nn\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">resnet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">ResNet</span> \n\n<span class=\"c1\"># Open image</span>\n<span class=\"n\">input_image</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"s2\">\"img.jpg\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Preprocess image</span>\n<span class=\"n\">preprocess</span> <span class=\"o\">=</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">([</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Resize</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">CenterCrop</span><span class=\"p\">(</span><span class=\"mi\">224</span><span class=\"p\">),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">(),</span>\n    <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Normalize</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.485</span><span class=\"p\">,</span> <span class=\"mf\">0.456</span><span class=\"p\">,</span> <span class=\"mf\">0.406</span><span class=\"p\">],</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.229</span><span class=\"p\">,</span> <span class=\"mf\">0.224</span><span class=\"p\">,</span> <span class=\"mf\">0.225</span><span class=\"p\">]),</span>\n<span class=\"p\">])</span>\n<span class=\"n\">input_tensor</span> <span class=\"o\">=</span> <span class=\"n\">preprocess</span><span class=\"p\">(</span><span class=\"n\">input_image</span><span class=\"p\">)</span>\n<span class=\"n\">input_batch</span> <span class=\"o\">=</span> <span class=\"n\">input_tensor</span><span class=\"o\">.</span><span class=\"n\">unsqueeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>  <span class=\"c1\"># create a mini-batch as expected by the model</span>\n\n<span class=\"c1\"># Load class names</span>\n<span class=\"n\">labels_map</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">\"labels_map.txt\"</span><span class=\"p\">))</span>\n<span class=\"n\">labels_map</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">labels_map</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)]</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">)]</span>\n\n<span class=\"c1\"># Classify with ResNet18</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ResNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s2\">\"resnet18\"</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># move the input and model to GPU for speed if available</span>\n<span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">is_available</span><span class=\"p\">():</span>\n    <span class=\"n\">input_batch</span> <span class=\"o\">=</span> <span class=\"n\">input_batch</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"s2\">\"cuda\"</span><span class=\"p\">)</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"s2\">\"cuda\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n    <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">input_batch</span><span class=\"p\">)</span>\n<span class=\"n\">preds</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">topk</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">indices</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"-----\"</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">idx</span> <span class=\"ow\">in</span> <span class=\"n\">preds</span><span class=\"p\">:</span>\n    <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"n\">labels_map</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">]</span>\n    <span class=\"n\">prob</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">idx</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"</span><span class=\"si\">{</span><span class=\"n\">label</span><span class=\"si\">:</span><span class=\"s2\">&lt;75</span><span class=\"si\">}</span><span class=\"s2\"> (</span><span class=\"si\">{</span><span class=\"n\">prob</span> <span class=\"o\">*</span> <span class=\"mi\">100</span><span class=\"si\">:</span><span class=\"s2\">.2f</span><span class=\"si\">}</span><span class=\"s2\">%)\"</span><span class=\"p\">)</span>\n</pre>\n<h4>Example: Feature Extraction</h4>\n<p>You can easily extract features with <code>model.extract_features</code>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">resnet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">ResNet</span> \n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ResNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'resnet18'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># ... image preprocessing as in the classification example ...</span>\n<span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># torch.Size([1, 3, 224, 224])</span>\n\n<span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">extract_features</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># torch.Size([1, 512, 1, 1])</span>\n</pre>\n<h4>Example: Export to ONNX</h4>\n<p>Exporting to ONNX for deploying to production is now simple:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span> \n<span class=\"kn\">from</span> <span class=\"nn\">resnet_pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">ResNet</span> \n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ResNet</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span><span class=\"s1\">'resnet18'</span><span class=\"p\">)</span>\n<span class=\"n\">dummy_input</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">)</span>\n\n<span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">onnx</span><span class=\"o\">.</span><span class=\"n\">export</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">dummy_input</span><span class=\"p\">,</span> <span class=\"s2\">\"demo.onnx\"</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<h4>Example: Visual</h4>\n<pre>cd $REPO$/framework\nsh start.sh\n</pre>\n<p>Then open the browser and type in the browser address <a href=\"http://127.0.0.1:10004/\" rel=\"nofollow\">http://127.0.0.1:10004/</a>.</p>\n<p>Enjoy it.</p>\n<h4>ImageNet</h4>\n<p>See <code>examples/imagenet</code> for details about evaluating on ImageNet.</p>\n<h3>Credit</h3>\n<h4>Deep Residual Learning for Image Recognition</h4>\n<p><em>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</em></p>\n<h5>Abstract</h5>\n<p>Deeper neural networks are more difficult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. On the ImageNet dataset we\nevaluate residual nets with a depth of up to 152 layers\u20148\u00d7\ndeeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep\nresidual nets are foundations of our submissions to ILSVRC\n&amp; COCO 2015 competitions1\n, where we also won the 1st\nplaces on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.</p>\n<p><a href=\"http://arxiv.org/abs/1512.03385\" rel=\"nofollow\">paper</a> <a href=\"https://github.com/KaimingHe/deep-residual-networks\" rel=\"nofollow\">code</a></p>\n<pre>@article{He2015,\n\tauthor = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n\ttitle = {Deep Residual Learning for Image Recognition},\n\tjournal = {arXiv preprint arXiv:1512.03385},\n\tyear = {2015}\n}\n</pre>\n\n          </div>"}, "last_serial": 6818985, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "778a296552ad006de78205904402c4a4", "sha256": "ed7adda01188896ebcab522d9c7bde523a600b69ea819c139b2fb0e536b7fa4b"}, "downloads": -1, "filename": "resnet_pytorch-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "778a296552ad006de78205904402c4a4", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5.0", "size": 17259, "upload_time": "2020-02-20T06:35:51", "upload_time_iso_8601": "2020-02-20T06:35:51.529845Z", "url": "https://files.pythonhosted.org/packages/4b/79/aacd87bced2ed90bcf6109063bd4c21867f57531b9a607191eddc335dea4/resnet_pytorch-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "aa21a9d62a9de304e10ebd298c930868", "sha256": "fecee9f41fb2368dca2b42c0ff95521e61528d55ebf93ea287ef6523fd40fdeb"}, "downloads": -1, "filename": "resnet_pytorch-0.1.0.tar.gz", "has_sig": false, "md5_digest": "aa21a9d62a9de304e10ebd298c930868", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5.0", "size": 9733, "upload_time": "2020-02-20T06:35:55", "upload_time_iso_8601": "2020-02-20T06:35:55.340944Z", "url": "https://files.pythonhosted.org/packages/ef/1a/3a371507c4b9d5f78def988485af7245eaded2e21cfca93d862a637eb086/resnet_pytorch-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "29afe215aa07750db64cfe0bf6840cfe", "sha256": "f95612bf4fedb89d54f3b9503889d1e4f9c1d68216ae51920d39d0d9eac3a01a"}, "downloads": -1, "filename": "resnet_pytorch-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "29afe215aa07750db64cfe0bf6840cfe", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 13887, "upload_time": "2020-03-16T06:34:25", "upload_time_iso_8601": "2020-03-16T06:34:25.657223Z", "url": "https://files.pythonhosted.org/packages/78/08/63f61c49fba28416244c98a425ac180d3cbea15884c5d29fafd720ae89e6/resnet_pytorch-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7f41e4854157869ad19f68e27532890e", "sha256": "ba8f228c847037cceaa8c0213c9c8bf0fd04c00f44687edb7cc636259f871315"}, "downloads": -1, "filename": "resnet_pytorch-0.2.0.tar.gz", "has_sig": false, "md5_digest": "7f41e4854157869ad19f68e27532890e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 9827, "upload_time": "2020-03-16T06:34:27", "upload_time_iso_8601": "2020-03-16T06:34:27.569409Z", "url": "https://files.pythonhosted.org/packages/04/ec/c0608ca4737a69631a1c78e9ba834ced47113f4e7321afa329e5aa9ef97d/resnet_pytorch-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "29afe215aa07750db64cfe0bf6840cfe", "sha256": "f95612bf4fedb89d54f3b9503889d1e4f9c1d68216ae51920d39d0d9eac3a01a"}, "downloads": -1, "filename": "resnet_pytorch-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "29afe215aa07750db64cfe0bf6840cfe", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 13887, "upload_time": "2020-03-16T06:34:25", "upload_time_iso_8601": "2020-03-16T06:34:25.657223Z", "url": "https://files.pythonhosted.org/packages/78/08/63f61c49fba28416244c98a425ac180d3cbea15884c5d29fafd720ae89e6/resnet_pytorch-0.2.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7f41e4854157869ad19f68e27532890e", "sha256": "ba8f228c847037cceaa8c0213c9c8bf0fd04c00f44687edb7cc636259f871315"}, "downloads": -1, "filename": "resnet_pytorch-0.2.0.tar.gz", "has_sig": false, "md5_digest": "7f41e4854157869ad19f68e27532890e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 9827, "upload_time": "2020-03-16T06:34:27", "upload_time_iso_8601": "2020-03-16T06:34:27.569409Z", "url": "https://files.pythonhosted.org/packages/04/ec/c0608ca4737a69631a1c78e9ba834ced47113f4e7321afa329e5aa9ef97d/resnet_pytorch-0.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:03:50 2020"}