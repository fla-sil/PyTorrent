{"info": {"author": "Kevin Musgrave", "author_email": "tkm45@cornell.edu", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "<h1 align=\"center\">\n <a href=\"https://arxiv.org/abs/2003.08505\">A Metric Learning Reality Check</a>\n</h2>\n<p align=\"center\">\n\n\n## [Benchmark results (in progress)](https://drive.google.com/open?id=1Y_stkiqlHA7HTMNrhyPCnYhR0oevphRR): \n- [Spreadsheet #1: Train/val 50/50](https://docs.google.com/spreadsheets/d/1kiJ5rKmneQvnYKpVO9vBFdMDNx-yLcXV2wbDXlb-SB8/edit?usp=sharing)\n- [Spreadsheet #2: 4-fold cross validation, test on 2nd-half of classes](https://docs.google.com/spreadsheets/d/1brUBishNxmld-KLDAJewIc43A4EVZk3gY6yKe8OIKbY/edit?usp=sharing)\n\n## Benefits of this library\n1. Highly configurable\n    - Use the default configs files, merge in your own, or override options via the command line.\n2. Extensive logging\n    - View experiment data in tensorboard, csv, and sqlite format.\n3. Easy hyperparameter optimization\n    - Simply append \\~BAYESIAN\\~ to the hyperparameters you want to optimize.\n4. Customizable\n    - Register your own losses, miners, datasets etc. with a simple function call.\n\n## Installation\n```\npip install powerful-benchmarker\n```\n\n## Usage\n\n### Set default flags\n\nThe easiest way to get started is to download the [example script](https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/examples/run.py). Then change the default values for the following flags:\n\n- pytorch_home is where you want to save downloaded pretrained models.\n- dataset_root is where your datasets are located.\n- root_experiment_folder is where you want all experiment data to be saved.\n\n\n### Download and organize the datasets\nDownload the datasets from here:\n- [CUB200](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)\n- [Cars196](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)\n- [Stanford Online Products](http://cvgl.stanford.edu/projects/lifted_struct)\n\nOrganize them as follows:\n```\n<dataset_root>\n|-cub2011\n  |-attributes.txt\n  |-CUB_200_2011\n    |-images\n|-cars196\n  |-cars_annos.mat\n  |-car_ims\n|-Stanford_Online_Products\n  |-bicycle_final\n  |-cabinet_final\n  ...\n```\n\n### Try a basic command\nThe following command will run an experiment using the [default config files](https://github.com/KevinMusgrave/powerful-benchmarker/tree/master/powerful_benchmarker/configs)\n```\npython run.py --experiment_name test1 \n```\nExperiment data is saved in the following format:\n```\n<root_experiment_folder>\n|-<experiment_name>\n  |-configs\n    |-config_eval.yaml\n    |-config_general.yaml\n    |-config_loss_and_miners.yaml\n    |-config_models.yaml\n    |-config_optimizers.yaml\n    |-config_transforms.yaml\n  |-<split scheme name>\n    |-saved_models\n    |-saved_csvs\n    |-tensorboard_logs\n  |-meta_logs\n    |-saved_csvs\n    |-tensorboard_logs\n```\n\n### Override config options at the command line\nThe default config files use a [batch size of 32](https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_models/default.yaml). What if you want to use a batch size of 256? Just write the flag at the command line:\n```\npython run.py --experiment_name test1 --batch_size 256\n```\nComplex options (i.e. nested dictionaries) can be specified at the command line:\n```\npython run.py \\\n--experiment_name test1 \\\n--mining_funcs {tuple_miner: {PairMarginMiner: {pos_margin: 0.5, neg_margin: 0.5}}}\n```\nThe ```~OVERRIDE~``` suffix is required to completely override complex config options. For example, the following overrides the [default loss function](https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_loss_and_miners/default.yaml):\n```\npython run.py \\\n--experiment_name test1 \\\n--loss_funcs {metric_loss~OVERRIDE~: {ArcFaceLoss: {margin: 30, scale: 64, embedding_size: 128}}}\n```\nLeave out the ```~OVERRIDE~``` suffix if you want to merge options. For example, we can add an optimizer for our loss function's parameters:\n```\npython run.py \\\n--experiment_name test1 \\\n--optimizers {metric_loss_optimizer: {SGD: {lr: 0.01}}} \n```\nThis will be included along with the [default optimizers](https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_optimizers/default.yaml). \n\nWe can change the learning rate of the trunk_optimizer, but keep all other parameters the same:\n```\npython run.py \\\n--experiment_name test1 \\\n--optimizers {trunk_optimizer: {RMSprop: {lr: 0.01}}} \n```\n\nOr we can make trunk_optimizer use Adam, but leave embedder_optimizer to the default setting: \n```\npython run.py \\\n--experiment_name test1 \\\n--optimizers {trunk_optimizer~OVERRIDE~: {Adam: {lr: 0.01}}} \n```\n\n### Combine yaml files at the command line\nThe following merges the ```with_cars196``` config file into the ```default``` config file, in the ```config_general``` category.\n```\npython run.py --experiment_name test1 --config_general default with_cars196\n```\nThis is convenient when you want to change a few settings (specified in ```with_cars196```), and keep all the other options unchanged (specified in ```default```). You can specify any number of config files to merge, and they get loaded and merged in the order that you specify.\n\n### Resume training\nThe following resumes training for the ```test1``` experiment:\n```\npython run.py --experiment_name test1 --resume_training\n```\nLet's say you finished training for 100 epochs, and decide you want to train for another 50 epochs, for a total of 150. You would run:\n```\npython run.py --experiment_name test1 --resume_training --num_epochs_train 150\n```\nNow in your experiments folder you'll see the original config files, and a new folder starting with ```resume_training```.\n```\n<root_experiment_folder>\n|-<experiment_name>\n  |-configs\n    |-config_eval.yaml\n    ...\n    |-resume_training_config_diffs_<underscore delimited numbers>\n  ...\n```\nThis folder contains all differences between the originally saved config files and the parameters that you've specified at the command line. In this particular case, there should just be a single file ```config_general.yaml``` with a single line: ```num_epochs_train: 150```. \n\nThe underscore delimited numbers in the folder name indicate which models were loaded for each [split scheme](#split-schemes-and-cross-validation). For example, let's say you are doing cross validation with 3 folds. The training process has finished 50, 30, and 0 epochs of folds 0, 1, and 2, respectively. You decide to stop training, and resume training with a different batch size. Now the config diff folder will be named ```resume_training_config_diffs_50_30_0```.\n\n### Reproducing benchmark results\nTo reproduce an experiment from the benchmark spreadsheets, use the ```--reproduce_results``` flag:\n1. In the benchmark spreadsheet, click on the google drive link under the \"config files\" column.\n2. Download the folders you want (for example ```cub200_old_approach_triplet_batch_all```), into some folder on your computer. For example, I downloaded into ```/home/tkm45/experiments_to_reproduce```\n3. Then run:\n```\npython run.py --reproduce_results /home/tkm45/experiments_to_reproduce/cub200_old_approach_triplet_batch_all \\\n--experiment_name cub200_old_approach_triplet_batch_all_reproduced\n```\n\n### Evaluation options\nBy default, your model will be saved and evaluated on the validation set every ```save_interval``` epochs.\n\nTo get accuracy for specific splits, use the ```--splits_to_eval``` flag and pass in a space-delimited list of split names. For example ```--splits_to_eval train test```\n\nTo run evaluation only, use the ```--evaluate``` flag.\n\n### Split schemes and cross validation\nOne weakness of many metric-learning papers is that they have been training and testing on the same handful of datasets for years. They have also been splitting data into a 50/50 train/test split scheme, instead of train/val/test. This has likely lead to overfitting on the \"test\" set, as people have tuned hyperparameters and created algorithms with direct feedback from the \"test\" set.\n\nTo remedy this situation, this benchmarker allows the user to specify the split scheme. Here's an example config:\n```yaml\ntest_size: 0.5\ntest_start_idx: 0.5\nnum_training_partitions: 10\nnum_training_sets: 5\n```\nTranslation:\n- The test set consists of classes with labels in ```[num_labels * test_start_idx, num_labels * (test_start_idx + test_size)]```. Note that if we set ```test_start_idx``` to 0.9, the range would wrap around to the beginning (0.9 to 1, 0 to 0.4). \n- The remaining classes will be split into 10 equal sized partitions. \n- 5 of those partitions will be used for training. In other words, 5-fold cross validation will be performed, but the size of the partitions will be the same as if 10-fold cross validation was being performed.\n\nWhen evaluating the cross-validated models, the best model from each fold will be loaded, and the results be averaged. Alternatively, you can set the config option ```meta_testing_method``` to ```ConcatenateEmbeddings```. This will load the best model from each fold, but treat them as one model during evaluation on the test set, by concatenating their outputs.\n\nIf instead you still want to use the old 50/50 train/test split, then set ```special_split_scheme_name``` to ```old_approach```. Otherwise, leave it as ```null```. \n\n### Meta logs\nWhen doing cross validation, a new set of meta records will be created. The meta records show the average of the best accuracies of your training runs. You can find these records on tensorboard and in the meta_logs folder.\n\n### Bayesian optimization to tune hyperparameters\nYou can use bayesian optimization using the same [example script](https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/examples/run.py). In your config files or at the command line, append ```~BAYESIAN~``` to any parameter that you want to tune, followed by a lower and upper bound in square brackets. If your parameter operates on a log scale (for example, learning rates), then append ```~LOG_BAYESIAN~```. You must also specify the number of iterations with the ```--bayes_opt_iters``` command line flag.\n\nHere is an example script which uses bayesian optimization to tune 3 hyperparameters for the multi similarity loss.\n```\npython run.py --bayes_opt_iters 50 \\\n--loss_funcs~OVERRIDE~ {metric_loss: {MultiSimilarityLoss: {alpha~LOG_BAYESIAN~: [0.01, 100], beta~LOG_BAYESIAN~: [0.01, 100], base~BAYESIAN~: [0, 1]}}} \\\n--experiment_name cub_bayes_opt \\\n```\n\nIf you stop and want to resume bayesian optimization, simply use ```run.py``` with the same ```experiment_name``` you were using before. (Do not use the ```resume_training``` flag.) \n\nYou can also run a number of reproductions for the best parameters, so that you can obtain a confidence interval for your results. Use the ```reproductions``` flag, and pass in the number of reproductions you want to perform at the end of bayesian optimization.\n\n```\npython run.py --bayes_opt_iters 50 --reproductions 10 \\\n--experiment_name cub_bayes_opt \\\n```\n\n### Register your own classes and modules\nBy default, the API gives you access to losses/miners/datasets/optimizers/schedulers/trainers etc that are available in powerful-benchmarker, PyTorch, and pytorch-metric-learning.\n\nLet's say you make your own loss and mining functions, and you'd like to have access to them via the API. You can accomplish this by replacing the last two lines of the [example script](https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/examples/run.py) with this:\n\n```python\nfrom pytorch_metric_learning import losses, miners\n\n# your custom loss function\nclass YourLossFunction(losses.BaseMetricLossFunction):\n   ...\n\n# your custom mining function\nclass YourMiningFunction(miners.BaseTupleMiner):\n   ...\n\nr = runner(**(args.__dict__))\n\n# make the runner aware of them\nr.register(\"loss\", YourLossFunction)\nr.register(\"miner\", YourMiningFunction)\nr.run()\n```\n\nNow you can access your custom classes just like any other class:\n```yaml\nloss_funcs:\n  metric_loss: \n    YourLossFunction:\n\nmining_funcs:\n  tuple_miner:\n    YourMiningFunction:\n```\n\nIf you have a module containing multiple classes and you want to register all those classes, you can simply register the module:\n```python\nimport YourModuleOfLosses\nr.register(\"loss\", YourModuleOfLosses)\n```\n\nRegistering your own trainer is a bit more involved, because you need to also create an associated API parser. The name of the api parser should be ```APIParser<name of your training method>```. \n\nHere's an example where I make a trainer that extends ```trainers.MetricLossOnly```, and takes in an additional argument ```foo```. In order to pass this in, the API parser needs to add ```foo``` to the trainer kwargs, and this is done in the ```get_trainer_kwargs``` method.\n\n```python\nfrom pytorch_metric_learning import trainers\nfrom powerful_benchmarker import api_parsers\n\nclass YourTrainer(trainers.MetricLossOnly):\n    def __init__(self, foo, **kwargs):\n\tsuper().__init__(**kwargs)\n\tself.foo = foo\n\tprint(\"foo = \", self.foo)\n\n\nclass APIYourTrainer(api_parsers.BaseAPIParser):\n    def get_foo(self):\n        return \"hello\"\n\n    def get_trainer_kwargs(self):\n        trainer_kwargs = super().get_trainer_kwargs()\n        trainer_kwargs[\"foo\"] = self.get_foo()\n        return trainer_kwargs\n\nr = runner(**(args.__dict__))\nr.register(\"trainer\", YourTrainer)\nr.register(\"api_parser\", APIYourTrainer)\nr.run()\n```\n\nIf your trainer's ```__init__``` arguments are identical to the one you're extending, then you can just set ```APIYourTrainer = api_parsers.BaseAPIParser```. (Note that ```BaseAPIParser``` is the same as ```APIMetricLossOnly```)\n\n\n## Config options guide\nBelow is the format for the various config files. Click on the links to see the default yaml file for each category.\n\n### [config_general](https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_general/default.yaml)\n```yaml\ntraining_method: <type> #options: MetricLossOnly, TrainWithClassifier, CascadedEmbeddings, DeepAdversarialMetricLearning\ntesting_method: <type> #options: GlobalEmbeddingSpaceTester, WithSameParentLabelTester\nmeta_testing_method: <type> #options: null or ConcatenateEmbeddings\ndataset:  \n  <type>: #options: CUB200, Cars196, StanfordOnlineProducts\n    <kwarg>: <value>\n    ...\nnum_epochs_train: <how long to train for>\niterations_per_epoch: <Optional. If set, an epoch will simply be a fixed number of iterations. Or you can set this to null or 0, and it will be ignored.>\nsave_interval: <how often (in number of epochs) models will be saved and evaluated>\nspecial_split_scheme_name: <string> #options: old_approach or predefined. Leave as null if you want to do cross validation.\ntest_size: <number> #number in (0, 1), which is the percent of classes that will be used in the test set.\ntest_start_idx: <number> #number in (0, 1), which is the percent that specifies the starting class index for the test set\nnum_training_partitions: <int> #number of partitions (excluding the test set) that are created for cross validation.\nnum_training_sets: <int> #number of partitions that are actually used as training sets cross validation.\n\nlabel_hierarchy_level: <number>\ndataloader_num_workers: <number>\ncheck_untrained_accuracy: <boolean>\npatience: <int> #Training will stop if validation accuracy has not improved after this number of epochs. If null, then it is ignored.\n\n```\n### [config_models](https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_models/default.yaml)\n```yaml\nmodels:\n  trunk:\n    <type>:\n      <kwarg>: <value>\n      ...\n  embedder:\n    <type>:\n      <kwarg>: <value>\n      ...\nbatch_size: <number>\nfreeze_batchnorm: <boolean>\n```\n### [config_loss_and_miners](https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_loss_and_miners/default.yaml)\n```yaml \nloss_funcs:\n  <name>: \n    <type>:\n      <kwarg>: <value>\n      ...\n  ...\n\nsampler:\n  <type>:\n    <kwarg>: <value>\n    ...\n\nmining_funcs:\n  <name>: \n    <type>: \n      <kwarg>: <value>\n      ...\n  ...\n```\n### [config_optimizers](https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_optimizers/default.yaml)\n```yaml\noptimizers:\n  trunk_optimizer:\n    <type>:\n      <kwarg>: <value>\n      ...\n  embedder_optimizer:\n    <type>:\n      <kwarg>: <value>\n      ...\n  ...\n```\n### [config_transforms](https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_transforms/default.yaml)\n```yaml\ntransforms:\n  train:\n    <type>\n      <kwarg>: <value>\n      ...\n    ...\n\n  eval:\n    <type>\n      <kwarg>: <value>\n      ...\n    ...\n```\n### [config_eval](https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_eval/default.yaml)\n```yaml\neval_reference_set: <name> #options: compared_to_self, compared_to_sets_combined, compared_to_training_set\neval_normalize_embeddings: <boolean>\neval_use_trunk_output: <boolean>\neval_batch_size: <number>\neval_metric_for_best_epoch: <name> #options: NMI, precision_at_1, r_precision, mean_average_r_precision\neval_dataloader_num_workers: <number>\neval_pca: <number> or null #options: number of dimensions to reduce embeddings to via PCA, or null if you don't want to use PCA.\neval_accuracy_calculator:\n    <type>\n      <kwarg>: <value>\n      ...\n```\n\n## Acknowledgements\nThank you to Ser-Nam Lim at Facebook AI, and my research advisor, Professor Serge Belongie. This project began during my internship at Facebook AI where I received valuable feedback from Ser-Nam, and his team of computer vision and machine learning engineers and research scientists.\n\n## Citing this library\nIf you'd like to cite powerful-benchmarker in your paper, you can use this bibtex:\n```latex\n@misc{Musgrave2019,\n  author = {Musgrave, Kevin and Lim, Ser-Nam and Belongie, Serge},\n  title = {Powerful Benchmarker},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/KevinMusgrave/powerful-benchmarker}},\n}\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/KevinMusgrave/powerful-benchmarker", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "powerful-benchmarker", "package_url": "https://pypi.org/project/powerful-benchmarker/", "platform": "", "project_url": "https://pypi.org/project/powerful-benchmarker/", "project_urls": {"Homepage": "https://github.com/KevinMusgrave/powerful-benchmarker"}, "release_url": "https://pypi.org/project/powerful-benchmarker/0.9.21/", "requires_dist": ["numpy", "scikit-learn", "torch", "torchvision", "easy-module-attribute-getter (>=0.9.34)", "record-keeper (>=0.9.24)", "tensorboard", "matplotlib", "pretrainedmodels", "pytorch-metric-learning (>=0.9.84)", "pandas", "ax-platform", "faiss-gpu"], "requires_python": ">=3.0", "summary": "A highly-configurable tool that enables thorough evaluation of deep metric learning algorithms.", "version": "0.9.21", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>\n <a href=\"https://arxiv.org/abs/2003.08505\" rel=\"nofollow\">A Metric Learning Reality Check</a>\n</h1>\n<p align=\"center\">\n</p><h2><a href=\"https://drive.google.com/open?id=1Y_stkiqlHA7HTMNrhyPCnYhR0oevphRR\" rel=\"nofollow\">Benchmark results (in progress)</a>:</h2>\n<ul>\n<li><a href=\"https://docs.google.com/spreadsheets/d/1kiJ5rKmneQvnYKpVO9vBFdMDNx-yLcXV2wbDXlb-SB8/edit?usp=sharing\" rel=\"nofollow\">Spreadsheet #1: Train/val 50/50</a></li>\n<li><a href=\"https://docs.google.com/spreadsheets/d/1brUBishNxmld-KLDAJewIc43A4EVZk3gY6yKe8OIKbY/edit?usp=sharing\" rel=\"nofollow\">Spreadsheet #2: 4-fold cross validation, test on 2nd-half of classes</a></li>\n</ul>\n<h2>Benefits of this library</h2>\n<ol>\n<li>Highly configurable\n<ul>\n<li>Use the default configs files, merge in your own, or override options via the command line.</li>\n</ul>\n</li>\n<li>Extensive logging\n<ul>\n<li>View experiment data in tensorboard, csv, and sqlite format.</li>\n</ul>\n</li>\n<li>Easy hyperparameter optimization\n<ul>\n<li>Simply append ~BAYESIAN~ to the hyperparameters you want to optimize.</li>\n</ul>\n</li>\n<li>Customizable\n<ul>\n<li>Register your own losses, miners, datasets etc. with a simple function call.</li>\n</ul>\n</li>\n</ol>\n<h2>Installation</h2>\n<pre><code>pip install powerful-benchmarker\n</code></pre>\n<h2>Usage</h2>\n<h3>Set default flags</h3>\n<p>The easiest way to get started is to download the <a href=\"https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/examples/run.py\" rel=\"nofollow\">example script</a>. Then change the default values for the following flags:</p>\n<ul>\n<li>pytorch_home is where you want to save downloaded pretrained models.</li>\n<li>dataset_root is where your datasets are located.</li>\n<li>root_experiment_folder is where you want all experiment data to be saved.</li>\n</ul>\n<h3>Download and organize the datasets</h3>\n<p>Download the datasets from here:</p>\n<ul>\n<li><a href=\"http://www.vision.caltech.edu/visipedia/CUB-200-2011.html\" rel=\"nofollow\">CUB200</a></li>\n<li><a href=\"https://ai.stanford.edu/%7Ejkrause/cars/car_dataset.html\" rel=\"nofollow\">Cars196</a></li>\n<li><a href=\"http://cvgl.stanford.edu/projects/lifted_struct\" rel=\"nofollow\">Stanford Online Products</a></li>\n</ul>\n<p>Organize them as follows:</p>\n<pre><code>&lt;dataset_root&gt;\n|-cub2011\n  |-attributes.txt\n  |-CUB_200_2011\n    |-images\n|-cars196\n  |-cars_annos.mat\n  |-car_ims\n|-Stanford_Online_Products\n  |-bicycle_final\n  |-cabinet_final\n  ...\n</code></pre>\n<h3>Try a basic command</h3>\n<p>The following command will run an experiment using the <a href=\"https://github.com/KevinMusgrave/powerful-benchmarker/tree/master/powerful_benchmarker/configs\" rel=\"nofollow\">default config files</a></p>\n<pre><code>python run.py --experiment_name test1 \n</code></pre>\n<p>Experiment data is saved in the following format:</p>\n<pre><code>&lt;root_experiment_folder&gt;\n|-&lt;experiment_name&gt;\n  |-configs\n    |-config_eval.yaml\n    |-config_general.yaml\n    |-config_loss_and_miners.yaml\n    |-config_models.yaml\n    |-config_optimizers.yaml\n    |-config_transforms.yaml\n  |-&lt;split scheme name&gt;\n    |-saved_models\n    |-saved_csvs\n    |-tensorboard_logs\n  |-meta_logs\n    |-saved_csvs\n    |-tensorboard_logs\n</code></pre>\n<h3>Override config options at the command line</h3>\n<p>The default config files use a <a href=\"https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_models/default.yaml\" rel=\"nofollow\">batch size of 32</a>. What if you want to use a batch size of 256? Just write the flag at the command line:</p>\n<pre><code>python run.py --experiment_name test1 --batch_size 256\n</code></pre>\n<p>Complex options (i.e. nested dictionaries) can be specified at the command line:</p>\n<pre><code>python run.py \\\n--experiment_name test1 \\\n--mining_funcs {tuple_miner: {PairMarginMiner: {pos_margin: 0.5, neg_margin: 0.5}}}\n</code></pre>\n<p>The <code>~OVERRIDE~</code> suffix is required to completely override complex config options. For example, the following overrides the <a href=\"https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_loss_and_miners/default.yaml\" rel=\"nofollow\">default loss function</a>:</p>\n<pre><code>python run.py \\\n--experiment_name test1 \\\n--loss_funcs {metric_loss~OVERRIDE~: {ArcFaceLoss: {margin: 30, scale: 64, embedding_size: 128}}}\n</code></pre>\n<p>Leave out the <code>~OVERRIDE~</code> suffix if you want to merge options. For example, we can add an optimizer for our loss function's parameters:</p>\n<pre><code>python run.py \\\n--experiment_name test1 \\\n--optimizers {metric_loss_optimizer: {SGD: {lr: 0.01}}} \n</code></pre>\n<p>This will be included along with the <a href=\"https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_optimizers/default.yaml\" rel=\"nofollow\">default optimizers</a>.</p>\n<p>We can change the learning rate of the trunk_optimizer, but keep all other parameters the same:</p>\n<pre><code>python run.py \\\n--experiment_name test1 \\\n--optimizers {trunk_optimizer: {RMSprop: {lr: 0.01}}} \n</code></pre>\n<p>Or we can make trunk_optimizer use Adam, but leave embedder_optimizer to the default setting:</p>\n<pre><code>python run.py \\\n--experiment_name test1 \\\n--optimizers {trunk_optimizer~OVERRIDE~: {Adam: {lr: 0.01}}} \n</code></pre>\n<h3>Combine yaml files at the command line</h3>\n<p>The following merges the <code>with_cars196</code> config file into the <code>default</code> config file, in the <code>config_general</code> category.</p>\n<pre><code>python run.py --experiment_name test1 --config_general default with_cars196\n</code></pre>\n<p>This is convenient when you want to change a few settings (specified in <code>with_cars196</code>), and keep all the other options unchanged (specified in <code>default</code>). You can specify any number of config files to merge, and they get loaded and merged in the order that you specify.</p>\n<h3>Resume training</h3>\n<p>The following resumes training for the <code>test1</code> experiment:</p>\n<pre><code>python run.py --experiment_name test1 --resume_training\n</code></pre>\n<p>Let's say you finished training for 100 epochs, and decide you want to train for another 50 epochs, for a total of 150. You would run:</p>\n<pre><code>python run.py --experiment_name test1 --resume_training --num_epochs_train 150\n</code></pre>\n<p>Now in your experiments folder you'll see the original config files, and a new folder starting with <code>resume_training</code>.</p>\n<pre><code>&lt;root_experiment_folder&gt;\n|-&lt;experiment_name&gt;\n  |-configs\n    |-config_eval.yaml\n    ...\n    |-resume_training_config_diffs_&lt;underscore delimited numbers&gt;\n  ...\n</code></pre>\n<p>This folder contains all differences between the originally saved config files and the parameters that you've specified at the command line. In this particular case, there should just be a single file <code>config_general.yaml</code> with a single line: <code>num_epochs_train: 150</code>.</p>\n<p>The underscore delimited numbers in the folder name indicate which models were loaded for each <a href=\"#split-schemes-and-cross-validation\" rel=\"nofollow\">split scheme</a>. For example, let's say you are doing cross validation with 3 folds. The training process has finished 50, 30, and 0 epochs of folds 0, 1, and 2, respectively. You decide to stop training, and resume training with a different batch size. Now the config diff folder will be named <code>resume_training_config_diffs_50_30_0</code>.</p>\n<h3>Reproducing benchmark results</h3>\n<p>To reproduce an experiment from the benchmark spreadsheets, use the <code>--reproduce_results</code> flag:</p>\n<ol>\n<li>In the benchmark spreadsheet, click on the google drive link under the \"config files\" column.</li>\n<li>Download the folders you want (for example <code>cub200_old_approach_triplet_batch_all</code>), into some folder on your computer. For example, I downloaded into <code>/home/tkm45/experiments_to_reproduce</code></li>\n<li>Then run:</li>\n</ol>\n<pre><code>python run.py --reproduce_results /home/tkm45/experiments_to_reproduce/cub200_old_approach_triplet_batch_all \\\n--experiment_name cub200_old_approach_triplet_batch_all_reproduced\n</code></pre>\n<h3>Evaluation options</h3>\n<p>By default, your model will be saved and evaluated on the validation set every <code>save_interval</code> epochs.</p>\n<p>To get accuracy for specific splits, use the <code>--splits_to_eval</code> flag and pass in a space-delimited list of split names. For example <code>--splits_to_eval train test</code></p>\n<p>To run evaluation only, use the <code>--evaluate</code> flag.</p>\n<h3>Split schemes and cross validation</h3>\n<p>One weakness of many metric-learning papers is that they have been training and testing on the same handful of datasets for years. They have also been splitting data into a 50/50 train/test split scheme, instead of train/val/test. This has likely lead to overfitting on the \"test\" set, as people have tuned hyperparameters and created algorithms with direct feedback from the \"test\" set.</p>\n<p>To remedy this situation, this benchmarker allows the user to specify the split scheme. Here's an example config:</p>\n<pre><span class=\"nt\">test_size</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">0.5</span>\n<span class=\"nt\">test_start_idx</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">0.5</span>\n<span class=\"nt\">num_training_partitions</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">10</span>\n<span class=\"nt\">num_training_sets</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">5</span>\n</pre>\n<p>Translation:</p>\n<ul>\n<li>The test set consists of classes with labels in <code>[num_labels * test_start_idx, num_labels * (test_start_idx + test_size)]</code>. Note that if we set <code>test_start_idx</code> to 0.9, the range would wrap around to the beginning (0.9 to 1, 0 to 0.4).</li>\n<li>The remaining classes will be split into 10 equal sized partitions.</li>\n<li>5 of those partitions will be used for training. In other words, 5-fold cross validation will be performed, but the size of the partitions will be the same as if 10-fold cross validation was being performed.</li>\n</ul>\n<p>When evaluating the cross-validated models, the best model from each fold will be loaded, and the results be averaged. Alternatively, you can set the config option <code>meta_testing_method</code> to <code>ConcatenateEmbeddings</code>. This will load the best model from each fold, but treat them as one model during evaluation on the test set, by concatenating their outputs.</p>\n<p>If instead you still want to use the old 50/50 train/test split, then set <code>special_split_scheme_name</code> to <code>old_approach</code>. Otherwise, leave it as <code>null</code>.</p>\n<h3>Meta logs</h3>\n<p>When doing cross validation, a new set of meta records will be created. The meta records show the average of the best accuracies of your training runs. You can find these records on tensorboard and in the meta_logs folder.</p>\n<h3>Bayesian optimization to tune hyperparameters</h3>\n<p>You can use bayesian optimization using the same <a href=\"https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/examples/run.py\" rel=\"nofollow\">example script</a>. In your config files or at the command line, append <code>~BAYESIAN~</code> to any parameter that you want to tune, followed by a lower and upper bound in square brackets. If your parameter operates on a log scale (for example, learning rates), then append <code>~LOG_BAYESIAN~</code>. You must also specify the number of iterations with the <code>--bayes_opt_iters</code> command line flag.</p>\n<p>Here is an example script which uses bayesian optimization to tune 3 hyperparameters for the multi similarity loss.</p>\n<pre><code>python run.py --bayes_opt_iters 50 \\\n--loss_funcs~OVERRIDE~ {metric_loss: {MultiSimilarityLoss: {alpha~LOG_BAYESIAN~: [0.01, 100], beta~LOG_BAYESIAN~: [0.01, 100], base~BAYESIAN~: [0, 1]}}} \\\n--experiment_name cub_bayes_opt \\\n</code></pre>\n<p>If you stop and want to resume bayesian optimization, simply use <code>run.py</code> with the same <code>experiment_name</code> you were using before. (Do not use the <code>resume_training</code> flag.)</p>\n<p>You can also run a number of reproductions for the best parameters, so that you can obtain a confidence interval for your results. Use the <code>reproductions</code> flag, and pass in the number of reproductions you want to perform at the end of bayesian optimization.</p>\n<pre><code>python run.py --bayes_opt_iters 50 --reproductions 10 \\\n--experiment_name cub_bayes_opt \\\n</code></pre>\n<h3>Register your own classes and modules</h3>\n<p>By default, the API gives you access to losses/miners/datasets/optimizers/schedulers/trainers etc that are available in powerful-benchmarker, PyTorch, and pytorch-metric-learning.</p>\n<p>Let's say you make your own loss and mining functions, and you'd like to have access to them via the API. You can accomplish this by replacing the last two lines of the <a href=\"https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/examples/run.py\" rel=\"nofollow\">example script</a> with this:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">pytorch_metric_learning</span> <span class=\"kn\">import</span> <span class=\"n\">losses</span><span class=\"p\">,</span> <span class=\"n\">miners</span>\n\n<span class=\"c1\"># your custom loss function</span>\n<span class=\"k\">class</span> <span class=\"nc\">YourLossFunction</span><span class=\"p\">(</span><span class=\"n\">losses</span><span class=\"o\">.</span><span class=\"n\">BaseMetricLossFunction</span><span class=\"p\">):</span>\n   <span class=\"o\">...</span>\n\n<span class=\"c1\"># your custom mining function</span>\n<span class=\"k\">class</span> <span class=\"nc\">YourMiningFunction</span><span class=\"p\">(</span><span class=\"n\">miners</span><span class=\"o\">.</span><span class=\"n\">BaseTupleMiner</span><span class=\"p\">):</span>\n   <span class=\"o\">...</span>\n\n<span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">runner</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"o\">.</span><span class=\"vm\">__dict__</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># make the runner aware of them</span>\n<span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">register</span><span class=\"p\">(</span><span class=\"s2\">\"loss\"</span><span class=\"p\">,</span> <span class=\"n\">YourLossFunction</span><span class=\"p\">)</span>\n<span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">register</span><span class=\"p\">(</span><span class=\"s2\">\"miner\"</span><span class=\"p\">,</span> <span class=\"n\">YourMiningFunction</span><span class=\"p\">)</span>\n<span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n</pre>\n<p>Now you can access your custom classes just like any other class:</p>\n<pre><span class=\"nt\">loss_funcs</span><span class=\"p\">:</span>\n  <span class=\"nt\">metric_loss</span><span class=\"p\">:</span> \n    <span class=\"nt\">YourLossFunction</span><span class=\"p\">:</span>\n\n<span class=\"nt\">mining_funcs</span><span class=\"p\">:</span>\n  <span class=\"nt\">tuple_miner</span><span class=\"p\">:</span>\n    <span class=\"nt\">YourMiningFunction</span><span class=\"p\">:</span>\n</pre>\n<p>If you have a module containing multiple classes and you want to register all those classes, you can simply register the module:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">YourModuleOfLosses</span>\n<span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">register</span><span class=\"p\">(</span><span class=\"s2\">\"loss\"</span><span class=\"p\">,</span> <span class=\"n\">YourModuleOfLosses</span><span class=\"p\">)</span>\n</pre>\n<p>Registering your own trainer is a bit more involved, because you need to also create an associated API parser. The name of the api parser should be <code>APIParser&lt;name of your training method&gt;</code>.</p>\n<p>Here's an example where I make a trainer that extends <code>trainers.MetricLossOnly</code>, and takes in an additional argument <code>foo</code>. In order to pass this in, the API parser needs to add <code>foo</code> to the trainer kwargs, and this is done in the <code>get_trainer_kwargs</code> method.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">pytorch_metric_learning</span> <span class=\"kn\">import</span> <span class=\"n\">trainers</span>\n<span class=\"kn\">from</span> <span class=\"nn\">powerful_benchmarker</span> <span class=\"kn\">import</span> <span class=\"n\">api_parsers</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">YourTrainer</span><span class=\"p\">(</span><span class=\"n\">trainers</span><span class=\"o\">.</span><span class=\"n\">MetricLossOnly</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">foo</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n\t<span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n\t<span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">foo</span> <span class=\"o\">=</span> <span class=\"n\">foo</span>\n\t<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"foo = \"</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">foo</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">APIYourTrainer</span><span class=\"p\">(</span><span class=\"n\">api_parsers</span><span class=\"o\">.</span><span class=\"n\">BaseAPIParser</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">get_foo</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"s2\">\"hello\"</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">get_trainer_kwargs</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"n\">trainer_kwargs</span> <span class=\"o\">=</span> <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">get_trainer_kwargs</span><span class=\"p\">()</span>\n        <span class=\"n\">trainer_kwargs</span><span class=\"p\">[</span><span class=\"s2\">\"foo\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">get_foo</span><span class=\"p\">()</span>\n        <span class=\"k\">return</span> <span class=\"n\">trainer_kwargs</span>\n\n<span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">runner</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"o\">.</span><span class=\"vm\">__dict__</span><span class=\"p\">))</span>\n<span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">register</span><span class=\"p\">(</span><span class=\"s2\">\"trainer\"</span><span class=\"p\">,</span> <span class=\"n\">YourTrainer</span><span class=\"p\">)</span>\n<span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">register</span><span class=\"p\">(</span><span class=\"s2\">\"api_parser\"</span><span class=\"p\">,</span> <span class=\"n\">APIYourTrainer</span><span class=\"p\">)</span>\n<span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n</pre>\n<p>If your trainer's <code>__init__</code> arguments are identical to the one you're extending, then you can just set <code>APIYourTrainer = api_parsers.BaseAPIParser</code>. (Note that <code>BaseAPIParser</code> is the same as <code>APIMetricLossOnly</code>)</p>\n<h2>Config options guide</h2>\n<p>Below is the format for the various config files. Click on the links to see the default yaml file for each category.</p>\n<h3><a href=\"https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_general/default.yaml\" rel=\"nofollow\">config_general</a></h3>\n<pre><span class=\"nt\">training_method</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;type&gt;</span> <span class=\"c1\">#options: MetricLossOnly, TrainWithClassifier, CascadedEmbeddings, DeepAdversarialMetricLearning</span>\n<span class=\"nt\">testing_method</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;type&gt;</span> <span class=\"c1\">#options: GlobalEmbeddingSpaceTester, WithSameParentLabelTester</span>\n<span class=\"nt\">meta_testing_method</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;type&gt;</span> <span class=\"c1\">#options: null or ConcatenateEmbeddings</span>\n<span class=\"nt\">dataset</span><span class=\"p\">:</span>  \n  <span class=\"nt\">&lt;type&gt;</span><span class=\"p\">:</span> <span class=\"c1\">#options: CUB200, Cars196, StanfordOnlineProducts</span>\n    <span class=\"nt\">&lt;kwarg&gt;</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;value&gt;</span>\n    <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n<span class=\"nt\">num_epochs_train</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;how long to train for&gt;</span>\n<span class=\"nt\">iterations_per_epoch</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;Optional. If set, an epoch will simply be a fixed number of iterations. Or you can set this to null or 0, and it will be ignored.&gt;</span>\n<span class=\"nt\">save_interval</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;how often (in number of epochs) models will be saved and evaluated&gt;</span>\n<span class=\"nt\">special_split_scheme_name</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;string&gt;</span> <span class=\"c1\">#options: old_approach or predefined. Leave as null if you want to do cross validation.</span>\n<span class=\"nt\">test_size</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;number&gt;</span> <span class=\"c1\">#number in (0, 1), which is the percent of classes that will be used in the test set.</span>\n<span class=\"nt\">test_start_idx</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;number&gt;</span> <span class=\"c1\">#number in (0, 1), which is the percent that specifies the starting class index for the test set</span>\n<span class=\"nt\">num_training_partitions</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;int&gt;</span> <span class=\"c1\">#number of partitions (excluding the test set) that are created for cross validation.</span>\n<span class=\"nt\">num_training_sets</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;int&gt;</span> <span class=\"c1\">#number of partitions that are actually used as training sets cross validation.</span>\n\n<span class=\"nt\">label_hierarchy_level</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;number&gt;</span>\n<span class=\"nt\">dataloader_num_workers</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;number&gt;</span>\n<span class=\"nt\">check_untrained_accuracy</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;boolean&gt;</span>\n<span class=\"nt\">patience</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;int&gt;</span> <span class=\"c1\">#Training will stop if validation accuracy has not improved after this number of epochs. If null, then it is ignored.</span>\n</pre>\n<h3><a href=\"https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_models/default.yaml\" rel=\"nofollow\">config_models</a></h3>\n<pre><span class=\"nt\">models</span><span class=\"p\">:</span>\n  <span class=\"nt\">trunk</span><span class=\"p\">:</span>\n    <span class=\"nt\">&lt;type&gt;</span><span class=\"p\">:</span>\n      <span class=\"nt\">&lt;kwarg&gt;</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;value&gt;</span>\n      <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n  <span class=\"nt\">embedder</span><span class=\"p\">:</span>\n    <span class=\"nt\">&lt;type&gt;</span><span class=\"p\">:</span>\n      <span class=\"nt\">&lt;kwarg&gt;</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;value&gt;</span>\n      <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n<span class=\"nt\">batch_size</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;number&gt;</span>\n<span class=\"nt\">freeze_batchnorm</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;boolean&gt;</span>\n</pre>\n<h3><a href=\"https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_loss_and_miners/default.yaml\" rel=\"nofollow\">config_loss_and_miners</a></h3>\n<pre><span class=\"nt\">loss_funcs</span><span class=\"p\">:</span>\n  <span class=\"nt\">&lt;name&gt;</span><span class=\"p\">:</span> \n    <span class=\"nt\">&lt;type&gt;</span><span class=\"p\">:</span>\n      <span class=\"nt\">&lt;kwarg&gt;</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;value&gt;</span>\n      <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n  <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n\n<span class=\"nt\">sampler</span><span class=\"p\">:</span>\n  <span class=\"nt\">&lt;type&gt;</span><span class=\"p\">:</span>\n    <span class=\"nt\">&lt;kwarg&gt;</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;value&gt;</span>\n    <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n\n<span class=\"nt\">mining_funcs</span><span class=\"p\">:</span>\n  <span class=\"nt\">&lt;name&gt;</span><span class=\"p\">:</span> \n    <span class=\"nt\">&lt;type&gt;</span><span class=\"p\">:</span> \n      <span class=\"nt\">&lt;kwarg&gt;</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;value&gt;</span>\n      <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n  <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n</pre>\n<h3><a href=\"https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_optimizers/default.yaml\" rel=\"nofollow\">config_optimizers</a></h3>\n<pre><span class=\"nt\">optimizers</span><span class=\"p\">:</span>\n  <span class=\"nt\">trunk_optimizer</span><span class=\"p\">:</span>\n    <span class=\"nt\">&lt;type&gt;</span><span class=\"p\">:</span>\n      <span class=\"nt\">&lt;kwarg&gt;</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;value&gt;</span>\n      <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n  <span class=\"nt\">embedder_optimizer</span><span class=\"p\">:</span>\n    <span class=\"nt\">&lt;type&gt;</span><span class=\"p\">:</span>\n      <span class=\"nt\">&lt;kwarg&gt;</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;value&gt;</span>\n      <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n  <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n</pre>\n<h3><a href=\"https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_transforms/default.yaml\" rel=\"nofollow\">config_transforms</a></h3>\n<pre><span class=\"nt\">transforms</span><span class=\"p\">:</span>\n  <span class=\"nt\">train</span><span class=\"p\">:</span>\n    <span class=\"l l-Scalar l-Scalar-Plain\">&lt;type&gt;</span>\n      <span class=\"l l-Scalar l-Scalar-Plain\">&lt;kwarg&gt;</span><span class=\"p p-Indicator\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;value&gt;</span>\n      <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n    <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n\n  <span class=\"nt\">eval</span><span class=\"p\">:</span>\n    <span class=\"l l-Scalar l-Scalar-Plain\">&lt;type&gt;</span>\n      <span class=\"l l-Scalar l-Scalar-Plain\">&lt;kwarg&gt;</span><span class=\"p p-Indicator\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;value&gt;</span>\n      <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n    <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n</pre>\n<h3><a href=\"https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_eval/default.yaml\" rel=\"nofollow\">config_eval</a></h3>\n<pre><span class=\"nt\">eval_reference_set</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;name&gt;</span> <span class=\"c1\">#options: compared_to_self, compared_to_sets_combined, compared_to_training_set</span>\n<span class=\"nt\">eval_normalize_embeddings</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;boolean&gt;</span>\n<span class=\"nt\">eval_use_trunk_output</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;boolean&gt;</span>\n<span class=\"nt\">eval_batch_size</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;number&gt;</span>\n<span class=\"nt\">eval_metric_for_best_epoch</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;name&gt;</span> <span class=\"c1\">#options: NMI, precision_at_1, r_precision, mean_average_r_precision</span>\n<span class=\"nt\">eval_dataloader_num_workers</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;number&gt;</span>\n<span class=\"nt\">eval_pca</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;number&gt; or null</span> <span class=\"c1\">#options: number of dimensions to reduce embeddings to via PCA, or null if you don't want to use PCA.</span>\n<span class=\"nt\">eval_accuracy_calculator</span><span class=\"p\">:</span>\n    <span class=\"l l-Scalar l-Scalar-Plain\">&lt;type&gt;</span>\n      <span class=\"l l-Scalar l-Scalar-Plain\">&lt;kwarg&gt;</span><span class=\"p p-Indicator\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">&lt;value&gt;</span>\n      <span class=\"l l-Scalar l-Scalar-Plain\">...</span>\n</pre>\n<h2>Acknowledgements</h2>\n<p>Thank you to Ser-Nam Lim at Facebook AI, and my research advisor, Professor Serge Belongie. This project began during my internship at Facebook AI where I received valuable feedback from Ser-Nam, and his team of computer vision and machine learning engineers and research scientists.</p>\n<h2>Citing this library</h2>\n<p>If you'd like to cite powerful-benchmarker in your paper, you can use this bibtex:</p>\n<pre>@misc<span class=\"nb\">{</span>Musgrave2019,\n  author = <span class=\"nb\">{</span>Musgrave, Kevin and Lim, Ser-Nam and Belongie, Serge<span class=\"nb\">}</span>,\n  title = <span class=\"nb\">{</span>Powerful Benchmarker<span class=\"nb\">}</span>,\n  year = <span class=\"nb\">{</span>2019<span class=\"nb\">}</span>,\n  publisher = <span class=\"nb\">{</span>GitHub<span class=\"nb\">}</span>,\n  journal = <span class=\"nb\">{</span>GitHub repository<span class=\"nb\">}</span>,\n  howpublished = <span class=\"nb\">{</span><span class=\"k\">\\url</span><span class=\"nb\">{</span>https://github.com/KevinMusgrave/powerful-benchmarker<span class=\"nb\">}}</span>,\n<span class=\"nb\">}</span>\n</pre>\n\n          </div>"}, "last_serial": 7148064, "releases": {"0.9.0": [{"comment_text": "", "digests": {"md5": "91750eae9081f95ef268ea43ab3c7e32", "sha256": "afee52e4ddd01fda72b7bf50467ef10b4f99c610c7262383dc37adbda3a5f6d8"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.0-py3-none-any.whl", "has_sig": false, "md5_digest": "91750eae9081f95ef268ea43ab3c7e32", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 31565, "upload_time": "2020-03-24T22:14:17", "upload_time_iso_8601": "2020-03-24T22:14:17.354792Z", "url": "https://files.pythonhosted.org/packages/e9/47/3cc47c42ffa335fa8a3051ccf602d83d22ddbcf47e65cceaaa486f04f8e3/powerful_benchmarker-0.9.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "86f2c71bfdf594297c5ed5b35db05f26", "sha256": "7a0b76a8590ff041cb322770dc068e7bc0cb7200608af555f67857512e1d6fa6"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.0.tar.gz", "has_sig": false, "md5_digest": "86f2c71bfdf594297c5ed5b35db05f26", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 31590, "upload_time": "2020-03-24T22:14:19", "upload_time_iso_8601": "2020-03-24T22:14:19.837462Z", "url": "https://files.pythonhosted.org/packages/76/d3/09f66762b1974be840c35007644120590a225eda78f2bdef1df7b59d704c/powerful-benchmarker-0.9.0.tar.gz", "yanked": false}], "0.9.1": [{"comment_text": "", "digests": {"md5": "f0f60ecab78c7960d70f188e6e99bf22", "sha256": "9304c44bcc8669983f719951da7d0960cdbd0c21ae761b0368ca87d6719e4a84"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.1-py3-none-any.whl", "has_sig": false, "md5_digest": "f0f60ecab78c7960d70f188e6e99bf22", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 31701, "upload_time": "2020-03-25T17:19:17", "upload_time_iso_8601": "2020-03-25T17:19:17.709421Z", "url": "https://files.pythonhosted.org/packages/71/44/1c30f0ec66fb7a96900f1306ebc33e5ac25f84ff11914a820e560c9e4b5c/powerful_benchmarker-0.9.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5e2b65173f255bb15f850f2d63a88823", "sha256": "a0ff8ae433c5e39a21cbd66012eeec88a57d997e7cc2a2cd1f69db7cbfd770b0"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.1.tar.gz", "has_sig": false, "md5_digest": "5e2b65173f255bb15f850f2d63a88823", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 31791, "upload_time": "2020-03-25T17:19:19", "upload_time_iso_8601": "2020-03-25T17:19:19.463432Z", "url": "https://files.pythonhosted.org/packages/ca/6b/37c05f77ee8ff40428f1eaa33ce4d697949c8a4ecce7338a776c490154a6/powerful-benchmarker-0.9.1.tar.gz", "yanked": false}], "0.9.11": [{"comment_text": "", "digests": {"md5": "04ed5a06d41b21f642ea3af4e4723ea6", "sha256": "309eeab30b2c48692da10c91154e9a36dd55951f7d4c0b349b9e1162f5362fa1"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.11-py3-none-any.whl", "has_sig": false, "md5_digest": "04ed5a06d41b21f642ea3af4e4723ea6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 41054, "upload_time": "2020-03-25T18:55:03", "upload_time_iso_8601": "2020-03-25T18:55:03.518509Z", "url": "https://files.pythonhosted.org/packages/75/40/557981672f1ea38fbc654fef0f8ca7d389be75c2903f3585665d961266d7/powerful_benchmarker-0.9.11-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d143e8acc73572a1341d8f0eda80c159", "sha256": "d6872a45f010819d689ec5a785eaab964defcee708acca60123f1ef19324b7b6"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.11.tar.gz", "has_sig": false, "md5_digest": "d143e8acc73572a1341d8f0eda80c159", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 34308, "upload_time": "2020-03-25T18:55:04", "upload_time_iso_8601": "2020-03-25T18:55:04.688064Z", "url": "https://files.pythonhosted.org/packages/22/5b/7239d43829547203066f1e0743a4e32e9ae9b7f03981f6dbe2737482fd0d/powerful-benchmarker-0.9.11.tar.gz", "yanked": false}], "0.9.12": [{"comment_text": "", "digests": {"md5": "284e8e4e5977a57076013fac01237dce", "sha256": "4e3477cc63ebfa978a9c453996b3f31dd024f53a71182ca0d10f30c050f62576"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.12-py3-none-any.whl", "has_sig": false, "md5_digest": "284e8e4e5977a57076013fac01237dce", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 41125, "upload_time": "2020-03-26T15:35:14", "upload_time_iso_8601": "2020-03-26T15:35:14.423622Z", "url": "https://files.pythonhosted.org/packages/02/0c/4c76d0ac26b393a97931ca8ab46a7e16e7a3a2906ef6df3b205d598c72bd/powerful_benchmarker-0.9.12-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0f75d117cd639be2d0736e1b621a9d1c", "sha256": "86a2ee797275c8073398377c55f2dfc3bdd455798ad8494b1cd25c7fcee0ae82"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.12.tar.gz", "has_sig": false, "md5_digest": "0f75d117cd639be2d0736e1b621a9d1c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 34369, "upload_time": "2020-03-26T15:35:15", "upload_time_iso_8601": "2020-03-26T15:35:15.693639Z", "url": "https://files.pythonhosted.org/packages/50/a5/047ae8b655cf34384400f703f7462080b76613fe9c7d470209663d945896/powerful-benchmarker-0.9.12.tar.gz", "yanked": false}], "0.9.13": [{"comment_text": "", "digests": {"md5": "89fc54563a19ecf4558c2c2ed6e0716b", "sha256": "941181a70158d1c3fb097e1a093efb42d9e3f804895e843a161abed359328919"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.13-py3-none-any.whl", "has_sig": false, "md5_digest": "89fc54563a19ecf4558c2c2ed6e0716b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 42939, "upload_time": "2020-03-27T00:03:20", "upload_time_iso_8601": "2020-03-27T00:03:20.996549Z", "url": "https://files.pythonhosted.org/packages/9a/3c/499378f309b6e8c3e6e2236c26eca2bb58a554d242f08094084e23b80901/powerful_benchmarker-0.9.13-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "379110f09c4805320946b9d60fa08fa5", "sha256": "714e03e0fa35c100a6236ba65ddd9f43d1041afb204ffe334708c22c7d72b1eb"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.13.tar.gz", "has_sig": false, "md5_digest": "379110f09c4805320946b9d60fa08fa5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 34917, "upload_time": "2020-03-27T00:03:22", "upload_time_iso_8601": "2020-03-27T00:03:22.673482Z", "url": "https://files.pythonhosted.org/packages/ee/e3/2ce989c1e5f2d3ee3999ae6699421a025f2c5edd2d97121804a3ce5ba306/powerful-benchmarker-0.9.13.tar.gz", "yanked": false}], "0.9.14": [{"comment_text": "", "digests": {"md5": "42ffb2ff10ba40ab3f34fb643d62fd6c", "sha256": "6ba7dff0695631363ab611ab791f2adcac5a900598229d9a5ddb726d4ba5db6e"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.14-py3-none-any.whl", "has_sig": false, "md5_digest": "42ffb2ff10ba40ab3f34fb643d62fd6c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 39353, "upload_time": "2020-03-30T04:06:15", "upload_time_iso_8601": "2020-03-30T04:06:15.243916Z", "url": "https://files.pythonhosted.org/packages/5d/63/e28cc2716a8607d41834b2421749d75775f90cb6ddd1c96379a9c7e41a2b/powerful_benchmarker-0.9.14-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8899663a1fd1fd334029053312f251c0", "sha256": "a84c5da58d031930cc61016e9418aa05a2c2160fd90ac13615e2a00c981bf331"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.14.tar.gz", "has_sig": false, "md5_digest": "8899663a1fd1fd334029053312f251c0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 35947, "upload_time": "2020-03-30T04:06:16", "upload_time_iso_8601": "2020-03-30T04:06:16.365195Z", "url": "https://files.pythonhosted.org/packages/de/cf/5fd9ad7916af2c2fe86ab5b19577dedf6a5fca63b85ab14dfa1223cb220d/powerful-benchmarker-0.9.14.tar.gz", "yanked": false}], "0.9.15": [{"comment_text": "", "digests": {"md5": "9bb59a5164c1ecd6471eda96a24155b1", "sha256": "515581531061c49fad8305b1cf861f4833a798b2361e1419cc7bc84e95af4995"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.15-py3-none-any.whl", "has_sig": false, "md5_digest": "9bb59a5164c1ecd6471eda96a24155b1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 40164, "upload_time": "2020-04-06T14:54:39", "upload_time_iso_8601": "2020-04-06T14:54:39.164720Z", "url": "https://files.pythonhosted.org/packages/9e/26/268f4aed251abc37af7fbec5e54276951db482fcf64b532731902f5ef256/powerful_benchmarker-0.9.15-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ca2f68b072345ada947ab96f8edc2b61", "sha256": "f0e85d0452a7e80dc13fa2fbece739f4762ae56b55d23ba5f19440b1b70ee68a"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.15.tar.gz", "has_sig": false, "md5_digest": "ca2f68b072345ada947ab96f8edc2b61", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 35828, "upload_time": "2020-04-06T14:54:40", "upload_time_iso_8601": "2020-04-06T14:54:40.283578Z", "url": "https://files.pythonhosted.org/packages/89/3b/4d6631771829a7ec75fa471887c738fd90afb8e56b6926b1afbbacd9b292/powerful-benchmarker-0.9.15.tar.gz", "yanked": false}], "0.9.16": [{"comment_text": "", "digests": {"md5": "20975ea3996bd1282bbbaa88b9e558a5", "sha256": "fe3c2924c0612b0d4ff31b70906b606abe1bd52869f0088f6af6f42bcedf382e"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.16-py3-none-any.whl", "has_sig": false, "md5_digest": "20975ea3996bd1282bbbaa88b9e558a5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 40137, "upload_time": "2020-04-06T16:38:44", "upload_time_iso_8601": "2020-04-06T16:38:44.152534Z", "url": "https://files.pythonhosted.org/packages/41/b2/5b08b8fe059bfa5415c46aced9ad2936e16d957546a552fe8a50da656c51/powerful_benchmarker-0.9.16-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d6d4a257e2a3a8a2ba7f4092efb6c311", "sha256": "f4f93cc5a5a7ed91ac57bdc474fce1c490e513e2c6f5933f666b0448bd0123c5"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.16.tar.gz", "has_sig": false, "md5_digest": "d6d4a257e2a3a8a2ba7f4092efb6c311", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 34874, "upload_time": "2020-04-06T16:38:45", "upload_time_iso_8601": "2020-04-06T16:38:45.728350Z", "url": "https://files.pythonhosted.org/packages/48/aa/4431f794c9ab050763117383fdb3d9a44b4312d37d9835ec399e469b378c/powerful-benchmarker-0.9.16.tar.gz", "yanked": false}], "0.9.17": [{"comment_text": "", "digests": {"md5": "d92c5bf10139a9303e9ee3a7d75ac44d", "sha256": "2efece4258e824d12ae8458eae6a73f0523c30dbeb623a31df9c2733f8c74e18"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.17-py3-none-any.whl", "has_sig": false, "md5_digest": "d92c5bf10139a9303e9ee3a7d75ac44d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 40484, "upload_time": "2020-04-08T16:42:44", "upload_time_iso_8601": "2020-04-08T16:42:44.616039Z", "url": "https://files.pythonhosted.org/packages/e9/14/c91a99a65e09ef1abe032926c4e69654937c9eea3e275fd7a15524b85cc4/powerful_benchmarker-0.9.17-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1aebf2fb50b9a149013a8759fd3f471d", "sha256": "383b34b44181aeb3c4239b604a5bb2240507720bed1fe2b81c3007ca2db26f32"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.17.tar.gz", "has_sig": false, "md5_digest": "1aebf2fb50b9a149013a8759fd3f471d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 35207, "upload_time": "2020-04-08T16:42:45", "upload_time_iso_8601": "2020-04-08T16:42:45.933915Z", "url": "https://files.pythonhosted.org/packages/63/ab/e935bcc01ef2decb32524411246d5c41095a7db611e828f71e066618a057/powerful-benchmarker-0.9.17.tar.gz", "yanked": false}], "0.9.18": [{"comment_text": "", "digests": {"md5": "63841a3a4643ba175f06154d67ffb3bd", "sha256": "864d680ae44bde9a6685e995f064bf10b145f272dbb28a226c0e93e95887c62c"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.18-py3-none-any.whl", "has_sig": false, "md5_digest": "63841a3a4643ba175f06154d67ffb3bd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 40537, "upload_time": "2020-04-10T00:03:22", "upload_time_iso_8601": "2020-04-10T00:03:22.102188Z", "url": "https://files.pythonhosted.org/packages/12/00/065b299e5066c7ceb20c8e63bb88953c0b288937b115dcfc9634bd36f29d/powerful_benchmarker-0.9.18-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8a582f7f507b6179cdd8348c090efd17", "sha256": "2337e59608daa30ad24d3bfeb356bcbaabb9af7455fb1e7eb824b2433e483fa8"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.18.tar.gz", "has_sig": false, "md5_digest": "8a582f7f507b6179cdd8348c090efd17", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 35240, "upload_time": "2020-04-10T00:03:23", "upload_time_iso_8601": "2020-04-10T00:03:23.266546Z", "url": "https://files.pythonhosted.org/packages/58/45/e6cb9731a95991734116cfcdbaafca05db6a2b87d04995ab00f236d36713/powerful-benchmarker-0.9.18.tar.gz", "yanked": false}], "0.9.19": [{"comment_text": "", "digests": {"md5": "19b6ce8c3c18449586b23e0915fbbc23", "sha256": "2ff50d4032938ab9c00e5c2680d8c1b5df6c6079093c421abd3dde782990bf03"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.19-py3-none-any.whl", "has_sig": false, "md5_digest": "19b6ce8c3c18449586b23e0915fbbc23", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 41026, "upload_time": "2020-04-11T09:44:37", "upload_time_iso_8601": "2020-04-11T09:44:37.437735Z", "url": "https://files.pythonhosted.org/packages/04/da/641cf4662d2ed6dcab24633c396a7d9fe80ec79096b400c0942f6d8df8fb/powerful_benchmarker-0.9.19-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "60ffc0d1f450abcc667ddefc39c40db8", "sha256": "8cf5f07f761ffc522aea7f89a203c56b84fa0999ac9eac2747939eeac8398ecf"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.19.tar.gz", "has_sig": false, "md5_digest": "60ffc0d1f450abcc667ddefc39c40db8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 36263, "upload_time": "2020-04-11T09:44:39", "upload_time_iso_8601": "2020-04-11T09:44:39.112734Z", "url": "https://files.pythonhosted.org/packages/f7/ac/39714cb4dc3ccede4f30f42f368e039c505b3960ad86b3a398b07ba850d6/powerful-benchmarker-0.9.19.tar.gz", "yanked": false}], "0.9.20": [{"comment_text": "", "digests": {"md5": "bdde000a99585fb2c14e675477160edf", "sha256": "4e3251d8ca2dab016c17cf1d5a3a758d3b6b1aef7b23644fe4435a282e7deb5e"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.20-py3-none-any.whl", "has_sig": false, "md5_digest": "bdde000a99585fb2c14e675477160edf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 41089, "upload_time": "2020-04-13T22:59:50", "upload_time_iso_8601": "2020-04-13T22:59:50.984482Z", "url": "https://files.pythonhosted.org/packages/ad/4f/8a839c06e1ec5b17c70fac66be15d2772493d22648070807a3ffa1358f54/powerful_benchmarker-0.9.20-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7011379a7f2860024a780cc49cc5bd75", "sha256": "9b1ebfb765c79958b4120625a64694d57095cb3d2500e93a95877eb6e82c4205"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.20.tar.gz", "has_sig": false, "md5_digest": "7011379a7f2860024a780cc49cc5bd75", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 36297, "upload_time": "2020-04-13T22:59:52", "upload_time_iso_8601": "2020-04-13T22:59:52.149366Z", "url": "https://files.pythonhosted.org/packages/7b/17/9bcae2c7f94a9979242b168b62241f8166025cff3984028138b83f3924ae/powerful-benchmarker-0.9.20.tar.gz", "yanked": false}], "0.9.21": [{"comment_text": "", "digests": {"md5": "1d273970b39258c0d3d2d7bd4538b08d", "sha256": "f24911958eb56204093518369943ca5ed4505b16c45d7c8095d2c29e3a0eb7cd"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.21-py3-none-any.whl", "has_sig": false, "md5_digest": "1d273970b39258c0d3d2d7bd4538b08d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 41035, "upload_time": "2020-05-01T20:37:29", "upload_time_iso_8601": "2020-05-01T20:37:29.827602Z", "url": "https://files.pythonhosted.org/packages/d8/25/55f89c680843c6c0defbc1141ae76e61922860af242abd145a2fdc4195db/powerful_benchmarker-0.9.21-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e2b36135ed8c4dac51aac408026d7b95", "sha256": "9eea99f4ab9879ebd22ec4f50caf9fab01b6a1413fba2cb6dcae28c48eb9a7c4"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.21.tar.gz", "has_sig": false, "md5_digest": "e2b36135ed8c4dac51aac408026d7b95", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 36206, "upload_time": "2020-05-01T20:37:31", "upload_time_iso_8601": "2020-05-01T20:37:31.343372Z", "url": "https://files.pythonhosted.org/packages/f4/1e/5295eb419388d5a5b071d9b9c2d8759d00ae6662a98d4d3e66cbc3260d3a/powerful-benchmarker-0.9.21.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1d273970b39258c0d3d2d7bd4538b08d", "sha256": "f24911958eb56204093518369943ca5ed4505b16c45d7c8095d2c29e3a0eb7cd"}, "downloads": -1, "filename": "powerful_benchmarker-0.9.21-py3-none-any.whl", "has_sig": false, "md5_digest": "1d273970b39258c0d3d2d7bd4538b08d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 41035, "upload_time": "2020-05-01T20:37:29", "upload_time_iso_8601": "2020-05-01T20:37:29.827602Z", "url": "https://files.pythonhosted.org/packages/d8/25/55f89c680843c6c0defbc1141ae76e61922860af242abd145a2fdc4195db/powerful_benchmarker-0.9.21-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e2b36135ed8c4dac51aac408026d7b95", "sha256": "9eea99f4ab9879ebd22ec4f50caf9fab01b6a1413fba2cb6dcae28c48eb9a7c4"}, "downloads": -1, "filename": "powerful-benchmarker-0.9.21.tar.gz", "has_sig": false, "md5_digest": "e2b36135ed8c4dac51aac408026d7b95", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 36206, "upload_time": "2020-05-01T20:37:31", "upload_time_iso_8601": "2020-05-01T20:37:31.343372Z", "url": "https://files.pythonhosted.org/packages/f4/1e/5295eb419388d5a5b071d9b9c2d8759d00ae6662a98d4d3e66cbc3260d3a/powerful-benchmarker-0.9.21.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:20:44 2020"}