{"info": {"author": "Adrian Garcia Badaracco", "author_email": "adrian@adriangb.com", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "# Scikit-Learn Wrapper for Keras\n\n\n[![build status](https://secure.travis-ci.org/adriangb/sklearn_keras_wrap.png?branch=master)](https://travis-ci.org/geopandas/geopandas) [![Coverage Status](https://codecov.io/gh/adriangb/sklearn_keras_wrap/branch/master/graph/badge.svg)](https://codecov.io/gh/geopandas/geopandas)\n\n\nThe goal of this project is to provide wrappers for Keras models so that they can be used as part of a Scikit-Learn workflow. These wrappers seeek to emulate the base classes found in `sklearn.base`. Three wrappers are provided:\n\n1. `BaseWrapper`: basic implementation that wraps Keras models for use with Scikit-Learn workflows. Inherit from this wrapper to build other types of estimators, ex a [Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html). Refer to `KerasClassifier` and `KerasRegressor` for inspiration.\n2. `KerasClassifier`: implements the Scikit-Learn classifier interface, akin to `sklearn.base.ClassifierMixin`. By default, scoring is done using `sklearn.metrics.accuracy_score`.\n3. `KerasRegressor`: implements the Scikit-Learn classifier interface, akin to `sklearn.base.RegressorMixin`. By default, scoring is done using `sklearn.metrics.r2_score`. Note that `Keras` does *not* have R2 as a built in loss function. A basic implementation of a `Keras` compatible R2 loss funciton is provided in `KerasRegressor.root_mean_squared_error`.\n\n\nThis project was originally part of Keras itself, but to simplify maintenence and implementation it is now hosted in this repository.\n\nLearn more about the [Scikit-Learn API](https://scikit-learn.org/stable/modules/classes.html).\n\nLearn more about [Keras](https://www.tensorflow.org/guide/keras), TensorFlow's Python API.\n\nPython versions supported: the maximum overlap between `Scikit-Learn` and `TensorFlow`. Currently, this means `Python >=3.5 and <=3.7`.\n\n## Basic Usage\n\nTo use the wrappers, you must specify how to build the `Keras` model. The wrappers support both [`Sequential`](https://www.tensorflow.org/guide/keras/overview) and [`Functional API`](https://www.tensorflow.org/guide/keras/functional) models. There are 2 basic options to specify how the model is built:\n1. Prebuilt model: pass an existing `Keras` model object to the wrapper, which will be copied to avoid modifying the existing model. You must pass the prebuilt model via the `build_fn` parameter when initializing the wrapper.\n2. Dynamically built model: pass a function that returns a model object. More details below.\n\n### Dynamically built models.\nThere are 2 ways to specify a model building function:\n1. Pass a callable function or an instance of a class implementing `__call__` as the `build_fn` parameter.\n2. Subclass the wrapper and implement `__call__` in your class.\n\nThe logic for selecting which method to use is in `BaseWrapper._check_build_fn`. For either method, the ultimate function used to build the model is stored by reference in `BaseWrapper.__call__`. From now on this will be refered to as `model building function`.\n\nThe signature of the model building function will be used to dynamically determine which parameters should be passed. Parameters are chosen from the arguments of `fit` and from the public parameters of the wrapper instance (ex: `n_classes_` or `n_outputs_`). For example, to create a Multi Layer Perceptron model that is able to dynamically set the input and output sizes as well as hidden layer sizes, you would add `X` and `n_outputs_` to your model building function's signature:\n\n```python3\nfrom sklearn_keras_wrap.wrappers import KerasRegressor\n\n\ndef model_building_function(X, n_outputs_, hidden_layer_sizes):\n    \"\"\"Dynamically build regressor.\"\"\"\n    model = Sequential()\n    model.add(Dense(X.shape[1], activation=\"relu\", input_shape=X.shape[1:]))\n    for size in hidden_layer_sizes:\n        model.add(Dense(size, activation=\"relu\"))\n    model.add(Dense(n_outputs_))\n    model.compile(\"adam\", loss=\"mean_squared_error\")\n    return model\n\nestimator = KerasRegressor(build_fn=model_building_function, hidden_layer_sizes=[200, 100])\n\nestimator.fit(X, y)\nestimator.score(X, y)\n```\n\nNote that in this example, hidden_layer_sizes was specified as an keyword argument to `Keras Regressor`. The value of this keyword argument will be stored as `estimator.hidden_layer_sizes` and will be available to the Scikit-Learn API for use with `sklearn.model_selection.GridSearchCV` and other hyperparameter tuning methods. Because `hidden_layer_sizes` is a public attribute of `estimator` as well as an argument to `model_build_function`, it's value will be passed to `model_build_function` when `fit` is called.\n\nAlso note that the input itself, `X` was passed. Finally, `n_outputs_` is generated by `KerasRegressor` and `KerasClassifier` when `fit` is called and is stored as a public attribute in `estimator.n_outputs_`. Because this is a public attribute of `estimator`, `model_build_function` can request it simply by having a parameter with that name.\n\nThe model parameters generated while fitting that are used by various parts of the Scikit-Learn API are:\n* `n_outputs`: number of outputs. For regression, this is always `y.shape[1]`.\n* `n_classes_`: The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems).\n* `classes_`: The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).\n\n### Subclassing wrappers\nIt may be convenient to subclass a wrapper to hardcode keyword arguments and defaults. In general, this is more compatible with the Scikit-Learn API. If the class also implements the model building function as `__call__`, this becaome a self-contained estimator that is fully compatible with the Scikit-Learn API. A brief example:\n\n```python3\nfrom sklearn_keras_wrap.wrappers import KerasRegressor\n\n\nclass MLPRegressor(KerasRegressor):\n\n    def __init__(self, hidden_layer_sizes=None):\n        self.hidden_layer_sizes = hidden_layer_sizes\n        super().__init__()   # this is very important!\n\n    def __call__(self, X, n_outputs_, hidden_layer_sizes):\n        \"\"\"Dynamically build regressor.\"\"\"\n        if hidden_layer_sizes is None:\n            hidden_layer_sizes = (100, )\n        model = Sequential()\n        model.add(Dense(X.shape[1], activation=\"relu\", input_shape=X.shape[1:]))\n        for size in hidden_layer_sizes:\n            model.add(Dense(size, activation=\"relu\"))\n        model.add(Dense(n_outputs_))\n        model.compile(\"adam\", loss=KerasRegressor.root_mean_squared_error)\n        return model\n```\n\nA couple of notes:\n1. It is very important to call super().__init__() to properly register kwargs and the model building function.\n2. You must assign all parameters to a public attribute of the same name and should *not* change it's value. To change the value from a default you can either (1) change the value in the model building function (as above) or save the parameter under another name (ex: `_hidden_layer_sizes`, remember to also use this name in the model building function).\n3. You should set a default for all tunable arguments (in this case, `hidden_layer_sizes=None`) as this is expected by the Scikit-Learn API.\n\n### Passing arguments to Keras methods\n\nThis section refers to passing arguments to `fit`, `predict`, `predict_proba`, and `score` methods of `Keras` models (e.g., `epochs`, `batch_size`),\n\nThere are 2 ways to pass these argumements:\n1. Pass directly to `KerasClassifier.fit` or `KerasRegressor.fit` (or `score`, etc.).\n2. Pass as a keyword argument when initalizing `KerasClassifier` or `KerasRegressor`. This will allow the parameter to be tunable by the Scikit-Learn hyperparameter tuning API (`GridSearchCV` or `RandomizedSearchCV`).\n\n## Advanced Usage\n\n### Multi-output problems\nScikit-Learn supports a limited number of multi-ouput problems and does not support any multi-input problems. See [`Multiclass and multilabel algorithms`](https://scikit-learn.org/stable/modules/multiclass.html) for more details.\n\nThese wrappers suppport all of the multi-output types that Scikit-Learn supports out of the box. So for example, you can create a model that has multiple `sigmoid` output layers, resulting in a multiple binary classification problem. This type of problem is denoted `multilabel classification` in Scikit-Learn. Another example is a model with multiple `softmax` outputs, resulting in what is known as a `multiouput-multiclass` classification. There are obviously many ways to pair up a `Keras` model with a Scikit-Learn output type. Conversion to and from `Keras` outputs are done in the wrappers `_pre_process_y` and `_post_process_y` methods. The signatures are:\n\n### `_pre_process_y`\nSignature: `_pre_process_y(y: np.array) -> np.array, dict`\nInputs:\n* `y` always a single `numpy.array`\nOutputs:\n* `y`: a single `numpy.array` for a single `Keras` output or a list of `numpy.array` for a `Keras` model with multiple outputs\n*  `extra_args`: a dictionary containing extra parameters determined within `_pre_process_y` such as `classes_`. If used within `fit`, these parameters will overwrite instance parameters of the same name.\n\n### `_post_process_y`\nSignature: `_post_process_y(y: np.array) -> np.array, dict`\nInputs:\n* `y` raw output form the `Keras` model's `predict` method.\nOutputs:\n* `y`: a single `numpy.array`. For classificaiton, this should contain class predictions.\n*  `extra_args`: for regression, this parameter is unused. For classification, this parameter contains prediction probabilities.\n\nTo support a custom mapping from `Keras` to `Scikit-Learn`, you can subclass a wrapper and modify `_pre_process_y` and `_post_process_y`. For example, to support a mixed  binary/multiclass classification:\n\n```python3\nclass FunctionalAPIMultiOutputClassifier(KerasClassifier):\n    \"\"\"Functional API Classifier with 2 outputs of different type.\n    \"\"\"\n\n    def __call__(self, X, n_classes_):\n        inp = Input((4,))\n\n        x1 = Dense(100)(inp)\n\n        binary_out = Dense(1, activation=\"sigmoid\")(x1)\n        cat_out = Dense(n_classes_[1], activation=\"softmax\")(x1)\n\n        model = Model([inp], [binary_out, cat_out])\n        losses = [\"binary_crossentropy\", \"categorical_crossentropy\"]\n        model.compile(optimizer=\"adam\", loss=losses, metrics=[\"accuracy\"])\n\n        return model\n\n    def _post_process_y(self, y):\n        \"\"\"To support targets of different type, we need to post-precess each one\n           manually, there is no way to determine the types accurately.\n\n           Takes KerasClassifier._post_process_y as a starting point and\n           hardcodes the post-processing.\n        \"\"\"\n        classes_ = self.classes_\n\n        class_predictions = [\n            classes_[0][np.where(y[0] > 0.5, 1, 0)],\n            classes_[1][np.argmax(y[1], axis=1)],\n        ]\n\n        class_probabilities = np.squeeze(np.column_stack(y))\n\n        y = np.squeeze(np.column_stack(class_predictions))\n\n        extra_args = {\"class_probabilities\": class_probabilities}\n\n        return y, extra_args\n\n    def score(self, X, y):\n        \"\"\"Taken from sklearn.multiouput.MultiOutputClassifier\n        \"\"\"\n        y_pred = self.predict(X)\n        return np.mean(np.all(y == y_pred, axis=1))\n```\n\nThe default implementation of `_pre_process_y` for `KerasClassifier` attempts to automatically determine the type of problem using `sklearn.utils.multiclass.type_of_target`. You may need to override this method if it is unable to determine the correct type for your data. The default implementation is provided as a static method so that you can test it without needing to instantiate a `KerasClassifier`.\n\n### Multi-input problems\n\nAs mentioned above, Scikit-Learn does not support multi-input problems since `X` must be a sinlge `numpy.array`. However, in order to extend this functionality, the wrappers provide a `_pre_process_X` method that allows mapping a single `numpy.arary` to a multi-input `Keras` model. For example:\n\n```python3\nclass FunctionalAPIMultiInputClassifier(KerasClassifier):\n    \"\"\"Functional API Classifier with 2 inputs.\n    \"\"\"\n\n    def __call__(self, n_classes_):\n        inp1 = Input((1,))\n        inp2 = Input((3,))\n\n        x1 = Dense(100)(inp1)\n        x2 = Dense(100)(inp2)\n\n        x3 = Concatenate(axis=-1)([x1, x2])\n\n        cat_out = Dense(n_classes_, activation=\"softmax\")(x3)\n\n        model = Model([inp1, inp2], [cat_out])\n        losses = [\"categorical_crossentropy\"]\n        model.compile(optimizer=\"adam\", loss=losses, metrics=[\"accuracy\"])\n\n        return model\n\n    @staticmethod  # _pre_process_X does not *need* to be a static method\n    def _pre_process_X(X):\n        \"\"\"To support multiple inputs, a custom method must be defined.\n        \"\"\"\n        return [X[:, 0], X[:, 1:4]], dict()\n```\n\nNote that similar to `_pre_process_y`, `_pre_process_X` returns the modified `X` along with a dictionary of extra parameters. This dictionary is currently unused, but is kept for symmetry with `_pre_process_x` and future flexibility.\n\n### Custom scorers\nTo override the function used for scoring, set the `_scorer` attribute of the wrapper to point to a scoring function with the signature `scorer(y_true: np.array, y_pred: np.array) -> float`.\n\n## Contributing\nContributions are very welcome. Please open an issue to ask for new features or preferable a PR to propose an implementation.\n\nIf submitting a PR, please make sure that:\n\n- All existing tests should pass. Please make sure that the test\n  suite passes, both locally and on\n  [Travis CI](https://travis-ci.org/geopandas/geopandas).  Status on\n  Travis will be visible on a pull request.\n\n- New functionality should include tests. Please write reasonable\n  tests for your code and make sure that they pass on your pull request. Testing is done with [`Pytest`](https://docs.pytest.org/en/latest/) and coverage is checked with [`CodeCov`](https://codecov.io/gh/adriangb/sklearn_keras_wrap)\n\n- Classes, methods, functions, etc. should have docstrings. The first line of a docstring should be a standalone summary. Parameters and return values should be documented explicitly.\n\n### Style\n- This project follows [the PEP 8\n  standard](http://www.python.org/dev/peps/pep-0008/) and uses\n  [Black](https://black.readthedocs.io/en/stable/) and\n  [Flake8](http://flake8.pycqa.org/en/latest/) to ensure a consistent\n  code format throughout the project.\n\n- Imports should be grouped with standard library imports first,\n  3rd-party libraries next, and GeoPandas imports third. Within each\n  grouping, imports should be alphabetized. Always use absolute\n  imports when possible, and explicit relative imports for local\n  imports when necessary in tests.\n\n- You can set up [pre-commit hooks](https://pre-commit.com/) to\n  automatically run `black` and `flake8` when you make a git\n  commit. This can be done by installing `pre-commit`:\n\n    $ python -m pip install pre-commit\n\n  From the root of the geopandas repository, you should then install\n  `pre-commit`:\n\n    $ pre-commit install\n\n  Then `black` and `flake8` will be run automatically each time you\n  commit changes. You can skip these checks with `git commit\n  --no-verify`.\n\n### Deployment\nDeployment to PyPi is done automatically by Travis for tagged commits.\nTo release a new version, you can use `bump2version`:\n```bash\nbump2version patch --message \"Tag commit message\"\ngit push --tags\n```\n\n\n#History\n\n## 0.1.4 (2020-04-12)\n\n* Offload output type detection for classification to `sklearn.utils.multiclass.type_of_target`.\n* Add documentation.\n* Some file cleanup.\n\n## 0.1.3 (2020-04-11)\n\n* First release on PyPI.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/adriangb/sklearn_keras_wrap", "keywords": "sklearn_keras_wrap", "license": "MIT license", "maintainer": "", "maintainer_email": "", "name": "sklearn-keras-wrap", "package_url": "https://pypi.org/project/sklearn-keras-wrap/", "platform": "", "project_url": "https://pypi.org/project/sklearn-keras-wrap/", "project_urls": {"Homepage": "https://github.com/adriangb/sklearn_keras_wrap"}, "release_url": "https://pypi.org/project/sklearn-keras-wrap/0.1.4/", "requires_dist": ["scikit-learn (>=0.21.0)", "tensorflow (>=2.1.0)"], "requires_python": ">=3.5", "summary": "Scikit-Learn API implementation for Keras.", "version": "0.1.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Scikit-Learn Wrapper for Keras</h1>\n<p><a href=\"https://travis-ci.org/geopandas/geopandas\" rel=\"nofollow\"><img alt=\"build status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/15a954cc1ccdb309aca71a0186fea8d6e993bec6/68747470733a2f2f7365637572652e7472617669732d63692e6f72672f61647269616e67622f736b6c6561726e5f6b657261735f777261702e706e673f6272616e63683d6d6173746572\"></a> <a href=\"https://codecov.io/gh/geopandas/geopandas\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6df304433a235e4eb57a29af19a9537bb3190401/68747470733a2f2f636f6465636f762e696f2f67682f61647269616e67622f736b6c6561726e5f6b657261735f777261702f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a></p>\n<p>The goal of this project is to provide wrappers for Keras models so that they can be used as part of a Scikit-Learn workflow. These wrappers seeek to emulate the base classes found in <code>sklearn.base</code>. Three wrappers are provided:</p>\n<ol>\n<li><code>BaseWrapper</code>: basic implementation that wraps Keras models for use with Scikit-Learn workflows. Inherit from this wrapper to build other types of estimators, ex a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html\" rel=\"nofollow\">Transformer</a>. Refer to <code>KerasClassifier</code> and <code>KerasRegressor</code> for inspiration.</li>\n<li><code>KerasClassifier</code>: implements the Scikit-Learn classifier interface, akin to <code>sklearn.base.ClassifierMixin</code>. By default, scoring is done using <code>sklearn.metrics.accuracy_score</code>.</li>\n<li><code>KerasRegressor</code>: implements the Scikit-Learn classifier interface, akin to <code>sklearn.base.RegressorMixin</code>. By default, scoring is done using <code>sklearn.metrics.r2_score</code>. Note that <code>Keras</code> does <em>not</em> have R2 as a built in loss function. A basic implementation of a <code>Keras</code> compatible R2 loss funciton is provided in <code>KerasRegressor.root_mean_squared_error</code>.</li>\n</ol>\n<p>This project was originally part of Keras itself, but to simplify maintenence and implementation it is now hosted in this repository.</p>\n<p>Learn more about the <a href=\"https://scikit-learn.org/stable/modules/classes.html\" rel=\"nofollow\">Scikit-Learn API</a>.</p>\n<p>Learn more about <a href=\"https://www.tensorflow.org/guide/keras\" rel=\"nofollow\">Keras</a>, TensorFlow's Python API.</p>\n<p>Python versions supported: the maximum overlap between <code>Scikit-Learn</code> and <code>TensorFlow</code>. Currently, this means <code>Python &gt;=3.5 and &lt;=3.7</code>.</p>\n<h2>Basic Usage</h2>\n<p>To use the wrappers, you must specify how to build the <code>Keras</code> model. The wrappers support both <a href=\"https://www.tensorflow.org/guide/keras/overview\" rel=\"nofollow\"><code>Sequential</code></a> and <a href=\"https://www.tensorflow.org/guide/keras/functional\" rel=\"nofollow\"><code>Functional API</code></a> models. There are 2 basic options to specify how the model is built:</p>\n<ol>\n<li>Prebuilt model: pass an existing <code>Keras</code> model object to the wrapper, which will be copied to avoid modifying the existing model. You must pass the prebuilt model via the <code>build_fn</code> parameter when initializing the wrapper.</li>\n<li>Dynamically built model: pass a function that returns a model object. More details below.</li>\n</ol>\n<h3>Dynamically built models.</h3>\n<p>There are 2 ways to specify a model building function:</p>\n<ol>\n<li>Pass a callable function or an instance of a class implementing <code>__call__</code> as the <code>build_fn</code> parameter.</li>\n<li>Subclass the wrapper and implement <code>__call__</code> in your class.</li>\n</ol>\n<p>The logic for selecting which method to use is in <code>BaseWrapper._check_build_fn</code>. For either method, the ultimate function used to build the model is stored by reference in <code>BaseWrapper.__call__</code>. From now on this will be refered to as <code>model building function</code>.</p>\n<p>The signature of the model building function will be used to dynamically determine which parameters should be passed. Parameters are chosen from the arguments of <code>fit</code> and from the public parameters of the wrapper instance (ex: <code>n_classes_</code> or <code>n_outputs_</code>). For example, to create a Multi Layer Perceptron model that is able to dynamically set the input and output sizes as well as hidden layer sizes, you would add <code>X</code> and <code>n_outputs_</code> to your model building function's signature:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sklearn_keras_wrap.wrappers</span> <span class=\"kn\">import</span> <span class=\"n\">KerasRegressor</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">model_building_function</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">n_outputs_</span><span class=\"p\">,</span> <span class=\"n\">hidden_layer_sizes</span><span class=\"p\">):</span>\n    <span class=\"sd\">\"\"\"Dynamically build regressor.\"\"\"</span>\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">()</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s2\">\"relu\"</span><span class=\"p\">,</span> <span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]))</span>\n    <span class=\"k\">for</span> <span class=\"n\">size</span> <span class=\"ow\">in</span> <span class=\"n\">hidden_layer_sizes</span><span class=\"p\">:</span>\n        <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s2\">\"relu\"</span><span class=\"p\">))</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">n_outputs_</span><span class=\"p\">))</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"s2\">\"adam\"</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s2\">\"mean_squared_error\"</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">model</span>\n\n<span class=\"n\">estimator</span> <span class=\"o\">=</span> <span class=\"n\">KerasRegressor</span><span class=\"p\">(</span><span class=\"n\">build_fn</span><span class=\"o\">=</span><span class=\"n\">model_building_function</span><span class=\"p\">,</span> <span class=\"n\">hidden_layer_sizes</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">200</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">])</span>\n\n<span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n</pre>\n<p>Note that in this example, hidden_layer_sizes was specified as an keyword argument to <code>Keras Regressor</code>. The value of this keyword argument will be stored as <code>estimator.hidden_layer_sizes</code> and will be available to the Scikit-Learn API for use with <code>sklearn.model_selection.GridSearchCV</code> and other hyperparameter tuning methods. Because <code>hidden_layer_sizes</code> is a public attribute of <code>estimator</code> as well as an argument to <code>model_build_function</code>, it's value will be passed to <code>model_build_function</code> when <code>fit</code> is called.</p>\n<p>Also note that the input itself, <code>X</code> was passed. Finally, <code>n_outputs_</code> is generated by <code>KerasRegressor</code> and <code>KerasClassifier</code> when <code>fit</code> is called and is stored as a public attribute in <code>estimator.n_outputs_</code>. Because this is a public attribute of <code>estimator</code>, <code>model_build_function</code> can request it simply by having a parameter with that name.</p>\n<p>The model parameters generated while fitting that are used by various parts of the Scikit-Learn API are:</p>\n<ul>\n<li><code>n_outputs</code>: number of outputs. For regression, this is always <code>y.shape[1]</code>.</li>\n<li><code>n_classes_</code>: The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems).</li>\n<li><code>classes_</code>: The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).</li>\n</ul>\n<h3>Subclassing wrappers</h3>\n<p>It may be convenient to subclass a wrapper to hardcode keyword arguments and defaults. In general, this is more compatible with the Scikit-Learn API. If the class also implements the model building function as <code>__call__</code>, this becaome a self-contained estimator that is fully compatible with the Scikit-Learn API. A brief example:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sklearn_keras_wrap.wrappers</span> <span class=\"kn\">import</span> <span class=\"n\">KerasRegressor</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">MLPRegressor</span><span class=\"p\">(</span><span class=\"n\">KerasRegressor</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">hidden_layer_sizes</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">hidden_layer_sizes</span> <span class=\"o\">=</span> <span class=\"n\">hidden_layer_sizes</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>   <span class=\"c1\"># this is very important!</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__call__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">n_outputs_</span><span class=\"p\">,</span> <span class=\"n\">hidden_layer_sizes</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"Dynamically build regressor.\"\"\"</span>\n        <span class=\"k\">if</span> <span class=\"n\">hidden_layer_sizes</span> <span class=\"ow\">is</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n            <span class=\"n\">hidden_layer_sizes</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"p\">)</span>\n        <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">()</span>\n        <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s2\">\"relu\"</span><span class=\"p\">,</span> <span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]))</span>\n        <span class=\"k\">for</span> <span class=\"n\">size</span> <span class=\"ow\">in</span> <span class=\"n\">hidden_layer_sizes</span><span class=\"p\">:</span>\n            <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s2\">\"relu\"</span><span class=\"p\">))</span>\n        <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">n_outputs_</span><span class=\"p\">))</span>\n        <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"s2\">\"adam\"</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"n\">KerasRegressor</span><span class=\"o\">.</span><span class=\"n\">root_mean_squared_error</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">model</span>\n</pre>\n<p>A couple of notes:</p>\n<ol>\n<li>It is very important to call super().<strong>init</strong>() to properly register kwargs and the model building function.</li>\n<li>You must assign all parameters to a public attribute of the same name and should <em>not</em> change it's value. To change the value from a default you can either (1) change the value in the model building function (as above) or save the parameter under another name (ex: <code>_hidden_layer_sizes</code>, remember to also use this name in the model building function).</li>\n<li>You should set a default for all tunable arguments (in this case, <code>hidden_layer_sizes=None</code>) as this is expected by the Scikit-Learn API.</li>\n</ol>\n<h3>Passing arguments to Keras methods</h3>\n<p>This section refers to passing arguments to <code>fit</code>, <code>predict</code>, <code>predict_proba</code>, and <code>score</code> methods of <code>Keras</code> models (e.g., <code>epochs</code>, <code>batch_size</code>),</p>\n<p>There are 2 ways to pass these argumements:</p>\n<ol>\n<li>Pass directly to <code>KerasClassifier.fit</code> or <code>KerasRegressor.fit</code> (or <code>score</code>, etc.).</li>\n<li>Pass as a keyword argument when initalizing <code>KerasClassifier</code> or <code>KerasRegressor</code>. This will allow the parameter to be tunable by the Scikit-Learn hyperparameter tuning API (<code>GridSearchCV</code> or <code>RandomizedSearchCV</code>).</li>\n</ol>\n<h2>Advanced Usage</h2>\n<h3>Multi-output problems</h3>\n<p>Scikit-Learn supports a limited number of multi-ouput problems and does not support any multi-input problems. See <a href=\"https://scikit-learn.org/stable/modules/multiclass.html\" rel=\"nofollow\"><code>Multiclass and multilabel algorithms</code></a> for more details.</p>\n<p>These wrappers suppport all of the multi-output types that Scikit-Learn supports out of the box. So for example, you can create a model that has multiple <code>sigmoid</code> output layers, resulting in a multiple binary classification problem. This type of problem is denoted <code>multilabel classification</code> in Scikit-Learn. Another example is a model with multiple <code>softmax</code> outputs, resulting in what is known as a <code>multiouput-multiclass</code> classification. There are obviously many ways to pair up a <code>Keras</code> model with a Scikit-Learn output type. Conversion to and from <code>Keras</code> outputs are done in the wrappers <code>_pre_process_y</code> and <code>_post_process_y</code> methods. The signatures are:</p>\n<h3><code>_pre_process_y</code></h3>\n<p>Signature: <code>_pre_process_y(y: np.array) -&gt; np.array, dict</code>\nInputs:</p>\n<ul>\n<li><code>y</code> always a single <code>numpy.array</code>\nOutputs:</li>\n<li><code>y</code>: a single <code>numpy.array</code> for a single <code>Keras</code> output or a list of <code>numpy.array</code> for a <code>Keras</code> model with multiple outputs</li>\n<li><code>extra_args</code>: a dictionary containing extra parameters determined within <code>_pre_process_y</code> such as <code>classes_</code>. If used within <code>fit</code>, these parameters will overwrite instance parameters of the same name.</li>\n</ul>\n<h3><code>_post_process_y</code></h3>\n<p>Signature: <code>_post_process_y(y: np.array) -&gt; np.array, dict</code>\nInputs:</p>\n<ul>\n<li><code>y</code> raw output form the <code>Keras</code> model's <code>predict</code> method.\nOutputs:</li>\n<li><code>y</code>: a single <code>numpy.array</code>. For classificaiton, this should contain class predictions.</li>\n<li><code>extra_args</code>: for regression, this parameter is unused. For classification, this parameter contains prediction probabilities.</li>\n</ul>\n<p>To support a custom mapping from <code>Keras</code> to <code>Scikit-Learn</code>, you can subclass a wrapper and modify <code>_pre_process_y</code> and <code>_post_process_y</code>. For example, to support a mixed  binary/multiclass classification:</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">FunctionalAPIMultiOutputClassifier</span><span class=\"p\">(</span><span class=\"n\">KerasClassifier</span><span class=\"p\">):</span>\n    <span class=\"sd\">\"\"\"Functional API Classifier with 2 outputs of different type.</span>\n<span class=\"sd\">    \"\"\"</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__call__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">n_classes_</span><span class=\"p\">):</span>\n        <span class=\"n\">inp</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">((</span><span class=\"mi\">4</span><span class=\"p\">,))</span>\n\n        <span class=\"n\">x1</span> <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)(</span><span class=\"n\">inp</span><span class=\"p\">)</span>\n\n        <span class=\"n\">binary_out</span> <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s2\">\"sigmoid\"</span><span class=\"p\">)(</span><span class=\"n\">x1</span><span class=\"p\">)</span>\n        <span class=\"n\">cat_out</span> <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">n_classes_</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s2\">\"softmax\"</span><span class=\"p\">)(</span><span class=\"n\">x1</span><span class=\"p\">)</span>\n\n        <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">([</span><span class=\"n\">inp</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"n\">binary_out</span><span class=\"p\">,</span> <span class=\"n\">cat_out</span><span class=\"p\">])</span>\n        <span class=\"n\">losses</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"binary_crossentropy\"</span><span class=\"p\">,</span> <span class=\"s2\">\"categorical_crossentropy\"</span><span class=\"p\">]</span>\n        <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s2\">\"adam\"</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"n\">losses</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"accuracy\"</span><span class=\"p\">])</span>\n\n        <span class=\"k\">return</span> <span class=\"n\">model</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">_post_process_y</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"To support targets of different type, we need to post-precess each one</span>\n<span class=\"sd\">           manually, there is no way to determine the types accurately.</span>\n\n<span class=\"sd\">           Takes KerasClassifier._post_process_y as a starting point and</span>\n<span class=\"sd\">           hardcodes the post-processing.</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"n\">classes_</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">classes_</span>\n\n        <span class=\"n\">class_predictions</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n            <span class=\"n\">classes_</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">&gt;</span> <span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)],</span>\n            <span class=\"n\">classes_</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">][</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)],</span>\n        <span class=\"p\">]</span>\n\n        <span class=\"n\">class_probabilities</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">column_stack</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">))</span>\n\n        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">column_stack</span><span class=\"p\">(</span><span class=\"n\">class_predictions</span><span class=\"p\">))</span>\n\n        <span class=\"n\">extra_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"class_probabilities\"</span><span class=\"p\">:</span> <span class=\"n\">class_probabilities</span><span class=\"p\">}</span>\n\n        <span class=\"k\">return</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">extra_args</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">score</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"Taken from sklearn.multiouput.MultiOutputClassifier</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">all</span><span class=\"p\">(</span><span class=\"n\">y</span> <span class=\"o\">==</span> <span class=\"n\">y_pred</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n</pre>\n<p>The default implementation of <code>_pre_process_y</code> for <code>KerasClassifier</code> attempts to automatically determine the type of problem using <code>sklearn.utils.multiclass.type_of_target</code>. You may need to override this method if it is unable to determine the correct type for your data. The default implementation is provided as a static method so that you can test it without needing to instantiate a <code>KerasClassifier</code>.</p>\n<h3>Multi-input problems</h3>\n<p>As mentioned above, Scikit-Learn does not support multi-input problems since <code>X</code> must be a sinlge <code>numpy.array</code>. However, in order to extend this functionality, the wrappers provide a <code>_pre_process_X</code> method that allows mapping a single <code>numpy.arary</code> to a multi-input <code>Keras</code> model. For example:</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">FunctionalAPIMultiInputClassifier</span><span class=\"p\">(</span><span class=\"n\">KerasClassifier</span><span class=\"p\">):</span>\n    <span class=\"sd\">\"\"\"Functional API Classifier with 2 inputs.</span>\n<span class=\"sd\">    \"\"\"</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__call__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">n_classes_</span><span class=\"p\">):</span>\n        <span class=\"n\">inp1</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,))</span>\n        <span class=\"n\">inp2</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">((</span><span class=\"mi\">3</span><span class=\"p\">,))</span>\n\n        <span class=\"n\">x1</span> <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)(</span><span class=\"n\">inp1</span><span class=\"p\">)</span>\n        <span class=\"n\">x2</span> <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)(</span><span class=\"n\">inp2</span><span class=\"p\">)</span>\n\n        <span class=\"n\">x3</span> <span class=\"o\">=</span> <span class=\"n\">Concatenate</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)([</span><span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"n\">x2</span><span class=\"p\">])</span>\n\n        <span class=\"n\">cat_out</span> <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">n_classes_</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s2\">\"softmax\"</span><span class=\"p\">)(</span><span class=\"n\">x3</span><span class=\"p\">)</span>\n\n        <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">([</span><span class=\"n\">inp1</span><span class=\"p\">,</span> <span class=\"n\">inp2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"n\">cat_out</span><span class=\"p\">])</span>\n        <span class=\"n\">losses</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"categorical_crossentropy\"</span><span class=\"p\">]</span>\n        <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s2\">\"adam\"</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"n\">losses</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"accuracy\"</span><span class=\"p\">])</span>\n\n        <span class=\"k\">return</span> <span class=\"n\">model</span>\n\n    <span class=\"nd\">@staticmethod</span>  <span class=\"c1\"># _pre_process_X does not *need* to be a static method</span>\n    <span class=\"k\">def</span> <span class=\"nf\">_pre_process_X</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"To support multiple inputs, a custom method must be defined.</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"mi\">4</span><span class=\"p\">]],</span> <span class=\"nb\">dict</span><span class=\"p\">()</span>\n</pre>\n<p>Note that similar to <code>_pre_process_y</code>, <code>_pre_process_X</code> returns the modified <code>X</code> along with a dictionary of extra parameters. This dictionary is currently unused, but is kept for symmetry with <code>_pre_process_x</code> and future flexibility.</p>\n<h3>Custom scorers</h3>\n<p>To override the function used for scoring, set the <code>_scorer</code> attribute of the wrapper to point to a scoring function with the signature <code>scorer(y_true: np.array, y_pred: np.array) -&gt; float</code>.</p>\n<h2>Contributing</h2>\n<p>Contributions are very welcome. Please open an issue to ask for new features or preferable a PR to propose an implementation.</p>\n<p>If submitting a PR, please make sure that:</p>\n<ul>\n<li>\n<p>All existing tests should pass. Please make sure that the test\nsuite passes, both locally and on\n<a href=\"https://travis-ci.org/geopandas/geopandas\" rel=\"nofollow\">Travis CI</a>.  Status on\nTravis will be visible on a pull request.</p>\n</li>\n<li>\n<p>New functionality should include tests. Please write reasonable\ntests for your code and make sure that they pass on your pull request. Testing is done with <a href=\"https://docs.pytest.org/en/latest/\" rel=\"nofollow\"><code>Pytest</code></a> and coverage is checked with <a href=\"https://codecov.io/gh/adriangb/sklearn_keras_wrap\" rel=\"nofollow\"><code>CodeCov</code></a></p>\n</li>\n<li>\n<p>Classes, methods, functions, etc. should have docstrings. The first line of a docstring should be a standalone summary. Parameters and return values should be documented explicitly.</p>\n</li>\n</ul>\n<h3>Style</h3>\n<ul>\n<li>\n<p>This project follows <a href=\"http://www.python.org/dev/peps/pep-0008/\" rel=\"nofollow\">the PEP 8\nstandard</a> and uses\n<a href=\"https://black.readthedocs.io/en/stable/\" rel=\"nofollow\">Black</a> and\n<a href=\"http://flake8.pycqa.org/en/latest/\" rel=\"nofollow\">Flake8</a> to ensure a consistent\ncode format throughout the project.</p>\n</li>\n<li>\n<p>Imports should be grouped with standard library imports first,\n3rd-party libraries next, and GeoPandas imports third. Within each\ngrouping, imports should be alphabetized. Always use absolute\nimports when possible, and explicit relative imports for local\nimports when necessary in tests.</p>\n</li>\n<li>\n<p>You can set up <a href=\"https://pre-commit.com/\" rel=\"nofollow\">pre-commit hooks</a> to\nautomatically run <code>black</code> and <code>flake8</code> when you make a git\ncommit. This can be done by installing <code>pre-commit</code>:</p>\n<p>$ python -m pip install pre-commit</p>\n<p>From the root of the geopandas repository, you should then install\n<code>pre-commit</code>:</p>\n<p>$ pre-commit install</p>\n<p>Then <code>black</code> and <code>flake8</code> will be run automatically each time you\ncommit changes. You can skip these checks with <code>git commit --no-verify</code>.</p>\n</li>\n</ul>\n<h3>Deployment</h3>\n<p>Deployment to PyPi is done automatically by Travis for tagged commits.\nTo release a new version, you can use <code>bump2version</code>:</p>\n<pre>bump2version patch --message <span class=\"s2\">\"Tag commit message\"</span>\ngit push --tags\n</pre>\n<p>#History</p>\n<h2>0.1.4 (2020-04-12)</h2>\n<ul>\n<li>Offload output type detection for classification to <code>sklearn.utils.multiclass.type_of_target</code>.</li>\n<li>Add documentation.</li>\n<li>Some file cleanup.</li>\n</ul>\n<h2>0.1.3 (2020-04-11)</h2>\n<ul>\n<li>First release on PyPI.</li>\n</ul>\n\n          </div>"}, "last_serial": 7005387, "releases": {"0.1.3": [{"comment_text": "", "digests": {"md5": "e57f9a1800c09638dfc447c39dbbd45a", "sha256": "83710f184425942df505e81b0391ac3ce2a26b4478d801c5fec23a8dd435818d"}, "downloads": -1, "filename": "sklearn_keras_wrap-0.1.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "e57f9a1800c09638dfc447c39dbbd45a", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 11526, "upload_time": "2020-04-12T04:43:21", "upload_time_iso_8601": "2020-04-12T04:43:21.815710Z", "url": "https://files.pythonhosted.org/packages/9d/aa/f314b4c9d85ef94ba331a7aac2e71abd21ffdbf39c4e94e4df70f6f477ed/sklearn_keras_wrap-0.1.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "53900627ebdf14f908ca8c96127c2dd6", "sha256": "e39a71af2092a9b8a9c14456642f65c053a6c34dce662002a694cf0b7bd9604b"}, "downloads": -1, "filename": "sklearn_keras_wrap-0.1.3.tar.gz", "has_sig": false, "md5_digest": "53900627ebdf14f908ca8c96127c2dd6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 18175, "upload_time": "2020-04-12T04:43:22", "upload_time_iso_8601": "2020-04-12T04:43:22.859021Z", "url": "https://files.pythonhosted.org/packages/f3/96/16f77d08da8c3c12fb7aa9f27787383ab100800331af0bf5928fbce20ef3/sklearn_keras_wrap-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "9f755329adfd06275f76e9fb7c9a6767", "sha256": "9c8f9da2477ad952e826dbfb6e880a1301530ced75572e08a03312422f24bf7b"}, "downloads": -1, "filename": "sklearn_keras_wrap-0.1.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "9f755329adfd06275f76e9fb7c9a6767", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 16419, "upload_time": "2020-04-12T16:42:36", "upload_time_iso_8601": "2020-04-12T16:42:36.474905Z", "url": "https://files.pythonhosted.org/packages/4b/87/164e7aae4fceb0588011d268844a0669941eeef5e141e00dd43adabe8c19/sklearn_keras_wrap-0.1.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d78178359ba8a2073a0281ae3800f93e", "sha256": "7569caee3844c862f4303608d57a25e977c93fd39f88f2124c20c5da85098fd1"}, "downloads": -1, "filename": "sklearn_keras_wrap-0.1.4.tar.gz", "has_sig": false, "md5_digest": "d78178359ba8a2073a0281ae3800f93e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 23950, "upload_time": "2020-04-12T16:42:37", "upload_time_iso_8601": "2020-04-12T16:42:37.634450Z", "url": "https://files.pythonhosted.org/packages/fc/dc/1484e5b3b975a681f92d0f0d3c9ad8d1793dbb8c9de39659a97972eac87a/sklearn_keras_wrap-0.1.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "9f755329adfd06275f76e9fb7c9a6767", "sha256": "9c8f9da2477ad952e826dbfb6e880a1301530ced75572e08a03312422f24bf7b"}, "downloads": -1, "filename": "sklearn_keras_wrap-0.1.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "9f755329adfd06275f76e9fb7c9a6767", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.5", "size": 16419, "upload_time": "2020-04-12T16:42:36", "upload_time_iso_8601": "2020-04-12T16:42:36.474905Z", "url": "https://files.pythonhosted.org/packages/4b/87/164e7aae4fceb0588011d268844a0669941eeef5e141e00dd43adabe8c19/sklearn_keras_wrap-0.1.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d78178359ba8a2073a0281ae3800f93e", "sha256": "7569caee3844c862f4303608d57a25e977c93fd39f88f2124c20c5da85098fd1"}, "downloads": -1, "filename": "sklearn_keras_wrap-0.1.4.tar.gz", "has_sig": false, "md5_digest": "d78178359ba8a2073a0281ae3800f93e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 23950, "upload_time": "2020-04-12T16:42:37", "upload_time_iso_8601": "2020-04-12T16:42:37.634450Z", "url": "https://files.pythonhosted.org/packages/fc/dc/1484e5b3b975a681f92d0f0d3c9ad8d1793dbb8c9de39659a97972eac87a/sklearn_keras_wrap-0.1.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:08:56 2020"}