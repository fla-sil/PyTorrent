{"info": {"author": "Malte Koch", "author_email": "malte-koch@gmx.net", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3"], "description": "# TF Semantic Segmentation\n\n[![Build Status](https://travis-ci.org/baudcode/tf-semantic-segmentation.svg?branch=master)](https://travis-ci.org/baudcode/tf-semantic-segmentation)\n[![PyPI Status Badge](https://badge.fury.io/py/tf-semantic-segmentation.svg)](https://pypi.org/project/tf-semantic-segmentation/)\n[![codecov](https://codecov.io/gh/baudcode/tf-semantic-segmentation/branch/dev/graph/badge.svg)](https://codecov.io/gh/baudcode/tf-semantic-segmentation)\n[![latest tag](https://img.shields.io/github/v/tag/baudcode/tf-semantic-segmentation)]()\n\n## Features\n\n- Datasets\n\n  - Ade20k\n  - Camvid\n  - Cityscapes\n  - MappingChallenge\n  - MotsChallenge\n  - Coco\n  - PascalVoc2012\n  - Taco\n  - Shapes (randomly creating triangles, rectangles and circles)\n  - Toy (Overlaying TinyImageNet with MNIST)\n  - ISIC2018\n  - CVC-ClinicDB\n\n- Distributed Training on Multiple GPUs\n- Hyper Parameter Optimization using WandB\n- WandB Integration\n- Easily create TFRecord from Directory\n- Tensorboard visualizations\n- Ensemble inference\n\n- Models:\n\n  - Unet\n  - Erfnet\n  - MultiResUnet\n  - SatelliteUnet\n  - MobilenetUnet (unet with mobilenet encoder pre-trained on imagenet)\n  - unet_inception_resnet_v2 (unet with inception-resnet v2 encoder pre-trained on imagenet)\n  - ResnetUnet (unet with resnet50 encoder pre-trained on imagenet)\n\n- Losses:\n\n  - Catagorical Crossentropy\n  - Binary Crossentropy\n  - Crossentropy + SSIM\n  - Dice\n  - Crossentropy + Dice\n  - Tversky\n  - Focal\n  - Focal + Tversky\n\n- Activations:\n\n  - mish\n  - swish\n  - relu6\n\n- Optimizers:\n\n  - Ranger\n  - RAdam\n\n- Normalization\n\n  - Instance Norm\n  - Batch Norm\n\n- On the fly Augmentations\n\n  - flip left/right\n  - flip up/down\n  - rot 180\n  - color\n\n## Requirements\n\n```shell\nsudo apt-get install libsm6 libxext6 libxrender-dev libyaml-dev libpython3-dev\n```\n\n#### Tensorflow (2.x) & Tensorflow Addons (optional)\n\n```shell\npip install tensorflow-gpu==2.1.0 --upgrade\npip install tensorflow-addons==0.7.0 --upgrade\n```\n\nor\n\n```shell\npip install tensorflow-gpu==2.0.0 --upgrade\npip install tensorflow-addons==0.6.0 --upgrade\n```\n\n## Training\n\n### Hint: To see train/test/val images you have to start tensorboard like this\n\n```bash\ntensorboard --logdir=logs/ --reload_multifile=true\n```\n\n### On inbuild datasets (generator)\n\n```bash\npython -m tf_semantic_segmentation.bin.train -ds 'tacobinary' -bs 8 -e 100 \\\n    -logdir 'logs/taco-binary-test' -o 'adam' -lr 5e-3 --size 256,256 \\\n    -l 'binary_crossentropy' -fa 'sigmoid' \\\n    --train_on_generator --gpus='0' \\\n    --tensorboard_train_images --tensorboard_val_images\n```\n\n### Using a fixed record path\n\n```bash\npython -m tf_semantic_segmentation.bin.train --record_dir=records/cityscapes-512x256-rgb/ \\\n    -bs 4 -e 100 -logdir 'logs/cityscapes-bs8-e100-512x256' -o 'adam' -lr 1e-4 -l 'categorical_crossentropy' \\\n    -fa 'softmax' -bufsize 50 --metrics='iou_score,f1_score' -m 'erfnet' --gpus='0' -a 'mish' \\\n    --tensorboard_train_images --tensorboard_val_images\n```\n\n### Multi GPU training\n\n```bash\npython -m tf_semantic_segmentation.bin.train --record_dir=records/cityscapes-512x256-rgb/ \\\n    -bs 4 -e 100 -logdir 'logs/cityscapes-bs8-e100-512x256' -o 'adam' -lr 1e-4 -l 'categorical_crossentropy' \\\n    -fa 'softmax' -bufsize 50 --metrics='iou_score,f1_score' -m 'erfnet' --gpus='0,1,2,3' -a 'mish'\n```\n\n## Using Code\n\n```python\nfrom tf_semantic_segmentation.bin.train import train_test_model, get_args\n\n# get the default args\nargs = get_args({})\n\n# change some parameters\n# !rm -r logs/\nargs.model = 'erfnet'\n# args['color_mode'] = 0\nargs.batch_size = 8\nargs.size = [128, 128] # resize input dataset to this size\nargs.epochs = 10\nargs.learning_rate = 1e-4\nargs.optimizer = 'adam' # ['adam', 'radam', 'ranger']\nargs.loss = 'dice'\nargs.logdir = 'logs'\nargs.record_dir = \"datasets/shapes/records\"\nargs.final_activation = 'softmax'\n\n# train and test\nresults, model = train_test_model(args)\n```\n\n## Models\n\n- Erfnet\n- Unet\n\n```python\nfrom tf_semantic_segmentation import models\n\n# print all available models\nprint(list(modes.models_by_name.keys()))\n\n# returns a model (without the final activation function) and the base model without the top layer\n# because the activation function depends on the loss function\nmodel, base_model = models.get_model_by_name('erfnet', {\"input_shape\": (128, 128, 3), \"num_classes\": 5})\n\n# call models directly\nmodel, base_model = models.erfnet(input_shape=(128, 128), num_classes=5)\n```\n\n## Use your own dataset\n\n- Accepted file types are: jpg(jpeg) and png\n\nIf you already have a train/test/val split then use the following data structure:\n\n```text\ndataset/\n    labels.txt\n    test/\n        images/\n        masks/\n    train/\n        images/\n        masks/\n    val/\n        images/\n        masks/\n```\n\nor use\n\n```text\ndataset/\n    labels.txt\n    images/\n    masks/\n```\n\nThe labels.txt should contain a list of labels separated by newline [/n]. For instance it looks like this:\n\n```text\nbackground\ncar\npedestrian\n```\n\n- To create a tfrecord using the original image size and color use the script like this:\n\n```shell\nINPUT_DIR = ...\ntf-semantic-segmentation-tfrecord-writer -dir $INPUT_DIR -r $INPUT_DIR/records\n```\n\nThere are the following addition arguments:\n\n- -s [--size] '$width,$height' (f.e. \"512,512\")\n- -rm [--resize_method] ('resize', 'resize_with_pad', 'resize_with_crop_or_pad)\n- cm [--color_mode] (0=RGB, 1=GRAY, 2=NONE (default))\n\n## Datasets\n\n```python\nfrom tf_semantic_sementation.datasets import get_dataset by name, datasets_by_name, DataType, get_cache_dir\n\n# print availiable dataset names\nprint(list(datasets_by_name.keys()))\n\n# get the binary (waste or not) dataset\ndata_dir = '/hdd/data/'\nname = 'tacobinary'\ncache_dir = get_cache_dir(data_dir, name.lower())\nds = get_dataset_by_name(name, cache_dir)\n\n# print labels and classes\nprint(ds.labels)\nprint(ds.num_classes)\n\n# print number of training examples\nprint(ds.num_examples(DataType.TRAIN))\n\n# or simply print the summary\nds.summary()\n```\n\nDebug datasets\n\n```bash\npython -m tf_semantic_segmentation.debug.dataset_vis -d ade20k\n```\n\n## TFRecords\n\n#### This library simplicifies the process of creating a tfrecord dataset for faster training.\n\nWrite tfrecords:\n\n```python\nfrom tf_semantic_segmentation.datasets import TFWriter\nds = ...\nwriter = TFWriter(record_dir)\nwriter.write(ds)\nwriter.validate(ds)\n```\n\nor use simple with this script (will be save with size 128 x 128 (width x height)):\n\n```bash\ntf-semantic-segmentation-tfrecord-writer -d 'toy' -c /hdd/datasets/ -s '128,128'\n```\n\nAnalyse already written tfrecord (with mean)\n\n```bash\npython -m tf_semantic_segmentation.bin.tfrecord_analyser -r records/ --mean\n```\n\n## Docker\n\n```shell\ndocker build -t tf_semantic_segmentation -f docker/Dockerfile ./\n```\n\nor pull the latest release\n\n```shell\ndocker pull baudcode/tf_semantic_segmentation:latest\n```\n\n## Prediction\n\n```shell\npip install matplotlib\n```\n\n#### Using Code\n\n```python\nfrom tensorflow.keras.models import load_model\nimport numpy as np\nfrom tf_semantic_segmentation.processing import dataset\nfrom tf_semantic_segmentation.visualizations import show, masks\n\n\nmodel = load_model('logs/model-best.h5', compile=False)\n\n# model parameters\nsize = tuple(model.input.shape[1:3])\ndepth = model.input.shape[-1]\ncolor_mode = dataset.ColorMode.GRAY if depth == 1 else dataset.ColorMode.RGB\n\n# define an image\nimage = np.zeros((256, 256, 3), np.uint8)\n\n# preprocessing\nimage = image.astype(np.float32) / 255.\nimage, _ = dataset.resize_and_change_color(image, None, size, color_mode, resize_method='resize')\n\nimage_batch = np.expand_dims(image, axis=0)\n\n# predict (returns probabilities)\np = model.predict(image_batch)\n\n# draw segmentation map\nnum_classes = p.shape[-1] if p.shape[-1] > 1 else 2\npredictions_rgb = masks.get_colored_segmentation_mask(p, num_classes, images=image_batch, binary_threshold=0.5)\n\n# show images using matplotlib\nshow.show_images([predictions_rgb[0], image_batch[0]])\n```\n\n#### Using scripts\n\n- On image\n\n```shell\npython -m tf_semantic_segmentation.evaluation.predict -m model-best.h5  -i image.png\n```\n\n- On TFRecord (data type 'val' is default)\n\n```shell\npython -m tf_semantic_segmentation.evaluation.predict -m model-best.h5 -r records/camvid/\n```\n\n- On TFRecord (with export to directory)\n\n```shell\npython -m tf_semantic_segmentation.evaluation.predict -m model-best.h5 -r records/cubbinary/ -o out/ -rm 'resize_with_pad'\n```\n\n- On Video\n\n```shell\npython -m tf_semantic_segmentation.evaluation.predict -m model-best.h5 -v video.mp4\n```\n\n- On Video (with export to out/p-video.mp4)\n\n```shell\npython -m tf_semantic_segmentation.evaluation.predict -m model-best.h5 -v video.mp4 -o out/\n```\n\n## Prediction using Tensorflow Model Server\n\n- Installation\n\n```bash\n# install\necho \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\ncurl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\nsudo apt-get update && apt-get install tensorflow-model-server\n```\n\n- Start Model Server\n\n```bash\n### using a single model\ntensorflow_model_server --rest_api_port=8501 --model_base_path=/home/user/models/mymodel/saved_model\n\n### or using an ensamble of multiple models\n\n# helper to write the ensamble config yaml file (models/ contains multiple logdirs/, logdir must contain the name 'unet')\npython -m tf_semantic_segmentation.bin.model_server_config_writer -d models/ -c 'unet'\n# start model server with written models.yaml\ntensorflow_model_server --model_config_file=models.yaml --rest_api_port=8501\n```\n\n### **Compare models and ensemnble**\n\n```bash\npython -m tf_semantic_segmentation.evaluation.compare_models -i logs/ -c 'taco' -data /hdd/datasets/ -d 'tacobinary'\n```\n\nParameters:\n\n- _-i_ (directory containing models)\n- _-c_ (model name (directory name) must contain this value)\n- _-data_ (data directory)\n- _-d_ (dataset name)\n\nUse **--help** to get more help\n\n#### Using Code\n\n```python\nfrom tf_semantic_segmentation.serving import predict, predict_on_batch, ensamble_prediction, get_models_from_directory\nfrom tf_semantic_segmentation.processing.dataset import resize_and_change_color\n\nimage = np.zeros((128, 128, 3))\nimage_size = (256, 256)\ncolor_mode = 0  # 0=RGB, 1=GRAY\nresize_method = 'resize'\nscale_mask = False # only scale mask when model output is scaled using sigmoid activation\nnum_classes = 3\n\n# preprocess image\nimage = image.astype(np.float32) / 255.\nimage, _ = resize_and_change_color(image, None, image_size, color_mode, resize_method='resize')\n\n# prediction on 1 image\np = predict(image.numpy(), host='localhost', port=8501, input_name='input_1', model_name='0')\n\n#############################################################################################################\n# if the image size should not match, the color mode does not match or the model_name does not match\n# you'll most likely get a `400 Client Error: Bad Request for url: http://localhost:8501/v1/models/0:predict`\n# hint: if you only started 1 model try using model_name 'default'\n#############################################################################################################\n\n# prediction on batch (for faster prediction of multiple images)\np = predict_on_batch([image], host='localhost', port=8501, input_name='input_1', model_name='0')\n\n# ensamble prediction (average the predictions of multiple models)\n\n# either specify models like this:\nmodels = [\n    {\n        \"name\": \"0\",\n        \"path\": \"/home/user/models/mymodel/saved_model/\",\n        \"version\": 0, # optional\n        \"input_name\": \"input_1\"\n    },\n    {\n        \"name\": \"1\",\n        \"path\": \"/home/user/models/mymodel2/saved_model/\",\n        \"input_name\": \"input_1\"\n    }\n]\n\n\n# or load from models in directory (models/) that contain the name 'unet'\nmodels = get_models_from_directory('models/', contains='unet')\n\n# returns the ensamble and all predictions made\nensamble, predictions = ensamble_prediction(models, image.numpy(), host='localhost', port=8501)\n```\n\n## TFLite support\n\n#### Convert the model\n\n```shell\npython -m tf_semantic_segmentation.bin.convert_tflite -i logs/mymodel/saved_model/0/ -o model.tflite\n```\n\n#### Test inference on the model\n\n```shell\npython -m tf_semantic_segmentation.debug.tflite_test -m model.tflite -i Harris_Sparrow_0001_116398.jpg\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/baudcode/tf-semantic-segmentation", "keywords": "keras,tensorflow,tf_semantic_segmentation,semantic,segmentation,ade20k,coco,pascalvoc,cityscapes", "license": "MIT", "maintainer": "Malte Koch", "maintainer_email": "malte-koch@gmx.net", "name": "tf-semantic-segmentation", "package_url": "https://pypi.org/project/tf-semantic-segmentation/", "platform": "", "project_url": "https://pypi.org/project/tf-semantic-segmentation/", "project_urls": {"Homepage": "https://github.com/baudcode/tf-semantic-segmentation"}, "release_url": "https://pypi.org/project/tf-semantic-segmentation/0.2.3/", "requires_dist": ["requests", "imageio", "opencv-python", "wandb", "tqdm", "scipy", "xmltodict", "pillow", "pytz", "pyyaml"], "requires_python": "", "summary": "Implementation of various semantic segmentation models in tensorflow & keras including popular datasets", "version": "0.2.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>TF Semantic Segmentation</h1>\n<p><a href=\"https://travis-ci.org/baudcode/tf-semantic-segmentation\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/35c0e797591964fe39004f1155cb2372e7c3de99/68747470733a2f2f7472617669732d63692e6f72672f62617564636f64652f74662d73656d616e7469632d7365676d656e746174696f6e2e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://pypi.org/project/tf-semantic-segmentation/\" rel=\"nofollow\"><img alt=\"PyPI Status Badge\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/732cc29de0f71731544fe09c48dce4264b33032d/68747470733a2f2f62616467652e667572792e696f2f70792f74662d73656d616e7469632d7365676d656e746174696f6e2e737667\"></a>\n<a href=\"https://codecov.io/gh/baudcode/tf-semantic-segmentation\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5ffc9b52e74c9a9b326b4189312e0b08d3181bb2/68747470733a2f2f636f6465636f762e696f2f67682f62617564636f64652f74662d73656d616e7469632d7365676d656e746174696f6e2f6272616e63682f6465762f67726170682f62616467652e737667\"></a>\n<a href=\"\" rel=\"nofollow\"><img alt=\"latest tag\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/01934997e402a33e2b76b1a5c6d4984fe8f9b3b1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f7461672f62617564636f64652f74662d73656d616e7469632d7365676d656e746174696f6e\"></a></p>\n<h2>Features</h2>\n<ul>\n<li>\n<p>Datasets</p>\n<ul>\n<li>Ade20k</li>\n<li>Camvid</li>\n<li>Cityscapes</li>\n<li>MappingChallenge</li>\n<li>MotsChallenge</li>\n<li>Coco</li>\n<li>PascalVoc2012</li>\n<li>Taco</li>\n<li>Shapes (randomly creating triangles, rectangles and circles)</li>\n<li>Toy (Overlaying TinyImageNet with MNIST)</li>\n<li>ISIC2018</li>\n<li>CVC-ClinicDB</li>\n</ul>\n</li>\n<li>\n<p>Distributed Training on Multiple GPUs</p>\n</li>\n<li>\n<p>Hyper Parameter Optimization using WandB</p>\n</li>\n<li>\n<p>WandB Integration</p>\n</li>\n<li>\n<p>Easily create TFRecord from Directory</p>\n</li>\n<li>\n<p>Tensorboard visualizations</p>\n</li>\n<li>\n<p>Ensemble inference</p>\n</li>\n<li>\n<p>Models:</p>\n<ul>\n<li>Unet</li>\n<li>Erfnet</li>\n<li>MultiResUnet</li>\n<li>SatelliteUnet</li>\n<li>MobilenetUnet (unet with mobilenet encoder pre-trained on imagenet)</li>\n<li>unet_inception_resnet_v2 (unet with inception-resnet v2 encoder pre-trained on imagenet)</li>\n<li>ResnetUnet (unet with resnet50 encoder pre-trained on imagenet)</li>\n</ul>\n</li>\n<li>\n<p>Losses:</p>\n<ul>\n<li>Catagorical Crossentropy</li>\n<li>Binary Crossentropy</li>\n<li>Crossentropy + SSIM</li>\n<li>Dice</li>\n<li>Crossentropy + Dice</li>\n<li>Tversky</li>\n<li>Focal</li>\n<li>Focal + Tversky</li>\n</ul>\n</li>\n<li>\n<p>Activations:</p>\n<ul>\n<li>mish</li>\n<li>swish</li>\n<li>relu6</li>\n</ul>\n</li>\n<li>\n<p>Optimizers:</p>\n<ul>\n<li>Ranger</li>\n<li>RAdam</li>\n</ul>\n</li>\n<li>\n<p>Normalization</p>\n<ul>\n<li>Instance Norm</li>\n<li>Batch Norm</li>\n</ul>\n</li>\n<li>\n<p>On the fly Augmentations</p>\n<ul>\n<li>flip left/right</li>\n<li>flip up/down</li>\n<li>rot 180</li>\n<li>color</li>\n</ul>\n</li>\n</ul>\n<h2>Requirements</h2>\n<pre>sudo apt-get install libsm6 libxext6 libxrender-dev libyaml-dev libpython3-dev\n</pre>\n<h4>Tensorflow (2.x) &amp; Tensorflow Addons (optional)</h4>\n<pre>pip install tensorflow-gpu<span class=\"o\">==</span><span class=\"m\">2</span>.1.0 --upgrade\npip install tensorflow-addons<span class=\"o\">==</span><span class=\"m\">0</span>.7.0 --upgrade\n</pre>\n<p>or</p>\n<pre>pip install tensorflow-gpu<span class=\"o\">==</span><span class=\"m\">2</span>.0.0 --upgrade\npip install tensorflow-addons<span class=\"o\">==</span><span class=\"m\">0</span>.6.0 --upgrade\n</pre>\n<h2>Training</h2>\n<h3>Hint: To see train/test/val images you have to start tensorboard like this</h3>\n<pre>tensorboard --logdir<span class=\"o\">=</span>logs/ --reload_multifile<span class=\"o\">=</span><span class=\"nb\">true</span>\n</pre>\n<h3>On inbuild datasets (generator)</h3>\n<pre>python -m tf_semantic_segmentation.bin.train -ds <span class=\"s1\">'tacobinary'</span> -bs <span class=\"m\">8</span> -e <span class=\"m\">100</span> <span class=\"se\">\\</span>\n    -logdir <span class=\"s1\">'logs/taco-binary-test'</span> -o <span class=\"s1\">'adam'</span> -lr 5e-3 --size <span class=\"m\">256</span>,256 <span class=\"se\">\\</span>\n    -l <span class=\"s1\">'binary_crossentropy'</span> -fa <span class=\"s1\">'sigmoid'</span> <span class=\"se\">\\</span>\n    --train_on_generator --gpus<span class=\"o\">=</span><span class=\"s1\">'0'</span> <span class=\"se\">\\</span>\n    --tensorboard_train_images --tensorboard_val_images\n</pre>\n<h3>Using a fixed record path</h3>\n<pre>python -m tf_semantic_segmentation.bin.train --record_dir<span class=\"o\">=</span>records/cityscapes-512x256-rgb/ <span class=\"se\">\\</span>\n    -bs <span class=\"m\">4</span> -e <span class=\"m\">100</span> -logdir <span class=\"s1\">'logs/cityscapes-bs8-e100-512x256'</span> -o <span class=\"s1\">'adam'</span> -lr 1e-4 -l <span class=\"s1\">'categorical_crossentropy'</span> <span class=\"se\">\\</span>\n    -fa <span class=\"s1\">'softmax'</span> -bufsize <span class=\"m\">50</span> --metrics<span class=\"o\">=</span><span class=\"s1\">'iou_score,f1_score'</span> -m <span class=\"s1\">'erfnet'</span> --gpus<span class=\"o\">=</span><span class=\"s1\">'0'</span> -a <span class=\"s1\">'mish'</span> <span class=\"se\">\\</span>\n    --tensorboard_train_images --tensorboard_val_images\n</pre>\n<h3>Multi GPU training</h3>\n<pre>python -m tf_semantic_segmentation.bin.train --record_dir<span class=\"o\">=</span>records/cityscapes-512x256-rgb/ <span class=\"se\">\\</span>\n    -bs <span class=\"m\">4</span> -e <span class=\"m\">100</span> -logdir <span class=\"s1\">'logs/cityscapes-bs8-e100-512x256'</span> -o <span class=\"s1\">'adam'</span> -lr 1e-4 -l <span class=\"s1\">'categorical_crossentropy'</span> <span class=\"se\">\\</span>\n    -fa <span class=\"s1\">'softmax'</span> -bufsize <span class=\"m\">50</span> --metrics<span class=\"o\">=</span><span class=\"s1\">'iou_score,f1_score'</span> -m <span class=\"s1\">'erfnet'</span> --gpus<span class=\"o\">=</span><span class=\"s1\">'0,1,2,3'</span> -a <span class=\"s1\">'mish'</span>\n</pre>\n<h2>Using Code</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tf_semantic_segmentation.bin.train</span> <span class=\"kn\">import</span> <span class=\"n\">train_test_model</span><span class=\"p\">,</span> <span class=\"n\">get_args</span>\n\n<span class=\"c1\"># get the default args</span>\n<span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"n\">get_args</span><span class=\"p\">({})</span>\n\n<span class=\"c1\"># change some parameters</span>\n<span class=\"c1\"># !rm -r logs/</span>\n<span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"s1\">'erfnet'</span>\n<span class=\"c1\"># args['color_mode'] = 0</span>\n<span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>\n<span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">]</span> <span class=\"c1\"># resize input dataset to this size</span>\n<span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n<span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">1e-4</span>\n<span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"s1\">'adam'</span> <span class=\"c1\"># ['adam', 'radam', 'ranger']</span>\n<span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"s1\">'dice'</span>\n<span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">logdir</span> <span class=\"o\">=</span> <span class=\"s1\">'logs'</span>\n<span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">record_dir</span> <span class=\"o\">=</span> <span class=\"s2\">\"datasets/shapes/records\"</span>\n<span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">final_activation</span> <span class=\"o\">=</span> <span class=\"s1\">'softmax'</span>\n\n<span class=\"c1\"># train and test</span>\n<span class=\"n\">results</span><span class=\"p\">,</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">train_test_model</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">)</span>\n</pre>\n<h2>Models</h2>\n<ul>\n<li>Erfnet</li>\n<li>Unet</li>\n</ul>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tf_semantic_segmentation</span> <span class=\"kn\">import</span> <span class=\"n\">models</span>\n\n<span class=\"c1\"># print all available models</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">modes</span><span class=\"o\">.</span><span class=\"n\">models_by_name</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">()))</span>\n\n<span class=\"c1\"># returns a model (without the final activation function) and the base model without the top layer</span>\n<span class=\"c1\"># because the activation function depends on the loss function</span>\n<span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">base_model</span> <span class=\"o\">=</span> <span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">get_model_by_name</span><span class=\"p\">(</span><span class=\"s1\">'erfnet'</span><span class=\"p\">,</span> <span class=\"p\">{</span><span class=\"s2\">\"input_shape\"</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"s2\">\"num_classes\"</span><span class=\"p\">:</span> <span class=\"mi\">5</span><span class=\"p\">})</span>\n\n<span class=\"c1\"># call models directly</span>\n<span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">base_model</span> <span class=\"o\">=</span> <span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">erfnet</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">),</span> <span class=\"n\">num_classes</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n</pre>\n<h2>Use your own dataset</h2>\n<ul>\n<li>Accepted file types are: jpg(jpeg) and png</li>\n</ul>\n<p>If you already have a train/test/val split then use the following data structure:</p>\n<pre>dataset/\n    labels.txt\n    test/\n        images/\n        masks/\n    train/\n        images/\n        masks/\n    val/\n        images/\n        masks/\n</pre>\n<p>or use</p>\n<pre>dataset/\n    labels.txt\n    images/\n    masks/\n</pre>\n<p>The labels.txt should contain a list of labels separated by newline [/n]. For instance it looks like this:</p>\n<pre>background\ncar\npedestrian\n</pre>\n<ul>\n<li>To create a tfrecord using the original image size and color use the script like this:</li>\n</ul>\n<pre><span class=\"nv\">INPUT_DIR</span> <span class=\"o\">=</span> ...\ntf-semantic-segmentation-tfrecord-writer -dir <span class=\"nv\">$INPUT_DIR</span> -r <span class=\"nv\">$INPUT_DIR</span>/records\n</pre>\n<p>There are the following addition arguments:</p>\n<ul>\n<li>-s [--size] '$width,$height' (f.e. \"512,512\")</li>\n<li>-rm [--resize_method] ('resize', 'resize_with_pad', 'resize_with_crop_or_pad)</li>\n<li>cm [--color_mode] (0=RGB, 1=GRAY, 2=NONE (default))</li>\n</ul>\n<h2>Datasets</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tf_semantic_sementation.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">get_dataset</span> <span class=\"n\">by</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">datasets_by_name</span><span class=\"p\">,</span> <span class=\"n\">DataType</span><span class=\"p\">,</span> <span class=\"n\">get_cache_dir</span>\n\n<span class=\"c1\"># print availiable dataset names</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">datasets_by_name</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">()))</span>\n\n<span class=\"c1\"># get the binary (waste or not) dataset</span>\n<span class=\"n\">data_dir</span> <span class=\"o\">=</span> <span class=\"s1\">'/hdd/data/'</span>\n<span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s1\">'tacobinary'</span>\n<span class=\"n\">cache_dir</span> <span class=\"o\">=</span> <span class=\"n\">get_cache_dir</span><span class=\"p\">(</span><span class=\"n\">data_dir</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">())</span>\n<span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"n\">get_dataset_by_name</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">cache_dir</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># print labels and classes</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">labels</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">num_classes</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># print number of training examples</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">num_examples</span><span class=\"p\">(</span><span class=\"n\">DataType</span><span class=\"o\">.</span><span class=\"n\">TRAIN</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># or simply print the summary</span>\n<span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"p\">()</span>\n</pre>\n<p>Debug datasets</p>\n<pre>python -m tf_semantic_segmentation.debug.dataset_vis -d ade20k\n</pre>\n<h2>TFRecords</h2>\n<h4>This library simplicifies the process of creating a tfrecord dataset for faster training.</h4>\n<p>Write tfrecords:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tf_semantic_segmentation.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">TFWriter</span>\n<span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n<span class=\"n\">writer</span> <span class=\"o\">=</span> <span class=\"n\">TFWriter</span><span class=\"p\">(</span><span class=\"n\">record_dir</span><span class=\"p\">)</span>\n<span class=\"n\">writer</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">ds</span><span class=\"p\">)</span>\n<span class=\"n\">writer</span><span class=\"o\">.</span><span class=\"n\">validate</span><span class=\"p\">(</span><span class=\"n\">ds</span><span class=\"p\">)</span>\n</pre>\n<p>or use simple with this script (will be save with size 128 x 128 (width x height)):</p>\n<pre>tf-semantic-segmentation-tfrecord-writer -d <span class=\"s1\">'toy'</span> -c /hdd/datasets/ -s <span class=\"s1\">'128,128'</span>\n</pre>\n<p>Analyse already written tfrecord (with mean)</p>\n<pre>python -m tf_semantic_segmentation.bin.tfrecord_analyser -r records/ --mean\n</pre>\n<h2>Docker</h2>\n<pre>docker build -t tf_semantic_segmentation -f docker/Dockerfile ./\n</pre>\n<p>or pull the latest release</p>\n<pre>docker pull baudcode/tf_semantic_segmentation:latest\n</pre>\n<h2>Prediction</h2>\n<pre>pip install matplotlib\n</pre>\n<h4>Using Code</h4>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">load_model</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tf_semantic_segmentation.processing</span> <span class=\"kn\">import</span> <span class=\"n\">dataset</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tf_semantic_segmentation.visualizations</span> <span class=\"kn\">import</span> <span class=\"n\">show</span><span class=\"p\">,</span> <span class=\"n\">masks</span>\n\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s1\">'logs/model-best.h5'</span><span class=\"p\">,</span> <span class=\"nb\">compile</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># model parameters</span>\n<span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"nb\">tuple</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">input</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"mi\">3</span><span class=\"p\">])</span>\n<span class=\"n\">depth</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">input</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"n\">color_mode</span> <span class=\"o\">=</span> <span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">ColorMode</span><span class=\"o\">.</span><span class=\"n\">GRAY</span> <span class=\"k\">if</span> <span class=\"n\">depth</span> <span class=\"o\">==</span> <span class=\"mi\">1</span> <span class=\"k\">else</span> <span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">ColorMode</span><span class=\"o\">.</span><span class=\"n\">RGB</span>\n\n<span class=\"c1\"># define an image</span>\n<span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># preprocessing</span>\n<span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">255.</span>\n<span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">resize_and_change_color</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">color_mode</span><span class=\"p\">,</span> <span class=\"n\">resize_method</span><span class=\"o\">=</span><span class=\"s1\">'resize'</span><span class=\"p\">)</span>\n\n<span class=\"n\">image_batch</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">expand_dims</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># predict (returns probabilities)</span>\n<span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">image_batch</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># draw segmentation map</span>\n<span class=\"n\">num_classes</span> <span class=\"o\">=</span> <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1</span> <span class=\"k\">else</span> <span class=\"mi\">2</span>\n<span class=\"n\">predictions_rgb</span> <span class=\"o\">=</span> <span class=\"n\">masks</span><span class=\"o\">.</span><span class=\"n\">get_colored_segmentation_mask</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"p\">,</span> <span class=\"n\">images</span><span class=\"o\">=</span><span class=\"n\">image_batch</span><span class=\"p\">,</span> <span class=\"n\">binary_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># show images using matplotlib</span>\n<span class=\"n\">show</span><span class=\"o\">.</span><span class=\"n\">show_images</span><span class=\"p\">([</span><span class=\"n\">predictions_rgb</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">image_batch</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]])</span>\n</pre>\n<h4>Using scripts</h4>\n<ul>\n<li>On image</li>\n</ul>\n<pre>python -m tf_semantic_segmentation.evaluation.predict -m model-best.h5  -i image.png\n</pre>\n<ul>\n<li>On TFRecord (data type 'val' is default)</li>\n</ul>\n<pre>python -m tf_semantic_segmentation.evaluation.predict -m model-best.h5 -r records/camvid/\n</pre>\n<ul>\n<li>On TFRecord (with export to directory)</li>\n</ul>\n<pre>python -m tf_semantic_segmentation.evaluation.predict -m model-best.h5 -r records/cubbinary/ -o out/ -rm <span class=\"s1\">'resize_with_pad'</span>\n</pre>\n<ul>\n<li>On Video</li>\n</ul>\n<pre>python -m tf_semantic_segmentation.evaluation.predict -m model-best.h5 -v video.mp4\n</pre>\n<ul>\n<li>On Video (with export to out/p-video.mp4)</li>\n</ul>\n<pre>python -m tf_semantic_segmentation.evaluation.predict -m model-best.h5 -v video.mp4 -o out/\n</pre>\n<h2>Prediction using Tensorflow Model Server</h2>\n<ul>\n<li>Installation</li>\n</ul>\n<pre><span class=\"c1\"># install</span>\n<span class=\"nb\">echo</span> <span class=\"s2\">\"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\"</span> <span class=\"p\">|</span> sudo tee /etc/apt/sources.list.d/tensorflow-serving.list <span class=\"o\">&amp;&amp;</span> <span class=\"se\">\\</span>\ncurl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg <span class=\"p\">|</span> sudo apt-key add -\nsudo apt-get update <span class=\"o\">&amp;&amp;</span> apt-get install tensorflow-model-server\n</pre>\n<ul>\n<li>Start Model Server</li>\n</ul>\n<pre><span class=\"c1\">### using a single model</span>\ntensorflow_model_server --rest_api_port<span class=\"o\">=</span><span class=\"m\">8501</span> --model_base_path<span class=\"o\">=</span>/home/user/models/mymodel/saved_model\n\n<span class=\"c1\">### or using an ensamble of multiple models</span>\n\n<span class=\"c1\"># helper to write the ensamble config yaml file (models/ contains multiple logdirs/, logdir must contain the name 'unet')</span>\npython -m tf_semantic_segmentation.bin.model_server_config_writer -d models/ -c <span class=\"s1\">'unet'</span>\n<span class=\"c1\"># start model server with written models.yaml</span>\ntensorflow_model_server --model_config_file<span class=\"o\">=</span>models.yaml --rest_api_port<span class=\"o\">=</span><span class=\"m\">8501</span>\n</pre>\n<h3><strong>Compare models and ensemnble</strong></h3>\n<pre>python -m tf_semantic_segmentation.evaluation.compare_models -i logs/ -c <span class=\"s1\">'taco'</span> -data /hdd/datasets/ -d <span class=\"s1\">'tacobinary'</span>\n</pre>\n<p>Parameters:</p>\n<ul>\n<li><em>-i</em> (directory containing models)</li>\n<li><em>-c</em> (model name (directory name) must contain this value)</li>\n<li><em>-data</em> (data directory)</li>\n<li><em>-d</em> (dataset name)</li>\n</ul>\n<p>Use <strong>--help</strong> to get more help</p>\n<h4>Using Code</h4>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tf_semantic_segmentation.serving</span> <span class=\"kn\">import</span> <span class=\"n\">predict</span><span class=\"p\">,</span> <span class=\"n\">predict_on_batch</span><span class=\"p\">,</span> <span class=\"n\">ensamble_prediction</span><span class=\"p\">,</span> <span class=\"n\">get_models_from_directory</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tf_semantic_segmentation.processing.dataset</span> <span class=\"kn\">import</span> <span class=\"n\">resize_and_change_color</span>\n\n<span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">))</span>\n<span class=\"n\">image_size</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">)</span>\n<span class=\"n\">color_mode</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>  <span class=\"c1\"># 0=RGB, 1=GRAY</span>\n<span class=\"n\">resize_method</span> <span class=\"o\">=</span> <span class=\"s1\">'resize'</span>\n<span class=\"n\">scale_mask</span> <span class=\"o\">=</span> <span class=\"kc\">False</span> <span class=\"c1\"># only scale mask when model output is scaled using sigmoid activation</span>\n<span class=\"n\">num_classes</span> <span class=\"o\">=</span> <span class=\"mi\">3</span>\n\n<span class=\"c1\"># preprocess image</span>\n<span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">255.</span>\n<span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">resize_and_change_color</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">image_size</span><span class=\"p\">,</span> <span class=\"n\">color_mode</span><span class=\"p\">,</span> <span class=\"n\">resize_method</span><span class=\"o\">=</span><span class=\"s1\">'resize'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># prediction on 1 image</span>\n<span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">(),</span> <span class=\"n\">host</span><span class=\"o\">=</span><span class=\"s1\">'localhost'</span><span class=\"p\">,</span> <span class=\"n\">port</span><span class=\"o\">=</span><span class=\"mi\">8501</span><span class=\"p\">,</span> <span class=\"n\">input_name</span><span class=\"o\">=</span><span class=\"s1\">'input_1'</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"s1\">'0'</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#############################################################################################################</span>\n<span class=\"c1\"># if the image size should not match, the color mode does not match or the model_name does not match</span>\n<span class=\"c1\"># you'll most likely get a `400 Client Error: Bad Request for url: http://localhost:8501/v1/models/0:predict`</span>\n<span class=\"c1\"># hint: if you only started 1 model try using model_name 'default'</span>\n<span class=\"c1\">#############################################################################################################</span>\n\n<span class=\"c1\"># prediction on batch (for faster prediction of multiple images)</span>\n<span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">predict_on_batch</span><span class=\"p\">([</span><span class=\"n\">image</span><span class=\"p\">],</span> <span class=\"n\">host</span><span class=\"o\">=</span><span class=\"s1\">'localhost'</span><span class=\"p\">,</span> <span class=\"n\">port</span><span class=\"o\">=</span><span class=\"mi\">8501</span><span class=\"p\">,</span> <span class=\"n\">input_name</span><span class=\"o\">=</span><span class=\"s1\">'input_1'</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"s1\">'0'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># ensamble prediction (average the predictions of multiple models)</span>\n\n<span class=\"c1\"># either specify models like this:</span>\n<span class=\"n\">models</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n        <span class=\"s2\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"0\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"path\"</span><span class=\"p\">:</span> <span class=\"s2\">\"/home/user/models/mymodel/saved_model/\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"version\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"c1\"># optional</span>\n        <span class=\"s2\">\"input_name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"input_1\"</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n        <span class=\"s2\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"1\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"path\"</span><span class=\"p\">:</span> <span class=\"s2\">\"/home/user/models/mymodel2/saved_model/\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"input_name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"input_1\"</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">]</span>\n\n\n<span class=\"c1\"># or load from models in directory (models/) that contain the name 'unet'</span>\n<span class=\"n\">models</span> <span class=\"o\">=</span> <span class=\"n\">get_models_from_directory</span><span class=\"p\">(</span><span class=\"s1\">'models/'</span><span class=\"p\">,</span> <span class=\"n\">contains</span><span class=\"o\">=</span><span class=\"s1\">'unet'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># returns the ensamble and all predictions made</span>\n<span class=\"n\">ensamble</span><span class=\"p\">,</span> <span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"n\">ensamble_prediction</span><span class=\"p\">(</span><span class=\"n\">models</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">(),</span> <span class=\"n\">host</span><span class=\"o\">=</span><span class=\"s1\">'localhost'</span><span class=\"p\">,</span> <span class=\"n\">port</span><span class=\"o\">=</span><span class=\"mi\">8501</span><span class=\"p\">)</span>\n</pre>\n<h2>TFLite support</h2>\n<h4>Convert the model</h4>\n<pre>python -m tf_semantic_segmentation.bin.convert_tflite -i logs/mymodel/saved_model/0/ -o model.tflite\n</pre>\n<h4>Test inference on the model</h4>\n<pre>python -m tf_semantic_segmentation.debug.tflite_test -m model.tflite -i Harris_Sparrow_0001_116398.jpg\n</pre>\n\n          </div>"}, "last_serial": 6762136, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "0d09ccee2a852a7e6ac6e788060963f2", "sha256": "42d33b61faeebc8fb51173d83d2ec086c99bfc66371a0b060719551bf663eb1e"}, "downloads": -1, "filename": "tf_semantic_segmentation-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0d09ccee2a852a7e6ac6e788060963f2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 73713, "upload_time": "2020-01-12T21:00:16", "upload_time_iso_8601": "2020-01-12T21:00:16.942780Z", "url": "https://files.pythonhosted.org/packages/63/eb/fe8cb5888e97efea5529d30b9045a1c14b76f0d2c5e1c8d776d6466c9d64/tf_semantic_segmentation-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ee98c2c54bc5e70bc899c931a550aa08", "sha256": "e5c9dce69ef5e09f0d7cadedf7a1460f20b7e8e1e6148a19b60b9c868a479d1e"}, "downloads": -1, "filename": "tf_semantic_segmentation-0.1.0.tar.gz", "has_sig": false, "md5_digest": "ee98c2c54bc5e70bc899c931a550aa08", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 50117, "upload_time": "2020-01-12T21:00:20", "upload_time_iso_8601": "2020-01-12T21:00:20.612708Z", "url": "https://files.pythonhosted.org/packages/bb/b9/1fe68e4ad85f30b01147db25c126e7a55008c9a73d2f4fb4b42d15ec086a/tf_semantic_segmentation-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "2bff69d714bdae41ef80eb82a9ad2665", "sha256": "4381e920c0aaf8e541a00fcb345df121bcbeeb7aca4420f91d732bcb5fbb8818"}, "downloads": -1, "filename": "tf_semantic_segmentation-0.2.0.tar.gz", "has_sig": false, "md5_digest": "2bff69d714bdae41ef80eb82a9ad2665", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59494, "upload_time": "2020-01-23T12:54:29", "upload_time_iso_8601": "2020-01-23T12:54:29.376612Z", "url": "https://files.pythonhosted.org/packages/19/1c/de8d76a75b46382c7cd9adaa63ca760137bd888334d4df844b9a3611730b/tf_semantic_segmentation-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "7b2a1375e995eb3f3bca1e0e324e0898", "sha256": "3f0d7c117f064c040c7b84ab938e5fc53621e5748d1412c7b36bae873d470713"}, "downloads": -1, "filename": "tf_semantic_segmentation-0.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "7b2a1375e995eb3f3bca1e0e324e0898", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 98596, "upload_time": "2020-01-23T19:15:19", "upload_time_iso_8601": "2020-01-23T19:15:19.977310Z", "url": "https://files.pythonhosted.org/packages/ad/d7/6d7713f48aa55c488e583664d2e891cc438dd9c65a2b4c2eac87312db80c/tf_semantic_segmentation-0.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9423927f9b6198918af9eabebfa902b4", "sha256": "e0f5d894cf4852df9b00f25e6a852a54c3a9ee1d1169eca16f03541379f59d43"}, "downloads": -1, "filename": "tf_semantic_segmentation-0.2.1.tar.gz", "has_sig": false, "md5_digest": "9423927f9b6198918af9eabebfa902b4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 59370, "upload_time": "2020-01-23T19:15:21", "upload_time_iso_8601": "2020-01-23T19:15:21.699272Z", "url": "https://files.pythonhosted.org/packages/d8/9d/4c6082c375aadb4df9516ff2d1e999ef97b41b607c7a349cf3f536311cac/tf_semantic_segmentation-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "ee1d07af5aa7bb4ff7b89a4aeaf6773e", "sha256": "835334c67b6152c45f20d08e3312f217cc7b31f49f7ce036aefd5f53951052c9"}, "downloads": -1, "filename": "tf_semantic_segmentation-0.2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "ee1d07af5aa7bb4ff7b89a4aeaf6773e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 119730, "upload_time": "2020-02-17T12:51:54", "upload_time_iso_8601": "2020-02-17T12:51:54.133995Z", "url": "https://files.pythonhosted.org/packages/d2/5a/ace3a95d4891f8765dc1aa655204842744d6f22e65adff8d6003caf4ab1e/tf_semantic_segmentation-0.2.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5ffcdc5dc5d4c44a7d13a5b3fdf4a46e", "sha256": "2b4ad6c529d2945e291be8416cf3c0caeb3e31b2d469f1317b31df11c65fdd73"}, "downloads": -1, "filename": "tf_semantic_segmentation-0.2.2.tar.gz", "has_sig": false, "md5_digest": "5ffcdc5dc5d4c44a7d13a5b3fdf4a46e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 73476, "upload_time": "2020-02-17T12:51:55", "upload_time_iso_8601": "2020-02-17T12:51:55.760960Z", "url": "https://files.pythonhosted.org/packages/bf/be/33403006d098b4e41e9a78c222577616abb92aa7e441ca8742f92895f5c7/tf_semantic_segmentation-0.2.2.tar.gz", "yanked": false}], "0.2.3": [{"comment_text": "", "digests": {"md5": "bc67f23cc6eab03d07196578d82a9fbe", "sha256": "d0b72625b8ca26c238b81c22b847e914a9bd6825d4fed2567bdb7e1c79cbc488"}, "downloads": -1, "filename": "tf_semantic_segmentation-0.2.3-py3-none-any.whl", "has_sig": false, "md5_digest": "bc67f23cc6eab03d07196578d82a9fbe", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 149274, "upload_time": "2020-03-06T14:04:24", "upload_time_iso_8601": "2020-03-06T14:04:24.367262Z", "url": "https://files.pythonhosted.org/packages/fb/d9/0d699c714de97b75a69ce920a44c6a93f388931c3894a2f8b159b33233e0/tf_semantic_segmentation-0.2.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7d24ebe7d3c6e64a4a9d59941ecf45a2", "sha256": "27e70c5945db37ad303989e75d4e2aed1654d6313daff9aab19f79e18a076ca3"}, "downloads": -1, "filename": "tf_semantic_segmentation-0.2.3.tar.gz", "has_sig": false, "md5_digest": "7d24ebe7d3c6e64a4a9d59941ecf45a2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 95933, "upload_time": "2020-03-06T14:04:25", "upload_time_iso_8601": "2020-03-06T14:04:25.861212Z", "url": "https://files.pythonhosted.org/packages/04/d1/12f07405ec75028efc3768d1b2eb93f26bd3289c1b54be859a79e6d947cd/tf_semantic_segmentation-0.2.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "bc67f23cc6eab03d07196578d82a9fbe", "sha256": "d0b72625b8ca26c238b81c22b847e914a9bd6825d4fed2567bdb7e1c79cbc488"}, "downloads": -1, "filename": "tf_semantic_segmentation-0.2.3-py3-none-any.whl", "has_sig": false, "md5_digest": "bc67f23cc6eab03d07196578d82a9fbe", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 149274, "upload_time": "2020-03-06T14:04:24", "upload_time_iso_8601": "2020-03-06T14:04:24.367262Z", "url": "https://files.pythonhosted.org/packages/fb/d9/0d699c714de97b75a69ce920a44c6a93f388931c3894a2f8b159b33233e0/tf_semantic_segmentation-0.2.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7d24ebe7d3c6e64a4a9d59941ecf45a2", "sha256": "27e70c5945db37ad303989e75d4e2aed1654d6313daff9aab19f79e18a076ca3"}, "downloads": -1, "filename": "tf_semantic_segmentation-0.2.3.tar.gz", "has_sig": false, "md5_digest": "7d24ebe7d3c6e64a4a9d59941ecf45a2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 95933, "upload_time": "2020-03-06T14:04:25", "upload_time_iso_8601": "2020-03-06T14:04:25.861212Z", "url": "https://files.pythonhosted.org/packages/04/d1/12f07405ec75028efc3768d1b2eb93f26bd3289c1b54be859a79e6d947cd/tf_semantic_segmentation-0.2.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:54:22 2020"}