{"info": {"author": "ProxyCrawl", "author_email": "info@proxycrawl.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Topic :: Utilities"], "description": "# ProxyCrawl API Python class\n\nA lightweight, dependency free Python class that acts as wrapper for ProxyCrawl API.\n\n## Installing\n\nChoose a way of installing:\n\n- Download the python class from Github.\n- Or use [PyPi](https://pypi.org/project/proxycrawl/) Python package manager. `pip install proxycrawl`\n\nThen import the ProxyCrawlAPI\n\nPython2:\n\n```python\nfrom proxycrawl import ProxyCrawlAPI\n```\n\nPython3:\n\n```python\nfrom proxycrawl.proxycrawl_api import ProxyCrawlAPI\n```\n\n## Class usage\n\nFirst initialize the ProxyCrawlAPI class\n\n```python\napi = ProxyCrawlAPI({ 'token': 'YOUR_PROXYCRAWL_TOKEN' })\n```\n\n### GET requests\n\nPass the url that you want to scrape plus any options from the ones available in the [API documentation](https://proxycrawl.com/dashboard/docs).\n\n```python\napi.get(url, options = {})\n```\n\nExample:\n\n```python\nresponse = api.get('https://www.facebook.com/britneyspears')\nif response['status_code'] == 200:\n    print(response['body'])\n```\n\nYou can pass any options from ProxyCrawl API.\n\nExample:\n\n```python\nresponse = api.get('https://www.reddit.com/r/pics/comments/5bx4bx/thanks_obama/', {\n    'user_agent': 'Mozilla/5.0 (Windows NT 6.2; rv:20.0) Gecko/20121202 Firefox/30.0',\n    'format': 'json'\n})\nif response['status_code'] == 200:\n    print(response['body'])\n```\n\n### POST requests\n\nPass the url that you want to scrape, the data that you want to send which can be either a json or a string, plus any options from the ones available in the [API documentation](https://proxycrawl.com/dashboard/docs).\n\n```python\napi.post(url, dictionary or string data, options = {})\n```\n\nExample:\n\n```python\nresponse = api.post('https://producthunt.com/search', { 'text': 'example search' })\nif response['status_code'] == 200:\n    print(response['body'])\n```\n\nYou can send the data as `application/json` instead of `x-www-form-urlencoded` by setting option `post_content_type` as json.\n\n```python\nimport json\nresponse = api.post('https://httpbin.org/post', json.dumps({ 'some_json': 'with some value' }), { 'post_content_type': 'json' })\nif response['status_code'] == 200:\n    print(response['body'])\n```\n\n### Javascript requests\n\nIf you need to scrape any website built with Javascript like React, Angular, Vue, etc. You just need to pass your javascript token and use the same calls. Note that only `.get` is available for javascript and not `.post`.\n\n```python\napi = ProxyCrawlAPI({ 'token': 'YOUR_JAVASCRIPT_TOKEN' })\n```\n\n```python\nresponse = api.get('https://www.nfl.com')\nif response['status_code'] == 200:\n    print(response['body'])\n```\n\nSame way you can pass javascript additional options.\n\n```python\nresponse = api.get('https://www.freelancer.com', { 'page_wait': 5000 })\nif response['status_code'] == 200:\n    print(response['body'])\n```\n\n## Original status\n\nYou can always get the original status and proxycrawl status from the response. Read the [ProxyCrawl documentation](https://proxycrawl.com/dashboard/docs) to learn more about those status.\n\n```python\nresponse = api.get('https://craiglist.com')\nprint(response['headers']['original_status'])\nprint(response['headers']['pc_status'])\n```\n\nIf you have questions or need help using the library, please open an issue or [contact us](https://proxycrawl.com/contact).\n\n## Custom timeout\n\nIf you need to use a custom timeout, you can pass it to the class instance creation like the following:\n\n```python\napi = ProxyCrawlAPI({ 'token': 'TOKEN', 'timeout': 120 })\n```\n\nTimeout is in seconds.\n\n---\n\nCopyright 2020 ProxyCrawl\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/proxycrawl/proxycrawl-python", "keywords": "scraping scraper crawler crawling proxycrawl api", "license": "Apache-2.0", "maintainer": "", "maintainer_email": "", "name": "proxycrawl", "package_url": "https://pypi.org/project/proxycrawl/", "platform": "", "project_url": "https://pypi.org/project/proxycrawl/", "project_urls": {"Homepage": "https://github.com/proxycrawl/proxycrawl-python"}, "release_url": "https://pypi.org/project/proxycrawl/2.1.0/", "requires_dist": null, "requires_python": "", "summary": "A Python class that acts as wrapper for ProxyCrawl scraping and crawling API", "version": "2.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>ProxyCrawl API Python class</h1>\n<p>A lightweight, dependency free Python class that acts as wrapper for ProxyCrawl API.</p>\n<h2>Installing</h2>\n<p>Choose a way of installing:</p>\n<ul>\n<li>Download the python class from Github.</li>\n<li>Or use <a href=\"https://pypi.org/project/proxycrawl/\" rel=\"nofollow\">PyPi</a> Python package manager. <code>pip install proxycrawl</code></li>\n</ul>\n<p>Then import the ProxyCrawlAPI</p>\n<p>Python2:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">proxycrawl</span> <span class=\"kn\">import</span> <span class=\"n\">ProxyCrawlAPI</span>\n</pre>\n<p>Python3:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">proxycrawl.proxycrawl_api</span> <span class=\"kn\">import</span> <span class=\"n\">ProxyCrawlAPI</span>\n</pre>\n<h2>Class usage</h2>\n<p>First initialize the ProxyCrawlAPI class</p>\n<pre><span class=\"n\">api</span> <span class=\"o\">=</span> <span class=\"n\">ProxyCrawlAPI</span><span class=\"p\">({</span> <span class=\"s1\">'token'</span><span class=\"p\">:</span> <span class=\"s1\">'YOUR_PROXYCRAWL_TOKEN'</span> <span class=\"p\">})</span>\n</pre>\n<h3>GET requests</h3>\n<p>Pass the url that you want to scrape plus any options from the ones available in the <a href=\"https://proxycrawl.com/dashboard/docs\" rel=\"nofollow\">API documentation</a>.</p>\n<pre><span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">options</span> <span class=\"o\">=</span> <span class=\"p\">{})</span>\n</pre>\n<p>Example:</p>\n<pre><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'https://www.facebook.com/britneyspears'</span><span class=\"p\">)</span>\n<span class=\"k\">if</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'status_code'</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"mi\">200</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'body'</span><span class=\"p\">])</span>\n</pre>\n<p>You can pass any options from ProxyCrawl API.</p>\n<p>Example:</p>\n<pre><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'https://www.reddit.com/r/pics/comments/5bx4bx/thanks_obama/'</span><span class=\"p\">,</span> <span class=\"p\">{</span>\n    <span class=\"s1\">'user_agent'</span><span class=\"p\">:</span> <span class=\"s1\">'Mozilla/5.0 (Windows NT 6.2; rv:20.0) Gecko/20121202 Firefox/30.0'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'format'</span><span class=\"p\">:</span> <span class=\"s1\">'json'</span>\n<span class=\"p\">})</span>\n<span class=\"k\">if</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'status_code'</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"mi\">200</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'body'</span><span class=\"p\">])</span>\n</pre>\n<h3>POST requests</h3>\n<p>Pass the url that you want to scrape, the data that you want to send which can be either a json or a string, plus any options from the ones available in the <a href=\"https://proxycrawl.com/dashboard/docs\" rel=\"nofollow\">API documentation</a>.</p>\n<pre><span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">dictionary</span> <span class=\"ow\">or</span> <span class=\"n\">string</span> <span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">options</span> <span class=\"o\">=</span> <span class=\"p\">{})</span>\n</pre>\n<p>Example:</p>\n<pre><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"s1\">'https://producthunt.com/search'</span><span class=\"p\">,</span> <span class=\"p\">{</span> <span class=\"s1\">'text'</span><span class=\"p\">:</span> <span class=\"s1\">'example search'</span> <span class=\"p\">})</span>\n<span class=\"k\">if</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'status_code'</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"mi\">200</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'body'</span><span class=\"p\">])</span>\n</pre>\n<p>You can send the data as <code>application/json</code> instead of <code>x-www-form-urlencoded</code> by setting option <code>post_content_type</code> as json.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">post</span><span class=\"p\">(</span><span class=\"s1\">'https://httpbin.org/post'</span><span class=\"p\">,</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">({</span> <span class=\"s1\">'some_json'</span><span class=\"p\">:</span> <span class=\"s1\">'with some value'</span> <span class=\"p\">}),</span> <span class=\"p\">{</span> <span class=\"s1\">'post_content_type'</span><span class=\"p\">:</span> <span class=\"s1\">'json'</span> <span class=\"p\">})</span>\n<span class=\"k\">if</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'status_code'</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"mi\">200</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'body'</span><span class=\"p\">])</span>\n</pre>\n<h3>Javascript requests</h3>\n<p>If you need to scrape any website built with Javascript like React, Angular, Vue, etc. You just need to pass your javascript token and use the same calls. Note that only <code>.get</code> is available for javascript and not <code>.post</code>.</p>\n<pre><span class=\"n\">api</span> <span class=\"o\">=</span> <span class=\"n\">ProxyCrawlAPI</span><span class=\"p\">({</span> <span class=\"s1\">'token'</span><span class=\"p\">:</span> <span class=\"s1\">'YOUR_JAVASCRIPT_TOKEN'</span> <span class=\"p\">})</span>\n</pre>\n<pre><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'https://www.nfl.com'</span><span class=\"p\">)</span>\n<span class=\"k\">if</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'status_code'</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"mi\">200</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'body'</span><span class=\"p\">])</span>\n</pre>\n<p>Same way you can pass javascript additional options.</p>\n<pre><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'https://www.freelancer.com'</span><span class=\"p\">,</span> <span class=\"p\">{</span> <span class=\"s1\">'page_wait'</span><span class=\"p\">:</span> <span class=\"mi\">5000</span> <span class=\"p\">})</span>\n<span class=\"k\">if</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'status_code'</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"mi\">200</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'body'</span><span class=\"p\">])</span>\n</pre>\n<h2>Original status</h2>\n<p>You can always get the original status and proxycrawl status from the response. Read the <a href=\"https://proxycrawl.com/dashboard/docs\" rel=\"nofollow\">ProxyCrawl documentation</a> to learn more about those status.</p>\n<pre><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'https://craiglist.com'</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'headers'</span><span class=\"p\">][</span><span class=\"s1\">'original_status'</span><span class=\"p\">])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s1\">'headers'</span><span class=\"p\">][</span><span class=\"s1\">'pc_status'</span><span class=\"p\">])</span>\n</pre>\n<p>If you have questions or need help using the library, please open an issue or <a href=\"https://proxycrawl.com/contact\" rel=\"nofollow\">contact us</a>.</p>\n<h2>Custom timeout</h2>\n<p>If you need to use a custom timeout, you can pass it to the class instance creation like the following:</p>\n<pre><span class=\"n\">api</span> <span class=\"o\">=</span> <span class=\"n\">ProxyCrawlAPI</span><span class=\"p\">({</span> <span class=\"s1\">'token'</span><span class=\"p\">:</span> <span class=\"s1\">'TOKEN'</span><span class=\"p\">,</span> <span class=\"s1\">'timeout'</span><span class=\"p\">:</span> <span class=\"mi\">120</span> <span class=\"p\">})</span>\n</pre>\n<p>Timeout is in seconds.</p>\n<hr>\n<p>Copyright 2020 ProxyCrawl</p>\n\n          </div>"}, "last_serial": 6577201, "releases": {"1.0.0": [], "1.0.1": [{"comment_text": "", "digests": {"md5": "fc4106271e1c3d62cafb873e32eca497", "sha256": "cc856fa8535e83fd210469ad8662c1bdafe28d7529859e57ab7d11850591cb57"}, "downloads": -1, "filename": "proxycrawl-1.0.1-py2-none-any.whl", "has_sig": false, "md5_digest": "fc4106271e1c3d62cafb873e32eca497", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 3641, "upload_time": "2018-05-31T12:25:27", "upload_time_iso_8601": "2018-05-31T12:25:27.166431Z", "url": "https://files.pythonhosted.org/packages/db/97/6e5b5b5e0193ef70acf623fd183bd5ba72b8c49e9414898a4a33da469bfe/proxycrawl-1.0.1-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b05cd3109b39f6b75477f5d2b56bb08d", "sha256": "864a9f639313dd980a982aab90b6aa546864db9ccd14780ebb6ff1c8ab0af633"}, "downloads": -1, "filename": "proxycrawl-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "b05cd3109b39f6b75477f5d2b56bb08d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 3641, "upload_time": "2018-05-31T12:25:28", "upload_time_iso_8601": "2018-05-31T12:25:28.119445Z", "url": "https://files.pythonhosted.org/packages/2e/a9/988421ef028df1902918d4c5ce2454b94bddebcb08f3586470ef3f2d9422/proxycrawl-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "086ae4a10aada7a8ccd328162ac955f0", "sha256": "bd241f3feb7603b778d18d70e64508e6c14c3bbf802cd25490ebc09866b9c4d6"}, "downloads": -1, "filename": "proxycrawl-1.0.1.tar.gz", "has_sig": false, "md5_digest": "086ae4a10aada7a8ccd328162ac955f0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3329, "upload_time": "2018-05-31T12:25:29", "upload_time_iso_8601": "2018-05-31T12:25:29.388940Z", "url": "https://files.pythonhosted.org/packages/d5/c4/b79a09ea0a1dbb8e35deb83da9e52fe8f80ab8280939e1acde14df00e6f7/proxycrawl-1.0.1.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "19251a9669e3a621769dfe0b07b6ace9", "sha256": "dd59831a17f75708b4ce96f8ecde4e9aa26db6da9e3b40c5befb33eb860aae3f"}, "downloads": -1, "filename": "proxycrawl-1.1.0-py2-none-any.whl", "has_sig": false, "md5_digest": "19251a9669e3a621769dfe0b07b6ace9", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 3636, "upload_time": "2018-06-07T09:38:24", "upload_time_iso_8601": "2018-06-07T09:38:24.443255Z", "url": "https://files.pythonhosted.org/packages/1b/05/5340eaabd2c71892672b5b8755d2e6e89166c36688b046960cc018575e06/proxycrawl-1.1.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c2ce9139746c0c94d3ae3af3d978cd7a", "sha256": "00e9d11681277a15869da6fbde154e4368af28ba28f1583e27840166fc6a1dbd"}, "downloads": -1, "filename": "proxycrawl-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "c2ce9139746c0c94d3ae3af3d978cd7a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 3637, "upload_time": "2018-06-07T09:38:25", "upload_time_iso_8601": "2018-06-07T09:38:25.701328Z", "url": "https://files.pythonhosted.org/packages/37/49/2904d0b2d078f293e3cb35c7262e1be7f86ee9b97406a11c1146025c88de/proxycrawl-1.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "48221f8a3a193f20136d0ef4ecbc5a16", "sha256": "d16ff3097c3317f588c6af057f4a88deae089b515ab3a45cd5ee906086e013a0"}, "downloads": -1, "filename": "proxycrawl-1.1.0.tar.gz", "has_sig": false, "md5_digest": "48221f8a3a193f20136d0ef4ecbc5a16", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3324, "upload_time": "2018-06-07T09:38:27", "upload_time_iso_8601": "2018-06-07T09:38:27.151646Z", "url": "https://files.pythonhosted.org/packages/81/0e/b89ee41dcddec38219563055270f4fe3977831fc035ef07b2209b052d166/proxycrawl-1.1.0.tar.gz", "yanked": false}], "1.1.1": [{"comment_text": "", "digests": {"md5": "7b882d5433ba710d26f14e01e1c4c975", "sha256": "61dcec13fcb27acccb68e4af290b224a4d97ed5dcac243becd7c8749cabf5e0b"}, "downloads": -1, "filename": "proxycrawl-1.1.1-py2-none-any.whl", "has_sig": false, "md5_digest": "7b882d5433ba710d26f14e01e1c4c975", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 3712, "upload_time": "2018-06-28T11:51:51", "upload_time_iso_8601": "2018-06-28T11:51:51.988016Z", "url": "https://files.pythonhosted.org/packages/71/93/cc15ab0aeed96c95c6dc45f48995099562f0e820cd59f5634e7a4cb7ac0f/proxycrawl-1.1.1-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d163ccd027bf378e74a08cd3e6c06a5c", "sha256": "b2b8c85954292cc505b004bed8f1ffcfa495f64eaed04ba112dffa451392249e"}, "downloads": -1, "filename": "proxycrawl-1.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "d163ccd027bf378e74a08cd3e6c06a5c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 3711, "upload_time": "2018-06-28T11:51:53", "upload_time_iso_8601": "2018-06-28T11:51:53.205609Z", "url": "https://files.pythonhosted.org/packages/41/1c/5170fc9ea41d026b6c60f988a61d3f4cdb766351724d1805f105ab898fc7/proxycrawl-1.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bf4b41ef91fdb6c2ecb2f7e95e6099be", "sha256": "9a037eee9bdd9206a98907004197e3d52e42b7560b1f3e286f27fe1476003265"}, "downloads": -1, "filename": "proxycrawl-1.1.1.tar.gz", "has_sig": false, "md5_digest": "bf4b41ef91fdb6c2ecb2f7e95e6099be", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3376, "upload_time": "2018-06-28T11:51:55", "upload_time_iso_8601": "2018-06-28T11:51:55.382974Z", "url": "https://files.pythonhosted.org/packages/73/33/c1e464505f8fe23a4cbf4d99c1e2c310c94eea3d3e4e0b478c20709230b2/proxycrawl-1.1.1.tar.gz", "yanked": false}], "2.0.0": [{"comment_text": "", "digests": {"md5": "9d3eb32aba73ec56fc833863e8cbe492", "sha256": "78b64ff6f18270f5b2609d0e3c81887248841b664f66d1da7737f5768033dd2a"}, "downloads": -1, "filename": "proxycrawl-2.0.0-py2-none-any.whl", "has_sig": false, "md5_digest": "9d3eb32aba73ec56fc833863e8cbe492", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 8095, "upload_time": "2019-05-30T07:51:53", "upload_time_iso_8601": "2019-05-30T07:51:53.432768Z", "url": "https://files.pythonhosted.org/packages/61/c3/bcd29ff03377e6a466ddb22a45b7b5c1ecb9cec57dc46b8d997570950689/proxycrawl-2.0.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5796b7e7ffeaca5642b449ba63cd3791", "sha256": "52b8db25c8133a6723e2f827fc4065be3c4eddf3ea3e4ee3f551f058b717532d"}, "downloads": -1, "filename": "proxycrawl-2.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "5796b7e7ffeaca5642b449ba63cd3791", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8096, "upload_time": "2019-05-30T07:51:55", "upload_time_iso_8601": "2019-05-30T07:51:55.203428Z", "url": "https://files.pythonhosted.org/packages/14/7f/cc3fbe6b458ce0dfc6aca67962547547a8a3d8a5526a8c0e2f84e0f454e5/proxycrawl-2.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "eb760018b0d7a8d52982d5fa683dcfc9", "sha256": "00c1d914c5299fb126a12a0a8963f5a6d2b28148b9029932c64d932bdc1294a3"}, "downloads": -1, "filename": "proxycrawl-2.0.0.tar.gz", "has_sig": false, "md5_digest": "eb760018b0d7a8d52982d5fa683dcfc9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3618, "upload_time": "2019-05-30T07:51:56", "upload_time_iso_8601": "2019-05-30T07:51:56.828044Z", "url": "https://files.pythonhosted.org/packages/e4/0b/f731ed62d984fb458bb303d362494a4df1a568d636ec40dedf0d78278c79/proxycrawl-2.0.0.tar.gz", "yanked": false}], "2.0.1": [{"comment_text": "", "digests": {"md5": "def3bc4dfcf068720b1e10f17a5f7c84", "sha256": "bbf4c819c8c3c40cefa59da2d1a97e79841c4f4856077f53ca6a6877f5577a3b"}, "downloads": -1, "filename": "proxycrawl-2.0.1-py2-none-any.whl", "has_sig": false, "md5_digest": "def3bc4dfcf068720b1e10f17a5f7c84", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 8096, "upload_time": "2019-08-06T08:24:52", "upload_time_iso_8601": "2019-08-06T08:24:52.422978Z", "url": "https://files.pythonhosted.org/packages/38/13/2986d14d17a13008f08ab4fd4c18dad714606e07130c174868bd20f6c7c9/proxycrawl-2.0.1-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ee7eb7d5d1ce06f181a79faad9fad160", "sha256": "b154117cd9f94449b618f7d590b2357cb8841bcc1a9f14607a70d730acb91720"}, "downloads": -1, "filename": "proxycrawl-2.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "ee7eb7d5d1ce06f181a79faad9fad160", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8093, "upload_time": "2019-08-06T08:24:54", "upload_time_iso_8601": "2019-08-06T08:24:54.196482Z", "url": "https://files.pythonhosted.org/packages/49/3a/b10a4c43ce4bebc1ec1ab716bec69a9175f4440f215647ac3102149916b6/proxycrawl-2.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f94936a661918109f7d32236ba139b91", "sha256": "03c97e44f7a24f3c561b53053f3685a5ee58f348cb441105dacb19ba0591f4c6"}, "downloads": -1, "filename": "proxycrawl-2.0.1.tar.gz", "has_sig": false, "md5_digest": "f94936a661918109f7d32236ba139b91", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3617, "upload_time": "2019-08-06T08:24:57", "upload_time_iso_8601": "2019-08-06T08:24:57.165660Z", "url": "https://files.pythonhosted.org/packages/bd/f2/67918850f8bf968f53be76dfaadd313e1525edfe121cd2ac57958f4487c9/proxycrawl-2.0.1.tar.gz", "yanked": false}], "2.0.3": [{"comment_text": "", "digests": {"md5": "3559edef5961c2e9abcdfaeed998e5e8", "sha256": "740aa02507e02af3886150b40491bc049bfca7d90ad35958158ee1b0727487ef"}, "downloads": -1, "filename": "proxycrawl-2.0.3-py2-none-any.whl", "has_sig": false, "md5_digest": "3559edef5961c2e9abcdfaeed998e5e8", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 8155, "upload_time": "2019-10-24T04:02:43", "upload_time_iso_8601": "2019-10-24T04:02:43.291876Z", "url": "https://files.pythonhosted.org/packages/a6/1d/ee4e6376d208e3763c66fc6df76acf8547d0876cf85b369d25626637e333/proxycrawl-2.0.3-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "13b9dc438ce4484cd52f6a10c6f1746a", "sha256": "d9a368eb5df961f7d7b5b4fad8723d62b1f87dba53ab8ef1f630309dbf33fef3"}, "downloads": -1, "filename": "proxycrawl-2.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "13b9dc438ce4484cd52f6a10c6f1746a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8157, "upload_time": "2019-10-24T04:02:45", "upload_time_iso_8601": "2019-10-24T04:02:45.442023Z", "url": "https://files.pythonhosted.org/packages/82/97/fe0e82de4e7470ff8e100e58e6ef4c5811988a2bca0ae9554cd5980f9ab3/proxycrawl-2.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6fbabc9c38e62ef271f7a1e80b8cd8e3", "sha256": "c20528c470100fc792bad623a107a91e90a2d34057c8dfaec272498e7a98748b"}, "downloads": -1, "filename": "proxycrawl-2.0.3.tar.gz", "has_sig": false, "md5_digest": "6fbabc9c38e62ef271f7a1e80b8cd8e3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3704, "upload_time": "2019-10-24T04:02:47", "upload_time_iso_8601": "2019-10-24T04:02:47.386778Z", "url": "https://files.pythonhosted.org/packages/c0/d5/d6747020d74697c646775b22bc700ae334587fa7a9fa55f3cb9013b7e9f8/proxycrawl-2.0.3.tar.gz", "yanked": false}], "2.1.0": [{"comment_text": "", "digests": {"md5": "f99c6fa7d273446bf4b36c51089e1e62", "sha256": "ac5ea70886027f44b6c63984fa8afb2f404613e0637558b0b4440be30d0a69f3"}, "downloads": -1, "filename": "proxycrawl-2.1.0-py2-none-any.whl", "has_sig": false, "md5_digest": "f99c6fa7d273446bf4b36c51089e1e62", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 8248, "upload_time": "2020-02-05T17:18:41", "upload_time_iso_8601": "2020-02-05T17:18:41.033757Z", "url": "https://files.pythonhosted.org/packages/76/59/48c9c4e82199aeb88d0483918213582df5b3c8f8c047ca3c43e53ec45b56/proxycrawl-2.1.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8863cf99e8a02256f1667b1f3239aeb2", "sha256": "f24b15337d3f6081dfe00a228a2f26eda8db5b647db6a0a4077881883451b470"}, "downloads": -1, "filename": "proxycrawl-2.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "8863cf99e8a02256f1667b1f3239aeb2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8248, "upload_time": "2020-02-05T17:18:43", "upload_time_iso_8601": "2020-02-05T17:18:43.718779Z", "url": "https://files.pythonhosted.org/packages/4d/85/2f3fa1c4ba3e8f3ce853ef33827dc7325405e714b55000136f4b3155f7be/proxycrawl-2.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "297eea115d8a248b6e639731b0836004", "sha256": "a7b34ef172f32f1b5ff4539288c16ee75662fcd587e65810e9e8041df6e53080"}, "downloads": -1, "filename": "proxycrawl-2.1.0.tar.gz", "has_sig": false, "md5_digest": "297eea115d8a248b6e639731b0836004", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3793, "upload_time": "2020-02-05T17:18:48", "upload_time_iso_8601": "2020-02-05T17:18:48.430980Z", "url": "https://files.pythonhosted.org/packages/7d/be/87b90283d2e1eed8ae496ed03ac1dff38a1202b2c6efbe250056f7641e85/proxycrawl-2.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f99c6fa7d273446bf4b36c51089e1e62", "sha256": "ac5ea70886027f44b6c63984fa8afb2f404613e0637558b0b4440be30d0a69f3"}, "downloads": -1, "filename": "proxycrawl-2.1.0-py2-none-any.whl", "has_sig": false, "md5_digest": "f99c6fa7d273446bf4b36c51089e1e62", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 8248, "upload_time": "2020-02-05T17:18:41", "upload_time_iso_8601": "2020-02-05T17:18:41.033757Z", "url": "https://files.pythonhosted.org/packages/76/59/48c9c4e82199aeb88d0483918213582df5b3c8f8c047ca3c43e53ec45b56/proxycrawl-2.1.0-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8863cf99e8a02256f1667b1f3239aeb2", "sha256": "f24b15337d3f6081dfe00a228a2f26eda8db5b647db6a0a4077881883451b470"}, "downloads": -1, "filename": "proxycrawl-2.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "8863cf99e8a02256f1667b1f3239aeb2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8248, "upload_time": "2020-02-05T17:18:43", "upload_time_iso_8601": "2020-02-05T17:18:43.718779Z", "url": "https://files.pythonhosted.org/packages/4d/85/2f3fa1c4ba3e8f3ce853ef33827dc7325405e714b55000136f4b3155f7be/proxycrawl-2.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "297eea115d8a248b6e639731b0836004", "sha256": "a7b34ef172f32f1b5ff4539288c16ee75662fcd587e65810e9e8041df6e53080"}, "downloads": -1, "filename": "proxycrawl-2.1.0.tar.gz", "has_sig": false, "md5_digest": "297eea115d8a248b6e639731b0836004", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3793, "upload_time": "2020-02-05T17:18:48", "upload_time_iso_8601": "2020-02-05T17:18:48.430980Z", "url": "https://files.pythonhosted.org/packages/7d/be/87b90283d2e1eed8ae496ed03ac1dff38a1202b2c6efbe250056f7641e85/proxycrawl-2.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:16:20 2020"}