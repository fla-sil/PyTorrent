{"info": {"author": "Jai Juneja", "author_email": "jai.juneja@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Operating System :: OS Independent", "Programming Language :: Python", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "Reddit NLP Package |Build Status|\n=================================\n\nA lightweight Python module that performs tokenization and processing of\ntext on Reddit. It allows you to analyze users, titles, comments and\nsubreddits to understand their vocabulary. The module comes packaged\nwith its own inverted index builder for storing vocabularies and word\nfrequencies, such that you can generate and manipulate large corpora of\ntf-idf weighted words without worrying about implementation. This is\nespecially useful if you're running scripts over long periods and wish\nto save intermediary results.\n\nLicense\n-------\n\nCopyright 2014 Jai Juneja.\n\nThis program is free software: you can redistribute it and/or modify it\nunder the terms of the GNU General Public License as published by the\nFree Software Foundation, either version 3 of the License, or (at your\noption) any later version.\n\nThis program is distributed in the hope that it will be useful, but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General\nPublic License for more details.\n\nYou should have received a copy of the GNU General Public License along\nwith this program. If not, see http://www.gnu.org/licenses/.\n\nInstallation\n------------\n\nUsing pip or easy\\_install\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can download the latest release version using ``pip`` or\n``easy_install``:\n\n::\n\n    pip install redditnlp\n\nLatest development version\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can alternatively download the latest development version directly\nfrom GitHub:\n\n::\n\n    git clone https://github.com/jaijuneja/reddit-nlp.git\n\nChange into the root directory:\n\n::\n\n    cd reddit-nlp\n\nThen install the package:\n\n::\n\n    python setup.py install\n\nError: the required version of setuptools is not available\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nUpon running ``pip install`` or the ``setup.py`` script you might get a\nmessage like this:\n\n::\n\n    The required version of setuptools (>=0.7) is not available, and can't be installed while this script is running. Please install a more recent version first, using 'easy_install -U setuptools'.\n\nThis is appearing because you have a very outdated version of the\nsetuptools package. The redditnlp package typically bootstraps a newer\nversion of setuptools during install, but it isn't working in this case.\nYou need to update setuptools using ``easy_install -U setuptools`` (you\nmay need to apply ``sudo`` to this command).\n\nIf the above command doesn't do anything then it is likely that your\nversion of setuptools was installed using a package manager such yum,\napt or pip. Check your package manager for a package called\npython-setuptools or try ``pip install setuptools --upgrade`` and then\nre-run the install.\n\nUsage\n-----\n\nA more complex sample program using the redditnlp module can be found at\n``https://github.com/jaijuneja/reddit-nlp/blob/master/example.py``. Here\nwe outline a basic word counter application.\n\nThe module consists of three classes:\n\n-  A basic word counter class, ``WordCounter``, which performs\n   tokenization and counting on input strings\n-  A Reddit word counter, ``RedditWordCounter``, which extends the\n   ``WordCounter`` class to allow interaction with the Reddit API\n-  A tf-idf corpus builder, which allows storing of large word corpora\n   in an inverted index\n\nThese three classes can be instantiated as follows:\n\n.. code:: python\n\n    from redditnlp import WordCounter, RedditWordCounter, TfidfCorpus\n\n    word_counter = WordCounter()\n    reddit_counter = RedditWordCounter('your_username')\n    corpus = TfidfCorpus()\n\nTo adhere to the Reddit API rules, it is asked that you use your actual\nReddit username in place of ``'your_username'`` above.\n\nFor further information on the attributes and methods of these classes\nyou can run:\n\n.. code:: python\n\n    help(WordCounter)\n    help(RedditWordCounter)\n    help(TfidfCorpus)\n\nNext, we can tokenize 1000 comments from a selection of subreddits,\nextract the most common words and save all of our data to disk:\n\n.. code:: python\n\n    for subreddit in ['funny', 'aww', 'pics']:\n        # Tokenize and count words for 1000 comments\n        word_counts = counter.subreddit_comments(subreddit, limit=1000)\n        \n        # Add the word counts to our corpus\n        corpus.add_document(word_counts, subreddit)\n\n    # Save the corpus to a specified path (must be JSON)\n    corpus.save(path='word_counts.json')\n\n    # Save the top 50 words (by tf-idf score) from each subreddit to a text file\n    for subreddit in corpus.get_document_list():\n        top_words = corpus.get_top_terms(document, num_terms=50)\n        with open('top_words.txt', 'ab') as f:\n            f.write(document + '\\n' + '\\n'.join(top_words.keys()))\n\nMachine learning\n~~~~~~~~~~~~~~~~\n\n``redditnlp`` now supports some of scikit-learn's machine learning\ncapability. Several in-built functions enable the user to:\n\n-  Convert a TfidfCorpus object into a scipy sparse feature matrix\n   (using ``build_feature_matrix()``)\n-  Train a classifier using the documents contained in a TfidfCorpus\n   (with ``train_classifier()``) and thereafter classify new documents\n   (with ``classify_document()``)\n\nBelow is an example of a simple machine learning application that loads\na corpus of subreddit comment data, uses it to train a classifier and\ndetermines which subreddit a user's comments most closely match:\n\n.. code:: python\n\n    # Load the corpus of subreddit comment data and use it to train a classifier\n    corpus = TfidfCorpus('path/to/subreddit_corpus.json')\n    corpus.train_classifier(classifier_type='LinearSVC', tfidf=True)\n\n    # Tokenize all of your comments\n    counter = RedditWordCounter('your_username')\n    user_comments = counter.user_comments('your_username')\n\n    # Classify your comments against the documents in the corpus\n    print corpus.classify_document(user_comments)\n\nMultiprocessing\n~~~~~~~~~~~~~~~\n\n``redditnlp`` uses the `PRAW <https://github.com/praw-dev/praw>`__\nReddit API wrapper. It supports multiprocessing, such that you can run\nmultiple instances of ``RedditWordCounter`` without exceeding Reddit's\nrate limit. There is more information about this in the `PRAW\ndocumentation <https://praw.readthedocs.org/en/latest/pages/multiprocess.html>`__\nbut for the sake of completeness an example is included below.\n\nFirst, you must initialise a request handling server on your local\nmachine. This is done using the terminal/command line:\n\n::\n\n    praw-multiprocess\n\nNext, you can instantiate multiple ``RedditWordCounter`` objects and set\nthe parameter ``multiprocess=True`` so that outgoing API calls are\nhandled:\n\n::\n\n    counter = RedditWordCounter('your_username', multiprocess=True)\n\nContact\n-------\n\nIf you have any questions or have encountered an error, feel free to\ncontact me at ``jai -dot- juneja -at- gmail -dot- com``.\n\n.. |Build Status| image:: https://travis-ci.org/jaijuneja/reddit-nlp.svg?branch=master\n   :target: https://travis-ci.org/jaijuneja/reddit-nlp", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/jaijuneja/reddit-nlp", "keywords": "reddit,natural language processing,machine learning", "license": "BSD", "maintainer": null, "maintainer_email": null, "name": "redditnlp", "package_url": "https://pypi.org/project/redditnlp/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/redditnlp/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/jaijuneja/reddit-nlp"}, "release_url": "https://pypi.org/project/redditnlp/0.1.3/", "requires_dist": null, "requires_python": null, "summary": "A tool to perform natural language processing of reddit content.", "version": "0.1.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>A lightweight Python module that performs tokenization and processing of\ntext on Reddit. It allows you to analyze users, titles, comments and\nsubreddits to understand their vocabulary. The module comes packaged\nwith its own inverted index builder for storing vocabularies and word\nfrequencies, such that you can generate and manipulate large corpora of\ntf-idf weighted words without worrying about implementation. This is\nespecially useful if you\u2019re running scripts over long periods and wish\nto save intermediary results.</p>\n<div id=\"license\">\n<h2>License</h2>\n<p>Copyright 2014 Jai Juneja.</p>\n<p>This program is free software: you can redistribute it and/or modify it\nunder the terms of the GNU General Public License as published by the\nFree Software Foundation, either version 3 of the License, or (at your\noption) any later version.</p>\n<p>This program is distributed in the hope that it will be useful, but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General\nPublic License for more details.</p>\n<p>You should have received a copy of the GNU General Public License along\nwith this program. If not, see <a href=\"http://www.gnu.org/licenses/\" rel=\"nofollow\">http://www.gnu.org/licenses/</a>.</p>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<div id=\"using-pip-or-easy-install\">\n<h3>Using pip or easy_install</h3>\n<p>You can download the latest release version using <tt>pip</tt> or\n<tt>easy_install</tt>:</p>\n<pre>pip install redditnlp\n</pre>\n</div>\n<div id=\"latest-development-version\">\n<h3>Latest development version</h3>\n<p>You can alternatively download the latest development version directly\nfrom GitHub:</p>\n<pre>git clone https://github.com/jaijuneja/reddit-nlp.git\n</pre>\n<p>Change into the root directory:</p>\n<pre>cd reddit-nlp\n</pre>\n<p>Then install the package:</p>\n<pre>python setup.py install\n</pre>\n</div>\n<div id=\"error-the-required-version-of-setuptools-is-not-available\">\n<h3>Error: the required version of setuptools is not available</h3>\n<p>Upon running <tt>pip install</tt> or the <tt>setup.py</tt> script you might get a\nmessage like this:</p>\n<pre>The required version of setuptools (&gt;=0.7) is not available, and can't be installed while this script is running. Please install a more recent version first, using 'easy_install -U setuptools'.\n</pre>\n<p>This is appearing because you have a very outdated version of the\nsetuptools package. The redditnlp package typically bootstraps a newer\nversion of setuptools during install, but it isn\u2019t working in this case.\nYou need to update setuptools using <tt>easy_install <span class=\"pre\">-U</span> setuptools</tt> (you\nmay need to apply <tt>sudo</tt> to this command).</p>\n<p>If the above command doesn\u2019t do anything then it is likely that your\nversion of setuptools was installed using a package manager such yum,\napt or pip. Check your package manager for a package called\npython-setuptools or try <tt>pip install setuptools <span class=\"pre\">--upgrade</span></tt> and then\nre-run the install.</p>\n</div>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>A more complex sample program using the redditnlp module can be found at\n<tt><span class=\"pre\">https://github.com/jaijuneja/reddit-nlp/blob/master/example.py</span></tt>. Here\nwe outline a basic word counter application.</p>\n<p>The module consists of three classes:</p>\n<ul>\n<li>A basic word counter class, <tt>WordCounter</tt>, which performs\ntokenization and counting on input strings</li>\n<li>A Reddit word counter, <tt>RedditWordCounter</tt>, which extends the\n<tt>WordCounter</tt> class to allow interaction with the Reddit API</li>\n<li>A tf-idf corpus builder, which allows storing of large word corpora\nin an inverted index</li>\n</ul>\n<p>These three classes can be instantiated as follows:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">redditnlp</span> <span class=\"kn\">import</span> <span class=\"n\">WordCounter</span><span class=\"p\">,</span> <span class=\"n\">RedditWordCounter</span><span class=\"p\">,</span> <span class=\"n\">TfidfCorpus</span>\n\n<span class=\"n\">word_counter</span> <span class=\"o\">=</span> <span class=\"n\">WordCounter</span><span class=\"p\">()</span>\n<span class=\"n\">reddit_counter</span> <span class=\"o\">=</span> <span class=\"n\">RedditWordCounter</span><span class=\"p\">(</span><span class=\"s1\">'your_username'</span><span class=\"p\">)</span>\n<span class=\"n\">corpus</span> <span class=\"o\">=</span> <span class=\"n\">TfidfCorpus</span><span class=\"p\">()</span>\n</pre>\n<p>To adhere to the Reddit API rules, it is asked that you use your actual\nReddit username in place of <tt>'your_username'</tt> above.</p>\n<p>For further information on the attributes and methods of these classes\nyou can run:</p>\n<pre><span class=\"n\">help</span><span class=\"p\">(</span><span class=\"n\">WordCounter</span><span class=\"p\">)</span>\n<span class=\"n\">help</span><span class=\"p\">(</span><span class=\"n\">RedditWordCounter</span><span class=\"p\">)</span>\n<span class=\"n\">help</span><span class=\"p\">(</span><span class=\"n\">TfidfCorpus</span><span class=\"p\">)</span>\n</pre>\n<p>Next, we can tokenize 1000 comments from a selection of subreddits,\nextract the most common words and save all of our data to disk:</p>\n<pre><span class=\"k\">for</span> <span class=\"n\">subreddit</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">'funny'</span><span class=\"p\">,</span> <span class=\"s1\">'aww'</span><span class=\"p\">,</span> <span class=\"s1\">'pics'</span><span class=\"p\">]:</span>\n    <span class=\"c1\"># Tokenize and count words for 1000 comments</span>\n    <span class=\"n\">word_counts</span> <span class=\"o\">=</span> <span class=\"n\">counter</span><span class=\"o\">.</span><span class=\"n\">subreddit_comments</span><span class=\"p\">(</span><span class=\"n\">subreddit</span><span class=\"p\">,</span> <span class=\"n\">limit</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Add the word counts to our corpus</span>\n    <span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">add_document</span><span class=\"p\">(</span><span class=\"n\">word_counts</span><span class=\"p\">,</span> <span class=\"n\">subreddit</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Save the corpus to a specified path (must be JSON)</span>\n<span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">'word_counts.json'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Save the top 50 words (by tf-idf score) from each subreddit to a text file</span>\n<span class=\"k\">for</span> <span class=\"n\">subreddit</span> <span class=\"ow\">in</span> <span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">get_document_list</span><span class=\"p\">():</span>\n    <span class=\"n\">top_words</span> <span class=\"o\">=</span> <span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">get_top_terms</span><span class=\"p\">(</span><span class=\"n\">document</span><span class=\"p\">,</span> <span class=\"n\">num_terms</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">)</span>\n    <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">'top_words.txt'</span><span class=\"p\">,</span> <span class=\"s1\">'ab'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n        <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">document</span> <span class=\"o\">+</span> <span class=\"s1\">'</span><span class=\"se\">\\n</span><span class=\"s1\">'</span> <span class=\"o\">+</span> <span class=\"s1\">'</span><span class=\"se\">\\n</span><span class=\"s1\">'</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">top_words</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">()))</span>\n</pre>\n<div id=\"machine-learning\">\n<h3>Machine learning</h3>\n<p><tt>redditnlp</tt> now supports some of scikit-learn\u2019s machine learning\ncapability. Several in-built functions enable the user to:</p>\n<ul>\n<li>Convert a TfidfCorpus object into a scipy sparse feature matrix\n(using <tt>build_feature_matrix()</tt>)</li>\n<li>Train a classifier using the documents contained in a TfidfCorpus\n(with <tt>train_classifier()</tt>) and thereafter classify new documents\n(with <tt>classify_document()</tt>)</li>\n</ul>\n<p>Below is an example of a simple machine learning application that loads\na corpus of subreddit comment data, uses it to train a classifier and\ndetermines which subreddit a user\u2019s comments most closely match:</p>\n<pre><span class=\"c1\"># Load the corpus of subreddit comment data and use it to train a classifier</span>\n<span class=\"n\">corpus</span> <span class=\"o\">=</span> <span class=\"n\">TfidfCorpus</span><span class=\"p\">(</span><span class=\"s1\">'path/to/subreddit_corpus.json'</span><span class=\"p\">)</span>\n<span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">train_classifier</span><span class=\"p\">(</span><span class=\"n\">classifier_type</span><span class=\"o\">=</span><span class=\"s1\">'LinearSVC'</span><span class=\"p\">,</span> <span class=\"n\">tfidf</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Tokenize all of your comments</span>\n<span class=\"n\">counter</span> <span class=\"o\">=</span> <span class=\"n\">RedditWordCounter</span><span class=\"p\">(</span><span class=\"s1\">'your_username'</span><span class=\"p\">)</span>\n<span class=\"n\">user_comments</span> <span class=\"o\">=</span> <span class=\"n\">counter</span><span class=\"o\">.</span><span class=\"n\">user_comments</span><span class=\"p\">(</span><span class=\"s1\">'your_username'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Classify your comments against the documents in the corpus</span>\n<span class=\"nb\">print</span> <span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">classify_document</span><span class=\"p\">(</span><span class=\"n\">user_comments</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"multiprocessing\">\n<h3>Multiprocessing</h3>\n<p><tt>redditnlp</tt> uses the <a href=\"https://github.com/praw-dev/praw\" rel=\"nofollow\">PRAW</a>\nReddit API wrapper. It supports multiprocessing, such that you can run\nmultiple instances of <tt>RedditWordCounter</tt> without exceeding Reddit\u2019s\nrate limit. There is more information about this in the <a href=\"https://praw.readthedocs.org/en/latest/pages/multiprocess.html\" rel=\"nofollow\">PRAW\ndocumentation</a>\nbut for the sake of completeness an example is included below.</p>\n<p>First, you must initialise a request handling server on your local\nmachine. This is done using the terminal/command line:</p>\n<pre>praw-multiprocess\n</pre>\n<p>Next, you can instantiate multiple <tt>RedditWordCounter</tt> objects and set\nthe parameter <tt>multiprocess=True</tt> so that outgoing API calls are\nhandled:</p>\n<pre>counter = RedditWordCounter('your_username', multiprocess=True)\n</pre>\n</div>\n</div>\n<div id=\"contact\">\n<h2>Contact</h2>\n<p>If you have any questions or have encountered an error, feel free to\ncontact me at <tt>jai <span class=\"pre\">-dot-</span> juneja <span class=\"pre\">-at-</span> gmail <span class=\"pre\">-dot-</span> com</tt>.</p>\n</div>\n\n          </div>"}, "last_serial": 1335176, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "c5822a639984a59a48eebbb27141a4bd", "sha256": "da4c5e6a80f30b980a1e897776a99d9220982a135130b3e3ab47ef999d4b11fa"}, "downloads": -1, "filename": "redditnlp-0.1-py2.7.egg", "has_sig": false, "md5_digest": "c5822a639984a59a48eebbb27141a4bd", "packagetype": "bdist_egg", "python_version": "2.7", "requires_python": null, "size": 18700, "upload_time": "2014-11-28T14:28:43", "upload_time_iso_8601": "2014-11-28T14:28:43.524855Z", "url": "https://files.pythonhosted.org/packages/50/b4/3fea7f6bfabff2e7902be5366d71cf7809c8b561f29a0879add18fa41259/redditnlp-0.1-py2.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "fd94a19de59b61839b5246d7c5652996", "sha256": "500d1d8836d68b541e599fd7ccf02a9e2abc2eaeb33f67bee5cac6d5246e8734"}, "downloads": -1, "filename": "redditnlp-0.1.tar.gz", "has_sig": false, "md5_digest": "fd94a19de59b61839b5246d7c5652996", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9580, "upload_time": "2014-11-28T14:28:41", "upload_time_iso_8601": "2014-11-28T14:28:41.394983Z", "url": "https://files.pythonhosted.org/packages/21/5e/ab91cf7c46b4fc61dd8ef47e8f71347a82650c3589dc973f84b13f3adb65/redditnlp-0.1.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "0a6826b6a5fa6242dd0de7518810398a", "sha256": "3180fed4d84fdcaa91717f9c479de95305c3468953631ddc34dea3fde795efd8"}, "downloads": -1, "filename": "redditnlp-0.1.1-py2.7.egg", "has_sig": false, "md5_digest": "0a6826b6a5fa6242dd0de7518810398a", "packagetype": "bdist_egg", "python_version": "2.7", "requires_python": null, "size": 31382, "upload_time": "2014-11-30T23:35:06", "upload_time_iso_8601": "2014-11-30T23:35:06.697830Z", "url": "https://files.pythonhosted.org/packages/42/f1/866bb01399c6eee9caeb08a1e45f0caa02d6323e6ef3b71b27dad45b5915/redditnlp-0.1.1-py2.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "e3f36ae0106b00fd3e8a3b02071940f7", "sha256": "67ab852bc76c23afb62d8bd16c69074ad3d8452b69d863181f3b37f4b558062e"}, "downloads": -1, "filename": "redditnlp-0.1.1.tar.gz", "has_sig": false, "md5_digest": "e3f36ae0106b00fd3e8a3b02071940f7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16847, "upload_time": "2014-11-30T23:34:59", "upload_time_iso_8601": "2014-11-30T23:34:59.883746Z", "url": "https://files.pythonhosted.org/packages/b9/bc/c36ff21dadffd4cabf8ab754f7a266a2af3b7b92d01db2a1b12f01b2f834/redditnlp-0.1.1.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "68e97b9ec8e0cf70490566030ee3033f", "sha256": "00c47cc02168530719f2254fb0004195d095102f28cf70344b5edcae19e69717"}, "downloads": -1, "filename": "redditnlp-0.1.3-py2.7.egg", "has_sig": false, "md5_digest": "68e97b9ec8e0cf70490566030ee3033f", "packagetype": "bdist_egg", "python_version": "2.7", "requires_python": null, "size": 34757, "upload_time": "2014-12-08T09:58:20", "upload_time_iso_8601": "2014-12-08T09:58:20.213424Z", "url": "https://files.pythonhosted.org/packages/bc/7a/32c006debfe0b2d5789e3c9377b5c8b57497a07f67ad8e58168ff0f5e75a/redditnlp-0.1.3-py2.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "75a788bfe61b94cdba142c75a04a8945", "sha256": "87f7aec22eb6f5ccf1c9b31f0b5a1d6103d7cf4551e457f8f6a7feed3f47d1e1"}, "downloads": -1, "filename": "redditnlp-0.1.3.tar.gz", "has_sig": false, "md5_digest": "75a788bfe61b94cdba142c75a04a8945", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20968, "upload_time": "2014-12-08T09:58:17", "upload_time_iso_8601": "2014-12-08T09:58:17.874849Z", "url": "https://files.pythonhosted.org/packages/e3/33/da7cc1d7eda4296e4925dfe78c9582ad5b8e7ac84d2d141fcec605c99fab/redditnlp-0.1.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "68e97b9ec8e0cf70490566030ee3033f", "sha256": "00c47cc02168530719f2254fb0004195d095102f28cf70344b5edcae19e69717"}, "downloads": -1, "filename": "redditnlp-0.1.3-py2.7.egg", "has_sig": false, "md5_digest": "68e97b9ec8e0cf70490566030ee3033f", "packagetype": "bdist_egg", "python_version": "2.7", "requires_python": null, "size": 34757, "upload_time": "2014-12-08T09:58:20", "upload_time_iso_8601": "2014-12-08T09:58:20.213424Z", "url": "https://files.pythonhosted.org/packages/bc/7a/32c006debfe0b2d5789e3c9377b5c8b57497a07f67ad8e58168ff0f5e75a/redditnlp-0.1.3-py2.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "75a788bfe61b94cdba142c75a04a8945", "sha256": "87f7aec22eb6f5ccf1c9b31f0b5a1d6103d7cf4551e457f8f6a7feed3f47d1e1"}, "downloads": -1, "filename": "redditnlp-0.1.3.tar.gz", "has_sig": false, "md5_digest": "75a788bfe61b94cdba142c75a04a8945", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20968, "upload_time": "2014-12-08T09:58:17", "upload_time_iso_8601": "2014-12-08T09:58:17.874849Z", "url": "https://files.pythonhosted.org/packages/e3/33/da7cc1d7eda4296e4925dfe78c9582ad5b8e7ac84d2d141fcec605c99fab/redditnlp-0.1.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:05:57 2020"}