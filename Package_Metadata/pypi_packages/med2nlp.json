{"info": {"author": "Daniel R. Armstrong", "author_email": "Armstrong.daniel.robert@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "<!--\n\n#################################################\n### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###\n#################################################\n# file to edit: index.ipynb\n# command to build the docs after a change: nbdev_build_docs\n\n-->\n\n# MED2NLP\n\n> MED(Minimum Effective Dose) is our philosophy(Tao Da MED), with a simple goal of making NLP as easy and effectual as possible.\n\n\n## Background\n\nDeep learning for NLP has rapidly undergone some amazing advancements over the last few years and there are a wide array of amazing resources out there. This project is an attempt to extrapolate and simply the work of many of the smartest data scientist industry. As an active member of the fast.ai community most of this project is based on the works of member of the fast.ai community and their blog posts. Some of the most notable resources used were: \n* Keita Kurita's article [A Tutorial to Fine-Tuning BERT with Fast AI](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/) \n* Dev Sharma's article [Using RoBERTa with Fastai for NLP](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) \n\n* Thilina Rajapakse article [Simple Transformers \u2014 Multi-Class Text Classification with BERT, RoBERTa, XLNet, XLM, and DistilBERT](https://medium.com/swlh/simple-transformers-multi-class-text-classification-with-bert-roberta-xlnet-xlm-and-8b585000ce3a)\n\n## Getting started\n\nSince we are going to be taking advantage of some the SOTA deep learning libraries and project you are going to need to install [Fastai]() and [\ud83e\udd17Transformers]() and I would also highly recommend that you use [anaconda](https://www.anaconda.com/distribution/) to set up a virtual environment. \n\n## Basic system setup\n( Anaconda / Text_editor / git )\n\n- add steps/requirements\n\n### 1.xx setup - virtual environment\nTo set up your virtual environment you will open your terminal and enter the following commands:\n\n1.1 - **Create a conda environment**  with python version 3.7, and the name **_med2conda_**  \n> ```conda create -n med2conda python=3.7``` \n\n1.2 - **Activate your conda environment**: _(Now you can add to the conda environment, it should show (**med2conda**) in the command line_\n  > ```conda activate med2conda```\n\n1.3 - Add **cuda toolkit** to your conda env (in this case it is cudatoolkit=10.0) [check here](TODO) for alternative\n> ```conda install pytorch cudatoolkit=10.0 -c pytorch``` \n\n1.4 - Add **pytorch** and **fastai** to your conda env\n> ```conda install -c pytorch -c fastai fastai```\n\n1.5 - Add **transformers** to your conda env\n> ```conda install -c conda-forge transformers```\n\n1.6 - Add **jupyter notebooks** to your conda env \n> ```conda install jupyter notebook``` \n\n1.6b Some people need it to tell conda which notebook to use\n> ```conda install nb_conda ```   \n\n1.7 - When you are done using the med2nlp library you will want to exit your conda env with:\n> ```conda deactivate```\n\n`pip install med2nlp`\n\n## Assumptions\n\nYour data is in a dataframe\n\n>The best way to simplify your life is format your data into a standard format so that can use your tools in a consistent manner. This is a fairly common practice is is used a lot in pipelines, some people refer to it as tidy data. As a starting point I am going to assume that your data is in a dataframe(pandas,rapid,etc.).\n\nTODO - add more assumptions\n\n## How to use\n\nWhen you want to use the med2nlp library you are going to start your med2conda env from the terminal with: \n> ```conda activate med2conda```  \n\nMake sure you are in the med2nlp folder then start a jupyter notebook with: > ```jupyter notebook```  \n\nget your data into a dataframe \n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Daniel-R_Armstrong/med2nlp", "keywords": "NLP,Deep Learning,Fastai,Pytorch,BERT,RoBERTa,XLNet,XLM,DistilBERT,ABSA,", "license": "Apache Software License 2.0", "maintainer": "", "maintainer_email": "", "name": "med2nlp", "package_url": "https://pypi.org/project/med2nlp/", "platform": "", "project_url": "https://pypi.org/project/med2nlp/", "project_urls": {"Homepage": "https://github.com/Daniel-R_Armstrong/med2nlp"}, "release_url": "https://pypi.org/project/med2nlp/0.0.1/", "requires_dist": null, "requires_python": ">=3.6", "summary": "MED(Minimum Effective Dose) to NLP", "version": "0.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>MED2NLP</h1>\n<blockquote>\n<p>MED(Minimum Effective Dose) is our philosophy(Tao Da MED), with a simple goal of making NLP as easy and effectual as possible.</p>\n</blockquote>\n<h2>Background</h2>\n<p>Deep learning for NLP has rapidly undergone some amazing advancements over the last few years and there are a wide array of amazing resources out there. This project is an attempt to extrapolate and simply the work of many of the smartest data scientist industry. As an active member of the fast.ai community most of this project is based on the works of member of the fast.ai community and their blog posts. Some of the most notable resources used were:</p>\n<ul>\n<li>\n<p>Keita Kurita's article <a href=\"https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/\" rel=\"nofollow\">A Tutorial to Fine-Tuning BERT with Fast AI</a></p>\n</li>\n<li>\n<p>Dev Sharma's article <a href=\"https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c\" rel=\"nofollow\">Using RoBERTa with Fastai for NLP</a></p>\n</li>\n<li>\n<p>Thilina Rajapakse article <a href=\"https://medium.com/swlh/simple-transformers-multi-class-text-classification-with-bert-roberta-xlnet-xlm-and-8b585000ce3a\" rel=\"nofollow\">Simple Transformers \u2014 Multi-Class Text Classification with BERT, RoBERTa, XLNet, XLM, and DistilBERT</a></p>\n</li>\n</ul>\n<h2>Getting started</h2>\n<p>Since we are going to be taking advantage of some the SOTA deep learning libraries and project you are going to need to install <a href=\"\" rel=\"nofollow\">Fastai</a> and <a href=\"\" rel=\"nofollow\">\ud83e\udd17Transformers</a> and I would also highly recommend that you use <a href=\"https://www.anaconda.com/distribution/\" rel=\"nofollow\">anaconda</a> to set up a virtual environment.</p>\n<h2>Basic system setup</h2>\n<p>( Anaconda / Text_editor / git )</p>\n<ul>\n<li>add steps/requirements</li>\n</ul>\n<h3>1.xx setup - virtual environment</h3>\n<p>To set up your virtual environment you will open your terminal and enter the following commands:</p>\n<p>1.1 - <strong>Create a conda environment</strong>  with python version 3.7, and the name <strong><em>med2conda</em></strong></p>\n<blockquote>\n<p><code>conda create -n med2conda python=3.7</code></p>\n</blockquote>\n<p>1.2 - <strong>Activate your conda environment</strong>: <em>(Now you can add to the conda environment, it should show (<strong>med2conda</strong>) in the command line</em></p>\n<blockquote>\n<p><code>conda activate med2conda</code></p>\n</blockquote>\n<p>1.3 - Add <strong>cuda toolkit</strong> to your conda env (in this case it is cudatoolkit=10.0) <a href=\"TODO\" rel=\"nofollow\">check here</a> for alternative</p>\n<blockquote>\n<p><code>conda install pytorch cudatoolkit=10.0 -c pytorch</code></p>\n</blockquote>\n<p>1.4 - Add <strong>pytorch</strong> and <strong>fastai</strong> to your conda env</p>\n<blockquote>\n<p><code>conda install -c pytorch -c fastai fastai</code></p>\n</blockquote>\n<p>1.5 - Add <strong>transformers</strong> to your conda env</p>\n<blockquote>\n<p><code>conda install -c conda-forge transformers</code></p>\n</blockquote>\n<p>1.6 - Add <strong>jupyter notebooks</strong> to your conda env</p>\n<blockquote>\n<p><code>conda install jupyter notebook</code></p>\n</blockquote>\n<p>1.6b Some people need it to tell conda which notebook to use</p>\n<blockquote>\n<p><code>conda install nb_conda</code></p>\n</blockquote>\n<p>1.7 - When you are done using the med2nlp library you will want to exit your conda env with:</p>\n<blockquote>\n<p><code>conda deactivate</code></p>\n</blockquote>\n<p><code>pip install med2nlp</code></p>\n<h2>Assumptions</h2>\n<p>Your data is in a dataframe</p>\n<blockquote>\n<p>The best way to simplify your life is format your data into a standard format so that can use your tools in a consistent manner. This is a fairly common practice is is used a lot in pipelines, some people refer to it as tidy data. As a starting point I am going to assume that your data is in a dataframe(pandas,rapid,etc.).</p>\n</blockquote>\n<p>TODO - add more assumptions</p>\n<h2>How to use</h2>\n<p>When you want to use the med2nlp library you are going to start your med2conda env from the terminal with:</p>\n<blockquote>\n<p><code>conda activate med2conda</code></p>\n</blockquote>\n<p>Make sure you are in the med2nlp folder then start a jupyter notebook with: &gt; <code>jupyter notebook</code></p>\n<p>get your data into a dataframe</p>\n\n          </div>"}, "last_serial": 6397835, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "bd0a28e316ab1a56ca9e891cc8ce240e", "sha256": "0ff6018bcf3063e822a37cb3d02387f8e71711b498b12b908d03a265f1c4d843"}, "downloads": -1, "filename": "med2nlp-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "bd0a28e316ab1a56ca9e891cc8ce240e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 8188, "upload_time": "2020-01-05T11:43:41", "upload_time_iso_8601": "2020-01-05T11:43:41.179401Z", "url": "https://files.pythonhosted.org/packages/d1/61/5a2c335ef1c460b5c15004a35bbc986d654739a4b4740f9088901648ac94/med2nlp-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a19a11890c8983de30a05e01041082bf", "sha256": "e643ed00ef30f0ffbfbe154a80c79318704f3b60404deac4f9db8eea97a1ea30"}, "downloads": -1, "filename": "med2nlp-0.0.1.tar.gz", "has_sig": false, "md5_digest": "a19a11890c8983de30a05e01041082bf", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 4050, "upload_time": "2020-01-05T11:43:43", "upload_time_iso_8601": "2020-01-05T11:43:43.301455Z", "url": "https://files.pythonhosted.org/packages/bd/73/de50ec4c0d65fa134c89e508f6cc9c649f8038e5e6ed6a27c537ddab13d7/med2nlp-0.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "bd0a28e316ab1a56ca9e891cc8ce240e", "sha256": "0ff6018bcf3063e822a37cb3d02387f8e71711b498b12b908d03a265f1c4d843"}, "downloads": -1, "filename": "med2nlp-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "bd0a28e316ab1a56ca9e891cc8ce240e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 8188, "upload_time": "2020-01-05T11:43:41", "upload_time_iso_8601": "2020-01-05T11:43:41.179401Z", "url": "https://files.pythonhosted.org/packages/d1/61/5a2c335ef1c460b5c15004a35bbc986d654739a4b4740f9088901648ac94/med2nlp-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a19a11890c8983de30a05e01041082bf", "sha256": "e643ed00ef30f0ffbfbe154a80c79318704f3b60404deac4f9db8eea97a1ea30"}, "downloads": -1, "filename": "med2nlp-0.0.1.tar.gz", "has_sig": false, "md5_digest": "a19a11890c8983de30a05e01041082bf", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 4050, "upload_time": "2020-01-05T11:43:43", "upload_time_iso_8601": "2020-01-05T11:43:43.301455Z", "url": "https://files.pythonhosted.org/packages/bd/73/de50ec4c0d65fa134c89e508f6cc9c649f8038e5e6ed6a27c537ddab13d7/med2nlp-0.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:56:39 2020"}