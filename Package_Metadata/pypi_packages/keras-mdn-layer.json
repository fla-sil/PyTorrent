{"info": {"author": "Charles Martin", "author_email": "charlepm@ifi.uio.no", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# Keras Mixture Density Network Layer\n\n[![Build Status](https://travis-ci.com/cpmpercussion/keras-mdn-layer.svg?branch=master)](https://travis-ci.com/cpmpercussion/keras-mdn-layer)\n![MIT License](https://img.shields.io/github/license/cpmpercussion/keras-mdn-layer.svg?style=flat)\n[![DOI](https://zenodo.org/badge/137585470.svg)](https://zenodo.org/badge/latestdoi/137585470)\n[![PyPI version](https://badge.fury.io/py/keras-mdn-layer.svg)](https://badge.fury.io/py/keras-mdn-layer)\n\nA mixture density network (MDN) Layer for Keras using TensorFlow's distributions module. This makes it a bit more simple to experiment with neural networks that predict multiple real-valued variables that can take on multiple equally likely values.\n\nThis layer can help build MDN-RNNs similar to those used in [RoboJam](https://github.com/cpmpercussion/robojam), [Sketch-RNN](https://experiments.withgoogle.com/sketch-rnn-demo), [handwriting generation](https://distill.pub/2016/handwriting/), and maybe even [world models](https://worldmodels.github.io).\u00a0You can do a lot of cool stuff with MDNs!\n\nOne benefit of this implementation is that you can predict any number of real-values. TensorFlow's `Mixture`, `Categorical`, and `MultivariateNormalDiag` distribution functions are used to generate the loss function (the probability density function of a mixture of multivariate normal distributions with a diagonal covariance matrix). In previous work, the loss function has often been specified by hand which is fine for 1D or 2D prediction, but becomes a bit more annoying after that.\n\nTwo important functions are provided for training and prediction:\n\n- `get_mixture_loss_func(output_dim, num_mixtures)`: This function generates a loss function with the correct output dimensiona and number of mixtures.\n- `sample_from_output(params, output_dim, num_mixtures, temp=1.0)`: This functions samples from the mixture distribution output by the model.\n\n## Installation \n\nThis project requires Python 3.6+. You can easily install this package from [PyPI](https://pypi.org/project/keras-mdn-layer/) via `pip` like so:\n\n    python3 -m pip install keras-mdn-layer\n\nAnd finally, import the `mdn` module in Python: `import mdn`\n\nAlternatively, you can clone or download this repository and then install via `python setup.py install`, or copy the `mdn` folder into your own project.\n\n## Examples\n\nSome examples are provided in the notebooks directory.\n\nThere's scripts for fitting multivalued functions, a standard MDN toy problem:\n\n<img src=\"https://preview.ibb.co/mZzkpd/Keras_MDN_Demo.jpg\" alt=\"Keras MDN Demo\" border=\"0\">\n\nThere's also a script for generating fake kanji characters:\n\n<img src=\"https://i.ibb.co/yFvtgkL/kanji-mdn-examples.png\" alt=\"kanji test 1\" border=\"0\" width=\"600\"/>\n\nAnd finally, for learning how to generate musical touch-screen performances with a temporal component:\n\n<img src=\"https://i.ibb.co/WpzSCV8/robojam-examples.png\" alt=\"Robojam Model Examples\" border=\"0\">\n\n## How to use\n\nThe MDN layer should be the last in your network and you should use `get_mixture_loss_func` to generate a loss function. Here's an example of a simple network with one Dense layer followed by the MDN.\n\n    import keras\n    import mdn\n\n    N_HIDDEN = 15  # number of hidden units in the Dense layer\n    N_MIXES = 10  # number of mixture components\n    OUTPUT_DIMS = 2  # number of real-values predicted by each mixture component\n\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(N_HIDDEN, batch_input_shape=(None, 1), activation='relu'))\n    model.add(mdn.MDN(OUTPUT_DIMS, N_MIXES))\n    model.compile(loss=mdn.get_mixture_loss_func(OUTPUT_DIMS,N_MIXES), optimizer=keras.optimizers.Adam())\n    model.summary()\n\nFit as normal:\n\n    history = model.fit(x=x_train, y=y_train)\n\nThe predictions from the network are parameters of the mixture models, so you have to apply the `sample_from_output` function to generate samples.\n\n    y_test = model.predict(x_test)\n    y_samples = np.apply_along_axis(sample_from_output, 1, y_test, OUTPUT_DIMS, N_MIXES, temp=1.0)\n\nSee the notebooks directory for examples in jupyter notebooks!\n\n### Load/Save Model\n\nSaving models is straight forward:\n\n    model.save('test_save.h5')\n\nBut loading requires `cutom_objects` to be filled with the MDN layer, and a loss function with the appropriate parameters:\n\n    m_2 = keras.models.load_model('test_save.h5', custom_objects={'MDN': mdn.MDN, 'mdn_loss_func': mdn.get_mixture_loss_func(1, N_MIXES)})\n\n\n## Acknowledgements\n\n- Hat tip to [Omimo's Keras MDN layer](https://github.com/omimo/Keras-MDN) for a starting point for this code.\n- Super hat tip to [hardmaru's MDN explanation, projects, and good ideas for sampling functions](http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/) etc.\n- Many good ideas from [Axel Brando's Master's Thesis](https://github.com/axelbrando/Mixture-Density-Networks-for-distribution-and-uncertainty-estimation)\n- Mixture Density Networks in Edward [tutorial](http://edwardlib.org/tutorials/mixture-density-network).\n\n## References\n\n1. Christopher M. Bishop. 1994. Mixture Density Networks. [Technical Report NCRG/94/004](http://publications.aston.ac.uk/373/). Neural Computing Research Group, Aston University. http://publications.aston.ac.uk/373/\n2. Axel Brando. 2017. Mixture Density Networks (MDN) for distribution and uncertainty estimation. Master\u2019s thesis. Universitat Polit\u00e8cnica de Catalunya.\n3. A. Graves. 2013. Generating Sequences With Recurrent Neural Networks. ArXiv e-prints (Aug. 2013). https://arxiv.org/abs/1308.0850\n4. David Ha and Douglas Eck. 2017. A Neural Representation of Sketch Drawings. ArXiv e-prints (April 2017). https://arxiv.org/abs/1704.03477\n5. Charles P. Martin and Jim Torresen. 2018. RoboJam: A Musical Mixture Density Network for Collaborative Touchscreen Interaction. In Evolutionary and Biologically Inspired Music, Sound, Art and Design: EvoMUSART \u201918, A. Liapis et al. (Ed.). Lecture Notes in Computer Science, Vol. 10783. Springer International Publishing. DOI:[10.1007/9778-3-319-77583-8_11](http://dx.doi.org/10.1007/9778-3-319-77583-8_11)", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/cpmpercussion/keras-mdn-layer", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "keras-mdn-layer", "package_url": "https://pypi.org/project/keras-mdn-layer/", "platform": "", "project_url": "https://pypi.org/project/keras-mdn-layer/", "project_urls": {"Homepage": "https://github.com/cpmpercussion/keras-mdn-layer"}, "release_url": "https://pypi.org/project/keras-mdn-layer/0.3.0/", "requires_dist": null, "requires_python": "", "summary": "An MDN Layer for Keras using TensorFlow Probability.", "version": "0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p># Keras Mixture Density Network Layer</p>\n<p>[![Build Status](<a href=\"https://travis-ci.com/cpmpercussion/keras-mdn-layer.svg?branch=master)%5D(https://travis-ci.com/cpmpercussion/keras-mdn-layer\" rel=\"nofollow\">https://travis-ci.com/cpmpercussion/keras-mdn-layer.svg?branch=master)](https://travis-ci.com/cpmpercussion/keras-mdn-layer</a>)\n![MIT License](<a href=\"https://img.shields.io/github/license/cpmpercussion/keras-mdn-layer.svg?style=flat\" rel=\"nofollow\">https://img.shields.io/github/license/cpmpercussion/keras-mdn-layer.svg?style=flat</a>)\n[![DOI](<a href=\"https://zenodo.org/badge/137585470.svg)%5D(https://zenodo.org/badge/latestdoi/137585470\" rel=\"nofollow\">https://zenodo.org/badge/137585470.svg)](https://zenodo.org/badge/latestdoi/137585470</a>)\n[![PyPI version](<a href=\"https://badge.fury.io/py/keras-mdn-layer.svg)%5D(https://badge.fury.io/py/keras-mdn-layer\" rel=\"nofollow\">https://badge.fury.io/py/keras-mdn-layer.svg)](https://badge.fury.io/py/keras-mdn-layer</a>)</p>\n<p>A mixture density network (MDN) Layer for Keras using TensorFlow\u2019s distributions module. This makes it a bit more simple to experiment with neural networks that predict multiple real-valued variables that can take on multiple equally likely values.</p>\n<p>This layer can help build MDN-RNNs similar to those used in [RoboJam](<a href=\"https://github.com/cpmpercussion/robojam\" rel=\"nofollow\">https://github.com/cpmpercussion/robojam</a>), [Sketch-RNN](<a href=\"https://experiments.withgoogle.com/sketch-rnn-demo\" rel=\"nofollow\">https://experiments.withgoogle.com/sketch-rnn-demo</a>), [handwriting generation](<a href=\"https://distill.pub/2016/handwriting/\" rel=\"nofollow\">https://distill.pub/2016/handwriting/</a>), and maybe even [world models](<a href=\"https://worldmodels.github.io\" rel=\"nofollow\">https://worldmodels.github.io</a>).\u00a0You can do a lot of cool stuff with MDNs!</p>\n<p>One benefit of this implementation is that you can predict any number of real-values. TensorFlow\u2019s <cite>Mixture</cite>, <cite>Categorical</cite>, and <cite>MultivariateNormalDiag</cite> distribution functions are used to generate the loss function (the probability density function of a mixture of multivariate normal distributions with a diagonal covariance matrix). In previous work, the loss function has often been specified by hand which is fine for 1D or 2D prediction, but becomes a bit more annoying after that.</p>\n<p>Two important functions are provided for training and prediction:</p>\n<ul>\n<li><cite>get_mixture_loss_func(output_dim, num_mixtures)</cite>: This function generates a loss function with the correct output dimensiona and number of mixtures.</li>\n<li><cite>sample_from_output(params, output_dim, num_mixtures, temp=1.0)</cite>: This functions samples from the mixture distribution output by the model.</li>\n</ul>\n<p>## Installation</p>\n<p>This project requires Python 3.6+. You can easily install this package from [PyPI](<a href=\"https://pypi.org/project/keras-mdn-layer/\" rel=\"nofollow\">https://pypi.org/project/keras-mdn-layer/</a>) via <cite>pip</cite> like so:</p>\n<blockquote>\npython3 -m pip install keras-mdn-layer</blockquote>\n<p>And finally, import the <cite>mdn</cite> module in Python: <cite>import mdn</cite></p>\n<p>Alternatively, you can clone or download this repository and then install via <cite>python setup.py install</cite>, or copy the <cite>mdn</cite> folder into your own project.</p>\n<p>## Examples</p>\n<p>Some examples are provided in the notebooks directory.</p>\n<p>There\u2019s scripts for fitting multivalued functions, a standard MDN toy problem:</p>\n<p>&lt;img src=\u201d<a href=\"https://preview.ibb.co/mZzkpd/Keras_MDN_Demo.jpg\" rel=\"nofollow\">https://preview.ibb.co/mZzkpd/Keras_MDN_Demo.jpg</a>\u201d alt=\u201dKeras MDN Demo\u201d border=\u201d0\u201d&gt;</p>\n<p>There\u2019s also a script for generating fake kanji characters:</p>\n<p>&lt;img src=\u201d<a href=\"https://i.ibb.co/yFvtgkL/kanji-mdn-examples.png\" rel=\"nofollow\">https://i.ibb.co/yFvtgkL/kanji-mdn-examples.png</a>\u201d alt=\u201dkanji test 1\u201d border=\u201d0\u201d width=\u201d600\u201d/&gt;</p>\n<p>And finally, for learning how to generate musical touch-screen performances with a temporal component:</p>\n<p>&lt;img src=\u201d<a href=\"https://i.ibb.co/WpzSCV8/robojam-examples.png\" rel=\"nofollow\">https://i.ibb.co/WpzSCV8/robojam-examples.png</a>\u201d alt=\u201dRobojam Model Examples\u201d border=\u201d0\u201d&gt;</p>\n<p>## How to use</p>\n<p>The MDN layer should be the last in your network and you should use <cite>get_mixture_loss_func</cite> to generate a loss function. Here\u2019s an example of a simple network with one Dense layer followed by the MDN.</p>\n<blockquote>\n<p>import keras\nimport mdn</p>\n<p>N_HIDDEN = 15  # number of hidden units in the Dense layer\nN_MIXES = 10  # number of mixture components\nOUTPUT_DIMS = 2  # number of real-values predicted by each mixture component</p>\n<p>model = keras.Sequential()\nmodel.add(keras.layers.Dense(N_HIDDEN, batch_input_shape=(None, 1), activation=\u2019relu\u2019))\nmodel.add(mdn.MDN(OUTPUT_DIMS, N_MIXES))\nmodel.compile(loss=mdn.get_mixture_loss_func(OUTPUT_DIMS,N_MIXES), optimizer=keras.optimizers.Adam())\nmodel.summary()</p>\n</blockquote>\n<p>Fit as normal:</p>\n<blockquote>\nhistory = model.fit(x=x_train, y=y_train)</blockquote>\n<p>The predictions from the network are parameters of the mixture models, so you have to apply the <cite>sample_from_output</cite> function to generate samples.</p>\n<blockquote>\ny_test = model.predict(x_test)\ny_samples = np.apply_along_axis(sample_from_output, 1, y_test, OUTPUT_DIMS, N_MIXES, temp=1.0)</blockquote>\n<p>See the notebooks directory for examples in jupyter notebooks!</p>\n<p>### Load/Save Model</p>\n<p>Saving models is straight forward:</p>\n<blockquote>\nmodel.save(\u2018test_save.h5\u2019)</blockquote>\n<p>But loading requires <cite>cutom_objects</cite> to be filled with the MDN layer, and a loss function with the appropriate parameters:</p>\n<blockquote>\nm_2 = keras.models.load_model(\u2018test_save.h5\u2019, custom_objects={\u2018MDN\u2019: mdn.MDN, \u2018mdn_loss_func\u2019: mdn.get_mixture_loss_func(1, N_MIXES)})</blockquote>\n<p>## Acknowledgements</p>\n<ul>\n<li>Hat tip to [Omimo\u2019s Keras MDN layer](<a href=\"https://github.com/omimo/Keras-MDN\" rel=\"nofollow\">https://github.com/omimo/Keras-MDN</a>) for a starting point for this code.</li>\n<li>Super hat tip to [hardmaru\u2019s MDN explanation, projects, and good ideas for sampling functions](<a href=\"http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/\" rel=\"nofollow\">http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/</a>) etc.</li>\n<li>Many good ideas from [Axel Brando\u2019s Master\u2019s Thesis](<a href=\"https://github.com/axelbrando/Mixture-Density-Networks-for-distribution-and-uncertainty-estimation\" rel=\"nofollow\">https://github.com/axelbrando/Mixture-Density-Networks-for-distribution-and-uncertainty-estimation</a>)</li>\n<li>Mixture Density Networks in Edward [tutorial](<a href=\"http://edwardlib.org/tutorials/mixture-density-network\" rel=\"nofollow\">http://edwardlib.org/tutorials/mixture-density-network</a>).</li>\n</ul>\n<p>## References</p>\n<ol>\n<li>Christopher M. Bishop. 1994. Mixture Density Networks. [Technical Report NCRG/94/004](<a href=\"http://publications.aston.ac.uk/373/\" rel=\"nofollow\">http://publications.aston.ac.uk/373/</a>). Neural Computing Research Group, Aston University. <a href=\"http://publications.aston.ac.uk/373/\" rel=\"nofollow\">http://publications.aston.ac.uk/373/</a></li>\n<li>Axel Brando. 2017. Mixture Density Networks (MDN) for distribution and uncertainty estimation. Master\u2019s thesis. Universitat Polit\u00e8cnica de Catalunya.</li>\n<li><ol>\n<li>Graves. 2013. Generating Sequences With Recurrent Neural Networks. ArXiv e-prints (Aug. 2013). <a href=\"https://arxiv.org/abs/1308.0850\" rel=\"nofollow\">https://arxiv.org/abs/1308.0850</a></li>\n</ol>\n</li>\n<li>David Ha and Douglas Eck. 2017. A Neural Representation of Sketch Drawings. ArXiv e-prints (April 2017). <a href=\"https://arxiv.org/abs/1704.03477\" rel=\"nofollow\">https://arxiv.org/abs/1704.03477</a></li>\n<li>Charles P. Martin and Jim Torresen. 2018. RoboJam: A Musical Mixture Density Network for Collaborative Touchscreen Interaction. In Evolutionary and Biologically Inspired Music, Sound, Art and Design: EvoMUSART \u201918, A. Liapis et al. (Ed.). Lecture Notes in Computer Science, Vol. 10783. Springer International Publishing. DOI:[10.1007/9778-3-319-77583-8_11](http://dx.doi.org/10.1007/9778-3-319-77583-8_11)</li>\n</ol>\n\n          </div>"}, "last_serial": 6072780, "releases": {"0.2.1": [{"comment_text": "", "digests": {"md5": "8961fc63acf675f2fb49f130d94b77aa", "sha256": "67c62f8737102b868d092d6203d618986333b8aaef0c044e0d317d6ca07fbc54"}, "downloads": -1, "filename": "keras_mdn_layer-0.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "8961fc63acf675f2fb49f130d94b77aa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7587, "upload_time": "2019-08-25T20:53:20", "upload_time_iso_8601": "2019-08-25T20:53:20.289554Z", "url": "https://files.pythonhosted.org/packages/53/19/9c15fab77bf1232bddbeca79328a0264204e30a03b828f7edf73b7c9aadf/keras_mdn_layer-0.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "dd6757048d2300950628564cfa8b323e", "sha256": "83fddd7f01a8d79a5acffabf90c1fd118e543f90850bdfaa17dd6fd2825f5dfc"}, "downloads": -1, "filename": "keras-mdn-layer-0.2.1.tar.gz", "has_sig": false, "md5_digest": "dd6757048d2300950628564cfa8b323e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6526, "upload_time": "2019-08-25T20:53:22", "upload_time_iso_8601": "2019-08-25T20:53:22.850783Z", "url": "https://files.pythonhosted.org/packages/92/9a/7a7945223cb92948c553763b67423019f72759196dbcfbbc594b62b011ae/keras-mdn-layer-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "0488794042e1a1efad2542e3b5a07dea", "sha256": "669978e1ecb85b5afbb97db9f985c3876ecef59f9c646758242b04776464f3ce"}, "downloads": -1, "filename": "keras_mdn_layer-0.2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "0488794042e1a1efad2542e3b5a07dea", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7619, "upload_time": "2019-08-25T21:00:26", "upload_time_iso_8601": "2019-08-25T21:00:26.460583Z", "url": "https://files.pythonhosted.org/packages/6e/21/82a1402ff0a6cbd85fd4bde839737d7651e5a6cf6f2ada58fd2d7399caf1/keras_mdn_layer-0.2.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ba0bab85eafc12d6c59983fc540d2d83", "sha256": "b2fd84ca4d38b3ac7a6e0728c04bc29472fec1e9acfe24ce4fbeb5ec11f9d8a0"}, "downloads": -1, "filename": "keras-mdn-layer-0.2.2.tar.gz", "has_sig": false, "md5_digest": "ba0bab85eafc12d6c59983fc540d2d83", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6552, "upload_time": "2019-08-25T21:00:28", "upload_time_iso_8601": "2019-08-25T21:00:28.967876Z", "url": "https://files.pythonhosted.org/packages/69/b0/a3910b414ae09d50ec4357c8faef674d1e6eb308072d1085f517ebd968fa/keras-mdn-layer-0.2.2.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "e047ecb14f274afb7523e20bc1581107", "sha256": "a4b5a015df8f47e558ff4b5cc7304e810207c3194b7a04cb5f4800a6ad01a204"}, "downloads": -1, "filename": "keras-mdn-layer-0.3.0.tar.gz", "has_sig": false, "md5_digest": "e047ecb14f274afb7523e20bc1581107", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6824, "upload_time": "2019-11-04T02:22:53", "upload_time_iso_8601": "2019-11-04T02:22:53.627265Z", "url": "https://files.pythonhosted.org/packages/f3/90/7c9233a1b334bf91bc7f9ec2534eb40f7bb418900f35cbd201864c600cf6/keras-mdn-layer-0.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e047ecb14f274afb7523e20bc1581107", "sha256": "a4b5a015df8f47e558ff4b5cc7304e810207c3194b7a04cb5f4800a6ad01a204"}, "downloads": -1, "filename": "keras-mdn-layer-0.3.0.tar.gz", "has_sig": false, "md5_digest": "e047ecb14f274afb7523e20bc1581107", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6824, "upload_time": "2019-11-04T02:22:53", "upload_time_iso_8601": "2019-11-04T02:22:53.627265Z", "url": "https://files.pythonhosted.org/packages/f3/90/7c9233a1b334bf91bc7f9ec2534eb40f7bb418900f35cbd201864c600cf6/keras-mdn-layer-0.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:11 2020"}