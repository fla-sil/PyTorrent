{"info": {"author": "Michal Hodur, Mikhail Korobov", "author_email": "michal.hodur@rentier.io, kmike84@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Framework :: Scrapy", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "scrapy-rotating-proxies\n=======================\n\n.. image:: https://img.shields.io/pypi/v/scrapy-rotating-proxies.svg\n   :target: https://pypi.python.org/pypi/scrapy-rotating-proxies\n   :alt: PyPI Version\n\n.. image:: https://travis-ci.org/TeamHG-Memex/scrapy-rotating-proxies.svg?branch=master\n   :target: http://travis-ci.org/TeamHG-Memex/scrapy-rotating-proxies\n   :alt: Build Status\n\n.. image:: http://codecov.io/github/TeamHG-Memex/scrapy-rotating-proxies/coverage.svg?branch=master\n   :target: http://codecov.io/github/TeamHG-Memex/scrapy-rotating-proxies?branch=master\n   :alt: Code Coverage\n\nThis package provides a Scrapy_ middleware to use rotating proxies,\ncheck that they are alive and adjust crawling speed.\n\n.. _Scrapy: https://scrapy.org/\n\nLicense is MIT.\n\nInstallation\n------------\n\n::\n\n    pip install scrapy-rotating-proxies\n\nUsage\n-----\n\nAdd ``ROTATING_PROXY_LIST`` option with a list of proxies to settings.py::\n\n    ROTATING_PROXY_LIST = [\n        'proxy1.com:8000',\n        'proxy2.com:8031',\n        # ...\n    ]\n\nAs an alternative, you can specify a ``ROTATING_PROXY_LIST_PATH`` options\nwith a path to a file with proxies, one per line::\n\n   ROTATING_PROXY_LIST_PATH = '/my/path/proxies.txt'\n\n``ROTATING_PROXY_LIST_PATH`` takes precedence over ``ROTATING_PROXY_LIST``\nif both options are present.\n\nThen add rotating_proxies middlewares to your DOWNLOADER_MIDDLEWARES::\n\n    DOWNLOADER_MIDDLEWARES = {\n        # ...\n        'rotating_proxies.middlewares.RotatingProxyMiddleware': 610,\n        'rotating_proxies.middlewares.BanDetectionMiddleware': 620,\n        # ...\n    }\n\nAfter this all requests will be proxied using one of the proxies from\nthe ``ROTATING_PROXY_LIST`` / ``ROTATING_PROXY_LIST_PATH``.\n\nRequests with \"proxy\" set in their meta are not handled by\nscrapy-rotating-proxies. To disable proxying for a request set\n``request.meta['proxy'] = None``; to set proxy explicitly use\n``request.meta['proxy'] = \"<my-proxy-address>\"``.\n\n\nConcurrency\n-----------\n\nBy default, all default Scrapy concurrency options (``DOWNLOAD_DELAY``,\n``AUTHTHROTTLE_...``, ``CONCURRENT_REQUESTS_PER_DOMAIN``, etc) become\nper-proxy for proxied requests when RotatingProxyMiddleware is enabled.\nFor example, if you set ``CONCURRENT_REQUESTS_PER_DOMAIN=2`` then\nspider will be making at most 2 concurrent connections to each proxy,\nregardless of request url domain.\n\nCustomization\n-------------\n\n``scrapy-rotating-proxies`` keeps track of working and non-working proxies,\nand re-checks non-working from time to time.\n\nDetection of a non-working proxy is site-specific.\nBy default, ``scrapy-rotating-proxies`` uses a simple heuristic:\nif a response status code is not 200, response body is empty or if\nthere was an exception then proxy is considered dead.\n\nYou can override ban detection method by passing a path to\na custom BanDectionPolicy in ``ROTATING_PROXY_BAN_POLICY`` option, e.g.::\n\n    # settings.py\n    ROTATING_PROXY_BAN_POLICY = 'myproject.policy.MyBanPolicy'\n\nThe policy must be a class with ``response_is_ban``\nand ``exception_is_ban`` methods. These methods can return True\n(ban detected), False (not a ban) or None (unknown). It can be convenient\nto subclass and modify default BanDetectionPolicy::\n\n    # myproject/policy.py\n    from rotating_proxies.policy import BanDetectionPolicy\n\n    class MyPolicy(BanDetectionPolicy):\n        def response_is_ban(self, request, response):\n            # use default rules, but also consider HTTP 200 responses\n            # a ban if there is 'captcha' word in response body.\n            ban = super(MyPolicy, self).response_is_ban(request, response)\n            ban = ban or b'captcha' in response.body\n            return ban\n\n        def exception_is_ban(self, request, exception):\n            # override method completely: don't take exceptions in account\n            return None\n\nInstead of creating a policy you can also implement ``response_is_ban``\nand ``exception_is_ban`` methods as spider methods, for example::\n\n    class MySpider(scrapy.Spider):\n        # ...\n\n        def response_is_ban(self, request, response):\n            return b'banned' in response.body\n\n        def exception_is_ban(self, request, exception):\n            return None\n\nIt is important to have these rules correct because action for a failed\nrequest and a bad proxy should be different: if it is a proxy to blame\nit makes sense to retry the request with a different proxy.\n\nNon-working proxies could become alive again after some time.\n``scrapy-rotating-proxies`` uses a randomized exponential backoff for these\nchecks - first check happens soon, if it still fails then next check is\ndelayed further, etc. Use ``ROTATING_PROXY_BACKOFF_BASE`` to adjust the\ninitial delay (by default it is random, from 0 to 5 minutes). The randomized\nexponential backoff is capped by ``ROTATING_PROXY_BACKOFF_CAP``.\n\nSettings\n--------\n\n* ``ROTATING_PROXY_LIST``  - a list of proxies to choose from;\n* ``ROTATING_PROXY_LIST_PATH``  - path to a file with a list of proxies;\n* ``ROTATING_PROXY_LOGSTATS_INTERVAL`` - stats logging interval in seconds,\n  30 by default;\n* ``ROTATING_PROXY_CLOSE_SPIDER`` - When True, spider is stopped if\n  there are no alive proxies. If False (default), then when there is no\n  alive proxies all dead proxies are re-checked.\n* ``ROTATING_PROXY_PAGE_RETRY_TIMES`` - a number of times to retry\n  downloading a page using a different proxy. After this amount of retries\n  failure is considered a page failure, not a proxy failure.\n  Think of it this way: every improperly detected ban cost you\n  ``ROTATING_PROXY_PAGE_RETRY_TIMES`` alive proxies. Default: 5.\n\n  It is possible to change this option per-request using\n  ``max_proxies_to_try`` request.meta key - for example, you can use a higher\n  value for certain pages if you're sure they should work.\n* ``ROTATING_PROXY_BACKOFF_BASE`` - base backoff time, in seconds.\n  Default is 300 (i.e. 5 min).\n* ``ROTATING_PROXY_BACKOFF_CAP`` - backoff time cap, in seconds.\n  Default is 3600 (i.e. 60 min).\n* ``ROTATING_PROXY_BAN_POLICY`` - path to a ban detection policy.\n  Default is ``'rotating_proxies.policy.BanDetectionPolicy'``.\n\n\nFAQ\n---\n\nQ: Where to get proxy lists? How to write and maintain ban rules?\n\nA: It is up to you to find proxies and maintain proper ban rules\nfor web sites; ``scrapy-rotating-proxies`` doesn't have anything built-in.\nThere are commercial proxy services like https://crawlera.com/ which can\nintegrate with Scrapy (see https://github.com/scrapy-plugins/scrapy-crawlera)\nand take care of all these details.\n\nContributing\n------------\n\n* source code: https://github.com/TeamHG-Memex/scrapy-rotating-proxies\n* bug tracker: https://github.com/TeamHG-Memex/scrapy-rotating-proxies/issues\n\nTo run tests, install tox_ and run ``tox`` from the source checkout.\n\n.. _tox: https://tox.readthedocs.io/en/latest/\n\n----\n\n.. image:: https://hyperiongray.s3.amazonaws.com/define-hg.svg\n    :target: https://www.hyperiongray.com/?pk_campaign=github&pk_kwd=scrapy-rotating-proxies\n    :alt: define hyperiongray\n\n\nCHANGES\n=======\n\n0.6 (2018-12-28)\n----------------\n\nProxy information is added to scrapy stats:\n\n* proxies/unchecked\n* proxies/reanimated\n* proxies/dead\n* proxies/good\n* proxies/mean_backoff\n\n0.5 (2017-10-09)\n----------------\n\n* ``ROTATING_PROXY_LIST_PATH`` option allows to pass file name\n  with a proxy list.\n\n0.4 (2017-06-06)\n----------------\n\n* ``ROTATING_PROXY_BACKOFF_CAP`` option allows to change max backoff time\n  from the default 1 hour.\n\n0.3.2 (2017-06-05)\n------------------\n\n* fixed proxy authentication issue.\n\n0.3.1 (2017-03-20)\n------------------\n\n* fixed OverflowError during backoff computation.\n\n0.3 (2017-03-14)\n----------------\n\n* redirects with empty bodies are no longer considered bans\n  (thanks Diga Widyaprana).\n* ``ROTATING_PROXY_BAN_POLICY`` option allows to customize ban detection\n  for all spiders.\n\n0.2.3 (2017-03-03)\n------------------\n\n* ``max_proxies_to_try`` request.meta key allows to override\n  ``ROTATING_PROXY_PAGE_RETRY_TIMES`` option per-request.\n\n0.2.2 (2017-03-01)\n------------------\n\n* Update default ban detection rules: scrapy.exceptions.IgnoreRequest\n  is not a ban.\n\n0.2.1 (2017-02-08)\n------------------\n\n* changed ``ROTATING_PROXY_PAGE_RETRY_TIMES`` default value - it is now 5.\n\n0.2 (2017-02-07)\n----------------\n\n* improved default ban detection rules;\n* log ban stats.\n\n0.1 (2017-02-01)\n----------------\n\nInitial release\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/rentieranalytics/rentier-scrapy-proxy-rotator", "keywords": "", "license": "MIT license", "maintainer": "", "maintainer_email": "", "name": "rentier-scrapy-proxy-rotator", "package_url": "https://pypi.org/project/rentier-scrapy-proxy-rotator/", "platform": "", "project_url": "https://pypi.org/project/rentier-scrapy-proxy-rotator/", "project_urls": {"Homepage": "https://github.com/rentieranalytics/rentier-scrapy-proxy-rotator"}, "release_url": "https://pypi.org/project/rentier-scrapy-proxy-rotator/0.6.2/", "requires_dist": ["attrs (>16.0.0)", "six", "typing"], "requires_python": "", "summary": "Rentier Analytics Scrapy Proxy Rotator", "version": "0.6.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"scrapy-rotating-proxies\">\n<h2>scrapy-rotating-proxies</h2>\n<a href=\"https://pypi.python.org/pypi/scrapy-rotating-proxies\" rel=\"nofollow\"><img alt=\"PyPI Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4fdb73afec735ef115e85aef3285089b59a405f8/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7363726170792d726f746174696e672d70726f786965732e737667\"></a>\n<a href=\"http://travis-ci.org/TeamHG-Memex/scrapy-rotating-proxies\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2ca9884f3fe147625d762ede7e25820b1e91ffff/68747470733a2f2f7472617669732d63692e6f72672f5465616d48472d4d656d65782f7363726170792d726f746174696e672d70726f786965732e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"http://codecov.io/github/TeamHG-Memex/scrapy-rotating-proxies?branch=master\" rel=\"nofollow\"><img alt=\"Code Coverage\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9878098262d2595b3d6a506fd3d9e79790e82dce/687474703a2f2f636f6465636f762e696f2f6769746875622f5465616d48472d4d656d65782f7363726170792d726f746174696e672d70726f786965732f636f7665726167652e7376673f6272616e63683d6d6173746572\"></a>\n<p>This package provides a <a href=\"https://scrapy.org/\" rel=\"nofollow\">Scrapy</a> middleware to use rotating proxies,\ncheck that they are alive and adjust crawling speed.</p>\n<p>License is MIT.</p>\n<div id=\"installation\">\n<h3>Installation</h3>\n<pre>pip install scrapy-rotating-proxies\n</pre>\n</div>\n<div id=\"usage\">\n<h3>Usage</h3>\n<p>Add <tt>ROTATING_PROXY_LIST</tt> option with a list of proxies to settings.py:</p>\n<pre>ROTATING_PROXY_LIST = [\n    'proxy1.com:8000',\n    'proxy2.com:8031',\n    # ...\n]\n</pre>\n<p>As an alternative, you can specify a <tt>ROTATING_PROXY_LIST_PATH</tt> options\nwith a path to a file with proxies, one per line:</p>\n<pre>ROTATING_PROXY_LIST_PATH = '/my/path/proxies.txt'\n</pre>\n<p><tt>ROTATING_PROXY_LIST_PATH</tt> takes precedence over <tt>ROTATING_PROXY_LIST</tt>\nif both options are present.</p>\n<p>Then add rotating_proxies middlewares to your DOWNLOADER_MIDDLEWARES:</p>\n<pre>DOWNLOADER_MIDDLEWARES = {\n    # ...\n    'rotating_proxies.middlewares.RotatingProxyMiddleware': 610,\n    'rotating_proxies.middlewares.BanDetectionMiddleware': 620,\n    # ...\n}\n</pre>\n<p>After this all requests will be proxied using one of the proxies from\nthe <tt>ROTATING_PROXY_LIST</tt> / <tt>ROTATING_PROXY_LIST_PATH</tt>.</p>\n<p>Requests with \u201cproxy\u201d set in their meta are not handled by\nscrapy-rotating-proxies. To disable proxying for a request set\n<tt><span class=\"pre\">request.meta['proxy']</span> = None</tt>; to set proxy explicitly use\n<tt><span class=\"pre\">request.meta['proxy']</span> = <span class=\"pre\">\"&lt;my-proxy-address&gt;\"</span></tt>.</p>\n</div>\n<div id=\"concurrency\">\n<h3>Concurrency</h3>\n<p>By default, all default Scrapy concurrency options (<tt>DOWNLOAD_DELAY</tt>,\n<tt><span class=\"pre\">AUTHTHROTTLE_...</span></tt>, <tt>CONCURRENT_REQUESTS_PER_DOMAIN</tt>, etc) become\nper-proxy for proxied requests when RotatingProxyMiddleware is enabled.\nFor example, if you set <tt>CONCURRENT_REQUESTS_PER_DOMAIN=2</tt> then\nspider will be making at most 2 concurrent connections to each proxy,\nregardless of request url domain.</p>\n</div>\n<div id=\"customization\">\n<h3>Customization</h3>\n<p><tt><span class=\"pre\">scrapy-rotating-proxies</span></tt> keeps track of working and non-working proxies,\nand re-checks non-working from time to time.</p>\n<p>Detection of a non-working proxy is site-specific.\nBy default, <tt><span class=\"pre\">scrapy-rotating-proxies</span></tt> uses a simple heuristic:\nif a response status code is not 200, response body is empty or if\nthere was an exception then proxy is considered dead.</p>\n<p>You can override ban detection method by passing a path to\na custom BanDectionPolicy in <tt>ROTATING_PROXY_BAN_POLICY</tt> option, e.g.:</p>\n<pre># settings.py\nROTATING_PROXY_BAN_POLICY = 'myproject.policy.MyBanPolicy'\n</pre>\n<p>The policy must be a class with <tt>response_is_ban</tt>\nand <tt>exception_is_ban</tt> methods. These methods can return True\n(ban detected), False (not a ban) or None (unknown). It can be convenient\nto subclass and modify default BanDetectionPolicy:</p>\n<pre># myproject/policy.py\nfrom rotating_proxies.policy import BanDetectionPolicy\n\nclass MyPolicy(BanDetectionPolicy):\n    def response_is_ban(self, request, response):\n        # use default rules, but also consider HTTP 200 responses\n        # a ban if there is 'captcha' word in response body.\n        ban = super(MyPolicy, self).response_is_ban(request, response)\n        ban = ban or b'captcha' in response.body\n        return ban\n\n    def exception_is_ban(self, request, exception):\n        # override method completely: don't take exceptions in account\n        return None\n</pre>\n<p>Instead of creating a policy you can also implement <tt>response_is_ban</tt>\nand <tt>exception_is_ban</tt> methods as spider methods, for example:</p>\n<pre>class MySpider(scrapy.Spider):\n    # ...\n\n    def response_is_ban(self, request, response):\n        return b'banned' in response.body\n\n    def exception_is_ban(self, request, exception):\n        return None\n</pre>\n<p>It is important to have these rules correct because action for a failed\nrequest and a bad proxy should be different: if it is a proxy to blame\nit makes sense to retry the request with a different proxy.</p>\n<p>Non-working proxies could become alive again after some time.\n<tt><span class=\"pre\">scrapy-rotating-proxies</span></tt> uses a randomized exponential backoff for these\nchecks - first check happens soon, if it still fails then next check is\ndelayed further, etc. Use <tt>ROTATING_PROXY_BACKOFF_BASE</tt> to adjust the\ninitial delay (by default it is random, from 0 to 5 minutes). The randomized\nexponential backoff is capped by <tt>ROTATING_PROXY_BACKOFF_CAP</tt>.</p>\n</div>\n<div id=\"settings\">\n<h3>Settings</h3>\n<ul>\n<li><p><tt>ROTATING_PROXY_LIST</tt>  - a list of proxies to choose from;</p>\n</li>\n<li><p><tt>ROTATING_PROXY_LIST_PATH</tt>  - path to a file with a list of proxies;</p>\n</li>\n<li><p><tt>ROTATING_PROXY_LOGSTATS_INTERVAL</tt> - stats logging interval in seconds,\n30 by default;</p>\n</li>\n<li><p><tt>ROTATING_PROXY_CLOSE_SPIDER</tt> - When True, spider is stopped if\nthere are no alive proxies. If False (default), then when there is no\nalive proxies all dead proxies are re-checked.</p>\n</li>\n<li><p><tt>ROTATING_PROXY_PAGE_RETRY_TIMES</tt> - a number of times to retry\ndownloading a page using a different proxy. After this amount of retries\nfailure is considered a page failure, not a proxy failure.\nThink of it this way: every improperly detected ban cost you\n<tt>ROTATING_PROXY_PAGE_RETRY_TIMES</tt> alive proxies. Default: 5.</p>\n<p>It is possible to change this option per-request using\n<tt>max_proxies_to_try</tt> request.meta key - for example, you can use a higher\nvalue for certain pages if you\u2019re sure they should work.</p>\n</li>\n<li><p><tt>ROTATING_PROXY_BACKOFF_BASE</tt> - base backoff time, in seconds.\nDefault is 300 (i.e. 5 min).</p>\n</li>\n<li><p><tt>ROTATING_PROXY_BACKOFF_CAP</tt> - backoff time cap, in seconds.\nDefault is 3600 (i.e. 60 min).</p>\n</li>\n<li><p><tt>ROTATING_PROXY_BAN_POLICY</tt> - path to a ban detection policy.\nDefault is <tt>'rotating_proxies.policy.BanDetectionPolicy'</tt>.</p>\n</li>\n</ul>\n</div>\n<div id=\"faq\">\n<h3>FAQ</h3>\n<p>Q: Where to get proxy lists? How to write and maintain ban rules?</p>\n<p>A: It is up to you to find proxies and maintain proper ban rules\nfor web sites; <tt><span class=\"pre\">scrapy-rotating-proxies</span></tt> doesn\u2019t have anything built-in.\nThere are commercial proxy services like <a href=\"https://crawlera.com/\" rel=\"nofollow\">https://crawlera.com/</a> which can\nintegrate with Scrapy (see <a href=\"https://github.com/scrapy-plugins/scrapy-crawlera\" rel=\"nofollow\">https://github.com/scrapy-plugins/scrapy-crawlera</a>)\nand take care of all these details.</p>\n</div>\n<div id=\"contributing\">\n<h3>Contributing</h3>\n<ul>\n<li>source code: <a href=\"https://github.com/TeamHG-Memex/scrapy-rotating-proxies\" rel=\"nofollow\">https://github.com/TeamHG-Memex/scrapy-rotating-proxies</a></li>\n<li>bug tracker: <a href=\"https://github.com/TeamHG-Memex/scrapy-rotating-proxies/issues\" rel=\"nofollow\">https://github.com/TeamHG-Memex/scrapy-rotating-proxies/issues</a></li>\n</ul>\n<p>To run tests, install <a href=\"https://tox.readthedocs.io/en/latest/\" rel=\"nofollow\">tox</a> and run <tt>tox</tt> from the source checkout.</p>\n<hr class=\"docutils\">\n<a href=\"https://www.hyperiongray.com/?pk_campaign=github&amp;pk_kwd=scrapy-rotating-proxies\" rel=\"nofollow\"><img alt=\"define hyperiongray\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7aa8c8491a6e01dd330d76e342a62981e8d5dd14/68747470733a2f2f6879706572696f6e677261792e73332e616d617a6f6e6177732e636f6d2f646566696e652d68672e737667\"></a>\n</div>\n</div>\n<div id=\"changes\">\n<h2>CHANGES</h2>\n<div id=\"id1\">\n<h3>0.6 (2018-12-28)</h3>\n<p>Proxy information is added to scrapy stats:</p>\n<ul>\n<li>proxies/unchecked</li>\n<li>proxies/reanimated</li>\n<li>proxies/dead</li>\n<li>proxies/good</li>\n<li>proxies/mean_backoff</li>\n</ul>\n</div>\n<div id=\"id2\">\n<h3>0.5 (2017-10-09)</h3>\n<ul>\n<li><tt>ROTATING_PROXY_LIST_PATH</tt> option allows to pass file name\nwith a proxy list.</li>\n</ul>\n</div>\n<div id=\"id3\">\n<h3>0.4 (2017-06-06)</h3>\n<ul>\n<li><tt>ROTATING_PROXY_BACKOFF_CAP</tt> option allows to change max backoff time\nfrom the default 1 hour.</li>\n</ul>\n</div>\n<div id=\"id4\">\n<h3>0.3.2 (2017-06-05)</h3>\n<ul>\n<li>fixed proxy authentication issue.</li>\n</ul>\n</div>\n<div id=\"id5\">\n<h3>0.3.1 (2017-03-20)</h3>\n<ul>\n<li>fixed OverflowError during backoff computation.</li>\n</ul>\n</div>\n<div id=\"id6\">\n<h3>0.3 (2017-03-14)</h3>\n<ul>\n<li>redirects with empty bodies are no longer considered bans\n(thanks Diga Widyaprana).</li>\n<li><tt>ROTATING_PROXY_BAN_POLICY</tt> option allows to customize ban detection\nfor all spiders.</li>\n</ul>\n</div>\n<div id=\"id7\">\n<h3>0.2.3 (2017-03-03)</h3>\n<ul>\n<li><tt>max_proxies_to_try</tt> request.meta key allows to override\n<tt>ROTATING_PROXY_PAGE_RETRY_TIMES</tt> option per-request.</li>\n</ul>\n</div>\n<div id=\"id8\">\n<h3>0.2.2 (2017-03-01)</h3>\n<ul>\n<li>Update default ban detection rules: scrapy.exceptions.IgnoreRequest\nis not a ban.</li>\n</ul>\n</div>\n<div id=\"id9\">\n<h3>0.2.1 (2017-02-08)</h3>\n<ul>\n<li>changed <tt>ROTATING_PROXY_PAGE_RETRY_TIMES</tt> default value - it is now 5.</li>\n</ul>\n</div>\n<div id=\"id10\">\n<h3>0.2 (2017-02-07)</h3>\n<ul>\n<li>improved default ban detection rules;</li>\n<li>log ban stats.</li>\n</ul>\n</div>\n<div id=\"id11\">\n<h3>0.1 (2017-02-01)</h3>\n<p>Initial release</p>\n</div>\n</div>\n\n          </div>"}, "last_serial": 4850974, "releases": {"0.6.2": [{"comment_text": "", "digests": {"md5": "facc8d2b764dd30d0672eb8311bcddc4", "sha256": "53cda5b289856470216ba951e03f3ded813d8c56577916aa959230451a684ce0"}, "downloads": -1, "filename": "rentier_scrapy_proxy_rotator-0.6.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "facc8d2b764dd30d0672eb8311bcddc4", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 12561, "upload_time": "2019-02-21T16:48:41", "upload_time_iso_8601": "2019-02-21T16:48:41.344639Z", "url": "https://files.pythonhosted.org/packages/73/6b/228b8676e235565786d35b6b176f0bd6f30a5e18b0264fafc408702c5a0f/rentier_scrapy_proxy_rotator-0.6.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "162e249484af2b1aed45a30aa772b4b1", "sha256": "237ec0147eef395a4ed8951997da222bdecb5c7acb53d8b7abc9406b3a082b0b"}, "downloads": -1, "filename": "rentier-scrapy-proxy-rotator-0.6.2.tar.gz", "has_sig": false, "md5_digest": "162e249484af2b1aed45a30aa772b4b1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13170, "upload_time": "2019-02-21T16:48:43", "upload_time_iso_8601": "2019-02-21T16:48:43.778587Z", "url": "https://files.pythonhosted.org/packages/ca/89/bdf101da507142cc9c5cc787a60041e0326aba1e1f79cc5d599378cc4893/rentier-scrapy-proxy-rotator-0.6.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "facc8d2b764dd30d0672eb8311bcddc4", "sha256": "53cda5b289856470216ba951e03f3ded813d8c56577916aa959230451a684ce0"}, "downloads": -1, "filename": "rentier_scrapy_proxy_rotator-0.6.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "facc8d2b764dd30d0672eb8311bcddc4", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 12561, "upload_time": "2019-02-21T16:48:41", "upload_time_iso_8601": "2019-02-21T16:48:41.344639Z", "url": "https://files.pythonhosted.org/packages/73/6b/228b8676e235565786d35b6b176f0bd6f30a5e18b0264fafc408702c5a0f/rentier_scrapy_proxy_rotator-0.6.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "162e249484af2b1aed45a30aa772b4b1", "sha256": "237ec0147eef395a4ed8951997da222bdecb5c7acb53d8b7abc9406b3a082b0b"}, "downloads": -1, "filename": "rentier-scrapy-proxy-rotator-0.6.2.tar.gz", "has_sig": false, "md5_digest": "162e249484af2b1aed45a30aa772b4b1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13170, "upload_time": "2019-02-21T16:48:43", "upload_time_iso_8601": "2019-02-21T16:48:43.778587Z", "url": "https://files.pythonhosted.org/packages/ca/89/bdf101da507142cc9c5cc787a60041e0326aba1e1f79cc5d599378cc4893/rentier-scrapy-proxy-rotator-0.6.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:04:43 2020"}