{"info": {"author": "Nathaniel Watson", "author_email": "nathan.watson86@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "SampleSheet Subscriber - ssub\n*******************************\n\nA downstream tool of smon_ that uses Pub/Sub notifications to initiate demultiplexing of an \nIllumina sequecing Run.\n\nUse case\n========\nsmon_ has done its job to persistantly store the raw sequencing runs in a Google Storage bucket.\nNow another tool is necessary that can automatially start demultiplexing. However, demultiplexing \ngenerally mustn't begin until it has a SampleSheet. But once a SampleSheet is readily available, \ndemultiplexing needs to start and the results need to be uploaded to Google Storage. \n\nHow it works\n============\nSampleSheets Subscriber (ssub) solves the aforementioned challenges by utilizing the power of GCP\nevents and triggers. At a high level, it works as follows:\n\n  #. User/application uploads a samplesheet to a dedicated bucket. The sample sheet naming convention \n     is ${RUN_NAME }.csv.\n  #. Google Storage immediately fires off an event to a Pub/Sub topic (whenever there is a new SampleSheet\n     or when an existing one is overwritten).\n  #. Meanwhile, ssub is running on a compute instance as a daemon process.  It is subscribed to that \n     same Pub/Sub topic. ssub polls the topic for new messages regularly, i.e. once a minute.\n  #. When ssub receives a new message, the script parses information about the event.\n  #. ssub will the query the Firestore collection - the same one used by smon_ - for a \n     document whose name is equal to the samplesheet name (minus the .csv part).\n     ssub will then download both the samplesheet and the run tarball.  The samplesheet location\n     is provided in the Pub/Sub message; the raw run tarball location is provided within the \n     Firestore document.\n  #. ssub will then kick off bcl2fastq. \n  #. Demultiplexing results are output in a folder name 'demux' within the local run directory.\n  #. ssub will upload the demux folder to the same Google Storage folder that\n     contains the raw sequencing run.\n  #. ssub will update the relevant Firestore document to add the location to the demux folder in \n     Google Storage.\n\nAll processing happens within a sub-directory of the calling directory that is named\nssub_runs. \n\nReanalysis\n==========\nReruns of the dmeultiplexing pipeline may be necessary for various reasons, i.e. the \ndemultiplexing results are no longer available and need to be regenerated, or there was a missing\nbarcode in the SampleSheet, or there was an incorrect barcode in the SampleSheet ...\n\nReanalysis is easily accomplished simply by re-uploading the SampleSheet, overwriting the previous one,\nto Google Storage. Google Storage will assign a generation number to the SampleSheet.  Think of the\ngeneration number as a type of versioning number that Google Storage assigns to each object each time\nthat the object changes. Even re-uploading the same exact same file again produces a new generation\nnumber.\n\nInternally, ssub does all of it's processing (file downloads, analysis) within a local directory\npath named after the run and the generation number of the SampleSheet. Thus, it's perfectly fine for a user to \nupload an incorrect SampleSheet, and then to immediately afterwards upload the correct one. \nIn such a scenario, there will be two runs of the pipeline, and they won't interfere with each other. \nYou will notice, however, that there will be two sets of demultplexing results uploaded to Google \nStorage, each of which exist within a folder named after the original generation number. \n\nScalablilty\n-----------\nWhile thare aren't any parallel steps in ssub, you can achieve scalability by launching two or more\ninstances of ssub, either on one single, beefy compute instance, or on separate ones. While it's \ncertainly possible that two running instances of ssub can pull the same message from Pub/Sub, only\none of these two insances will actually make use of it. It works as follows: \n\n    #. Instance 1 of ssub receives a new message from Pub/Sub and immediately begings to process it.\n    #. Instances 1 downloads and parses the corresponding Firestore document that's related to the\n       SampleSheet detailed within the Pub/Sub message.\n    #. Instance 1 notices that the document doesn't have the `ssub.FIRESTORE_ATTR_SS_PUBSUB_DATA` \n       attribute set, so then sets it to the value of the JSON serialized of the PUb/Sub message\n       data.\n    #. Meanwhile, Instance 2 of ssub has also pulled down the same Pub/Sub message.\n    #. Instance 2 queries Firestore and downloads the corresponding document. \n    #. Instance 2 notices that the document attribute `ssub.FIRESTORE_ATTR_SS_PUBSUB_DATA` is already\n       set, so it downloades this JSON value.\n    #. Instance 2 then parses the generation number out of the JSON value it downloaded from\n       Firestore and notices that the generation number is the same as the generation number in the\n       Pub/Sub message that it is currently working on.\n    #. Instance 2 logs a message that it is deferring further processing - thus leaving the rest \n       of the work to be fulfilled by Instance 1. \n\nLet's now take a few steps back and pose the question - What if Instance 2 noticed that the generation\nnumbers differ? Well, in this case, it will continue on to run the demultiplexing workflow since\nthere are different versions of the SampleSheet at hand. It will also, however, first set the \nFirestore document's `ssub.FIRESTORE_ATTR_SS_PUBSUB_DATA` attribute to the JSON serialization of the\nPub/Sub message data that it's working on. \n\nNote: If using more than one deployment of ssub on the same instance, it is recommended to run each in a\nseparate working directory.  \n\n\nSetup\n-----\n\n#. You should already have a Firestore collection for smon's use.  smon will create one for you\n   if necessary, but you can create one in advance if you'd like for manual testing. See the\n   documentation for smon_ for details on the structure of the documents stored in this collection.\n#. Create a dedicated Google Storage bucket for storing your SampleSheets and give it a useful name,\n   i.e. samplesheets.  Make sure to set the bucket to use Fine-Grained access control rather than Uniform.\n#. Create a dedicated Pub/Sub topic and give it a useful name, i.e. samplesheets.\n#. Create a `notification configuration`_ so that your samplesheets storage bucket will notify\n   the samplesheets Pub/Sub topic whenever a new file is added or modified. Note that you can use\n   gsutil for this as detailed `here <https://cloud.google.com/storage/docs/gsutil/commands/notification>`_.\n   Here is an example command::\n\n     gsutil notification create -e OBJECT_FINALIZE -f json -t samplesheets gs://samplesheets\n\n   If you get an access denied error while doing this, give the included script named \n   **create_notification_configuration.py** a try. It uses the GCP Python API and is much easier to\n   work with in regards to how permissions are configured. \n\n#. Create a Pub/Sub subscription. For example::\n\n     gcloud beta pubsub subscriptions create --topic samplesheets ssub\n\n#. Locate the Cloud Storage service account and grant it the IAM role pubsub.publisher.\n   By default, a bucket doesn't have the priviledge to send notifications to Pub/Sub. Follow the \n   instructions in steps 5 and 6 in this `GCP documentation  <https://cloud.google.com/storage/docs/reporting-changes>`_.\n\n\nMail notifications\n------------------\nIf the 'mail' JSON object is set in your JSON configuration file, then the designated recipients will\nreceive email notifications under the folowing events:\n\n  * There is an Exception in the main thread\n  * A new Pub/Sub message is being processed (duplicates excluded). \n\nYou can use the script `send_test_email.py` to test that the mail configuration you provide is\nworking. If it is, you should receive an email with the subject \"ssub test email\". \n\nThe configuration file\n======================\nThis is a small JSON file that lets the monitor know things such as which GCP bucket and Firestore\ncollection to use, for example. The possible keys are:\n\n  * `name`: The client name of the subscriber. The name will appear in the subject line if email \n    notification is configured, as well as in other places, i.e. log messages.\n  * `cycle_pause_sec`: The number of seconds to wait in-between polls of the Pub/Sub topic. Defaults to 60.\n  * `firestore_collection`: The name of the Google Firestore collection to use for\n    persistent workflow state that downstream tools can query. If it doesn't exist yet, it will be\n    created. If this parameter is not provided, support for Firestore is turned off. \n  * `sweep_age_sec`: When an analysis directory (within the ssub_runs directory)\n     is older than this many seconds, remove it. Defaults to 604800 (1 week).\n\nThe user-supplied configuration file is validated against a built-in schema. \n\nInstallation\n============\nIn a later version of Python3, run::\n\n  pip3 install ssub\n\nIt is recommended to start your compute instance (that will run the monitor) using a service account\nwith the following roles:\n\n  * roles/storage.objectAdmin\n  * roles/datastore.owner\n\nAlternatively, give your compute instance the cloud-platform scope.\n\nDeployment:\n===========\nIt is suggested to use the Dockerfile that comes in the respository.\n\n\n.. _smon: https://pypi.org/project/sruns-monitor/\n.. _`notification configuration`: https://cloud.google.com/storage/docs/pubsub-notifications\n\n\n\n\n", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://pypi.org/project/ssub/", "keywords": "bcl2fastq sequencing samplesheet monitor", "license": "", "maintainer": "", "maintainer_email": "", "name": "ssub", "package_url": "https://pypi.org/project/ssub/", "platform": "", "project_url": "https://pypi.org/project/ssub/", "project_urls": {"Homepage": "https://pypi.org/project/ssub/", "Read the Docs": "https://ssub.readthedocs.io/en/latest"}, "release_url": "https://pypi.org/project/ssub/0.2.1/", "requires_dist": ["sruns-monitor"], "requires_python": "", "summary": "Polls a GCP Pub/Sub topic for new SampleSheet notification messages in order to initiate bcl2fastq.", "version": "0.2.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>A downstream tool of <a href=\"https://pypi.org/project/sruns-monitor/\" rel=\"nofollow\">smon</a> that uses Pub/Sub notifications to initiate demultiplexing of an\nIllumina sequecing Run.</p>\n<div id=\"use-case\">\n<h2>Use case</h2>\n<p><a href=\"https://pypi.org/project/sruns-monitor/\" rel=\"nofollow\">smon</a> has done its job to persistantly store the raw sequencing runs in a Google Storage bucket.\nNow another tool is necessary that can automatially start demultiplexing. However, demultiplexing\ngenerally mustn\u2019t begin until it has a SampleSheet. But once a SampleSheet is readily available,\ndemultiplexing needs to start and the results need to be uploaded to Google Storage.</p>\n</div>\n<div id=\"how-it-works\">\n<h2>How it works</h2>\n<p>SampleSheets Subscriber (ssub) solves the aforementioned challenges by utilizing the power of GCP\nevents and triggers. At a high level, it works as follows:</p>\n<blockquote>\n<ol>\n<li>User/application uploads a samplesheet to a dedicated bucket. The sample sheet naming convention\nis ${RUN_NAME }.csv.</li>\n<li>Google Storage immediately fires off an event to a Pub/Sub topic (whenever there is a new SampleSheet\nor when an existing one is overwritten).</li>\n<li>Meanwhile, ssub is running on a compute instance as a daemon process.  It is subscribed to that\nsame Pub/Sub topic. ssub polls the topic for new messages regularly, i.e. once a minute.</li>\n<li>When ssub receives a new message, the script parses information about the event.</li>\n<li>ssub will the query the Firestore collection - the same one used by <a href=\"https://pypi.org/project/sruns-monitor/\" rel=\"nofollow\">smon</a> - for a\ndocument whose name is equal to the samplesheet name (minus the .csv part).\nssub will then download both the samplesheet and the run tarball.  The samplesheet location\nis provided in the Pub/Sub message; the raw run tarball location is provided within the\nFirestore document.</li>\n<li>ssub will then kick off bcl2fastq.</li>\n<li>Demultiplexing results are output in a folder name \u2018demux\u2019 within the local run directory.</li>\n<li>ssub will upload the demux folder to the same Google Storage folder that\ncontains the raw sequencing run.</li>\n<li>ssub will update the relevant Firestore document to add the location to the demux folder in\nGoogle Storage.</li>\n</ol>\n</blockquote>\n<p>All processing happens within a sub-directory of the calling directory that is named\nssub_runs.</p>\n</div>\n<div id=\"reanalysis\">\n<h2>Reanalysis</h2>\n<p>Reruns of the dmeultiplexing pipeline may be necessary for various reasons, i.e. the\ndemultiplexing results are no longer available and need to be regenerated, or there was a missing\nbarcode in the SampleSheet, or there was an incorrect barcode in the SampleSheet \u2026</p>\n<p>Reanalysis is easily accomplished simply by re-uploading the SampleSheet, overwriting the previous one,\nto Google Storage. Google Storage will assign a generation number to the SampleSheet.  Think of the\ngeneration number as a type of versioning number that Google Storage assigns to each object each time\nthat the object changes. Even re-uploading the same exact same file again produces a new generation\nnumber.</p>\n<p>Internally, ssub does all of it\u2019s processing (file downloads, analysis) within a local directory\npath named after the run and the generation number of the SampleSheet. Thus, it\u2019s perfectly fine for a user to\nupload an incorrect SampleSheet, and then to immediately afterwards upload the correct one.\nIn such a scenario, there will be two runs of the pipeline, and they won\u2019t interfere with each other.\nYou will notice, however, that there will be two sets of demultplexing results uploaded to Google\nStorage, each of which exist within a folder named after the original generation number.</p>\n<div id=\"scalablilty\">\n<h3>Scalablilty</h3>\n<p>While thare aren\u2019t any parallel steps in ssub, you can achieve scalability by launching two or more\ninstances of ssub, either on one single, beefy compute instance, or on separate ones. While it\u2019s\ncertainly possible that two running instances of ssub can pull the same message from Pub/Sub, only\none of these two insances will actually make use of it. It works as follows:</p>\n<blockquote>\n<ol>\n<li>Instance 1 of ssub receives a new message from Pub/Sub and immediately begings to process it.</li>\n<li>Instances 1 downloads and parses the corresponding Firestore document that\u2019s related to the\nSampleSheet detailed within the Pub/Sub message.</li>\n<li>Instance 1 notices that the document doesn\u2019t have the <cite>ssub.FIRESTORE_ATTR_SS_PUBSUB_DATA</cite>\nattribute set, so then sets it to the value of the JSON serialized of the PUb/Sub message\ndata.</li>\n<li>Meanwhile, Instance 2 of ssub has also pulled down the same Pub/Sub message.</li>\n<li>Instance 2 queries Firestore and downloads the corresponding document.</li>\n<li>Instance 2 notices that the document attribute <cite>ssub.FIRESTORE_ATTR_SS_PUBSUB_DATA</cite> is already\nset, so it downloades this JSON value.</li>\n<li>Instance 2 then parses the generation number out of the JSON value it downloaded from\nFirestore and notices that the generation number is the same as the generation number in the\nPub/Sub message that it is currently working on.</li>\n<li>Instance 2 logs a message that it is deferring further processing - thus leaving the rest\nof the work to be fulfilled by Instance 1.</li>\n</ol>\n</blockquote>\n<p>Let\u2019s now take a few steps back and pose the question - What if Instance 2 noticed that the generation\nnumbers differ? Well, in this case, it will continue on to run the demultiplexing workflow since\nthere are different versions of the SampleSheet at hand. It will also, however, first set the\nFirestore document\u2019s <cite>ssub.FIRESTORE_ATTR_SS_PUBSUB_DATA</cite> attribute to the JSON serialization of the\nPub/Sub message data that it\u2019s working on.</p>\n<p>Note: If using more than one deployment of ssub on the same instance, it is recommended to run each in a\nseparate working directory.</p>\n</div>\n<div id=\"setup\">\n<h3>Setup</h3>\n<ol>\n<li><p>You should already have a Firestore collection for smon\u2019s use.  smon will create one for you\nif necessary, but you can create one in advance if you\u2019d like for manual testing. See the\ndocumentation for <a href=\"https://pypi.org/project/sruns-monitor/\" rel=\"nofollow\">smon</a> for details on the structure of the documents stored in this collection.</p>\n</li>\n<li><p>Create a dedicated Google Storage bucket for storing your SampleSheets and give it a useful name,\ni.e. samplesheets.  Make sure to set the bucket to use Fine-Grained access control rather than Uniform.</p>\n</li>\n<li><p>Create a dedicated Pub/Sub topic and give it a useful name, i.e. samplesheets.</p>\n</li>\n<li><p>Create a <a href=\"https://cloud.google.com/storage/docs/pubsub-notifications\" rel=\"nofollow\">notification configuration</a> so that your samplesheets storage bucket will notify\nthe samplesheets Pub/Sub topic whenever a new file is added or modified. Note that you can use\ngsutil for this as detailed <a href=\"https://cloud.google.com/storage/docs/gsutil/commands/notification\" rel=\"nofollow\">here</a>.\nHere is an example command:</p>\n<pre>gsutil notification create -e OBJECT_FINALIZE -f json -t samplesheets gs://samplesheets\n</pre>\n<p>If you get an access denied error while doing this, give the included script named\n<strong>create_notification_configuration.py</strong> a try. It uses the GCP Python API and is much easier to\nwork with in regards to how permissions are configured.</p>\n</li>\n<li><p>Create a Pub/Sub subscription. For example:</p>\n<pre>gcloud beta pubsub subscriptions create --topic samplesheets ssub\n</pre>\n</li>\n<li><p>Locate the Cloud Storage service account and grant it the IAM role pubsub.publisher.\nBy default, a bucket doesn\u2019t have the priviledge to send notifications to Pub/Sub. Follow the\ninstructions in steps 5 and 6 in this <a href=\"https://cloud.google.com/storage/docs/reporting-changes\" rel=\"nofollow\">GCP documentation</a>.</p>\n</li>\n</ol>\n</div>\n<div id=\"mail-notifications\">\n<h3>Mail notifications</h3>\n<p>If the \u2018mail\u2019 JSON object is set in your JSON configuration file, then the designated recipients will\nreceive email notifications under the folowing events:</p>\n<blockquote>\n<ul>\n<li>There is an Exception in the main thread</li>\n<li>A new Pub/Sub message is being processed (duplicates excluded).</li>\n</ul>\n</blockquote>\n<p>You can use the script <cite>send_test_email.py</cite> to test that the mail configuration you provide is\nworking. If it is, you should receive an email with the subject \u201cssub test email\u201d.</p>\n</div>\n</div>\n<div id=\"the-configuration-file\">\n<h2>The configuration file</h2>\n<p>This is a small JSON file that lets the monitor know things such as which GCP bucket and Firestore\ncollection to use, for example. The possible keys are:</p>\n<blockquote>\n<ul>\n<li><cite>name</cite>: The client name of the subscriber. The name will appear in the subject line if email\nnotification is configured, as well as in other places, i.e. log messages.</li>\n<li><cite>cycle_pause_sec</cite>: The number of seconds to wait in-between polls of the Pub/Sub topic. Defaults to 60.</li>\n<li><cite>firestore_collection</cite>: The name of the Google Firestore collection to use for\npersistent workflow state that downstream tools can query. If it doesn\u2019t exist yet, it will be\ncreated. If this parameter is not provided, support for Firestore is turned off.</li>\n<li><dl>\n<dt><cite>sweep_age_sec</cite>: When an analysis directory (within the ssub_runs directory)</dt>\n<dd>is older than this many seconds, remove it. Defaults to 604800 (1 week).</dd>\n</dl>\n</li>\n</ul>\n</blockquote>\n<p>The user-supplied configuration file is validated against a built-in schema.</p>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>In a later version of Python3, run:</p>\n<pre>pip3 install ssub\n</pre>\n<p>It is recommended to start your compute instance (that will run the monitor) using a service account\nwith the following roles:</p>\n<blockquote>\n<ul>\n<li>roles/storage.objectAdmin</li>\n<li>roles/datastore.owner</li>\n</ul>\n</blockquote>\n<p>Alternatively, give your compute instance the cloud-platform scope.</p>\n</div>\n<div id=\"deployment\">\n<h2>Deployment:</h2>\n<p>It is suggested to use the Dockerfile that comes in the respository.</p>\n</div>\n\n          </div>"}, "last_serial": 7029415, "releases": {"0.1.2": [{"comment_text": "", "digests": {"md5": "f1d90fa98e85d3727b76bed1b774eb91", "sha256": "0879faa76ac64d7017933086b505574631c63f5ce10c24aa645f412dfc68f45e"}, "downloads": -1, "filename": "ssub-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "f1d90fa98e85d3727b76bed1b774eb91", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 16516, "upload_time": "2020-02-14T19:44:01", "upload_time_iso_8601": "2020-02-14T19:44:01.614714Z", "url": "https://files.pythonhosted.org/packages/41/ca/032a4560cbd6241456c7f09cf9c58451750a402bf816da399530903142fa/ssub-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1fe93a03adcfee0e4196a19fca0cfd5c", "sha256": "413cf31202bd6e80b7b5bcf9320f88dd3a151776e84ca23e6ea5d4ae0405f291"}, "downloads": -1, "filename": "ssub-0.1.2.tar.gz", "has_sig": false, "md5_digest": "1fe93a03adcfee0e4196a19fca0cfd5c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17152, "upload_time": "2020-02-14T19:44:04", "upload_time_iso_8601": "2020-02-14T19:44:04.460505Z", "url": "https://files.pythonhosted.org/packages/25/04/15b36549a8f1932ac7e0e4764e01853498f6dc32b0eda5fedba276cfe87e/ssub-0.1.2.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "795e0aa19cab34cbcdc0f1a1161b1307", "sha256": "f568f0f6fead1a00541392a592efdec010e4cd4baa63a1f383dfb5f4c765daf0"}, "downloads": -1, "filename": "ssub-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "795e0aa19cab34cbcdc0f1a1161b1307", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18484, "upload_time": "2020-04-16T02:25:16", "upload_time_iso_8601": "2020-04-16T02:25:16.057585Z", "url": "https://files.pythonhosted.org/packages/84/44/3c406537c99ece9b4ce538455d251cd404dfed2e8476bae388fe1b569fb3/ssub-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "95428e12743036dfcab335ab27a3f1a8", "sha256": "951b75e1592c82a4163d2198c6d91780590d76973f867775a5ebcba08b588528"}, "downloads": -1, "filename": "ssub-0.2.0.tar.gz", "has_sig": false, "md5_digest": "95428e12743036dfcab335ab27a3f1a8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17231, "upload_time": "2020-04-16T02:25:17", "upload_time_iso_8601": "2020-04-16T02:25:17.682172Z", "url": "https://files.pythonhosted.org/packages/ce/57/5982c470bb9c70c909a4228790b04ee23cfde5c9cee298c5b49e950605df/ssub-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "81ccc17fa16dd0f2274f0b9a8b46e4a3", "sha256": "8a1ba8bb8e026f6a197e762cfcec75df7bdf4996cf638066aada3a6e94720720"}, "downloads": -1, "filename": "ssub-0.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "81ccc17fa16dd0f2274f0b9a8b46e4a3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18609, "upload_time": "2020-04-16T04:23:01", "upload_time_iso_8601": "2020-04-16T04:23:01.737891Z", "url": "https://files.pythonhosted.org/packages/96/f0/137f861f8eaf78ca47f1298e1bba5975d13bb68d6e53c39c0033b4a5c989/ssub-0.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "48d20ca74b69d41b26791697db8300ca", "sha256": "6fc4f81cbedf175312256023c08b7954ed59d547bc55c57b2de0b088e4b1f1c0"}, "downloads": -1, "filename": "ssub-0.2.1.tar.gz", "has_sig": false, "md5_digest": "48d20ca74b69d41b26791697db8300ca", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17325, "upload_time": "2020-04-16T04:23:03", "upload_time_iso_8601": "2020-04-16T04:23:03.812927Z", "url": "https://files.pythonhosted.org/packages/d5/25/d14917a037ccf69904cec7b2af11ea4445c768741bf2f8d98a564798863a/ssub-0.2.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "81ccc17fa16dd0f2274f0b9a8b46e4a3", "sha256": "8a1ba8bb8e026f6a197e762cfcec75df7bdf4996cf638066aada3a6e94720720"}, "downloads": -1, "filename": "ssub-0.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "81ccc17fa16dd0f2274f0b9a8b46e4a3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18609, "upload_time": "2020-04-16T04:23:01", "upload_time_iso_8601": "2020-04-16T04:23:01.737891Z", "url": "https://files.pythonhosted.org/packages/96/f0/137f861f8eaf78ca47f1298e1bba5975d13bb68d6e53c39c0033b4a5c989/ssub-0.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "48d20ca74b69d41b26791697db8300ca", "sha256": "6fc4f81cbedf175312256023c08b7954ed59d547bc55c57b2de0b088e4b1f1c0"}, "downloads": -1, "filename": "ssub-0.2.1.tar.gz", "has_sig": false, "md5_digest": "48d20ca74b69d41b26791697db8300ca", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17325, "upload_time": "2020-04-16T04:23:03", "upload_time_iso_8601": "2020-04-16T04:23:03.812927Z", "url": "https://files.pythonhosted.org/packages/d5/25/d14917a037ccf69904cec7b2af11ea4445c768741bf2f8d98a564798863a/ssub-0.2.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:02:56 2020"}