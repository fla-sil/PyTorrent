{"info": {"author": "David Stephane Belemkoabga", "author_email": "bdavidstephane@hotmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# EntropySGD\n\nEntropySGD is a machine learning optimization method that has been pusblished in 2017 (https://arxiv.org/pdf/1611.01838.pdf). This method despite others like SGD, Adam, etc is built to ACTIVELY  search and converge towards flat region minimas, which are known to give better generalization performances than sharp region minimas.\n\nEven though it derives from SGD theoretically, in practice, we can include improvements such as Nesterov's momentum or Adam optimization that one can decide to use or not (if one wants to stick to the original version of Entropy SGD).\nIn order to facilitate the usage of this method in a deep learning framework, I built a **keras** version of this optimizer.\n\n## Dependencies\n\n- Keras\n- Tensorflow 2.0.0\n- numpy\n- math\n\n## Installation\n\nUse the package manager [pip](https://pip.pypa.io/en/stable/) to install EntropySGD.\n\n```bash\npip install EntropySGD\n```\n\n## Usage\nIn this package, we have implemented 3 classes :\n- **EntropySGD** : which is the original implementation of the optimizer (with SGD and optionnaly a  Nesterov's Momentum on the outer loop update (the main update))\n- **EntropyAdam** : this is an adaptation of EntropySGD to Adam optimizer (on the outer loop update ).\n- **History** : this is a callback used to log the training and evaluation losses. Given the particularity of this optimizer (two loops) in comparison to a more classical one as SGD , one must discard the keras logger per default and use this one. At the end of the training we can find 4 arrays:\n    - loss : the training losses after each epoch\n    - val_loss : the evaluation loss at the end of each epoch\n    - eff_loss : or effective loss. In EntropySGD the main update is done once every L iterations (L being the number of langevin descent steps). One effective epoch corresponds then to L regular epochs. eff_loss stores then the average loss after L regular epochs.\n    - eff_val_loss : effective evaluation loss.\n```python\nfrom EntropySGD import EntropySgd, EntropyAdam, History\n\n#create your keras model\nmodel = ...\n\n#create your optimizer\n## EntropySGD optimizer\noptimizer = EntropySgd(lr=.001, sgld_step=0.1, \n                        L=20, gamma=0.03, sgld_noise=1e-4, alpha=0.75, \n                        scoping=1e-3, momentum=0., nesterov=False, decay=.0)\n## or EntropyAdam optimizer\noptimizer = EntropyAdam(lr=.001, sgld_step=0.1, L=20, gamma=0.03, \n                        sgld_noise=1e-4, alpha=0.75, scoping=1e-3, \n                        beta_1=0.9, beta_2=0.999, amsgrad=False, decay=0.)\n\n# create the logger callback\nhistory = History()\n\n#Compile and train\nmodel.compile(optimizer, loss = ...)\nmodel.fit(..., callbacks=[history], verbose=0)\n##very important : set the verbose to 0 to deactivate keras default logger.\n\nprint(\"Training loss : \", history.loss)\nprint(\"Training effective loss :\", history.eff_loss)\nprint(\"Val loss\", history.eff_val_loss)\nprint(\"Effective Val loss\", history.eff_val_loss)\n```\n\n## Contributing\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n\n## License\n[MIT](https://choosealicense.com/licenses/mit/)", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/steph1793/EntropySGD", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "EntropySGD", "package_url": "https://pypi.org/project/EntropySGD/", "platform": "", "project_url": "https://pypi.org/project/EntropySGD/", "project_urls": {"Homepage": "https://github.com/steph1793/EntropySGD"}, "release_url": "https://pypi.org/project/EntropySGD/1.2.2/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Entropy SGD optimizer", "version": "1.2.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>EntropySGD</h1>\n<p>EntropySGD is a machine learning optimization method that has been pusblished in 2017 (<a href=\"https://arxiv.org/pdf/1611.01838.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1611.01838.pdf</a>). This method despite others like SGD, Adam, etc is built to ACTIVELY  search and converge towards flat region minimas, which are known to give better generalization performances than sharp region minimas.</p>\n<p>Even though it derives from SGD theoretically, in practice, we can include improvements such as Nesterov's momentum or Adam optimization that one can decide to use or not (if one wants to stick to the original version of Entropy SGD).\nIn order to facilitate the usage of this method in a deep learning framework, I built a <strong>keras</strong> version of this optimizer.</p>\n<h2>Dependencies</h2>\n<ul>\n<li>Keras</li>\n<li>Tensorflow 2.0.0</li>\n<li>numpy</li>\n<li>math</li>\n</ul>\n<h2>Installation</h2>\n<p>Use the package manager <a href=\"https://pip.pypa.io/en/stable/\" rel=\"nofollow\">pip</a> to install EntropySGD.</p>\n<pre>pip install EntropySGD\n</pre>\n<h2>Usage</h2>\n<p>In this package, we have implemented 3 classes :</p>\n<ul>\n<li><strong>EntropySGD</strong> : which is the original implementation of the optimizer (with SGD and optionnaly a  Nesterov's Momentum on the outer loop update (the main update))</li>\n<li><strong>EntropyAdam</strong> : this is an adaptation of EntropySGD to Adam optimizer (on the outer loop update ).</li>\n<li><strong>History</strong> : this is a callback used to log the training and evaluation losses. Given the particularity of this optimizer (two loops) in comparison to a more classical one as SGD , one must discard the keras logger per default and use this one. At the end of the training we can find 4 arrays:\n<ul>\n<li>loss : the training losses after each epoch</li>\n<li>val_loss : the evaluation loss at the end of each epoch</li>\n<li>eff_loss : or effective loss. In EntropySGD the main update is done once every L iterations (L being the number of langevin descent steps). One effective epoch corresponds then to L regular epochs. eff_loss stores then the average loss after L regular epochs.</li>\n<li>eff_val_loss : effective evaluation loss.</li>\n</ul>\n</li>\n</ul>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">EntropySGD</span> <span class=\"kn\">import</span> <span class=\"n\">EntropySgd</span><span class=\"p\">,</span> <span class=\"n\">EntropyAdam</span><span class=\"p\">,</span> <span class=\"n\">History</span>\n\n<span class=\"c1\">#create your keras model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n\n<span class=\"c1\">#create your optimizer</span>\n<span class=\"c1\">## EntropySGD optimizer</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">EntropySgd</span><span class=\"p\">(</span><span class=\"n\">lr</span><span class=\"o\">=.</span><span class=\"mi\">001</span><span class=\"p\">,</span> <span class=\"n\">sgld_step</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> \n                        <span class=\"n\">L</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">0.03</span><span class=\"p\">,</span> <span class=\"n\">sgld_noise</span><span class=\"o\">=</span><span class=\"mf\">1e-4</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.75</span><span class=\"p\">,</span> \n                        <span class=\"n\">scoping</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span> <span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"n\">nesterov</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">decay</span><span class=\"o\">=.</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"c1\">## or EntropyAdam optimizer</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">EntropyAdam</span><span class=\"p\">(</span><span class=\"n\">lr</span><span class=\"o\">=.</span><span class=\"mi\">001</span><span class=\"p\">,</span> <span class=\"n\">sgld_step</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">0.03</span><span class=\"p\">,</span> \n                        <span class=\"n\">sgld_noise</span><span class=\"o\">=</span><span class=\"mf\">1e-4</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.75</span><span class=\"p\">,</span> <span class=\"n\">scoping</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span> \n                        <span class=\"n\">beta_1</span><span class=\"o\">=</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"n\">beta_2</span><span class=\"o\">=</span><span class=\"mf\">0.999</span><span class=\"p\">,</span> <span class=\"n\">amsgrad</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">decay</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># create the logger callback</span>\n<span class=\"n\">history</span> <span class=\"o\">=</span> <span class=\"n\">History</span><span class=\"p\">()</span>\n\n<span class=\"c1\">#Compile and train</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"o\">...</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">history</span><span class=\"p\">],</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"c1\">##very important : set the verbose to 0 to deactivate keras default logger.</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Training loss : \"</span><span class=\"p\">,</span> <span class=\"n\">history</span><span class=\"o\">.</span><span class=\"n\">loss</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Training effective loss :\"</span><span class=\"p\">,</span> <span class=\"n\">history</span><span class=\"o\">.</span><span class=\"n\">eff_loss</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Val loss\"</span><span class=\"p\">,</span> <span class=\"n\">history</span><span class=\"o\">.</span><span class=\"n\">eff_val_loss</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Effective Val loss\"</span><span class=\"p\">,</span> <span class=\"n\">history</span><span class=\"o\">.</span><span class=\"n\">eff_val_loss</span><span class=\"p\">)</span>\n</pre>\n<h2>Contributing</h2>\n<p>Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.</p>\n<h2>License</h2>\n<p><a href=\"https://choosealicense.com/licenses/mit/\" rel=\"nofollow\">MIT</a></p>\n\n          </div>"}, "last_serial": 6850110, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "d5ecf3501c60b6b0e9db8cb12e306445", "sha256": "e4aa9fd95d6b25b4345e65989df029de72013a431a79dd19b8576cdaab2cef43"}, "downloads": -1, "filename": "EntropySGD-1.0.0.tar.gz", "has_sig": false, "md5_digest": "d5ecf3501c60b6b0e9db8cb12e306445", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 3138, "upload_time": "2020-03-17T20:10:21", "upload_time_iso_8601": "2020-03-17T20:10:21.197932Z", "url": "https://files.pythonhosted.org/packages/da/90/d01afe0ffe7af2c7a477d9b68686d5240a46784ae0f28ea4a6b65bffe4f9/EntropySGD-1.0.0.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "32aef2e5303ff48ff7c03fe9b46af61d", "sha256": "80aba1afaf65db36a5e9dc0d9ba17a2f97f103e62405d95e1d4256e7460278ba"}, "downloads": -1, "filename": "EntropySGD-1.1.0.tar.gz", "has_sig": false, "md5_digest": "32aef2e5303ff48ff7c03fe9b46af61d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 3164, "upload_time": "2020-03-17T20:19:43", "upload_time_iso_8601": "2020-03-17T20:19:43.501635Z", "url": "https://files.pythonhosted.org/packages/2f/05/85b6107008f728d3c00359c6e62768b852107687890be6737ac6276d30cd/EntropySGD-1.1.0.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "390898a90bd8eaa8a1e4587991bfe38e", "sha256": "550ec06bb1df64d8e4ba03bcf48890bacb62b641cb4c951d8ac694472811441f"}, "downloads": -1, "filename": "EntropySGD-1.2.0.tar.gz", "has_sig": false, "md5_digest": "390898a90bd8eaa8a1e4587991bfe38e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 3169, "upload_time": "2020-03-17T20:25:19", "upload_time_iso_8601": "2020-03-17T20:25:19.198747Z", "url": "https://files.pythonhosted.org/packages/b3/c7/b4f819eaa3a73482c3c0ae74107b91102f133ff070c1dd3873ae5eb4f038/EntropySGD-1.2.0.tar.gz", "yanked": false}], "1.2.1": [{"comment_text": "", "digests": {"md5": "a665b9e5ca317ee9a922a88983ff34d6", "sha256": "c7d16fef1bbafabd18ba3bf73f8058854642178ef83af528621e13bd02ce1868"}, "downloads": -1, "filename": "EntropySGD-1.2.1.tar.gz", "has_sig": false, "md5_digest": "a665b9e5ca317ee9a922a88983ff34d6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6131, "upload_time": "2020-03-20T14:26:14", "upload_time_iso_8601": "2020-03-20T14:26:14.677858Z", "url": "https://files.pythonhosted.org/packages/f4/e3/a596a6e3415126860015badd2227fed06a02d142bbec04cf481879a1d230/EntropySGD-1.2.1.tar.gz", "yanked": false}], "1.2.2": [{"comment_text": "", "digests": {"md5": "9c7c326c4c4911ba17318836dcaf85d2", "sha256": "7ffd620ddb38415be74db74d0b6628fdded367b7635d9de9c79428eacf7b0d11"}, "downloads": -1, "filename": "EntropySGD-1.2.2.tar.gz", "has_sig": false, "md5_digest": "9c7c326c4c4911ba17318836dcaf85d2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6116, "upload_time": "2020-03-20T14:33:03", "upload_time_iso_8601": "2020-03-20T14:33:03.263687Z", "url": "https://files.pythonhosted.org/packages/a6/63/005239a1eae77cfc173a8321a36c8ba6103d418332b12ba9ae7378d70878/EntropySGD-1.2.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "9c7c326c4c4911ba17318836dcaf85d2", "sha256": "7ffd620ddb38415be74db74d0b6628fdded367b7635d9de9c79428eacf7b0d11"}, "downloads": -1, "filename": "EntropySGD-1.2.2.tar.gz", "has_sig": false, "md5_digest": "9c7c326c4c4911ba17318836dcaf85d2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6116, "upload_time": "2020-03-20T14:33:03", "upload_time_iso_8601": "2020-03-20T14:33:03.263687Z", "url": "https://files.pythonhosted.org/packages/a6/63/005239a1eae77cfc173a8321a36c8ba6103d418332b12ba9ae7378d70878/EntropySGD-1.2.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:46:13 2020"}