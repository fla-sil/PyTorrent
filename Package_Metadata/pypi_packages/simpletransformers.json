{"info": {"author": "Thilina Rajapakse", "author_email": "chaturangarajapakshe@gmail.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![Downloads](https://pepy.tech/badge/simpletransformers)](https://pepy.tech/project/simpletransformers)\n<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\n[![All Contributors](https://img.shields.io/badge/all_contributors-25-orange.svg?style=flat-square)](#contributors-)\n<!-- ALL-CONTRIBUTORS-BADGE:END -->\n\n# Simple Transformers\n\nThis library is based on the [Transformers](https://github.com/huggingface/transformers) library by HuggingFace. Simple Transformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize a model, train the model, and evaluate a model.\n\nSupports\n- Sequence Classification\n- Token Classification (NER)\n- Question Answering\n- Language Model Fine-Tuning\n- Language Model Training\n- Language Generation\n- T5 Model\n- Multi-Modal Classification\n- Conversational AI.\n\n### Latest\n\n#### 2020-05-05\n\n- T5 Model support added\n\n#### 2020-05-03\n\n**New documentation is now live at https://thilinarajapakse.github.io/simpletransformers/**\n\nOnly text classification tasks are added so far but the others will follow in the next few days. Any feedback will be immensely helpful in improving the documentation! If you have any feedback, please leave a comment in the [issue](https://github.com/ThilinaRajapakse/simpletransformers/issues/342) I've opened for this.\n\n#### 2020-04-27\n\n* ELECTRA models can now be used with Language Model Training, Named Entity Recognition (Token Classification), Sequence Classification, and Question Answering.\n\n# Table of contents\n\n<!--ts-->\n- [Simple Transformers](#simple-transformers)\n    - [Latest](#latest)\n      - [2020-05-05](#2020-05-05)\n      - [2020-05-03](#2020-05-03)\n      - [2020-04-27](#2020-04-27)\n- [Table of contents](#table-of-contents)\n  - [Setup](#setup)\n    - [With Conda](#with-conda)\n      - [Optional](#optional)\n  - [Usage](#usage)\n    - [Structure](#structure)\n  - [Text Classification](#text-classification)\n    - [Task Specific Notes](#task-specific-notes)\n      - [Minimal Start for Binary Classification](#minimal-start-for-binary-classification)\n      - [Minimal Start for Multiclass Classification](#minimal-start-for-multiclass-classification)\n      - [Minimal Start for Multilabel Classification](#minimal-start-for-multilabel-classification)\n        - [Special Attributes](#special-attributes)\n      - [Minimal Start for Sentence Pair Classification](#minimal-start-for-sentence-pair-classification)\n      - [Real Dataset Examples](#real-dataset-examples)\n      - [ClassificationModel](#classificationmodel)\n  - [Named Entity Recognition](#named-entity-recognition)\n      - [Minimal Start](#minimal-start)\n      - [Real Dataset Examples](#real-dataset-examples-1)\n      - [NERModel](#nermodel)\n  - [Question Answering](#question-answering)\n    - [Data format](#data-format)\n    - [Minimal Example](#minimal-example)\n    - [Real Dataset Examples](#real-dataset-examples-2)\n    - [QuestionAnsweringModel](#questionansweringmodel)\n    - [Additional attributes for Question Answering tasks](#additional-attributes-for-question-answering-tasks)\n      - [*doc_stride: int*](#docstride-int)\n      - [*max_query_length: int*](#maxquerylength-int)\n      - [*n_best_size: int*](#nbestsize-int)\n      - [*max_answer_length: int*](#maxanswerlength-int)\n      - [*null_score_diff_threshold: float*](#nullscorediffthreshold-float)\n  - [Language Model Training](#language-model-training)\n    - [Data format](#data-format-1)\n    - [Minimal Example For Language Model Fine Tuning](#minimal-example-for-language-model-fine-tuning)\n      - [Example (Medium Article)](#example-medium-article)\n    - [Minimal Example For Language Model Training From Scratch](#minimal-example-for-language-model-training-from-scratch)\n    - [Minimal Example For Language Model Training With ELECTRA](#minimal-example-for-language-model-training-with-electra)\n    - [Real Dataset Example For Training a Language Model](#real-dataset-example-for-training-a-language-model)\n    - [LanguageModelingModel](#languagemodelingmodel)\n    - [Additional attributes for Language Modeling tasks](#additional-attributes-for-language-modeling-tasks)\n      - [*dataset_type: str*](#datasettype-str)\n      - [*dataset_class: Subclass of Pytorch Dataset*](#datasetclass-subclass-of-pytorch-dataset)\n      - [*block_size: int*](#blocksize-int)\n      - [*mlm: bool*](#mlm-bool)\n      - [*mlm_probability: float*](#mlmprobability-float)\n      - [*max_steps: int*](#maxsteps-int)\n      - [*config_name: str*](#configname-str)\n      - [*tokenizer_name: str*](#tokenizername-str)\n      - [*min_frequencey: int*](#minfrequencey-int)\n      - [*special_tokens: list*](#specialtokens-list)\n      - [*sliding_window: bool*](#slidingwindow-bool)\n      - [*stride: float*](#stride-float)\n    - [*config: dict*](#config-dict)\n    - [*generator_config: dict*](#generatorconfig-dict)\n    - [*discriminator_config: dict*](#discriminatorconfig-dict)\n  - [Language Generation](#language-generation)\n      - [Minimal Start](#minimal-start-1)\n      - [LanguageGenerationModel](#languagegenerationmodel)\n    - [Additional attributes for Language Generation tasks](#additional-attributes-for-language-generation-tasks)\n      - [*do_sample: bool*](#dosample-bool)\n      - [*prompt: str*](#prompt-str)\n      - [*length: int*](#length-int)\n      - [*stop_token: str*](#stoptoken-str)\n      - [*temperature: float*](#temperature-float)\n      - [*repetition_penalty: float*](#repetitionpenalty-float)\n      - [*k: int*](#k-int)\n      - [*p: float*](#p-float)\n      - [*padding_text: str*](#paddingtext-str)\n      - [*xlm_language: str*](#xlmlanguage-str)\n      - [*num_return_sequences: int*](#numreturnsequences-int)\n      - [*config: dict*](#config-dict-1)\n  - [T5 Transformer](#t5-transformer)\n    - [Data Format](#data-format-2)\n      - [Train and evaluation input formats](#train-and-evaluation-input-formats)\n      - [Prediction data format](#prediction-data-format)\n      - [Minimal Start](#minimal-start-2)\n      - [Evaluating with custom metrics](#evaluating-with-custom-metrics)\n      - [T5Model](#t5model)\n    - [Additional attributes for T5 Model](#additional-attributes-for-t5-model)\n      - [*dataset_class: Subclass of Pytorch Dataset*](#datasetclass-subclass-of-pytorch-dataset-1)\n      - [*do_sample: bool*](#dosample-bool-1)\n      - [*max_steps: int*](#maxsteps-int-1)\n      - [*evaluate_generated_text: bool*](#evaluategeneratedtext-bool)\n      - [*num_beams: int*](#numbeams-int)\n      - [*max_lemgth: int*](#maxlemgth-int)\n      - [*repetition_penalty: float*](#repetitionpenalty-float-1)\n      - [*length_penalty: float*](#lengthpenalty-float)\n      - [*early_stopping: bool*](#earlystopping-bool)\n      - [*preprocess_inputs: bool*](#preprocessinputs-bool)\n  - [Conversational AI](#conversational-ai)\n    - [Data format](#data-format-3)\n    - [Minimal Example](#minimal-example-1)\n    - [Real Dataset Example](#real-dataset-example)\n    - [ConvAIModel](#convaimodel)\n    - [Additional attributes for Conversational AI](#additional-attributes-for-conversational-ai)\n      - [*num_candidates: int*](#numcandidates-int)\n      - [*personality_permutations: int*](#personalitypermutations-int)\n      - [*max_history: int*](#maxhistory-int)\n      - [*lm_coef: int*](#lmcoef-int)\n      - [*mc_coef: int*](#mccoef-int)\n      - [*no_sample: bool*](#nosample-bool)\n      - [*max_length: int*](#maxlength-int)\n      - [*min_length: int*](#minlength-int)\n      - [*temperature: float*](#temperature-float-1)\n      - [*top_k: float*](#topk-float)\n      - [*top_p: float*](#topp-float)\n  - [Multi-Modal Classification](#multi-modal-classification)\n    - [Data format](#data-format-4)\n      - [1 - Directory based](#1---directory-based)\n      - [2 - Directory and file list](#2---directory-and-file-list)\n      - [3 - Pandas DataFrame](#3---pandas-dataframe)\n    - [Using custom names for column names or fields in JSON files](#using-custom-names-for-column-names-or-fields-in-json-files)\n    - [Specifying the file type extension for image and text files](#specifying-the-file-type-extension-for-image-and-text-files)\n    - [Label formats](#label-formats)\n    - [Creating a Model](#creating-a-model)\n    - [Training a Model](#training-a-model)\n    - [Evaluating a Model](#evaluating-a-model)\n    - [Predicting from a trained Model](#predicting-from-a-trained-model)\n  - [Regression](#regression)\n      - [Minimal Start for Regression](#minimal-start-for-regression)\n  - [Visualization Support](#visualization-support)\n  - [Experimental Features](#experimental-features)\n    - [Sliding Window For Long Sequences](#sliding-window-for-long-sequences)\n  - [Loading Saved Models](#loading-saved-models)\n  - [Default Settings](#default-settings)\n    - [Args Explained](#args-explained)\n      - [*output_dir: str*](#outputdir-str)\n      - [*cache_dir: str*](#cachedir-str)\n      - [*best_model_dir: str*](#bestmodeldir-str)\n      - [*fp16: bool*](#fp16-bool)\n      - [*fp16_opt_level: str*](#fp16optlevel-str)\n      - [*max_seq_length: int*](#maxseqlength-int)\n      - [*train_batch_size: int*](#trainbatchsize-int)\n      - [*gradient_accumulation_steps: int*](#gradientaccumulationsteps-int)\n      - [*eval_batch_size: int*](#evalbatchsize-int)\n      - [*num_train_epochs: int*](#numtrainepochs-int)\n      - [*weight_decay: float*](#weightdecay-float)\n      - [*learning_rate: float*](#learningrate-float)\n      - [*adam_epsilon: float*](#adamepsilon-float)\n      - [*max_grad_norm: float*](#maxgradnorm-float)\n      - [*do_lower_case: bool*](#dolowercase-bool)\n      - [*evaluate_during_training*](#evaluateduringtraining)\n      - [*evaluate_during_training_steps*](#evaluateduringtrainingsteps)\n      - [*evaluate_during_training_verbose*](#evaluateduringtrainingverbose)\n      - [*use_cached_eval_features*](#usecachedevalfeatures)\n      - [*save_eval_checkpoints*](#saveevalcheckpoints)\n      - [*logging_steps: int*](#loggingsteps-int)\n      - [*save_steps: int*](#savesteps-int)\n      - [*no_cache: bool*](#nocache-bool)\n      - [*save_model_every_epoch: bool*](#savemodeleveryepoch-bool)\n      - [*tensorboard_dir: str*](#tensorboarddir-str)\n      - [*overwrite_output_dir: bool*](#overwriteoutputdir-bool)\n      - [*reprocess_input_data: bool*](#reprocessinputdata-bool)\n      - [*process_count: int*](#processcount-int)\n      - [*n_gpu: int*](#ngpu-int)\n      - [*silent: bool*](#silent-bool)\n      - [*use_multiprocessing: bool*](#usemultiprocessing-bool)\n      - [*wandb_project: str*](#wandbproject-str)\n      - [*wandb_kwargs: dict*](#wandbkwargs-dict)\n      - [*use_early_stopping*](#useearlystopping)\n      - [*early_stopping_patience*](#earlystoppingpatience)\n      - [*early_stopping_delta*](#earlystoppingdelta)\n      - [*early_stopping_metric*](#earlystoppingmetric)\n      - [*early_stopping_metric_minimize*](#earlystoppingmetricminimize)\n      - [*manual_seed*](#manualseed)\n      - [*encoding*](#encoding)\n      - [*config*](#config)\n  - [Current Pretrained Models](#current-pretrained-models)\n  - [Acknowledgements](#acknowledgements)\n  - [Contributors \u2728](#contributors-%e2%9c%a8)\n<!--te-->\n\n## Setup\n\n### With Conda\n\n1. Install Anaconda or Miniconda Package Manager from [here](https://www.anaconda.com/distribution/)\n2. Create a new virtual environment and install packages.  \n`conda create -n transformers python pandas tqdm`  \n`conda activate transformers`  \nIf using cuda:  \n&nbsp;&nbsp;&nbsp;&nbsp;`conda install pytorch cudatoolkit=10.1 -c pytorch`  \nelse:  \n&nbsp;&nbsp;&nbsp;&nbsp;`conda install pytorch cpuonly -c pytorch`  \n\n3. Install Apex if you are using fp16 training. Please follow the instructions [here](https://github.com/NVIDIA/apex). (Installing Apex from pip has caused issues for several people.)\n\n4. Install simpletransformers.  \n`pip install simpletransformers` \n\n#### Optional\n\n1. Install Weights and Biases (wandb) for tracking and visualizing training in a web browser.  \n`pip install wandb`\n\n## Usage\n\nMost available hyperparameters are common for all tasks. Any special hyperparameters will be listed in the docs section for the corresponding class. See [Default Settings](#default-settings) and [Args Explained](#args-explained) sections for more information.\n\nExample scripts can be found in the `examples` directory.\n\nSee the [Changelog](https://github.com/ThilinaRajapakse/simpletransformers/blob/master/CHANGELOG.md) for up-to-date changes to the project.\n\n### Structure\n\n_The file structure has been updated starting with version 0.6.0. This should only affect import statements. The old import paths should still be functional although it is recommended to use the updated paths given below and in the minimal start examples_.\n\n* `simpletransformers.classification` - Includes all Classification models.\n  * `ClassificationModel`\n  * `MultiLabelClassificationModel`\n* `simpletransformers.ner` - Includes all Named Entity Recognition models.\n  * `NERModel`\n* `simpletransformers.question_answering` - Includes all Question Answering models.\n  * `QuestionAnsweringModel`\n\n\n_[Back to Table of Contents](#table-of-contents)_\n\n---\n\n## Text Classification\n\nSupports Binary Classification, Multiclass Classification, and Multilabel Classification.\n\nSupported model types:\n\n* ALBERT\n* BERT\n* CamemBERT\n* RoBERTa\n* DistilBERT\n* ELECTRA\n* FlauBERT\n* XLM\n* XLM-RoBERTa\n* XLNet\n\n### Task Specific Notes\n\n* Set `'sliding_window': True` in `args` to prevent text being truncated. The default *stride* is `'stride': 0.8` which is `0.8 * max_seq_length`. Training text will be split using a sliding window and each window will be assigned the label from the original text. During evaluation and prediction, the mode of the predictions for each window will be the final prediction on each sample. The `tie_value` (default `1`) will be used in the case of a tie.  \n*Currently not available for Multilabel Classification*\n\n#### Minimal Start for Binary Classification\n\n```python\nfrom simpletransformers.classification import ClassificationModel\nimport pandas as pd\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n# Train and Evaluation data needs to be in a Pandas Dataframe of two columns. The first column is the text with type str, and the second column is the label with type int.\ntrain_data = [['Example sentence belonging to class 1', 1], ['Example sentence belonging to class 0', 0]]\ntrain_df = pd.DataFrame(train_data)\n\neval_data = [['Example eval sentence belonging to class 1', 1], ['Example eval sentence belonging to class 0', 0]]\neval_df = pd.DataFrame(eval_data)\n\n# Create a ClassificationModel\nmodel = ClassificationModel('roberta', 'roberta-base') # You can set class weights by using the optional weight argument\n\n# Train the model\nmodel.train_model(train_df)\n\n# Evaluate the model\nresult, model_outputs, wrong_predictions = model.eval_model(eval_df)\n```\n\nIf you wish to add any custom metrics, simply pass them as additional keyword arguments. The keyword is the name to be given to the metric, and the value is the function that will calculate the metric. Make sure that the function expects two parameters with the first one being the true label, and the second being the predictions. (This is the default for sklearn metrics)\n\n```python\nimport sklearn\n\n\nresult, model_outputs, wrong_predictions = model.eval_model(eval_df, acc=sklearn.metrics.accuracy_score)\n```\n\n\nTo make predictions on arbitary data, the `predict(to_predict)` function can be used. For a list of text, it returns the model predictions and the raw model outputs.\n\n```python\npredictions, raw_outputs = model.predict(['Some arbitary sentence'])\n```\n\n#### Minimal Start for Multiclass Classification\n\nFor multiclass classification, simply pass in the number of classes to the `num_labels` optional parameter of `ClassificationModel`.\n\n```python\nfrom simpletransformers.classification import ClassificationModel\nimport pandas as pd\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n# Train and Evaluation data needs to be in a Pandas Dataframe containing at least two columns. If the Dataframe has a header, it should contain a 'text' and a 'labels' column. If no header is present, the Dataframe should contain at least two columns, with the first column is the text with type str, and the second column in the label with type int.\ntrain_data = [['Example sentence belonging to class 1', 1], ['Example sentence belonging to class 0', 0], ['Example eval senntence belonging to class 2', 2]]\ntrain_df = pd.DataFrame(train_data)\n\neval_data = [['Example eval sentence belonging to class 1', 1], ['Example eval sentence belonging to class 0', 0], ['Example eval senntence belonging to class 2', 2]]\neval_df = pd.DataFrame(eval_data)\n\n# Create a ClassificationModel\nmodel = ClassificationModel('bert', 'bert-base-cased', num_labels=3, args={'reprocess_input_data': True, 'overwrite_output_dir': True}) \n# You can set class weights by using the optional weight argument\n\n# Train the model\nmodel.train_model(train_df)\n\n# Evaluate the model\nresult, model_outputs, wrong_predictions = model.eval_model(eval_df)\n\npredictions, raw_outputs = model.predict([\"Some arbitary sentence\"])\n```\n\n#### Minimal Start for Multilabel Classification\n\nFor Multi-Label Classification, the labels should be multi-hot encoded. The number of classes can be specified (default is 2) by passing it to the `num_labels` optional parameter of `MultiLabelClassificationModel`.\n\n_Warning: Pandas can cause issues when saving and loading lists stored in a column. Check whether your list has been converted to a String!_\n\nThe default evaluation metric used is Label Ranking Average Precision ([LRAP](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html)) Score.\n\n```python\nfrom simpletransformers.classification import MultiLabelClassificationModel\nimport pandas as pd\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n# Train and Evaluation data needs to be in a Pandas Dataframe containing at least two columns, a 'text' and a 'labels' column. The `labels` column should contain multi-hot encoded lists.\ntrain_data = [['Example sentence 1 for multilabel classification.', [1, 1, 1, 1, 0, 1]]], [['This is another example sentence. ', [0, 1, 1, 0, 0, 0]]]\ntrain_df = pd.DataFrame(train_data, columns=['text', 'labels'])\n\neval_data = [['Example eval sentence for multilabel classification.', [1, 1, 1, 1, 0, 1]], ['Example eval senntence belonging to class 2', [0, 1, 1, 0, 0, 0]]]\neval_df = pd.DataFrame(eval_data)\n\n# Create a MultiLabelClassificationModel\nmodel = MultiLabelClassificationModel('roberta', 'roberta-base', num_labels=6, args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs': 5})\n# You can set class weights by using the optional weight argument\nprint(train_df.head())\n\n# Train the model\nmodel.train_model(train_df)\n\n# Evaluate the model\nresult, model_outputs, wrong_predictions = model.eval_model(eval_df)\nprint(result)\nprint(model_outputs)\n\npredictions, raw_outputs = model.predict(['This thing is entirely different from the other thing. '])\nprint(predictions)\nprint(raw_outputs)\n```\n\n##### Special Attributes\n\n* The args dict of `MultiLabelClassificationModel` has an additional `threshold` parameter with default value 0.5. The threshold is the value at which a given label flips from 0 to 1 when predicting. The `threshold` may be a single value or a list of value with the same length as the number of labels. This enables the use of seperate threshold values for each label.\n* `MultiLabelClassificationModel` takes in an additional optional argument `pos_weight`. This should be a list with the same length as the number of labels. This enables using different weights for each label when calculating loss during training and evaluation.\n\n#### Minimal Start for Sentence Pair Classification\n\n* Training and evaluation Dataframes must contain a `text_a`, `text_b`, and a `labels` column.\n* The `predict()` function expects a list of lists in the format below. A single sample input should also be a list of lists like `[[text_a, text_b]]`.\n\n```python\n[\n    [sample_1_text_a, sample_1_text_b],\n    [sample_2_text_a, sample_2_text_b],\n    [sample_3_text_a, sample_3_text_b],\n    # More samples\n]\n```\n\n```python\nfrom simpletransformers.classification import ClassificationModel\nimport pandas as pd\nimport sklearn\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\ntrain_data = [\n    ['Example sentence belonging to class 1', 'Yep, this is 1', 1],\n    ['Example sentence belonging to class 0', 'Yep, this is 0', 0],\n    ['Example  2 sentence belonging to class 0', 'Yep, this is 0', 0]\n]\n\ntrain_df = pd.DataFrame(train_data, columns=['text_a', 'text_b', 'labels'])\n\neval_data = [\n    ['Example sentence belonging to class 1', 'Yep, this is 1', 1],\n    ['Example sentence belonging to class 0', 'Yep, this is 0', 0],\n    ['Example  2 sentence belonging to class 0', 'Yep, this is 0', 0]\n]\n\neval_df = pd.DataFrame(eval_data, columns=['text_a', 'text_b', 'labels'])\n\ntrain_args={\n    'reprocess_input_data': True,\n    'overwrite_output_dir': True,\n    'num_train_epochs': 3,\n}\n\n# Create a ClassificationModel\nmodel = ClassificationModel('roberta', 'roberta-base', num_labels=2, use_cuda=True, cuda_device=0, args=train_args)\nprint(train_df.head())\n\n# Train the model\nmodel.train_model(train_df, eval_df=eval_df)\n\n# Evaluate the model\nresult, model_outputs, wrong_predictions = model.eval_model(eval_df, acc=sklearn.metrics.accuracy_score)\n\npredictions, raw_outputs = model.predict([[\"I'd like to puts some CD-ROMS on my iPad, is that possible?'\", \"Yes, but wouldn't that block the screen?\"]])\nprint(predictions)\nprint(raw_outputs)\n```\n\n#### Real Dataset Examples\n\n* [Yelp Reviews Dataset - Binary Classification](https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3?source=friends_link&sk=40726ceeadf99e1120abc9521a10a55c)\n* [AG News Dataset - Multiclass Classification](https://medium.com/swlh/simple-transformers-multi-class-text-classification-with-bert-roberta-xlnet-xlm-and-8b585000ce3a?source=friends_link&sk=90e1c97255b65cedf4910a99041d9dfc)\n* [Toxic Comments Dataset - Multilabel Classification](https://towardsdatascience.com/multi-label-classification-using-bert-roberta-xlnet-xlm-and-distilbert-with-simple-transformers-b3e0cda12ce5?source=friends_link&sk=354e688fe238bfb43e9a575216816219)\n* [Semantic Textual Similarity Benchmark - Sentence Pair](https://medium.com/@chaturangarajapakshe/solving-sentence-pair-tasks-using-simple-transformers-2496fe79d616?source=friends_link&sk=fbf7439e9c31f7aefa1613d423a0fd40)\n* [AG News Dataset - BERT (base and distilled), RoBERTa (base and distilled), and XLNet compared](https://towardsdatascience.com/to-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8?source=friends_link&sk=6a3c7940b18066ded94aeee95e354ed1)\n\n\n#### ClassificationModel\n\n`class simpletransformers.classification.ClassificationModel (model_type, model_name, args=None, use_cuda=True)`  \nThis class  is used for Text Classification tasks.\n\n`Class attributes`\n* `tokenizer`: The tokenizer to be used.\n* `model`: The model to be used.\n* `model_name`: model_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).\n* `device`: The device on which the model will be trained and evaluated.\n* `results`: A python dict of past evaluation results for the TransformerModel object.\n* `args`: A python dict of arguments used for training and evaluation.\n- `cuda_device`: (optional) int - Default = -1. Used to specify which GPU should be used.\n\n`Parameters`\n* `model_type`: (required) str - The type of model to use. Currently, BERT, XLNet, XLM, and RoBERTa models are available.\n* `model_name`: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See [Current Pretrained Models](#current-pretrained-models) for all available models.\n* `num_labels` (optional): The number of labels or classes in the dataset.\n* `weight` (optional): A list of length num_labels containing the weights to assign to each label for loss calculation.\n* `args`: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.\n* `use_cuda`: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.\n\n`class methods`  \n**`train_model(self, train_df, output_dir=None, show_running_loss=True, args=None, eval_df=None)`**\n\nTrains the model using 'train_df'\n\nArgs:  \n* `train_df`: Pandas Dataframe containing at least two columns. If the Dataframe has a header, it should contain a 'text' and a 'labels' column. If no header is present, the Dataframe should contain at least two columns, with the first column containing the text, and the second column containing the label. The model will be trained on this Dataframe.\n\n* `output_dir` (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.\n\n* `args` (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n\n* show_running_loss (optional): Set to False to disable printing running training loss to the terminal.\n\n* `eval_df` (optional): A DataFrame against which evaluation will be performed when `evaluate_during_training` is enabled. Is required if `evaluate_during_training` is enabled.\n\n* `**kwargs`: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). E.g. f1=sklearn.metrics.f1_score.\nA metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.\n\nReturns:  \n* None\n\n**`eval_model(self, eval_df, output_dir=None, verbose=False)`**\n\nEvaluates the model on eval_df. Saves results to output_dir.\n\nArgs:  \n* eval_df: Pandas Dataframe containing at least two columns. If the Dataframe has a header, it should contain a 'text' and a 'labels' column. If no header is present, the Dataframe should contain at least two columns, with the first column containing the text, and the second column containing the label. The model will be evaluated on this Dataframe.\n\n* output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.  \n\n* verbose: If verbose, results will be printed to the console on completion of evaluation.  \n\n* silent: If silent, tqdm progress bars will be hidden.\n\n* `**kwargs`: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). E.g. f1=sklearn.metrics.f1_score.\nA metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.\n\nReturns:  \n* result: Dictionary containing evaluation results. (Matthews correlation coefficient, tp, tn, fp, fn)  \n\n* model_outputs: List of model outputs for each row in eval_df  \n\n* wrong_preds: List of InputExample objects corresponding to each incorrect prediction by the model  \n\n**`predict(self, to_predict)`**\n\nPerforms predictions on a list of text.\n\nArgs:\n* to_predict: A python list of text (str) to be sent to the model for prediction.\n\nReturns:\n* preds: A python list of the predictions (0 or 1) for each text.  \n* model_outputs: A python list of the raw model outputs for each text.\n\nIf `config: {\"output_hidden_states\": True}`, two additional values will be returned.\n* all_embedding_outputs: Numpy array of shape *(batch_size, sequence_length, hidden_size)*\n* all_layer_hidden_states: Numpy array of shape *(num_hidden_layers, batch_size, sequence_length, hidden_size)*\n\n\n**`train(self, train_dataset, output_dir)`**\n\nTrains the model on train_dataset.\n*Utility function to be used by the train_model() method. Not intended to be used directly.*\n\n**`evaluate(self, eval_df, output_dir, prefix=\"\")`**\n\nEvaluates the model on eval_df.\n*Utility function to be used by the eval_model() method. Not intended to be used directly*\n\n**`load_and_cache_examples(self, examples, evaluate=False)`**\n\nConverts a list of InputExample objects to a TensorDataset containing InputFeatures. Caches the InputFeatures.\n*Utility function for train() and eval() methods. Not intended to be used directly*\n\n**`compute_metrics(self, preds, labels, eval_examples, **kwargs):`**\n\nComputes the evaluation metrics for the model predictions.\n\nArgs:\n* preds: Model predictions  \n\n* labels: Ground truth labels  \n\n* eval_examples: List of examples on which evaluation was performed  \n\nReturns:\n* result: Dictionary containing evaluation results. (Matthews correlation coefficient, tp, tn, fp, fn)  \n\n* wrong: List of InputExample objects corresponding to each incorrect prediction by the model  \n\n_[Back to Table of Contents](#table-of-contents)_\n\n---\n\n## Named Entity Recognition\n\nThis section describes how to use Simple Transformers for Named Entity Recognition. (If you are updating from a Simple Transformers before 0.5.0, note that `seqeval` needs to be installed to perform NER.)\n\n*This model can also be used for any other NLP task involving token level classification. Make sure you pass in your list of labels to the model if they are different from the defaults.*\n\nSupported model types:\n* BERT\n* CamemBERT\n* DistilBERT\n* ELECTRA\n* RoBERTa\n* XLM-RoBERTa\n\n```python\nmodel = NERModel('bert', 'bert-base-cased', labels=[\"LABEL_1\", \"LABEL_2\", \"LABEL_3\"])\n```\n\n#### Minimal Start\n\n```python\nfrom simpletransformers.ner import NERModel\nimport pandas as pd\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n# Creating train_df  and eval_df for demonstration\ntrain_data = [\n    [0, 'Simple', 'B-MISC'], [0, 'Transformers', 'I-MISC'], [0, 'started', 'O'], [1, 'with', 'O'], [0, 'text', 'O'], [0, 'classification', 'B-MISC'],\n    [1, 'Simple', 'B-MISC'], [1, 'Transformers', 'I-MISC'], [1, 'can', 'O'], [1, 'now', 'O'], [1, 'perform', 'O'], [1, 'NER', 'B-MISC']\n]\ntrain_df = pd.DataFrame(train_data, columns=['sentence_id', 'words', 'labels'])\n\neval_data = [\n    [0, 'Simple', 'B-MISC'], [0, 'Transformers', 'I-MISC'], [0, 'was', 'O'], [1, 'built', 'O'], [1, 'for', 'O'], [0, 'text', 'O'], [0, 'classification', 'B-MISC'],\n    [1, 'Simple', 'B-MISC'], [1, 'Transformers', 'I-MISC'], [1, 'then', 'O'], [1, 'expanded', 'O'], [1, 'to', 'O'], [1, 'perform', 'O'], [1, 'NER', 'B-MISC']\n]\neval_df = pd.DataFrame(eval_data, columns=['sentence_id', 'words', 'labels'])\n\n# Create a NERModel\nmodel = NERModel('bert', 'bert-base-cased', args={'overwrite_output_dir': True, 'reprocess_input_data': True})\n\n# Train the model\nmodel.train_model(train_df)\n\n# Evaluate the model\nresult, model_outputs, predictions = model.eval_model(eval_df)\n\n# Predictions on arbitary text strings\npredictions, raw_outputs = model.predict([\"Some arbitary sentence\"])\n\nprint(predictions)\n```\n\n#### Real Dataset Examples\n\n* [CoNLL Dataset Example](https://towardsdatascience.com/simple-transformers-named-entity-recognition-with-transformer-models-c04b9242a2a0?source=friends_link&sk=e8b98c994173cd5219f01e075727b096)\n\n#### NERModel\n\n`class simpletransformers.ner.ner_model.NERModel (model_type, model_name, labels=None, args=None, use_cuda=True)`  \nThis class  is used for Named Entity Recognition.\n\n`Class attributes`\n* `tokenizer`: The tokenizer to be used.\n* `model`: The model to be used.\n            model_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).\n* `device`: The device on which the model will be trained and evaluated.\n* `results`: A python dict of past evaluation results for the TransformerModel object.\n* `args`: A python dict of arguments used for training and evaluation.\n- `cuda_device`: (optional) int - Default = -1. Used to specify which GPU should be used.\n\n`Parameters`\n* `model_type`: (required) str - The type of model to use. Currently, BERT, XLNet, XLM, and RoBERTa models are available.\n* `model_name`: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See [Current Pretrained Models](#current-pretrained-models) for all available models.\n* `labels` (optional): A list of all Named Entity labels.  If not given, [\"O\", \"B-MISC\", \"I-MISC\",  \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"] will be used.\n* `args`: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.\n* `use_cuda`: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.\n\n`class methods`  \n**`train_model(self, train_data, output_dir=None, args=None, eval_df=None)`**\n\nTrains the model using 'train_data'\n\nArgs:  \n* train_data: train_data should be the path to a .txt file containing the training data OR a pandas DataFrame with 3 columns.\nIf a text file is used the data should be in the CoNLL format. i.e. One word per line, with sentences seperated by an empty line. \nThe first word of the line should be a word, and the last should be a Name Entity Tag.\nIf a DataFrame is given, each sentence should be split into words, with each word assigned a tag, and with all words from the same sentence given the same sentence_id.\n\n* output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.\n\n* show_running_loss (optional): Set to False to prevent running loss from being printed to console. Defaults to True.\n\n* args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n\n* eval_df (optional): A DataFrame against which evaluation will be performed when `evaluate_during_training` is enabled. Is required if `evaluate_during_training` is enabled.\n\n\nReturns:  \n* None\n\n**`eval_model(self, eval_data, output_dir=None, verbose=True)`**\n\nEvaluates the model on eval_data. Saves results to output_dir.\n\nArgs:  \n* eval_data: eval_data should be the path to a .txt file containing the evaluation data or a pandas DataFrame. If a text file is used the data should be in the CoNLL format. I.e. One word per line, with sentences seperated by an empty line. The first word of the line should be a word, and the last should be a Name Entity Tag. If a DataFrame is given, each sentence should be split into words, with each word assigned a tag, and with all words from the same sentence given the same sentence_id.\n\n* output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.  \n\n* verbose: If verbose, results will be printed to the console on completion of evaluation.  \n\nReturns:  \n* result: Dictionary containing evaluation results. (eval_loss, precision, recall, f1_score)\n\n* model_outputs: List of raw model outputs\n\n* preds_list: List of predicted tags\n\n**`predict(self, to_predict)`**\n\nPerforms predictions on a list of text.\n\nArgs:\n* to_predict: A python list of text (str) to be sent to the model for prediction.\n\nReturns:\n* preds: A Python list of lists with dicts containg each word mapped to its NER tag.\n* model_outputs: A python list of the raw model outputs for each text.\n\n\n**`train(self, train_dataset, output_dir)`**\n\nTrains the model on train_dataset.\n*Utility function to be used by the train_model() method. Not intended to be used directly.*\n\n**`evaluate(self, eval_dataset, output_dir, prefix=\"\")`**\n\nEvaluates the model on eval_dataset.\n*Utility function to be used by the eval_model() method. Not intended to be used directly*\n\n**`load_and_cache_examples(self, data, evaluate=False, no_cache=False, to_predict=None)`**\n\nConverts a list of InputExample objects to a TensorDataset containing InputFeatures. Caches the InputFeatures.\n*Utility function for train() and eval() methods. Not intended to be used directly*\n\n_[Back to Table of Contents](#table-of-contents)_\n\n---\n\n## Question Answering\n\nSupported model types:\n\n* ALBERT\n* BERT\n* DistilBERT\n* ELECTRA\n* XLM\n* XLNet\n\n### Data format\n\nFor question answering tasks, the input data can be in JSON files or in a Python list of dicts in the correct format.\n\nThe file should contain a single list of dictionaries. A dictionary represents a single context and its associated questions.\n\nEach such dictionary contains two attributes, the `\"context\"` and `\"qas\"`.\n* `context`: The paragraph or text from which the question is asked.\n* `qas`: A list of questions and answers.\n\nQuestions and answers are represented as dictionaries. Each dictionary in `qas` has the following format.\n* `id`: (string) A unique ID for the question. Should be unique across the entire dataset.\n* `question`: (string) A question. \n* `is_impossible`: (bool) Indicates whether the question can be answered correctly from the context. \n* `answers`: (list) The list of correct answers to the question.\n\nA single answer is represented by a dictionary with the following attributes.\n* `answer`: (string) The answer to the question. Must be a substring of the context.\n* `answer_start`: (int) Starting index of the answer in the context.\n\n### Minimal Example\n\n```python\nfrom simpletransformers.question_answering import QuestionAnsweringModel\nimport json\nimport os\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n# Create dummy data to use for training.\ntrain_data = [\n    {\n        'context': \"This is the first context\",\n        'qas': [\n            {\n                'id': \"00001\",\n                'is_impossible': False,\n                'question': \"Which context is this?\",\n                'answers': [\n                    {\n                        'text': \"the first\",\n                        'answer_start': 8\n                    }\n                ]\n            }\n        ]\n    },\n    {\n        'context': \"Other legislation followed, including the Migratory Bird Conservation Act of 1929, a 1937 treaty prohibiting the hunting of right and gray whales,\n            and the Bald Eagle Protection Act of 1940. These later laws had a low cost to society\u2014the species were relatively rare\u2014and little opposition was raised\",\n        'qas': [\n            {\n                'id': \"00002\",\n                'is_impossible': False,\n                'question': \"What was the cost to society?\",\n                'answers': [\n                    {\n                        'text': \"low cost\",\n                        'answer_start': 225\n                    }\n                ]\n            },\n            {\n                'id': \"00003\",\n                'is_impossible': False,\n                'question': \"What was the name of the 1937 treaty?\",\n                'answers': [\n                    {\n                        'text': \"Bald Eagle Protection Act\",\n                        'answer_start': 167\n                    }\n                ]\n            }\n        ]\n    }\n]\n\n# Save as a JSON file\nos.makedirs('data', exist_ok=True)\nwith open('data/train.json', 'w') as f:\n    json.dump(train_data, f)\n\n\n# Create the QuestionAnsweringModel\nmodel = QuestionAnsweringModel('distilbert', 'distilbert-base-uncased-distilled-squad', args={'reprocess_input_data': True, 'overwrite_output_dir': True})\n\n# Train the model with JSON file\nmodel.train_model('data/train.json')\n\n# The list can also be used directly\n# model.train_model(train_data)\n\n# Evaluate the model. (Being lazy and evaluating on the train data itself)\nresult, text = model.eval_model('data/train.json')\n\nprint(result)\nprint(text)\n\nprint('-------------------')\n\n# Making predictions using the model.\nto_predict = [{'context': 'This is the context used for demonstrating predictions.', 'qas': [{'question': 'What is this context?', 'id': '0'}]}]\n\nprint(model.predict(to_predict))\n```\n\n### Real Dataset Examples\n\n* [SQuAD 2.0 - Question Answering](https://towardsdatascience.com/question-answering-with-bert-xlnet-xlm-and-distilbert-using-simple-transformers-4d8785ee762a?source=friends_link&sk=e8e6f9a39f20b5aaf08bbcf8b0a0e1c2)\n\n### QuestionAnsweringModel\n\n`class simpletransformers.question_answering.QuestionAnsweringModel (model_type, model_name, args=None, use_cuda=True)`  \nThis class is used for Question Answering tasks.\n\n`Class attributes`\n- `tokenizer`: The tokenizer to be used.\n- `model`: The model to be used.\n            model_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).\n- `device`: The device on which the model will be trained and evaluated.\n- `results`: A python dict of past evaluation results for the TransformerModel object.\n- `args`: A python dict of arguments used for training and evaluation.\n- `cuda_device`: (optional) int - Default = -1. Used to specify which GPU should be used.\n\n`Parameters`\n- `model_type`: (required) str - The type of model to use.\n- `model_name`: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See [Current Pretrained Models](#current-pretrained-models) for all available models.\n- `args`: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.\n- `use_cuda`: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.\n\n`class methods`  \n**`train_model(self, train_df, output_dir=None, args=None, eval_df=None)`**\n\nTrains the model using 'train_file'\n\nArgs:  \n\n- `train_df`: Path to JSON file containing training data. The model will be trained on this file.\n            output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.\n\n- `output_dir` (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.\n\n- `show_running_loss` (Optional): Set to False to prevent training loss being printed.\n\n- `args` (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n\n- `eval_file` (optional): Path to JSON file containing evaluation data against which evaluation will be performed when evaluate_during_training is enabled. Is required if evaluate_during_training is enabled.\n\n- `**kwargs`: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n    A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.\n\nReturns:\n\n- None\n\n**`eval_model(self, eval_df, output_dir=None, verbose=False)`**\n\nEvaluates the model on eval_file. Saves results to output_dir.\n\nArgs:  \n\n- `eval_file`: Path to JSON file containing evaluation data. The model will be evaluated on this file.\n\n- `output_dir`: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.  \n\n- `verbose`: If verbose, results will be printed to the console on completion of evaluation.  \n\n- `**kwargs`: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n    A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.\n\nReturns:  \n\n- `result`: Dictionary containing evaluation results. (correct, similar, incorrect)\n\n- `text`: A dictionary containing the 3 dictionaries correct_text, similar_text (the predicted answer is a substring of the correct answer or vise versa), incorrect_text.\n\n**`predict(self, to_predict)`**\n\nPerforms predictions on a list of text.\n\nArgs:\n\n- `to_predict`: A python list of python dicts containing contexts and questions to be sent to the model for prediction.\n\n```python\nE.g: predict([\n    {\n        'context': \"Some context as a demo\",\n        'qas': [\n            {'id': '0', 'question': 'What is the context here?'},\n            {'id': '1', 'question': 'What is this for?'}\n        ]\n    }\n])\n```\n\n- `n_best_size` (Optional): Number of predictions to return. args['n_best_size'] will be used if not specified.\n\nReturns:\n\n- `preds`: A python list containg the predicted answer, and id for each question in to_predict.\n\n**`train(self, train_dataset, output_dir, show_running_loss=True, eval_file=None)`**\n\nTrains the model on train_dataset.\n*Utility function to be used by the train_model() method. Not intended to be used directly.*\n\n**`evaluate(self, eval_df, output_dir, , verbose=False)`**\n\nEvaluates the model on eval_df.\n*Utility function to be used by the eval_model() method. Not intended to be used directly*\n\n**`load_and_cache_examples(self, examples, evaluate=False, no_cache=False, output_examples=False)`**\n\nConverts a list of InputExample objects to a TensorDataset containing InputFeatures. Caches the InputFeatures.\n*Utility function for train() and eval() methods. Not intended to be used directly*\n\n### Additional attributes for Question Answering tasks\n\nQuestionAnsweringModel has a few additional attributes in its `args` dictionary, given below with their default values.\n\n```python\n  'doc_stride': 384,\n  'max_query_length': 64,\n  'n_best_size': 20,\n  'max_answer_length': 100,\n  'null_score_diff_threshold': 0.0\n```\n\n#### *doc_stride: int*\n\nWhen splitting up a long document into chunks, how much stride to take between chunks.\n\n#### *max_query_length: int*\n\nMaximum token length for questions. Any questions longer than this will be truncated to this length.\n\n#### *n_best_size: int*\n\nThe number of predictions given per question.\n\n#### *max_answer_length: int*\n\nThe maximum token length of an answer that can be generated.\n\n#### *null_score_diff_threshold: float*\n\nIf null_score - best_non_null is greater than the threshold predict null.\n\n_[Back to Table of Contents](#table-of-contents)_\n\n---\n\n## Language Model Training\n\nSupported model types:\n\n- BERT\n- CamemBERT\n- DistilBERT\n- ELECTRA\n- GPT-2\n- OpenAI-GPT\n- RoBERTa\n\n### Data format\n\nThe data should simply be placed in a text file. E.g.: [WikiText-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)\n\n### Minimal Example For Language Model Fine Tuning\n\nThe minimal example given below assumes that you have downloaded the WikiText-2 dataset.\n\n```python\nfrom simpletransformers.language_modeling import LanguageModelingModel\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\ntrain_args = {\n    \"reprocess_input_data\": True,\n    \"overwrite_output_dir\": True,\n}\n\nmodel = LanguageModelingModel('bert', 'bert-base-cased', args=train_args)\n\nmodel.train_model(\"wikitext-2/wiki.train.tokens\", eval_file=\"wikitext-2/wiki.test.tokens\")\n\nmodel.eval_model(\"wikitext-2/wiki.test.tokens\")\n\n```\n\n#### Example (Medium Article)\n\n- [Language Model Fine-tuning](https://medium.com/skilai/language-model-fine-tuning-for-pre-trained-transformers-b7262774a7ee?source=friends_link&sk=1f9f834447db7e748ae333c6490064fa)\n\n### Minimal Example For Language Model Training From Scratch\n\nYou can use any text file/files for training a new language model. Setting `model_name` to `None` will indicate that the language model should be trained from scratch.\n\nRequired for Language Model Training From Scratch:\n\n- `train_files` must be specifief when creating the `LanguagModelingModel`. This may be a path to a single file or a list of paths to multiple files.\n- `vocab_size` (in args dictionary)\n\n```python\nfrom simpletransformers.language_modeling import LanguageModelingModel\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\ntrain_args = {\n    \"reprocess_input_data\": True,\n    \"overwrite_output_dir\": True,\n    \"vocab_size\": 52000,\n}\n\nmodel = LanguageModelingModel('roberta', None, args=train_args)\n\nmodel.train_model(\"wikitext-2/wiki.train.tokens\", eval_file=\"wikitext-2/wiki.test.tokens\")\n\nmodel.eval_model(\"wikitext-2/wiki.test.tokens\")\n\n```\n\n### Minimal Example For Language Model Training With ELECTRA\n\n[ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB) is a new approach to pretraining Transformer Language Models. This method is comparatively less compute-intensive.\n\nYou can use the `save_discriminator()` and `save_generator()` methods to extract the pretrained models. The two models will be saved to `<output_dir>/discriminator_model` and `<output_dir>/generator_model` by default.\n\n```python\nfrom simpletransformers.language_modeling import LanguageModelingModel\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\ntrain_args = {\n    \"reprocess_input_data\": True,\n    \"overwrite_output_dir\": True,\n    \"vocab_size\": 52000,\n}\n\nmodel = LanguageModelingModel('electra', None, args=train_args, train_files=\"wikitext-2/wiki.train.tokens\")\n\n# Mixing standard ELECTRA architectures example\n# model = LanguageModelingModel(\n#     \"electra\",\n#     None,\n#     generator_name=\"google/electra-small-generator\",\n#     discriminator_name=\"google/electra-large-discriminator\",\n#     args=train_args,\n#     train_files=\"wikitext-2/wiki.train.tokens\",\n# )\n\nmodel.train_model(\"wikitext-2/wiki.train.tokens\", eval_file=\"wikitext-2/wiki.test.tokens\")\n\nmodel.eval_model(\"wikitext-2/wiki.test.tokens\")\n\n```\n\n### Real Dataset Example For Training a Language Model\n\n- [Esparanto Model trained with ELECTRA](https://medium.com/@chaturangarajapakshe/understanding-electra-and-training-an-electra-language-model-3d33e3a9660d?source=friends_link&sk=2b4b4a79954e3d7c84ab863efaea8c65)\n\n### LanguageModelingModel\n\n`class simpletransformers.language_modeling.LanguageModelingModel (model_type, model_name, generator_name=None, discriminator_name=None, args=None, use_cuda=True, cuda_device=-1)`  \nThis class is used for Question Answering tasks.\n\n`Class attributes`\n\n- `tokenizer`: The tokenizer to be used.\n- `model`: The model to be used.\n            model_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).\n- `device`: The device on which the model will be trained and evaluated.\n- `results`: A python dict of past evaluation results for the TransformerModel object.\n- `args`: A python dict of arguments used for training and evaluation.\n- `cuda_device`: (optional) int - Default = -1. Used to specify which GPU should be used.\n\n`Parameters`\n\n- `model_type`: (required) str - The type of model to use.\n- `model_name`: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See [Current Pretrained Models](#current-pretrained-models) for all available models. Set to `None` for language model training from scratch.\n- `generator_name`: (optional) A pretrained model name or path to a directory containing an ELECTRA generator model.\n- `discriminator_name`: (optional) A pretrained model name or path to a directory containing an ELECTRA discriminator model.\n- `args`: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.\n- `train_files`: (optional) List of files to be used when training the tokenizer.\n- `use_cuda`: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.\n- `cuda_device`: (optional) Specific GPU that should be used. Will use the first available GPU by default.\n\n`class methods`  \n**`train_model(self, train_file, output_dir=None, show_running_loss=True, args=None, eval_file=None, verbose=True,)`**\n\nTrains the model using 'train_file'\n\nArgs:  \n\n- `train_file`: Path to text file containing the text to train the language model on.\n\n- `output_dir` (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.\n\n- `show_running_loss` (Optional): Set to False to prevent training loss being printed.\n\n- `args` (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n\n- `eval_file` (optional): Path to eval file containing the text to evaluate the language model on. Is required if evaluate_during_training is enabled.\n\nReturns:\n\n- None\n\n**`eval_model(self, eval_file, output_dir=None, verbose=True, silent=False,)`**\n\nEvaluates the model on eval_file. Saves results to output_dir.\n\nArgs:  \n\n- `eval_file`: Path to eval file containing the text to evaluate the language model on.\n\n- `output_dir` (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.  \n\n- `verbose`: If verbose, results will be printed to the console on completion of evaluation.  \n\n- `silent`: If silent, tqdm progress bars will be hidden.\n\nReturns:  \n\n- `result`: Dictionary containing evaluation results. (correct, similar, incorrect)\n\n- `text`: A dictionary containing the 3 dictionaries correct_text, similar_text (the predicted answer is a substring of the correct answer or vise versa), incorrect_text.\n\n**`train_tokenizer(self, train_files, tokenizer_name=None, output_dir=None, use_trained_tokenizer=True)`\n\nTrain a new tokenizer on `train_files`.\n\nArgs:\n\n- `train_files`: List of files to be used when training the tokenizer.\n\n- `tokenizer_name`: Name of a pretrained tokenizer or a path to a directory containing a tokenizer.\n\n- `output_dir` (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.  \n\n- `use_trained_tokenizer` (optional): Load the trained tokenizer once training completes.\n\nReturns: None\n\n**`train(self, train_dataset, output_dir, show_running_loss=True, eval_file=None)`**\n\nTrains the model on train_dataset.\n*Utility function to be used by the train_model() method. Not intended to be used directly.*\n\n**`evaluate(self, eval_dataset, output_dir, , verbose=False)`**\n\nEvaluates the model on eval_dataset.\n*Utility function to be used by the eval_model() method. Not intended to be used directly*\n\n**`load_and_cache_examples(self, examples, evaluate=False, no_cache=False, output_examples=False)`**\n\nReads a text file from file_path and creates training features.\n*Utility function for train() and eval() methods. Not intended to be used directly*\n\n### Additional attributes for Language Modeling tasks\n\nLanguageModelingModel has a few additional attributes in its `args` dictionary, given below with their default values.\n\n```python\n    \"dataset_type\": \"None\",\n    \"dataset_class\": None,\n    \"custom_tokenizer\": None,\n    \"block_size\": 512,\n    \"mlm\": True,\n    \"mlm_probability\": 0.15,\n    \"max_steps\": -1,\n    \"config_name\": None,\n    \"tokenizer_name\": None,\n    \"min_frequency\": 2,\n    \"special_tokens\": [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"],\n    \"sliding_window\": False,\n    \"stride\": 0.8\n    \"config\": {},\n    \"generator_config\": {},\n    \"discriminator_config\": {},\n```\n\n#### *dataset_type: str*\n\nUsed to specify the Dataset type to be used. The choices are given below.\n\n- `simple` (or None) - Each line in the train files are considered to be a single, separate sample. `sliding_window` can be set to True to \nautomatically split longer sequences into samples of length `max_seq_length`. Uses multiprocessing for significantly improved performance on multicore systems.\n\n- `line_by_line` - Treats each line in the train files as a seperate sample.\n\n- `text` - Treats each file in `train_files` as a seperate sample.\n\n*Using `simple` is recommended.*\n\n#### *dataset_class: Subclass of Pytorch Dataset*\n\nA custom dataset class to use.\n\n#### *block_size: int*\n\nOptional input sequence length after tokenization.\nThe training dataset will be truncated in block of this size for training.\nDefault to the model max input length for single sentence inputs (take into account special tokens).\n\n#### *mlm: bool*\n\nTrain with masked-language modeling loss instead of language modeling\n\n#### *mlm_probability: float*\n\nRatio of tokens to mask for masked language modeling loss\n\n#### *max_steps: int*\n\nIf > 0: set total number of training steps to perform. Override num_train_epochs.\n\n#### *config_name: str*\n\nName of pretrained config or path to a directory containing a `config.json` file.\n\n#### *tokenizer_name: str*\n\nName of pretrained tokenizer or path to a directory containing tokenizer files.\n\n#### *min_frequencey: int*\n\nMinimum frequency required for a word to be added to the vocabulary.\n\n#### *special_tokens: list*\n\nList of special tokens to be used when training a new tokenizer.\n\n#### *sliding_window: bool*\n\nWhether sliding window technique should be used when preparing data. *Only works with SimpleDataset.*\n\n#### *stride: float*\n\nA fraction of the `max_seq_length` to use as the stride when using a sliding window\n\n### *config: dict*\n\nKey-values given here will override the default values used in a model `Config`.\n\n### *generator_config: dict*\n\nKey-values given here will override the default values used in an Electra generator model `Config`.\n\n### *discriminator_config: dict*\n\nKey-values given here will override the default values used in an Electra discriminator model `Config`.\n\n_[Back to Table of Contents](#table-of-contents)_\n\n---\n\n## Language Generation\n\nThis section describes how to use Simple Transformers for Langauge Generation.\n\nSupported model types:\n\n* CTRL\n* GPT-2\n* OpenAI-GPT\n* Transformer-XL\n* XLM\n* XLNet\n\n#### Minimal Start\n\n```python\nimport logging\nfrom simpletransformers.language_generation import LanguageGenerationModel\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\nmodel = LanguageGenerationModel(\"gpt2\", \"gpt2\")\nmodel.generate(\"Let's give a minimal start to the model like\")\n```\n\n#### LanguageGenerationModel\n\n`class simpletransformers.language_generation.language_generation_model.LanguageGenerationModel (self, model_type, model_name, args=None, use_cuda=True, cuda_device=-1, **kwargs)`  \nThis class  is used for Language Generation.\n\n`Class attributes`\n* `tokenizer`: The tokenizer to be used.\n* `model`: The model to be used.\n            model_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).\n* `device`: The device on which the model will be trained and evaluated.\n* `args`: A python dict of arguments used for training and evaluation.\n- `cuda_device`: (optional) int - Default = -1. Used to specify which GPU should be used.\n\n`Parameters`\n* `model_type`: (required) str - The type of model to use.\n\n* `model_name`: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See [Current Pretrained Models](#current-pretrained-models) for all available models.\n\n* `args`: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.\n\n* `use_cuda`: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.\n\n* `cuda_device` (optional): Specific GPU that should be used. Will use the first available GPU by default.\n\n* `**kwargs` (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n\n\n`class methods`  \n\n**`generate(self, prompt=None, args=None, verbose=True)`**\n\nGenerate text using a `LanguageGenerationModel`\n\nArgs:\n\n* `prompt` (optional): A prompt text for the model. If given, will override args[\"prompt\"]\n\n* `args` (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n\n* `verbose` (optional): If verbose, generated text will be logged to the console.\n\n\nReturns:\n\n* `generated_sequences`: Sequences of text generated by the model.\n\n### Additional attributes for Language Generation tasks\n\nLanguageGenerationModel has a few additional attributes in its `args` dictionary, given below with their default values.\n\n```python\n    \"do_sample\": True,\n    \"prompt\": \"\",\n    \"length\": 20,\n    \"stop_token\": None,\n    \"temperature\": 1.0,\n    \"repetition_penalty\": 1.0,\n    \"k\": 0,\n    \"p\": 0.9,\n    \"padding_text\": \"\",\n    \"xlm_language\": \"\",\n    \"num_return_sequences\": 1,\n    \"config_name\": None,\n    \"tokenizer_name\": None,\n```\n\n#### *do_sample: bool*\n\nIf set to `False` greedy decoding is used. Otherwise sampling is used. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n\n#### *prompt: str*\n\nA prompt text for the model.\n\n#### *length: int*\n\nLength of the text to generate\n\n#### *stop_token: str*\n\nToken at which text generation is stopped\n\n#### *temperature: float*\n\nTemperature of 1.0 is the default. Lowering this makes the sampling *greedier*\n\n#### *repetition_penalty: float*\n\nPrimarily useful for CTRL model; in that case, use 1.2\n\n#### *k: int*\n\n*k* value for top-k sampling\n\n#### *p: float*\n\n*p* value for top-p (nucleus) sampling\n\n#### *padding_text: str*\n\nPadding text for Transfo-XL and XLNet.\n\n#### *xlm_language: str*\n\nOptional language when used with the XLM model.\n\n#### *num_return_sequences: int*\n\nThe number of samples to generate.\n\n#### *config: dict*\nKey-values given here will override the default values used in a model Config.\n\n\n_[Back to Table of Contents](#table-of-contents)_\n\n---\n\n## T5 Transformer\n\n*T5 model seems to be working fine, but please open an issue if you run across any problems*\n\nThe T5 Transformer is an Encoder-Decoder architecture where both the input and targets are text sequences. The task that should be performed on the input is defined by a *prefix*. This means that the same T5 model can perform multiple tasks.\n\nYou can train the T5 model on a completely new task by simply using a new `prefix`.\n\n### Data Format\n\nThe input to a T5 model has the following pattern;\n\n```python\n\"<prefix>: <input_text> </s>\"\n```\n\nThe *label* sequence has the following pattern;\n\n```python\n\"<target_sequence> </s>\"\n```\n\n#### Train and evaluation input formats\n\nThe inputs to both the `train_model()` and `eval_model()` methods should be a Pandas DataFrame containing the 3 columns - `prefix`, `input_text`, `target_text`.\n\n- `prefix`: A string indicating the task to perform. (E.g. `\"question\"`, `\"stsb\"`)\n- `input_text`: The input text sequence. `prefix` is automatically prepended to form the full input. (`<prefix>: <input_text>`)\n- `target_text`: The target sequence\n\n\nIf `preprocess_inputs` is set to `True` in the model `args`, then the `< /s>` tokens (including preceeding space) and the `: ` *(prefix separator including trailing separator)* between `prefix`  and `input_text` are automatically added. Otherwise, the input DataFrames must contain the `< /s>` tokens (including preceeding space) and the `:` *(prefix separator including trailing separator)*.\n\n\n#### Prediction data format\n\nThe prediction data should be a list of strings with the `prefix` and the `: ` *(prefix separator)* included.\n\nIf `preprocess_inputs` is set to `True` in the model `args`, then the ` < /s>` token (including preceeding space) is automatically added to each string in the list. Otherwise, the strings must have the ` < /s>` (including preceeding space) must be included.\n\n\n#### Minimal Start\n\n```python\nimport logging\n\nimport pandas as pd\nfrom simpletransformers.t5 import T5Model\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n\ntrain_data = [\n    [\"convert\", \"one\", \"1\"],\n    [\"convert\", \"two\", \"2\"],\n]\n\ntrain_df = pd.DataFrame(train_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n\neval_data = [\n    [\"convert\", \"three\", \"3\"],\n    [\"convert\", \"four\", \"4\"],\n]\n\neval_df = pd.DataFrame(eval_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n\neval_df = train_df.copy()\n\nmodel_args = {\n    \"reprocess_input_data\": True,\n    \"overwrite_output_dir\": True,\n    \"max_seq_length\": 10,\n    \"train_batch_size\": 2,\n    \"num_train_epochs\": 200,\n}\n\n# Create T5 Model\nmodel = T5Model(\"t5-base\", args=model_args)\n\n# Train T5 Model on new task\nmodel.train_model(train_df)\n\n# Evaluate T5 Model on new task\nresults = model.eval_model(eval_df)\n\n# Predict with trained T5 model\nprint(model.predict([\"convert: four\"]))\n\n```\n\n#### Evaluating with custom metrics\n\nYou can evaluate the models' generated sequences using custom metric functions (including evaluation during training). However, due to the way T5 outputs are generated, this may be significantly slower than evaluation with other models.\n\nNote, you must set `evaluate_generated_text` to `True` to evaluate generated sequences.\n\n```python\nimport logging\n\nimport pandas as pd\nfrom simpletransformers.t5 import T5Model\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n\ntrain_data = [\n    [\"convert\", \"one\", \"1\"],\n    [\"convert\", \"two\", \"2\"],\n]\n\ntrain_df = pd.DataFrame(train_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n\neval_data = [\n    [\"convert\", \"three\", \"3\"],\n    [\"convert\", \"four\", \"4\"],\n]\n\neval_df = pd.DataFrame(eval_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n\neval_df = train_df.copy()\n\nmodel_args = {\n    \"reprocess_input_data\": True,\n    \"overwrite_output_dir\": True,\n    \"max_seq_length\": 10,\n    \"train_batch_size\": 2,\n    \"num_train_epochs\": 200,\n    \"save_eval_checkpoints\": False,\n    \"save_model_every_epoch\": False,\n    # \"silent\": True,\n    \"evaluate_generated_text\": True,\n    \"evaluate_during_training\": True,\n    \"evaluate_during_training_verbose\": True,\n}\n\nmodel = T5Model(\"t5-base\", args=model_args)\n\n\ndef count_matches(labels, preds):\n    print(labels)\n    print(preds)\n    return sum([1 if label == pred else 0 for label, pred in zip(labels, preds)])\n\n\nmodel.train_model(train_df, eval_data=eval_df, matches=count_matches)\n\nprint(model.eval_model(eval_df, matches=count_matches))\n\n```\n\n#### T5Model\n\n`class simpletransformers.t5.t5_model.T5Model (self, model_name, args=None, use_cuda=True, cuda_device=-1, **kwargs)`  \nThis class  is used for the T5 Transformer.\n\n`Class attributes`\n* `tokenizer`: The tokenizer to be used.\n* `model`: The model to be used.\n            model_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).\n* `device`: The device on which the model will be trained and evaluated.\n* `args`: A python dict of arguments used for training and evaluation.\n- `cuda_device`: (optional) int - Default = -1. Used to specify which GPU should be used.\n\n`Parameters`\n\n* `model_name`: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See [Current Pretrained Models](#current-pretrained-models) for all available models.\n\n* `args`: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.\n\n* `cuda_device`: (optional) Specific GPU that should be used. Will use the first available GPU by default.\n\n* `use_cuda`: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.\n\n* `**kwargs`: (optional) For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n\n\n\n`class methods`  \n**`train_model(self, train_data, output_dir=None, show_running_loss=True, args=None, eval_df=None)`**\n\nTrains the model using 'train_data'\n\nArgs:  \n* `train_data`: Pandas DataFrame containing the 3 columns - `prefix`, `input_text`, `target_text`.\n                    - `prefix`: A string indicating the task to perform. (E.g. `\"question\"`, `\"stsb\"`)\n                    - `input_text`: The input text sequence. `prefix` is automatically prepended to form the full input. (`<prefix>: <input_text>`)\n                    - `target_text`: The target sequence\n\n* `output_dir` (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.\n\n* `args` (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n\n* show_running_loss (optional): Set to False to disable printing running training loss to the terminal.\n\n* `eval_data` (optional): A DataFrame against which evaluation will be performed when `evaluate_during_training` is enabled. Is required if `evaluate_during_training` is enabled.\n\n* `**kwargs`: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\nA metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs will be lists of strings. Note that this will slow down training significantly as the predicted sequences need to be generated.\n\nReturns:  \n* None\n\n**`eval_model(self, eval_data, output_dir=None, verbose=True, silent=False)`**\n\nEvaluates the model on eval_data. Saves results to output_dir.\n\nArgs:  \n* eval_data: Pandas DataFrame containing the 3 columns - `prefix`, `input_text`, `target_text`.\n  - `prefix`: A string indicating the task to perform. (E.g. `\"question\"`, `\"stsb\"`)\n  - `input_text`: The input text sequence. `prefix` is automatically prepended to form the full input. (`<prefix>: <input_text>`)\n  - `target_text`: The target sequence  \n* output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.  \n\n* verbose: If verbose, results will be printed to the console on completion of evaluation.  \n\n* silent: If silent, tqdm progress bars will be hidden.\n\n* `**kwargs`: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.\n\n\nReturns:  \n* result: Dictionary containing evaluation results.\n\n**`predict(self, to_predict)`**\n\nPerforms predictions on a list of text.\n\nArgs:\n* to_predict: A python list of text (str) to be sent to the model for prediction. Note that the prefix should be prepended to the text.\n\nReturns:\n* preds: A python list of the generated sequences.\n\n\n**`train(self, train_dataset, output_dir)`**\n\nTrains the model on train_dataset.\n*Utility function to be used by the train_model() method. Not intended to be used directly.*\n\n**`evaluate(self, eval_dataset, output_dir, prefix=\"\")`**\n\nEvaluates the model on eval_dataset.\n*Utility function to be used by the eval_model() method. Not intended to be used directly*\n\n**`load_and_cache_examples(self, examples, evaluate=False)`**\n\nCreates a `T5Dataset` from data.\n\n*Utility function for train() and eval() methods. Not intended to be used directly*\n\n**`compute_metrics(self, preds, labels, **kwargs):`**\n\nComputes the evaluation metrics for the model predictions.\n\nArgs:\n\n* `labels`: List of target sequences\n* `preds`: List of model generated outputs\n* `**kwargs`: Custom metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.\n\nReturns:\n\n* result: Dictionary containing evaluation results.\n\n\n### Additional attributes for T5 Model\n\n`T5Model` has a few additional attributes in its `args` dictionary, given below with their default values.\n\n```python\n{\n    \"dataset_class\": None,\n    \"max_steps\": -1,\n    \"evaluate_generated_text\": False,\n    \"num_beams\": 1,\n    \"repetition_penalty\": 2.5,\n    \"length_penalty\": 1.0,\n    \"preprocess_inputs\": True,\n}\n```\n\n#### *dataset_class: Subclass of Pytorch Dataset*\n\nA custom dataset class to use.\n\n#### *do_sample: bool*\n\nIf set to `False` greedy decoding is used. Otherwise sampling is used. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n\n#### *max_steps: int*\n\nMaximum number of training steps. Will override the effect of `num_train_epochs`.\n\n#### *evaluate_generated_text: bool*\n\nGenerate sequences for evaluation.\n\n#### *num_beams: int*\n\nNumber of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\n\n#### *max_lemgth: int*\n\nThe max length of the sequence to be generated.  Between `min_length` and infinity. Default to 20.\n\n#### *repetition_penalty: float*\n\nThe parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\n\n#### *length_penalty: float*\n\nExponential penalty to the length. Default to 1.\n\n#### *early_stopping: bool*\n\nif set to `True` beam search is stopped when at least `num_beams` sentences finished per batch.\n\n#### *preprocess_inputs: bool*\n\nAutomatically add `:` and `< /s>` tokens to `train_model()` and `eval_model()` inputs. Automatically add `< /s>` to each string in `to_predict` in `predict()`.\n\n\n_[Back to Table of Contents](#table-of-contents)_\n\n---\n\n## Conversational AI\n\nChatbot creation based on the Hugging Face [State-of-the-Art Conversational AI](https://github.com/huggingface/transfer-learning-conv-ai).\n\nSupported model types:\n\n- GPT\n- GPT2\n\n### Data format\n\nData format follows the [Facebook Persona-Chat](http://arxiv.org/abs/1801.07243) format. A JSON formatted version by Hugging Face is found [here](https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json). The JSON file is directly compatible with this library (and it will be automatically downloaded and used if no dataset is specified).\n\nEach entry in personachat is a **dict** with two keys `personality` and `utterances`, the dataset is a list of entries.\n\n- `personality`:  **list of strings** containing the personality of the agent\n- `utterances`: **list of dictionaries**, each of which has two keys which are **lists of strings**.\n  - `candidates`: [next_utterance_candidate_1, ..., next_utterance_candidate_19]  \n        The last candidate is the ground truth response observed in the conversational data\n  - `history`: [dialog_turn_0, ... dialog_turn N], where N is an odd number since the other user starts every conversation.\n\nPreprocessing:  \n\n- Spaces before periods at end of sentences\n- everything lowercase\n\nExample train data:\n\n```json\n[\n    {\n        \"personality\": [\n            \"i like computers .\",\n            \"i like reading books .\",\n            \"i like talking to chatbots .\",\n            \"i love listening to classical music .\"\n        ],\n        \"utterances\": [\n            {\n                \"candidates\": [\n                    \"i try to wear all black every day . it makes me feel comfortable .\",\n                    \"well nursing stresses you out so i wish luck with sister\",\n                    \"yeah just want to pick up nba nfl getting old\",\n                    \"i really like celine dion . what about you ?\",\n                    \"no . i live near farms .\",\n                    \"mother taught me to cook ! we are looking for an exterminator .\",\n                    \"i enjoy romantic movie . what is your favorite season ? mine is summer .\",\n                    \"editing photos takes a lot of work .\",\n                    \"you must be very fast . hunting is one of my favorite hobbies .\",\n                    \"hi there . i'm feeling great! how about you ?\"\n                ],\n                \"history\": [\n                    \"hi , how are you ?\"\n                ]\n            },\n            {\n                \"candidates\": [\n                    \"i have trouble getting along with family .\",\n                    \"i live in texas , what kind of stuff do you do in \",\n                    \"toronto ?\",\n                    \"that's so unique ! veganism and line dancing usually don't mix !\",\n                    \"no , it isn't that big . do you travel a lot\",\n                    \"that's because they are real ; what do you do for work ?\",\n                    \"i am lazy all day lol . my mom wants me to get a job and move out\",\n                    \"i was born on arbor day , so plant a tree in my name\",\n                    \"okay , i should not tell you , its against the rules \",\n                    \"i like to talk to chatbots too ! do you know why ? .\"\n                ],\n                \"history\": [\n                    \"hi , how are you ?\",\n                    \"hi there . i'm feeling great! how about you ?\",\n                    \"not bad ! i am trying out this chatbot .\"\n                ]\n            },\n            {\n                \"candidates\": [\n                    \"ll something like that . do you play games ?\",\n                    \"does anything give you relief ? i hate taking medicine for mine .\",\n                    \"i decorate cakes at a local bakery ! and you ?\",\n                    \"do you eat lots of meat\",\n                    \"i am so weird that i like to collect people and cats\",\n                    \"how are your typing skills ?\",\n                    \"yeah . i am headed to the gym in a bit to weight lift .\",\n                    \"yeah you have plenty of time\",\n                    \"metal is my favorite , but i can accept that people listen to country . haha\",\n                    \"that's why you desire to be controlled . let me control you person one .\",\n                    \"two dogs they are the best , how about you ?\",\n                    \"you do art ? what kind of art do you do ?\",\n                    \"i love watching baseball outdoors on sunny days .\",\n                    \"oh i see . do you ever think about moving ? i do , it is what i want .\",\n                    \"because i am a chatbot too, silly !\"\n                ],\n                \"history\": [\n                    \"hi , how are you ?\",\n                    \"hi there . i'm feeling great! how about you ?\",\n                    \"not bad ! i am trying out this chatbot .\",\n                    \"i like to talk to chatbots too ! do you know why ? .\",\n                    \"no clue, why don't you tell me ?\"\n                ]\n            }\n        ]\n    }\n]\n```\n\n### Minimal Example\n\nYou can download the pretrained (OpenAI GPT based) Conversation AI model open-sourced by Hugging Face [here](https://s3.amazonaws.com/models.huggingface.co/transfer-learning-chatbot/gpt_personachat_cache.tar.gz).\n\nFor the minimal example given below, you can download the model and extract it to `gpt_personachat_cache`. Note that you can use any of the other GPT or GPT-2 models but they will require more training.\n\nYou will also need to create the JSON file given in the Data Format section and save it as `data/minimal_train.json`.\n\n```python\nfrom simpletransformers.conv_ai import ConvAIModel\n\n\ntrain_args = {\n    \"num_train_epochs\": 50,\n    \"save_model_every_epoch\": False,\n}\n\n# Create a ConvAIModel\nmodel = ConvAIModel(\"gpt\", \"gpt_personachat_cache\", use_cuda=True, args=train_args)\n\n# Train the model\nmodel.train_model(\"data/minimal_train.json\")\n\n# Evaluate the model\nmodel.eval_model()\n\n# Interact with the trained model.\nmodel.interact()\n\n```\n\nThe `interact()` method can be given a list of Strings which will be used to build a personality. If a list of Strings is not given, a random personality will be chosen from PERSONA-CHAT instead.\n\n### Real Dataset Example\n\n- [Persona-Chat Conversational AI](https://medium.com/@chaturangarajapakshe/how-to-train-your-chatbot-with-simple-transformers-da25160859f4?sk=edd04e406e9a3523fcfc46102529e775)\n\n### ConvAIModel\n\n`class simpletransformers.conv_ai.ConvAIModel ( model_type, model_name, args=None, use_cuda=True, cuda_device=-1, **kwargs)`  \nThis class is used to build Conversational AI.\n\n`Class attributes`\n\n- `tokenizer`: The tokenizer to be used.\n- `model`: The model to be used.\n            model_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).\n- `device`: The device on which the model will be trained and evaluated.\n- `results`: A python dict of past evaluation results for the TransformerModel object.\n- `args`: A python dict of arguments used for training and evaluation.\n\n`Parameters`\n\n- `model_type`: (required) str - The type of model to use.\n- `model_name`: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See [Current Pretrained Models](#current-pretrained-models) for all available models.\n- `args`: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.\n- `use_cuda`: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.\n- `cuda_device`: (optional) int - Default = -1. Used to specify which GPU should be used.\n\n`class methods`  \n**`train_model(self, train_file=None, output_dir=None, show_running_loss=True, args=None, eval_file=None)`**\n\nTrains the model using 'train_file'\n\nArgs:  \n\n- train_df: ath to JSON file containing training data. The model will be trained on this file.\n            output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.\n\n- output_dir (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.\n\n- show_running_loss (Optional): Set to False to prevent training loss being printed.\n\n- args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n\n- eval_file (optional): Evaluation data against which evaluation will be performed when evaluate_during_training is enabled. If not given when evaluate_during_training is enabled, the evaluation data from PERSONA-CHAT will be used.\n\nReturns:\n\n- None\n\n**`eval_model(self, eval_file, output_dir=None, verbose=True, silent=False)`**\n\nEvaluates the model on eval_file. Saves results to output_dir.\n\nArgs:  \n\n- eval_file: Path to JSON file containing evaluation data. The model will be evaluated on this file.  \nIf not given, eval dataset from PERSONA-CHAT will be used.\n\n- output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.  \n\n- verbose: If verbose, results will be printed to the console on completion of evaluation.\n\n- silent: If silent, tqdm progress bars will be hidden.\n\nReturns:  \n\n- result: Dictionary containing evaluation results. (correct, similar, incorrect)\n\n- text: A dictionary containing the 3 dictionaries correct_text, similar_text (the predicted answer is a substring of the correct answer or vise versa), incorrect_text.\n\n**`interact(self, personality=None)`**\n\nInteract with a model in the terminal.\n\nArgs:\n\n- personality (optional): A list of sentences that the model will use to build a personality.  \nIf not given, a random personality from PERSONA-CHAT will be picked.\n\n```python\nmodel.interact(\n    personality=[\n        \"i like computers .\",\n        \"i like reading books .\",\n        \"i love classical music .\",\n        \"i am very social .\"\n    ]\n)\n```\n\nReturns:\n\n- None\n\n**`train(self, train_dataloader, output_dir, show_running_loss=True, eval_dataloader=None, verbose=verbose)`**\n\nTrains the model on train_dataset.\n*Utility function to be used by the train_model() method. Not intended to be used directly.*\n\n**`evaluate(self, eval_file, output_dir, verbose=True, silent=False)`**\n\nEvaluates the model on eval_file.\n*Utility function to be used by the eval_model() method. Not intended to be used directly*\n\n**`load_and_cache_examples(self, dataset_path=None, evaluate=False, no_cache=False, verbose=True, silent=False)`**\n\nLoads, tokenizes, and prepares data for training and/or evaluation.\n*Utility function for train() and eval() methods. Not intended to be used directly*\n\n### Additional attributes for Conversational AI\n\nConvAIModel has a few additional attributes in its `args` dictionary, given below with their default values.\n\n```python\n    \"num_candidates\": 2,\n    \"personality_permutations\": 1,\n    \"max_history\": 2,\n    \"lm_coef\": 2.0,\n    \"mc_coef\": 1.0,\n    \"no_sample\": False,\n    \"max_length\": 20,\n    \"min_length\": 1,\n    \"temperature\": 0.7,\n    \"top_k\": 0,\n    \"top_p\": 0.9,\n```\n\n#### *num_candidates: int*\n\nNumber of candidates for training\n\n#### *personality_permutations: int*\n\nNumber of permutations of personality sentences\".\n\n#### *max_history: int*\n\nNumber of previous exchanges to keep in history\n\n#### *lm_coef: int*\n\nLM loss coefficient\n\n#### *mc_coef: int*\n\nMultiple-choice loss coefficient\n\n#### *no_sample: bool*\n\nSet to use greedy decoding instead of sampling\n\n#### *max_length: int*\n\nMaximum length of the output utterances\n\n#### *min_length: int*\n\nMinimum length of the output utterances\n\n#### *temperature: float*\n\nSampling softmax temperature\n\n#### *top_k: float*\n\nFilter top-k tokens before sampling (<=0: no filtering)\n\n#### *top_p: float*\n\nNucleus filtering (top-p) before sampling (<=0.0: no filtering)\n\n_[Back to Table of Contents](#table-of-contents)_\n\n---\n\n## Multi-Modal Classification\n\nMulti-Modal Classification fuses text and image data. This is performed using multi-modal bitransformer models\nintroduced in the paper [Supervised Multimodal Bitransformers for Classifying Images and Text](https://arxiv.org/abs/1909.02950).\n\nSupported model types:\n\n- BERT\n\n### Data format\n\nThere are several possible input formats you may use. The input formats are inspired by the [MM-IMDb](http://lisi1.unal.edu.co/mmimdb/) format.\nNote that several options for data preprocessing have been added for convenience and flexibility when dealing with\ncomplex datasets which can be found after the input format definitions.\n\n#### 1 - Directory based\n\nEach subset of data (E.g: train and test) should be in its own directory. The path to the directory can then be given\ndirectly to either `train_model()` or `eval_model()`.\n\nEach data sample should have a text portion and an image associated with it (and a label/labels for training and evaluation data).\nThe text for each sample should be in a separate JSON file. The JSON file may contain other fields in addition to the text\nitself but they will be ignored. The image associated with each sample should be in the same directory and both the text\nand the image must have the same identifier except for the file extension (E.g: 000001.json and 000001.jpg).\n\n#### 2 - Directory and file list\n\nAll data (including both train and test data) should be in the same directory. The path to this directory should be given\nto both `train_model()` and `eval_model()`. A second argument, `files_list` specifies which files should be taken from\nthe directory. `files_list` can be a Python list or the path to a JSON file containing the list of files.\n\nEach data sample should have a text portion and an image associated with it (and a label/labels for training and evaluation data).\nThe text for each sample should be in a separate JSON file. The JSON file may contain other fields in addition to the text\nitself but they will be ignored. The image associated with each sample should be in the same directory and both the text\nand the image must have the same identifier except for the file extension (E.g: 000001.json and 000001.jpg).\n\n#### 3 - Pandas DataFrame\n\nData can also be given in a Pandas DataFrame. When using this format, the `image_path` argument must be specified and\nit should be a String of the path to the directory containing the images. The DataFrame should contain at least 3\ncolumns as detailed below.\n\n- `text` (str) - The text associated with the sample.\n- `images` (str) - The relative path to the image file from `image_path` directory.\n- `labels` (str) - The label (or list of labels for multilabel tasks) associated with the sample.\n\n### Using custom names for column names or fields in JSON files\n\nBy default, Simple Transformers will look for column/field names `text`, `images`, and `labels`. However, you can define\nyour own names to use in place of these names. This behaviour is controlled using the three attributes `text_label`, `labels_label`,\n and `images_label` in the `args` dictionary.\n\nYou can set your custom names when creating the model by assigning the custom name to the corresponding attribute in the\n`args` dictionary.\n\nYou can also change these values at training and/or evaluation time (but not with the `predict()` method) by passing the\nnames to the arguments `text_label`, `labels_label`, and `images_label`. Note that the change will persist even after\nthe method call terminates. That is, the `args` dictionary of the model itself will be modified.\n\n### Specifying the file type extension for image and text files\n\nBy default, Simple Transformers will assume that any paths will also include the file type extension (E.g: .json or .jpg).\nAlternatively, you can specify the extensions using the `image_type_extension` and `data_type_extension` attributes (for\nimage file extensions and text file extensions respectively) in the `args` dictionary.\n\nThis too can be done when creating the model or when running the `train_model()` or `eval_model()` methods. The changes\nwill persist in the `args` dictionary when using these methods.\n\nThe `image_type_extension` can be specified when using the `predict()` method but the change WILL NOT persist.\n\n### Label formats\n\nWith Multi-Modal Classification, labels are always given as strings. You may specify a list of labels by passing in the\nlist to `label_list` argument when creating the model. If `label_list` is given, `num_labels` is not required.\n\nIf `label_list` is not given, `num_labels` is required and the labels should be Strings starting from `\"0\"` up to\n`\"<num_labels>\"`.\n\n### Creating a Model\n\nCreate a `MultiModalClassificationModel`.\n\n```python\nfrom simpletransformers.classification.multi_modal_classification_model import MultiModalClassificationModel\n\n\nmodel = MultiModalClassificationModel(\"bert\", \"bert-base-uncased\")\n```\n\nAvailable arguments:\n\n```python\n\"\"\"\nArgs:\n    model_type: The type of model (bert, xlnet, xlm, roberta, distilbert, albert)\n    model_name: Default Transformer model name or path to a directory containing Transformer model file (pytorch_model.bin).\n    multi_label (optional): Set to True for multi label tasks.\n    label_list (optional) : A list of all the labels (str) in the dataset.\n    num_labels (optional): The number of labels or classes in the dataset.\n    pos_weight (optional): A list of length num_labels containing the weights to assign to each label for loss calculation.\n    args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n    use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.\n    cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.\n    **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n\"\"\"\n```\n\n### Training a Model\n\nUse the `train_model()` method to train. You can use the `auto_weights` feature to balance out unbalanced datasets.\n\nAvailable arguments:\n\n```python\n\"\"\"\nArgs:\n    data: Path to data directory containing text files (JSON) and image files OR a Pandas DataFrame.\n        If a DataFrame is given, it should contain the columns [text, labels, images]. When using a DataFrame,\n        image_path MUST be specified. The image column of the DataFrame should contain the relative path from\n        image_path to the image.\n        E.g:\n            For an image file 1.jpeg located in \"data/train/\";\n                image_path = \"data/train/\"\n                images = \"1.jpeg\"\n    files_list (optional): If given, only the files specified in this list will be taken from data directory.\n        files_list can be a Python list or the path (str) to a JSON file containing a list of files.\n    image_path (optional): Must be specified when using DataFrame as input. Path to the directory containing the\n        images.\n    text_label (optional): Column name to look for instead of the default \"text\"\n    labels_label (optional): Column name to look for instead of the default \"labels\"\n    images_label (optional): Column name to look for instead of the default \"images\"\n    image_type_extension (optional): If given, this will be added to the end of each value in \"images\".\n    data_type_extension (optional): If given, this will be added to the end of each value in \"files_list\".\n    auto_weights (optional): If True, weights will be used to balance the classes.\n    output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.\n    show_running_loss (optional): Set to False to prevent running loss from being printed to console. Defaults to True.\n    args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n    eval_data (optional): A DataFrame against which evaluation will be performed when evaluate_during_training is enabled. Is required if evaluate_during_training is enabled.\n    **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). E.g. f1=sklearn.metrics.f1_score.\n                A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.\n\"\"\"\n```\n\n### Evaluating a Model\n\nUse the `eval_model()` method to evaluate. You can load a saved model by giving the path to the model directory as\n`model_name`. Note that you need to provide the same arguments when loading a saved model as you did when creating the\noriginal model.\n\n```python\nmodel = MultiModalClassificationModel(\"bert\", \"outputs\")\nresults, _ = model.eval_model(\"data/dataset/\", \"data/dev.json\")\n```\n\nAvailable arguments:\n\n```python\n\"\"\"\nArgs:\n    data: Path to data directory containing text files (JSON) and image files OR a Pandas DataFrame.\n        If a DataFrame is given, it should contain the columns [text, labels, images]. When using a DataFrame,\n        image_path MUST be specified. The image column of the DataFrame should contain the relative path from\n        image_path to the image.\n        E.g:\n            For an image file 1.jpeg located in \"data/train/\";\n                image_path = \"data/train/\"\n                images = \"1.jpeg\"\n    files_list (optional): If given, only the files specified in this list will be taken from data directory.\n        files_list can be a Python list or the path (str) to a JSON file containing a list of files.\n    image_path (optional): Must be specified when using DataFrame as input. Path to the directory containing the\n        images.\n    text_label (optional): Column name to look for instead of the default \"text\"\n    labels_label (optional): Column name to look for instead of the default \"labels\"\n    images_label (optional): Column name to look for instead of the default \"images\"\n    image_type_extension (optional): If given, this will be added to the end of each value in \"images\".\n    data_type_extension (optional): If given, this will be added to the end of each value in \"files_list\".\n    output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.\n    verbose: If verbose, results will be printed to the console on completion of evaluation.\n    silent: If silent, tqdm progress bars will be hidden.\n    **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). E.g. f1=sklearn.metrics.f1_score.\n                A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.\n\n\"\"\"\n```\n\n### Predicting from a trained Model\n\nUse the `predict()` method to make predictions. You can load a saved model by giving the path to the model directory as\n`model_name`. Note that you need to provide the same arguments when loading a saved model as you did when creating the\noriginal model.\n\n```python\nmodel = MultiModalClassificationModel(\"bert\", \"outputs\")\nmodel.predict(\n    {\n        \"text\": [\n            \"A lawyer is forced to defend a guilty judge, while defending other innocent clients, and trying to find punishment for the guilty and provide justice for the innocent.\"\n        ],\n        \"labels\": [\"Crime\"],\n        \"images\": [\"0078718\"]\n    },\n    image_path=\"data/dataset\",\n    image_type_extension=\".jpeg\"\n)\n```\n\n_[Back to Table of Contents](#table-of-contents)_\n\n---\n\n## Regression\n\nRegression tasks also use the ClassificationModel with 2 caveats.\n\n1. `num_labels` should be 1.\n2. `regression` should be `True` in `args` dict.\n\nRegression can be used with either single sentence or sentence pair tasks.\n\n#### Minimal Start for Regression\n\n```python\nfrom simpletransformers.classification import ClassificationModel\nimport pandas as pd\n\n\ntrain_data = [\n    ['Example sentence belonging to class 1', 'Yep, this is 1', 1.8],\n    ['Example sentence belonging to class 0', 'Yep, this is 0', 0.2],\n    ['Example  2 sentence belonging to class 0', 'Yep, this is 0', 4.5]\n]\n\ntrain_df = pd.DataFrame(train_data, columns=['text_a', 'text_b', 'labels'])\n\neval_data = [\n    ['Example sentence belonging to class 1', 'Yep, this is 1', 1.9],\n    ['Example sentence belonging to class 0', 'Yep, this is 0', 0.1],\n    ['Example  2 sentence belonging to class 0', 'Yep, this is 0', 5]\n]\n\neval_df = pd.DataFrame(eval_data, columns=['text_a', 'text_b', 'labels'])\n\ntrain_args={\n    'reprocess_input_data': True,\n    'overwrite_output_dir': True,\n    'num_train_epochs': 3,\n\n    'regression': True,\n}\n\n# Create a ClassificationModel\nmodel = ClassificationModel('roberta', 'roberta-base', num_labels=1, use_cuda=True, cuda_device=0, args=train_args)\nprint(train_df.head())\n\n# Train the model\nmodel.train_model(train_df, eval_df=eval_df)\n\n# Evaluate the model\nresult, model_outputs, wrong_predictions = model.eval_model(eval_df)\n\npredictions, raw_outputs = model.predict([[\"I'd like to puts some CD-ROMS on my iPad, is that possible?'\", \"Yes, but wouldn't that block the screen?\"]])\nprint(predictions)\nprint(raw_outputs)\n```\n\n---\n\n## Visualization Support\n\nThe [Weights & Biases](https://www.wandb.com/) framework is supported for visualizing model training.\n\nTo use this, simply set a project name for W&B in the `wandb_project` attribute of the `args` dictionary. This will log all hyperparameter values, training losses, and evaluation metrics to the given project.\n\n```python\nmodel = ClassificationModel('roberta', 'roberta-base', args={'wandb_project': 'project-name'})\n```\n\nFor a complete example, see [here](https://medium.com/skilai/to-see-is-to-believe-visualizing-the-training-of-machine-learning-models-664ef3fe4f49).\n\n_[Back to Table of Contents](#table-of-contents)_\n\n---\n\n## Experimental Features\n\nTo use experimental features, import from `simpletransformers.experimental.X`\n\n```python\nfrom simpletransformers.experimental.classification import ClassificationModel\n```\n\n### Sliding Window For Long Sequences\n\nNormally, sequences longer than `max_seq_length` are unceremoniously truncated.\n\nThis experimental feature moves a sliding window over each sequence and generates sub-sequences with length `max_seq_length`. The model output for each sub-sequence is averaged into a single output before being sent to the linear classifier.\n\nCurrently available on binary and multiclass classification models of the following types:\n\n* BERT\n* DistilBERT\n* RoBERTa\n* AlBERT\n* XLNet\n* CamemBERT\n\nSet `sliding_window` to `True` for the ClassificationModel to enable this feature.\n\n```python\nfrom simpletransformers.classification import ClassificationModel\nimport pandas as pd\nimport sklearn\n\n# Train and Evaluation data needs to be in a Pandas Dataframe of two columns. The first column is the text with type str, and the second column in the label with type int.\ntrain_data = [['Example sentence belonging to class 1' * 50, 1], ['Example sentence belonging to class 0', 0], ['Example  2 sentence belonging to class 0', 0]] + [['Example sentence belonging to class 0', 0] for i in range(12)]\ntrain_df = pd.DataFrame(train_data, columns=['text', 'labels'])\n\n\neval_data = [['Example eval sentence belonging to class 1', 1], ['Example eval sentence belonging to class 0', 0]]\neval_df = pd.DataFrame(eval_data)\n\ntrain_args={\n    'sliding_window': True,\n    'reprocess_input_data': True,\n    'overwrite_output_dir': True,\n    'evaluate_during_training': True,\n    'logging_steps': 5,\n    'stride': 0.8,\n    'max_seq_length': 128\n}\n\n# Create a TransformerModel\nmodel = ClassificationModel('camembert', 'camembert-base', args=train_args, use_cuda=False)\nprint(train_df.head())\n\n# Train the model\nmodel.train_model(train_df, eval_df=eval_df)\n\n# Evaluate the model\nresult, model_outputs, wrong_predictions = model.eval_model(eval_df, acc=sklearn.metrics.accuracy_score)\n\npredictions, raw_outputs = model.predict([\"I'd like to puts some CD-ROMS on my iPad, is that possible?' \u2014 Yes, but wouldn't that block the screen?\" * 25])\nprint(predictions)\nprint(raw_outputs)\n```\n\n_[Back to Table of Contents](#table-of-contents)_\n\n---\n## Loading Saved Models\n\nTo load a saved model, provide the path to the directory containing the saved model as the `model_name`.\n_Note that you will need to specify the correct (usually the same used in training) `args` when loading the model_\n\n```python\nmodel = ClassificationModel('roberta', 'outputs/', args={})\n```\n\n```python\nmodel = NERModel('bert', 'outputs/', args={})\n```\n\n_[Back to Table of Contents](#table-of-contents)_\n\n---\n\n\n## Default Settings\n\nThe default args used are given below. Any of these can be overridden by passing a dict containing the corresponding \nkey: value pairs to the the init method of a Model class.\n\n```python\nself.args = {\n    \"output_dir\": \"outputs/\",\n    \"cache_dir\": \"cache/\",\n    \"best_model_dir\": \"outputs/best_model/\",\n\n    \"fp16\": True,\n    \"fp16_opt_level\": \"O1\",\n    \"max_seq_length\": 128,\n    \"train_batch_size\": 8,\n    \"eval_batch_size\": 8,\n    \"gradient_accumulation_steps\": 1,\n    \"num_train_epochs\": 1,\n    \"weight_decay\": 0,\n    \"learning_rate\": 4e-5,\n    \"adam_epsilon\": 1e-8,\n    \"warmup_ratio\": 0.06,\n    \"warmup_steps\": 0,\n    \"max_grad_norm\": 1.0,\n    \"do_lower_case\": False,\n\n    \"logging_steps\": 50,\n    \"evaluate_during_training\": False,\n    \"evaluate_during_training_steps\": 2000,\n    \"evaluate_during_training_verbose\": False,\n    \"use_cached_eval_features\": False,\n    \"save_eval_checkpoints\": True\n    \"save_steps\": 2000,\n    \"no_cache\": False,\n    \"save_model_every_epoch\": True,\n    \"tensorboard_dir\": None,\n\n    \"overwrite_output_dir\": False,\n    \"reprocess_input_data\": True,\n\n    \"process_count\": cpu_count() - 2 if cpu_count() > 2 else 1\n    \"n_gpu\": 1,\n    \"silent\": False,\n    \"use_multiprocessing\": True,\n\n    \"wandb_project\": None,\n    \"wandb_kwargs\": {},\n\n    \"use_early_stopping\": True,\n    \"early_stopping_patience\": 3,\n    \"early_stopping_delta\": 0,\n    \"early_stopping_metric\": \"eval_loss\",\n    \"early_stopping_metric_minimize\": True,\n\n    \"manual_seed\": None,\n    \"encoding\": None,\n    \"config\": {},\n}\n```\n\n### Args Explained\n\n#### *output_dir: str*\nThe directory where all outputs will be stored. This includes model checkpoints and evaluation results.\n\n#### *cache_dir: str*\nThe directory where cached files will be saved.\n\n#### *best_model_dir: str*\nThe directory where the best model (model checkpoints) will be saved if evaluate_during_training is enabled and the training loop achieves a lowest evaluation loss calculated after every evaluate_during_training_steps, or an epoch.\n\n#### *fp16: bool*\nWhether or not fp16 mode should be used. Requires NVidia Apex library.\n\n#### *fp16_opt_level: str*\nCan be '01', '02', '03'. See the [Apex docs](https://nvidia.github.io/apex/amp.html) for an explanation of the different optimization levels (opt_levels).\n\n#### *max_seq_length: int*\nMaximum sequence level the model will support.\n\n#### *train_batch_size: int*\nThe training batch size.\n\n#### *gradient_accumulation_steps: int*\nThe number of training steps to execute before performing a `optimizer.step()`. Effectively increases the training batch size while sacrificing training time to lower memory consumption.\n\n#### *eval_batch_size: int*\nThe evaluation batch size.\n\n#### *num_train_epochs: int*\nThe number of epochs the model will be trained for.\n\n#### *weight_decay: float*\nAdds L2 penalty.\n\n#### *learning_rate: float*\nThe learning rate for training.\n\n#### *adam_epsilon: float*\nEpsilon hyperparameter used in AdamOptimizer.\n\n#### *max_grad_norm: float*\nMaximum gradient clipping.\n\n#### *do_lower_case: bool*\nSet to True when using uncased models.\n\n#### *evaluate_during_training*\nSet to True to perform evaluation while training models. Make sure `eval_df` is passed to the training method if enabled.\n\n#### *evaluate_during_training_steps*\nPerform evaluation at every specified number of steps. A checkpoint model and the evaluation results will be saved.\n\n#### *evaluate_during_training_verbose*\nPrint results from evaluation during training.\n\n#### *use_cached_eval_features*\nEvaluation during training uses cached features. Setting this to `False` will cause features to be recomputed at every evaluation step.\n\n#### *save_eval_checkpoints*\nSave a model checkpoint for every evaluation performed.\n\n#### *logging_steps: int*\nLog training loss and learning at every specified number of steps.\n\n#### *save_steps: int*\nSave a model checkpoint at every specified number of steps.\n\n#### *no_cache: bool*\nCache features to disk.\n\n#### *save_model_every_epoch: bool*\nSave a model at the end of every epoch.\n\n#### *tensorboard_dir: str*\nThe directory where Tensorboard events will be stored during training. By default, Tensorboard events will be saved in a subfolder inside `runs/`  like `runs/Dec02_09-32-58_36d9e58955b0/`.\n\n#### *overwrite_output_dir: bool*\nIf True, the trained model will be saved to the ouput_dir and will overwrite existing saved models in the same directory.\n\n#### *reprocess_input_data: bool*\nIf True, the input data will be reprocessed even if a cached file of the input data exists in the cache_dir.\n\n#### *process_count: int*\nNumber of cpu cores (processes) to use when converting examples to features. Default is (number of cores - 2) or 1 if (number of cores <= 2)\n\n#### *n_gpu: int*\nNumber of GPUs to use.\n\n#### *silent: bool*\nDisables progress bars.\n\n#### *use_multiprocessing: bool*\nIf True, multiprocessing will be used when converting data into features. Disabling can reduce memory usage, but may substantially slow down processing.\n\n\n#### *wandb_project: str*\nName of W&B project. This will log all hyperparameter values, training losses, and evaluation metrics to the given project.\n\n#### *wandb_kwargs: dict*\nDictionary of keyword arguments to be passed to the W&B project.\n\n#### *use_early_stopping*\nUse early stopping to stop training when `early_stopping_metric` doesn't improve (based on `early_stopping_patience`, and `early_stopping_delta`)\n\n#### *early_stopping_patience*\nTerminate training after this many evaluations without an improvement in `eval_loss` greater then `early_stopping_delta`.\n\n#### *early_stopping_delta*\nThe improvement over `best_eval_loss` necessary to count as a better checkpoint.\n\n#### *early_stopping_metric*\nThe metric that should be used with early stopping. (Should be computed during `eval_during_training`).\n\n#### *early_stopping_metric_minimize*\nWhether `early_stopping_metric` should be minimized (or maximized).\n\n#### *manual_seed*\nSet a manual seed if necessary for reproducible results.\n\n#### *encoding*\nSpecify an encoding to be used when reading text files.\n\n#### *config*\nA dictionary containing configuration options that should be overriden in a model's config.\n\n---\n\n## Current Pretrained Models\n\nFor a list of pretrained models, see [Hugging Face docs](https://huggingface.co/pytorch-transformers/pretrained_models.html).\n\nThe `model_types` available for each task can be found under their respective section. Any pretrained model of that type\nfound in the Hugging Face docs should work. To use any of them set the correct `model_type` and `model_name` in the `args` \ndictionary.\n\n_[Back to Table of Contents](#table-of-contents)_\n\n---\n\n## Acknowledgements\n\nNone of this would have been possible without the hard work by the HuggingFace team in developing the [Pytorch-Transformers](https://github.com/huggingface/pytorch-transformers) library.\n\n_<div>Icon for the Social Media Preview made by <a href=\"https://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a></div>_\n\n## Contributors \u2728\n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/hawktang\"><img src=\"https://avatars0.githubusercontent.com/u/2004071?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>hawktang</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=hawktang\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://datawizzards.io\"><img src=\"https://avatars0.githubusercontent.com/u/22409996?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Mabu Manaileng</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=mabu-dev\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://www.facebook.com/aliosm97\"><img src=\"https://avatars3.githubusercontent.com/u/7662492?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Ali Hamdi Ali Fadel</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=AliOsm\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://tovly.co\"><img src=\"https://avatars0.githubusercontent.com/u/12242351?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Tovly Deutsch</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=TovlyDeutsch\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/hlo-world\"><img src=\"https://avatars0.githubusercontent.com/u/9633055?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>hlo-world</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=hlo-world\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/huntertl\"><img src=\"https://avatars1.githubusercontent.com/u/15113885?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>huntertl</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=huntertl\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://whattheshot.com\"><img src=\"https://avatars2.githubusercontent.com/u/623763?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Yann Defretin</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=kinoute\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=kinoute\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"#question-kinoute\" title=\"Answering Questions\">\ud83d\udcac</a> <a href=\"#ideas-kinoute\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/mananeau\"><img src=\"https://avatars0.githubusercontent.com/u/29440170?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Manuel </b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=mananeau\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=mananeau\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://jacobsgill.es\"><img src=\"https://avatars2.githubusercontent.com/u/9109832?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Gilles Jacobs</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=GillesJ\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    <td align=\"center\"><a href=\"https://github.com/shasha79\"><img src=\"https://avatars2.githubusercontent.com/u/5512649?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>shasha79</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=shasha79\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://www-lium.univ-lemans.fr/~garcia\"><img src=\"https://avatars2.githubusercontent.com/u/14233427?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Mercedes Garcia</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=merc85garcia\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/hammad26\"><img src=\"https://avatars1.githubusercontent.com/u/12643784?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Hammad Hassan Tarar</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=hammad26\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=hammad26\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    <td align=\"center\"><a href=\"https://github.com/todd-cook\"><img src=\"https://avatars3.githubusercontent.com/u/665389?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Todd Cook</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=todd-cook\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://knuthellan.com/\"><img src=\"https://avatars2.githubusercontent.com/u/51441?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Knut O. Hellan</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=khellan\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=khellan\" title=\"Documentation\">\ud83d\udcd6</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/nagenshukla\"><img src=\"https://avatars0.githubusercontent.com/u/39196228?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>nagenshukla</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=nagenshukla\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://www.linkedin.com/in/flaviussn/\"><img src=\"https://avatars0.githubusercontent.com/u/20523032?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>flaviussn</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=flaviussn\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=flaviussn\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    <td align=\"center\"><a href=\"http://marctorrellas.github.com\"><img src=\"https://avatars1.githubusercontent.com/u/22045779?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Marc Torrellas</b></sub></a><br /><a href=\"#maintenance-marctorrellas\" title=\"Maintenance\">\ud83d\udea7</a></td>\n    <td align=\"center\"><a href=\"https://github.com/adrienrenaud\"><img src=\"https://avatars3.githubusercontent.com/u/6208157?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Adrien Renaud</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=adrienrenaud\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/jacky18008\"><img src=\"https://avatars0.githubusercontent.com/u/9031441?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>jacky18008</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=jacky18008\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/seo-95\"><img src=\"https://avatars0.githubusercontent.com/u/38254541?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Matteo Senese</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=seo-95\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/sarthakTUM\"><img src=\"https://avatars2.githubusercontent.com/u/23062869?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>sarthakTUM</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=sarthakTUM\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=sarthakTUM\" title=\"Code\">\ud83d\udcbb</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/djstrong\"><img src=\"https://avatars1.githubusercontent.com/u/1849959?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>djstrong</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=djstrong\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://kozistr.tech\"><img src=\"https://avatars2.githubusercontent.com/u/15344796?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Hyeongchan Kim</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=kozistr\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    <td align=\"center\"><a href=\"https://github.com/Pradhy729\"><img src=\"https://avatars3.githubusercontent.com/u/49659913?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Pradhy729</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=Pradhy729\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://iknoorjobs.github.io/\"><img src=\"https://avatars2.githubusercontent.com/u/22852967?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Iknoor Singh</b></sub></a><br /><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=iknoorjobs\" title=\"Documentation\">\ud83d\udcd6</a></td>\n  </tr>\n</table>\n\n<!-- markdownlint-enable -->\n<!-- prettier-ignore-end -->\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!\n\n*If you should be on this list but you aren't, or you are on the list but don't want to be, please don't hesitate to contact me!*\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ThilinaRajapakse/simpletransformers/", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "simpletransformers", "package_url": "https://pypi.org/project/simpletransformers/", "platform": "", "project_url": "https://pypi.org/project/simpletransformers/", "project_urls": {"Homepage": "https://github.com/ThilinaRajapakse/simpletransformers/"}, "release_url": "https://pypi.org/project/simpletransformers/0.27.1/", "requires_dist": ["numpy", "requests", "tqdm", "regex", "transformers (>=2.8.0)", "scipy", "scikit-learn", "seqeval", "tensorboardx", "pandas", "tokenizers"], "requires_python": ">=3.6", "summary": "An easy-to-use wrapper library for the Transformers library.", "version": "0.27.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://opensource.org/licenses/Apache-2.0\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b97ca76cf5d8fd16c7bc4731270e0bbe53df7aa1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667\"></a> <a href=\"https://pepy.tech/project/simpletransformers\" rel=\"nofollow\"><img alt=\"Downloads\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b21a308369a9a8ea66f70263ae645670905a8dbe/68747470733a2f2f706570792e746563682f62616467652f73696d706c657472616e73666f726d657273\"></a></p>\n\n<p><a href=\"#contributors-\" rel=\"nofollow\"><img alt=\"All Contributors\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/531158b11a06df2d853ff5016c879c32323ad1dd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f616c6c5f636f6e7472696275746f72732d32352d6f72616e67652e7376673f7374796c653d666c61742d737175617265\"></a></p>\n\n<h1>Simple Transformers</h1>\n<p>This library is based on the <a href=\"https://github.com/huggingface/transformers\" rel=\"nofollow\">Transformers</a> library by HuggingFace. Simple Transformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize a model, train the model, and evaluate a model.</p>\n<p>Supports</p>\n<ul>\n<li>Sequence Classification</li>\n<li>Token Classification (NER)</li>\n<li>Question Answering</li>\n<li>Language Model Fine-Tuning</li>\n<li>Language Model Training</li>\n<li>Language Generation</li>\n<li>T5 Model</li>\n<li>Multi-Modal Classification</li>\n<li>Conversational AI.</li>\n</ul>\n<h3>Latest</h3>\n<h4>2020-05-05</h4>\n<ul>\n<li>T5 Model support added</li>\n</ul>\n<h4>2020-05-03</h4>\n<p><strong>New documentation is now live at <a href=\"https://thilinarajapakse.github.io/simpletransformers/\" rel=\"nofollow\">https://thilinarajapakse.github.io/simpletransformers/</a></strong></p>\n<p>Only text classification tasks are added so far but the others will follow in the next few days. Any feedback will be immensely helpful in improving the documentation! If you have any feedback, please leave a comment in the <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/issues/342\" rel=\"nofollow\">issue</a> I've opened for this.</p>\n<h4>2020-04-27</h4>\n<ul>\n<li>ELECTRA models can now be used with Language Model Training, Named Entity Recognition (Token Classification), Sequence Classification, and Question Answering.</li>\n</ul>\n<h1>Table of contents</h1>\n\n<ul>\n<li><a href=\"#simple-transformers\" rel=\"nofollow\">Simple Transformers</a>\n<ul>\n<li><a href=\"#latest\" rel=\"nofollow\">Latest</a>\n<ul>\n<li><a href=\"#2020-05-05\" rel=\"nofollow\">2020-05-05</a></li>\n<li><a href=\"#2020-05-03\" rel=\"nofollow\">2020-05-03</a></li>\n<li><a href=\"#2020-04-27\" rel=\"nofollow\">2020-04-27</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#table-of-contents\" rel=\"nofollow\">Table of contents</a>\n<ul>\n<li><a href=\"#setup\" rel=\"nofollow\">Setup</a>\n<ul>\n<li><a href=\"#with-conda\" rel=\"nofollow\">With Conda</a>\n<ul>\n<li><a href=\"#optional\" rel=\"nofollow\">Optional</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#usage\" rel=\"nofollow\">Usage</a>\n<ul>\n<li><a href=\"#structure\" rel=\"nofollow\">Structure</a></li>\n</ul>\n</li>\n<li><a href=\"#text-classification\" rel=\"nofollow\">Text Classification</a>\n<ul>\n<li><a href=\"#task-specific-notes\" rel=\"nofollow\">Task Specific Notes</a>\n<ul>\n<li><a href=\"#minimal-start-for-binary-classification\" rel=\"nofollow\">Minimal Start for Binary Classification</a></li>\n<li><a href=\"#minimal-start-for-multiclass-classification\" rel=\"nofollow\">Minimal Start for Multiclass Classification</a></li>\n<li><a href=\"#minimal-start-for-multilabel-classification\" rel=\"nofollow\">Minimal Start for Multilabel Classification</a>\n<ul>\n<li><a href=\"#special-attributes\" rel=\"nofollow\">Special Attributes</a></li>\n</ul>\n</li>\n<li><a href=\"#minimal-start-for-sentence-pair-classification\" rel=\"nofollow\">Minimal Start for Sentence Pair Classification</a></li>\n<li><a href=\"#real-dataset-examples\" rel=\"nofollow\">Real Dataset Examples</a></li>\n<li><a href=\"#classificationmodel\" rel=\"nofollow\">ClassificationModel</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#named-entity-recognition\" rel=\"nofollow\">Named Entity Recognition</a>\n<ul>\n<li><a href=\"#minimal-start\" rel=\"nofollow\">Minimal Start</a></li>\n<li><a href=\"#real-dataset-examples-1\" rel=\"nofollow\">Real Dataset Examples</a></li>\n<li><a href=\"#nermodel\" rel=\"nofollow\">NERModel</a></li>\n</ul>\n</li>\n<li><a href=\"#question-answering\" rel=\"nofollow\">Question Answering</a>\n<ul>\n<li><a href=\"#data-format\" rel=\"nofollow\">Data format</a></li>\n<li><a href=\"#minimal-example\" rel=\"nofollow\">Minimal Example</a></li>\n<li><a href=\"#real-dataset-examples-2\" rel=\"nofollow\">Real Dataset Examples</a></li>\n<li><a href=\"#questionansweringmodel\" rel=\"nofollow\">QuestionAnsweringModel</a></li>\n<li><a href=\"#additional-attributes-for-question-answering-tasks\" rel=\"nofollow\">Additional attributes for Question Answering tasks</a>\n<ul>\n<li><a href=\"#docstride-int\" rel=\"nofollow\"><em>doc_stride: int</em></a></li>\n<li><a href=\"#maxquerylength-int\" rel=\"nofollow\"><em>max_query_length: int</em></a></li>\n<li><a href=\"#nbestsize-int\" rel=\"nofollow\"><em>n_best_size: int</em></a></li>\n<li><a href=\"#maxanswerlength-int\" rel=\"nofollow\"><em>max_answer_length: int</em></a></li>\n<li><a href=\"#nullscorediffthreshold-float\" rel=\"nofollow\"><em>null_score_diff_threshold: float</em></a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#language-model-training\" rel=\"nofollow\">Language Model Training</a>\n<ul>\n<li><a href=\"#data-format-1\" rel=\"nofollow\">Data format</a></li>\n<li><a href=\"#minimal-example-for-language-model-fine-tuning\" rel=\"nofollow\">Minimal Example For Language Model Fine Tuning</a>\n<ul>\n<li><a href=\"#example-medium-article\" rel=\"nofollow\">Example (Medium Article)</a></li>\n</ul>\n</li>\n<li><a href=\"#minimal-example-for-language-model-training-from-scratch\" rel=\"nofollow\">Minimal Example For Language Model Training From Scratch</a></li>\n<li><a href=\"#minimal-example-for-language-model-training-with-electra\" rel=\"nofollow\">Minimal Example For Language Model Training With ELECTRA</a></li>\n<li><a href=\"#real-dataset-example-for-training-a-language-model\" rel=\"nofollow\">Real Dataset Example For Training a Language Model</a></li>\n<li><a href=\"#languagemodelingmodel\" rel=\"nofollow\">LanguageModelingModel</a></li>\n<li><a href=\"#additional-attributes-for-language-modeling-tasks\" rel=\"nofollow\">Additional attributes for Language Modeling tasks</a>\n<ul>\n<li><a href=\"#datasettype-str\" rel=\"nofollow\"><em>dataset_type: str</em></a></li>\n<li><a href=\"#datasetclass-subclass-of-pytorch-dataset\" rel=\"nofollow\"><em>dataset_class: Subclass of Pytorch Dataset</em></a></li>\n<li><a href=\"#blocksize-int\" rel=\"nofollow\"><em>block_size: int</em></a></li>\n<li><a href=\"#mlm-bool\" rel=\"nofollow\"><em>mlm: bool</em></a></li>\n<li><a href=\"#mlmprobability-float\" rel=\"nofollow\"><em>mlm_probability: float</em></a></li>\n<li><a href=\"#maxsteps-int\" rel=\"nofollow\"><em>max_steps: int</em></a></li>\n<li><a href=\"#configname-str\" rel=\"nofollow\"><em>config_name: str</em></a></li>\n<li><a href=\"#tokenizername-str\" rel=\"nofollow\"><em>tokenizer_name: str</em></a></li>\n<li><a href=\"#minfrequencey-int\" rel=\"nofollow\"><em>min_frequencey: int</em></a></li>\n<li><a href=\"#specialtokens-list\" rel=\"nofollow\"><em>special_tokens: list</em></a></li>\n<li><a href=\"#slidingwindow-bool\" rel=\"nofollow\"><em>sliding_window: bool</em></a></li>\n<li><a href=\"#stride-float\" rel=\"nofollow\"><em>stride: float</em></a></li>\n</ul>\n</li>\n<li><a href=\"#config-dict\" rel=\"nofollow\"><em>config: dict</em></a></li>\n<li><a href=\"#generatorconfig-dict\" rel=\"nofollow\"><em>generator_config: dict</em></a></li>\n<li><a href=\"#discriminatorconfig-dict\" rel=\"nofollow\"><em>discriminator_config: dict</em></a></li>\n</ul>\n</li>\n<li><a href=\"#language-generation\" rel=\"nofollow\">Language Generation</a>\n<ul>\n<li><a href=\"#minimal-start-1\" rel=\"nofollow\">Minimal Start</a></li>\n<li><a href=\"#languagegenerationmodel\" rel=\"nofollow\">LanguageGenerationModel</a></li>\n<li><a href=\"#additional-attributes-for-language-generation-tasks\" rel=\"nofollow\">Additional attributes for Language Generation tasks</a>\n<ul>\n<li><a href=\"#dosample-bool\" rel=\"nofollow\"><em>do_sample: bool</em></a></li>\n<li><a href=\"#prompt-str\" rel=\"nofollow\"><em>prompt: str</em></a></li>\n<li><a href=\"#length-int\" rel=\"nofollow\"><em>length: int</em></a></li>\n<li><a href=\"#stoptoken-str\" rel=\"nofollow\"><em>stop_token: str</em></a></li>\n<li><a href=\"#temperature-float\" rel=\"nofollow\"><em>temperature: float</em></a></li>\n<li><a href=\"#repetitionpenalty-float\" rel=\"nofollow\"><em>repetition_penalty: float</em></a></li>\n<li><a href=\"#k-int\" rel=\"nofollow\"><em>k: int</em></a></li>\n<li><a href=\"#p-float\" rel=\"nofollow\"><em>p: float</em></a></li>\n<li><a href=\"#paddingtext-str\" rel=\"nofollow\"><em>padding_text: str</em></a></li>\n<li><a href=\"#xlmlanguage-str\" rel=\"nofollow\"><em>xlm_language: str</em></a></li>\n<li><a href=\"#numreturnsequences-int\" rel=\"nofollow\"><em>num_return_sequences: int</em></a></li>\n<li><a href=\"#config-dict-1\" rel=\"nofollow\"><em>config: dict</em></a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#t5-transformer\" rel=\"nofollow\">T5 Transformer</a>\n<ul>\n<li><a href=\"#data-format-2\" rel=\"nofollow\">Data Format</a>\n<ul>\n<li><a href=\"#train-and-evaluation-input-formats\" rel=\"nofollow\">Train and evaluation input formats</a></li>\n<li><a href=\"#prediction-data-format\" rel=\"nofollow\">Prediction data format</a></li>\n<li><a href=\"#minimal-start-2\" rel=\"nofollow\">Minimal Start</a></li>\n<li><a href=\"#evaluating-with-custom-metrics\" rel=\"nofollow\">Evaluating with custom metrics</a></li>\n<li><a href=\"#t5model\" rel=\"nofollow\">T5Model</a></li>\n</ul>\n</li>\n<li><a href=\"#additional-attributes-for-t5-model\" rel=\"nofollow\">Additional attributes for T5 Model</a>\n<ul>\n<li><a href=\"#datasetclass-subclass-of-pytorch-dataset-1\" rel=\"nofollow\"><em>dataset_class: Subclass of Pytorch Dataset</em></a></li>\n<li><a href=\"#dosample-bool-1\" rel=\"nofollow\"><em>do_sample: bool</em></a></li>\n<li><a href=\"#maxsteps-int-1\" rel=\"nofollow\"><em>max_steps: int</em></a></li>\n<li><a href=\"#evaluategeneratedtext-bool\" rel=\"nofollow\"><em>evaluate_generated_text: bool</em></a></li>\n<li><a href=\"#numbeams-int\" rel=\"nofollow\"><em>num_beams: int</em></a></li>\n<li><a href=\"#maxlemgth-int\" rel=\"nofollow\"><em>max_lemgth: int</em></a></li>\n<li><a href=\"#repetitionpenalty-float-1\" rel=\"nofollow\"><em>repetition_penalty: float</em></a></li>\n<li><a href=\"#lengthpenalty-float\" rel=\"nofollow\"><em>length_penalty: float</em></a></li>\n<li><a href=\"#earlystopping-bool\" rel=\"nofollow\"><em>early_stopping: bool</em></a></li>\n<li><a href=\"#preprocessinputs-bool\" rel=\"nofollow\"><em>preprocess_inputs: bool</em></a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#conversational-ai\" rel=\"nofollow\">Conversational AI</a>\n<ul>\n<li><a href=\"#data-format-3\" rel=\"nofollow\">Data format</a></li>\n<li><a href=\"#minimal-example-1\" rel=\"nofollow\">Minimal Example</a></li>\n<li><a href=\"#real-dataset-example\" rel=\"nofollow\">Real Dataset Example</a></li>\n<li><a href=\"#convaimodel\" rel=\"nofollow\">ConvAIModel</a></li>\n<li><a href=\"#additional-attributes-for-conversational-ai\" rel=\"nofollow\">Additional attributes for Conversational AI</a>\n<ul>\n<li><a href=\"#numcandidates-int\" rel=\"nofollow\"><em>num_candidates: int</em></a></li>\n<li><a href=\"#personalitypermutations-int\" rel=\"nofollow\"><em>personality_permutations: int</em></a></li>\n<li><a href=\"#maxhistory-int\" rel=\"nofollow\"><em>max_history: int</em></a></li>\n<li><a href=\"#lmcoef-int\" rel=\"nofollow\"><em>lm_coef: int</em></a></li>\n<li><a href=\"#mccoef-int\" rel=\"nofollow\"><em>mc_coef: int</em></a></li>\n<li><a href=\"#nosample-bool\" rel=\"nofollow\"><em>no_sample: bool</em></a></li>\n<li><a href=\"#maxlength-int\" rel=\"nofollow\"><em>max_length: int</em></a></li>\n<li><a href=\"#minlength-int\" rel=\"nofollow\"><em>min_length: int</em></a></li>\n<li><a href=\"#temperature-float-1\" rel=\"nofollow\"><em>temperature: float</em></a></li>\n<li><a href=\"#topk-float\" rel=\"nofollow\"><em>top_k: float</em></a></li>\n<li><a href=\"#topp-float\" rel=\"nofollow\"><em>top_p: float</em></a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#multi-modal-classification\" rel=\"nofollow\">Multi-Modal Classification</a>\n<ul>\n<li><a href=\"#data-format-4\" rel=\"nofollow\">Data format</a>\n<ul>\n<li><a href=\"#1---directory-based\" rel=\"nofollow\">1 - Directory based</a></li>\n<li><a href=\"#2---directory-and-file-list\" rel=\"nofollow\">2 - Directory and file list</a></li>\n<li><a href=\"#3---pandas-dataframe\" rel=\"nofollow\">3 - Pandas DataFrame</a></li>\n</ul>\n</li>\n<li><a href=\"#using-custom-names-for-column-names-or-fields-in-json-files\" rel=\"nofollow\">Using custom names for column names or fields in JSON files</a></li>\n<li><a href=\"#specifying-the-file-type-extension-for-image-and-text-files\" rel=\"nofollow\">Specifying the file type extension for image and text files</a></li>\n<li><a href=\"#label-formats\" rel=\"nofollow\">Label formats</a></li>\n<li><a href=\"#creating-a-model\" rel=\"nofollow\">Creating a Model</a></li>\n<li><a href=\"#training-a-model\" rel=\"nofollow\">Training a Model</a></li>\n<li><a href=\"#evaluating-a-model\" rel=\"nofollow\">Evaluating a Model</a></li>\n<li><a href=\"#predicting-from-a-trained-model\" rel=\"nofollow\">Predicting from a trained Model</a></li>\n</ul>\n</li>\n<li><a href=\"#regression\" rel=\"nofollow\">Regression</a>\n<ul>\n<li><a href=\"#minimal-start-for-regression\" rel=\"nofollow\">Minimal Start for Regression</a></li>\n</ul>\n</li>\n<li><a href=\"#visualization-support\" rel=\"nofollow\">Visualization Support</a></li>\n<li><a href=\"#experimental-features\" rel=\"nofollow\">Experimental Features</a>\n<ul>\n<li><a href=\"#sliding-window-for-long-sequences\" rel=\"nofollow\">Sliding Window For Long Sequences</a></li>\n</ul>\n</li>\n<li><a href=\"#loading-saved-models\" rel=\"nofollow\">Loading Saved Models</a></li>\n<li><a href=\"#default-settings\" rel=\"nofollow\">Default Settings</a>\n<ul>\n<li><a href=\"#args-explained\" rel=\"nofollow\">Args Explained</a>\n<ul>\n<li><a href=\"#outputdir-str\" rel=\"nofollow\"><em>output_dir: str</em></a></li>\n<li><a href=\"#cachedir-str\" rel=\"nofollow\"><em>cache_dir: str</em></a></li>\n<li><a href=\"#bestmodeldir-str\" rel=\"nofollow\"><em>best_model_dir: str</em></a></li>\n<li><a href=\"#fp16-bool\" rel=\"nofollow\"><em>fp16: bool</em></a></li>\n<li><a href=\"#fp16optlevel-str\" rel=\"nofollow\"><em>fp16_opt_level: str</em></a></li>\n<li><a href=\"#maxseqlength-int\" rel=\"nofollow\"><em>max_seq_length: int</em></a></li>\n<li><a href=\"#trainbatchsize-int\" rel=\"nofollow\"><em>train_batch_size: int</em></a></li>\n<li><a href=\"#gradientaccumulationsteps-int\" rel=\"nofollow\"><em>gradient_accumulation_steps: int</em></a></li>\n<li><a href=\"#evalbatchsize-int\" rel=\"nofollow\"><em>eval_batch_size: int</em></a></li>\n<li><a href=\"#numtrainepochs-int\" rel=\"nofollow\"><em>num_train_epochs: int</em></a></li>\n<li><a href=\"#weightdecay-float\" rel=\"nofollow\"><em>weight_decay: float</em></a></li>\n<li><a href=\"#learningrate-float\" rel=\"nofollow\"><em>learning_rate: float</em></a></li>\n<li><a href=\"#adamepsilon-float\" rel=\"nofollow\"><em>adam_epsilon: float</em></a></li>\n<li><a href=\"#maxgradnorm-float\" rel=\"nofollow\"><em>max_grad_norm: float</em></a></li>\n<li><a href=\"#dolowercase-bool\" rel=\"nofollow\"><em>do_lower_case: bool</em></a></li>\n<li><a href=\"#evaluateduringtraining\" rel=\"nofollow\"><em>evaluate_during_training</em></a></li>\n<li><a href=\"#evaluateduringtrainingsteps\" rel=\"nofollow\"><em>evaluate_during_training_steps</em></a></li>\n<li><a href=\"#evaluateduringtrainingverbose\" rel=\"nofollow\"><em>evaluate_during_training_verbose</em></a></li>\n<li><a href=\"#usecachedevalfeatures\" rel=\"nofollow\"><em>use_cached_eval_features</em></a></li>\n<li><a href=\"#saveevalcheckpoints\" rel=\"nofollow\"><em>save_eval_checkpoints</em></a></li>\n<li><a href=\"#loggingsteps-int\" rel=\"nofollow\"><em>logging_steps: int</em></a></li>\n<li><a href=\"#savesteps-int\" rel=\"nofollow\"><em>save_steps: int</em></a></li>\n<li><a href=\"#nocache-bool\" rel=\"nofollow\"><em>no_cache: bool</em></a></li>\n<li><a href=\"#savemodeleveryepoch-bool\" rel=\"nofollow\"><em>save_model_every_epoch: bool</em></a></li>\n<li><a href=\"#tensorboarddir-str\" rel=\"nofollow\"><em>tensorboard_dir: str</em></a></li>\n<li><a href=\"#overwriteoutputdir-bool\" rel=\"nofollow\"><em>overwrite_output_dir: bool</em></a></li>\n<li><a href=\"#reprocessinputdata-bool\" rel=\"nofollow\"><em>reprocess_input_data: bool</em></a></li>\n<li><a href=\"#processcount-int\" rel=\"nofollow\"><em>process_count: int</em></a></li>\n<li><a href=\"#ngpu-int\" rel=\"nofollow\"><em>n_gpu: int</em></a></li>\n<li><a href=\"#silent-bool\" rel=\"nofollow\"><em>silent: bool</em></a></li>\n<li><a href=\"#usemultiprocessing-bool\" rel=\"nofollow\"><em>use_multiprocessing: bool</em></a></li>\n<li><a href=\"#wandbproject-str\" rel=\"nofollow\"><em>wandb_project: str</em></a></li>\n<li><a href=\"#wandbkwargs-dict\" rel=\"nofollow\"><em>wandb_kwargs: dict</em></a></li>\n<li><a href=\"#useearlystopping\" rel=\"nofollow\"><em>use_early_stopping</em></a></li>\n<li><a href=\"#earlystoppingpatience\" rel=\"nofollow\"><em>early_stopping_patience</em></a></li>\n<li><a href=\"#earlystoppingdelta\" rel=\"nofollow\"><em>early_stopping_delta</em></a></li>\n<li><a href=\"#earlystoppingmetric\" rel=\"nofollow\"><em>early_stopping_metric</em></a></li>\n<li><a href=\"#earlystoppingmetricminimize\" rel=\"nofollow\"><em>early_stopping_metric_minimize</em></a></li>\n<li><a href=\"#manualseed\" rel=\"nofollow\"><em>manual_seed</em></a></li>\n<li><a href=\"#encoding\" rel=\"nofollow\"><em>encoding</em></a></li>\n<li><a href=\"#config\" rel=\"nofollow\"><em>config</em></a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#current-pretrained-models\" rel=\"nofollow\">Current Pretrained Models</a></li>\n<li><a href=\"#acknowledgements\" rel=\"nofollow\">Acknowledgements</a></li>\n<li><a href=\"#contributors-%e2%9c%a8\" rel=\"nofollow\">Contributors \u2728</a></li>\n</ul>\n</li>\n</ul>\n\n<h2>Setup</h2>\n<h3>With Conda</h3>\n<ol>\n<li>\n<p>Install Anaconda or Miniconda Package Manager from <a href=\"https://www.anaconda.com/distribution/\" rel=\"nofollow\">here</a></p>\n</li>\n<li>\n<p>Create a new virtual environment and install packages.<br>\n<code>conda create -n transformers python pandas tqdm</code><br>\n<code>conda activate transformers</code><br>\nIf using cuda:<br>\n\u00a0\u00a0\u00a0\u00a0<code>conda install pytorch cudatoolkit=10.1 -c pytorch</code><br>\nelse:<br>\n\u00a0\u00a0\u00a0\u00a0<code>conda install pytorch cpuonly -c pytorch</code></p>\n</li>\n<li>\n<p>Install Apex if you are using fp16 training. Please follow the instructions <a href=\"https://github.com/NVIDIA/apex\" rel=\"nofollow\">here</a>. (Installing Apex from pip has caused issues for several people.)</p>\n</li>\n<li>\n<p>Install simpletransformers.<br>\n<code>pip install simpletransformers</code></p>\n</li>\n</ol>\n<h4>Optional</h4>\n<ol>\n<li>Install Weights and Biases (wandb) for tracking and visualizing training in a web browser.<br>\n<code>pip install wandb</code></li>\n</ol>\n<h2>Usage</h2>\n<p>Most available hyperparameters are common for all tasks. Any special hyperparameters will be listed in the docs section for the corresponding class. See <a href=\"#default-settings\" rel=\"nofollow\">Default Settings</a> and <a href=\"#args-explained\" rel=\"nofollow\">Args Explained</a> sections for more information.</p>\n<p>Example scripts can be found in the <code>examples</code> directory.</p>\n<p>See the <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/blob/master/CHANGELOG.md\" rel=\"nofollow\">Changelog</a> for up-to-date changes to the project.</p>\n<h3>Structure</h3>\n<p><em>The file structure has been updated starting with version 0.6.0. This should only affect import statements. The old import paths should still be functional although it is recommended to use the updated paths given below and in the minimal start examples</em>.</p>\n<ul>\n<li><code>simpletransformers.classification</code> - Includes all Classification models.\n<ul>\n<li><code>ClassificationModel</code></li>\n<li><code>MultiLabelClassificationModel</code></li>\n</ul>\n</li>\n<li><code>simpletransformers.ner</code> - Includes all Named Entity Recognition models.\n<ul>\n<li><code>NERModel</code></li>\n</ul>\n</li>\n<li><code>simpletransformers.question_answering</code> - Includes all Question Answering models.\n<ul>\n<li><code>QuestionAnsweringModel</code></li>\n</ul>\n</li>\n</ul>\n<p><em><a href=\"#table-of-contents\" rel=\"nofollow\">Back to Table of Contents</a></em></p>\n<hr>\n<h2>Text Classification</h2>\n<p>Supports Binary Classification, Multiclass Classification, and Multilabel Classification.</p>\n<p>Supported model types:</p>\n<ul>\n<li>ALBERT</li>\n<li>BERT</li>\n<li>CamemBERT</li>\n<li>RoBERTa</li>\n<li>DistilBERT</li>\n<li>ELECTRA</li>\n<li>FlauBERT</li>\n<li>XLM</li>\n<li>XLM-RoBERTa</li>\n<li>XLNet</li>\n</ul>\n<h3>Task Specific Notes</h3>\n<ul>\n<li>Set <code>'sliding_window': True</code> in <code>args</code> to prevent text being truncated. The default <em>stride</em> is <code>'stride': 0.8</code> which is <code>0.8 * max_seq_length</code>. Training text will be split using a sliding window and each window will be assigned the label from the original text. During evaluation and prediction, the mode of the predictions for each window will be the final prediction on each sample. The <code>tie_value</code> (default <code>1</code>) will be used in the case of a tie.<br>\n<em>Currently not available for Multilabel Classification</em></li>\n</ul>\n<h4>Minimal Start for Binary Classification</h4>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.classification</span> <span class=\"kn\">import</span> <span class=\"n\">ClassificationModel</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n\n\n<span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"s2\">\"transformers\"</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">WARNING</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Train and Evaluation data needs to be in a Pandas Dataframe of two columns. The first column is the text with type str, and the second column is the label with type int.</span>\n<span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"p\">[[</span><span class=\"s1\">'Example sentence belonging to class 1'</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'Example sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]]</span>\n<span class=\"n\">train_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">train_data</span><span class=\"p\">)</span>\n\n<span class=\"n\">eval_data</span> <span class=\"o\">=</span> <span class=\"p\">[[</span><span class=\"s1\">'Example eval sentence belonging to class 1'</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'Example eval sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]]</span>\n<span class=\"n\">eval_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">eval_data</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create a ClassificationModel</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ClassificationModel</span><span class=\"p\">(</span><span class=\"s1\">'roberta'</span><span class=\"p\">,</span> <span class=\"s1\">'roberta-base'</span><span class=\"p\">)</span> <span class=\"c1\"># You can set class weights by using the optional weight argument</span>\n\n<span class=\"c1\"># Train the model</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"n\">train_df</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Evaluate the model</span>\n<span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">model_outputs</span><span class=\"p\">,</span> <span class=\"n\">wrong_predictions</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"n\">eval_df</span><span class=\"p\">)</span>\n</pre>\n<p>If you wish to add any custom metrics, simply pass them as additional keyword arguments. The keyword is the name to be given to the metric, and the value is the function that will calculate the metric. Make sure that the function expects two parameters with the first one being the true label, and the second being the predictions. (This is the default for sklearn metrics)</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">sklearn</span>\n\n\n<span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">model_outputs</span><span class=\"p\">,</span> <span class=\"n\">wrong_predictions</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"n\">eval_df</span><span class=\"p\">,</span> <span class=\"n\">acc</span><span class=\"o\">=</span><span class=\"n\">sklearn</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">accuracy_score</span><span class=\"p\">)</span>\n</pre>\n<p>To make predictions on arbitary data, the <code>predict(to_predict)</code> function can be used. For a list of text, it returns the model predictions and the raw model outputs.</p>\n<pre><span class=\"n\">predictions</span><span class=\"p\">,</span> <span class=\"n\">raw_outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([</span><span class=\"s1\">'Some arbitary sentence'</span><span class=\"p\">])</span>\n</pre>\n<h4>Minimal Start for Multiclass Classification</h4>\n<p>For multiclass classification, simply pass in the number of classes to the <code>num_labels</code> optional parameter of <code>ClassificationModel</code>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.classification</span> <span class=\"kn\">import</span> <span class=\"n\">ClassificationModel</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n\n\n<span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"s2\">\"transformers\"</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">WARNING</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Train and Evaluation data needs to be in a Pandas Dataframe containing at least two columns. If the Dataframe has a header, it should contain a 'text' and a 'labels' column. If no header is present, the Dataframe should contain at least two columns, with the first column is the text with type str, and the second column in the label with type int.</span>\n<span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"p\">[[</span><span class=\"s1\">'Example sentence belonging to class 1'</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'Example sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'Example eval senntence belonging to class 2'</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">]]</span>\n<span class=\"n\">train_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">train_data</span><span class=\"p\">)</span>\n\n<span class=\"n\">eval_data</span> <span class=\"o\">=</span> <span class=\"p\">[[</span><span class=\"s1\">'Example eval sentence belonging to class 1'</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'Example eval sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'Example eval senntence belonging to class 2'</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">]]</span>\n<span class=\"n\">eval_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">eval_data</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create a ClassificationModel</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ClassificationModel</span><span class=\"p\">(</span><span class=\"s1\">'bert'</span><span class=\"p\">,</span> <span class=\"s1\">'bert-base-cased'</span><span class=\"p\">,</span> <span class=\"n\">num_labels</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'reprocess_input_data'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"s1\">'overwrite_output_dir'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">})</span> \n<span class=\"c1\"># You can set class weights by using the optional weight argument</span>\n\n<span class=\"c1\"># Train the model</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"n\">train_df</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Evaluate the model</span>\n<span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">model_outputs</span><span class=\"p\">,</span> <span class=\"n\">wrong_predictions</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"n\">eval_df</span><span class=\"p\">)</span>\n\n<span class=\"n\">predictions</span><span class=\"p\">,</span> <span class=\"n\">raw_outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([</span><span class=\"s2\">\"Some arbitary sentence\"</span><span class=\"p\">])</span>\n</pre>\n<h4>Minimal Start for Multilabel Classification</h4>\n<p>For Multi-Label Classification, the labels should be multi-hot encoded. The number of classes can be specified (default is 2) by passing it to the <code>num_labels</code> optional parameter of <code>MultiLabelClassificationModel</code>.</p>\n<p><em>Warning: Pandas can cause issues when saving and loading lists stored in a column. Check whether your list has been converted to a String!</em></p>\n<p>The default evaluation metric used is Label Ranking Average Precision (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html\" rel=\"nofollow\">LRAP</a>) Score.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.classification</span> <span class=\"kn\">import</span> <span class=\"n\">MultiLabelClassificationModel</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n\n\n<span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"s2\">\"transformers\"</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">WARNING</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Train and Evaluation data needs to be in a Pandas Dataframe containing at least two columns, a 'text' and a 'labels' column. The `labels` column should contain multi-hot encoded lists.</span>\n<span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"p\">[[</span><span class=\"s1\">'Example sentence 1 for multilabel classification.'</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]]],</span> <span class=\"p\">[[</span><span class=\"s1\">'This is another example sentence. '</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]]]</span>\n<span class=\"n\">train_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">train_data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'text'</span><span class=\"p\">,</span> <span class=\"s1\">'labels'</span><span class=\"p\">])</span>\n\n<span class=\"n\">eval_data</span> <span class=\"o\">=</span> <span class=\"p\">[[</span><span class=\"s1\">'Example eval sentence for multilabel classification.'</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]],</span> <span class=\"p\">[</span><span class=\"s1\">'Example eval senntence belonging to class 2'</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]]]</span>\n<span class=\"n\">eval_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">eval_data</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create a MultiLabelClassificationModel</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">MultiLabelClassificationModel</span><span class=\"p\">(</span><span class=\"s1\">'roberta'</span><span class=\"p\">,</span> <span class=\"s1\">'roberta-base'</span><span class=\"p\">,</span> <span class=\"n\">num_labels</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'reprocess_input_data'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"s1\">'overwrite_output_dir'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"s1\">'num_train_epochs'</span><span class=\"p\">:</span> <span class=\"mi\">5</span><span class=\"p\">})</span>\n<span class=\"c1\"># You can set class weights by using the optional weight argument</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">train_df</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">())</span>\n\n<span class=\"c1\"># Train the model</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"n\">train_df</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Evaluate the model</span>\n<span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">model_outputs</span><span class=\"p\">,</span> <span class=\"n\">wrong_predictions</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"n\">eval_df</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">model_outputs</span><span class=\"p\">)</span>\n\n<span class=\"n\">predictions</span><span class=\"p\">,</span> <span class=\"n\">raw_outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([</span><span class=\"s1\">'This thing is entirely different from the other thing. '</span><span class=\"p\">])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">predictions</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">raw_outputs</span><span class=\"p\">)</span>\n</pre>\n<h5>Special Attributes</h5>\n<ul>\n<li>The args dict of <code>MultiLabelClassificationModel</code> has an additional <code>threshold</code> parameter with default value 0.5. The threshold is the value at which a given label flips from 0 to 1 when predicting. The <code>threshold</code> may be a single value or a list of value with the same length as the number of labels. This enables the use of seperate threshold values for each label.</li>\n<li><code>MultiLabelClassificationModel</code> takes in an additional optional argument <code>pos_weight</code>. This should be a list with the same length as the number of labels. This enables using different weights for each label when calculating loss during training and evaluation.</li>\n</ul>\n<h4>Minimal Start for Sentence Pair Classification</h4>\n<ul>\n<li>Training and evaluation Dataframes must contain a <code>text_a</code>, <code>text_b</code>, and a <code>labels</code> column.</li>\n<li>The <code>predict()</code> function expects a list of lists in the format below. A single sample input should also be a list of lists like <code>[[text_a, text_b]]</code>.</li>\n</ul>\n<pre><span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"n\">sample_1_text_a</span><span class=\"p\">,</span> <span class=\"n\">sample_1_text_b</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"n\">sample_2_text_a</span><span class=\"p\">,</span> <span class=\"n\">sample_2_text_b</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"n\">sample_3_text_a</span><span class=\"p\">,</span> <span class=\"n\">sample_3_text_b</span><span class=\"p\">],</span>\n    <span class=\"c1\"># More samples</span>\n<span class=\"p\">]</span>\n</pre>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.classification</span> <span class=\"kn\">import</span> <span class=\"n\">ClassificationModel</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">sklearn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n\n\n<span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"s2\">\"transformers\"</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">WARNING</span><span class=\"p\">)</span>\n\n<span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s1\">'Example sentence belonging to class 1'</span><span class=\"p\">,</span> <span class=\"s1\">'Yep, this is 1'</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'Example sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"s1\">'Yep, this is 0'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'Example  2 sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"s1\">'Yep, this is 0'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">train_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">train_data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'text_a'</span><span class=\"p\">,</span> <span class=\"s1\">'text_b'</span><span class=\"p\">,</span> <span class=\"s1\">'labels'</span><span class=\"p\">])</span>\n\n<span class=\"n\">eval_data</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s1\">'Example sentence belonging to class 1'</span><span class=\"p\">,</span> <span class=\"s1\">'Yep, this is 1'</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'Example sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"s1\">'Yep, this is 0'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'Example  2 sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"s1\">'Yep, this is 0'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">eval_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">eval_data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'text_a'</span><span class=\"p\">,</span> <span class=\"s1\">'text_b'</span><span class=\"p\">,</span> <span class=\"s1\">'labels'</span><span class=\"p\">])</span>\n\n<span class=\"n\">train_args</span><span class=\"o\">=</span><span class=\"p\">{</span>\n    <span class=\"s1\">'reprocess_input_data'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s1\">'overwrite_output_dir'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s1\">'num_train_epochs'</span><span class=\"p\">:</span> <span class=\"mi\">3</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"c1\"># Create a ClassificationModel</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ClassificationModel</span><span class=\"p\">(</span><span class=\"s1\">'roberta'</span><span class=\"p\">,</span> <span class=\"s1\">'roberta-base'</span><span class=\"p\">,</span> <span class=\"n\">num_labels</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">use_cuda</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">cuda_device</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">train_args</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">train_df</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">())</span>\n\n<span class=\"c1\"># Train the model</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"n\">train_df</span><span class=\"p\">,</span> <span class=\"n\">eval_df</span><span class=\"o\">=</span><span class=\"n\">eval_df</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Evaluate the model</span>\n<span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">model_outputs</span><span class=\"p\">,</span> <span class=\"n\">wrong_predictions</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"n\">eval_df</span><span class=\"p\">,</span> <span class=\"n\">acc</span><span class=\"o\">=</span><span class=\"n\">sklearn</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">accuracy_score</span><span class=\"p\">)</span>\n\n<span class=\"n\">predictions</span><span class=\"p\">,</span> <span class=\"n\">raw_outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([[</span><span class=\"s2\">\"I'd like to puts some CD-ROMS on my iPad, is that possible?'\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Yes, but wouldn't that block the screen?\"</span><span class=\"p\">]])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">predictions</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">raw_outputs</span><span class=\"p\">)</span>\n</pre>\n<h4>Real Dataset Examples</h4>\n<ul>\n<li><a href=\"https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3?source=friends_link&amp;sk=40726ceeadf99e1120abc9521a10a55c\" rel=\"nofollow\">Yelp Reviews Dataset - Binary Classification</a></li>\n<li><a href=\"https://medium.com/swlh/simple-transformers-multi-class-text-classification-with-bert-roberta-xlnet-xlm-and-8b585000ce3a?source=friends_link&amp;sk=90e1c97255b65cedf4910a99041d9dfc\" rel=\"nofollow\">AG News Dataset - Multiclass Classification</a></li>\n<li><a href=\"https://towardsdatascience.com/multi-label-classification-using-bert-roberta-xlnet-xlm-and-distilbert-with-simple-transformers-b3e0cda12ce5?source=friends_link&amp;sk=354e688fe238bfb43e9a575216816219\" rel=\"nofollow\">Toxic Comments Dataset - Multilabel Classification</a></li>\n<li><a href=\"https://medium.com/@chaturangarajapakshe/solving-sentence-pair-tasks-using-simple-transformers-2496fe79d616?source=friends_link&amp;sk=fbf7439e9c31f7aefa1613d423a0fd40\" rel=\"nofollow\">Semantic Textual Similarity Benchmark - Sentence Pair</a></li>\n<li><a href=\"https://towardsdatascience.com/to-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8?source=friends_link&amp;sk=6a3c7940b18066ded94aeee95e354ed1\" rel=\"nofollow\">AG News Dataset - BERT (base and distilled), RoBERTa (base and distilled), and XLNet compared</a></li>\n</ul>\n<h4>ClassificationModel</h4>\n<p><code>class simpletransformers.classification.ClassificationModel (model_type, model_name, args=None, use_cuda=True)</code><br>\nThis class  is used for Text Classification tasks.</p>\n<p><code>Class attributes</code></p>\n<ul>\n<li><code>tokenizer</code>: The tokenizer to be used.</li>\n<li><code>model</code>: The model to be used.</li>\n<li><code>model_name</code>: model_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).</li>\n<li><code>device</code>: The device on which the model will be trained and evaluated.</li>\n<li><code>results</code>: A python dict of past evaluation results for the TransformerModel object.</li>\n<li><code>args</code>: A python dict of arguments used for training and evaluation.</li>\n</ul>\n<ul>\n<li><code>cuda_device</code>: (optional) int - Default = -1. Used to specify which GPU should be used.</li>\n</ul>\n<p><code>Parameters</code></p>\n<ul>\n<li><code>model_type</code>: (required) str - The type of model to use. Currently, BERT, XLNet, XLM, and RoBERTa models are available.</li>\n<li><code>model_name</code>: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See <a href=\"#current-pretrained-models\" rel=\"nofollow\">Current Pretrained Models</a> for all available models.</li>\n<li><code>num_labels</code> (optional): The number of labels or classes in the dataset.</li>\n<li><code>weight</code> (optional): A list of length num_labels containing the weights to assign to each label for loss calculation.</li>\n<li><code>args</code>: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.</li>\n<li><code>use_cuda</code>: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.</li>\n</ul>\n<p><code>class methods</code><br>\n<strong><code>train_model(self, train_df, output_dir=None, show_running_loss=True, args=None, eval_df=None)</code></strong></p>\n<p>Trains the model using 'train_df'</p>\n<p>Args:</p>\n<ul>\n<li>\n<p><code>train_df</code>: Pandas Dataframe containing at least two columns. If the Dataframe has a header, it should contain a 'text' and a 'labels' column. If no header is present, the Dataframe should contain at least two columns, with the first column containing the text, and the second column containing the label. The model will be trained on this Dataframe.</p>\n</li>\n<li>\n<p><code>output_dir</code> (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p><code>args</code> (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.</p>\n</li>\n<li>\n<p>show_running_loss (optional): Set to False to disable printing running training loss to the terminal.</p>\n</li>\n<li>\n<p><code>eval_df</code> (optional): A DataFrame against which evaluation will be performed when <code>evaluate_during_training</code> is enabled. Is required if <code>evaluate_during_training</code> is enabled.</p>\n</li>\n<li>\n<p><code>**kwargs</code>: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). E.g. f1=sklearn.metrics.f1_score.\nA metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>None</li>\n</ul>\n<p><strong><code>eval_model(self, eval_df, output_dir=None, verbose=False)</code></strong></p>\n<p>Evaluates the model on eval_df. Saves results to output_dir.</p>\n<p>Args:</p>\n<ul>\n<li>\n<p>eval_df: Pandas Dataframe containing at least two columns. If the Dataframe has a header, it should contain a 'text' and a 'labels' column. If no header is present, the Dataframe should contain at least two columns, with the first column containing the text, and the second column containing the label. The model will be evaluated on this Dataframe.</p>\n</li>\n<li>\n<p>output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p>verbose: If verbose, results will be printed to the console on completion of evaluation.</p>\n</li>\n<li>\n<p>silent: If silent, tqdm progress bars will be hidden.</p>\n</li>\n<li>\n<p><code>**kwargs</code>: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). E.g. f1=sklearn.metrics.f1_score.\nA metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>\n<p>result: Dictionary containing evaluation results. (Matthews correlation coefficient, tp, tn, fp, fn)</p>\n</li>\n<li>\n<p>model_outputs: List of model outputs for each row in eval_df</p>\n</li>\n<li>\n<p>wrong_preds: List of InputExample objects corresponding to each incorrect prediction by the model</p>\n</li>\n</ul>\n<p><strong><code>predict(self, to_predict)</code></strong></p>\n<p>Performs predictions on a list of text.</p>\n<p>Args:</p>\n<ul>\n<li>to_predict: A python list of text (str) to be sent to the model for prediction.</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>preds: A python list of the predictions (0 or 1) for each text.</li>\n<li>model_outputs: A python list of the raw model outputs for each text.</li>\n</ul>\n<p>If <code>config: {\"output_hidden_states\": True}</code>, two additional values will be returned.</p>\n<ul>\n<li>all_embedding_outputs: Numpy array of shape <em>(batch_size, sequence_length, hidden_size)</em></li>\n<li>all_layer_hidden_states: Numpy array of shape <em>(num_hidden_layers, batch_size, sequence_length, hidden_size)</em></li>\n</ul>\n<p><strong><code>train(self, train_dataset, output_dir)</code></strong></p>\n<p>Trains the model on train_dataset.\n<em>Utility function to be used by the train_model() method. Not intended to be used directly.</em></p>\n<p><strong><code>evaluate(self, eval_df, output_dir, prefix=\"\")</code></strong></p>\n<p>Evaluates the model on eval_df.\n<em>Utility function to be used by the eval_model() method. Not intended to be used directly</em></p>\n<p><strong><code>load_and_cache_examples(self, examples, evaluate=False)</code></strong></p>\n<p>Converts a list of InputExample objects to a TensorDataset containing InputFeatures. Caches the InputFeatures.\n<em>Utility function for train() and eval() methods. Not intended to be used directly</em></p>\n<p><strong><code>compute_metrics(self, preds, labels, eval_examples, **kwargs):</code></strong></p>\n<p>Computes the evaluation metrics for the model predictions.</p>\n<p>Args:</p>\n<ul>\n<li>\n<p>preds: Model predictions</p>\n</li>\n<li>\n<p>labels: Ground truth labels</p>\n</li>\n<li>\n<p>eval_examples: List of examples on which evaluation was performed</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>\n<p>result: Dictionary containing evaluation results. (Matthews correlation coefficient, tp, tn, fp, fn)</p>\n</li>\n<li>\n<p>wrong: List of InputExample objects corresponding to each incorrect prediction by the model</p>\n</li>\n</ul>\n<p><em><a href=\"#table-of-contents\" rel=\"nofollow\">Back to Table of Contents</a></em></p>\n<hr>\n<h2>Named Entity Recognition</h2>\n<p>This section describes how to use Simple Transformers for Named Entity Recognition. (If you are updating from a Simple Transformers before 0.5.0, note that <code>seqeval</code> needs to be installed to perform NER.)</p>\n<p><em>This model can also be used for any other NLP task involving token level classification. Make sure you pass in your list of labels to the model if they are different from the defaults.</em></p>\n<p>Supported model types:</p>\n<ul>\n<li>BERT</li>\n<li>CamemBERT</li>\n<li>DistilBERT</li>\n<li>ELECTRA</li>\n<li>RoBERTa</li>\n<li>XLM-RoBERTa</li>\n</ul>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">NERModel</span><span class=\"p\">(</span><span class=\"s1\">'bert'</span><span class=\"p\">,</span> <span class=\"s1\">'bert-base-cased'</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"LABEL_1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"LABEL_2\"</span><span class=\"p\">,</span> <span class=\"s2\">\"LABEL_3\"</span><span class=\"p\">])</span>\n</pre>\n<h4>Minimal Start</h4>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.ner</span> <span class=\"kn\">import</span> <span class=\"n\">NERModel</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n\n\n<span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"s2\">\"transformers\"</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">WARNING</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Creating train_df  and eval_df for demonstration</span>\n<span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">'Simple'</span><span class=\"p\">,</span> <span class=\"s1\">'B-MISC'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">'Transformers'</span><span class=\"p\">,</span> <span class=\"s1\">'I-MISC'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">'started'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'with'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">'text'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">'classification'</span><span class=\"p\">,</span> <span class=\"s1\">'B-MISC'</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'Simple'</span><span class=\"p\">,</span> <span class=\"s1\">'B-MISC'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'Transformers'</span><span class=\"p\">,</span> <span class=\"s1\">'I-MISC'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'can'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'now'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'perform'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'NER'</span><span class=\"p\">,</span> <span class=\"s1\">'B-MISC'</span><span class=\"p\">]</span>\n<span class=\"p\">]</span>\n<span class=\"n\">train_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">train_data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'sentence_id'</span><span class=\"p\">,</span> <span class=\"s1\">'words'</span><span class=\"p\">,</span> <span class=\"s1\">'labels'</span><span class=\"p\">])</span>\n\n<span class=\"n\">eval_data</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">'Simple'</span><span class=\"p\">,</span> <span class=\"s1\">'B-MISC'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">'Transformers'</span><span class=\"p\">,</span> <span class=\"s1\">'I-MISC'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">'was'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'built'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'for'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">'text'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s1\">'classification'</span><span class=\"p\">,</span> <span class=\"s1\">'B-MISC'</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'Simple'</span><span class=\"p\">,</span> <span class=\"s1\">'B-MISC'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'Transformers'</span><span class=\"p\">,</span> <span class=\"s1\">'I-MISC'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'then'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'expanded'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'to'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'perform'</span><span class=\"p\">,</span> <span class=\"s1\">'O'</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'NER'</span><span class=\"p\">,</span> <span class=\"s1\">'B-MISC'</span><span class=\"p\">]</span>\n<span class=\"p\">]</span>\n<span class=\"n\">eval_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">eval_data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'sentence_id'</span><span class=\"p\">,</span> <span class=\"s1\">'words'</span><span class=\"p\">,</span> <span class=\"s1\">'labels'</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Create a NERModel</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">NERModel</span><span class=\"p\">(</span><span class=\"s1\">'bert'</span><span class=\"p\">,</span> <span class=\"s1\">'bert-base-cased'</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'overwrite_output_dir'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"s1\">'reprocess_input_data'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">})</span>\n\n<span class=\"c1\"># Train the model</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"n\">train_df</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Evaluate the model</span>\n<span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">model_outputs</span><span class=\"p\">,</span> <span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"n\">eval_df</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Predictions on arbitary text strings</span>\n<span class=\"n\">predictions</span><span class=\"p\">,</span> <span class=\"n\">raw_outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([</span><span class=\"s2\">\"Some arbitary sentence\"</span><span class=\"p\">])</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">predictions</span><span class=\"p\">)</span>\n</pre>\n<h4>Real Dataset Examples</h4>\n<ul>\n<li><a href=\"https://towardsdatascience.com/simple-transformers-named-entity-recognition-with-transformer-models-c04b9242a2a0?source=friends_link&amp;sk=e8b98c994173cd5219f01e075727b096\" rel=\"nofollow\">CoNLL Dataset Example</a></li>\n</ul>\n<h4>NERModel</h4>\n<p><code>class simpletransformers.ner.ner_model.NERModel (model_type, model_name, labels=None, args=None, use_cuda=True)</code><br>\nThis class  is used for Named Entity Recognition.</p>\n<p><code>Class attributes</code></p>\n<ul>\n<li><code>tokenizer</code>: The tokenizer to be used.</li>\n<li><code>model</code>: The model to be used.\nmodel_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).</li>\n<li><code>device</code>: The device on which the model will be trained and evaluated.</li>\n<li><code>results</code>: A python dict of past evaluation results for the TransformerModel object.</li>\n<li><code>args</code>: A python dict of arguments used for training and evaluation.</li>\n</ul>\n<ul>\n<li><code>cuda_device</code>: (optional) int - Default = -1. Used to specify which GPU should be used.</li>\n</ul>\n<p><code>Parameters</code></p>\n<ul>\n<li><code>model_type</code>: (required) str - The type of model to use. Currently, BERT, XLNet, XLM, and RoBERTa models are available.</li>\n<li><code>model_name</code>: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See <a href=\"#current-pretrained-models\" rel=\"nofollow\">Current Pretrained Models</a> for all available models.</li>\n<li><code>labels</code> (optional): A list of all Named Entity labels.  If not given, [\"O\", \"B-MISC\", \"I-MISC\",  \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"] will be used.</li>\n<li><code>args</code>: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.</li>\n<li><code>use_cuda</code>: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.</li>\n</ul>\n<p><code>class methods</code><br>\n<strong><code>train_model(self, train_data, output_dir=None, args=None, eval_df=None)</code></strong></p>\n<p>Trains the model using 'train_data'</p>\n<p>Args:</p>\n<ul>\n<li>\n<p>train_data: train_data should be the path to a .txt file containing the training data OR a pandas DataFrame with 3 columns.\nIf a text file is used the data should be in the CoNLL format. i.e. One word per line, with sentences seperated by an empty line.\nThe first word of the line should be a word, and the last should be a Name Entity Tag.\nIf a DataFrame is given, each sentence should be split into words, with each word assigned a tag, and with all words from the same sentence given the same sentence_id.</p>\n</li>\n<li>\n<p>output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p>show_running_loss (optional): Set to False to prevent running loss from being printed to console. Defaults to True.</p>\n</li>\n<li>\n<p>args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.</p>\n</li>\n<li>\n<p>eval_df (optional): A DataFrame against which evaluation will be performed when <code>evaluate_during_training</code> is enabled. Is required if <code>evaluate_during_training</code> is enabled.</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>None</li>\n</ul>\n<p><strong><code>eval_model(self, eval_data, output_dir=None, verbose=True)</code></strong></p>\n<p>Evaluates the model on eval_data. Saves results to output_dir.</p>\n<p>Args:</p>\n<ul>\n<li>\n<p>eval_data: eval_data should be the path to a .txt file containing the evaluation data or a pandas DataFrame. If a text file is used the data should be in the CoNLL format. I.e. One word per line, with sentences seperated by an empty line. The first word of the line should be a word, and the last should be a Name Entity Tag. If a DataFrame is given, each sentence should be split into words, with each word assigned a tag, and with all words from the same sentence given the same sentence_id.</p>\n</li>\n<li>\n<p>output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p>verbose: If verbose, results will be printed to the console on completion of evaluation.</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>\n<p>result: Dictionary containing evaluation results. (eval_loss, precision, recall, f1_score)</p>\n</li>\n<li>\n<p>model_outputs: List of raw model outputs</p>\n</li>\n<li>\n<p>preds_list: List of predicted tags</p>\n</li>\n</ul>\n<p><strong><code>predict(self, to_predict)</code></strong></p>\n<p>Performs predictions on a list of text.</p>\n<p>Args:</p>\n<ul>\n<li>to_predict: A python list of text (str) to be sent to the model for prediction.</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>preds: A Python list of lists with dicts containg each word mapped to its NER tag.</li>\n<li>model_outputs: A python list of the raw model outputs for each text.</li>\n</ul>\n<p><strong><code>train(self, train_dataset, output_dir)</code></strong></p>\n<p>Trains the model on train_dataset.\n<em>Utility function to be used by the train_model() method. Not intended to be used directly.</em></p>\n<p><strong><code>evaluate(self, eval_dataset, output_dir, prefix=\"\")</code></strong></p>\n<p>Evaluates the model on eval_dataset.\n<em>Utility function to be used by the eval_model() method. Not intended to be used directly</em></p>\n<p><strong><code>load_and_cache_examples(self, data, evaluate=False, no_cache=False, to_predict=None)</code></strong></p>\n<p>Converts a list of InputExample objects to a TensorDataset containing InputFeatures. Caches the InputFeatures.\n<em>Utility function for train() and eval() methods. Not intended to be used directly</em></p>\n<p><em><a href=\"#table-of-contents\" rel=\"nofollow\">Back to Table of Contents</a></em></p>\n<hr>\n<h2>Question Answering</h2>\n<p>Supported model types:</p>\n<ul>\n<li>ALBERT</li>\n<li>BERT</li>\n<li>DistilBERT</li>\n<li>ELECTRA</li>\n<li>XLM</li>\n<li>XLNet</li>\n</ul>\n<h3>Data format</h3>\n<p>For question answering tasks, the input data can be in JSON files or in a Python list of dicts in the correct format.</p>\n<p>The file should contain a single list of dictionaries. A dictionary represents a single context and its associated questions.</p>\n<p>Each such dictionary contains two attributes, the <code>\"context\"</code> and <code>\"qas\"</code>.</p>\n<ul>\n<li><code>context</code>: The paragraph or text from which the question is asked.</li>\n<li><code>qas</code>: A list of questions and answers.</li>\n</ul>\n<p>Questions and answers are represented as dictionaries. Each dictionary in <code>qas</code> has the following format.</p>\n<ul>\n<li><code>id</code>: (string) A unique ID for the question. Should be unique across the entire dataset.</li>\n<li><code>question</code>: (string) A question.</li>\n<li><code>is_impossible</code>: (bool) Indicates whether the question can be answered correctly from the context.</li>\n<li><code>answers</code>: (list) The list of correct answers to the question.</li>\n</ul>\n<p>A single answer is represented by a dictionary with the following attributes.</p>\n<ul>\n<li><code>answer</code>: (string) The answer to the question. Must be a substring of the context.</li>\n<li><code>answer_start</code>: (int) Starting index of the answer in the context.</li>\n</ul>\n<h3>Minimal Example</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.question_answering</span> <span class=\"kn\">import</span> <span class=\"n\">QuestionAnsweringModel</span>\n<span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n\n\n<span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"s2\">\"transformers\"</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">WARNING</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create dummy data to use for training.</span>\n<span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n        <span class=\"s1\">'context'</span><span class=\"p\">:</span> <span class=\"s2\">\"This is the first context\"</span><span class=\"p\">,</span>\n        <span class=\"s1\">'qas'</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"p\">{</span>\n                <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"s2\">\"00001\"</span><span class=\"p\">,</span>\n                <span class=\"s1\">'is_impossible'</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                <span class=\"s1\">'question'</span><span class=\"p\">:</span> <span class=\"s2\">\"Which context is this?\"</span><span class=\"p\">,</span>\n                <span class=\"s1\">'answers'</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                    <span class=\"p\">{</span>\n                        <span class=\"s1\">'text'</span><span class=\"p\">:</span> <span class=\"s2\">\"the first\"</span><span class=\"p\">,</span>\n                        <span class=\"s1\">'answer_start'</span><span class=\"p\">:</span> <span class=\"mi\">8</span>\n                    <span class=\"p\">}</span>\n                <span class=\"p\">]</span>\n            <span class=\"p\">}</span>\n        <span class=\"p\">]</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n        <span class=\"s1\">'context'</span><span class=\"p\">:</span> <span class=\"s2\">\"Other legislation followed, including the Migratory Bird Conservation Act of 1929, a 1937 treaty prohibiting the hunting of right and gray whales,</span>\n            <span class=\"ow\">and</span> <span class=\"n\">the</span> <span class=\"n\">Bald</span> <span class=\"n\">Eagle</span> <span class=\"n\">Protection</span> <span class=\"n\">Act</span> <span class=\"n\">of</span> <span class=\"mf\">1940.</span> <span class=\"n\">These</span> <span class=\"n\">later</span> <span class=\"n\">laws</span> <span class=\"n\">had</span> <span class=\"n\">a</span> <span class=\"n\">low</span> <span class=\"n\">cost</span> <span class=\"n\">to</span> <span class=\"n\">society</span><span class=\"err\">\u2014</span><span class=\"n\">the</span> <span class=\"n\">species</span> <span class=\"n\">were</span> <span class=\"n\">relatively</span> <span class=\"n\">rare</span><span class=\"err\">\u2014</span><span class=\"ow\">and</span> <span class=\"n\">little</span> <span class=\"n\">opposition</span> <span class=\"n\">was</span> <span class=\"n\">raised</span><span class=\"s2\">\",</span>\n        <span class=\"s1\">'qas'</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"p\">{</span>\n                <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"s2\">\"00002\"</span><span class=\"p\">,</span>\n                <span class=\"s1\">'is_impossible'</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                <span class=\"s1\">'question'</span><span class=\"p\">:</span> <span class=\"s2\">\"What was the cost to society?\"</span><span class=\"p\">,</span>\n                <span class=\"s1\">'answers'</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                    <span class=\"p\">{</span>\n                        <span class=\"s1\">'text'</span><span class=\"p\">:</span> <span class=\"s2\">\"low cost\"</span><span class=\"p\">,</span>\n                        <span class=\"s1\">'answer_start'</span><span class=\"p\">:</span> <span class=\"mi\">225</span>\n                    <span class=\"p\">}</span>\n                <span class=\"p\">]</span>\n            <span class=\"p\">},</span>\n            <span class=\"p\">{</span>\n                <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"s2\">\"00003\"</span><span class=\"p\">,</span>\n                <span class=\"s1\">'is_impossible'</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n                <span class=\"s1\">'question'</span><span class=\"p\">:</span> <span class=\"s2\">\"What was the name of the 1937 treaty?\"</span><span class=\"p\">,</span>\n                <span class=\"s1\">'answers'</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                    <span class=\"p\">{</span>\n                        <span class=\"s1\">'text'</span><span class=\"p\">:</span> <span class=\"s2\">\"Bald Eagle Protection Act\"</span><span class=\"p\">,</span>\n                        <span class=\"s1\">'answer_start'</span><span class=\"p\">:</span> <span class=\"mi\">167</span>\n                    <span class=\"p\">}</span>\n                <span class=\"p\">]</span>\n            <span class=\"p\">}</span>\n        <span class=\"p\">]</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">]</span>\n\n<span class=\"c1\"># Save as a JSON file</span>\n<span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">makedirs</span><span class=\"p\">(</span><span class=\"s1\">'data'</span><span class=\"p\">,</span> <span class=\"n\">exist_ok</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">'data/train.json'</span><span class=\"p\">,</span> <span class=\"s1\">'w'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dump</span><span class=\"p\">(</span><span class=\"n\">train_data</span><span class=\"p\">,</span> <span class=\"n\">f</span><span class=\"p\">)</span>\n\n\n<span class=\"c1\"># Create the QuestionAnsweringModel</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">QuestionAnsweringModel</span><span class=\"p\">(</span><span class=\"s1\">'distilbert'</span><span class=\"p\">,</span> <span class=\"s1\">'distilbert-base-uncased-distilled-squad'</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'reprocess_input_data'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"s1\">'overwrite_output_dir'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">})</span>\n\n<span class=\"c1\"># Train the model with JSON file</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"s1\">'data/train.json'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># The list can also be used directly</span>\n<span class=\"c1\"># model.train_model(train_data)</span>\n\n<span class=\"c1\"># Evaluate the model. (Being lazy and evaluating on the train data itself)</span>\n<span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"s1\">'data/train.json'</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'-------------------'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Making predictions using the model.</span>\n<span class=\"n\">to_predict</span> <span class=\"o\">=</span> <span class=\"p\">[{</span><span class=\"s1\">'context'</span><span class=\"p\">:</span> <span class=\"s1\">'This is the context used for demonstrating predictions.'</span><span class=\"p\">,</span> <span class=\"s1\">'qas'</span><span class=\"p\">:</span> <span class=\"p\">[{</span><span class=\"s1\">'question'</span><span class=\"p\">:</span> <span class=\"s1\">'What is this context?'</span><span class=\"p\">,</span> <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"s1\">'0'</span><span class=\"p\">}]}]</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">to_predict</span><span class=\"p\">))</span>\n</pre>\n<h3>Real Dataset Examples</h3>\n<ul>\n<li><a href=\"https://towardsdatascience.com/question-answering-with-bert-xlnet-xlm-and-distilbert-using-simple-transformers-4d8785ee762a?source=friends_link&amp;sk=e8e6f9a39f20b5aaf08bbcf8b0a0e1c2\" rel=\"nofollow\">SQuAD 2.0 - Question Answering</a></li>\n</ul>\n<h3>QuestionAnsweringModel</h3>\n<p><code>class simpletransformers.question_answering.QuestionAnsweringModel (model_type, model_name, args=None, use_cuda=True)</code><br>\nThis class is used for Question Answering tasks.</p>\n<p><code>Class attributes</code></p>\n<ul>\n<li><code>tokenizer</code>: The tokenizer to be used.</li>\n<li><code>model</code>: The model to be used.\nmodel_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).</li>\n<li><code>device</code>: The device on which the model will be trained and evaluated.</li>\n<li><code>results</code>: A python dict of past evaluation results for the TransformerModel object.</li>\n<li><code>args</code>: A python dict of arguments used for training and evaluation.</li>\n<li><code>cuda_device</code>: (optional) int - Default = -1. Used to specify which GPU should be used.</li>\n</ul>\n<p><code>Parameters</code></p>\n<ul>\n<li><code>model_type</code>: (required) str - The type of model to use.</li>\n<li><code>model_name</code>: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See <a href=\"#current-pretrained-models\" rel=\"nofollow\">Current Pretrained Models</a> for all available models.</li>\n<li><code>args</code>: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.</li>\n<li><code>use_cuda</code>: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.</li>\n</ul>\n<p><code>class methods</code><br>\n<strong><code>train_model(self, train_df, output_dir=None, args=None, eval_df=None)</code></strong></p>\n<p>Trains the model using 'train_file'</p>\n<p>Args:</p>\n<ul>\n<li>\n<p><code>train_df</code>: Path to JSON file containing training data. The model will be trained on this file.\noutput_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p><code>output_dir</code> (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p><code>show_running_loss</code> (Optional): Set to False to prevent training loss being printed.</p>\n</li>\n<li>\n<p><code>args</code> (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.</p>\n</li>\n<li>\n<p><code>eval_file</code> (optional): Path to JSON file containing evaluation data against which evaluation will be performed when evaluate_during_training is enabled. Is required if evaluate_during_training is enabled.</p>\n</li>\n<li>\n<p><code>**kwargs</code>: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\nA metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>None</li>\n</ul>\n<p><strong><code>eval_model(self, eval_df, output_dir=None, verbose=False)</code></strong></p>\n<p>Evaluates the model on eval_file. Saves results to output_dir.</p>\n<p>Args:</p>\n<ul>\n<li>\n<p><code>eval_file</code>: Path to JSON file containing evaluation data. The model will be evaluated on this file.</p>\n</li>\n<li>\n<p><code>output_dir</code>: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p><code>verbose</code>: If verbose, results will be printed to the console on completion of evaluation.</p>\n</li>\n<li>\n<p><code>**kwargs</code>: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\nA metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>\n<p><code>result</code>: Dictionary containing evaluation results. (correct, similar, incorrect)</p>\n</li>\n<li>\n<p><code>text</code>: A dictionary containing the 3 dictionaries correct_text, similar_text (the predicted answer is a substring of the correct answer or vise versa), incorrect_text.</p>\n</li>\n</ul>\n<p><strong><code>predict(self, to_predict)</code></strong></p>\n<p>Performs predictions on a list of text.</p>\n<p>Args:</p>\n<ul>\n<li><code>to_predict</code>: A python list of python dicts containing contexts and questions to be sent to the model for prediction.</li>\n</ul>\n<pre><span class=\"n\">E</span><span class=\"o\">.</span><span class=\"n\">g</span><span class=\"p\">:</span> <span class=\"n\">predict</span><span class=\"p\">([</span>\n    <span class=\"p\">{</span>\n        <span class=\"s1\">'context'</span><span class=\"p\">:</span> <span class=\"s2\">\"Some context as a demo\"</span><span class=\"p\">,</span>\n        <span class=\"s1\">'qas'</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"p\">{</span><span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"s1\">'0'</span><span class=\"p\">,</span> <span class=\"s1\">'question'</span><span class=\"p\">:</span> <span class=\"s1\">'What is the context here?'</span><span class=\"p\">},</span>\n            <span class=\"p\">{</span><span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"s1\">'1'</span><span class=\"p\">,</span> <span class=\"s1\">'question'</span><span class=\"p\">:</span> <span class=\"s1\">'What is this for?'</span><span class=\"p\">}</span>\n        <span class=\"p\">]</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">])</span>\n</pre>\n<ul>\n<li><code>n_best_size</code> (Optional): Number of predictions to return. args['n_best_size'] will be used if not specified.</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li><code>preds</code>: A python list containg the predicted answer, and id for each question in to_predict.</li>\n</ul>\n<p><strong><code>train(self, train_dataset, output_dir, show_running_loss=True, eval_file=None)</code></strong></p>\n<p>Trains the model on train_dataset.\n<em>Utility function to be used by the train_model() method. Not intended to be used directly.</em></p>\n<p><strong><code>evaluate(self, eval_df, output_dir, , verbose=False)</code></strong></p>\n<p>Evaluates the model on eval_df.\n<em>Utility function to be used by the eval_model() method. Not intended to be used directly</em></p>\n<p><strong><code>load_and_cache_examples(self, examples, evaluate=False, no_cache=False, output_examples=False)</code></strong></p>\n<p>Converts a list of InputExample objects to a TensorDataset containing InputFeatures. Caches the InputFeatures.\n<em>Utility function for train() and eval() methods. Not intended to be used directly</em></p>\n<h3>Additional attributes for Question Answering tasks</h3>\n<p>QuestionAnsweringModel has a few additional attributes in its <code>args</code> dictionary, given below with their default values.</p>\n<pre>  <span class=\"s1\">'doc_stride'</span><span class=\"p\">:</span> <span class=\"mi\">384</span><span class=\"p\">,</span>\n  <span class=\"s1\">'max_query_length'</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span>\n  <span class=\"s1\">'n_best_size'</span><span class=\"p\">:</span> <span class=\"mi\">20</span><span class=\"p\">,</span>\n  <span class=\"s1\">'max_answer_length'</span><span class=\"p\">:</span> <span class=\"mi\">100</span><span class=\"p\">,</span>\n  <span class=\"s1\">'null_score_diff_threshold'</span><span class=\"p\">:</span> <span class=\"mf\">0.0</span>\n</pre>\n<h4><em>doc_stride: int</em></h4>\n<p>When splitting up a long document into chunks, how much stride to take between chunks.</p>\n<h4><em>max_query_length: int</em></h4>\n<p>Maximum token length for questions. Any questions longer than this will be truncated to this length.</p>\n<h4><em>n_best_size: int</em></h4>\n<p>The number of predictions given per question.</p>\n<h4><em>max_answer_length: int</em></h4>\n<p>The maximum token length of an answer that can be generated.</p>\n<h4><em>null_score_diff_threshold: float</em></h4>\n<p>If null_score - best_non_null is greater than the threshold predict null.</p>\n<p><em><a href=\"#table-of-contents\" rel=\"nofollow\">Back to Table of Contents</a></em></p>\n<hr>\n<h2>Language Model Training</h2>\n<p>Supported model types:</p>\n<ul>\n<li>BERT</li>\n<li>CamemBERT</li>\n<li>DistilBERT</li>\n<li>ELECTRA</li>\n<li>GPT-2</li>\n<li>OpenAI-GPT</li>\n<li>RoBERTa</li>\n</ul>\n<h3>Data format</h3>\n<p>The data should simply be placed in a text file. E.g.: <a href=\"https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/\" rel=\"nofollow\">WikiText-2</a></p>\n<h3>Minimal Example For Language Model Fine Tuning</h3>\n<p>The minimal example given below assumes that you have downloaded the WikiText-2 dataset.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.language_modeling</span> <span class=\"kn\">import</span> <span class=\"n\">LanguageModelingModel</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n\n\n<span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"s2\">\"transformers\"</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">WARNING</span><span class=\"p\">)</span>\n\n<span class=\"n\">train_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">\"reprocess_input_data\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"overwrite_output_dir\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">LanguageModelingModel</span><span class=\"p\">(</span><span class=\"s1\">'bert'</span><span class=\"p\">,</span> <span class=\"s1\">'bert-base-cased'</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">train_args</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"s2\">\"wikitext-2/wiki.train.tokens\"</span><span class=\"p\">,</span> <span class=\"n\">eval_file</span><span class=\"o\">=</span><span class=\"s2\">\"wikitext-2/wiki.test.tokens\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"s2\">\"wikitext-2/wiki.test.tokens\"</span><span class=\"p\">)</span>\n</pre>\n<h4>Example (Medium Article)</h4>\n<ul>\n<li><a href=\"https://medium.com/skilai/language-model-fine-tuning-for-pre-trained-transformers-b7262774a7ee?source=friends_link&amp;sk=1f9f834447db7e748ae333c6490064fa\" rel=\"nofollow\">Language Model Fine-tuning</a></li>\n</ul>\n<h3>Minimal Example For Language Model Training From Scratch</h3>\n<p>You can use any text file/files for training a new language model. Setting <code>model_name</code> to <code>None</code> will indicate that the language model should be trained from scratch.</p>\n<p>Required for Language Model Training From Scratch:</p>\n<ul>\n<li><code>train_files</code> must be specifief when creating the <code>LanguagModelingModel</code>. This may be a path to a single file or a list of paths to multiple files.</li>\n<li><code>vocab_size</code> (in args dictionary)</li>\n</ul>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.language_modeling</span> <span class=\"kn\">import</span> <span class=\"n\">LanguageModelingModel</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n\n\n<span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"s2\">\"transformers\"</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">WARNING</span><span class=\"p\">)</span>\n\n<span class=\"n\">train_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">\"reprocess_input_data\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"overwrite_output_dir\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"vocab_size\"</span><span class=\"p\">:</span> <span class=\"mi\">52000</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">LanguageModelingModel</span><span class=\"p\">(</span><span class=\"s1\">'roberta'</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">train_args</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"s2\">\"wikitext-2/wiki.train.tokens\"</span><span class=\"p\">,</span> <span class=\"n\">eval_file</span><span class=\"o\">=</span><span class=\"s2\">\"wikitext-2/wiki.test.tokens\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"s2\">\"wikitext-2/wiki.test.tokens\"</span><span class=\"p\">)</span>\n</pre>\n<h3>Minimal Example For Language Model Training With ELECTRA</h3>\n<p><a href=\"https://openreview.net/pdf?id=r1xMH1BtvB\" rel=\"nofollow\">ELECTRA</a> is a new approach to pretraining Transformer Language Models. This method is comparatively less compute-intensive.</p>\n<p>You can use the <code>save_discriminator()</code> and <code>save_generator()</code> methods to extract the pretrained models. The two models will be saved to <code>&lt;output_dir&gt;/discriminator_model</code> and <code>&lt;output_dir&gt;/generator_model</code> by default.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.language_modeling</span> <span class=\"kn\">import</span> <span class=\"n\">LanguageModelingModel</span>\n<span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n\n\n<span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"s2\">\"transformers\"</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">WARNING</span><span class=\"p\">)</span>\n\n<span class=\"n\">train_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">\"reprocess_input_data\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"overwrite_output_dir\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"vocab_size\"</span><span class=\"p\">:</span> <span class=\"mi\">52000</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">LanguageModelingModel</span><span class=\"p\">(</span><span class=\"s1\">'electra'</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">train_args</span><span class=\"p\">,</span> <span class=\"n\">train_files</span><span class=\"o\">=</span><span class=\"s2\">\"wikitext-2/wiki.train.tokens\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Mixing standard ELECTRA architectures example</span>\n<span class=\"c1\"># model = LanguageModelingModel(</span>\n<span class=\"c1\">#     \"electra\",</span>\n<span class=\"c1\">#     None,</span>\n<span class=\"c1\">#     generator_name=\"google/electra-small-generator\",</span>\n<span class=\"c1\">#     discriminator_name=\"google/electra-large-discriminator\",</span>\n<span class=\"c1\">#     args=train_args,</span>\n<span class=\"c1\">#     train_files=\"wikitext-2/wiki.train.tokens\",</span>\n<span class=\"c1\"># )</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"s2\">\"wikitext-2/wiki.train.tokens\"</span><span class=\"p\">,</span> <span class=\"n\">eval_file</span><span class=\"o\">=</span><span class=\"s2\">\"wikitext-2/wiki.test.tokens\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"s2\">\"wikitext-2/wiki.test.tokens\"</span><span class=\"p\">)</span>\n</pre>\n<h3>Real Dataset Example For Training a Language Model</h3>\n<ul>\n<li><a href=\"https://medium.com/@chaturangarajapakshe/understanding-electra-and-training-an-electra-language-model-3d33e3a9660d?source=friends_link&amp;sk=2b4b4a79954e3d7c84ab863efaea8c65\" rel=\"nofollow\">Esparanto Model trained with ELECTRA</a></li>\n</ul>\n<h3>LanguageModelingModel</h3>\n<p><code>class simpletransformers.language_modeling.LanguageModelingModel (model_type, model_name, generator_name=None, discriminator_name=None, args=None, use_cuda=True, cuda_device=-1)</code><br>\nThis class is used for Question Answering tasks.</p>\n<p><code>Class attributes</code></p>\n<ul>\n<li><code>tokenizer</code>: The tokenizer to be used.</li>\n<li><code>model</code>: The model to be used.\nmodel_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).</li>\n<li><code>device</code>: The device on which the model will be trained and evaluated.</li>\n<li><code>results</code>: A python dict of past evaluation results for the TransformerModel object.</li>\n<li><code>args</code>: A python dict of arguments used for training and evaluation.</li>\n<li><code>cuda_device</code>: (optional) int - Default = -1. Used to specify which GPU should be used.</li>\n</ul>\n<p><code>Parameters</code></p>\n<ul>\n<li><code>model_type</code>: (required) str - The type of model to use.</li>\n<li><code>model_name</code>: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See <a href=\"#current-pretrained-models\" rel=\"nofollow\">Current Pretrained Models</a> for all available models. Set to <code>None</code> for language model training from scratch.</li>\n<li><code>generator_name</code>: (optional) A pretrained model name or path to a directory containing an ELECTRA generator model.</li>\n<li><code>discriminator_name</code>: (optional) A pretrained model name or path to a directory containing an ELECTRA discriminator model.</li>\n<li><code>args</code>: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.</li>\n<li><code>train_files</code>: (optional) List of files to be used when training the tokenizer.</li>\n<li><code>use_cuda</code>: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.</li>\n<li><code>cuda_device</code>: (optional) Specific GPU that should be used. Will use the first available GPU by default.</li>\n</ul>\n<p><code>class methods</code><br>\n<strong><code>train_model(self, train_file, output_dir=None, show_running_loss=True, args=None, eval_file=None, verbose=True,)</code></strong></p>\n<p>Trains the model using 'train_file'</p>\n<p>Args:</p>\n<ul>\n<li>\n<p><code>train_file</code>: Path to text file containing the text to train the language model on.</p>\n</li>\n<li>\n<p><code>output_dir</code> (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p><code>show_running_loss</code> (Optional): Set to False to prevent training loss being printed.</p>\n</li>\n<li>\n<p><code>args</code> (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.</p>\n</li>\n<li>\n<p><code>eval_file</code> (optional): Path to eval file containing the text to evaluate the language model on. Is required if evaluate_during_training is enabled.</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>None</li>\n</ul>\n<p><strong><code>eval_model(self, eval_file, output_dir=None, verbose=True, silent=False,)</code></strong></p>\n<p>Evaluates the model on eval_file. Saves results to output_dir.</p>\n<p>Args:</p>\n<ul>\n<li>\n<p><code>eval_file</code>: Path to eval file containing the text to evaluate the language model on.</p>\n</li>\n<li>\n<p><code>output_dir</code> (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p><code>verbose</code>: If verbose, results will be printed to the console on completion of evaluation.</p>\n</li>\n<li>\n<p><code>silent</code>: If silent, tqdm progress bars will be hidden.</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>\n<p><code>result</code>: Dictionary containing evaluation results. (correct, similar, incorrect)</p>\n</li>\n<li>\n<p><code>text</code>: A dictionary containing the 3 dictionaries correct_text, similar_text (the predicted answer is a substring of the correct answer or vise versa), incorrect_text.</p>\n</li>\n</ul>\n<p>**<code>train_tokenizer(self, train_files, tokenizer_name=None, output_dir=None, use_trained_tokenizer=True)</code></p>\n<p>Train a new tokenizer on <code>train_files</code>.</p>\n<p>Args:</p>\n<ul>\n<li>\n<p><code>train_files</code>: List of files to be used when training the tokenizer.</p>\n</li>\n<li>\n<p><code>tokenizer_name</code>: Name of a pretrained tokenizer or a path to a directory containing a tokenizer.</p>\n</li>\n<li>\n<p><code>output_dir</code> (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p><code>use_trained_tokenizer</code> (optional): Load the trained tokenizer once training completes.</p>\n</li>\n</ul>\n<p>Returns: None</p>\n<p><strong><code>train(self, train_dataset, output_dir, show_running_loss=True, eval_file=None)</code></strong></p>\n<p>Trains the model on train_dataset.\n<em>Utility function to be used by the train_model() method. Not intended to be used directly.</em></p>\n<p><strong><code>evaluate(self, eval_dataset, output_dir, , verbose=False)</code></strong></p>\n<p>Evaluates the model on eval_dataset.\n<em>Utility function to be used by the eval_model() method. Not intended to be used directly</em></p>\n<p><strong><code>load_and_cache_examples(self, examples, evaluate=False, no_cache=False, output_examples=False)</code></strong></p>\n<p>Reads a text file from file_path and creates training features.\n<em>Utility function for train() and eval() methods. Not intended to be used directly</em></p>\n<h3>Additional attributes for Language Modeling tasks</h3>\n<p>LanguageModelingModel has a few additional attributes in its <code>args</code> dictionary, given below with their default values.</p>\n<pre>    <span class=\"s2\">\"dataset_type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"None\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"dataset_class\"</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"custom_tokenizer\"</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"block_size\"</span><span class=\"p\">:</span> <span class=\"mi\">512</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"mlm\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"mlm_probability\"</span><span class=\"p\">:</span> <span class=\"mf\">0.15</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"max_steps\"</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"config_name\"</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"tokenizer_name\"</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"min_frequency\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"special_tokens\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"&lt;s&gt;\"</span><span class=\"p\">,</span> <span class=\"s2\">\"&lt;pad&gt;\"</span><span class=\"p\">,</span> <span class=\"s2\">\"&lt;/s&gt;\"</span><span class=\"p\">,</span> <span class=\"s2\">\"&lt;unk&gt;\"</span><span class=\"p\">,</span> <span class=\"s2\">\"&lt;mask&gt;\"</span><span class=\"p\">],</span>\n    <span class=\"s2\">\"sliding_window\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"stride\"</span><span class=\"p\">:</span> <span class=\"mf\">0.8</span>\n    <span class=\"s2\">\"config\"</span><span class=\"p\">:</span> <span class=\"p\">{},</span>\n    <span class=\"s2\">\"generator_config\"</span><span class=\"p\">:</span> <span class=\"p\">{},</span>\n    <span class=\"s2\">\"discriminator_config\"</span><span class=\"p\">:</span> <span class=\"p\">{},</span>\n</pre>\n<h4><em>dataset_type: str</em></h4>\n<p>Used to specify the Dataset type to be used. The choices are given below.</p>\n<ul>\n<li>\n<p><code>simple</code> (or None) - Each line in the train files are considered to be a single, separate sample. <code>sliding_window</code> can be set to True to\nautomatically split longer sequences into samples of length <code>max_seq_length</code>. Uses multiprocessing for significantly improved performance on multicore systems.</p>\n</li>\n<li>\n<p><code>line_by_line</code> - Treats each line in the train files as a seperate sample.</p>\n</li>\n<li>\n<p><code>text</code> - Treats each file in <code>train_files</code> as a seperate sample.</p>\n</li>\n</ul>\n<p><em>Using <code>simple</code> is recommended.</em></p>\n<h4><em>dataset_class: Subclass of Pytorch Dataset</em></h4>\n<p>A custom dataset class to use.</p>\n<h4><em>block_size: int</em></h4>\n<p>Optional input sequence length after tokenization.\nThe training dataset will be truncated in block of this size for training.\nDefault to the model max input length for single sentence inputs (take into account special tokens).</p>\n<h4><em>mlm: bool</em></h4>\n<p>Train with masked-language modeling loss instead of language modeling</p>\n<h4><em>mlm_probability: float</em></h4>\n<p>Ratio of tokens to mask for masked language modeling loss</p>\n<h4><em>max_steps: int</em></h4>\n<p>If &gt; 0: set total number of training steps to perform. Override num_train_epochs.</p>\n<h4><em>config_name: str</em></h4>\n<p>Name of pretrained config or path to a directory containing a <code>config.json</code> file.</p>\n<h4><em>tokenizer_name: str</em></h4>\n<p>Name of pretrained tokenizer or path to a directory containing tokenizer files.</p>\n<h4><em>min_frequencey: int</em></h4>\n<p>Minimum frequency required for a word to be added to the vocabulary.</p>\n<h4><em>special_tokens: list</em></h4>\n<p>List of special tokens to be used when training a new tokenizer.</p>\n<h4><em>sliding_window: bool</em></h4>\n<p>Whether sliding window technique should be used when preparing data. <em>Only works with SimpleDataset.</em></p>\n<h4><em>stride: float</em></h4>\n<p>A fraction of the <code>max_seq_length</code> to use as the stride when using a sliding window</p>\n<h3><em>config: dict</em></h3>\n<p>Key-values given here will override the default values used in a model <code>Config</code>.</p>\n<h3><em>generator_config: dict</em></h3>\n<p>Key-values given here will override the default values used in an Electra generator model <code>Config</code>.</p>\n<h3><em>discriminator_config: dict</em></h3>\n<p>Key-values given here will override the default values used in an Electra discriminator model <code>Config</code>.</p>\n<p><em><a href=\"#table-of-contents\" rel=\"nofollow\">Back to Table of Contents</a></em></p>\n<hr>\n<h2>Language Generation</h2>\n<p>This section describes how to use Simple Transformers for Langauge Generation.</p>\n<p>Supported model types:</p>\n<ul>\n<li>CTRL</li>\n<li>GPT-2</li>\n<li>OpenAI-GPT</li>\n<li>Transformer-XL</li>\n<li>XLM</li>\n<li>XLNet</li>\n</ul>\n<h4>Minimal Start</h4>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n<span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.language_generation</span> <span class=\"kn\">import</span> <span class=\"n\">LanguageGenerationModel</span>\n\n\n<span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"s2\">\"transformers\"</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">WARNING</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">LanguageGenerationModel</span><span class=\"p\">(</span><span class=\"s2\">\"gpt2\"</span><span class=\"p\">,</span> <span class=\"s2\">\"gpt2\"</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"s2\">\"Let's give a minimal start to the model like\"</span><span class=\"p\">)</span>\n</pre>\n<h4>LanguageGenerationModel</h4>\n<p><code>class simpletransformers.language_generation.language_generation_model.LanguageGenerationModel (self, model_type, model_name, args=None, use_cuda=True, cuda_device=-1, **kwargs)</code><br>\nThis class  is used for Language Generation.</p>\n<p><code>Class attributes</code></p>\n<ul>\n<li><code>tokenizer</code>: The tokenizer to be used.</li>\n<li><code>model</code>: The model to be used.\nmodel_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).</li>\n<li><code>device</code>: The device on which the model will be trained and evaluated.</li>\n<li><code>args</code>: A python dict of arguments used for training and evaluation.</li>\n</ul>\n<ul>\n<li><code>cuda_device</code>: (optional) int - Default = -1. Used to specify which GPU should be used.</li>\n</ul>\n<p><code>Parameters</code></p>\n<ul>\n<li>\n<p><code>model_type</code>: (required) str - The type of model to use.</p>\n</li>\n<li>\n<p><code>model_name</code>: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See <a href=\"#current-pretrained-models\" rel=\"nofollow\">Current Pretrained Models</a> for all available models.</p>\n</li>\n<li>\n<p><code>args</code>: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.</p>\n</li>\n<li>\n<p><code>use_cuda</code>: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.</p>\n</li>\n<li>\n<p><code>cuda_device</code> (optional): Specific GPU that should be used. Will use the first available GPU by default.</p>\n</li>\n<li>\n<p><code>**kwargs</code> (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.</p>\n</li>\n</ul>\n<p><code>class methods</code></p>\n<p><strong><code>generate(self, prompt=None, args=None, verbose=True)</code></strong></p>\n<p>Generate text using a <code>LanguageGenerationModel</code></p>\n<p>Args:</p>\n<ul>\n<li>\n<p><code>prompt</code> (optional): A prompt text for the model. If given, will override args[\"prompt\"]</p>\n</li>\n<li>\n<p><code>args</code> (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.</p>\n</li>\n<li>\n<p><code>verbose</code> (optional): If verbose, generated text will be logged to the console.</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li><code>generated_sequences</code>: Sequences of text generated by the model.</li>\n</ul>\n<h3>Additional attributes for Language Generation tasks</h3>\n<p>LanguageGenerationModel has a few additional attributes in its <code>args</code> dictionary, given below with their default values.</p>\n<pre>    <span class=\"s2\">\"do_sample\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"prompt\"</span><span class=\"p\">:</span> <span class=\"s2\">\"\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"length\"</span><span class=\"p\">:</span> <span class=\"mi\">20</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"stop_token\"</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"temperature\"</span><span class=\"p\">:</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"repetition_penalty\"</span><span class=\"p\">:</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"k\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"p\"</span><span class=\"p\">:</span> <span class=\"mf\">0.9</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"padding_text\"</span><span class=\"p\">:</span> <span class=\"s2\">\"\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"xlm_language\"</span><span class=\"p\">:</span> <span class=\"s2\">\"\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"num_return_sequences\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"config_name\"</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"tokenizer_name\"</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n</pre>\n<h4><em>do_sample: bool</em></h4>\n<p>If set to <code>False</code> greedy decoding is used. Otherwise sampling is used. Defaults to <code>False</code> as defined in <code>configuration_utils.PretrainedConfig</code>.</p>\n<h4><em>prompt: str</em></h4>\n<p>A prompt text for the model.</p>\n<h4><em>length: int</em></h4>\n<p>Length of the text to generate</p>\n<h4><em>stop_token: str</em></h4>\n<p>Token at which text generation is stopped</p>\n<h4><em>temperature: float</em></h4>\n<p>Temperature of 1.0 is the default. Lowering this makes the sampling <em>greedier</em></p>\n<h4><em>repetition_penalty: float</em></h4>\n<p>Primarily useful for CTRL model; in that case, use 1.2</p>\n<h4><em>k: int</em></h4>\n<p><em>k</em> value for top-k sampling</p>\n<h4><em>p: float</em></h4>\n<p><em>p</em> value for top-p (nucleus) sampling</p>\n<h4><em>padding_text: str</em></h4>\n<p>Padding text for Transfo-XL and XLNet.</p>\n<h4><em>xlm_language: str</em></h4>\n<p>Optional language when used with the XLM model.</p>\n<h4><em>num_return_sequences: int</em></h4>\n<p>The number of samples to generate.</p>\n<h4><em>config: dict</em></h4>\n<p>Key-values given here will override the default values used in a model Config.</p>\n<p><em><a href=\"#table-of-contents\" rel=\"nofollow\">Back to Table of Contents</a></em></p>\n<hr>\n<h2>T5 Transformer</h2>\n<p><em>T5 model seems to be working fine, but please open an issue if you run across any problems</em></p>\n<p>The T5 Transformer is an Encoder-Decoder architecture where both the input and targets are text sequences. The task that should be performed on the input is defined by a <em>prefix</em>. This means that the same T5 model can perform multiple tasks.</p>\n<p>You can train the T5 model on a completely new task by simply using a new <code>prefix</code>.</p>\n<h3>Data Format</h3>\n<p>The input to a T5 model has the following pattern;</p>\n<pre><span class=\"s2\">\"&lt;prefix&gt;: &lt;input_text&gt; &lt;/s&gt;\"</span>\n</pre>\n<p>The <em>label</em> sequence has the following pattern;</p>\n<pre><span class=\"s2\">\"&lt;target_sequence&gt; &lt;/s&gt;\"</span>\n</pre>\n<h4>Train and evaluation input formats</h4>\n<p>The inputs to both the <code>train_model()</code> and <code>eval_model()</code> methods should be a Pandas DataFrame containing the 3 columns - <code>prefix</code>, <code>input_text</code>, <code>target_text</code>.</p>\n<ul>\n<li><code>prefix</code>: A string indicating the task to perform. (E.g. <code>\"question\"</code>, <code>\"stsb\"</code>)</li>\n<li><code>input_text</code>: The input text sequence. <code>prefix</code> is automatically prepended to form the full input. (<code>&lt;prefix&gt;: &lt;input_text&gt;</code>)</li>\n<li><code>target_text</code>: The target sequence</li>\n</ul>\n<p>If <code>preprocess_inputs</code> is set to <code>True</code> in the model <code>args</code>, then the <code>&lt; /s&gt;</code> tokens (including preceeding space) and the <code>:</code> <em>(prefix separator including trailing separator)</em> between <code>prefix</code>  and <code>input_text</code> are automatically added. Otherwise, the input DataFrames must contain the <code>&lt; /s&gt;</code> tokens (including preceeding space) and the <code>:</code> <em>(prefix separator including trailing separator)</em>.</p>\n<h4>Prediction data format</h4>\n<p>The prediction data should be a list of strings with the <code>prefix</code> and the <code>:</code> <em>(prefix separator)</em> included.</p>\n<p>If <code>preprocess_inputs</code> is set to <code>True</code> in the model <code>args</code>, then the <code>&lt; /s&gt;</code> token (including preceeding space) is automatically added to each string in the list. Otherwise, the strings must have the <code>&lt; /s&gt;</code> (including preceeding space) must be included.</p>\n<h4>Minimal Start</h4>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.t5</span> <span class=\"kn\">import</span> <span class=\"n\">T5Model</span>\n\n<span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"s2\">\"transformers\"</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">WARNING</span><span class=\"p\">)</span>\n\n\n<span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"convert\"</span><span class=\"p\">,</span> <span class=\"s2\">\"one\"</span><span class=\"p\">,</span> <span class=\"s2\">\"1\"</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"convert\"</span><span class=\"p\">,</span> <span class=\"s2\">\"two\"</span><span class=\"p\">,</span> <span class=\"s2\">\"2\"</span><span class=\"p\">],</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">train_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">train_data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"prefix\"</span><span class=\"p\">,</span> <span class=\"s2\">\"input_text\"</span><span class=\"p\">,</span> <span class=\"s2\">\"target_text\"</span><span class=\"p\">])</span>\n\n<span class=\"n\">eval_data</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"convert\"</span><span class=\"p\">,</span> <span class=\"s2\">\"three\"</span><span class=\"p\">,</span> <span class=\"s2\">\"3\"</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"convert\"</span><span class=\"p\">,</span> <span class=\"s2\">\"four\"</span><span class=\"p\">,</span> <span class=\"s2\">\"4\"</span><span class=\"p\">],</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">eval_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">eval_data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"prefix\"</span><span class=\"p\">,</span> <span class=\"s2\">\"input_text\"</span><span class=\"p\">,</span> <span class=\"s2\">\"target_text\"</span><span class=\"p\">])</span>\n\n<span class=\"n\">eval_df</span> <span class=\"o\">=</span> <span class=\"n\">train_df</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n\n<span class=\"n\">model_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">\"reprocess_input_data\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"overwrite_output_dir\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"max_seq_length\"</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"train_batch_size\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"num_train_epochs\"</span><span class=\"p\">:</span> <span class=\"mi\">200</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"c1\"># Create T5 Model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">T5Model</span><span class=\"p\">(</span><span class=\"s2\">\"t5-base\"</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">model_args</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Train T5 Model on new task</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"n\">train_df</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Evaluate T5 Model on new task</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"n\">eval_df</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Predict with trained T5 model</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([</span><span class=\"s2\">\"convert: four\"</span><span class=\"p\">]))</span>\n</pre>\n<h4>Evaluating with custom metrics</h4>\n<p>You can evaluate the models' generated sequences using custom metric functions (including evaluation during training). However, due to the way T5 outputs are generated, this may be significantly slower than evaluation with other models.</p>\n<p>Note, you must set <code>evaluate_generated_text</code> to <code>True</code> to evaluate generated sequences.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">logging</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.t5</span> <span class=\"kn\">import</span> <span class=\"n\">T5Model</span>\n\n<span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span> <span class=\"o\">=</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">getLogger</span><span class=\"p\">(</span><span class=\"s2\">\"transformers\"</span><span class=\"p\">)</span>\n<span class=\"n\">transformers_logger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"p\">(</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">WARNING</span><span class=\"p\">)</span>\n\n\n<span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"convert\"</span><span class=\"p\">,</span> <span class=\"s2\">\"one\"</span><span class=\"p\">,</span> <span class=\"s2\">\"1\"</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"convert\"</span><span class=\"p\">,</span> <span class=\"s2\">\"two\"</span><span class=\"p\">,</span> <span class=\"s2\">\"2\"</span><span class=\"p\">],</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">train_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">train_data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"prefix\"</span><span class=\"p\">,</span> <span class=\"s2\">\"input_text\"</span><span class=\"p\">,</span> <span class=\"s2\">\"target_text\"</span><span class=\"p\">])</span>\n\n<span class=\"n\">eval_data</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"convert\"</span><span class=\"p\">,</span> <span class=\"s2\">\"three\"</span><span class=\"p\">,</span> <span class=\"s2\">\"3\"</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"convert\"</span><span class=\"p\">,</span> <span class=\"s2\">\"four\"</span><span class=\"p\">,</span> <span class=\"s2\">\"4\"</span><span class=\"p\">],</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">eval_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">eval_data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"prefix\"</span><span class=\"p\">,</span> <span class=\"s2\">\"input_text\"</span><span class=\"p\">,</span> <span class=\"s2\">\"target_text\"</span><span class=\"p\">])</span>\n\n<span class=\"n\">eval_df</span> <span class=\"o\">=</span> <span class=\"n\">train_df</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n\n<span class=\"n\">model_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">\"reprocess_input_data\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"overwrite_output_dir\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"max_seq_length\"</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"train_batch_size\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"num_train_epochs\"</span><span class=\"p\">:</span> <span class=\"mi\">200</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"save_eval_checkpoints\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"save_model_every_epoch\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"c1\"># \"silent\": True,</span>\n    <span class=\"s2\">\"evaluate_generated_text\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"evaluate_during_training\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"evaluate_during_training_verbose\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">T5Model</span><span class=\"p\">(</span><span class=\"s2\">\"t5-base\"</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">model_args</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">count_matches</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">preds</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">preds</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"nb\">sum</span><span class=\"p\">([</span><span class=\"mi\">1</span> <span class=\"k\">if</span> <span class=\"n\">label</span> <span class=\"o\">==</span> <span class=\"n\">pred</span> <span class=\"k\">else</span> <span class=\"mi\">0</span> <span class=\"k\">for</span> <span class=\"n\">label</span><span class=\"p\">,</span> <span class=\"n\">pred</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">preds</span><span class=\"p\">)])</span>\n\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"n\">train_df</span><span class=\"p\">,</span> <span class=\"n\">eval_data</span><span class=\"o\">=</span><span class=\"n\">eval_df</span><span class=\"p\">,</span> <span class=\"n\">matches</span><span class=\"o\">=</span><span class=\"n\">count_matches</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"n\">eval_df</span><span class=\"p\">,</span> <span class=\"n\">matches</span><span class=\"o\">=</span><span class=\"n\">count_matches</span><span class=\"p\">))</span>\n</pre>\n<h4>T5Model</h4>\n<p><code>class simpletransformers.t5.t5_model.T5Model (self, model_name, args=None, use_cuda=True, cuda_device=-1, **kwargs)</code><br>\nThis class  is used for the T5 Transformer.</p>\n<p><code>Class attributes</code></p>\n<ul>\n<li><code>tokenizer</code>: The tokenizer to be used.</li>\n<li><code>model</code>: The model to be used.\nmodel_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).</li>\n<li><code>device</code>: The device on which the model will be trained and evaluated.</li>\n<li><code>args</code>: A python dict of arguments used for training and evaluation.</li>\n</ul>\n<ul>\n<li><code>cuda_device</code>: (optional) int - Default = -1. Used to specify which GPU should be used.</li>\n</ul>\n<p><code>Parameters</code></p>\n<ul>\n<li>\n<p><code>model_name</code>: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See <a href=\"#current-pretrained-models\" rel=\"nofollow\">Current Pretrained Models</a> for all available models.</p>\n</li>\n<li>\n<p><code>args</code>: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.</p>\n</li>\n<li>\n<p><code>cuda_device</code>: (optional) Specific GPU that should be used. Will use the first available GPU by default.</p>\n</li>\n<li>\n<p><code>use_cuda</code>: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.</p>\n</li>\n<li>\n<p><code>**kwargs</code>: (optional) For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.</p>\n</li>\n</ul>\n<p><code>class methods</code><br>\n<strong><code>train_model(self, train_data, output_dir=None, show_running_loss=True, args=None, eval_df=None)</code></strong></p>\n<p>Trains the model using 'train_data'</p>\n<p>Args:</p>\n<ul>\n<li>\n<p><code>train_data</code>: Pandas DataFrame containing the 3 columns - <code>prefix</code>, <code>input_text</code>, <code>target_text</code>.\n- <code>prefix</code>: A string indicating the task to perform. (E.g. <code>\"question\"</code>, <code>\"stsb\"</code>)\n- <code>input_text</code>: The input text sequence. <code>prefix</code> is automatically prepended to form the full input. (<code>&lt;prefix&gt;: &lt;input_text&gt;</code>)\n- <code>target_text</code>: The target sequence</p>\n</li>\n<li>\n<p><code>output_dir</code> (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p><code>args</code> (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.</p>\n</li>\n<li>\n<p>show_running_loss (optional): Set to False to disable printing running training loss to the terminal.</p>\n</li>\n<li>\n<p><code>eval_data</code> (optional): A DataFrame against which evaluation will be performed when <code>evaluate_during_training</code> is enabled. Is required if <code>evaluate_during_training</code> is enabled.</p>\n</li>\n<li>\n<p><code>**kwargs</code>: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\nA metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs will be lists of strings. Note that this will slow down training significantly as the predicted sequences need to be generated.</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>None</li>\n</ul>\n<p><strong><code>eval_model(self, eval_data, output_dir=None, verbose=True, silent=False)</code></strong></p>\n<p>Evaluates the model on eval_data. Saves results to output_dir.</p>\n<p>Args:</p>\n<ul>\n<li>\n<p>eval_data: Pandas DataFrame containing the 3 columns - <code>prefix</code>, <code>input_text</code>, <code>target_text</code>.</p>\n<ul>\n<li><code>prefix</code>: A string indicating the task to perform. (E.g. <code>\"question\"</code>, <code>\"stsb\"</code>)</li>\n<li><code>input_text</code>: The input text sequence. <code>prefix</code> is automatically prepended to form the full input. (<code>&lt;prefix&gt;: &lt;input_text&gt;</code>)</li>\n<li><code>target_text</code>: The target sequence</li>\n</ul>\n</li>\n<li>\n<p>output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p>verbose: If verbose, results will be printed to the console on completion of evaluation.</p>\n</li>\n<li>\n<p>silent: If silent, tqdm progress bars will be hidden.</p>\n</li>\n<li>\n<p><code>**kwargs</code>: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>result: Dictionary containing evaluation results.</li>\n</ul>\n<p><strong><code>predict(self, to_predict)</code></strong></p>\n<p>Performs predictions on a list of text.</p>\n<p>Args:</p>\n<ul>\n<li>to_predict: A python list of text (str) to be sent to the model for prediction. Note that the prefix should be prepended to the text.</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>preds: A python list of the generated sequences.</li>\n</ul>\n<p><strong><code>train(self, train_dataset, output_dir)</code></strong></p>\n<p>Trains the model on train_dataset.\n<em>Utility function to be used by the train_model() method. Not intended to be used directly.</em></p>\n<p><strong><code>evaluate(self, eval_dataset, output_dir, prefix=\"\")</code></strong></p>\n<p>Evaluates the model on eval_dataset.\n<em>Utility function to be used by the eval_model() method. Not intended to be used directly</em></p>\n<p><strong><code>load_and_cache_examples(self, examples, evaluate=False)</code></strong></p>\n<p>Creates a <code>T5Dataset</code> from data.</p>\n<p><em>Utility function for train() and eval() methods. Not intended to be used directly</em></p>\n<p><strong><code>compute_metrics(self, preds, labels, **kwargs):</code></strong></p>\n<p>Computes the evaluation metrics for the model predictions.</p>\n<p>Args:</p>\n<ul>\n<li><code>labels</code>: List of target sequences</li>\n<li><code>preds</code>: List of model generated outputs</li>\n<li><code>**kwargs</code>: Custom metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>result: Dictionary containing evaluation results.</li>\n</ul>\n<h3>Additional attributes for T5 Model</h3>\n<p><code>T5Model</code> has a few additional attributes in its <code>args</code> dictionary, given below with their default values.</p>\n<pre><span class=\"p\">{</span>\n    <span class=\"s2\">\"dataset_class\"</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"max_steps\"</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"evaluate_generated_text\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"num_beams\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"repetition_penalty\"</span><span class=\"p\">:</span> <span class=\"mf\">2.5</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"length_penalty\"</span><span class=\"p\">:</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"preprocess_inputs\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n</pre>\n<h4><em>dataset_class: Subclass of Pytorch Dataset</em></h4>\n<p>A custom dataset class to use.</p>\n<h4><em>do_sample: bool</em></h4>\n<p>If set to <code>False</code> greedy decoding is used. Otherwise sampling is used. Defaults to <code>False</code> as defined in <code>configuration_utils.PretrainedConfig</code>.</p>\n<h4><em>max_steps: int</em></h4>\n<p>Maximum number of training steps. Will override the effect of <code>num_train_epochs</code>.</p>\n<h4><em>evaluate_generated_text: bool</em></h4>\n<p>Generate sequences for evaluation.</p>\n<h4><em>num_beams: int</em></h4>\n<p>Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.</p>\n<h4><em>max_lemgth: int</em></h4>\n<p>The max length of the sequence to be generated.  Between <code>min_length</code> and infinity. Default to 20.</p>\n<h4><em>repetition_penalty: float</em></h4>\n<p>The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.</p>\n<h4><em>length_penalty: float</em></h4>\n<p>Exponential penalty to the length. Default to 1.</p>\n<h4><em>early_stopping: bool</em></h4>\n<p>if set to <code>True</code> beam search is stopped when at least <code>num_beams</code> sentences finished per batch.</p>\n<h4><em>preprocess_inputs: bool</em></h4>\n<p>Automatically add <code>:</code> and <code>&lt; /s&gt;</code> tokens to <code>train_model()</code> and <code>eval_model()</code> inputs. Automatically add <code>&lt; /s&gt;</code> to each string in <code>to_predict</code> in <code>predict()</code>.</p>\n<p><em><a href=\"#table-of-contents\" rel=\"nofollow\">Back to Table of Contents</a></em></p>\n<hr>\n<h2>Conversational AI</h2>\n<p>Chatbot creation based on the Hugging Face <a href=\"https://github.com/huggingface/transfer-learning-conv-ai\" rel=\"nofollow\">State-of-the-Art Conversational AI</a>.</p>\n<p>Supported model types:</p>\n<ul>\n<li>GPT</li>\n<li>GPT2</li>\n</ul>\n<h3>Data format</h3>\n<p>Data format follows the <a href=\"http://arxiv.org/abs/1801.07243\" rel=\"nofollow\">Facebook Persona-Chat</a> format. A JSON formatted version by Hugging Face is found <a href=\"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\" rel=\"nofollow\">here</a>. The JSON file is directly compatible with this library (and it will be automatically downloaded and used if no dataset is specified).</p>\n<p>Each entry in personachat is a <strong>dict</strong> with two keys <code>personality</code> and <code>utterances</code>, the dataset is a list of entries.</p>\n<ul>\n<li><code>personality</code>:  <strong>list of strings</strong> containing the personality of the agent</li>\n<li><code>utterances</code>: <strong>list of dictionaries</strong>, each of which has two keys which are <strong>lists of strings</strong>.\n<ul>\n<li><code>candidates</code>: [next_utterance_candidate_1, ..., next_utterance_candidate_19]<br>\nThe last candidate is the ground truth response observed in the conversational data</li>\n<li><code>history</code>: [dialog_turn_0, ... dialog_turn N], where N is an odd number since the other user starts every conversation.</li>\n</ul>\n</li>\n</ul>\n<p>Preprocessing:</p>\n<ul>\n<li>Spaces before periods at end of sentences</li>\n<li>everything lowercase</li>\n</ul>\n<p>Example train data:</p>\n<pre><span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n        <span class=\"nt\">\"personality\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"s2\">\"i like computers .\"</span><span class=\"p\">,</span>\n            <span class=\"s2\">\"i like reading books .\"</span><span class=\"p\">,</span>\n            <span class=\"s2\">\"i like talking to chatbots .\"</span><span class=\"p\">,</span>\n            <span class=\"s2\">\"i love listening to classical music .\"</span>\n        <span class=\"p\">],</span>\n        <span class=\"nt\">\"utterances\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"p\">{</span>\n                <span class=\"nt\">\"candidates\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                    <span class=\"s2\">\"i try to wear all black every day . it makes me feel comfortable .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"well nursing stresses you out so i wish luck with sister\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"yeah just want to pick up nba nfl getting old\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"i really like celine dion . what about you ?\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"no . i live near farms .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"mother taught me to cook ! we are looking for an exterminator .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"i enjoy romantic movie . what is your favorite season ? mine is summer .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"editing photos takes a lot of work .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"you must be very fast . hunting is one of my favorite hobbies .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"hi there . i'm feeling great! how about you ?\"</span>\n                <span class=\"p\">],</span>\n                <span class=\"nt\">\"history\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                    <span class=\"s2\">\"hi , how are you ?\"</span>\n                <span class=\"p\">]</span>\n            <span class=\"p\">},</span>\n            <span class=\"p\">{</span>\n                <span class=\"nt\">\"candidates\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                    <span class=\"s2\">\"i have trouble getting along with family .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"i live in texas , what kind of stuff do you do in \"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"toronto ?\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"that's so unique ! veganism and line dancing usually don't mix !\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"no , it isn't that big . do you travel a lot\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"that's because they are real ; what do you do for work ?\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"i am lazy all day lol . my mom wants me to get a job and move out\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"i was born on arbor day , so plant a tree in my name\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"okay , i should not tell you , its against the rules \"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"i like to talk to chatbots too ! do you know why ? .\"</span>\n                <span class=\"p\">],</span>\n                <span class=\"nt\">\"history\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                    <span class=\"s2\">\"hi , how are you ?\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"hi there . i'm feeling great! how about you ?\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"not bad ! i am trying out this chatbot .\"</span>\n                <span class=\"p\">]</span>\n            <span class=\"p\">},</span>\n            <span class=\"p\">{</span>\n                <span class=\"nt\">\"candidates\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                    <span class=\"s2\">\"ll something like that . do you play games ?\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"does anything give you relief ? i hate taking medicine for mine .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"i decorate cakes at a local bakery ! and you ?\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"do you eat lots of meat\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"i am so weird that i like to collect people and cats\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"how are your typing skills ?\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"yeah . i am headed to the gym in a bit to weight lift .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"yeah you have plenty of time\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"metal is my favorite , but i can accept that people listen to country . haha\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"that's why you desire to be controlled . let me control you person one .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"two dogs they are the best , how about you ?\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"you do art ? what kind of art do you do ?\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"i love watching baseball outdoors on sunny days .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"oh i see . do you ever think about moving ? i do , it is what i want .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"because i am a chatbot too, silly !\"</span>\n                <span class=\"p\">],</span>\n                <span class=\"nt\">\"history\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                    <span class=\"s2\">\"hi , how are you ?\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"hi there . i'm feeling great! how about you ?\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"not bad ! i am trying out this chatbot .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"i like to talk to chatbots too ! do you know why ? .\"</span><span class=\"p\">,</span>\n                    <span class=\"s2\">\"no clue, why don't you tell me ?\"</span>\n                <span class=\"p\">]</span>\n            <span class=\"p\">}</span>\n        <span class=\"p\">]</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">]</span>\n</pre>\n<h3>Minimal Example</h3>\n<p>You can download the pretrained (OpenAI GPT based) Conversation AI model open-sourced by Hugging Face <a href=\"https://s3.amazonaws.com/models.huggingface.co/transfer-learning-chatbot/gpt_personachat_cache.tar.gz\" rel=\"nofollow\">here</a>.</p>\n<p>For the minimal example given below, you can download the model and extract it to <code>gpt_personachat_cache</code>. Note that you can use any of the other GPT or GPT-2 models but they will require more training.</p>\n<p>You will also need to create the JSON file given in the Data Format section and save it as <code>data/minimal_train.json</code>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.conv_ai</span> <span class=\"kn\">import</span> <span class=\"n\">ConvAIModel</span>\n\n\n<span class=\"n\">train_args</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">\"num_train_epochs\"</span><span class=\"p\">:</span> <span class=\"mi\">50</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"save_model_every_epoch\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"c1\"># Create a ConvAIModel</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ConvAIModel</span><span class=\"p\">(</span><span class=\"s2\">\"gpt\"</span><span class=\"p\">,</span> <span class=\"s2\">\"gpt_personachat_cache\"</span><span class=\"p\">,</span> <span class=\"n\">use_cuda</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">train_args</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Train the model</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"s2\">\"data/minimal_train.json\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Evaluate the model</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Interact with the trained model.</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">interact</span><span class=\"p\">()</span>\n</pre>\n<p>The <code>interact()</code> method can be given a list of Strings which will be used to build a personality. If a list of Strings is not given, a random personality will be chosen from PERSONA-CHAT instead.</p>\n<h3>Real Dataset Example</h3>\n<ul>\n<li><a href=\"https://medium.com/@chaturangarajapakshe/how-to-train-your-chatbot-with-simple-transformers-da25160859f4?sk=edd04e406e9a3523fcfc46102529e775\" rel=\"nofollow\">Persona-Chat Conversational AI</a></li>\n</ul>\n<h3>ConvAIModel</h3>\n<p><code>class simpletransformers.conv_ai.ConvAIModel ( model_type, model_name, args=None, use_cuda=True, cuda_device=-1, **kwargs)</code><br>\nThis class is used to build Conversational AI.</p>\n<p><code>Class attributes</code></p>\n<ul>\n<li><code>tokenizer</code>: The tokenizer to be used.</li>\n<li><code>model</code>: The model to be used.\nmodel_name: Default Transformer model name or path to Transformer model file (pytorch_model.bin).</li>\n<li><code>device</code>: The device on which the model will be trained and evaluated.</li>\n<li><code>results</code>: A python dict of past evaluation results for the TransformerModel object.</li>\n<li><code>args</code>: A python dict of arguments used for training and evaluation.</li>\n</ul>\n<p><code>Parameters</code></p>\n<ul>\n<li><code>model_type</code>: (required) str - The type of model to use.</li>\n<li><code>model_name</code>: (required) str - The exact model to use. Could be a pretrained model name or path to a directory containing a model. See <a href=\"#current-pretrained-models\" rel=\"nofollow\">Current Pretrained Models</a> for all available models.</li>\n<li><code>args</code>: (optional) python dict - A dictionary containing any settings that should be overwritten from the default values.</li>\n<li><code>use_cuda</code>: (optional) bool - Default = True. Flag used to indicate whether CUDA should be used.</li>\n<li><code>cuda_device</code>: (optional) int - Default = -1. Used to specify which GPU should be used.</li>\n</ul>\n<p><code>class methods</code><br>\n<strong><code>train_model(self, train_file=None, output_dir=None, show_running_loss=True, args=None, eval_file=None)</code></strong></p>\n<p>Trains the model using 'train_file'</p>\n<p>Args:</p>\n<ul>\n<li>\n<p>train_df: ath to JSON file containing training data. The model will be trained on this file.\noutput_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p>output_dir (optional): The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p>show_running_loss (Optional): Set to False to prevent training loss being printed.</p>\n</li>\n<li>\n<p>args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.</p>\n</li>\n<li>\n<p>eval_file (optional): Evaluation data against which evaluation will be performed when evaluate_during_training is enabled. If not given when evaluate_during_training is enabled, the evaluation data from PERSONA-CHAT will be used.</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>None</li>\n</ul>\n<p><strong><code>eval_model(self, eval_file, output_dir=None, verbose=True, silent=False)</code></strong></p>\n<p>Evaluates the model on eval_file. Saves results to output_dir.</p>\n<p>Args:</p>\n<ul>\n<li>\n<p>eval_file: Path to JSON file containing evaluation data. The model will be evaluated on this file.<br>\nIf not given, eval dataset from PERSONA-CHAT will be used.</p>\n</li>\n<li>\n<p>output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</p>\n</li>\n<li>\n<p>verbose: If verbose, results will be printed to the console on completion of evaluation.</p>\n</li>\n<li>\n<p>silent: If silent, tqdm progress bars will be hidden.</p>\n</li>\n</ul>\n<p>Returns:</p>\n<ul>\n<li>\n<p>result: Dictionary containing evaluation results. (correct, similar, incorrect)</p>\n</li>\n<li>\n<p>text: A dictionary containing the 3 dictionaries correct_text, similar_text (the predicted answer is a substring of the correct answer or vise versa), incorrect_text.</p>\n</li>\n</ul>\n<p><strong><code>interact(self, personality=None)</code></strong></p>\n<p>Interact with a model in the terminal.</p>\n<p>Args:</p>\n<ul>\n<li>personality (optional): A list of sentences that the model will use to build a personality.<br>\nIf not given, a random personality from PERSONA-CHAT will be picked.</li>\n</ul>\n<pre><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">interact</span><span class=\"p\">(</span>\n    <span class=\"n\">personality</span><span class=\"o\">=</span><span class=\"p\">[</span>\n        <span class=\"s2\">\"i like computers .\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"i like reading books .\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"i love classical music .\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"i am very social .\"</span>\n    <span class=\"p\">]</span>\n<span class=\"p\">)</span>\n</pre>\n<p>Returns:</p>\n<ul>\n<li>None</li>\n</ul>\n<p><strong><code>train(self, train_dataloader, output_dir, show_running_loss=True, eval_dataloader=None, verbose=verbose)</code></strong></p>\n<p>Trains the model on train_dataset.\n<em>Utility function to be used by the train_model() method. Not intended to be used directly.</em></p>\n<p><strong><code>evaluate(self, eval_file, output_dir, verbose=True, silent=False)</code></strong></p>\n<p>Evaluates the model on eval_file.\n<em>Utility function to be used by the eval_model() method. Not intended to be used directly</em></p>\n<p><strong><code>load_and_cache_examples(self, dataset_path=None, evaluate=False, no_cache=False, verbose=True, silent=False)</code></strong></p>\n<p>Loads, tokenizes, and prepares data for training and/or evaluation.\n<em>Utility function for train() and eval() methods. Not intended to be used directly</em></p>\n<h3>Additional attributes for Conversational AI</h3>\n<p>ConvAIModel has a few additional attributes in its <code>args</code> dictionary, given below with their default values.</p>\n<pre>    <span class=\"s2\">\"num_candidates\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"personality_permutations\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"max_history\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"lm_coef\"</span><span class=\"p\">:</span> <span class=\"mf\">2.0</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"mc_coef\"</span><span class=\"p\">:</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"no_sample\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"max_length\"</span><span class=\"p\">:</span> <span class=\"mi\">20</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"min_length\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"temperature\"</span><span class=\"p\">:</span> <span class=\"mf\">0.7</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"top_k\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"top_p\"</span><span class=\"p\">:</span> <span class=\"mf\">0.9</span><span class=\"p\">,</span>\n</pre>\n<h4><em>num_candidates: int</em></h4>\n<p>Number of candidates for training</p>\n<h4><em>personality_permutations: int</em></h4>\n<p>Number of permutations of personality sentences\".</p>\n<h4><em>max_history: int</em></h4>\n<p>Number of previous exchanges to keep in history</p>\n<h4><em>lm_coef: int</em></h4>\n<p>LM loss coefficient</p>\n<h4><em>mc_coef: int</em></h4>\n<p>Multiple-choice loss coefficient</p>\n<h4><em>no_sample: bool</em></h4>\n<p>Set to use greedy decoding instead of sampling</p>\n<h4><em>max_length: int</em></h4>\n<p>Maximum length of the output utterances</p>\n<h4><em>min_length: int</em></h4>\n<p>Minimum length of the output utterances</p>\n<h4><em>temperature: float</em></h4>\n<p>Sampling softmax temperature</p>\n<h4><em>top_k: float</em></h4>\n<p>Filter top-k tokens before sampling (&lt;=0: no filtering)</p>\n<h4><em>top_p: float</em></h4>\n<p>Nucleus filtering (top-p) before sampling (&lt;=0.0: no filtering)</p>\n<p><em><a href=\"#table-of-contents\" rel=\"nofollow\">Back to Table of Contents</a></em></p>\n<hr>\n<h2>Multi-Modal Classification</h2>\n<p>Multi-Modal Classification fuses text and image data. This is performed using multi-modal bitransformer models\nintroduced in the paper <a href=\"https://arxiv.org/abs/1909.02950\" rel=\"nofollow\">Supervised Multimodal Bitransformers for Classifying Images and Text</a>.</p>\n<p>Supported model types:</p>\n<ul>\n<li>BERT</li>\n</ul>\n<h3>Data format</h3>\n<p>There are several possible input formats you may use. The input formats are inspired by the <a href=\"http://lisi1.unal.edu.co/mmimdb/\" rel=\"nofollow\">MM-IMDb</a> format.\nNote that several options for data preprocessing have been added for convenience and flexibility when dealing with\ncomplex datasets which can be found after the input format definitions.</p>\n<h4>1 - Directory based</h4>\n<p>Each subset of data (E.g: train and test) should be in its own directory. The path to the directory can then be given\ndirectly to either <code>train_model()</code> or <code>eval_model()</code>.</p>\n<p>Each data sample should have a text portion and an image associated with it (and a label/labels for training and evaluation data).\nThe text for each sample should be in a separate JSON file. The JSON file may contain other fields in addition to the text\nitself but they will be ignored. The image associated with each sample should be in the same directory and both the text\nand the image must have the same identifier except for the file extension (E.g: 000001.json and 000001.jpg).</p>\n<h4>2 - Directory and file list</h4>\n<p>All data (including both train and test data) should be in the same directory. The path to this directory should be given\nto both <code>train_model()</code> and <code>eval_model()</code>. A second argument, <code>files_list</code> specifies which files should be taken from\nthe directory. <code>files_list</code> can be a Python list or the path to a JSON file containing the list of files.</p>\n<p>Each data sample should have a text portion and an image associated with it (and a label/labels for training and evaluation data).\nThe text for each sample should be in a separate JSON file. The JSON file may contain other fields in addition to the text\nitself but they will be ignored. The image associated with each sample should be in the same directory and both the text\nand the image must have the same identifier except for the file extension (E.g: 000001.json and 000001.jpg).</p>\n<h4>3 - Pandas DataFrame</h4>\n<p>Data can also be given in a Pandas DataFrame. When using this format, the <code>image_path</code> argument must be specified and\nit should be a String of the path to the directory containing the images. The DataFrame should contain at least 3\ncolumns as detailed below.</p>\n<ul>\n<li><code>text</code> (str) - The text associated with the sample.</li>\n<li><code>images</code> (str) - The relative path to the image file from <code>image_path</code> directory.</li>\n<li><code>labels</code> (str) - The label (or list of labels for multilabel tasks) associated with the sample.</li>\n</ul>\n<h3>Using custom names for column names or fields in JSON files</h3>\n<p>By default, Simple Transformers will look for column/field names <code>text</code>, <code>images</code>, and <code>labels</code>. However, you can define\nyour own names to use in place of these names. This behaviour is controlled using the three attributes <code>text_label</code>, <code>labels_label</code>,\nand <code>images_label</code> in the <code>args</code> dictionary.</p>\n<p>You can set your custom names when creating the model by assigning the custom name to the corresponding attribute in the\n<code>args</code> dictionary.</p>\n<p>You can also change these values at training and/or evaluation time (but not with the <code>predict()</code> method) by passing the\nnames to the arguments <code>text_label</code>, <code>labels_label</code>, and <code>images_label</code>. Note that the change will persist even after\nthe method call terminates. That is, the <code>args</code> dictionary of the model itself will be modified.</p>\n<h3>Specifying the file type extension for image and text files</h3>\n<p>By default, Simple Transformers will assume that any paths will also include the file type extension (E.g: .json or .jpg).\nAlternatively, you can specify the extensions using the <code>image_type_extension</code> and <code>data_type_extension</code> attributes (for\nimage file extensions and text file extensions respectively) in the <code>args</code> dictionary.</p>\n<p>This too can be done when creating the model or when running the <code>train_model()</code> or <code>eval_model()</code> methods. The changes\nwill persist in the <code>args</code> dictionary when using these methods.</p>\n<p>The <code>image_type_extension</code> can be specified when using the <code>predict()</code> method but the change WILL NOT persist.</p>\n<h3>Label formats</h3>\n<p>With Multi-Modal Classification, labels are always given as strings. You may specify a list of labels by passing in the\nlist to <code>label_list</code> argument when creating the model. If <code>label_list</code> is given, <code>num_labels</code> is not required.</p>\n<p>If <code>label_list</code> is not given, <code>num_labels</code> is required and the labels should be Strings starting from <code>\"0\"</code> up to\n<code>\"&lt;num_labels&gt;\"</code>.</p>\n<h3>Creating a Model</h3>\n<p>Create a <code>MultiModalClassificationModel</code>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.classification.multi_modal_classification_model</span> <span class=\"kn\">import</span> <span class=\"n\">MultiModalClassificationModel</span>\n\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">MultiModalClassificationModel</span><span class=\"p\">(</span><span class=\"s2\">\"bert\"</span><span class=\"p\">,</span> <span class=\"s2\">\"bert-base-uncased\"</span><span class=\"p\">)</span>\n</pre>\n<p>Available arguments:</p>\n<pre><span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">Args:</span>\n<span class=\"sd\">    model_type: The type of model (bert, xlnet, xlm, roberta, distilbert, albert)</span>\n<span class=\"sd\">    model_name: Default Transformer model name or path to a directory containing Transformer model file (pytorch_model.bin).</span>\n<span class=\"sd\">    multi_label (optional): Set to True for multi label tasks.</span>\n<span class=\"sd\">    label_list (optional) : A list of all the labels (str) in the dataset.</span>\n<span class=\"sd\">    num_labels (optional): The number of labels or classes in the dataset.</span>\n<span class=\"sd\">    pos_weight (optional): A list of length num_labels containing the weights to assign to each label for loss calculation.</span>\n<span class=\"sd\">    args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.</span>\n<span class=\"sd\">    use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.</span>\n<span class=\"sd\">    cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.</span>\n<span class=\"sd\">    **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.</span>\n<span class=\"sd\">\"\"\"</span>\n</pre>\n<h3>Training a Model</h3>\n<p>Use the <code>train_model()</code> method to train. You can use the <code>auto_weights</code> feature to balance out unbalanced datasets.</p>\n<p>Available arguments:</p>\n<pre><span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">Args:</span>\n<span class=\"sd\">    data: Path to data directory containing text files (JSON) and image files OR a Pandas DataFrame.</span>\n<span class=\"sd\">        If a DataFrame is given, it should contain the columns [text, labels, images]. When using a DataFrame,</span>\n<span class=\"sd\">        image_path MUST be specified. The image column of the DataFrame should contain the relative path from</span>\n<span class=\"sd\">        image_path to the image.</span>\n<span class=\"sd\">        E.g:</span>\n<span class=\"sd\">            For an image file 1.jpeg located in \"data/train/\";</span>\n<span class=\"sd\">                image_path = \"data/train/\"</span>\n<span class=\"sd\">                images = \"1.jpeg\"</span>\n<span class=\"sd\">    files_list (optional): If given, only the files specified in this list will be taken from data directory.</span>\n<span class=\"sd\">        files_list can be a Python list or the path (str) to a JSON file containing a list of files.</span>\n<span class=\"sd\">    image_path (optional): Must be specified when using DataFrame as input. Path to the directory containing the</span>\n<span class=\"sd\">        images.</span>\n<span class=\"sd\">    text_label (optional): Column name to look for instead of the default \"text\"</span>\n<span class=\"sd\">    labels_label (optional): Column name to look for instead of the default \"labels\"</span>\n<span class=\"sd\">    images_label (optional): Column name to look for instead of the default \"images\"</span>\n<span class=\"sd\">    image_type_extension (optional): If given, this will be added to the end of each value in \"images\".</span>\n<span class=\"sd\">    data_type_extension (optional): If given, this will be added to the end of each value in \"files_list\".</span>\n<span class=\"sd\">    auto_weights (optional): If True, weights will be used to balance the classes.</span>\n<span class=\"sd\">    output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</span>\n<span class=\"sd\">    show_running_loss (optional): Set to False to prevent running loss from being printed to console. Defaults to True.</span>\n<span class=\"sd\">    args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.</span>\n<span class=\"sd\">    eval_data (optional): A DataFrame against which evaluation will be performed when evaluate_during_training is enabled. Is required if evaluate_during_training is enabled.</span>\n<span class=\"sd\">    **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). E.g. f1=sklearn.metrics.f1_score.</span>\n<span class=\"sd\">                A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.</span>\n<span class=\"sd\">\"\"\"</span>\n</pre>\n<h3>Evaluating a Model</h3>\n<p>Use the <code>eval_model()</code> method to evaluate. You can load a saved model by giving the path to the model directory as\n<code>model_name</code>. Note that you need to provide the same arguments when loading a saved model as you did when creating the\noriginal model.</p>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">MultiModalClassificationModel</span><span class=\"p\">(</span><span class=\"s2\">\"bert\"</span><span class=\"p\">,</span> <span class=\"s2\">\"outputs\"</span><span class=\"p\">)</span>\n<span class=\"n\">results</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"s2\">\"data/dataset/\"</span><span class=\"p\">,</span> <span class=\"s2\">\"data/dev.json\"</span><span class=\"p\">)</span>\n</pre>\n<p>Available arguments:</p>\n<pre><span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">Args:</span>\n<span class=\"sd\">    data: Path to data directory containing text files (JSON) and image files OR a Pandas DataFrame.</span>\n<span class=\"sd\">        If a DataFrame is given, it should contain the columns [text, labels, images]. When using a DataFrame,</span>\n<span class=\"sd\">        image_path MUST be specified. The image column of the DataFrame should contain the relative path from</span>\n<span class=\"sd\">        image_path to the image.</span>\n<span class=\"sd\">        E.g:</span>\n<span class=\"sd\">            For an image file 1.jpeg located in \"data/train/\";</span>\n<span class=\"sd\">                image_path = \"data/train/\"</span>\n<span class=\"sd\">                images = \"1.jpeg\"</span>\n<span class=\"sd\">    files_list (optional): If given, only the files specified in this list will be taken from data directory.</span>\n<span class=\"sd\">        files_list can be a Python list or the path (str) to a JSON file containing a list of files.</span>\n<span class=\"sd\">    image_path (optional): Must be specified when using DataFrame as input. Path to the directory containing the</span>\n<span class=\"sd\">        images.</span>\n<span class=\"sd\">    text_label (optional): Column name to look for instead of the default \"text\"</span>\n<span class=\"sd\">    labels_label (optional): Column name to look for instead of the default \"labels\"</span>\n<span class=\"sd\">    images_label (optional): Column name to look for instead of the default \"images\"</span>\n<span class=\"sd\">    image_type_extension (optional): If given, this will be added to the end of each value in \"images\".</span>\n<span class=\"sd\">    data_type_extension (optional): If given, this will be added to the end of each value in \"files_list\".</span>\n<span class=\"sd\">    output_dir: The directory where model files will be saved. If not given, self.args['output_dir'] will be used.</span>\n<span class=\"sd\">    verbose: If verbose, results will be printed to the console on completion of evaluation.</span>\n<span class=\"sd\">    silent: If silent, tqdm progress bars will be hidden.</span>\n<span class=\"sd\">    **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). E.g. f1=sklearn.metrics.f1_score.</span>\n<span class=\"sd\">                A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.</span>\n\n<span class=\"sd\">\"\"\"</span>\n</pre>\n<h3>Predicting from a trained Model</h3>\n<p>Use the <code>predict()</code> method to make predictions. You can load a saved model by giving the path to the model directory as\n<code>model_name</code>. Note that you need to provide the same arguments when loading a saved model as you did when creating the\noriginal model.</p>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">MultiModalClassificationModel</span><span class=\"p\">(</span><span class=\"s2\">\"bert\"</span><span class=\"p\">,</span> <span class=\"s2\">\"outputs\"</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span>\n    <span class=\"p\">{</span>\n        <span class=\"s2\">\"text\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"s2\">\"A lawyer is forced to defend a guilty judge, while defending other innocent clients, and trying to find punishment for the guilty and provide justice for the innocent.\"</span>\n        <span class=\"p\">],</span>\n        <span class=\"s2\">\"labels\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"Crime\"</span><span class=\"p\">],</span>\n        <span class=\"s2\">\"images\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"0078718\"</span><span class=\"p\">]</span>\n    <span class=\"p\">},</span>\n    <span class=\"n\">image_path</span><span class=\"o\">=</span><span class=\"s2\">\"data/dataset\"</span><span class=\"p\">,</span>\n    <span class=\"n\">image_type_extension</span><span class=\"o\">=</span><span class=\"s2\">\".jpeg\"</span>\n<span class=\"p\">)</span>\n</pre>\n<p><em><a href=\"#table-of-contents\" rel=\"nofollow\">Back to Table of Contents</a></em></p>\n<hr>\n<h2>Regression</h2>\n<p>Regression tasks also use the ClassificationModel with 2 caveats.</p>\n<ol>\n<li><code>num_labels</code> should be 1.</li>\n<li><code>regression</code> should be <code>True</code> in <code>args</code> dict.</li>\n</ol>\n<p>Regression can be used with either single sentence or sentence pair tasks.</p>\n<h4>Minimal Start for Regression</h4>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.classification</span> <span class=\"kn\">import</span> <span class=\"n\">ClassificationModel</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n\n\n<span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s1\">'Example sentence belonging to class 1'</span><span class=\"p\">,</span> <span class=\"s1\">'Yep, this is 1'</span><span class=\"p\">,</span> <span class=\"mf\">1.8</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'Example sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"s1\">'Yep, this is 0'</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'Example  2 sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"s1\">'Yep, this is 0'</span><span class=\"p\">,</span> <span class=\"mf\">4.5</span><span class=\"p\">]</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">train_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">train_data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'text_a'</span><span class=\"p\">,</span> <span class=\"s1\">'text_b'</span><span class=\"p\">,</span> <span class=\"s1\">'labels'</span><span class=\"p\">])</span>\n\n<span class=\"n\">eval_data</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s1\">'Example sentence belonging to class 1'</span><span class=\"p\">,</span> <span class=\"s1\">'Yep, this is 1'</span><span class=\"p\">,</span> <span class=\"mf\">1.9</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'Example sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"s1\">'Yep, this is 0'</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'Example  2 sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"s1\">'Yep, this is 0'</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">]</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">eval_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">eval_data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'text_a'</span><span class=\"p\">,</span> <span class=\"s1\">'text_b'</span><span class=\"p\">,</span> <span class=\"s1\">'labels'</span><span class=\"p\">])</span>\n\n<span class=\"n\">train_args</span><span class=\"o\">=</span><span class=\"p\">{</span>\n    <span class=\"s1\">'reprocess_input_data'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s1\">'overwrite_output_dir'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s1\">'num_train_epochs'</span><span class=\"p\">:</span> <span class=\"mi\">3</span><span class=\"p\">,</span>\n\n    <span class=\"s1\">'regression'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"c1\"># Create a ClassificationModel</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ClassificationModel</span><span class=\"p\">(</span><span class=\"s1\">'roberta'</span><span class=\"p\">,</span> <span class=\"s1\">'roberta-base'</span><span class=\"p\">,</span> <span class=\"n\">num_labels</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">use_cuda</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">cuda_device</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">train_args</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">train_df</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">())</span>\n\n<span class=\"c1\"># Train the model</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"n\">train_df</span><span class=\"p\">,</span> <span class=\"n\">eval_df</span><span class=\"o\">=</span><span class=\"n\">eval_df</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Evaluate the model</span>\n<span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">model_outputs</span><span class=\"p\">,</span> <span class=\"n\">wrong_predictions</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"n\">eval_df</span><span class=\"p\">)</span>\n\n<span class=\"n\">predictions</span><span class=\"p\">,</span> <span class=\"n\">raw_outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([[</span><span class=\"s2\">\"I'd like to puts some CD-ROMS on my iPad, is that possible?'\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Yes, but wouldn't that block the screen?\"</span><span class=\"p\">]])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">predictions</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">raw_outputs</span><span class=\"p\">)</span>\n</pre>\n<hr>\n<h2>Visualization Support</h2>\n<p>The <a href=\"https://www.wandb.com/\" rel=\"nofollow\">Weights &amp; Biases</a> framework is supported for visualizing model training.</p>\n<p>To use this, simply set a project name for W&amp;B in the <code>wandb_project</code> attribute of the <code>args</code> dictionary. This will log all hyperparameter values, training losses, and evaluation metrics to the given project.</p>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ClassificationModel</span><span class=\"p\">(</span><span class=\"s1\">'roberta'</span><span class=\"p\">,</span> <span class=\"s1\">'roberta-base'</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'wandb_project'</span><span class=\"p\">:</span> <span class=\"s1\">'project-name'</span><span class=\"p\">})</span>\n</pre>\n<p>For a complete example, see <a href=\"https://medium.com/skilai/to-see-is-to-believe-visualizing-the-training-of-machine-learning-models-664ef3fe4f49\" rel=\"nofollow\">here</a>.</p>\n<p><em><a href=\"#table-of-contents\" rel=\"nofollow\">Back to Table of Contents</a></em></p>\n<hr>\n<h2>Experimental Features</h2>\n<p>To use experimental features, import from <code>simpletransformers.experimental.X</code></p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.experimental.classification</span> <span class=\"kn\">import</span> <span class=\"n\">ClassificationModel</span>\n</pre>\n<h3>Sliding Window For Long Sequences</h3>\n<p>Normally, sequences longer than <code>max_seq_length</code> are unceremoniously truncated.</p>\n<p>This experimental feature moves a sliding window over each sequence and generates sub-sequences with length <code>max_seq_length</code>. The model output for each sub-sequence is averaged into a single output before being sent to the linear classifier.</p>\n<p>Currently available on binary and multiclass classification models of the following types:</p>\n<ul>\n<li>BERT</li>\n<li>DistilBERT</li>\n<li>RoBERTa</li>\n<li>AlBERT</li>\n<li>XLNet</li>\n<li>CamemBERT</li>\n</ul>\n<p>Set <code>sliding_window</code> to <code>True</code> for the ClassificationModel to enable this feature.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simpletransformers.classification</span> <span class=\"kn\">import</span> <span class=\"n\">ClassificationModel</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">sklearn</span>\n\n<span class=\"c1\"># Train and Evaluation data needs to be in a Pandas Dataframe of two columns. The first column is the text with type str, and the second column in the label with type int.</span>\n<span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"p\">[[</span><span class=\"s1\">'Example sentence belonging to class 1'</span> <span class=\"o\">*</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'Example sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'Example  2 sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]]</span> <span class=\"o\">+</span> <span class=\"p\">[[</span><span class=\"s1\">'Example sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">)]</span>\n<span class=\"n\">train_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">train_data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'text'</span><span class=\"p\">,</span> <span class=\"s1\">'labels'</span><span class=\"p\">])</span>\n\n\n<span class=\"n\">eval_data</span> <span class=\"o\">=</span> <span class=\"p\">[[</span><span class=\"s1\">'Example eval sentence belonging to class 1'</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'Example eval sentence belonging to class 0'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]]</span>\n<span class=\"n\">eval_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">eval_data</span><span class=\"p\">)</span>\n\n<span class=\"n\">train_args</span><span class=\"o\">=</span><span class=\"p\">{</span>\n    <span class=\"s1\">'sliding_window'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s1\">'reprocess_input_data'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s1\">'overwrite_output_dir'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s1\">'evaluate_during_training'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s1\">'logging_steps'</span><span class=\"p\">:</span> <span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"s1\">'stride'</span><span class=\"p\">:</span> <span class=\"mf\">0.8</span><span class=\"p\">,</span>\n    <span class=\"s1\">'max_seq_length'</span><span class=\"p\">:</span> <span class=\"mi\">128</span>\n<span class=\"p\">}</span>\n\n<span class=\"c1\"># Create a TransformerModel</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ClassificationModel</span><span class=\"p\">(</span><span class=\"s1\">'camembert'</span><span class=\"p\">,</span> <span class=\"s1\">'camembert-base'</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">train_args</span><span class=\"p\">,</span> <span class=\"n\">use_cuda</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">train_df</span><span class=\"o\">.</span><span class=\"n\">head</span><span class=\"p\">())</span>\n\n<span class=\"c1\"># Train the model</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"n\">train_df</span><span class=\"p\">,</span> <span class=\"n\">eval_df</span><span class=\"o\">=</span><span class=\"n\">eval_df</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Evaluate the model</span>\n<span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">model_outputs</span><span class=\"p\">,</span> <span class=\"n\">wrong_predictions</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"n\">eval_df</span><span class=\"p\">,</span> <span class=\"n\">acc</span><span class=\"o\">=</span><span class=\"n\">sklearn</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">accuracy_score</span><span class=\"p\">)</span>\n\n<span class=\"n\">predictions</span><span class=\"p\">,</span> <span class=\"n\">raw_outputs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([</span><span class=\"s2\">\"I'd like to puts some CD-ROMS on my iPad, is that possible?' \u2014 Yes, but wouldn't that block the screen?\"</span> <span class=\"o\">*</span> <span class=\"mi\">25</span><span class=\"p\">])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">predictions</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">raw_outputs</span><span class=\"p\">)</span>\n</pre>\n<p><em><a href=\"#table-of-contents\" rel=\"nofollow\">Back to Table of Contents</a></em></p>\n<hr>\n<h2>Loading Saved Models</h2>\n<p>To load a saved model, provide the path to the directory containing the saved model as the <code>model_name</code>.\n<em>Note that you will need to specify the correct (usually the same used in training) <code>args</code> when loading the model</em></p>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ClassificationModel</span><span class=\"p\">(</span><span class=\"s1\">'roberta'</span><span class=\"p\">,</span> <span class=\"s1\">'outputs/'</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{})</span>\n</pre>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">NERModel</span><span class=\"p\">(</span><span class=\"s1\">'bert'</span><span class=\"p\">,</span> <span class=\"s1\">'outputs/'</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{})</span>\n</pre>\n<p><em><a href=\"#table-of-contents\" rel=\"nofollow\">Back to Table of Contents</a></em></p>\n<hr>\n<h2>Default Settings</h2>\n<p>The default args used are given below. Any of these can be overridden by passing a dict containing the corresponding\nkey: value pairs to the the init method of a Model class.</p>\n<pre><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">\"output_dir\"</span><span class=\"p\">:</span> <span class=\"s2\">\"outputs/\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"cache_dir\"</span><span class=\"p\">:</span> <span class=\"s2\">\"cache/\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"best_model_dir\"</span><span class=\"p\">:</span> <span class=\"s2\">\"outputs/best_model/\"</span><span class=\"p\">,</span>\n\n    <span class=\"s2\">\"fp16\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"fp16_opt_level\"</span><span class=\"p\">:</span> <span class=\"s2\">\"O1\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"max_seq_length\"</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"train_batch_size\"</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"eval_batch_size\"</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"gradient_accumulation_steps\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"num_train_epochs\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"weight_decay\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"learning_rate\"</span><span class=\"p\">:</span> <span class=\"mf\">4e-5</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"adam_epsilon\"</span><span class=\"p\">:</span> <span class=\"mf\">1e-8</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"warmup_ratio\"</span><span class=\"p\">:</span> <span class=\"mf\">0.06</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"warmup_steps\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"max_grad_norm\"</span><span class=\"p\">:</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"do_lower_case\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n\n    <span class=\"s2\">\"logging_steps\"</span><span class=\"p\">:</span> <span class=\"mi\">50</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"evaluate_during_training\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"evaluate_during_training_steps\"</span><span class=\"p\">:</span> <span class=\"mi\">2000</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"evaluate_during_training_verbose\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"use_cached_eval_features\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"save_eval_checkpoints\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span>\n    <span class=\"s2\">\"save_steps\"</span><span class=\"p\">:</span> <span class=\"mi\">2000</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"no_cache\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"save_model_every_epoch\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"tensorboard_dir\"</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n\n    <span class=\"s2\">\"overwrite_output_dir\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"reprocess_input_data\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n\n    <span class=\"s2\">\"process_count\"</span><span class=\"p\">:</span> <span class=\"n\">cpu_count</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"mi\">2</span> <span class=\"k\">if</span> <span class=\"n\">cpu_count</span><span class=\"p\">()</span> <span class=\"o\">&gt;</span> <span class=\"mi\">2</span> <span class=\"k\">else</span> <span class=\"mi\">1</span>\n    <span class=\"s2\">\"n_gpu\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"silent\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"use_multiprocessing\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n\n    <span class=\"s2\">\"wandb_project\"</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"wandb_kwargs\"</span><span class=\"p\">:</span> <span class=\"p\">{},</span>\n\n    <span class=\"s2\">\"use_early_stopping\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"early_stopping_patience\"</span><span class=\"p\">:</span> <span class=\"mi\">3</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"early_stopping_delta\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"early_stopping_metric\"</span><span class=\"p\">:</span> <span class=\"s2\">\"eval_loss\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"early_stopping_metric_minimize\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n\n    <span class=\"s2\">\"manual_seed\"</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"encoding\"</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"config\"</span><span class=\"p\">:</span> <span class=\"p\">{},</span>\n<span class=\"p\">}</span>\n</pre>\n<h3>Args Explained</h3>\n<h4><em>output_dir: str</em></h4>\n<p>The directory where all outputs will be stored. This includes model checkpoints and evaluation results.</p>\n<h4><em>cache_dir: str</em></h4>\n<p>The directory where cached files will be saved.</p>\n<h4><em>best_model_dir: str</em></h4>\n<p>The directory where the best model (model checkpoints) will be saved if evaluate_during_training is enabled and the training loop achieves a lowest evaluation loss calculated after every evaluate_during_training_steps, or an epoch.</p>\n<h4><em>fp16: bool</em></h4>\n<p>Whether or not fp16 mode should be used. Requires NVidia Apex library.</p>\n<h4><em>fp16_opt_level: str</em></h4>\n<p>Can be '01', '02', '03'. See the <a href=\"https://nvidia.github.io/apex/amp.html\" rel=\"nofollow\">Apex docs</a> for an explanation of the different optimization levels (opt_levels).</p>\n<h4><em>max_seq_length: int</em></h4>\n<p>Maximum sequence level the model will support.</p>\n<h4><em>train_batch_size: int</em></h4>\n<p>The training batch size.</p>\n<h4><em>gradient_accumulation_steps: int</em></h4>\n<p>The number of training steps to execute before performing a <code>optimizer.step()</code>. Effectively increases the training batch size while sacrificing training time to lower memory consumption.</p>\n<h4><em>eval_batch_size: int</em></h4>\n<p>The evaluation batch size.</p>\n<h4><em>num_train_epochs: int</em></h4>\n<p>The number of epochs the model will be trained for.</p>\n<h4><em>weight_decay: float</em></h4>\n<p>Adds L2 penalty.</p>\n<h4><em>learning_rate: float</em></h4>\n<p>The learning rate for training.</p>\n<h4><em>adam_epsilon: float</em></h4>\n<p>Epsilon hyperparameter used in AdamOptimizer.</p>\n<h4><em>max_grad_norm: float</em></h4>\n<p>Maximum gradient clipping.</p>\n<h4><em>do_lower_case: bool</em></h4>\n<p>Set to True when using uncased models.</p>\n<h4><em>evaluate_during_training</em></h4>\n<p>Set to True to perform evaluation while training models. Make sure <code>eval_df</code> is passed to the training method if enabled.</p>\n<h4><em>evaluate_during_training_steps</em></h4>\n<p>Perform evaluation at every specified number of steps. A checkpoint model and the evaluation results will be saved.</p>\n<h4><em>evaluate_during_training_verbose</em></h4>\n<p>Print results from evaluation during training.</p>\n<h4><em>use_cached_eval_features</em></h4>\n<p>Evaluation during training uses cached features. Setting this to <code>False</code> will cause features to be recomputed at every evaluation step.</p>\n<h4><em>save_eval_checkpoints</em></h4>\n<p>Save a model checkpoint for every evaluation performed.</p>\n<h4><em>logging_steps: int</em></h4>\n<p>Log training loss and learning at every specified number of steps.</p>\n<h4><em>save_steps: int</em></h4>\n<p>Save a model checkpoint at every specified number of steps.</p>\n<h4><em>no_cache: bool</em></h4>\n<p>Cache features to disk.</p>\n<h4><em>save_model_every_epoch: bool</em></h4>\n<p>Save a model at the end of every epoch.</p>\n<h4><em>tensorboard_dir: str</em></h4>\n<p>The directory where Tensorboard events will be stored during training. By default, Tensorboard events will be saved in a subfolder inside <code>runs/</code>  like <code>runs/Dec02_09-32-58_36d9e58955b0/</code>.</p>\n<h4><em>overwrite_output_dir: bool</em></h4>\n<p>If True, the trained model will be saved to the ouput_dir and will overwrite existing saved models in the same directory.</p>\n<h4><em>reprocess_input_data: bool</em></h4>\n<p>If True, the input data will be reprocessed even if a cached file of the input data exists in the cache_dir.</p>\n<h4><em>process_count: int</em></h4>\n<p>Number of cpu cores (processes) to use when converting examples to features. Default is (number of cores - 2) or 1 if (number of cores &lt;= 2)</p>\n<h4><em>n_gpu: int</em></h4>\n<p>Number of GPUs to use.</p>\n<h4><em>silent: bool</em></h4>\n<p>Disables progress bars.</p>\n<h4><em>use_multiprocessing: bool</em></h4>\n<p>If True, multiprocessing will be used when converting data into features. Disabling can reduce memory usage, but may substantially slow down processing.</p>\n<h4><em>wandb_project: str</em></h4>\n<p>Name of W&amp;B project. This will log all hyperparameter values, training losses, and evaluation metrics to the given project.</p>\n<h4><em>wandb_kwargs: dict</em></h4>\n<p>Dictionary of keyword arguments to be passed to the W&amp;B project.</p>\n<h4><em>use_early_stopping</em></h4>\n<p>Use early stopping to stop training when <code>early_stopping_metric</code> doesn't improve (based on <code>early_stopping_patience</code>, and <code>early_stopping_delta</code>)</p>\n<h4><em>early_stopping_patience</em></h4>\n<p>Terminate training after this many evaluations without an improvement in <code>eval_loss</code> greater then <code>early_stopping_delta</code>.</p>\n<h4><em>early_stopping_delta</em></h4>\n<p>The improvement over <code>best_eval_loss</code> necessary to count as a better checkpoint.</p>\n<h4><em>early_stopping_metric</em></h4>\n<p>The metric that should be used with early stopping. (Should be computed during <code>eval_during_training</code>).</p>\n<h4><em>early_stopping_metric_minimize</em></h4>\n<p>Whether <code>early_stopping_metric</code> should be minimized (or maximized).</p>\n<h4><em>manual_seed</em></h4>\n<p>Set a manual seed if necessary for reproducible results.</p>\n<h4><em>encoding</em></h4>\n<p>Specify an encoding to be used when reading text files.</p>\n<h4><em>config</em></h4>\n<p>A dictionary containing configuration options that should be overriden in a model's config.</p>\n<hr>\n<h2>Current Pretrained Models</h2>\n<p>For a list of pretrained models, see <a href=\"https://huggingface.co/pytorch-transformers/pretrained_models.html\" rel=\"nofollow\">Hugging Face docs</a>.</p>\n<p>The <code>model_types</code> available for each task can be found under their respective section. Any pretrained model of that type\nfound in the Hugging Face docs should work. To use any of them set the correct <code>model_type</code> and <code>model_name</code> in the <code>args</code>\ndictionary.</p>\n<p><em><a href=\"#table-of-contents\" rel=\"nofollow\">Back to Table of Contents</a></em></p>\n<hr>\n<h2>Acknowledgements</h2>\n<p>None of this would have been possible without the hard work by the HuggingFace team in developing the <a href=\"https://github.com/huggingface/pytorch-transformers\" rel=\"nofollow\">Pytorch-Transformers</a> library.</p>\n<p><em></em></p><div><em>Icon for the Social Media Preview made by <a href=\"https://www.flaticon.com/authors/freepik\" rel=\"nofollow\" title=\"Freepik\">Freepik</a> from <a href=\"https://www.flaticon.com/\" rel=\"nofollow\" title=\"Flaticon\">www.flaticon.com</a></em></div><p></p>\n<h2>Contributors \u2728</h2>\n<p>Thanks goes to these wonderful people (<a href=\"https://allcontributors.org/docs/en/emoji-key\" rel=\"nofollow\">emoji key</a>):</p>\n\n\n\n<table>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/hawktang\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1720993c7930ef5d31e2c148d34b0ff6a28fad37/68747470733a2f2f61766174617273302e67697468756275736572636f6e74656e742e636f6d2f752f323030343037313f763d34\" width=\"100px;\"><br><sub><b>hawktang</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=hawktang\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://datawizzards.io\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e293cc4d0d576b86eb721a1522aa0ca087076242/68747470733a2f2f61766174617273302e67697468756275736572636f6e74656e742e636f6d2f752f32323430393939363f763d34\" width=\"100px;\"><br><sub><b>Mabu Manaileng</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=mabu-dev\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://www.facebook.com/aliosm97\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/538e92abcf7938bd5289c681262aa38a28eabab7/68747470733a2f2f61766174617273332e67697468756275736572636f6e74656e742e636f6d2f752f373636323439323f763d34\" width=\"100px;\"><br><sub><b>Ali Hamdi Ali Fadel</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=AliOsm\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://tovly.co\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0676a7d524d14958bb52ea1f8e5030158df018aa/68747470733a2f2f61766174617273302e67697468756275736572636f6e74656e742e636f6d2f752f31323234323335313f763d34\" width=\"100px;\"><br><sub><b>Tovly Deutsch</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=TovlyDeutsch\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/hlo-world\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/613b1ea770d3b8703012ca61d208ed08d67e9c88/68747470733a2f2f61766174617273302e67697468756275736572636f6e74656e742e636f6d2f752f393633333035353f763d34\" width=\"100px;\"><br><sub><b>hlo-world</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=hlo-world\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/huntertl\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a7af7f274052741c9f6afc59502573453abbe09f/68747470733a2f2f61766174617273312e67697468756275736572636f6e74656e742e636f6d2f752f31353131333838353f763d34\" width=\"100px;\"><br><sub><b>huntertl</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=huntertl\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://whattheshot.com\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/26bc3ef97fdff88b84a9d1df5c7be2b896a5874d/68747470733a2f2f61766174617273322e67697468756275736572636f6e74656e742e636f6d2f752f3632333736333f763d34\" width=\"100px;\"><br><sub><b>Yann Defretin</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=kinoute\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=kinoute\" rel=\"nofollow\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"#question-kinoute\" rel=\"nofollow\" title=\"Answering Questions\">\ud83d\udcac</a> <a href=\"#ideas-kinoute\" rel=\"nofollow\" title=\"Ideas, Planning, &amp; Feedback\">\ud83e\udd14</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/mananeau\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/cb017b3f2b0715f8601e1acfd6093609fe0e3802/68747470733a2f2f61766174617273302e67697468756275736572636f6e74656e742e636f6d2f752f32393434303137303f763d34\" width=\"100px;\"><br><sub><b>Manuel </b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=mananeau\" rel=\"nofollow\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=mananeau\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://jacobsgill.es\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9d22346cfee728483df469ced514a73c92d7ce1d/68747470733a2f2f61766174617273322e67697468756275736572636f6e74656e742e636f6d2f752f393130393833323f763d34\" width=\"100px;\"><br><sub><b>Gilles Jacobs</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=GillesJ\" rel=\"nofollow\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    <td align=\"center\"><a href=\"https://github.com/shasha79\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/08af40b430b6912c53747ee08d5bc2f44f99b46f/68747470733a2f2f61766174617273322e67697468756275736572636f6e74656e742e636f6d2f752f353531323634393f763d34\" width=\"100px;\"><br><sub><b>shasha79</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=shasha79\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://www-lium.univ-lemans.fr/~garcia\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/cf544e794b4d60627014657b7ec2b21e443279b4/68747470733a2f2f61766174617273322e67697468756275736572636f6e74656e742e636f6d2f752f31343233333432373f763d34\" width=\"100px;\"><br><sub><b>Mercedes Garcia</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=merc85garcia\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/hammad26\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/786ced9fdfd4612c07c1f29fb498384ed4a66264/68747470733a2f2f61766174617273312e67697468756275736572636f6e74656e742e636f6d2f752f31323634333738343f763d34\" width=\"100px;\"><br><sub><b>Hammad Hassan Tarar</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=hammad26\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=hammad26\" rel=\"nofollow\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    <td align=\"center\"><a href=\"https://github.com/todd-cook\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6dd45b1d0cadf7b763f287ab656d433bbce0fccf/68747470733a2f2f61766174617273332e67697468756275736572636f6e74656e742e636f6d2f752f3636353338393f763d34\" width=\"100px;\"><br><sub><b>Todd Cook</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=todd-cook\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://knuthellan.com/\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/31751566bae741981283f040e0ebb35e8715aebb/68747470733a2f2f61766174617273322e67697468756275736572636f6e74656e742e636f6d2f752f35313434313f763d34\" width=\"100px;\"><br><sub><b>Knut O. Hellan</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=khellan\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=khellan\" rel=\"nofollow\" title=\"Documentation\">\ud83d\udcd6</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/nagenshukla\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7831387aae6ebb27e5ee2edef15a542625ae3f9b/68747470733a2f2f61766174617273302e67697468756275736572636f6e74656e742e636f6d2f752f33393139363232383f763d34\" width=\"100px;\"><br><sub><b>nagenshukla</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=nagenshukla\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://www.linkedin.com/in/flaviussn/\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8d80df20a02a2e8af934e73b3e79e5e4c87f5c10/68747470733a2f2f61766174617273302e67697468756275736572636f6e74656e742e636f6d2f752f32303532333033323f763d34\" width=\"100px;\"><br><sub><b>flaviussn</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=flaviussn\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=flaviussn\" rel=\"nofollow\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    <td align=\"center\"><a href=\"http://marctorrellas.github.com\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/af8d7dee5e0b94b6329a51bc4f14faddfe1bdb3a/68747470733a2f2f61766174617273312e67697468756275736572636f6e74656e742e636f6d2f752f32323034353737393f763d34\" width=\"100px;\"><br><sub><b>Marc Torrellas</b></sub></a><br><a href=\"#maintenance-marctorrellas\" rel=\"nofollow\" title=\"Maintenance\">\ud83d\udea7</a></td>\n    <td align=\"center\"><a href=\"https://github.com/adrienrenaud\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ecb6f94e3165bd9cfc1a262173c410ef442c11df/68747470733a2f2f61766174617273332e67697468756275736572636f6e74656e742e636f6d2f752f363230383135373f763d34\" width=\"100px;\"><br><sub><b>Adrien Renaud</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=adrienrenaud\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/jacky18008\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/781bccf239a79cdb749d5bd25e6fa1bb802a358c/68747470733a2f2f61766174617273302e67697468756275736572636f6e74656e742e636f6d2f752f393033313434313f763d34\" width=\"100px;\"><br><sub><b>jacky18008</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=jacky18008\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/seo-95\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0573f8e801c9d2166319869f6186e7ddfba3fa86/68747470733a2f2f61766174617273302e67697468756275736572636f6e74656e742e636f6d2f752f33383235343534313f763d34\" width=\"100px;\"><br><sub><b>Matteo Senese</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=seo-95\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/sarthakTUM\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b8af77ddf9c8433b0a6ca2cffc1db88e36ac70f1/68747470733a2f2f61766174617273322e67697468756275736572636f6e74656e742e636f6d2f752f32333036323836393f763d34\" width=\"100px;\"><br><sub><b>sarthakTUM</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=sarthakTUM\" rel=\"nofollow\" title=\"Documentation\">\ud83d\udcd6</a> <a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=sarthakTUM\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/djstrong\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b6adfbf2fa3ec4f108e7cb9167c43f0fcd18163d/68747470733a2f2f61766174617273312e67697468756275736572636f6e74656e742e636f6d2f752f313834393935393f763d34\" width=\"100px;\"><br><sub><b>djstrong</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=djstrong\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://kozistr.tech\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/26d72c74b6c707c9c0e150b98d3b930bac2e6049/68747470733a2f2f61766174617273322e67697468756275736572636f6e74656e742e636f6d2f752f31353334343739363f763d34\" width=\"100px;\"><br><sub><b>Hyeongchan Kim</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=kozistr\" rel=\"nofollow\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    <td align=\"center\"><a href=\"https://github.com/Pradhy729\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1fe6f37cd551ccb5e554091688d411b5322d85d3/68747470733a2f2f61766174617273332e67697468756275736572636f6e74656e742e636f6d2f752f34393635393931333f763d34\" width=\"100px;\"><br><sub><b>Pradhy729</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=Pradhy729\" rel=\"nofollow\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://iknoorjobs.github.io/\" rel=\"nofollow\"><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b0138daf82a1a502899b6c121cbbcf4c19083b0f/68747470733a2f2f61766174617273322e67697468756275736572636f6e74656e742e636f6d2f752f32323835323936373f763d34\" width=\"100px;\"><br><sub><b>Iknoor Singh</b></sub></a><br><a href=\"https://github.com/ThilinaRajapakse/simpletransformers/commits?author=iknoorjobs\" rel=\"nofollow\" title=\"Documentation\">\ud83d\udcd6</a></td>\n  </tr>\n</table>\n\n\n\n<p>This project follows the <a href=\"https://github.com/all-contributors/all-contributors\" rel=\"nofollow\">all-contributors</a> specification. Contributions of any kind welcome!</p>\n<p><em>If you should be on this list but you aren't, or you are on the list but don't want to be, please don't hesitate to contact me!</em></p>\n\n          </div>"}, "last_serial": 7179716, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "1000e18048af3d30a34f649e4fe8c47b", "sha256": "7cbbae2ab881ccdba7c00849fe349fb37b46330dc931325ca3c5ce2e70bb8389"}, "downloads": -1, "filename": "simpletransformers-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "1000e18048af3d30a34f649e4fe8c47b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 25157, "upload_time": "2019-10-04T06:20:15", "upload_time_iso_8601": "2019-10-04T06:20:15.197341Z", "url": "https://files.pythonhosted.org/packages/b4/f8/3e7267879415e6a46db7e1c869369f7d2dbd1590fa89e0e0df2e7819e3a4/simpletransformers-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "48ceca89bf08b652b5b09958c5af1a6c", "sha256": "933f5ed48f9b0e59e919ae1b2e7a223c2dea1416e33812590b94af8edeb82329"}, "downloads": -1, "filename": "simpletransformers-0.1.1.tar.gz", "has_sig": false, "md5_digest": "48ceca89bf08b652b5b09958c5af1a6c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13110, "upload_time": "2019-10-04T06:20:17", "upload_time_iso_8601": "2019-10-04T06:20:17.740228Z", "url": "https://files.pythonhosted.org/packages/63/14/8534b81bd47697aa924ccfecd01fee90a014dc7c660cc15869fd5a9131c9/simpletransformers-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "25c33b7222b59a26849ac73a05c0e19f", "sha256": "bcc8a01a851ad68c2153816b827d0fae4f2b56d1d060f5911ede946bd0366104"}, "downloads": -1, "filename": "simpletransformers-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "25c33b7222b59a26849ac73a05c0e19f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 25187, "upload_time": "2019-10-04T06:23:22", "upload_time_iso_8601": "2019-10-04T06:23:22.857488Z", "url": "https://files.pythonhosted.org/packages/06/0a/6c691b39936ea7701b00209614b70d21ad5dd227ca299526cdcabe8d462d/simpletransformers-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9ca0f8b68b2e59db0bcd39e999874441", "sha256": "fe8c975a2ed42e1ae430e10630ad6ae594d1a611bb0cb9c875e0be3b0f64d5db"}, "downloads": -1, "filename": "simpletransformers-0.1.2.tar.gz", "has_sig": false, "md5_digest": "9ca0f8b68b2e59db0bcd39e999874441", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13152, "upload_time": "2019-10-04T06:23:24", "upload_time_iso_8601": "2019-10-04T06:23:24.849420Z", "url": "https://files.pythonhosted.org/packages/3f/7c/c7225c0a434dca4c5253431a2f152666df3b659eea4adb016ee7ca14320b/simpletransformers-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "761fda016a0201bf079c4c425793d4e8", "sha256": "76cd70095e99b5fbdbe834294abf1a8a9cc944e45765833c5bbb8c8550eb514c"}, "downloads": -1, "filename": "simpletransformers-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "761fda016a0201bf079c4c425793d4e8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 25194, "upload_time": "2019-10-04T06:25:13", "upload_time_iso_8601": "2019-10-04T06:25:13.207271Z", "url": "https://files.pythonhosted.org/packages/db/60/75f0ff09482d56ba87d6c99819d785a6ea7589403422ffc38512ed0ebf70/simpletransformers-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d9973cdc06a52fce4c1309d07ba174a8", "sha256": "3cfa7f4022077de6eee65d7bffb7966615d6314c7cba7dd8e2e086cfa7a00244"}, "downloads": -1, "filename": "simpletransformers-0.1.3.tar.gz", "has_sig": false, "md5_digest": "d9973cdc06a52fce4c1309d07ba174a8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13171, "upload_time": "2019-10-04T06:25:16", "upload_time_iso_8601": "2019-10-04T06:25:16.365992Z", "url": "https://files.pythonhosted.org/packages/a3/9e/bd4742478a5f553d7ebe41a32e73a425667f7f71ef127010fb02b120a718/simpletransformers-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "69bcb58e9efc652e5ff77ed50573c426", "sha256": "c85260e0b9d26d7a6c73a4b047d851cb2d255472b63af01cb28553aaa743ce6d"}, "downloads": -1, "filename": "simpletransformers-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "69bcb58e9efc652e5ff77ed50573c426", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 25618, "upload_time": "2019-10-04T06:49:15", "upload_time_iso_8601": "2019-10-04T06:49:15.633082Z", "url": "https://files.pythonhosted.org/packages/2a/b5/309313b3b69431d1a1c8508846463e127da9a7ba3d78e44e0e61744b8a87/simpletransformers-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "70bb03749d22b32538734f6ae3642eaf", "sha256": "558b0b071627c73a0b2bc0c1143d7ebca365eed9e76ebf5ce57520d4edf73bb5"}, "downloads": -1, "filename": "simpletransformers-0.1.4.tar.gz", "has_sig": false, "md5_digest": "70bb03749d22b32538734f6ae3642eaf", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13616, "upload_time": "2019-10-04T06:49:17", "upload_time_iso_8601": "2019-10-04T06:49:17.795804Z", "url": "https://files.pythonhosted.org/packages/ed/ec/447c8a5de1fd2c1599f6140d425f1174b5900d9602726a0764e94276cb77/simpletransformers-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "898b08fb51a9abb553c5a8596f469f80", "sha256": "32149a9ac0717be2151acf598f8ece7789912a2d3572f5cab33715095a815e9f"}, "downloads": -1, "filename": "simpletransformers-0.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "898b08fb51a9abb553c5a8596f469f80", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 25576, "upload_time": "2019-10-04T10:42:26", "upload_time_iso_8601": "2019-10-04T10:42:26.479052Z", "url": "https://files.pythonhosted.org/packages/41/4e/9189ab8e7b5974bf6b35b74aebf551cb487f4b24ca444172e56e4d4063c0/simpletransformers-0.1.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "62fc4750ef7ff6aa9cf724dbef1dff0e", "sha256": "ec0e34e5805a7c9c96d1ad2fc2ed32f6a1a22c43e324fe968a03976be5d574c2"}, "downloads": -1, "filename": "simpletransformers-0.1.5.tar.gz", "has_sig": false, "md5_digest": "62fc4750ef7ff6aa9cf724dbef1dff0e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13583, "upload_time": "2019-10-04T10:42:30", "upload_time_iso_8601": "2019-10-04T10:42:30.301933Z", "url": "https://files.pythonhosted.org/packages/3f/d1/0935ddf06513398e6e21579073ad3326c5e8b5c40076ace6e703f1b74434/simpletransformers-0.1.5.tar.gz", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "bc756bcefe411ca4abcadbbf6045ca61", "sha256": "f99e96a2a310be93c65f2f6232a6b917d6ec0074f01027409a5ff77dd2aa7fc0"}, "downloads": -1, "filename": "simpletransformers-0.1.6-py3-none-any.whl", "has_sig": false, "md5_digest": "bc756bcefe411ca4abcadbbf6045ca61", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 24952, "upload_time": "2019-10-04T11:30:32", "upload_time_iso_8601": "2019-10-04T11:30:32.158798Z", "url": "https://files.pythonhosted.org/packages/73/7a/12846331031be3ea8457cac0db190f2e29074ce9fe2bf567d66a2f95dfe9/simpletransformers-0.1.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6b14ca37d286014061cf48480b7c0304", "sha256": "4b20b307e19e56acf18e6f54c5288ed81ad4f5249c81e7c47f73cde4cfd733cf"}, "downloads": -1, "filename": "simpletransformers-0.1.6.tar.gz", "has_sig": false, "md5_digest": "6b14ca37d286014061cf48480b7c0304", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 12962, "upload_time": "2019-10-04T11:30:34", "upload_time_iso_8601": "2019-10-04T11:30:34.172144Z", "url": "https://files.pythonhosted.org/packages/14/12/b10abf8da24bc152e5739b51f50aaf273ac6b1df2205c50eeebcf43f7f48/simpletransformers-0.1.6.tar.gz", "yanked": false}], "0.1.7": [{"comment_text": "", "digests": {"md5": "bc1d04af2708245d6b16c86fff0e0e34", "sha256": "5f3c102360eecfb1b07aa13fd1105b82f89ba5853bdf6fd3e7601ebd860df391"}, "downloads": -1, "filename": "simpletransformers-0.1.7-py3-none-any.whl", "has_sig": false, "md5_digest": "bc1d04af2708245d6b16c86fff0e0e34", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 24951, "upload_time": "2019-10-04T12:12:00", "upload_time_iso_8601": "2019-10-04T12:12:00.873108Z", "url": "https://files.pythonhosted.org/packages/22/87/5c9288930e96714a2ad000eab8de636f0437ee0fb68981a2f2af0314396b/simpletransformers-0.1.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8dcfef7e95d53ecae5ede229adb29414", "sha256": "9af07c987c5c451d3254b696a93d5e4e04a47f8ce1d54e854f6a7c88ec3cd372"}, "downloads": -1, "filename": "simpletransformers-0.1.7.tar.gz", "has_sig": false, "md5_digest": "8dcfef7e95d53ecae5ede229adb29414", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 12968, "upload_time": "2019-10-04T12:12:03", "upload_time_iso_8601": "2019-10-04T12:12:03.150780Z", "url": "https://files.pythonhosted.org/packages/8b/01/34653fce42a761d5e9c87a5051daee4da7d397d564864f2f509b37233f26/simpletransformers-0.1.7.tar.gz", "yanked": false}], "0.1.8": [{"comment_text": "", "digests": {"md5": "3cefb219d44ce86b12c6d1c37d9fe830", "sha256": "44ff770954a7ecb48e0da14f89f8cdb3a0f74110f6b61a0190d7df9243db64ba"}, "downloads": -1, "filename": "simpletransformers-0.1.8-py3-none-any.whl", "has_sig": false, "md5_digest": "3cefb219d44ce86b12c6d1c37d9fe830", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 26498, "upload_time": "2019-10-04T14:35:17", "upload_time_iso_8601": "2019-10-04T14:35:17.303016Z", "url": "https://files.pythonhosted.org/packages/ee/bc/5e99c99896faba10787cd0ca2ced43dd78e8ca82c843403552d0aa69be7d/simpletransformers-0.1.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4f687bf74e14a8ad93cf9fa3f88dad02", "sha256": "399fd169707e397aa2832edaf96cb37c41b1b21db577aa6b48d39eca64f90b18"}, "downloads": -1, "filename": "simpletransformers-0.1.8.tar.gz", "has_sig": false, "md5_digest": "4f687bf74e14a8ad93cf9fa3f88dad02", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 15597, "upload_time": "2019-10-04T14:35:19", "upload_time_iso_8601": "2019-10-04T14:35:19.318515Z", "url": "https://files.pythonhosted.org/packages/ae/df/d89fc5efbb2ec0931bc587b756a5e0b4492285d8e0c4d8013a74a67bd180/simpletransformers-0.1.8.tar.gz", "yanked": false}], "0.1.9": [{"comment_text": "", "digests": {"md5": "1e73bf1cfa1ace7ce1163f1338ee182b", "sha256": "15a7c86d07ef351e056af19dbd5ccbb6d41f07e6e3af525d64a8ddb1a3327d33"}, "downloads": -1, "filename": "simpletransformers-0.1.9-py3-none-any.whl", "has_sig": false, "md5_digest": "1e73bf1cfa1ace7ce1163f1338ee182b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 26644, "upload_time": "2019-10-04T18:12:16", "upload_time_iso_8601": "2019-10-04T18:12:16.038782Z", "url": "https://files.pythonhosted.org/packages/03/2f/185216455bb7a9c2551b2f56c49b9a36cc480efaefd2dbd59a63b269419e/simpletransformers-0.1.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0d702cb1d8b57162cfb43cf707747d10", "sha256": "2ef88cd40adddf121aa73129976c5e132225d8ac24d40eb9bac12d6bd8595193"}, "downloads": -1, "filename": "simpletransformers-0.1.9.tar.gz", "has_sig": false, "md5_digest": "0d702cb1d8b57162cfb43cf707747d10", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 15941, "upload_time": "2019-10-04T18:12:18", "upload_time_iso_8601": "2019-10-04T18:12:18.099524Z", "url": "https://files.pythonhosted.org/packages/f7/37/68fa2611ebb6c5b544d4f61688108b02ee6c312b83bcb1781e6894ed5a93/simpletransformers-0.1.9.tar.gz", "yanked": false}], "0.10.1": [{"comment_text": "", "digests": {"md5": "e0a33bda58800689554c2666e43b92a6", "sha256": "1d85467fd356d3cdaced3bd343b6bf8586c996814b4bd83fed285f60f1bc4894"}, "downloads": -1, "filename": "simpletransformers-0.10.1-py3-none-any.whl", "has_sig": false, "md5_digest": "e0a33bda58800689554c2666e43b92a6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 93121, "upload_time": "2019-12-01T10:42:52", "upload_time_iso_8601": "2019-12-01T10:42:52.978947Z", "url": "https://files.pythonhosted.org/packages/0d/44/be46b3063e4d724105625394126cce7e8bf865651ab0962b96369a114361/simpletransformers-0.10.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "42e282e7977c3ed7a75492fba9a9ec2e", "sha256": "bfcbc635d62a08e0b07ab216afe5ff317c34e6184aa211985d58e6eb1221de58"}, "downloads": -1, "filename": "simpletransformers-0.10.1.tar.gz", "has_sig": false, "md5_digest": "42e282e7977c3ed7a75492fba9a9ec2e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 83234, "upload_time": "2019-12-01T10:42:54", "upload_time_iso_8601": "2019-12-01T10:42:54.736325Z", "url": "https://files.pythonhosted.org/packages/00/3a/66e423872441063d3f11ba0eddff731bbea5c69854e60a97802508359d94/simpletransformers-0.10.1.tar.gz", "yanked": false}], "0.10.2": [{"comment_text": "", "digests": {"md5": "1d2d7599f8af70682a24a44fb57e9022", "sha256": "43653e5e39dcb499b4c1d6987c286e0d2ef53eeeaeb423aba41731078fa07cd5"}, "downloads": -1, "filename": "simpletransformers-0.10.2-py3-none-any.whl", "has_sig": false, "md5_digest": "1d2d7599f8af70682a24a44fb57e9022", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 93296, "upload_time": "2019-12-02T12:25:53", "upload_time_iso_8601": "2019-12-02T12:25:53.500658Z", "url": "https://files.pythonhosted.org/packages/34/58/eb37623d9671c123d21f1ed0b1f96fe0501586ae62f9d261dedde202a817/simpletransformers-0.10.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d19c30b7e86dabf66c0282ea80c4bba8", "sha256": "1d4e42923f4b9b4f9762c931f52da3c31bc1f429185cc0fbb627a4a9941ba52b"}, "downloads": -1, "filename": "simpletransformers-0.10.2.tar.gz", "has_sig": false, "md5_digest": "d19c30b7e86dabf66c0282ea80c4bba8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 83580, "upload_time": "2019-12-02T12:25:55", "upload_time_iso_8601": "2019-12-02T12:25:55.170612Z", "url": "https://files.pythonhosted.org/packages/4b/8f/7574e797ddcaf1bc944687bdc383b49f5ad6d2a50a64b6c75d499fac0cc9/simpletransformers-0.10.2.tar.gz", "yanked": false}], "0.10.4": [{"comment_text": "", "digests": {"md5": "00e5e0fed50b74acf085bae534ea63dc", "sha256": "10ab85d7efc38566c06d87b9f0be1ec8306f399eb557e93f366f24077977ff84"}, "downloads": -1, "filename": "simpletransformers-0.10.4-py3-none-any.whl", "has_sig": false, "md5_digest": "00e5e0fed50b74acf085bae534ea63dc", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 93318, "upload_time": "2019-12-03T17:30:51", "upload_time_iso_8601": "2019-12-03T17:30:51.376834Z", "url": "https://files.pythonhosted.org/packages/21/22/26cbb0f4c4951b672854f2e4df22404fd9e86ebd80024bb95476b1ec5425/simpletransformers-0.10.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5aacdd50be17865555873d6f5ca6f504", "sha256": "ef5e945cec03da0590552e484060f1b9a7c16eeb3a422695026482d896b0506a"}, "downloads": -1, "filename": "simpletransformers-0.10.4.tar.gz", "has_sig": false, "md5_digest": "5aacdd50be17865555873d6f5ca6f504", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 83661, "upload_time": "2019-12-03T17:30:53", "upload_time_iso_8601": "2019-12-03T17:30:53.114036Z", "url": "https://files.pythonhosted.org/packages/9e/ee/921214292a1a60773d718a3241a22689960853f2776dd19cf859745e8414/simpletransformers-0.10.4.tar.gz", "yanked": false}], "0.10.5": [{"comment_text": "", "digests": {"md5": "757a90f5ff52ad23d09ba95c7b8976f4", "sha256": "de557c03198ec53e8e465edd2a3afc008a47e5739199f4bcb6e7c73e134a3f91"}, "downloads": -1, "filename": "simpletransformers-0.10.5-py3-none-any.whl", "has_sig": false, "md5_digest": "757a90f5ff52ad23d09ba95c7b8976f4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 93311, "upload_time": "2019-12-13T11:32:31", "upload_time_iso_8601": "2019-12-13T11:32:31.752649Z", "url": "https://files.pythonhosted.org/packages/ff/22/8bcbc9c78b0a75d65feb16b325ed95fbf39f9e2c37201425176bf9e6c3cc/simpletransformers-0.10.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "89e79a17dc16c51e9c78bb047d532ca8", "sha256": "66f84f8b078ea653777a0643e7871fb030247ea6d20319a88d221a23c212a607"}, "downloads": -1, "filename": "simpletransformers-0.10.5.tar.gz", "has_sig": false, "md5_digest": "89e79a17dc16c51e9c78bb047d532ca8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 83643, "upload_time": "2019-12-13T11:32:33", "upload_time_iso_8601": "2019-12-13T11:32:33.541373Z", "url": "https://files.pythonhosted.org/packages/f4/6b/6d48fb7be93db284f7a9cbe5758a26f6262edfd513139c3a6e6d18376795/simpletransformers-0.10.5.tar.gz", "yanked": false}], "0.10.6": [{"comment_text": "", "digests": {"md5": "dfb40c2c4fdaab3155b9ccfc77b7f97b", "sha256": "c73e1d60c8f31c650ce7bfa25260c82c7a14f5567aacdd4ffffe5997443e0f75"}, "downloads": -1, "filename": "simpletransformers-0.10.6-py3-none-any.whl", "has_sig": false, "md5_digest": "dfb40c2c4fdaab3155b9ccfc77b7f97b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 93394, "upload_time": "2019-12-13T11:48:22", "upload_time_iso_8601": "2019-12-13T11:48:22.687612Z", "url": "https://files.pythonhosted.org/packages/ca/21/1bf169ff21a6fc977b248313be1af8c8049caf5666341e4c29ae9650d974/simpletransformers-0.10.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f031782c90a6207c6454f681d7ae89d3", "sha256": "975d94f5774ef7e7fb8d23cdd0ec2896b1882ffed52c820279589554215bf87c"}, "downloads": -1, "filename": "simpletransformers-0.10.6.tar.gz", "has_sig": false, "md5_digest": "f031782c90a6207c6454f681d7ae89d3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 84373, "upload_time": "2019-12-13T11:48:24", "upload_time_iso_8601": "2019-12-13T11:48:24.727942Z", "url": "https://files.pythonhosted.org/packages/0f/92/9963dbd32f75926ab47048b6fe3c89a87c591f6751b8a237bba13f43039d/simpletransformers-0.10.6.tar.gz", "yanked": false}], "0.10.7": [{"comment_text": "", "digests": {"md5": "b8fca25084b917f42c39bb39123f457b", "sha256": "84a586d4e84d0d3c266050a88d147e7e30eea8fb70743f0fc5e71e3635043620"}, "downloads": -1, "filename": "simpletransformers-0.10.7-py3-none-any.whl", "has_sig": false, "md5_digest": "b8fca25084b917f42c39bb39123f457b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 93396, "upload_time": "2019-12-15T17:36:16", "upload_time_iso_8601": "2019-12-15T17:36:16.749128Z", "url": "https://files.pythonhosted.org/packages/c3/e5/696c04b95cc36ac325af5006fb067e66a761559d57d79f5af5c070f6b39a/simpletransformers-0.10.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c57f0b34090030ba4eaeda8929451de0", "sha256": "49d1b44387791020617a94ddff260cf74f4b0cd00d5ad6dbeb82c24a71b97b84"}, "downloads": -1, "filename": "simpletransformers-0.10.7.tar.gz", "has_sig": false, "md5_digest": "c57f0b34090030ba4eaeda8929451de0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 84359, "upload_time": "2019-12-15T17:36:18", "upload_time_iso_8601": "2019-12-15T17:36:18.531902Z", "url": "https://files.pythonhosted.org/packages/0a/79/4fab621e6b0f76b51c51856bde091302afcdfb1978fb45be45aba6bece6a/simpletransformers-0.10.7.tar.gz", "yanked": false}], "0.10.8": [{"comment_text": "", "digests": {"md5": "6c7c61e9cd809f4efd3be925b8447ad4", "sha256": "cf081718cdf2924ca69d81f1d597007d6481f942973271658072344667bc6c48"}, "downloads": -1, "filename": "simpletransformers-0.10.8-py3-none-any.whl", "has_sig": false, "md5_digest": "6c7c61e9cd809f4efd3be925b8447ad4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 93433, "upload_time": "2019-12-15T17:36:15", "upload_time_iso_8601": "2019-12-15T17:36:15.452165Z", "url": "https://files.pythonhosted.org/packages/2d/64/0e153874501d9741d0fa0fdb6dd73dd722aefec5243baeb9c0bd1d03e068/simpletransformers-0.10.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b2836d8a4e34aecbc75c35debdd6f5e9", "sha256": "cba80ea8f07c35ec5ccc795b55f6599a3e72930e27928f65700f495e4f84aeb4"}, "downloads": -1, "filename": "simpletransformers-0.10.8.tar.gz", "has_sig": false, "md5_digest": "b2836d8a4e34aecbc75c35debdd6f5e9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 84438, "upload_time": "2019-12-15T17:36:17", "upload_time_iso_8601": "2019-12-15T17:36:17.486957Z", "url": "https://files.pythonhosted.org/packages/d7/8a/868743eb44e43c571ab837255110e2a142d691826ceae60c9b6442906b0f/simpletransformers-0.10.8.tar.gz", "yanked": false}], "0.11.0": [{"comment_text": "", "digests": {"md5": "05030fa674bf5bd9f5f31c41232de0ea", "sha256": "972c3be883f802c87fe7c0b2f153f6e5e033be007229b4b34629e1db3a1dcdaa"}, "downloads": -1, "filename": "simpletransformers-0.11.0-py3-none-any.whl", "has_sig": false, "md5_digest": "05030fa674bf5bd9f5f31c41232de0ea", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 93423, "upload_time": "2019-12-15T17:38:48", "upload_time_iso_8601": "2019-12-15T17:38:48.504712Z", "url": "https://files.pythonhosted.org/packages/f6/e3/21d0bb3a7725b6231905f6f6bc9d07fec4dde9151f77bfb6e4d3335c8770/simpletransformers-0.11.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8279b5d802846b9590a9830bad86659c", "sha256": "60ebdb37f8679cdfa39909041ba5a54684789690eb5b5d1e223f16a3e8ad7c20"}, "downloads": -1, "filename": "simpletransformers-0.11.0.tar.gz", "has_sig": false, "md5_digest": "8279b5d802846b9590a9830bad86659c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 84384, "upload_time": "2019-12-15T17:38:49", "upload_time_iso_8601": "2019-12-15T17:38:49.987809Z", "url": "https://files.pythonhosted.org/packages/fa/9c/4ae4e8caea7a8f7ed444885086c27ca8e2218ae44de20eb2970317f188a4/simpletransformers-0.11.0.tar.gz", "yanked": false}], "0.11.1": [{"comment_text": "", "digests": {"md5": "c00a9abfb789262a3913cd86b80a87e4", "sha256": "d4a51bfc2c30db7121bd9b9d082581e803c0b44cbddc75b97b8d103ab48f334b"}, "downloads": -1, "filename": "simpletransformers-0.11.1-py3-none-any.whl", "has_sig": false, "md5_digest": "c00a9abfb789262a3913cd86b80a87e4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 93623, "upload_time": "2019-12-17T19:15:16", "upload_time_iso_8601": "2019-12-17T19:15:16.137276Z", "url": "https://files.pythonhosted.org/packages/b3/92/f6c8cfb612229f849b644043bf89c794984aebf4db4a12dc79dfcdd598e1/simpletransformers-0.11.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "270779d36d1e89ef20b6743706dcb593", "sha256": "d3e44d1959bc5c5d0179879dc39ce7537615d191e9274dc3906d834e28d70708"}, "downloads": -1, "filename": "simpletransformers-0.11.1.tar.gz", "has_sig": false, "md5_digest": "270779d36d1e89ef20b6743706dcb593", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 84677, "upload_time": "2019-12-17T19:15:17", "upload_time_iso_8601": "2019-12-17T19:15:17.557573Z", "url": "https://files.pythonhosted.org/packages/88/50/f4b65fb9c0c1205802588c12188da2d6a2e0cac2c97653df9f2576ef5217/simpletransformers-0.11.1.tar.gz", "yanked": false}], "0.11.2": [{"comment_text": "", "digests": {"md5": "c2602db1e28d87d27d46f5a058cb43a7", "sha256": "1b96a4d8696c04cb8070e0ee20d6fa85a58cd5f59dd0cf3772f696d7e9eac8d5"}, "downloads": -1, "filename": "simpletransformers-0.11.2-py3-none-any.whl", "has_sig": false, "md5_digest": "c2602db1e28d87d27d46f5a058cb43a7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 93623, "upload_time": "2019-12-18T05:08:30", "upload_time_iso_8601": "2019-12-18T05:08:30.714214Z", "url": "https://files.pythonhosted.org/packages/3e/c2/b927ac2c570ae181be3b01def59b1f9b8233a0b78422e4a660f1c74fb2ae/simpletransformers-0.11.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d2c41538f844884a4c042631e7cb0a2a", "sha256": "0962174d0ed059b9f3abc0eb57f2fae7292bcd85b068be85d39d378c10ecdc59"}, "downloads": -1, "filename": "simpletransformers-0.11.2.tar.gz", "has_sig": false, "md5_digest": "d2c41538f844884a4c042631e7cb0a2a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 84675, "upload_time": "2019-12-18T05:08:32", "upload_time_iso_8601": "2019-12-18T05:08:32.426485Z", "url": "https://files.pythonhosted.org/packages/2e/41/3b433698f7ac03dfdbc3be5f51fd42c007b88b16e49a74b4d3f92d609036/simpletransformers-0.11.2.tar.gz", "yanked": false}], "0.12.0": [{"comment_text": "", "digests": {"md5": "9061099abddf188d4057d44578f7b941", "sha256": "5b22a6ef56dc926f726005de414c1f9ffee1b33d5f8dbe23f0e87991373dca1d"}, "downloads": -1, "filename": "simpletransformers-0.12.0-py3-none-any.whl", "has_sig": false, "md5_digest": "9061099abddf188d4057d44578f7b941", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 93655, "upload_time": "2019-12-19T09:52:07", "upload_time_iso_8601": "2019-12-19T09:52:07.391977Z", "url": "https://files.pythonhosted.org/packages/96/1e/1fbd04ac0592d3c14358601c512a68e186e594fc4f2d0f4d647489483b23/simpletransformers-0.12.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d40ec82a01a8ec977f1ca18492ae3361", "sha256": "afc9ca9afb8dd93456111c5378b61b05c1ef5459831c6b8bcc5ea14edc322031"}, "downloads": -1, "filename": "simpletransformers-0.12.0.tar.gz", "has_sig": false, "md5_digest": "d40ec82a01a8ec977f1ca18492ae3361", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 84852, "upload_time": "2019-12-19T09:52:09", "upload_time_iso_8601": "2019-12-19T09:52:09.251544Z", "url": "https://files.pythonhosted.org/packages/83/33/4f78769c2c0120a87f9cddea34274686c1819d3c10961bbccae5c58975d7/simpletransformers-0.12.0.tar.gz", "yanked": false}], "0.13.0": [{"comment_text": "", "digests": {"md5": "8ef62617fa537b53b518df9e5dc1b14c", "sha256": "99a72812aa2319a4c498fb68acb52b56b1fa0a6f6812ffa1b10fa8a0bcb914ca"}, "downloads": -1, "filename": "simpletransformers-0.13.0-py3-none-any.whl", "has_sig": false, "md5_digest": "8ef62617fa537b53b518df9e5dc1b14c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 94727, "upload_time": "2019-12-19T17:38:32", "upload_time_iso_8601": "2019-12-19T17:38:32.531191Z", "url": "https://files.pythonhosted.org/packages/ef/e9/047e9fc60728537f0b3f770d798e0fe154a4db6fd435139051bbc8ca5745/simpletransformers-0.13.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "913912e96a6b5d62063439a71ca9c012", "sha256": "61da451b1659b0444daed147939978e72b54bb1356e83cb82495049afc2cfe77"}, "downloads": -1, "filename": "simpletransformers-0.13.0.tar.gz", "has_sig": false, "md5_digest": "913912e96a6b5d62063439a71ca9c012", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 86226, "upload_time": "2019-12-19T17:38:34", "upload_time_iso_8601": "2019-12-19T17:38:34.262722Z", "url": "https://files.pythonhosted.org/packages/09/b4/b7ed5fc38599dad1acd8baf6ae59a6b557f6fc9e52da578c28d82db9bcdd/simpletransformers-0.13.0.tar.gz", "yanked": false}], "0.13.1": [{"comment_text": "", "digests": {"md5": "f8825f460cb65e050fa6bef34edc47b9", "sha256": "2b8a3b1b39abfe63a858066ee15853cf4b421542562e8a5aed18a8fbafb2a9d3"}, "downloads": -1, "filename": "simpletransformers-0.13.1-py3-none-any.whl", "has_sig": false, "md5_digest": "f8825f460cb65e050fa6bef34edc47b9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 94749, "upload_time": "2019-12-20T05:45:48", "upload_time_iso_8601": "2019-12-20T05:45:48.997192Z", "url": "https://files.pythonhosted.org/packages/31/c9/4864fc8ccb66e52c42ee1542270e98bfa39ca0b14d67d0c93bccc8d2bdba/simpletransformers-0.13.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1d5ef214b2ada610dd58bf858a612f34", "sha256": "d111b5423fc351befa50e50744b21984e34a06963b31bd06c523396bc58ce4ef"}, "downloads": -1, "filename": "simpletransformers-0.13.1.tar.gz", "has_sig": false, "md5_digest": "1d5ef214b2ada610dd58bf858a612f34", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 86234, "upload_time": "2019-12-20T05:45:51", "upload_time_iso_8601": "2019-12-20T05:45:51.022743Z", "url": "https://files.pythonhosted.org/packages/73/a3/e0311f09795e8ec4892bba71d743fe55cc1634a0c8d144ca3db0b4c66d04/simpletransformers-0.13.1.tar.gz", "yanked": false}], "0.13.2": [{"comment_text": "", "digests": {"md5": "e10a3876177f0d578569783a356f91d5", "sha256": "6bd28e70716b46701c7bf80d92faa3aa6c3b7f79327973f32080e3f54ab83aaf"}, "downloads": -1, "filename": "simpletransformers-0.13.2-py3-none-any.whl", "has_sig": false, "md5_digest": "e10a3876177f0d578569783a356f91d5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 94759, "upload_time": "2019-12-20T07:09:45", "upload_time_iso_8601": "2019-12-20T07:09:45.915524Z", "url": "https://files.pythonhosted.org/packages/df/e3/9755821b91db6d40fda45a7713810ecf4311e91dcead5797b0163d0edfa0/simpletransformers-0.13.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f9571b85c20204204cca02f93c04321b", "sha256": "9cb89964fe1d6a1aba20ae03e2177d8ec5769961a48c02963be76de9442da4f5"}, "downloads": -1, "filename": "simpletransformers-0.13.2.tar.gz", "has_sig": false, "md5_digest": "f9571b85c20204204cca02f93c04321b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 86295, "upload_time": "2019-12-20T07:09:47", "upload_time_iso_8601": "2019-12-20T07:09:47.564355Z", "url": "https://files.pythonhosted.org/packages/34/2d/6850719877ea5e7185e43f224c2e6ba43a2b4f8ead82cbc7ed8053a2bb6c/simpletransformers-0.13.2.tar.gz", "yanked": false}], "0.13.3": [{"comment_text": "", "digests": {"md5": "e671f93004305a53daa194a8e13919a7", "sha256": "da7cdb4f5f1aa8302a1c9359815b5fb39c3f810a426b71bb994ce1e5f0260f0e"}, "downloads": -1, "filename": "simpletransformers-0.13.3-py3-none-any.whl", "has_sig": false, "md5_digest": "e671f93004305a53daa194a8e13919a7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 94749, "upload_time": "2019-12-21T07:47:48", "upload_time_iso_8601": "2019-12-21T07:47:48.447609Z", "url": "https://files.pythonhosted.org/packages/03/49/e9c0bcd268d498132d698e479c55da028203902191d3d5e477e20b81d108/simpletransformers-0.13.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d649ee5667196973de0a0724ec58fd14", "sha256": "abcdc587fe20e9afa1921d11e70d2d810460d4c25897011f55ea0116faeeb3ca"}, "downloads": -1, "filename": "simpletransformers-0.13.3.tar.gz", "has_sig": false, "md5_digest": "d649ee5667196973de0a0724ec58fd14", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 86240, "upload_time": "2019-12-21T07:47:50", "upload_time_iso_8601": "2019-12-21T07:47:50.341306Z", "url": "https://files.pythonhosted.org/packages/1c/a3/fa4f767d4c8c287fda4d0fe98c96a34047ee20d813ebfc45760e1b87b40f/simpletransformers-0.13.3.tar.gz", "yanked": false}], "0.13.4": [{"comment_text": "", "digests": {"md5": "c000fa28b918624123c300b95cbae8af", "sha256": "535ca83d6dc7b175f4e7c20e28ba87cd033c66989b87c7e5339cbed255806170"}, "downloads": -1, "filename": "simpletransformers-0.13.4-py3-none-any.whl", "has_sig": false, "md5_digest": "c000fa28b918624123c300b95cbae8af", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 94749, "upload_time": "2019-12-21T16:33:18", "upload_time_iso_8601": "2019-12-21T16:33:18.448270Z", "url": "https://files.pythonhosted.org/packages/b5/06/2b17067972e79bb4c2f9b337002f6fac990ef833f1b461dfa8c0b3c3e96e/simpletransformers-0.13.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "182c07b2f39de2f828acf3bffc150fb9", "sha256": "2b722cda8e4fd0b9a3318d786b1925821aed47db6071efc288da2e953d2762a0"}, "downloads": -1, "filename": "simpletransformers-0.13.4.tar.gz", "has_sig": false, "md5_digest": "182c07b2f39de2f828acf3bffc150fb9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 86233, "upload_time": "2019-12-21T16:33:20", "upload_time_iso_8601": "2019-12-21T16:33:20.077765Z", "url": "https://files.pythonhosted.org/packages/33/69/09385118072f30cb7f92082fd884bd31ab5ea715474fd03dd0c20098af4b/simpletransformers-0.13.4.tar.gz", "yanked": false}], "0.14.0": [{"comment_text": "", "digests": {"md5": "36a43953eae5f330e62ea9312af4a41d", "sha256": "d3cb8efec235c263b71a19b7f4d94d5f2110f5e85d51c3b4e86357d797168492"}, "downloads": -1, "filename": "simpletransformers-0.14.0-py3-none-any.whl", "has_sig": false, "md5_digest": "36a43953eae5f330e62ea9312af4a41d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 95051, "upload_time": "2019-12-24T06:25:56", "upload_time_iso_8601": "2019-12-24T06:25:56.143240Z", "url": "https://files.pythonhosted.org/packages/4e/78/383bba7cdc69e442ccde71a95cacd58d3589a0c511c756304aa6119cd4c0/simpletransformers-0.14.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7e36629a4b404ad4a081466de2b0f299", "sha256": "025bc8d667ff0d5cdc8e091eec14f234fba0e273ed58b2293be91cc5bebce294"}, "downloads": -1, "filename": "simpletransformers-0.14.0.tar.gz", "has_sig": false, "md5_digest": "7e36629a4b404ad4a081466de2b0f299", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 86669, "upload_time": "2019-12-24T06:25:58", "upload_time_iso_8601": "2019-12-24T06:25:58.086142Z", "url": "https://files.pythonhosted.org/packages/80/0b/ea7c9d2aaaae21608fa7e4dbe2d8d5dc360eaf6e53c323ab39b070cd9d6e/simpletransformers-0.14.0.tar.gz", "yanked": false}], "0.15.0": [{"comment_text": "", "digests": {"md5": "18464694cba64c6209dc6d323b54bb90", "sha256": "3454a4daa6776ce3a5913f1361c84302d52321aa98972928cce56cf18f44432a"}, "downloads": -1, "filename": "simpletransformers-0.15.0-py3-none-any.whl", "has_sig": false, "md5_digest": "18464694cba64c6209dc6d323b54bb90", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 95543, "upload_time": "2019-12-24T08:18:33", "upload_time_iso_8601": "2019-12-24T08:18:33.865314Z", "url": "https://files.pythonhosted.org/packages/aa/95/88c4a9180b0a87d8f9ced8af99939bf24dcf15a00f8adeab7fb138ab002a/simpletransformers-0.15.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c5d923c32e98af2c16af71b78bc66243", "sha256": "b57bdde3f206e6fc9b8db1ffcd3169ab1ec90545a3add60378b284630a067e91"}, "downloads": -1, "filename": "simpletransformers-0.15.0.tar.gz", "has_sig": false, "md5_digest": "c5d923c32e98af2c16af71b78bc66243", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 87421, "upload_time": "2019-12-24T08:18:35", "upload_time_iso_8601": "2019-12-24T08:18:35.517475Z", "url": "https://files.pythonhosted.org/packages/0c/86/3387a753750f9e8fbcdb276a724a501e5a2093146f21bf3da3b77caba5df/simpletransformers-0.15.0.tar.gz", "yanked": false}], "0.15.1": [{"comment_text": "", "digests": {"md5": "4667200af45fe991b9e34457ce5d8167", "sha256": "05c7d21de308dfb33eaffd397f982bd495b9ca8a679a76501310dde9ac9c31c3"}, "downloads": -1, "filename": "simpletransformers-0.15.1-py3-none-any.whl", "has_sig": false, "md5_digest": "4667200af45fe991b9e34457ce5d8167", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 95604, "upload_time": "2019-12-28T06:28:24", "upload_time_iso_8601": "2019-12-28T06:28:24.940827Z", "url": "https://files.pythonhosted.org/packages/c8/49/188dba11f629a5581e66a2fd9315c7a9b536b3ce7c42d9847a8d22d84a3f/simpletransformers-0.15.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ab5ea7266c3e37d2b85502eeb21e8a9b", "sha256": "791964b3d32c6e54d5b007fb7f9e46cd31782788b56ba57ff5cf3e216ade9591"}, "downloads": -1, "filename": "simpletransformers-0.15.1.tar.gz", "has_sig": false, "md5_digest": "ab5ea7266c3e37d2b85502eeb21e8a9b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 87679, "upload_time": "2019-12-28T06:28:26", "upload_time_iso_8601": "2019-12-28T06:28:26.659698Z", "url": "https://files.pythonhosted.org/packages/64/d4/5fdb4c76b2a1c139c2986cb64905f86714e5d1764c0132fb11b8d44fb4db/simpletransformers-0.15.1.tar.gz", "yanked": false}], "0.15.2": [{"comment_text": "", "digests": {"md5": "94c50a6542d3dd16b13f1a08e61c7c99", "sha256": "8ae7890002690385b0ae7cc4acab1cec5710954c60ba33ffc0e0cd5d69ec3062"}, "downloads": -1, "filename": "simpletransformers-0.15.2-py3-none-any.whl", "has_sig": false, "md5_digest": "94c50a6542d3dd16b13f1a08e61c7c99", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 95749, "upload_time": "2019-12-28T14:05:15", "upload_time_iso_8601": "2019-12-28T14:05:15.936896Z", "url": "https://files.pythonhosted.org/packages/f4/70/1c93136815eada45e38014ac8b188d41a32bac0df0baaf0ea56a5ee2a672/simpletransformers-0.15.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "07f886b78a1e378f91c2d38470d12915", "sha256": "c931633917c3bd8a8e5d518f2d68bd16baba4a058c225a649fb76f3d543f9f19"}, "downloads": -1, "filename": "simpletransformers-0.15.2.tar.gz", "has_sig": false, "md5_digest": "07f886b78a1e378f91c2d38470d12915", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 88012, "upload_time": "2019-12-28T14:05:17", "upload_time_iso_8601": "2019-12-28T14:05:17.757294Z", "url": "https://files.pythonhosted.org/packages/29/9b/bb78335c7939765735fccffbdb24585e19ebc2f43e10591862f4c66eb128/simpletransformers-0.15.2.tar.gz", "yanked": false}], "0.15.3": [{"comment_text": "", "digests": {"md5": "9e6a7bf6e713068268f6b1bec988f004", "sha256": "fe609ddb9feead44730c5b946e9671cc9dada89107dff3ca92d11e4fc9a3766d"}, "downloads": -1, "filename": "simpletransformers-0.15.3-py3-none-any.whl", "has_sig": false, "md5_digest": "9e6a7bf6e713068268f6b1bec988f004", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 95747, "upload_time": "2019-12-31T15:29:18", "upload_time_iso_8601": "2019-12-31T15:29:18.205018Z", "url": "https://files.pythonhosted.org/packages/43/d0/8d77f554702e311e61e88e0387e1f842c1101baf5a8ef206a6b355b5dff6/simpletransformers-0.15.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d550c7b417ec6e4e319ef1c3f6fd1e82", "sha256": "5b23abceb1c7a0e801ff1c53e0dd0331c0bc249f114a2f02aaebbb02fc832ad0"}, "downloads": -1, "filename": "simpletransformers-0.15.3.tar.gz", "has_sig": false, "md5_digest": "d550c7b417ec6e4e319ef1c3f6fd1e82", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 87986, "upload_time": "2019-12-31T15:29:20", "upload_time_iso_8601": "2019-12-31T15:29:20.029293Z", "url": "https://files.pythonhosted.org/packages/4c/a0/3f377fdb6589717099407c04f33444f97d24e9d89adae64218f37c124c3d/simpletransformers-0.15.3.tar.gz", "yanked": false}], "0.15.4": [{"comment_text": "", "digests": {"md5": "0715c2753045e479d7d4d857eb13035f", "sha256": "283c5d6dceb20836a1282b25e24040580bf5970f581f7d43b1e2b054cb45a605"}, "downloads": -1, "filename": "simpletransformers-0.15.4-py3-none-any.whl", "has_sig": false, "md5_digest": "0715c2753045e479d7d4d857eb13035f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 95746, "upload_time": "2020-01-03T06:30:05", "upload_time_iso_8601": "2020-01-03T06:30:05.329997Z", "url": "https://files.pythonhosted.org/packages/1c/eb/bb211c0fe9b87bce5d84922530d4f86f1fa5d43672f4a3801d2859e72e41/simpletransformers-0.15.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a014d8797b73eb88fc31db05dac8079a", "sha256": "d0fd07217c5ff90fa40858a2b03231b9352adb2208e5f486d63af35dcecb2660"}, "downloads": -1, "filename": "simpletransformers-0.15.4.tar.gz", "has_sig": false, "md5_digest": "a014d8797b73eb88fc31db05dac8079a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 87982, "upload_time": "2020-01-03T06:30:07", "upload_time_iso_8601": "2020-01-03T06:30:07.108803Z", "url": "https://files.pythonhosted.org/packages/df/b3/c7f4a02659fc38f45a3be1c5aa25752985325fab9446838d3bdfebbcbb36/simpletransformers-0.15.4.tar.gz", "yanked": false}], "0.15.5": [{"comment_text": "", "digests": {"md5": "b900949dca18842d9a9452c14c5746b7", "sha256": "d6ba6666c6536cf2e84958d1fd18f6bb1e9f681166ca7e3ac10dfc8e642760ab"}, "downloads": -1, "filename": "simpletransformers-0.15.5-py3-none-any.whl", "has_sig": false, "md5_digest": "b900949dca18842d9a9452c14c5746b7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 96223, "upload_time": "2020-01-05T15:32:16", "upload_time_iso_8601": "2020-01-05T15:32:16.557321Z", "url": "https://files.pythonhosted.org/packages/a3/5b/4e770ff238ffa3bf3efeed1995ac6b03b6efed84957127838b8080763115/simpletransformers-0.15.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "95f67a4dab63dee86eb124beed9ac6f4", "sha256": "2a3e4889c11ea4f772ec7c8849a0a17658a4d0c1a482a784aad24ebd63f42589"}, "downloads": -1, "filename": "simpletransformers-0.15.5.tar.gz", "has_sig": false, "md5_digest": "95f67a4dab63dee86eb124beed9ac6f4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 88657, "upload_time": "2020-01-05T15:32:18", "upload_time_iso_8601": "2020-01-05T15:32:18.657244Z", "url": "https://files.pythonhosted.org/packages/93/ed/7aec6e2a984a8570e58b5698e9fcb96f3b0a8dc8743949d7f3311ffcc0ac/simpletransformers-0.15.5.tar.gz", "yanked": false}], "0.15.6": [{"comment_text": "", "digests": {"md5": "3ca47a6a0bf826e27219765a14379df7", "sha256": "1bc3cd0a66e7a3951218652a1a0891c004de5878486144473331c612b368d49b"}, "downloads": -1, "filename": "simpletransformers-0.15.6-py3-none-any.whl", "has_sig": false, "md5_digest": "3ca47a6a0bf826e27219765a14379df7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 96301, "upload_time": "2020-01-05T18:13:52", "upload_time_iso_8601": "2020-01-05T18:13:52.573677Z", "url": "https://files.pythonhosted.org/packages/6b/04/0da407a164f92c2d72ec25a5dda0d3f60ba3126a78395232cd1bbaa6bdd0/simpletransformers-0.15.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "713e73b5e7a55142f57e484506f5ae49", "sha256": "bf22f0e958e36d66529dc193149ad353439e8ae03428a591cb748c649df071e8"}, "downloads": -1, "filename": "simpletransformers-0.15.6.tar.gz", "has_sig": false, "md5_digest": "713e73b5e7a55142f57e484506f5ae49", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 88718, "upload_time": "2020-01-05T18:13:54", "upload_time_iso_8601": "2020-01-05T18:13:54.380941Z", "url": "https://files.pythonhosted.org/packages/61/ac/d4c8e34d706d6265cadcda6fb77a17105b0dfe1c33f7e6cf1832c64a59e0/simpletransformers-0.15.6.tar.gz", "yanked": false}], "0.15.7": [{"comment_text": "", "digests": {"md5": "15501827062f856248654602bba385a1", "sha256": "a232d9f1037e2531476d1a7d2c988207ee3ac9af937ef59f3155ca15d92476fd"}, "downloads": -1, "filename": "simpletransformers-0.15.7-py3-none-any.whl", "has_sig": false, "md5_digest": "15501827062f856248654602bba385a1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 96304, "upload_time": "2020-01-06T04:08:15", "upload_time_iso_8601": "2020-01-06T04:08:15.090429Z", "url": "https://files.pythonhosted.org/packages/5d/2d/c6c36332bd1be6b36344629effb3061fac178ce13f91fb9453567b309411/simpletransformers-0.15.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4be016911a6a1ac940225a5eb7919759", "sha256": "b9f9da095e9baed700a98a6cf12ae6e66c1c2ba25ee35436e97b66a4433d3767"}, "downloads": -1, "filename": "simpletransformers-0.15.7.tar.gz", "has_sig": false, "md5_digest": "4be016911a6a1ac940225a5eb7919759", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 88713, "upload_time": "2020-01-06T04:08:16", "upload_time_iso_8601": "2020-01-06T04:08:16.979182Z", "url": "https://files.pythonhosted.org/packages/af/52/e21297c3f7cc53c72a2853ba96b0c5b856976e77b82f0c0a3d1467b1c818/simpletransformers-0.15.7.tar.gz", "yanked": false}], "0.16.0": [{"comment_text": "", "digests": {"md5": "cae5a83eab13296ed7a10248f3b28615", "sha256": "18771cb027dda3e14dcce9b3956700a0083a5923a03dfd0508a21c8c582c73ef"}, "downloads": -1, "filename": "simpletransformers-0.16.0-py3-none-any.whl", "has_sig": false, "md5_digest": "cae5a83eab13296ed7a10248f3b28615", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 96907, "upload_time": "2020-01-07T13:55:31", "upload_time_iso_8601": "2020-01-07T13:55:31.886914Z", "url": "https://files.pythonhosted.org/packages/86/33/90893ee1e8a383384c2902c3fe52eeb038112a9ee4d21d99cb5f7ee5c350/simpletransformers-0.16.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a6232cde67207487aa313edc2ee04814", "sha256": "95f0bd99ad0ee717947a9b63ea374ee0e7747d6b190fdc4094381456f787f4f7"}, "downloads": -1, "filename": "simpletransformers-0.16.0.tar.gz", "has_sig": false, "md5_digest": "a6232cde67207487aa313edc2ee04814", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 89478, "upload_time": "2020-01-07T13:55:33", "upload_time_iso_8601": "2020-01-07T13:55:33.554787Z", "url": "https://files.pythonhosted.org/packages/cb/5d/825c029b7de9ac43471905cb130be97818b240d32a64866225c8a43e3d2a/simpletransformers-0.16.0.tar.gz", "yanked": false}], "0.16.1": [{"comment_text": "", "digests": {"md5": "76a0781593eec419ef59c2e5fe715e8b", "sha256": "dceeb2c73b5907c32bc301eae7f7db22ab8d5682605c6a671ef783858007fd2b"}, "downloads": -1, "filename": "simpletransformers-0.16.1-py3-none-any.whl", "has_sig": false, "md5_digest": "76a0781593eec419ef59c2e5fe715e8b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 97228, "upload_time": "2020-01-07T19:39:52", "upload_time_iso_8601": "2020-01-07T19:39:52.838851Z", "url": "https://files.pythonhosted.org/packages/11/de/d1716181bb909eebb122eb4c2efc8b3dbe8d3ca375ff23fe7a1ef682b9a2/simpletransformers-0.16.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e42fff1c5551f2204a546e66eba7abe9", "sha256": "17bf0662c224718d973830f9b3f8270d3f65f291f0368e4cbe463d4a9d73c9f8"}, "downloads": -1, "filename": "simpletransformers-0.16.1.tar.gz", "has_sig": false, "md5_digest": "e42fff1c5551f2204a546e66eba7abe9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 90467, "upload_time": "2020-01-07T19:39:54", "upload_time_iso_8601": "2020-01-07T19:39:54.526712Z", "url": "https://files.pythonhosted.org/packages/aa/86/0dc4892c5817e4937af3137664fe6b0d66d00796e6b6d0c448ce200e95dd/simpletransformers-0.16.1.tar.gz", "yanked": false}], "0.16.2": [{"comment_text": "", "digests": {"md5": "d196756579320236017b609131efa520", "sha256": "c8e5a4183637b209e5adbd02ac8bb4eb98272bc70c64d90ff28f40ec96828fbb"}, "downloads": -1, "filename": "simpletransformers-0.16.2-py3-none-any.whl", "has_sig": false, "md5_digest": "d196756579320236017b609131efa520", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 97477, "upload_time": "2020-01-08T04:15:52", "upload_time_iso_8601": "2020-01-08T04:15:52.615231Z", "url": "https://files.pythonhosted.org/packages/6f/46/372a8050cf21a9406feda0683acb8564cf8674714c78f1602cd6d182fa5e/simpletransformers-0.16.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "85be4eb6dbfe21b8255c0bdb2214afe8", "sha256": "1a051b45f5fbd4826e3f8d7945d8577f658814842004f637ead6351b666c4f2c"}, "downloads": -1, "filename": "simpletransformers-0.16.2.tar.gz", "has_sig": false, "md5_digest": "85be4eb6dbfe21b8255c0bdb2214afe8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 90692, "upload_time": "2020-01-08T04:15:55", "upload_time_iso_8601": "2020-01-08T04:15:55.248138Z", "url": "https://files.pythonhosted.org/packages/6a/28/fc61abf359c726ba3df9825126fc9686ed04c5d8e99eedcd0b63bf7f12a7/simpletransformers-0.16.2.tar.gz", "yanked": false}], "0.16.4": [{"comment_text": "", "digests": {"md5": "77677d0020af499291634b12ec09a952", "sha256": "ba7c35bf56186fb8a05c75a1f46aeead93ddf2655779480c60d53dbda6462b79"}, "downloads": -1, "filename": "simpletransformers-0.16.4-py3-none-any.whl", "has_sig": false, "md5_digest": "77677d0020af499291634b12ec09a952", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 97290, "upload_time": "2020-01-09T03:59:59", "upload_time_iso_8601": "2020-01-09T03:59:59.096834Z", "url": "https://files.pythonhosted.org/packages/7a/9c/1b9c8c0558d664d021e6c5aa637ccd775463ba4eda9c172f6c193c2ca514/simpletransformers-0.16.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0c0978fece5454ef5c24f056f07e582d", "sha256": "1852a123326c188437b58dc0ae9b2ce1ee34c2806ca19ac8ec51b9fdcfa45280"}, "downloads": -1, "filename": "simpletransformers-0.16.4.tar.gz", "has_sig": false, "md5_digest": "0c0978fece5454ef5c24f056f07e582d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 90417, "upload_time": "2020-01-09T04:00:01", "upload_time_iso_8601": "2020-01-09T04:00:01.123842Z", "url": "https://files.pythonhosted.org/packages/49/b3/1e992fd92b3239d62bed705a148aa9daa442e226ea2e2c4a0263332b7155/simpletransformers-0.16.4.tar.gz", "yanked": false}], "0.16.5": [{"comment_text": "", "digests": {"md5": "67959ccedad4508e1347278b3da8206a", "sha256": "b64b7ea4dbeff67f8b3f550ca95d8ccb0ee949332da265656420b7b6c9d395fa"}, "downloads": -1, "filename": "simpletransformers-0.16.5-py3-none-any.whl", "has_sig": false, "md5_digest": "67959ccedad4508e1347278b3da8206a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 97097, "upload_time": "2020-01-10T17:29:34", "upload_time_iso_8601": "2020-01-10T17:29:34.342841Z", "url": "https://files.pythonhosted.org/packages/f5/10/5b429b1097c4ecfae15e94c01f4a3bfc0fb5d88d89c4a93d4b943edb4119/simpletransformers-0.16.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "aaa8696b9d6e55ffd87de12e2f064f9e", "sha256": "fe9e7f5a0b87b2ffded24141b0c116abacf2d04721c5ca14d2a9ebcfd67d7d73"}, "downloads": -1, "filename": "simpletransformers-0.16.5.tar.gz", "has_sig": false, "md5_digest": "aaa8696b9d6e55ffd87de12e2f064f9e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 90246, "upload_time": "2020-01-10T17:29:36", "upload_time_iso_8601": "2020-01-10T17:29:36.492667Z", "url": "https://files.pythonhosted.org/packages/5e/dc/6f6a07220c9ded52bb6c1d8ee349c1d4a7aa0c791d4bab732249370b1e31/simpletransformers-0.16.5.tar.gz", "yanked": false}], "0.16.6": [{"comment_text": "", "digests": {"md5": "c72589d9080cee0ca257bb720b66e298", "sha256": "d21e789311ffbab7f64af952157d6fc51349cdd6e750c76494c8eedc17a1b5de"}, "downloads": -1, "filename": "simpletransformers-0.16.6-py3-none-any.whl", "has_sig": false, "md5_digest": "c72589d9080cee0ca257bb720b66e298", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 97118, "upload_time": "2020-01-13T05:58:39", "upload_time_iso_8601": "2020-01-13T05:58:39.940798Z", "url": "https://files.pythonhosted.org/packages/fc/28/221baa40d443c26b74fc0d39ce583e61435147e55daf48aad5b494f4f968/simpletransformers-0.16.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f93ea56eb7118986fb646e540ecd48cd", "sha256": "a8eced9c4ad3d2a92df2cf4b55e496f4f0e2265dd696e68162f7ff017f8f83ed"}, "downloads": -1, "filename": "simpletransformers-0.16.6.tar.gz", "has_sig": false, "md5_digest": "f93ea56eb7118986fb646e540ecd48cd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 90266, "upload_time": "2020-01-13T05:58:41", "upload_time_iso_8601": "2020-01-13T05:58:41.904694Z", "url": "https://files.pythonhosted.org/packages/b0/49/ce425309eeaa34762fcbed4365d299a7d95f1819e0ed50bdeafd73cbee41/simpletransformers-0.16.6.tar.gz", "yanked": false}], "0.17.0": [{"comment_text": "", "digests": {"md5": "c080abbba86b69bfce29af873e93fc9a", "sha256": "b5e941eed0ade080ebae494e3080966bc08e2e8b199667a21b3b457a606b9d44"}, "downloads": -1, "filename": "simpletransformers-0.17.0-py3-none-any.whl", "has_sig": false, "md5_digest": "c080abbba86b69bfce29af873e93fc9a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 97537, "upload_time": "2020-01-13T18:52:25", "upload_time_iso_8601": "2020-01-13T18:52:25.815360Z", "url": "https://files.pythonhosted.org/packages/99/3f/3b6181fee94fae05802c2a9dab215d03b19cbaca9edb7dbb28edbcba4f0d/simpletransformers-0.17.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7b38cfb6a834cf2fe5dcd2996547896d", "sha256": "e56f6eaaf717f519b3dc73e48e26ebac42c0d232ba77bd5176c5810e6c3aa3c2"}, "downloads": -1, "filename": "simpletransformers-0.17.0.tar.gz", "has_sig": false, "md5_digest": "7b38cfb6a834cf2fe5dcd2996547896d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 90357, "upload_time": "2020-01-13T18:52:27", "upload_time_iso_8601": "2020-01-13T18:52:27.592618Z", "url": "https://files.pythonhosted.org/packages/40/ec/b7498170dbd00fb6ce73210b77586341bf64e8f39af5d1760918e696869e/simpletransformers-0.17.0.tar.gz", "yanked": false}], "0.17.1": [{"comment_text": "", "digests": {"md5": "4702ce08ede332ed03d3c8b38c3ed92e", "sha256": "dea19dc5f950ad6574279f74b1600a3cd6b50e7c1db55605be91f33f5d542e1a"}, "downloads": -1, "filename": "simpletransformers-0.17.1-py3-none-any.whl", "has_sig": false, "md5_digest": "4702ce08ede332ed03d3c8b38c3ed92e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 97555, "upload_time": "2020-01-13T21:16:22", "upload_time_iso_8601": "2020-01-13T21:16:22.345805Z", "url": "https://files.pythonhosted.org/packages/70/70/f754ec4ed21e50a4ac496429e29ee8b1c6e919a1105812cec44b6cead667/simpletransformers-0.17.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "50540074a2c31d3a82596604ff173324", "sha256": "11821a37a9fb77bb66d60bf2d0c7069c6b348b317243810d24ed445ee79dda09"}, "downloads": -1, "filename": "simpletransformers-0.17.1.tar.gz", "has_sig": false, "md5_digest": "50540074a2c31d3a82596604ff173324", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 90373, "upload_time": "2020-01-13T21:16:24", "upload_time_iso_8601": "2020-01-13T21:16:24.009088Z", "url": "https://files.pythonhosted.org/packages/d4/5c/e4cbb654f14b7d11a277ef9d3d26054d507020f1283151c147c6d9c7759f/simpletransformers-0.17.1.tar.gz", "yanked": false}], "0.18.0": [{"comment_text": "", "digests": {"md5": "1f20f290584b04b11ef62c6af642fb31", "sha256": "37b50d226ece99043daa0ebb17cde5816f72ef4b0c07f07fd9e20037234bec20"}, "downloads": -1, "filename": "simpletransformers-0.18.0-py3-none-any.whl", "has_sig": false, "md5_digest": "1f20f290584b04b11ef62c6af642fb31", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 98447, "upload_time": "2020-01-14T08:12:22", "upload_time_iso_8601": "2020-01-14T08:12:22.768030Z", "url": "https://files.pythonhosted.org/packages/ea/b9/af7de5ce4babd591497287c6c826097de77659d4c1c6d085ef638f88d170/simpletransformers-0.18.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d028e24cc7921649dc471deb9ad17bea", "sha256": "624486f394ebd465cbdb2ed9dc7f3b856c6cbf6b809b4b4a65c0833454b81d45"}, "downloads": -1, "filename": "simpletransformers-0.18.0.tar.gz", "has_sig": false, "md5_digest": "d028e24cc7921649dc471deb9ad17bea", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 92258, "upload_time": "2020-01-14T08:12:25", "upload_time_iso_8601": "2020-01-14T08:12:25.167970Z", "url": "https://files.pythonhosted.org/packages/76/b1/2b1ed0a9889e25dde040ec91457e3452de04328182f1d938920a5d2205fc/simpletransformers-0.18.0.tar.gz", "yanked": false}], "0.18.1": [{"comment_text": "", "digests": {"md5": "d5bdda6cef4d8d83b479c22c6d7be666", "sha256": "23cef8f4b9511eace2d0aaeab50b0ce3a5dbc78bb92bbf43bfd492c4c18af7f3"}, "downloads": -1, "filename": "simpletransformers-0.18.1-py3-none-any.whl", "has_sig": false, "md5_digest": "d5bdda6cef4d8d83b479c22c6d7be666", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 98477, "upload_time": "2020-01-14T18:46:09", "upload_time_iso_8601": "2020-01-14T18:46:09.188990Z", "url": "https://files.pythonhosted.org/packages/0a/05/4f10a2a7dd6f0d578e84d7a9f102163e6a23bfda77cb13a3658e7542bd76/simpletransformers-0.18.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3355f7c3fa60121da3368a5e4b9cdbf0", "sha256": "0b7a0f92a02e8649de1c96a347317143886706bb49c9d49c311a1273548891c4"}, "downloads": -1, "filename": "simpletransformers-0.18.1.tar.gz", "has_sig": false, "md5_digest": "3355f7c3fa60121da3368a5e4b9cdbf0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 92283, "upload_time": "2020-01-14T18:46:11", "upload_time_iso_8601": "2020-01-14T18:46:11.004223Z", "url": "https://files.pythonhosted.org/packages/67/77/8cbfcf7250d4cacb2f54e0039fab9773c1618208095890503b702f51b9b7/simpletransformers-0.18.1.tar.gz", "yanked": false}], "0.18.10": [{"comment_text": "", "digests": {"md5": "b6db325c5fdbe3f49a4e74f91339221c", "sha256": "4300a166fab839c27601e85e426170e99c43edf2e87d98cf2cbc065f8e656d7a"}, "downloads": -1, "filename": "simpletransformers-0.18.10-py3-none-any.whl", "has_sig": false, "md5_digest": "b6db325c5fdbe3f49a4e74f91339221c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 99781, "upload_time": "2020-01-20T16:31:28", "upload_time_iso_8601": "2020-01-20T16:31:28.751480Z", "url": "https://files.pythonhosted.org/packages/46/b6/719c988e3af96d35e0aacf6442e09d61d608122569b7f53d3fc08fb8053c/simpletransformers-0.18.10-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8abc1503a833994819d9daadca6ae006", "sha256": "a21c5d6ca0438f4217af268ec787af907a11fce13d2fee7f97b52462df185fde"}, "downloads": -1, "filename": "simpletransformers-0.18.10.tar.gz", "has_sig": false, "md5_digest": "8abc1503a833994819d9daadca6ae006", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 97607, "upload_time": "2020-01-20T16:31:30", "upload_time_iso_8601": "2020-01-20T16:31:30.672916Z", "url": "https://files.pythonhosted.org/packages/d9/8a/5da2cd480daca4cd248b2259d2ff43151baf40803b6d0080ac00f627d510/simpletransformers-0.18.10.tar.gz", "yanked": false}], "0.18.11": [{"comment_text": "", "digests": {"md5": "2a36454fa3e3e9c5bb9817be9a50175d", "sha256": "8b74a496cd6ff132d5fce72451c6b7b2879bbaee54a04bf0c5d6face65d5d6a9"}, "downloads": -1, "filename": "simpletransformers-0.18.11-py3-none-any.whl", "has_sig": false, "md5_digest": "2a36454fa3e3e9c5bb9817be9a50175d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 99905, "upload_time": "2020-01-21T17:29:07", "upload_time_iso_8601": "2020-01-21T17:29:07.874772Z", "url": "https://files.pythonhosted.org/packages/b3/db/801999c78cf949652d16096fd2d998e0f405c433f9a16983aa2dfbcd3987/simpletransformers-0.18.11-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "62272c5a4082ac0152db19599152ec67", "sha256": "0d506613379b8d3b7d77e601fed563a7f647fe45c1fa16e4f05d63c4ea15f547"}, "downloads": -1, "filename": "simpletransformers-0.18.11.tar.gz", "has_sig": false, "md5_digest": "62272c5a4082ac0152db19599152ec67", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 97925, "upload_time": "2020-01-21T17:29:10", "upload_time_iso_8601": "2020-01-21T17:29:10.010125Z", "url": "https://files.pythonhosted.org/packages/19/95/440cec19cc84d19db4b6856a548ae7235b5cad2938686c0ad8cdf2f11651/simpletransformers-0.18.11.tar.gz", "yanked": false}], "0.18.12": [{"comment_text": "", "digests": {"md5": "05db7bd4b750e80a095496e218e915a5", "sha256": "ebdbe9d4493e2059540e07360f87cca2ee66c033c9b69f70e3fd6050ce8e0b44"}, "downloads": -1, "filename": "simpletransformers-0.18.12-py3-none-any.whl", "has_sig": false, "md5_digest": "05db7bd4b750e80a095496e218e915a5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 99939, "upload_time": "2020-01-25T17:29:15", "upload_time_iso_8601": "2020-01-25T17:29:15.251009Z", "url": "https://files.pythonhosted.org/packages/69/19/9df52f368374af8012418c2a44495ec6d498394e537a6410c87a71cf1dd4/simpletransformers-0.18.12-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9a032f07ee8cc5fe416ee9f74e9c35ef", "sha256": "be8870bb6b0ca1469b3ed3ae1f44eafa94b06a44a792c7fcff94f24bc26d311c"}, "downloads": -1, "filename": "simpletransformers-0.18.12.tar.gz", "has_sig": false, "md5_digest": "9a032f07ee8cc5fe416ee9f74e9c35ef", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 97948, "upload_time": "2020-01-25T17:29:17", "upload_time_iso_8601": "2020-01-25T17:29:17.142952Z", "url": "https://files.pythonhosted.org/packages/ab/09/a605f55cf5805725fc6f4fc2849cbedd5e1c3665f710e43382847124a2d7/simpletransformers-0.18.12.tar.gz", "yanked": false}], "0.18.2": [{"comment_text": "", "digests": {"md5": "9963ac4cbdeda49395b2fa25ad063181", "sha256": "87842ea0be1e9ab6ef873344dda713b7366e290e2409a9b58df4cf4e12fc34a1"}, "downloads": -1, "filename": "simpletransformers-0.18.2-py3-none-any.whl", "has_sig": false, "md5_digest": "9963ac4cbdeda49395b2fa25ad063181", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 98503, "upload_time": "2020-01-15T07:27:27", "upload_time_iso_8601": "2020-01-15T07:27:27.047423Z", "url": "https://files.pythonhosted.org/packages/da/cd/73ec93aa0ddefc75a963928d8b670e0ddbbb97456479fe3f10eb71f11d82/simpletransformers-0.18.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "30fe3f7cf5d3c8ae191e178af69140ca", "sha256": "bbc269704cd8ce45c34eb72067060e15918ae690dc270bfc40e5a79c08e2c1bc"}, "downloads": -1, "filename": "simpletransformers-0.18.2.tar.gz", "has_sig": false, "md5_digest": "30fe3f7cf5d3c8ae191e178af69140ca", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 92289, "upload_time": "2020-01-15T07:27:28", "upload_time_iso_8601": "2020-01-15T07:27:28.663734Z", "url": "https://files.pythonhosted.org/packages/89/dc/7b59f1a04924a507239e2a3cecb32e7ec69de2fccf73e7c7e5eacebb8138/simpletransformers-0.18.2.tar.gz", "yanked": false}], "0.18.3": [{"comment_text": "", "digests": {"md5": "e9d8f4f4e908c6de397564d7ddb99a70", "sha256": "53ae62e49974efcfa0a645bbbf2ff6e000a614b9b6943da9a911f2898705df88"}, "downloads": -1, "filename": "simpletransformers-0.18.3-py3-none-any.whl", "has_sig": false, "md5_digest": "e9d8f4f4e908c6de397564d7ddb99a70", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 98148, "upload_time": "2020-01-15T17:15:27", "upload_time_iso_8601": "2020-01-15T17:15:27.113482Z", "url": "https://files.pythonhosted.org/packages/32/9e/28a14a6356e6e0f873637347e4725f26f13b29ea1553e7677613f49257f3/simpletransformers-0.18.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "42f0aed07d680117584e6ca78e30cb14", "sha256": "3a0b69dab237cf9be04a64581c405e1d260b5a1cbebd9ee816077b3b428635ef"}, "downloads": -1, "filename": "simpletransformers-0.18.3.tar.gz", "has_sig": false, "md5_digest": "42f0aed07d680117584e6ca78e30cb14", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 92250, "upload_time": "2020-01-15T17:15:29", "upload_time_iso_8601": "2020-01-15T17:15:29.118260Z", "url": "https://files.pythonhosted.org/packages/71/2a/6221198c4f878e7450d916d8ae52dfc90a0fd945645c3f8a83a98b3957eb/simpletransformers-0.18.3.tar.gz", "yanked": false}], "0.18.4": [{"comment_text": "", "digests": {"md5": "b8482b99c1830d25cef4d767cc5f1bed", "sha256": "b43efafc941f7e745bf6a4a803ca766db56b1566c87c2f35a059390cd317a75e"}, "downloads": -1, "filename": "simpletransformers-0.18.4-py3-none-any.whl", "has_sig": false, "md5_digest": "b8482b99c1830d25cef4d767cc5f1bed", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 98188, "upload_time": "2020-01-17T05:25:29", "upload_time_iso_8601": "2020-01-17T05:25:29.562559Z", "url": "https://files.pythonhosted.org/packages/99/a5/be477ec01978139e4d056d03d43cd28d380f3ffd0b0fd36744f7abdd8a3a/simpletransformers-0.18.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b2256c582fa49ddd371ba09175706fa9", "sha256": "f915c1d32af1d948bfdece2feab76c84169c7918dcedab010255d33099d1f76a"}, "downloads": -1, "filename": "simpletransformers-0.18.4.tar.gz", "has_sig": false, "md5_digest": "b2256c582fa49ddd371ba09175706fa9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 92290, "upload_time": "2020-01-17T05:25:31", "upload_time_iso_8601": "2020-01-17T05:25:31.691446Z", "url": "https://files.pythonhosted.org/packages/f4/e1/cb19c650acfa1afe8bdc187af7c8f031654e8816e5091aed5af6c9624885/simpletransformers-0.18.4.tar.gz", "yanked": false}], "0.18.5": [{"comment_text": "", "digests": {"md5": "decdbe73de241efc6355d5d7d0b2332c", "sha256": "08d634a58b0717c008b80f736d5e58046bdbd42d805c4d0c644448c0ad44ff30"}, "downloads": -1, "filename": "simpletransformers-0.18.5-py3-none-any.whl", "has_sig": false, "md5_digest": "decdbe73de241efc6355d5d7d0b2332c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 98189, "upload_time": "2020-01-18T10:29:41", "upload_time_iso_8601": "2020-01-18T10:29:41.430478Z", "url": "https://files.pythonhosted.org/packages/49/c3/9402d7e283d8c43dbdf4c46702d32886ada561f4c4cae0b47b2802eea3d8/simpletransformers-0.18.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5471cd7bfe3550d6a9bfe11439694254", "sha256": "f08c61aed776683fbf666690d4809edc3be5dfa046c6ce044492ec381314875e"}, "downloads": -1, "filename": "simpletransformers-0.18.5.tar.gz", "has_sig": false, "md5_digest": "5471cd7bfe3550d6a9bfe11439694254", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 92301, "upload_time": "2020-01-18T10:29:43", "upload_time_iso_8601": "2020-01-18T10:29:43.453830Z", "url": "https://files.pythonhosted.org/packages/9b/9a/0085889c6cff393c42cdc42d6e9d15663cb38b8e5d3e802f970e9b8394e5/simpletransformers-0.18.5.tar.gz", "yanked": false}], "0.18.6": [{"comment_text": "", "digests": {"md5": "997e53303959ae8a4aac7810247123ec", "sha256": "7ab816c9d15d78ec02abf7b801f4a0c72e98b2002ef1421b4495212ad2416b50"}, "downloads": -1, "filename": "simpletransformers-0.18.6-py3-none-any.whl", "has_sig": false, "md5_digest": "997e53303959ae8a4aac7810247123ec", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 98147, "upload_time": "2020-01-18T16:58:16", "upload_time_iso_8601": "2020-01-18T16:58:16.973550Z", "url": "https://files.pythonhosted.org/packages/7a/5f/b4e2c380fa4e013989b3804e4d2ba065aecafecdb0518048a992f3fd1e11/simpletransformers-0.18.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "06bf52252edcfcbfbd16510b3d0d66f9", "sha256": "d809eec313e50ce103ee497fb4306d447f2996184d5f3b9c22658f4b02d9e51e"}, "downloads": -1, "filename": "simpletransformers-0.18.6.tar.gz", "has_sig": false, "md5_digest": "06bf52252edcfcbfbd16510b3d0d66f9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 92255, "upload_time": "2020-01-18T16:58:18", "upload_time_iso_8601": "2020-01-18T16:58:18.552259Z", "url": "https://files.pythonhosted.org/packages/28/f2/15e5433bb6d645cfbe8981899f11b371e5deaef4c41b9182f27972ebe89c/simpletransformers-0.18.6.tar.gz", "yanked": false}], "0.18.7": [{"comment_text": "", "digests": {"md5": "a7c6b2314438bf74078519b03850932c", "sha256": "f95f183734774c913408bb0d773109ecef0e61f5cec5d836c9af372aa4ac3023"}, "downloads": -1, "filename": "simpletransformers-0.18.7-py3-none-any.whl", "has_sig": false, "md5_digest": "a7c6b2314438bf74078519b03850932c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 98143, "upload_time": "2020-01-18T19:28:38", "upload_time_iso_8601": "2020-01-18T19:28:38.798738Z", "url": "https://files.pythonhosted.org/packages/d9/8a/bd537b9ffd5825e34314b5add550da696e6d6c5e0d0f22a454e591485156/simpletransformers-0.18.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a40b527fb438c1d9cd65b9dbd8017149", "sha256": "a05c2cca2da057636f68d9a67f94579ef35171184ccc78439caf0a0c8026c22d"}, "downloads": -1, "filename": "simpletransformers-0.18.7.tar.gz", "has_sig": false, "md5_digest": "a40b527fb438c1d9cd65b9dbd8017149", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 92257, "upload_time": "2020-01-18T19:28:40", "upload_time_iso_8601": "2020-01-18T19:28:40.980847Z", "url": "https://files.pythonhosted.org/packages/98/aa/441182dc1801b3cd927a74e58e4a43d6f7dc12ef60c704eb6d3fe6937b0d/simpletransformers-0.18.7.tar.gz", "yanked": false}], "0.18.8": [{"comment_text": "", "digests": {"md5": "57b9969578e467b3cb97650cc631c420", "sha256": "e97e666aea471b0d399c1b8ee76d64ac1a73140d69ee18233d0c8c8963724b87"}, "downloads": -1, "filename": "simpletransformers-0.18.8-py3-none-any.whl", "has_sig": false, "md5_digest": "57b9969578e467b3cb97650cc631c420", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 98327, "upload_time": "2020-01-20T09:35:22", "upload_time_iso_8601": "2020-01-20T09:35:22.428834Z", "url": "https://files.pythonhosted.org/packages/f3/96/7a0ec7b05f06969cef4255ab43a8076fb65f82426f0e43a461a745ddbc1c/simpletransformers-0.18.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0859dc807ff4af250b0a22b66ed865d2", "sha256": "1471a57a6e4e7692e00a848b5db3b007826116b647484b8cfc0948be82a7b0bd"}, "downloads": -1, "filename": "simpletransformers-0.18.8.tar.gz", "has_sig": false, "md5_digest": "0859dc807ff4af250b0a22b66ed865d2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 92435, "upload_time": "2020-01-20T09:35:24", "upload_time_iso_8601": "2020-01-20T09:35:24.121024Z", "url": "https://files.pythonhosted.org/packages/ea/e3/20fffbfd7fa7390bd17eed04c5474a523e2d60541b30e644d40f3e883092/simpletransformers-0.18.8.tar.gz", "yanked": false}], "0.18.9": [{"comment_text": "", "digests": {"md5": "fea806acb01b1abe212e5ffeaaa26344", "sha256": "7e21a277ea30c4a103ec86392be8b818eadfa6e1dcddaf725e057a3457560ddb"}, "downloads": -1, "filename": "simpletransformers-0.18.9-py3-none-any.whl", "has_sig": false, "md5_digest": "fea806acb01b1abe212e5ffeaaa26344", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 98196, "upload_time": "2020-01-20T10:54:05", "upload_time_iso_8601": "2020-01-20T10:54:05.516286Z", "url": "https://files.pythonhosted.org/packages/35/37/02303f1aba5b5eec0db39eb2a84be91677837bee5aa392af0d3f98f07cf0/simpletransformers-0.18.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ed71a68f7fa64c976a4a29b68e5d9115", "sha256": "128525d2d4965de7a30c1bfec78a330e27b247c6b0a3db29bd6af909cbfd21cd"}, "downloads": -1, "filename": "simpletransformers-0.18.9.tar.gz", "has_sig": false, "md5_digest": "ed71a68f7fa64c976a4a29b68e5d9115", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 92328, "upload_time": "2020-01-20T10:54:07", "upload_time_iso_8601": "2020-01-20T10:54:07.272474Z", "url": "https://files.pythonhosted.org/packages/66/36/eeb48385d28e76dcd1fbc538c3aa79366d198dd826093438f1dbf50cc719/simpletransformers-0.18.9.tar.gz", "yanked": false}], "0.19.0": [{"comment_text": "", "digests": {"md5": "f8d517a322927753eca19d5c6cb26a91", "sha256": "b4d72269411a7f9431fed90042b43a8283828e24e15cf606416f65284f192e30"}, "downloads": -1, "filename": "simpletransformers-0.19.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f8d517a322927753eca19d5c6cb26a91", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 100899, "upload_time": "2020-01-26T14:19:31", "upload_time_iso_8601": "2020-01-26T14:19:31.854777Z", "url": "https://files.pythonhosted.org/packages/ce/ae/dbbf9e2ad8846242f3d07e9458e89316f29113f454190da94df3fc70e9db/simpletransformers-0.19.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d8ea17e6eacc58e1119aec2150d99170", "sha256": "cf0284453d7b3ddc96ed88e5ca5b0aaa9ff5c6eac57c1b9b99ed359e1b1549b3"}, "downloads": -1, "filename": "simpletransformers-0.19.0.tar.gz", "has_sig": false, "md5_digest": "d8ea17e6eacc58e1119aec2150d99170", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 100050, "upload_time": "2020-01-26T14:19:33", "upload_time_iso_8601": "2020-01-26T14:19:33.472846Z", "url": "https://files.pythonhosted.org/packages/60/7b/fe0829d1ed72fc1b567a961bb418640fde29d004f75c4c54b8867e6bc838/simpletransformers-0.19.0.tar.gz", "yanked": false}], "0.19.1": [{"comment_text": "", "digests": {"md5": "7874f50a8158bb17c43740d09d39dc2b", "sha256": "a17b5d971e3197f2efe02338c39abd310fa7ed5464edb27acd2467d4825f616f"}, "downloads": -1, "filename": "simpletransformers-0.19.1-py3-none-any.whl", "has_sig": false, "md5_digest": "7874f50a8158bb17c43740d09d39dc2b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 100981, "upload_time": "2020-01-27T04:29:38", "upload_time_iso_8601": "2020-01-27T04:29:38.089408Z", "url": "https://files.pythonhosted.org/packages/cb/1c/20cb64c49ff2359ea1973815544492b4656ae02952940dc4b543697e3eb6/simpletransformers-0.19.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0a1985476acf07fe2e9f6be8e37779d7", "sha256": "d42377b08b763da033fac7cc3633b125c652c75b8b4f31fb143b1af6641d0432"}, "downloads": -1, "filename": "simpletransformers-0.19.1.tar.gz", "has_sig": false, "md5_digest": "0a1985476acf07fe2e9f6be8e37779d7", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 100132, "upload_time": "2020-01-27T04:29:39", "upload_time_iso_8601": "2020-01-27T04:29:39.878831Z", "url": "https://files.pythonhosted.org/packages/40/1d/9acddcb20a8a4a0d328fc6f49e47df93b011ff9a6a5cb6e86ff421bc391b/simpletransformers-0.19.1.tar.gz", "yanked": false}], "0.19.2": [{"comment_text": "", "digests": {"md5": "057088fe6f1f027d8dd59997cf1a1301", "sha256": "37559442f4011a3c818c313a22560fa347c22bfe04832497b1c436fc8c83be0c"}, "downloads": -1, "filename": "simpletransformers-0.19.2-py3-none-any.whl", "has_sig": false, "md5_digest": "057088fe6f1f027d8dd59997cf1a1301", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 101075, "upload_time": "2020-01-31T18:18:39", "upload_time_iso_8601": "2020-01-31T18:18:39.262905Z", "url": "https://files.pythonhosted.org/packages/1b/e9/1fbc57f897dcc59e603dceaa2fe3fdf21bb996007e44cae80a4f0464b857/simpletransformers-0.19.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "61b36c0de35307ccd48efab5a177213a", "sha256": "bbf9cf68117bc04f3108bff9f5f7a15426b97e862cfd34acb6f270253fae52a2"}, "downloads": -1, "filename": "simpletransformers-0.19.2.tar.gz", "has_sig": false, "md5_digest": "61b36c0de35307ccd48efab5a177213a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 100266, "upload_time": "2020-01-31T18:18:41", "upload_time_iso_8601": "2020-01-31T18:18:41.900439Z", "url": "https://files.pythonhosted.org/packages/4b/29/189de40e497d520c6cce1ff29933429c8bd89645c22bba41e3c17387bd38/simpletransformers-0.19.2.tar.gz", "yanked": false}], "0.19.3": [{"comment_text": "", "digests": {"md5": "5aff9c2035b6dbccc4c4d6447c89bc70", "sha256": "2b880f8c24059899d7c92e5a548f6c3aaf280bde4504c4f8852d3a84aec0b6a8"}, "downloads": -1, "filename": "simpletransformers-0.19.3-py3-none-any.whl", "has_sig": false, "md5_digest": "5aff9c2035b6dbccc4c4d6447c89bc70", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 101311, "upload_time": "2020-02-03T16:56:53", "upload_time_iso_8601": "2020-02-03T16:56:53.819020Z", "url": "https://files.pythonhosted.org/packages/79/e0/de87860681c6ca8089b21c93c0d9c583135e1b8f735a1fcdd7b2b172c824/simpletransformers-0.19.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "14da6768874af5cc2b098e9d141ad623", "sha256": "b20400e9de6ac0ee08484718faec02186deb16991b4564231552b9b51749354a"}, "downloads": -1, "filename": "simpletransformers-0.19.3.tar.gz", "has_sig": false, "md5_digest": "14da6768874af5cc2b098e9d141ad623", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 100537, "upload_time": "2020-02-03T16:56:56", "upload_time_iso_8601": "2020-02-03T16:56:56.076187Z", "url": "https://files.pythonhosted.org/packages/59/cf/ee0c21456949cd920231ecbdd69b13454a9e13b51ce86394653d3779ad3c/simpletransformers-0.19.3.tar.gz", "yanked": false}], "0.19.4": [{"comment_text": "", "digests": {"md5": "c21707a1185719251eef9f4e9a5b0165", "sha256": "1334fd492650052b0db53f36e6ca0cae4f60d473253d5ceb74b929a44d7ba9d6"}, "downloads": -1, "filename": "simpletransformers-0.19.4-py3-none-any.whl", "has_sig": false, "md5_digest": "c21707a1185719251eef9f4e9a5b0165", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 101325, "upload_time": "2020-02-04T11:40:43", "upload_time_iso_8601": "2020-02-04T11:40:43.028129Z", "url": "https://files.pythonhosted.org/packages/50/38/8aa5d194930c7fcd7a331defba1813968469b3638d5c0f4d14dc8cd8f7f1/simpletransformers-0.19.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ea8c4727a396d768e95109ceea296a63", "sha256": "3589e4bde5b23fd1277bdb884e1d79279ce44965117f703da9e9e2478976830a"}, "downloads": -1, "filename": "simpletransformers-0.19.4.tar.gz", "has_sig": false, "md5_digest": "ea8c4727a396d768e95109ceea296a63", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 100548, "upload_time": "2020-02-04T11:40:45", "upload_time_iso_8601": "2020-02-04T11:40:45.373037Z", "url": "https://files.pythonhosted.org/packages/a9/90/32b85c05e2f4ce0db5c7aa34d2788e2c90f68c5fd6b8b5c735b44f97ddc5/simpletransformers-0.19.4.tar.gz", "yanked": false}], "0.19.5": [{"comment_text": "", "digests": {"md5": "bb921e161e5566dba37ffa058adb0783", "sha256": "048f562c5817fdbb81f6847aec2e59244683240917bf05dc2788471db385d134"}, "downloads": -1, "filename": "simpletransformers-0.19.5-py3-none-any.whl", "has_sig": false, "md5_digest": "bb921e161e5566dba37ffa058adb0783", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 103028, "upload_time": "2020-02-10T18:56:21", "upload_time_iso_8601": "2020-02-10T18:56:21.434782Z", "url": "https://files.pythonhosted.org/packages/8a/f9/4e8fa640a06d93bc8bb23b7a9b98de4174297e4641e1a762c46d17fea094/simpletransformers-0.19.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8b5eb5fbd2e690ce1144c48d4b221a2b", "sha256": "0f5e4f49d211418c851df0976184a0eebdcc64c5270de9a5cd7552df31ddce40"}, "downloads": -1, "filename": "simpletransformers-0.19.5.tar.gz", "has_sig": false, "md5_digest": "8b5eb5fbd2e690ce1144c48d4b221a2b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 101225, "upload_time": "2020-02-10T18:56:23", "upload_time_iso_8601": "2020-02-10T18:56:23.681895Z", "url": "https://files.pythonhosted.org/packages/92/e5/9a4bb16945911e08b858de3ecb4e4e2950a3f3437ef079f11a68b7b85c0b/simpletransformers-0.19.5.tar.gz", "yanked": false}], "0.19.6": [{"comment_text": "", "digests": {"md5": "7b87d425547402c78ac504817fbc8787", "sha256": "7248cf08b81679140075bdb035efd47cf6b19125e4382c13ebec42133f3476e2"}, "downloads": -1, "filename": "simpletransformers-0.19.6-py3-none-any.whl", "has_sig": false, "md5_digest": "7b87d425547402c78ac504817fbc8787", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 103140, "upload_time": "2020-02-11T08:52:35", "upload_time_iso_8601": "2020-02-11T08:52:35.194307Z", "url": "https://files.pythonhosted.org/packages/80/77/d89d474f2a607114a70839e6ed6b2d35eee0c078ef734d7592b5776140ac/simpletransformers-0.19.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ff178d19384ea10fc2394452358f8a5f", "sha256": "408b48407ba89ae4dce18fbaa7213f565f402da7f06b31f2dc4cabd4c48c82b7"}, "downloads": -1, "filename": "simpletransformers-0.19.6.tar.gz", "has_sig": false, "md5_digest": "ff178d19384ea10fc2394452358f8a5f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 101562, "upload_time": "2020-02-11T08:52:37", "upload_time_iso_8601": "2020-02-11T08:52:37.213951Z", "url": "https://files.pythonhosted.org/packages/3c/7e/26f056eb2c17a1be2888bbf2132621cc350253eb7a513b77643a49b3d3fc/simpletransformers-0.19.6.tar.gz", "yanked": false}], "0.19.7": [{"comment_text": "", "digests": {"md5": "4927342f80baebcf4d41828139a454a7", "sha256": "4e37a577eae1d518c348821651da9adfc8c541744b28ce07a63f03b9790dabc6"}, "downloads": -1, "filename": "simpletransformers-0.19.7-py3-none-any.whl", "has_sig": false, "md5_digest": "4927342f80baebcf4d41828139a454a7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 103471, "upload_time": "2020-02-11T09:52:34", "upload_time_iso_8601": "2020-02-11T09:52:34.519062Z", "url": "https://files.pythonhosted.org/packages/8c/e3/b36352e0b704778cc6a44741a51b8631e3d6275525742f0d5a41f0dcb4b4/simpletransformers-0.19.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d2a3d831048b1f41bf52db73f993c85e", "sha256": "af90417603b20d229c9e641fb085a3d8b79a3e2f1e755798aa73839e755624a5"}, "downloads": -1, "filename": "simpletransformers-0.19.7.tar.gz", "has_sig": false, "md5_digest": "d2a3d831048b1f41bf52db73f993c85e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 101994, "upload_time": "2020-02-11T09:52:36", "upload_time_iso_8601": "2020-02-11T09:52:36.842860Z", "url": "https://files.pythonhosted.org/packages/1d/30/e2db0ee998e6d6b7a884a309fd45520c27249a65e193b69058c923a86734/simpletransformers-0.19.7.tar.gz", "yanked": false}], "0.19.8": [{"comment_text": "", "digests": {"md5": "3f1fe9f1be722eff676d7f924cf6adca", "sha256": "1a2c472e773cda334c1591257275799fa6369ec01bfe3770bd831ef909d60691"}, "downloads": -1, "filename": "simpletransformers-0.19.8-py3-none-any.whl", "has_sig": false, "md5_digest": "3f1fe9f1be722eff676d7f924cf6adca", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 103473, "upload_time": "2020-02-14T13:39:01", "upload_time_iso_8601": "2020-02-14T13:39:01.795564Z", "url": "https://files.pythonhosted.org/packages/6c/df/032c8a194eceacac7f28c26ec68dbe1c28a6a7acc8dd3dc07fa7a0a0953f/simpletransformers-0.19.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "df294ba5c283726ebc99fd211137015c", "sha256": "e7ba885b58ee6d6092f3fcc7c76782e2fd8b58130995907e43f512c60d616357"}, "downloads": -1, "filename": "simpletransformers-0.19.8.tar.gz", "has_sig": false, "md5_digest": "df294ba5c283726ebc99fd211137015c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 101979, "upload_time": "2020-02-14T13:39:04", "upload_time_iso_8601": "2020-02-14T13:39:04.091717Z", "url": "https://files.pythonhosted.org/packages/ca/3e/f742e8b33f2d7e9edb613ea9af61f4b96f0d015b1ab563baa515fd4bf0e3/simpletransformers-0.19.8.tar.gz", "yanked": false}], "0.19.9": [{"comment_text": "", "digests": {"md5": "8e1c13001b677a460cc9e89df3e45ba2", "sha256": "9ad86a035dfb6328cd16173aa67c6074c4709df485da0a81e776f3a23e762032"}, "downloads": -1, "filename": "simpletransformers-0.19.9-py3-none-any.whl", "has_sig": false, "md5_digest": "8e1c13001b677a460cc9e89df3e45ba2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 103509, "upload_time": "2020-02-18T09:26:13", "upload_time_iso_8601": "2020-02-18T09:26:13.402970Z", "url": "https://files.pythonhosted.org/packages/d7/f2/0c95afc418e6f81f4c99aed4b40c2ba3093366a1829f3a0d100284bd6e68/simpletransformers-0.19.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9736a5ab46d1f2e3fcf1e76cfc7c6eb0", "sha256": "3d4371d6c8aa56e50e605d408e4fcd67943f590a833f69bb274a585e698c82d5"}, "downloads": -1, "filename": "simpletransformers-0.19.9.tar.gz", "has_sig": false, "md5_digest": "9736a5ab46d1f2e3fcf1e76cfc7c6eb0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 102007, "upload_time": "2020-02-18T09:26:15", "upload_time_iso_8601": "2020-02-18T09:26:15.632437Z", "url": "https://files.pythonhosted.org/packages/33/fc/e3073b5e2d67f644d9cbcdbc1bce1c4cd0c7abebae9ae1323a8bd307c75a/simpletransformers-0.19.9.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "3c474f2243129c803e3c3a9a6f7d6444", "sha256": "28f9b2af8d2206f511accf016e2eda5c7d20c7ce3e797d3f1e0a64e05866a7d0"}, "downloads": -1, "filename": "simpletransformers-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3c474f2243129c803e3c3a9a6f7d6444", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 26745, "upload_time": "2019-10-05T15:08:24", "upload_time_iso_8601": "2019-10-05T15:08:24.651395Z", "url": "https://files.pythonhosted.org/packages/21/25/c9c9a6cf57a56859a0cf3656aa808bf8446562d95e3587e99dfdee05864a/simpletransformers-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7ea3be62c57291e34a6c7bcfdad196c3", "sha256": "d4d7a3bcfc5c2052cfbbbd34461a23f6e83cb0b2ec8a3f012a294493608e709f"}, "downloads": -1, "filename": "simpletransformers-0.2.0.tar.gz", "has_sig": false, "md5_digest": "7ea3be62c57291e34a6c7bcfdad196c3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16192, "upload_time": "2019-10-05T15:08:26", "upload_time_iso_8601": "2019-10-05T15:08:26.519768Z", "url": "https://files.pythonhosted.org/packages/48/61/acb8b769aae3d06a8b8a2811911e8be0ce137725893b1bad4b82d2074b2a/simpletransformers-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "4932f22be251766da9523a13c85c1592", "sha256": "a5c9b1266e99fade8509f44e70317df07bc53655877c65d006a995987137b7c7"}, "downloads": -1, "filename": "simpletransformers-0.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "4932f22be251766da9523a13c85c1592", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 26754, "upload_time": "2019-10-05T16:02:46", "upload_time_iso_8601": "2019-10-05T16:02:46.272417Z", "url": "https://files.pythonhosted.org/packages/50/75/b9844ab0786387fb092bf6dc561626da1c60670d72f011bd7971f42de867/simpletransformers-0.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ccce807233887c6d64db0a6f00ea89c1", "sha256": "799b0d01618ad631050d45c75493e301150f7ced9b9b8bd069c3e7d6b855a81b"}, "downloads": -1, "filename": "simpletransformers-0.2.1.tar.gz", "has_sig": false, "md5_digest": "ccce807233887c6d64db0a6f00ea89c1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16202, "upload_time": "2019-10-05T16:02:48", "upload_time_iso_8601": "2019-10-05T16:02:48.414586Z", "url": "https://files.pythonhosted.org/packages/a2/11/4876a55256e750f0bb343b18d344656bfd81434bfa75d40c9435b810b0f1/simpletransformers-0.2.1.tar.gz", "yanked": false}], "0.2.10": [{"comment_text": "", "digests": {"md5": "0d426e5755411abc96ead7de358cc86e", "sha256": "cfed8bff1d70f2cb4ebcb4ab2e3a464ddfd2bbbfea320a2360c437f4e68b5841"}, "downloads": -1, "filename": "simpletransformers-0.2.10-py3-none-any.whl", "has_sig": false, "md5_digest": "0d426e5755411abc96ead7de358cc86e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 17811, "upload_time": "2019-10-07T13:52:34", "upload_time_iso_8601": "2019-10-07T13:52:34.938804Z", "url": "https://files.pythonhosted.org/packages/47/08/b1e9c177572379ab9ce5a38645fe1a1252b93eed4fa8001826bff6d30c30/simpletransformers-0.2.10-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fc686e0336a7dc29ba651be57c1ae416", "sha256": "a6cfef28e37fac94b302f27691e341cd1686055851a88360c268a22ad61289eb"}, "downloads": -1, "filename": "simpletransformers-0.2.10.tar.gz", "has_sig": false, "md5_digest": "fc686e0336a7dc29ba651be57c1ae416", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16394, "upload_time": "2019-10-07T13:52:39", "upload_time_iso_8601": "2019-10-07T13:52:39.435620Z", "url": "https://files.pythonhosted.org/packages/22/98/eacf69f253d89c36c7e1b8bdb5f19cee181d375f5144888438a143ce9f93/simpletransformers-0.2.10.tar.gz", "yanked": false}], "0.2.11": [{"comment_text": "", "digests": {"md5": "f5785cc48414b7c5c2f3e122c4d7a653", "sha256": "6f136afd1a653fbd2929cdbfcb7eaef6ab068cb0434563e7c49c5aa49c0faf2c"}, "downloads": -1, "filename": "simpletransformers-0.2.11-py3-none-any.whl", "has_sig": false, "md5_digest": "f5785cc48414b7c5c2f3e122c4d7a653", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 17816, "upload_time": "2019-10-07T13:54:10", "upload_time_iso_8601": "2019-10-07T13:54:10.785841Z", "url": "https://files.pythonhosted.org/packages/aa/a3/46d79792da536a52bfd69d28b574f890d8aa9f0bf45fa6d53b70d8ba2e6a/simpletransformers-0.2.11-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bc05d608b4abc0a6aa172f4eb6a22cf1", "sha256": "25d02af569418784d5b8cecfcbe6a71e8ff8a170fba5806409684a844b3edcce"}, "downloads": -1, "filename": "simpletransformers-0.2.11.tar.gz", "has_sig": false, "md5_digest": "bc05d608b4abc0a6aa172f4eb6a22cf1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16394, "upload_time": "2019-10-07T13:54:12", "upload_time_iso_8601": "2019-10-07T13:54:12.606418Z", "url": "https://files.pythonhosted.org/packages/b8/4f/467ad9e13661a386f186df703b13b740d3be296ef60a1abba64567303e55/simpletransformers-0.2.11.tar.gz", "yanked": false}], "0.2.12": [{"comment_text": "", "digests": {"md5": "4550a2b53ce2fb81d9b0aab9b02722eb", "sha256": "14c8738dcca59d9b3c7217aae910ed1f881901e54382ab66e42f3ba8907dab04"}, "downloads": -1, "filename": "simpletransformers-0.2.12-py3-none-any.whl", "has_sig": false, "md5_digest": "4550a2b53ce2fb81d9b0aab9b02722eb", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 17815, "upload_time": "2019-10-07T13:56:28", "upload_time_iso_8601": "2019-10-07T13:56:28.957519Z", "url": "https://files.pythonhosted.org/packages/8e/1c/748b414cb63ec3018be137a1a0fca934bac2ed95c45c7ae7cc13741ec3d7/simpletransformers-0.2.12-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ac66c1e7101c5fc9090839f76a9e4b56", "sha256": "89c7aa4d3554e902d87afef51f0ad2a2dfbd94dfb83f80c053e91608ce0c3bca"}, "downloads": -1, "filename": "simpletransformers-0.2.12.tar.gz", "has_sig": false, "md5_digest": "ac66c1e7101c5fc9090839f76a9e4b56", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16392, "upload_time": "2019-10-07T13:56:32", "upload_time_iso_8601": "2019-10-07T13:56:32.689584Z", "url": "https://files.pythonhosted.org/packages/25/8f/afe6d62e3325a0b02ed97ebba27b8095ba8c9b5ce7de2181da01372d6d71/simpletransformers-0.2.12.tar.gz", "yanked": false}], "0.2.13": [{"comment_text": "", "digests": {"md5": "43f87dc1866eeed196562590f91cbc28", "sha256": "c0c83da99e23ea7f430cd97f052a51581e994143e524ced70b17f5fe75067971"}, "downloads": -1, "filename": "simpletransformers-0.2.13-py3-none-any.whl", "has_sig": false, "md5_digest": "43f87dc1866eeed196562590f91cbc28", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 17937, "upload_time": "2019-10-10T14:10:18", "upload_time_iso_8601": "2019-10-10T14:10:18.450938Z", "url": "https://files.pythonhosted.org/packages/25/0e/d476859224ffe4133716c375380b715e26f9b4f32718d93cc71c5382d125/simpletransformers-0.2.13-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7f6e8c2b31ac6b802e156a9b7c0bebe3", "sha256": "25f3c34ec429808396daec0c8bc6fdbc80042f5afc9ae27f1bff4f691d8592aa"}, "downloads": -1, "filename": "simpletransformers-0.2.13.tar.gz", "has_sig": false, "md5_digest": "7f6e8c2b31ac6b802e156a9b7c0bebe3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16595, "upload_time": "2019-10-10T14:10:28", "upload_time_iso_8601": "2019-10-10T14:10:28.158784Z", "url": "https://files.pythonhosted.org/packages/57/69/bcc4975e672c1e0d974a1a5e7b704a0fd14651f6eeffaee77553add3fdf8/simpletransformers-0.2.13.tar.gz", "yanked": false}], "0.2.4": [{"comment_text": "", "digests": {"md5": "6fbb23789fd04600c928ec7a1df26f6e", "sha256": "fd89980b29e4a750aba5369b5e98b6e84fd3322046dfc89d1208a950e3f2c7c2"}, "downloads": -1, "filename": "simpletransformers-0.2.4-py3-none-any.whl", "has_sig": false, "md5_digest": "6fbb23789fd04600c928ec7a1df26f6e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 26756, "upload_time": "2019-10-05T16:06:37", "upload_time_iso_8601": "2019-10-05T16:06:37.850149Z", "url": "https://files.pythonhosted.org/packages/e0/f1/5fec57d08fbcbfcb49dae2cde27d4be4de24674f59147b9554e6b03b282a/simpletransformers-0.2.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a62d15c1fa2119e5a600f4fdca2fc086", "sha256": "6fb4b7589885018c403243318e652e3de8fea8cf8665cdb813b2aabd9471658a"}, "downloads": -1, "filename": "simpletransformers-0.2.4.tar.gz", "has_sig": false, "md5_digest": "a62d15c1fa2119e5a600f4fdca2fc086", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16201, "upload_time": "2019-10-05T16:06:39", "upload_time_iso_8601": "2019-10-05T16:06:39.519254Z", "url": "https://files.pythonhosted.org/packages/69/3e/96b78bf3c67e54f199479ec096d56673634d003f79dd8eca8c6eb9bb19df/simpletransformers-0.2.4.tar.gz", "yanked": false}], "0.2.5": [{"comment_text": "", "digests": {"md5": "9d1920c28d42a54b5d8daaaf94d483cc", "sha256": "cc6f78a8d3a1670cebd6d136ecb97b9a0599adc198ed19b5a9bac352677ddd4b"}, "downloads": -1, "filename": "simpletransformers-0.2.5-py3-none-any.whl", "has_sig": false, "md5_digest": "9d1920c28d42a54b5d8daaaf94d483cc", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 26817, "upload_time": "2019-10-07T13:30:23", "upload_time_iso_8601": "2019-10-07T13:30:23.126705Z", "url": "https://files.pythonhosted.org/packages/b6/31/a826a0e9d9c9f868c88f1ef888515126d2afa60fb5dbfd5520cabe347529/simpletransformers-0.2.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6222edbad0a8945851ca96bb271870fe", "sha256": "648bd991917ece7ba70b2d79d28e04ead0b418370796979811bd02145fa87547"}, "downloads": -1, "filename": "simpletransformers-0.2.5.tar.gz", "has_sig": false, "md5_digest": "6222edbad0a8945851ca96bb271870fe", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16230, "upload_time": "2019-10-05T16:09:30", "upload_time_iso_8601": "2019-10-05T16:09:30.562039Z", "url": "https://files.pythonhosted.org/packages/b5/49/b4c9bd60462a979f795e24845b06612bafd044718b1256b77e8ca422b264/simpletransformers-0.2.5.tar.gz", "yanked": false}], "0.2.6": [{"comment_text": "", "digests": {"md5": "d72967f7d4c53d4528a83ddb2a788421", "sha256": "e4ebb7f2d007d8a0b54f2cf6755b3875473d8030284815864004e58466e59fb6"}, "downloads": -1, "filename": "simpletransformers-0.2.6-py3-none-any.whl", "has_sig": false, "md5_digest": "d72967f7d4c53d4528a83ddb2a788421", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 26879, "upload_time": "2019-10-07T13:34:13", "upload_time_iso_8601": "2019-10-07T13:34:13.321637Z", "url": "https://files.pythonhosted.org/packages/1b/3a/33e904bfa40115cf16b3bc3d8dac7c1cb6a3b9eb6d1addf42186f81244ea/simpletransformers-0.2.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fc683a97f7c9a360e9ea257d2e1df313", "sha256": "55cec2cd192d72e2f5ab41544f93547a366ab632eba655b05f1a6bc24d0a895f"}, "downloads": -1, "filename": "simpletransformers-0.2.6.tar.gz", "has_sig": false, "md5_digest": "fc683a97f7c9a360e9ea257d2e1df313", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16396, "upload_time": "2019-10-07T13:34:15", "upload_time_iso_8601": "2019-10-07T13:34:15.653267Z", "url": "https://files.pythonhosted.org/packages/a5/3c/125727580b67bc9f19bd1298faefd53c06a0a7641105cdffc1371f13c4c5/simpletransformers-0.2.6.tar.gz", "yanked": false}], "0.2.7": [{"comment_text": "", "digests": {"md5": "7fbeb204624eba0ddb2f91e395abb745", "sha256": "813d635c4ac819da9a567545a619e61754017c31dfd02f965f62933a5a410113"}, "downloads": -1, "filename": "simpletransformers-0.2.7-py3-none-any.whl", "has_sig": false, "md5_digest": "7fbeb204624eba0ddb2f91e395abb745", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 26884, "upload_time": "2019-10-07T13:40:52", "upload_time_iso_8601": "2019-10-07T13:40:52.311608Z", "url": "https://files.pythonhosted.org/packages/56/9e/4e42f0b7c06b6d40c6b24a2fd923c4dea0880554d0eb88cdd2ccc0df08c3/simpletransformers-0.2.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6df8acb62063bb2a99aa390f033c8874", "sha256": "a256989db430b54c51fe832ff74a87578a4e6a0df92cdab7b1a802dffa20ea41"}, "downloads": -1, "filename": "simpletransformers-0.2.7.tar.gz", "has_sig": false, "md5_digest": "6df8acb62063bb2a99aa390f033c8874", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16391, "upload_time": "2019-10-07T13:40:54", "upload_time_iso_8601": "2019-10-07T13:40:54.525041Z", "url": "https://files.pythonhosted.org/packages/69/1c/355763bb8892b34505c67aaa997dadf94c065e6779a5c18d5881d934759f/simpletransformers-0.2.7.tar.gz", "yanked": false}], "0.2.8": [{"comment_text": "", "digests": {"md5": "2c7772ee74a23ded69ad197197efc1a8", "sha256": "56ee43d8d804cfadeb432ec0709f95bc59f5665fc73a1fa47d7b6790162fd9cb"}, "downloads": -1, "filename": "simpletransformers-0.2.8-py3-none-any.whl", "has_sig": false, "md5_digest": "2c7772ee74a23ded69ad197197efc1a8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 17804, "upload_time": "2019-10-07T13:47:16", "upload_time_iso_8601": "2019-10-07T13:47:16.194782Z", "url": "https://files.pythonhosted.org/packages/57/16/be424c1ad8b1e1a73712efdd2bec1a64ee0fa86af9215fa118ce11ed4614/simpletransformers-0.2.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f291c0fd845437a387c8f922c151d5db", "sha256": "4359bac3f5489be5b0db0a3463bdda79bbb18ef418ebf5631d445697cec56f36"}, "downloads": -1, "filename": "simpletransformers-0.2.8.tar.gz", "has_sig": false, "md5_digest": "f291c0fd845437a387c8f922c151d5db", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16396, "upload_time": "2019-10-07T13:47:18", "upload_time_iso_8601": "2019-10-07T13:47:18.546784Z", "url": "https://files.pythonhosted.org/packages/5c/40/1cab827dd98a813e0aff3a4eef27260019c224acb9503be8e41d82f40cf1/simpletransformers-0.2.8.tar.gz", "yanked": false}], "0.2.9": [{"comment_text": "", "digests": {"md5": "8812a5c589dcee72611d4b36a0db98a1", "sha256": "9636a2d4dbac3c1dfaab7f0de44a97be13cf3e46b88d34fb6b30df764ed74913"}, "downloads": -1, "filename": "simpletransformers-0.2.9-py3-none-any.whl", "has_sig": false, "md5_digest": "8812a5c589dcee72611d4b36a0db98a1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 17804, "upload_time": "2019-10-07T13:52:37", "upload_time_iso_8601": "2019-10-07T13:52:37.066784Z", "url": "https://files.pythonhosted.org/packages/e7/d2/8918a4470b1d1bb3ef48d27e9910126b6b8418cfcd8dd6d248adf21fb467/simpletransformers-0.2.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "35016b5ba9464bf1c22d906d23902930", "sha256": "c0d2d1306467b27d0d06c69e44c97c5cef3c75b481adfff3a32531edcad28de5"}, "downloads": -1, "filename": "simpletransformers-0.2.9.tar.gz", "has_sig": false, "md5_digest": "35016b5ba9464bf1c22d906d23902930", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16391, "upload_time": "2019-10-07T13:52:41", "upload_time_iso_8601": "2019-10-07T13:52:41.638781Z", "url": "https://files.pythonhosted.org/packages/77/df/33c1048e08f80597d913283a735481f79a41f8b504ab9ad5e4674a11a592/simpletransformers-0.2.9.tar.gz", "yanked": false}], "0.20.0": [{"comment_text": "", "digests": {"md5": "8e8ea06e694551b4012453dedff013fb", "sha256": "9e80b63dc6202b949338b3251d822b4aa4eaee6479be7241065e7abf1dad6419"}, "downloads": -1, "filename": "simpletransformers-0.20.0-py3-none-any.whl", "has_sig": false, "md5_digest": "8e8ea06e694551b4012453dedff013fb", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 118999, "upload_time": "2020-02-21T10:53:09", "upload_time_iso_8601": "2020-02-21T10:53:09.150784Z", "url": "https://files.pythonhosted.org/packages/aa/b5/6458a67b3597a93c17f6b52a6924646274d3fc9d09e580215832017ffa32/simpletransformers-0.20.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "17805d10daabdac9d4360a89df50a113", "sha256": "9522fbbae6dbfbdf6bc865c5cfe448d010b31104ef5e587f24e1c89e5dc28564"}, "downloads": -1, "filename": "simpletransformers-0.20.0.tar.gz", "has_sig": false, "md5_digest": "17805d10daabdac9d4360a89df50a113", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 122578, "upload_time": "2020-02-21T10:53:11", "upload_time_iso_8601": "2020-02-21T10:53:11.317269Z", "url": "https://files.pythonhosted.org/packages/79/e8/48555b91bc4524bc793ac00873335ab1444fadd265d0078ef0d772d7fe5a/simpletransformers-0.20.0.tar.gz", "yanked": false}], "0.20.1": [{"comment_text": "", "digests": {"md5": "9381651fb8c83647bfe6ae062ee37e08", "sha256": "ce36ee0e7e4def51bd520b331fd6a16196ccb5159b57711a264d626d41087499"}, "downloads": -1, "filename": "simpletransformers-0.20.1-py3-none-any.whl", "has_sig": false, "md5_digest": "9381651fb8c83647bfe6ae062ee37e08", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 119011, "upload_time": "2020-02-21T20:07:41", "upload_time_iso_8601": "2020-02-21T20:07:41.087783Z", "url": "https://files.pythonhosted.org/packages/2a/92/66d4ae6e07d0c511de36eb1f595af99ec36ce85575ee45549f01796d3883/simpletransformers-0.20.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5885fe0b8393418995723f8bded535ae", "sha256": "30ede4b115aa5c6268e9c5790e2702941658475561cf93e2881937b5435254bf"}, "downloads": -1, "filename": "simpletransformers-0.20.1.tar.gz", "has_sig": false, "md5_digest": "5885fe0b8393418995723f8bded535ae", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 122586, "upload_time": "2020-02-21T20:07:44", "upload_time_iso_8601": "2020-02-21T20:07:44.086781Z", "url": "https://files.pythonhosted.org/packages/f1/8a/367c53f866a5dbeb06d01b7511e5adfa705ed417ebe16fbe52f1482f6d00/simpletransformers-0.20.1.tar.gz", "yanked": false}], "0.20.2": [{"comment_text": "", "digests": {"md5": "f8e5ca053219ba52ad5a1df55fc21bc8", "sha256": "6bc1ca9dfe260872f03b551c25506cf43ec61a30950413aba05e20fd07e0e2ec"}, "downloads": -1, "filename": "simpletransformers-0.20.2-py3-none-any.whl", "has_sig": false, "md5_digest": "f8e5ca053219ba52ad5a1df55fc21bc8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 119020, "upload_time": "2020-02-22T06:07:16", "upload_time_iso_8601": "2020-02-22T06:07:16.627607Z", "url": "https://files.pythonhosted.org/packages/d0/c8/83ce1723d398a8effb1713fd7a1cd8274e77b1fb0a8329b18f114adeb12e/simpletransformers-0.20.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5e8abc8b8481621705d85bcf6e55769b", "sha256": "7b0225c803a9d0c6262b3ffbeddd8ac34c01c348e54f458e3f624dfe955fda4d"}, "downloads": -1, "filename": "simpletransformers-0.20.2.tar.gz", "has_sig": false, "md5_digest": "5e8abc8b8481621705d85bcf6e55769b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 122567, "upload_time": "2020-02-22T06:07:18", "upload_time_iso_8601": "2020-02-22T06:07:18.750782Z", "url": "https://files.pythonhosted.org/packages/92/a3/84fd82a0cf393411fc2ea2726160b7877894a2226d688d6704df0ae45e7d/simpletransformers-0.20.2.tar.gz", "yanked": false}], "0.20.3": [{"comment_text": "", "digests": {"md5": "d5ddc73990a2f460e0ff4ca5d30730e4", "sha256": "64b6f79683a6dee0696abae215ea6cfdc3ad74f75cd65bf678ef21ff817d004c"}, "downloads": -1, "filename": "simpletransformers-0.20.3-py3-none-any.whl", "has_sig": false, "md5_digest": "d5ddc73990a2f460e0ff4ca5d30730e4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 119021, "upload_time": "2020-02-22T08:15:55", "upload_time_iso_8601": "2020-02-22T08:15:55.686092Z", "url": "https://files.pythonhosted.org/packages/78/c4/598c0bed13c5d0ed63a5afbfdaaddf9b2a67ce51015a1800886b392327d2/simpletransformers-0.20.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "33d4ac2a68e1ccc2159c61d5e245f37c", "sha256": "a950e4657920895a2243b86319444e8ae40faa21e74824278beb7fb0e6af46cd"}, "downloads": -1, "filename": "simpletransformers-0.20.3.tar.gz", "has_sig": false, "md5_digest": "33d4ac2a68e1ccc2159c61d5e245f37c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 122585, "upload_time": "2020-02-22T08:15:57", "upload_time_iso_8601": "2020-02-22T08:15:57.743912Z", "url": "https://files.pythonhosted.org/packages/a0/db/f3736783fe0e1f42ecc47c7c849df9d05f4ab414f42582723c84ab3ad15d/simpletransformers-0.20.3.tar.gz", "yanked": false}], "0.21.0": [{"comment_text": "", "digests": {"md5": "ce5bfc914a5057375b6a37ca7c4bc661", "sha256": "3054dc5a2a2dbc67294bbca1c89a43567d2a056e51820d84eaaefc20e2918d27"}, "downloads": -1, "filename": "simpletransformers-0.21.0-py3-none-any.whl", "has_sig": false, "md5_digest": "ce5bfc914a5057375b6a37ca7c4bc661", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 132851, "upload_time": "2020-02-28T20:29:00", "upload_time_iso_8601": "2020-02-28T20:29:00.877377Z", "url": "https://files.pythonhosted.org/packages/0b/93/944caa1748db96144e3b0a1f4ce4bc28de158eb6ebbe9f42d63f2ad92266/simpletransformers-0.21.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ccea6ee89a29aa6d5aa349b733b00bad", "sha256": "0d8fab1f789885e66b7c687f0e4fd28a435eceb871db8daa33a8551ae81055d4"}, "downloads": -1, "filename": "simpletransformers-0.21.0.tar.gz", "has_sig": false, "md5_digest": "ccea6ee89a29aa6d5aa349b733b00bad", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 141969, "upload_time": "2020-02-28T20:29:03", "upload_time_iso_8601": "2020-02-28T20:29:03.505193Z", "url": "https://files.pythonhosted.org/packages/9a/87/25154e870c88e9a5ae57097e9ea49ea872bc2ad369568f91f08a52c576b6/simpletransformers-0.21.0.tar.gz", "yanked": false}], "0.21.1": [{"comment_text": "", "digests": {"md5": "238271240931dea4be78fe603b0bad7b", "sha256": "1b3294fcf19301d21368335c01d0438dc941c56d77c048ea508346925c067839"}, "downloads": -1, "filename": "simpletransformers-0.21.1-py3-none-any.whl", "has_sig": false, "md5_digest": "238271240931dea4be78fe603b0bad7b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 132893, "upload_time": "2020-02-29T08:31:19", "upload_time_iso_8601": "2020-02-29T08:31:19.555384Z", "url": "https://files.pythonhosted.org/packages/07/3d/60b67da1f5386561c1c40891f46a2eb7dc4da7995a619b8dec6a1b814198/simpletransformers-0.21.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "320736f561c242e16da16d4153880045", "sha256": "e2927ed3cb478e9efcdb8c70191d05a8350e96f77e7bc514a0324284cef5f98c"}, "downloads": -1, "filename": "simpletransformers-0.21.1.tar.gz", "has_sig": false, "md5_digest": "320736f561c242e16da16d4153880045", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 142097, "upload_time": "2020-02-29T08:31:22", "upload_time_iso_8601": "2020-02-29T08:31:22.055584Z", "url": "https://files.pythonhosted.org/packages/e6/64/61e9408cf96e261ba0d09c8b3cdcf87509015f3ca5e79c2908740e125e08/simpletransformers-0.21.1.tar.gz", "yanked": false}], "0.21.2": [{"comment_text": "", "digests": {"md5": "ffdd59ce0bfd38dfd5ecef38f3c93b69", "sha256": "40f875d67c7d0b848ef261b22fccfac3497a04c64d48ddd5a669ea5b676c51f4"}, "downloads": -1, "filename": "simpletransformers-0.21.2-py3-none-any.whl", "has_sig": false, "md5_digest": "ffdd59ce0bfd38dfd5ecef38f3c93b69", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 132978, "upload_time": "2020-03-02T17:36:30", "upload_time_iso_8601": "2020-03-02T17:36:30.563192Z", "url": "https://files.pythonhosted.org/packages/07/34/984d240156c157149a3f4b3ef2cd6bd72a38eeaaa201becd3f6f5917ef4b/simpletransformers-0.21.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a7b9ff4bbe45652ad988cbf667f2f3b2", "sha256": "761ddf37ef7db5b0e48b84783775f18970fb406298a7386211b221a8a00c0620"}, "downloads": -1, "filename": "simpletransformers-0.21.2.tar.gz", "has_sig": false, "md5_digest": "a7b9ff4bbe45652ad988cbf667f2f3b2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 142368, "upload_time": "2020-03-02T17:36:32", "upload_time_iso_8601": "2020-03-02T17:36:32.634312Z", "url": "https://files.pythonhosted.org/packages/4a/df/eccaf8935061be8339cb9494e98e6eef3a1791739bae832a5e1fd6e271a2/simpletransformers-0.21.2.tar.gz", "yanked": false}], "0.21.3": [{"comment_text": "", "digests": {"md5": "7ea47097b931bcee104be836154de521", "sha256": "8ea57490126706aad7734b668a100269fd1de61362b1e05466e7d014114e2499"}, "downloads": -1, "filename": "simpletransformers-0.21.3-py3-none-any.whl", "has_sig": false, "md5_digest": "7ea47097b931bcee104be836154de521", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 133085, "upload_time": "2020-03-03T13:25:34", "upload_time_iso_8601": "2020-03-03T13:25:34.454791Z", "url": "https://files.pythonhosted.org/packages/f4/41/418b2e4ffad5f165079a2ab4ec0be6cec4ff48bbddd2193c334b3f2a77db/simpletransformers-0.21.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8430997d148b4fdd86238ab2c33b6274", "sha256": "a933f3391f77e868645cab11960e4e3d2d4fa4188cfdb0d1ee8464eabaf6df02"}, "downloads": -1, "filename": "simpletransformers-0.21.3.tar.gz", "has_sig": false, "md5_digest": "8430997d148b4fdd86238ab2c33b6274", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 142518, "upload_time": "2020-03-03T13:25:36", "upload_time_iso_8601": "2020-03-03T13:25:36.694800Z", "url": "https://files.pythonhosted.org/packages/f0/94/ce8ce32fc36c1d997b80a2101fd1b388b4209015cf8539d54d0f5eb645b5/simpletransformers-0.21.3.tar.gz", "yanked": false}], "0.21.4": [{"comment_text": "", "digests": {"md5": "2994dad9f715aec16445bbed155ae55f", "sha256": "d1d993719f77e452f5a9c9c772a5367654256a5c3274b9fbaea7e3903243a2fb"}, "downloads": -1, "filename": "simpletransformers-0.21.4-py3-none-any.whl", "has_sig": false, "md5_digest": "2994dad9f715aec16445bbed155ae55f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 133276, "upload_time": "2020-03-11T19:14:07", "upload_time_iso_8601": "2020-03-11T19:14:07.242371Z", "url": "https://files.pythonhosted.org/packages/95/c3/626e19dba737b40444fa3eb1843c6cf9341de632a266aeeec96ca62f8801/simpletransformers-0.21.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "22c5d3dacd525f0d5e5a0671dae1edfa", "sha256": "a5d7bc601117d32ec6a15d158754f49c0b3e8415a30a8de0eb721a585078923b"}, "downloads": -1, "filename": "simpletransformers-0.21.4.tar.gz", "has_sig": false, "md5_digest": "22c5d3dacd525f0d5e5a0671dae1edfa", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 143043, "upload_time": "2020-03-11T19:14:09", "upload_time_iso_8601": "2020-03-11T19:14:09.327412Z", "url": "https://files.pythonhosted.org/packages/7e/03/bbe4cc217e65539803d7d58c2bb2ed7c9300ce5104fac610df94f4a042af/simpletransformers-0.21.4.tar.gz", "yanked": false}], "0.21.5": [{"comment_text": "", "digests": {"md5": "763a5a2429c9c4090a4b2d75920a8e2a", "sha256": "c2ae89bbe765f41c7e1868f083a453501a9e6dfa6c666996086663e12dcea4d9"}, "downloads": -1, "filename": "simpletransformers-0.21.5-py3-none-any.whl", "has_sig": false, "md5_digest": "763a5a2429c9c4090a4b2d75920a8e2a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 133282, "upload_time": "2020-03-12T17:52:06", "upload_time_iso_8601": "2020-03-12T17:52:06.066944Z", "url": "https://files.pythonhosted.org/packages/7c/45/f8e5dbf40673dad394939f38988bd41df8bfc6b35964a696cfba1ac3f003/simpletransformers-0.21.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e12a9b129e702d15f508a25a9c9b0618", "sha256": "11a70d6ad5a494f441c12e855c6f19257a750aa1982d5180d3d19cf6e5209645"}, "downloads": -1, "filename": "simpletransformers-0.21.5.tar.gz", "has_sig": false, "md5_digest": "e12a9b129e702d15f508a25a9c9b0618", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 143054, "upload_time": "2020-03-12T17:52:08", "upload_time_iso_8601": "2020-03-12T17:52:08.701583Z", "url": "https://files.pythonhosted.org/packages/3a/7b/16bbead875e79114497d2c83c12901654d1815081e8d56d00124d17b0034/simpletransformers-0.21.5.tar.gz", "yanked": false}], "0.22.0": [{"comment_text": "", "digests": {"md5": "c1f45fcfa0fad418d1ce139921105631", "sha256": "d1347c6fae8967b139da530f1ad4878295e3cd26125ed7948ea95d0bb436d622"}, "downloads": -1, "filename": "simpletransformers-0.22.0-py3-none-any.whl", "has_sig": false, "md5_digest": "c1f45fcfa0fad418d1ce139921105631", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 144410, "upload_time": "2020-03-13T20:17:29", "upload_time_iso_8601": "2020-03-13T20:17:29.915045Z", "url": "https://files.pythonhosted.org/packages/3c/cd/bf795740c4c029ff604aba16a162d5f424411f6e30b46cd21b047c5b8db8/simpletransformers-0.22.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "12dad7e7da4cd18a443b915075f25749", "sha256": "6fcb03f44268c53b6c095c5f650806437be02ce76ef9add12213fb1dbbb60b30"}, "downloads": -1, "filename": "simpletransformers-0.22.0.tar.gz", "has_sig": false, "md5_digest": "12dad7e7da4cd18a443b915075f25749", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 154922, "upload_time": "2020-03-13T20:17:32", "upload_time_iso_8601": "2020-03-13T20:17:32.474682Z", "url": "https://files.pythonhosted.org/packages/6d/8c/0d54044075431c9b8db69f1f64021ba9728d7491ac836824f9e3e17b8d48/simpletransformers-0.22.0.tar.gz", "yanked": false}], "0.22.1": [{"comment_text": "", "digests": {"md5": "c58e43497dd8e51055905a4cb876be28", "sha256": "0c1f00824ec0dd4138cd5eac80fdc6e1ea047ccc91438ce27736db6cebcec5c6"}, "downloads": -1, "filename": "simpletransformers-0.22.1-py3-none-any.whl", "has_sig": false, "md5_digest": "c58e43497dd8e51055905a4cb876be28", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 144484, "upload_time": "2020-03-19T11:28:32", "upload_time_iso_8601": "2020-03-19T11:28:32.689277Z", "url": "https://files.pythonhosted.org/packages/7b/ba/2864d9f710147d7f33523b8b9e00500e4c0928c3fd150104c1b51f9a32a0/simpletransformers-0.22.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "10926a84d5efd101b60ac70502f98e8f", "sha256": "38393a25e574ac199be7c5f908f67d8396c72e19f21b814848d937c0079940b7"}, "downloads": -1, "filename": "simpletransformers-0.22.1.tar.gz", "has_sig": false, "md5_digest": "10926a84d5efd101b60ac70502f98e8f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 155029, "upload_time": "2020-03-19T11:28:34", "upload_time_iso_8601": "2020-03-19T11:28:34.925055Z", "url": "https://files.pythonhosted.org/packages/7b/ed/0beb31ada4c2519ed9b2731794b19776ec51b17d145d943fa5ddf2479f09/simpletransformers-0.22.1.tar.gz", "yanked": false}], "0.23.2": [{"comment_text": "", "digests": {"md5": "624129b9cd969d42247988f64ab31d85", "sha256": "0331d80683ea06c4eb27ade2546bfcd855a3ca83ec159c9eda12616e70a9dc0f"}, "downloads": -1, "filename": "simpletransformers-0.23.2-py3-none-any.whl", "has_sig": false, "md5_digest": "624129b9cd969d42247988f64ab31d85", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 148200, "upload_time": "2020-04-02T13:06:40", "upload_time_iso_8601": "2020-04-02T13:06:40.394993Z", "url": "https://files.pythonhosted.org/packages/e7/75/836693dae9f2fab7dae172c9f7e2e1292898df377e113be06b061f7f9e0a/simpletransformers-0.23.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b2cc5b4893a486ddb84ee9c30bfca8f8", "sha256": "f3f6c6dae4ca3eb2c36de22881c71795bc0a4854390595bbbdec5ee4411699bf"}, "downloads": -1, "filename": "simpletransformers-0.23.2.tar.gz", "has_sig": false, "md5_digest": "b2cc5b4893a486ddb84ee9c30bfca8f8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 161081, "upload_time": "2020-04-02T13:06:42", "upload_time_iso_8601": "2020-04-02T13:06:42.954185Z", "url": "https://files.pythonhosted.org/packages/7c/86/30acb81abdbbd67e56755099be5a27807d008c654de71acbc9694183a769/simpletransformers-0.23.2.tar.gz", "yanked": false}], "0.23.3": [{"comment_text": "", "digests": {"md5": "49eac58762d8ac5a536ec10d98d52269", "sha256": "4be4a83067801de9cf0c70c9c77859a83b81718737042004475f09d7ae3a8a4a"}, "downloads": -1, "filename": "simpletransformers-0.23.3-py3-none-any.whl", "has_sig": false, "md5_digest": "49eac58762d8ac5a536ec10d98d52269", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 148527, "upload_time": "2020-04-05T10:46:08", "upload_time_iso_8601": "2020-04-05T10:46:08.471093Z", "url": "https://files.pythonhosted.org/packages/a3/e8/eae3b97e617d9f30efdb365298e963e2df406647b8165dec14fa6c2a52bd/simpletransformers-0.23.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8fd95d59f61f881afe074d06dbc58689", "sha256": "794189054b3e051acce1d19127ea1a94e5addb37b9cd2834695e7206d6d41d14"}, "downloads": -1, "filename": "simpletransformers-0.23.3.tar.gz", "has_sig": false, "md5_digest": "8fd95d59f61f881afe074d06dbc58689", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 161356, "upload_time": "2020-04-05T10:46:10", "upload_time_iso_8601": "2020-04-05T10:46:10.799590Z", "url": "https://files.pythonhosted.org/packages/66/41/94bae0c723efbbb2dfde4f418473da47995fa01e68a9bfd0abda85f8e56b/simpletransformers-0.23.3.tar.gz", "yanked": false}], "0.24.0": [{"comment_text": "", "digests": {"md5": "76109a302f12e2e87ebd58d2ab6af247", "sha256": "b160d1ba9e1a008694c2b01593d5e640c92df0e2d4039c0e74ae6f03281a773e"}, "downloads": -1, "filename": "simpletransformers-0.24.0-py3-none-any.whl", "has_sig": false, "md5_digest": "76109a302f12e2e87ebd58d2ab6af247", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 150628, "upload_time": "2020-04-09T14:16:04", "upload_time_iso_8601": "2020-04-09T14:16:04.841529Z", "url": "https://files.pythonhosted.org/packages/28/6a/34775dd061cf0903f31f60e9325ec29da8e0cc1ef7c41894f506dcde0023/simpletransformers-0.24.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "48c6de90273393379748bac575134e07", "sha256": "a5df839a7276e49878c58c2ad8f55652943591afb2cdc4ef3fbfe89a3bd8f772"}, "downloads": -1, "filename": "simpletransformers-0.24.0.tar.gz", "has_sig": false, "md5_digest": "48c6de90273393379748bac575134e07", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 164727, "upload_time": "2020-04-09T14:16:08", "upload_time_iso_8601": "2020-04-09T14:16:08.645131Z", "url": "https://files.pythonhosted.org/packages/0b/26/99fb41326b1d5f2a034f90de53d23d5cd6eea151ddbc34c93b432d741fd0/simpletransformers-0.24.0.tar.gz", "yanked": false}], "0.24.1": [{"comment_text": "", "digests": {"md5": "ce642a7e9d827761d35ca482eba3fc1d", "sha256": "a2e4480877a9f399f3a7b33dbf20885f3554c019ede37d93edfa6e9021915d4d"}, "downloads": -1, "filename": "simpletransformers-0.24.1-py3-none-any.whl", "has_sig": false, "md5_digest": "ce642a7e9d827761d35ca482eba3fc1d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 150664, "upload_time": "2020-04-09T17:58:29", "upload_time_iso_8601": "2020-04-09T17:58:29.644820Z", "url": "https://files.pythonhosted.org/packages/bf/fc/79db44a62fc73ea32f8bb9ef2523937bd0b7f2dedce441da292809e38e0f/simpletransformers-0.24.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d906f3e1d9d49ae2734cb5ab8861f854", "sha256": "dd4a3da11e9cc49cd9878abd184624e3e8ab91cbe9e2c36df71995622e20c604"}, "downloads": -1, "filename": "simpletransformers-0.24.1.tar.gz", "has_sig": false, "md5_digest": "d906f3e1d9d49ae2734cb5ab8861f854", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 164749, "upload_time": "2020-04-09T17:58:32", "upload_time_iso_8601": "2020-04-09T17:58:32.709663Z", "url": "https://files.pythonhosted.org/packages/68/62/998d991def7e3ccdaf39d587a7dc759ce2b1c0efbdc593d63b3701bea72e/simpletransformers-0.24.1.tar.gz", "yanked": false}], "0.24.2": [{"comment_text": "", "digests": {"md5": "7d02f680322acfa16ee9510dac74d02a", "sha256": "6804b8890efe32ee559a9f3548bcfecb5a2cd53777acd70b7d3ee44cc046bba2"}, "downloads": -1, "filename": "simpletransformers-0.24.2-py3-none-any.whl", "has_sig": false, "md5_digest": "7d02f680322acfa16ee9510dac74d02a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 150776, "upload_time": "2020-04-09T19:47:31", "upload_time_iso_8601": "2020-04-09T19:47:31.247221Z", "url": "https://files.pythonhosted.org/packages/12/05/8aa9bacaa98be07d35ff014411688ee5621e29cfdc687e81fca5eb18d08a/simpletransformers-0.24.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1939a26d3347ff34401deb22eec4c536", "sha256": "8ce043d63afaff364fe134210283132efecd5f9d6cbdb43b8e4d296e2e5f9bb5"}, "downloads": -1, "filename": "simpletransformers-0.24.2.tar.gz", "has_sig": false, "md5_digest": "1939a26d3347ff34401deb22eec4c536", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 164863, "upload_time": "2020-04-09T19:47:33", "upload_time_iso_8601": "2020-04-09T19:47:33.998086Z", "url": "https://files.pythonhosted.org/packages/12/a8/11dfd2e9d0452cbe4cb9d81f20f6c07451de473a619901d4da450cf02fd8/simpletransformers-0.24.2.tar.gz", "yanked": false}], "0.24.3": [{"comment_text": "", "digests": {"md5": "2b6d1d04c32c0768fbfc5579d48d63f8", "sha256": "8d2c0d66a315c477e49faccf61af763ff7fdb5b48ec8c9ff3efda2412220be01"}, "downloads": -1, "filename": "simpletransformers-0.24.3-py3-none-any.whl", "has_sig": false, "md5_digest": "2b6d1d04c32c0768fbfc5579d48d63f8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 150804, "upload_time": "2020-04-10T09:13:54", "upload_time_iso_8601": "2020-04-10T09:13:54.434533Z", "url": "https://files.pythonhosted.org/packages/44/0c/f005971985639cb521ec23e56ba3eec6a6dfbf79f683d03e6b0ef2827ada/simpletransformers-0.24.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "73c82c2f1bfb24fa607bd31768abee2b", "sha256": "6f35c7571d0f75b9422dd24516f79d06b237bab3efd058216db638f5ee0a3053"}, "downloads": -1, "filename": "simpletransformers-0.24.3.tar.gz", "has_sig": false, "md5_digest": "73c82c2f1bfb24fa607bd31768abee2b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 164967, "upload_time": "2020-04-10T09:13:56", "upload_time_iso_8601": "2020-04-10T09:13:56.822048Z", "url": "https://files.pythonhosted.org/packages/d2/14/a890b74a13d6bdc35316a8878c447ee99bdd0f90638efc93f8b913dcbeed/simpletransformers-0.24.3.tar.gz", "yanked": false}], "0.24.4": [{"comment_text": "", "digests": {"md5": "6b8acc8f0bc57b64d1e674db65a8c277", "sha256": "fac52a45c83f9585793d94306fe14bb02947be379751ff47969813fb102d0c49"}, "downloads": -1, "filename": "simpletransformers-0.24.4-py3-none-any.whl", "has_sig": false, "md5_digest": "6b8acc8f0bc57b64d1e674db65a8c277", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 150858, "upload_time": "2020-04-10T19:49:07", "upload_time_iso_8601": "2020-04-10T19:49:07.142321Z", "url": "https://files.pythonhosted.org/packages/f2/19/d316ceba21d40e0638d0ef6427f4f58459fc2373656c1094bab76a44ccf4/simpletransformers-0.24.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d1de02bbc83c30bc0cbc6308d162569d", "sha256": "22be7e37f7ec4097d633fec4d9bd01a001ebf734c940fd67c15717174290df97"}, "downloads": -1, "filename": "simpletransformers-0.24.4.tar.gz", "has_sig": false, "md5_digest": "d1de02bbc83c30bc0cbc6308d162569d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 165022, "upload_time": "2020-04-10T19:49:09", "upload_time_iso_8601": "2020-04-10T19:49:09.543622Z", "url": "https://files.pythonhosted.org/packages/13/a7/6b3fba78841cd73af504678352c1170c139b1b10c273d05464de529d0b73/simpletransformers-0.24.4.tar.gz", "yanked": false}], "0.24.5": [{"comment_text": "", "digests": {"md5": "7c8a84b6775899e7f68cb07c5d1c31c8", "sha256": "6529589a2ba09313dcf236f552986ec09c02efa86056b0b8c85e519d6cde1c80"}, "downloads": -1, "filename": "simpletransformers-0.24.5-py3-none-any.whl", "has_sig": false, "md5_digest": "7c8a84b6775899e7f68cb07c5d1c31c8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 150876, "upload_time": "2020-04-11T18:41:08", "upload_time_iso_8601": "2020-04-11T18:41:08.064497Z", "url": "https://files.pythonhosted.org/packages/11/bc/1621be41830b26cbe593eec9e042f28d811669461b3ce946cb1dfd3d0d8a/simpletransformers-0.24.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "281d58331be5ec8d976da01f6fc947be", "sha256": "664d4cddd0590bac7ca745f4a3eeb742bc2f7d5f9fb0e387983ca3e27742a736"}, "downloads": -1, "filename": "simpletransformers-0.24.5.tar.gz", "has_sig": false, "md5_digest": "281d58331be5ec8d976da01f6fc947be", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 165125, "upload_time": "2020-04-11T18:41:10", "upload_time_iso_8601": "2020-04-11T18:41:10.316419Z", "url": "https://files.pythonhosted.org/packages/af/9e/7005199638b3e094df98cdc0e8fa23f166d78d89176c6b1d25a2ed4657d1/simpletransformers-0.24.5.tar.gz", "yanked": false}], "0.24.6": [{"comment_text": "", "digests": {"md5": "31b5c14765b7c4ac8c8eab3eee2c9a39", "sha256": "40d597e852bf5067c0151e7291f7455e193ffa340b3aedb9cfd8f4b9891c5773"}, "downloads": -1, "filename": "simpletransformers-0.24.6-py3-none-any.whl", "has_sig": false, "md5_digest": "31b5c14765b7c4ac8c8eab3eee2c9a39", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 151012, "upload_time": "2020-04-12T13:00:15", "upload_time_iso_8601": "2020-04-12T13:00:15.014033Z", "url": "https://files.pythonhosted.org/packages/6a/a5/10ab114fc92ab5e4ee44a3dd4587c3441adb756fa3220bb0f4cd6daef3e7/simpletransformers-0.24.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8f90516f0f20cd26f991a221aa41b46a", "sha256": "f67ba8c48b823c89bb8846a161762cbe9330740ac8a4cae7e917289386fe1e7e"}, "downloads": -1, "filename": "simpletransformers-0.24.6.tar.gz", "has_sig": false, "md5_digest": "8f90516f0f20cd26f991a221aa41b46a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 165404, "upload_time": "2020-04-12T13:00:17", "upload_time_iso_8601": "2020-04-12T13:00:17.403820Z", "url": "https://files.pythonhosted.org/packages/48/a6/ef824d9e09ddaa28bab9701d467c615dab1387aee9d2a68c3c538fbc1b65/simpletransformers-0.24.6.tar.gz", "yanked": false}], "0.24.7": [{"comment_text": "", "digests": {"md5": "3a51de09ee70919971f0ab622422d95a", "sha256": "ee85f3c67d0ce0ef8310c9fbb334145de19a1b1ebafa4e49f95cfa41cec90eed"}, "downloads": -1, "filename": "simpletransformers-0.24.7-py3-none-any.whl", "has_sig": false, "md5_digest": "3a51de09ee70919971f0ab622422d95a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 151440, "upload_time": "2020-04-13T15:36:34", "upload_time_iso_8601": "2020-04-13T15:36:34.391417Z", "url": "https://files.pythonhosted.org/packages/1a/e0/2b0ba704052dfdb4c1fe8f64080ecd14fc46e6e11335428a2356e2393b69/simpletransformers-0.24.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "54b07d14a10c44dedcbbe1f64de742a1", "sha256": "e5c23be4fd2285121375d10c6ad69c47f8fbfdb3141c988bdb5f212c1dc7250d"}, "downloads": -1, "filename": "simpletransformers-0.24.7.tar.gz", "has_sig": false, "md5_digest": "54b07d14a10c44dedcbbe1f64de742a1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 166282, "upload_time": "2020-04-13T15:36:36", "upload_time_iso_8601": "2020-04-13T15:36:36.843056Z", "url": "https://files.pythonhosted.org/packages/7d/aa/4ce800b022e1fad63b96e8f1420956f6d4199c1a98284b16343196451259/simpletransformers-0.24.7.tar.gz", "yanked": false}], "0.24.8": [{"comment_text": "", "digests": {"md5": "234f63a8d92422379d14094a9267c706", "sha256": "6958e853f85783e6a0df7dfdbd58981dfc214f9f999a17a0dba7658ebc4ee459"}, "downloads": -1, "filename": "simpletransformers-0.24.8-py3-none-any.whl", "has_sig": false, "md5_digest": "234f63a8d92422379d14094a9267c706", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 151436, "upload_time": "2020-04-13T17:05:09", "upload_time_iso_8601": "2020-04-13T17:05:09.904906Z", "url": "https://files.pythonhosted.org/packages/a2/cd/184543483da9b6a5d23a6eb21ec2d0575716b12dcdfbdf2d87c6871bc31e/simpletransformers-0.24.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ac6bfd1f6968b9374f82b54919629042", "sha256": "ef6950734fda1af5e3dce93a2b503a27ee93f26520697ad6aff434c80129b160"}, "downloads": -1, "filename": "simpletransformers-0.24.8.tar.gz", "has_sig": false, "md5_digest": "ac6bfd1f6968b9374f82b54919629042", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 166259, "upload_time": "2020-04-13T17:05:12", "upload_time_iso_8601": "2020-04-13T17:05:12.480900Z", "url": "https://files.pythonhosted.org/packages/e6/ce/1575d816a2013d4b436268a5aafb296cf0cd8b88f60f193bfb3576018795/simpletransformers-0.24.8.tar.gz", "yanked": false}], "0.24.9": [{"comment_text": "", "digests": {"md5": "96628bfbd60d30890668d14bd99d6e57", "sha256": "1ff88074a6a6735a2d077948c4ab9fcd44f36078c1d15955f1a6ff29a325e00a"}, "downloads": -1, "filename": "simpletransformers-0.24.9-py3-none-any.whl", "has_sig": false, "md5_digest": "96628bfbd60d30890668d14bd99d6e57", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 152269, "upload_time": "2020-04-22T15:19:09", "upload_time_iso_8601": "2020-04-22T15:19:09.036890Z", "url": "https://files.pythonhosted.org/packages/90/d6/e778a2b18c18b3d52a33295ab102dba0518541baae18166a5b7a181be90f/simpletransformers-0.24.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "17aee0e482fcf4058e53a0f22734cfa9", "sha256": "0e2285eb86fe7ce8e0f64f964877b8ec0a4423a9fa705ee569e0ac374a65c604"}, "downloads": -1, "filename": "simpletransformers-0.24.9.tar.gz", "has_sig": false, "md5_digest": "17aee0e482fcf4058e53a0f22734cfa9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 167564, "upload_time": "2020-04-22T15:19:11", "upload_time_iso_8601": "2020-04-22T15:19:11.798343Z", "url": "https://files.pythonhosted.org/packages/ca/b4/fde4ece5cf9124f9e534da511b30ea9055d3cbb12da9d874894b2a81e4b7/simpletransformers-0.24.9.tar.gz", "yanked": false}], "0.25.0": [{"comment_text": "", "digests": {"md5": "f8296982575758a53b2de4e811e7e1b3", "sha256": "64d4d6d1f5d19d5dc966bfa9dbe0ca0938d968723979f07127fc452fad0d651b"}, "downloads": -1, "filename": "simpletransformers-0.25.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f8296982575758a53b2de4e811e7e1b3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 157879, "upload_time": "2020-04-23T20:28:54", "upload_time_iso_8601": "2020-04-23T20:28:54.869560Z", "url": "https://files.pythonhosted.org/packages/ce/cc/4b42c1c362c7c3b939ebf5b628145abf69aeb8e1ac3f79770577466319c1/simpletransformers-0.25.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5f40bf5f7e833743c7fb7959c001c11c", "sha256": "94965eb5fc63007b993fae1c9aa15cad7399e40afb2f18b5a2b6b197602939b2"}, "downloads": -1, "filename": "simpletransformers-0.25.0.tar.gz", "has_sig": false, "md5_digest": "5f40bf5f7e833743c7fb7959c001c11c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 173191, "upload_time": "2020-04-23T20:28:57", "upload_time_iso_8601": "2020-04-23T20:28:57.353606Z", "url": "https://files.pythonhosted.org/packages/30/fa/cd1caca5f3b437dd99295ea8991842b0af5bca04cb2a2e709c6df9183a50/simpletransformers-0.25.0.tar.gz", "yanked": false}], "0.26.0": [{"comment_text": "", "digests": {"md5": "926b60fbaae0ca1159c3c12ee6914ffa", "sha256": "826a5b07757a68942ba702d5b0680f92618c729b21c84d4d1fa23bf5644cb6f2"}, "downloads": -1, "filename": "simpletransformers-0.26.0-py3-none-any.whl", "has_sig": false, "md5_digest": "926b60fbaae0ca1159c3c12ee6914ffa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 159351, "upload_time": "2020-04-24T20:11:31", "upload_time_iso_8601": "2020-04-24T20:11:31.420263Z", "url": "https://files.pythonhosted.org/packages/36/fe/63b7df08f0412e3462332145009ddb303903fc3c6a15ec511c365c4c5bce/simpletransformers-0.26.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "407710b5dfe10f55fd1c62d297c7a92e", "sha256": "06027bdf687093ec199e96961f72eae54eeab2f6680001d111b86e16851aff4d"}, "downloads": -1, "filename": "simpletransformers-0.26.0.tar.gz", "has_sig": false, "md5_digest": "407710b5dfe10f55fd1c62d297c7a92e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 174733, "upload_time": "2020-04-24T20:11:33", "upload_time_iso_8601": "2020-04-24T20:11:33.758695Z", "url": "https://files.pythonhosted.org/packages/df/83/40c5872000dba17362be641fc05b93920c8670c09b4cc7b6373a05943da7/simpletransformers-0.26.0.tar.gz", "yanked": false}], "0.27.0": [{"comment_text": "", "digests": {"md5": "f3e9a8dc5fab0395986938f0cfaba09b", "sha256": "94cf0037374843108d77a2e64f2eb713602f3266259e3cc5822d71212ae49449"}, "downloads": -1, "filename": "simpletransformers-0.27.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f3e9a8dc5fab0395986938f0cfaba09b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 170705, "upload_time": "2020-05-05T15:56:45", "upload_time_iso_8601": "2020-05-05T15:56:45.474865Z", "url": "https://files.pythonhosted.org/packages/91/9c/37fa57fe8bc73c9d61ec9ccfcca087297f0489151af9c59fbdbfb7f94a55/simpletransformers-0.27.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a4dee3f2e92b891696765d98c164e6d5", "sha256": "a6d25643a95cef8dca3e2f11cb2bc967f98c7e20a3ad4e83e2de7144f67e4778"}, "downloads": -1, "filename": "simpletransformers-0.27.0.tar.gz", "has_sig": false, "md5_digest": "a4dee3f2e92b891696765d98c164e6d5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 190300, "upload_time": "2020-05-05T15:56:47", "upload_time_iso_8601": "2020-05-05T15:56:47.538287Z", "url": "https://files.pythonhosted.org/packages/a7/f5/ce4aebabb93b391e393bdd1b534533ae826c13de9e6afe59b5ab9acff326/simpletransformers-0.27.0.tar.gz", "yanked": false}], "0.27.1": [{"comment_text": "", "digests": {"md5": "f2802c5c26d1eee2c8a7c41001a2f9b7", "sha256": "781e01d0e4379294d929476e006b6587a7556e137471f750513a98e6bebd982a"}, "downloads": -1, "filename": "simpletransformers-0.27.1-py3-none-any.whl", "has_sig": false, "md5_digest": "f2802c5c26d1eee2c8a7c41001a2f9b7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 170729, "upload_time": "2020-05-06T11:50:50", "upload_time_iso_8601": "2020-05-06T11:50:50.155867Z", "url": "https://files.pythonhosted.org/packages/39/fc/f26659f003809f08ad34ceec8f1c2d998c3f1c9ffc81b5c912a8c4a9e1e5/simpletransformers-0.27.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b7bfc7b97322f38d63cc9d9e66d6e95c", "sha256": "6f75f1dccccd13c6e0db8d4532e06bf6c7eafdb9820d70d228eadec3b9abc3e9"}, "downloads": -1, "filename": "simpletransformers-0.27.1.tar.gz", "has_sig": false, "md5_digest": "b7bfc7b97322f38d63cc9d9e66d6e95c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 190389, "upload_time": "2020-05-06T11:50:52", "upload_time_iso_8601": "2020-05-06T11:50:52.299811Z", "url": "https://files.pythonhosted.org/packages/57/00/36ce328b33287f98a04e80a6cb04572054b4e1bed2413671d5064d980476/simpletransformers-0.27.1.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "fb1d494aa7331606a2395796ee029d3f", "sha256": "523eb31f84ba892a31b42521798fa2f094a2f2c1e9bc19881a803ead7205e794"}, "downloads": -1, "filename": "simpletransformers-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "fb1d494aa7331606a2395796ee029d3f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 17926, "upload_time": "2019-10-10T14:12:33", "upload_time_iso_8601": "2019-10-10T14:12:33.842783Z", "url": "https://files.pythonhosted.org/packages/d2/24/5e0000f0cd1fb7cc8608d09c22e2c9840318fe2fb243e5ac96256a944d38/simpletransformers-0.3.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c7313eb06dcc07414ded3f5bc6e3e1d8", "sha256": "cb26922567c79519130d9481ae05e8e6f5ff9a797c51702c548f99e3508fdba5"}, "downloads": -1, "filename": "simpletransformers-0.3.0.tar.gz", "has_sig": false, "md5_digest": "c7313eb06dcc07414ded3f5bc6e3e1d8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16586, "upload_time": "2019-10-10T14:12:39", "upload_time_iso_8601": "2019-10-10T14:12:39.914588Z", "url": "https://files.pythonhosted.org/packages/86/73/e42f43d1c710acedaa55d12d339f668fe4730c801bf39f41fdc32ee70f1d/simpletransformers-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "330016d5f371fdd482f7128b2150d267", "sha256": "052b1dcbf50bd9f410705889e1f700fb1d91c6e1a63d131da91b83c1cb48fd58"}, "downloads": -1, "filename": "simpletransformers-0.3.1-py3-none-any.whl", "has_sig": false, "md5_digest": "330016d5f371fdd482f7128b2150d267", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18074, "upload_time": "2019-10-10T15:49:22", "upload_time_iso_8601": "2019-10-10T15:49:22.594859Z", "url": "https://files.pythonhosted.org/packages/a6/63/54aaef2a52963a164df0da7255de306dc5322d7f26633cccb14a23ed471b/simpletransformers-0.3.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d22465c9d644fd7af48bbcb1a3f00ba1", "sha256": "e62b0443978a7c49c44359003c13b8548804c2ac822a078eb9a5ea03655c63f9"}, "downloads": -1, "filename": "simpletransformers-0.3.1.tar.gz", "has_sig": false, "md5_digest": "d22465c9d644fd7af48bbcb1a3f00ba1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17066, "upload_time": "2019-10-10T15:49:27", "upload_time_iso_8601": "2019-10-10T15:49:27.687456Z", "url": "https://files.pythonhosted.org/packages/73/45/cc994ca30b5a404efe355c5e79c0062047606e574dae7072316819fcc926/simpletransformers-0.3.1.tar.gz", "yanked": false}], "0.3.2": [{"comment_text": "", "digests": {"md5": "f349991588b43aa223f7e1fea7bcdacd", "sha256": "a5577090c1134f1cca423a91a4d3d5dad4bf9874b52f5dd7870fade70173181d"}, "downloads": -1, "filename": "simpletransformers-0.3.2-py3-none-any.whl", "has_sig": false, "md5_digest": "f349991588b43aa223f7e1fea7bcdacd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18081, "upload_time": "2019-10-10T15:57:06", "upload_time_iso_8601": "2019-10-10T15:57:06.674792Z", "url": "https://files.pythonhosted.org/packages/31/7e/74e8bfa6ba64390fe25ffdf1cd7fbd628a0be5575e73cb1f4203086a1a47/simpletransformers-0.3.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "31934d66d0ac768003d6a741ab144e58", "sha256": "1cb82cdbd37409a47ea2aaa63b5135abddadc34b6929ef91ec6b95d2ae185fa1"}, "downloads": -1, "filename": "simpletransformers-0.3.2.tar.gz", "has_sig": false, "md5_digest": "31934d66d0ac768003d6a741ab144e58", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17076, "upload_time": "2019-10-10T15:57:12", "upload_time_iso_8601": "2019-10-10T15:57:12.534875Z", "url": "https://files.pythonhosted.org/packages/90/55/4c03a1f9680440d9013894237d63ee9362b4c7df1477ada548d680875991/simpletransformers-0.3.2.tar.gz", "yanked": false}], "0.3.3": [{"comment_text": "", "digests": {"md5": "67fa4fb07df8573e849c43dc74b6748f", "sha256": "4ba1bf3b21af27706115e946e9c5549c3419323dcd61677ccc5afcf9003450cf"}, "downloads": -1, "filename": "simpletransformers-0.3.3-py3-none-any.whl", "has_sig": false, "md5_digest": "67fa4fb07df8573e849c43dc74b6748f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18264, "upload_time": "2019-10-11T12:58:12", "upload_time_iso_8601": "2019-10-11T12:58:12.286795Z", "url": "https://files.pythonhosted.org/packages/96/81/21cb223e31fed119568cf7bae3fbc4dbb1ba83691dfc7f2406b2ad8208f2/simpletransformers-0.3.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f622fc4c47c12183270ff458f45a1655", "sha256": "9570b98db54b8bc61c250c9c4dbfc7e5892e242bb7afcbe75d90409a2c51c159"}, "downloads": -1, "filename": "simpletransformers-0.3.3.tar.gz", "has_sig": false, "md5_digest": "f622fc4c47c12183270ff458f45a1655", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17249, "upload_time": "2019-10-11T12:58:14", "upload_time_iso_8601": "2019-10-11T12:58:14.442789Z", "url": "https://files.pythonhosted.org/packages/5e/a6/963818b4cab27eab559939414ccfc9d7c81f4cd7d7bb93ed47de9e519b37/simpletransformers-0.3.3.tar.gz", "yanked": false}], "0.3.4": [{"comment_text": "", "digests": {"md5": "22b780c1de79856a9311dfa0f87b3339", "sha256": "de1f8c010ede5e58e98e527d7cbe6d207ee701131b5bb2623cc11e903742a541"}, "downloads": -1, "filename": "simpletransformers-0.3.4-py3-none-any.whl", "has_sig": false, "md5_digest": "22b780c1de79856a9311dfa0f87b3339", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18477, "upload_time": "2019-10-11T19:25:47", "upload_time_iso_8601": "2019-10-11T19:25:47.620032Z", "url": "https://files.pythonhosted.org/packages/28/50/a87495f7a47bfd85169aece3351a3d34a00f47847feac08dc9b4ded62aed/simpletransformers-0.3.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "90b21e35e34b69e6991de6e30bffad10", "sha256": "934187d493adb91ada2316afd2f7717d6c399fc777cb5fc81454f5f83359a1f5"}, "downloads": -1, "filename": "simpletransformers-0.3.4.tar.gz", "has_sig": false, "md5_digest": "90b21e35e34b69e6991de6e30bffad10", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17666, "upload_time": "2019-10-11T19:25:51", "upload_time_iso_8601": "2019-10-11T19:25:51.563119Z", "url": "https://files.pythonhosted.org/packages/4f/49/90983ef3e9f5c10529119dce431768c026be04ddbb5d549a6097f4b7503c/simpletransformers-0.3.4.tar.gz", "yanked": false}], "0.3.5": [{"comment_text": "", "digests": {"md5": "a5b8451494c1624d25c64c653647cc74", "sha256": "af53182873dbd1f77b1da3458941f84638b2a271aca891ddba898725bd0f96b3"}, "downloads": -1, "filename": "simpletransformers-0.3.5-py3-none-any.whl", "has_sig": false, "md5_digest": "a5b8451494c1624d25c64c653647cc74", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18494, "upload_time": "2019-10-11T20:13:38", "upload_time_iso_8601": "2019-10-11T20:13:38.541118Z", "url": "https://files.pythonhosted.org/packages/e3/f2/1e7171243944aada56eac8631c6873c2a5eab4ea13c81c119e1b927db47b/simpletransformers-0.3.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "57de7017f76dee3dcb0a4753fdd34984", "sha256": "6d506d7ef985f1bbe1a3252ceb91e5ab7eab30c56f5b82033624d548a59aaa0d"}, "downloads": -1, "filename": "simpletransformers-0.3.5.tar.gz", "has_sig": false, "md5_digest": "57de7017f76dee3dcb0a4753fdd34984", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17683, "upload_time": "2019-10-11T20:13:40", "upload_time_iso_8601": "2019-10-11T20:13:40.779678Z", "url": "https://files.pythonhosted.org/packages/d4/0d/86b13d88f9403bd5f5c9178c6257f2fa60828b7a15c80ec93b9a68b120e6/simpletransformers-0.3.5.tar.gz", "yanked": false}], "0.3.6": [{"comment_text": "", "digests": {"md5": "d6142d40456b6624d6a3b34a87e2438c", "sha256": "9fd1e095eb1de1b10777b91a6d47492036be5e0de086c28dc53606890d90fb3c"}, "downloads": -1, "filename": "simpletransformers-0.3.6-py3-none-any.whl", "has_sig": false, "md5_digest": "d6142d40456b6624d6a3b34a87e2438c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18516, "upload_time": "2019-10-12T04:40:37", "upload_time_iso_8601": "2019-10-12T04:40:37.058595Z", "url": "https://files.pythonhosted.org/packages/89/47/862d68e1acc71c9a40495520d41246d0a6ba586b4cbe79a2ec6f9aa9cd42/simpletransformers-0.3.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "41c35554c7f5e70ac19625dc6e57d099", "sha256": "38a06f24c990c4d80da140fe4a7684b40db854fa2a2fd3bb6c0ae02396b68b20"}, "downloads": -1, "filename": "simpletransformers-0.3.6.tar.gz", "has_sig": false, "md5_digest": "41c35554c7f5e70ac19625dc6e57d099", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17708, "upload_time": "2019-10-12T04:40:40", "upload_time_iso_8601": "2019-10-12T04:40:40.542060Z", "url": "https://files.pythonhosted.org/packages/93/0b/997179995854f75518579fd1a824dbf002508a728f4329c3c26dc2d70130/simpletransformers-0.3.6.tar.gz", "yanked": false}], "0.3.7": [{"comment_text": "", "digests": {"md5": "123e9ad7f92944d66ee6dc480f98c1a6", "sha256": "9729f64b7b0d0f7301fd7ba74bb108b2a2dccc6315e807b8428a80082b5e44c0"}, "downloads": -1, "filename": "simpletransformers-0.3.7-py3-none-any.whl", "has_sig": false, "md5_digest": "123e9ad7f92944d66ee6dc480f98c1a6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18618, "upload_time": "2019-10-12T05:04:33", "upload_time_iso_8601": "2019-10-12T05:04:33.086794Z", "url": "https://files.pythonhosted.org/packages/12/a8/ed4415a477892ced2a529e796d58daadb70e66c0eb620c8f8762c3156494/simpletransformers-0.3.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fd40c7255abb2a11452eb187b7c3cd1a", "sha256": "0ca32f3d43741a41c59b807b7d2ee3f71e2632d9885791a81baf2689d2e39a5e"}, "downloads": -1, "filename": "simpletransformers-0.3.7.tar.gz", "has_sig": false, "md5_digest": "fd40c7255abb2a11452eb187b7c3cd1a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17804, "upload_time": "2019-10-12T05:04:35", "upload_time_iso_8601": "2019-10-12T05:04:35.127565Z", "url": "https://files.pythonhosted.org/packages/e3/f0/83113a19655dea798cbd292b8875e954670cc90fad5ac32908916366258a/simpletransformers-0.3.7.tar.gz", "yanked": false}], "0.3.8": [{"comment_text": "", "digests": {"md5": "0c730c3de31063d9ed30d67b37ddcb8c", "sha256": "eb94b2ca03d7c84b6ee548d5a3737291a1aaa5194871343c75bcf9272e57ed6d"}, "downloads": -1, "filename": "simpletransformers-0.3.8-py3-none-any.whl", "has_sig": false, "md5_digest": "0c730c3de31063d9ed30d67b37ddcb8c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18814, "upload_time": "2019-10-13T05:50:10", "upload_time_iso_8601": "2019-10-13T05:50:10.526680Z", "url": "https://files.pythonhosted.org/packages/9c/28/029dac2a0d97e7c4520366c955d7222cd1e7559a0a2adcb2df73411a6d60/simpletransformers-0.3.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "463ae5d4b8dee2274366aac9a797be53", "sha256": "7e660a94b8a9d133d873c472bddfb67f3c773a2698c01313d047baa65b22b17a"}, "downloads": -1, "filename": "simpletransformers-0.3.8.tar.gz", "has_sig": false, "md5_digest": "463ae5d4b8dee2274366aac9a797be53", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 18024, "upload_time": "2019-10-13T05:50:14", "upload_time_iso_8601": "2019-10-13T05:50:14.254788Z", "url": "https://files.pythonhosted.org/packages/76/9f/eac7832c47316a6876b2f2da6744bba3be42d947cfb59c5e24ba93a0f824/simpletransformers-0.3.8.tar.gz", "yanked": false}], "0.4.0": [{"comment_text": "", "digests": {"md5": "3eeccca21896eaad4b8aaf138218aa34", "sha256": "2a0e7a9a554760cb2019a94606b2239d56081107bcc5cd01e10e5bc1226ed1a5"}, "downloads": -1, "filename": "simpletransformers-0.4.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3eeccca21896eaad4b8aaf138218aa34", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 18805, "upload_time": "2019-10-13T11:52:32", "upload_time_iso_8601": "2019-10-13T11:52:32.542627Z", "url": "https://files.pythonhosted.org/packages/ee/9c/29be70ef5505c160032318897f57ec6e11219390e2b9621d6bfc3c3575b9/simpletransformers-0.4.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "17d2201ae34a25cdfa915a30587a94b6", "sha256": "11e1111342d75a7d2cb61b84e9b7612542e7d0a93d5676d28b4508c1fb80e0d6"}, "downloads": -1, "filename": "simpletransformers-0.4.0.tar.gz", "has_sig": false, "md5_digest": "17d2201ae34a25cdfa915a30587a94b6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 18021, "upload_time": "2019-10-13T11:52:34", "upload_time_iso_8601": "2019-10-13T11:52:34.810010Z", "url": "https://files.pythonhosted.org/packages/8e/33/4e6c680674ca74a6d18281c82a2cc278eb0ca3baba4285c12dbdc3594af5/simpletransformers-0.4.0.tar.gz", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "7d17e353b63610583decaabd59af3023", "sha256": "48dd66ffaba6768b106a26ffbe2bed4b0a104f6bd2b9c5337d38a18c02fc3fbb"}, "downloads": -1, "filename": "simpletransformers-0.4.1-py3-none-any.whl", "has_sig": false, "md5_digest": "7d17e353b63610583decaabd59af3023", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 19006, "upload_time": "2019-10-19T14:37:16", "upload_time_iso_8601": "2019-10-19T14:37:16.535569Z", "url": "https://files.pythonhosted.org/packages/4e/82/f3217e524b234899481e2a87e181674283c04652f63975b092a71a529a62/simpletransformers-0.4.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6de74f28f720fba7a2f1c247fcedfbb9", "sha256": "cbf811ec6e01fec5b72a6e4504caf53e59a5aa8c1b4ed55af42fc3804e6b3ba8"}, "downloads": -1, "filename": "simpletransformers-0.4.1.tar.gz", "has_sig": false, "md5_digest": "6de74f28f720fba7a2f1c247fcedfbb9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 18365, "upload_time": "2019-10-19T14:37:20", "upload_time_iso_8601": "2019-10-19T14:37:20.283861Z", "url": "https://files.pythonhosted.org/packages/b8/92/fad2fb96a50c73b2f695a99d7b2582f845e33737306051177b47be11e0f2/simpletransformers-0.4.1.tar.gz", "yanked": false}], "0.4.2": [{"comment_text": "", "digests": {"md5": "c066f6e0dd958643b58f877e9e60930b", "sha256": "1ce299c27ff05456425ccd7ea3ddfe5f732cdadbcd9150318849bb9807b11681"}, "downloads": -1, "filename": "simpletransformers-0.4.2-py3-none-any.whl", "has_sig": false, "md5_digest": "c066f6e0dd958643b58f877e9e60930b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 19007, "upload_time": "2019-10-19T14:44:30", "upload_time_iso_8601": "2019-10-19T14:44:30.746778Z", "url": "https://files.pythonhosted.org/packages/e3/3a/10cc9045ce3eeaac3c26d48cac30284802a135c4847e779bb27b27ce3024/simpletransformers-0.4.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "229205e9bc6b65543502c4aa8c30eb2f", "sha256": "eb7a769b394e86a48eeedac74e2f294c3fdbabd972617d958d70b583087d868b"}, "downloads": -1, "filename": "simpletransformers-0.4.2.tar.gz", "has_sig": false, "md5_digest": "229205e9bc6b65543502c4aa8c30eb2f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 18364, "upload_time": "2019-10-19T14:44:35", "upload_time_iso_8601": "2019-10-19T14:44:35.845467Z", "url": "https://files.pythonhosted.org/packages/73/29/be684f25732f05b12cc49d2da796d1ebb74d5a34403edd296524b77eb4ee/simpletransformers-0.4.2.tar.gz", "yanked": false}], "0.4.3": [{"comment_text": "", "digests": {"md5": "b180a1e7c175f7084935a98e8b35f032", "sha256": "3e9600b1d7f4323b6e9a49ff90df752c39ae61c2089823da2fb483223c5d7041"}, "downloads": -1, "filename": "simpletransformers-0.4.3-py3-none-any.whl", "has_sig": false, "md5_digest": "b180a1e7c175f7084935a98e8b35f032", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 19045, "upload_time": "2019-10-19T16:31:40", "upload_time_iso_8601": "2019-10-19T16:31:40.154777Z", "url": "https://files.pythonhosted.org/packages/51/36/1f18d45311a12e270f370c445fbb6b8c1351c529e41bd18b0f952f54f3d6/simpletransformers-0.4.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7797b8b7eb32b9443119eb87ad29e817", "sha256": "35c23f41d90db454c4af935fe983d2c90b7675b204c7ebe35722b483ffec3f75"}, "downloads": -1, "filename": "simpletransformers-0.4.3.tar.gz", "has_sig": false, "md5_digest": "7797b8b7eb32b9443119eb87ad29e817", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 18367, "upload_time": "2019-10-19T16:31:48", "upload_time_iso_8601": "2019-10-19T16:31:48.808903Z", "url": "https://files.pythonhosted.org/packages/3a/07/75947712172b29fc018912f4f6af13c520d5486d255d2edd6c79977d2d15/simpletransformers-0.4.3.tar.gz", "yanked": false}], "0.4.4": [{"comment_text": "", "digests": {"md5": "08a11bce6dccff7f45bfa8362d15668c", "sha256": "3c89d1f2f2d4f3c13fd8cf76e0d67fe1c7c804daad2ad5d1f3c07e740f577382"}, "downloads": -1, "filename": "simpletransformers-0.4.4-py3-none-any.whl", "has_sig": false, "md5_digest": "08a11bce6dccff7f45bfa8362d15668c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 19048, "upload_time": "2019-10-22T03:00:37", "upload_time_iso_8601": "2019-10-22T03:00:37.411204Z", "url": "https://files.pythonhosted.org/packages/75/26/4324e4e41b5b2f72e03ee36d81630185300c56f60e9075bc706bb2e671c0/simpletransformers-0.4.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "81395b8607e3940968d896771cd299f9", "sha256": "41192c9bf09ef2517933708e76157fe84f7792cde78e541fe95525f41f387bc0"}, "downloads": -1, "filename": "simpletransformers-0.4.4.tar.gz", "has_sig": false, "md5_digest": "81395b8607e3940968d896771cd299f9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 18373, "upload_time": "2019-10-22T03:00:47", "upload_time_iso_8601": "2019-10-22T03:00:47.758785Z", "url": "https://files.pythonhosted.org/packages/4c/e3/7a0f514d9c41452edc3c114936d805e6dab67186e4095bf0d629d3131bb0/simpletransformers-0.4.4.tar.gz", "yanked": false}], "0.4.5": [{"comment_text": "", "digests": {"md5": "37bdb9ac62dffc39d7998efcba3dc5f1", "sha256": "c930b4d79a8570961d082204453cba0931d6a3803fc24b12ec4e18ad4b1cc7cb"}, "downloads": -1, "filename": "simpletransformers-0.4.5-py3-none-any.whl", "has_sig": false, "md5_digest": "37bdb9ac62dffc39d7998efcba3dc5f1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 19104, "upload_time": "2019-10-26T07:15:24", "upload_time_iso_8601": "2019-10-26T07:15:24.176393Z", "url": "https://files.pythonhosted.org/packages/d2/2f/0d3c73afc13c0f5d34ca9af6fd0ba63208c949ac75950a834d62d1f3077f/simpletransformers-0.4.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b2e265146dad5d939c4a0483dd584726", "sha256": "9f41f473e645cf8915012bccc44a5744994fc31498c4628667a145df805da771"}, "downloads": -1, "filename": "simpletransformers-0.4.5.tar.gz", "has_sig": false, "md5_digest": "b2e265146dad5d939c4a0483dd584726", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 18443, "upload_time": "2019-10-26T07:15:32", "upload_time_iso_8601": "2019-10-26T07:15:32.560292Z", "url": "https://files.pythonhosted.org/packages/b7/48/c1169502d0e9a98bd9e36ef4a1709db7dfe17dfd9721d680862fe97d7d1e/simpletransformers-0.4.5.tar.gz", "yanked": false}], "0.5.0": [{"comment_text": "", "digests": {"md5": "a3bcf97d416e50ba345311b3e6543ae3", "sha256": "40c3bae70ab5f1132d1c1688f45de76f8d5ff3b494b3f4e6577b22100f6b9181"}, "downloads": -1, "filename": "simpletransformers-0.5.0-py3-none-any.whl", "has_sig": false, "md5_digest": "a3bcf97d416e50ba345311b3e6543ae3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 29007, "upload_time": "2019-10-29T04:34:11", "upload_time_iso_8601": "2019-10-29T04:34:11.053683Z", "url": "https://files.pythonhosted.org/packages/e5/7f/8d744ab9a0f247ab0ddd22ffbe31902f8ecb8855c51cf10160d5f3ccc377/simpletransformers-0.5.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "141dbc3bec109dead6e757e7d05612b0", "sha256": "d52a46a441bc9f34fc6b5f1e96abe917caa9235edda2ad922e0d475770a09744"}, "downloads": -1, "filename": "simpletransformers-0.5.0.tar.gz", "has_sig": false, "md5_digest": "141dbc3bec109dead6e757e7d05612b0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 24198, "upload_time": "2019-10-29T04:34:13", "upload_time_iso_8601": "2019-10-29T04:34:13.054777Z", "url": "https://files.pythonhosted.org/packages/d6/e1/9b5c575a6234a4abc343e2e65c678f1844317acaeabbc69cb4a37ad2b5c4/simpletransformers-0.5.0.tar.gz", "yanked": false}], "0.6.0": [{"comment_text": "", "digests": {"md5": "b1d453d559ffd1189505ab0283165025", "sha256": "fea1bc6faf5e8c31258d86f3e0cdd1e2adcff27fe69e833b0ac18a4260d544e0"}, "downloads": -1, "filename": "simpletransformers-0.6.0-py3-none-any.whl", "has_sig": false, "md5_digest": "b1d453d559ffd1189505ab0283165025", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 37055, "upload_time": "2019-11-07T14:48:42", "upload_time_iso_8601": "2019-11-07T14:48:42.787837Z", "url": "https://files.pythonhosted.org/packages/32/bc/5725794284d531d7ad8b2eacdb1e7072ab7cb3674fe01e2d1a2de7d366f5/simpletransformers-0.6.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1404d3a17ce2dbd3fdf1933ee0c8e4ee", "sha256": "80326fdd569279cb64a1ae596e3b48896362af9d445d77284d030eb32a5b4fd7"}, "downloads": -1, "filename": "simpletransformers-0.6.0.tar.gz", "has_sig": false, "md5_digest": "1404d3a17ce2dbd3fdf1933ee0c8e4ee", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 31423, "upload_time": "2019-11-07T14:48:44", "upload_time_iso_8601": "2019-11-07T14:48:44.487885Z", "url": "https://files.pythonhosted.org/packages/a3/57/ef5beee6927a66f2bbf4a3ed2adcd68124eda23c91eda6076ccf1363fbfb/simpletransformers-0.6.0.tar.gz", "yanked": false}], "0.6.1": [{"comment_text": "", "digests": {"md5": "2a2f1b070e3269c7f420b661a88a3e49", "sha256": "11cfe86d81d177d48e7b2e6a57874b2f4a3a93caf791d9fa68b57a1f9ecf6915"}, "downloads": -1, "filename": "simpletransformers-0.6.1-py3-none-any.whl", "has_sig": false, "md5_digest": "2a2f1b070e3269c7f420b661a88a3e49", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 37368, "upload_time": "2019-11-09T08:42:58", "upload_time_iso_8601": "2019-11-09T08:42:58.118819Z", "url": "https://files.pythonhosted.org/packages/78/85/a46bcf3f2527a2bd2f66ba4eef7e217f98126e2b0d195af2b6732d141668/simpletransformers-0.6.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fb3b6c3e1d7511619ff2a7a6519fa21c", "sha256": "a36706149477bde5597d37901fad31168baa383c3f5aaab696123a1cc1404419"}, "downloads": -1, "filename": "simpletransformers-0.6.1.tar.gz", "has_sig": false, "md5_digest": "fb3b6c3e1d7511619ff2a7a6519fa21c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32283, "upload_time": "2019-11-09T08:43:02", "upload_time_iso_8601": "2019-11-09T08:43:02.312973Z", "url": "https://files.pythonhosted.org/packages/79/69/dbd735241dc8a9fc19d1773200cb5b87635008a637285a4a29f4445b8e24/simpletransformers-0.6.1.tar.gz", "yanked": false}], "0.6.13": [{"comment_text": "", "digests": {"md5": "2ce3a5c038d91e6099374921b05e3c82", "sha256": "86a55d70ce1af2695cb7b29ac4e421c93218d9f06601e9a25db7175f8acf4490"}, "downloads": -1, "filename": "simpletransformers-0.6.13-py3-none-any.whl", "has_sig": false, "md5_digest": "2ce3a5c038d91e6099374921b05e3c82", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 56410, "upload_time": "2019-11-12T12:18:20", "upload_time_iso_8601": "2019-11-12T12:18:20.053623Z", "url": "https://files.pythonhosted.org/packages/6f/f2/ac81a770fe1d2715f92636237cdf7f32dd6fd71fe1805e9ed404eb3054cb/simpletransformers-0.6.13-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c05abf10a39f8e9317ce1b2b1962a698", "sha256": "6dab98298a6403a5832ac389e9683f1b496a85d0f5c3558c42c7ec0b03170490"}, "downloads": -1, "filename": "simpletransformers-0.6.13.tar.gz", "has_sig": false, "md5_digest": "c05abf10a39f8e9317ce1b2b1962a698", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 50921, "upload_time": "2019-11-12T12:18:24", "upload_time_iso_8601": "2019-11-12T12:18:24.599053Z", "url": "https://files.pythonhosted.org/packages/a9/3f/b2af078c685df99935178b8c56a6c2e110450d4aba9c0d4068f2600450c4/simpletransformers-0.6.13.tar.gz", "yanked": false}], "0.6.2": [{"comment_text": "", "digests": {"md5": "0018ac0556a7488cd25949c54af11fda", "sha256": "b7e294f774f0a24d37df12f2bcb108fb40976c16650649ca69d999589bee3da8"}, "downloads": -1, "filename": "simpletransformers-0.6.2-py3-none-any.whl", "has_sig": false, "md5_digest": "0018ac0556a7488cd25949c54af11fda", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 37698, "upload_time": "2019-11-09T09:32:03", "upload_time_iso_8601": "2019-11-09T09:32:03.877103Z", "url": "https://files.pythonhosted.org/packages/d6/18/d8a5d513c4cbc7571751422b12fc982c5e9856805d2e704ace9126309279/simpletransformers-0.6.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "617d50db11612f9b7a72cc5445c3259b", "sha256": "fef5afbc2e081887902b380dbf0eb44a3a30a903288cb9dbf9f1a1e63618bc49"}, "downloads": -1, "filename": "simpletransformers-0.6.2.tar.gz", "has_sig": false, "md5_digest": "617d50db11612f9b7a72cc5445c3259b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 32902, "upload_time": "2019-11-09T09:32:09", "upload_time_iso_8601": "2019-11-09T09:32:09.065339Z", "url": "https://files.pythonhosted.org/packages/36/3c/da5f884ee9c843fb14039e5e9ca6a44a9a91ee2d65980c7eec133dbdf283/simpletransformers-0.6.2.tar.gz", "yanked": false}], "0.6.3": [{"comment_text": "", "digests": {"md5": "ceb667974669b69ab41845d23911afa3", "sha256": "e5f51e8ee3855d8a7cb120c9a0124e4263320989ce9afcdb057584a6f1f6915b"}, "downloads": -1, "filename": "simpletransformers-0.6.3-py3-none-any.whl", "has_sig": false, "md5_digest": "ceb667974669b69ab41845d23911afa3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 38305, "upload_time": "2019-11-09T12:33:18", "upload_time_iso_8601": "2019-11-09T12:33:18.004175Z", "url": "https://files.pythonhosted.org/packages/8a/0f/482fe5169d0a77e5cad8d74c685140e19b7c2b1d613f3d005ce7e835185e/simpletransformers-0.6.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "37abf7d07c3fbf9d34721f9e9a4207d0", "sha256": "b93f6b0a128fea1814ffdc5033d40a1698a1fab8c40f230f50ad5d81aedf64c7"}, "downloads": -1, "filename": "simpletransformers-0.6.3.tar.gz", "has_sig": false, "md5_digest": "37abf7d07c3fbf9d34721f9e9a4207d0", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 33479, "upload_time": "2019-11-09T12:33:25", "upload_time_iso_8601": "2019-11-09T12:33:25.364896Z", "url": "https://files.pythonhosted.org/packages/8e/92/35400f7c1d17fee1ed04dcb5ac8e15268a9a15a3bf6d1642639424fad806/simpletransformers-0.6.3.tar.gz", "yanked": false}], "0.6.4": [{"comment_text": "", "digests": {"md5": "5dc78a27b396992521930a23694293c2", "sha256": "09a11533d993eb299a61680a93bec08c491044f6572d617cebe2435758c81af3"}, "downloads": -1, "filename": "simpletransformers-0.6.4-py3-none-any.whl", "has_sig": false, "md5_digest": "5dc78a27b396992521930a23694293c2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 38702, "upload_time": "2019-11-09T15:43:08", "upload_time_iso_8601": "2019-11-09T15:43:08.871759Z", "url": "https://files.pythonhosted.org/packages/e2/89/9ea37d82a0362adaaef0258a151d7ad71daa8053e3deeac667091c9197fd/simpletransformers-0.6.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fa1f6fb31d317950820a2ad9ffddc8ca", "sha256": "f8e3ea6cefa78059a8aed377f161da9eadff277da3fd7f45402a4e7fffad7510"}, "downloads": -1, "filename": "simpletransformers-0.6.4.tar.gz", "has_sig": false, "md5_digest": "fa1f6fb31d317950820a2ad9ffddc8ca", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 34010, "upload_time": "2019-11-09T15:43:16", "upload_time_iso_8601": "2019-11-09T15:43:16.182610Z", "url": "https://files.pythonhosted.org/packages/f4/14/f1d8558b400cf4f8f715bee19e73f630ba42d395bdebbdc9dea918936ab5/simpletransformers-0.6.4.tar.gz", "yanked": false}], "0.7.0": [{"comment_text": "", "digests": {"md5": "acaa463ccb66346ed83dc55229b183b8", "sha256": "6645ed54cfe7b9825e00596f92864093faad83c5850f9ea1dbca23b88e620b7f"}, "downloads": -1, "filename": "simpletransformers-0.7.0-py3-none-any.whl", "has_sig": false, "md5_digest": "acaa463ccb66346ed83dc55229b183b8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 58014, "upload_time": "2019-11-12T12:18:22", "upload_time_iso_8601": "2019-11-12T12:18:22.482815Z", "url": "https://files.pythonhosted.org/packages/2d/94/623ebb991fa1abf6a132a69a7d31572842918e7e4b9d9183051ded88674b/simpletransformers-0.7.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "68912447c9ca07bd75d812e1bec042a2", "sha256": "668d763936d3a3217b0b2f7f4e0b73cf1ef8d1efc518f02938fb63725683c834"}, "downloads": -1, "filename": "simpletransformers-0.7.0.tar.gz", "has_sig": false, "md5_digest": "68912447c9ca07bd75d812e1bec042a2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 59628, "upload_time": "2019-11-12T12:18:27", "upload_time_iso_8601": "2019-11-12T12:18:27.160927Z", "url": "https://files.pythonhosted.org/packages/bb/5c/0d81a4fca00492edadd8bd2d07497dbfca554adfc82e7a50004f2be998b9/simpletransformers-0.7.0.tar.gz", "yanked": false}], "0.7.10": [{"comment_text": "", "digests": {"md5": "113d10112d72e2b0d278e607815f755d", "sha256": "37495a726b663cdd705f5c7d5119a5727c42a2ee0e11290498ad9ca1ef66cf63"}, "downloads": -1, "filename": "simpletransformers-0.7.10-py3-none-any.whl", "has_sig": false, "md5_digest": "113d10112d72e2b0d278e607815f755d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 63842, "upload_time": "2019-11-20T03:01:34", "upload_time_iso_8601": "2019-11-20T03:01:34.670791Z", "url": "https://files.pythonhosted.org/packages/9f/91/848c8bbadbf92f97b604f48e634d93bc5403d301c570a273bb477db6b729/simpletransformers-0.7.10-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "abddfb7cd956c168d058d45e7b6dd8e6", "sha256": "d051d13c1c20e333e6455684d9e8b3b23f7fbb6d36553f9500a8f3efaaefcd13"}, "downloads": -1, "filename": "simpletransformers-0.7.10.tar.gz", "has_sig": false, "md5_digest": "abddfb7cd956c168d058d45e7b6dd8e6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 65533, "upload_time": "2019-11-20T03:01:36", "upload_time_iso_8601": "2019-11-20T03:01:36.361222Z", "url": "https://files.pythonhosted.org/packages/3f/fd/bbe1461a22b1c4b58e7bb337c152fb3cb430c1cf68cb0c10f4faca59c1d7/simpletransformers-0.7.10.tar.gz", "yanked": false}], "0.7.11": [{"comment_text": "", "digests": {"md5": "fce699f105755b30de66384aecdec9bc", "sha256": "84ae82c6f3445556185db2b0f06a28e27663ce5197d6baa09b4ce7705a9b6e1e"}, "downloads": -1, "filename": "simpletransformers-0.7.11-py3-none-any.whl", "has_sig": false, "md5_digest": "fce699f105755b30de66384aecdec9bc", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 63860, "upload_time": "2019-11-22T05:35:26", "upload_time_iso_8601": "2019-11-22T05:35:26.840207Z", "url": "https://files.pythonhosted.org/packages/a6/2d/35d330ac0eb653bda7d38d88aae927b57429b1610652b76af958d91ec77a/simpletransformers-0.7.11-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f0a869442c529747c6e249e777b8c19b", "sha256": "4b4d95f979bd7ad57a78ba1d75a1db18486131b4d2632b4ec4828bc5793ef4c1"}, "downloads": -1, "filename": "simpletransformers-0.7.11.tar.gz", "has_sig": false, "md5_digest": "f0a869442c529747c6e249e777b8c19b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 65570, "upload_time": "2019-11-22T05:35:28", "upload_time_iso_8601": "2019-11-22T05:35:28.555796Z", "url": "https://files.pythonhosted.org/packages/20/d9/a731b89d79d3445efcaf007545a715ad09896f0ff0b43a01938d08dd3d10/simpletransformers-0.7.11.tar.gz", "yanked": false}], "0.7.12": [{"comment_text": "", "digests": {"md5": "15bfa0b3c86f68a553db428497644010", "sha256": "26d2ce05791da1b66e8092731e10d48fa38bfe65f7e68e441ba2b36719310429"}, "downloads": -1, "filename": "simpletransformers-0.7.12-py3-none-any.whl", "has_sig": false, "md5_digest": "15bfa0b3c86f68a553db428497644010", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 63863, "upload_time": "2019-11-22T07:53:39", "upload_time_iso_8601": "2019-11-22T07:53:39.492919Z", "url": "https://files.pythonhosted.org/packages/2f/55/dcaffa12852f57156a9501953187fc97f5b0111f51bb8a944a078a7bbaea/simpletransformers-0.7.12-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5a5895357e93a895f6697d6d43363d08", "sha256": "ce7a46b819e10452c08332acfaaa002cda0bab30c0b57ae908f10349377fea47"}, "downloads": -1, "filename": "simpletransformers-0.7.12.tar.gz", "has_sig": false, "md5_digest": "5a5895357e93a895f6697d6d43363d08", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 65557, "upload_time": "2019-11-22T07:53:41", "upload_time_iso_8601": "2019-11-22T07:53:41.119652Z", "url": "https://files.pythonhosted.org/packages/8d/b6/a52de040aedfcc578e00981d6ac63a2053f49f33c692bd8b22a590c900c3/simpletransformers-0.7.12.tar.gz", "yanked": false}], "0.7.5": [{"comment_text": "", "digests": {"md5": "4beada48bb8b7d84887f9df997623ed3", "sha256": "5b399cbfdb38a753511149d21d834a4da9957f7ced9a0bcfc09dd16d4864fe4a"}, "downloads": -1, "filename": "simpletransformers-0.7.5-py3-none-any.whl", "has_sig": false, "md5_digest": "4beada48bb8b7d84887f9df997623ed3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 55421, "upload_time": "2019-11-16T15:56:22", "upload_time_iso_8601": "2019-11-16T15:56:22.013903Z", "url": "https://files.pythonhosted.org/packages/19/c5/17efcd642761c5d5e605550cf5f73408aac8147a34bf87fdeb83cab3339d/simpletransformers-0.7.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8eb475a3d33828947c332a10860123ae", "sha256": "3b1892a5c6b69a952a7a8183b25d9311ece6de6c9e3ade5bc4171078877d2959"}, "downloads": -1, "filename": "simpletransformers-0.7.5.tar.gz", "has_sig": false, "md5_digest": "8eb475a3d33828947c332a10860123ae", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 61884, "upload_time": "2019-11-16T15:56:23", "upload_time_iso_8601": "2019-11-16T15:56:23.393670Z", "url": "https://files.pythonhosted.org/packages/2b/41/64a3e6aea0e27642b61b9a11c0c1a8432f149198d71640f63984d6159569/simpletransformers-0.7.5.tar.gz", "yanked": false}], "0.7.6": [{"comment_text": "", "digests": {"md5": "81e1964e776c4b881c85174f7e6757a1", "sha256": "d090ed568109e893dfb8f6fa4bf23fa3f77d733734f8cc6bc91510c22ef9fc2c"}, "downloads": -1, "filename": "simpletransformers-0.7.6-py3-none-any.whl", "has_sig": false, "md5_digest": "81e1964e776c4b881c85174f7e6757a1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 55537, "upload_time": "2019-11-17T08:23:37", "upload_time_iso_8601": "2019-11-17T08:23:37.260808Z", "url": "https://files.pythonhosted.org/packages/67/12/9b2a339da7347c682c23c860721ea4ace4a8d5f882374362af0272aa6dff/simpletransformers-0.7.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "85acb34891902f98b4b78d0b7d285763", "sha256": "118ac7cdea85f347e495445e145a449cd2826f0601a34c9c8f2dfd304f67ad28"}, "downloads": -1, "filename": "simpletransformers-0.7.6.tar.gz", "has_sig": false, "md5_digest": "85acb34891902f98b4b78d0b7d285763", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 62046, "upload_time": "2019-11-17T08:23:39", "upload_time_iso_8601": "2019-11-17T08:23:39.001937Z", "url": "https://files.pythonhosted.org/packages/fb/37/0bea9fc965074b1263da5b9031f60a204f18fd75617579a3e57add0f723b/simpletransformers-0.7.6.tar.gz", "yanked": false}], "0.7.7": [{"comment_text": "", "digests": {"md5": "9b80c55edc7cad76515e4d5dcf25359c", "sha256": "f1df2598e9503a14444b76d560c0080f485dbbf835deff7e0c0a5dd5052159bc"}, "downloads": -1, "filename": "simpletransformers-0.7.7-py3-none-any.whl", "has_sig": false, "md5_digest": "9b80c55edc7cad76515e4d5dcf25359c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 55838, "upload_time": "2019-11-17T19:05:45", "upload_time_iso_8601": "2019-11-17T19:05:45.047647Z", "url": "https://files.pythonhosted.org/packages/46/46/4f3a2b8a485187b501dc4afa8e4c0afbc1b34071d634b8e7efed2d9762ae/simpletransformers-0.7.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c69d92120148479916d9fa3dac0b754d", "sha256": "b1f7ed6d4f2bbd73be556bd0d6e3df1967675f53198955fd074e21e8a0a567dd"}, "downloads": -1, "filename": "simpletransformers-0.7.7.tar.gz", "has_sig": false, "md5_digest": "c69d92120148479916d9fa3dac0b754d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 62679, "upload_time": "2019-11-17T19:05:46", "upload_time_iso_8601": "2019-11-17T19:05:46.775488Z", "url": "https://files.pythonhosted.org/packages/94/ea/992b2835de4b3e546ca184366d325f648f8757536bb371e18347866f050f/simpletransformers-0.7.7.tar.gz", "yanked": false}], "0.7.8": [{"comment_text": "", "digests": {"md5": "f4cf759078c3625f08679e735a24e2f4", "sha256": "c24d03c987e9d24b7c4facc3e181eca336e60d2e7cc2a872bca260d5fce56c90"}, "downloads": -1, "filename": "simpletransformers-0.7.8-py3-none-any.whl", "has_sig": false, "md5_digest": "f4cf759078c3625f08679e735a24e2f4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 63726, "upload_time": "2019-11-17T20:38:19", "upload_time_iso_8601": "2019-11-17T20:38:19.014384Z", "url": "https://files.pythonhosted.org/packages/ce/c9/b181e94a4365b46784540bfdfade8aa826d2272263c61fc6d81508fe52fc/simpletransformers-0.7.8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e718d90ec8bc858ac21290e829e39102", "sha256": "04bc996cc6027040623fec86f66143b301bc22cedb0d43b50a5f6af6396bd155"}, "downloads": -1, "filename": "simpletransformers-0.7.8.tar.gz", "has_sig": false, "md5_digest": "e718d90ec8bc858ac21290e829e39102", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 65296, "upload_time": "2019-11-17T20:38:20", "upload_time_iso_8601": "2019-11-17T20:38:20.558360Z", "url": "https://files.pythonhosted.org/packages/11/f9/ad4d4c3e51196aa410fdc2b3f973379ead5112758b73cb03f49d07d72fd4/simpletransformers-0.7.8.tar.gz", "yanked": false}], "0.7.9": [{"comment_text": "", "digests": {"md5": "fa3ccb2b30710765c372a92102e59319", "sha256": "f8656f092ae11e4b724b02b195abe09b720632999cfb74873870462c7cf1fda2"}, "downloads": -1, "filename": "simpletransformers-0.7.9-py3-none-any.whl", "has_sig": false, "md5_digest": "fa3ccb2b30710765c372a92102e59319", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 63827, "upload_time": "2019-11-18T17:05:10", "upload_time_iso_8601": "2019-11-18T17:05:10.288192Z", "url": "https://files.pythonhosted.org/packages/03/cd/961f80a66589de431e80be3b87c943617bacb01dd8535a464f1fc1ef6676/simpletransformers-0.7.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d3c76a2194262e3c5cbdc81296fe6267", "sha256": "6b9f052dd6d94e5f2c9cbcf43b45388b094f5f483cf62b0ded7dde047cc720f8"}, "downloads": -1, "filename": "simpletransformers-0.7.9.tar.gz", "has_sig": false, "md5_digest": "d3c76a2194262e3c5cbdc81296fe6267", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 65528, "upload_time": "2019-11-18T17:05:11", "upload_time_iso_8601": "2019-11-18T17:05:11.711007Z", "url": "https://files.pythonhosted.org/packages/d6/98/be1eadc79f9ab5283427c01a9e8e821166e326befdd05f5381f01b054c36/simpletransformers-0.7.9.tar.gz", "yanked": false}], "0.8.0": [{"comment_text": "", "digests": {"md5": "d387d16b88c3c1a9a90bfa4640338326", "sha256": "58e913caf94d2f4dccb2d4f0d00e769bc6ade4c1bd96c83cbe6714ce8d84eda6"}, "downloads": -1, "filename": "simpletransformers-0.8.0-py3-none-any.whl", "has_sig": false, "md5_digest": "d387d16b88c3c1a9a90bfa4640338326", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 63903, "upload_time": "2019-11-27T14:21:00", "upload_time_iso_8601": "2019-11-27T14:21:00.318348Z", "url": "https://files.pythonhosted.org/packages/92/37/26b5befb3cd47468dc5417ea387ed2e3edd47d696cc7ca6ae6f6a2657e09/simpletransformers-0.8.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ea0c7d3a62b7949425fa823abbfcca52", "sha256": "e18970d6757d6aac31df20665a9e50411c8816308197fc588af8e4bbb65d1ba2"}, "downloads": -1, "filename": "simpletransformers-0.8.0.tar.gz", "has_sig": false, "md5_digest": "ea0c7d3a62b7949425fa823abbfcca52", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 65670, "upload_time": "2019-11-27T14:21:02", "upload_time_iso_8601": "2019-11-27T14:21:02.118359Z", "url": "https://files.pythonhosted.org/packages/53/16/6a1422be0840be65d896f8cd1d7ab4dd25975651173ab8b570083d975747/simpletransformers-0.8.0.tar.gz", "yanked": false}], "0.8.1": [{"comment_text": "", "digests": {"md5": "bdd2288610854f731b043bd01d50fa2c", "sha256": "7f0df99e26eb14bd86ace7f5127232c0550c40d04a7e804dc0596e705783e99f"}, "downloads": -1, "filename": "simpletransformers-0.8.1-py3-none-any.whl", "has_sig": false, "md5_digest": "bdd2288610854f731b043bd01d50fa2c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 65582, "upload_time": "2019-11-27T20:58:28", "upload_time_iso_8601": "2019-11-27T20:58:28.378139Z", "url": "https://files.pythonhosted.org/packages/a6/61/d427c43c9cfa77b371429299b46cc2e100c8d52ea81aa9dda85487197b83/simpletransformers-0.8.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0fa508cfc1a78b435960e6beaf0460b4", "sha256": "618985888d21b791603fe64200a8135be2fe554df7e12028d1be4793a0a4c190"}, "downloads": -1, "filename": "simpletransformers-0.8.1.tar.gz", "has_sig": false, "md5_digest": "0fa508cfc1a78b435960e6beaf0460b4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 66097, "upload_time": "2019-11-27T20:58:30", "upload_time_iso_8601": "2019-11-27T20:58:30.422204Z", "url": "https://files.pythonhosted.org/packages/32/de/c62aca6c7c1d701428517c8314f7e5f75a75e8d7f0ace9d8db4fc4d397d6/simpletransformers-0.8.1.tar.gz", "yanked": false}], "0.8.2": [{"comment_text": "", "digests": {"md5": "041864e280d6b5bacc43ce8f9fb2fad8", "sha256": "2aedcfba16faca49f08e9838f09b214844bf1120a0b3f00a15d924c379acea48"}, "downloads": -1, "filename": "simpletransformers-0.8.2-py3-none-any.whl", "has_sig": false, "md5_digest": "041864e280d6b5bacc43ce8f9fb2fad8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 65587, "upload_time": "2019-11-27T22:11:40", "upload_time_iso_8601": "2019-11-27T22:11:40.847426Z", "url": "https://files.pythonhosted.org/packages/f0/ef/3ee71a431944221a99ca37bd14826cd9a89c3760df50594d230d75c31a09/simpletransformers-0.8.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c98da6ac19aeb9196bec64b995208ad4", "sha256": "09942d1d1a1f0f2a8d53f2f96d9167053c8ac95ce3b0ea2f12810ce752b9ab63"}, "downloads": -1, "filename": "simpletransformers-0.8.2.tar.gz", "has_sig": false, "md5_digest": "c98da6ac19aeb9196bec64b995208ad4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 66082, "upload_time": "2019-11-27T22:11:42", "upload_time_iso_8601": "2019-11-27T22:11:42.830997Z", "url": "https://files.pythonhosted.org/packages/fe/1f/d5f988e23a126f88d95376f7c09acfedcc968330a34ac41d911f76e40b82/simpletransformers-0.8.2.tar.gz", "yanked": false}], "0.9.0": [{"comment_text": "", "digests": {"md5": "272b7e7cdcb61b254eadc8120982391f", "sha256": "d9f40774595df01c50153db5a83b387faf31d6479ba716afcce72cd1742430d5"}, "downloads": -1, "filename": "simpletransformers-0.9.0-py3-none-any.whl", "has_sig": false, "md5_digest": "272b7e7cdcb61b254eadc8120982391f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 69820, "upload_time": "2019-11-30T11:51:53", "upload_time_iso_8601": "2019-11-30T11:51:53.005292Z", "url": "https://files.pythonhosted.org/packages/5b/f9/bc43a08b8a5e160502703cd4555237f569fa5341313db9a13149db357f1d/simpletransformers-0.9.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "27e2e88a9eebc37519f21ba2b37bdd2c", "sha256": "f2deeec0409ef521b42f95076b4000ed9aeeb8c2817d85466e344bb9da95e0ce"}, "downloads": -1, "filename": "simpletransformers-0.9.0.tar.gz", "has_sig": false, "md5_digest": "27e2e88a9eebc37519f21ba2b37bdd2c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 70036, "upload_time": "2019-11-30T11:51:54", "upload_time_iso_8601": "2019-11-30T11:51:54.747626Z", "url": "https://files.pythonhosted.org/packages/d4/67/12d20098319ab96f84d7f15aff09bc1a0cf1c86715391b4cbaaa1c45b6bd/simpletransformers-0.9.0.tar.gz", "yanked": false}], "0.9.1": [{"comment_text": "", "digests": {"md5": "4b424c0b1d16a917f8f9b2332dd59d91", "sha256": "97d55da273e84c4c4b13fd7798b3f1d807d3b85d8e9475bde055ca5a24d8d4fb"}, "downloads": -1, "filename": "simpletransformers-0.9.1-py3-none-any.whl", "has_sig": false, "md5_digest": "4b424c0b1d16a917f8f9b2332dd59d91", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 69839, "upload_time": "2019-11-30T13:18:33", "upload_time_iso_8601": "2019-11-30T13:18:33.438451Z", "url": "https://files.pythonhosted.org/packages/cc/f6/84a9cfe05133569f0868c0aff4b379ea8dd62a457db104981218398cfb4d/simpletransformers-0.9.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5286df08ba1f0a4b9602f41227fecec9", "sha256": "0c165300a5969e14be1d51bf47908185013cf36b4047a8b7da74a9539735915f"}, "downloads": -1, "filename": "simpletransformers-0.9.1.tar.gz", "has_sig": false, "md5_digest": "5286df08ba1f0a4b9602f41227fecec9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 70036, "upload_time": "2019-11-30T13:18:35", "upload_time_iso_8601": "2019-11-30T13:18:35.077322Z", "url": "https://files.pythonhosted.org/packages/ee/9e/dc0ca8470a79a4faf24f853caae048af29dab7d43daba739b285120ee223/simpletransformers-0.9.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f2802c5c26d1eee2c8a7c41001a2f9b7", "sha256": "781e01d0e4379294d929476e006b6587a7556e137471f750513a98e6bebd982a"}, "downloads": -1, "filename": "simpletransformers-0.27.1-py3-none-any.whl", "has_sig": false, "md5_digest": "f2802c5c26d1eee2c8a7c41001a2f9b7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 170729, "upload_time": "2020-05-06T11:50:50", "upload_time_iso_8601": "2020-05-06T11:50:50.155867Z", "url": "https://files.pythonhosted.org/packages/39/fc/f26659f003809f08ad34ceec8f1c2d998c3f1c9ffc81b5c912a8c4a9e1e5/simpletransformers-0.27.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b7bfc7b97322f38d63cc9d9e66d6e95c", "sha256": "6f75f1dccccd13c6e0db8d4532e06bf6c7eafdb9820d70d228eadec3b9abc3e9"}, "downloads": -1, "filename": "simpletransformers-0.27.1.tar.gz", "has_sig": false, "md5_digest": "b7bfc7b97322f38d63cc9d9e66d6e95c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 190389, "upload_time": "2020-05-06T11:50:52", "upload_time_iso_8601": "2020-05-06T11:50:52.299811Z", "url": "https://files.pythonhosted.org/packages/57/00/36ce328b33287f98a04e80a6cb04572054b4e1bed2413671d5064d980476/simpletransformers-0.27.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:09:53 2020"}