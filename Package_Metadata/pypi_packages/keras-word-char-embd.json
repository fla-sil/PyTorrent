{"info": {"author": "CyberZHG", "author_email": "CyberZHG@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.6"], "description": "# Word/Character Embeddings in Keras\n\n[![Travis](https://travis-ci.org/CyberZHG/keras-word-char-embd.svg)](https://travis-ci.org/CyberZHG/keras-word-char-embd)\n[![Coverage](https://coveralls.io/repos/github/CyberZHG/keras-word-char-embd/badge.svg?branch=master)](https://coveralls.io/github/CyberZHG/keras-word-char-embd)\n\n## Introduction\n\n![image](https://user-images.githubusercontent.com/853842/43352939-c84b9724-925e-11e8-9488-29ef159a69ed.png)\n\nOut-of-vocabulary words are drawbacks of word embeddings. Sometimes both word and character features are used. The characters in a word are first mapped to character embeddings, then a bidirectional recurrent neural layer is used to encode the character embeddings to a single vector. The final feature of a word is the concatenation of the word embedding and the encoded character feature.\n\nThe repository contains some functions and a wrapper class that could be used to generate the first few layers that encodes the features of words and characters.\n\n## Install\n\n```bash\npip install keras-word-char-embd\n```\n\n### Demo\n\nThere is a sentiment analysis demo in the `demo` directory. Run the following commands, then your model should have about 70% accuracy:\n\n```bash\ncd demo\n./get_data.sh\npython sentiment_analysis.py\n```\n\n### Functions\n\nThis section only introduces the basic usages of the functions. For more detailed information please refer to the demo and the doc comments describing the functions in the source code.\n\n#### `get_dicts_generator`\n\nThe function returns a closure used to generate word and character dictionaries. The closure should be invoked for all the training sentences in order to record the frequencies of each word or character. After that, setting the parameter `return_dict=True` the dictionaries would be returned.\n\n```python\nfrom keras_wc_embd import get_dicts_generator\n\nsentences = [\n    ['All', 'work', 'and', 'no', 'play'],\n    ['makes', 'Jack', 'a', 'dull', 'boy', '.'],\n]\ndict_generator = get_dicts_generator(\n    word_min_freq=2,\n    char_min_freq=2,\n    word_ignore_case=False,\n    char_ignore_case=False,\n)\nfor sentence in sentences:\n    dict_generator(sentence)\n\nword_dict, char_dict, max_word_len = dict_generator(return_dict=True)\n```\n\nYou can generate dictionaries on your own, but make sure index `0` and index for `<UNK>` are preserved.\n\n#### `get_embedding_layer`\n\nGenerate the first few layers that encodes words in a sentence:\n\n```python\nimport keras\nfrom keras_wc_embd import get_embedding_layer\n\ninputs, embd_layer = get_embedding_layer(\n    word_dict_len=len(word_dict),\n    char_dict_len=len(char_dict),\n    max_word_len=max_word_len,\n    word_embd_dim=300,\n    char_embd_dim=50,\n    char_hidden_dim=150,\n    char_hidden_layer_type='lstm',\n)\nmodel = keras.models.Model(inputs=inputs, outputs=embd_layer)\nmodel.summary()\n```\n\nThe output shape of `embd_layer` should be `(None, None, 600)`, which represents the batch size, the length of sentence and the length of encoded word feature.\n\n`char_hidden_layer_type` could be `'lstm'`, `'gru'`, `'cnn'`, a Keras layer or a list of Keras layers. Remember to add `MaskedConv1D` and `MaskedFlatten` to custom objects if you are using `'cnn'`:\n\n```python\nimport keras\nfrom keras_wc_embd import MaskedConv1D, MaskedFlatten\n\nkeras.models.load_model(filepath, custom_objects={\n    'MaskedConv1D': MaskedConv1D,\n    'MaskedFlatten': MaskedFlatten,\n})\n```\n\n#### `get_batch_input`\n\nThe function is used to generate the batch inputs for the model.\n\n```python\nfrom keras_wc_embd import get_batch_input\n\nword_embd_input, char_embd_input = get_batch_input(\n    sentences,\n    max_word_len=max_word_len,\n    word_dict=word_dict,\n    char_dict=char_dict,\n)\n```\n\n#### `get_embedding_weights_from_file`\n\nA helper function that loads pre-trained embeddings for initializing the weights of the embedding layer. The format of the file should be similar to GloVe.\n\n```python\nfrom keras_wc_embd import get_embedding_layer, get_embedding_weights_from_file\n\nword_embd_weights = get_embedding_weights_from_file(word_dict, 'glove.6B.100d.txt', ignore_case=True)\ninputs, embd_layer = get_embedding_layer(\n    word_dict_len=len(word_dict),\n    char_dict_len=len(char_dict),\n    max_word_len=max_word_len,\n    word_embd_dim=300,\n    char_embd_dim=50,\n    char_hidden_dim=150,\n    word_embd_weights=word_embd_weights,\n    char_hidden_layer_type='lstm',\n)\n```\n\n### Wrapper Class `WordCharEmbd`\n\nThere is a wrapper class that makes things easier.\n\n```python\nfrom keras_wc_embd import WordCharEmbd\n\nsentences = [\n    ['All', 'work', 'and', 'no', 'play'],\n    ['makes', 'Jack', 'a', 'dull', 'boy', '.'],\n]\nwc_embd = WordCharEmbd(\n    word_min_freq=0,\n    char_min_freq=0,\n    word_ignore_case=False,\n    char_ignore_case=False,\n)\nfor sentence in sentences:\n    wc_embd.update_dicts(sentence)\n\ninputs, embd_layer = wc_embd.get_embedding_layer()\nlstm_layer = keras.layers.LSTM(units=5, name='LSTM')(embd_layer)\nsoftmax_layer = keras.layers.Dense(units=2, activation='softmax', name='Softmax')(lstm_layer)\nmodel = keras.models.Model(inputs=inputs, outputs=softmax_layer)\nmodel.compile(\n    optimizer='adam',\n    loss=keras.losses.sparse_categorical_crossentropy,\n    metrics=[keras.metrics.sparse_categorical_accuracy],\n)\nmodel.summary()\n\n\ndef batch_generator():\n    while True:\n        yield wc_embd.get_batch_input(sentences), np.asarray([0, 1])\n\nmodel.fit_generator(\n    generator=batch_generator(),\n    steps_per_epoch=200,\n    epochs=1,\n)\n```\n\n## Citation\n\nSeveral papers have done the same thing. Just choose the one you have seen.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/CyberZHG/keras-word-char-embd", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "keras-word-char-embd", "package_url": "https://pypi.org/project/keras-word-char-embd/", "platform": "", "project_url": "https://pypi.org/project/keras-word-char-embd/", "project_urls": {"Homepage": "https://github.com/CyberZHG/keras-word-char-embd"}, "release_url": "https://pypi.org/project/keras-word-char-embd/0.20.0/", "requires_dist": null, "requires_python": "", "summary": "Concatenate word and character embeddings in Keras", "version": "0.20.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Word/Character Embeddings in Keras</h1>\n<p><a href=\"https://travis-ci.org/CyberZHG/keras-word-char-embd\" rel=\"nofollow\"><img alt=\"Travis\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3fd1120d51d2c372bc082e3c1a07742bd92a692f/68747470733a2f2f7472617669732d63692e6f72672f43796265725a48472f6b657261732d776f72642d636861722d656d62642e737667\"></a>\n<a href=\"https://coveralls.io/github/CyberZHG/keras-word-char-embd\" rel=\"nofollow\"><img alt=\"Coverage\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/26d66e5f2ab745433431a669bb9dc977fae90e6a/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f43796265725a48472f6b657261732d776f72642d636861722d656d62642f62616467652e7376673f6272616e63683d6d6173746572\"></a></p>\n<h2>Introduction</h2>\n<p><img alt=\"image\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5f808f9fc6216d06353d2874c5a024898ff71ecf/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f3835333834322f34333335323933392d63383462393732342d393235652d313165382d393438382d3239656631353961363965642e706e67\"></p>\n<p>Out-of-vocabulary words are drawbacks of word embeddings. Sometimes both word and character features are used. The characters in a word are first mapped to character embeddings, then a bidirectional recurrent neural layer is used to encode the character embeddings to a single vector. The final feature of a word is the concatenation of the word embedding and the encoded character feature.</p>\n<p>The repository contains some functions and a wrapper class that could be used to generate the first few layers that encodes the features of words and characters.</p>\n<h2>Install</h2>\n<pre>pip install keras-word-char-embd\n</pre>\n<h3>Demo</h3>\n<p>There is a sentiment analysis demo in the <code>demo</code> directory. Run the following commands, then your model should have about 70% accuracy:</p>\n<pre><span class=\"nb\">cd</span> demo\n./get_data.sh\npython sentiment_analysis.py\n</pre>\n<h3>Functions</h3>\n<p>This section only introduces the basic usages of the functions. For more detailed information please refer to the demo and the doc comments describing the functions in the source code.</p>\n<h4><code>get_dicts_generator</code></h4>\n<p>The function returns a closure used to generate word and character dictionaries. The closure should be invoked for all the training sentences in order to record the frequencies of each word or character. After that, setting the parameter <code>return_dict=True</code> the dictionaries would be returned.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">keras_wc_embd</span> <span class=\"kn\">import</span> <span class=\"n\">get_dicts_generator</span>\n\n<span class=\"n\">sentences</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s1\">'All'</span><span class=\"p\">,</span> <span class=\"s1\">'work'</span><span class=\"p\">,</span> <span class=\"s1\">'and'</span><span class=\"p\">,</span> <span class=\"s1\">'no'</span><span class=\"p\">,</span> <span class=\"s1\">'play'</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'makes'</span><span class=\"p\">,</span> <span class=\"s1\">'Jack'</span><span class=\"p\">,</span> <span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'dull'</span><span class=\"p\">,</span> <span class=\"s1\">'boy'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">],</span>\n<span class=\"p\">]</span>\n<span class=\"n\">dict_generator</span> <span class=\"o\">=</span> <span class=\"n\">get_dicts_generator</span><span class=\"p\">(</span>\n    <span class=\"n\">word_min_freq</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n    <span class=\"n\">char_min_freq</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n    <span class=\"n\">word_ignore_case</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"n\">char_ignore_case</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">sentence</span> <span class=\"ow\">in</span> <span class=\"n\">sentences</span><span class=\"p\">:</span>\n    <span class=\"n\">dict_generator</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">)</span>\n\n<span class=\"n\">word_dict</span><span class=\"p\">,</span> <span class=\"n\">char_dict</span><span class=\"p\">,</span> <span class=\"n\">max_word_len</span> <span class=\"o\">=</span> <span class=\"n\">dict_generator</span><span class=\"p\">(</span><span class=\"n\">return_dict</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<p>You can generate dictionaries on your own, but make sure index <code>0</code> and index for <code>&lt;UNK&gt;</code> are preserved.</p>\n<h4><code>get_embedding_layer</code></h4>\n<p>Generate the first few layers that encodes words in a sentence:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">keras</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras_wc_embd</span> <span class=\"kn\">import</span> <span class=\"n\">get_embedding_layer</span>\n\n<span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">embd_layer</span> <span class=\"o\">=</span> <span class=\"n\">get_embedding_layer</span><span class=\"p\">(</span>\n    <span class=\"n\">word_dict_len</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">word_dict</span><span class=\"p\">),</span>\n    <span class=\"n\">char_dict_len</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">char_dict</span><span class=\"p\">),</span>\n    <span class=\"n\">max_word_len</span><span class=\"o\">=</span><span class=\"n\">max_word_len</span><span class=\"p\">,</span>\n    <span class=\"n\">word_embd_dim</span><span class=\"o\">=</span><span class=\"mi\">300</span><span class=\"p\">,</span>\n    <span class=\"n\">char_embd_dim</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">,</span>\n    <span class=\"n\">char_hidden_dim</span><span class=\"o\">=</span><span class=\"mi\">150</span><span class=\"p\">,</span>\n    <span class=\"n\">char_hidden_layer_type</span><span class=\"o\">=</span><span class=\"s1\">'lstm'</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"n\">embd_layer</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"p\">()</span>\n</pre>\n<p>The output shape of <code>embd_layer</code> should be <code>(None, None, 600)</code>, which represents the batch size, the length of sentence and the length of encoded word feature.</p>\n<p><code>char_hidden_layer_type</code> could be <code>'lstm'</code>, <code>'gru'</code>, <code>'cnn'</code>, a Keras layer or a list of Keras layers. Remember to add <code>MaskedConv1D</code> and <code>MaskedFlatten</code> to custom objects if you are using <code>'cnn'</code>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">keras</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras_wc_embd</span> <span class=\"kn\">import</span> <span class=\"n\">MaskedConv1D</span><span class=\"p\">,</span> <span class=\"n\">MaskedFlatten</span>\n\n<span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"n\">filepath</span><span class=\"p\">,</span> <span class=\"n\">custom_objects</span><span class=\"o\">=</span><span class=\"p\">{</span>\n    <span class=\"s1\">'MaskedConv1D'</span><span class=\"p\">:</span> <span class=\"n\">MaskedConv1D</span><span class=\"p\">,</span>\n    <span class=\"s1\">'MaskedFlatten'</span><span class=\"p\">:</span> <span class=\"n\">MaskedFlatten</span><span class=\"p\">,</span>\n<span class=\"p\">})</span>\n</pre>\n<h4><code>get_batch_input</code></h4>\n<p>The function is used to generate the batch inputs for the model.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">keras_wc_embd</span> <span class=\"kn\">import</span> <span class=\"n\">get_batch_input</span>\n\n<span class=\"n\">word_embd_input</span><span class=\"p\">,</span> <span class=\"n\">char_embd_input</span> <span class=\"o\">=</span> <span class=\"n\">get_batch_input</span><span class=\"p\">(</span>\n    <span class=\"n\">sentences</span><span class=\"p\">,</span>\n    <span class=\"n\">max_word_len</span><span class=\"o\">=</span><span class=\"n\">max_word_len</span><span class=\"p\">,</span>\n    <span class=\"n\">word_dict</span><span class=\"o\">=</span><span class=\"n\">word_dict</span><span class=\"p\">,</span>\n    <span class=\"n\">char_dict</span><span class=\"o\">=</span><span class=\"n\">char_dict</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</pre>\n<h4><code>get_embedding_weights_from_file</code></h4>\n<p>A helper function that loads pre-trained embeddings for initializing the weights of the embedding layer. The format of the file should be similar to GloVe.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">keras_wc_embd</span> <span class=\"kn\">import</span> <span class=\"n\">get_embedding_layer</span><span class=\"p\">,</span> <span class=\"n\">get_embedding_weights_from_file</span>\n\n<span class=\"n\">word_embd_weights</span> <span class=\"o\">=</span> <span class=\"n\">get_embedding_weights_from_file</span><span class=\"p\">(</span><span class=\"n\">word_dict</span><span class=\"p\">,</span> <span class=\"s1\">'glove.6B.100d.txt'</span><span class=\"p\">,</span> <span class=\"n\">ignore_case</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">embd_layer</span> <span class=\"o\">=</span> <span class=\"n\">get_embedding_layer</span><span class=\"p\">(</span>\n    <span class=\"n\">word_dict_len</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">word_dict</span><span class=\"p\">),</span>\n    <span class=\"n\">char_dict_len</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">char_dict</span><span class=\"p\">),</span>\n    <span class=\"n\">max_word_len</span><span class=\"o\">=</span><span class=\"n\">max_word_len</span><span class=\"p\">,</span>\n    <span class=\"n\">word_embd_dim</span><span class=\"o\">=</span><span class=\"mi\">300</span><span class=\"p\">,</span>\n    <span class=\"n\">char_embd_dim</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">,</span>\n    <span class=\"n\">char_hidden_dim</span><span class=\"o\">=</span><span class=\"mi\">150</span><span class=\"p\">,</span>\n    <span class=\"n\">word_embd_weights</span><span class=\"o\">=</span><span class=\"n\">word_embd_weights</span><span class=\"p\">,</span>\n    <span class=\"n\">char_hidden_layer_type</span><span class=\"o\">=</span><span class=\"s1\">'lstm'</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</pre>\n<h3>Wrapper Class <code>WordCharEmbd</code></h3>\n<p>There is a wrapper class that makes things easier.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">keras_wc_embd</span> <span class=\"kn\">import</span> <span class=\"n\">WordCharEmbd</span>\n\n<span class=\"n\">sentences</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[</span><span class=\"s1\">'All'</span><span class=\"p\">,</span> <span class=\"s1\">'work'</span><span class=\"p\">,</span> <span class=\"s1\">'and'</span><span class=\"p\">,</span> <span class=\"s1\">'no'</span><span class=\"p\">,</span> <span class=\"s1\">'play'</span><span class=\"p\">],</span>\n    <span class=\"p\">[</span><span class=\"s1\">'makes'</span><span class=\"p\">,</span> <span class=\"s1\">'Jack'</span><span class=\"p\">,</span> <span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'dull'</span><span class=\"p\">,</span> <span class=\"s1\">'boy'</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">],</span>\n<span class=\"p\">]</span>\n<span class=\"n\">wc_embd</span> <span class=\"o\">=</span> <span class=\"n\">WordCharEmbd</span><span class=\"p\">(</span>\n    <span class=\"n\">word_min_freq</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">char_min_freq</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">word_ignore_case</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"n\">char_ignore_case</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">sentence</span> <span class=\"ow\">in</span> <span class=\"n\">sentences</span><span class=\"p\">:</span>\n    <span class=\"n\">wc_embd</span><span class=\"o\">.</span><span class=\"n\">update_dicts</span><span class=\"p\">(</span><span class=\"n\">sentence</span><span class=\"p\">)</span>\n\n<span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">embd_layer</span> <span class=\"o\">=</span> <span class=\"n\">wc_embd</span><span class=\"o\">.</span><span class=\"n\">get_embedding_layer</span><span class=\"p\">()</span>\n<span class=\"n\">lstm_layer</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">LSTM</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'LSTM'</span><span class=\"p\">)(</span><span class=\"n\">embd_layer</span><span class=\"p\">)</span>\n<span class=\"n\">softmax_layer</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'softmax'</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'Softmax'</span><span class=\"p\">)(</span><span class=\"n\">lstm_layer</span><span class=\"p\">)</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"n\">softmax_layer</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span>\n    <span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s1\">'adam'</span><span class=\"p\">,</span>\n    <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">losses</span><span class=\"o\">.</span><span class=\"n\">sparse_categorical_crossentropy</span><span class=\"p\">,</span>\n    <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">sparse_categorical_accuracy</span><span class=\"p\">],</span>\n<span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"p\">()</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">batch_generator</span><span class=\"p\">():</span>\n    <span class=\"k\">while</span> <span class=\"kc\">True</span><span class=\"p\">:</span>\n        <span class=\"k\">yield</span> <span class=\"n\">wc_embd</span><span class=\"o\">.</span><span class=\"n\">get_batch_input</span><span class=\"p\">(</span><span class=\"n\">sentences</span><span class=\"p\">),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">asarray</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit_generator</span><span class=\"p\">(</span>\n    <span class=\"n\">generator</span><span class=\"o\">=</span><span class=\"n\">batch_generator</span><span class=\"p\">(),</span>\n    <span class=\"n\">steps_per_epoch</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span>\n    <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</pre>\n<h2>Citation</h2>\n<p>Several papers have done the same thing. Just choose the one you have seen.</p>\n\n          </div>"}, "last_serial": 5011626, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "82865fe3635dfe2a3626ed5d86832df8", "sha256": "72033721fe06321bdf004d3db4f3c79bbbafb95e337faf12b9a10e69432468f2"}, "downloads": -1, "filename": "keras_word_char_embd-0.0.1-py3.6.egg", "has_sig": false, "md5_digest": "82865fe3635dfe2a3626ed5d86832df8", "packagetype": "bdist_egg", "python_version": "3.6", "requires_python": null, "size": 5838, "upload_time": "2018-07-28T04:35:00", "upload_time_iso_8601": "2018-07-28T04:35:00.599113Z", "url": "https://files.pythonhosted.org/packages/48/ef/3a46bbcbe43266c9925504dc67c9bc29712e311fe703087522f77249508c/keras_word_char_embd-0.0.1-py3.6.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "e0420443b6878a2d9b5c7a1542cc745e", "sha256": "6cc861a0b60b598e4ab728fd9125ce03adc2961e9ecc60099fe37cf748da34a3"}, "downloads": -1, "filename": "keras_word_char_embd-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "e0420443b6878a2d9b5c7a1542cc745e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 4238, "upload_time": "2018-07-28T04:34:59", "upload_time_iso_8601": "2018-07-28T04:34:59.159561Z", "url": "https://files.pythonhosted.org/packages/7e/66/d3489a55395a2d9701690438bc8e619af2ddcb37bfa4b4ba7f5b6d91306a/keras_word_char_embd-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c29c7b7edec3d0c1fb701b9f8e6bd38e", "sha256": "caf5b65367f0626fd12a355358b2fffbdaadabbd3f56300251a03365c6c22cd7"}, "downloads": -1, "filename": "keras_word_char_embd-0.0.1.tar.gz", "has_sig": false, "md5_digest": "c29c7b7edec3d0c1fb701b9f8e6bd38e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2594, "upload_time": "2018-07-28T04:35:01", "upload_time_iso_8601": "2018-07-28T04:35:01.734483Z", "url": "https://files.pythonhosted.org/packages/80/0b/0c9fd7842938989482e5c158f4ef1dc2feaf83472df80828ae51577670e7/keras_word_char_embd-0.0.1.tar.gz", "yanked": false}], "0.0.10": [{"comment_text": "", "digests": {"md5": "47a135cfed1e7cbfd2cb4d1511e5de41", "sha256": "18ac3fb7e1f82e72e7d1b62f7a548d2cfcf5998c94e461da602d8a1c67b23b50"}, "downloads": -1, "filename": "keras-word-char-embd-0.0.10.tar.gz", "has_sig": false, "md5_digest": "47a135cfed1e7cbfd2cb4d1511e5de41", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5330, "upload_time": "2018-08-15T07:47:03", "upload_time_iso_8601": "2018-08-15T07:47:03.844074Z", "url": "https://files.pythonhosted.org/packages/c3/d6/350fd2b21b6cadc143177305bc05b64655cb01626c3b0dfde5ce6c6ce104/keras-word-char-embd-0.0.10.tar.gz", "yanked": false}], "0.0.11": [{"comment_text": "", "digests": {"md5": "580fa9f55f1465276f85db5a4f9d7a0c", "sha256": "c2cb2494b57b0f33a1da902192e267f1ba5654cf6aa7923f9df22d8d8bbc7532"}, "downloads": -1, "filename": "keras-word-char-embd-0.0.11.tar.gz", "has_sig": false, "md5_digest": "580fa9f55f1465276f85db5a4f9d7a0c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5286, "upload_time": "2018-08-17T09:34:47", "upload_time_iso_8601": "2018-08-17T09:34:47.792393Z", "url": "https://files.pythonhosted.org/packages/07/cd/3cef531631ed3785990c3f8472496cadca1e0389e75505575d63ce145ed5/keras-word-char-embd-0.0.11.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "27be977a07d20bbaa3a8b94ba6b0abd7", "sha256": "e541aa392de0db2deafc14d274c77c50f47448d050911e321f9ac5b33d80eff3"}, "downloads": -1, "filename": "keras_word_char_embd-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "27be977a07d20bbaa3a8b94ba6b0abd7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 4892, "upload_time": "2018-07-28T07:41:19", "upload_time_iso_8601": "2018-07-28T07:41:19.367345Z", "url": "https://files.pythonhosted.org/packages/3c/01/75e48d362b06ad94c45446f2f320f4b51d387fc754d852741599a4777bb6/keras_word_char_embd-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "32ca8795b6f6525dbeeb40e4b7fe8515", "sha256": "e64e67a05d4cd955c21e72767b4a925ef6cd4933b6e533a0110f80d8efb70172"}, "downloads": -1, "filename": "keras_word_char_embd-0.0.2.tar.gz", "has_sig": false, "md5_digest": "32ca8795b6f6525dbeeb40e4b7fe8515", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3158, "upload_time": "2018-07-28T07:41:22", "upload_time_iso_8601": "2018-07-28T07:41:22.354801Z", "url": "https://files.pythonhosted.org/packages/05/2d/a60a4010d128a5d3f0d6ab24b6aaa7c0c34cf5e40555b4ed68bc166ab15a/keras_word_char_embd-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "523c1f1966bb063ef856ff82e8807184", "sha256": "706380502a826736b751ef5769aba7873eef97db34cc957e5c32351c30ef00bc"}, "downloads": -1, "filename": "keras_word_char_embd-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "523c1f1966bb063ef856ff82e8807184", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7213, "upload_time": "2018-07-28T11:05:50", "upload_time_iso_8601": "2018-07-28T11:05:50.165161Z", "url": "https://files.pythonhosted.org/packages/88/13/356fa342fe1a1b5ddffcfe94f0c97586c23a6df021deafc67b3bd74f4388/keras_word_char_embd-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "21d614325fe638d174813021c4c266d9", "sha256": "c159a0cc13b1cdc2f41ebe503044d0adef99a11258134d950a66f1418cb43a6f"}, "downloads": -1, "filename": "keras_word_char_embd-0.0.3.tar.gz", "has_sig": false, "md5_digest": "21d614325fe638d174813021c4c266d9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4501, "upload_time": "2018-07-28T11:05:53", "upload_time_iso_8601": "2018-07-28T11:05:53.864099Z", "url": "https://files.pythonhosted.org/packages/43/d3/1074541c57f91b50cdbe85fcf276ee9f7929a40978173b20de7a00fa0c02/keras_word_char_embd-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "beb23649e421cb82ba86ecbcae3d04f5", "sha256": "f263f7f48d4c1791ca52f242e3ebbddc2845af714a7a455706b2fa258bda773f"}, "downloads": -1, "filename": "keras_word_char_embd-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "beb23649e421cb82ba86ecbcae3d04f5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 4925, "upload_time": "2018-07-31T01:18:03", "upload_time_iso_8601": "2018-07-31T01:18:03.001340Z", "url": "https://files.pythonhosted.org/packages/c2/33/3254a92555bc0f6e17a11ef644e5a1a8d99c46bf6eca1e04f2498a5bdc83/keras_word_char_embd-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a849d49e699728108e6b90f002c0082b", "sha256": "2f758e368c35d5a8321a96d498e3c026d101067c0c8428f882439bf133de02d4"}, "downloads": -1, "filename": "keras-word-char-embd-0.0.4.tar.gz", "has_sig": false, "md5_digest": "a849d49e699728108e6b90f002c0082b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4504, "upload_time": "2018-07-31T01:18:04", "upload_time_iso_8601": "2018-07-31T01:18:04.239735Z", "url": "https://files.pythonhosted.org/packages/93/02/aa1cd81bda26436d83c78666c05654e00c96a2a30250e3be3fc48e31db62/keras-word-char-embd-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "27701d91d64d551e1d23a56fe1b698da", "sha256": "b1f119fb27ae2f4698b08438232c41d8a1b6c4359e1e4ffecd8cbd8ca60707fb"}, "downloads": -1, "filename": "keras_word_char_embd-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "27701d91d64d551e1d23a56fe1b698da", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8161, "upload_time": "2018-07-31T02:51:58", "upload_time_iso_8601": "2018-07-31T02:51:58.242945Z", "url": "https://files.pythonhosted.org/packages/3a/3e/e8d4f1e8c560ad3d4a6dfdd11a2169fc7d1ea09337530a138850d5b7317c/keras_word_char_embd-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "34f59be596d806262a97fa599816cb48", "sha256": "b801c877f4a6260f25502882f252a25cdeef993053f5d313896c5c7ac971da8d"}, "downloads": -1, "filename": "keras-word-char-embd-0.0.5.tar.gz", "has_sig": false, "md5_digest": "34f59be596d806262a97fa599816cb48", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5256, "upload_time": "2018-07-31T02:52:00", "upload_time_iso_8601": "2018-07-31T02:52:00.593902Z", "url": "https://files.pythonhosted.org/packages/87/c9/bddda5a7abbf31f00fba133c1b6cd8e408ad12fe72848038e0748afe240d/keras-word-char-embd-0.0.5.tar.gz", "yanked": false}], "0.0.6": [{"comment_text": "", "digests": {"md5": "b04b60d51eec6838ed8b5a7c9a685337", "sha256": "4dd415b10a3c02956903f4eb0cf497c476800c370d5affbf576235da03b520a1"}, "downloads": -1, "filename": "keras_word_char_embd-0.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "b04b60d51eec6838ed8b5a7c9a685337", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8183, "upload_time": "2018-07-31T03:13:26", "upload_time_iso_8601": "2018-07-31T03:13:26.880140Z", "url": "https://files.pythonhosted.org/packages/ef/43/8ef41972609fd49461cd6e098e9a8f7fef5133de270df3dda3f629b39a43/keras_word_char_embd-0.0.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b4ebbf96826bd8bcbb056e1f95c41ab8", "sha256": "c376df77bbbeeb75d1887fb5e25a2776b51bd177c9d9aacc6ad86ddee5b91c70"}, "downloads": -1, "filename": "keras-word-char-embd-0.0.6.tar.gz", "has_sig": false, "md5_digest": "b4ebbf96826bd8bcbb056e1f95c41ab8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5266, "upload_time": "2018-07-31T03:13:30", "upload_time_iso_8601": "2018-07-31T03:13:30.104090Z", "url": "https://files.pythonhosted.org/packages/3f/84/25425d4bb2f65cf2d8ab29868c4fb5bcb042c90134692b700911e37033a3/keras-word-char-embd-0.0.6.tar.gz", "yanked": false}], "0.0.7": [{"comment_text": "", "digests": {"md5": "df9a87f671b6b3bc4343310e425ff98d", "sha256": "e892b77c58e17b869443200c18d1029363199d87c84cb82bbf28ecb2f7b03034"}, "downloads": -1, "filename": "keras_word_char_embd-0.0.7-py3-none-any.whl", "has_sig": false, "md5_digest": "df9a87f671b6b3bc4343310e425ff98d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8236, "upload_time": "2018-08-03T00:56:40", "upload_time_iso_8601": "2018-08-03T00:56:40.457836Z", "url": "https://files.pythonhosted.org/packages/32/61/e5209748e652ee8ad2d8a239220c09b67c5b1e5824f0e63b0382c9dfa50e/keras_word_char_embd-0.0.7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a11d18c5970a6f03366ca483933e07d2", "sha256": "6cd6a5b29db720b154a9e833c1cc7100d1c1184e77d37771dc5cd7f11d41638c"}, "downloads": -1, "filename": "keras-word-char-embd-0.0.7.tar.gz", "has_sig": false, "md5_digest": "a11d18c5970a6f03366ca483933e07d2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5324, "upload_time": "2018-08-03T00:56:42", "upload_time_iso_8601": "2018-08-03T00:56:42.813116Z", "url": "https://files.pythonhosted.org/packages/78/0e/58093586aa57b74c8588fce5b29350d9b7ff72e6295bc1fdcdad1a0d55c3/keras-word-char-embd-0.0.7.tar.gz", "yanked": false}], "0.0.8": [{"comment_text": "", "digests": {"md5": "70bbc1b80c635e1b3156a9c876da65b2", "sha256": "c6bdd2a2f7184a0c06d18108082461d00efa554a14be5f11c7768bfd04309d1f"}, "downloads": -1, "filename": "keras-word-char-embd-0.0.8.tar.gz", "has_sig": false, "md5_digest": "70bbc1b80c635e1b3156a9c876da65b2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5334, "upload_time": "2018-08-07T02:28:08", "upload_time_iso_8601": "2018-08-07T02:28:08.966290Z", "url": "https://files.pythonhosted.org/packages/46/15/702c84c53c9bc64bf005a4d3da9025f326df059f3f081292f7b74ec19ab8/keras-word-char-embd-0.0.8.tar.gz", "yanked": false}], "0.0.9": [{"comment_text": "", "digests": {"md5": "89a396754f37d658d5dedfd68ae4c486", "sha256": "9165b6fb3a82c43e32dab4ccdbcb2f4c64314242f35f7a86a5e3033d19d7a769"}, "downloads": -1, "filename": "keras-word-char-embd-0.0.9.tar.gz", "has_sig": false, "md5_digest": "89a396754f37d658d5dedfd68ae4c486", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5323, "upload_time": "2018-08-07T03:20:33", "upload_time_iso_8601": "2018-08-07T03:20:33.960706Z", "url": "https://files.pythonhosted.org/packages/44/3c/9401d71b4219b624fef03361fa8203b993b4c2663663cc2370216898825f/keras-word-char-embd-0.0.9.tar.gz", "yanked": false}], "0.12": [{"comment_text": "", "digests": {"md5": "01227d497eeba0687bac4f176072efbf", "sha256": "8d182e93937ce0a0accc9b42a37c48ef076c0d724589c2043c3afa5a79818183"}, "downloads": -1, "filename": "keras-word-char-embd-0.12.tar.gz", "has_sig": false, "md5_digest": "01227d497eeba0687bac4f176072efbf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6940, "upload_time": "2018-09-13T09:00:09", "upload_time_iso_8601": "2018-09-13T09:00:09.445479Z", "url": "https://files.pythonhosted.org/packages/87/e4/a595edb707e9de3e6ed2e1f0f26d44001d3739f86daa48ec8bff90bdb10d/keras-word-char-embd-0.12.tar.gz", "yanked": false}], "0.13": [{"comment_text": "", "digests": {"md5": "45aa0cfd4377612741bbc5b14dc20772", "sha256": "f883522c1993cd6e3d3a80aaf76643ca5508026ad2510916c98aaff6173e5e8b"}, "downloads": -1, "filename": "keras-word-char-embd-0.13.tar.gz", "has_sig": false, "md5_digest": "45aa0cfd4377612741bbc5b14dc20772", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6940, "upload_time": "2018-09-13T09:23:03", "upload_time_iso_8601": "2018-09-13T09:23:03.254070Z", "url": "https://files.pythonhosted.org/packages/ab/bf/3b273e18fe9c5a78cbadf44b7d8a25d4e6bbaafbdf01a11ef52502c27408/keras-word-char-embd-0.13.tar.gz", "yanked": false}], "0.14": [{"comment_text": "", "digests": {"md5": "2efa0fa3c702813e59c632f26c3ba9da", "sha256": "6b524ec70cb80973dcc0f0f158ab6b6dbedc258f8e3302d4a6402c2124fb8041"}, "downloads": -1, "filename": "keras-word-char-embd-0.14.tar.gz", "has_sig": false, "md5_digest": "2efa0fa3c702813e59c632f26c3ba9da", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7259, "upload_time": "2018-09-26T02:45:34", "upload_time_iso_8601": "2018-09-26T02:45:34.559554Z", "url": "https://files.pythonhosted.org/packages/4b/42/23d920aace48b568114aedf6da7d9e2945c5fe0e5246c21e762f40822181/keras-word-char-embd-0.14.tar.gz", "yanked": false}], "0.15": [{"comment_text": "", "digests": {"md5": "60ab85377e4f77ff37c20b17cc070830", "sha256": "2d70a47434c6afc9c14bd1c17302f4e4cec25ba40873f0a1375109fda1ff72c7"}, "downloads": -1, "filename": "keras-word-char-embd-0.15.tar.gz", "has_sig": false, "md5_digest": "60ab85377e4f77ff37c20b17cc070830", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7257, "upload_time": "2018-09-26T03:07:48", "upload_time_iso_8601": "2018-09-26T03:07:48.904683Z", "url": "https://files.pythonhosted.org/packages/2b/5b/d3b38aac8b4f6884bde6eafa35be1ebe9c13aa3060bf4465ccbc061c8a95/keras-word-char-embd-0.15.tar.gz", "yanked": false}], "0.16.0": [{"comment_text": "", "digests": {"md5": "962580c1df169435f17526ada3b7a5af", "sha256": "36ea66e9da16cb9565e7d6f9b9e493f656971649785761c80db73bdad2b52f87"}, "downloads": -1, "filename": "keras-word-char-embd-0.16.0.tar.gz", "has_sig": false, "md5_digest": "962580c1df169435f17526ada3b7a5af", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7282, "upload_time": "2018-12-26T07:31:14", "upload_time_iso_8601": "2018-12-26T07:31:14.625917Z", "url": "https://files.pythonhosted.org/packages/82/83/fc2e0a1f869b7216b81c85351620205a9f51f1eb7adbeb3ea1ab414665ac/keras-word-char-embd-0.16.0.tar.gz", "yanked": false}], "0.17.0": [{"comment_text": "", "digests": {"md5": "7507df62ecbde201ec242b4a5e93ef51", "sha256": "67eb15538fa90964d37876d6eaa443f3da54fe595ecc99714261eb5a718430e3"}, "downloads": -1, "filename": "keras-word-char-embd-0.17.0.tar.gz", "has_sig": false, "md5_digest": "7507df62ecbde201ec242b4a5e93ef51", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7172, "upload_time": "2019-02-01T03:26:31", "upload_time_iso_8601": "2019-02-01T03:26:31.409241Z", "url": "https://files.pythonhosted.org/packages/b2/23/f59ba904c9533df7d73d1c0281e4d53d8d69c88ad49fba87dc1f4d3b72ad/keras-word-char-embd-0.17.0.tar.gz", "yanked": false}], "0.18.0": [{"comment_text": "", "digests": {"md5": "1baf2e0049524a5e12fc6dd424f8ba4f", "sha256": "1e49d7257a6b586eafab11c6c0e79e47fc9f7a7f132c4fe8e94b97e347e5441f"}, "downloads": -1, "filename": "keras-word-char-embd-0.18.0.tar.gz", "has_sig": false, "md5_digest": "1baf2e0049524a5e12fc6dd424f8ba4f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7340, "upload_time": "2019-02-01T03:28:08", "upload_time_iso_8601": "2019-02-01T03:28:08.587217Z", "url": "https://files.pythonhosted.org/packages/85/c3/e0ce7ad1bbffb60632bb0be0796eabce3b7f3390cbb1a304267ab4eec041/keras-word-char-embd-0.18.0.tar.gz", "yanked": false}], "0.19.0": [{"comment_text": "", "digests": {"md5": "95aaeaf4d1f600a8a553bbe94e59dc69", "sha256": "2bef1f06a1b153831872f76b3635726f7ed778f8e3914de72643bbf1a622a662"}, "downloads": -1, "filename": "keras-word-char-embd-0.19.0.tar.gz", "has_sig": false, "md5_digest": "95aaeaf4d1f600a8a553bbe94e59dc69", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7331, "upload_time": "2019-03-11T06:38:48", "upload_time_iso_8601": "2019-03-11T06:38:48.542793Z", "url": "https://files.pythonhosted.org/packages/6e/24/7417e44a587a485385781087f3964d737bcb087d0b732baa46a0b1addf69/keras-word-char-embd-0.19.0.tar.gz", "yanked": false}], "0.20.0": [{"comment_text": "", "digests": {"md5": "647ea1f51256be7382721c68edc510a6", "sha256": "0d8e5a9bcf6ee89447a9f4a50ca10398425503922ad38634a77b13a51fd4e834"}, "downloads": -1, "filename": "keras-word-char-embd-0.20.0.tar.gz", "has_sig": false, "md5_digest": "647ea1f51256be7382721c68edc510a6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7672, "upload_time": "2019-04-01T06:31:49", "upload_time_iso_8601": "2019-04-01T06:31:49.147808Z", "url": "https://files.pythonhosted.org/packages/cd/31/b32b253197a159bde1385b394f5858edb3cbc7cde1cc111e629869a7b72b/keras-word-char-embd-0.20.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "647ea1f51256be7382721c68edc510a6", "sha256": "0d8e5a9bcf6ee89447a9f4a50ca10398425503922ad38634a77b13a51fd4e834"}, "downloads": -1, "filename": "keras-word-char-embd-0.20.0.tar.gz", "has_sig": false, "md5_digest": "647ea1f51256be7382721c68edc510a6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7672, "upload_time": "2019-04-01T06:31:49", "upload_time_iso_8601": "2019-04-01T06:31:49.147808Z", "url": "https://files.pythonhosted.org/packages/cd/31/b32b253197a159bde1385b394f5858edb3cbc7cde1cc111e629869a7b72b/keras-word-char-embd-0.20.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:07 2020"}