{"info": {"author": "Jonathan Raiman", "author_email": "jraiman at mit dot edu", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "Operating System :: OS Independent", "Programming Language :: Python :: 3.3", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Mathematics", "Topic :: Text Processing :: Linguistic"], "description": "# Gradient Optimizers\n\nOptimize you Theano Models with [Adagrad](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf), Hessian Free optimization, or linear updates.\n\n\n    pip3 install gradient-optimizers\n\n\nSee example notebook (TBD) for tutorial.\n\nTwo classes **GradientModel**, and **GradientHFModel**, for optimizing gradient\nbased models (specifically built with indexed parameters in mind (e.g.\nfor language models))\n\n## GradientModel\n\nA gradient model for updating your model with\nhessian free, adagrad, or linear decay updates.\n\nYou will need to define the following attributes,\nand fill them as appropriate:\n    \n    # a forward method for getting errors:\n    projection = self.projection_function(ivector <indices/>)\n\n    # a cost function (that takes the result of projection function and labels as input)\n    # and returns a symbolic differentiable theano variable\n    self.cost_function(projection, ivector <label/>).sum()\n\n    self.params = []\n    self.indexed_params = set()\n\n    self._l2_regularization = True / False\n\n    self.store_max_updates = True / False\n\n    # set this theano setting\n    self.theano_mode = \"FAST_RUN\"\n\n    # set this theano setting\n    self.disconnected_inputs = 'ignore' / None\n\n    # if L2 is true store this parameter:\n    self._l2_regularization_parameter = theano.shared(np.float64(l2_regularization).astype(REAL), name='l2_regularization_parameter')\n\nUpon initialization you must run:\n\n    self._select_update_mechanism(update_method_name)\n\n    # then to compile this mechanism:\n    self.create_update_fun()\n\nThe update methods expect the input to be of the form:\n\n    ivector <indices/>, ivector <labels/>\n\nIf this is not the case you can modify them as appropriate.\n\n## GradientHFModel\n\nImplements an symbolic one step of hessian-free [1]\noptimization that approximates the curvature,\nrequires a _compute_cost method that takes an example\nas input or a _compute_cost_gradients that returns\ngradients for each example provided.\n\nModel should have a params property containing symbolic\ntheano variables.\n\n[[1] James Martens, ``Deep learning via Hessian-free optimization\", ICML 2010](http://www.icml2010.org/papers/458.pdf)\n\nMake sure the following parameters are not tampered with:\n\n    self._additional_params\n\n    self._num_updates", "description_content_type": null, "docs_url": null, "download_url": "https://github.com/JonathanRaiman/gradient_optimizers", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/JonathanRaiman/gradient_optimizers", "keywords": "Machine Learning,Gradient Descent,NLP,Optimization,Hessian Free optimization", "license": "MIT", "maintainer": null, "maintainer_email": null, "name": "gradient-optimizers", "package_url": "https://pypi.org/project/gradient-optimizers/", "platform": "any", "project_url": "https://pypi.org/project/gradient-optimizers/", "project_urls": {"Download": "https://github.com/JonathanRaiman/gradient_optimizers", "Homepage": "https://github.com/JonathanRaiman/gradient_optimizers"}, "release_url": "https://pypi.org/project/gradient-optimizers/0.0.4/", "requires_dist": null, "requires_python": null, "summary": "Python package for wrapping gradient optimizers for models in Theano", "version": "0.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            # Gradient Optimizers<br><br>Optimize you Theano Models with [Adagrad](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf), Hessian Free optimization, or linear updates.<br><br><br>    pip3 install gradient-optimizers<br><br><br>See example notebook (TBD) for tutorial.<br><br>Two classes **GradientModel**, and **GradientHFModel**, for optimizing gradient<br>based models (specifically built with indexed parameters in mind (e.g.<br>for language models))<br><br>## GradientModel<br><br>A gradient model for updating your model with<br>hessian free, adagrad, or linear decay updates.<br><br>You will need to define the following attributes,<br>and fill them as appropriate:<br>    <br>    # a forward method for getting errors:<br>    projection = self.projection_function(ivector &lt;indices/&gt;)<br><br>    # a cost function (that takes the result of projection function and labels as input)<br>    # and returns a symbolic differentiable theano variable<br>    self.cost_function(projection, ivector &lt;label/&gt;).sum()<br><br>    self.params = []<br>    self.indexed_params = set()<br><br>    self._l2_regularization = True / False<br><br>    self.store_max_updates = True / False<br><br>    # set this theano setting<br>    self.theano_mode = \"FAST_RUN\"<br><br>    # set this theano setting<br>    self.disconnected_inputs = 'ignore' / None<br><br>    # if L2 is true store this parameter:<br>    self._l2_regularization_parameter = theano.shared(np.float64(l2_regularization).astype(REAL), name='l2_regularization_parameter')<br><br>Upon initialization you must run:<br><br>    self._select_update_mechanism(update_method_name)<br><br>    # then to compile this mechanism:<br>    self.create_update_fun()<br><br>The update methods expect the input to be of the form:<br><br>    ivector &lt;indices/&gt;, ivector &lt;labels/&gt;<br><br>If this is not the case you can modify them as appropriate.<br><br>## GradientHFModel<br><br>Implements an symbolic one step of hessian-free [1]<br>optimization that approximates the curvature,<br>requires a _compute_cost method that takes an example<br>as input or a _compute_cost_gradients that returns<br>gradients for each example provided.<br><br>Model should have a params property containing symbolic<br>theano variables.<br><br>[[1] James Martens, ``Deep learning via Hessian-free optimization\", ICML 2010](http://www.icml2010.org/papers/458.pdf)<br><br>Make sure the following parameters are not tampered with:<br><br>    self._additional_params<br><br>    self._num_updates\n          </div>"}, "last_serial": 1289311, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "a26fb25aa269cd67d366849a18c726da", "sha256": "4357aeacb7c7a067c83764161697058eae598bb77bf1189dc48dd6e1afc22e6d"}, "downloads": -1, "filename": "gradient-optimizers-0.0.1.tar.gz", "has_sig": false, "md5_digest": "a26fb25aa269cd67d366849a18c726da", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8602, "upload_time": "2014-10-30T22:34:04", "upload_time_iso_8601": "2014-10-30T22:34:04.928867Z", "url": "https://files.pythonhosted.org/packages/3e/f6/204264dba9f61f13e9f5412cafef1f2f05539af476b3ec13b9c43aeb807e/gradient-optimizers-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "bf4e69456da104c77f5e8e420e81237f", "sha256": "0c0d7b53f61dff6168817535f967ec602237176ec09d12d2d18bf9389ad6854a"}, "downloads": -1, "filename": "gradient-optimizers-0.0.2.tar.gz", "has_sig": false, "md5_digest": "bf4e69456da104c77f5e8e420e81237f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8750, "upload_time": "2014-10-30T22:46:00", "upload_time_iso_8601": "2014-10-30T22:46:00.769322Z", "url": "https://files.pythonhosted.org/packages/b3/15/739c19a273c96b9d0b3773083533c4e6bc7304b82a676fbe58154cef072d/gradient-optimizers-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "3a2618f0479842bbaacf3c4e646b5ef0", "sha256": "fb73a3265416a53199c27bb12fa606f13c604da142e94beddf61128e6813de06"}, "downloads": -1, "filename": "gradient-optimizers-0.0.3.tar.gz", "has_sig": false, "md5_digest": "3a2618f0479842bbaacf3c4e646b5ef0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9143, "upload_time": "2014-10-30T22:55:49", "upload_time_iso_8601": "2014-10-30T22:55:49.286651Z", "url": "https://files.pythonhosted.org/packages/b1/c3/d7e02b3efa96a4685e01c2c79e81507b6cee688d16495e4fe1473101f491/gradient-optimizers-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "2f09e8041063a3dfd53dd364b2418844", "sha256": "9ccfeb3b13297ade4506c1cdb1574d1f09f63018df7046fb5c0f9450a3bf9d4f"}, "downloads": -1, "filename": "gradient-optimizers-0.0.4.tar.gz", "has_sig": false, "md5_digest": "2f09e8041063a3dfd53dd364b2418844", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9186, "upload_time": "2014-10-30T23:07:07", "upload_time_iso_8601": "2014-10-30T23:07:07.247664Z", "url": "https://files.pythonhosted.org/packages/04/72/2a6824a6af69c4fa8eb2aa2c3348625490bb58d5b18f149b28aa7a719643/gradient-optimizers-0.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2f09e8041063a3dfd53dd364b2418844", "sha256": "9ccfeb3b13297ade4506c1cdb1574d1f09f63018df7046fb5c0f9450a3bf9d4f"}, "downloads": -1, "filename": "gradient-optimizers-0.0.4.tar.gz", "has_sig": false, "md5_digest": "2f09e8041063a3dfd53dd364b2418844", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9186, "upload_time": "2014-10-30T23:07:07", "upload_time_iso_8601": "2014-10-30T23:07:07.247664Z", "url": "https://files.pythonhosted.org/packages/04/72/2a6824a6af69c4fa8eb2aa2c3348625490bb58d5b18f149b28aa7a719643/gradient-optimizers-0.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:54:58 2020"}