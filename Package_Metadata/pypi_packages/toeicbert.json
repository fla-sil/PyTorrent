{"info": {"author": "Tae Hwan Jung(@graykode)", "author_email": "nlkey2022@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "## TOEIC-BERT\n\n### 76% Correct rate with ONLY Pre-Trained BERT model in TOEIC!!\n\n\n\nThis is project as topic: `TOEIC(Test of English for International Communication) problem solving using pytorch-pretrained-BERT model.` The reason why I used huggingface's [pytorch-pretrained-BERT model](<https://github.com/huggingface/pytorch-pretrained-BERT>) is for pre-training or to do fine-tune more easily.  **I've solved the only blank problem, not the whole problem.** There are two types of blank issues:\n\n1. Selecting Correct Grammar Type.\n\n```\nQ) The teacher had me _________ scales several times a day.\n  1. play (Answer)\n  2. to play\n  3. played\n  4. playing\n```\n\n2. Selecting Correct Vocabulary Type.\n\n```\nQ) The wet weather _________ her from going shopping.\n  1. interrupted\n  2. obstructed\n  3. impeded\n  4. discouraged (Answer)\n```\n\n\n\n#### Why BERT?\n\nIn pretrained BERT, It contains contextual information. So It can find more contextual or grammatical sentences, not clear, a little bit. I was inspired by grammar checker from [blog post](<https://www.scribendi.ai/can-we-use-bert-as-a-language-model-to-assign-score-of-a-sentence/>).\n\n> [Can We Use BERT as a Language Model to Assign a Score to a Sentence?](<https://www.scribendi.ai/can-we-use-bert-as-a-language-model-to-assign-score-of-a-sentence/>)\n>\n> BERT uses a bidirectional encoder to encapsulate a sentence from left to right and from right to left. Thus, it learns two representations of each word-one from left to right and one from right to left-and then concatenates them for many downstream tasks.\n\n\n\n## Evaluation\n\n<p align=\"center\"><img width=\"500\" src=\"images/baseline.gif\" /></p>\n\nI had evaluated with only **pretrained BERT model(not fine-tuning)** to check grammatical or lexical error. Above mathematical expression, `X` is a question sentence. and `n` is number of questions : `{a, b, c, d}`. `C` subset means answer candidate tokens : `C` of `warranty` is `['warrant', '##y']`. `V` means total Vocabulary.\n\nThere's a problem with more than one token. I solved this problem by getting the average value of each tensor. ex) `is being formed` as `['is', 'being', 'formed']` \n\nThen, we find argmax in `L_n(T_n)`.\n\n![](images/prediction.gif)\n\n```python\npredictions = model(question_tensors, segment_tensors)\n\n# predictions : [batch_size, sequence_length, vocab_size]\npredictions_candidates = predictions[0, masked_index, candidate_ids].mean()\n```\n\n\n\n#### Result of Evaluation.\n\nFantastic result with **only pretrained BERT model**\n\n- `bert-base-uncased`: 12-layer, 768-hidden, 12-heads, 110M parameters\n- `bert-large-uncased`: 24-layer, 1024-hidden, 16-heads, 340M parameters\n- `bert-base-cased`: 12-layer, 768-hidden, 12-heads , 110M parameters\n- `bert-large-cased`: 24-layer, 1024-hidden, 16-heads, 340M parameters\n\nTotal 7067 datasets: make non-deterministic with `model.eval()`\n\n|             | bert-base-uncased | bert-base-cased | bert-large-uncased | bert-large-cased |\n| :---------: | :---------------: | :-------------: | :----------------: | :--------------: |\n| Correct Num |       5192        |      5398       |        5321        |       5148       |\n|   Percent   |      73.46%       |     76.38%      |       75.29%       |      72.84       |\n\n\n\n## Quick Start with Python pip Package.\n\n**Start with pip**\n\n```shell\n$ pip install toeicbert\n```\n\n\n\n**Run & Option**\n\n```shell\n$ python toeicbert -m bert-base-uncased -f test.json\n```\n\n- `-m, --model` : bert-model name in huggingface's pytorch-pretrained-BERT : `bert-base-uncased`, `bert-large-uncased`, `bert-base-cased`, `bert-large-cased`.\n\n- `-f, --file` : json file to evalution, see json format, [test.json](test.json). \n\n  **key(question, 1, 2, 3, 4)  is required options, but answer not.**\n\n  `_` in question will be replaced to `[MASK]`\n\n```json\n{\n    \"1\" : {\n        \"question\" : \"The teacher had me _ scales several times a day.\",\n        \"answer\" : \"play\",\n        \"1\" : \"play\",\n        \"2\" : \"to play\",\n        \"3\" : \"played\",\n        \"4\" : \"playing\"\n    },\n    \"2\" : {\n        \"question\" : \"The teacher had me _ scales several times a day.\",\n        \"1\" : \"play\",\n        \"2\" : \"to play\",\n        \"3\" : \"played\",\n        \"4\" : \"playing\"\n    }\n}\n```\n\n\n\n## Author\n\n- Tae Hwan Jung(Jeff Jung) @graykode, Kyung Hee Univ CE(Undergraduate).\n- Author Email : [nlkey2022@gmail.com](mailto:nlkey2022@gmail.com)\n\nThanks for Hwan Suk Gang(Kyung Hee Univ.) for collecting Dataset(`7114` datasets)", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/graykode/toeicbert", "keywords": "BERT TOEIC pytorch-pretrained-BERT bert nlp NLP", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "toeicbert", "package_url": "https://pypi.org/project/toeicbert/", "platform": "", "project_url": "https://pypi.org/project/toeicbert/", "project_urls": {"Homepage": "https://github.com/graykode/toeicbert"}, "release_url": "https://pypi.org/project/toeicbert/0.0.2/", "requires_dist": null, "requires_python": "", "summary": "TOEIC blank problem solving using pytorch-pretrained-BERT model.", "version": "0.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h2>TOEIC-BERT</h2>\n<h3>76% Correct rate with ONLY Pre-Trained BERT model in TOEIC!!</h3>\n<p>This is project as topic: <code>TOEIC(Test of English for International Communication) problem solving using pytorch-pretrained-BERT model.</code> The reason why I used huggingface's <a href=\"https://github.com/huggingface/pytorch-pretrained-BERT\" rel=\"nofollow\">pytorch-pretrained-BERT model</a> is for pre-training or to do fine-tune more easily.  <strong>I've solved the only blank problem, not the whole problem.</strong> There are two types of blank issues:</p>\n<ol>\n<li>Selecting Correct Grammar Type.</li>\n</ol>\n<pre><code>Q) The teacher had me _________ scales several times a day.\n  1. play (Answer)\n  2. to play\n  3. played\n  4. playing\n</code></pre>\n<ol>\n<li>Selecting Correct Vocabulary Type.</li>\n</ol>\n<pre><code>Q) The wet weather _________ her from going shopping.\n  1. interrupted\n  2. obstructed\n  3. impeded\n  4. discouraged (Answer)\n</code></pre>\n<h4>Why BERT?</h4>\n<p>In pretrained BERT, It contains contextual information. So It can find more contextual or grammatical sentences, not clear, a little bit. I was inspired by grammar checker from <a href=\"https://www.scribendi.ai/can-we-use-bert-as-a-language-model-to-assign-score-of-a-sentence/\" rel=\"nofollow\">blog post</a>.</p>\n<blockquote>\n<p><a href=\"https://www.scribendi.ai/can-we-use-bert-as-a-language-model-to-assign-score-of-a-sentence/\" rel=\"nofollow\">Can We Use BERT as a Language Model to Assign a Score to a Sentence?</a></p>\n<p>BERT uses a bidirectional encoder to encapsulate a sentence from left to right and from right to left. Thus, it learns two representations of each word-one from left to right and one from right to left-and then concatenates them for many downstream tasks.</p>\n</blockquote>\n<h2>Evaluation</h2>\n<p align=\"center\"><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fbe059d547be1db71e4687b99ff787994dff28b7/696d616765732f626173656c696e652e676966\" width=\"500\"></p>\n<p>I had evaluated with only <strong>pretrained BERT model(not fine-tuning)</strong> to check grammatical or lexical error. Above mathematical expression, <code>X</code> is a question sentence. and <code>n</code> is number of questions : <code>{a, b, c, d}</code>. <code>C</code> subset means answer candidate tokens : <code>C</code> of <code>warranty</code> is <code>['warrant', '##y']</code>. <code>V</code> means total Vocabulary.</p>\n<p>There's a problem with more than one token. I solved this problem by getting the average value of each tensor. ex) <code>is being formed</code> as <code>['is', 'being', 'formed']</code></p>\n<p>Then, we find argmax in <code>L_n(T_n)</code>.</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bb2a14587a530d5c3c02531965c5d0e0e183e289/696d616765732f70726564696374696f6e2e676966\"></p>\n<pre><span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">question_tensors</span><span class=\"p\">,</span> <span class=\"n\">segment_tensors</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># predictions : [batch_size, sequence_length, vocab_size]</span>\n<span class=\"n\">predictions_candidates</span> <span class=\"o\">=</span> <span class=\"n\">predictions</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">masked_index</span><span class=\"p\">,</span> <span class=\"n\">candidate_ids</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n</pre>\n<h4>Result of Evaluation.</h4>\n<p>Fantastic result with <strong>only pretrained BERT model</strong></p>\n<ul>\n<li><code>bert-base-uncased</code>: 12-layer, 768-hidden, 12-heads, 110M parameters</li>\n<li><code>bert-large-uncased</code>: 24-layer, 1024-hidden, 16-heads, 340M parameters</li>\n<li><code>bert-base-cased</code>: 12-layer, 768-hidden, 12-heads , 110M parameters</li>\n<li><code>bert-large-cased</code>: 24-layer, 1024-hidden, 16-heads, 340M parameters</li>\n</ul>\n<p>Total 7067 datasets: make non-deterministic with <code>model.eval()</code></p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">bert-base-uncased</th>\n<th align=\"center\">bert-base-cased</th>\n<th align=\"center\">bert-large-uncased</th>\n<th align=\"center\">bert-large-cased</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">Correct Num</td>\n<td align=\"center\">5192</td>\n<td align=\"center\">5398</td>\n<td align=\"center\">5321</td>\n<td align=\"center\">5148</td>\n</tr>\n<tr>\n<td align=\"center\">Percent</td>\n<td align=\"center\">73.46%</td>\n<td align=\"center\">76.38%</td>\n<td align=\"center\">75.29%</td>\n<td align=\"center\">72.84</td>\n</tr></tbody></table>\n<h2>Quick Start with Python pip Package.</h2>\n<p><strong>Start with pip</strong></p>\n<pre>$ pip install toeicbert\n</pre>\n<p><strong>Run &amp; Option</strong></p>\n<pre>$ python toeicbert -m bert-base-uncased -f test.json\n</pre>\n<ul>\n<li>\n<p><code>-m, --model</code> : bert-model name in huggingface's pytorch-pretrained-BERT : <code>bert-base-uncased</code>, <code>bert-large-uncased</code>, <code>bert-base-cased</code>, <code>bert-large-cased</code>.</p>\n</li>\n<li>\n<p><code>-f, --file</code> : json file to evalution, see json format, <a href=\"test.json\" rel=\"nofollow\">test.json</a>.</p>\n<p><strong>key(question, 1, 2, 3, 4)  is required options, but answer not.</strong></p>\n<p><code>_</code> in question will be replaced to <code>[MASK]</code></p>\n</li>\n</ul>\n<pre><span class=\"p\">{</span>\n    <span class=\"nt\">\"1\"</span> <span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"question\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"The teacher had me _ scales several times a day.\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"answer\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"play\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"1\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"play\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"2\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"to play\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"3\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"played\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"4\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"playing\"</span>\n    <span class=\"p\">},</span>\n    <span class=\"nt\">\"2\"</span> <span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"question\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"The teacher had me _ scales several times a day.\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"1\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"play\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"2\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"to play\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"3\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"played\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"4\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"playing\"</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</pre>\n<h2>Author</h2>\n<ul>\n<li>Tae Hwan Jung(Jeff Jung) @graykode, Kyung Hee Univ CE(Undergraduate).</li>\n<li>Author Email : <a href=\"mailto:nlkey2022@gmail.com\">nlkey2022@gmail.com</a></li>\n</ul>\n<p>Thanks for Hwan Suk Gang(Kyung Hee Univ.) for collecting Dataset(<code>7114</code> datasets)</p>\n\n          </div>"}, "last_serial": 5204358, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "6f6e6a7ee0067f35f242bc45eac1e09a", "sha256": "05e68d9d0cd60c73f60c6759d493c65a82fbe49e75600e9f1c4e5ca4b80c95e4"}, "downloads": -1, "filename": "toeicbert-0.0.1.tar.gz", "has_sig": false, "md5_digest": "6f6e6a7ee0067f35f242bc45eac1e09a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4447, "upload_time": "2019-04-29T18:00:59", "upload_time_iso_8601": "2019-04-29T18:00:59.459542Z", "url": "https://files.pythonhosted.org/packages/30/3c/5e46abf8cd3df21776889e25b254cd2083ee28b57172eb51d5cc7cffad1d/toeicbert-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "1943c684df30dde25cbc3ef28212b779", "sha256": "547824420c9ecf7a55de546d79dcd5eeb6a3faf62f6ba61d8f327c6b0dde4b2c"}, "downloads": -1, "filename": "toeicbert-0.0.2.tar.gz", "has_sig": false, "md5_digest": "1943c684df30dde25cbc3ef28212b779", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4443, "upload_time": "2019-04-29T18:06:03", "upload_time_iso_8601": "2019-04-29T18:06:03.346781Z", "url": "https://files.pythonhosted.org/packages/9e/04/58275d8074cf90f66cf8dfe867768970b71c9740bf7522058f3724e82b2a/toeicbert-0.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1943c684df30dde25cbc3ef28212b779", "sha256": "547824420c9ecf7a55de546d79dcd5eeb6a3faf62f6ba61d8f327c6b0dde4b2c"}, "downloads": -1, "filename": "toeicbert-0.0.2.tar.gz", "has_sig": false, "md5_digest": "1943c684df30dde25cbc3ef28212b779", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4443, "upload_time": "2019-04-29T18:06:03", "upload_time_iso_8601": "2019-04-29T18:06:03.346781Z", "url": "https://files.pythonhosted.org/packages/9e/04/58275d8074cf90f66cf8dfe867768970b71c9740bf7522058f3724e82b2a/toeicbert-0.0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:51:49 2020"}