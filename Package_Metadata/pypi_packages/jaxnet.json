{"info": {"author": "", "author_email": "", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# JAXnet [![Build Status](https://travis-ci.org/JuliusKunze/jaxnet.svg?branch=master)](https://travis-ci.org/JuliusKunze/jaxnet) \n\nJAXnet is a deep learning library based on [JAX](https://github.com/google/jax).\nJAXnet's functional API provides unique benefits over TensorFlow2, Keras and PyTorch,\nwhile maintaining user-friendliness, modularity and scalability:\n- More robustness through immutable weights, no global compute graph.\n- GPU-compiled `numpy` code for networks, training loops, pre- and postprocessing.\n- Regularization and reparametrization of any module or whole networks in one line.\n- No global random state, flexible random key control.\n\nIf you already know stax, read [this](STAX.md).\n\n### Modularity\n\n```python\nnet = Sequential(Dense(1024), relu, Dense(1024), relu, Dense(4), log_softmax)\n```\ncreates a neural net model from predefined modules.\n\n### Extensibility\n\nDefine your own modules using `@parametrized` functions. You can reuse other modules:\n\n```python\nfrom jax import numpy as np\n\n@parametrized\ndef loss(inputs, targets):\n    return -np.mean(net(inputs) * targets)\n```\n\nAll modules are composed in this way.\n[`jax.numpy`](https://github.com/google/jax#whats-supported) is mirroring `numpy`,\nmeaning that if you know how to use `numpy`, you know most of JAXnet.\nCompare this to TensorFlow2/Keras:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Lambda\n\nnet = Sequential([Dense(1024, 'relu'), Dense(1024, 'relu'), Dense(4), Lambda(tf.nn.log_softmax)])\n\ndef loss(inputs, targets):\n    return -tf.reduce_mean(net(inputs) * targets)\n```\n\nNotice how `Lambda` layers are not needed in JAXnet.\n`relu` and `logsoftmax` are plain Python functions.\n\n### Immutable weights\n\nDifferent from TensorFlow2/Keras, JAXnet has no global compute graph.\nModules like `net` and `loss` do not contain mutable weights.\nInstead, weights are contained in separate, immutable objects.\nThey are initialized with `init_parameters`, provided example inputs and a random key:\n\n```python\nfrom jax.random import PRNGKey\n\ndef next_batch(): return np.zeros((3, 784)), np.zeros((3, 4))\n\nparams = loss.init_parameters(*next_batch(), key=PRNGKey(0))\n\nprint(params.sequential.dense2.bias)  # [-0.01101029, -0.00749435, -0.00952365,  0.00493979]\n```\n\nInstead of mutating weights inline, optimizers return updated versions of weights.\nThey are returned as part of a new optimizer state, and can be retrieved via `get_parameters`:\n\n```python\nopt = optimizers.Adam()\nstate = opt.init(params)\nfor _ in range(10):\n    state = opt.update(loss.apply, state, *next_batch()) # accelerate with jit=True\n\ntrained_params = opt.get_parameters(state)\n```\n\n`apply` evaluates a network:\n\n```python\ntest_loss = loss.apply(trained_params, *test_batch) # accelerate with jit=True\n```\n\n### GPU support and compilation\n\nJAX allows functional `numpy`/`scipy` code to be accelerated.\nMake it run on GPU by replacing your `numpy` import with `jax.numpy`.\nCompile a function by decorating it with [`jit`](https://github.com/google/jax#compilation-with-jit).\nThis will free your function from slow Python interpretation, parallelize operations where possible and optimize your compute graph.\nIt provides speed and scalability at the level of TensorFlow2 or PyTorch.\n\nDue to immutable weights, whole training loops can be compiled / run on GPU ([demo](examples/mnist_vae.py#L96)).\n`jit` will make your training as fast as mutating weights inline, and weights will not leave the GPU.\nYou can write functional code without worrying about performance.\n\nYou can easily accelerate `numpy`/`scipy` pre-/postprocessing code in the same way ([demo](examples/mnist_vae.py#L61)).\n\n### Regularization and reparametrization\n\nIn JAXnet, regularizing a model can be done in one line ([demo](examples/wavenet.py#L167)):\n\n```python\nloss = L2Regularized(loss, scale=.1)\n```\n\n`loss` is now just another module that can be used as above.\nReparametrized layers are one-liners, too (see [API](API.md#regularization-and-reparametrization)).\nJAXnet allows regularizing or reparametrizing any module or subnetwork without changing its code.\nThis is possible because modules do not instantiate any variables.\nInstead each module provides a function (`apply`) with parameters as an argument.\nThis function can be wrapped to build layers like `L2Regularized`.\n\nIn contrast, TensorFlow2/Keras/PyTorch have mutable variables baked into their model API. They therefore require:\n- Regularization arguments on layer level, with separate code necessary for each layer.\n- Reparametrization arguments on layer level, and separate implementations for [each](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseReparameterization) [layer](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution1DReparameterization).\n\n### Random key control\nJAXnet does not have global random state.\nRandom keys are provided explicitly, making code deterministic and independent of previously executed code by default.\nThis can help debugging and is more flexible ([demo](examples/mnist_vae.py#L81)).\nRead more on random numbers in JAX [here](https://github.com/google/jax#random-numbers-are-different).\n\n### Step-by-step debugging\nJAXnet allows step-by-step debugging with concrete values like any plain Python function\n(when [`jit`](https://github.com/google/jax#compilation-with-jit) compilation is not used).\n\n## API and demos\nFind more details on the API [here](API.md).\n\nSee JAXnet in action in your browser:\n[Mnist Classifier](https://colab.research.google.com/drive/18kICTUbjqnfg5Lk3xFVQtUj6ahct9Vmv),\n[Mnist VAE](https://colab.research.google.com/drive/19web5SnmIFglLcnpXE34phiTY03v39-g),\n[OCR with RNNs](https://colab.research.google.com/drive/1YuI6GUtMgnMiWtqoaPznwAiSCe9hMR1E),\n[ResNet](https://colab.research.google.com/drive/1q6yoK_Zscv-57ZzPM4qNy3LgjeFzJ5xN),\n[WaveNet](https://colab.research.google.com/drive/111cKRfwYX4YFuPH3FF4V46XLfsPG1icZ),\n[PixelCNN++](https://colab.research.google.com/drive/1DMRbUPAxTlk0Awf3D_HR3Oz3P3MBahaJ) and\n[Policy Gradient RL](https://colab.research.google.com/drive/171timtUnCOOAsc-eKoC2TjHK9dQFrY7B).\n\n## Installation [![PyPI](https://img.shields.io/pypi/v/jaxnet.svg)](https://pypi.python.org/pypi/jaxnet/#history)\n**This is a preview. Expect breaking changes!** Install with\n\n```\npip3 install jaxnet\n```\n\nTo use GPU, first install the [right version of jaxlib](https://github.com/google/jax#installation).\n\n## Questions\n\nPlease feel free to create an issue on GitHub.\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/JuliusKunze/jaxnet", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "jaxnet", "package_url": "https://pypi.org/project/jaxnet/", "platform": "", "project_url": "https://pypi.org/project/jaxnet/", "project_urls": {"Homepage": "http://github.com/JuliusKunze/jaxnet"}, "release_url": "https://pypi.org/project/jaxnet/0.2.5/", "requires_dist": ["jax (>=0.1.42)", "dill"], "requires_python": "", "summary": "Neural Nets for JAX", "version": "0.2.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>JAXnet <a href=\"https://travis-ci.org/JuliusKunze/jaxnet\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1cba5b15254c55d7b99c41f815410e1dc19b38ce/68747470733a2f2f7472617669732d63692e6f72672f4a756c6975734b756e7a652f6a61786e65742e7376673f6272616e63683d6d6173746572\"></a></h1>\n<p>JAXnet is a deep learning library based on <a href=\"https://github.com/google/jax\" rel=\"nofollow\">JAX</a>.\nJAXnet's functional API provides unique benefits over TensorFlow2, Keras and PyTorch,\nwhile maintaining user-friendliness, modularity and scalability:</p>\n<ul>\n<li>More robustness through immutable weights, no global compute graph.</li>\n<li>GPU-compiled <code>numpy</code> code for networks, training loops, pre- and postprocessing.</li>\n<li>Regularization and reparametrization of any module or whole networks in one line.</li>\n<li>No global random state, flexible random key control.</li>\n</ul>\n<p>If you already know stax, read <a href=\"STAX.md\" rel=\"nofollow\">this</a>.</p>\n<h3>Modularity</h3>\n<pre><span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">(</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1024</span><span class=\"p\">),</span> <span class=\"n\">relu</span><span class=\"p\">,</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1024</span><span class=\"p\">),</span> <span class=\"n\">relu</span><span class=\"p\">,</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">),</span> <span class=\"n\">log_softmax</span><span class=\"p\">)</span>\n</pre>\n<p>creates a neural net model from predefined modules.</p>\n<h3>Extensibility</h3>\n<p>Define your own modules using <code>@parametrized</code> functions. You can reuse other modules:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">jax</span> <span class=\"kn\">import</span> <span class=\"n\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"nd\">@parametrized</span>\n<span class=\"k\">def</span> <span class=\"nf\">loss</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">targets</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"o\">-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">targets</span><span class=\"p\">)</span>\n</pre>\n<p>All modules are composed in this way.\n<a href=\"https://github.com/google/jax#whats-supported\" rel=\"nofollow\"><code>jax.numpy</code></a> is mirroring <code>numpy</code>,\nmeaning that if you know how to use <code>numpy</code>, you know most of JAXnet.\nCompare this to TensorFlow2/Keras:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"nn\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras</span> <span class=\"kn\">import</span> <span class=\"n\">Sequential</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Dense</span><span class=\"p\">,</span> <span class=\"n\">Lambda</span>\n\n<span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">([</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1024</span><span class=\"p\">,</span> <span class=\"s1\">'relu'</span><span class=\"p\">),</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1024</span><span class=\"p\">,</span> <span class=\"s1\">'relu'</span><span class=\"p\">),</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">),</span> <span class=\"n\">Lambda</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">log_softmax</span><span class=\"p\">)])</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">loss</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">targets</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"o\">-</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reduce_mean</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">targets</span><span class=\"p\">)</span>\n</pre>\n<p>Notice how <code>Lambda</code> layers are not needed in JAXnet.\n<code>relu</code> and <code>logsoftmax</code> are plain Python functions.</p>\n<h3>Immutable weights</h3>\n<p>Different from TensorFlow2/Keras, JAXnet has no global compute graph.\nModules like <code>net</code> and <code>loss</code> do not contain mutable weights.\nInstead, weights are contained in separate, immutable objects.\nThey are initialized with <code>init_parameters</code>, provided example inputs and a random key:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">jax.random</span> <span class=\"kn\">import</span> <span class=\"n\">PRNGKey</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">next_batch</span><span class=\"p\">():</span> <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">784</span><span class=\"p\">)),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">))</span>\n\n<span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">init_parameters</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">next_batch</span><span class=\"p\">(),</span> <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"n\">PRNGKey</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"o\">.</span><span class=\"n\">sequential</span><span class=\"o\">.</span><span class=\"n\">dense2</span><span class=\"o\">.</span><span class=\"n\">bias</span><span class=\"p\">)</span>  <span class=\"c1\"># [-0.01101029, -0.00749435, -0.00952365,  0.00493979]</span>\n</pre>\n<p>Instead of mutating weights inline, optimizers return updated versions of weights.\nThey are returned as part of a new optimizer state, and can be retrieved via <code>get_parameters</code>:</p>\n<pre><span class=\"n\">opt</span> <span class=\"o\">=</span> <span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">()</span>\n<span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">init</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">):</span>\n    <span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">,</span> <span class=\"n\">state</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">next_batch</span><span class=\"p\">())</span> <span class=\"c1\"># accelerate with jit=True</span>\n\n<span class=\"n\">trained_params</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">get_parameters</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">)</span>\n</pre>\n<p><code>apply</code> evaluates a network:</p>\n<pre><span class=\"n\">test_loss</span> <span class=\"o\">=</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">trained_params</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">test_batch</span><span class=\"p\">)</span> <span class=\"c1\"># accelerate with jit=True</span>\n</pre>\n<h3>GPU support and compilation</h3>\n<p>JAX allows functional <code>numpy</code>/<code>scipy</code> code to be accelerated.\nMake it run on GPU by replacing your <code>numpy</code> import with <code>jax.numpy</code>.\nCompile a function by decorating it with <a href=\"https://github.com/google/jax#compilation-with-jit\" rel=\"nofollow\"><code>jit</code></a>.\nThis will free your function from slow Python interpretation, parallelize operations where possible and optimize your compute graph.\nIt provides speed and scalability at the level of TensorFlow2 or PyTorch.</p>\n<p>Due to immutable weights, whole training loops can be compiled / run on GPU (<a href=\"examples/mnist_vae.py#L96\" rel=\"nofollow\">demo</a>).\n<code>jit</code> will make your training as fast as mutating weights inline, and weights will not leave the GPU.\nYou can write functional code without worrying about performance.</p>\n<p>You can easily accelerate <code>numpy</code>/<code>scipy</code> pre-/postprocessing code in the same way (<a href=\"examples/mnist_vae.py#L61\" rel=\"nofollow\">demo</a>).</p>\n<h3>Regularization and reparametrization</h3>\n<p>In JAXnet, regularizing a model can be done in one line (<a href=\"examples/wavenet.py#L167\" rel=\"nofollow\">demo</a>):</p>\n<pre><span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">L2Regularized</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">scale</span><span class=\"o\">=.</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n</pre>\n<p><code>loss</code> is now just another module that can be used as above.\nReparametrized layers are one-liners, too (see <a href=\"API.md#regularization-and-reparametrization\" rel=\"nofollow\">API</a>).\nJAXnet allows regularizing or reparametrizing any module or subnetwork without changing its code.\nThis is possible because modules do not instantiate any variables.\nInstead each module provides a function (<code>apply</code>) with parameters as an argument.\nThis function can be wrapped to build layers like <code>L2Regularized</code>.</p>\n<p>In contrast, TensorFlow2/Keras/PyTorch have mutable variables baked into their model API. They therefore require:</p>\n<ul>\n<li>Regularization arguments on layer level, with separate code necessary for each layer.</li>\n<li>Reparametrization arguments on layer level, and separate implementations for <a href=\"https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseReparameterization\" rel=\"nofollow\">each</a> <a href=\"https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution1DReparameterization\" rel=\"nofollow\">layer</a>.</li>\n</ul>\n<h3>Random key control</h3>\n<p>JAXnet does not have global random state.\nRandom keys are provided explicitly, making code deterministic and independent of previously executed code by default.\nThis can help debugging and is more flexible (<a href=\"examples/mnist_vae.py#L81\" rel=\"nofollow\">demo</a>).\nRead more on random numbers in JAX <a href=\"https://github.com/google/jax#random-numbers-are-different\" rel=\"nofollow\">here</a>.</p>\n<h3>Step-by-step debugging</h3>\n<p>JAXnet allows step-by-step debugging with concrete values like any plain Python function\n(when <a href=\"https://github.com/google/jax#compilation-with-jit\" rel=\"nofollow\"><code>jit</code></a> compilation is not used).</p>\n<h2>API and demos</h2>\n<p>Find more details on the API <a href=\"API.md\" rel=\"nofollow\">here</a>.</p>\n<p>See JAXnet in action in your browser:\n<a href=\"https://colab.research.google.com/drive/18kICTUbjqnfg5Lk3xFVQtUj6ahct9Vmv\" rel=\"nofollow\">Mnist Classifier</a>,\n<a href=\"https://colab.research.google.com/drive/19web5SnmIFglLcnpXE34phiTY03v39-g\" rel=\"nofollow\">Mnist VAE</a>,\n<a href=\"https://colab.research.google.com/drive/1YuI6GUtMgnMiWtqoaPznwAiSCe9hMR1E\" rel=\"nofollow\">OCR with RNNs</a>,\n<a href=\"https://colab.research.google.com/drive/1q6yoK_Zscv-57ZzPM4qNy3LgjeFzJ5xN\" rel=\"nofollow\">ResNet</a>,\n<a href=\"https://colab.research.google.com/drive/111cKRfwYX4YFuPH3FF4V46XLfsPG1icZ\" rel=\"nofollow\">WaveNet</a>,\n<a href=\"https://colab.research.google.com/drive/1DMRbUPAxTlk0Awf3D_HR3Oz3P3MBahaJ\" rel=\"nofollow\">PixelCNN++</a> and\n<a href=\"https://colab.research.google.com/drive/171timtUnCOOAsc-eKoC2TjHK9dQFrY7B\" rel=\"nofollow\">Policy Gradient RL</a>.</p>\n<h2>Installation <a href=\"https://pypi.python.org/pypi/jaxnet/#history\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/735e370db2b371a7a9c379ceefce6e3a3f99a000/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6a61786e65742e737667\"></a></h2>\n<p><strong>This is a preview. Expect breaking changes!</strong> Install with</p>\n<pre><code>pip3 install jaxnet\n</code></pre>\n<p>To use GPU, first install the <a href=\"https://github.com/google/jax#installation\" rel=\"nofollow\">right version of jaxlib</a>.</p>\n<h2>Questions</h2>\n<p>Please feel free to create an issue on GitHub.</p>\n\n          </div>"}, "last_serial": 6216273, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "8db1784149f25548669eebbb231b4472", "sha256": "50b7e1355f7c2dd9eaf48184979dd7b3170fe7613705b252c81f634222059a79"}, "downloads": -1, "filename": "jaxnet-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "8db1784149f25548669eebbb231b4472", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 11365, "upload_time": "2019-08-12T20:03:39", "upload_time_iso_8601": "2019-08-12T20:03:39.809137Z", "url": "https://files.pythonhosted.org/packages/f7/32/55e76e8a4e64f510b62c282e85e64c25cd8b14d199eafa339fc1ab5000a8/jaxnet-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4da08703b7674d45d7e1b3e1adc6d5cf", "sha256": "da3fdbfa6660ad00711d15b40e737be02258564812f086283092f41e32608cb8"}, "downloads": -1, "filename": "jaxnet-0.1.1.tar.gz", "has_sig": false, "md5_digest": "4da08703b7674d45d7e1b3e1adc6d5cf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6867, "upload_time": "2019-08-12T20:03:41", "upload_time_iso_8601": "2019-08-12T20:03:41.511473Z", "url": "https://files.pythonhosted.org/packages/49/4f/9e2568e107b5585fbe6c5f3e6cb372b0f0056ea357162a9bcab07c836ff9/jaxnet-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "39ec922823e07515a42c9c1e5c782063", "sha256": "26e1233f5be479cc8adddf4eb0a0d488c1fb2b854f3034a2bd7bb594db322b2b"}, "downloads": -1, "filename": "jaxnet-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "39ec922823e07515a42c9c1e5c782063", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 12411, "upload_time": "2019-08-14T00:18:18", "upload_time_iso_8601": "2019-08-14T00:18:18.720171Z", "url": "https://files.pythonhosted.org/packages/8a/12/6c0f67e5ce0b24d61126699d2b89f5a6dee2c42e07893189bec0ba1aed8c/jaxnet-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ee627744f2b1f91e185a7a664c00ea4e", "sha256": "6fc8ad35c7b4bdf318a649ed1af5df1cddd7ac1f2771199ef6409cae462de497"}, "downloads": -1, "filename": "jaxnet-0.1.2.tar.gz", "has_sig": false, "md5_digest": "ee627744f2b1f91e185a7a664c00ea4e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8540, "upload_time": "2019-08-14T00:18:20", "upload_time_iso_8601": "2019-08-14T00:18:20.569994Z", "url": "https://files.pythonhosted.org/packages/8a/80/177d480fa583be531757292bddf53b131d4b514271363fd858e0c54b3002/jaxnet-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "1fd10cb316da4c3b80d3764780b9ea46", "sha256": "e3a8252ffe5df1a4b769ec0e4fc45607b97526b088fd62fd2835836b8103bf8d"}, "downloads": -1, "filename": "jaxnet-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "1fd10cb316da4c3b80d3764780b9ea46", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 8820, "upload_time": "2019-08-14T22:39:18", "upload_time_iso_8601": "2019-08-14T22:39:18.938908Z", "url": "https://files.pythonhosted.org/packages/b8/14/16b140c1fbc443ebe85ff2355542a1e10e85addd6bedb26f90bdd869a06c/jaxnet-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5eb15bb6729bd70689f35416bbf47fd4", "sha256": "0dce9b79f8a3dcf5431f672b0a3097ef42fd5f4083c9c1e77f66582ec00bdf94"}, "downloads": -1, "filename": "jaxnet-0.1.3.tar.gz", "has_sig": false, "md5_digest": "5eb15bb6729bd70689f35416bbf47fd4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8496, "upload_time": "2019-08-14T22:39:20", "upload_time_iso_8601": "2019-08-14T22:39:20.836332Z", "url": "https://files.pythonhosted.org/packages/8e/f6/369eee1788c7e7fde3145ccb32a3b21426007a0b7c7382c35d43780f77ab/jaxnet-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "e288c293175081ca1cce28e974e042a0", "sha256": "5cba3b80e616aed610e9b64af02067b0aa5ae9f30dda36581ea7bee274e2d845"}, "downloads": -1, "filename": "jaxnet-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "e288c293175081ca1cce28e974e042a0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9046, "upload_time": "2019-08-16T19:48:44", "upload_time_iso_8601": "2019-08-16T19:48:44.962005Z", "url": "https://files.pythonhosted.org/packages/79/65/efbc58d301b1d17c55d02140a7f01801a09ca65aa017dab1b1697b321f3e/jaxnet-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8f7eb80e575fa32ad553de83f82cefc4", "sha256": "c5bffeda1889d110c406ea5d15a017bd567a6907c3bad17bf76d93e86d6e2562"}, "downloads": -1, "filename": "jaxnet-0.1.4.tar.gz", "has_sig": false, "md5_digest": "8f7eb80e575fa32ad553de83f82cefc4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8735, "upload_time": "2019-08-16T19:48:46", "upload_time_iso_8601": "2019-08-16T19:48:46.735208Z", "url": "https://files.pythonhosted.org/packages/0f/27/881e3f24f97b5ade3cd0bb40eb93b5597633e3ac88bbd9109ddd9e02cf98/jaxnet-0.1.4.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "70604e2ddeb0080f0dd38dfcfd5c8959", "sha256": "0c6b7ab9d4f45034884db7d85e2ddd0b62672ae11ce5db38aaa9a64c29410ffd"}, "downloads": -1, "filename": "jaxnet-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "70604e2ddeb0080f0dd38dfcfd5c8959", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 12620, "upload_time": "2019-08-24T22:41:12", "upload_time_iso_8601": "2019-08-24T22:41:12.052444Z", "url": "https://files.pythonhosted.org/packages/ae/f5/22208df7a3eabf9afe972b0a92241cab1cc2fc24737372f6a178acab167e/jaxnet-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "dfaddad78dd6720f9d9877a3e6f6ee87", "sha256": "36d0bed7101011164b7b63c5fb5442849fedbb150910195495a3a27b6d7c063f"}, "downloads": -1, "filename": "jaxnet-0.2.0.tar.gz", "has_sig": false, "md5_digest": "dfaddad78dd6720f9d9877a3e6f6ee87", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14149, "upload_time": "2019-08-24T22:41:13", "upload_time_iso_8601": "2019-08-24T22:41:13.554837Z", "url": "https://files.pythonhosted.org/packages/a9/54/50c5a49e9254da35b8333b56158f9d762b09e8f546a19c018b498efded01/jaxnet-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "93d68bb2a7156209dfadeefbf2eaff47", "sha256": "6547cc79a3729d486168cb000d42532cf882c850468858495ec9e28d26b69f10"}, "downloads": -1, "filename": "jaxnet-0.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "93d68bb2a7156209dfadeefbf2eaff47", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 13710, "upload_time": "2019-08-27T09:53:12", "upload_time_iso_8601": "2019-08-27T09:53:12.909374Z", "url": "https://files.pythonhosted.org/packages/4a/2d/baa736ceed15faf2eab45e973f50af5ecb239be050cee03459d0f28dd812/jaxnet-0.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f0d879bb24ec261011b2b0fd08b85be9", "sha256": "cf6d5988ab4a34d11da11da45405188b79a8616b1f84e6a27e43c944eb128f02"}, "downloads": -1, "filename": "jaxnet-0.2.1.tar.gz", "has_sig": false, "md5_digest": "f0d879bb24ec261011b2b0fd08b85be9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15174, "upload_time": "2019-08-27T09:53:14", "upload_time_iso_8601": "2019-08-27T09:53:14.768932Z", "url": "https://files.pythonhosted.org/packages/e4/23/826c3d12f069a3a244ded2d25f62ef20703e64392313af709cc8dbdb4cee/jaxnet-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "67c24e3a6273af6eae68f227bd771660", "sha256": "4d6b986a9f1e3eaa377ca1fe16d7d3f16610f03029570adea9f0403626d95219"}, "downloads": -1, "filename": "jaxnet-0.2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "67c24e3a6273af6eae68f227bd771660", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 17931, "upload_time": "2019-09-03T06:24:40", "upload_time_iso_8601": "2019-09-03T06:24:40.709306Z", "url": "https://files.pythonhosted.org/packages/b5/50/89d95e5c9cf3020c5b7518737ed5c513840ac7f4d32be6a55787ea075155/jaxnet-0.2.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "28d2193a8645712db97956e4075b5f38", "sha256": "5bb38384c2eb43b476b5c67357c00fc5dcef48e93e497eaf1199f6ce77f6e766"}, "downloads": -1, "filename": "jaxnet-0.2.2.tar.gz", "has_sig": false, "md5_digest": "28d2193a8645712db97956e4075b5f38", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15712, "upload_time": "2019-09-03T06:24:42", "upload_time_iso_8601": "2019-09-03T06:24:42.558780Z", "url": "https://files.pythonhosted.org/packages/0d/27/77e409e467ac3a055c25f26533e668b0819e65878ca8ffe1bed35ab61df6/jaxnet-0.2.2.tar.gz", "yanked": false}], "0.2.3": [{"comment_text": "", "digests": {"md5": "97697a63d5b29aea58c844f7dc68192f", "sha256": "9a1987db2056a12b3bd1059be4a5b970f4df6e0ba3655a59fa99718e862d0aa7"}, "downloads": -1, "filename": "jaxnet-0.2.3.tar.gz", "has_sig": false, "md5_digest": "97697a63d5b29aea58c844f7dc68192f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15943, "upload_time": "2019-11-09T23:07:50", "upload_time_iso_8601": "2019-11-09T23:07:50.807660Z", "url": "https://files.pythonhosted.org/packages/25/0a/75eb46310292e0e00f194371e0455e399c216e85e197acf93af609d7c2a7/jaxnet-0.2.3.tar.gz", "yanked": false}], "0.2.4": [{"comment_text": "", "digests": {"md5": "ddcb2899586bc3bc0492a279084cb911", "sha256": "c87e554c2c36c03db5ee9d0a3cc4e94a7f094840e781b81f49fdb01eb03a58a1"}, "downloads": -1, "filename": "jaxnet-0.2.4-py3-none-any.whl", "has_sig": false, "md5_digest": "ddcb2899586bc3bc0492a279084cb911", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 19035, "upload_time": "2019-11-22T16:57:15", "upload_time_iso_8601": "2019-11-22T16:57:15.308425Z", "url": "https://files.pythonhosted.org/packages/52/21/e5ffcb6cb24f2223aaa07cf592a7fa9c176f46fe323b0d2e5cec698314cf/jaxnet-0.2.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6e9d7408cf3bc00ec424ee686689cbfb", "sha256": "30042b52ae8543cbbc6e0e452685ea66280ec551f76f85eca824942250413b35"}, "downloads": -1, "filename": "jaxnet-0.2.4.tar.gz", "has_sig": false, "md5_digest": "6e9d7408cf3bc00ec424ee686689cbfb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16803, "upload_time": "2019-11-22T16:57:17", "upload_time_iso_8601": "2019-11-22T16:57:17.204622Z", "url": "https://files.pythonhosted.org/packages/78/2d/4382f57c80b9b726d20c91629ef6d5db675a24d047d4d62fe01cd5b24d6b/jaxnet-0.2.4.tar.gz", "yanked": false}], "0.2.5": [{"comment_text": "", "digests": {"md5": "27d0bdd0f414f9c899e6b3dfd398669d", "sha256": "c0c47376f2529a8e0babba1b381f21d859afa271a31738d74c5020df9d7289ff"}, "downloads": -1, "filename": "jaxnet-0.2.5-py3-none-any.whl", "has_sig": false, "md5_digest": "27d0bdd0f414f9c899e6b3dfd398669d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18318, "upload_time": "2019-11-29T00:07:16", "upload_time_iso_8601": "2019-11-29T00:07:16.112487Z", "url": "https://files.pythonhosted.org/packages/a9/76/ccdd74ec1cb9a94c0f6371ee44be1717c86c1da458b5985ee87eb16133c4/jaxnet-0.2.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8afe420da1e96dc8887071f5cc6e2afb", "sha256": "d8573502c9354799428bbc960fa3fa700283c7047845318725da88157b3aed28"}, "downloads": -1, "filename": "jaxnet-0.2.5.tar.gz", "has_sig": false, "md5_digest": "8afe420da1e96dc8887071f5cc6e2afb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16382, "upload_time": "2019-11-29T00:07:18", "upload_time_iso_8601": "2019-11-29T00:07:18.130493Z", "url": "https://files.pythonhosted.org/packages/f5/59/c6c58a38a483886954cf904527dc216d9711a846c32e77c193255ebe6fc9/jaxnet-0.2.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "27d0bdd0f414f9c899e6b3dfd398669d", "sha256": "c0c47376f2529a8e0babba1b381f21d859afa271a31738d74c5020df9d7289ff"}, "downloads": -1, "filename": "jaxnet-0.2.5-py3-none-any.whl", "has_sig": false, "md5_digest": "27d0bdd0f414f9c899e6b3dfd398669d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18318, "upload_time": "2019-11-29T00:07:16", "upload_time_iso_8601": "2019-11-29T00:07:16.112487Z", "url": "https://files.pythonhosted.org/packages/a9/76/ccdd74ec1cb9a94c0f6371ee44be1717c86c1da458b5985ee87eb16133c4/jaxnet-0.2.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8afe420da1e96dc8887071f5cc6e2afb", "sha256": "d8573502c9354799428bbc960fa3fa700283c7047845318725da88157b3aed28"}, "downloads": -1, "filename": "jaxnet-0.2.5.tar.gz", "has_sig": false, "md5_digest": "8afe420da1e96dc8887071f5cc6e2afb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16382, "upload_time": "2019-11-29T00:07:18", "upload_time_iso_8601": "2019-11-29T00:07:18.130493Z", "url": "https://files.pythonhosted.org/packages/f5/59/c6c58a38a483886954cf904527dc216d9711a846c32e77c193255ebe6fc9/jaxnet-0.2.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:53:02 2020"}