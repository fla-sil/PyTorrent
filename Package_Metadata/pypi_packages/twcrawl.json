{"info": {"author": "Michael Hohl", "author_email": "me@michaelhohl.net", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# twcrawl - Twitter Network Crawler\n\nStarts at any given Twitter accounts (for example `realDonaldTrump` and \n`elonmusk`) and follows their relationships to download the profiles of all \nTwitter users into a local database for later batch processing (like \nanalyzing the social sentiment).\n\n**Note:** This crawler has been designed to prioritize crawling of accounts with\nthe most followers. Depending on your use case you might need to first tweak \nthe parameters a bit.\n\n\n## Setup \n\nAll required dependencies are defined in the `requirements.txt` file. Run\n`pip install -r requirements.txt` to install all of them if needed. Then copy\nthe `config.example.json` into a `config.json` and fill in your Twitter\nAPI credentials.\n\n\n## Usage\n\nSimply run `src/main.py --users users.txt`. `users.txt` should be\na list of twitter handles to use as entry points of the crawling process, one\nper each line of the text file.\n\nThis will launch an endless running process, which crawls as many users as\npossible (and as fast as allowed by the Twitter API limits). You can pause the \nprocess by simply killing it and continue the crawling process by starting it \nagain by executing `src/main.py` again (no more need for the `-i` parameter).\n\nThe crawled database will be stored into `data/twitter.sqlite3` (or anywhere \nelse if you override the default values in your config file).", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/hohl/twcrawl", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "twcrawl", "package_url": "https://pypi.org/project/twcrawl/", "platform": "", "project_url": "https://pypi.org/project/twcrawl/", "project_urls": {"Homepage": "https://github.com/hohl/twcrawl"}, "release_url": "https://pypi.org/project/twcrawl/1.0.3/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Twitter crawler to download a followers graph and statuses into a local database.", "version": "1.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>twcrawl - Twitter Network Crawler</h1>\n<p>Starts at any given Twitter accounts (for example <code>realDonaldTrump</code> and\n<code>elonmusk</code>) and follows their relationships to download the profiles of all\nTwitter users into a local database for later batch processing (like\nanalyzing the social sentiment).</p>\n<p><strong>Note:</strong> This crawler has been designed to prioritize crawling of accounts with\nthe most followers. Depending on your use case you might need to first tweak\nthe parameters a bit.</p>\n<h2>Setup</h2>\n<p>All required dependencies are defined in the <code>requirements.txt</code> file. Run\n<code>pip install -r requirements.txt</code> to install all of them if needed. Then copy\nthe <code>config.example.json</code> into a <code>config.json</code> and fill in your Twitter\nAPI credentials.</p>\n<h2>Usage</h2>\n<p>Simply run <code>src/main.py --users users.txt</code>. <code>users.txt</code> should be\na list of twitter handles to use as entry points of the crawling process, one\nper each line of the text file.</p>\n<p>This will launch an endless running process, which crawls as many users as\npossible (and as fast as allowed by the Twitter API limits). You can pause the\nprocess by simply killing it and continue the crawling process by starting it\nagain by executing <code>src/main.py</code> again (no more need for the <code>-i</code> parameter).</p>\n<p>The crawled database will be stored into <code>data/twitter.sqlite3</code> (or anywhere\nelse if you override the default values in your config file).</p>\n\n          </div>"}, "last_serial": 6075521, "releases": {"1.0.1": [{"comment_text": "", "digests": {"md5": "f525c68d9a3a9d0f34cc67082c2dce61", "sha256": "2d001b9976339b3734be4fd9dba11abed2dc5554830d242d1d58cefbaf998c52"}, "downloads": -1, "filename": "twcrawl-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "f525c68d9a3a9d0f34cc67082c2dce61", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 8933, "upload_time": "2019-10-08T11:18:11", "upload_time_iso_8601": "2019-10-08T11:18:11.581177Z", "url": "https://files.pythonhosted.org/packages/b8/3c/ff76b93798764ffd40e7033af9ce2bbb318450ffa83ff78ffa5f92fd3bca/twcrawl-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a5cfd3cf327f1521fd11770414b7a953", "sha256": "203b6dcdab8586c891365b76c6b1af9401a533796d52576b159e9426e9d00cba"}, "downloads": -1, "filename": "twcrawl-1.0.1.tar.gz", "has_sig": false, "md5_digest": "a5cfd3cf327f1521fd11770414b7a953", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6716, "upload_time": "2019-10-08T11:18:12", "upload_time_iso_8601": "2019-10-08T11:18:12.913412Z", "url": "https://files.pythonhosted.org/packages/16/42/81fdfaf0efa9135d31dbfe5c5fd2c97d6cc9ad4e2167cfcb96b0a36909db/twcrawl-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "c1b643765d6cea9af7f961e5493a3bbb", "sha256": "997c9e5c8e5f15a3911785c46c0e92de5460974b37b1f0b20d220718703bbd7c"}, "downloads": -1, "filename": "twcrawl-1.0.2.tar.gz", "has_sig": false, "md5_digest": "c1b643765d6cea9af7f961e5493a3bbb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6743, "upload_time": "2019-11-04T13:24:10", "upload_time_iso_8601": "2019-11-04T13:24:10.061091Z", "url": "https://files.pythonhosted.org/packages/ed/49/e3dba06ff1d3b61bda2ef50cbc2a7f3b58f676809d7977f54162588bcfd9/twcrawl-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "d39ea22b04207c085e2a05447a42974c", "sha256": "6db6d34e1271d6c6001af208fbc59f63e394a1a59d702999a12a03c830337df1"}, "downloads": -1, "filename": "twcrawl-1.0.3.tar.gz", "has_sig": false, "md5_digest": "d39ea22b04207c085e2a05447a42974c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6833, "upload_time": "2019-11-04T13:34:55", "upload_time_iso_8601": "2019-11-04T13:34:55.029824Z", "url": "https://files.pythonhosted.org/packages/38/0e/381e5628b1f107722d3bdabd4da1dde95c433ca5831735ceb77540dccd9b/twcrawl-1.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d39ea22b04207c085e2a05447a42974c", "sha256": "6db6d34e1271d6c6001af208fbc59f63e394a1a59d702999a12a03c830337df1"}, "downloads": -1, "filename": "twcrawl-1.0.3.tar.gz", "has_sig": false, "md5_digest": "d39ea22b04207c085e2a05447a42974c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 6833, "upload_time": "2019-11-04T13:34:55", "upload_time_iso_8601": "2019-11-04T13:34:55.029824Z", "url": "https://files.pythonhosted.org/packages/38/0e/381e5628b1f107722d3bdabd4da1dde95c433ca5831735ceb77540dccd9b/twcrawl-1.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:44:12 2020"}