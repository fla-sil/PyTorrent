{"info": {"author": "Johannes Filter", "author_email": "hi@jfilter.de", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: BSD License", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Information Analysis"], "description": "# hyperhyper [![Build Status](https://travis-ci.com/jfilter/hyperhyper.svg?branch=master)](https://travis-ci.com/jfilter/hyperhyper) [![PyPI](https://img.shields.io/pypi/v/hyperhyper.svg)](https://pypi.org/project/hyperhyper/) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/hyperhyper.svg)](https://pypi.org/project/hyperhyper/)\n\nPython Library to Construct Word Embeddings for Small Data. Still work in progress.\n\nBuilding upon the work by Omer Levy et al. for [Hyperwords](https://bitbucket.org/omerlevy/hyperwords).\n\n## Why?\n\nNowadays, [word embeddings](https://en.wikipedia.org/wiki/Word_embedding) are mostly associated with [Word2vec](https://en.wikipedia.org/wiki/Word2vec) or [fastText](https://en.wikipedia.org/wiki/FastText). Those approaches focus on scenarios, where an abundance of data is available. But to make them work, you also need a lot of data. This is not always the case. There exists alternative methods based on counting word pairs and some math magic around matrix operations. They need less data. This Python library implements the approaches (somewhat) efficiently (but there is there is still room for improvement.)\n\n`hyperhyper` is based on [a paper](https://aclweb.org/anthology/papers/Q/Q15/Q15-1016/) from 2015. The authors, Omer Levy et al., published their research code as [Hyperwods](https://bitbucket.org/omerlevy/hyperwords).\nI [tried](https://github.com/jfilter/hyperwords) to the port their original software to Python 3 but I ended up re-writing large parts of it. So this library was born.\n\nLimitations: With `hyperhyper` you will run into (memory) problems, if you need large vocabularies (set of possible words). It's fine if you have a vocabulary up until 50k. Word2vec and fastText especially solve this [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).\n\n## Installation\n\n```bash\npip install hyperhyper\n```\n\nIf you have an Intel CPU, it's recommended to use the MKL library for `numpy`. It can be challening to correctly set up MKL. A package by intel may help you.\n\n```bash\nconda install -c intel intelpython3_core\npip install hyperhyper\n```\n\nVerify wheter `mkl_info` is present:\n\n```python\n>>> import numpy\n>>> numpy.__config__.show()\n```\n\nDisable internal multithreading ability of MKL or OpenBLAS.\n\n```bash\nexport OPENBLAS_NUM_THREADS=1\nexport MKL_NUM_THREADS=1\n```\n\nThis speeds up computation because we are using multiprocessing on an outer loop.\n\n## Usage\n\n```python\nimport hyperhyper as hy\n\ncorpus = hy.Corpus.from_file('news.2010.en.shuffled')\nbunch = hy.Bunch(\"news_bunch\", corpus)\nvectors, results = bunch.svd(keyed_vectors=True)\n\nresults['results'][1]\n>>> {'name': 'en_ws353',\n 'score': 0.6510955349164682,\n 'oov': 0.014164305949008499,\n 'fullscore': 0.641873218557878}\n\nvectors.most_similar('berlin')\n>>> [('vienna', 0.6323208808898926),\n ('frankfurt', 0.5965485572814941),\n ('munich', 0.5737138986587524),\n ('amsterdam', 0.5511572360992432),\n ('stockholm', 0.5423270463943481)]\n```\n\nSee [examples](./examples) for more.\n\nThe general concepts:\n\n-   Preprocess data once and save them in a `bunch`\n-   Cache all results and also record their perfomance on test data\n-   Make it easy to fine-tune paramters for you data\n\nMore documenation may be forthcoming. Until then you have to read the [source code](./hyperhyper).\n\n## Scientific Background\n\nThis software is based on the following papers:\n\n-   Improving Distributional Similarity with Lessons Learned from Word Embeddings, Omer Levy, Yoav Goldberg, Ido Dagan, TACL 2015. [Paper](https://aclweb.org/anthology/papers/Q/Q15/Q15-1016/) [Code](https://bitbucket.org/omerlevy/hyperwords)\n    > Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.\n-   The Influence of Down-Sampling Strategies on SVD Word Embedding Stability, Johannes Hellrich, Bernd Kampe, Udo Hahn, NAACL 2019. [Paper](https://aclweb.org/anthology/papers/W/W19/W19-2003/) [Code](https://github.com/hellrich/hyperwords) [Code](https://github.com/hellrich/embedding_downsampling_comparison)\n    > The stability of word embedding algorithms, i.e., the consistency of the word representations they reveal when trained repeatedly on the same data set, has recently raised concerns. We here compare word embedding algorithms on three corpora of different sizes, and evaluate both their stability and accuracy. We find strong evidence that down-sampling strategies (used as part of their training procedures) are particularly influential for the stability of SVD-PPMI-type embeddings. This finding seems to explain diverging reports on their stability and lead us to a simple modification which provides superior stability as well as accuracy on par with skip-gram embedding\n\n## Development\n\n1. Install [pipenv](https://docs.pipenv.org/en/latest/).\n2. `git clone https://github.com/jfilter/hyperhyper && cd hyperhyper && pipenv install && pipenv shell`\n3. `python -m spacy download en_core_web_sm`\n4. `pytest tests`\n\n## Contributing\n\nIf you have a **question**, found a **bug** or want to propose a new **feature**, have a look at the [issues page](https://github.com/jfilter/hyperhyper/issues).\n\n**Pull requests** are especially welcomed when they fix bugs or improve the code quality.\n\n## Future Work / TODO\n\n-   evaluation for analogies\n-   replace pipenv if they still don't ship any newer release\n-   implement counting in a more efficient programming language, e.g. Cython.\n\n## Why is this library named `hyperhyper`?\n\n[![Scooter \u2013 Hyper Hyper (Song)](https://img.youtube.com/vi/7Twnmhe948A/0.jpg)](https://www.youtube.com/watch?v=7Twnmhe948A \"Scooter \u2013 Hyper Hyper\")\n\n## License\n\nBSD-2-Clause.\n\n## Sponsoring\n\nThis work was created as part of a [project](https://github.com/jfilter/ptf) that was funded by the German [Federal Ministry of Education and Research](https://www.bmbf.de/en/index.html).\n\n<img src=\"./bmbf_funded.svg\">\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/jfilter/hyperhyper", "keywords": "", "license": "BSD", "maintainer": "", "maintainer_email": "", "name": "hyperhyper", "package_url": "https://pypi.org/project/hyperhyper/", "platform": "", "project_url": "https://pypi.org/project/hyperhyper/", "project_urls": {"Homepage": "https://github.com/jfilter/hyperhyper"}, "release_url": "https://pypi.org/project/hyperhyper/0.1.1/", "requires_dist": ["dataset (==1.*)", "tqdm", "gensim (==3.*)", "importlib-resources ; python_version < \"3.7\"", "spacy (==2.*) ; extra == 'full'", "scikit-learn ; extra == 'full'"], "requires_python": ">=3.6", "summary": "Python Library to Construct Word Embeddings for Small Data", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>hyperhyper <a href=\"https://travis-ci.com/jfilter/hyperhyper\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9f0cdcef6066d5a84dd93ba05582cadc11557e77/68747470733a2f2f7472617669732d63692e636f6d2f6a66696c7465722f687970657268797065722e7376673f6272616e63683d6d6173746572\"></a> <a href=\"https://pypi.org/project/hyperhyper/\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1d7639750753ad8c2e969aabc05e5fe857513174/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f687970657268797065722e737667\"></a> <a href=\"https://pypi.org/project/hyperhyper/\" rel=\"nofollow\"><img alt=\"PyPI - Python Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/952998565afe80d6ff2f49f7a04e5cfe4d0cddb2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f687970657268797065722e737667\"></a></h1>\n<p>Python Library to Construct Word Embeddings for Small Data. Still work in progress.</p>\n<p>Building upon the work by Omer Levy et al. for <a href=\"https://bitbucket.org/omerlevy/hyperwords\" rel=\"nofollow\">Hyperwords</a>.</p>\n<h2>Why?</h2>\n<p>Nowadays, <a href=\"https://en.wikipedia.org/wiki/Word_embedding\" rel=\"nofollow\">word embeddings</a> are mostly associated with <a href=\"https://en.wikipedia.org/wiki/Word2vec\" rel=\"nofollow\">Word2vec</a> or <a href=\"https://en.wikipedia.org/wiki/FastText\" rel=\"nofollow\">fastText</a>. Those approaches focus on scenarios, where an abundance of data is available. But to make them work, you also need a lot of data. This is not always the case. There exists alternative methods based on counting word pairs and some math magic around matrix operations. They need less data. This Python library implements the approaches (somewhat) efficiently (but there is there is still room for improvement.)</p>\n<p><code>hyperhyper</code> is based on <a href=\"https://aclweb.org/anthology/papers/Q/Q15/Q15-1016/\" rel=\"nofollow\">a paper</a> from 2015. The authors, Omer Levy et al., published their research code as <a href=\"https://bitbucket.org/omerlevy/hyperwords\" rel=\"nofollow\">Hyperwods</a>.\nI <a href=\"https://github.com/jfilter/hyperwords\" rel=\"nofollow\">tried</a> to the port their original software to Python 3 but I ended up re-writing large parts of it. So this library was born.</p>\n<p>Limitations: With <code>hyperhyper</code> you will run into (memory) problems, if you need large vocabularies (set of possible words). It's fine if you have a vocabulary up until 50k. Word2vec and fastText especially solve this <a href=\"https://en.wikipedia.org/wiki/Curse_of_dimensionality\" rel=\"nofollow\">curse of dimensionality</a>.</p>\n<h2>Installation</h2>\n<pre>pip install hyperhyper\n</pre>\n<p>If you have an Intel CPU, it's recommended to use the MKL library for <code>numpy</code>. It can be challening to correctly set up MKL. A package by intel may help you.</p>\n<pre>conda install -c intel intelpython3_core\npip install hyperhyper\n</pre>\n<p>Verify wheter <code>mkl_info</code> is present:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">numpy</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">__config__</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</pre>\n<p>Disable internal multithreading ability of MKL or OpenBLAS.</p>\n<pre><span class=\"nb\">export</span> <span class=\"nv\">OPENBLAS_NUM_THREADS</span><span class=\"o\">=</span><span class=\"m\">1</span>\n<span class=\"nb\">export</span> <span class=\"nv\">MKL_NUM_THREADS</span><span class=\"o\">=</span><span class=\"m\">1</span>\n</pre>\n<p>This speeds up computation because we are using multiprocessing on an outer loop.</p>\n<h2>Usage</h2>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">hyperhyper</span> <span class=\"k\">as</span> <span class=\"nn\">hy</span>\n\n<span class=\"n\">corpus</span> <span class=\"o\">=</span> <span class=\"n\">hy</span><span class=\"o\">.</span><span class=\"n\">Corpus</span><span class=\"o\">.</span><span class=\"n\">from_file</span><span class=\"p\">(</span><span class=\"s1\">'news.2010.en.shuffled'</span><span class=\"p\">)</span>\n<span class=\"n\">bunch</span> <span class=\"o\">=</span> <span class=\"n\">hy</span><span class=\"o\">.</span><span class=\"n\">Bunch</span><span class=\"p\">(</span><span class=\"s2\">\"news_bunch\"</span><span class=\"p\">,</span> <span class=\"n\">corpus</span><span class=\"p\">)</span>\n<span class=\"n\">vectors</span><span class=\"p\">,</span> <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">bunch</span><span class=\"o\">.</span><span class=\"n\">svd</span><span class=\"p\">(</span><span class=\"n\">keyed_vectors</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"n\">results</span><span class=\"p\">[</span><span class=\"s1\">'results'</span><span class=\"p\">][</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"p\">{</span><span class=\"s1\">'name'</span><span class=\"p\">:</span> <span class=\"s1\">'en_ws353'</span><span class=\"p\">,</span>\n <span class=\"s1\">'score'</span><span class=\"p\">:</span> <span class=\"mf\">0.6510955349164682</span><span class=\"p\">,</span>\n <span class=\"s1\">'oov'</span><span class=\"p\">:</span> <span class=\"mf\">0.014164305949008499</span><span class=\"p\">,</span>\n <span class=\"s1\">'fullscore'</span><span class=\"p\">:</span> <span class=\"mf\">0.641873218557878</span><span class=\"p\">}</span>\n\n<span class=\"n\">vectors</span><span class=\"o\">.</span><span class=\"n\">most_similar</span><span class=\"p\">(</span><span class=\"s1\">'berlin'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"p\">[(</span><span class=\"s1\">'vienna'</span><span class=\"p\">,</span> <span class=\"mf\">0.6323208808898926</span><span class=\"p\">),</span>\n <span class=\"p\">(</span><span class=\"s1\">'frankfurt'</span><span class=\"p\">,</span> <span class=\"mf\">0.5965485572814941</span><span class=\"p\">),</span>\n <span class=\"p\">(</span><span class=\"s1\">'munich'</span><span class=\"p\">,</span> <span class=\"mf\">0.5737138986587524</span><span class=\"p\">),</span>\n <span class=\"p\">(</span><span class=\"s1\">'amsterdam'</span><span class=\"p\">,</span> <span class=\"mf\">0.5511572360992432</span><span class=\"p\">),</span>\n <span class=\"p\">(</span><span class=\"s1\">'stockholm'</span><span class=\"p\">,</span> <span class=\"mf\">0.5423270463943481</span><span class=\"p\">)]</span>\n</pre>\n<p>See <a href=\"./examples\" rel=\"nofollow\">examples</a> for more.</p>\n<p>The general concepts:</p>\n<ul>\n<li>Preprocess data once and save them in a <code>bunch</code></li>\n<li>Cache all results and also record their perfomance on test data</li>\n<li>Make it easy to fine-tune paramters for you data</li>\n</ul>\n<p>More documenation may be forthcoming. Until then you have to read the <a href=\"./hyperhyper\" rel=\"nofollow\">source code</a>.</p>\n<h2>Scientific Background</h2>\n<p>This software is based on the following papers:</p>\n<ul>\n<li>Improving Distributional Similarity with Lessons Learned from Word Embeddings, Omer Levy, Yoav Goldberg, Ido Dagan, TACL 2015. <a href=\"https://aclweb.org/anthology/papers/Q/Q15/Q15-1016/\" rel=\"nofollow\">Paper</a> <a href=\"https://bitbucket.org/omerlevy/hyperwords\" rel=\"nofollow\">Code</a>\n<blockquote>\n<p>Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.</p>\n</blockquote>\n</li>\n<li>The Influence of Down-Sampling Strategies on SVD Word Embedding Stability, Johannes Hellrich, Bernd Kampe, Udo Hahn, NAACL 2019. <a href=\"https://aclweb.org/anthology/papers/W/W19/W19-2003/\" rel=\"nofollow\">Paper</a> <a href=\"https://github.com/hellrich/hyperwords\" rel=\"nofollow\">Code</a> <a href=\"https://github.com/hellrich/embedding_downsampling_comparison\" rel=\"nofollow\">Code</a>\n<blockquote>\n<p>The stability of word embedding algorithms, i.e., the consistency of the word representations they reveal when trained repeatedly on the same data set, has recently raised concerns. We here compare word embedding algorithms on three corpora of different sizes, and evaluate both their stability and accuracy. We find strong evidence that down-sampling strategies (used as part of their training procedures) are particularly influential for the stability of SVD-PPMI-type embeddings. This finding seems to explain diverging reports on their stability and lead us to a simple modification which provides superior stability as well as accuracy on par with skip-gram embedding</p>\n</blockquote>\n</li>\n</ul>\n<h2>Development</h2>\n<ol>\n<li>Install <a href=\"https://docs.pipenv.org/en/latest/\" rel=\"nofollow\">pipenv</a>.</li>\n<li><code>git clone https://github.com/jfilter/hyperhyper &amp;&amp; cd hyperhyper &amp;&amp; pipenv install &amp;&amp; pipenv shell</code></li>\n<li><code>python -m spacy download en_core_web_sm</code></li>\n<li><code>pytest tests</code></li>\n</ol>\n<h2>Contributing</h2>\n<p>If you have a <strong>question</strong>, found a <strong>bug</strong> or want to propose a new <strong>feature</strong>, have a look at the <a href=\"https://github.com/jfilter/hyperhyper/issues\" rel=\"nofollow\">issues page</a>.</p>\n<p><strong>Pull requests</strong> are especially welcomed when they fix bugs or improve the code quality.</p>\n<h2>Future Work / TODO</h2>\n<ul>\n<li>evaluation for analogies</li>\n<li>replace pipenv if they still don't ship any newer release</li>\n<li>implement counting in a more efficient programming language, e.g. Cython.</li>\n</ul>\n<h2>Why is this library named <code>hyperhyper</code>?</h2>\n<p><a href=\"https://www.youtube.com/watch?v=7Twnmhe948A\" rel=\"nofollow\" title=\"Scooter \u2013 Hyper Hyper\"><img alt=\"Scooter \u2013 Hyper Hyper (Song)\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a83541ba643b852c24380ada1a542465e25bfc71/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f3754776e6d6865393438412f302e6a7067\"></a></p>\n<h2>License</h2>\n<p>BSD-2-Clause.</p>\n<h2>Sponsoring</h2>\n<p>This work was created as part of a <a href=\"https://github.com/jfilter/ptf\" rel=\"nofollow\">project</a> that was funded by the German <a href=\"https://www.bmbf.de/en/index.html\" rel=\"nofollow\">Federal Ministry of Education and Research</a>.</p>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/cb4a0649b76fc8eaf250ec7fd118799949ef8d13/2e2f626d62665f66756e6465642e737667\">\n\n          </div>"}, "last_serial": 6769504, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "f7d48d086ebc5c7c10916c56fb885317", "sha256": "08a9a054b97c949ccc08a51aa340715f53518f1f50b614e9c5da4fb5be8843b0"}, "downloads": -1, "filename": "hyperhyper-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f7d48d086ebc5c7c10916c56fb885317", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 21873, "upload_time": "2020-02-29T22:53:31", "upload_time_iso_8601": "2020-02-29T22:53:31.962783Z", "url": "https://files.pythonhosted.org/packages/0a/39/b2700df4cbf7959ef02d2cc0156c9cc8af77033189355d87391937ed5e7e/hyperhyper-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f3ffbe2369a56378dc2362b42930ab18", "sha256": "edd15d4a50e688bd6de6be834bedbbb8e57a2669fbf836f63fc8f7436fda389c"}, "downloads": -1, "filename": "hyperhyper-0.1.0.tar.gz", "has_sig": false, "md5_digest": "f3ffbe2369a56378dc2362b42930ab18", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 20548, "upload_time": "2020-02-29T22:53:34", "upload_time_iso_8601": "2020-02-29T22:53:34.236110Z", "url": "https://files.pythonhosted.org/packages/4b/7e/a91a876fa3d211cf143d13305f6af84133f8be14293b947a5b5cc99a2b91/hyperhyper-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "9b4959d8c684a66c92dcc1108a6806b6", "sha256": "7a59c503bc923ebbbc85e08e85d44bce9be8a8d990a38d9b9bd422411cd2b7d6"}, "downloads": -1, "filename": "hyperhyper-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "9b4959d8c684a66c92dcc1108a6806b6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 312319, "upload_time": "2020-03-07T21:16:21", "upload_time_iso_8601": "2020-03-07T21:16:21.022113Z", "url": "https://files.pythonhosted.org/packages/28/6a/803aa3ef2491849c776a45283a3bedfcad289cde7b5820e01bcad278e0dd/hyperhyper-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "befd9c621695559d79b1d0f618dcbbc4", "sha256": "2b543a40dd8261d04d62499ccfcb6cc7b258f8ab641874ca8d4fc27b5ae56a03"}, "downloads": -1, "filename": "hyperhyper-0.1.1.tar.gz", "has_sig": false, "md5_digest": "befd9c621695559d79b1d0f618dcbbc4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 305959, "upload_time": "2020-03-07T21:16:23", "upload_time_iso_8601": "2020-03-07T21:16:23.714781Z", "url": "https://files.pythonhosted.org/packages/83/e1/18eb986a36d09846ebd74b955557ce02907ff6e18c8f5d3c06d876754182/hyperhyper-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "9b4959d8c684a66c92dcc1108a6806b6", "sha256": "7a59c503bc923ebbbc85e08e85d44bce9be8a8d990a38d9b9bd422411cd2b7d6"}, "downloads": -1, "filename": "hyperhyper-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "9b4959d8c684a66c92dcc1108a6806b6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 312319, "upload_time": "2020-03-07T21:16:21", "upload_time_iso_8601": "2020-03-07T21:16:21.022113Z", "url": "https://files.pythonhosted.org/packages/28/6a/803aa3ef2491849c776a45283a3bedfcad289cde7b5820e01bcad278e0dd/hyperhyper-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "befd9c621695559d79b1d0f618dcbbc4", "sha256": "2b543a40dd8261d04d62499ccfcb6cc7b258f8ab641874ca8d4fc27b5ae56a03"}, "downloads": -1, "filename": "hyperhyper-0.1.1.tar.gz", "has_sig": false, "md5_digest": "befd9c621695559d79b1d0f618dcbbc4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 305959, "upload_time": "2020-03-07T21:16:23", "upload_time_iso_8601": "2020-03-07T21:16:23.714781Z", "url": "https://files.pythonhosted.org/packages/83/e1/18eb986a36d09846ebd74b955557ce02907ff6e18c8f5d3c06d876754182/hyperhyper-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:49:33 2020"}