{"info": {"author": "Lasagne contributors", "author_email": "lasagne-users@googlegroups.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": ".. image:: http://img.shields.io/badge/docs-latest-brightgreen.svg\n    :target: http://lasagne.readthedocs.org/en/latest/\n\n.. image:: https://travis-ci.org/Lasagne/Lasagne.svg?branch=master\n    :target: https://travis-ci.org/Lasagne/Lasagne\n\n.. image:: https://img.shields.io/coveralls/Lasagne/Lasagne.svg\n    :target: https://coveralls.io/r/Lasagne/Lasagne\n\n.. image:: https://img.shields.io/badge/license-MIT-blue.svg\n    :target: https://github.com/Lasagne/Lasagne/blob/master/LICENSE\n\n.. image:: https://zenodo.org/badge/16974/Lasagne/Lasagne.svg\n   :target: https://zenodo.org/badge/latestdoi/16974/Lasagne/Lasagne\n\nLasagne\n=======\n\nLasagne is a lightweight library to build and train neural networks in Theano.\nIts main features are:\n\n* Supports feed-forward networks such as Convolutional Neural Networks (CNNs),\n  recurrent networks including Long Short-Term Memory (LSTM), and any\n  combination thereof\n* Allows architectures of multiple inputs and multiple outputs, including\n  auxiliary classifiers\n* Many optimization methods including Nesterov momentum, RMSprop and ADAM\n* Freely definable cost function and no need to derive gradients due to\n  Theano's symbolic differentiation\n* Transparent support of CPUs and GPUs due to Theano's expression compiler\n\nIts design is governed by `six principles\n<http://lasagne.readthedocs.org/en/latest/user/development.html#philosophy>`_:\n\n* Simplicity: Be easy to use, easy to understand and easy to extend, to\n  facilitate use in research\n* Transparency: Do not hide Theano behind abstractions, directly process and\n  return Theano expressions or Python / numpy data types\n* Modularity: Allow all parts (layers, regularizers, optimizers, ...) to be\n  used independently of Lasagne\n* Pragmatism: Make common use cases easy, do not overrate uncommon cases\n* Restraint: Do not obstruct users with features they decide not to use\n* Focus: \"Do one thing and do it well\"\n\n\nInstallation\n------------\n\nIn short, you can install a known compatible version of Theano and the latest\nLasagne development version via:\n\n.. code-block:: bash\n\n  pip install -r https://raw.githubusercontent.com/Lasagne/Lasagne/master/requirements.txt\n  pip install https://github.com/Lasagne/Lasagne/archive/master.zip\n\nFor more details and alternatives, please see the `Installation instructions\n<http://lasagne.readthedocs.org/en/latest/user/installation.html>`_.\n\n\nDocumentation\n-------------\n\nDocumentation is available online: http://lasagne.readthedocs.org/\n\nFor support, please refer to the `lasagne-users mailing list\n<https://groups.google.com/forum/#!forum/lasagne-users>`_.\n\n\nExample\n-------\n\n.. code-block:: python\n\n  import lasagne\n  import theano\n  import theano.tensor as T\n\n  # create Theano variables for input and target minibatch\n  input_var = T.tensor4('X')\n  target_var = T.ivector('y')\n\n  # create a small convolutional neural network\n  from lasagne.nonlinearities import leaky_rectify, softmax\n  network = lasagne.layers.InputLayer((None, 3, 32, 32), input_var)\n  network = lasagne.layers.Conv2DLayer(network, 64, (3, 3),\n                                       nonlinearity=leaky_rectify)\n  network = lasagne.layers.Conv2DLayer(network, 32, (3, 3),\n                                       nonlinearity=leaky_rectify)\n  network = lasagne.layers.Pool2DLayer(network, (3, 3), stride=2, mode='max')\n  network = lasagne.layers.DenseLayer(lasagne.layers.dropout(network, 0.5),\n                                      128, nonlinearity=leaky_rectify,\n                                      W=lasagne.init.Orthogonal())\n  network = lasagne.layers.DenseLayer(lasagne.layers.dropout(network, 0.5),\n                                      10, nonlinearity=softmax)\n\n  # create loss function\n  prediction = lasagne.layers.get_output(network)\n  loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n  loss = loss.mean() + 1e-4 * lasagne.regularization.regularize_network_params(\n          network, lasagne.regularization.l2)\n\n  # create parameter update expressions\n  params = lasagne.layers.get_all_params(network, trainable=True)\n  updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01,\n                                              momentum=0.9)\n\n  # compile training function that updates parameters and returns training loss\n  train_fn = theano.function([input_var, target_var], loss, updates=updates)\n\n  # train network (assuming you've got some training data in numpy arrays)\n  for epoch in range(100):\n      loss = 0\n      for input_batch, target_batch in training_data:\n          loss += train_fn(input_batch, target_batch)\n      print(\"Epoch %d: Loss %g\" % (epoch + 1, loss / len(training_data)))\n\n  # use trained network for predictions\n  test_prediction = lasagne.layers.get_output(network, deterministic=True)\n  predict_fn = theano.function([input_var], T.argmax(test_prediction, axis=1))\n  print(\"Predicted class for first test input: %r\" % predict_fn(test_data[0]))\n\nFor a fully-functional example, see `examples/mnist.py <examples/mnist.py>`_,\nand check the `Tutorial\n<http://lasagne.readthedocs.org/en/latest/user/tutorial.html>`_ for in-depth\nexplanations of the same. More examples, code snippets and reproductions of\nrecent research papers are maintained in the separate `Lasagne Recipes\n<https://github.com/Lasagne/Recipes>`_ repository.\n\n\nDevelopment\n-----------\n\nLasagne is a work in progress, input is welcome.\n\nPlease see the `Contribution instructions\n<http://lasagne.readthedocs.org/en/latest/user/development.html>`_ for details\non how you can contribute!\n\n\nChangelog\n---------\n\n0.1 (2015-08-13)\n~~~~~~~~~~~~~~~~\n\nFirst release.\n\n* core contributors, in alphabetical order:\n\n  * Eric Battenberg (@ebattenberg)\n  * Sander Dieleman (@benanne)\n  * Daniel Nouri (@dnouri)\n  * Eben Olson (@ebenolson)\n  * A\u00e4ron van den Oord (@avdnoord)\n  * Colin Raffel (@craffel)\n  * Jan Schl\u00fcter (@f0k)\n  * S\u00f8ren Kaae S\u00f8nderby (@skaae)\n\n* extra contributors, in chronological order:\n\n  * Daniel Maturana (@dimatura): documentation, cuDNN layers, LRN\n  * Jonas Degrave (@317070): get_all_param_values() fix\n  * Jack Kelly (@JackKelly): help with recurrent layers\n  * G\u00e1bor Tak\u00e1cs (@takacsg84): support broadcastable parameters in lasagne.updates\n  * Diogo Moitinho de Almeida (@diogo149): MNIST example fixes\n  * Brian McFee (@bmcfee): MaxPool2DLayer fix\n  * Martin Thoma (@MartinThoma): documentation\n  * Jeffrey De Fauw (@JeffreyDF): documentation, ADAM fix\n  * Michael Heilman (@mheilman): NonlinearityLayer, lasagne.random\n  * Gregory Sanders (@instagibbs): documentation fix\n  * Jon Crall (@erotemic): check for non-positive input shapes\n  * Hendrik Weideman (@hjweide): set_all_param_values() test, MaxPool2DCCLayer fix\n  * Kashif Rasul (@kashif): ADAM simplification\n  * Peter de Rivaz (@peterderivaz): documentation fix", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Lasagne/Lasagne", "keywords": "", "license": "MIT", "maintainer": null, "maintainer_email": null, "name": "Lasagne", "package_url": "https://pypi.org/project/Lasagne/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/Lasagne/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/Lasagne/Lasagne"}, "release_url": "https://pypi.org/project/Lasagne/0.1/", "requires_dist": null, "requires_python": null, "summary": "A lightweight library to build and train neural networks in Theano", "version": "0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"http://lasagne.readthedocs.org/en/latest/\" rel=\"nofollow\"><img alt=\"http://img.shields.io/badge/docs-latest-brightgreen.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c68bd8b1f88e696c00a8f9a7fecf70c78f99244c/687474703a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d627269676874677265656e2e737667\"></a>\n<a href=\"https://travis-ci.org/Lasagne/Lasagne\" rel=\"nofollow\"><img alt=\"https://travis-ci.org/Lasagne/Lasagne.svg?branch=master\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4139c99ed3ebfae95516ac81dabbe728db32ace7/68747470733a2f2f7472617669732d63692e6f72672f4c617361676e652f4c617361676e652e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://coveralls.io/r/Lasagne/Lasagne\" rel=\"nofollow\"><img alt=\"https://img.shields.io/coveralls/Lasagne/Lasagne.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/013dfece9c564361a31ca83ef271b377108d327d/68747470733a2f2f696d672e736869656c64732e696f2f636f766572616c6c732f4c617361676e652f4c617361676e652e737667\"></a>\n<a href=\"https://github.com/Lasagne/Lasagne/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"https://img.shields.io/badge/license-MIT-blue.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c88fab50b4a1dc0cd91faeb7ba5654d56e380260/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667\"></a>\n<a href=\"https://zenodo.org/badge/latestdoi/16974/Lasagne/Lasagne\" rel=\"nofollow\"><img alt=\"https://zenodo.org/badge/16974/Lasagne/Lasagne.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/81b78072b2bc0fff9abd0623556867e9353f9fbf/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f31363937342f4c617361676e652f4c617361676e652e737667\"></a>\n<div id=\"lasagne\">\n<h2>Lasagne</h2>\n<p>Lasagne is a lightweight library to build and train neural networks in Theano.\nIts main features are:</p>\n<ul>\n<li>Supports feed-forward networks such as Convolutional Neural Networks (CNNs),\nrecurrent networks including Long Short-Term Memory (LSTM), and any\ncombination thereof</li>\n<li>Allows architectures of multiple inputs and multiple outputs, including\nauxiliary classifiers</li>\n<li>Many optimization methods including Nesterov momentum, RMSprop and ADAM</li>\n<li>Freely definable cost function and no need to derive gradients due to\nTheano\u2019s symbolic differentiation</li>\n<li>Transparent support of CPUs and GPUs due to Theano\u2019s expression compiler</li>\n</ul>\n<p>Its design is governed by <a href=\"http://lasagne.readthedocs.org/en/latest/user/development.html#philosophy\" rel=\"nofollow\">six principles</a>:</p>\n<ul>\n<li>Simplicity: Be easy to use, easy to understand and easy to extend, to\nfacilitate use in research</li>\n<li>Transparency: Do not hide Theano behind abstractions, directly process and\nreturn Theano expressions or Python / numpy data types</li>\n<li>Modularity: Allow all parts (layers, regularizers, optimizers, \u2026) to be\nused independently of Lasagne</li>\n<li>Pragmatism: Make common use cases easy, do not overrate uncommon cases</li>\n<li>Restraint: Do not obstruct users with features they decide not to use</li>\n<li>Focus: \u201cDo one thing and do it well\u201d</li>\n</ul>\n<div id=\"installation\">\n<h3>Installation</h3>\n<p>In short, you can install a known compatible version of Theano and the latest\nLasagne development version via:</p>\n<pre>pip install -r https://raw.githubusercontent.com/Lasagne/Lasagne/master/requirements.txt\npip install https://github.com/Lasagne/Lasagne/archive/master.zip\n</pre>\n<p>For more details and alternatives, please see the <a href=\"http://lasagne.readthedocs.org/en/latest/user/installation.html\" rel=\"nofollow\">Installation instructions</a>.</p>\n</div>\n<div id=\"documentation\">\n<h3>Documentation</h3>\n<p>Documentation is available online: <a href=\"http://lasagne.readthedocs.org/\" rel=\"nofollow\">http://lasagne.readthedocs.org/</a></p>\n<p>For support, please refer to the <a href=\"https://groups.google.com/forum/#!forum/lasagne-users\" rel=\"nofollow\">lasagne-users mailing list</a>.</p>\n</div>\n<div id=\"example\">\n<h3>Example</h3>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">lasagne</span>\n<span class=\"kn\">import</span> <span class=\"nn\">theano</span>\n<span class=\"kn\">import</span> <span class=\"nn\">theano.tensor</span> <span class=\"k\">as</span> <span class=\"nn\">T</span>\n\n<span class=\"c1\"># create Theano variables for input and target minibatch</span>\n<span class=\"n\">input_var</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">tensor4</span><span class=\"p\">(</span><span class=\"s1\">'X'</span><span class=\"p\">)</span>\n<span class=\"n\">target_var</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">ivector</span><span class=\"p\">(</span><span class=\"s1\">'y'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># create a small convolutional neural network</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lasagne.nonlinearities</span> <span class=\"kn\">import</span> <span class=\"n\">leaky_rectify</span><span class=\"p\">,</span> <span class=\"n\">softmax</span>\n<span class=\"n\">network</span> <span class=\"o\">=</span> <span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">InputLayer</span><span class=\"p\">((</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">),</span> <span class=\"n\">input_var</span><span class=\"p\">)</span>\n<span class=\"n\">network</span> <span class=\"o\">=</span> <span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Conv2DLayer</span><span class=\"p\">(</span><span class=\"n\">network</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span>\n                                     <span class=\"n\">nonlinearity</span><span class=\"o\">=</span><span class=\"n\">leaky_rectify</span><span class=\"p\">)</span>\n<span class=\"n\">network</span> <span class=\"o\">=</span> <span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Conv2DLayer</span><span class=\"p\">(</span><span class=\"n\">network</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span>\n                                     <span class=\"n\">nonlinearity</span><span class=\"o\">=</span><span class=\"n\">leaky_rectify</span><span class=\"p\">)</span>\n<span class=\"n\">network</span> <span class=\"o\">=</span> <span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Pool2DLayer</span><span class=\"p\">(</span><span class=\"n\">network</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">'max'</span><span class=\"p\">)</span>\n<span class=\"n\">network</span> <span class=\"o\">=</span> <span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">DenseLayer</span><span class=\"p\">(</span><span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">dropout</span><span class=\"p\">(</span><span class=\"n\">network</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">),</span>\n                                    <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">nonlinearity</span><span class=\"o\">=</span><span class=\"n\">leaky_rectify</span><span class=\"p\">,</span>\n                                    <span class=\"n\">W</span><span class=\"o\">=</span><span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">init</span><span class=\"o\">.</span><span class=\"n\">Orthogonal</span><span class=\"p\">())</span>\n<span class=\"n\">network</span> <span class=\"o\">=</span> <span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">DenseLayer</span><span class=\"p\">(</span><span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">dropout</span><span class=\"p\">(</span><span class=\"n\">network</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">),</span>\n                                    <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">nonlinearity</span><span class=\"o\">=</span><span class=\"n\">softmax</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># create loss function</span>\n<span class=\"n\">prediction</span> <span class=\"o\">=</span> <span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">get_output</span><span class=\"p\">(</span><span class=\"n\">network</span><span class=\"p\">)</span>\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">objectives</span><span class=\"o\">.</span><span class=\"n\">categorical_crossentropy</span><span class=\"p\">(</span><span class=\"n\">prediction</span><span class=\"p\">,</span> <span class=\"n\">target_var</span><span class=\"p\">)</span>\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"mf\">1e-4</span> <span class=\"o\">*</span> <span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">regularization</span><span class=\"o\">.</span><span class=\"n\">regularize_network_params</span><span class=\"p\">(</span>\n        <span class=\"n\">network</span><span class=\"p\">,</span> <span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">regularization</span><span class=\"o\">.</span><span class=\"n\">l2</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># create parameter update expressions</span>\n<span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">get_all_params</span><span class=\"p\">(</span><span class=\"n\">network</span><span class=\"p\">,</span> <span class=\"n\">trainable</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">updates</span> <span class=\"o\">=</span> <span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">updates</span><span class=\"o\">.</span><span class=\"n\">nesterov_momentum</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">,</span>\n                                            <span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mf\">0.9</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># compile training function that updates parameters and returns training loss</span>\n<span class=\"n\">train_fn</span> <span class=\"o\">=</span> <span class=\"n\">theano</span><span class=\"o\">.</span><span class=\"n\">function</span><span class=\"p\">([</span><span class=\"n\">input_var</span><span class=\"p\">,</span> <span class=\"n\">target_var</span><span class=\"p\">],</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">updates</span><span class=\"o\">=</span><span class=\"n\">updates</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># train network (assuming you've got some training data in numpy arrays)</span>\n<span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n    <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n    <span class=\"k\">for</span> <span class=\"n\">input_batch</span><span class=\"p\">,</span> <span class=\"n\">target_batch</span> <span class=\"ow\">in</span> <span class=\"n\">training_data</span><span class=\"p\">:</span>\n        <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"n\">train_fn</span><span class=\"p\">(</span><span class=\"n\">input_batch</span><span class=\"p\">,</span> <span class=\"n\">target_batch</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Epoch </span><span class=\"si\">%d</span><span class=\"s2\">: Loss </span><span class=\"si\">%g</span><span class=\"s2\">\"</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">epoch</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">loss</span> <span class=\"o\">/</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">training_data</span><span class=\"p\">)))</span>\n\n<span class=\"c1\"># use trained network for predictions</span>\n<span class=\"n\">test_prediction</span> <span class=\"o\">=</span> <span class=\"n\">lasagne</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">get_output</span><span class=\"p\">(</span><span class=\"n\">network</span><span class=\"p\">,</span> <span class=\"n\">deterministic</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">predict_fn</span> <span class=\"o\">=</span> <span class=\"n\">theano</span><span class=\"o\">.</span><span class=\"n\">function</span><span class=\"p\">([</span><span class=\"n\">input_var</span><span class=\"p\">],</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">test_prediction</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Predicted class for first test input: </span><span class=\"si\">%r</span><span class=\"s2\">\"</span> <span class=\"o\">%</span> <span class=\"n\">predict_fn</span><span class=\"p\">(</span><span class=\"n\">test_data</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]))</span>\n</pre>\n<p>For a fully-functional example, see <a href=\"examples/mnist.py\" rel=\"nofollow\">examples/mnist.py</a>,\nand check the <a href=\"http://lasagne.readthedocs.org/en/latest/user/tutorial.html\" rel=\"nofollow\">Tutorial</a> for in-depth\nexplanations of the same. More examples, code snippets and reproductions of\nrecent research papers are maintained in the separate <a href=\"https://github.com/Lasagne/Recipes\" rel=\"nofollow\">Lasagne Recipes</a> repository.</p>\n</div>\n<div id=\"development\">\n<h3>Development</h3>\n<p>Lasagne is a work in progress, input is welcome.</p>\n<p>Please see the <a href=\"http://lasagne.readthedocs.org/en/latest/user/development.html\" rel=\"nofollow\">Contribution instructions</a> for details\non how you can contribute!</p>\n</div>\n<div id=\"changelog\">\n<h3>Changelog</h3>\n<h3 id=\"id1\"><span class=\"section-subtitle\">0.1 (2015-08-13)</span></h3>\n<p>First release.</p>\n<ul>\n<li>core contributors, in alphabetical order:<ul>\n<li>Eric Battenberg (@ebattenberg)</li>\n<li>Sander Dieleman (@benanne)</li>\n<li>Daniel Nouri (@dnouri)</li>\n<li>Eben Olson (@ebenolson)</li>\n<li>A\u00e4ron van den Oord (@avdnoord)</li>\n<li>Colin Raffel (@craffel)</li>\n<li>Jan Schl\u00fcter (@f0k)</li>\n<li>S\u00f8ren Kaae S\u00f8nderby (@skaae)</li>\n</ul>\n</li>\n<li>extra contributors, in chronological order:<ul>\n<li>Daniel Maturana (@dimatura): documentation, cuDNN layers, LRN</li>\n<li>Jonas Degrave (@317070): get_all_param_values() fix</li>\n<li>Jack Kelly (@JackKelly): help with recurrent layers</li>\n<li>G\u00e1bor Tak\u00e1cs (@takacsg84): support broadcastable parameters in lasagne.updates</li>\n<li>Diogo Moitinho de Almeida (@diogo149): MNIST example fixes</li>\n<li>Brian McFee (@bmcfee): MaxPool2DLayer fix</li>\n<li>Martin Thoma (@MartinThoma): documentation</li>\n<li>Jeffrey De Fauw (@JeffreyDF): documentation, ADAM fix</li>\n<li>Michael Heilman (@mheilman): NonlinearityLayer, lasagne.random</li>\n<li>Gregory Sanders (@instagibbs): documentation fix</li>\n<li>Jon Crall (@erotemic): check for non-positive input shapes</li>\n<li>Hendrik Weideman (@hjweide): set_all_param_values() test, MaxPool2DCCLayer fix</li>\n<li>Kashif Rasul (@kashif): ADAM simplification</li>\n<li>Peter de Rivaz (@peterderivaz): documentation fix</li>\n</ul>\n</li>\n</ul>\n</div>\n</div>\n\n          </div>"}, "last_serial": 1676667, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "44212b92bf5f3b1be3021fa0b64b5fdb", "sha256": "3c634ecab67e43e4f18520932bfd88bd3c678ec723c48177f18799dab2411233"}, "downloads": -1, "filename": "Lasagne-0.1.tar.gz", "has_sig": false, "md5_digest": "44212b92bf5f3b1be3021fa0b64b5fdb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 125077, "upload_time": "2015-08-13T21:10:53", "upload_time_iso_8601": "2015-08-13T21:10:53.821520Z", "url": "https://files.pythonhosted.org/packages/98/bf/4b2336e4dbc8c8859c4dd81b1cff18eef2066b4973a1bd2b0ca2e5435f35/Lasagne-0.1.tar.gz", "yanked": false}], "0.1dev": []}, "urls": [{"comment_text": "", "digests": {"md5": "44212b92bf5f3b1be3021fa0b64b5fdb", "sha256": "3c634ecab67e43e4f18520932bfd88bd3c678ec723c48177f18799dab2411233"}, "downloads": -1, "filename": "Lasagne-0.1.tar.gz", "has_sig": false, "md5_digest": "44212b92bf5f3b1be3021fa0b64b5fdb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 125077, "upload_time": "2015-08-13T21:10:53", "upload_time_iso_8601": "2015-08-13T21:10:53.821520Z", "url": "https://files.pythonhosted.org/packages/98/bf/4b2336e4dbc8c8859c4dd81b1cff18eef2066b4973a1bd2b0ca2e5435f35/Lasagne-0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:47:49 2020"}