{"info": {"author": "Nando Florestan", "author_email": "nandoflorestan@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Console", "Environment :: Other Environment", "Intended Audience :: Developers", "Intended Audience :: System Administrators", "License :: OSI Approved :: GNU General Public License (GPL)", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 2.6", "Topic :: Database", "Topic :: System :: Archiving :: Backup"], "description": "Backalaika is not a russian musical\ninstrument, it is a simple backup solution for small offices.\n\nhttp://sourceforge.net/projects/backalaika/\n\nFeatures\n========\n\n* zip, tar.gzip and tar.bz2 compression\n* predefined backup profiles\n* differential and incremental backups\n* file filters\n* file database with hashes\n* media sizes\n* multi-volume\n* etc.\n\nRequirements\n============\n\nBackalaika requires Python 2.6.x and these libraries:\n\n* Unipath - http://pypi.python.org/pypi/Unipath\n* SQLAlchemy - http://pypi.python.org/pypi/SQLAlchemy\n\nWriting backup jobs\n===================\n\nYou define backup jobs in small text files (actually small Python files).\nThey are simple. Here is what a backup job looks like:\n\n    job = dict(name='nandos-work',\n        sources = [\n            ('/home/nando/2010',    'zip,separate_dirs'),\n            (\"/home/nando/.mozilla-thunderbird/\", \"bz2\"),\n            # more sources here if you like...\n        ]\n    )\n\nThe job name ('nandos-work') will be used as part of the\nbackup directory name.\n\n*sources* is a list of tuples. Each tuple contains a directory\nto be backed up and the corresponding options. Each directory will\nend up in its own compressed file. What kind of compression?\n\n* Zip if you say 'zip' in the options, or\n* tar.gz  if you say 'gz' in the options, or\n* tar.bz2 if you say 'bz2' in the options.\n\nYou should prefer bz2 for very big archives.\n\nZip was originally limited to 2GB; now it is limited to 4GB but many\nprograms still only read zip files shorter than 2 GB.\n\nbz2 compresses better than the others, therefore the default is bz2.\n\nThe directories are backed up recursively; in other words,\nsubfolders are included.\n\nThe *separate_dirs* option causes the *subfolders* of the specified\ndirectory to be backed up -- each in its own compressed file.\n\nYou must separate the options with commas, in the options string.\n\nIf you are a Python programmer, note that in the job file you can have\nother things happen too, in addition to defining the *job* global\nvariable. For instance, you could mount a smbfs share. If you define an *after_backup()* function, it is called before exiting.\n\nTo perform a backup, you specify one backup job file:\n\n    backalaika.py -j jobs/myjob.py\n\nBackalaika then asks 3 questions on the console:\n\n1. Only backup a file once\n==========================\n\nNow just let me explain that Backalaika can be used to maintain a\ndatabase containing information about all your backup media and files.\n\nThis database is able to uniquely identify a file (by its hash), so\nwhen making a backup, the database can be looked up to prevent inclusion\nof repeated (not modified) files.\n\nThe first question the program asks is about this.\nShould we skip known files or not.\n\n2. Minimum file modification date\n=================================\n\nBackalaika then asks what is the minimum file modification date.\nThis is another mechanism useful for incremental or differential backups.\nFor instance, suppose the last backup you did was on December 25th, 2008.\nYou might then answer:\n\n    2008-12-25\n\n...causing only files modified later than that to be backed up.\nIf you want a *full* backup, just hit Enter for that question.\n\n3. A brief comment\n==================\n\nNext, the program asks for a brief comment. You can type one or two\nwords, these will be added to the name of the directory containing\nthe backup.\n\nThe backup process\n==================\n\nFinally, the program presents information about the job and asks you\nto press Enter to start.\n(When you want to interrupt the program, you press CTRL-C.)\n\nDuring backup, the program keeps writing the names of the files on\nthe screen. If some files cannot be accessed for some reason,\nthe error is shown.\n\nErrors are collected and shown again *after* the backup is complete.\n\nMain configuration\n==================\n\nYou may write configuration in two places:\n1) a \".backalaika\" file in your home directory, and\n2) in each job file itself. And the settings in here win over the others.\n\nHere is the default configuration:\n\n    config = dict(\n        dir_backups = '~/backups',\n        \n        # File extensions of files that should be stored, not recompressed:\n        store_only = 'pdf|ods|odt|jpg|jpeg|png|gif|'                      'zip|rar|gz|tgz|bz2|jar|deb|rpm|cab'                      '|mpg|mpeg|avi|mov|flv|wmv|wma|mp3|ogg',\n        \n        # Worthless files that shall NOT be backed up:\n        skip_files = '.DS_Store|.localized|Thumbs.db|*.pyc',\n        \n        # How long can each archive volume be?\n        # max_size = 1024 * 1024 * 699  # CD size (untested),\n        max_size = 1024 * 1024 * 4350  # DVD size (untested),\n        \n        # Database connection string\n        db_uri = 'sqlite:///backalaika.sqlite',\n    )\n\nAs you saw above, configuration consists of declaring a dictionary\ncontaining the following keys:\n\n* *dir_backups* is the directory where the backup files will be written.\n\n* *store_only* is for zip archives, they can just store files as\nopposed to compressing them. This is useful for files that are already\ncompressed. So in this variable you should write extensions of files\nthat cannot be further compressed. The extensions are separated by\na pipe character: |\n\n* *skip_files* lets you specify file names that should not be backed up.\nIn this variable you may use the asterisk for wildcards.\n\n* *max_size* is the desired archive length, for multiple volume backups.\nFor instance, you could set it to the size of a DVD (in bytes).\nWhen performing a backup job, the program creates the next volume\nwhenever a file possibly wouldn't fit in the current volume.\n\n* *db_uri* is a database connection string as supported by SQLAlchemy -\nhttp://www.sqlalchemy.org\n\nThere are a couple more keys:\n\nVolume sizes\n============\n\nYou can add to the config dict a list of media sizes (for instance,\nto fill some multisession CDs), in megabytes:\n\n    volume_sizes = [596, 396, 640, 2700, 4350, 700] # partially used CDs\n\nThe program first consumes all volume_sizes, then applies max_size.\n\nDatabase URI\n============\n\nBackalaika uses SQLAlchemy to support various database backends such as\nPostgres, MySQL and sqlite. The default is ~/backups/backalaika.sqlite.\nTo change this, create the database, then configure Backalaika.\n\nLike any configuration, it can be done in the job file. But usually you\nwant to use a single database, so you create a ~/.backalaika file and\nadd to it your connection string like this:\n\n    config = dict(\n        db_uri = 'postgresql://user:password@localhost/database_name',\n    )\n\nFor more help creating a db_uri, consult SQLALchemy docs.\n\nCommand examples\n================\n\nNow try this command, it lists the command-line switches:\n\n    backalaika.py -h\n\nAfter you burn your archives to DVDs, you can insert the media and have\nBackalaika add all its files to the database:\n\n    backalaika.py -a 'MyBackupMedia001' -d 'Description here'\n\nThe program enters zip, tar.gz and tar.bz2 files, as if they were\ndirectories, adding THEIR contents to the database.\n\nIt hashes files by default. You can turn hashing off with -n:\n\n    backalaika.py -n -a 'AnotherMedia002' -d 'Videos'\n\nThis lists all volume titles in the database:\n\n    backalaika.py -v\n\nYou can view the files of a specific volume:\n\n    backalaika.py -f 'AnotherMedia002'\n\n...or all volumes:\n\n    backalaika.py -f ''\n\nUse -r to remove a volume and its contents from the database:\n\n    backalaika.py -r 'AnotherMedia002'\n\nPlease give feedback\n====================\n\nBackalaika is a work in progress. Do send us feature requests etc.", "description_content_type": null, "docs_url": null, "download_url": "http://sourceforge.net/projects/backalaika/files", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://sourceforge.net/projects/backalaika/", "keywords": "backup,python,sqlalchemy,file hash,console", "license": "GPL 3", "maintainer": null, "maintainer_email": null, "name": "backalaika", "package_url": "https://pypi.org/project/backalaika/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/backalaika/", "project_urls": {"Download": "http://sourceforge.net/projects/backalaika/files", "Homepage": "http://sourceforge.net/projects/backalaika/"}, "release_url": "https://pypi.org/project/backalaika/0.2.1/", "requires_dist": null, "requires_python": null, "summary": "Backup utility for small offices", "version": "0.2.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            Backalaika is not a russian musical<br>instrument, it is a simple backup solution for small offices.<br><br>http://sourceforge.net/projects/backalaika/<br><br>Features<br>========<br><br>* zip, tar.gzip and tar.bz2 compression<br>* predefined backup profiles<br>* differential and incremental backups<br>* file filters<br>* file database with hashes<br>* media sizes<br>* multi-volume<br>* etc.<br><br>Requirements<br>============<br><br>Backalaika requires Python 2.6.x and these libraries:<br><br>* Unipath - http://pypi.python.org/pypi/Unipath<br>* SQLAlchemy - http://pypi.python.org/pypi/SQLAlchemy<br><br>Writing backup jobs<br>===================<br><br>You define backup jobs in small text files (actually small Python files).<br>They are simple. Here is what a backup job looks like:<br><br>    job = dict(name='nandos-work',<br>        sources = [<br>            ('/home/nando/2010',    'zip,separate_dirs'),<br>            (\"/home/nando/.mozilla-thunderbird/\", \"bz2\"),<br>            # more sources here if you like...<br>        ]<br>    )<br><br>The job name ('nandos-work') will be used as part of the<br>backup directory name.<br><br>*sources* is a list of tuples. Each tuple contains a directory<br>to be backed up and the corresponding options. Each directory will<br>end up in its own compressed file. What kind of compression?<br><br>* Zip if you say 'zip' in the options, or<br>* tar.gz  if you say 'gz' in the options, or<br>* tar.bz2 if you say 'bz2' in the options.<br><br>You should prefer bz2 for very big archives.<br><br>Zip was originally limited to 2GB; now it is limited to 4GB but many<br>programs still only read zip files shorter than 2 GB.<br><br>bz2 compresses better than the others, therefore the default is bz2.<br><br>The directories are backed up recursively; in other words,<br>subfolders are included.<br><br>The *separate_dirs* option causes the *subfolders* of the specified<br>directory to be backed up -- each in its own compressed file.<br><br>You must separate the options with commas, in the options string.<br><br>If you are a Python programmer, note that in the job file you can have<br>other things happen too, in addition to defining the *job* global<br>variable. For instance, you could mount a smbfs share. If you define an *after_backup()* function, it is called before exiting.<br><br>To perform a backup, you specify one backup job file:<br><br>    backalaika.py -j jobs/myjob.py<br><br>Backalaika then asks 3 questions on the console:<br><br>1. Only backup a file once<br>==========================<br><br>Now just let me explain that Backalaika can be used to maintain a<br>database containing information about all your backup media and files.<br><br>This database is able to uniquely identify a file (by its hash), so<br>when making a backup, the database can be looked up to prevent inclusion<br>of repeated (not modified) files.<br><br>The first question the program asks is about this.<br>Should we skip known files or not.<br><br>2. Minimum file modification date<br>=================================<br><br>Backalaika then asks what is the minimum file modification date.<br>This is another mechanism useful for incremental or differential backups.<br>For instance, suppose the last backup you did was on December 25th, 2008.<br>You might then answer:<br><br>    2008-12-25<br><br>...causing only files modified later than that to be backed up.<br>If you want a *full* backup, just hit Enter for that question.<br><br>3. A brief comment<br>==================<br><br>Next, the program asks for a brief comment. You can type one or two<br>words, these will be added to the name of the directory containing<br>the backup.<br><br>The backup process<br>==================<br><br>Finally, the program presents information about the job and asks you<br>to press Enter to start.<br>(When you want to interrupt the program, you press CTRL-C.)<br><br>During backup, the program keeps writing the names of the files on<br>the screen. If some files cannot be accessed for some reason,<br>the error is shown.<br><br>Errors are collected and shown again *after* the backup is complete.<br><br>Main configuration<br>==================<br><br>You may write configuration in two places:<br>1) a \".backalaika\" file in your home directory, and<br>2) in each job file itself. And the settings in here win over the others.<br><br>Here is the default configuration:<br><br>    config = dict(<br>        dir_backups = '~/backups',<br>        <br>        # File extensions of files that should be stored, not recompressed:<br>        store_only = 'pdf|ods|odt|jpg|jpeg|png|gif|'                      'zip|rar|gz|tgz|bz2|jar|deb|rpm|cab'                      '|mpg|mpeg|avi|mov|flv|wmv|wma|mp3|ogg',<br>        <br>        # Worthless files that shall NOT be backed up:<br>        skip_files = '.DS_Store|.localized|Thumbs.db|*.pyc',<br>        <br>        # How long can each archive volume be?<br>        # max_size = 1024 * 1024 * 699  # CD size (untested),<br>        max_size = 1024 * 1024 * 4350  # DVD size (untested),<br>        <br>        # Database connection string<br>        db_uri = 'sqlite:///backalaika.sqlite',<br>    )<br><br>As you saw above, configuration consists of declaring a dictionary<br>containing the following keys:<br><br>* *dir_backups* is the directory where the backup files will be written.<br><br>* *store_only* is for zip archives, they can just store files as<br>opposed to compressing them. This is useful for files that are already<br>compressed. So in this variable you should write extensions of files<br>that cannot be further compressed. The extensions are separated by<br>a pipe character: |<br><br>* *skip_files* lets you specify file names that should not be backed up.<br>In this variable you may use the asterisk for wildcards.<br><br>* *max_size* is the desired archive length, for multiple volume backups.<br>For instance, you could set it to the size of a DVD (in bytes).<br>When performing a backup job, the program creates the next volume<br>whenever a file possibly wouldn't fit in the current volume.<br><br>* *db_uri* is a database connection string as supported by SQLAlchemy -<br>http://www.sqlalchemy.org<br><br>There are a couple more keys:<br><br>Volume sizes<br>============<br><br>You can add to the config dict a list of media sizes (for instance,<br>to fill some multisession CDs), in megabytes:<br><br>    volume_sizes = [596, 396, 640, 2700, 4350, 700] # partially used CDs<br><br>The program first consumes all volume_sizes, then applies max_size.<br><br>Database URI<br>============<br><br>Backalaika uses SQLAlchemy to support various database backends such as<br>Postgres, MySQL and sqlite. The default is ~/backups/backalaika.sqlite.<br>To change this, create the database, then configure Backalaika.<br><br>Like any configuration, it can be done in the job file. But usually you<br>want to use a single database, so you create a ~/.backalaika file and<br>add to it your connection string like this:<br><br>    config = dict(<br>        db_uri = 'postgresql://user:password@localhost/database_name',<br>    )<br><br>For more help creating a db_uri, consult SQLALchemy docs.<br><br>Command examples<br>================<br><br>Now try this command, it lists the command-line switches:<br><br>    backalaika.py -h<br><br>After you burn your archives to DVDs, you can insert the media and have<br>Backalaika add all its files to the database:<br><br>    backalaika.py -a 'MyBackupMedia001' -d 'Description here'<br><br>The program enters zip, tar.gz and tar.bz2 files, as if they were<br>directories, adding THEIR contents to the database.<br><br>It hashes files by default. You can turn hashing off with -n:<br><br>    backalaika.py -n -a 'AnotherMedia002' -d 'Videos'<br><br>This lists all volume titles in the database:<br><br>    backalaika.py -v<br><br>You can view the files of a specific volume:<br><br>    backalaika.py -f 'AnotherMedia002'<br><br>...or all volumes:<br><br>    backalaika.py -f ''<br><br>Use -r to remove a volume and its contents from the database:<br><br>    backalaika.py -r 'AnotherMedia002'<br><br>Please give feedback<br>====================<br><br>Backalaika is a work in progress. Do send us feature requests etc.\n          </div>"}, "last_serial": 160243, "releases": {"0.2.1": []}, "urls": [], "timestamp": "Thu May  7 18:15:09 2020"}