{"info": {"author": "Alberto Pettarin", "author_email": "alberto@albertopettarin.it", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Environment :: Console", "Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: End Users/Desktop", "Intended Audience :: Science/Research", "License :: OSI Approved :: GNU Affero General Public License v3", "Natural Language :: English", "Operating System :: MacOS :: MacOS X", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX :: Linux", "Programming Language :: C", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Topic :: Education", "Topic :: Multimedia", "Topic :: Multimedia :: Sound/Audio", "Topic :: Multimedia :: Sound/Audio :: Analysis", "Topic :: Multimedia :: Sound/Audio :: Speech", "Topic :: Printing", "Topic :: Scientific/Engineering", "Topic :: Scientific/Engineering :: Mathematics", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Text Processing", "Topic :: Text Processing :: Linguistic", "Topic :: Text Processing :: Markup", "Topic :: Text Processing :: Markup :: HTML", "Topic :: Text Processing :: Markup :: XML", "Topic :: Utilities"], "description": "lachesis\n========\n\n**lachesis** automates the segmentation of a transcript into closed\ncaptions\n\n-  Version: 0.0.3\n-  Date: 2017-01-26\n-  Developed by: `Alberto Pettarin <http://www.albertopettarin.it/>`__\n-  License: the GNU Affero General Public License Version 3 (AGPL v3)\n-  Contact: info@readbeyond.it\n\n**DO NOT USE THIS PACKAGE IN PRODUCTION UNTIL IT REACHES v1.0.0 !!!**\n\nGoal\n----\n\n**lachesis** automates the segmentation of a transcript into closed\ncaptions (CCs).\n\nThe general idea is that writing a transcription (raw text) is easier\nand faster than writing CCs, especially if you need to respect\nconstraints like a certain minimum/maximum number of characters per\nline, a maximum number of lines per CC, etc.\n\nYou can transcribe your video into raw text and ``lachesis`` will take\non the job of segmenting the text into CCs for you. Once you have the\nCCs, you can use a `forced\naligner <https://github.com/pettarin/forced-alignment-tools/>`__ like\n`aeneas <https://github.com/readbeyond/aeneas/>`__ to align them with\nthe audio of your video, obtaining a subtitle file (SRT, TTML, VTT,\netc.).\n\nWith ``lachesis`` and a forced aligner, the manual labor for producing\nCCs for a video is reduced to a. transcribing the video in raw text\nform, and b. checking the final CCs and audio alignment. Instead of\ntranscribing from scratch, you can even start by checking/editing a\nrough transcription made by an automated speech recognition engine, like\nthe \"automatic CCs\" from YouTube, speeding the process up further.\n\nThe \"magic\" behind ``lachesis`` consists in combining machine learning\ntechniques like `conditional random\nfields <https://en.wikipedia.org/wiki/Conditional_random_field>`__ (CRF)\nand classical NLP tools like `POS\ntagging <https://en.wikipedia.org/wiki/Part-of-speech_tagging>`__ and\n`sentence\nsegmentation <https://en.wikipedia.org/wiki/Text_segmentation>`__ to\nsplit the text into CC lines. The machine learning models are learned\nfrom existing, manually-edited, high-quality CCs, like those of\n`TED <https://www.youtube.com/user/TEDtalksDirector>`__/`TEDx <https://www.youtube.com/user/TEDxTalks>`__\ntalks on YouTube. The NLP tools come from the well-established, free NLP\nlibraries for Python listed below.\n\nIn summary, ``lachesis`` contains the following major functions:\n\n-  download closed captions from YouTube;\n-  parse closed caption TTML files (downloaded from YouTube);\n-  add POS tags to a given text or closed caption file;\n-  segment a given text into sentences;\n-  segment a given text into closed captions (several algorithms are\n   available);\n-  train and use machine learning models to segment raw text into CC\n   lines.\n\nInstallation\n------------\n\n**DO NOT USE THIS PACKAGE IN PRODUCTION UNTIL IT REACHES v1.0.0 !!!**\n\n.. code:: bash\n\n    pip install lachesis\n\nInstalling dependencies\n~~~~~~~~~~~~~~~~~~~~~~~\n\nYou might need additional packages, depending on how you plan to use\n``lachesis``:\n\n-  ``lxml >= 3.6.0`` for reading or downloading TTML files;\n-  ``youtube-dl >= 2017.1.16`` for downloading TTML files;\n-  ``python-crfsuite >= 0.9.1`` for training and using CRF-based\n   splitters.\n\nBy design choice, none of the above dependencies is installed by\n``pip install lachesis``. If you want to install them all, you can use:\n\n.. code:: bash\n\n    pip install lachesis[full]\n\nAlternatively, manually install only the dependencies you need. (You can\ndo it before or after installing ``lachesis``, the order does not\nmatter.)\n\nInstalling NLP Libraries\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn addition to the dependencies listed above, to perform POS tagging and\nsentence segmentation ``lachesis`` can use one or more of the following\nlibraries:\n\n-  ``Pattern`` (install with ``pip install pattern``, `see\n   here <http://www.clips.ua.ac.be/pattern>`__)\n-  ``NLTK`` (install with ``pip install nltk``, `see\n   here <http://www.nltk.org/>`__)\n-  ``spaCy`` (install with ``pip install spacy``, `see\n   here <https://spacy.io/>`__)\n-  ``UDPipe`` (install with ``pip install ufal.udpipe``, `see\n   here <https://ufal.mff.cuni.cz/>`__)\n\nIf you want to install them all, you can use:\n\n.. code:: bash\n\n    pip install lachesis[nlp]\n\nor ``[fullnlp]`` if you also want ``[full]`` as above.\n\nEach NLP library also needs language models which you need to\ndownload/install separately. Consult the documentation of your NLP\nlibrary for details.\n\n``lachesis`` expects the following directories in your home directory\n(you can symlink them, if you installed each NLP library in a different\nplace):\n\n-  ``~/lachesis_data/nltk_data`` for ``NLTK`` (`see\n   here <http://www.nltk.org/data.html>`__);\n-  ``~/lachesis_data/spacy_data`` for ``spaCy`` (`see\n   here <https://spacy.io/docs/usage/>`__);\n-  ``~/lachesis_data/udpipe_data`` for ``UDPipe`` (`see\n   here <https://ufal.mff.cuni.cz/udpipe>`__).\n\nThe NLP library ``Pattern`` does not need a separate download of its\nlanguage models, as they are bundled in the file you download when\ninstalling through ``pip install pattern``.\n\nThe following table summarizes the languages supported by each library\nin their standard language models pack. (Additional languages might be\nsupported by third party projects/downloads or added over time.)\n\n+-----------------------+-----------+--------+---------+----------+\n| Language / Library    | Pattern   | NLTK   | spaCy   | UDPipe   |\n+=======================+===========+========+=========+==========+\n| Arabic                |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Basque                |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Bulgarian             |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Croatian              |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Czech                 |           | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Danish                |           | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Dutch                 | \u2713         | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| English               | \u2713         | \u2713      | \u2713       | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Estonian              |           | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Finnish               |           | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| French                | \u2713         | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| German                | \u2713         | \u2713      | \u2713       | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Gothic                |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Greek                 |           | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Greek (ancient)       |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Hebrew                |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Hindi                 |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Hungarian             |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Indonesian            |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Irish                 |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Italian               | \u2713         | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Latin                 |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Norwegian             |           | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Old Church Slavonic   |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Persian               |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Polish                |           | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Portuguese            |           | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Romanian              |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Slovenian             |           | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Spanish               | \u2713         | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Swedish               |           | \u2713      |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Tamil                 |           |        |         | \u2713        |\n+-----------------------+-----------+--------+---------+----------+\n| Turkish               |           | \u2713      |         |          |\n+-----------------------+-----------+--------+---------+----------+\n\nUsage\n-----\n\nDownload closed captions from YouTube\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n    from lachesis.downloaders import Downloader\n    from lachesis.language import Language\n\n    # set URL of the video and language of the CCs\n    url = u\"http://www.youtube.com/watch?v=NSL_xx2Qnyc\"\n    language = Language.ENGLISH\n\n    # download automatic CC, do not save to file\n    options = { \"auto\": True }\n    doc = Downloader.download_closed_captions(url, language, options)\n    print(doc)\n\n    # download manually-edited CC, saving the raw TTML file to disk\n    options = { \"auto\": False, \"output_file_path\": \"/tmp/ccs.ttml\" }\n    doc = Downloader.download_closed_captions(url, language, options)\n    print(doc)\n\nParse an existing TTML file downloaded from YouTube\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n    from lachesis.downloaders import Downloader\n\n    # parse a given TTML file downloaded from YouTube\n    ifp = \"/tmp/ccs.ttml\"\n    doc = Downloader.read_closed_captions(ifp, options={u\"downloader\": u\"youtube\"})\n    print(doc.language)\n\n    # print several representations of the CCs\n    print(doc.raw_string)                       # multi line string, similar to SRT but w/o ids or times\n    print(doc.raw_flat_clean_string)            # single line string, w/o CC line marks\n    print(doc.raw.string(flat=True, eol=u\"|\"))  # single line string, CC lines separated by '|' characters\n\nTokenize, split sentences, and POS tagging\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n    from lachesis.elements import Document\n    from lachesis.language import Language\n    from lachesis.nlpwrappers import NLPEngine\n\n    # work on this Unicode string\n    s = u\"Hello, World. This is a second sentence, with a comma too! And a third sentence.\"\n\n    # but you can also pass a list with pre-split sentences\n    # s = [u\"Hello World.\", u\"This is a second sentence.\", u\"Third one, bla bla\"]\n\n    # create a Text object from the Unicode string\n    doc = Document(raw=s, language=Language.ENGLISH)\n\n    # tokenize, split sentences, and POS tagging\n    # the best available NLP library will be chosen\n    nlp1 = NLPEngine()\n    nlp1.analyze(doc)\n\n    # the text has been divided into tokens, grouped in sentences\n    for s in doc.sentences:\n        print(s)                                        # raw\n        print(s.string(tagged=True))                    # tagged\n        print(s.string(raw=True, eol=u\"|\", eos=u\"\"))    # raw w/o CC line and sentence marks\n\n    # explicitly specify the NLP library NLTK,\n    # other options include: \"pattern\", \"spacy\", \"udpipe\"\n    nlp2 = NLPEngine()\n    nlp2.analyze(doc, wrapper=u\"nltk\")\n    ...\n\n    # if you need to analyze many documents,\n    # preload (and keep in cache) an NLP library,\n    # even different ones for different languages\n    nlp3 = NLPEngine(preload=[\n        (u\"en\", u\"spacy\"),\n        (u\"de\", u\"nltk\"),\n        (u\"it\", u\"pattern\"),\n        (u\"fr\", u\"udpipe\")\n    ])\n    nlp3.analyze(doc)\n    ...\n\nSplit into closed captions\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n    from lachesis.elements import Document\n    from lachesis.language import Language\n    from lachesis.nlpwrappers import NLPEngine\n    from lachesis.splitters import CRFSplitter\n    from lachesis.splitters import GreedySplitter\n\n    # create a document from a raw string\n    s = u\"Hello, World. This is a second sentence, with a comma too! And a third sentence.\"\n    doc = Document(raw=s, language=Language.ENGLISH)\n\n    # analyze it using the NLP library Pattern\n    nlpe = NLPEngine()\n    nlpe.analyze(doc, wrapper=u\"pattern\")\n\n    # feed the document into the CRF splitter (max 42 chars/line, max 2 lines/cc)\n    spl = CRFSplitter(doc.language, 42, 2)\n    spl.split(doc)\n\n    # print the segmented CCs\n    for cc in doc.ccs:\n        for line in cc.elements:\n            print(line)\n        print(u\"\")\n\n    # the default location for CRF model files is ~/lachesis_data/crf_data/\n    # but you can also specify a different path\n    spl = CRFSplitter(doc.language, 42, 2, model_file_path=\"/tmp/yourmodel.crfsuite\")\n    spl.split(doc)\n\n    # if you do not have pycrfsuite installed\n    # or the CRF model file for the document language,\n    # you can use the GreedySplitter\n    gs = GreedySplitter(doc.language, 42, 2)\n    gs.split(doc)\n\nTrain a CRF model to segment raw text into CC lines\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: bash\n\n    $ # /tmp/ccs/train contains several TTML files to learn from\n    $ # you can download them from YouTube using lachesis (see above)\n    $ ls /tmp/ccs/train\n    0001.ttml\n    0002.ttml\n    ...\n\n    $ # extract features and labels from them:\n    $ python -m lachesis.ml.crf dump eng /tmp/ccs/train/ /tmp/ccs/train.pickle\n    ...\n\n    $ # train the CRF model:\n    $ python -m lachesis.ml.crf train eng /tmp/ccs/train.pickle /tmp/ccs/model.crfsuite\n    ...\n\n    $ # evaluate the model on the training set\n    $ python -m lachesis.ml.crf test eng /tmp/ccs/train.pickle /tmp/ccs/model.crfsuite\n    ...\n\n    $ # you might want to evaluate on a test set, disjoint from the training set,\n    $ # that is, the test set contains CCs not seen during the training:\n    $ ls /tmp/css/test\n    1001.ttml\n    1002.ttml\n    ...\n    $ python -m lachesis.ml.crf dump eng /tmp/ccs/test/ /tmp/ccs/test.pickle\n    $ python -m lachesis.ml.crf test eng /tmp/ccs/test.pickle /tmp/ccs/model.crfsuite\n    ...\n    $ # now you can build a CRFSplitter\n    $ # with model_file_path=\"/tmp/ccs/model.crfsuite\" as shown above\n\nTODO: decide and document where pre-trained model files can be\ndownloaded\n\nLicense\n-------\n\n**lachesis** is released under the terms of the GNU Affero General\nPublic License Version 3. See the `LICENSE <LICENSE>`__ file for\ndetails.", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/readbeyond/lachesis", "keywords": "ReadBeyond Sync,ReadBeyond,SBV,SRT,SSV,SUB,TSV,TTML,VTT,aeneas,captioning,captions,closed captions,forced alignment,lachesis,media overlay,speech to text,subtitles,sync,synchronization,transcript,video captions", "license": "GNU Affero General Public License v3 (AGPL v3)", "maintainer": null, "maintainer_email": null, "name": "lachesis", "package_url": "https://pypi.org/project/lachesis/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/lachesis/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/readbeyond/lachesis"}, "release_url": "https://pypi.org/project/lachesis/0.0.3.0/", "requires_dist": null, "requires_python": null, "summary": "lachesis automates the segmentation of a transcript into closed captions", "version": "0.0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><strong>lachesis</strong> automates the segmentation of a transcript into closed\ncaptions</p>\n<ul>\n<li>Version: 0.0.3</li>\n<li>Date: 2017-01-26</li>\n<li>Developed by: <a href=\"http://www.albertopettarin.it/\" rel=\"nofollow\">Alberto Pettarin</a></li>\n<li>License: the GNU Affero General Public License Version 3 (AGPL v3)</li>\n<li>Contact: <a href=\"mailto:info%40readbeyond.it\">info<span>@</span>readbeyond<span>.</span>it</a></li>\n</ul>\n<p><strong>DO NOT USE THIS PACKAGE IN PRODUCTION UNTIL IT REACHES v1.0.0 !!!</strong></p>\n<div id=\"goal\">\n<h2>Goal</h2>\n<p><strong>lachesis</strong> automates the segmentation of a transcript into closed\ncaptions (CCs).</p>\n<p>The general idea is that writing a transcription (raw text) is easier\nand faster than writing CCs, especially if you need to respect\nconstraints like a certain minimum/maximum number of characters per\nline, a maximum number of lines per CC, etc.</p>\n<p>You can transcribe your video into raw text and <tt>lachesis</tt> will take\non the job of segmenting the text into CCs for you. Once you have the\nCCs, you can use a <a href=\"https://github.com/pettarin/forced-alignment-tools/\" rel=\"nofollow\">forced\naligner</a> like\n<a href=\"https://github.com/readbeyond/aeneas/\" rel=\"nofollow\">aeneas</a> to align them with\nthe audio of your video, obtaining a subtitle file (SRT, TTML, VTT,\netc.).</p>\n<p>With <tt>lachesis</tt> and a forced aligner, the manual labor for producing\nCCs for a video is reduced to a. transcribing the video in raw text\nform, and b. checking the final CCs and audio alignment. Instead of\ntranscribing from scratch, you can even start by checking/editing a\nrough transcription made by an automated speech recognition engine, like\nthe \u201cautomatic CCs\u201d from YouTube, speeding the process up further.</p>\n<p>The \u201cmagic\u201d behind <tt>lachesis</tt> consists in combining machine learning\ntechniques like <a href=\"https://en.wikipedia.org/wiki/Conditional_random_field\" rel=\"nofollow\">conditional random\nfields</a> (CRF)\nand classical NLP tools like <a href=\"https://en.wikipedia.org/wiki/Part-of-speech_tagging\" rel=\"nofollow\">POS\ntagging</a> and\n<a href=\"https://en.wikipedia.org/wiki/Text_segmentation\" rel=\"nofollow\">sentence\nsegmentation</a> to\nsplit the text into CC lines. The machine learning models are learned\nfrom existing, manually-edited, high-quality CCs, like those of\n<a href=\"https://www.youtube.com/user/TEDtalksDirector\" rel=\"nofollow\">TED</a>/<a href=\"https://www.youtube.com/user/TEDxTalks\" rel=\"nofollow\">TEDx</a>\ntalks on YouTube. The NLP tools come from the well-established, free NLP\nlibraries for Python listed below.</p>\n<p>In summary, <tt>lachesis</tt> contains the following major functions:</p>\n<ul>\n<li>download closed captions from YouTube;</li>\n<li>parse closed caption TTML files (downloaded from YouTube);</li>\n<li>add POS tags to a given text or closed caption file;</li>\n<li>segment a given text into sentences;</li>\n<li>segment a given text into closed captions (several algorithms are\navailable);</li>\n<li>train and use machine learning models to segment raw text into CC\nlines.</li>\n</ul>\n</div>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p><strong>DO NOT USE THIS PACKAGE IN PRODUCTION UNTIL IT REACHES v1.0.0 !!!</strong></p>\n<pre>pip install lachesis\n</pre>\n<div id=\"installing-dependencies\">\n<h3>Installing dependencies</h3>\n<p>You might need additional packages, depending on how you plan to use\n<tt>lachesis</tt>:</p>\n<ul>\n<li><tt>lxml &gt;= 3.6.0</tt> for reading or downloading TTML files;</li>\n<li><tt><span class=\"pre\">youtube-dl</span> &gt;= 2017.1.16</tt> for downloading TTML files;</li>\n<li><tt><span class=\"pre\">python-crfsuite</span> &gt;= 0.9.1</tt> for training and using CRF-based\nsplitters.</li>\n</ul>\n<p>By design choice, none of the above dependencies is installed by\n<tt>pip install lachesis</tt>. If you want to install them all, you can use:</p>\n<pre>pip install lachesis<span class=\"o\">[</span>full<span class=\"o\">]</span>\n</pre>\n<p>Alternatively, manually install only the dependencies you need. (You can\ndo it before or after installing <tt>lachesis</tt>, the order does not\nmatter.)</p>\n</div>\n<div id=\"installing-nlp-libraries\">\n<h3>Installing NLP Libraries</h3>\n<p>In addition to the dependencies listed above, to perform POS tagging and\nsentence segmentation <tt>lachesis</tt> can use one or more of the following\nlibraries:</p>\n<ul>\n<li><tt>Pattern</tt> (install with <tt>pip install pattern</tt>, <a href=\"http://www.clips.ua.ac.be/pattern\" rel=\"nofollow\">see\nhere</a>)</li>\n<li><tt>NLTK</tt> (install with <tt>pip install nltk</tt>, <a href=\"http://www.nltk.org/\" rel=\"nofollow\">see\nhere</a>)</li>\n<li><tt>spaCy</tt> (install with <tt>pip install spacy</tt>, <a href=\"https://spacy.io/\" rel=\"nofollow\">see\nhere</a>)</li>\n<li><tt>UDPipe</tt> (install with <tt>pip install ufal.udpipe</tt>, <a href=\"https://ufal.mff.cuni.cz/\" rel=\"nofollow\">see\nhere</a>)</li>\n</ul>\n<p>If you want to install them all, you can use:</p>\n<pre>pip install lachesis<span class=\"o\">[</span>nlp<span class=\"o\">]</span>\n</pre>\n<p>or <tt>[fullnlp]</tt> if you also want <tt>[full]</tt> as above.</p>\n<p>Each NLP library also needs language models which you need to\ndownload/install separately. Consult the documentation of your NLP\nlibrary for details.</p>\n<p><tt>lachesis</tt> expects the following directories in your home directory\n(you can symlink them, if you installed each NLP library in a different\nplace):</p>\n<ul>\n<li><tt>~/lachesis_data/nltk_data</tt> for <tt>NLTK</tt> (<a href=\"http://www.nltk.org/data.html\" rel=\"nofollow\">see\nhere</a>);</li>\n<li><tt>~/lachesis_data/spacy_data</tt> for <tt>spaCy</tt> (<a href=\"https://spacy.io/docs/usage/\" rel=\"nofollow\">see\nhere</a>);</li>\n<li><tt>~/lachesis_data/udpipe_data</tt> for <tt>UDPipe</tt> (<a href=\"https://ufal.mff.cuni.cz/udpipe\" rel=\"nofollow\">see\nhere</a>).</li>\n</ul>\n<p>The NLP library <tt>Pattern</tt> does not need a separate download of its\nlanguage models, as they are bundled in the file you download when\ninstalling through <tt>pip install pattern</tt>.</p>\n<p>The following table summarizes the languages supported by each library\nin their standard language models pack. (Additional languages might be\nsupported by third party projects/downloads or added over time.)</p>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>Language / Library</th>\n<th>Pattern</th>\n<th>NLTK</th>\n<th>spaCy</th>\n<th>UDPipe</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>Arabic</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Basque</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Bulgarian</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Croatian</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Czech</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Danish</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Dutch</td>\n<td>\u2713</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>English</td>\n<td>\u2713</td>\n<td>\u2713</td>\n<td>\u2713</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Estonian</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Finnish</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>French</td>\n<td>\u2713</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>German</td>\n<td>\u2713</td>\n<td>\u2713</td>\n<td>\u2713</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Gothic</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Greek</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Greek (ancient)</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Hebrew</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Hindi</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Hungarian</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Indonesian</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Irish</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Italian</td>\n<td>\u2713</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Latin</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Norwegian</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Old Church Slavonic</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Persian</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Polish</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Portuguese</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Romanian</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Slovenian</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Spanish</td>\n<td>\u2713</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Swedish</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Tamil</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n</tr>\n<tr><td>Turkish</td>\n<td>\u00a0</td>\n<td>\u2713</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<div id=\"download-closed-captions-from-youtube\">\n<h3>Download closed captions from YouTube</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">lachesis.downloaders</span> <span class=\"kn\">import</span> <span class=\"n\">Downloader</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lachesis.language</span> <span class=\"kn\">import</span> <span class=\"n\">Language</span>\n\n<span class=\"c1\"># set URL of the video and language of the CCs</span>\n<span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"sa\">u</span><span class=\"s2\">\"http://www.youtube.com/watch?v=NSL_xx2Qnyc\"</span>\n<span class=\"n\">language</span> <span class=\"o\">=</span> <span class=\"n\">Language</span><span class=\"o\">.</span><span class=\"n\">ENGLISH</span>\n\n<span class=\"c1\"># download automatic CC, do not save to file</span>\n<span class=\"n\">options</span> <span class=\"o\">=</span> <span class=\"p\">{</span> <span class=\"s2\">\"auto\"</span><span class=\"p\">:</span> <span class=\"kc\">True</span> <span class=\"p\">}</span>\n<span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"n\">Downloader</span><span class=\"o\">.</span><span class=\"n\">download_closed_captions</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">language</span><span class=\"p\">,</span> <span class=\"n\">options</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># download manually-edited CC, saving the raw TTML file to disk</span>\n<span class=\"n\">options</span> <span class=\"o\">=</span> <span class=\"p\">{</span> <span class=\"s2\">\"auto\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"s2\">\"output_file_path\"</span><span class=\"p\">:</span> <span class=\"s2\">\"/tmp/ccs.ttml\"</span> <span class=\"p\">}</span>\n<span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"n\">Downloader</span><span class=\"o\">.</span><span class=\"n\">download_closed_captions</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">language</span><span class=\"p\">,</span> <span class=\"n\">options</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"parse-an-existing-ttml-file-downloaded-from-youtube\">\n<h3>Parse an existing TTML file downloaded from YouTube</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">lachesis.downloaders</span> <span class=\"kn\">import</span> <span class=\"n\">Downloader</span>\n\n<span class=\"c1\"># parse a given TTML file downloaded from YouTube</span>\n<span class=\"n\">ifp</span> <span class=\"o\">=</span> <span class=\"s2\">\"/tmp/ccs.ttml\"</span>\n<span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"n\">Downloader</span><span class=\"o\">.</span><span class=\"n\">read_closed_captions</span><span class=\"p\">(</span><span class=\"n\">ifp</span><span class=\"p\">,</span> <span class=\"n\">options</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"sa\">u</span><span class=\"s2\">\"downloader\"</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s2\">\"youtube\"</span><span class=\"p\">})</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">language</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># print several representations of the CCs</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">raw_string</span><span class=\"p\">)</span>                       <span class=\"c1\"># multi line string, similar to SRT but w/o ids or times</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">raw_flat_clean_string</span><span class=\"p\">)</span>            <span class=\"c1\"># single line string, w/o CC line marks</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">raw</span><span class=\"o\">.</span><span class=\"n\">string</span><span class=\"p\">(</span><span class=\"n\">flat</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">eol</span><span class=\"o\">=</span><span class=\"sa\">u</span><span class=\"s2\">\"|\"</span><span class=\"p\">))</span>  <span class=\"c1\"># single line string, CC lines separated by '|' characters</span>\n</pre>\n</div>\n<div id=\"tokenize-split-sentences-and-pos-tagging\">\n<h3>Tokenize, split sentences, and POS tagging</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">lachesis.elements</span> <span class=\"kn\">import</span> <span class=\"n\">Document</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lachesis.language</span> <span class=\"kn\">import</span> <span class=\"n\">Language</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lachesis.nlpwrappers</span> <span class=\"kn\">import</span> <span class=\"n\">NLPEngine</span>\n\n<span class=\"c1\"># work on this Unicode string</span>\n<span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"sa\">u</span><span class=\"s2\">\"Hello, World. This is a second sentence, with a comma too! And a third sentence.\"</span>\n\n<span class=\"c1\"># but you can also pass a list with pre-split sentences</span>\n<span class=\"c1\"># s = [u\"Hello World.\", u\"This is a second sentence.\", u\"Third one, bla bla\"]</span>\n\n<span class=\"c1\"># create a Text object from the Unicode string</span>\n<span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"n\">Document</span><span class=\"p\">(</span><span class=\"n\">raw</span><span class=\"o\">=</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">language</span><span class=\"o\">=</span><span class=\"n\">Language</span><span class=\"o\">.</span><span class=\"n\">ENGLISH</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># tokenize, split sentences, and POS tagging</span>\n<span class=\"c1\"># the best available NLP library will be chosen</span>\n<span class=\"n\">nlp1</span> <span class=\"o\">=</span> <span class=\"n\">NLPEngine</span><span class=\"p\">()</span>\n<span class=\"n\">nlp1</span><span class=\"o\">.</span><span class=\"n\">analyze</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># the text has been divided into tokens, grouped in sentences</span>\n<span class=\"k\">for</span> <span class=\"n\">s</span> <span class=\"ow\">in</span> <span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">sentences</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span>                                        <span class=\"c1\"># raw</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">string</span><span class=\"p\">(</span><span class=\"n\">tagged</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">))</span>                    <span class=\"c1\"># tagged</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">string</span><span class=\"p\">(</span><span class=\"n\">raw</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">eol</span><span class=\"o\">=</span><span class=\"sa\">u</span><span class=\"s2\">\"|\"</span><span class=\"p\">,</span> <span class=\"n\">eos</span><span class=\"o\">=</span><span class=\"sa\">u</span><span class=\"s2\">\"\"</span><span class=\"p\">))</span>    <span class=\"c1\"># raw w/o CC line and sentence marks</span>\n\n<span class=\"c1\"># explicitly specify the NLP library NLTK,</span>\n<span class=\"c1\"># other options include: \"pattern\", \"spacy\", \"udpipe\"</span>\n<span class=\"n\">nlp2</span> <span class=\"o\">=</span> <span class=\"n\">NLPEngine</span><span class=\"p\">()</span>\n<span class=\"n\">nlp2</span><span class=\"o\">.</span><span class=\"n\">analyze</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">,</span> <span class=\"n\">wrapper</span><span class=\"o\">=</span><span class=\"sa\">u</span><span class=\"s2\">\"nltk\"</span><span class=\"p\">)</span>\n<span class=\"o\">...</span>\n\n<span class=\"c1\"># if you need to analyze many documents,</span>\n<span class=\"c1\"># preload (and keep in cache) an NLP library,</span>\n<span class=\"c1\"># even different ones for different languages</span>\n<span class=\"n\">nlp3</span> <span class=\"o\">=</span> <span class=\"n\">NLPEngine</span><span class=\"p\">(</span><span class=\"n\">preload</span><span class=\"o\">=</span><span class=\"p\">[</span>\n    <span class=\"p\">(</span><span class=\"sa\">u</span><span class=\"s2\">\"en\"</span><span class=\"p\">,</span> <span class=\"sa\">u</span><span class=\"s2\">\"spacy\"</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"sa\">u</span><span class=\"s2\">\"de\"</span><span class=\"p\">,</span> <span class=\"sa\">u</span><span class=\"s2\">\"nltk\"</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"sa\">u</span><span class=\"s2\">\"it\"</span><span class=\"p\">,</span> <span class=\"sa\">u</span><span class=\"s2\">\"pattern\"</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"sa\">u</span><span class=\"s2\">\"fr\"</span><span class=\"p\">,</span> <span class=\"sa\">u</span><span class=\"s2\">\"udpipe\"</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n<span class=\"n\">nlp3</span><span class=\"o\">.</span><span class=\"n\">analyze</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n<span class=\"o\">...</span>\n</pre>\n</div>\n<div id=\"split-into-closed-captions\">\n<h3>Split into closed captions</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">lachesis.elements</span> <span class=\"kn\">import</span> <span class=\"n\">Document</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lachesis.language</span> <span class=\"kn\">import</span> <span class=\"n\">Language</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lachesis.nlpwrappers</span> <span class=\"kn\">import</span> <span class=\"n\">NLPEngine</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lachesis.splitters</span> <span class=\"kn\">import</span> <span class=\"n\">CRFSplitter</span>\n<span class=\"kn\">from</span> <span class=\"nn\">lachesis.splitters</span> <span class=\"kn\">import</span> <span class=\"n\">GreedySplitter</span>\n\n<span class=\"c1\"># create a document from a raw string</span>\n<span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"sa\">u</span><span class=\"s2\">\"Hello, World. This is a second sentence, with a comma too! And a third sentence.\"</span>\n<span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"n\">Document</span><span class=\"p\">(</span><span class=\"n\">raw</span><span class=\"o\">=</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">language</span><span class=\"o\">=</span><span class=\"n\">Language</span><span class=\"o\">.</span><span class=\"n\">ENGLISH</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># analyze it using the NLP library Pattern</span>\n<span class=\"n\">nlpe</span> <span class=\"o\">=</span> <span class=\"n\">NLPEngine</span><span class=\"p\">()</span>\n<span class=\"n\">nlpe</span><span class=\"o\">.</span><span class=\"n\">analyze</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">,</span> <span class=\"n\">wrapper</span><span class=\"o\">=</span><span class=\"sa\">u</span><span class=\"s2\">\"pattern\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># feed the document into the CRF splitter (max 42 chars/line, max 2 lines/cc)</span>\n<span class=\"n\">spl</span> <span class=\"o\">=</span> <span class=\"n\">CRFSplitter</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">language</span><span class=\"p\">,</span> <span class=\"mi\">42</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">spl</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># print the segmented CCs</span>\n<span class=\"k\">for</span> <span class=\"n\">cc</span> <span class=\"ow\">in</span> <span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">ccs</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">cc</span><span class=\"o\">.</span><span class=\"n\">elements</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">u</span><span class=\"s2\">\"\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># the default location for CRF model files is ~/lachesis_data/crf_data/</span>\n<span class=\"c1\"># but you can also specify a different path</span>\n<span class=\"n\">spl</span> <span class=\"o\">=</span> <span class=\"n\">CRFSplitter</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">language</span><span class=\"p\">,</span> <span class=\"mi\">42</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">model_file_path</span><span class=\"o\">=</span><span class=\"s2\">\"/tmp/yourmodel.crfsuite\"</span><span class=\"p\">)</span>\n<span class=\"n\">spl</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># if you do not have pycrfsuite installed</span>\n<span class=\"c1\"># or the CRF model file for the document language,</span>\n<span class=\"c1\"># you can use the GreedySplitter</span>\n<span class=\"n\">gs</span> <span class=\"o\">=</span> <span class=\"n\">GreedySplitter</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">language</span><span class=\"p\">,</span> <span class=\"mi\">42</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">gs</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"train-a-crf-model-to-segment-raw-text-into-cc-lines\">\n<h3>Train a CRF model to segment raw text into CC lines</h3>\n<pre>$ <span class=\"c1\"># /tmp/ccs/train contains several TTML files to learn from\n</span>$ <span class=\"c1\"># you can download them from YouTube using lachesis (see above)\n</span>$ ls /tmp/ccs/train\n<span class=\"m\">0001</span>.ttml\n<span class=\"m\">0002</span>.ttml\n...\n\n$ <span class=\"c1\"># extract features and labels from them:\n</span>$ python -m lachesis.ml.crf dump eng /tmp/ccs/train/ /tmp/ccs/train.pickle\n...\n\n$ <span class=\"c1\"># train the CRF model:\n</span>$ python -m lachesis.ml.crf train eng /tmp/ccs/train.pickle /tmp/ccs/model.crfsuite\n...\n\n$ <span class=\"c1\"># evaluate the model on the training set\n</span>$ python -m lachesis.ml.crf <span class=\"nb\">test</span> eng /tmp/ccs/train.pickle /tmp/ccs/model.crfsuite\n...\n\n$ <span class=\"c1\"># you might want to evaluate on a test set, disjoint from the training set,\n</span>$ <span class=\"c1\"># that is, the test set contains CCs not seen during the training:\n</span>$ ls /tmp/css/test\n<span class=\"m\">1001</span>.ttml\n<span class=\"m\">1002</span>.ttml\n...\n$ python -m lachesis.ml.crf dump eng /tmp/ccs/test/ /tmp/ccs/test.pickle\n$ python -m lachesis.ml.crf <span class=\"nb\">test</span> eng /tmp/ccs/test.pickle /tmp/ccs/model.crfsuite\n...\n$ <span class=\"c1\"># now you can build a CRFSplitter\n</span>$ <span class=\"c1\"># with model_file_path=\"/tmp/ccs/model.crfsuite\" as shown above</span>\n</pre>\n<p>TODO: decide and document where pre-trained model files can be\ndownloaded</p>\n</div>\n</div>\n<div id=\"license\">\n<h2>License</h2>\n<p><strong>lachesis</strong> is released under the terms of the GNU Affero General\nPublic License Version 3. See the <a href=\"LICENSE\" rel=\"nofollow\">LICENSE</a> file for\ndetails.</p>\n</div>\n\n          </div>"}, "last_serial": 2600532, "releases": {"0.0.1.0": [{"comment_text": "", "digests": {"md5": "dd3c0fcf47181bfbb752245e3d6b3c92", "sha256": "7fba2dad4ac30f2909d3d76e2ae7e932bf258dd6b0771663b05ee6bc793644a4"}, "downloads": -1, "filename": "lachesis-0.0.1.0.tar.gz", "has_sig": false, "md5_digest": "dd3c0fcf47181bfbb752245e3d6b3c92", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32962, "upload_time": "2017-01-18T14:19:29", "upload_time_iso_8601": "2017-01-18T14:19:29.987073Z", "url": "https://files.pythonhosted.org/packages/58/20/96cd95cfc072164e96bd9d97411ae0e621fc2df3de4458425a5a0180097d/lachesis-0.0.1.0.tar.gz", "yanked": false}], "0.0.3.0": [{"comment_text": "", "digests": {"md5": "f9e332a6964f8cf4b5fe055ade146ad5", "sha256": "c1b9d6f6f6dd96582dc1e35cf3c59ed63dfd5223719ccdaed600eb6a311fcb39"}, "downloads": -1, "filename": "lachesis-0.0.3.0.tar.gz", "has_sig": false, "md5_digest": "f9e332a6964f8cf4b5fe055ade146ad5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 48202, "upload_time": "2017-01-26T21:14:44", "upload_time_iso_8601": "2017-01-26T21:14:44.656722Z", "url": "https://files.pythonhosted.org/packages/93/db/79b429b28a32f2485faf17f6adfdfce042ead6f305c02b4378644baece14/lachesis-0.0.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f9e332a6964f8cf4b5fe055ade146ad5", "sha256": "c1b9d6f6f6dd96582dc1e35cf3c59ed63dfd5223719ccdaed600eb6a311fcb39"}, "downloads": -1, "filename": "lachesis-0.0.3.0.tar.gz", "has_sig": false, "md5_digest": "f9e332a6964f8cf4b5fe055ade146ad5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 48202, "upload_time": "2017-01-26T21:14:44", "upload_time_iso_8601": "2017-01-26T21:14:44.656722Z", "url": "https://files.pythonhosted.org/packages/93/db/79b429b28a32f2485faf17f6adfdfce042ead6f305c02b4378644baece14/lachesis-0.0.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:48:18 2020"}