{"info": {"author": "Mohamed Fazil R", "author_email": "rmohamedfazil@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "\n.. figure:: https://travis-ci.org/heroku/salesforce-bulk.svg?branch=master\n   :alt: travis-badge\n\nSalesforce Bulk\n===============\n\nPython client library for accessing the asynchronous Salesforce.com Bulk\nAPI.\n\nInstallation\n------------\n.. code-block:: bash\n\n    pip install bulk-support\n\nAuthentication\n--------------\n\nTo access the Bulk API you need to authenticate a user into Salesforce.\nThe easiest way to do this is just to supply ``username``, ``password``\nand ``security_token``. This library will use the ``simple-salesforce``\npackage to handle password based authentication.\n\n.. code-block:: python\n\n    from bulk_support import SalesforceBulk\n\n    bulk = SalesforceBulk(username=username, password=password, security_token=security_token)\n    ...\n\nAlternatively if you run have access to a session ID and instance\\_url\nyou can use those directly:\n\n.. code-block:: python\n\n    from urlparse import urlparse\n    from bulk_support import SalesforceBulk\n\n    bulk = SalesforceBulk(sessionId=sessionId, host=urlparse(instance_url).hostname)\n    ...\n\nOperations\n----------\n\nThe basic sequence for driving the Bulk API is:\n\n1. Create a new job\n2. Add one or more batches to the job\n3. Close the job\n4. Wait for each batch to finish\n\nBulk Query\n----------\n\n``bulk.create_query_job(object_name, contentType='JSON')``\n\nUsing API v45.0 or higher, you can also use the queryAll operation:\n\n``bulk.create_queryall_job(object_name, contentType='JSON')``\n\nExample\n\n.. code-block:: python\n\n    import json\n    from bulk_support.util import IteratorBytesIO\n\n    job = bulk.create_query_job(\"Contact\", contentType='JSON')\n    batch = bulk.query(job, \"select Id,LastName from Contact\")\n    bulk.close_job(job)\n    while not bulk.is_batch_done(batch):\n        sleep(10)\n\n    for result in bulk.get_all_results_for_query_batch(batch):\n        result = json.load(IteratorBytesIO(result))\n        for row in result:\n            print row # dictionary rows\n\nSame example but for CSV:\n\n.. code-block:: python\n\n    import unicodecsv\n\n    job = bulk.create_query_job(\"Contact\", contentType='CSV')\n    batch = bulk.query(job, \"select Id,LastName from Contact\")\n    bulk.close_job(job)\n    while not bulk.is_batch_done(batch):\n        sleep(10)\n\n    for result in bulk.get_all_results_for_query_batch(batch):\n        reader = unicodecsv.DictReader(result, encoding='utf-8')\n        for row in reader:\n            print(row) # dictionary rows\n\nNote that while CSV is the default for historical reasons, JSON should\nbe prefered since CSV has some drawbacks including its handling of NULL\nvs empty string.\n\nPK Chunk Header\n^^^^^^^^^^^^^^^\n\nIf you are querying a large number of records you probably want to turn on `PK Chunking\n<https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/async_api_headers_enable_pk_chunking.htm>`_:\n\n``bulk.create_query_job(object_name, contentType='CSV', pk_chunking=True)``\n\nThat will use the default setting for chunk size. You can use a different chunk size by providing a\nnumber of records per chunk:\n\n``bulk.create_query_job(object_name, contentType='CSV', pk_chunking=100000)``\n\nAdditionally if you want to do something more sophisticated you can provide a header value:\n\n``bulk.create_query_job(object_name, contentType='CSV', pk_chunking='chunkSize=50000; startRow=00130000000xEftMGH')``\n\nBulk Insert, Update, Delete\n---------------------------\n\nAll Bulk upload operations work the same. You set the operation when you\ncreate the job. Then you submit one or more documents that specify\nrecords with columns to insert/update/delete. When deleting you should\nonly submit the Id for each record.\n\nFor efficiency you should use the ``post_batch`` method to post each\nbatch of data. (Note that a batch can have a maximum 10,000 records and\nbe 1GB in size.) You pass a generator or iterator into this function and\nit will stream data via POST to Salesforce. For help sending CSV\nformatted data you can use the salesforce\\_bulk.CsvDictsAdapter class.\nIt takes an iterator returning dictionaries and returns an iterator\nwhich produces CSV data.\n\nFull example:\n\n.. code-block:: python\n\n    from bulk_support import CsvDictsAdapter\n\n    job = bulk.create_insert_job(\"Account\", contentType='CSV')\n    accounts = [dict(Name=\"Account%d\" % idx) for idx in xrange(5)]\n    csv_iter = CsvDictsAdapter(iter(accounts))\n    batch = bulk.post_batch(job, csv_iter)\n    bulk.wait_for_batch(job, batch)\n    bulk.close_job(job)\n    print(\"Done. Accounts uploaded.\")\n\nConcurrency mode\n^^^^^^^^^^^^^^^^\n\nWhen creating the job, pass ``concurrency='Serial'`` or\n``concurrency='Parallel'`` to set the concurrency mode for the job.\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/rmohamedfazil/sfdc-bulk", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "bulk-support", "package_url": "https://pypi.org/project/bulk-support/", "platform": "", "project_url": "https://pypi.org/project/bulk-support/", "project_urls": {"Homepage": "https://github.com/rmohamedfazil/sfdc-bulk"}, "release_url": "https://pypi.org/project/bulk-support/0.3/", "requires_dist": ["six", "requests (>=2.2.1)", "unicodecsv (>=0.14.1)", "simple-salesforce (>=0.69)"], "requires_python": "", "summary": "Python interface to the Salesforce.com Bulk API.", "version": "0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div>\n<img alt=\"travis-badge\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8949a69e513d3b4322805683f296138ba8b11108/68747470733a2f2f7472617669732d63692e6f72672f6865726f6b752f73616c6573666f7263652d62756c6b2e7376673f6272616e63683d6d6173746572\">\n</div>\n<div id=\"salesforce-bulk\">\n<h2>Salesforce Bulk</h2>\n<p>Python client library for accessing the asynchronous Salesforce.com Bulk\nAPI.</p>\n<div id=\"installation\">\n<h3>Installation</h3>\n<pre>pip install bulk-support\n</pre>\n</div>\n<div id=\"authentication\">\n<h3>Authentication</h3>\n<p>To access the Bulk API you need to authenticate a user into Salesforce.\nThe easiest way to do this is just to supply <tt>username</tt>, <tt>password</tt>\nand <tt>security_token</tt>. This library will use the <tt><span class=\"pre\">simple-salesforce</span></tt>\npackage to handle password based authentication.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">bulk_support</span> <span class=\"kn\">import</span> <span class=\"n\">SalesforceBulk</span>\n\n<span class=\"n\">bulk</span> <span class=\"o\">=</span> <span class=\"n\">SalesforceBulk</span><span class=\"p\">(</span><span class=\"n\">username</span><span class=\"o\">=</span><span class=\"n\">username</span><span class=\"p\">,</span> <span class=\"n\">password</span><span class=\"o\">=</span><span class=\"n\">password</span><span class=\"p\">,</span> <span class=\"n\">security_token</span><span class=\"o\">=</span><span class=\"n\">security_token</span><span class=\"p\">)</span>\n<span class=\"o\">...</span>\n</pre>\n<p>Alternatively if you run have access to a session ID and instance_url\nyou can use those directly:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">urlparse</span> <span class=\"kn\">import</span> <span class=\"n\">urlparse</span>\n<span class=\"kn\">from</span> <span class=\"nn\">bulk_support</span> <span class=\"kn\">import</span> <span class=\"n\">SalesforceBulk</span>\n\n<span class=\"n\">bulk</span> <span class=\"o\">=</span> <span class=\"n\">SalesforceBulk</span><span class=\"p\">(</span><span class=\"n\">sessionId</span><span class=\"o\">=</span><span class=\"n\">sessionId</span><span class=\"p\">,</span> <span class=\"n\">host</span><span class=\"o\">=</span><span class=\"n\">urlparse</span><span class=\"p\">(</span><span class=\"n\">instance_url</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">hostname</span><span class=\"p\">)</span>\n<span class=\"o\">...</span>\n</pre>\n</div>\n<div id=\"operations\">\n<h3>Operations</h3>\n<p>The basic sequence for driving the Bulk API is:</p>\n<ol>\n<li>Create a new job</li>\n<li>Add one or more batches to the job</li>\n<li>Close the job</li>\n<li>Wait for each batch to finish</li>\n</ol>\n</div>\n<div id=\"bulk-query\">\n<h3>Bulk Query</h3>\n<p><tt>bulk.create_query_job(object_name, <span class=\"pre\">contentType='JSON')</span></tt></p>\n<p>Using API v45.0 or higher, you can also use the queryAll operation:</p>\n<p><tt>bulk.create_queryall_job(object_name, <span class=\"pre\">contentType='JSON')</span></tt></p>\n<p>Example</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"kn\">from</span> <span class=\"nn\">bulk_support.util</span> <span class=\"kn\">import</span> <span class=\"n\">IteratorBytesIO</span>\n\n<span class=\"n\">job</span> <span class=\"o\">=</span> <span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">create_query_job</span><span class=\"p\">(</span><span class=\"s2\">\"Contact\"</span><span class=\"p\">,</span> <span class=\"n\">contentType</span><span class=\"o\">=</span><span class=\"s1\">'JSON'</span><span class=\"p\">)</span>\n<span class=\"n\">batch</span> <span class=\"o\">=</span> <span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">query</span><span class=\"p\">(</span><span class=\"n\">job</span><span class=\"p\">,</span> <span class=\"s2\">\"select Id,LastName from Contact\"</span><span class=\"p\">)</span>\n<span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">close_job</span><span class=\"p\">(</span><span class=\"n\">job</span><span class=\"p\">)</span>\n<span class=\"k\">while</span> <span class=\"ow\">not</span> <span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">is_batch_done</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">):</span>\n    <span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">result</span> <span class=\"ow\">in</span> <span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">get_all_results_for_query_batch</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">):</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">IteratorBytesIO</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">))</span>\n    <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">result</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span> <span class=\"n\">row</span> <span class=\"c1\"># dictionary rows</span>\n</pre>\n<p>Same example but for CSV:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">unicodecsv</span>\n\n<span class=\"n\">job</span> <span class=\"o\">=</span> <span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">create_query_job</span><span class=\"p\">(</span><span class=\"s2\">\"Contact\"</span><span class=\"p\">,</span> <span class=\"n\">contentType</span><span class=\"o\">=</span><span class=\"s1\">'CSV'</span><span class=\"p\">)</span>\n<span class=\"n\">batch</span> <span class=\"o\">=</span> <span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">query</span><span class=\"p\">(</span><span class=\"n\">job</span><span class=\"p\">,</span> <span class=\"s2\">\"select Id,LastName from Contact\"</span><span class=\"p\">)</span>\n<span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">close_job</span><span class=\"p\">(</span><span class=\"n\">job</span><span class=\"p\">)</span>\n<span class=\"k\">while</span> <span class=\"ow\">not</span> <span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">is_batch_done</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">):</span>\n    <span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">result</span> <span class=\"ow\">in</span> <span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">get_all_results_for_query_batch</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">):</span>\n    <span class=\"n\">reader</span> <span class=\"o\">=</span> <span class=\"n\">unicodecsv</span><span class=\"o\">.</span><span class=\"n\">DictReader</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s1\">'utf-8'</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">reader</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">row</span><span class=\"p\">)</span> <span class=\"c1\"># dictionary rows</span>\n</pre>\n<p>Note that while CSV is the default for historical reasons, JSON should\nbe prefered since CSV has some drawbacks including its handling of NULL\nvs empty string.</p>\n<div id=\"pk-chunk-header\">\n<h4>PK Chunk Header</h4>\n<p>If you are querying a large number of records you probably want to turn on <a href=\"https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/async_api_headers_enable_pk_chunking.htm\" rel=\"nofollow\">PK Chunking</a>:</p>\n<p><tt>bulk.create_query_job(object_name, <span class=\"pre\">contentType='CSV',</span> pk_chunking=True)</tt></p>\n<p>That will use the default setting for chunk size. You can use a different chunk size by providing a\nnumber of records per chunk:</p>\n<p><tt>bulk.create_query_job(object_name, <span class=\"pre\">contentType='CSV',</span> pk_chunking=100000)</tt></p>\n<p>Additionally if you want to do something more sophisticated you can provide a header value:</p>\n<p><tt>bulk.create_query_job(object_name, <span class=\"pre\">contentType='CSV',</span> <span class=\"pre\">pk_chunking='chunkSize=50000;</span> startRow=00130000000xEftMGH')</tt></p>\n</div>\n</div>\n<div id=\"bulk-insert-update-delete\">\n<h3>Bulk Insert, Update, Delete</h3>\n<p>All Bulk upload operations work the same. You set the operation when you\ncreate the job. Then you submit one or more documents that specify\nrecords with columns to insert/update/delete. When deleting you should\nonly submit the Id for each record.</p>\n<p>For efficiency you should use the <tt>post_batch</tt> method to post each\nbatch of data. (Note that a batch can have a maximum 10,000 records and\nbe 1GB in size.) You pass a generator or iterator into this function and\nit will stream data via POST to Salesforce. For help sending CSV\nformatted data you can use the salesforce_bulk.CsvDictsAdapter class.\nIt takes an iterator returning dictionaries and returns an iterator\nwhich produces CSV data.</p>\n<p>Full example:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">bulk_support</span> <span class=\"kn\">import</span> <span class=\"n\">CsvDictsAdapter</span>\n\n<span class=\"n\">job</span> <span class=\"o\">=</span> <span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">create_insert_job</span><span class=\"p\">(</span><span class=\"s2\">\"Account\"</span><span class=\"p\">,</span> <span class=\"n\">contentType</span><span class=\"o\">=</span><span class=\"s1\">'CSV'</span><span class=\"p\">)</span>\n<span class=\"n\">accounts</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">Name</span><span class=\"o\">=</span><span class=\"s2\">\"Account</span><span class=\"si\">%d</span><span class=\"s2\">\"</span> <span class=\"o\">%</span> <span class=\"n\">idx</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">idx</span> <span class=\"ow\">in</span> <span class=\"n\">xrange</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">)]</span>\n<span class=\"n\">csv_iter</span> <span class=\"o\">=</span> <span class=\"n\">CsvDictsAdapter</span><span class=\"p\">(</span><span class=\"nb\">iter</span><span class=\"p\">(</span><span class=\"n\">accounts</span><span class=\"p\">))</span>\n<span class=\"n\">batch</span> <span class=\"o\">=</span> <span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">post_batch</span><span class=\"p\">(</span><span class=\"n\">job</span><span class=\"p\">,</span> <span class=\"n\">csv_iter</span><span class=\"p\">)</span>\n<span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">wait_for_batch</span><span class=\"p\">(</span><span class=\"n\">job</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">)</span>\n<span class=\"n\">bulk</span><span class=\"o\">.</span><span class=\"n\">close_job</span><span class=\"p\">(</span><span class=\"n\">job</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Done. Accounts uploaded.\"</span><span class=\"p\">)</span>\n</pre>\n<div id=\"concurrency-mode\">\n<h4>Concurrency mode</h4>\n<p>When creating the job, pass <tt><span class=\"pre\">concurrency='Serial'</span></tt> or\n<tt><span class=\"pre\">concurrency='Parallel'</span></tt> to set the concurrency mode for the job.</p>\n</div>\n</div>\n</div>\n\n          </div>"}, "last_serial": 5338341, "releases": {"0.2": [{"comment_text": "", "digests": {"md5": "3866e8441d9a3436818f991bf1597f46", "sha256": "40df91ec56d9659db168188c80b7d62616a03166081070d76d69fa293f3bd8c9"}, "downloads": -1, "filename": "bulk_support-0.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "3866e8441d9a3436818f991bf1597f46", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 10312, "upload_time": "2019-05-30T15:39:05", "upload_time_iso_8601": "2019-05-30T15:39:05.872393Z", "url": "https://files.pythonhosted.org/packages/11/58/06a47070962dcea02a5b4675070fe44a97107913dacb896aaec1ab0e0c78/bulk_support-0.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6f7c438eecc20d7e15b8b777221bb1ea", "sha256": "fd56329d93816f82fdc4faff33dd21f0020e7591072b61eca3f4793230b38e80"}, "downloads": -1, "filename": "bulk_support-0.2.tar.gz", "has_sig": false, "md5_digest": "6f7c438eecc20d7e15b8b777221bb1ea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11929, "upload_time": "2019-05-30T15:39:08", "upload_time_iso_8601": "2019-05-30T15:39:08.394299Z", "url": "https://files.pythonhosted.org/packages/cb/6f/390cb87a7d147c6d9bfa2be2958a02822534b4df9ed8c40e8bf187cd1f18/bulk_support-0.2.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "cda65d33cd72e3cfd8beaffc9ef2ebb7", "sha256": "f6c8d37f63779ce994c027ace6c2fbe71c1e3f9dffc3a22a6f97f3ecf08792ed"}, "downloads": -1, "filename": "bulk_support-0.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "cda65d33cd72e3cfd8beaffc9ef2ebb7", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 10315, "upload_time": "2019-05-30T16:09:11", "upload_time_iso_8601": "2019-05-30T16:09:11.902887Z", "url": "https://files.pythonhosted.org/packages/a2/9a/17c020eba779e69273af914a7e12b9c19e01af68d3e7022c3758f88d856c/bulk_support-0.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ae6fca0e2fa9837b2edd31c8119d625f", "sha256": "8c8cf2d25445a830987d90549fc7ffe148b4f676c43af7e71e62ff76a559ec89"}, "downloads": -1, "filename": "bulk_support-0.3.tar.gz", "has_sig": false, "md5_digest": "ae6fca0e2fa9837b2edd31c8119d625f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11942, "upload_time": "2019-05-30T16:09:14", "upload_time_iso_8601": "2019-05-30T16:09:14.585939Z", "url": "https://files.pythonhosted.org/packages/26/dd/6f12a97a4a03abe53b0cc42b7028d7a6f5e318c44347d8e10b7de0e2d9da/bulk_support-0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "cda65d33cd72e3cfd8beaffc9ef2ebb7", "sha256": "f6c8d37f63779ce994c027ace6c2fbe71c1e3f9dffc3a22a6f97f3ecf08792ed"}, "downloads": -1, "filename": "bulk_support-0.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "cda65d33cd72e3cfd8beaffc9ef2ebb7", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 10315, "upload_time": "2019-05-30T16:09:11", "upload_time_iso_8601": "2019-05-30T16:09:11.902887Z", "url": "https://files.pythonhosted.org/packages/a2/9a/17c020eba779e69273af914a7e12b9c19e01af68d3e7022c3758f88d856c/bulk_support-0.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ae6fca0e2fa9837b2edd31c8119d625f", "sha256": "8c8cf2d25445a830987d90549fc7ffe148b4f676c43af7e71e62ff76a559ec89"}, "downloads": -1, "filename": "bulk_support-0.3.tar.gz", "has_sig": false, "md5_digest": "ae6fca0e2fa9837b2edd31c8119d625f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11942, "upload_time": "2019-05-30T16:09:14", "upload_time_iso_8601": "2019-05-30T16:09:14.585939Z", "url": "https://files.pythonhosted.org/packages/26/dd/6f12a97a4a03abe53b0cc42b7028d7a6f5e318c44347d8e10b7de0e2d9da/bulk_support-0.3.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:36:00 2020"}