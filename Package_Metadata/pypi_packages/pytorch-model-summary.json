{"info": {"author": "Alison Marczewski", "author_email": "alison.marczewski@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "## Pytorch Model Summary\n\nIt is a Keras style model.summary() implementation for PyTorch\n\nThis is an Improved PyTorch library of [modelsummary](https://github.com/graykode/modelsummary). Like in `modelsummary`, **It does not care with number of Input parameter!**\n\n### Improvements:\n- For user defined pytorch layers, now `summary` can show layers inside it\n    - some assumptions: when is an user defined layer, if any weight/params/bias is trainable, then it is assumed that this layer is trainable (but only trainable params are counted in Tr. Params #)\n- Adding column counting only trainable parameters (it makes sense when there are user defined layers)\n- Showing all input/output shapes, instead of showing only the first one\n    - example: LSTM layer return a Tensor and a tuple (Tensor, Tensor), then output_shape has three set of values\n- Printing: table width defined dynamically\n- Adding option to add hierarchical summary in output\n- Adding batch_size value (when provided) in table footer\n- fix bugs\n\n### Parameters\n**Default values have keras behavior**\n```python\nsummary(model, *inputs, batch_size=-1, show_input=False, show_hierarchical=False,\n        print_summary=False, max_depth=1, show_parent_layers=False):\n```\n\n- `model`: pytorch model object\n- `*inputs`: ...\n- `batch_size`: if provided, it is printed in summary table\n- `show_input`: show input shape. Otherwise, output shape for each layer. **(Default: False)**\n- `show_hierarchical`: in addition of summary table, return hierarchical view of the model **(Default: False)**\n- `print_summary`: when true, is not required to use print function outside `summary` method **(Default: False)**\n- `max_depth`: it specifies how many times it can go inside user defined layers to show them **(Default: 1)**\n- `show_parent_layer`: it adds a column to show parent layers path until reaching current layer in depth. **(Default: False)**\n\n\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pytorch_model_summary import summary\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\n# show input shape\nprint(summary(Net(), torch.zeros((1, 1, 28, 28)), show_input=True))\n\n# show output shape\nprint(summary(Net(), torch.zeros((1, 1, 28, 28)), show_input=False))\n\n# show output shape and hierarchical view of net\nprint(summary(Net(), torch.zeros((1, 1, 28, 28)), show_input=False, show_hierarchical=True))\n\n```\n\n```\n-----------------------------------------------------------------------\n      Layer (type)         Input Shape         Param #     Tr. Param #\n=======================================================================\n          Conv2d-1      [1, 1, 28, 28]             260             260\n          Conv2d-2     [1, 10, 12, 12]           5,020           5,020\n       Dropout2d-3       [1, 20, 8, 8]               0               0\n          Linear-4            [1, 320]          16,050          16,050\n          Linear-5             [1, 50]             510             510\n=======================================================================\nTotal params: 21,840\nTrainable params: 21,840\nNon-trainable params: 0\n-----------------------------------------------------------------------\n```\n```\n-----------------------------------------------------------------------\n      Layer (type)        Output Shape         Param #     Tr. Param #\n=======================================================================\n          Conv2d-1     [1, 10, 24, 24]             260             260\n          Conv2d-2       [1, 20, 8, 8]           5,020           5,020\n       Dropout2d-3       [1, 20, 8, 8]               0               0\n          Linear-4             [1, 50]          16,050          16,050\n          Linear-5             [1, 10]             510             510\n=======================================================================\nTotal params: 21,840\nTrainable params: 21,840\nNon-trainable params: 0\n-----------------------------------------------------------------------\n```\n```\n-----------------------------------------------------------------------\n      Layer (type)        Output Shape         Param #     Tr. Param #\n=======================================================================\n          Conv2d-1     [1, 10, 24, 24]             260             260\n          Conv2d-2       [1, 20, 8, 8]           5,020           5,020\n       Dropout2d-3       [1, 20, 8, 8]               0               0\n          Linear-4             [1, 50]          16,050          16,050\n          Linear-5             [1, 10]             510             510\n=======================================================================\nTotal params: 21,840\nTrainable params: 21,840\nNon-trainable params: 0\n-----------------------------------------------------------------------\n=========================== Hierarchical Summary ===========================\nNet(\n  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1)), 260 params\n  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1)), 5,020 params\n  (conv2_drop): Dropout2d(p=0.5), 0 params\n  (fc1): Linear(in_features=320, out_features=50, bias=True), 16,050 params\n  (fc2): Linear(in_features=50, out_features=10, bias=True), 510 params\n), 21,840 params\n============================================================================\n```\n\n\n## Quick Start \n\nJust download with **pip**\n\n`pip install pytorch-model-summary` and\n```python\nfrom pytorch_model_summary import summary\n``` \nor \n```python\nimport pytorch_model_summary as pms\npms.summary([params])\n```\nto avoid reference conflicts with other methods in your code\n\nYou can use this library like this. If you want to see more detail, Please see examples below.\n\n## Examples using different set of parameters\n\nRun example using Transformer Model in [Attention is all you need paper(2017)](https://arxiv.org/abs/1706.03762)\n\n1) showing **input shape**\n```python\n# show input shape\npms.summary(model, enc_inputs, dec_inputs, show_input=True, print_summary=True)\n```\n```\n-----------------------------------------------------------------------------------\n      Layer (type)                     Input Shape         Param #     Tr. Param #\n===================================================================================\n         Encoder-1                          [1, 5]      17,332,224      17,329,152\n         Decoder-2     [1, 5], [1, 5], [1, 5, 512]      22,060,544      22,057,472\n          Linear-3                     [1, 5, 512]           3,584           3,584\n===================================================================================\nTotal params: 39,396,352\nTrainable params: 39,390,208\nNon-trainable params: 6,144\n-----------------------------------------------------------------------------------\n```\n\n2) showing **output shape** and **batch_size** in table. In addition, also **hierarchical summary** version\n```python\n# show output shape and batch_size in table. In addition, also hierarchical summary version\npms.summary(model, enc_inputs, dec_inputs, batch_size=1, show_hierarchical=True, print_summary=True)\n```\n```\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n      Layer (type)                                                                                                                                                                            Output Shape         Param #     Tr. Param #\n===========================================================================================================================================================================================================================================\n         Encoder-1                                                                                         [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5]      17,332,224      17,329,152\n         Decoder-2     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5]      22,060,544      22,057,472\n          Linear-3                                                                                                                                                                               [1, 5, 7]           3,584           3,584\n===========================================================================================================================================================================================================================================\nTotal params: 39,396,352\nTrainable params: 39,390,208\nNon-trainable params: 6,144\nBatch size: 1\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n================================ Hierarchical Summary ================================\n\nTransformer(\n  (encoder): Encoder(\n    (src_emb): Embedding(6, 512), 3,072 params\n    (pos_emb): Embedding(6, 512), 3,072 params\n    (layers): ModuleList(\n      (0): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 2,887,680 params\n      (1): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 2,887,680 params\n      (2): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 2,887,680 params\n      (3): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 2,887,680 params\n      (4): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 2,887,680 params\n      (5): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 2,887,680 params\n    ), 17,326,080 params\n  ), 17,332,224 params\n  (decoder): Decoder(\n    (tgt_emb): Embedding(7, 512), 3,584 params\n    (pos_emb): Embedding(6, 512), 3,072 params\n    (layers): ModuleList(\n      (0): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (dec_enc_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 3,675,648 params\n      (1): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (dec_enc_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 3,675,648 params\n      (2): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (dec_enc_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 3,675,648 params\n      (3): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (dec_enc_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 3,675,648 params\n      (4): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (dec_enc_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 3,675,648 params\n      (5): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (dec_enc_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 3,675,648 params\n    ), 22,053,888 params\n  ), 22,060,544 params\n  (projection): Linear(in_features=512, out_features=7, bias=False), 3,584 params\n), 39,396,352 params\n\n\n======================================================================================\n```\n\n3) showing **layers until depth 2**\n```python\n# show layers until depth 2\npms.summary(model, enc_inputs, dec_inputs, max_depth=2, print_summary=True)\n```\n```\n-----------------------------------------------------------------------------------------------\n      Layer (type)                                Output Shape         Param #     Tr. Param #\n===============================================================================================\n       Embedding-1                                 [1, 5, 512]           3,072           3,072\n       Embedding-2                                 [1, 5, 512]           3,072               0\n    EncoderLayer-3                   [1, 5, 512], [1, 8, 5, 5]       2,887,680       2,887,680\n    EncoderLayer-4                   [1, 5, 512], [1, 8, 5, 5]       2,887,680       2,887,680\n    EncoderLayer-5                   [1, 5, 512], [1, 8, 5, 5]       2,887,680       2,887,680\n    EncoderLayer-6                   [1, 5, 512], [1, 8, 5, 5]       2,887,680       2,887,680\n    EncoderLayer-7                   [1, 5, 512], [1, 8, 5, 5]       2,887,680       2,887,680\n    EncoderLayer-8                   [1, 5, 512], [1, 8, 5, 5]       2,887,680       2,887,680\n       Embedding-9                                 [1, 5, 512]           3,584           3,584\n      Embedding-10                                 [1, 5, 512]           3,072               0\n   DecoderLayer-11     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5]       3,675,648       3,675,648\n   DecoderLayer-12     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5]       3,675,648       3,675,648\n   DecoderLayer-13     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5]       3,675,648       3,675,648\n   DecoderLayer-14     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5]       3,675,648       3,675,648\n   DecoderLayer-15     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5]       3,675,648       3,675,648\n   DecoderLayer-16     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5]       3,675,648       3,675,648\n         Linear-17                                   [1, 5, 7]           3,584           3,584\n===============================================================================================\nTotal params: 39,396,352\nTrainable params: 39,390,208\nNon-trainable params: 6,144\n-----------------------------------------------------------------------------------------------\n```\n\n4) showing **deepest layers**\n```python\n# show deepest layers\npms.summary(model, enc_inputs, dec_inputs, max_depth=None, print_summary=True)\n```\n```\n-----------------------------------------------------------------------\n      Layer (type)        Output Shape         Param #     Tr. Param #\n=======================================================================\n       Embedding-1         [1, 5, 512]           3,072           3,072\n       Embedding-2         [1, 5, 512]           3,072               0\n          Linear-3         [1, 5, 512]         262,656         262,656\n          Linear-4         [1, 5, 512]         262,656         262,656\n          Linear-5         [1, 5, 512]         262,656         262,656\n          Conv1d-6        [1, 2048, 5]       1,050,624       1,050,624\n          Conv1d-7         [1, 512, 5]       1,049,088       1,049,088\n          Linear-8         [1, 5, 512]         262,656         262,656\n          Linear-9         [1, 5, 512]         262,656         262,656\n         Linear-10         [1, 5, 512]         262,656         262,656\n         Conv1d-11        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-12         [1, 512, 5]       1,049,088       1,049,088\n         Linear-13         [1, 5, 512]         262,656         262,656\n         Linear-14         [1, 5, 512]         262,656         262,656\n         Linear-15         [1, 5, 512]         262,656         262,656\n         Conv1d-16        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-17         [1, 512, 5]       1,049,088       1,049,088\n         Linear-18         [1, 5, 512]         262,656         262,656\n         Linear-19         [1, 5, 512]         262,656         262,656\n         Linear-20         [1, 5, 512]         262,656         262,656\n         Conv1d-21        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-22         [1, 512, 5]       1,049,088       1,049,088\n         Linear-23         [1, 5, 512]         262,656         262,656\n         Linear-24         [1, 5, 512]         262,656         262,656\n         Linear-25         [1, 5, 512]         262,656         262,656\n         Conv1d-26        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-27         [1, 512, 5]       1,049,088       1,049,088\n         Linear-28         [1, 5, 512]         262,656         262,656\n         Linear-29         [1, 5, 512]         262,656         262,656\n         Linear-30         [1, 5, 512]         262,656         262,656\n         Conv1d-31        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-32         [1, 512, 5]       1,049,088       1,049,088\n      Embedding-33         [1, 5, 512]           3,584           3,584\n      Embedding-34         [1, 5, 512]           3,072               0\n         Linear-35         [1, 5, 512]         262,656         262,656\n         Linear-36         [1, 5, 512]         262,656         262,656\n         Linear-37         [1, 5, 512]         262,656         262,656\n         Linear-38         [1, 5, 512]         262,656         262,656\n         Linear-39         [1, 5, 512]         262,656         262,656\n         Linear-40         [1, 5, 512]         262,656         262,656\n         Conv1d-41        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-42         [1, 512, 5]       1,049,088       1,049,088\n         Linear-43         [1, 5, 512]         262,656         262,656\n         Linear-44         [1, 5, 512]         262,656         262,656\n         Linear-45         [1, 5, 512]         262,656         262,656\n         Linear-46         [1, 5, 512]         262,656         262,656\n         Linear-47         [1, 5, 512]         262,656         262,656\n         Linear-48         [1, 5, 512]         262,656         262,656\n         Conv1d-49        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-50         [1, 512, 5]       1,049,088       1,049,088\n         Linear-51         [1, 5, 512]         262,656         262,656\n         Linear-52         [1, 5, 512]         262,656         262,656\n         Linear-53         [1, 5, 512]         262,656         262,656\n         Linear-54         [1, 5, 512]         262,656         262,656\n         Linear-55         [1, 5, 512]         262,656         262,656\n         Linear-56         [1, 5, 512]         262,656         262,656\n         Conv1d-57        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-58         [1, 512, 5]       1,049,088       1,049,088\n         Linear-59         [1, 5, 512]         262,656         262,656\n         Linear-60         [1, 5, 512]         262,656         262,656\n         Linear-61         [1, 5, 512]         262,656         262,656\n         Linear-62         [1, 5, 512]         262,656         262,656\n         Linear-63         [1, 5, 512]         262,656         262,656\n         Linear-64         [1, 5, 512]         262,656         262,656\n         Conv1d-65        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-66         [1, 512, 5]       1,049,088       1,049,088\n         Linear-67         [1, 5, 512]         262,656         262,656\n         Linear-68         [1, 5, 512]         262,656         262,656\n         Linear-69         [1, 5, 512]         262,656         262,656\n         Linear-70         [1, 5, 512]         262,656         262,656\n         Linear-71         [1, 5, 512]         262,656         262,656\n         Linear-72         [1, 5, 512]         262,656         262,656\n         Conv1d-73        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-74         [1, 512, 5]       1,049,088       1,049,088\n         Linear-75         [1, 5, 512]         262,656         262,656\n         Linear-76         [1, 5, 512]         262,656         262,656\n         Linear-77         [1, 5, 512]         262,656         262,656\n         Linear-78         [1, 5, 512]         262,656         262,656\n         Linear-79         [1, 5, 512]         262,656         262,656\n         Linear-80         [1, 5, 512]         262,656         262,656\n         Conv1d-81        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-82         [1, 512, 5]       1,049,088       1,049,088\n         Linear-83           [1, 5, 7]           3,584           3,584\n=======================================================================\nTotal params: 39,396,352\nTrainable params: 39,390,208\nNon-trainable params: 6,144\n-----------------------------------------------------------------------\n```\n\n5) showing **layers until depth 3** and adding column with **parent layers**\n```python\n# show layers until depth 3 and add column with parent layers\npms.summary(model, enc_inputs, dec_inputs, max_depth=3, show_parent_layers=True, print_summary=True)\n```\n```\n-----------------------------------------------------------------------------------------------------------------------------\n                      Parent Layers                Layer (type)                  Output Shape         Param #     Tr. Param #\n=============================================================================================================================\n                Transformer/Encoder                 Embedding-1                   [1, 5, 512]           3,072           3,072\n                Transformer/Encoder                 Embedding-2                   [1, 5, 512]           3,072               0\n   Transformer/Encoder/EncoderLayer        MultiHeadAttention-3     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Encoder/EncoderLayer     PoswiseFeedForwardNet-4                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Encoder/EncoderLayer        MultiHeadAttention-5     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Encoder/EncoderLayer     PoswiseFeedForwardNet-6                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Encoder/EncoderLayer        MultiHeadAttention-7     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Encoder/EncoderLayer     PoswiseFeedForwardNet-8                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Encoder/EncoderLayer        MultiHeadAttention-9     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Encoder/EncoderLayer    PoswiseFeedForwardNet-10                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Encoder/EncoderLayer       MultiHeadAttention-11     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Encoder/EncoderLayer    PoswiseFeedForwardNet-12                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Encoder/EncoderLayer       MultiHeadAttention-13     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Encoder/EncoderLayer    PoswiseFeedForwardNet-14                   [1, 5, 512]       2,099,712       2,099,712\n                Transformer/Decoder                Embedding-15                   [1, 5, 512]           3,584           3,584\n                Transformer/Decoder                Embedding-16                   [1, 5, 512]           3,072               0\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-17     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-18     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer    PoswiseFeedForwardNet-19                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-20     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-21     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer    PoswiseFeedForwardNet-22                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-23     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-24     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer    PoswiseFeedForwardNet-25                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-26     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-27     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer    PoswiseFeedForwardNet-28                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-29     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-30     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer    PoswiseFeedForwardNet-31                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-32     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-33     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer    PoswiseFeedForwardNet-34                   [1, 5, 512]       2,099,712       2,099,712\n                        Transformer                   Linear-35                     [1, 5, 7]           3,584           3,584\n=============================================================================================================================\nTotal params: 39,396,352\nTrainable params: 39,390,208\nNon-trainable params: 6,144\n-----------------------------------------------------------------------------------------------------------------------------\n```\n\n\n## Reference\n\n```python\ncode_reference = { \t'https://github.com/graykode/modelsummary', \n\t\t\t\t\t'https://github.com/pytorch/pytorch/issues/2001',\n\t\t\t\t\t'https://gist.github.com/HTLife/b6640af9d6e7d765411f8aa9aa94b837',\n\t\t\t\t\t'https://github.com/sksq96/pytorch-summary',\n\t\t\t\t\t'Inspired by https://github.com/sksq96/pytorch-summary'}\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/amarczew/pytorch_model_summary", "keywords": "pytorch model summary model.summary() keras", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "pytorch-model-summary", "package_url": "https://pypi.org/project/pytorch-model-summary/", "platform": "", "project_url": "https://pypi.org/project/pytorch-model-summary/", "project_urls": {"Homepage": "https://github.com/amarczew/pytorch_model_summary"}, "release_url": "https://pypi.org/project/pytorch-model-summary/0.1.1/", "requires_dist": ["tqdm", "torch", "numpy"], "requires_python": ">=3.6", "summary": "It is a Keras style model.summary() implementation for PyTorch", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h2>Pytorch Model Summary</h2>\n<p>It is a Keras style model.summary() implementation for PyTorch</p>\n<p>This is an Improved PyTorch library of <a href=\"https://github.com/graykode/modelsummary\" rel=\"nofollow\">modelsummary</a>. Like in <code>modelsummary</code>, <strong>It does not care with number of Input parameter!</strong></p>\n<h3>Improvements:</h3>\n<ul>\n<li>For user defined pytorch layers, now <code>summary</code> can show layers inside it\n<ul>\n<li>some assumptions: when is an user defined layer, if any weight/params/bias is trainable, then it is assumed that this layer is trainable (but only trainable params are counted in Tr. Params #)</li>\n</ul>\n</li>\n<li>Adding column counting only trainable parameters (it makes sense when there are user defined layers)</li>\n<li>Showing all input/output shapes, instead of showing only the first one\n<ul>\n<li>example: LSTM layer return a Tensor and a tuple (Tensor, Tensor), then output_shape has three set of values</li>\n</ul>\n</li>\n<li>Printing: table width defined dynamically</li>\n<li>Adding option to add hierarchical summary in output</li>\n<li>Adding batch_size value (when provided) in table footer</li>\n<li>fix bugs</li>\n</ul>\n<h3>Parameters</h3>\n<p><strong>Default values have keras behavior</strong></p>\n<pre><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">show_input</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">show_hierarchical</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n        <span class=\"n\">print_summary</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">show_parent_layers</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">):</span>\n</pre>\n<ul>\n<li><code>model</code>: pytorch model object</li>\n<li><code>*inputs</code>: ...</li>\n<li><code>batch_size</code>: if provided, it is printed in summary table</li>\n<li><code>show_input</code>: show input shape. Otherwise, output shape for each layer. <strong>(Default: False)</strong></li>\n<li><code>show_hierarchical</code>: in addition of summary table, return hierarchical view of the model <strong>(Default: False)</strong></li>\n<li><code>print_summary</code>: when true, is not required to use print function outside <code>summary</code> method <strong>(Default: False)</strong></li>\n<li><code>max_depth</code>: it specifies how many times it can go inside user defined layers to show them <strong>(Default: 1)</strong></li>\n<li><code>show_parent_layer</code>: it adds a column to show parent layers path until reaching current layer in depth. <strong>(Default: False)</strong></li>\n</ul>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"nn\">nn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span> <span class=\"k\">as</span> <span class=\"nn\">F</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">pytorch_model_summary</span> <span class=\"kn\">import</span> <span class=\"n\">summary</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">Net</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">Net</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2_drop</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Dropout2d</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">320</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">max_pool2d</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">max_pool2d</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2_drop</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">conv2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)),</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">320</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">dropout</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">training</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">training</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">fc2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">log_softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n\n<span class=\"c1\"># show input shape</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">Net</span><span class=\"p\">(),</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">)),</span> <span class=\"n\">show_input</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># show output shape</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">Net</span><span class=\"p\">(),</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">)),</span> <span class=\"n\">show_input</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># show output shape and hierarchical view of net</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">Net</span><span class=\"p\">(),</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">)),</span> <span class=\"n\">show_input</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">show_hierarchical</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">))</span>\n</pre>\n<pre><code>-----------------------------------------------------------------------\n      Layer (type)         Input Shape         Param #     Tr. Param #\n=======================================================================\n          Conv2d-1      [1, 1, 28, 28]             260             260\n          Conv2d-2     [1, 10, 12, 12]           5,020           5,020\n       Dropout2d-3       [1, 20, 8, 8]               0               0\n          Linear-4            [1, 320]          16,050          16,050\n          Linear-5             [1, 50]             510             510\n=======================================================================\nTotal params: 21,840\nTrainable params: 21,840\nNon-trainable params: 0\n-----------------------------------------------------------------------\n</code></pre>\n<pre><code>-----------------------------------------------------------------------\n      Layer (type)        Output Shape         Param #     Tr. Param #\n=======================================================================\n          Conv2d-1     [1, 10, 24, 24]             260             260\n          Conv2d-2       [1, 20, 8, 8]           5,020           5,020\n       Dropout2d-3       [1, 20, 8, 8]               0               0\n          Linear-4             [1, 50]          16,050          16,050\n          Linear-5             [1, 10]             510             510\n=======================================================================\nTotal params: 21,840\nTrainable params: 21,840\nNon-trainable params: 0\n-----------------------------------------------------------------------\n</code></pre>\n<pre><code>-----------------------------------------------------------------------\n      Layer (type)        Output Shape         Param #     Tr. Param #\n=======================================================================\n          Conv2d-1     [1, 10, 24, 24]             260             260\n          Conv2d-2       [1, 20, 8, 8]           5,020           5,020\n       Dropout2d-3       [1, 20, 8, 8]               0               0\n          Linear-4             [1, 50]          16,050          16,050\n          Linear-5             [1, 10]             510             510\n=======================================================================\nTotal params: 21,840\nTrainable params: 21,840\nNon-trainable params: 0\n-----------------------------------------------------------------------\n=========================== Hierarchical Summary ===========================\nNet(\n  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1)), 260 params\n  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1)), 5,020 params\n  (conv2_drop): Dropout2d(p=0.5), 0 params\n  (fc1): Linear(in_features=320, out_features=50, bias=True), 16,050 params\n  (fc2): Linear(in_features=50, out_features=10, bias=True), 510 params\n), 21,840 params\n============================================================================\n</code></pre>\n<h2>Quick Start</h2>\n<p>Just download with <strong>pip</strong></p>\n<p><code>pip install pytorch-model-summary</code> and</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">pytorch_model_summary</span> <span class=\"kn\">import</span> <span class=\"n\">summary</span>\n</pre>\n<p>or</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">pytorch_model_summary</span> <span class=\"k\">as</span> <span class=\"nn\">pms</span>\n<span class=\"n\">pms</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"p\">([</span><span class=\"n\">params</span><span class=\"p\">])</span>\n</pre>\n<p>to avoid reference conflicts with other methods in your code</p>\n<p>You can use this library like this. If you want to see more detail, Please see examples below.</p>\n<h2>Examples using different set of parameters</h2>\n<p>Run example using Transformer Model in <a href=\"https://arxiv.org/abs/1706.03762\" rel=\"nofollow\">Attention is all you need paper(2017)</a></p>\n<ol>\n<li>showing <strong>input shape</strong></li>\n</ol>\n<pre><span class=\"c1\"># show input shape</span>\n<span class=\"n\">pms</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">enc_inputs</span><span class=\"p\">,</span> <span class=\"n\">dec_inputs</span><span class=\"p\">,</span> <span class=\"n\">show_input</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">print_summary</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<pre><code>-----------------------------------------------------------------------------------\n      Layer (type)                     Input Shape         Param #     Tr. Param #\n===================================================================================\n         Encoder-1                          [1, 5]      17,332,224      17,329,152\n         Decoder-2     [1, 5], [1, 5], [1, 5, 512]      22,060,544      22,057,472\n          Linear-3                     [1, 5, 512]           3,584           3,584\n===================================================================================\nTotal params: 39,396,352\nTrainable params: 39,390,208\nNon-trainable params: 6,144\n-----------------------------------------------------------------------------------\n</code></pre>\n<ol>\n<li>showing <strong>output shape</strong> and <strong>batch_size</strong> in table. In addition, also <strong>hierarchical summary</strong> version</li>\n</ol>\n<pre><span class=\"c1\"># show output shape and batch_size in table. In addition, also hierarchical summary version</span>\n<span class=\"n\">pms</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">enc_inputs</span><span class=\"p\">,</span> <span class=\"n\">dec_inputs</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">show_hierarchical</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">print_summary</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<pre><code>-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n      Layer (type)                                                                                                                                                                            Output Shape         Param #     Tr. Param #\n===========================================================================================================================================================================================================================================\n         Encoder-1                                                                                         [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5]      17,332,224      17,329,152\n         Decoder-2     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5], [1, 8, 5, 5]      22,060,544      22,057,472\n          Linear-3                                                                                                                                                                               [1, 5, 7]           3,584           3,584\n===========================================================================================================================================================================================================================================\nTotal params: 39,396,352\nTrainable params: 39,390,208\nNon-trainable params: 6,144\nBatch size: 1\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n================================ Hierarchical Summary ================================\n\nTransformer(\n  (encoder): Encoder(\n    (src_emb): Embedding(6, 512), 3,072 params\n    (pos_emb): Embedding(6, 512), 3,072 params\n    (layers): ModuleList(\n      (0): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 2,887,680 params\n      (1): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 2,887,680 params\n      (2): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 2,887,680 params\n      (3): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 2,887,680 params\n      (4): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 2,887,680 params\n      (5): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 2,887,680 params\n    ), 17,326,080 params\n  ), 17,332,224 params\n  (decoder): Decoder(\n    (tgt_emb): Embedding(7, 512), 3,584 params\n    (pos_emb): Embedding(6, 512), 3,072 params\n    (layers): ModuleList(\n      (0): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (dec_enc_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 3,675,648 params\n      (1): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (dec_enc_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 3,675,648 params\n      (2): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (dec_enc_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 3,675,648 params\n      (3): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (dec_enc_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 3,675,648 params\n      (4): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (dec_enc_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 3,675,648 params\n      (5): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (dec_enc_attn): MultiHeadAttention(\n          (W_Q): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_K): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n          (W_V): Linear(in_features=512, out_features=512, bias=True), 262,656 params\n        ), 787,968 params\n        (pos_ffn): PoswiseFeedForwardNet(\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,)), 1,050,624 params\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,)), 1,049,088 params\n        ), 2,099,712 params\n      ), 3,675,648 params\n    ), 22,053,888 params\n  ), 22,060,544 params\n  (projection): Linear(in_features=512, out_features=7, bias=False), 3,584 params\n), 39,396,352 params\n\n\n======================================================================================\n</code></pre>\n<ol>\n<li>showing <strong>layers until depth 2</strong></li>\n</ol>\n<pre><span class=\"c1\"># show layers until depth 2</span>\n<span class=\"n\">pms</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">enc_inputs</span><span class=\"p\">,</span> <span class=\"n\">dec_inputs</span><span class=\"p\">,</span> <span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">print_summary</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<pre><code>-----------------------------------------------------------------------------------------------\n      Layer (type)                                Output Shape         Param #     Tr. Param #\n===============================================================================================\n       Embedding-1                                 [1, 5, 512]           3,072           3,072\n       Embedding-2                                 [1, 5, 512]           3,072               0\n    EncoderLayer-3                   [1, 5, 512], [1, 8, 5, 5]       2,887,680       2,887,680\n    EncoderLayer-4                   [1, 5, 512], [1, 8, 5, 5]       2,887,680       2,887,680\n    EncoderLayer-5                   [1, 5, 512], [1, 8, 5, 5]       2,887,680       2,887,680\n    EncoderLayer-6                   [1, 5, 512], [1, 8, 5, 5]       2,887,680       2,887,680\n    EncoderLayer-7                   [1, 5, 512], [1, 8, 5, 5]       2,887,680       2,887,680\n    EncoderLayer-8                   [1, 5, 512], [1, 8, 5, 5]       2,887,680       2,887,680\n       Embedding-9                                 [1, 5, 512]           3,584           3,584\n      Embedding-10                                 [1, 5, 512]           3,072               0\n   DecoderLayer-11     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5]       3,675,648       3,675,648\n   DecoderLayer-12     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5]       3,675,648       3,675,648\n   DecoderLayer-13     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5]       3,675,648       3,675,648\n   DecoderLayer-14     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5]       3,675,648       3,675,648\n   DecoderLayer-15     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5]       3,675,648       3,675,648\n   DecoderLayer-16     [1, 5, 512], [1, 8, 5, 5], [1, 8, 5, 5]       3,675,648       3,675,648\n         Linear-17                                   [1, 5, 7]           3,584           3,584\n===============================================================================================\nTotal params: 39,396,352\nTrainable params: 39,390,208\nNon-trainable params: 6,144\n-----------------------------------------------------------------------------------------------\n</code></pre>\n<ol>\n<li>showing <strong>deepest layers</strong></li>\n</ol>\n<pre><span class=\"c1\"># show deepest layers</span>\n<span class=\"n\">pms</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">enc_inputs</span><span class=\"p\">,</span> <span class=\"n\">dec_inputs</span><span class=\"p\">,</span> <span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">print_summary</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<pre><code>-----------------------------------------------------------------------\n      Layer (type)        Output Shape         Param #     Tr. Param #\n=======================================================================\n       Embedding-1         [1, 5, 512]           3,072           3,072\n       Embedding-2         [1, 5, 512]           3,072               0\n          Linear-3         [1, 5, 512]         262,656         262,656\n          Linear-4         [1, 5, 512]         262,656         262,656\n          Linear-5         [1, 5, 512]         262,656         262,656\n          Conv1d-6        [1, 2048, 5]       1,050,624       1,050,624\n          Conv1d-7         [1, 512, 5]       1,049,088       1,049,088\n          Linear-8         [1, 5, 512]         262,656         262,656\n          Linear-9         [1, 5, 512]         262,656         262,656\n         Linear-10         [1, 5, 512]         262,656         262,656\n         Conv1d-11        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-12         [1, 512, 5]       1,049,088       1,049,088\n         Linear-13         [1, 5, 512]         262,656         262,656\n         Linear-14         [1, 5, 512]         262,656         262,656\n         Linear-15         [1, 5, 512]         262,656         262,656\n         Conv1d-16        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-17         [1, 512, 5]       1,049,088       1,049,088\n         Linear-18         [1, 5, 512]         262,656         262,656\n         Linear-19         [1, 5, 512]         262,656         262,656\n         Linear-20         [1, 5, 512]         262,656         262,656\n         Conv1d-21        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-22         [1, 512, 5]       1,049,088       1,049,088\n         Linear-23         [1, 5, 512]         262,656         262,656\n         Linear-24         [1, 5, 512]         262,656         262,656\n         Linear-25         [1, 5, 512]         262,656         262,656\n         Conv1d-26        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-27         [1, 512, 5]       1,049,088       1,049,088\n         Linear-28         [1, 5, 512]         262,656         262,656\n         Linear-29         [1, 5, 512]         262,656         262,656\n         Linear-30         [1, 5, 512]         262,656         262,656\n         Conv1d-31        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-32         [1, 512, 5]       1,049,088       1,049,088\n      Embedding-33         [1, 5, 512]           3,584           3,584\n      Embedding-34         [1, 5, 512]           3,072               0\n         Linear-35         [1, 5, 512]         262,656         262,656\n         Linear-36         [1, 5, 512]         262,656         262,656\n         Linear-37         [1, 5, 512]         262,656         262,656\n         Linear-38         [1, 5, 512]         262,656         262,656\n         Linear-39         [1, 5, 512]         262,656         262,656\n         Linear-40         [1, 5, 512]         262,656         262,656\n         Conv1d-41        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-42         [1, 512, 5]       1,049,088       1,049,088\n         Linear-43         [1, 5, 512]         262,656         262,656\n         Linear-44         [1, 5, 512]         262,656         262,656\n         Linear-45         [1, 5, 512]         262,656         262,656\n         Linear-46         [1, 5, 512]         262,656         262,656\n         Linear-47         [1, 5, 512]         262,656         262,656\n         Linear-48         [1, 5, 512]         262,656         262,656\n         Conv1d-49        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-50         [1, 512, 5]       1,049,088       1,049,088\n         Linear-51         [1, 5, 512]         262,656         262,656\n         Linear-52         [1, 5, 512]         262,656         262,656\n         Linear-53         [1, 5, 512]         262,656         262,656\n         Linear-54         [1, 5, 512]         262,656         262,656\n         Linear-55         [1, 5, 512]         262,656         262,656\n         Linear-56         [1, 5, 512]         262,656         262,656\n         Conv1d-57        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-58         [1, 512, 5]       1,049,088       1,049,088\n         Linear-59         [1, 5, 512]         262,656         262,656\n         Linear-60         [1, 5, 512]         262,656         262,656\n         Linear-61         [1, 5, 512]         262,656         262,656\n         Linear-62         [1, 5, 512]         262,656         262,656\n         Linear-63         [1, 5, 512]         262,656         262,656\n         Linear-64         [1, 5, 512]         262,656         262,656\n         Conv1d-65        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-66         [1, 512, 5]       1,049,088       1,049,088\n         Linear-67         [1, 5, 512]         262,656         262,656\n         Linear-68         [1, 5, 512]         262,656         262,656\n         Linear-69         [1, 5, 512]         262,656         262,656\n         Linear-70         [1, 5, 512]         262,656         262,656\n         Linear-71         [1, 5, 512]         262,656         262,656\n         Linear-72         [1, 5, 512]         262,656         262,656\n         Conv1d-73        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-74         [1, 512, 5]       1,049,088       1,049,088\n         Linear-75         [1, 5, 512]         262,656         262,656\n         Linear-76         [1, 5, 512]         262,656         262,656\n         Linear-77         [1, 5, 512]         262,656         262,656\n         Linear-78         [1, 5, 512]         262,656         262,656\n         Linear-79         [1, 5, 512]         262,656         262,656\n         Linear-80         [1, 5, 512]         262,656         262,656\n         Conv1d-81        [1, 2048, 5]       1,050,624       1,050,624\n         Conv1d-82         [1, 512, 5]       1,049,088       1,049,088\n         Linear-83           [1, 5, 7]           3,584           3,584\n=======================================================================\nTotal params: 39,396,352\nTrainable params: 39,390,208\nNon-trainable params: 6,144\n-----------------------------------------------------------------------\n</code></pre>\n<ol>\n<li>showing <strong>layers until depth 3</strong> and adding column with <strong>parent layers</strong></li>\n</ol>\n<pre><span class=\"c1\"># show layers until depth 3 and add column with parent layers</span>\n<span class=\"n\">pms</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">enc_inputs</span><span class=\"p\">,</span> <span class=\"n\">dec_inputs</span><span class=\"p\">,</span> <span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">show_parent_layers</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">print_summary</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<pre><code>-----------------------------------------------------------------------------------------------------------------------------\n                      Parent Layers                Layer (type)                  Output Shape         Param #     Tr. Param #\n=============================================================================================================================\n                Transformer/Encoder                 Embedding-1                   [1, 5, 512]           3,072           3,072\n                Transformer/Encoder                 Embedding-2                   [1, 5, 512]           3,072               0\n   Transformer/Encoder/EncoderLayer        MultiHeadAttention-3     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Encoder/EncoderLayer     PoswiseFeedForwardNet-4                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Encoder/EncoderLayer        MultiHeadAttention-5     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Encoder/EncoderLayer     PoswiseFeedForwardNet-6                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Encoder/EncoderLayer        MultiHeadAttention-7     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Encoder/EncoderLayer     PoswiseFeedForwardNet-8                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Encoder/EncoderLayer        MultiHeadAttention-9     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Encoder/EncoderLayer    PoswiseFeedForwardNet-10                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Encoder/EncoderLayer       MultiHeadAttention-11     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Encoder/EncoderLayer    PoswiseFeedForwardNet-12                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Encoder/EncoderLayer       MultiHeadAttention-13     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Encoder/EncoderLayer    PoswiseFeedForwardNet-14                   [1, 5, 512]       2,099,712       2,099,712\n                Transformer/Decoder                Embedding-15                   [1, 5, 512]           3,584           3,584\n                Transformer/Decoder                Embedding-16                   [1, 5, 512]           3,072               0\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-17     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-18     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer    PoswiseFeedForwardNet-19                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-20     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-21     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer    PoswiseFeedForwardNet-22                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-23     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-24     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer    PoswiseFeedForwardNet-25                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-26     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-27     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer    PoswiseFeedForwardNet-28                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-29     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-30     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer    PoswiseFeedForwardNet-31                   [1, 5, 512]       2,099,712       2,099,712\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-32     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer       MultiHeadAttention-33     [1, 5, 512], [1, 8, 5, 5]         787,968         787,968\n   Transformer/Decoder/DecoderLayer    PoswiseFeedForwardNet-34                   [1, 5, 512]       2,099,712       2,099,712\n                        Transformer                   Linear-35                     [1, 5, 7]           3,584           3,584\n=============================================================================================================================\nTotal params: 39,396,352\nTrainable params: 39,390,208\nNon-trainable params: 6,144\n-----------------------------------------------------------------------------------------------------------------------------\n</code></pre>\n<h2>Reference</h2>\n<pre><span class=\"n\">code_reference</span> <span class=\"o\">=</span> <span class=\"p\">{</span> \t<span class=\"s1\">'https://github.com/graykode/modelsummary'</span><span class=\"p\">,</span> \n\t\t\t\t\t<span class=\"s1\">'https://github.com/pytorch/pytorch/issues/2001'</span><span class=\"p\">,</span>\n\t\t\t\t\t<span class=\"s1\">'https://gist.github.com/HTLife/b6640af9d6e7d765411f8aa9aa94b837'</span><span class=\"p\">,</span>\n\t\t\t\t\t<span class=\"s1\">'https://github.com/sksq96/pytorch-summary'</span><span class=\"p\">,</span>\n\t\t\t\t\t<span class=\"s1\">'Inspired by https://github.com/sksq96/pytorch-summary'</span><span class=\"p\">}</span>\n</pre>\n\n          </div>"}, "last_serial": 6355974, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "196ba3a51c10cd6770efa8d2ec07a7d7", "sha256": "859cac3fea6163c0afa6d146638cc0f3702ac45702c709180da10a0bcaa4ad3a"}, "downloads": -1, "filename": "pytorch_model_summary-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "196ba3a51c10cd6770efa8d2ec07a7d7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 9127, "upload_time": "2019-12-23T13:40:11", "upload_time_iso_8601": "2019-12-23T13:40:11.667921Z", "url": "https://files.pythonhosted.org/packages/25/64/8765eba0a082e0808b6d845860f5e3b0c13b1c7ed43cd35a7eecc8a1e14c/pytorch_model_summary-0.1-py3-none-any.whl", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "d577e625abe26fe6fa3053206c7693ff", "sha256": "197831357a78c003d3ed150a46405086f5435dda8a159bffd415fa7275f83df0"}, "downloads": -1, "filename": "pytorch_model_summary-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "d577e625abe26fe6fa3053206c7693ff", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 9187, "upload_time": "2019-12-24T16:26:59", "upload_time_iso_8601": "2019-12-24T16:26:59.386652Z", "url": "https://files.pythonhosted.org/packages/a0/de/f3548f3081045cfc4020fc297cc9db74839a6849da8a41b89c48a3307da7/pytorch_model_summary-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "335dd76c1a58611637664812d814d8a8", "sha256": "a24f2de5d6f08cbf8a19e88a6e80982f904aa27fb791522a9fbec23098dd7027"}, "downloads": -1, "filename": "pytorch_model_summary-0.1.1.tar.gz", "has_sig": false, "md5_digest": "335dd76c1a58611637664812d814d8a8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13925, "upload_time": "2019-12-24T16:27:21", "upload_time_iso_8601": "2019-12-24T16:27:21.412638Z", "url": "https://files.pythonhosted.org/packages/f5/5b/a07a5368f0604c0d2dfda00986d7954112e1976de516bba11404430510b0/pytorch_model_summary-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d577e625abe26fe6fa3053206c7693ff", "sha256": "197831357a78c003d3ed150a46405086f5435dda8a159bffd415fa7275f83df0"}, "downloads": -1, "filename": "pytorch_model_summary-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "d577e625abe26fe6fa3053206c7693ff", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 9187, "upload_time": "2019-12-24T16:26:59", "upload_time_iso_8601": "2019-12-24T16:26:59.386652Z", "url": "https://files.pythonhosted.org/packages/a0/de/f3548f3081045cfc4020fc297cc9db74839a6849da8a41b89c48a3307da7/pytorch_model_summary-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "335dd76c1a58611637664812d814d8a8", "sha256": "a24f2de5d6f08cbf8a19e88a6e80982f904aa27fb791522a9fbec23098dd7027"}, "downloads": -1, "filename": "pytorch_model_summary-0.1.1.tar.gz", "has_sig": false, "md5_digest": "335dd76c1a58611637664812d814d8a8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13925, "upload_time": "2019-12-24T16:27:21", "upload_time_iso_8601": "2019-12-24T16:27:21.412638Z", "url": "https://files.pythonhosted.org/packages/f5/5b/a07a5368f0604c0d2dfda00986d7954112e1976de516bba11404430510b0/pytorch_model_summary-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:13:47 2020"}