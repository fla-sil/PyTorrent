{"info": {"author": "Praveen Palanisamy", "author_email": "praveen.palanisamy@outlook.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3.9", "Topic :: Scientific/Engineering", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "\n![MACAD-Gym learning environment 1](https://raw.githubusercontent.com/praveen-palanisamy/macad-gym/master/docs/images/macad-gym-urban_4way_intrx_2c1p1m.png)\nMACAD-Gym is a training platform for Multi-Agent Connected Autonomous\n Driving (MACAD) built on top of the CARLA Autonomous Driving simulator.\n\nMACAD-Gym provides OpenAI Gym-compatible learning environments for various\ndriving scenarios for training Deep RL algorithms in homogeneous/heterogenous,\ncommunicating/non-communicating and other multi-agent settings. New environments and scenarios\n can be easily added using a simple, JSON-like configuration.\n\n### Quick Start\n\nInstall MACAD-Gym using `pip install macad-gym`.\n If you have CARLA installed, you can get going using the following 3 lines of code. If not, follow the\n[Getting started steps](#getting-started).\n\n```python\nimport gym\nimport macad_gym\nenv = gym.make(\"HomoNcomIndePOIntrxMASS3CTWN3-v0\")\n\n# Your agent code here\n```\n\n Any RL library that supports the OpenAI-Gym API can be used to train agents in MACAD-Gym. The MACAD-Agents repository provides sample agents as a starter.\n\n### Usage guide\n\n1. [Getting Started](#getting-started)\n1. [Learning platform & agent interface](#learning-platform-and-agent-interface)\n1. [Developer Contribution Guide](CONTRIBUTING.md)\n\n### Getting Started\n\n> Assumes an Ubuntu (16.04/18.04 or later) system.\n\n1. Install the system requirements:\n\t- Miniconda/Anaconda 3.x\n\t\t- `wget -P ~ https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash ~/Miniconda3-latest-Linux-x86_64.sh`\n\t- cmake (`sudo apt install cmake`)\n\t- zlib (`sudo apt install zlib1g-dev`)\n\t- [optional] ffmpeg (`sudo apt install ffmpeg`)\n\n1. Setup CARLA (0.9.x)\n\n    3.1 `mkdir ~/software && cd ~/software`\n\n    3.2 Example: Download the 0.9.4 release version from: [Here](https://drive.google.com/file/d/1p5qdXU4hVS2k5BOYSlEm7v7_ez3Et9bP/view)\n    Extract it into `~/software/CARLA_0.9.4`\n\n    3.3 `echo \"export CARLA_SERVER=${HOME}/software/CARLA_0.9.4/CarlaUE4.sh\" >> ~/.bashrc`\n\n1. Install MACAD-Gym:\n   - **Option1 for users** : `pip install macad-gym`\n   - **Option2 for developers**:\n     - Fork/Clone the repository to your workspace:\n        `git clone https://github.com/praveen-palanisamy/macad-gym.git && cd macad-gym`\n     - Create a new conda env named \"macad-gym\" and install the required packages:\n      `conda env create -f conda_env.yaml`\n     - Activate the `macad-gym` conda python env:\n      `source activate carla-gym`\n     - Install the `macad-gym` package:\n\t  `pip install -e .`\n     - Install CARLA PythonAPI: `pip install carla==0.9.4`\n     > NOTE: Change the carla client PyPI package version number to match with your CARLA server version\n\n\n### Learning Platform and Agent Interface\n\nThe MACAD-Gym platform provides learning environments for training agents in both,\nsingle-agent and multi-agent settings for various autonomous driving tasks and \nscenarios that enables training agents in homogeneous/heterogeneous\nThe learning environments follows naming convention for the ID to be consistent\nand to support versioned benchmarking of agent algorithms.\nThe naming convention is illustrated below with `HeteCommCoopPOUrbanMgoalMAUSID`\nas an example:\n![MACAD-Gym Naming Conventions](https://raw.githubusercontent.com/praveen-palanisamy/macad-gym/master/docs/images/macad-gym-naming-conventions.png)\n\nThe number of training environments in MACAD-Gym is expected to grow over time\n(PRs are very welcome!). \n\n#### Environments\n\nThe environment interface is simple and follows the widely adopted OpenAI-Gym\ninterface. You can create an instance of a learning environment using the \nfollowing 3 lines of code:\n\n```python\nimport gym\nimport macad_gym\nenv = gym.make(\"HomoNcomIndePOIntrxMASS3CTWN3-v0\")\n```\n\nLike any OpenAI Gym environment, you can obtain the observation space and action\nspaces as shown below:\n\n```bash\n>>> print(env.observation_space)\nDict(car1:Box(168, 168, 3), car2:Box(168, 168, 3), car3:Box(168, 168, 3))\n>>> print(env.action_space)\nDict(car1:Discrete(9), car2:Discrete(9), car3:Discrete(9))\n```\n\nTo get a list of available environments, you can use\nthe `list_available_envs()` function as shown in the code snippet below:\n\n```python\nimport gym\nimport macad_gym\nmacad_gym.list_available_envs()\n```\nThis will print the available environments. Sample output is provided below for reference:\n\n```bash\nEnvironment-ID: Short description\n{'HeteNcomIndePOIntrxMATLS1B2C1PTWN3-v0': 'Heterogeneous, Non-communicating, '\n                                          'Independent,Partially-Observable '\n                                          'Intersection Multi-Agent scenario '\n                                          'with Traffic-Light Signal, 1-Bike, '\n                                          '2-Car,1-Pedestrian in Town3, '\n                                          'version 0',\n 'HomoNcomIndePOIntrxMASS3CTWN3-v0': 'Homogenous, Non-communicating, '\n                                     'Independed, Partially-Observable '\n                                     'Intersection Multi-Agent scenario with '\n                                     'Stop-Sign, 3 Cars in Town3, version 0'}\n```\n\n#### Agent interface\nThe Agent-Environment interface is compatible with the OpenAI-Gym interface\nthus, allowing for easy experimentation with existing RL agent algorithm \nimplementations and libraries. You can use any existing Deep RL library that supports the Open AI Gym API to train your agents.\n\nThe basic agent-environment interaction loop is as follows:\n\n\n```python\nimport gym\nimport macad_gym\n\n\nenv = gym.make(\"HomoNComIndePOIntrxMASS3CTWN3-v0\")\nconfigs = env.configs()\nenv_config = configs[\"env\"]\nactor_configs = configs[\"actors\"]\n\n\nclass SimpleAgent(object):\n    def __init__(self, actor_configs):\n        \"\"\"A simple, deterministic agent for an example\n        Args:\n            actor_configs: Actor config dict\n        \"\"\"\n        self.actor_configs = actor_configs\n        self.action_dict = {}\n\n\n    def get_action(self, obs):\n        \"\"\" Returns `action_dict` containing actions for each agent in the env\n        \"\"\"\n        for actor_id in self.actor_configs.keys():\n            # ... Process obs of each agent and generate action ...\n            if env_config[\"discrete_actions\"]:\n                self.action_dict[actor_id] = 3  # Drive forward\n            else:\n                self.action_dict[actor_id] = [1, 0]  # Full-throttle \n        return self.action_dict\n\n\nagent = SimpleAgent(actor_configs)  # Plug-in your agent or use MACAD-Agents\nfor ep in range(2):\n    obs = env.reset()\n    done = {\"__all__\": False}\n    step = 0\n    while not done[\"__all__\"]:\n        obs, reward, done, info = env.step(agent.get_action(obs))\n        print(f\"Step#:{step}  Rew:{reward}  Done:{done}\")\n        step += 1\n```\n\n###### **NOTEs**:\n> MACAD-Gym is for CARLA 0.9.x . If you are\nlooking for an OpenAI Gym-compatible agent learning environment for CARLA 0.8.x (stable release),\nuse [this carla_gym environment](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch8/environment).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/praveen-palanisamy/macad-gym", "keywords": "multi-agent learning environments connected autonomous driving OpenAI Gym CARLA", "license": "", "maintainer": "", "maintainer_email": "", "name": "macad-gym", "package_url": "https://pypi.org/project/macad-gym/", "platform": "", "project_url": "https://pypi.org/project/macad-gym/", "project_urls": {"Author website": "https://praveenp.com", "Homepage": "https://github.com/praveen-palanisamy/macad-gym", "Report bug": "https://github.com/praveen-palanisamy/macad-gym/issues", "Source": "https://github.com/praveen-palanisamy/macad-gym"}, "release_url": "https://pypi.org/project/macad-gym/0.1.2/", "requires_dist": ["gym", "carla (>=0.9.3)", "GPUtil", "pygame", "opencv-python", "networkx", "tox; extra == 'test'", "pytest; extra == 'test'", "pytest-xdist; extra == 'test'"], "requires_python": ">=3.0", "summary": "Learning environments for Multi-Agent Connected Autonomous Driving (MACAD) with OpenAI Gym compatible interfaces", "version": "0.1.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><img alt=\"MACAD-Gym learning environment 1\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d88d5260fcfd073fa429b6dee69808bb47c94e3c/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f7072617665656e2d70616c616e6973616d792f6d616361642d67796d2f6d61737465722f646f63732f696d616765732f6d616361642d67796d2d757262616e5f347761795f696e7472785f32633170316d2e706e67\">\nMACAD-Gym is a training platform for Multi-Agent Connected Autonomous\nDriving (MACAD) built on top of the CARLA Autonomous Driving simulator.</p>\n<p>MACAD-Gym provides OpenAI Gym-compatible learning environments for various\ndriving scenarios for training Deep RL algorithms in homogeneous/heterogenous,\ncommunicating/non-communicating and other multi-agent settings. New environments and scenarios\ncan be easily added using a simple, JSON-like configuration.</p>\n<h3>Quick Start</h3>\n<p>Install MACAD-Gym using <code>pip install macad-gym</code>.\nIf you have CARLA installed, you can get going using the following 3 lines of code. If not, follow the\n<a href=\"#getting-started\" rel=\"nofollow\">Getting started steps</a>.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">gym</span>\n<span class=\"kn\">import</span> <span class=\"nn\">macad_gym</span>\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s2\">\"HomoNcomIndePOIntrxMASS3CTWN3-v0\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Your agent code here</span>\n</pre>\n<p>Any RL library that supports the OpenAI-Gym API can be used to train agents in MACAD-Gym. The MACAD-Agents repository provides sample agents as a starter.</p>\n<h3>Usage guide</h3>\n<ol>\n<li><a href=\"#getting-started\" rel=\"nofollow\">Getting Started</a></li>\n<li><a href=\"#learning-platform-and-agent-interface\" rel=\"nofollow\">Learning platform &amp; agent interface</a></li>\n<li><a href=\"CONTRIBUTING.md\" rel=\"nofollow\">Developer Contribution Guide</a></li>\n</ol>\n<h3>Getting Started</h3>\n<blockquote>\n<p>Assumes an Ubuntu (16.04/18.04 or later) system.</p>\n</blockquote>\n<ol>\n<li>\n<p>Install the system requirements:</p>\n<ul>\n<li>Miniconda/Anaconda 3.x\n<ul>\n<li><code>wget -P ~ https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash ~/Miniconda3-latest-Linux-x86_64.sh</code></li>\n</ul>\n</li>\n<li>cmake (<code>sudo apt install cmake</code>)</li>\n<li>zlib (<code>sudo apt install zlib1g-dev</code>)</li>\n<li>[optional] ffmpeg (<code>sudo apt install ffmpeg</code>)</li>\n</ul>\n</li>\n<li>\n<p>Setup CARLA (0.9.x)</p>\n<p>3.1 <code>mkdir ~/software &amp;&amp; cd ~/software</code></p>\n<p>3.2 Example: Download the 0.9.4 release version from: <a href=\"https://drive.google.com/file/d/1p5qdXU4hVS2k5BOYSlEm7v7_ez3Et9bP/view\" rel=\"nofollow\">Here</a>\nExtract it into <code>~/software/CARLA_0.9.4</code></p>\n<p>3.3 <code>echo \"export CARLA_SERVER=${HOME}/software/CARLA_0.9.4/CarlaUE4.sh\" &gt;&gt; ~/.bashrc</code></p>\n</li>\n<li>\n<p>Install MACAD-Gym:</p>\n<ul>\n<li><strong>Option1 for users</strong> : <code>pip install macad-gym</code></li>\n<li><strong>Option2 for developers</strong>:\n<ul>\n<li>Fork/Clone the repository to your workspace:\n<code>git clone https://github.com/praveen-palanisamy/macad-gym.git &amp;&amp; cd macad-gym</code></li>\n<li>Create a new conda env named \"macad-gym\" and install the required packages:\n<code>conda env create -f conda_env.yaml</code></li>\n<li>Activate the <code>macad-gym</code> conda python env:\n<code>source activate carla-gym</code></li>\n<li>Install the <code>macad-gym</code> package:\n<code>pip install -e .</code></li>\n<li>Install CARLA PythonAPI: <code>pip install carla==0.9.4</code></li>\n</ul>\n<blockquote>\n<p>NOTE: Change the carla client PyPI package version number to match with your CARLA server version</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ol>\n<h3>Learning Platform and Agent Interface</h3>\n<p>The MACAD-Gym platform provides learning environments for training agents in both,\nsingle-agent and multi-agent settings for various autonomous driving tasks and\nscenarios that enables training agents in homogeneous/heterogeneous\nThe learning environments follows naming convention for the ID to be consistent\nand to support versioned benchmarking of agent algorithms.\nThe naming convention is illustrated below with <code>HeteCommCoopPOUrbanMgoalMAUSID</code>\nas an example:\n<img alt=\"MACAD-Gym Naming Conventions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/46242fe0e07cc9a706b59ba4b796e638f5298474/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f7072617665656e2d70616c616e6973616d792f6d616361642d67796d2f6d61737465722f646f63732f696d616765732f6d616361642d67796d2d6e616d696e672d636f6e76656e74696f6e732e706e67\"></p>\n<p>The number of training environments in MACAD-Gym is expected to grow over time\n(PRs are very welcome!).</p>\n<h4>Environments</h4>\n<p>The environment interface is simple and follows the widely adopted OpenAI-Gym\ninterface. You can create an instance of a learning environment using the\nfollowing 3 lines of code:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">gym</span>\n<span class=\"kn\">import</span> <span class=\"nn\">macad_gym</span>\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s2\">\"HomoNcomIndePOIntrxMASS3CTWN3-v0\"</span><span class=\"p\">)</span>\n</pre>\n<p>Like any OpenAI Gym environment, you can obtain the observation space and action\nspaces as shown below:</p>\n<pre>&gt;&gt;&gt; print<span class=\"o\">(</span>env.observation_space<span class=\"o\">)</span>\nDict<span class=\"o\">(</span>car1:Box<span class=\"o\">(</span><span class=\"m\">168</span>, <span class=\"m\">168</span>, <span class=\"m\">3</span><span class=\"o\">)</span>, car2:Box<span class=\"o\">(</span><span class=\"m\">168</span>, <span class=\"m\">168</span>, <span class=\"m\">3</span><span class=\"o\">)</span>, car3:Box<span class=\"o\">(</span><span class=\"m\">168</span>, <span class=\"m\">168</span>, <span class=\"m\">3</span><span class=\"o\">))</span>\n&gt;&gt;&gt; print<span class=\"o\">(</span>env.action_space<span class=\"o\">)</span>\nDict<span class=\"o\">(</span>car1:Discrete<span class=\"o\">(</span><span class=\"m\">9</span><span class=\"o\">)</span>, car2:Discrete<span class=\"o\">(</span><span class=\"m\">9</span><span class=\"o\">)</span>, car3:Discrete<span class=\"o\">(</span><span class=\"m\">9</span><span class=\"o\">))</span>\n</pre>\n<p>To get a list of available environments, you can use\nthe <code>list_available_envs()</code> function as shown in the code snippet below:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">gym</span>\n<span class=\"kn\">import</span> <span class=\"nn\">macad_gym</span>\n<span class=\"n\">macad_gym</span><span class=\"o\">.</span><span class=\"n\">list_available_envs</span><span class=\"p\">()</span>\n</pre>\n<p>This will print the available environments. Sample output is provided below for reference:</p>\n<pre>Environment-ID: Short description\n<span class=\"o\">{</span><span class=\"s1\">'HeteNcomIndePOIntrxMATLS1B2C1PTWN3-v0'</span>: <span class=\"s1\">'Heterogeneous, Non-communicating, '</span>\n                                          <span class=\"s1\">'Independent,Partially-Observable '</span>\n                                          <span class=\"s1\">'Intersection Multi-Agent scenario '</span>\n                                          <span class=\"s1\">'with Traffic-Light Signal, 1-Bike, '</span>\n                                          <span class=\"s1\">'2-Car,1-Pedestrian in Town3, '</span>\n                                          <span class=\"s1\">'version 0'</span>,\n <span class=\"s1\">'HomoNcomIndePOIntrxMASS3CTWN3-v0'</span>: <span class=\"s1\">'Homogenous, Non-communicating, '</span>\n                                     <span class=\"s1\">'Independed, Partially-Observable '</span>\n                                     <span class=\"s1\">'Intersection Multi-Agent scenario with '</span>\n                                     <span class=\"s1\">'Stop-Sign, 3 Cars in Town3, version 0'</span><span class=\"o\">}</span>\n</pre>\n<h4>Agent interface</h4>\n<p>The Agent-Environment interface is compatible with the OpenAI-Gym interface\nthus, allowing for easy experimentation with existing RL agent algorithm\nimplementations and libraries. You can use any existing Deep RL library that supports the Open AI Gym API to train your agents.</p>\n<p>The basic agent-environment interaction loop is as follows:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">gym</span>\n<span class=\"kn\">import</span> <span class=\"nn\">macad_gym</span>\n\n\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s2\">\"HomoNComIndePOIntrxMASS3CTWN3-v0\"</span><span class=\"p\">)</span>\n<span class=\"n\">configs</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">configs</span><span class=\"p\">()</span>\n<span class=\"n\">env_config</span> <span class=\"o\">=</span> <span class=\"n\">configs</span><span class=\"p\">[</span><span class=\"s2\">\"env\"</span><span class=\"p\">]</span>\n<span class=\"n\">actor_configs</span> <span class=\"o\">=</span> <span class=\"n\">configs</span><span class=\"p\">[</span><span class=\"s2\">\"actors\"</span><span class=\"p\">]</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">SimpleAgent</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">actor_configs</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"A simple, deterministic agent for an example</span>\n<span class=\"sd\">        Args:</span>\n<span class=\"sd\">            actor_configs: Actor config dict</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">actor_configs</span> <span class=\"o\">=</span> <span class=\"n\">actor_configs</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">action_dict</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n\n\n    <span class=\"k\">def</span> <span class=\"nf\">get_action</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">obs</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\" Returns `action_dict` containing actions for each agent in the env</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"k\">for</span> <span class=\"n\">actor_id</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">actor_configs</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">():</span>\n            <span class=\"c1\"># ... Process obs of each agent and generate action ...</span>\n            <span class=\"k\">if</span> <span class=\"n\">env_config</span><span class=\"p\">[</span><span class=\"s2\">\"discrete_actions\"</span><span class=\"p\">]:</span>\n                <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">action_dict</span><span class=\"p\">[</span><span class=\"n\">actor_id</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">3</span>  <span class=\"c1\"># Drive forward</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">action_dict</span><span class=\"p\">[</span><span class=\"n\">actor_id</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span>  <span class=\"c1\"># Full-throttle </span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">action_dict</span>\n\n\n<span class=\"n\">agent</span> <span class=\"o\">=</span> <span class=\"n\">SimpleAgent</span><span class=\"p\">(</span><span class=\"n\">actor_configs</span><span class=\"p\">)</span>  <span class=\"c1\"># Plug-in your agent or use MACAD-Agents</span>\n<span class=\"k\">for</span> <span class=\"n\">ep</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">):</span>\n    <span class=\"n\">obs</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n    <span class=\"n\">done</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"__all__\"</span><span class=\"p\">:</span> <span class=\"kc\">False</span><span class=\"p\">}</span>\n    <span class=\"n\">step</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n    <span class=\"k\">while</span> <span class=\"ow\">not</span> <span class=\"n\">done</span><span class=\"p\">[</span><span class=\"s2\">\"__all__\"</span><span class=\"p\">]:</span>\n        <span class=\"n\">obs</span><span class=\"p\">,</span> <span class=\"n\">reward</span><span class=\"p\">,</span> <span class=\"n\">done</span><span class=\"p\">,</span> <span class=\"n\">info</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">get_action</span><span class=\"p\">(</span><span class=\"n\">obs</span><span class=\"p\">))</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">\"Step#:</span><span class=\"si\">{</span><span class=\"n\">step</span><span class=\"si\">}</span><span class=\"s2\">  Rew:</span><span class=\"si\">{</span><span class=\"n\">reward</span><span class=\"si\">}</span><span class=\"s2\">  Done:</span><span class=\"si\">{</span><span class=\"n\">done</span><span class=\"si\">}</span><span class=\"s2\">\"</span><span class=\"p\">)</span>\n        <span class=\"n\">step</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n</pre>\n<h6><strong>NOTEs</strong>:</h6>\n<blockquote>\n<p>MACAD-Gym is for CARLA 0.9.x . If you are\nlooking for an OpenAI Gym-compatible agent learning environment for CARLA 0.8.x (stable release),\nuse <a href=\"https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch8/environment\" rel=\"nofollow\">this carla_gym environment</a>.</p>\n</blockquote>\n\n          </div>"}, "last_serial": 6040361, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "e1c3ad52c6fc191cf94d9d0ec27c83e0", "sha256": "4f7375457fea88bf509ec4269a7501299135adbf7943873ef7aee865f20a3e5b"}, "downloads": -1, "filename": "macad_gym-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "e1c3ad52c6fc191cf94d9d0ec27c83e0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 109818, "upload_time": "2019-10-13T06:01:05", "upload_time_iso_8601": "2019-10-13T06:01:05.026672Z", "url": "https://files.pythonhosted.org/packages/dc/5f/bd20bf794c12b280ebba4bea52e40c73b2a515f78a951c8524b45fe37835/macad_gym-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e88b3ff5ce6323ea0b2dc7f1a0e260ae", "sha256": "f969f451513d18012c2a99262855a8ea1add9f1462884a09df03dcdddf484ba5"}, "downloads": -1, "filename": "macad-gym-0.1.1.tar.gz", "has_sig": false, "md5_digest": "e88b3ff5ce6323ea0b2dc7f1a0e260ae", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 91900, "upload_time": "2019-10-13T06:01:08", "upload_time_iso_8601": "2019-10-13T06:01:08.663090Z", "url": "https://files.pythonhosted.org/packages/50/9c/2cd891cd669f46854532518b00277126b67943dd89b41f7a00e7bd4d0ba1/macad-gym-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "7b8a6851b29beb4b2adc8adc4941bb6f", "sha256": "fd20f0aba570d8d6d0b6dfc53af360bf94d6c6995d3106ac221485050ff01d17"}, "downloads": -1, "filename": "macad_gym-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "7b8a6851b29beb4b2adc8adc4941bb6f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 114563, "upload_time": "2019-10-28T09:41:11", "upload_time_iso_8601": "2019-10-28T09:41:11.416779Z", "url": "https://files.pythonhosted.org/packages/79/c9/4cff33fecce2230e5b3dd946135b5413bf1a996a63e5bc9c16c244f7dfce/macad_gym-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "edfd346cae8f0bd3a3d1e40872fd33e5", "sha256": "5e59b1309a94883b849a86613109417ab642c17b0f3f7ae4448479ca8e96d3b7"}, "downloads": -1, "filename": "macad-gym-0.1.2.tar.gz", "has_sig": false, "md5_digest": "edfd346cae8f0bd3a3d1e40872fd33e5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 92783, "upload_time": "2019-10-28T09:41:13", "upload_time_iso_8601": "2019-10-28T09:41:13.319071Z", "url": "https://files.pythonhosted.org/packages/a5/0b/e0ffa01158c10982f5fadbc3c57f12998b8d6be2c58904270f2f4cebf151/macad-gym-0.1.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "7b8a6851b29beb4b2adc8adc4941bb6f", "sha256": "fd20f0aba570d8d6d0b6dfc53af360bf94d6c6995d3106ac221485050ff01d17"}, "downloads": -1, "filename": "macad_gym-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "7b8a6851b29beb4b2adc8adc4941bb6f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 114563, "upload_time": "2019-10-28T09:41:11", "upload_time_iso_8601": "2019-10-28T09:41:11.416779Z", "url": "https://files.pythonhosted.org/packages/79/c9/4cff33fecce2230e5b3dd946135b5413bf1a996a63e5bc9c16c244f7dfce/macad_gym-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "edfd346cae8f0bd3a3d1e40872fd33e5", "sha256": "5e59b1309a94883b849a86613109417ab642c17b0f3f7ae4448479ca8e96d3b7"}, "downloads": -1, "filename": "macad-gym-0.1.2.tar.gz", "has_sig": false, "md5_digest": "edfd346cae8f0bd3a3d1e40872fd33e5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 92783, "upload_time": "2019-10-28T09:41:13", "upload_time_iso_8601": "2019-10-28T09:41:13.319071Z", "url": "https://files.pythonhosted.org/packages/a5/0b/e0ffa01158c10982f5fadbc3c57f12998b8d6be2c58904270f2f4cebf151/macad-gym-0.1.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:42:25 2020"}