{"info": {"author": "JL Peyret", "author_email": "jpeyret@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8"], "description": "Travis CI: [![Build Status](https://travis-ci.org/jpeyret/lazy-regression-tests.svg?branch=master)](https://travis-ci.org/jpeyret/lazy-regression-tests)  ![Coverage](https://codecov.io/gh/jpeyret/lazy-regression-tests/branch/master/graph/badge.svg)\n\n## Automated regression tests\n\n### Let's take a simple web page that has some variable data.\n\nDriving it is some simplistic markup, and a mocked up http unittest that varies some data like the time stamp and a hidden csrf token.\n\n![screenshot of page with constantly-changing elements](https://raw.githubusercontent.com/jpeyret/lazy-regression-tests/049.lazy.000.packaging/docs/screenshots/001.first_run.png)\n\n\n### Goals \n\nWe want to do the following, *with less than 10 lines of code*:\n\n- regression test that the page HTML is the same from run to run\n- validate basic HTML behavior, like `title`, 200 `status_code` and http `content_type`\n- check that `name` in the greeting corresponds to what's set on the unittest configuration\n- an additional feature is the presence of a **csrf** random token.\n\nObviously, both the csrf token and the time stamp need to be disregarded from run to run.  \n\nIf anything else changes, we want to fail the test automatically.  The tester can then examine what changed and determine whether to accept the new HTML as the new baseline.  **Cosmetic HTML changes are for designers to worry about, test code shouldn't have to change.**\n\n\n### What a full test looks like, minus server/response mocking:\n\n[full test code, under `class Test_Features`](https://github.com/jpeyret/lazy-regression-tests/blob/master/tests/test_usage.py)\n\n\n````\n# generic lazy functionality\nfrom lazy_regression_tests import (\n    LazyMixin,\n    ValidationDirective,\n    FilterDirective,\n    AutoExp,\n    RegexRemoveSaver,\n)\n\n# specific to HTTP/HTML validations\nfrom lazy_regression_tests.http_validators import (\n    HTMLValidationMixin,\n    ResponseHTML,\n    HtmlFilterManager,\n    CSSRemoveFilter,\n    CSSValidator,\n)\n\nclass LazyMixinBasic(LazyMixin):\n    \"\"\" base Mixin class for the lazy test framework \"\"\"\n\n    lazy_filename = LazyMixin.get_basename(__name__, __file__, __module__)\n    cls_filters = dict(html=HtmlFilterManager())\n\n\n# \ud83d\udc47  \ud83e\udd66\ud83e\udd66 filter changing contents\nfilter_variables = [\n    FilterDirective(\n        \"timestamp\", filter_=CSSRemoveFilter(\"span.timestamp\", \"timestamp\", scalar=True)\n    ),\n    FilterDirective(\"csrf\", filter_=RegexRemoveSaver(\"csrf_token\", \"csrf_token\")),\n]\n\n\nclass Test_Features(Helper, HTMLValidationMixin, LazyMixinBasic, unittest.TestCase):\n\n    cls_filters = dict(html=filter_variables)  # \ud83d\udc48 This is how we add the filters\n\n    name = \"Mr. Rabbit\"  # picked up by `AutoExp` below\n\n    # \ud83d\udc47 setting up the validations \ud83e\udd66\ud83e\udd66\n    cls_validators = [\n        ValidationDirective(\"title\", exp=\"Your order\"),\n        ValidationDirective(\"name\", exp=AutoExp, validator=CSSValidator(\"#name\")),\n    ]\n\n    @mock.patch.dict(os.environ, di_mock_env)\n    def test_it(self):\n        try:\n            # this comes from `requests`, could be a Django testserver, your code...\n            http_response = get_fake_response_from_template(self)\n\n            # if we re-use filters and validations, yes, we use a lot less than\n            # 10 lines of code \ud83d\udc47 \ud83c\udf70\ud83c\udf70\ud83c\udf70\ud83c\udf70\n\n            # \"adapt\" standard http_response by tracking content_type, status_code, headers...\n            response = ResponseHTML(http_response)\n            # Check validation such as content_type and status code\n            self.check_expectations(response=response)\n            # Regression test - did we get the same contents as the last time?\n            tmp = self.assert_exp(response.content, \"html\")\n\n        except (Exception,) as e:\n            raise\n\n````\n\n\n\n## THE DETAILS...\n\nLet's start out by assuming we **didn't** have the filters configured yet, so data changes from run to run.\n\n[full test code, with more examples, under `class Test_Features`](https://github.com/jpeyret/lazy-regression-tests/blob/master/tests/test_doc.py)\n\n\n### Trigger a failure by running the test twice.\n\n````\npytest -q test_doc::Test_Features::test_it\npytest -q test_doc::Test_Features::test_it\n````\n\n\nThe first time works just fine - this test has never been run so the received data is taken as the expected data for this test.\n\nThe second run however triggers a test failure because the crsf token and timestamp don't match the first pass.\n\n````\nE           \u274c\u274c\u274cRegressions found:\n\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\n\nE               <script>\nE           -    const csrf_token = 'VRzFbhbVZnzWZQlmr6xd';\nE           +    const csrf_token = 'E0z05wqHH0I6msv7iouB';\n\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\nE               <br/>\nE               It is now\nE               <span class=\"timestamp\">\nE           -    2020-02-23 19:04:52.777187\nE           ?                  ^^    --- ^^\nE           +    2020-02-23 19:17:52.106223\nE           ?                  ^^     ^^^^^\n\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\nE           \u274c\u274c\u274c\n\n````\n\n## Fix : filter out changing data\n\nWe want disregard the csrf token and the timestamp data when comparing runs.  To do that, we add two filters to our processing of `HTML` data, one for the csrf and one for the time stamp.\n\n```` \nfilter_variables = [\n    FilterDirective(\n        \"timestamp\"\n        ,filter_=CSSRemoveFilter(\"span.timestamp\", \"timestamp\", scalar=True)\n    ),\n    FilterDirective(\n    \t\"csrf\"\n    \t, filter_=RegexRemoveSaver(\"csrf_token\", \"csrf_token\")),\n]\n````\n\n#### Add the filters to the test class:\n\nNote that our unittest inherits from `LazyMixinBasic`, which enables the whole lazy-test framework.\n\n````\n#  \u2699\ufe0f   This enables the lazy-test framework     \ud83d\udc47   \nclass Test_Features(Helper, HTMLValidationMixin, LazyMixinBasic, unittest.TestCase):\n    \"\"\" this is the test we are running here \"\"\"\n\n    cls_filters = dict(html=filter_variables)  # \ud83d\udc48 This is how we add the filters\n\n    name = \"Mr. Rabbit\"\n    line1 = \"Item 1\"\n    ...\n````\n\n### Write as little as possible:\n\nThe actual test code to use in each test method is very limited.  Yes, you will need to declare those filters and validations, but they are designed to be shared across multiple test classes.\n\nMost of the behavior is built-in, once you inherit from a base class.  Filters, and validators which we will see later, are inherited from class to class via Python's standard MRO and some metaclass tweaking.  No `setUp/tearDown` are needed, but you can use yours as usual.\n\n\n### The actual test class: `Test_Features`\n\nWe define `Test_Features` to be both a LazyMixin - which enables lazy-testing - and a TestCase.  `Helper` isn't super important, it's just there to support the fake html requests.\n\n````\n                            # \ud83d\udc47\u2699\ufe0fThis enables the lazy-test framework\nclass Test_Features(Helper, HTMLValidationMixin, LazyMixinBasic, unittest.TestCase):\n    \"\"\" this is the test we are running here \"\"\"\n\n    cls_filters = dict(html=filter_variables)  # \ud83d\udc48 This is how we add the filters\n\n    name = \"Mr. Rabbit\"\n    line1 = \"Item 1\"\n    line2 = \"Item 2\"\n    line3 = \"Item 3\"\n\n\t # \ud83d\udc47 setting up the validations\n    cls_validators = [\n        ValidationDirective(\"title\", exp=\"Your order\"),\n        ValidationDirective(\"name\", exp=AutoExp, validator=CSSValidator(\"#name\")),\n    ]\n\n    # the template used to generate the fake html\n    template = \"\"\"\n<title>Your order</title>\n<script>\nconst csrf_token = '{{csrf}}';\n</script>\n<body>\n    Hi&nbsp;<span id=\"name\">{{ name }}</span>&nbsp;!<br/> \n    It is now<span class=\"timestamp\">{{ timestamp }}</span>.<br/>\n    Your order is:\n    <ul>\n        <li class=\"orderline\">{{line1}}</li>\n        <li class=\"orderline\">{{line2}}</li>\n        <li class=\"orderline\">{{line3}}</li>\n    </ul>\n</body>\n    \"\"\"\n````\n\n\n## Run the test again\n\nThe test still fails - the csrf token and timestamps are gone, *but only from the newly received data*.\n\nThings *are* slightly different.  Before, we had 2 csrf tokens and two time stamps, one before (`19:04`), one after (`19:23`).  Now it seems we only see the old ones, the new ones are being filtered out.\n\nWe need a way to accept the new data as the reference.  There are two ways to do that.\n\n````\nE           AssertionError:\nE\nE           \u274c\u274c\u274cRegressions found:\n\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\nE               <script>\nE           -    const csrf_token = 'VRzFbhbVZnzWZQlmr6xd';\nE               </script>\n\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\nE               <br/>\nE               It is now\nE           -   <span class=\"timestamp\">\nE           -    2020-02-23 19:04:52.777187\nE           -   </span>\nE               <ul>\n\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\n````\n\n### Option #1 - reset expectations via `baseline` directive.\n\nWe need to tell the test to accept the new format as valid.  Telling `assert_exp` that you want to treat new data as valid is what the **`directive=baseline`  mechanism** is made for, which uses the environment variable `$lzrt_directive`.\n\nLet's try it out:\n\n````\nlzrt_directive=baseline pytest -q test_doc::Test_Features::test_it\n.                                                                                                  [100%]\n1 passed in 0.25s\n````\n\nWhat does `lzrt_directive=baseline` do?  Whenever it is set, calling `assert_exp` overwrites the saved expectations file with the incoming, now filtered, data.\n\n### Option #2 - overwrite the expectations file\n\nInstead of using `baseline`, you can also just copy the received/got formatted file over to the expected reference file.  Or you can just delete the expectations file entirely.\n\nThe error message lists the paths:\n\n````\n\u274c\u274c\u274cRegressions found:\n\nExpected contents are in file:\n  ...tests/lazy/exp/Test_Features/test_doc.Test_Features.test_it.html\n\nReceived contents are in file:\n  .../lazy/got/Test_Features/test_doc.Test_Features.test_it.html\n\n....\n````\n\ncopy Received to Expected and the test will also pass next time.\n\n### Test-driven development:\n\nLet's say we want to support styles on the order lines as in:\n\n````\n<li class=\"orderline\">Item 1</li>\n````\n\nJust edit it manually in the expectations file.\n\n````\n  Your order is:\n  <ul> \ud83d\udc47\n   <li class=\"orderline\">\n    Item 1\n  ...\n````\n\nand now the test fails until the application code has been updated:\n\n````\n\u274c\u274c\u274cRegressions found:\n....\n    <ul>\n-    <li class=\"orderline\">\n+    <li>\n      Item 1\n````\nand succeeds afterwards:\n\n````\npytest -q test_doc::Test_Features::test_it\n.                                                                                                  [100%]\n1 passed in 0.29s\n````\n\n## Validations\n\nValidations are separate from the regression tests, but work with them.  If you get an HTML 404 response instead of your expected JSON response, it is much more informative to be told that your response is in error status and of the wrong content type rather than looking at a big dump of mismatched expected JSON vs received HTML.\n\nLet's fake out a bad response by setting it on the response.\n\n````\nresponse = ResponseHTML(http_response)\n\nresponse.status_code = 404  \ud83d\udc48 fake!\nself.validationmgr.debug()  \ud83d\udc48 and we will use this inspect the validations\n \n````\n\nWe get the expected error:\n\n````\nE   AssertionError:\nE\nE   \u274c\u274c\u274c\u274c\u274c\u274c\u274c\nE\nE   Validation failure @ status_code [StatusCodeValidator[selector=status_code]]:\nE\nE   False is not true : status_code exp:200<>404:got\nE\nE   \u274c\u274c\u274c\u274c\u274c\u274c\u274c\n\n````\n\nWhere did this come from?  class `Test_Features`'s validations has 2 validations defined on it:\n\n#### name \n\nwill use a CSS selector to find the matching element and the expection for it is `AutoExp`.  AutoExp basically will cause the validator to look for a matching attribute in our unittest, i.e. `name` i.e. \"Mr. Rabbit\".\n\n#### title\n\nonly has a hardcoded expectation and no apparent strategy to validate it.\n\n\n````\n                            # HTTP/HTML validation inheritance\n                            # \ud83d\udc47 \nclass Test_Features(Helper, HTMLValidationMixin, LazyMixinBasic, unittest.TestCase):\n\n     # \ud83d\udc47 setting up the validations\n    cls_validators = [\n        ValidationDirective(\"title\", exp=\"Your order\"),\n        ValidationDirective(\"name\", exp=AutoExp, validator=CSSValidator(\"#name\")),\n    ]\n\n````\n\nLet's see what that debug function call has to say.  Turns out there are a number of validations.  What we are interested in is the **class-level inheritance**:\n\n\n````\n\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f  lazy-tests configuration  \u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\nValidationManager for test_it (__main__.Test_Features)  validators:\n  status_code         : active:True exp:200        StatusCodeValidator\n  content_type        : active:True exp:html       ContentTypeValidator\n  title               : active:True exp:Your order TitleCSSValidator\n  name                : active:True exp:Mr. Rabbit CSSValidator\n\n\n \u2699\ufe0fclass-level inheritance:\nlazy_regression_tests.lazy3.http_validators.HTTPValidationMixin\nlazy_regression_tests.lazy3.http_validators.HTMLValidationMixin\n__main__.Test_Features\n\n````\n\n### Validations default from class inheritance\n\nTo avoid side effects of sharing those validators and filters, the actual objects are always copied whenever they are inherited from another level.  So you can for example *reach into* a CSSValidator and change its selector for 1 class.\n\nGoing from generic to specific (i.e. reverse MRO order):\n\nFirst we set the `content_type` validation to be active, but leaving out the expectations.\n\n````\nclass HTTPValidationMixin:\n    \"\"\" sets basic expectations \n        - http is expected to return a status_code, typically 200 (exp can be changed later)\n        - and has a content_type, which changes depending on end points\n    \"\"\"\n\n    cls_validators = [\n        ValidationDirective(\"status_code\", exp=200, validator=StatusCodeValidator()),\n        ValidationDirective(\"content_type\", active=True, validator=ContentTypeValidator()),\n    ]\n\n````\nNext comes the HTML validations, which look for a `title` and also sets expections on the `content_type`:\n\n````\ntitle_validation = ValidationDirective(\"title\", active=True, validator=TitleCSSValidator())\n\nclass HTMLValidationMixin(HTTPValidationMixin):\n    \"\"\" set `content_type` expectations\n        always want to validate `title`, by default \n    \"\"\"\n\n    cls_validators = [title_validation, ValidationDirective(\"content_type\", exp=\"html\")]\n````\n\n\n\n## Writing your own validations\n\nWe haven't done that line item validation (they all need to start with \"Item\").  To do that we'll copy our class and add that validation.\n\n### Easy way - use built-in features:\n\nHere we just add a CSS selector, with a regular expression as an expectation:\n\nWe expect our test to fail, because of the bad `line3`\n\n````\n    cls_validators = [              #\ud83d\udc47                      #\ud83d\udc47\n        ValidationDirective(\"item\", exp=re.compile(\"^Item\"), validator=CSSValidator(\"li.orderline\")),\n    ]\n\n    name = \"Mr. Rabbit\"\n    line1 = \"Item 1\"\n    line2 = \"Item 2\"\n    line3 = \"Bad line 3\"  \ud83d\udc48\u274c we're expecting a failure here\n````\n\nSure enough:\n\n\n````\n\u274c\u274c\u274c\u274c\u274c\u274c\u274c\n\nValidation failure @ item [CSSValidator[selector=li.orderline]]:\n\nNone is not true : item pattern:^Item:does not match:['Item 1', 'Item 2', 'Bad line 3']:got\n\n\u274c\u274c\u274c\u274c\u274c\u274c\u274c\n````\n\n### Or, write your own validation:\n\nWe can write our own validation function, which always receives:\n- `testee` : the unittest to assert against\n- `got` :which is what the validator selection criteria has found \n- the validator itself\n\nWe inherit our failed Regex-based test, so we can re-use the CSS selector but just provide a custom validation function.\n\n````\ndef check_lineitems(testee: \"unittest.TestCase\", got, validator: \"Validator\"):\n    \"\"\"\n    `got` will be a list of strings here \n    \ud83d\udc49keeping this as stand alone function, rather a method of your TestCase\n    means it can be used anywhere.  It still behaves just like a test method\n    \"\"\"\n    try:\n        for igot in got:\n            if not igot.endswith(\"3\"):\n                testee.assertTrue(igot.startswith(\"Item\"))\n    # pragma: no cover pylint: disable=unused-variable\n    except (Exception,) as e:\n        if cpdb(): \n            pdb.set_trace()\n        raise\n\nclass Test_Features_CustomLineValidation(Test_Features_Regex):\n    \"\"\" This should pass, we are re-using the CSSValidation lookup for `item`\"\"\"\n\n    #   \ud83d\udc47\n    cls_validators = [ValidationDirective(\"item\", exp=check_lineitems)]  #   \ud83d\udc47\n\n    name = \"Mr. Rabbit\"\n    line1 = \"Item 1\"\n    line2 = \"Item 2\"\n    line3 = \"Bad line 3\"\n````\n\n#### The reason this validator sends a list of string:  \n\nConfigured differently it could give you one or more DOM nodes to validate.  \n\n````\nCSSValidator[selector=li.orderline] ''\ncargo=None\nscalar=False \ud83d\udc48\nselector='li.orderline'\nsourcename='response.selectable'\nto_text=True \ud83d\udc48\n````\n\nFor example, the `title` validator is preset differently because there's only one `<title>` tag.\n\n````\nTitleCSSValidator[selector=title] ''\nscalar=True\nselector='title'\nsourcename='response.selectable'\n````\n\n### Adjustments can be made within a test method\n\nHere our validations would fail on the `name` and `title`, so we turn them off (which we could have done in the `cls_validators` as wel. We also turn off timestamp filtering.\n\n````\nclass Test_Turning_ThingsOff(Test_Features):\n    \"\"\" we don't have a title or a greeting anymore\n        and we don't need to filter out the timestamp either\n    \"\"\"\n\n    template = \"\"\"\n<body>\n    It is now<span class=\"timestamp\">fake, fixed, timestamp</span>.<br/>\n</body>\n\"\"\"\n    def setUp(self):\n            # \ud83d\udc47 turn these off to avoid validation errors\n            self.set_expectation(\"title\", active=False)\n            self.set_expectation(\"name\", active=False)\n            self.filters[\"html\"].set_filter(\"timestamp\", active=False)  #\ud83d\udc48 keep it\n\n````\n\n## Wrapping things up\n\n### Access filtered-out data:\n\nYou may have noticed a little `tmp` variable by each `assert_exp` call.\n\n````\ntmp = self.assert_exp(response.content, \"html\")\n````\n\nThat result object has a number of attributes and is available even if there is an AssertionError, via `self.lazytemp`.  What we are most interested in is `tmp.filtered` which is a dictionary that holds lists of filtered values (but only if the filter was given a name).\n\n````\nclass.Dummy ''\ncsrf_token=[\"   const csrf_token = 'M334JV6eNwJh9yFaFa89';\"]\ntimestamp=?\n````\n\n### Throttling validations:\n\nSometimes you want to turn off most of your validations in a test method without having to redefine things at the class level.  Let's say I want to check a 404 in a complex URL - I may want to provide all the correct config for the test, using my base TestCase, but then just provide an incorrect ID for the data lookup.  \n\nIn that case, most of my validations for the normal pages would fail, so I only to run a subset, maybe confirm `status_code=404` and the `content_type`.\n\n````\n# fake a 404\nresponse.content = \"<div>404, to you, buddy!</div>\"\nresponse.status_code = 404\n\t\n# This handles the 404, however, the title and name would error out           \nself.set_expectation(\"status_code\", 404)\n\t\n# \ud83d\udc47 turn off what you don't need\nconstants_keep_on_404 = [\"status_code\", \"content_type\"]\nself.check_expectations(response=response, lazy_skip_except=constants_keep_on_404)\n\n````\n\n### Not limited to HTML\n\n#### JSON, YAML and SQL work too.\n\nThis was written to validate both HTML and embedded JSON within that HTML.\n\nThe JSON Filter has its own Validators and Filters, which operate on dictionaries.  Aside from the serialization dump format, YAML works almost the same way.\n\nThe SQL manager takes any big SQL query string, applies some formatting to normalize it a bit and can be used to keep track of changes to these queries.  It can be useful if you have an ORM-layer that you want to monitor for feature stability from version to version.\n\nOther formats can be supported, provided there is a consistent way to store them in files and load them back up.\n\n````\nfrom lazy_regression_tests.lazy3.filters import JsonFilterManager,\n\n\nclass Test_JSON_Too(LazyMixinBasic, unittest.TestCase):\n    \"\"\" just connect it to the appropriate filter manager for \n    the extension type\n    \"\"\"\n\n    from lazy_regression_tests.lazy3.filters import JsonFilterManager\n    cls_filters = dict(json=JsonFilterManager())\n    extension = \"json\"\n\n    def test_it(self, data={}):\n        \"\"\" fetch data, run validations, regression test \"\"\"\n        try:\n            data = dict(\n                var1=\"the_same\",\n                var2=\"will_change\",\n                )\n            tmp = self.assert_exp(data, self.extension)\n            data.update(var2=\"told you so\")\n            tmp = self.assert_exp(data, self.extension)\n        except (Exception,) as e:\n            raise\n\n\nclass Test_YAML(Test_JSON_Too):\n    \"\"\" hey, most of the work was done by the JSON guys already\n    \"\"\"\n\n    from lazy_regression_tests.lazy3.yaml_validators import YAMLFilter\n\n    extension = \"yaml\"\n    cls_filters = dict(yaml=YAMLFilter())\n````\n\nAs, expected with this test scenario, both error out:\n\n#### JSON:\n\n````\nE             {\nE                 \"var1\": \"the_same\",\nE           -     \"var2\": \"will_change\"\nE           +     \"var2\": \"told you so\"\nE             }\nE           \u274c\u274c\u274c\n\n../lazy_regression_tests/lazy3/core.py:498: AssertionError\n======================================== short test summary info =========================================\nFAILED test_doc.py::Test_JSON_Too::test_it - AssertionError:\n````\n\n#### You can, of course, view differences in diff-type tools:\n\n![view via a GUI diff](https://raw.githubusercontent.com/jpeyret/lazy-regression-tests/049.lazy.000.packaging/docs/screenshots/002.diff_json.png)\n\n\n\n\n#### YAML: \n\n````\nE           Original exception:\nE            'var1: the_same\\nvar2: will_change' != 'var1: the_same\\nvar2: told you so'\nE             var1: the_same\nE           - var2: will_change+ var2: told you so\nE           \u274c\u274c\u274c\n\n../lazy_regression_tests/lazy3/core.py:498: AssertionError\n======================================== short test summary info =========================================\nFAILED test_doc.py::Test_YAML::test_it - AssertionError:\n````\n\n### Filter JSON/YAML data:\n\n````\nclass Test_JSON_Filter(Test_JSON_Too):\n    \"\"\" let's fix the above error by filtering out the changing key\"\"\"\n\n    cls_filters = dict(\n        json=FilterDirective(\n            \"changing\",\n            # DictFilter work by looking for matching keys in the target dictionary.\n            # and then call their value if it's a callable.  \n            # value=None is the default behavior and just deletes the key in the target\n            # \ud83d\udc47\n            DictFilter(dict(var2=None), \"changing\"))\n    )\n````\n\n#### fixed:\n\n````\n(venv) jluc@exp$ pytest -q $v3oiut::Test_JSON_Filter\n.                                                                                                  [100%]\n\n1 passed in 0.23s\n````\n\n\n#### You can even look at arbitrary object graphs\n\n(As long as it is YAML-serializable)\n\n````\nclass Subvar:\n    def __init__(self, value):\n        self.value = value\n\nclass SomethingToTest:\n    def __init__(self):\n        self.var1 = 11\n        self.var2 = 12\n        self.var4 = dict(FF=\"Fantastic\")\n\nclass Test_YAML_Graphs(Test_YAML):\n\n    def test_going_down_the_rabbit_hole(self, data={}):\n        \"\"\" fetch data, run validations, regression test \"\"\"\n        try:\n            from yaml import dump as ydump, load as yload\n\n            somethingtotest = SomethingToTest()\n            somethingtotest.var3 = Subvar(\"3\")\n\n            yaml_ = ydump(somethingtotest)\n\n            #probably not a good idea with untrusted data\n            data = yload(yaml_)\n\n            self.assert_exp(data, self.extension)\n            somethingtotest.added_this = dict(somevar=\"somevalue\")\n            somethingtotest.var3.value=\"3++\"\n\n            yaml_ = ydump(somethingtotest)\n            data = yload(yaml_)\n            self.assert_exp(data, self.extension)\n\n        except (Exception,) as e:\n            raise\n\n````\n#### Yay! An error.\n\n````\nE           + added_this:\nE           +   somevar: somevalue\nE             var1: 11\nE             var2: 12\nE             var3: !!python/object:tests.test_doc.Subvar\nE           -   value: '3'\nE           ?          - ^\nE           +   value: 3++\nE           ?           ^^\nE             var4:\nE               FF: Fantastic\nE           \u274c\u274c\u274c\n../lazy_regression_tests/lazy3/core.py:498: AssertionError\n======================================== short test summary info =========================================\nFAILED test_doc.py::Test_YAML_Graphs::test_going_down_the_rabbit_hole - AssertionError:\n1 failed in 0.33s\n````\n\n\n### Full test code: <a name=\"full_test_code\"></a>\n\nEach module needs to define a base mixin to add the lazy-test behavior.  To support saving files according to the module name, this needs to be done once for each test module.  The test method, `test_it` was added to this class here, but only because its pretty generic at this point.\n\n\n````\n\nclass LazyMixinBasic(LazyMixin):\n\t\"\"\" base Mixin class for the lazy test framework \"\"\"\n\n    # \ud83d\udc47 \u2699\ufe0f tracks where expectation and received files are saved\n    lazy_filename = LazyMixin.get_basename(__name__, __file__, __module__)\n\n    extension = \"html\"\n\n    # \ud83d\udc47 \u2699\ufe0f Tells the framework what extensions/content to expect\n    cls_filters = dict(html=HtmlFilterManager())\n\n\t#.... cut out the test_it() method... we've seen it befor...\n\n\nclass Test_Features(Helper, HTMLValidationMixin, LazyMixinBasic, unittest.TestCase):\n    \"\"\" this is the test we are running here \"\"\"\n\n    cls_filters = dict(html=filter_variables)  # \ud83d\udc48 This is how we add the filters\n\n    name = \"Mr. Rabbit\"\n    line1 = \"Item 1\"\n    line2 = \"Item 2\"\n    line3 = \"Item 3\"\n\n    # \ud83d\udc47 setting up the validations\n    cls_validators = [\n        ValidationDirective(\"title\", exp=\"Your order\"),\n        ValidationDirective(\"name\", exp=AutoExp, validator=CSSValidator(\"#name\")),\n    ]\n\n    # the template used to generate the fake html\n    template = \"\"\"\n<title>Your order</title>\n<script>\nconst csrf_token = '{{csrf}}';\n</script>\n<body>\n    Hi&nbsp;<span id=\"name\">{{ name }}</span>&nbsp;!<br/> \n    It is now<span class=\"timestamp\">{{ timestamp }}</span>.<br/>\n    Your order is:\n    <ul>\n        <li class=\"orderline\">{{line1}}</li>\n        <li class=\"orderline\">{{line2}}</li>\n        <li class=\"orderline\">{{line3}}</li>\n    </ul>\n</body>\n    \"\"\"\n\n\n@unittest.skipUnless(False, \"this is an intentional failure\")\nclass Test_Features_Regex(Test_Features):\n    \"\"\" This should fail \"\"\"\n\n    cls_validators = [  # \ud83d\udc47                      #\ud83d\udc47\n        ValidationDirective(\n            \"item\", exp=re.compile(\"^Item\"), validator=CSSValidator(\"li.orderline\")\n        )\n    ]\n\n    name = \"Mr. Rabbit\"\n    line1 = \"Item 1\"\n    line2 = \"Item 2\"\n    line3 = \"Bad line 3\"\n\n\n````\n\n\n\n=======\nHistory\n=======\n\n0.1.0 (2018-07-09)\n------------------\n\n* package created\n\n\n0.2.0 (2019-08-14)\n------------------\n\n* First release on PyPI.\n  \n\n0.2.1 (2019-08-15)\n------------------\n\n*  fixed the bad urls from lazy_regression_tests to lazy-regression-tests.  Github link should work now\n   \n0.2.3 (2019-08-18)\n------------------\n\n*  updated README.md\n\n0.5.4 (2020-03-03)\n------------------\n\n*  updated README.md\n*  reworked architecture around FilterDirectives and ValidationDirectives, with a metaclass to provide class inheritance\n*  better tests and test coverage", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/jpeyret/lazy-regression-tests", "keywords": "lazy_regression_tests", "license": "MIT license", "maintainer": "", "maintainer_email": "", "name": "lazy-regression-tests", "package_url": "https://pypi.org/project/lazy-regression-tests/", "platform": "", "project_url": "https://pypi.org/project/lazy-regression-tests/", "project_urls": {"Homepage": "https://github.com/jpeyret/lazy-regression-tests"}, "release_url": "https://pypi.org/project/lazy-regression-tests/0.5.4/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Easier automatic validation and regression testing", "version": "0.5.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Travis CI: <a href=\"https://travis-ci.org/jpeyret/lazy-regression-tests\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4cb1c1fe55069af477d92d8c2dc0ec9a6da9c78e/68747470733a2f2f7472617669732d63692e6f72672f6a7065797265742f6c617a792d72656772657373696f6e2d74657374732e7376673f6272616e63683d6d6173746572\"></a>  <img alt=\"Coverage\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e49af82882c883db09205537a7759a17bb4465a6/68747470733a2f2f636f6465636f762e696f2f67682f6a7065797265742f6c617a792d72656772657373696f6e2d74657374732f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></p>\n<h2>Automated regression tests</h2>\n<h3>Let's take a simple web page that has some variable data.</h3>\n<p>Driving it is some simplistic markup, and a mocked up http unittest that varies some data like the time stamp and a hidden csrf token.</p>\n<p><img alt=\"screenshot of page with constantly-changing elements\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2a774637cd6134a7e1cd2d8c8ae78660ec5aed26/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a7065797265742f6c617a792d72656772657373696f6e2d74657374732f3034392e6c617a792e3030302e7061636b6167696e672f646f63732f73637265656e73686f74732f3030312e66697273745f72756e2e706e67\"></p>\n<h3>Goals</h3>\n<p>We want to do the following, <em>with less than 10 lines of code</em>:</p>\n<ul>\n<li>regression test that the page HTML is the same from run to run</li>\n<li>validate basic HTML behavior, like <code>title</code>, 200 <code>status_code</code> and http <code>content_type</code></li>\n<li>check that <code>name</code> in the greeting corresponds to what's set on the unittest configuration</li>\n<li>an additional feature is the presence of a <strong>csrf</strong> random token.</li>\n</ul>\n<p>Obviously, both the csrf token and the time stamp need to be disregarded from run to run.</p>\n<p>If anything else changes, we want to fail the test automatically.  The tester can then examine what changed and determine whether to accept the new HTML as the new baseline.  <strong>Cosmetic HTML changes are for designers to worry about, test code shouldn't have to change.</strong></p>\n<h3>What a full test looks like, minus server/response mocking:</h3>\n<p><a href=\"https://github.com/jpeyret/lazy-regression-tests/blob/master/tests/test_usage.py\" rel=\"nofollow\">full test code, under <code>class Test_Features</code></a></p>\n<pre><code># generic lazy functionality\nfrom lazy_regression_tests import (\n    LazyMixin,\n    ValidationDirective,\n    FilterDirective,\n    AutoExp,\n    RegexRemoveSaver,\n)\n\n# specific to HTTP/HTML validations\nfrom lazy_regression_tests.http_validators import (\n    HTMLValidationMixin,\n    ResponseHTML,\n    HtmlFilterManager,\n    CSSRemoveFilter,\n    CSSValidator,\n)\n\nclass LazyMixinBasic(LazyMixin):\n    \"\"\" base Mixin class for the lazy test framework \"\"\"\n\n    lazy_filename = LazyMixin.get_basename(__name__, __file__, __module__)\n    cls_filters = dict(html=HtmlFilterManager())\n\n\n# \ud83d\udc47  \ud83e\udd66\ud83e\udd66 filter changing contents\nfilter_variables = [\n    FilterDirective(\n        \"timestamp\", filter_=CSSRemoveFilter(\"span.timestamp\", \"timestamp\", scalar=True)\n    ),\n    FilterDirective(\"csrf\", filter_=RegexRemoveSaver(\"csrf_token\", \"csrf_token\")),\n]\n\n\nclass Test_Features(Helper, HTMLValidationMixin, LazyMixinBasic, unittest.TestCase):\n\n    cls_filters = dict(html=filter_variables)  # \ud83d\udc48 This is how we add the filters\n\n    name = \"Mr. Rabbit\"  # picked up by `AutoExp` below\n\n    # \ud83d\udc47 setting up the validations \ud83e\udd66\ud83e\udd66\n    cls_validators = [\n        ValidationDirective(\"title\", exp=\"Your order\"),\n        ValidationDirective(\"name\", exp=AutoExp, validator=CSSValidator(\"#name\")),\n    ]\n\n    @mock.patch.dict(os.environ, di_mock_env)\n    def test_it(self):\n        try:\n            # this comes from `requests`, could be a Django testserver, your code...\n            http_response = get_fake_response_from_template(self)\n\n            # if we re-use filters and validations, yes, we use a lot less than\n            # 10 lines of code \ud83d\udc47 \ud83c\udf70\ud83c\udf70\ud83c\udf70\ud83c\udf70\n\n            # \"adapt\" standard http_response by tracking content_type, status_code, headers...\n            response = ResponseHTML(http_response)\n            # Check validation such as content_type and status code\n            self.check_expectations(response=response)\n            # Regression test - did we get the same contents as the last time?\n            tmp = self.assert_exp(response.content, \"html\")\n\n        except (Exception,) as e:\n            raise\n\n</code></pre>\n<h2>THE DETAILS...</h2>\n<p>Let's start out by assuming we <strong>didn't</strong> have the filters configured yet, so data changes from run to run.</p>\n<p><a href=\"https://github.com/jpeyret/lazy-regression-tests/blob/master/tests/test_doc.py\" rel=\"nofollow\">full test code, with more examples, under <code>class Test_Features</code></a></p>\n<h3>Trigger a failure by running the test twice.</h3>\n<pre><code>pytest -q test_doc::Test_Features::test_it\npytest -q test_doc::Test_Features::test_it\n</code></pre>\n<p>The first time works just fine - this test has never been run so the received data is taken as the expected data for this test.</p>\n<p>The second run however triggers a test failure because the crsf token and timestamp don't match the first pass.</p>\n<pre><code>E           \u274c\u274c\u274cRegressions found:\n\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\n\nE               &lt;script&gt;\nE           -    const csrf_token = 'VRzFbhbVZnzWZQlmr6xd';\nE           +    const csrf_token = 'E0z05wqHH0I6msv7iouB';\n\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\nE               &lt;br/&gt;\nE               It is now\nE               &lt;span class=\"timestamp\"&gt;\nE           -    2020-02-23 19:04:52.777187\nE           ?                  ^^    --- ^^\nE           +    2020-02-23 19:17:52.106223\nE           ?                  ^^     ^^^^^\n\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\nE           \u274c\u274c\u274c\n\n</code></pre>\n<h2>Fix : filter out changing data</h2>\n<p>We want disregard the csrf token and the timestamp data when comparing runs.  To do that, we add two filters to our processing of <code>HTML</code> data, one for the csrf and one for the time stamp.</p>\n<pre><code>filter_variables = [\n    FilterDirective(\n        \"timestamp\"\n        ,filter_=CSSRemoveFilter(\"span.timestamp\", \"timestamp\", scalar=True)\n    ),\n    FilterDirective(\n    \t\"csrf\"\n    \t, filter_=RegexRemoveSaver(\"csrf_token\", \"csrf_token\")),\n]\n</code></pre>\n<h4>Add the filters to the test class:</h4>\n<p>Note that our unittest inherits from <code>LazyMixinBasic</code>, which enables the whole lazy-test framework.</p>\n<pre><code>#  \u2699\ufe0f   This enables the lazy-test framework     \ud83d\udc47   \nclass Test_Features(Helper, HTMLValidationMixin, LazyMixinBasic, unittest.TestCase):\n    \"\"\" this is the test we are running here \"\"\"\n\n    cls_filters = dict(html=filter_variables)  # \ud83d\udc48 This is how we add the filters\n\n    name = \"Mr. Rabbit\"\n    line1 = \"Item 1\"\n    ...\n</code></pre>\n<h3>Write as little as possible:</h3>\n<p>The actual test code to use in each test method is very limited.  Yes, you will need to declare those filters and validations, but they are designed to be shared across multiple test classes.</p>\n<p>Most of the behavior is built-in, once you inherit from a base class.  Filters, and validators which we will see later, are inherited from class to class via Python's standard MRO and some metaclass tweaking.  No <code>setUp/tearDown</code> are needed, but you can use yours as usual.</p>\n<h3>The actual test class: <code>Test_Features</code></h3>\n<p>We define <code>Test_Features</code> to be both a LazyMixin - which enables lazy-testing - and a TestCase.  <code>Helper</code> isn't super important, it's just there to support the fake html requests.</p>\n<pre><code>                            # \ud83d\udc47\u2699\ufe0fThis enables the lazy-test framework\nclass Test_Features(Helper, HTMLValidationMixin, LazyMixinBasic, unittest.TestCase):\n    \"\"\" this is the test we are running here \"\"\"\n\n    cls_filters = dict(html=filter_variables)  # \ud83d\udc48 This is how we add the filters\n\n    name = \"Mr. Rabbit\"\n    line1 = \"Item 1\"\n    line2 = \"Item 2\"\n    line3 = \"Item 3\"\n\n\t # \ud83d\udc47 setting up the validations\n    cls_validators = [\n        ValidationDirective(\"title\", exp=\"Your order\"),\n        ValidationDirective(\"name\", exp=AutoExp, validator=CSSValidator(\"#name\")),\n    ]\n\n    # the template used to generate the fake html\n    template = \"\"\"\n&lt;title&gt;Your order&lt;/title&gt;\n&lt;script&gt;\nconst csrf_token = '{{csrf}}';\n&lt;/script&gt;\n&lt;body&gt;\n    Hi&amp;nbsp;&lt;span id=\"name\"&gt;{{ name }}&lt;/span&gt;&amp;nbsp;!&lt;br/&gt; \n    It is now&lt;span class=\"timestamp\"&gt;{{ timestamp }}&lt;/span&gt;.&lt;br/&gt;\n    Your order is:\n    &lt;ul&gt;\n        &lt;li class=\"orderline\"&gt;{{line1}}&lt;/li&gt;\n        &lt;li class=\"orderline\"&gt;{{line2}}&lt;/li&gt;\n        &lt;li class=\"orderline\"&gt;{{line3}}&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/body&gt;\n    \"\"\"\n</code></pre>\n<h2>Run the test again</h2>\n<p>The test still fails - the csrf token and timestamps are gone, <em>but only from the newly received data</em>.</p>\n<p>Things <em>are</em> slightly different.  Before, we had 2 csrf tokens and two time stamps, one before (<code>19:04</code>), one after (<code>19:23</code>).  Now it seems we only see the old ones, the new ones are being filtered out.</p>\n<p>We need a way to accept the new data as the reference.  There are two ways to do that.</p>\n<pre><code>E           AssertionError:\nE\nE           \u274c\u274c\u274cRegressions found:\n\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\nE               &lt;script&gt;\nE           -    const csrf_token = 'VRzFbhbVZnzWZQlmr6xd';\nE               &lt;/script&gt;\n\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\nE               &lt;br/&gt;\nE               It is now\nE           -   &lt;span class=\"timestamp\"&gt;\nE           -    2020-02-23 19:04:52.777187\nE           -   &lt;/span&gt;\nE               &lt;ul&gt;\n\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\u2702\ufe0f\n</code></pre>\n<h3>Option #1 - reset expectations via <code>baseline</code> directive.</h3>\n<p>We need to tell the test to accept the new format as valid.  Telling <code>assert_exp</code> that you want to treat new data as valid is what the <strong><code>directive=baseline</code>  mechanism</strong> is made for, which uses the environment variable <code>$lzrt_directive</code>.</p>\n<p>Let's try it out:</p>\n<pre><code>lzrt_directive=baseline pytest -q test_doc::Test_Features::test_it\n.                                                                                                  [100%]\n1 passed in 0.25s\n</code></pre>\n<p>What does <code>lzrt_directive=baseline</code> do?  Whenever it is set, calling <code>assert_exp</code> overwrites the saved expectations file with the incoming, now filtered, data.</p>\n<h3>Option #2 - overwrite the expectations file</h3>\n<p>Instead of using <code>baseline</code>, you can also just copy the received/got formatted file over to the expected reference file.  Or you can just delete the expectations file entirely.</p>\n<p>The error message lists the paths:</p>\n<pre><code>\u274c\u274c\u274cRegressions found:\n\nExpected contents are in file:\n  ...tests/lazy/exp/Test_Features/test_doc.Test_Features.test_it.html\n\nReceived contents are in file:\n  .../lazy/got/Test_Features/test_doc.Test_Features.test_it.html\n\n....\n</code></pre>\n<p>copy Received to Expected and the test will also pass next time.</p>\n<h3>Test-driven development:</h3>\n<p>Let's say we want to support styles on the order lines as in:</p>\n<pre><code>&lt;li class=\"orderline\"&gt;Item 1&lt;/li&gt;\n</code></pre>\n<p>Just edit it manually in the expectations file.</p>\n<pre><code>  Your order is:\n  &lt;ul&gt; \ud83d\udc47\n   &lt;li class=\"orderline\"&gt;\n    Item 1\n  ...\n</code></pre>\n<p>and now the test fails until the application code has been updated:</p>\n<pre><code>\u274c\u274c\u274cRegressions found:\n....\n    &lt;ul&gt;\n-    &lt;li class=\"orderline\"&gt;\n+    &lt;li&gt;\n      Item 1\n</code></pre>\n<p>and succeeds afterwards:</p>\n<pre><code>pytest -q test_doc::Test_Features::test_it\n.                                                                                                  [100%]\n1 passed in 0.29s\n</code></pre>\n<h2>Validations</h2>\n<p>Validations are separate from the regression tests, but work with them.  If you get an HTML 404 response instead of your expected JSON response, it is much more informative to be told that your response is in error status and of the wrong content type rather than looking at a big dump of mismatched expected JSON vs received HTML.</p>\n<p>Let's fake out a bad response by setting it on the response.</p>\n<pre><code>response = ResponseHTML(http_response)\n\nresponse.status_code = 404  \ud83d\udc48 fake!\nself.validationmgr.debug()  \ud83d\udc48 and we will use this inspect the validations\n \n</code></pre>\n<p>We get the expected error:</p>\n<pre><code>E   AssertionError:\nE\nE   \u274c\u274c\u274c\u274c\u274c\u274c\u274c\nE\nE   Validation failure @ status_code [StatusCodeValidator[selector=status_code]]:\nE\nE   False is not true : status_code exp:200&lt;&gt;404:got\nE\nE   \u274c\u274c\u274c\u274c\u274c\u274c\u274c\n\n</code></pre>\n<p>Where did this come from?  class <code>Test_Features</code>'s validations has 2 validations defined on it:</p>\n<h4>name</h4>\n<p>will use a CSS selector to find the matching element and the expection for it is <code>AutoExp</code>.  AutoExp basically will cause the validator to look for a matching attribute in our unittest, i.e. <code>name</code> i.e. \"Mr. Rabbit\".</p>\n<h4>title</h4>\n<p>only has a hardcoded expectation and no apparent strategy to validate it.</p>\n<pre><code>                            # HTTP/HTML validation inheritance\n                            # \ud83d\udc47 \nclass Test_Features(Helper, HTMLValidationMixin, LazyMixinBasic, unittest.TestCase):\n\n     # \ud83d\udc47 setting up the validations\n    cls_validators = [\n        ValidationDirective(\"title\", exp=\"Your order\"),\n        ValidationDirective(\"name\", exp=AutoExp, validator=CSSValidator(\"#name\")),\n    ]\n\n</code></pre>\n<p>Let's see what that debug function call has to say.  Turns out there are a number of validations.  What we are interested in is the <strong>class-level inheritance</strong>:</p>\n<pre><code>\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f  lazy-tests configuration  \u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\u2699\ufe0f\nValidationManager for test_it (__main__.Test_Features)  validators:\n  status_code         : active:True exp:200        StatusCodeValidator\n  content_type        : active:True exp:html       ContentTypeValidator\n  title               : active:True exp:Your order TitleCSSValidator\n  name                : active:True exp:Mr. Rabbit CSSValidator\n\n\n \u2699\ufe0fclass-level inheritance:\nlazy_regression_tests.lazy3.http_validators.HTTPValidationMixin\nlazy_regression_tests.lazy3.http_validators.HTMLValidationMixin\n__main__.Test_Features\n\n</code></pre>\n<h3>Validations default from class inheritance</h3>\n<p>To avoid side effects of sharing those validators and filters, the actual objects are always copied whenever they are inherited from another level.  So you can for example <em>reach into</em> a CSSValidator and change its selector for 1 class.</p>\n<p>Going from generic to specific (i.e. reverse MRO order):</p>\n<p>First we set the <code>content_type</code> validation to be active, but leaving out the expectations.</p>\n<pre><code>class HTTPValidationMixin:\n    \"\"\" sets basic expectations \n        - http is expected to return a status_code, typically 200 (exp can be changed later)\n        - and has a content_type, which changes depending on end points\n    \"\"\"\n\n    cls_validators = [\n        ValidationDirective(\"status_code\", exp=200, validator=StatusCodeValidator()),\n        ValidationDirective(\"content_type\", active=True, validator=ContentTypeValidator()),\n    ]\n\n</code></pre>\n<p>Next comes the HTML validations, which look for a <code>title</code> and also sets expections on the <code>content_type</code>:</p>\n<pre><code>title_validation = ValidationDirective(\"title\", active=True, validator=TitleCSSValidator())\n\nclass HTMLValidationMixin(HTTPValidationMixin):\n    \"\"\" set `content_type` expectations\n        always want to validate `title`, by default \n    \"\"\"\n\n    cls_validators = [title_validation, ValidationDirective(\"content_type\", exp=\"html\")]\n</code></pre>\n<h2>Writing your own validations</h2>\n<p>We haven't done that line item validation (they all need to start with \"Item\").  To do that we'll copy our class and add that validation.</p>\n<h3>Easy way - use built-in features:</h3>\n<p>Here we just add a CSS selector, with a regular expression as an expectation:</p>\n<p>We expect our test to fail, because of the bad <code>line3</code></p>\n<pre><code>    cls_validators = [              #\ud83d\udc47                      #\ud83d\udc47\n        ValidationDirective(\"item\", exp=re.compile(\"^Item\"), validator=CSSValidator(\"li.orderline\")),\n    ]\n\n    name = \"Mr. Rabbit\"\n    line1 = \"Item 1\"\n    line2 = \"Item 2\"\n    line3 = \"Bad line 3\"  \ud83d\udc48\u274c we're expecting a failure here\n</code></pre>\n<p>Sure enough:</p>\n<pre><code>\u274c\u274c\u274c\u274c\u274c\u274c\u274c\n\nValidation failure @ item [CSSValidator[selector=li.orderline]]:\n\nNone is not true : item pattern:^Item:does not match:['Item 1', 'Item 2', 'Bad line 3']:got\n\n\u274c\u274c\u274c\u274c\u274c\u274c\u274c\n</code></pre>\n<h3>Or, write your own validation:</h3>\n<p>We can write our own validation function, which always receives:</p>\n<ul>\n<li><code>testee</code> : the unittest to assert against</li>\n<li><code>got</code> :which is what the validator selection criteria has found</li>\n<li>the validator itself</li>\n</ul>\n<p>We inherit our failed Regex-based test, so we can re-use the CSS selector but just provide a custom validation function.</p>\n<pre><code>def check_lineitems(testee: \"unittest.TestCase\", got, validator: \"Validator\"):\n    \"\"\"\n    `got` will be a list of strings here \n    \ud83d\udc49keeping this as stand alone function, rather a method of your TestCase\n    means it can be used anywhere.  It still behaves just like a test method\n    \"\"\"\n    try:\n        for igot in got:\n            if not igot.endswith(\"3\"):\n                testee.assertTrue(igot.startswith(\"Item\"))\n    # pragma: no cover pylint: disable=unused-variable\n    except (Exception,) as e:\n        if cpdb(): \n            pdb.set_trace()\n        raise\n\nclass Test_Features_CustomLineValidation(Test_Features_Regex):\n    \"\"\" This should pass, we are re-using the CSSValidation lookup for `item`\"\"\"\n\n    #   \ud83d\udc47\n    cls_validators = [ValidationDirective(\"item\", exp=check_lineitems)]  #   \ud83d\udc47\n\n    name = \"Mr. Rabbit\"\n    line1 = \"Item 1\"\n    line2 = \"Item 2\"\n    line3 = \"Bad line 3\"\n</code></pre>\n<h4>The reason this validator sends a list of string:</h4>\n<p>Configured differently it could give you one or more DOM nodes to validate.</p>\n<pre><code>CSSValidator[selector=li.orderline] ''\ncargo=None\nscalar=False \ud83d\udc48\nselector='li.orderline'\nsourcename='response.selectable'\nto_text=True \ud83d\udc48\n</code></pre>\n<p>For example, the <code>title</code> validator is preset differently because there's only one <code>&lt;title&gt;</code> tag.</p>\n<pre><code>TitleCSSValidator[selector=title] ''\nscalar=True\nselector='title'\nsourcename='response.selectable'\n</code></pre>\n<h3>Adjustments can be made within a test method</h3>\n<p>Here our validations would fail on the <code>name</code> and <code>title</code>, so we turn them off (which we could have done in the <code>cls_validators</code> as wel. We also turn off timestamp filtering.</p>\n<pre><code>class Test_Turning_ThingsOff(Test_Features):\n    \"\"\" we don't have a title or a greeting anymore\n        and we don't need to filter out the timestamp either\n    \"\"\"\n\n    template = \"\"\"\n&lt;body&gt;\n    It is now&lt;span class=\"timestamp\"&gt;fake, fixed, timestamp&lt;/span&gt;.&lt;br/&gt;\n&lt;/body&gt;\n\"\"\"\n    def setUp(self):\n            # \ud83d\udc47 turn these off to avoid validation errors\n            self.set_expectation(\"title\", active=False)\n            self.set_expectation(\"name\", active=False)\n            self.filters[\"html\"].set_filter(\"timestamp\", active=False)  #\ud83d\udc48 keep it\n\n</code></pre>\n<h2>Wrapping things up</h2>\n<h3>Access filtered-out data:</h3>\n<p>You may have noticed a little <code>tmp</code> variable by each <code>assert_exp</code> call.</p>\n<pre><code>tmp = self.assert_exp(response.content, \"html\")\n</code></pre>\n<p>That result object has a number of attributes and is available even if there is an AssertionError, via <code>self.lazytemp</code>.  What we are most interested in is <code>tmp.filtered</code> which is a dictionary that holds lists of filtered values (but only if the filter was given a name).</p>\n<pre><code>class.Dummy ''\ncsrf_token=[\"   const csrf_token = 'M334JV6eNwJh9yFaFa89';\"]\ntimestamp=?\n</code></pre>\n<h3>Throttling validations:</h3>\n<p>Sometimes you want to turn off most of your validations in a test method without having to redefine things at the class level.  Let's say I want to check a 404 in a complex URL - I may want to provide all the correct config for the test, using my base TestCase, but then just provide an incorrect ID for the data lookup.</p>\n<p>In that case, most of my validations for the normal pages would fail, so I only to run a subset, maybe confirm <code>status_code=404</code> and the <code>content_type</code>.</p>\n<pre><code># fake a 404\nresponse.content = \"&lt;div&gt;404, to you, buddy!&lt;/div&gt;\"\nresponse.status_code = 404\n\t\n# This handles the 404, however, the title and name would error out           \nself.set_expectation(\"status_code\", 404)\n\t\n# \ud83d\udc47 turn off what you don't need\nconstants_keep_on_404 = [\"status_code\", \"content_type\"]\nself.check_expectations(response=response, lazy_skip_except=constants_keep_on_404)\n\n</code></pre>\n<h3>Not limited to HTML</h3>\n<h4>JSON, YAML and SQL work too.</h4>\n<p>This was written to validate both HTML and embedded JSON within that HTML.</p>\n<p>The JSON Filter has its own Validators and Filters, which operate on dictionaries.  Aside from the serialization dump format, YAML works almost the same way.</p>\n<p>The SQL manager takes any big SQL query string, applies some formatting to normalize it a bit and can be used to keep track of changes to these queries.  It can be useful if you have an ORM-layer that you want to monitor for feature stability from version to version.</p>\n<p>Other formats can be supported, provided there is a consistent way to store them in files and load them back up.</p>\n<pre><code>from lazy_regression_tests.lazy3.filters import JsonFilterManager,\n\n\nclass Test_JSON_Too(LazyMixinBasic, unittest.TestCase):\n    \"\"\" just connect it to the appropriate filter manager for \n    the extension type\n    \"\"\"\n\n    from lazy_regression_tests.lazy3.filters import JsonFilterManager\n    cls_filters = dict(json=JsonFilterManager())\n    extension = \"json\"\n\n    def test_it(self, data={}):\n        \"\"\" fetch data, run validations, regression test \"\"\"\n        try:\n            data = dict(\n                var1=\"the_same\",\n                var2=\"will_change\",\n                )\n            tmp = self.assert_exp(data, self.extension)\n            data.update(var2=\"told you so\")\n            tmp = self.assert_exp(data, self.extension)\n        except (Exception,) as e:\n            raise\n\n\nclass Test_YAML(Test_JSON_Too):\n    \"\"\" hey, most of the work was done by the JSON guys already\n    \"\"\"\n\n    from lazy_regression_tests.lazy3.yaml_validators import YAMLFilter\n\n    extension = \"yaml\"\n    cls_filters = dict(yaml=YAMLFilter())\n</code></pre>\n<p>As, expected with this test scenario, both error out:</p>\n<h4>JSON:</h4>\n<pre><code>E             {\nE                 \"var1\": \"the_same\",\nE           -     \"var2\": \"will_change\"\nE           +     \"var2\": \"told you so\"\nE             }\nE           \u274c\u274c\u274c\n\n../lazy_regression_tests/lazy3/core.py:498: AssertionError\n======================================== short test summary info =========================================\nFAILED test_doc.py::Test_JSON_Too::test_it - AssertionError:\n</code></pre>\n<h4>You can, of course, view differences in diff-type tools:</h4>\n<p><img alt=\"view via a GUI diff\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b3bc2ad31f3bf690645908bc11a0a600534b52f0/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a7065797265742f6c617a792d72656772657373696f6e2d74657374732f3034392e6c617a792e3030302e7061636b6167696e672f646f63732f73637265656e73686f74732f3030322e646966665f6a736f6e2e706e67\"></p>\n<h4>YAML:</h4>\n<pre><code>E           Original exception:\nE            'var1: the_same\\nvar2: will_change' != 'var1: the_same\\nvar2: told you so'\nE             var1: the_same\nE           - var2: will_change+ var2: told you so\nE           \u274c\u274c\u274c\n\n../lazy_regression_tests/lazy3/core.py:498: AssertionError\n======================================== short test summary info =========================================\nFAILED test_doc.py::Test_YAML::test_it - AssertionError:\n</code></pre>\n<h3>Filter JSON/YAML data:</h3>\n<pre><code>class Test_JSON_Filter(Test_JSON_Too):\n    \"\"\" let's fix the above error by filtering out the changing key\"\"\"\n\n    cls_filters = dict(\n        json=FilterDirective(\n            \"changing\",\n            # DictFilter work by looking for matching keys in the target dictionary.\n            # and then call their value if it's a callable.  \n            # value=None is the default behavior and just deletes the key in the target\n            # \ud83d\udc47\n            DictFilter(dict(var2=None), \"changing\"))\n    )\n</code></pre>\n<h4>fixed:</h4>\n<pre><code>(venv) jluc@exp$ pytest -q $v3oiut::Test_JSON_Filter\n.                                                                                                  [100%]\n\n1 passed in 0.23s\n</code></pre>\n<h4>You can even look at arbitrary object graphs</h4>\n<p>(As long as it is YAML-serializable)</p>\n<pre><code>class Subvar:\n    def __init__(self, value):\n        self.value = value\n\nclass SomethingToTest:\n    def __init__(self):\n        self.var1 = 11\n        self.var2 = 12\n        self.var4 = dict(FF=\"Fantastic\")\n\nclass Test_YAML_Graphs(Test_YAML):\n\n    def test_going_down_the_rabbit_hole(self, data={}):\n        \"\"\" fetch data, run validations, regression test \"\"\"\n        try:\n            from yaml import dump as ydump, load as yload\n\n            somethingtotest = SomethingToTest()\n            somethingtotest.var3 = Subvar(\"3\")\n\n            yaml_ = ydump(somethingtotest)\n\n            #probably not a good idea with untrusted data\n            data = yload(yaml_)\n\n            self.assert_exp(data, self.extension)\n            somethingtotest.added_this = dict(somevar=\"somevalue\")\n            somethingtotest.var3.value=\"3++\"\n\n            yaml_ = ydump(somethingtotest)\n            data = yload(yaml_)\n            self.assert_exp(data, self.extension)\n\n        except (Exception,) as e:\n            raise\n\n</code></pre>\n<h4>Yay! An error.</h4>\n<pre><code>E           + added_this:\nE           +   somevar: somevalue\nE             var1: 11\nE             var2: 12\nE             var3: !!python/object:tests.test_doc.Subvar\nE           -   value: '3'\nE           ?          - ^\nE           +   value: 3++\nE           ?           ^^\nE             var4:\nE               FF: Fantastic\nE           \u274c\u274c\u274c\n../lazy_regression_tests/lazy3/core.py:498: AssertionError\n======================================== short test summary info =========================================\nFAILED test_doc.py::Test_YAML_Graphs::test_going_down_the_rabbit_hole - AssertionError:\n1 failed in 0.33s\n</code></pre>\n<h3>Full test code: <a></a></h3>\n<p>Each module needs to define a base mixin to add the lazy-test behavior.  To support saving files according to the module name, this needs to be done once for each test module.  The test method, <code>test_it</code> was added to this class here, but only because its pretty generic at this point.</p>\n<pre><code>\nclass LazyMixinBasic(LazyMixin):\n\t\"\"\" base Mixin class for the lazy test framework \"\"\"\n\n    # \ud83d\udc47 \u2699\ufe0f tracks where expectation and received files are saved\n    lazy_filename = LazyMixin.get_basename(__name__, __file__, __module__)\n\n    extension = \"html\"\n\n    # \ud83d\udc47 \u2699\ufe0f Tells the framework what extensions/content to expect\n    cls_filters = dict(html=HtmlFilterManager())\n\n\t#.... cut out the test_it() method... we've seen it befor...\n\n\nclass Test_Features(Helper, HTMLValidationMixin, LazyMixinBasic, unittest.TestCase):\n    \"\"\" this is the test we are running here \"\"\"\n\n    cls_filters = dict(html=filter_variables)  # \ud83d\udc48 This is how we add the filters\n\n    name = \"Mr. Rabbit\"\n    line1 = \"Item 1\"\n    line2 = \"Item 2\"\n    line3 = \"Item 3\"\n\n    # \ud83d\udc47 setting up the validations\n    cls_validators = [\n        ValidationDirective(\"title\", exp=\"Your order\"),\n        ValidationDirective(\"name\", exp=AutoExp, validator=CSSValidator(\"#name\")),\n    ]\n\n    # the template used to generate the fake html\n    template = \"\"\"\n&lt;title&gt;Your order&lt;/title&gt;\n&lt;script&gt;\nconst csrf_token = '{{csrf}}';\n&lt;/script&gt;\n&lt;body&gt;\n    Hi&amp;nbsp;&lt;span id=\"name\"&gt;{{ name }}&lt;/span&gt;&amp;nbsp;!&lt;br/&gt; \n    It is now&lt;span class=\"timestamp\"&gt;{{ timestamp }}&lt;/span&gt;.&lt;br/&gt;\n    Your order is:\n    &lt;ul&gt;\n        &lt;li class=\"orderline\"&gt;{{line1}}&lt;/li&gt;\n        &lt;li class=\"orderline\"&gt;{{line2}}&lt;/li&gt;\n        &lt;li class=\"orderline\"&gt;{{line3}}&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/body&gt;\n    \"\"\"\n\n\n@unittest.skipUnless(False, \"this is an intentional failure\")\nclass Test_Features_Regex(Test_Features):\n    \"\"\" This should fail \"\"\"\n\n    cls_validators = [  # \ud83d\udc47                      #\ud83d\udc47\n        ValidationDirective(\n            \"item\", exp=re.compile(\"^Item\"), validator=CSSValidator(\"li.orderline\")\n        )\n    ]\n\n    name = \"Mr. Rabbit\"\n    line1 = \"Item 1\"\n    line2 = \"Item 2\"\n    line3 = \"Bad line 3\"\n\n\n</code></pre>\n<h1>=======\nHistory</h1>\n<h2>0.1.0 (2018-07-09)</h2>\n<ul>\n<li>package created</li>\n</ul>\n<h2>0.2.0 (2019-08-14)</h2>\n<ul>\n<li>First release on PyPI.</li>\n</ul>\n<h2>0.2.1 (2019-08-15)</h2>\n<ul>\n<li>fixed the bad urls from lazy_regression_tests to lazy-regression-tests.  Github link should work now</li>\n</ul>\n<h2>0.2.3 (2019-08-18)</h2>\n<ul>\n<li>updated README.md</li>\n</ul>\n<h2>0.5.4 (2020-03-03)</h2>\n<ul>\n<li>updated README.md</li>\n<li>reworked architecture around FilterDirectives and ValidationDirectives, with a metaclass to provide class inheritance</li>\n<li>better tests and test coverage</li>\n</ul>\n\n          </div>"}, "last_serial": 6744963, "releases": {"0.2.0": [{"comment_text": "", "digests": {"md5": "6ef1874f71e5ce0326f8eec107b7ab17", "sha256": "b058e43b53e5cf7385173cba817d92637a0690e3efa513f02702088fce8fa311"}, "downloads": -1, "filename": "lazy_regression_tests-0.2.0.tar.gz", "has_sig": false, "md5_digest": "6ef1874f71e5ce0326f8eec107b7ab17", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 177153, "upload_time": "2019-08-14T21:14:35", "upload_time_iso_8601": "2019-08-14T21:14:35.647077Z", "url": "https://files.pythonhosted.org/packages/8b/b9/13e17049068c18309964c24b6bc6a91a09709d06d18aa2606ad7e0d1735c/lazy_regression_tests-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "247ff586b05104bba4e45c5a418b5a45", "sha256": "fd29d5fca479b36a824dffa751f57e900eeba64e716ae033a75ee1116e53524c"}, "downloads": -1, "filename": "lazy_regression_tests-0.2.1.tar.gz", "has_sig": false, "md5_digest": "247ff586b05104bba4e45c5a418b5a45", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 177159, "upload_time": "2019-08-15T18:14:37", "upload_time_iso_8601": "2019-08-15T18:14:37.320026Z", "url": "https://files.pythonhosted.org/packages/ba/1b/58c322b66c16e9eb00d120975f17c2fe92d4d60f45efecab195210ac1388/lazy_regression_tests-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "b623c36565423eefb6ee6f0de6db0758", "sha256": "89681bc8ae594bf74b46eb87e2b6371afac0c1c8ea55e4575b612d86a4a4ccb7"}, "downloads": -1, "filename": "lazy_regression_tests-0.2.2.tar.gz", "has_sig": false, "md5_digest": "b623c36565423eefb6ee6f0de6db0758", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 125807, "upload_time": "2019-08-18T19:31:22", "upload_time_iso_8601": "2019-08-18T19:31:22.328908Z", "url": "https://files.pythonhosted.org/packages/5c/dc/5eac477cda599e0e1bdc8edc7bf937aa3e41799fc2788f05b58040ff80f4/lazy_regression_tests-0.2.2.tar.gz", "yanked": false}], "0.2.3": [{"comment_text": "", "digests": {"md5": "c17c40e73d4a1c3f50938e0fbbb37add", "sha256": "ce73662a90977071ef2e92f29b0acef8f70d0f78e5a269eb1f862c77b91ca0c7"}, "downloads": -1, "filename": "lazy_regression_tests-0.2.3.tar.gz", "has_sig": false, "md5_digest": "c17c40e73d4a1c3f50938e0fbbb37add", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 125855, "upload_time": "2019-08-18T19:50:42", "upload_time_iso_8601": "2019-08-18T19:50:42.887021Z", "url": "https://files.pythonhosted.org/packages/fe/3d/98a390d6018231e68abfd6bff1bda042cfea1beac25499eacdbebb9b65ce/lazy_regression_tests-0.2.3.tar.gz", "yanked": false}], "0.5.2": [{"comment_text": "", "digests": {"md5": "559396451061b062e2c294d2cdd322da", "sha256": "64b330e6bbf7a951930b79db42267ab915af412f4a5a6a36309efbe0e004e174"}, "downloads": -1, "filename": "lazy_regression_tests-0.5.2.tar.gz", "has_sig": false, "md5_digest": "559396451061b062e2c294d2cdd322da", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 288393, "upload_time": "2020-03-04T02:34:55", "upload_time_iso_8601": "2020-03-04T02:34:55.276065Z", "url": "https://files.pythonhosted.org/packages/56/85/f80397eac00cfa8fa71b9ea5565a8f7f7e01f78403f1b8859d23b171f790/lazy_regression_tests-0.5.2.tar.gz", "yanked": false}], "0.5.3": [{"comment_text": "", "digests": {"md5": "4acc5cc58b1df472775555d6df41e447", "sha256": "0ee3e1d67b0adec3e35fe53c37c559f5d250f8e1293a4e3c2fe1e1e2a2fc348c"}, "downloads": -1, "filename": "lazy_regression_tests-0.5.3.tar.gz", "has_sig": false, "md5_digest": "4acc5cc58b1df472775555d6df41e447", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 288510, "upload_time": "2020-03-04T02:34:56", "upload_time_iso_8601": "2020-03-04T02:34:56.837155Z", "url": "https://files.pythonhosted.org/packages/ce/55/a5dcb70485008d5e0c0d789a3084ca419742cd99d8b07c4aa68d1211cd9f/lazy_regression_tests-0.5.3.tar.gz", "yanked": false}], "0.5.4": [{"comment_text": "", "digests": {"md5": "423a0fccbf51e44a98b7640ac8754c6f", "sha256": "54ef3be40d6da057fa0eb83eedad95d68cd96371139817bab8e283fd2e7f7b75"}, "downloads": -1, "filename": "lazy_regression_tests-0.5.4.tar.gz", "has_sig": false, "md5_digest": "423a0fccbf51e44a98b7640ac8754c6f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 288550, "upload_time": "2020-03-04T02:34:58", "upload_time_iso_8601": "2020-03-04T02:34:58.450362Z", "url": "https://files.pythonhosted.org/packages/3f/c0/69bb082366d210fc86ba1f38fff8729e902ec18f396b9157af9d6d08f755/lazy_regression_tests-0.5.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "423a0fccbf51e44a98b7640ac8754c6f", "sha256": "54ef3be40d6da057fa0eb83eedad95d68cd96371139817bab8e283fd2e7f7b75"}, "downloads": -1, "filename": "lazy_regression_tests-0.5.4.tar.gz", "has_sig": false, "md5_digest": "423a0fccbf51e44a98b7640ac8754c6f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 288550, "upload_time": "2020-03-04T02:34:58", "upload_time_iso_8601": "2020-03-04T02:34:58.450362Z", "url": "https://files.pythonhosted.org/packages/3f/c0/69bb082366d210fc86ba1f38fff8729e902ec18f396b9157af9d6d08f755/lazy_regression_tests-0.5.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:47:35 2020"}