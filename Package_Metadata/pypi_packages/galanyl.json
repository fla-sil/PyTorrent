{"info": {"author": "Nathan Goldbaum, John Forbes", "author_email": "nathan12343@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "#Galaxy Analysis Toolkit\n\nA collection of scripts and analysis tools used in my thesis research.\n\nThis also includes the text and version history of two as yet unpublished papers\nthat were the primary research result of the code.\n\nThe code is split up into two main pieces. The `galanyl` \"library\" lives in the\n`galanyl` directory, while `helper_scripts` contains a number of scripts that\nmake use of `yt` and `galanyl` to process simulation data.\n\n#Using these scripts\n\nFirst, you must download the data you would like to analyze. Right now the\nhelper scripts are written in a such a way that they expect the full simulation\ndataset to be present, but they should be easily modifiable or adaptable if you\nonly want to look at one or a few simulation outputs.\n\nThe data for Paper I are available at https://hub.yt/data/goldbaum2015a/\n\nThe data for Paper II are available at https://hub.yt/data/goldbaum2016a\n\n##Generating uniform resolution grid slabs\n\nNote: you can skip this step if you want to download the processed data. This\nis only necessary if you would like to generate the processed data from the\nraw simulation outputs.\n\nFor example, if you would like to regenerate the processed data for the\n``nofeedback_20pc`` simulation, you will need to download one or more of\nthe simulation outputs, unzip the tarball, and place the simulation outputs\nin a directory named ``nofeedback_20pc``.\n\nOnce you have downloaded the simulation outputs you would like to analyze, you\nneed to create the needed ancillary data in two steps:\n\n```\n$ python generate_covering_grids.py nofeedback_20pc\n```\n\nThis script uses `yt` to convert the raw simulation outputs into uniform\nresolution \"covering grids\" that the subsequent analysis scripts need. This will\nonly generate covering grid data for the fields present in the simulation\noutputs. To generate the gravitational potential covering grids, you will need\nto do:\n\n```\n$ python generate_gravitational_covering_grid.py nofeedback_20pc\n```\n\nThis script needs a copy of the `Enzo` executable (see\n[here](https://enzo.readthedocs.org/en/latest/tutorials/building_enzo.html) for\ninformation about compiling the Enzo code) since the script uses the `-g` option\nof the `Enzo` executable to to solve the Poisson equation.\n\nFinally, you can run the `validate_covering_grid.py` script verify the integrity\nof the covering grid data after it is written to disk. This script merely checks\nto make sure all hdf5 files have the same internal structure, so don't\ncompletely trust it against all possible data corruption. I created it\noriginally to avoid errors after creating incomplete covering grid files when\nthe generation script crashes or the filesystem hangs.\n\nNote that if you are doing this for the full simulation dataset it will take *a\nlong time* even if you are running on multiple cores.  Note that both\n`generate_covering_grids.py` and `generate_gravitational_covering_grid.py` are\nMPI parallelized, so you can them using e.g. `mpirun` on multiple cores to\nparallelize the analysis over multiple datasets. Note that this won't scale very\nwell unless you are running on a parallel filesystem since both scripts are\nlargely IO bound.\n\n##Generating final processed data\n\nFinally, to generate the final processed data, including maps of surface\ndensities, velocity dispersions, and Toomre Q parameters, you should use the\n`analyze_data.py` script.  This script uses the covering grid data generated\nin the previous step to create `GalaxyAnalyzer` objects -- the main analysis\nclass provided by the galaxy analysis toolkit. The `GalaxyAnalyzer` class offers\na number of analysis options to process a subset of the abailable data but also\noffers an interface to calculate all the derived data that it knows how to\ncalculate. This is the interface that `analyze_data.py` uses. The script can\nbe easily modified to only calculate a subset of the data if you do not want\n*all* of the processed data. To run the script, simply do:\n\n```\n$ python analyze_data.py\n```\n\nLike the covering grid generators, this script is also MPI-parallel, so run it\nwith `mpirun` to speed up the analysis. This script is also largely IO-bound, so\nyou will liekly not see a very good scaling unless you are running on a parallel\nfilesystem.\n\nNote also that some of the expensive calculations are parallelized using OpenMP\nso long as your operating system and compiler supports it. Right now, that means\nyou will need gcc although supposedly LLVM/Clang is also getting OpenMP support\nsoon. I optimized to iterate over the data on a single node, using MPI\nparallelism to iterate over the datasets, but breaking up the work necessary to\nprocess a given dataset using OpenMP. As a rule of thumb, I used a node with 16\ncores and used 4 MPI tasks, so each MPI task used 4 OpenMP threads in the\nOpenMP-parallelized sections of the analysis.\n\n#Loading and working with the data\n\nThe available data can be accessed via an open-source python analysis\nenvironment. The raw simulation data are written to disk in Enzo's HDF5-based\ndata format. The processed data are written to disk in a directory structure\ncontaining many simple hdf5 files that contain a single dataset.\n\n##Simulation outputs\n\nFor the raw simulation outputs, the recommended way to load them for analysis\nand visualization is using yt. You will need to install yt using either the\ninstallation script, conda, or pip, depending on whether you have a python\nenvironment set up and how you set it up. See\nhttp://yt-project.org/doc/installing.html for more details.\n\nOnce you have yt installed, download one of the raw datasets, unzip it, and then\nload it:\n\n```\n$ wget https://hub.yt/data/goldbaum2015b/feedback_20pc/simulation_outputs/DD0600.tar.gz\n$ tar xzvf DD0600.tar.gz\n$ python\n>>> import yt\n>>> ds = yt.load('DD0600/DD0600')\n```\n\nYou can then pass the `ds` object to yt's plotting commands or access yt data\nobjects through it to get at any on-disk or derived fields.\n\n##Processed Data\n\nThe easiest way to deal with the processed data is via the galaxy analysis\ntoolkit. This can be installed via pip or from the source code. To install via\npip:\n\n    $ pip install galanyl\n\nTo install from the source code:\n\n```\n$ hg clone https://bitbucket.org/ngoldbaum/galaxy_analysis\n$ cd galaxy_analysis\n$ python setup.py develop\n```\n\nNote that the latter version will make the source code the \"live\" version that\nthe python interpreter imports, so edits there will be immediately available\nthe next time you import the toolkit.\n\nTo load one of the example datasets, you will need to download it, unzip it, and\nthen load it in memory:\n\n```\n$ wget https://hub.yt/data/goldbaum2015b/feedback_20pc/processed_data/DD0600_toomre.tar.gz\n$ tar xzvf DD0600_toomre.tar.gz\n$ python\n>>> from galanyl import GalaxyAnalyzer\n>>> from matplotlib import pyplot as plt\n>>> g = GalaxyAnalyzer.from_hdf5_file('DD0600_toomre')\n>>> print g.gas.surface_density\n>>> print g.stars.velocity_dispersion\n>>> plt.imshow(g.gas.total_toome_q)\n```\n\nAll of the data will be available via attributes of the GalaxyAnalyzer instance,\nas in the above example. All quantities are in CGS units. The easiest way to\nexplore the various attributes of these objects is via tab completion in IPython\nor by reading the `GalaxyAnalyzer` source code.\n\nAlternatively, the data are also split up into one hdf5 file per dataset, so any\nhdf5 reader should be able to access the data without going through\nGalaxyAnalyzer.\n", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://bitbucket.org/ngoldbaum/galaxy_analysis", "keywords": null, "license": "UNKNOWN", "maintainer": null, "maintainer_email": null, "name": "galanyl", "package_url": "https://pypi.org/project/galanyl/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/galanyl/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://bitbucket.org/ngoldbaum/galaxy_analysis"}, "release_url": "https://pypi.org/project/galanyl/0.1.7/", "requires_dist": null, "requires_python": null, "summary": "UNKNOWN", "version": "0.1.7", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>#Galaxy Analysis Toolkit</p>\n<p>A collection of scripts and analysis tools used in my thesis research.</p>\n<p>This also includes the text and version history of two as yet unpublished papers\nthat were the primary research result of the code.</p>\n<p>The code is split up into two main pieces. The <cite>galanyl</cite> \u201clibrary\u201d lives in the\n<cite>galanyl</cite> directory, while <cite>helper_scripts</cite> contains a number of scripts that\nmake use of <cite>yt</cite> and <cite>galanyl</cite> to process simulation data.</p>\n<p>#Using these scripts</p>\n<p>First, you must download the data you would like to analyze. Right now the\nhelper scripts are written in a such a way that they expect the full simulation\ndataset to be present, but they should be easily modifiable or adaptable if you\nonly want to look at one or a few simulation outputs.</p>\n<p>The data for Paper I are available at <a href=\"https://hub.yt/data/goldbaum2015a/\" rel=\"nofollow\">https://hub.yt/data/goldbaum2015a/</a></p>\n<p>The data for Paper II are available at <a href=\"https://hub.yt/data/goldbaum2016a\" rel=\"nofollow\">https://hub.yt/data/goldbaum2016a</a></p>\n<p>##Generating uniform resolution grid slabs</p>\n<p>Note: you can skip this step if you want to download the processed data. This\nis only necessary if you would like to generate the processed data from the\nraw simulation outputs.</p>\n<p>For example, if you would like to regenerate the processed data for the\n<tt>nofeedback_20pc</tt> simulation, you will need to download one or more of\nthe simulation outputs, unzip the tarball, and place the simulation outputs\nin a directory named <tt>nofeedback_20pc</tt>.</p>\n<p>Once you have downloaded the simulation outputs you would like to analyze, you\nneed to create the needed ancillary data in two steps:</p>\n<p><tt>`\n$ python generate_covering_grids.py nofeedback_20pc\n`</tt></p>\n<p>This script uses <cite>yt</cite> to convert the raw simulation outputs into uniform\nresolution \u201ccovering grids\u201d that the subsequent analysis scripts need. This will\nonly generate covering grid data for the fields present in the simulation\noutputs. To generate the gravitational potential covering grids, you will need\nto do:</p>\n<p><tt>`\n$ python generate_gravitational_covering_grid.py nofeedback_20pc\n`</tt></p>\n<p>This script needs a copy of the <cite>Enzo</cite> executable (see\n[here](<a href=\"https://enzo.readthedocs.org/en/latest/tutorials/building_enzo.html\" rel=\"nofollow\">https://enzo.readthedocs.org/en/latest/tutorials/building_enzo.html</a>) for\ninformation about compiling the Enzo code) since the script uses the <cite>-g</cite> option\nof the <cite>Enzo</cite> executable to to solve the Poisson equation.</p>\n<p>Finally, you can run the <cite>validate_covering_grid.py</cite> script verify the integrity\nof the covering grid data after it is written to disk. This script merely checks\nto make sure all hdf5 files have the same internal structure, so don\u2019t\ncompletely trust it against all possible data corruption. I created it\noriginally to avoid errors after creating incomplete covering grid files when\nthe generation script crashes or the filesystem hangs.</p>\n<p>Note that if you are doing this for the full simulation dataset it will take <em>a\nlong time</em> even if you are running on multiple cores.  Note that both\n<cite>generate_covering_grids.py</cite> and <cite>generate_gravitational_covering_grid.py</cite> are\nMPI parallelized, so you can them using e.g. <cite>mpirun</cite> on multiple cores to\nparallelize the analysis over multiple datasets. Note that this won\u2019t scale very\nwell unless you are running on a parallel filesystem since both scripts are\nlargely IO bound.</p>\n<p>##Generating final processed data</p>\n<p>Finally, to generate the final processed data, including maps of surface\ndensities, velocity dispersions, and Toomre Q parameters, you should use the\n<cite>analyze_data.py</cite> script.  This script uses the covering grid data generated\nin the previous step to create <cite>GalaxyAnalyzer</cite> objects \u2013 the main analysis\nclass provided by the galaxy analysis toolkit. The <cite>GalaxyAnalyzer</cite> class offers\na number of analysis options to process a subset of the abailable data but also\noffers an interface to calculate all the derived data that it knows how to\ncalculate. This is the interface that <cite>analyze_data.py</cite> uses. The script can\nbe easily modified to only calculate a subset of the data if you do not want\n<em>all</em> of the processed data. To run the script, simply do:</p>\n<p><tt>`\n$ python analyze_data.py\n`</tt></p>\n<p>Like the covering grid generators, this script is also MPI-parallel, so run it\nwith <cite>mpirun</cite> to speed up the analysis. This script is also largely IO-bound, so\nyou will liekly not see a very good scaling unless you are running on a parallel\nfilesystem.</p>\n<p>Note also that some of the expensive calculations are parallelized using OpenMP\nso long as your operating system and compiler supports it. Right now, that means\nyou will need gcc although supposedly LLVM/Clang is also getting OpenMP support\nsoon. I optimized to iterate over the data on a single node, using MPI\nparallelism to iterate over the datasets, but breaking up the work necessary to\nprocess a given dataset using OpenMP. As a rule of thumb, I used a node with 16\ncores and used 4 MPI tasks, so each MPI task used 4 OpenMP threads in the\nOpenMP-parallelized sections of the analysis.</p>\n<p>#Loading and working with the data</p>\n<p>The available data can be accessed via an open-source python analysis\nenvironment. The raw simulation data are written to disk in Enzo\u2019s HDF5-based\ndata format. The processed data are written to disk in a directory structure\ncontaining many simple hdf5 files that contain a single dataset.</p>\n<p>##Simulation outputs</p>\n<p>For the raw simulation outputs, the recommended way to load them for analysis\nand visualization is using yt. You will need to install yt using either the\ninstallation script, conda, or pip, depending on whether you have a python\nenvironment set up and how you set it up. See\n<a href=\"http://yt-project.org/doc/installing.html\" rel=\"nofollow\">http://yt-project.org/doc/installing.html</a> for more details.</p>\n<p>Once you have yt installed, download one of the raw datasets, unzip it, and then\nload it:</p>\n<p><tt>`\n$ wget <span class=\"pre\">https://hub.yt/data/goldbaum2015b/feedback_20pc/simulation_outputs/DD0600.tar.gz</span>\n$ tar xzvf DD0600.tar.gz\n$ python\n&gt;&gt;&gt; import yt\n&gt;&gt;&gt; ds = <span class=\"pre\">yt.load('DD0600/DD0600')</span>\n`</tt></p>\n<p>You can then pass the <cite>ds</cite> object to yt\u2019s plotting commands or access yt data\nobjects through it to get at any on-disk or derived fields.</p>\n<p>##Processed Data</p>\n<p>The easiest way to deal with the processed data is via the galaxy analysis\ntoolkit. This can be installed via pip or from the source code. To install via\npip:</p>\n<blockquote>\n$ pip install galanyl</blockquote>\n<p>To install from the source code:</p>\n<p><tt>`\n$ hg clone <span class=\"pre\">https://bitbucket.org/ngoldbaum/galaxy_analysis</span>\n$ cd galaxy_analysis\n$ python setup.py develop\n`</tt></p>\n<p>Note that the latter version will make the source code the \u201clive\u201d version that\nthe python interpreter imports, so edits there will be immediately available\nthe next time you import the toolkit.</p>\n<p>To load one of the example datasets, you will need to download it, unzip it, and\nthen load it in memory:</p>\n<p><tt>`\n$ wget <span class=\"pre\">https://hub.yt/data/goldbaum2015b/feedback_20pc/processed_data/DD0600_toomre.tar.gz</span>\n$ tar xzvf DD0600_toomre.tar.gz\n$ python\n&gt;&gt;&gt; from galanyl import GalaxyAnalyzer\n&gt;&gt;&gt; from matplotlib import pyplot as plt\n&gt;&gt;&gt; g = <span class=\"pre\">GalaxyAnalyzer.from_hdf5_file('DD0600_toomre')</span>\n&gt;&gt;&gt; print g.gas.surface_density\n&gt;&gt;&gt; print g.stars.velocity_dispersion\n&gt;&gt;&gt; plt.imshow(g.gas.total_toome_q)\n`</tt></p>\n<p>All of the data will be available via attributes of the GalaxyAnalyzer instance,\nas in the above example. All quantities are in CGS units. The easiest way to\nexplore the various attributes of these objects is via tab completion in IPython\nor by reading the <cite>GalaxyAnalyzer</cite> source code.</p>\n<p>Alternatively, the data are also split up into one hdf5 file per dataset, so any\nhdf5 reader should be able to access the data without going through\nGalaxyAnalyzer.</p>\n\n          </div>"}, "last_serial": 2092694, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "1ee3eed8ece67d835636e0571120ca0d", "sha256": "16a80ce59945cd89edb22164fd7449bfcdb92882f28502e250d1e4a7a550aef8"}, "downloads": -1, "filename": "galanyl-0.0.1.tar.gz", "has_sig": false, "md5_digest": "1ee3eed8ece67d835636e0571120ca0d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 396751, "upload_time": "2015-10-18T02:27:00", "upload_time_iso_8601": "2015-10-18T02:27:00.639116Z", "url": "https://files.pythonhosted.org/packages/2a/6b/9e29aead741976339911e5124d8df69980880173c11191e2758845e159df/galanyl-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "2c6c901f31ba8886ef53e5f3109a9753", "sha256": "9358b590e37c1958e72b339d8027b639dcaa1093fed1d7892c5f7065f4a3643b"}, "downloads": -1, "filename": "galanyl-0.0.2.tar.gz", "has_sig": false, "md5_digest": "2c6c901f31ba8886ef53e5f3109a9753", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 396757, "upload_time": "2015-10-18T02:41:08", "upload_time_iso_8601": "2015-10-18T02:41:08.135910Z", "url": "https://files.pythonhosted.org/packages/66/16/16a074c2b78951753d3f030796111170ee25f7f1e031f610529b104a5321/galanyl-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "18f877587d5b65e878889dbcf75e5e5f", "sha256": "23497205bbbc0ef5dc2aac67ea4860888bc33c417b86ef14e633eb211b62d5ee"}, "downloads": -1, "filename": "galanyl-0.0.3.tar.gz", "has_sig": false, "md5_digest": "18f877587d5b65e878889dbcf75e5e5f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 396788, "upload_time": "2015-10-18T03:56:55", "upload_time_iso_8601": "2015-10-18T03:56:55.676022Z", "url": "https://files.pythonhosted.org/packages/0f/a6/a7575e97194d51dafbf1f61e80bfc5982cb36039f551cf45b3e0f868066f/galanyl-0.0.3.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "4f66fb07a69e2d28102340e31283f9a3", "sha256": "a8527d9c8286641602d01fc16ed5d506f8d76819de3a2c92c8e53b7b41784cd7"}, "downloads": -1, "filename": "galanyl-0.1.0.tar.gz", "has_sig": false, "md5_digest": "4f66fb07a69e2d28102340e31283f9a3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 396807, "upload_time": "2015-10-18T04:24:23", "upload_time_iso_8601": "2015-10-18T04:24:23.995038Z", "url": "https://files.pythonhosted.org/packages/07/fb/e8e040b55eedddcd41b30602b83b4a4dec140f8461b806331bd1681060d4/galanyl-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "c6184f0ed64fe12cfdc407730b4962f3", "sha256": "96bb50b0e1b0bb9634873d26e4891a08f688e8696cba3fc59d9948152dd5fb34"}, "downloads": -1, "filename": "galanyl-0.1.1.tar.gz", "has_sig": false, "md5_digest": "c6184f0ed64fe12cfdc407730b4962f3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 396813, "upload_time": "2015-10-19T21:45:58", "upload_time_iso_8601": "2015-10-19T21:45:58.905728Z", "url": "https://files.pythonhosted.org/packages/eb/97/3fc219494e9a3e570c6650013d9787c0013126de5b0d806418380e79f0de/galanyl-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "e9cf4d84b18bb4e191b84c8ffd945c43", "sha256": "1c54cc16fef478c92ed2304cd076c71f8a71738e2fbac61b56046b19bbf96498"}, "downloads": -1, "filename": "galanyl-0.1.2.tar.gz", "has_sig": false, "md5_digest": "e9cf4d84b18bb4e191b84c8ffd945c43", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 396839, "upload_time": "2015-10-19T22:15:42", "upload_time_iso_8601": "2015-10-19T22:15:42.901990Z", "url": "https://files.pythonhosted.org/packages/19/2c/4e36a77d5514f57846d3c27faa58d886063f0119663cbf4db176c4b5971c/galanyl-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "4d90114ffe27fdf10e7316960156a343", "sha256": "2ca9d871cf05e947dc34de723ae1ba1e4b7d382248404502aff93e6e96e0b0a5"}, "downloads": -1, "filename": "galanyl-0.1.3.tar.gz", "has_sig": false, "md5_digest": "4d90114ffe27fdf10e7316960156a343", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 397699, "upload_time": "2016-01-04T17:44:00", "upload_time_iso_8601": "2016-01-04T17:44:00.640051Z", "url": "https://files.pythonhosted.org/packages/49/0b/1e5b24781b6295a6138ac205127674d670d1fb64825b7462697628225997/galanyl-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "71c2403ad39a21eb318fed28f05c6d26", "sha256": "a865d4806201b7655b94a94cea0de068ad7c46ab63effece3539933e22614cee"}, "downloads": -1, "filename": "galanyl-0.1.4.tar.gz", "has_sig": false, "md5_digest": "71c2403ad39a21eb318fed28f05c6d26", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 397458, "upload_time": "2016-01-07T16:34:49", "upload_time_iso_8601": "2016-01-07T16:34:49.561776Z", "url": "https://files.pythonhosted.org/packages/f6/d1/a900fd51f15673911397bcd19da8f3004252d63b94cc1d87f87c63068630/galanyl-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "e13169d072d4654a42d052ecc6b77973", "sha256": "9997968f3d501e9b9e2ecc832887ad86137cdc252a8a546c40af165407436bb4"}, "downloads": -1, "filename": "galanyl-0.1.5.tar.gz", "has_sig": false, "md5_digest": "e13169d072d4654a42d052ecc6b77973", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 398999, "upload_time": "2016-03-09T19:26:06", "upload_time_iso_8601": "2016-03-09T19:26:06.604956Z", "url": "https://files.pythonhosted.org/packages/1b/38/081503cbe14c836cd0fafb3829bb26fb157c33d3aaabdb6a9c713eacf1a1/galanyl-0.1.5.tar.gz", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "de79f06f654b04a6a271afc51ffb054c", "sha256": "7f488818e91327df9eed031d8dcd731ae6f56631a20f3bbeb263c249aaddfb7a"}, "downloads": -1, "filename": "galanyl-0.1.6.tar.gz", "has_sig": false, "md5_digest": "de79f06f654b04a6a271afc51ffb054c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 399045, "upload_time": "2016-03-09T21:44:13", "upload_time_iso_8601": "2016-03-09T21:44:13.181920Z", "url": "https://files.pythonhosted.org/packages/d5/fc/6cc7133e2f9fe956141657541623638ef81416e5a57490095da39d2de1b0/galanyl-0.1.6.tar.gz", "yanked": false}], "0.1.7": [{"comment_text": "", "digests": {"md5": "2c6343bd2ef07983a50d41397d45a025", "sha256": "a900ccce35b0535ac9383cfe9cfaa386c38c3e654dc72dd1a1db0aaa18b9137e"}, "downloads": -1, "filename": "galanyl-0.1.7.tar.gz", "has_sig": false, "md5_digest": "2c6343bd2ef07983a50d41397d45a025", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 404510, "upload_time": "2016-04-30T18:21:43", "upload_time_iso_8601": "2016-04-30T18:21:43.344920Z", "url": "https://files.pythonhosted.org/packages/aa/a6/affce9a13753757599ffded4f35998216f1c8bbcd46a89873baac85c718d/galanyl-0.1.7.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2c6343bd2ef07983a50d41397d45a025", "sha256": "a900ccce35b0535ac9383cfe9cfaa386c38c3e654dc72dd1a1db0aaa18b9137e"}, "downloads": -1, "filename": "galanyl-0.1.7.tar.gz", "has_sig": false, "md5_digest": "2c6343bd2ef07983a50d41397d45a025", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 404510, "upload_time": "2016-04-30T18:21:43", "upload_time_iso_8601": "2016-04-30T18:21:43.344920Z", "url": "https://files.pythonhosted.org/packages/aa/a6/affce9a13753757599ffded4f35998216f1c8bbcd46a89873baac85c718d/galanyl-0.1.7.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:59:19 2020"}