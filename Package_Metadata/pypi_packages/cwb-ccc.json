{"info": {"author": "Philipp Heinrich", "author_email": "philipp.heinrich@fau.de", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Collocation and Concordance Computation #\n\n* [Introduction](#introduction)\n* [Installation](#installation)\n* [Usage](#usage)\n\t* [Defining your corpus](#corpus-setup)\n\t* [Extracting concordance lines](#concordancing)\n\t* [Dealing with anchored queries](#anchored-queries)\n\t* [Calculating collocates](#collocation-analyses)\n\t* [Extracting keywords](#keyword-analyses)\n\t* [Argument queries](#argument-queries)\n* [Acknowledgements](#acknowledgements)\n\n\n## Introduction ##\nThis module is a wrapper around the [IMS Open Corpus Workbench\n(CWB)](http://cwb.sourceforge.net/).  Main purpose of the module is to\nextract concordance lines, calculate keywords and collocates, and run\nqueries with several anchor points.\n\nIf you want to extract the results of queries with more than two\nanchor points, the module requires CWB version 3.4.16 or later.\n\n\n## Installation ##\nYou can install this module with pip from PyPI:\n\n\tpip3 install cwb-ccc\n\nYou can also clone the repository from\n[github](https://github.com/ausgerechnet/cwb-ccc), `cd` in the\nrespective folder, and use `setup.py`:\n\n\tpython3 setup.py install\n\n\n## Usage ##\n\n### Corpus Setup\nAll methods rely on the `Corpus` class, which establishes the\nconnection to your CWB-indexed corpus:\n\n```python\nfrom ccc import Corpus\ncorpus = Corpus(\n\tcorpus_name=\"EXAMPLE_CORPUS\",\n\tregistry_path=\"/path/to/your/cwb/registry/\"\n)\n```\n\nThis will raise a `KeyError` if the named corpus is not in the\nspecified registry.\n\nIf you are using macros and wordlists, you have to store them in a\nseparate folder (with subfolders `wordlists` and `macros`).  Make sure\nyou specify this folder via `lib_path` when initializing the\ncorpus.\n\nIf you want to compare your query results according to meta data,\nset the `s_meta` parameter to the structural attribute that links your\ndata base (e.g. \"text_id\").\n\nYou can use the `cqp_bin` to point the module to a specific version of\n`cqp` (this is also helpful if `cqp` is not in your `PATH`).\n\nBy default, the `cache_path` points to \"/tmp/ccc-cache\". Make sure\nthat \"/tmp/\" exists and appropriate rights are granted. Otherwise,\nchange the parameter when initializing the corpus (or set it to\n`None`).\n\n### Concordancing ###\n\nBefore you can display concordances, you have to run a query with the\n`corpus.query()` method, which accepts valid CQP queries such as\n\n```python\nquery = '[lemma=\"Angela\"]? [lemma=\"Merkel\"] [word=\"\\\\(\"] [lemma=\"CDU\"] [word=\"\\\\)\"]'\ncorpus.query(query)\n```\n\nThe default context window is 20 tokens to the left and 20 tokens to\nthe right of the query match and matchend, respectively. You can\nchange this via the `context` parameter.\n\nNote that queries _may_ end on a \"within\" clause (`s_query`), which\nwill limit the matches to regions defined by this structural\nattributes. Additionally, you can specify an `s_break` parameter,\nwhich will cut the context. NB: The implementation assumes that\n`s_query` regions are confined by `s_break` regions, and both of them\nare within the `s_meta` regions.\n\nNow you are set up to get the query concordance:\n\n```\nconcordance = corpus.concordance()\n```\n\nYou can access the query frequency breakdown via\n`concordance.breakdown`:\n\n| *type*                 | freq |\n|------------------------|------|\n| Angela Merkel ( CDU )  | 2253 |\n| Merkel ( CDU )         | 29   |\n| Angela Merkels ( CDU ) | 2    |\n\nAll query matches and their respective `s_meta` identifiers are listed\nin `concordance.meta` (if `s_meta=None`, it will use the CQP\nidentifiers of the `s_break` parameter as `s_id`):\n\n| *match* | s_id      |\n|---------|-----------|\n| 48349   | A44847086 |\n| 48856   | A44855701 |\n| 52966   | A44847097 |\n| 53395   | A44847526 |\n| ...     | ...       |\n\nYou can use `concordance.lines()` to get concordance lines. This\nmethod returns a dictionary with the _cpos_ of the match as keys and\nthe entries one concordance line each. Each concordance line is\nformatted as a `pandas.DataFrame` with the _cpos_ of each token as\nindex:\n\n| *cpos* | offset | word                | anchor |\n|--------|--------|---------------------|--------|\n| 48344  | -5     | Eine                | None   |\n| 48345  | -4     | entsprechende       | None   |\n| 48346  | -3     | Steuererleichterung | None   |\n| 48347  | -2     | hat                 | None   |\n| 48348  | -1     | Kanzlerin           | None   |\n| 48349  | 0      | Angela              | None   |\n| 48350  | 0      | Merkel              | None   |\n| 48351  | 0      | (                   | None   |\n| 48352  | 0      | CDU                 | None   |\n| 48353  | 0      | )                   | None   |\n| 48354  | 1      | bisher              | None   |\n| 48355  | 2      | ausgeschlossen      | None   |\n| 48356  | 3      | .                   | None   |\n\nYou can decide which and how many concordance lines you want to\nretrieve by means of the parameters `order` (\"first\", \"last\", or\n\"random\") and `cut_off`. You can also provide a list of `matches`\n(from `concordance.meta.index`) to get a `dict` of specific\nconcordance lines.\n\nYou can specify a `list` of additional p-attributes besides the\nprimary word layer to show via the `p_show` parameter of\n`concordance.lines()` (these will be added as additional columns).\n\n### Anchored Queries ###\n\nThe concordancer detects anchored queries automatically. The following\nquery\n\n```python\nconcordance.query(\n\t'@0[lemma=\"Angela\"]? @1[lemma=\"Merkel\"] [word=\"\\\\(\"] @2[lemma=\"CDU\"] [word=\"\\\\)\"]'\n)\n```\n\nthus returns `DataFrame`s with appropriate anchors in the anchor\ncolumn:\n\n| *cpos* | offset | word                | anchor |\n|--------|--------|---------------------|--------|\n| 48344  | -5     | Eine                | None   |\n| 48345  | -4     | entsprechende       | None   |\n| 48346  | -3     | Steuererleichterung | None   |\n| 48347  | -2     | hat                 | None   |\n| 48348  | -1     | Kanzlerin           | None   |\n| 48349  | 0      | Angela              | 0      |\n| 48350  | 0      | Merkel              | 1      |\n| 48351  | 0      | (                   | None   |\n| 48352  | 0      | CDU                 | 2      |\n| 48353  | 0      | )                   | None   |\n| 48354  | 1      | bisher              | None   |\n| 48355  | 2      | ausgeschlossen      | None   |\n| 48356  | 3      | .                   | None   |\n\n\n### Collocation Analyses ###\nAfter executing a query, you can use the `corpus.collocates()` class\nto extract collocates for a given window size (symmetric windows\naround the corpus matches):\n\n```python\nquery = '[lemma=\"Angela\"] [lemma=\"Merkel\"]'\ncorpus.query(query, s_break='s', context=20)\ncollocates = corpus.collocates()\n```\n\n`collocates()` will create a dataframe of the context of the query\nmatches. You can specify a smaller maximum window size via the `mws`\nparameter (this might be reasonable for queries with many hits). You\nwill only be able to score collocates up to this parameter. Note that\n`mws` must not be larger than the `context` parameter of your initial\nquery.\n\nBy default, collocates are calculated on the \"lemma\"-layer, assuming\nthat this is a valid p-attribute in the corpus. The corresponding\nparameter is `p_query` (which will fall back to \"word\" if the\nspecified attribute is not annotated in the corpus).\n\nUsing the marginal frequencies of the items in the whole corpus as a\nreference, you can directly annotate the co-occurrence counts in a\ngiven window:\n\n```python\ncollocates.show(window=5)\n```\n\nThe result will be a `DataFrame` with lexical items (`p_query` layer)\nas index and frequency signatures and association measures as columns:\n\n| *item*          | O11  | f2       | N         | f1    | O12   | O21      | O22       | E11         | E12          | E21          | E22          | log_likelihood | ... |\n|-----------------|------|----------|-----------|-------|-------|----------|-----------|-------------|--------------|--------------|--------------|----------------|-----|\n| die             | 1799 | 25125329 | 300917702 | 22832 | 21033 | 25123530 | 275771340 | 1906.373430 | 20925.626570 | 2.512342e+07 | 2.757714e+08 | -2.459194      | ... |\n| Bundeskanzlerin | 1491 | 8816     | 300917702 | 22832 | 21341 | 7325     | 300887545 | 0.668910    | 22831.331090 | 8.815331e+03 | 3.008861e+08 | 1822.211827    | ... |\n| .               | 1123 | 13677811 | 300917702 | 22832 | 21709 | 13676688 | 287218182 | 1037.797972 | 21794.202028 | 1.367677e+07 | 2.872181e+08 | 2.644804       | ... |\n| ,               | 814  | 17562059 | 300917702 | 22832 | 22018 | 17561245 | 283333625 | 1332.513602 | 21499.486398 | 1.756073e+07 | 2.833341e+08 | -14.204447     | ... |\n| Kanzlerin       | 648  | 17622    | 300917702 | 22832 | 22184 | 16974    | 300877896 | 1.337062    | 22830.662938 | 1.762066e+04 | 3.008772e+08 | 559.245198     | ... |\n\nFor improved performance, all hapax legomena in the context are\ndropped after calculating the context size. You can change this\nbehaviour via the `drop_hapaxes` parameter of `collocates.show()`.\n\nBy default, the dataframe is annotated with \"z_score\", \"t_score\",\n\"dice\", \"log_likelihood\", and \"mutual_information\" (parameter `ams`).\nFor notation and further information regarding association measures,\nsee\n[collocations.de](http://www.collocations.de/AM/index.html). Available\nassociation measures depend on their implementation in the\n[association-measures](https://pypi.org/project/association-measures/)\nmodule.\n\nThe dataframe is sorted by co-occurrence frequency (O11), and only the\nfirst 100 most frequently co-occurring collocates are retrieved. You\ncan change this behaviour via the `order` and `cut_off` parameters.\n\n### Keyword Anayses\nFor keyword analyses, you will have to define a subcorpus. The natural\nway of doing so is by selecting text identifiers (on the `s_meta`\nannotations) via spreadsheets or relational databases. If you have\ncollected a set of identifiers, you can create a subcorpus via the\n`corpus.subcorpus_from_ids()` method:\n\n```python\ncorpus.subcorpus_from_ids(ids)\nkeywords = corpus.keywords()\nkeywords.show()\n```\n\nJust as with collocates, the result will be a `DataFrame` with lexical\nitems (`p_query` layer) as index and frequency signatures and\nassociation measures as columns.\n\n### Argument Queries\nArgument queries are anchored queries with additional information. (1)\nEach anchor can be modified by an offset (usually used to capture\nunderspecified tokens near an anchor point). (2) Anchors can be mapped\nto external identifiers for further logical processing. (3) Anchors\nmay be given a clear name:\n\n| anchor | offset | idx  | clear name |\n|--------|--------|------|------------|\n| 0      | 0      | None | None       |\n| 1      | -1     | None | None       |\n| 2      | 0      | None | None       |\n| 3      | -1     | None | None       |\n\nFurthermore, several anchor points can be combined to form regions,\nwhich in turn can be mapped to identifiers and be given a clear name:\n\n| start | end | idx | clear name |\n|-------|-----|-----|------------|\n| 0     | 1   | \"0\" | \"person X\" |\n| 2     | 3   | \"1\" | \"person Y\" |\n\nExample: Given the definition of anchors and regions above as well as\nsuitable wordlists, the following complex query extracts corpus\npositions where there's some correlation between \"person X\" (the\nregion from anchor 0 to anchor 1) and \"person Y\" (anchor 2 to 3):\n```python\nquery = (\n\t\"<np> []* /ap[]* [lemma = $nouns_similarity] \"\n\t\"[]*</np> \\\"between\\\" @0:[::](<np>[pos_simple=\\\"D|A\\\"]* \"\n\t\"([pos_simple=\\\"Z|P\\\" | lemma = $nouns_person_common | \"\n\t\"lemma = $nouns_person_origin | lemma = $nouns_person_support | \"\n\t\"lemma = $nouns_person_negative | \"\n\t\"lemma = $nouns_person_profession] |/region[ner])+ \"\n\t\"[]*</np>)+@1:[::] \\\"and\\\" @2:[::](<np>[pos_simple=\\\"D|A\\\"]* \"\n\t\"([pos_simple=\\\"Z|P\\\" | lemma = $nouns_person_common | \"\n\t\"lemma = $nouns_person_origin | lemma = $nouns_person_support | \"\n\t\"lemma = $nouns_person_negative | \"\n\t\"lemma = $nouns_person_profession] | /region[ner])+ \"\n\t\"[]*</np>) (/region[np] | <vp>[lemma!=\\\"be\\\"]</vp> | \"\n\t\"/region[pp] |/be_ap[])* @3:[::]\"\n)\n```\n\nNB: the set of identifiers defined in the table of anchors and in the\ntable of regions, respectively, should not overlap.\n\nIt is customary to store these queries in json objects (see an\n[example](tests/gold/query-example.json) in the repository). \n\nYou can use the `concordancer` to process argument queries and display\nthe results:\n\n```python\n# read the query file\nimport json\nquery_path = \"tests/gold/query-example.json\"\nwith open(query_path, \"rt\") as f:\n\tquery = json.loads(f.read())\n\n# query the corpus and initialize the concordancer\ncorpus.query(query['query'], context=None, s_break='tweet', match_strategy='longest')\nconcordance = corpus.concordance()\n\n# show results\nconcordance.show_argmin(query['anchors'], query['regions'])\n```\n\nThe `show_argmin` method returns the result as a `dict` with the\nfollowing keys:\n\n- \"nr_matches\": the number of query matches in the corpus.\n- \"holes\": a global list of all tokens of the entities specified in\n  the \"idx\" columns (default: lemma layer).\n- \"meta\": the meta ids of the concordance lines.\n- \"settings\": the query settings.\n- \"matches\": a list of concordance lines. Each concordance line\n  contains:\n  - \"position\": the corpus position of the match\n  - \"df\": the actual concordance line as returned from\n\t`Concordance().query()` (see above) converted to a `dict`\n  - \"holes\": a mapping from the IDs specified in the anchor and region\n\ttables to the tokens or token sequences, respectively (default:\n\tlemma layer)\n  - \"full\": a reconstruction of the concordance line as a sequence of\n\ttokens (word layer)\n\n\n## Acknowledgements ##\nThe module relies on\n[cwb-python](https://pypi.org/project/cwb-python/), thanks to Yannick\nVersley and Jorg Asmussen for the implementation. Special thanks to\nMarkus Opolka for the implementation of\n[association-measures](https://pypi.org/project/association-measures/)\nand for forcing me to write tests.\n\nThis work was supported by the [Emerging Fields Initiative\n(EFI)](https://www.fau.eu/research/collaborative-research/emerging-fields-initiative/)\nof Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg, project title\n[Exploring the *Fukushima\nEffect*](https://www.linguistik.phil.fau.de/projects/efe/).\n\nFurther development of the package has been funded by the Deutsche\nForschungsgemeinschaft (DFG) within the project *Reconstructing\nArguments from Noisy Text*, grant number 377333057, as part of the\nPriority Program [Robust Argumentation\nMachines](http://www.spp-ratio.de/home/) (SPP-1999).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://gitlab.cs.fau.de/pheinrich/ccc", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "cwb-ccc", "package_url": "https://pypi.org/project/cwb-ccc/", "platform": "", "project_url": "https://pypi.org/project/cwb-ccc/", "project_urls": {"Homepage": "https://gitlab.cs.fau.de/pheinrich/ccc"}, "release_url": "https://pypi.org/project/cwb-ccc/0.9.6/", "requires_dist": ["pandas (>=0.24.2)", "cwb-python (>=0.2.2)", "association-measures (>=0.1.3)"], "requires_python": ">=3.5", "summary": "CWB wrapper to extract concordances and collocates", "version": "0.9.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Collocation and Concordance Computation</h1>\n<ul>\n<li><a href=\"#introduction\" rel=\"nofollow\">Introduction</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#usage\" rel=\"nofollow\">Usage</a>\n<ul>\n<li><a href=\"#corpus-setup\" rel=\"nofollow\">Defining your corpus</a></li>\n<li><a href=\"#concordancing\" rel=\"nofollow\">Extracting concordance lines</a></li>\n<li><a href=\"#anchored-queries\" rel=\"nofollow\">Dealing with anchored queries</a></li>\n<li><a href=\"#collocation-analyses\" rel=\"nofollow\">Calculating collocates</a></li>\n<li><a href=\"#keyword-analyses\" rel=\"nofollow\">Extracting keywords</a></li>\n<li><a href=\"#argument-queries\" rel=\"nofollow\">Argument queries</a></li>\n</ul>\n</li>\n<li><a href=\"#acknowledgements\" rel=\"nofollow\">Acknowledgements</a></li>\n</ul>\n<h2>Introduction</h2>\n<p>This module is a wrapper around the <a href=\"http://cwb.sourceforge.net/\" rel=\"nofollow\">IMS Open Corpus Workbench\n(CWB)</a>.  Main purpose of the module is to\nextract concordance lines, calculate keywords and collocates, and run\nqueries with several anchor points.</p>\n<p>If you want to extract the results of queries with more than two\nanchor points, the module requires CWB version 3.4.16 or later.</p>\n<h2>Installation</h2>\n<p>You can install this module with pip from PyPI:</p>\n<pre><code>pip3 install cwb-ccc\n</code></pre>\n<p>You can also clone the repository from\n<a href=\"https://github.com/ausgerechnet/cwb-ccc\" rel=\"nofollow\">github</a>, <code>cd</code> in the\nrespective folder, and use <code>setup.py</code>:</p>\n<pre><code>python3 setup.py install\n</code></pre>\n<h2>Usage</h2>\n<h3>Corpus Setup</h3>\n<p>All methods rely on the <code>Corpus</code> class, which establishes the\nconnection to your CWB-indexed corpus:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">ccc</span> <span class=\"kn\">import</span> <span class=\"n\">Corpus</span>\n<span class=\"n\">corpus</span> <span class=\"o\">=</span> <span class=\"n\">Corpus</span><span class=\"p\">(</span>\n\t<span class=\"n\">corpus_name</span><span class=\"o\">=</span><span class=\"s2\">\"EXAMPLE_CORPUS\"</span><span class=\"p\">,</span>\n\t<span class=\"n\">registry_path</span><span class=\"o\">=</span><span class=\"s2\">\"/path/to/your/cwb/registry/\"</span>\n<span class=\"p\">)</span>\n</pre>\n<p>This will raise a <code>KeyError</code> if the named corpus is not in the\nspecified registry.</p>\n<p>If you are using macros and wordlists, you have to store them in a\nseparate folder (with subfolders <code>wordlists</code> and <code>macros</code>).  Make sure\nyou specify this folder via <code>lib_path</code> when initializing the\ncorpus.</p>\n<p>If you want to compare your query results according to meta data,\nset the <code>s_meta</code> parameter to the structural attribute that links your\ndata base (e.g. \"text_id\").</p>\n<p>You can use the <code>cqp_bin</code> to point the module to a specific version of\n<code>cqp</code> (this is also helpful if <code>cqp</code> is not in your <code>PATH</code>).</p>\n<p>By default, the <code>cache_path</code> points to \"/tmp/ccc-cache\". Make sure\nthat \"/tmp/\" exists and appropriate rights are granted. Otherwise,\nchange the parameter when initializing the corpus (or set it to\n<code>None</code>).</p>\n<h3>Concordancing</h3>\n<p>Before you can display concordances, you have to run a query with the\n<code>corpus.query()</code> method, which accepts valid CQP queries such as</p>\n<pre><span class=\"n\">query</span> <span class=\"o\">=</span> <span class=\"s1\">'[lemma=\"Angela\"]? [lemma=\"Merkel\"] [word=\"</span><span class=\"se\">\\\\</span><span class=\"s1\">(\"] [lemma=\"CDU\"] [word=\"</span><span class=\"se\">\\\\</span><span class=\"s1\">)\"]'</span>\n<span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">query</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">)</span>\n</pre>\n<p>The default context window is 20 tokens to the left and 20 tokens to\nthe right of the query match and matchend, respectively. You can\nchange this via the <code>context</code> parameter.</p>\n<p>Note that queries <em>may</em> end on a \"within\" clause (<code>s_query</code>), which\nwill limit the matches to regions defined by this structural\nattributes. Additionally, you can specify an <code>s_break</code> parameter,\nwhich will cut the context. NB: The implementation assumes that\n<code>s_query</code> regions are confined by <code>s_break</code> regions, and both of them\nare within the <code>s_meta</code> regions.</p>\n<p>Now you are set up to get the query concordance:</p>\n<pre><code>concordance = corpus.concordance()\n</code></pre>\n<p>You can access the query frequency breakdown via\n<code>concordance.breakdown</code>:</p>\n<table>\n<thead>\n<tr>\n<th><em>type</em></th>\n<th>freq</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Angela Merkel ( CDU )</td>\n<td>2253</td>\n</tr>\n<tr>\n<td>Merkel ( CDU )</td>\n<td>29</td>\n</tr>\n<tr>\n<td>Angela Merkels ( CDU )</td>\n<td>2</td>\n</tr></tbody></table>\n<p>All query matches and their respective <code>s_meta</code> identifiers are listed\nin <code>concordance.meta</code> (if <code>s_meta=None</code>, it will use the CQP\nidentifiers of the <code>s_break</code> parameter as <code>s_id</code>):</p>\n<table>\n<thead>\n<tr>\n<th><em>match</em></th>\n<th>s_id</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>48349</td>\n<td>A44847086</td>\n</tr>\n<tr>\n<td>48856</td>\n<td>A44855701</td>\n</tr>\n<tr>\n<td>52966</td>\n<td>A44847097</td>\n</tr>\n<tr>\n<td>53395</td>\n<td>A44847526</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n</tr></tbody></table>\n<p>You can use <code>concordance.lines()</code> to get concordance lines. This\nmethod returns a dictionary with the <em>cpos</em> of the match as keys and\nthe entries one concordance line each. Each concordance line is\nformatted as a <code>pandas.DataFrame</code> with the <em>cpos</em> of each token as\nindex:</p>\n<table>\n<thead>\n<tr>\n<th><em>cpos</em></th>\n<th>offset</th>\n<th>word</th>\n<th>anchor</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>48344</td>\n<td>-5</td>\n<td>Eine</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48345</td>\n<td>-4</td>\n<td>entsprechende</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48346</td>\n<td>-3</td>\n<td>Steuererleichterung</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48347</td>\n<td>-2</td>\n<td>hat</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48348</td>\n<td>-1</td>\n<td>Kanzlerin</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48349</td>\n<td>0</td>\n<td>Angela</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48350</td>\n<td>0</td>\n<td>Merkel</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48351</td>\n<td>0</td>\n<td>(</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48352</td>\n<td>0</td>\n<td>CDU</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48353</td>\n<td>0</td>\n<td>)</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48354</td>\n<td>1</td>\n<td>bisher</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48355</td>\n<td>2</td>\n<td>ausgeschlossen</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48356</td>\n<td>3</td>\n<td>.</td>\n<td>None</td>\n</tr></tbody></table>\n<p>You can decide which and how many concordance lines you want to\nretrieve by means of the parameters <code>order</code> (\"first\", \"last\", or\n\"random\") and <code>cut_off</code>. You can also provide a list of <code>matches</code>\n(from <code>concordance.meta.index</code>) to get a <code>dict</code> of specific\nconcordance lines.</p>\n<p>You can specify a <code>list</code> of additional p-attributes besides the\nprimary word layer to show via the <code>p_show</code> parameter of\n<code>concordance.lines()</code> (these will be added as additional columns).</p>\n<h3>Anchored Queries</h3>\n<p>The concordancer detects anchored queries automatically. The following\nquery</p>\n<pre><span class=\"n\">concordance</span><span class=\"o\">.</span><span class=\"n\">query</span><span class=\"p\">(</span>\n\t<span class=\"s1\">'@0[lemma=\"Angela\"]? @1[lemma=\"Merkel\"] [word=\"</span><span class=\"se\">\\\\</span><span class=\"s1\">(\"] @2[lemma=\"CDU\"] [word=\"</span><span class=\"se\">\\\\</span><span class=\"s1\">)\"]'</span>\n<span class=\"p\">)</span>\n</pre>\n<p>thus returns <code>DataFrame</code>s with appropriate anchors in the anchor\ncolumn:</p>\n<table>\n<thead>\n<tr>\n<th><em>cpos</em></th>\n<th>offset</th>\n<th>word</th>\n<th>anchor</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>48344</td>\n<td>-5</td>\n<td>Eine</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48345</td>\n<td>-4</td>\n<td>entsprechende</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48346</td>\n<td>-3</td>\n<td>Steuererleichterung</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48347</td>\n<td>-2</td>\n<td>hat</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48348</td>\n<td>-1</td>\n<td>Kanzlerin</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48349</td>\n<td>0</td>\n<td>Angela</td>\n<td>0</td>\n</tr>\n<tr>\n<td>48350</td>\n<td>0</td>\n<td>Merkel</td>\n<td>1</td>\n</tr>\n<tr>\n<td>48351</td>\n<td>0</td>\n<td>(</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48352</td>\n<td>0</td>\n<td>CDU</td>\n<td>2</td>\n</tr>\n<tr>\n<td>48353</td>\n<td>0</td>\n<td>)</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48354</td>\n<td>1</td>\n<td>bisher</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48355</td>\n<td>2</td>\n<td>ausgeschlossen</td>\n<td>None</td>\n</tr>\n<tr>\n<td>48356</td>\n<td>3</td>\n<td>.</td>\n<td>None</td>\n</tr></tbody></table>\n<h3>Collocation Analyses</h3>\n<p>After executing a query, you can use the <code>corpus.collocates()</code> class\nto extract collocates for a given window size (symmetric windows\naround the corpus matches):</p>\n<pre><span class=\"n\">query</span> <span class=\"o\">=</span> <span class=\"s1\">'[lemma=\"Angela\"] [lemma=\"Merkel\"]'</span>\n<span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">query</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">,</span> <span class=\"n\">s_break</span><span class=\"o\">=</span><span class=\"s1\">'s'</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">)</span>\n<span class=\"n\">collocates</span> <span class=\"o\">=</span> <span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">collocates</span><span class=\"p\">()</span>\n</pre>\n<p><code>collocates()</code> will create a dataframe of the context of the query\nmatches. You can specify a smaller maximum window size via the <code>mws</code>\nparameter (this might be reasonable for queries with many hits). You\nwill only be able to score collocates up to this parameter. Note that\n<code>mws</code> must not be larger than the <code>context</code> parameter of your initial\nquery.</p>\n<p>By default, collocates are calculated on the \"lemma\"-layer, assuming\nthat this is a valid p-attribute in the corpus. The corresponding\nparameter is <code>p_query</code> (which will fall back to \"word\" if the\nspecified attribute is not annotated in the corpus).</p>\n<p>Using the marginal frequencies of the items in the whole corpus as a\nreference, you can directly annotate the co-occurrence counts in a\ngiven window:</p>\n<pre><span class=\"n\">collocates</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">(</span><span class=\"n\">window</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n</pre>\n<p>The result will be a <code>DataFrame</code> with lexical items (<code>p_query</code> layer)\nas index and frequency signatures and association measures as columns:</p>\n<table>\n<thead>\n<tr>\n<th><em>item</em></th>\n<th>O11</th>\n<th>f2</th>\n<th>N</th>\n<th>f1</th>\n<th>O12</th>\n<th>O21</th>\n<th>O22</th>\n<th>E11</th>\n<th>E12</th>\n<th>E21</th>\n<th>E22</th>\n<th>log_likelihood</th>\n<th>...</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>die</td>\n<td>1799</td>\n<td>25125329</td>\n<td>300917702</td>\n<td>22832</td>\n<td>21033</td>\n<td>25123530</td>\n<td>275771340</td>\n<td>1906.373430</td>\n<td>20925.626570</td>\n<td>2.512342e+07</td>\n<td>2.757714e+08</td>\n<td>-2.459194</td>\n<td>...</td>\n</tr>\n<tr>\n<td>Bundeskanzlerin</td>\n<td>1491</td>\n<td>8816</td>\n<td>300917702</td>\n<td>22832</td>\n<td>21341</td>\n<td>7325</td>\n<td>300887545</td>\n<td>0.668910</td>\n<td>22831.331090</td>\n<td>8.815331e+03</td>\n<td>3.008861e+08</td>\n<td>1822.211827</td>\n<td>...</td>\n</tr>\n<tr>\n<td>.</td>\n<td>1123</td>\n<td>13677811</td>\n<td>300917702</td>\n<td>22832</td>\n<td>21709</td>\n<td>13676688</td>\n<td>287218182</td>\n<td>1037.797972</td>\n<td>21794.202028</td>\n<td>1.367677e+07</td>\n<td>2.872181e+08</td>\n<td>2.644804</td>\n<td>...</td>\n</tr>\n<tr>\n<td>,</td>\n<td>814</td>\n<td>17562059</td>\n<td>300917702</td>\n<td>22832</td>\n<td>22018</td>\n<td>17561245</td>\n<td>283333625</td>\n<td>1332.513602</td>\n<td>21499.486398</td>\n<td>1.756073e+07</td>\n<td>2.833341e+08</td>\n<td>-14.204447</td>\n<td>...</td>\n</tr>\n<tr>\n<td>Kanzlerin</td>\n<td>648</td>\n<td>17622</td>\n<td>300917702</td>\n<td>22832</td>\n<td>22184</td>\n<td>16974</td>\n<td>300877896</td>\n<td>1.337062</td>\n<td>22830.662938</td>\n<td>1.762066e+04</td>\n<td>3.008772e+08</td>\n<td>559.245198</td>\n<td>...</td>\n</tr></tbody></table>\n<p>For improved performance, all hapax legomena in the context are\ndropped after calculating the context size. You can change this\nbehaviour via the <code>drop_hapaxes</code> parameter of <code>collocates.show()</code>.</p>\n<p>By default, the dataframe is annotated with \"z_score\", \"t_score\",\n\"dice\", \"log_likelihood\", and \"mutual_information\" (parameter <code>ams</code>).\nFor notation and further information regarding association measures,\nsee\n<a href=\"http://www.collocations.de/AM/index.html\" rel=\"nofollow\">collocations.de</a>. Available\nassociation measures depend on their implementation in the\n<a href=\"https://pypi.org/project/association-measures/\" rel=\"nofollow\">association-measures</a>\nmodule.</p>\n<p>The dataframe is sorted by co-occurrence frequency (O11), and only the\nfirst 100 most frequently co-occurring collocates are retrieved. You\ncan change this behaviour via the <code>order</code> and <code>cut_off</code> parameters.</p>\n<h3>Keyword Anayses</h3>\n<p>For keyword analyses, you will have to define a subcorpus. The natural\nway of doing so is by selecting text identifiers (on the <code>s_meta</code>\nannotations) via spreadsheets or relational databases. If you have\ncollected a set of identifiers, you can create a subcorpus via the\n<code>corpus.subcorpus_from_ids()</code> method:</p>\n<pre><span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">subcorpus_from_ids</span><span class=\"p\">(</span><span class=\"n\">ids</span><span class=\"p\">)</span>\n<span class=\"n\">keywords</span> <span class=\"o\">=</span> <span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">keywords</span><span class=\"p\">()</span>\n<span class=\"n\">keywords</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</pre>\n<p>Just as with collocates, the result will be a <code>DataFrame</code> with lexical\nitems (<code>p_query</code> layer) as index and frequency signatures and\nassociation measures as columns.</p>\n<h3>Argument Queries</h3>\n<p>Argument queries are anchored queries with additional information. (1)\nEach anchor can be modified by an offset (usually used to capture\nunderspecified tokens near an anchor point). (2) Anchors can be mapped\nto external identifiers for further logical processing. (3) Anchors\nmay be given a clear name:</p>\n<table>\n<thead>\n<tr>\n<th>anchor</th>\n<th>offset</th>\n<th>idx</th>\n<th>clear name</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>0</td>\n<td>None</td>\n<td>None</td>\n</tr>\n<tr>\n<td>1</td>\n<td>-1</td>\n<td>None</td>\n<td>None</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0</td>\n<td>None</td>\n<td>None</td>\n</tr>\n<tr>\n<td>3</td>\n<td>-1</td>\n<td>None</td>\n<td>None</td>\n</tr></tbody></table>\n<p>Furthermore, several anchor points can be combined to form regions,\nwhich in turn can be mapped to identifiers and be given a clear name:</p>\n<table>\n<thead>\n<tr>\n<th>start</th>\n<th>end</th>\n<th>idx</th>\n<th>clear name</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>1</td>\n<td>\"0\"</td>\n<td>\"person X\"</td>\n</tr>\n<tr>\n<td>2</td>\n<td>3</td>\n<td>\"1\"</td>\n<td>\"person Y\"</td>\n</tr></tbody></table>\n<p>Example: Given the definition of anchors and regions above as well as\nsuitable wordlists, the following complex query extracts corpus\npositions where there's some correlation between \"person X\" (the\nregion from anchor 0 to anchor 1) and \"person Y\" (anchor 2 to 3):</p>\n<pre><span class=\"n\">query</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n\t<span class=\"s2\">\"&lt;np&gt; []* /ap[]* [lemma = $nouns_similarity] \"</span>\n\t<span class=\"s2\">\"[]*&lt;/np&gt; </span><span class=\"se\">\\\"</span><span class=\"s2\">between</span><span class=\"se\">\\\"</span><span class=\"s2\"> @0:[::](&lt;np&gt;[pos_simple=</span><span class=\"se\">\\\"</span><span class=\"s2\">D|A</span><span class=\"se\">\\\"</span><span class=\"s2\">]* \"</span>\n\t<span class=\"s2\">\"([pos_simple=</span><span class=\"se\">\\\"</span><span class=\"s2\">Z|P</span><span class=\"se\">\\\"</span><span class=\"s2\"> | lemma = $nouns_person_common | \"</span>\n\t<span class=\"s2\">\"lemma = $nouns_person_origin | lemma = $nouns_person_support | \"</span>\n\t<span class=\"s2\">\"lemma = $nouns_person_negative | \"</span>\n\t<span class=\"s2\">\"lemma = $nouns_person_profession] |/region[ner])+ \"</span>\n\t<span class=\"s2\">\"[]*&lt;/np&gt;)+@1:[::] </span><span class=\"se\">\\\"</span><span class=\"s2\">and</span><span class=\"se\">\\\"</span><span class=\"s2\"> @2:[::](&lt;np&gt;[pos_simple=</span><span class=\"se\">\\\"</span><span class=\"s2\">D|A</span><span class=\"se\">\\\"</span><span class=\"s2\">]* \"</span>\n\t<span class=\"s2\">\"([pos_simple=</span><span class=\"se\">\\\"</span><span class=\"s2\">Z|P</span><span class=\"se\">\\\"</span><span class=\"s2\"> | lemma = $nouns_person_common | \"</span>\n\t<span class=\"s2\">\"lemma = $nouns_person_origin | lemma = $nouns_person_support | \"</span>\n\t<span class=\"s2\">\"lemma = $nouns_person_negative | \"</span>\n\t<span class=\"s2\">\"lemma = $nouns_person_profession] | /region[ner])+ \"</span>\n\t<span class=\"s2\">\"[]*&lt;/np&gt;) (/region[np] | &lt;vp&gt;[lemma!=</span><span class=\"se\">\\\"</span><span class=\"s2\">be</span><span class=\"se\">\\\"</span><span class=\"s2\">]&lt;/vp&gt; | \"</span>\n\t<span class=\"s2\">\"/region[pp] |/be_ap[])* @3:[::]\"</span>\n<span class=\"p\">)</span>\n</pre>\n<p>NB: the set of identifiers defined in the table of anchors and in the\ntable of regions, respectively, should not overlap.</p>\n<p>It is customary to store these queries in json objects (see an\n<a href=\"tests/gold/query-example.json\" rel=\"nofollow\">example</a> in the repository).</p>\n<p>You can use the <code>concordancer</code> to process argument queries and display\nthe results:</p>\n<pre><span class=\"c1\"># read the query file</span>\n<span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"n\">query_path</span> <span class=\"o\">=</span> <span class=\"s2\">\"tests/gold/query-example.json\"</span>\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">query_path</span><span class=\"p\">,</span> <span class=\"s2\">\"rt\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n\t<span class=\"n\">query</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">())</span>\n\n<span class=\"c1\"># query the corpus and initialize the concordancer</span>\n<span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">query</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">[</span><span class=\"s1\">'query'</span><span class=\"p\">],</span> <span class=\"n\">context</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">s_break</span><span class=\"o\">=</span><span class=\"s1\">'tweet'</span><span class=\"p\">,</span> <span class=\"n\">match_strategy</span><span class=\"o\">=</span><span class=\"s1\">'longest'</span><span class=\"p\">)</span>\n<span class=\"n\">concordance</span> <span class=\"o\">=</span> <span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">concordance</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># show results</span>\n<span class=\"n\">concordance</span><span class=\"o\">.</span><span class=\"n\">show_argmin</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">[</span><span class=\"s1\">'anchors'</span><span class=\"p\">],</span> <span class=\"n\">query</span><span class=\"p\">[</span><span class=\"s1\">'regions'</span><span class=\"p\">])</span>\n</pre>\n<p>The <code>show_argmin</code> method returns the result as a <code>dict</code> with the\nfollowing keys:</p>\n<ul>\n<li>\"nr_matches\": the number of query matches in the corpus.</li>\n<li>\"holes\": a global list of all tokens of the entities specified in\nthe \"idx\" columns (default: lemma layer).</li>\n<li>\"meta\": the meta ids of the concordance lines.</li>\n<li>\"settings\": the query settings.</li>\n<li>\"matches\": a list of concordance lines. Each concordance line\ncontains:\n<ul>\n<li>\"position\": the corpus position of the match</li>\n<li>\"df\": the actual concordance line as returned from\n<code>Concordance().query()</code> (see above) converted to a <code>dict</code></li>\n<li>\"holes\": a mapping from the IDs specified in the anchor and region\ntables to the tokens or token sequences, respectively (default:\nlemma layer)</li>\n<li>\"full\": a reconstruction of the concordance line as a sequence of\ntokens (word layer)</li>\n</ul>\n</li>\n</ul>\n<h2>Acknowledgements</h2>\n<p>The module relies on\n<a href=\"https://pypi.org/project/cwb-python/\" rel=\"nofollow\">cwb-python</a>, thanks to Yannick\nVersley and Jorg Asmussen for the implementation. Special thanks to\nMarkus Opolka for the implementation of\n<a href=\"https://pypi.org/project/association-measures/\" rel=\"nofollow\">association-measures</a>\nand for forcing me to write tests.</p>\n<p>This work was supported by the <a href=\"https://www.fau.eu/research/collaborative-research/emerging-fields-initiative/\" rel=\"nofollow\">Emerging Fields Initiative\n(EFI)</a>\nof Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg, project title\n<a href=\"https://www.linguistik.phil.fau.de/projects/efe/\" rel=\"nofollow\">Exploring the <em>Fukushima\nEffect</em></a>.</p>\n<p>Further development of the package has been funded by the Deutsche\nForschungsgemeinschaft (DFG) within the project <em>Reconstructing\nArguments from Noisy Text</em>, grant number 377333057, as part of the\nPriority Program <a href=\"http://www.spp-ratio.de/home/\" rel=\"nofollow\">Robust Argumentation\nMachines</a> (SPP-1999).</p>\n\n          </div>"}, "last_serial": 6599146, "releases": {"0.9.2": [{"comment_text": "", "digests": {"md5": "5a6b90126c3cbfcf48d29c2ac738f528", "sha256": "81b3b792ed461a9176fe6e4b241b7414bccf021da169aa926ae1ef56c1d6d47c"}, "downloads": -1, "filename": "cwb_ccc-0.9.2-py3.6.egg", "has_sig": false, "md5_digest": "5a6b90126c3cbfcf48d29c2ac738f528", "packagetype": "bdist_egg", "python_version": "3.6", "requires_python": ">=3.5", "size": 31556, "upload_time": "2020-01-27T14:08:04", "upload_time_iso_8601": "2020-01-27T14:08:04.263872Z", "url": "https://files.pythonhosted.org/packages/44/94/af4daf47202e482c42f0df13219763a32164a08797cf2074c407e68daea9/cwb_ccc-0.9.2-py3.6.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "899bb57b5c999ac52e85b77933577476", "sha256": "c41a8c008b3aceec643c7551ad5f5be1df6d64acb693321d6d92a1717102ba7b"}, "downloads": -1, "filename": "cwb_ccc-0.9.2-py3-none-any.whl", "has_sig": false, "md5_digest": "899bb57b5c999ac52e85b77933577476", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 28822, "upload_time": "2020-01-27T14:08:00", "upload_time_iso_8601": "2020-01-27T14:08:00.179106Z", "url": "https://files.pythonhosted.org/packages/cc/e1/98ce004b940b1153d3f68fb20f29027ec07781d9fb6eb1fb93dd016af63f/cwb_ccc-0.9.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "825f63a26359a7313bf7029073bbc6a3", "sha256": "c402589493d1bd09ea550aada4411f52f9f045ffc3330e14222230cadb4faab8"}, "downloads": -1, "filename": "cwb-ccc-0.9.2.tar.gz", "has_sig": false, "md5_digest": "825f63a26359a7313bf7029073bbc6a3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 15371, "upload_time": "2020-01-27T14:08:05", "upload_time_iso_8601": "2020-01-27T14:08:05.461650Z", "url": "https://files.pythonhosted.org/packages/83/a1/de7dafeb56f36fb7af51d7f8cecf61e1ad9edd825971f7dc0e9cb7006099/cwb-ccc-0.9.2.tar.gz", "yanked": false}], "0.9.3": [{"comment_text": "", "digests": {"md5": "693f6013a3cfdce56add7f8cf4f36d3c", "sha256": "305d19bb961862c4f8dc12cbb797f66fbc2764bffcc2b22be5040c7cf4b88dd5"}, "downloads": -1, "filename": "cwb_ccc-0.9.3-py3-none-any.whl", "has_sig": false, "md5_digest": "693f6013a3cfdce56add7f8cf4f36d3c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 28909, "upload_time": "2020-01-27T14:58:29", "upload_time_iso_8601": "2020-01-27T14:58:29.876870Z", "url": "https://files.pythonhosted.org/packages/f4/d4/2ca2fc6ffeb47e86f5c9e6a31db1541373f9356f195d9847668565998ddd/cwb_ccc-0.9.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "662501132ff09462f2a157eb855bd245", "sha256": "71fbfd129f34ee6a08a35c0b3970272e9df04309739da26a335f8e49321b49be"}, "downloads": -1, "filename": "cwb-ccc-0.9.3.tar.gz", "has_sig": false, "md5_digest": "662501132ff09462f2a157eb855bd245", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 15523, "upload_time": "2020-01-27T14:58:31", "upload_time_iso_8601": "2020-01-27T14:58:31.122776Z", "url": "https://files.pythonhosted.org/packages/6f/61/401145be1e2e4863a268182b079c4356ace787276ddae00938e07d38d8cc/cwb-ccc-0.9.3.tar.gz", "yanked": false}], "0.9.4": [{"comment_text": "", "digests": {"md5": "9be78bdba3a24f158919f1c50cb01d2d", "sha256": "e2651e126aa264e071cff6f8439b7a8b4998dd4582eeab4a4494c3df1cbe10bd"}, "downloads": -1, "filename": "cwb_ccc-0.9.4-py3-none-any.whl", "has_sig": false, "md5_digest": "9be78bdba3a24f158919f1c50cb01d2d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 31760, "upload_time": "2020-01-28T16:46:47", "upload_time_iso_8601": "2020-01-28T16:46:47.817613Z", "url": "https://files.pythonhosted.org/packages/fa/6c/b55ec668c80371371dbf955fd6c548e8c241d437ef59232d832d8cdbde77/cwb_ccc-0.9.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f23f73e5046df8a16b7b9e49a4774f24", "sha256": "6acf37356bc59fb44a069877a8e4ac367a8c588e52bb4d34106e5b40115bd82b"}, "downloads": -1, "filename": "cwb-ccc-0.9.4.tar.gz", "has_sig": false, "md5_digest": "f23f73e5046df8a16b7b9e49a4774f24", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 19235, "upload_time": "2020-01-28T16:46:49", "upload_time_iso_8601": "2020-01-28T16:46:49.425098Z", "url": "https://files.pythonhosted.org/packages/48/6f/93ed0225cb0a265ea2c26dfd12f679b59d899e9057b0a6c3bc764de5a9c2/cwb-ccc-0.9.4.tar.gz", "yanked": false}], "0.9.5": [{"comment_text": "", "digests": {"md5": "f78fe65a9a4ea7c0d028dd80e9ec07db", "sha256": "9f381f3e8552c60990ac155cc3b0bede45ad969fdf9c51b987370d7181d67ffe"}, "downloads": -1, "filename": "cwb_ccc-0.9.5-py3-none-any.whl", "has_sig": false, "md5_digest": "f78fe65a9a4ea7c0d028dd80e9ec07db", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 33528, "upload_time": "2020-02-03T12:46:45", "upload_time_iso_8601": "2020-02-03T12:46:45.377007Z", "url": "https://files.pythonhosted.org/packages/43/d3/d58652d3425a17dec52a93034df8b0840d91b13dd5b2a76d20975efbe1f2/cwb_ccc-0.9.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "745cd8bda0eeb0e4a786c831b69aace6", "sha256": "d9431100879897fee6028c5b1cb2acdd230767c507b5e635cc49ac8d1472ce80"}, "downloads": -1, "filename": "cwb-ccc-0.9.5.tar.gz", "has_sig": false, "md5_digest": "745cd8bda0eeb0e4a786c831b69aace6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 21058, "upload_time": "2020-02-03T12:46:47", "upload_time_iso_8601": "2020-02-03T12:46:47.096961Z", "url": "https://files.pythonhosted.org/packages/25/a1/d4cbcc8130724ad9db85f334c9f91bfe2974e0c339f70bf0c762f119d755/cwb-ccc-0.9.5.tar.gz", "yanked": false}], "0.9.6": [{"comment_text": "", "digests": {"md5": "4dc08796f18dddcac4fb016d22c46b71", "sha256": "b6f476285230294479002a5bc1ceda5f906fce1c20085feeeeab1de543303a10"}, "downloads": -1, "filename": "cwb_ccc-0.9.6-py3-none-any.whl", "has_sig": false, "md5_digest": "4dc08796f18dddcac4fb016d22c46b71", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 36556, "upload_time": "2020-02-09T21:13:22", "upload_time_iso_8601": "2020-02-09T21:13:22.093537Z", "url": "https://files.pythonhosted.org/packages/1d/da/198be37abf46602d57eed29402c288a71cb31fd101b63e547ac426abc700/cwb_ccc-0.9.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "33e8c0c4adf56d8d4131d36f6b919bff", "sha256": "918cd3702f6c1f533a51e03f552ccd300789f27421cce62cbfe051b648e32737"}, "downloads": -1, "filename": "cwb-ccc-0.9.6.tar.gz", "has_sig": false, "md5_digest": "33e8c0c4adf56d8d4131d36f6b919bff", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 23965, "upload_time": "2020-02-09T21:13:24", "upload_time_iso_8601": "2020-02-09T21:13:24.053768Z", "url": "https://files.pythonhosted.org/packages/fb/8f/fc7ff387a6f85b077288c9a2fee2142c0030c5d05cb13b4b34fbeecbbc06/cwb-ccc-0.9.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "4dc08796f18dddcac4fb016d22c46b71", "sha256": "b6f476285230294479002a5bc1ceda5f906fce1c20085feeeeab1de543303a10"}, "downloads": -1, "filename": "cwb_ccc-0.9.6-py3-none-any.whl", "has_sig": false, "md5_digest": "4dc08796f18dddcac4fb016d22c46b71", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 36556, "upload_time": "2020-02-09T21:13:22", "upload_time_iso_8601": "2020-02-09T21:13:22.093537Z", "url": "https://files.pythonhosted.org/packages/1d/da/198be37abf46602d57eed29402c288a71cb31fd101b63e547ac426abc700/cwb_ccc-0.9.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "33e8c0c4adf56d8d4131d36f6b919bff", "sha256": "918cd3702f6c1f533a51e03f552ccd300789f27421cce62cbfe051b648e32737"}, "downloads": -1, "filename": "cwb-ccc-0.9.6.tar.gz", "has_sig": false, "md5_digest": "33e8c0c4adf56d8d4131d36f6b919bff", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 23965, "upload_time": "2020-02-09T21:13:24", "upload_time_iso_8601": "2020-02-09T21:13:24.053768Z", "url": "https://files.pythonhosted.org/packages/fb/8f/fc7ff387a6f85b077288c9a2fee2142c0030c5d05cb13b4b34fbeecbbc06/cwb-ccc-0.9.6.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:41:12 2020"}