{"info": {"author": "Marc Brinkmann", "author_email": "git@marcbrinkmann.de", "bugtrack_url": null, "classifiers": [], "description": "from ragstoriches\n=================\n\n``ragstoriches`` is a combined library/framework to ease writing web scrapers\nusing gevent and requests.\n\nA simple example to tell the story:\n\n.. code-block:: python\n\n  #!/usr/bin/env python\n  # -*- coding: utf-8 -*-\n\n  from urlparse import urljoin\n  import re\n\n  from bs4 import BeautifulSoup\n  from ragstoriches.scraper import Scraper\n\n  rr = Scraper(__name__)\n\n  @rr.scraper\n  def index(requests, context,\n            url='http://eastidaho.craigslist.org/search/act?query=+'):\n      soup = BeautifulSoup(requests.get(url).text)\n\n      for row in soup.find_all(class_='row'):\n          yield 'posting', context, urljoin(url, row.find('a').attrs['href'])\n\n      nextpage = soup.find(class_='nextpage')\n      if nextpage:\n          yield 'index', context, urljoin(url, nextpage.find('a').attrs['href'])\n\n\n  @rr.scraper\n  def posting(requests, context, url):\n      soup = BeautifulSoup(requests.get(url).text)\n      infos = soup.find(class_='postinginfos').find_all(class_='postinginfo')\n\n      title = soup.find(class_='postingtitle').text.strip()\n      id = re.findall('\\d+', infos[0].text)[0]\n      date = infos[1].find('date').text.strip()\n      body = soup.find(id='postingbody').text.strip()\n\n      print title\n      print '=' * len(title)\n      print 'post *%s*, posted on %s' % (id, date)\n      print body\n      print\n\nInstall the library and `BeautifulSoup 4\n<https://pypi.python.org/pypi/beautifulsoup4>`_ using ``pip install\nragstoriches beautifulsoup4``, then save the above as ``craigs.py``,\nfinally run with ``ragstoriches craigs.py``.\n\nYou will get a bunch of jumbled input, so next step is redirecting ``stdout``\nto a file::\n\n   ragstoriches craigs.py > output.md\n\nTry giving different urls for this scraper on the command-line:\n\n.. code-block:: sh\n\n   ragstoriches craigs.py http://newyork.craigslist.org/mnh/acc/  > output.md  # hustle\n   ragstoriches craigs.py http://orangecounty.craigslist.org/wet/ > output.md  # writing OC\n   ragstoriches craigs.py http://seattle.craigslist.org/w4m/      > output.md  # sleepless in seattle\n\nThere are a lot of commandline-options available, see ``ragstoriches --help``\nfor a list.\n\n\nWriting scrapers\n----------------\n\nA scraper module consists of some initialization code and a number of\nsubscrapers. Scraping starts by calling the a scraper named ``index`` on the\nscraper ``rr`` in the moduel (see the example above).\n\nThe ``requests`` argument should be treated like the `requests\n<http://python-requests.org>`_ module (it actually is an instance of requests\nPool). As long as you use it for fetching webpages, you never have to worry\nabout blocking or exceeding concurrency limits.\n\nThe ``context`` variable is arbitrary, but by convention a dictionary. It's a\nway of passing state from one scraper to another or sharing it. It is only\npassed on by ``ragstoriches`` and never touched otherwise.\n\nThe ``url`` is the url to scrape and parse.\n\nReturn values of scrapers are ignored. However, if a scraper is a generater\n(i.e. contains a yield statement), any value it yields must be a 3-tuple\nconsisting of the name of a scraper, a context object and another url. These\nare added to the queue of jobs to scrape.\n\nGood friends of ``ragstoriches`` are the `urlparse.urljoin\n<http://docs.python.org/2/library/urlparse.html#urlparse.urljoin>`_ function\nand `BeautifulSoup4 <https://beautiful-soup-4.readthedocs.org/en/latest/>`_.\n\n\nUsage as a library\n------------------\n\nYou can use ragstoriches as a library as well by not using the commandline\ntools but simply importing a scraper and running it with the ``scrape()``\nmethod. Remember to monkey-patch using gevent first.\n\nSee the source files for details, as there is not that much documentation\navailable at this point.", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/mbr/ragstoriches", "keywords": null, "license": "MIT", "maintainer": null, "maintainer_email": null, "name": "ragstoriches", "package_url": "https://pypi.org/project/ragstoriches/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/ragstoriches/", "project_urls": {"Download": "UNKNOWN", "Homepage": "http://github.com/mbr/ragstoriches"}, "release_url": "https://pypi.org/project/ragstoriches/0.2/", "requires_dist": null, "requires_python": null, "summary": "Develop highly-concurrent web scrapers, easily.", "version": "0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><tt>ragstoriches</tt> is a combined library/framework to ease writing web scrapers\nusing gevent and requests.</p>\n<p>A simple example to tell the story:</p>\n<pre><span class=\"ch\">#!/usr/bin/env python</span>\n<span class=\"c1\"># -*- coding: utf-8 -*-</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">urlparse</span> <span class=\"kn\">import</span> <span class=\"n\">urljoin</span>\n<span class=\"kn\">import</span> <span class=\"nn\">re</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">bs4</span> <span class=\"kn\">import</span> <span class=\"n\">BeautifulSoup</span>\n<span class=\"kn\">from</span> <span class=\"nn\">ragstoriches.scraper</span> <span class=\"kn\">import</span> <span class=\"n\">Scraper</span>\n\n<span class=\"n\">rr</span> <span class=\"o\">=</span> <span class=\"n\">Scraper</span><span class=\"p\">(</span><span class=\"vm\">__name__</span><span class=\"p\">)</span>\n\n<span class=\"nd\">@rr</span><span class=\"o\">.</span><span class=\"n\">scraper</span>\n<span class=\"k\">def</span> <span class=\"nf\">index</span><span class=\"p\">(</span><span class=\"n\">requests</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"p\">,</span>\n          <span class=\"n\">url</span><span class=\"o\">=</span><span class=\"s1\">'http://eastidaho.craigslist.org/search/act?query=+'</span><span class=\"p\">):</span>\n    <span class=\"n\">soup</span> <span class=\"o\">=</span> <span class=\"n\">BeautifulSoup</span><span class=\"p\">(</span><span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">soup</span><span class=\"o\">.</span><span class=\"n\">find_all</span><span class=\"p\">(</span><span class=\"n\">class_</span><span class=\"o\">=</span><span class=\"s1\">'row'</span><span class=\"p\">):</span>\n        <span class=\"k\">yield</span> <span class=\"s1\">'posting'</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"p\">,</span> <span class=\"n\">urljoin</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">row</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">attrs</span><span class=\"p\">[</span><span class=\"s1\">'href'</span><span class=\"p\">])</span>\n\n    <span class=\"n\">nextpage</span> <span class=\"o\">=</span> <span class=\"n\">soup</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"n\">class_</span><span class=\"o\">=</span><span class=\"s1\">'nextpage'</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">nextpage</span><span class=\"p\">:</span>\n        <span class=\"k\">yield</span> <span class=\"s1\">'index'</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"p\">,</span> <span class=\"n\">urljoin</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">nextpage</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">attrs</span><span class=\"p\">[</span><span class=\"s1\">'href'</span><span class=\"p\">])</span>\n\n\n<span class=\"nd\">@rr</span><span class=\"o\">.</span><span class=\"n\">scraper</span>\n<span class=\"k\">def</span> <span class=\"nf\">posting</span><span class=\"p\">(</span><span class=\"n\">requests</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"p\">,</span> <span class=\"n\">url</span><span class=\"p\">):</span>\n    <span class=\"n\">soup</span> <span class=\"o\">=</span> <span class=\"n\">BeautifulSoup</span><span class=\"p\">(</span><span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">)</span>\n    <span class=\"n\">infos</span> <span class=\"o\">=</span> <span class=\"n\">soup</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"n\">class_</span><span class=\"o\">=</span><span class=\"s1\">'postinginfos'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">find_all</span><span class=\"p\">(</span><span class=\"n\">class_</span><span class=\"o\">=</span><span class=\"s1\">'postinginfo'</span><span class=\"p\">)</span>\n\n    <span class=\"n\">title</span> <span class=\"o\">=</span> <span class=\"n\">soup</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"n\">class_</span><span class=\"o\">=</span><span class=\"s1\">'postingtitle'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span>\n    <span class=\"nb\">id</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">findall</span><span class=\"p\">(</span><span class=\"s1\">'\\d+'</span><span class=\"p\">,</span> <span class=\"n\">infos</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">date</span> <span class=\"o\">=</span> <span class=\"n\">infos</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">'date'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span>\n    <span class=\"n\">body</span> <span class=\"o\">=</span> <span class=\"n\">soup</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"s1\">'postingbody'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span>\n\n    <span class=\"nb\">print</span> <span class=\"n\">title</span>\n    <span class=\"nb\">print</span> <span class=\"s1\">'='</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">title</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span> <span class=\"s1\">'post *</span><span class=\"si\">%s</span><span class=\"s1\">*, posted on </span><span class=\"si\">%s</span><span class=\"s1\">'</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"nb\">id</span><span class=\"p\">,</span> <span class=\"n\">date</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span> <span class=\"n\">body</span>\n    <span class=\"nb\">print</span>\n</pre>\n<p>Install the library and <a href=\"https://pypi.python.org/pypi/beautifulsoup4\" rel=\"nofollow\">BeautifulSoup 4</a> using <tt>pip install\nragstoriches beautifulsoup4</tt>, then save the above as <tt>craigs.py</tt>,\nfinally run with <tt>ragstoriches craigs.py</tt>.</p>\n<p>You will get a bunch of jumbled input, so next step is redirecting <tt>stdout</tt>\nto a file:</p>\n<pre>ragstoriches craigs.py &gt; output.md\n</pre>\n<p>Try giving different urls for this scraper on the command-line:</p>\n<pre>ragstoriches craigs.py http://newyork.craigslist.org/mnh/acc/  &gt; output.md  <span class=\"c1\"># hustle\n</span>ragstoriches craigs.py http://orangecounty.craigslist.org/wet/ &gt; output.md  <span class=\"c1\"># writing OC\n</span>ragstoriches craigs.py http://seattle.craigslist.org/w4m/      &gt; output.md  <span class=\"c1\"># sleepless in seattle</span>\n</pre>\n<p>There are a lot of commandline-options available, see <tt>ragstoriches <span class=\"pre\">--help</span></tt>\nfor a list.</p>\n<div id=\"writing-scrapers\">\n<h2>Writing scrapers</h2>\n<p>A scraper module consists of some initialization code and a number of\nsubscrapers. Scraping starts by calling the a scraper named <tt>index</tt> on the\nscraper <tt>rr</tt> in the moduel (see the example above).</p>\n<p>The <tt>requests</tt> argument should be treated like the <a href=\"http://python-requests.org\" rel=\"nofollow\">requests</a> module (it actually is an instance of requests\nPool). As long as you use it for fetching webpages, you never have to worry\nabout blocking or exceeding concurrency limits.</p>\n<p>The <tt>context</tt> variable is arbitrary, but by convention a dictionary. It\u2019s a\nway of passing state from one scraper to another or sharing it. It is only\npassed on by <tt>ragstoriches</tt> and never touched otherwise.</p>\n<p>The <tt>url</tt> is the url to scrape and parse.</p>\n<p>Return values of scrapers are ignored. However, if a scraper is a generater\n(i.e. contains a yield statement), any value it yields must be a 3-tuple\nconsisting of the name of a scraper, a context object and another url. These\nare added to the queue of jobs to scrape.</p>\n<p>Good friends of <tt>ragstoriches</tt> are the <a href=\"http://docs.python.org/2/library/urlparse.html#urlparse.urljoin\" rel=\"nofollow\">urlparse.urljoin</a> function\nand <a href=\"https://beautiful-soup-4.readthedocs.org/en/latest/\" rel=\"nofollow\">BeautifulSoup4</a>.</p>\n</div>\n<div id=\"usage-as-a-library\">\n<h2>Usage as a library</h2>\n<p>You can use ragstoriches as a library as well by not using the commandline\ntools but simply importing a scraper and running it with the <tt>scrape()</tt>\nmethod. Remember to monkey-patch using gevent first.</p>\n<p>See the source files for details, as there is not that much documentation\navailable at this point.</p>\n</div>\n\n          </div>"}, "last_serial": 798439, "releases": {"0.2": [{"comment_text": "", "digests": {"md5": "1bfd0bc2e8d04ddea35222e1b87ebd6b", "sha256": "dfc981d0adf3a0d294c2783bd1630d7d38af1cc9b5c81a6f7359f6c7b319bbee"}, "downloads": -1, "filename": "ragstoriches-0.2.tar.gz", "has_sig": true, "md5_digest": "1bfd0bc2e8d04ddea35222e1b87ebd6b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4872, "upload_time": "2013-03-10T16:43:01", "upload_time_iso_8601": "2013-03-10T16:43:01.803599Z", "url": "https://files.pythonhosted.org/packages/52/0e/46ae7bfb921137c59e449b55c2a7f64df08ded8c4f78d02c18ca54efaa91/ragstoriches-0.2.tar.gz", "yanked": false}], "0.3.1dev": [{"comment_text": "", "digests": {"md5": "ba336638539376f7c7380d1bad645ebb", "sha256": "8c572bb9a58364b5e767708fcd35c2120f4716cd1a13821bf6395158ac34bb79"}, "downloads": -1, "filename": "ragstoriches-0.3.1dev.tar.gz", "has_sig": true, "md5_digest": "ba336638539376f7c7380d1bad645ebb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6589, "upload_time": "2013-03-13T01:20:30", "upload_time_iso_8601": "2013-03-13T01:20:30.553083Z", "url": "https://files.pythonhosted.org/packages/b6/71/fa3f9a5e97ad97467bf75784b6302a713db5d7ca9f3a3a50e857adef09e8/ragstoriches-0.3.1dev.tar.gz", "yanked": false}], "0.3dev": [{"comment_text": "", "digests": {"md5": "4e7d5dc6ccd3946daa01251ea3d971e9", "sha256": "a216eca9cbfcd8bd0c2923e674a55ef6600fc28dec5d422fbc3ebd5f44046401"}, "downloads": -1, "filename": "ragstoriches-0.3dev.tar.gz", "has_sig": true, "md5_digest": "4e7d5dc6ccd3946daa01251ea3d971e9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5619, "upload_time": "2013-03-12T19:12:04", "upload_time_iso_8601": "2013-03-12T19:12:04.884704Z", "url": "https://files.pythonhosted.org/packages/a8/69/ee1c274e2c28046b632c2c0b16f10dc5229d5581db395d66f0b90da7357d/ragstoriches-0.3dev.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1bfd0bc2e8d04ddea35222e1b87ebd6b", "sha256": "dfc981d0adf3a0d294c2783bd1630d7d38af1cc9b5c81a6f7359f6c7b319bbee"}, "downloads": -1, "filename": "ragstoriches-0.2.tar.gz", "has_sig": true, "md5_digest": "1bfd0bc2e8d04ddea35222e1b87ebd6b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4872, "upload_time": "2013-03-10T16:43:01", "upload_time_iso_8601": "2013-03-10T16:43:01.803599Z", "url": "https://files.pythonhosted.org/packages/52/0e/46ae7bfb921137c59e449b55c2a7f64df08ded8c4f78d02c18ca54efaa91/ragstoriches-0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:07:53 2020"}