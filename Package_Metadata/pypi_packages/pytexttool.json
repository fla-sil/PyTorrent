{"info": {"author": "Clementine", "author_email": "iclementine@outlook.com", "bugtrack_url": null, "classifiers": [], "description": "pytext\n+++++++++\n\nThis repository consists of:\n\n* `pytext.data <#data>`_: Generic data loaders, abstractions, and iterators for text (including vocabulary and word vectors)\n* `pytext.datasets <#datasets>`_: Pre-built loaders for common NLP datasets\n\nIt is a fork of torchtext, but use numpy ndarray for dataset instead of torch.Tensor or Variable, so as to make it a more generic toolbox for NLP users.\n\nInstallation\n============\n\n\nMake sure you have Python 2.7 or 3.5+ and PyTorch 0.4.0 or newer. You can then install torchtext using pip::\n\n    pip install pytexttool\n\n\nOptional requirements\n---------------------\n\nIf you want to use English tokenizer from `SpaCy <http://spacy.io/>`_, you need to install SpaCy and download its English model::\n\n    pip install spacy\n    python -m spacy download en\n\nAlternatively, you might want to use Moses tokenizer from `NLTK <http://nltk.org/>`_. You have to install NLTK and download the data needed::\n\n    pip install nltk\n    python -m nltk.downloader perluniprops nonbreaking_prefixes\n\nData\n====\n\nThe data module provides the following:\n\n* Ability to describe declaratively how to load a custom NLP dataset that's in a \"normal\" format:\n\n  .. code-block:: python\n\n      >>> pos = data.TabularDataset(\n      ...    path='data/pos/pos_wsj_train.tsv', format='tsv',\n      ...    fields=[('text', data.Field()),\n      ...            ('labels', data.Field())])\n      ...\n      >>> sentiment = data.TabularDataset(\n      ...    path='data/sentiment/train.json', format='json',\n      ...    fields={'sentence_tokenized': ('text', data.Field(sequential=True)),\n      ...            'sentiment_gold': ('labels', data.Field(sequential=False))})\n\n* Ability to define a preprocessing pipeline:\n\n  .. code-block:: python\n\n      >>> src = data.Field(tokenize=my_custom_tokenizer)\n      >>> trg = data.Field(tokenize=my_custom_tokenizer)\n      >>> mt_train = datasets.TranslationDataset(\n      ...     path='data/mt/wmt16-ende.train', exts=('.en', '.de'),\n      ...     fields=(src, trg))\n\n* Batching, padding, and numericalizing (including building a vocabulary object):\n\n  .. code-block:: python\n\n      >>> # continuing from above\n      >>> mt_dev = datasets.TranslationDataset(\n      ...     path='data/mt/newstest2014', exts=('.en', '.de'),\n      ...     fields=(src, trg))\n      >>> src.build_vocab(mt_train, max_size=80000)\n      >>> trg.build_vocab(mt_train, max_size=40000)\n      >>> # mt_dev shares the fields, so it shares their vocab objects\n      >>>\n      >>> train_iter = data.BucketIterator(\n      ...     dataset=mt_train, batch_size=32,\n      ...     sort_key=lambda x: data.interleave_keys(len(x.src), len(x.trg)))\n      >>> # usage\n      >>> next(iter(train_iter))\n      <data.Batch(batch_size=32, src=[LongTensor (32, 25)], trg=[LongTensor (32, 28)])>\n\n* Wrapper for dataset splits (train, validation, test):\n\n  .. code-block:: python\n\n      >>> TEXT = data.Field()\n      >>> LABELS = data.Field()\n      >>>\n      >>> train, val, test = data.TabularDataset.splits(\n      ...     path='/data/pos_wsj/pos_wsj', train='_train.tsv',\n      ...     validation='_dev.tsv', test='_test.tsv', format='tsv',\n      ...     fields=[('text', TEXT), ('labels', LABELS)])\n      >>>\n      >>> train_iter, val_iter, test_iter = data.BucketIterator.splits(\n      ...     (train, val, test), batch_sizes=(16, 256, 256),\n      >>>     sort_key=lambda x: len(x.text), device=0)\n      >>>\n      >>> TEXT.build_vocab(train)\n      >>> LABELS.build_vocab(train)\n\nDatasets\n========\n\nThe datasets module currently contains:\n\n* Sentiment analysis: SST and IMDb\n* Question classification: TREC\n* Entailment: SNLI\n* Language modeling: abstract class + WikiText-2\n* Machine translation: abstract class + Multi30k, IWSLT, WMT14\n* Sequence tagging (e.g. POS/NER): abstract class + UDPOS\n\nOthers are planned or a work in progress:\n\n* Question answering: SQuAD\n\nSee the ``test`` directory for examples of dataset usage.\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/iclementine/text", "keywords": "", "license": "BSD", "maintainer": "", "maintainer_email": "", "name": "pytexttool", "package_url": "https://pypi.org/project/pytexttool/", "platform": "", "project_url": "https://pypi.org/project/pytexttool/", "project_urls": {"Homepage": "https://github.com/iclementine/text"}, "release_url": "https://pypi.org/project/pytexttool/0.3.1/", "requires_dist": ["requests", "tqdm"], "requires_python": "", "summary": "Text utilities and datasets for generic deep learning, Fork from torchtext but uses numpy to store datasets for more generic use.", "version": "0.3.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>This repository consists of:</p>\n<ul>\n<li><a href=\"#data\" rel=\"nofollow\">pytext.data</a>: Generic data loaders, abstractions, and iterators for text (including vocabulary and word vectors)</li>\n<li><a href=\"#datasets\" rel=\"nofollow\">pytext.datasets</a>: Pre-built loaders for common NLP datasets</li>\n</ul>\n<p>It is a fork of torchtext, but use numpy ndarray for dataset instead of torch.Tensor or Variable, so as to make it a more generic toolbox for NLP users.</p>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>Make sure you have Python 2.7 or 3.5+ and PyTorch 0.4.0 or newer. You can then install torchtext using pip:</p>\n<pre>pip install pytexttool\n</pre>\n<div id=\"optional-requirements\">\n<h3>Optional requirements</h3>\n<p>If you want to use English tokenizer from <a href=\"http://spacy.io/\" rel=\"nofollow\">SpaCy</a>, you need to install SpaCy and download its English model:</p>\n<pre>pip install spacy\npython -m spacy download en\n</pre>\n<p>Alternatively, you might want to use Moses tokenizer from <a href=\"http://nltk.org/\" rel=\"nofollow\">NLTK</a>. You have to install NLTK and download the data needed:</p>\n<pre>pip install nltk\npython -m nltk.downloader perluniprops nonbreaking_prefixes\n</pre>\n</div>\n</div>\n<div id=\"data\">\n<h2>Data</h2>\n<p>The data module provides the following:</p>\n<ul>\n<li><p>Ability to describe declaratively how to load a custom NLP dataset that\u2019s in a \u201cnormal\u201d format:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">pos</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">TabularDataset</span><span class=\"p\">(</span>\n<span class=\"o\">...</span>    <span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">'data/pos/pos_wsj_train.tsv'</span><span class=\"p\">,</span> <span class=\"nb\">format</span><span class=\"o\">=</span><span class=\"s1\">'tsv'</span><span class=\"p\">,</span>\n<span class=\"o\">...</span>    <span class=\"n\">fields</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">'text'</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()),</span>\n<span class=\"o\">...</span>            <span class=\"p\">(</span><span class=\"s1\">'labels'</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">())])</span>\n<span class=\"o\">...</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">sentiment</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">TabularDataset</span><span class=\"p\">(</span>\n<span class=\"o\">...</span>    <span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">'data/sentiment/train.json'</span><span class=\"p\">,</span> <span class=\"nb\">format</span><span class=\"o\">=</span><span class=\"s1\">'json'</span><span class=\"p\">,</span>\n<span class=\"o\">...</span>    <span class=\"n\">fields</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'sentence_tokenized'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"s1\">'text'</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">sequential</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)),</span>\n<span class=\"o\">...</span>            <span class=\"s1\">'sentiment_gold'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"s1\">'labels'</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">sequential</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">))})</span>\n</pre>\n</li>\n<li><p>Ability to define a preprocessing pipeline:</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">src</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">tokenize</span><span class=\"o\">=</span><span class=\"n\">my_custom_tokenizer</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">trg</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">tokenize</span><span class=\"o\">=</span><span class=\"n\">my_custom_tokenizer</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">mt_train</span> <span class=\"o\">=</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">TranslationDataset</span><span class=\"p\">(</span>\n<span class=\"o\">...</span>     <span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">'data/mt/wmt16-ende.train'</span><span class=\"p\">,</span> <span class=\"n\">exts</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"s1\">'.en'</span><span class=\"p\">,</span> <span class=\"s1\">'.de'</span><span class=\"p\">),</span>\n<span class=\"o\">...</span>     <span class=\"n\">fields</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">src</span><span class=\"p\">,</span> <span class=\"n\">trg</span><span class=\"p\">))</span>\n</pre>\n</li>\n<li><p>Batching, padding, and numericalizing (including building a vocabulary object):</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># continuing from above</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">mt_dev</span> <span class=\"o\">=</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">TranslationDataset</span><span class=\"p\">(</span>\n<span class=\"o\">...</span>     <span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">'data/mt/newstest2014'</span><span class=\"p\">,</span> <span class=\"n\">exts</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"s1\">'.en'</span><span class=\"p\">,</span> <span class=\"s1\">'.de'</span><span class=\"p\">),</span>\n<span class=\"o\">...</span>     <span class=\"n\">fields</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">src</span><span class=\"p\">,</span> <span class=\"n\">trg</span><span class=\"p\">))</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">build_vocab</span><span class=\"p\">(</span><span class=\"n\">mt_train</span><span class=\"p\">,</span> <span class=\"n\">max_size</span><span class=\"o\">=</span><span class=\"mi\">80000</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">trg</span><span class=\"o\">.</span><span class=\"n\">build_vocab</span><span class=\"p\">(</span><span class=\"n\">mt_train</span><span class=\"p\">,</span> <span class=\"n\">max_size</span><span class=\"o\">=</span><span class=\"mi\">40000</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># mt_dev shares the fields, so it shares their vocab objects</span>\n<span class=\"o\">&gt;&gt;&gt;</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">train_iter</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">BucketIterator</span><span class=\"p\">(</span>\n<span class=\"o\">...</span>     <span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"n\">mt_train</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span>\n<span class=\"o\">...</span>     <span class=\"n\">sort_key</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">interleave_keys</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"p\">),</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">trg</span><span class=\"p\">)))</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># usage</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"nb\">iter</span><span class=\"p\">(</span><span class=\"n\">train_iter</span><span class=\"p\">))</span>\n<span class=\"o\">&lt;</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Batch</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"n\">src</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">LongTensor</span> <span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">25</span><span class=\"p\">)],</span> <span class=\"n\">trg</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">LongTensor</span> <span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">)])</span><span class=\"o\">&gt;</span>\n</pre>\n</li>\n<li><p>Wrapper for dataset splits (train, validation, test):</p>\n<pre><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">TEXT</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">LABELS</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">val</span><span class=\"p\">,</span> <span class=\"n\">test</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">TabularDataset</span><span class=\"o\">.</span><span class=\"n\">splits</span><span class=\"p\">(</span>\n<span class=\"o\">...</span>     <span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">'/data/pos_wsj/pos_wsj'</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"s1\">'_train.tsv'</span><span class=\"p\">,</span>\n<span class=\"o\">...</span>     <span class=\"n\">validation</span><span class=\"o\">=</span><span class=\"s1\">'_dev.tsv'</span><span class=\"p\">,</span> <span class=\"n\">test</span><span class=\"o\">=</span><span class=\"s1\">'_test.tsv'</span><span class=\"p\">,</span> <span class=\"nb\">format</span><span class=\"o\">=</span><span class=\"s1\">'tsv'</span><span class=\"p\">,</span>\n<span class=\"o\">...</span>     <span class=\"n\">fields</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">'text'</span><span class=\"p\">,</span> <span class=\"n\">TEXT</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">'labels'</span><span class=\"p\">,</span> <span class=\"n\">LABELS</span><span class=\"p\">)])</span>\n<span class=\"o\">&gt;&gt;&gt;</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">train_iter</span><span class=\"p\">,</span> <span class=\"n\">val_iter</span><span class=\"p\">,</span> <span class=\"n\">test_iter</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">BucketIterator</span><span class=\"o\">.</span><span class=\"n\">splits</span><span class=\"p\">(</span>\n<span class=\"o\">...</span>     <span class=\"p\">(</span><span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">val</span><span class=\"p\">,</span> <span class=\"n\">test</span><span class=\"p\">),</span> <span class=\"n\">batch_sizes</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">),</span>\n<span class=\"o\">&gt;&gt;&gt;</span>     <span class=\"n\">sort_key</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">TEXT</span><span class=\"o\">.</span><span class=\"n\">build_vocab</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">LABELS</span><span class=\"o\">.</span><span class=\"n\">build_vocab</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"p\">)</span>\n</pre>\n</li>\n</ul>\n</div>\n<div id=\"datasets\">\n<h2>Datasets</h2>\n<p>The datasets module currently contains:</p>\n<ul>\n<li>Sentiment analysis: SST and IMDb</li>\n<li>Question classification: TREC</li>\n<li>Entailment: SNLI</li>\n<li>Language modeling: abstract class + WikiText-2</li>\n<li>Machine translation: abstract class + Multi30k, IWSLT, WMT14</li>\n<li>Sequence tagging (e.g. POS/NER): abstract class + UDPOS</li>\n</ul>\n<p>Others are planned or a work in progress:</p>\n<ul>\n<li>Question answering: SQuAD</li>\n</ul>\n<p>See the <tt>test</tt> directory for examples of dataset usage.</p>\n</div>\n\n          </div>"}, "last_serial": 4206543, "releases": {"0.3.1": [{"comment_text": "", "digests": {"md5": "46f272a1c6d492eae96273f1384f4b73", "sha256": "abd399bb54948264fb9416048ea0c0fdf967dee98513e82e22862dbb6e3986cd"}, "downloads": -1, "filename": "pytexttool-0.3.1-py2-none-any.whl", "has_sig": false, "md5_digest": "46f272a1c6d492eae96273f1384f4b73", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 60524, "upload_time": "2018-08-25T13:23:49", "upload_time_iso_8601": "2018-08-25T13:23:49.524967Z", "url": "https://files.pythonhosted.org/packages/86/1e/e42c90663c9840425a6b640d4808c0507b137b1dcb7c3d2a73f362eb627e/pytexttool-0.3.1-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1300de3e55bf1f90168f342989b83bf8", "sha256": "5ba8bef67e8698b04374fed400fd4f64b6ce460737bf4228d04a11c3b2ec2c14"}, "downloads": -1, "filename": "pytexttool-0.3.1-py3-none-any.whl", "has_sig": false, "md5_digest": "1300de3e55bf1f90168f342989b83bf8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 60518, "upload_time": "2018-08-25T13:23:51", "upload_time_iso_8601": "2018-08-25T13:23:51.250278Z", "url": "https://files.pythonhosted.org/packages/84/e2/c7ea7b4b65acd7b63917699cf2a5a9dc6d6430148e0d799cf41b81dc9969/pytexttool-0.3.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "36b1174cbf7129ff4b210cc4f2ebd6ee", "sha256": "a0718c0a6ac5a63266ebaff5780eba63e5b01d137b8f740adb3aab593a853915"}, "downloads": -1, "filename": "pytexttool-0.3.1.tar.gz", "has_sig": false, "md5_digest": "36b1174cbf7129ff4b210cc4f2ebd6ee", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 47953, "upload_time": "2018-08-25T13:23:53", "upload_time_iso_8601": "2018-08-25T13:23:53.329293Z", "url": "https://files.pythonhosted.org/packages/17/a7/3dae3ef0f428183ccca084fd7a73747c50b6222be0fdbab4a17542a8d416/pytexttool-0.3.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "46f272a1c6d492eae96273f1384f4b73", "sha256": "abd399bb54948264fb9416048ea0c0fdf967dee98513e82e22862dbb6e3986cd"}, "downloads": -1, "filename": "pytexttool-0.3.1-py2-none-any.whl", "has_sig": false, "md5_digest": "46f272a1c6d492eae96273f1384f4b73", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 60524, "upload_time": "2018-08-25T13:23:49", "upload_time_iso_8601": "2018-08-25T13:23:49.524967Z", "url": "https://files.pythonhosted.org/packages/86/1e/e42c90663c9840425a6b640d4808c0507b137b1dcb7c3d2a73f362eb627e/pytexttool-0.3.1-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1300de3e55bf1f90168f342989b83bf8", "sha256": "5ba8bef67e8698b04374fed400fd4f64b6ce460737bf4228d04a11c3b2ec2c14"}, "downloads": -1, "filename": "pytexttool-0.3.1-py3-none-any.whl", "has_sig": false, "md5_digest": "1300de3e55bf1f90168f342989b83bf8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 60518, "upload_time": "2018-08-25T13:23:51", "upload_time_iso_8601": "2018-08-25T13:23:51.250278Z", "url": "https://files.pythonhosted.org/packages/84/e2/c7ea7b4b65acd7b63917699cf2a5a9dc6d6430148e0d799cf41b81dc9969/pytexttool-0.3.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "36b1174cbf7129ff4b210cc4f2ebd6ee", "sha256": "a0718c0a6ac5a63266ebaff5780eba63e5b01d137b8f740adb3aab593a853915"}, "downloads": -1, "filename": "pytexttool-0.3.1.tar.gz", "has_sig": false, "md5_digest": "36b1174cbf7129ff4b210cc4f2ebd6ee", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 47953, "upload_time": "2018-08-25T13:23:53", "upload_time_iso_8601": "2018-08-25T13:23:53.329293Z", "url": "https://files.pythonhosted.org/packages/17/a7/3dae3ef0f428183ccca084fd7a73747c50b6222be0fdbab4a17542a8d416/pytexttool-0.3.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:54:31 2020"}