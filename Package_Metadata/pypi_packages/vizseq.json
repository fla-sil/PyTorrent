{"info": {"author": "", "author_email": "", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3 :: Only", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "[![PyPI](https://img.shields.io/pypi/v/vizseq?style=flat-square)](https://pypi.org/project/vizseq/)\n[![CircleCI](https://img.shields.io/circleci/build/github/facebookresearch/vizseq?style=flat-square)](https://circleci.com/gh/facebookresearch/vizseq)\n![PyPI - License](https://img.shields.io/pypi/l/vizseq?style=flat-square)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/vizseq?style=flat-square)\n\n# <img src=\"logo.png\" alt=\"VizSeq\" width=\"160\">\nVizSeq is a Python toolkit for visual analysis on text generation tasks like machine translation, summarization,\nimage captioning, speech translation and video description. It takes multi-modal sources,\ntext references as well as text predictions as inputs, and analyzes them visually\nin [Jupyter Notebook](https://facebookresearch.github.io/vizseq/docs/getting_started/ipynb_example) or a\nbuilt-in [Web App](https://facebookresearch.github.io/vizseq/docs/getting_started/web_app_example)\n(the former has [Fairseq integration](https://facebookresearch.github.io/vizseq/docs/getting_started/fairseq_example)).\nVizSeq also provides a collection of [multi-process scorers](https://facebookresearch.github.io/vizseq/docs/features/metrics) as\na normal Python package.\n\n[[Paper]](https://arxiv.org/pdf/1909.05424.pdf)\n[[Documentation]](https://facebookresearch.github.io/vizseq)\n[[Blog]](https://ai.facebook.com/blog/vizseq-a-visual-analysis-toolkit-for-accelerating-text-generation-research)\n\n<p align=\"center\">\n<img src=\"overview.png\" alt=\"VizSeq Overview\" width=\"480\">\n<img src=\"teaser.gif\" alt=\"VizSeq Teaser\" width=\"480\">\n</p>\n\n### Task Coverage\n\n| Source | Example Tasks |\n| :--- | :--- |\n| Text | Machine translation, text summarization, dialog generation, grammatical error correction, open-domain question answering |\n| Image | Image captioning, image question answering, optical character recognition                                                |\n| Audio | Speech recognition, speech translation                                                                                   |\n| Video | Video description                                                                                                        |\n| Multimodal | Multimodal machine translation\n\n### Metric Coverage\n**Accelerated with multi-processing/multi-threading.**\n\n| Type | Metrics |\n| :--- | :--- |\n| N-gram-based | BLEU ([Papineni et al., 2002](https://www.aclweb.org/anthology/P02-1040)), NIST ([Doddington, 2002](http://www.mt-archive.info/HLT-2002-Doddington.pdf)), METEOR ([Banerjee et al., 2005](https://www.aclweb.org/anthology/W05-0909)), TER ([Snover et al., 2006](http://mt-archive.info/AMTA-2006-Snover.pdf)), RIBES ([Isozaki et al., 2010](https://www.aclweb.org/anthology/D10-1092)), chrF ([Popovi\u0107 et al., 2015](https://www.aclweb.org/anthology/W15-3049)), GLEU ([Wu et al., 2016](https://arxiv.org/pdf/1609.08144.pdf)), ROUGE ([Lin, 2004](https://www.aclweb.org/anthology/W04-1013)), CIDEr ([Vedantam et al., 2015](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf)), WER |\n| Embedding-based | LASER ([Artetxe and Schwenk, 2018](https://arxiv.org/pdf/1812.10464.pdf)), BERTScore ([Zhang et al., 2019](https://arxiv.org/pdf/1904.09675.pdf)) |\n\n\n## Getting Started\n\n### Installation\nVizSeq requires **Python 3.6+** and currently runs on **Unix/Linux** and **macOS/OS X**. It will support **Windows** as well in the future.\n\nYou can install VizSeq from PyPI repository:\n```bash\n$ pip install vizseq\n```\n\nOr install it from source:\n```bash\n$ git clone https://github.com/facebookresearch/vizseq\n$ cd vizseq\n$ pip install -e .\n```\n\n### [Documentation](https://facebookresearch.github.io/vizseq)\n\n### Jupyter Notebook Examples\n- [Basic example](https://facebookresearch.github.io/vizseq/docs/getting_started/ipynb_example)\n- [Multimodal Machine Translation](examples/multimodal_machine_translation.ipynb)\n- [Multilingual Machine Translation](examples/multilingual_machine_translation.ipynb)\n- [Speech Translation](examples/speech_translation.ipynb)\n\n### [Fairseq integration](https://facebookresearch.github.io/vizseq/docs/getting_started/fairseq_example)\n\n### [Web App Example](https://facebookresearch.github.io/vizseq/docs/getting_started/web_app_example)\nDownload example data:\n```bash\n$ git clone https://github.com/facebookresearch/vizseq\n$ cd vizseq\n$ bash get_example_data.sh\n```\nLaunch the web server:\n```bash\n$ python -m vizseq.server --port 9001 --data-root ./examples/data\n```\nAnd then, navigate to the following URL in your web browser:\n```text\nhttp://localhost:9001\n```\n\n## License\nVizSeq is licensed under [MIT](https://github.com/facebookresearch/vizseq/blob/master/LICENSE). See the [LICENSE](https://github.com/facebookresearch/vizseq/blob/master/LICENSE) file for details.\n\n## Citation\nPlease cite as\n```\n@inproceedings{wang2019vizseq,\n  title = {VizSeq: A Visual Analysis Toolkit for Text Generation Tasks},\n  author = {Changhan Wang, Anirudh Jain, Danlu Chen, Jiatao Gu},\n  booktitle = {In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},\n  year = {2019},\n}\n```\n\n## Contact\nChanghan Wang ([changhan@fb.com](mailto:changhan@fb.com)), Jiatao Gu ([jgu@fb.com](mailto:jgu@fb.com))\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/facebookresearch/vizseq", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "vizseq", "package_url": "https://pypi.org/project/vizseq/", "platform": "", "project_url": "https://pypi.org/project/vizseq/", "project_urls": {"Homepage": "https://github.com/facebookresearch/vizseq"}, "release_url": "https://pypi.org/project/vizseq/0.1.13/", "requires_dist": ["numpy", "sacrebleu (==1.4.7)", "torch", "tqdm", "nltk", "py-rouge", "langid", "google-cloud-translate", "jinja2", "IPython", "matplotlib", "tornado", "pandas", "soundfile", "laserembeddings", "bert-score"], "requires_python": "", "summary": "Visual Analysis Toolkit for Text Generation Tasks", "version": "0.1.13", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://pypi.org/project/vizseq/\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/45f5ff2331dc8b67e974198085ca922c5c260fe0/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f76697a7365713f7374796c653d666c61742d737175617265\"></a>\n<a href=\"https://circleci.com/gh/facebookresearch/vizseq\" rel=\"nofollow\"><img alt=\"CircleCI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/835cf7b8efd9d309aaba446c2fac77d9d4806e72/68747470733a2f2f696d672e736869656c64732e696f2f636972636c6563692f6275696c642f6769746875622f66616365626f6f6b72657365617263682f76697a7365713f7374796c653d666c61742d737175617265\"></a>\n<img alt=\"PyPI - License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fa4970156eefd1da13a9307962b685114f4b2366/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f76697a7365713f7374796c653d666c61742d737175617265\">\n<img alt=\"PyPI - Python Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2b16240969946ff0abbcba013e21021770880bec/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f76697a7365713f7374796c653d666c61742d737175617265\"></p>\n<h1><img alt=\"VizSeq\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/388655114bc901a8aeb49270f41e805988057feb/6c6f676f2e706e67\" width=\"160\"></h1>\n<p>VizSeq is a Python toolkit for visual analysis on text generation tasks like machine translation, summarization,\nimage captioning, speech translation and video description. It takes multi-modal sources,\ntext references as well as text predictions as inputs, and analyzes them visually\nin <a href=\"https://facebookresearch.github.io/vizseq/docs/getting_started/ipynb_example\" rel=\"nofollow\">Jupyter Notebook</a> or a\nbuilt-in <a href=\"https://facebookresearch.github.io/vizseq/docs/getting_started/web_app_example\" rel=\"nofollow\">Web App</a>\n(the former has <a href=\"https://facebookresearch.github.io/vizseq/docs/getting_started/fairseq_example\" rel=\"nofollow\">Fairseq integration</a>).\nVizSeq also provides a collection of <a href=\"https://facebookresearch.github.io/vizseq/docs/features/metrics\" rel=\"nofollow\">multi-process scorers</a> as\na normal Python package.</p>\n<p><a href=\"https://arxiv.org/pdf/1909.05424.pdf\" rel=\"nofollow\">[Paper]</a>\n<a href=\"https://facebookresearch.github.io/vizseq\" rel=\"nofollow\">[Documentation]</a>\n<a href=\"https://ai.facebook.com/blog/vizseq-a-visual-analysis-toolkit-for-accelerating-text-generation-research\" rel=\"nofollow\">[Blog]</a></p>\n<p align=\"center\">\n<img alt=\"VizSeq Overview\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d063c9494002fb43c720762d04d5dcac7df66f23/6f766572766965772e706e67\" width=\"480\">\n<img alt=\"VizSeq Teaser\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3e0b4b03578254e3640260d973e27ba524f9284a/7465617365722e676966\" width=\"480\">\n</p>\n<h3>Task Coverage</h3>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Source</th>\n<th align=\"left\">Example Tasks</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">Text</td>\n<td align=\"left\">Machine translation, text summarization, dialog generation, grammatical error correction, open-domain question answering</td>\n</tr>\n<tr>\n<td align=\"left\">Image</td>\n<td align=\"left\">Image captioning, image question answering, optical character recognition</td>\n</tr>\n<tr>\n<td align=\"left\">Audio</td>\n<td align=\"left\">Speech recognition, speech translation</td>\n</tr>\n<tr>\n<td align=\"left\">Video</td>\n<td align=\"left\">Video description</td>\n</tr>\n<tr>\n<td align=\"left\">Multimodal</td>\n<td align=\"left\">Multimodal machine translation</td>\n</tr></tbody></table>\n<h3>Metric Coverage</h3>\n<p><strong>Accelerated with multi-processing/multi-threading.</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Type</th>\n<th align=\"left\">Metrics</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">N-gram-based</td>\n<td align=\"left\">BLEU (<a href=\"https://www.aclweb.org/anthology/P02-1040\" rel=\"nofollow\">Papineni et al., 2002</a>), NIST (<a href=\"http://www.mt-archive.info/HLT-2002-Doddington.pdf\" rel=\"nofollow\">Doddington, 2002</a>), METEOR (<a href=\"https://www.aclweb.org/anthology/W05-0909\" rel=\"nofollow\">Banerjee et al., 2005</a>), TER (<a href=\"http://mt-archive.info/AMTA-2006-Snover.pdf\" rel=\"nofollow\">Snover et al., 2006</a>), RIBES (<a href=\"https://www.aclweb.org/anthology/D10-1092\" rel=\"nofollow\">Isozaki et al., 2010</a>), chrF (<a href=\"https://www.aclweb.org/anthology/W15-3049\" rel=\"nofollow\">Popovi\u0107 et al., 2015</a>), GLEU (<a href=\"https://arxiv.org/pdf/1609.08144.pdf\" rel=\"nofollow\">Wu et al., 2016</a>), ROUGE (<a href=\"https://www.aclweb.org/anthology/W04-1013\" rel=\"nofollow\">Lin, 2004</a>), CIDEr (<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf\" rel=\"nofollow\">Vedantam et al., 2015</a>), WER</td>\n</tr>\n<tr>\n<td align=\"left\">Embedding-based</td>\n<td align=\"left\">LASER (<a href=\"https://arxiv.org/pdf/1812.10464.pdf\" rel=\"nofollow\">Artetxe and Schwenk, 2018</a>), BERTScore (<a href=\"https://arxiv.org/pdf/1904.09675.pdf\" rel=\"nofollow\">Zhang et al., 2019</a>)</td>\n</tr></tbody></table>\n<h2>Getting Started</h2>\n<h3>Installation</h3>\n<p>VizSeq requires <strong>Python 3.6+</strong> and currently runs on <strong>Unix/Linux</strong> and <strong>macOS/OS X</strong>. It will support <strong>Windows</strong> as well in the future.</p>\n<p>You can install VizSeq from PyPI repository:</p>\n<pre>$ pip install vizseq\n</pre>\n<p>Or install it from source:</p>\n<pre>$ git clone https://github.com/facebookresearch/vizseq\n$ <span class=\"nb\">cd</span> vizseq\n$ pip install -e .\n</pre>\n<h3><a href=\"https://facebookresearch.github.io/vizseq\" rel=\"nofollow\">Documentation</a></h3>\n<h3>Jupyter Notebook Examples</h3>\n<ul>\n<li><a href=\"https://facebookresearch.github.io/vizseq/docs/getting_started/ipynb_example\" rel=\"nofollow\">Basic example</a></li>\n<li><a href=\"examples/multimodal_machine_translation.ipynb\" rel=\"nofollow\">Multimodal Machine Translation</a></li>\n<li><a href=\"examples/multilingual_machine_translation.ipynb\" rel=\"nofollow\">Multilingual Machine Translation</a></li>\n<li><a href=\"examples/speech_translation.ipynb\" rel=\"nofollow\">Speech Translation</a></li>\n</ul>\n<h3><a href=\"https://facebookresearch.github.io/vizseq/docs/getting_started/fairseq_example\" rel=\"nofollow\">Fairseq integration</a></h3>\n<h3><a href=\"https://facebookresearch.github.io/vizseq/docs/getting_started/web_app_example\" rel=\"nofollow\">Web App Example</a></h3>\n<p>Download example data:</p>\n<pre>$ git clone https://github.com/facebookresearch/vizseq\n$ <span class=\"nb\">cd</span> vizseq\n$ bash get_example_data.sh\n</pre>\n<p>Launch the web server:</p>\n<pre>$ python -m vizseq.server --port <span class=\"m\">9001</span> --data-root ./examples/data\n</pre>\n<p>And then, navigate to the following URL in your web browser:</p>\n<pre>http://localhost:9001\n</pre>\n<h2>License</h2>\n<p>VizSeq is licensed under <a href=\"https://github.com/facebookresearch/vizseq/blob/master/LICENSE\" rel=\"nofollow\">MIT</a>. See the <a href=\"https://github.com/facebookresearch/vizseq/blob/master/LICENSE\" rel=\"nofollow\">LICENSE</a> file for details.</p>\n<h2>Citation</h2>\n<p>Please cite as</p>\n<pre><code>@inproceedings{wang2019vizseq,\n  title = {VizSeq: A Visual Analysis Toolkit for Text Generation Tasks},\n  author = {Changhan Wang, Anirudh Jain, Danlu Chen, Jiatao Gu},\n  booktitle = {In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},\n  year = {2019},\n}\n</code></pre>\n<h2>Contact</h2>\n<p>Changhan Wang (<a href=\"mailto:changhan@fb.com\">changhan@fb.com</a>), Jiatao Gu (<a href=\"mailto:jgu@fb.com\">jgu@fb.com</a>)</p>\n\n          </div>"}, "last_serial": 7133940, "releases": {"0.1.10": [{"comment_text": "", "digests": {"md5": "14b2457605a53b508400152901ff0bf7", "sha256": "44ea1990a4a0880248bf32371f6a92438f785be9e4ad2b69b8a0761bb1ccec54"}, "downloads": -1, "filename": "vizseq-0.1.10-py3-none-any.whl", "has_sig": false, "md5_digest": "14b2457605a53b508400152901ff0bf7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 80103, "upload_time": "2019-11-12T11:01:28", "upload_time_iso_8601": "2019-11-12T11:01:28.084501Z", "url": "https://files.pythonhosted.org/packages/86/18/e68c76fde1e2c04c3d7bf091026bdbed28c6f50f33b258767993440c9973/vizseq-0.1.10-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "07d2c2dfaedfb1050f993ca151cc20a5", "sha256": "2815a2c67f2d79356295b04b9964b99b827c26c6c82c9ce3e311184f979c6156"}, "downloads": -1, "filename": "vizseq-0.1.10.tar.gz", "has_sig": false, "md5_digest": "07d2c2dfaedfb1050f993ca151cc20a5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 43907, "upload_time": "2019-11-12T11:01:29", "upload_time_iso_8601": "2019-11-12T11:01:29.491370Z", "url": "https://files.pythonhosted.org/packages/a9/c3/1d9f724edd78b98416616917c285a4a03b2955be9162f6e52cdc28e15339/vizseq-0.1.10.tar.gz", "yanked": false}], "0.1.11": [{"comment_text": "", "digests": {"md5": "6a0ee43bb9c230043fa44e4b03829497", "sha256": "310760fb475e0daa86d5dedc312b47f28b3329838abcb16833ee97774672d466"}, "downloads": -1, "filename": "vizseq-0.1.11-py3-none-any.whl", "has_sig": false, "md5_digest": "6a0ee43bb9c230043fa44e4b03829497", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 81089, "upload_time": "2019-12-08T10:35:26", "upload_time_iso_8601": "2019-12-08T10:35:26.596012Z", "url": "https://files.pythonhosted.org/packages/55/b7/77bd36173a5cba5ff6a2834115db2fc76f538079ce7e04a8a4bc7adba188/vizseq-0.1.11-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2ddb27a7977057596eb902add133b6ec", "sha256": "49451cdb81f91ea04df1b59e3d1e5f9ec2eea86e679e9507339446722da02381"}, "downloads": -1, "filename": "vizseq-0.1.11.tar.gz", "has_sig": false, "md5_digest": "2ddb27a7977057596eb902add133b6ec", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 44495, "upload_time": "2019-12-08T10:35:28", "upload_time_iso_8601": "2019-12-08T10:35:28.229680Z", "url": "https://files.pythonhosted.org/packages/fa/20/076a12fa5e903d2f665b2712b42ff9ca9274a2a8095446f537a8c8df2b62/vizseq-0.1.11.tar.gz", "yanked": false}], "0.1.12": [{"comment_text": "", "digests": {"md5": "77a6d732c661c4d392ad7c9877db0f45", "sha256": "c05040fbc9cfce4bdde7b2e6f28aefad7e383390dec84af475ba1fbb1926dc21"}, "downloads": -1, "filename": "vizseq-0.1.12-py3-none-any.whl", "has_sig": false, "md5_digest": "77a6d732c661c4d392ad7c9877db0f45", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 81430, "upload_time": "2020-04-27T01:37:23", "upload_time_iso_8601": "2020-04-27T01:37:23.063340Z", "url": "https://files.pythonhosted.org/packages/cf/4a/8389b315a5744fabfd3211e968f2d08b6f7cf8e112c898da6d9f5c5b5cb7/vizseq-0.1.12-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "919edefff77365d85f3a1f0828c92d8e", "sha256": "7675e55d6a0dcf621b9fa39599a7f765d7cf88dd4855a04f7785fada6897ba02"}, "downloads": -1, "filename": "vizseq-0.1.12.tar.gz", "has_sig": false, "md5_digest": "919edefff77365d85f3a1f0828c92d8e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 45057, "upload_time": "2020-04-27T01:37:24", "upload_time_iso_8601": "2020-04-27T01:37:24.548826Z", "url": "https://files.pythonhosted.org/packages/fa/72/9479ba3057284f3c8a7854b90ccc281dfbf0cf22b652c948aecf29a514ec/vizseq-0.1.12.tar.gz", "yanked": false}], "0.1.13": [{"comment_text": "", "digests": {"md5": "31f2d399735411dc48d4ed86cb0a5224", "sha256": "99507c43acb67f148dccb1be835d18a5c0fdb0e4c629b3417a09e42813965ef0"}, "downloads": -1, "filename": "vizseq-0.1.13-py3-none-any.whl", "has_sig": false, "md5_digest": "31f2d399735411dc48d4ed86cb0a5224", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 81437, "upload_time": "2020-04-30T00:28:03", "upload_time_iso_8601": "2020-04-30T00:28:03.111845Z", "url": "https://files.pythonhosted.org/packages/f2/2c/e3a9cb1822b83761710b51d5acc2338d5aa35092599bcdea2f8fa9f5708d/vizseq-0.1.13-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "69784ba6c4e94d3afcc2ae93a8bb2edf", "sha256": "a2d19f4a74216d4ed41e3caf1c250e2fb83862568a3f887f3d6b2280b95e4e83"}, "downloads": -1, "filename": "vizseq-0.1.13.tar.gz", "has_sig": false, "md5_digest": "69784ba6c4e94d3afcc2ae93a8bb2edf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 45076, "upload_time": "2020-04-30T00:28:04", "upload_time_iso_8601": "2020-04-30T00:28:04.255332Z", "url": "https://files.pythonhosted.org/packages/5d/a7/5bb08678d27cfb410c63beb92fa40029095f67ae32a199d222aa9fbc1040/vizseq-0.1.13.tar.gz", "yanked": false}], "0.1.9": [{"comment_text": "", "digests": {"md5": "5455d8f574fff4d7fe31b0ca09291abf", "sha256": "aa6971504010be7ebeadfec785e2d5f6c28351da87e440df2f02459f0f6a70f5"}, "downloads": -1, "filename": "vizseq-0.1.9-py3-none-any.whl", "has_sig": false, "md5_digest": "5455d8f574fff4d7fe31b0ca09291abf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 78948, "upload_time": "2019-11-07T04:16:45", "upload_time_iso_8601": "2019-11-07T04:16:45.833760Z", "url": "https://files.pythonhosted.org/packages/80/af/3d5992e76d32904a63b2be63f71fe1d3092019636628b1803836bbf70490/vizseq-0.1.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4873fa00ff5eb2fc468083364691e79a", "sha256": "8d3a241145b2462362c9905434cf0291a70f2b5170698804788ffb6a387a2dc7"}, "downloads": -1, "filename": "vizseq-0.1.9.tar.gz", "has_sig": false, "md5_digest": "4873fa00ff5eb2fc468083364691e79a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 43513, "upload_time": "2019-11-07T04:16:47", "upload_time_iso_8601": "2019-11-07T04:16:47.633109Z", "url": "https://files.pythonhosted.org/packages/69/ff/45bdf1bc13ed6145f92ebd2387c0169699154abe387e193198114e92b344/vizseq-0.1.9.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "31f2d399735411dc48d4ed86cb0a5224", "sha256": "99507c43acb67f148dccb1be835d18a5c0fdb0e4c629b3417a09e42813965ef0"}, "downloads": -1, "filename": "vizseq-0.1.13-py3-none-any.whl", "has_sig": false, "md5_digest": "31f2d399735411dc48d4ed86cb0a5224", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 81437, "upload_time": "2020-04-30T00:28:03", "upload_time_iso_8601": "2020-04-30T00:28:03.111845Z", "url": "https://files.pythonhosted.org/packages/f2/2c/e3a9cb1822b83761710b51d5acc2338d5aa35092599bcdea2f8fa9f5708d/vizseq-0.1.13-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "69784ba6c4e94d3afcc2ae93a8bb2edf", "sha256": "a2d19f4a74216d4ed41e3caf1c250e2fb83862568a3f887f3d6b2280b95e4e83"}, "downloads": -1, "filename": "vizseq-0.1.13.tar.gz", "has_sig": false, "md5_digest": "69784ba6c4e94d3afcc2ae93a8bb2edf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 45076, "upload_time": "2020-04-30T00:28:04", "upload_time_iso_8601": "2020-04-30T00:28:04.255332Z", "url": "https://files.pythonhosted.org/packages/5d/a7/5bb08678d27cfb410c63beb92fa40029095f67ae32a199d222aa9fbc1040/vizseq-0.1.13.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:35:03 2020"}