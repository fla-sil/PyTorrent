{"info": {"author": "Adam J. Plowman", "author_email": "adam.plowman@manchester.ac.uk", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Science/Research", "Operating System :: POSIX", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering"], "description": "# hpcflow\n\n## Installation\n\n### Package installation\n\n#### Install with `pip`\n\n`hpcflow` is a Python package that can be installed from PyPI by typing:\n\n`pip install --user hpcflow`\n\n### Set up\n\nSome console entry points (scripts/executables) will be added to your system as part of the package installation. These are:\n\n1. `hpcflow`, and all of its sub-commands (found by typing `hpcflow --help` at the command line)\n2. `hfmake`, which is an alias for `hpcflow make`.\n3. `hfsub`, which is an alias for `hpcflow submit`.\n4. `hfstat`, which is an alias for `hpcflow stat`.\n\nGlobal settings are stored in a YAML file named `_config.yml`, which is stored in the `hpcflow` data directory. If it does not exist, the data directory is generated whenever `hpcflow` is run. By default, the data directory is placed under the user's home directory, as: `~/.hpcflow`, but this can be customised using the environment variable: `HPCFLOW_DATA_DIR`.\n\nInheritable YAML profile files can be added to the data directory under a sub-directory named `profiles`, i.e. at `~/.hpcflow/profiles/` if the data directory is located in the default place.\n\n## Workflow parametrisation\n\nThere are four equivalent methods to generate a new `Workflow`, all of which are accessible via the API, and three of which are accessible via the CLI:\n\n1. YAML profile files (API and CLI) -- *recommended*\n2. JSON file (API and CLI)\n3. JSON string (API and CLI)\n4. Python `dict` (API only)\n\nIn general, instantiation of a `Workflow` requires two parts: a list of command groups, and a list of variable definitions that are referenced within the command groups. Each command group can have the following keys. More details can be found in the docstring of the `CommandGroup` constructor (`hpcflow.models.CommandGroup.__init__`).\n\n|     Key      | Required |                                      Description                                       | \n| ------------ | -------- | -------------------------------------------------------------------------------------- |\n| `commands`   | \u2705        | List of commands to execute within one jobscript.                                      |\n| `exec_order` | -        | Execution order of this command group.                                                 |\n| `sub_order`  | -        | Indexes a command group that has sibling command groups with the same execution order. |\n| `options`    | -        | Scheduler options to be passed directly to the jobscript.                                        |\n| `directory` | - | The working directory for this command group. |\n| `modules` | - | List of modules to load. |\n| `job_array` | - | Whether this command group is submitted as a job array. |\n| `profile_name` | - | If this command group was submitted as part of a job profile file, this is the `profile_name` of that job profile. |\n| `profile_order` | - | If this command group was submitted as part of a job profile file, this is the `profile_order` of that job profile. |\n\n### Workflow channels\n\nIf a command group shares its execution order index (`exec_order`) with other command groups, this set of command groups must be distinguished using the `sub_order` key. We use the term \"channel\" to denote a distinct `sub_order` index. So, if, for a given execution order, there are two command groups (with `sub_order`s of `0` and `1`), we say the Workflow has two channels at that execution order. The maximum number of channels in the Workflow is the number of channels for the initial command groups (i.e. with `exec_order=0`). Channels may merge together in subsequent command groups, but can never split apart.\n\nTODO: rename `sub_order` key to `channel`? Is there any value in maintaining these two terms?\n\n### Generating a `Workflow` with YAML profile files (recommended)\n\nUsing profile files is a convenient way to use `hpcflow` from the command line, since you can set up a hierarchy of common, reusable profile files that are inheritable. For instance, in your working directory (where, for example, your simulation input files reside), you may have two profiles: `run.yml` and `process.yml`. Perhaps the software you need to use to run the simulations (say, `DAMASK`) is also required for the processing of results. In this case you could add to your `hpcflow` data directory a common profile, say `damask.yml`, which includes things like loading the correct modules for DAMASK. Then in both of the profiles in your working directory (`run.yml` and `process.yml`), you can import/inherit from the common `DAMASK` profile by adding the YAML line: `inherit: damask`.\n\nThere are two additional settings that need to be specified when using YAML profile files instead of passing the workflow `dict` directly to the `Workflow` constructor. These are the `profile_name` and the `profile_order` parameters. These two settings can optionally be encoded in the file names of the profile files themselves. The format of profile file names is customisable, using the `profile_filename_fmt` key in the configuration file (`_config.yml`). When `hpcflow` searches for profiles in the working directory, the file names must match this format. All profiles must follow this format, including inheritable profiles that are contained in the `hpcflow` data directory. If either of `profile_name` or `profile_order` are note encoded in the profile file names, then they must be specified within the profile itself.\n\n#### Specifying task ranges\n\nUsing the command-line interface, the command `hpcflow submit` (or `hfsub`) has an option `-t` (or `--task-ranges`), which allows us to specify which tasks should be submitted. If the `-t` option is omitted, then all tasks will be submitted. If the `-t` option is specified, then it must be a list of task ranges, where each task range has the format `n[-m[:s]]`, where `n` is the first task index to submit, `m` (optional) is the last, and `s` (optional) is the step size. The number of task ranges specified must be equal to the number of channels in the Workflow.\n\nIf multiple channels merge into one command group, the channel of the resulting command group will be the lowest of the channels of the parent command groups.\n\n#### Submit-time versus run-time\n\nThere are two times of importance in the life cycle of a Workflow command group. To enable automation of the Workflow, all command groups are generally submitted at the same time, with holding rules that prevent their execution until (part of) the parent command group has completed.\n\n#### Profile settings\n\n|      Name       | Required |                                                    Description                                                     | Filename encodable? |\n| --------------- | -------- | ------------------------------------------------------------------------------------------------------------------ | ------------------- |\n| `profile_name`  | \ufe0f\ufe0f\u2705    | A short name for the profile. For instance, `run` or `process`.                                                    | \u2705                  |\n| `profile_order` | \u2705       | The relative execution order of the profile.                                                                       | \u2705                  |\n| `options`       | -        | Common scheduler options to be passed directly to the jobscripts for each command group belonging to this profile. | -                   |\n| `directory`     | -        | The working directory for all command groups belonging to this profile.                                            | -                   |\n| `variable_definitions` | - | Definitions for variables that appear in the command group commands. | - |\n| `variable_scope` | - | Variable scope within the variable lookup file for this job profile  | - |\n| `modules` | - | List of modules to load. |\n| `job_array` | - | Whether this command group is submitted as a job array. |\n\nNote that the `options`, `directory`, `modules` and `job_array` keys may be specified at the *profile* level in addition to the *command group* level. This is a useful convenience. If these keys are also specified within a command group, the command group keys take precedence.\n\nThe code associated with generating `Workflow`s from YAML profile files is mostly found in the `hpcflow.profiles.py` module.\n\n### Generating a `Workflow` with a JSON file, JSON string or from a Python `dict`\n\nSince JSON objects closely match the structure of Python `dict`s, these cases are all similar.\n\n## Command-line interface\n\nHere is a list of `hpcflow` (sub-)commands. Type `<command> --help` to show the options/arguments for a given sub-command.\n\n|       Command       | Alias | Implemented |                                               Description                                                |\n| ------------------- | ----- | ----------- | -------------------------------------------------------------------------------------------------------- |\n| `hcpflow --help`    | -     | \u2705          | Show the command-line help.                                                                              |\n| `hpcflow --version` | -     | \ufe0f\ufe0f\ufe0f\ufe0f\ufe0f\ufe0f\u2705    | Show the version of `hpcflow`.                                                                           |\n| `hpcflow --info`    | -     | \u274c           | Show information about the current `hpcflow` installation, including where the data directory is located. |\n| `hpcflow --config`  | -     | \u274c           | Show the contents and location of the `hpcflow` global configuration file.                               |\n| `hpcflow make`      |`hfmake`| \u2705           | Generate a Workflow. | \n| `hpcflow submit`    |`hfsub` | \u2705           | Generate a Workflow if it doesn't exist and then submit (write jobscripts and execute) all command groups in the current working directory. | \n|`hpcflow install-example` | - | \u274c           | Install an example set of profiles from the `examples` directory (files with the same name will be overwritten). |\n| `hpcflow add-inputs` | - | \u274c           | Add an example set of input files to be used with the example profile the `examples` directory. This involves merging `_variable_lookup.yml` and `_archive_locations.yml` from the example into the user's profile directory. | \n| `hpcflow write-cmd` | - | \u2705           | Write the command file for a given jobscript. This script is invoked within jobscripts at execution-time and is not expected to be invoked by the user. The `write-cmd` process involves opening the JSON representation of the profile set and resolving variables for the set of commands within a given command group. |\n| `hpcflow show-stats` | - | \u274c           | Show statistics like CPU walltime for the profile set in the current directory. |\n| `hpcflow clean` | - | \u2705           | Remove all `hpcflow`-generated files from the current directory (use confirmation?). |\n| `hpcflow stat` | `hfstat` | \u274c           | Show status of running tasks and how many completed tasks within this directory. |\n| `hpcflow kill` | `hfkill` | \u274c           | Kill one or more jobscripts associated with a workflow. |\n| `hpcflow archive` | - | \u2705           | Archive the working directory of a given command group. |\n\n### Commands that interact with the local database\n\n- Of the above commands, the following interact with the local database:\n    - `hpcflow make` (or `hfmake`)\n    - `hpcflow submit` (or `hfsub`)\n    - `hpcflow write-cmd`\n    - `hpcflow show-stats`\n    - `hpcflow stat` (or `hfstat`)\n- Invoking any of these commands should therefore set up the relevant database connection.\n- Only `hpcflow make` and `hpcflow submit` may invoke the `create_all(engine)` method, all other commands should fail if no database exists.\n\n## Other notes:\n\n- If using Dropbox archiving, make sure, if necessary, a proxy is correctly configured to allow the machine to communicate with the outside world.\n- If using Windows, Windows must be at least version 1703, and switched to \"Developer mode\" (reason is creating a symlink; this could be disabled, but we also need developer mode for WSL I think.)\n\n## Database schema\n\nHere is a [link](https://app.sqldbm.com/MySQL/Share/zcKfw4XqIlAxd5gBe-VZtEGFrngIE8md_DYjF4jNYw0) to view the local SQLite database schema using the `sqldbm.com` web app.\n\n## Glossary\n\n**PyPI**: Python package index --- a public repository for Python packages and the default source location when packages are installed with `pip install <package_name>`.\n\n**API**: Application programming interface --- an interface that allows other Python packages to conveniently interact with this package (`hpcflow`).\n\n**CLI**: Command-line interface --- the interface that allows us to interact with `hpcflow` from the command line (i.e. shell).\n\n**YAML**: YAML Ain't Markup Language (pronounced to rhyme with \"camel\") --- a human-readable data-serialisation language, commonly used for configuration files. It is a superset of JSON.\n\n**JSON**: JavaScript Object Notation (pronounced like the male name \"Jason\") --- a human-readable data-serialisation language.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "hpcflow", "package_url": "https://pypi.org/project/hpcflow/", "platform": "", "project_url": "https://pypi.org/project/hpcflow/", "project_urls": null, "release_url": "https://pypi.org/project/hpcflow/0.1.5/", "requires_dist": ["click (>7.0)", "pyyaml", "sqlalchemy (>1.3.2)", "dropbox", "beautifultable"], "requires_python": "", "summary": "Generate and submit jobscripts for an automated simulate, process, archive workflow on high performance computing (HPC) systems.", "version": "0.1.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>hpcflow</h1>\n<h2>Installation</h2>\n<h3>Package installation</h3>\n<h4>Install with <code>pip</code></h4>\n<p><code>hpcflow</code> is a Python package that can be installed from PyPI by typing:</p>\n<p><code>pip install --user hpcflow</code></p>\n<h3>Set up</h3>\n<p>Some console entry points (scripts/executables) will be added to your system as part of the package installation. These are:</p>\n<ol>\n<li><code>hpcflow</code>, and all of its sub-commands (found by typing <code>hpcflow --help</code> at the command line)</li>\n<li><code>hfmake</code>, which is an alias for <code>hpcflow make</code>.</li>\n<li><code>hfsub</code>, which is an alias for <code>hpcflow submit</code>.</li>\n<li><code>hfstat</code>, which is an alias for <code>hpcflow stat</code>.</li>\n</ol>\n<p>Global settings are stored in a YAML file named <code>_config.yml</code>, which is stored in the <code>hpcflow</code> data directory. If it does not exist, the data directory is generated whenever <code>hpcflow</code> is run. By default, the data directory is placed under the user's home directory, as: <code>~/.hpcflow</code>, but this can be customised using the environment variable: <code>HPCFLOW_DATA_DIR</code>.</p>\n<p>Inheritable YAML profile files can be added to the data directory under a sub-directory named <code>profiles</code>, i.e. at <code>~/.hpcflow/profiles/</code> if the data directory is located in the default place.</p>\n<h2>Workflow parametrisation</h2>\n<p>There are four equivalent methods to generate a new <code>Workflow</code>, all of which are accessible via the API, and three of which are accessible via the CLI:</p>\n<ol>\n<li>YAML profile files (API and CLI) -- <em>recommended</em></li>\n<li>JSON file (API and CLI)</li>\n<li>JSON string (API and CLI)</li>\n<li>Python <code>dict</code> (API only)</li>\n</ol>\n<p>In general, instantiation of a <code>Workflow</code> requires two parts: a list of command groups, and a list of variable definitions that are referenced within the command groups. Each command group can have the following keys. More details can be found in the docstring of the <code>CommandGroup</code> constructor (<code>hpcflow.models.CommandGroup.__init__</code>).</p>\n<table>\n<thead>\n<tr>\n<th>Key</th>\n<th>Required</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>commands</code></td>\n<td>\u2705</td>\n<td>List of commands to execute within one jobscript.</td>\n</tr>\n<tr>\n<td><code>exec_order</code></td>\n<td>-</td>\n<td>Execution order of this command group.</td>\n</tr>\n<tr>\n<td><code>sub_order</code></td>\n<td>-</td>\n<td>Indexes a command group that has sibling command groups with the same execution order.</td>\n</tr>\n<tr>\n<td><code>options</code></td>\n<td>-</td>\n<td>Scheduler options to be passed directly to the jobscript.</td>\n</tr>\n<tr>\n<td><code>directory</code></td>\n<td>-</td>\n<td>The working directory for this command group.</td>\n</tr>\n<tr>\n<td><code>modules</code></td>\n<td>-</td>\n<td>List of modules to load.</td>\n</tr>\n<tr>\n<td><code>job_array</code></td>\n<td>-</td>\n<td>Whether this command group is submitted as a job array.</td>\n</tr>\n<tr>\n<td><code>profile_name</code></td>\n<td>-</td>\n<td>If this command group was submitted as part of a job profile file, this is the <code>profile_name</code> of that job profile.</td>\n</tr>\n<tr>\n<td><code>profile_order</code></td>\n<td>-</td>\n<td>If this command group was submitted as part of a job profile file, this is the <code>profile_order</code> of that job profile.</td>\n</tr></tbody></table>\n<h3>Workflow channels</h3>\n<p>If a command group shares its execution order index (<code>exec_order</code>) with other command groups, this set of command groups must be distinguished using the <code>sub_order</code> key. We use the term \"channel\" to denote a distinct <code>sub_order</code> index. So, if, for a given execution order, there are two command groups (with <code>sub_order</code>s of <code>0</code> and <code>1</code>), we say the Workflow has two channels at that execution order. The maximum number of channels in the Workflow is the number of channels for the initial command groups (i.e. with <code>exec_order=0</code>). Channels may merge together in subsequent command groups, but can never split apart.</p>\n<p>TODO: rename <code>sub_order</code> key to <code>channel</code>? Is there any value in maintaining these two terms?</p>\n<h3>Generating a <code>Workflow</code> with YAML profile files (recommended)</h3>\n<p>Using profile files is a convenient way to use <code>hpcflow</code> from the command line, since you can set up a hierarchy of common, reusable profile files that are inheritable. For instance, in your working directory (where, for example, your simulation input files reside), you may have two profiles: <code>run.yml</code> and <code>process.yml</code>. Perhaps the software you need to use to run the simulations (say, <code>DAMASK</code>) is also required for the processing of results. In this case you could add to your <code>hpcflow</code> data directory a common profile, say <code>damask.yml</code>, which includes things like loading the correct modules for DAMASK. Then in both of the profiles in your working directory (<code>run.yml</code> and <code>process.yml</code>), you can import/inherit from the common <code>DAMASK</code> profile by adding the YAML line: <code>inherit: damask</code>.</p>\n<p>There are two additional settings that need to be specified when using YAML profile files instead of passing the workflow <code>dict</code> directly to the <code>Workflow</code> constructor. These are the <code>profile_name</code> and the <code>profile_order</code> parameters. These two settings can optionally be encoded in the file names of the profile files themselves. The format of profile file names is customisable, using the <code>profile_filename_fmt</code> key in the configuration file (<code>_config.yml</code>). When <code>hpcflow</code> searches for profiles in the working directory, the file names must match this format. All profiles must follow this format, including inheritable profiles that are contained in the <code>hpcflow</code> data directory. If either of <code>profile_name</code> or <code>profile_order</code> are note encoded in the profile file names, then they must be specified within the profile itself.</p>\n<h4>Specifying task ranges</h4>\n<p>Using the command-line interface, the command <code>hpcflow submit</code> (or <code>hfsub</code>) has an option <code>-t</code> (or <code>--task-ranges</code>), which allows us to specify which tasks should be submitted. If the <code>-t</code> option is omitted, then all tasks will be submitted. If the <code>-t</code> option is specified, then it must be a list of task ranges, where each task range has the format <code>n[-m[:s]]</code>, where <code>n</code> is the first task index to submit, <code>m</code> (optional) is the last, and <code>s</code> (optional) is the step size. The number of task ranges specified must be equal to the number of channels in the Workflow.</p>\n<p>If multiple channels merge into one command group, the channel of the resulting command group will be the lowest of the channels of the parent command groups.</p>\n<h4>Submit-time versus run-time</h4>\n<p>There are two times of importance in the life cycle of a Workflow command group. To enable automation of the Workflow, all command groups are generally submitted at the same time, with holding rules that prevent their execution until (part of) the parent command group has completed.</p>\n<h4>Profile settings</h4>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Required</th>\n<th>Description</th>\n<th>Filename encodable?</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>profile_name</code></td>\n<td>\ufe0f\ufe0f\u2705</td>\n<td>A short name for the profile. For instance, <code>run</code> or <code>process</code>.</td>\n<td>\u2705</td>\n</tr>\n<tr>\n<td><code>profile_order</code></td>\n<td>\u2705</td>\n<td>The relative execution order of the profile.</td>\n<td>\u2705</td>\n</tr>\n<tr>\n<td><code>options</code></td>\n<td>-</td>\n<td>Common scheduler options to be passed directly to the jobscripts for each command group belonging to this profile.</td>\n<td>-</td>\n</tr>\n<tr>\n<td><code>directory</code></td>\n<td>-</td>\n<td>The working directory for all command groups belonging to this profile.</td>\n<td>-</td>\n</tr>\n<tr>\n<td><code>variable_definitions</code></td>\n<td>-</td>\n<td>Definitions for variables that appear in the command group commands.</td>\n<td>-</td>\n</tr>\n<tr>\n<td><code>variable_scope</code></td>\n<td>-</td>\n<td>Variable scope within the variable lookup file for this job profile</td>\n<td>-</td>\n</tr>\n<tr>\n<td><code>modules</code></td>\n<td>-</td>\n<td>List of modules to load.</td>\n<td></td>\n</tr>\n<tr>\n<td><code>job_array</code></td>\n<td>-</td>\n<td>Whether this command group is submitted as a job array.</td>\n<td></td>\n</tr></tbody></table>\n<p>Note that the <code>options</code>, <code>directory</code>, <code>modules</code> and <code>job_array</code> keys may be specified at the <em>profile</em> level in addition to the <em>command group</em> level. This is a useful convenience. If these keys are also specified within a command group, the command group keys take precedence.</p>\n<p>The code associated with generating <code>Workflow</code>s from YAML profile files is mostly found in the <code>hpcflow.profiles.py</code> module.</p>\n<h3>Generating a <code>Workflow</code> with a JSON file, JSON string or from a Python <code>dict</code></h3>\n<p>Since JSON objects closely match the structure of Python <code>dict</code>s, these cases are all similar.</p>\n<h2>Command-line interface</h2>\n<p>Here is a list of <code>hpcflow</code> (sub-)commands. Type <code>&lt;command&gt; --help</code> to show the options/arguments for a given sub-command.</p>\n<table>\n<thead>\n<tr>\n<th>Command</th>\n<th>Alias</th>\n<th>Implemented</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>hcpflow --help</code></td>\n<td>-</td>\n<td>\u2705</td>\n<td>Show the command-line help.</td>\n</tr>\n<tr>\n<td><code>hpcflow --version</code></td>\n<td>-</td>\n<td>\ufe0f\ufe0f\ufe0f\ufe0f\ufe0f\ufe0f\u2705</td>\n<td>Show the version of <code>hpcflow</code>.</td>\n</tr>\n<tr>\n<td><code>hpcflow --info</code></td>\n<td>-</td>\n<td>\u274c</td>\n<td>Show information about the current <code>hpcflow</code> installation, including where the data directory is located.</td>\n</tr>\n<tr>\n<td><code>hpcflow --config</code></td>\n<td>-</td>\n<td>\u274c</td>\n<td>Show the contents and location of the <code>hpcflow</code> global configuration file.</td>\n</tr>\n<tr>\n<td><code>hpcflow make</code></td>\n<td><code>hfmake</code></td>\n<td>\u2705</td>\n<td>Generate a Workflow.</td>\n</tr>\n<tr>\n<td><code>hpcflow submit</code></td>\n<td><code>hfsub</code></td>\n<td>\u2705</td>\n<td>Generate a Workflow if it doesn't exist and then submit (write jobscripts and execute) all command groups in the current working directory.</td>\n</tr>\n<tr>\n<td><code>hpcflow install-example</code></td>\n<td>-</td>\n<td>\u274c</td>\n<td>Install an example set of profiles from the <code>examples</code> directory (files with the same name will be overwritten).</td>\n</tr>\n<tr>\n<td><code>hpcflow add-inputs</code></td>\n<td>-</td>\n<td>\u274c</td>\n<td>Add an example set of input files to be used with the example profile the <code>examples</code> directory. This involves merging <code>_variable_lookup.yml</code> and <code>_archive_locations.yml</code> from the example into the user's profile directory.</td>\n</tr>\n<tr>\n<td><code>hpcflow write-cmd</code></td>\n<td>-</td>\n<td>\u2705</td>\n<td>Write the command file for a given jobscript. This script is invoked within jobscripts at execution-time and is not expected to be invoked by the user. The <code>write-cmd</code> process involves opening the JSON representation of the profile set and resolving variables for the set of commands within a given command group.</td>\n</tr>\n<tr>\n<td><code>hpcflow show-stats</code></td>\n<td>-</td>\n<td>\u274c</td>\n<td>Show statistics like CPU walltime for the profile set in the current directory.</td>\n</tr>\n<tr>\n<td><code>hpcflow clean</code></td>\n<td>-</td>\n<td>\u2705</td>\n<td>Remove all <code>hpcflow</code>-generated files from the current directory (use confirmation?).</td>\n</tr>\n<tr>\n<td><code>hpcflow stat</code></td>\n<td><code>hfstat</code></td>\n<td>\u274c</td>\n<td>Show status of running tasks and how many completed tasks within this directory.</td>\n</tr>\n<tr>\n<td><code>hpcflow kill</code></td>\n<td><code>hfkill</code></td>\n<td>\u274c</td>\n<td>Kill one or more jobscripts associated with a workflow.</td>\n</tr>\n<tr>\n<td><code>hpcflow archive</code></td>\n<td>-</td>\n<td>\u2705</td>\n<td>Archive the working directory of a given command group.</td>\n</tr></tbody></table>\n<h3>Commands that interact with the local database</h3>\n<ul>\n<li>Of the above commands, the following interact with the local database:\n<ul>\n<li><code>hpcflow make</code> (or <code>hfmake</code>)</li>\n<li><code>hpcflow submit</code> (or <code>hfsub</code>)</li>\n<li><code>hpcflow write-cmd</code></li>\n<li><code>hpcflow show-stats</code></li>\n<li><code>hpcflow stat</code> (or <code>hfstat</code>)</li>\n</ul>\n</li>\n<li>Invoking any of these commands should therefore set up the relevant database connection.</li>\n<li>Only <code>hpcflow make</code> and <code>hpcflow submit</code> may invoke the <code>create_all(engine)</code> method, all other commands should fail if no database exists.</li>\n</ul>\n<h2>Other notes:</h2>\n<ul>\n<li>If using Dropbox archiving, make sure, if necessary, a proxy is correctly configured to allow the machine to communicate with the outside world.</li>\n<li>If using Windows, Windows must be at least version 1703, and switched to \"Developer mode\" (reason is creating a symlink; this could be disabled, but we also need developer mode for WSL I think.)</li>\n</ul>\n<h2>Database schema</h2>\n<p>Here is a <a href=\"https://app.sqldbm.com/MySQL/Share/zcKfw4XqIlAxd5gBe-VZtEGFrngIE8md_DYjF4jNYw0\" rel=\"nofollow\">link</a> to view the local SQLite database schema using the <code>sqldbm.com</code> web app.</p>\n<h2>Glossary</h2>\n<p><strong>PyPI</strong>: Python package index --- a public repository for Python packages and the default source location when packages are installed with <code>pip install &lt;package_name&gt;</code>.</p>\n<p><strong>API</strong>: Application programming interface --- an interface that allows other Python packages to conveniently interact with this package (<code>hpcflow</code>).</p>\n<p><strong>CLI</strong>: Command-line interface --- the interface that allows us to interact with <code>hpcflow</code> from the command line (i.e. shell).</p>\n<p><strong>YAML</strong>: YAML Ain't Markup Language (pronounced to rhyme with \"camel\") --- a human-readable data-serialisation language, commonly used for configuration files. It is a superset of JSON.</p>\n<p><strong>JSON</strong>: JavaScript Object Notation (pronounced like the male name \"Jason\") --- a human-readable data-serialisation language.</p>\n\n          </div>"}, "last_serial": 7184619, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "230e510f0caddfcdc4e59b6b3c506884", "sha256": "dc8ceea5d8e678d2c318be74626ee23dbac81ed99dffc5d48fb1119114f0d33f"}, "downloads": -1, "filename": "hpcflow-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "230e510f0caddfcdc4e59b6b3c506884", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 101088, "upload_time": "2019-06-14T15:24:26", "upload_time_iso_8601": "2019-06-14T15:24:26.085899Z", "url": "https://files.pythonhosted.org/packages/6c/99/61e3deafc561d98a88e6b3261b03544c036990379ad504bcb26b64bf26fa/hpcflow-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c76c97bda02e06673c44e1627d1d0328", "sha256": "a9e7138c23dcc5dd0b6532956a5bc8e0cc2807f9732bb9ee4793df8e9d0e8949"}, "downloads": -1, "filename": "hpcflow-0.1.1.tar.gz", "has_sig": false, "md5_digest": "c76c97bda02e06673c44e1627d1d0328", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 91312, "upload_time": "2019-06-14T15:24:28", "upload_time_iso_8601": "2019-06-14T15:24:28.910862Z", "url": "https://files.pythonhosted.org/packages/ea/0a/47f41eae24328f31d3707f41e342f861f801846bab4c961438e7e44a6864/hpcflow-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "170b922f24000fd4ac594f84bc0f2564", "sha256": "1bbf9984b2db00c9a4ccaabfc832d3d9783a5194b86569968ba327c536378046"}, "downloads": -1, "filename": "hpcflow-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "170b922f24000fd4ac594f84bc0f2564", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 114761, "upload_time": "2020-05-06T22:35:10", "upload_time_iso_8601": "2020-05-06T22:35:10.239237Z", "url": "https://files.pythonhosted.org/packages/d7/eb/4f0675b4e87df319cedb18ba619f8a55584f34b54bd83076323d4cfd28b4/hpcflow-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f488d81116dfe8b6d2a516f3a5d5ded9", "sha256": "63ad3c1e0418478f8f4d084c92d9925dfdd566c7e9eddbcec21e3a80a74c4bac"}, "downloads": -1, "filename": "hpcflow-0.1.2.tar.gz", "has_sig": false, "md5_digest": "f488d81116dfe8b6d2a516f3a5d5ded9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 110149, "upload_time": "2020-05-06T22:35:12", "upload_time_iso_8601": "2020-05-06T22:35:12.452154Z", "url": "https://files.pythonhosted.org/packages/94/53/139a8b67ca83828298c3652de9c5f3e1bf2a5f047a8c906683616b414612/hpcflow-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "b5d7f3e98f6cc92b065f14ea35400637", "sha256": "b5f84cb8069225369d4edec400ce4480c2edf5844dd0dd155aaccc140854ee1c"}, "downloads": -1, "filename": "hpcflow-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "b5d7f3e98f6cc92b065f14ea35400637", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 114761, "upload_time": "2020-05-07T00:21:07", "upload_time_iso_8601": "2020-05-07T00:21:07.031536Z", "url": "https://files.pythonhosted.org/packages/fe/5c/1f5dbfafc4e6525aab9209cbf34d78ba8ed2d346cd467705e9e059625789/hpcflow-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7f156cee9b5682c1e4036dfb9bfe9237", "sha256": "7a6646841a6e928831847312aa8438650cfb2c24c97b588eef4c51d08191f979"}, "downloads": -1, "filename": "hpcflow-0.1.3.tar.gz", "has_sig": false, "md5_digest": "7f156cee9b5682c1e4036dfb9bfe9237", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 110164, "upload_time": "2020-05-07T00:21:10", "upload_time_iso_8601": "2020-05-07T00:21:10.348047Z", "url": "https://files.pythonhosted.org/packages/5e/ad/542d278eb0564f442bcdae7d517f717cd0cad6065667523c353b4b2d3b54/hpcflow-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "6c75d59889558b71755a9217159a48d7", "sha256": "e313d6907c40cbfd2d8302afa948b56dddc7db17178bce4fb455d0004e62613e"}, "downloads": -1, "filename": "hpcflow-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "6c75d59889558b71755a9217159a48d7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 118086, "upload_time": "2020-05-07T01:11:25", "upload_time_iso_8601": "2020-05-07T01:11:25.076836Z", "url": "https://files.pythonhosted.org/packages/ee/ba/347725e5d7d73b7d1f5a2acbe10b0adcd814dc5117f8bb3cf1e6abda87b3/hpcflow-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a95cd9b209d8141764367a9cc292a36b", "sha256": "4a5426cd76c592cdb8c14f6bc2956b16e2b969c0d9d6ffbc947425b2e3ec9863"}, "downloads": -1, "filename": "hpcflow-0.1.4.tar.gz", "has_sig": false, "md5_digest": "a95cd9b209d8141764367a9cc292a36b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 113504, "upload_time": "2020-05-07T01:11:26", "upload_time_iso_8601": "2020-05-07T01:11:26.386169Z", "url": "https://files.pythonhosted.org/packages/75/81/cc71f471198d6ca9283d368300bc9f9e311cdd42383ee913603047d57cf8/hpcflow-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "8e3c17410887e39eea853160685f179a", "sha256": "d3948d81ff58fc86839e0fa198a178985a2315a32dbd9b82ad347a4b134ace20"}, "downloads": -1, "filename": "hpcflow-0.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "8e3c17410887e39eea853160685f179a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 122424, "upload_time": "2020-05-07T01:17:21", "upload_time_iso_8601": "2020-05-07T01:17:21.464928Z", "url": "https://files.pythonhosted.org/packages/99/05/e930a9990428eee72f98af56c335e6e12552011ef19f54b35e9b6968a6d0/hpcflow-0.1.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "df9eae6778cc6d9c1f94009a968aff6d", "sha256": "39bbe15e0b6a4a205eb8fd09c7be372a65fa83ee5a248121534faf430c35c5e4"}, "downloads": -1, "filename": "hpcflow-0.1.5.tar.gz", "has_sig": false, "md5_digest": "df9eae6778cc6d9c1f94009a968aff6d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 117018, "upload_time": "2020-05-07T01:17:22", "upload_time_iso_8601": "2020-05-07T01:17:22.915599Z", "url": "https://files.pythonhosted.org/packages/7e/80/42d2b3fa177eb2bac94f0851748c54f83fd3fdb76e544f23da7a37a376fa/hpcflow-0.1.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "8e3c17410887e39eea853160685f179a", "sha256": "d3948d81ff58fc86839e0fa198a178985a2315a32dbd9b82ad347a4b134ace20"}, "downloads": -1, "filename": "hpcflow-0.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "8e3c17410887e39eea853160685f179a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 122424, "upload_time": "2020-05-07T01:17:21", "upload_time_iso_8601": "2020-05-07T01:17:21.464928Z", "url": "https://files.pythonhosted.org/packages/99/05/e930a9990428eee72f98af56c335e6e12552011ef19f54b35e9b6968a6d0/hpcflow-0.1.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "df9eae6778cc6d9c1f94009a968aff6d", "sha256": "39bbe15e0b6a4a205eb8fd09c7be372a65fa83ee5a248121534faf430c35c5e4"}, "downloads": -1, "filename": "hpcflow-0.1.5.tar.gz", "has_sig": false, "md5_digest": "df9eae6778cc6d9c1f94009a968aff6d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 117018, "upload_time": "2020-05-07T01:17:22", "upload_time_iso_8601": "2020-05-07T01:17:22.915599Z", "url": "https://files.pythonhosted.org/packages/7e/80/42d2b3fa177eb2bac94f0851748c54f83fd3fdb76e544f23da7a37a376fa/hpcflow-0.1.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:28 2020"}