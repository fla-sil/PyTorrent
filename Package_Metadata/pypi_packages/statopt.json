{"info": {"author": "", "author_email": "", "bugtrack_url": null, "classifiers": [], "description": "# Statistical Adaptive Stochastic Gradient Methods\n\nA package of PyTorch optimizers that can automatically schedule learning rates based on online statistical tests.\n\n* main algorithms: SALSA and SASA\n* auxiliary codes: QHM and SSLS\n\nCompanion paper: [Statistical Adaptive Stochastic Gradient Methods](https://www.microsoft.com/en-us/research/publication/statistical-adaptive-stochastic-gradient-methods) by Zhang, Lang, Liu and Xiao, 2020.\n\n## Install\n\n    pip install statopt\n\nOr from Github:\n\n    pip install git+git://github.com/microsoft/statopt.git#egg=statopt\n\n\n## Usage of SALSA and SASA\n\nHere we outline the key steps on CIFAR10.\nComplete Python code is given in [examples/cifar_example.py](examples/cifar_example.py). \n\n### Common setups\n\nFirst, choose a batch size and prepare the dataset and data loader as in [this PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html):\n\n```python\nimport torch, torchvision\n\nbatch_size = 128\ntrainset = torchvision.datasets.CIFAR10(root='../data', train=True, ...)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, ...)\n```\n\nChoose device, network model, and loss function:\n\n```python\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nnet = torchvision.models.resnet18().to(device)\nloss_func = torch.nn.CrossEntropyLoss()\n```\n\n### SALSA\nImport ```statopt```, and initialize SALSA with a small learning rate and two extra parameters:\n\n```python\nimport statopt\n\ngamma = math.sqrt(batch_size/len(trainset))             # smoothing parameter for line search\ntestfreq = min(1000, len(trainloader))                  # frequency to perform statistical test \n\noptimizer = statopt.SALSA(net.parameters(), lr=1e-3,            # any small initial learning rate \n                          momentum=0.9, weight_decay=5e-4,      # common choices for CIFAR10/100\n                          gamma=gamma, testfreq=testfreq)       # two extra parameters for SALSA\n```\n\nTraining code using SALSA\n\n```python\nfor epoch in range(100):\n    for (images, labels) in trainloader:\n        net.train()\t# always switch to train() mode\n\n        # Compute model outputs and loss function \n        images, labels = images.to(device), labels.to(device)\n        loss = loss_func(net(images), labels)\n\n        # Compute gradient with back-propagation \n        optimizer.zero_grad()\n        loss.backward()\n\n        # SALSA requires a closure function for line search\n        def eval_loss(eval_mode=True):\n            if eval_mode:\n                net.eval()\n            with torch.no_grad():\n                loss = loss_func(net(images), labels)\n            return loss\n\n        optimizer.step(closure=eval_loss)\n\n```\n\n### SASA\n\nSASA requires a good (hand-tuned) initial learning rate like most other optimizers, but do not use line search:\n\n```python\noptimizer = statopt.SASA(net.parameters(), lr=1.0,              # need a good initial learning rate \n                         momentum=0.9, weight_decay=5e-4,       # common choices for CIFAR10/100\n                         testfreq=testfreq)                     # frequency for statistical tests\n```\n\nWithin the training loop: ```optimizer.step()``` does NOT need any closure function.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/microsoft/statopt", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "statopt", "package_url": "https://pypi.org/project/statopt/", "platform": "", "project_url": "https://pypi.org/project/statopt/", "project_urls": {"Homepage": "https://github.com/microsoft/statopt"}, "release_url": "https://pypi.org/project/statopt/0.2/", "requires_dist": ["numpy", "scipy", "torch"], "requires_python": "", "summary": "Statistical adaptive stochastic optimization methods", "version": "0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Statistical Adaptive Stochastic Gradient Methods</h1>\n<p>A package of PyTorch optimizers that can automatically schedule learning rates based on online statistical tests.</p>\n<ul>\n<li>main algorithms: SALSA and SASA</li>\n<li>auxiliary codes: QHM and SSLS</li>\n</ul>\n<p>Companion paper: <a href=\"https://www.microsoft.com/en-us/research/publication/statistical-adaptive-stochastic-gradient-methods\" rel=\"nofollow\">Statistical Adaptive Stochastic Gradient Methods</a> by Zhang, Lang, Liu and Xiao, 2020.</p>\n<h2>Install</h2>\n<pre><code>pip install statopt\n</code></pre>\n<p>Or from Github:</p>\n<pre><code>pip install git+git://github.com/microsoft/statopt.git#egg=statopt\n</code></pre>\n<h2>Usage of SALSA and SASA</h2>\n<p>Here we outline the key steps on CIFAR10.\nComplete Python code is given in <a href=\"examples/cifar_example.py\" rel=\"nofollow\">examples/cifar_example.py</a>.</p>\n<h3>Common setups</h3>\n<p>First, choose a batch size and prepare the dataset and data loader as in <a href=\"https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\" rel=\"nofollow\">this PyTorch tutorial</a>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span><span class=\"o\">,</span> <span class=\"nn\">torchvision</span>\n\n<span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>\n<span class=\"n\">trainset</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">CIFAR10</span><span class=\"p\">(</span><span class=\"n\">root</span><span class=\"o\">=</span><span class=\"s1\">'../data'</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">)</span>\n<span class=\"n\">trainloader</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">trainset</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">)</span>\n</pre>\n<p>Choose device, network model, and loss function:</p>\n<pre><span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"s1\">'cuda'</span> <span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">is_available</span><span class=\"p\">()</span> <span class=\"k\">else</span> <span class=\"s1\">'cpu'</span>\n<span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">resnet18</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n<span class=\"n\">loss_func</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">CrossEntropyLoss</span><span class=\"p\">()</span>\n</pre>\n<h3>SALSA</h3>\n<p>Import <code>statopt</code>, and initialize SALSA with a small learning rate and two extra parameters:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">statopt</span>\n\n<span class=\"n\">gamma</span> <span class=\"o\">=</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"o\">/</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">trainset</span><span class=\"p\">))</span>             <span class=\"c1\"># smoothing parameter for line search</span>\n<span class=\"n\">testfreq</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">trainloader</span><span class=\"p\">))</span>                  <span class=\"c1\"># frequency to perform statistical test </span>\n\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">statopt</span><span class=\"o\">.</span><span class=\"n\">SALSA</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span>            <span class=\"c1\"># any small initial learning rate </span>\n                          <span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">5e-4</span><span class=\"p\">,</span>      <span class=\"c1\"># common choices for CIFAR10/100</span>\n                          <span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"n\">gamma</span><span class=\"p\">,</span> <span class=\"n\">testfreq</span><span class=\"o\">=</span><span class=\"n\">testfreq</span><span class=\"p\">)</span>       <span class=\"c1\"># two extra parameters for SALSA</span>\n</pre>\n<p>Training code using SALSA</p>\n<pre><span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"n\">trainloader</span><span class=\"p\">:</span>\n        <span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\t<span class=\"c1\"># always switch to train() mode</span>\n\n        <span class=\"c1\"># Compute model outputs and loss function </span>\n        <span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">images</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">),</span> <span class=\"n\">labels</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">loss_func</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">),</span> <span class=\"n\">labels</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Compute gradient with back-propagation </span>\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># SALSA requires a closure function for line search</span>\n        <span class=\"k\">def</span> <span class=\"nf\">eval_loss</span><span class=\"p\">(</span><span class=\"n\">eval_mode</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">):</span>\n            <span class=\"k\">if</span> <span class=\"n\">eval_mode</span><span class=\"p\">:</span>\n                <span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\n            <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">():</span>\n                <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">loss_func</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">),</span> <span class=\"n\">labels</span><span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"n\">loss</span>\n\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">closure</span><span class=\"o\">=</span><span class=\"n\">eval_loss</span><span class=\"p\">)</span>\n</pre>\n<h3>SASA</h3>\n<p>SASA requires a good (hand-tuned) initial learning rate like most other optimizers, but do not use line search:</p>\n<pre><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">statopt</span><span class=\"o\">.</span><span class=\"n\">SASA</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">,</span>              <span class=\"c1\"># need a good initial learning rate </span>\n                         <span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">5e-4</span><span class=\"p\">,</span>       <span class=\"c1\"># common choices for CIFAR10/100</span>\n                         <span class=\"n\">testfreq</span><span class=\"o\">=</span><span class=\"n\">testfreq</span><span class=\"p\">)</span>                     <span class=\"c1\"># frequency for statistical tests</span>\n</pre>\n<p>Within the training loop: <code>optimizer.step()</code> does NOT need any closure function.</p>\n\n          </div>"}, "last_serial": 6899764, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "6b1d9483a0936397b86cd3601ccb0216", "sha256": "4d09cb6b791738841f9935097204f4153019b8579510bdfe413a1085c25a36ff"}, "downloads": -1, "filename": "statopt-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "6b1d9483a0936397b86cd3601ccb0216", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 17578, "upload_time": "2020-02-27T21:45:58", "upload_time_iso_8601": "2020-02-27T21:45:58.334293Z", "url": "https://files.pythonhosted.org/packages/8b/57/7a4c171f3202fc42165516315b17dc22d297fce0ffb4ca7942522d69c096/statopt-0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e1d3cec46dbca0d81eeaea398834e6e1", "sha256": "9ba09e8112b065f2e2f49af3a73c44dc9ed974bf2a748525db1b3aa29ad01f8b"}, "downloads": -1, "filename": "statopt-0.1.tar.gz", "has_sig": false, "md5_digest": "e1d3cec46dbca0d81eeaea398834e6e1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12172, "upload_time": "2020-02-27T21:46:00", "upload_time_iso_8601": "2020-02-27T21:46:00.697960Z", "url": "https://files.pythonhosted.org/packages/45/c8/1ee351af87fabfa935688880e372d042b293b21ee41844563bc7606be33b/statopt-0.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "ad48f09eabd30034199af961b7698d64", "sha256": "061f96a60eeca44ce2279876fcc93ec877b053785945aaf1b27361c9766ddf57"}, "downloads": -1, "filename": "statopt-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "ad48f09eabd30034199af961b7698d64", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 17533, "upload_time": "2020-03-28T00:19:21", "upload_time_iso_8601": "2020-03-28T00:19:21.088660Z", "url": "https://files.pythonhosted.org/packages/61/a9/f0f6f45f3bd55926ca86f99e719bc137d05a374957549b49f67755b70633/statopt-0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c931f2f1af824f66b44999f188fafc3f", "sha256": "e522e36218abe879640cf125bac29371ee051dd5f75b72a04772a38b9ce53360"}, "downloads": -1, "filename": "statopt-0.2.tar.gz", "has_sig": false, "md5_digest": "c931f2f1af824f66b44999f188fafc3f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12152, "upload_time": "2020-03-28T00:19:22", "upload_time_iso_8601": "2020-03-28T00:19:22.315013Z", "url": "https://files.pythonhosted.org/packages/89/24/836a5a115edf5fb0f3a81891f93b21df081f5424f36076e4273f8c5681cb/statopt-0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "ad48f09eabd30034199af961b7698d64", "sha256": "061f96a60eeca44ce2279876fcc93ec877b053785945aaf1b27361c9766ddf57"}, "downloads": -1, "filename": "statopt-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "ad48f09eabd30034199af961b7698d64", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 17533, "upload_time": "2020-03-28T00:19:21", "upload_time_iso_8601": "2020-03-28T00:19:21.088660Z", "url": "https://files.pythonhosted.org/packages/61/a9/f0f6f45f3bd55926ca86f99e719bc137d05a374957549b49f67755b70633/statopt-0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c931f2f1af824f66b44999f188fafc3f", "sha256": "e522e36218abe879640cf125bac29371ee051dd5f75b72a04772a38b9ce53360"}, "downloads": -1, "filename": "statopt-0.2.tar.gz", "has_sig": false, "md5_digest": "c931f2f1af824f66b44999f188fafc3f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12152, "upload_time": "2020-03-28T00:19:22", "upload_time_iso_8601": "2020-03-28T00:19:22.315013Z", "url": "https://files.pythonhosted.org/packages/89/24/836a5a115edf5fb0f3a81891f93b21df081f5424f36076e4273f8c5681cb/statopt-0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:02:23 2020"}