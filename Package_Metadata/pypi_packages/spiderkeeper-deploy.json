{"info": {"author": "Sam Kleiner", "author_email": "sam@skleiner.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX", "Operating System :: Unix", "Programming Language :: Python", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy", "Topic :: Utilities"], "description": "========\nOverview\n========\n\nCLI tool to deploy Scrapy projects to SpiderKeeper\n\nInstallation\n============\n\nPyPi\n\n::\n\n    pip install spiderkeeper-deploy\n\n\nUsage\n=====\n\nConfig values can be supplied at runtime as arguments. If arguments are not supplied\nspiderkeeper-deploy will try to find scrapy.cfg in the current project. Config values\nin scrapy.cfg will be loaded from [skdeploy] section. Any value not supplied as argument\nor in scrapy.cfg can be entered in the interactive prompt.\n\nThe one caveat to using scrapy.cfg is that passwords will not be read. They must be supplied\nat run time or set in SK_PASSWORD environment variable. Don't save passwords in config files!\n\n::\n\n    spiderkeeper-deploy --help\n\n    Usage: spiderkeeper-deploy [OPTIONS]\n\n        Deploy Scrapy projects to SpiderKeeper.\n\n        Hint: you can define CLI args in scrapy.cfg file in your project.\n\n        CLI args override scrapy.cfg\n\n    Options:\n        -u, --url       Server name or ip. Default: http://localhost:8080\n        -p, --project   Project name.\n        -j, --jobs      Jobs in json format\n        --user          Default: admin\n        --password      Will use ENV SK_PASSWORD if present. Default: admin\n        --help          Show this message and exit.\n\nExample scrapy.cfg\n------------------\n\nNote: jobs format is exactly the same as the api provided by SpiderKeeper to add and update jobs.\nThis array must be indented at least 1 level. Proper JSON formatting is required so double quotes\nand no trailing comma.\n\n::\n\n    [settings]\n    default = project.settings\n\n    [deploy]\n    url = http://localhost:6800/\n    project = project\n\n    [skdeploy]\n    url = http://localhost:5000/\n    project = project\n    user = me\n    jobs = [\n            {\n                \"spider_name\": \"spider_name\",\n                \"spider_arguments\": \"arg1,arg2\",\n                \"run_type\": \"periodic\",\n                \"desc\": \"description\",\n                \"tags\": \"tag1,tag2\",\n                \"priority\": 1,\n                \"cron_minutes\": \"0\",\n                \"cron_hour\": \"*\",\n                \"cron_day_of_month\": \"*\",\n                \"cron_day_of_week\": \"*\",\n                \"cron_month\": \"*\"\n            }\n        ]\n\nDeploying\n=========\n\n- If the project does not already exist it will be created\n- Any jobs not yet in SpiderKeeper will be added\n- Jobs already in SpiderKeeper will be updated (i.e. tags, desc can be updated)\n- **Jobs not in config will be deleted**\n\nA job is defined as already existing in SpiderKeeper if spider_name,\nspider_arguments, cron_minutes, cron_hour, cron_day_of_month, cron_day_of_week,\nand cron_month all match what is in config.\n\nNote: spider_name is always required. spider_arguments can be omitted and will\ndefault to None. cron settings can be omitted and will default to \"*\"\n\nSpiderKeeper uses numbers to identify projects. This means it is possible to\nhave two projects with the same name. spiderkeeper-deploy will use the first\nproject who's name matches the project config value. If you have an existing\ndeployment with duplicates you should keep this in mind. If not you should\nnever get duplicates as long as you only use spiderkeeper-deploy. It is still\npossible to get duplicates with a manual deployment.", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/StoicPerlman/spiderkeeper-deploy", "keywords": "spiderkeeper,scrapy,devops", "license": "MIT license", "maintainer": "", "maintainer_email": "", "name": "spiderkeeper-deploy", "package_url": "https://pypi.org/project/spiderkeeper-deploy/", "platform": "", "project_url": "https://pypi.org/project/spiderkeeper-deploy/", "project_urls": {"Homepage": "https://github.com/StoicPerlman/spiderkeeper-deploy"}, "release_url": "https://pypi.org/project/spiderkeeper-deploy/0.1.3/", "requires_dist": null, "requires_python": "", "summary": "Deploy to SpiderKeeper", "version": "0.1.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>CLI tool to deploy Scrapy projects to SpiderKeeper</p>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>PyPi</p>\n<pre>pip install spiderkeeper-deploy\n</pre>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>Config values can be supplied at runtime as arguments. If arguments are not supplied\nspiderkeeper-deploy will try to find scrapy.cfg in the current project. Config values\nin scrapy.cfg will be loaded from [skdeploy] section. Any value not supplied as argument\nor in scrapy.cfg can be entered in the interactive prompt.</p>\n<p>The one caveat to using scrapy.cfg is that passwords will not be read. They must be supplied\nat run time or set in SK_PASSWORD environment variable. Don\u2019t save passwords in config files!</p>\n<pre>spiderkeeper-deploy --help\n\nUsage: spiderkeeper-deploy [OPTIONS]\n\n    Deploy Scrapy projects to SpiderKeeper.\n\n    Hint: you can define CLI args in scrapy.cfg file in your project.\n\n    CLI args override scrapy.cfg\n\nOptions:\n    -u, --url       Server name or ip. Default: http://localhost:8080\n    -p, --project   Project name.\n    -j, --jobs      Jobs in json format\n    --user          Default: admin\n    --password      Will use ENV SK_PASSWORD if present. Default: admin\n    --help          Show this message and exit.\n</pre>\n<div id=\"example-scrapy-cfg\">\n<h3>Example scrapy.cfg</h3>\n<p>Note: jobs format is exactly the same as the api provided by SpiderKeeper to add and update jobs.\nThis array must be indented at least 1 level. Proper JSON formatting is required so double quotes\nand no trailing comma.</p>\n<pre>[settings]\ndefault = project.settings\n\n[deploy]\nurl = http://localhost:6800/\nproject = project\n\n[skdeploy]\nurl = http://localhost:5000/\nproject = project\nuser = me\njobs = [\n        {\n            \"spider_name\": \"spider_name\",\n            \"spider_arguments\": \"arg1,arg2\",\n            \"run_type\": \"periodic\",\n            \"desc\": \"description\",\n            \"tags\": \"tag1,tag2\",\n            \"priority\": 1,\n            \"cron_minutes\": \"0\",\n            \"cron_hour\": \"*\",\n            \"cron_day_of_month\": \"*\",\n            \"cron_day_of_week\": \"*\",\n            \"cron_month\": \"*\"\n        }\n    ]\n</pre>\n</div>\n</div>\n<div id=\"deploying\">\n<h2>Deploying</h2>\n<ul>\n<li>If the project does not already exist it will be created</li>\n<li>Any jobs not yet in SpiderKeeper will be added</li>\n<li>Jobs already in SpiderKeeper will be updated (i.e. tags, desc can be updated)</li>\n<li><strong>Jobs not in config will be deleted</strong></li>\n</ul>\n<p>A job is defined as already existing in SpiderKeeper if spider_name,\nspider_arguments, cron_minutes, cron_hour, cron_day_of_month, cron_day_of_week,\nand cron_month all match what is in config.</p>\n<p>Note: spider_name is always required. spider_arguments can be omitted and will\ndefault to None. cron settings can be omitted and will default to \u201c*\u201d</p>\n<p>SpiderKeeper uses numbers to identify projects. This means it is possible to\nhave two projects with the same name. spiderkeeper-deploy will use the first\nproject who\u2019s name matches the project config value. If you have an existing\ndeployment with duplicates you should keep this in mind. If not you should\nnever get duplicates as long as you only use spiderkeeper-deploy. It is still\npossible to get duplicates with a manual deployment.</p>\n</div>\n\n          </div>"}, "last_serial": 4549633, "releases": {"0.1.2": [{"comment_text": "", "digests": {"md5": "3f823ea9f15153e9970c5fb257ad75ae", "sha256": "b146a0869529cc24944c3c5e9cb5b2127eb75300be42d168df5fe24b451ec22e"}, "downloads": -1, "filename": "spiderkeeper_deploy-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "3f823ea9f15153e9970c5fb257ad75ae", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10811, "upload_time": "2018-09-23T18:15:02", "upload_time_iso_8601": "2018-09-23T18:15:02.917112Z", "url": "https://files.pythonhosted.org/packages/68/f8/30454d7a127bc9e09d5e427069f1e252f81db81870282c607a6e77ede295/spiderkeeper_deploy-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4031c5546c066b38af44caf0b66c3900", "sha256": "e41d5a530c25417d69ff5f812e828df1999a3c1200667b0ccf23716bb48cc1e7"}, "downloads": -1, "filename": "spiderkeeper-deploy-0.1.2.tar.gz", "has_sig": false, "md5_digest": "4031c5546c066b38af44caf0b66c3900", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7243, "upload_time": "2018-09-23T18:15:04", "upload_time_iso_8601": "2018-09-23T18:15:04.274425Z", "url": "https://files.pythonhosted.org/packages/be/cf/d9d032c9a46ddc03835eeac3f491f8fb8df2831b5cdbe2b83c323d277e89/spiderkeeper-deploy-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "d19a4d8d5ed7fa573407f0063114acbf", "sha256": "463d5caae15471837d7377eb2c606b94c703a47d411da6fb1e51ab48a252948a"}, "downloads": -1, "filename": "spiderkeeper-deploy-0.1.3.tar.gz", "has_sig": false, "md5_digest": "d19a4d8d5ed7fa573407f0063114acbf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7191, "upload_time": "2018-12-01T05:50:56", "upload_time_iso_8601": "2018-12-01T05:50:56.512459Z", "url": "https://files.pythonhosted.org/packages/85/ba/26647f2d488d431264b58a40fa5454b80bfab8ab8c78bd141759b37d4ff3/spiderkeeper-deploy-0.1.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d19a4d8d5ed7fa573407f0063114acbf", "sha256": "463d5caae15471837d7377eb2c606b94c703a47d411da6fb1e51ab48a252948a"}, "downloads": -1, "filename": "spiderkeeper-deploy-0.1.3.tar.gz", "has_sig": false, "md5_digest": "d19a4d8d5ed7fa573407f0063114acbf", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7191, "upload_time": "2018-12-01T05:50:56", "upload_time_iso_8601": "2018-12-01T05:50:56.512459Z", "url": "https://files.pythonhosted.org/packages/85/ba/26647f2d488d431264b58a40fa5454b80bfab8ab8c78bd141759b37d4ff3/spiderkeeper-deploy-0.1.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:04:54 2020"}