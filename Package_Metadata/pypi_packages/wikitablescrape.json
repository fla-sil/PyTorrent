{"info": {"author": "Andy Roche", "author_email": "andy@roche.io", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# wiki-table-scrape\n\nScrape HTML tables from a Wikipedia page into CSV format.\n\n`wikitablescrape` can be used as a shell command or imported as a Python package.\n\n## Why?\n\nThis tool makes it easy to download any Wikipedia table via CLI in a format ready for text processing.\n\nThis is especially useful when combined with a tool like [`xsv`](https://github.com/BurntSushi/xsv).\n\n##### Year Distribution of Costliest Atlantic Hurricanes\n\n```\nwikitablescrape --url='https://en.wikipedia.org/wiki/List_of_costliest_Atlantic_hurricanes' --header='costliest' | xsv select \"Season\" | xsv stats --median | xsv select field,min,max,median,mean,stddev | xsv table\n```\n```\nfield   min   max   median  mean                stddev\nSeason  1965  2018  2002    1999.1228070175441  12.900523823770502\n```\n\n##### Country / Market Distribution of Best-selling Music Artists\n\n```\nwikitablescrape --url='https://en.wikipedia.org/wiki/List_of_best-selling_music_artists' --header='100 million' | xsv select 'Country / Market' | xsv frequency | xsv table\n```\n```\nfield             value                         count\nCountry / Market  United States                 26\nCountry / Market  United Kingdom                10\nCountry / Market  United Kingdom United States  1\nCountry / Market  Australia                     1\nCountry / Market  Spain                         1\nCountry / Market  Japan                         1\n```\n\n## Installation\n\nYou can download the package from [PyPI](https://pypi.org/project/wikitablescrape/) or build from source using [Python 3](https://www.python.org/downloads/).\n\n### As a system-level Python package\n\n```sh\npython3 -m pip install wikitablescrape\nwikitablescrape --help\n```\n\n### In a virtual environment\n\n```sh\npython3 -m venv venv\n. venv/bin/activate\npip install wikitablescrape\nwikitablescrape --help\n```\n\n### Build from source\n\n```sh\ngit clone https://github.com/rocheio/wiki-table-scrape\ncd ./wiki-table-scrape\npython3 -m venv venv\n. venv/bin/activate\npython setup.py install\nwikitablescrape --help\n```\n\n## Sample Commands\n\n##### Write a single table to stdout\n\n```sh\nwikitablescrape --url=\"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\" --header=\"films by year\" | tee >(head -1) >(tail -5) >/dev/null\n```\n```csv\n\"Year\",\"Title\",\"Worldwide gross\",\"Budget\",\"Reference(s)\"\n\"2015\",\"Star Wars: The Force Awakens\",\"$2,068,223,624\",\"$245,000,000\",\"\"\n\"2016\",\"Captain America: Civil War\",\"$1,153,304,495\",\"$250,000,000\",\"\"\n\"2017\",\"Star Wars: The Last Jedi\",\"$1,332,539,889\",\"$200,000,000\",\"\"\n\"2018\",\"Avengers: Infinity War\",\"$2,048,359,754\",\"$316,000,000\u2013400,000,000\",\"\"\n\"2019\",\"Avengers: Endgame\",\"$2,796,255,086\",\"$356,000,000\",\"\"\n```\n\n##### Download all tables on a page into a folder of CSV files\n\n```sh\nwikitablescrape --url=\"https://en.wikipedia.org/wiki/Wikipedia:Multiyear_ranking_of_most_viewed_pages#Top-100_list\" --output-folder=\"/tmp/scrape\"\n```\n```\nParsing all tables from 'https://en.wikipedia.org/wiki/Wikipedia:Multiyear_ranking_of_most_viewed_pages#Top-100_list' into '/tmp/scrape'\nWriting table 1 to /tmp/scrape/table_1_top_100_list.csv\nWriting table 2 to /tmp/scrape/table_2_countries.csv\nWriting table 3 to /tmp/scrape/table_3_cities.csv\nWriting table 4 to /tmp/scrape/table_4_buildings_&_structures_&_statues.csv\nWriting table 5 to /tmp/scrape/table_5_people.csv\nWriting table 6 to /tmp/scrape/table_6_people_singers.csv\nWriting table 7 to /tmp/scrape/table_7_people_actors.csv\nWriting table 8 to /tmp/scrape/table_8_people_romantic_actors.csv\nWriting table 9 to /tmp/scrape/table_9_people_athletes.csv\nWriting table 10 to /tmp/scrape/table_10_people_modern_political_leaders.csv\nWriting table 11 to /tmp/scrape/table_11_people_pre_modern_people.csv\nWriting table 12 to /tmp/scrape/table_12_people_3rd_millennium_people.csv\nWriting table 13 to /tmp/scrape/table_13_progression_of_the_most_viewed_millennial_persons_on_wikipedia.csv\nWriting table 14 to /tmp/scrape/table_14_music_bands_historical_most_viewed_3rd_millennium_persons.csv\nWriting table 15 to /tmp/scrape/table_15_sport_teams_historical_most_viewed_3rd_millennium_persons.csv\nWriting table 16 to /tmp/scrape/table_16_films_and_tv_series_historical_most_viewed_3rd_millennium_persons.csv\nWriting table 17 to /tmp/scrape/table_17_albums_historical_most_viewed_3rd_millennium_persons.csv\nWriting table 18 to /tmp/scrape/table_18_books_and_book_series_historical_most_viewed_3rd_millennium_persons.csv\nWriting table 19 to /tmp/scrape/table_19_books_and_book_series_pre_modern_books_and_texts.csv\n```\n\n```sh\nhead -5 /tmp/scrape/table_3_cities.csv\n```\n```csv\n\"Rank\",\"Page\",\"Continent\",\"Views in millions\"\n\"1\",\"New York City\",\"North America\",\"75\"\n\"2\",\"Singapore\",\"Asia\",\"63\"\n\"3\",\"London\",\"Europe\",\"61\"\n\"4\",\"Hong Kong\",\"Asia\",\"50\"\n```\n\n## Testing\n\n```sh\n./scripts/test.sh\n\n# Show coverage data in a browser\ncoverage html && open htmlcov/index.html\n```\n\n## Sample Articles for Scraping\n\n- [Top 25 Articles this Month](https://en.wikipedia.org/wiki/Wikipedia:Top_25_Report)\n- [Top 100 Articles of All Time](https://en.wikipedia.org/wiki/Wikipedia:Multiyear_ranking_of_most_viewed_pages#Top-100_list)\n\n## Contributing\n\nIf you would like to contribute to this module, please open an issue or pull request.\n\n## More Information\n\nIf you'd like to read more about this module, please check out [my blog post][blog-post] from the initial release.\n\n[blog-post]: https://roche.io/2016/05/scrape-wikipedia-with-python\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/rocheio/wiki-table-scrape", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "wikitablescrape", "package_url": "https://pypi.org/project/wikitablescrape/", "platform": "", "project_url": "https://pypi.org/project/wikitablescrape/", "project_urls": {"Homepage": "https://github.com/rocheio/wiki-table-scrape"}, "release_url": "https://pypi.org/project/wikitablescrape/1.0.4/", "requires_dist": ["beautifulsoup4 (==4.*)", "lxml (==4.*)", "requests (==2.*)"], "requires_python": "", "summary": "Scrape HTML tables from a Wikipedia page into CSV format.", "version": "1.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>wiki-table-scrape</h1>\n<p>Scrape HTML tables from a Wikipedia page into CSV format.</p>\n<p><code>wikitablescrape</code> can be used as a shell command or imported as a Python package.</p>\n<h2>Why?</h2>\n<p>This tool makes it easy to download any Wikipedia table via CLI in a format ready for text processing.</p>\n<p>This is especially useful when combined with a tool like <a href=\"https://github.com/BurntSushi/xsv\" rel=\"nofollow\"><code>xsv</code></a>.</p>\n<h5>Year Distribution of Costliest Atlantic Hurricanes</h5>\n<pre><code>wikitablescrape --url='https://en.wikipedia.org/wiki/List_of_costliest_Atlantic_hurricanes' --header='costliest' | xsv select \"Season\" | xsv stats --median | xsv select field,min,max,median,mean,stddev | xsv table\n</code></pre>\n<pre><code>field   min   max   median  mean                stddev\nSeason  1965  2018  2002    1999.1228070175441  12.900523823770502\n</code></pre>\n<h5>Country / Market Distribution of Best-selling Music Artists</h5>\n<pre><code>wikitablescrape --url='https://en.wikipedia.org/wiki/List_of_best-selling_music_artists' --header='100 million' | xsv select 'Country / Market' | xsv frequency | xsv table\n</code></pre>\n<pre><code>field             value                         count\nCountry / Market  United States                 26\nCountry / Market  United Kingdom                10\nCountry / Market  United Kingdom United States  1\nCountry / Market  Australia                     1\nCountry / Market  Spain                         1\nCountry / Market  Japan                         1\n</code></pre>\n<h2>Installation</h2>\n<p>You can download the package from <a href=\"https://pypi.org/project/wikitablescrape/\" rel=\"nofollow\">PyPI</a> or build from source using <a href=\"https://www.python.org/downloads/\" rel=\"nofollow\">Python 3</a>.</p>\n<h3>As a system-level Python package</h3>\n<pre>python3 -m pip install wikitablescrape\nwikitablescrape --help\n</pre>\n<h3>In a virtual environment</h3>\n<pre>python3 -m venv venv\n. venv/bin/activate\npip install wikitablescrape\nwikitablescrape --help\n</pre>\n<h3>Build from source</h3>\n<pre>git clone https://github.com/rocheio/wiki-table-scrape\n<span class=\"nb\">cd</span> ./wiki-table-scrape\npython3 -m venv venv\n. venv/bin/activate\npython setup.py install\nwikitablescrape --help\n</pre>\n<h2>Sample Commands</h2>\n<h5>Write a single table to stdout</h5>\n<pre>wikitablescrape --url<span class=\"o\">=</span><span class=\"s2\">\"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"</span> --header<span class=\"o\">=</span><span class=\"s2\">\"films by year\"</span> <span class=\"p\">|</span> tee &gt;<span class=\"o\">(</span>head -1<span class=\"o\">)</span> &gt;<span class=\"o\">(</span>tail -5<span class=\"o\">)</span> &gt;/dev/null\n</pre>\n<pre>\"Year\",\"Title\",\"Worldwide gross\",\"Budget\",\"Reference(s)\"\n\"2015\",\"Star Wars: The Force Awakens\",\"$2,068,223,624\",\"$245,000,000\",\"\"\n\"2016\",\"Captain America: Civil War\",\"$1,153,304,495\",\"$250,000,000\",\"\"\n\"2017\",\"Star Wars: The Last Jedi\",\"$1,332,539,889\",\"$200,000,000\",\"\"\n\"2018\",\"Avengers: Infinity War\",\"$2,048,359,754\",\"$316,000,000\u2013400,000,000\",\"\"\n\"2019\",\"Avengers: Endgame\",\"$2,796,255,086\",\"$356,000,000\",\"\"\n</pre>\n<h5>Download all tables on a page into a folder of CSV files</h5>\n<pre>wikitablescrape --url<span class=\"o\">=</span><span class=\"s2\">\"https://en.wikipedia.org/wiki/Wikipedia:Multiyear_ranking_of_most_viewed_pages#Top-100_list\"</span> --output-folder<span class=\"o\">=</span><span class=\"s2\">\"/tmp/scrape\"</span>\n</pre>\n<pre><code>Parsing all tables from 'https://en.wikipedia.org/wiki/Wikipedia:Multiyear_ranking_of_most_viewed_pages#Top-100_list' into '/tmp/scrape'\nWriting table 1 to /tmp/scrape/table_1_top_100_list.csv\nWriting table 2 to /tmp/scrape/table_2_countries.csv\nWriting table 3 to /tmp/scrape/table_3_cities.csv\nWriting table 4 to /tmp/scrape/table_4_buildings_&amp;_structures_&amp;_statues.csv\nWriting table 5 to /tmp/scrape/table_5_people.csv\nWriting table 6 to /tmp/scrape/table_6_people_singers.csv\nWriting table 7 to /tmp/scrape/table_7_people_actors.csv\nWriting table 8 to /tmp/scrape/table_8_people_romantic_actors.csv\nWriting table 9 to /tmp/scrape/table_9_people_athletes.csv\nWriting table 10 to /tmp/scrape/table_10_people_modern_political_leaders.csv\nWriting table 11 to /tmp/scrape/table_11_people_pre_modern_people.csv\nWriting table 12 to /tmp/scrape/table_12_people_3rd_millennium_people.csv\nWriting table 13 to /tmp/scrape/table_13_progression_of_the_most_viewed_millennial_persons_on_wikipedia.csv\nWriting table 14 to /tmp/scrape/table_14_music_bands_historical_most_viewed_3rd_millennium_persons.csv\nWriting table 15 to /tmp/scrape/table_15_sport_teams_historical_most_viewed_3rd_millennium_persons.csv\nWriting table 16 to /tmp/scrape/table_16_films_and_tv_series_historical_most_viewed_3rd_millennium_persons.csv\nWriting table 17 to /tmp/scrape/table_17_albums_historical_most_viewed_3rd_millennium_persons.csv\nWriting table 18 to /tmp/scrape/table_18_books_and_book_series_historical_most_viewed_3rd_millennium_persons.csv\nWriting table 19 to /tmp/scrape/table_19_books_and_book_series_pre_modern_books_and_texts.csv\n</code></pre>\n<pre>head -5 /tmp/scrape/table_3_cities.csv\n</pre>\n<pre>\"Rank\",\"Page\",\"Continent\",\"Views in millions\"\n\"1\",\"New York City\",\"North America\",\"75\"\n\"2\",\"Singapore\",\"Asia\",\"63\"\n\"3\",\"London\",\"Europe\",\"61\"\n\"4\",\"Hong Kong\",\"Asia\",\"50\"\n</pre>\n<h2>Testing</h2>\n<pre>./scripts/test.sh\n\n<span class=\"c1\"># Show coverage data in a browser</span>\ncoverage html <span class=\"o\">&amp;&amp;</span> open htmlcov/index.html\n</pre>\n<h2>Sample Articles for Scraping</h2>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Wikipedia:Top_25_Report\" rel=\"nofollow\">Top 25 Articles this Month</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Wikipedia:Multiyear_ranking_of_most_viewed_pages#Top-100_list\" rel=\"nofollow\">Top 100 Articles of All Time</a></li>\n</ul>\n<h2>Contributing</h2>\n<p>If you would like to contribute to this module, please open an issue or pull request.</p>\n<h2>More Information</h2>\n<p>If you'd like to read more about this module, please check out <a href=\"https://roche.io/2016/05/scrape-wikipedia-with-python\" rel=\"nofollow\">my blog post</a> from the initial release.</p>\n\n          </div>"}, "last_serial": 6689575, "releases": {"1.0.2": [{"comment_text": "", "digests": {"md5": "53f131428088857a3fdef0ce7fdf581d", "sha256": "4b158d21d6c640c7da247c604d09c88b54820a3424f969583e69abf43880955e"}, "downloads": -1, "filename": "wikitablescrape-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "53f131428088857a3fdef0ce7fdf581d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 9078, "upload_time": "2019-09-07T15:40:52", "upload_time_iso_8601": "2019-09-07T15:40:52.453633Z", "url": "https://files.pythonhosted.org/packages/d0/22/e2330078775be99eb7d42e84d96e788b9c51af45f82f3761cc6b998733b7/wikitablescrape-1.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fdda0e8fea660928d6658f7c24ca48ea", "sha256": "5379beb358fe04702d38d4d245bebebd2b504ab9c358ee597a911c060ac0834e"}, "downloads": -1, "filename": "wikitablescrape-1.0.2.tar.gz", "has_sig": false, "md5_digest": "fdda0e8fea660928d6658f7c24ca48ea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6884, "upload_time": "2019-09-07T15:40:54", "upload_time_iso_8601": "2019-09-07T15:40:54.814975Z", "url": "https://files.pythonhosted.org/packages/64/04/b6fb81a8d28d10a47dad9b118a909884406b0bcef6bc9fb216212e5cff64/wikitablescrape-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "3e324d3c8da1cce999e0dd3728814c7f", "sha256": "2e24136fa8a9b42b5743f16aad59568339649a796555c4884a6cf557b2694f25"}, "downloads": -1, "filename": "wikitablescrape-1.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "3e324d3c8da1cce999e0dd3728814c7f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10097, "upload_time": "2019-12-12T17:55:48", "upload_time_iso_8601": "2019-12-12T17:55:48.518782Z", "url": "https://files.pythonhosted.org/packages/1e/e7/e83b6162b2ffe5219a7542dc424fe2f8c569d6e8be00fb8482181d73faa6/wikitablescrape-1.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d277a908e5d3197a4d8456d820f1b37c", "sha256": "cdb736a83cf68926255b24e4fc3f6bac9cbd4648d17bb1d7f43abd7d41848502"}, "downloads": -1, "filename": "wikitablescrape-1.0.3.tar.gz", "has_sig": false, "md5_digest": "d277a908e5d3197a4d8456d820f1b37c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8639, "upload_time": "2019-12-12T17:55:49", "upload_time_iso_8601": "2019-12-12T17:55:49.628192Z", "url": "https://files.pythonhosted.org/packages/c3/42/a8b4f23befdb6c489177902c18a6c419d35b4e21a25f58f3f8b887da0fd3/wikitablescrape-1.0.3.tar.gz", "yanked": false}], "1.0.4": [{"comment_text": "", "digests": {"md5": "17241509f2d7a664ccd414c99890b67c", "sha256": "054d8a2d3b5c1598fc931940cfcb491503305ca992f7e38e90c28eb1a43067fe"}, "downloads": -1, "filename": "wikitablescrape-1.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "17241509f2d7a664ccd414c99890b67c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10583, "upload_time": "2020-02-24T16:32:29", "upload_time_iso_8601": "2020-02-24T16:32:29.400025Z", "url": "https://files.pythonhosted.org/packages/ac/ca/3153b24bbc4dad3bda2b38f74f3135b2fd07d548d2dd6538d9c19b3361a4/wikitablescrape-1.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "26d07b295750c49e002308765111a19a", "sha256": "c16e2ae44669ffbb9d431f063cf36265f863150943440f0c9add58f9083a9ef3"}, "downloads": -1, "filename": "wikitablescrape-1.0.4.tar.gz", "has_sig": false, "md5_digest": "26d07b295750c49e002308765111a19a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9940, "upload_time": "2020-02-24T16:32:31", "upload_time_iso_8601": "2020-02-24T16:32:31.179562Z", "url": "https://files.pythonhosted.org/packages/26/3b/9d16d51b0e753763eb3d4d0576fc7c0cf97c172b51ef84ace05cc4002f5c/wikitablescrape-1.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "17241509f2d7a664ccd414c99890b67c", "sha256": "054d8a2d3b5c1598fc931940cfcb491503305ca992f7e38e90c28eb1a43067fe"}, "downloads": -1, "filename": "wikitablescrape-1.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "17241509f2d7a664ccd414c99890b67c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10583, "upload_time": "2020-02-24T16:32:29", "upload_time_iso_8601": "2020-02-24T16:32:29.400025Z", "url": "https://files.pythonhosted.org/packages/ac/ca/3153b24bbc4dad3bda2b38f74f3135b2fd07d548d2dd6538d9c19b3361a4/wikitablescrape-1.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "26d07b295750c49e002308765111a19a", "sha256": "c16e2ae44669ffbb9d431f063cf36265f863150943440f0c9add58f9083a9ef3"}, "downloads": -1, "filename": "wikitablescrape-1.0.4.tar.gz", "has_sig": false, "md5_digest": "26d07b295750c49e002308765111a19a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9940, "upload_time": "2020-02-24T16:32:31", "upload_time_iso_8601": "2020-02-24T16:32:31.179562Z", "url": "https://files.pythonhosted.org/packages/26/3b/9d16d51b0e753763eb3d4d0576fc7c0cf97c172b51ef84ace05cc4002f5c/wikitablescrape-1.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:29:14 2020"}