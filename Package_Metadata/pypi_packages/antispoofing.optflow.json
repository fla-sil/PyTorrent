{"info": {"author": "Andre Anjos", "author_email": "andre.anjos@idiap.ch", "bugtrack_url": null, "classifiers": [], "description": "====================================================\n Optical Flow Counter-Measures for Spoofing Attacks\n====================================================\n\nThis package contains our published Optical Flow algorithm for face recognition\nanti-spoofing. This document explains how to install it and use it to produce\nour paper results.\n\nIf you use this package and/or its results, please cite the following\npublications:\n\n1. The original paper with the counter-measure explained in details (under\n   review)::\n\n    @article{Anjos_IETBMT_2013,\n      author = {Anjos, Andr{\\'{e}} and Murali Mohan Chakka and Marcel, S{\\'{e}}bastien},\n      keywords = {Attack, Counter-Measures, Counter-Spoofing, Disguise, Dishonest Acts, Face Recognition, Face Verification, Forgery, Liveness Detection, Replay, Spoofing, Trick, Optical Flow},\n      month = apr,\n      title = {Motion-Based Counter-Measures to Photo Attacks in Face Recognition},\n      journal = {Institution of Engineering and Technology - Biometrics},\n      year = {2013},\n    }\n\n2. Bob as the core framework used to run the experiments::\n\n    @inproceedings{Anjos_ACMMM_2012,\n      author = {A. Anjos AND L. El Shafey AND R. Wallace AND M. G\\\"unther AND C. McCool AND S. Marcel},\n      title = {Bob: a free signal processing and machine learning toolbox for researchers},\n      year = {2012},\n      month = oct,\n      booktitle = {20th ACM Conference on Multimedia Systems (ACMMM), Nara, Japan},\n      publisher = {ACM Press},\n      url = {http://publications.idiap.ch/downloads/papers/2012/Anjos_Bob_ACMMM12.pdf},\n    }\n\n.. 3. To use the REPLAY-ATTACK database, you should also mention the following\n   paper, where it is introduced::\n\n  @inproceedings{Chingovska_BIOSIG_2012,\n    author = {Chingovska, Ivana and Anjos, Andr{\\'{e}} and Marcel, S{\\'{e}}bastien},\n    keywords = {Attack, Counter-Measures, Counter-Spoofing, Face Recognition, Liveness Detection, Replay, Spoofing},\n    month = sep,\n    title = {On the Effectiveness of Local Binary Patterns in Face Anti-spoofing},\n    booktitle = {IEEE Biometrics Special Interest Group},\n    year = {2012},\n    url = {http://publications.idiap.ch/downloads/papers/2012/Chingovska_IEEEBIOSIG2012_2012.pdf},\n  }\n\nIf you wish to report problems or improvements concerning this code, please\ncontact the authors of the above mentioned papers.\n\nRaw data\n--------\n\nThe data used in the paper is publicly available and should be downloaded and\ninstalled **prior** to try using the programs described in this package. Visit\n`the PHOTO-ATTACK database portal <https://www.idiap.ch/dataset/photoattack>`_\nfor more information.\n\nInstallation\n------------\n\n.. note::\n\n  If you are reading this page through our GitHub portal and not through PyPI,\n  note **the development tip of the package may not be stable** or become\n  unstable in a matter of moments.\n\n  Go to `http://pypi.python.org/pypi/antispoofing.optflow\n  <http://pypi.python.org/pypi/antispoofing.optflow>`_ to download the latest\n  stable version of this package.\n\nThere are 2 options you can follow to get this package installed and\noperational on your computer: you can use automatic installers like `pip\n<http://pypi.python.org/pypi/pip/>`_ (or `easy_install\n<http://pypi.python.org/pypi/setuptools>`_) or manually download, unpack and\nuse `zc.buildout <http://pypi.python.org/pypi/zc.buildout>`_ to create a\nvirtual work environment just for this package.\n\nUsing an automatic installer\n============================\n\nUsing ``pip`` is the easiest (shell commands are marked with a ``$`` signal)::\n\n  $ pip install antispoofing.optflow\n\nYou can also do the same with ``easy_install``::\n\n  $ easy_install antispoofing.optflow\n\nThis will download and install this package plus any other required\ndependencies. It will also verify if the version of Bob you have installed\nis compatible.\n\nThis scheme works well with virtual environments by `virtualenv\n<http://pypi.python.org/pypi/virtualenv>`_ or if you have root access to your\nmachine. Otherwise, we recommend you use the next option.\n\nUsing ``zc.buildout``\n=====================\n\nDownload the latest version of this package from `PyPI\n<http://pypi.python.org/pypi/antispoofing.optflow>`_ and unpack it in your\nworking area. The installation of the toolkit itself uses `buildout\n<http://www.buildout.org/>`_. You don't need to understand its inner workings\nto use this package. Here is a recipe to get you started::\n\n  $ python bootstrap.py\n  $ ./bin/buildout\n\nThese 2 commands should download and install all non-installed dependencies and\nget you a fully operational test and development environment.\n\n.. note::\n\n  The python shell used in the first line of the previous command set\n  determines the python interpreter that will be used for all scripts developed\n  inside this package. Because this package makes use of `Bob\n  <http://idiap.github.com/bob>`_, you must make sure that the ``bootstrap.py``\n  script is called with the **same** interpreter used to build Bob, or\n  unexpected problems might occur.\n\n  If Bob is installed by the administrator of your system, it is safe to\n  consider it uses the default python interpreter. In this case, the above 3\n  command lines should work as expected. If you have Bob installed somewhere\n  else on a private directory, edit the file ``buildout.cfg`` **before**\n  running ``./bin/buildout``. Find the section named ``buildout`` and edit or\n  add the line ``prefixes`` to point to the directory where Bob is installed or\n  built. For example::\n\n    [buildout]\n    ...\n    prefixes=/Users/crazyfox/work/bob/build\n\nUser Guide\n----------\n\nIt is assumed you have followed the installation instructions for the package\nand got this package installed and the PHOTO-ATTACK database downloaded and\nuncompressed in a directory. You should have all required utilities sitting\ninside a binary directory depending on your installation strategy (utilities\nwill be inside the ``bin`` if you used the buildout option). We expect that the\nvideo files downloaded for the PHOTO-ATTACK database are installed in a\nsub-directory called ``database`` at the root of the package.  You can use a\nlink to the location of the database files, if you don't want to have the\ndatabase installed on the root of this package::\n\n  $ ln -s /path/where/you/installed/the/photo-attack-database database\n\nIf you don't want to create a link, use the ``--input-dir`` flag to specify\nthe root directory containing the database files. That would be the directory\nthat *contains* the sub-directories ``train``, ``test``, ``devel`` and\n``face-locations``.\n\nPaper Layout: How to Reproduce our Results\n==========================================\n\nThe paper studies 4 algorithms (the first 3 are published elsewhere and are\n**not** a contribution to this paper):\n\nReference System 1 (RS1) - Kollreider's Optical Flow anti-spoofing::\n\n  @article{Kollreider_2009,\n    author={K. Kollreider AND H. Fronthaler AND J. Bigun},\n    title={Non-intrusive liveness detection by face images},\n    volume={27},\n    number={3},\n    journal={Image and Vision Computing},\n    publisher={Elsevier B.V.},\n    year={2009},\n    pages={233--244},\n  }\n\nReference System 2 (RS2) - Bao's Optical Flow anti-spoofing::\n\n  @inproceedings{Bao_2009,\n    author={Wei Bao AND H. Li AND Nan Li AND Wei Jiang},\n    title={A liveness detection method for face recognition based on optical flow field},\n    booktitle={2009 International Conference on Image Analysis and Signal Processi\n  ng},\n    publisher={IEEE},\n    year={2009},\n    pages={233--236},\n  }\n\nReference System 3 (RS3) - Our own Frame Difference's based anti-spoofing::\n\n  @inproceedings{Anjos_IJCB_2011,\n    author = {Anjos, Andr{\\'{e}} and Marcel, S{\\'{e}}bastien},\n    keywords = {Attack, Counter-Measures, Counter-Spoofing, Disguise, Dishonest Acts, Face Recognition, Face Verification, Forgery, Liveness Detection, Replay, Spoofing, Trick},\n    month = oct,\n    title = {Counter-Measures to Photo Attacks in Face Recognition: a public database and a baseline},\n    booktitle = {International Joint Conference on Biometrics 2011},\n    year = {2011},\n    url = {http://publications.idiap.ch/downloads/papers/2011/Anjos_IJCB_2011.pdf}\n  }\n\nThe final algorithm based on Optical Flow Correlation (OFC) **represents our\ncontribution in this paper**.\n\nTo reproduce the results for RS3, you can follow the instructions on `its own\nsatellite package for Bob <http://pypi.python.org/pypi/antispoofing.motion>`_.\nThe scripts for that package should be auto-generated and made available for\nyou under your ``bin`` as well (this package depends on that one).\n\nIn this manual, we address how to extract results for RS1, 2 and OFC, which\noperate on the top of a previously estimated Optical Flow (OF) field.  OF is,\ntherefore, the first topic in this manual.\n\nExtract the Optical Flow Features\n=================================\n\nWe ship this package with a preset to use `Ce Liu's OF framework\n<http://pypi.python.org/pypi/xbob.optflow.liu>`_. This is of course not\nrequired, but it is the framework we have tested our method with, therefore the\none we recommend you to start using. This framework estimates the *dense* OF\nfield between any two successive frames. It is quite slow. Be warned, it may\ntake quite some time to get through all the videos. To run the extraction\nsequentially, for all videos, use the following command::\n\n  $ ./bin/optflow_estimate.py --verbose /root/of/database results/flows replay --protocol=photo\n\n.. note::\n\n  The command line above is going to take a lot of time to complete. You may\n  need to parallelize the job. If you are Idiap, you can use the ``gridtk``\n  package, which should be downloaded and installed on your current\n  environment::\n\n    $ ./bin/jman submit --array=800 --queue=q1d ./bin/optflow_estimate.py --verbose /root/of/database results/flows replay --protocol=photo\n\n  The ``magic`` number of ``800`` entries can be found by executing::\n\n    $ ./bin/optflow_estimate.py --grid-count replay --protocol=photo\n\n  Which just prints the number of jobs it requires for the grid execution. Each\n  job is consuming less than 2 gigabytes of RAM, but more than 1 gigabyte.\n  Therefore, you must choose the right queue and may need to set memory\n  requirements for the machines you will be running at.\n\n.. note::\n\n  In case you want to replace this phase by another algorithm of your own.\n  Notice that the output format is 1 HDF5 file per input video in the database,\n  organized in the same way as in the original database. Each input video\n  should contain a single 4D 64-bit float point array which has the following\n  shape: (frames, u+v, height, width). The second dimension corresponds to the\n  U (horizontal) and V (vertical) velocities as output by your algorithm, for\n  every pixel in the image. If you have not used a dense OF estimator, please\n  extrapolate yourself the values before calling the next scripts.\n\n  If you respect this requirement, than you can test the results of this\n  framework with any OF estimation technique of your choice.\n\nOnce you are in possession of the flow fields. You can start calculating the\nscores required by each of the methods reviewed in the paper. It can help in\nterms of processing speed to have the features located on a local hard-drive.\nThe HDF5 files tend to be huge.\n\n.. important::\n\n  Depending on the version of FFmpeg you have installed on your platform when\n  you estimate the OF, you may get slightly different results at this step.\n  These are due to imprecisions on the video decoding.\n\n  You can also use the Matlab version of Ce Liu's code directly to produce the\n  flow fields. In this case, you may also find small differences on the\n  estimated velocities. The differences are due to the movie decoding and\n  gray-scale conversion, which are different than Bob's.\n\n  In any of these conditions, our tests show these do not affect the overall\n  performance of our method. It may slightly change the final results you can\n  obtain.\n\nReference System 1: Scores from Kollreider's\n============================================\n\nTo calculate scores using Kollreider's method, use the script\n``optflow_kollreider.py`` in the following way::\n\n  $ ./bin/optflow_kollreider.py --verbose /root/of/database results/flows results/kollreider replay --protocol=photo\n\nYou can modify the :math:`\\tau` parameter required by the method with the\nprogram option ``--tau=<float-value>``. By default, this parameter is set to\n``1.0``. Refer to the original paper by Kollreider to understand the meaning\nand how to tune this parameter. If you tune the parameter and execute the error\nanalysis as explained below, you will get to the results shown on Table 1 of\nour paper.\n\n.. note::\n\n  The above program can be somewhat slow, even if it is much faster than the\n  flow field estimation itself. If you want to speed things, up, just run it on\n  the grid::\n\n    $ ./bin/jman submit --array=800 ./bin/optflow_kollreider.py --verbose /root/of/database results/flows results/kollreider replay --protocol=photo\n\n  The program ``optflow_kollreider.py`` can also print the number of jobs it can\n  be broken into with the ``--grid-count`` option::\n\n    ./bin/optflow_kollreider.py --grid-count replay --protocol=photo\n\n.. important::\n\n  We estimate the position of the face center and the ears based on the\n  bounding-box provided by the face locations. This way, we could compare all\n  algorithms using the same input set. We have not tested if specialized\n  key-point localizers would give better results than ours for this method.\n\nBesides generating output for the tests in the paper, you can also generate an\nannotated video, showing how our extrapolation of the face bounding boxes work\nfor finding out the regions of interest to apply Kollreider's work on. To do\nthis, use the script `optflow_kollreider_annotate.py`. It works on a similar\nway to the above script and will process the whole database if not told\notherwise. This can be somewhat long as well, but you can grid-fy it if you\nwish or use filtering options for the database to limit the number of videos\nanalysed. For example::\n\n  $ bin/optflow_kollreider_annotate.py -v /idiap/group/replay/database/protocols/replayattack-database tmp replay --protocol=photo --client=101 --light=adverse\n\nReference System 2: Scores from Bao's\n=====================================\n\nTo calculate scores for Bao's method, use the script\n``optflow_bao.py`` in the following way::\n\n  $ ./bin/optflow_bao.py --verbose /root/of/database results/flows results/bao replay --protocol=photo\n\nYou can modify the border parameter required by the method with the program\noption ``--border=<integer-value>``. By default, this parameter is set to ``5``\n(pixels). The original paper by Bao and others does not suggest such a\nparameter or mention how does the face bounding-boxes are set. We assume a\ndefault value of pixels surrounding our detected face. In the paper, we scan\nthis value from ``0`` (zero) to a number of pixels to test the method. If you\ntune the parameter and execute the error analysis as explained below, you will\nget to the results shown on Table 2 of our paper.\n\n.. note::\n\n  The above program can be somewhat slow, even if it is much faster than the\n  flow field estimation itself. If you want to speed things, up, just run it on\n  the grid::\n\n    $ ./bin/jman submit --array=800 ./bin/optflow_bao.py --verbose /root/of/database results/flows results/bao replay --protocol=photo\n\n  The program ``optflow_bao.py`` can also print the number of jobs it can\n  be broken into with the ``--grid-count`` option::\n\n    ./bin/optflow_bao.py --grid-count replay --protocol=photo\n\nReference System 3: Frame-differences\n=====================================\n\nAs mentioned before, you should follow the instructions on `its own satellite\npackage for Bob`_. The scripts for that package should be auto-generated and\nmade available for you under your ``bin`` as well (this package depends on that\none).\n\nOptical Flow Correlation (OFC)\n==============================\n\nTo reproduce the results on our paper, you will need first to generate the\nscores for the :math:`\\chi^2` comparison for every frame in the sequence.\nFrames with no faces detected generate a score valued ``numpy.NaN``, similar to\nother counter-measures implemented by our group. To generate each score per\nframe, you can use the application ``optflow_histocomp.py``::\n\n  $ ./bin/optflow_histocomp.py --verbose /root/of/database results/flows results/histocomp replay --protocol=photo\n\n.. note::\n\n  The above program can be somewhat slow, even if it is much faster than the\n  flow field estimation itself. If you want to speed things, up, just run it on\n  the grid::\n\n    $ ./bin/jman submit --array=800 ./bin/optflow_histocomp.py --verbose /root/of/database results/flows results/histocomp replay --protocol=photo\n\n  The program ``optflow_histocomp.py`` can also print the number of jobs it can\n  be broken into with the ``--grid-count`` option::\n\n    ./bin/optflow_histocomp.py --grid-count replay --protocol=photo\n\nYou can generate the results in Figure 5 and 6 of our paper by setting 2\nparameters on the above script:\n\n  ``--number-of-bins``\n    This changes the parameter :math:`Q`, explained on the paper, related to\n    the quantization of the angle space. (see results in Figure 5.)\n\n  ``--offset``\n    This changes the offset for the quantization. Its effect is studied in\n    Figure 6, for ``--number-of-bins=2``, as explained in the paper.\n\nBy modifying the above parameters and executing an error analysis as\nexplained bellow, with ``--window-size=220``, you will get to the results\nplotted.\n\nError Analysis\n==============\n\nOnce the scores you want to analyze are produced by one of the methods above,\nyou can calculate the error on the database using the application\n``score_analysis.py``. This program receives one directory (containing the\nscores output by a given method) and produces a console analysis of such a\nmethod, which is used by the paper::\n\n  $ ./bin/score_analysis.py results/histocomp replay --protocol=photo\n\nThat command will calculate a development set threshold at the Equal Error Rate\n(EER) and will apply it to the test set, reporting errors on both sets. A\ntypical output would be like this::\n\n  Input data: /idiap/temp/aanjos/spoofing/scores/optflow_histocomp\n  Thres. at EER of development set: 6.9459e-02\n  [EER @devel] FAR: 37.04% (15601 / 42120) | FRR: 37.04% (8312 / 22440) | HTER: 37.04%\n  [HTER @test] FAR: 37.11% (20843 / 56160) | FRR: 35.75% (10696 / 29920) | HTER: 36.43%\n\nThe error analysis program considers, by default, every frame analyzed as **an\nindividual (independent) observation** and calculates the error rates based on\nthe overall set of frames found on the whole development and test sets. The\nnumbers printed inside the parentheses indicate how many frames were evaluated\nin each set (denominator) and how many of those contributed to the percentage\ndisplayed (numerator). The Half-Total Error Rate (HTER) is evaluated for both\nthe development set and test sets. The HTER for the develpment set is equal to\nthe EER on the same set, naturally.\n\nThe ``score_analysis.py`` script contains 2 parameters that can be used to\nfine-tune the program behaviour, to be known:\n\n  ``--window-size=<integer>``\n\n    Defines a window size to which the scores are going to be averaged to,\n    within the same score sequence. So, for example, if one of the files\n    produced by ``optflow_histocomp.py`` contains a sequence of scores that\n    reads like ``[1.0, 2.0, 1.5, 3.5, 0.5]``, and the window-size parameter is\n    set to 2, then, the scores evaluated by this procedure are ``[1.5, 1.75,\n    2.5, 2.0]``, which represent the averages of ``[1.0, 2.0]``, ``[2.0,\n    1.5]``, ``[1.5, 3.5]`` and ``[3.5, 0.5]``.\n\n  ``--overlap=<integer>``\n\n    Controls the amount of overlap between the windows. If not set, the default\n    overlap is set to ``window-size - 1``. You can modify this behaviour by\n    setting this parameter to a different value. Taking the example above, if\n    you set the window-size to 2 and the overlap to zero, then the score set\n    produced by this analysis would be ``[1.5, 2.5]``. Notice that the frame\n    value ``0.5`` (the last of the sequence) is ignored.\n\nYou will observe the effect of setting the window-size on the score analysis by\nlooking at the number of *averaged frames* analyzed::\n\n  $ ./bin/score_analysis.py --window-size=220 --overlap=80 results/histocomp replay --protocol=photo\n\nAnd the output::\n\n  Input data: /idiap/temp/aanjos/spoofing/scores/optflow_histocomp\n  Window size: 220 (overlap = 80)\n  Thres. at EER of development set: 1.4863e-01\n  [EER @devel] FAR: 2.78% (5 / 180) | FRR: 2.50% (3 / 120) | HTER: 2.64%\n  [HTER @test] FAR: 2.92% (7 / 240) | FRR: 1.88% (3 / 160) | HTER: 2.40%\n\nYou can generate the results in Figure 7 and Table III on the paper just by\nmanipulating this program.\n\nOur paper also shows a break-down analysis (by device attack type and support)\non Figure 8 (last figure). To generate such a figure, one must produce the\nbreak-down analysis per device (Figure 8.a) and attack support (Figure 8.b). To\ndo this, pass the ``--breakdown`` option to the ``score_analysis.py`` script::\n\n  $ ./bin/score_analysis.py --window-size=220 --overlap=80 --breakdown results/histocomp replay --protocol=photo\n\nOur paper also discusses the impact of skipping the OF calculation on certain\nframes (see Discussion section) in the interest of saving computational\nresources. You can generate the table presented at the paper by playing with\nthe ``--skip`` parameter of ``score_analysis.py``. By default, we don't skip\nany frames, if you set this parameter to 1, then we'd skip every other frame.\nIf you set it to 2, then we only consider 1 in every 3 frames, and so on.\n\nProblems\n--------\n\nIn case of problems, please contact any of the authors of the paper.", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://pypi.python.org/pypi/antispoofing.optflow", "keywords": null, "license": "GPLv3", "maintainer": null, "maintainer_email": null, "name": "antispoofing.optflow", "package_url": "https://pypi.org/project/antispoofing.optflow/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/antispoofing.optflow/", "project_urls": {"Download": "UNKNOWN", "Homepage": "http://pypi.python.org/pypi/antispoofing.optflow"}, "release_url": "https://pypi.org/project/antispoofing.optflow/2.0.0/", "requires_dist": null, "requires_python": null, "summary": "Optical Flow counter-measures for the REPLAY-ATTACK database", "version": "2.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            ====================================================<br> Optical Flow Counter-Measures for Spoofing Attacks<br>====================================================<br><br>This package contains our published Optical Flow algorithm for face recognition<br>anti-spoofing. This document explains how to install it and use it to produce<br>our paper results.<br><br>If you use this package and/or its results, please cite the following<br>publications:<br><br>1. The original paper with the counter-measure explained in details (under<br>   review)::<br><br>    @article{Anjos_IETBMT_2013,<br>      author = {Anjos, Andr{\\'{e}} and Murali Mohan Chakka and Marcel, S{\\'{e}}bastien},<br>      keywords = {Attack, Counter-Measures, Counter-Spoofing, Disguise, Dishonest Acts, Face Recognition, Face Verification, Forgery, Liveness Detection, Replay, Spoofing, Trick, Optical Flow},<br>      month = apr,<br>      title = {Motion-Based Counter-Measures to Photo Attacks in Face Recognition},<br>      journal = {Institution of Engineering and Technology - Biometrics},<br>      year = {2013},<br>    }<br><br>2. Bob as the core framework used to run the experiments::<br><br>    @inproceedings{Anjos_ACMMM_2012,<br>      author = {A. Anjos AND L. El Shafey AND R. Wallace AND M. G\\\"unther AND C. McCool AND S. Marcel},<br>      title = {Bob: a free signal processing and machine learning toolbox for researchers},<br>      year = {2012},<br>      month = oct,<br>      booktitle = {20th ACM Conference on Multimedia Systems (ACMMM), Nara, Japan},<br>      publisher = {ACM Press},<br>      url = {http://publications.idiap.ch/downloads/papers/2012/Anjos_Bob_ACMMM12.pdf},<br>    }<br><br>.. 3. To use the REPLAY-ATTACK database, you should also mention the following<br>   paper, where it is introduced::<br><br>  @inproceedings{Chingovska_BIOSIG_2012,<br>    author = {Chingovska, Ivana and Anjos, Andr{\\'{e}} and Marcel, S{\\'{e}}bastien},<br>    keywords = {Attack, Counter-Measures, Counter-Spoofing, Face Recognition, Liveness Detection, Replay, Spoofing},<br>    month = sep,<br>    title = {On the Effectiveness of Local Binary Patterns in Face Anti-spoofing},<br>    booktitle = {IEEE Biometrics Special Interest Group},<br>    year = {2012},<br>    url = {http://publications.idiap.ch/downloads/papers/2012/Chingovska_IEEEBIOSIG2012_2012.pdf},<br>  }<br><br>If you wish to report problems or improvements concerning this code, please<br>contact the authors of the above mentioned papers.<br><br>Raw data<br>--------<br><br>The data used in the paper is publicly available and should be downloaded and<br>installed **prior** to try using the programs described in this package. Visit<br>`the PHOTO-ATTACK database portal &lt;https://www.idiap.ch/dataset/photoattack&gt;`_<br>for more information.<br><br>Installation<br>------------<br><br>.. note::<br><br>  If you are reading this page through our GitHub portal and not through PyPI,<br>  note **the development tip of the package may not be stable** or become<br>  unstable in a matter of moments.<br><br>  Go to `http://pypi.python.org/pypi/antispoofing.optflow<br>  &lt;http://pypi.python.org/pypi/antispoofing.optflow&gt;`_ to download the latest<br>  stable version of this package.<br><br>There are 2 options you can follow to get this package installed and<br>operational on your computer: you can use automatic installers like `pip<br>&lt;http://pypi.python.org/pypi/pip/&gt;`_ (or `easy_install<br>&lt;http://pypi.python.org/pypi/setuptools&gt;`_) or manually download, unpack and<br>use `zc.buildout &lt;http://pypi.python.org/pypi/zc.buildout&gt;`_ to create a<br>virtual work environment just for this package.<br><br>Using an automatic installer<br>============================<br><br>Using ``pip`` is the easiest (shell commands are marked with a ``$`` signal)::<br><br>  $ pip install antispoofing.optflow<br><br>You can also do the same with ``easy_install``::<br><br>  $ easy_install antispoofing.optflow<br><br>This will download and install this package plus any other required<br>dependencies. It will also verify if the version of Bob you have installed<br>is compatible.<br><br>This scheme works well with virtual environments by `virtualenv<br>&lt;http://pypi.python.org/pypi/virtualenv&gt;`_ or if you have root access to your<br>machine. Otherwise, we recommend you use the next option.<br><br>Using ``zc.buildout``<br>=====================<br><br>Download the latest version of this package from `PyPI<br>&lt;http://pypi.python.org/pypi/antispoofing.optflow&gt;`_ and unpack it in your<br>working area. The installation of the toolkit itself uses `buildout<br>&lt;http://www.buildout.org/&gt;`_. You don't need to understand its inner workings<br>to use this package. Here is a recipe to get you started::<br><br>  $ python bootstrap.py<br>  $ ./bin/buildout<br><br>These 2 commands should download and install all non-installed dependencies and<br>get you a fully operational test and development environment.<br><br>.. note::<br><br>  The python shell used in the first line of the previous command set<br>  determines the python interpreter that will be used for all scripts developed<br>  inside this package. Because this package makes use of `Bob<br>  &lt;http://idiap.github.com/bob&gt;`_, you must make sure that the ``bootstrap.py``<br>  script is called with the **same** interpreter used to build Bob, or<br>  unexpected problems might occur.<br><br>  If Bob is installed by the administrator of your system, it is safe to<br>  consider it uses the default python interpreter. In this case, the above 3<br>  command lines should work as expected. If you have Bob installed somewhere<br>  else on a private directory, edit the file ``buildout.cfg`` **before**<br>  running ``./bin/buildout``. Find the section named ``buildout`` and edit or<br>  add the line ``prefixes`` to point to the directory where Bob is installed or<br>  built. For example::<br><br>    [buildout]<br>    ...<br>    prefixes=/Users/crazyfox/work/bob/build<br><br>User Guide<br>----------<br><br>It is assumed you have followed the installation instructions for the package<br>and got this package installed and the PHOTO-ATTACK database downloaded and<br>uncompressed in a directory. You should have all required utilities sitting<br>inside a binary directory depending on your installation strategy (utilities<br>will be inside the ``bin`` if you used the buildout option). We expect that the<br>video files downloaded for the PHOTO-ATTACK database are installed in a<br>sub-directory called ``database`` at the root of the package.  You can use a<br>link to the location of the database files, if you don't want to have the<br>database installed on the root of this package::<br><br>  $ ln -s /path/where/you/installed/the/photo-attack-database database<br><br>If you don't want to create a link, use the ``--input-dir`` flag to specify<br>the root directory containing the database files. That would be the directory<br>that *contains* the sub-directories ``train``, ``test``, ``devel`` and<br>``face-locations``.<br><br>Paper Layout: How to Reproduce our Results<br>==========================================<br><br>The paper studies 4 algorithms (the first 3 are published elsewhere and are<br>**not** a contribution to this paper):<br><br>Reference System 1 (RS1) - Kollreider's Optical Flow anti-spoofing::<br><br>  @article{Kollreider_2009,<br>    author={K. Kollreider AND H. Fronthaler AND J. Bigun},<br>    title={Non-intrusive liveness detection by face images},<br>    volume={27},<br>    number={3},<br>    journal={Image and Vision Computing},<br>    publisher={Elsevier B.V.},<br>    year={2009},<br>    pages={233--244},<br>  }<br><br>Reference System 2 (RS2) - Bao's Optical Flow anti-spoofing::<br><br>  @inproceedings{Bao_2009,<br>    author={Wei Bao AND H. Li AND Nan Li AND Wei Jiang},<br>    title={A liveness detection method for face recognition based on optical flow field},<br>    booktitle={2009 International Conference on Image Analysis and Signal Processi<br>  ng},<br>    publisher={IEEE},<br>    year={2009},<br>    pages={233--236},<br>  }<br><br>Reference System 3 (RS3) - Our own Frame Difference's based anti-spoofing::<br><br>  @inproceedings{Anjos_IJCB_2011,<br>    author = {Anjos, Andr{\\'{e}} and Marcel, S{\\'{e}}bastien},<br>    keywords = {Attack, Counter-Measures, Counter-Spoofing, Disguise, Dishonest Acts, Face Recognition, Face Verification, Forgery, Liveness Detection, Replay, Spoofing, Trick},<br>    month = oct,<br>    title = {Counter-Measures to Photo Attacks in Face Recognition: a public database and a baseline},<br>    booktitle = {International Joint Conference on Biometrics 2011},<br>    year = {2011},<br>    url = {http://publications.idiap.ch/downloads/papers/2011/Anjos_IJCB_2011.pdf}<br>  }<br><br>The final algorithm based on Optical Flow Correlation (OFC) **represents our<br>contribution in this paper**.<br><br>To reproduce the results for RS3, you can follow the instructions on `its own<br>satellite package for Bob &lt;http://pypi.python.org/pypi/antispoofing.motion&gt;`_.<br>The scripts for that package should be auto-generated and made available for<br>you under your ``bin`` as well (this package depends on that one).<br><br>In this manual, we address how to extract results for RS1, 2 and OFC, which<br>operate on the top of a previously estimated Optical Flow (OF) field.  OF is,<br>therefore, the first topic in this manual.<br><br>Extract the Optical Flow Features<br>=================================<br><br>We ship this package with a preset to use `Ce Liu's OF framework<br>&lt;http://pypi.python.org/pypi/xbob.optflow.liu&gt;`_. This is of course not<br>required, but it is the framework we have tested our method with, therefore the<br>one we recommend you to start using. This framework estimates the *dense* OF<br>field between any two successive frames. It is quite slow. Be warned, it may<br>take quite some time to get through all the videos. To run the extraction<br>sequentially, for all videos, use the following command::<br><br>  $ ./bin/optflow_estimate.py --verbose /root/of/database results/flows replay --protocol=photo<br><br>.. note::<br><br>  The command line above is going to take a lot of time to complete. You may<br>  need to parallelize the job. If you are Idiap, you can use the ``gridtk``<br>  package, which should be downloaded and installed on your current<br>  environment::<br><br>    $ ./bin/jman submit --array=800 --queue=q1d ./bin/optflow_estimate.py --verbose /root/of/database results/flows replay --protocol=photo<br><br>  The ``magic`` number of ``800`` entries can be found by executing::<br><br>    $ ./bin/optflow_estimate.py --grid-count replay --protocol=photo<br><br>  Which just prints the number of jobs it requires for the grid execution. Each<br>  job is consuming less than 2 gigabytes of RAM, but more than 1 gigabyte.<br>  Therefore, you must choose the right queue and may need to set memory<br>  requirements for the machines you will be running at.<br><br>.. note::<br><br>  In case you want to replace this phase by another algorithm of your own.<br>  Notice that the output format is 1 HDF5 file per input video in the database,<br>  organized in the same way as in the original database. Each input video<br>  should contain a single 4D 64-bit float point array which has the following<br>  shape: (frames, u+v, height, width). The second dimension corresponds to the<br>  U (horizontal) and V (vertical) velocities as output by your algorithm, for<br>  every pixel in the image. If you have not used a dense OF estimator, please<br>  extrapolate yourself the values before calling the next scripts.<br><br>  If you respect this requirement, than you can test the results of this<br>  framework with any OF estimation technique of your choice.<br><br>Once you are in possession of the flow fields. You can start calculating the<br>scores required by each of the methods reviewed in the paper. It can help in<br>terms of processing speed to have the features located on a local hard-drive.<br>The HDF5 files tend to be huge.<br><br>.. important::<br><br>  Depending on the version of FFmpeg you have installed on your platform when<br>  you estimate the OF, you may get slightly different results at this step.<br>  These are due to imprecisions on the video decoding.<br><br>  You can also use the Matlab version of Ce Liu's code directly to produce the<br>  flow fields. In this case, you may also find small differences on the<br>  estimated velocities. The differences are due to the movie decoding and<br>  gray-scale conversion, which are different than Bob's.<br><br>  In any of these conditions, our tests show these do not affect the overall<br>  performance of our method. It may slightly change the final results you can<br>  obtain.<br><br>Reference System 1: Scores from Kollreider's<br>============================================<br><br>To calculate scores using Kollreider's method, use the script<br>``optflow_kollreider.py`` in the following way::<br><br>  $ ./bin/optflow_kollreider.py --verbose /root/of/database results/flows results/kollreider replay --protocol=photo<br><br>You can modify the :math:`\\tau` parameter required by the method with the<br>program option ``--tau=&lt;float-value&gt;``. By default, this parameter is set to<br>``1.0``. Refer to the original paper by Kollreider to understand the meaning<br>and how to tune this parameter. If you tune the parameter and execute the error<br>analysis as explained below, you will get to the results shown on Table 1 of<br>our paper.<br><br>.. note::<br><br>  The above program can be somewhat slow, even if it is much faster than the<br>  flow field estimation itself. If you want to speed things, up, just run it on<br>  the grid::<br><br>    $ ./bin/jman submit --array=800 ./bin/optflow_kollreider.py --verbose /root/of/database results/flows results/kollreider replay --protocol=photo<br><br>  The program ``optflow_kollreider.py`` can also print the number of jobs it can<br>  be broken into with the ``--grid-count`` option::<br><br>    ./bin/optflow_kollreider.py --grid-count replay --protocol=photo<br><br>.. important::<br><br>  We estimate the position of the face center and the ears based on the<br>  bounding-box provided by the face locations. This way, we could compare all<br>  algorithms using the same input set. We have not tested if specialized<br>  key-point localizers would give better results than ours for this method.<br><br>Besides generating output for the tests in the paper, you can also generate an<br>annotated video, showing how our extrapolation of the face bounding boxes work<br>for finding out the regions of interest to apply Kollreider's work on. To do<br>this, use the script `optflow_kollreider_annotate.py`. It works on a similar<br>way to the above script and will process the whole database if not told<br>otherwise. This can be somewhat long as well, but you can grid-fy it if you<br>wish or use filtering options for the database to limit the number of videos<br>analysed. For example::<br><br>  $ bin/optflow_kollreider_annotate.py -v /idiap/group/replay/database/protocols/replayattack-database tmp replay --protocol=photo --client=101 --light=adverse<br><br>Reference System 2: Scores from Bao's<br>=====================================<br><br>To calculate scores for Bao's method, use the script<br>``optflow_bao.py`` in the following way::<br><br>  $ ./bin/optflow_bao.py --verbose /root/of/database results/flows results/bao replay --protocol=photo<br><br>You can modify the border parameter required by the method with the program<br>option ``--border=&lt;integer-value&gt;``. By default, this parameter is set to ``5``<br>(pixels). The original paper by Bao and others does not suggest such a<br>parameter or mention how does the face bounding-boxes are set. We assume a<br>default value of pixels surrounding our detected face. In the paper, we scan<br>this value from ``0`` (zero) to a number of pixels to test the method. If you<br>tune the parameter and execute the error analysis as explained below, you will<br>get to the results shown on Table 2 of our paper.<br><br>.. note::<br><br>  The above program can be somewhat slow, even if it is much faster than the<br>  flow field estimation itself. If you want to speed things, up, just run it on<br>  the grid::<br><br>    $ ./bin/jman submit --array=800 ./bin/optflow_bao.py --verbose /root/of/database results/flows results/bao replay --protocol=photo<br><br>  The program ``optflow_bao.py`` can also print the number of jobs it can<br>  be broken into with the ``--grid-count`` option::<br><br>    ./bin/optflow_bao.py --grid-count replay --protocol=photo<br><br>Reference System 3: Frame-differences<br>=====================================<br><br>As mentioned before, you should follow the instructions on `its own satellite<br>package for Bob`_. The scripts for that package should be auto-generated and<br>made available for you under your ``bin`` as well (this package depends on that<br>one).<br><br>Optical Flow Correlation (OFC)<br>==============================<br><br>To reproduce the results on our paper, you will need first to generate the<br>scores for the :math:`\\chi^2` comparison for every frame in the sequence.<br>Frames with no faces detected generate a score valued ``numpy.NaN``, similar to<br>other counter-measures implemented by our group. To generate each score per<br>frame, you can use the application ``optflow_histocomp.py``::<br><br>  $ ./bin/optflow_histocomp.py --verbose /root/of/database results/flows results/histocomp replay --protocol=photo<br><br>.. note::<br><br>  The above program can be somewhat slow, even if it is much faster than the<br>  flow field estimation itself. If you want to speed things, up, just run it on<br>  the grid::<br><br>    $ ./bin/jman submit --array=800 ./bin/optflow_histocomp.py --verbose /root/of/database results/flows results/histocomp replay --protocol=photo<br><br>  The program ``optflow_histocomp.py`` can also print the number of jobs it can<br>  be broken into with the ``--grid-count`` option::<br><br>    ./bin/optflow_histocomp.py --grid-count replay --protocol=photo<br><br>You can generate the results in Figure 5 and 6 of our paper by setting 2<br>parameters on the above script:<br><br>  ``--number-of-bins``<br>    This changes the parameter :math:`Q`, explained on the paper, related to<br>    the quantization of the angle space. (see results in Figure 5.)<br><br>  ``--offset``<br>    This changes the offset for the quantization. Its effect is studied in<br>    Figure 6, for ``--number-of-bins=2``, as explained in the paper.<br><br>By modifying the above parameters and executing an error analysis as<br>explained bellow, with ``--window-size=220``, you will get to the results<br>plotted.<br><br>Error Analysis<br>==============<br><br>Once the scores you want to analyze are produced by one of the methods above,<br>you can calculate the error on the database using the application<br>``score_analysis.py``. This program receives one directory (containing the<br>scores output by a given method) and produces a console analysis of such a<br>method, which is used by the paper::<br><br>  $ ./bin/score_analysis.py results/histocomp replay --protocol=photo<br><br>That command will calculate a development set threshold at the Equal Error Rate<br>(EER) and will apply it to the test set, reporting errors on both sets. A<br>typical output would be like this::<br><br>  Input data: /idiap/temp/aanjos/spoofing/scores/optflow_histocomp<br>  Thres. at EER of development set: 6.9459e-02<br>  [EER @devel] FAR: 37.04% (15601 / 42120) | FRR: 37.04% (8312 / 22440) | HTER: 37.04%<br>  [HTER @test] FAR: 37.11% (20843 / 56160) | FRR: 35.75% (10696 / 29920) | HTER: 36.43%<br><br>The error analysis program considers, by default, every frame analyzed as **an<br>individual (independent) observation** and calculates the error rates based on<br>the overall set of frames found on the whole development and test sets. The<br>numbers printed inside the parentheses indicate how many frames were evaluated<br>in each set (denominator) and how many of those contributed to the percentage<br>displayed (numerator). The Half-Total Error Rate (HTER) is evaluated for both<br>the development set and test sets. The HTER for the develpment set is equal to<br>the EER on the same set, naturally.<br><br>The ``score_analysis.py`` script contains 2 parameters that can be used to<br>fine-tune the program behaviour, to be known:<br><br>  ``--window-size=&lt;integer&gt;``<br><br>    Defines a window size to which the scores are going to be averaged to,<br>    within the same score sequence. So, for example, if one of the files<br>    produced by ``optflow_histocomp.py`` contains a sequence of scores that<br>    reads like ``[1.0, 2.0, 1.5, 3.5, 0.5]``, and the window-size parameter is<br>    set to 2, then, the scores evaluated by this procedure are ``[1.5, 1.75,<br>    2.5, 2.0]``, which represent the averages of ``[1.0, 2.0]``, ``[2.0,<br>    1.5]``, ``[1.5, 3.5]`` and ``[3.5, 0.5]``.<br><br>  ``--overlap=&lt;integer&gt;``<br><br>    Controls the amount of overlap between the windows. If not set, the default<br>    overlap is set to ``window-size - 1``. You can modify this behaviour by<br>    setting this parameter to a different value. Taking the example above, if<br>    you set the window-size to 2 and the overlap to zero, then the score set<br>    produced by this analysis would be ``[1.5, 2.5]``. Notice that the frame<br>    value ``0.5`` (the last of the sequence) is ignored.<br><br>You will observe the effect of setting the window-size on the score analysis by<br>looking at the number of *averaged frames* analyzed::<br><br>  $ ./bin/score_analysis.py --window-size=220 --overlap=80 results/histocomp replay --protocol=photo<br><br>And the output::<br><br>  Input data: /idiap/temp/aanjos/spoofing/scores/optflow_histocomp<br>  Window size: 220 (overlap = 80)<br>  Thres. at EER of development set: 1.4863e-01<br>  [EER @devel] FAR: 2.78% (5 / 180) | FRR: 2.50% (3 / 120) | HTER: 2.64%<br>  [HTER @test] FAR: 2.92% (7 / 240) | FRR: 1.88% (3 / 160) | HTER: 2.40%<br><br>You can generate the results in Figure 7 and Table III on the paper just by<br>manipulating this program.<br><br>Our paper also shows a break-down analysis (by device attack type and support)<br>on Figure 8 (last figure). To generate such a figure, one must produce the<br>break-down analysis per device (Figure 8.a) and attack support (Figure 8.b). To<br>do this, pass the ``--breakdown`` option to the ``score_analysis.py`` script::<br><br>  $ ./bin/score_analysis.py --window-size=220 --overlap=80 --breakdown results/histocomp replay --protocol=photo<br><br>Our paper also discusses the impact of skipping the OF calculation on certain<br>frames (see Discussion section) in the interest of saving computational<br>resources. You can generate the table presented at the paper by playing with<br>the ``--skip`` parameter of ``score_analysis.py``. By default, we don't skip<br>any frames, if you set this parameter to 1, then we'd skip every other frame.<br>If you set it to 2, then we only consider 1 in every 3 frames, and so on.<br><br>Problems<br>--------<br><br>In case of problems, please contact any of the authors of the paper.\n          </div>"}, "last_serial": 1596136, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "e25886553dbef4388dd2c0e48ed19501", "sha256": "a43725813620325f50ad7ec97fab84a76e31bffc85e28947c96e84ebca2ea7fe"}, "downloads": -1, "filename": "antispoofing.optflow-1.0.0.zip", "has_sig": false, "md5_digest": "e25886553dbef4388dd2c0e48ed19501", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 51556, "upload_time": "2013-04-04T11:46:08", "upload_time_iso_8601": "2013-04-04T11:46:08.516205Z", "url": "https://files.pythonhosted.org/packages/6f/60/6989b1753ff1214217ebe62e17a53e9f43f483c7ad33b708b102e04534d9/antispoofing.optflow-1.0.0.zip", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "ca3f2b37445d69c1f113e401282a8451", "sha256": "5580fe5990819f7fc4709313348da22c2f80b692a2572e09aba1a14b6461f154"}, "downloads": -1, "filename": "antispoofing.optflow-1.0.1.zip", "has_sig": false, "md5_digest": "ca3f2b37445d69c1f113e401282a8451", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 52650, "upload_time": "2013-07-04T08:07:19", "upload_time_iso_8601": "2013-07-04T08:07:19.904288Z", "url": "https://files.pythonhosted.org/packages/43/d3/ada745c6e1d718035463ee74159a7b04a7d856aabcd31cc0022878f53afe/antispoofing.optflow-1.0.1.zip", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "b6eee3273620cf8bfda3ea44a0a48678", "sha256": "2a9641b30bd5ec291be40936d241c6d0e8c39ae44e24de1ab9fc7a680e81b38b"}, "downloads": -1, "filename": "antispoofing.optflow-1.0.2.zip", "has_sig": false, "md5_digest": "b6eee3273620cf8bfda3ea44a0a48678", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 52759, "upload_time": "2013-07-19T12:13:43", "upload_time_iso_8601": "2013-07-19T12:13:43.126284Z", "url": "https://files.pythonhosted.org/packages/52/91/944236ad04eeae585ab36c6e255ac5ccaf60777041de40101d4c51072241/antispoofing.optflow-1.0.2.zip", "yanked": false}], "1.0.5": [{"comment_text": "", "digests": {"md5": "e8dea224d74953692ad55d1c4e08d777", "sha256": "f1b39dcd295ce7444ab259767a74a710397e947f44b98725b517a556e89b153f"}, "downloads": -1, "filename": "antispoofing.optflow-1.0.5.zip", "has_sig": false, "md5_digest": "e8dea224d74953692ad55d1c4e08d777", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 52867, "upload_time": "2015-02-25T10:54:14", "upload_time_iso_8601": "2015-02-25T10:54:14.352557Z", "url": "https://files.pythonhosted.org/packages/7e/81/89797e0589dcd9c3a29e8523f0b648819467d364acae46b31be795265d76/antispoofing.optflow-1.0.5.zip", "yanked": false}], "2.0.0": [{"comment_text": "", "digests": {"md5": "4f86666c97dcd1ad595bbe7c8dca0a31", "sha256": "1206e2d859b66675c1e433705fd5fed0ffa09af794da61f104437f94a54ac49a"}, "downloads": -1, "filename": "antispoofing.optflow-2.0.0.zip", "has_sig": false, "md5_digest": "4f86666c97dcd1ad595bbe7c8dca0a31", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 50674, "upload_time": "2015-06-17T16:36:43", "upload_time_iso_8601": "2015-06-17T16:36:43.503230Z", "url": "https://files.pythonhosted.org/packages/7f/bc/8ad966b6be1b6a5f59fc16a721893856032be78de2e9ad39c80bab11c58f/antispoofing.optflow-2.0.0.zip", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "4f86666c97dcd1ad595bbe7c8dca0a31", "sha256": "1206e2d859b66675c1e433705fd5fed0ffa09af794da61f104437f94a54ac49a"}, "downloads": -1, "filename": "antispoofing.optflow-2.0.0.zip", "has_sig": false, "md5_digest": "4f86666c97dcd1ad595bbe7c8dca0a31", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 50674, "upload_time": "2015-06-17T16:36:43", "upload_time_iso_8601": "2015-06-17T16:36:43.503230Z", "url": "https://files.pythonhosted.org/packages/7f/bc/8ad966b6be1b6a5f59fc16a721893856032be78de2e9ad39c80bab11c58f/antispoofing.optflow-2.0.0.zip", "yanked": false}], "timestamp": "Thu May  7 18:17:59 2020"}