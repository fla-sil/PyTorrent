{"info": {"author": "Fran\u00e7ois-Guillaume Fernandez", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Mathematics", "Topic :: Software Development", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "\n# Torchscan: meaningful module insights\n\n[![License](https://img.shields.io/badge/License-MIT-brightgreen.svg)](LICENSE) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/c38368837c2b4a91a59bed8b95c1c19c)](https://www.codacy.com/manual/frgfm/torch-scan?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=frgfm/torch-scan&amp;utm_campaign=Badge_Grade)  ![Build Status](https://github.com/frgfm/torch-scan/workflows/python-package/badge.svg) [![codecov](https://codecov.io/gh/frgfm/torch-scan/branch/master/graph/badge.svg)](https://codecov.io/gh/frgfm/torch-scan) [![Docs](https://img.shields.io/badge/docs-available-blue.svg)](https://frgfm.github.io/torch-scan)\n\nThe very useful [summary](https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary) method of `tf.keras.Model` but for PyTorch, with more useful information.\n\n\n\n## Table of Contents\n\n* [Getting Started](#getting-started)\n  * [Prerequisites](#prerequisites)\n  * [Installation](#installation)\n* [Usage](#usage)\n* [Benchmark](#benchmark)\n* [Technical Roadmap](#technical-roadmap)\n* [Documentation](#documentation)\n* [Contributing](#contributing)\n* [Credits](#credits)\n* [License](#license)\n\n\n\n## Getting started\n\n### Prerequisites\n\n- Python 3.6 (or more recent)\n- [pip](https://pip.pypa.io/en/stable/)\n\n### Installation\n\nYou can install the package using [pypi](https://pypi.org/project/torch-scan/) as follows:\n\n```shell\npip install torchscan\n```\n\n\n\n## Usage\n\nSimilarly to the `torchsummary` implementation, `torchscan` brings useful module information into readable format. For nested complex architectures, you can use a maximum depth of display as follows:\n\n```python\nfrom torchvision.models import densenet121\nfrom torchscan import summary\n\nmodel = densenet121().eval().cuda()\nsummary(model, (3, 224, 224), max_depth=2)\n```\n\nwhich would yield\n\n```shell\n__________________________________________________________________________________________\nLayer                        Type                  Output Shape              Param #        \n==========================================================================================\ndensenet                     DenseNet              (-1, 1000)                0              \n\u251c\u2500features                   Sequential            (-1, 1024, 7, 7)          0              \n|    \u2514\u2500conv0                 Conv2d                (-1, 64, 112, 112)        9,408          \n|    \u2514\u2500norm0                 BatchNorm2d           (-1, 64, 112, 112)        257            \n|    \u2514\u2500relu0                 ReLU                  (-1, 64, 112, 112)        0              \n|    \u2514\u2500pool0                 MaxPool2d             (-1, 64, 56, 56)          0              \n|    \u2514\u2500denseblock1           _DenseBlock           (-1, 256, 56, 56)         338,316        \n|    \u2514\u2500transition1           _Transition           (-1, 128, 28, 28)         33,793         \n|    \u2514\u2500denseblock2           _DenseBlock           (-1, 512, 28, 28)         930,072        \n|    \u2514\u2500transition2           _Transition           (-1, 256, 14, 14)         133,121        \n|    \u2514\u2500denseblock3           _DenseBlock           (-1, 1024, 14, 14)        2,873,904      \n|    \u2514\u2500transition3           _Transition           (-1, 512, 7, 7)           528,385        \n|    \u2514\u2500denseblock4           _DenseBlock           (-1, 1024, 7, 7)          2,186,272      \n|    \u2514\u2500norm5                 BatchNorm2d           (-1, 1024, 7, 7)          4,097          \n\u251c\u2500classifier                 Linear                (-1, 1000)                1,025,000      \n==========================================================================================\nTrainable params: 7,978,856\nNon-trainable params: 0\nTotal params: 7,978,856\n------------------------------------------------------------------------------------------\nModel size (params + buffers): 30.76 Mb\nFramework & CUDA overhead: 423.57 Mb\nTotal RAM usage: 454.32 Mb\n------------------------------------------------------------------------------------------\nFloating Point Operations on forward: 5.74 GFLOPs\nMultiply-Accumulations on forward: 2.87 GMACs\nDirect memory accesses on forward: 2.90 GDMAs\n__________________________________________________________________________________________\n```\n\nResults are aggregated to the selected depth for improved readability.\n\nFor reference, here are explanations of a few acronyms:\n\n- **FLOPs**: floating-point operations (not to be confused with FLOPS which is FLOPs per second)\n- **MACs**: mutiply-accumulate operations (cf. [wikipedia](https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation))\n- **DMAs**: direct memory accesses (many argue that it is more relevant than FLOPs or MACs to compare model inference speeds cf. [wikipedia](https://en.wikipedia.org/wiki/Direct_memory_access))\n\n\n\n##\u00a0Benchmark\n\nBelow are the results for classification models supported by `torchvision` for a single image with 3 color channels of size `224x224` (apart from  `inception_v3`   which uses `299x299`).\n\n| Model              | Params (M) | FLOPs (G) | MACs (G) | DMAs (G) |\n| ------------------ | ---------- | --------- | -------- | -------- |\n| alexnet            | 61.1       | 1.43      | 0.71     | 0.72     |\n| googlenet          | 6.62       | 3.01      | 1.51     | 1.53     |\n| vgg11              | 132.86     | 15.23     | 7.61     | 7.64     |\n| vgg11_bn           | 132.87     | 15.26     | 7.63     | 7.66     |\n| vgg13              | 133.05     | 22.63     | 11.31    | 11.35    |\n| vgg13_bn           | 133.05     | 22.68     | 11.33    | 11.37    |\n| vgg16              | 138.36     | 30.96     | 15.47    | 15.52    |\n| vgg16_bn           | 138.37     | 31.01     | 15.5     | 15.55    |\n| vgg19              | 143.67     | 39.28     | 19.63    | 19.69    |\n| vgg19_bn           | 143.68     | 39.34     | 19.66    | 19.72    |\n| resnet18           | 11.69      | 3.64      | 1.82     | 1.84     |\n| resnet34           | 21.8       | 7.34      | 3.67     | 3.7      |\n| resnet50           | 25.56      | 8.21      | 4.11     | 4.15     |\n| resnet101          | 44.55      | 15.66     | 7.83     | 7.9      |\n| resnet152          | 60.19      | 23.1      | 11.56    | 11.65    |\n| inception_v3       | 27.16      | 11.45     | 5.73     | 5.76     |\n| squeezenet1_0      | 1.25       | 1.64      | 0.82     | 0.83     |\n| squeezenet1_1      | 1.24       | 0.7       | 0.35     | 0.36     |\n| wide_resnet50_2    | 68.88      | 22.84     | 11.43    | 11.51    |\n| wide_resnet101_2   | 126.89     | 45.58     | 22.8     | 22.95    |\n| densenet121        | 7.98       | 5.74      | 2.87     | 2.9      |\n| densenet161        | 28.68      | 15.59     | 7.79     | 7.86     |\n| densenet169        | 14.15      | 6.81      | 3.4      | 3.44     |\n| densenet201        | 20.01      | 8.7       | 4.34     | 4.39     |\n| resnext50_32x4d    | 25.03      | 8.51      | 4.26     | 4.3      |\n| resnext101_32x8d   | 88.79      | 32.93     | 16.48    | 16.61    |\n| mobilenet_v2       | 3.5        | 0.63      | 0.31     | 0.32     |\n| shufflenet_v2_x0_5 | 1.37       | 0.09      | 0.04     | 0.05     |\n| shufflenet_v2_x1_0 | 2.28       | 0.3       | 0.15     | 0.15     |\n| shufflenet_v2_x1_5 | 3.5        | 0.6       | 0.3      | 0.31     |\n| shufflenet_v2_x2_0 | 7.39       | 1.18      | 0.59     | 0.6      |\n| mnasnet0_5         | 2.22       | 0.22      | 0.11     | 0.12     |\n| mnasnet0_75        | 3.17       | 0.45      | 0.23     | 0.24     |\n| mnasnet1_0         | 4.38       | 0.65      | 0.33     | 0.34     |\n| mnasnet1_3         | 6.28       | 1.08      | 0.54     | 0.56     |\n\nThe above results were produced using the `scripts/benchmark.py` script.\n\n\n\n## Technical roadmap\n\nThe project is currently under development, here are the objectives for the next releases:\n\n- [x] Support of `torch.nn.Module` layers: ConvTranspose, Identity.\n\n- [ ] Package distribution: add a conda package.\n\n- [ ] Shared parameter support (cf. [discussion](https://discuss.pytorch.org/t/repeated-model-layers-real-or-torchsummary-bug/26489))\n\n- [ ] Result exporting: add a csv export option.\n\n- [ ] Forward pass stat support: RAM usage for each layer, on forward pass.\n\n- [ ] Backward pass stat support: RAM usage for each layer, on backward pass.\n\n- [ ] Support of `torch.nn.Module` layers: GroupNorm, Upsample, PixelShuffle, RNN, LSTM, GRU, Embedding, Transformer.\n\n- [ ] Support of scripted modules\n\n- [ ] Support of `torch.nn` functional API\n\n- [ ] I/O handling: multiple inputs or outputs, non-tensor I/O\n\n- [ ] Add computational graph (cf. [pytorchviz](https://github.com/szagoruyko/pytorchviz))\n\n\n\n## Documentation\n\nThe full package documentation is available [here](<https://frgfm.github.io/torch-scan/>) for detailed specifications. The documentation was built with [Sphinx](sphinx-doc.org) using a [theme](github.com/readthedocs/sphinx_rtd_theme) provided by [Read the Docs](readthedocs.org).\n\n\n\n## Contributing\n\nPlease refer to `CONTRIBUTING` if you wish to contribute to this project.\n\n\n\n## Credits\n\nThis project is developed and maintained by the repo owner, but the implementation was inspired or helped by the following contributions:\n\n- [Pytorch summary](https://github.com/sksq96/pytorch-summary): existing PyTorch porting of `tf.keras.Model.summary`\n- [Torchstat](https://github.com/Swall0w/torchstat): another module inspection tool\n- [Flops counter Pytorch](https://github.com/sovrasov/flops-counter.pytorch): operation counter tool\n- [THOP](https://github.com/Lyken17/pytorch-OpCounter): PyTorch Op counter\n- Number of operations and memory estimation articles by [Matthijs Hollemans](https://machinethink.net/blog/how-fast-is-my-model/), and [Sicara](https://www.sicara.ai/blog/2019-28-10-deep-learning-memory-usage-and-pytorch-optimization-tricks)\n- [Pruning Convolutional Neural Networks for Resource Efficient Inference](https://arxiv.org/abs/1611.06440)\n\n\n\n## License\n\nDistributed under the MIT License. See `LICENSE` for more information.\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/frgfm/torch-scan/tags", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/frgfm/torch-scan", "keywords": "pytorch,deep learning,summary,memory,ram", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "torchscan", "package_url": "https://pypi.org/project/torchscan/", "platform": "", "project_url": "https://pypi.org/project/torchscan/", "project_urls": {"Download": "https://github.com/frgfm/torch-scan/tags", "Homepage": "https://github.com/frgfm/torch-scan"}, "release_url": "https://pypi.org/project/torchscan/0.1.0/", "requires_dist": ["torch (>=1.1.0)"], "requires_python": ">=3.6.0", "summary": "Useful information about your Pytorch module", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Torchscan: meaningful module insights</h1>\n<p><a href=\"LICENSE\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/13402995be86cde517cc34ba2cc02d3de74b86c4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d627269676874677265656e2e737667\"></a> <a href=\"https://www.codacy.com/manual/frgfm/torch-scan?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=frgfm/torch-scan&amp;utm_campaign=Badge_Grade\" rel=\"nofollow\"><img alt=\"Codacy Badge\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5c81243719acd5aa49b382bbc0e2e4c86057f76c/68747470733a2f2f6170692e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f6333383336383833376332623461393161353962656438623935633163313963\"></a>  <img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/295f1a05e26313d56891815432b07ba6e7957cb3/68747470733a2f2f6769746875622e636f6d2f667267666d2f746f7263682d7363616e2f776f726b666c6f77732f707974686f6e2d7061636b6167652f62616467652e737667\"> <a href=\"https://codecov.io/gh/frgfm/torch-scan\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/dc842ab968fdde0ac710b6b58305e74c1b681284/68747470733a2f2f636f6465636f762e696f2f67682f667267666d2f746f7263682d7363616e2f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a> <a href=\"https://frgfm.github.io/torch-scan\" rel=\"nofollow\"><img alt=\"Docs\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/80e1a75e270226d8519827e9772efdc9697c54a8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d617661696c61626c652d626c75652e737667\"></a></p>\n<p>The very useful <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary\" rel=\"nofollow\">summary</a> method of <code>tf.keras.Model</code> but for PyTorch, with more useful information.</p>\n<h2>Table of Contents</h2>\n<ul>\n<li><a href=\"#getting-started\" rel=\"nofollow\">Getting Started</a>\n<ul>\n<li><a href=\"#prerequisites\" rel=\"nofollow\">Prerequisites</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n</ul>\n</li>\n<li><a href=\"#usage\" rel=\"nofollow\">Usage</a></li>\n<li><a href=\"#benchmark\" rel=\"nofollow\">Benchmark</a></li>\n<li><a href=\"#technical-roadmap\" rel=\"nofollow\">Technical Roadmap</a></li>\n<li><a href=\"#documentation\" rel=\"nofollow\">Documentation</a></li>\n<li><a href=\"#contributing\" rel=\"nofollow\">Contributing</a></li>\n<li><a href=\"#credits\" rel=\"nofollow\">Credits</a></li>\n<li><a href=\"#license\" rel=\"nofollow\">License</a></li>\n</ul>\n<h2>Getting started</h2>\n<h3>Prerequisites</h3>\n<ul>\n<li>Python 3.6 (or more recent)</li>\n<li><a href=\"https://pip.pypa.io/en/stable/\" rel=\"nofollow\">pip</a></li>\n</ul>\n<h3>Installation</h3>\n<p>You can install the package using <a href=\"https://pypi.org/project/torch-scan/\" rel=\"nofollow\">pypi</a> as follows:</p>\n<pre>pip install torchscan\n</pre>\n<h2>Usage</h2>\n<p>Similarly to the <code>torchsummary</code> implementation, <code>torchscan</code> brings useful module information into readable format. For nested complex architectures, you can use a maximum depth of display as follows:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">torchvision.models</span> <span class=\"kn\">import</span> <span class=\"n\">densenet121</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torchscan</span> <span class=\"kn\">import</span> <span class=\"n\">summary</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">densenet121</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n<span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">),</span> <span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n</pre>\n<p>which would yield</p>\n<pre>__________________________________________________________________________________________\nLayer                        Type                  Output Shape              Param <span class=\"c1\">#        </span>\n<span class=\"o\">==========================================================================================</span>\ndensenet                     DenseNet              <span class=\"o\">(</span>-1, <span class=\"m\">1000</span><span class=\"o\">)</span>                <span class=\"m\">0</span>              \n\u251c\u2500features                   Sequential            <span class=\"o\">(</span>-1, <span class=\"m\">1024</span>, <span class=\"m\">7</span>, <span class=\"m\">7</span><span class=\"o\">)</span>          <span class=\"m\">0</span>              \n<span class=\"p\">|</span>    \u2514\u2500conv0                 Conv2d                <span class=\"o\">(</span>-1, <span class=\"m\">64</span>, <span class=\"m\">112</span>, <span class=\"m\">112</span><span class=\"o\">)</span>        <span class=\"m\">9</span>,408          \n<span class=\"p\">|</span>    \u2514\u2500norm0                 BatchNorm2d           <span class=\"o\">(</span>-1, <span class=\"m\">64</span>, <span class=\"m\">112</span>, <span class=\"m\">112</span><span class=\"o\">)</span>        <span class=\"m\">257</span>            \n<span class=\"p\">|</span>    \u2514\u2500relu0                 ReLU                  <span class=\"o\">(</span>-1, <span class=\"m\">64</span>, <span class=\"m\">112</span>, <span class=\"m\">112</span><span class=\"o\">)</span>        <span class=\"m\">0</span>              \n<span class=\"p\">|</span>    \u2514\u2500pool0                 MaxPool2d             <span class=\"o\">(</span>-1, <span class=\"m\">64</span>, <span class=\"m\">56</span>, <span class=\"m\">56</span><span class=\"o\">)</span>          <span class=\"m\">0</span>              \n<span class=\"p\">|</span>    \u2514\u2500denseblock1           _DenseBlock           <span class=\"o\">(</span>-1, <span class=\"m\">256</span>, <span class=\"m\">56</span>, <span class=\"m\">56</span><span class=\"o\">)</span>         <span class=\"m\">338</span>,316        \n<span class=\"p\">|</span>    \u2514\u2500transition1           _Transition           <span class=\"o\">(</span>-1, <span class=\"m\">128</span>, <span class=\"m\">28</span>, <span class=\"m\">28</span><span class=\"o\">)</span>         <span class=\"m\">33</span>,793         \n<span class=\"p\">|</span>    \u2514\u2500denseblock2           _DenseBlock           <span class=\"o\">(</span>-1, <span class=\"m\">512</span>, <span class=\"m\">28</span>, <span class=\"m\">28</span><span class=\"o\">)</span>         <span class=\"m\">930</span>,072        \n<span class=\"p\">|</span>    \u2514\u2500transition2           _Transition           <span class=\"o\">(</span>-1, <span class=\"m\">256</span>, <span class=\"m\">14</span>, <span class=\"m\">14</span><span class=\"o\">)</span>         <span class=\"m\">133</span>,121        \n<span class=\"p\">|</span>    \u2514\u2500denseblock3           _DenseBlock           <span class=\"o\">(</span>-1, <span class=\"m\">1024</span>, <span class=\"m\">14</span>, <span class=\"m\">14</span><span class=\"o\">)</span>        <span class=\"m\">2</span>,873,904      \n<span class=\"p\">|</span>    \u2514\u2500transition3           _Transition           <span class=\"o\">(</span>-1, <span class=\"m\">512</span>, <span class=\"m\">7</span>, <span class=\"m\">7</span><span class=\"o\">)</span>           <span class=\"m\">528</span>,385        \n<span class=\"p\">|</span>    \u2514\u2500denseblock4           _DenseBlock           <span class=\"o\">(</span>-1, <span class=\"m\">1024</span>, <span class=\"m\">7</span>, <span class=\"m\">7</span><span class=\"o\">)</span>          <span class=\"m\">2</span>,186,272      \n<span class=\"p\">|</span>    \u2514\u2500norm5                 BatchNorm2d           <span class=\"o\">(</span>-1, <span class=\"m\">1024</span>, <span class=\"m\">7</span>, <span class=\"m\">7</span><span class=\"o\">)</span>          <span class=\"m\">4</span>,097          \n\u251c\u2500classifier                 Linear                <span class=\"o\">(</span>-1, <span class=\"m\">1000</span><span class=\"o\">)</span>                <span class=\"m\">1</span>,025,000      \n<span class=\"o\">==========================================================================================</span>\nTrainable params: <span class=\"m\">7</span>,978,856\nNon-trainable params: <span class=\"m\">0</span>\nTotal params: <span class=\"m\">7</span>,978,856\n------------------------------------------------------------------------------------------\nModel size <span class=\"o\">(</span>params + buffers<span class=\"o\">)</span>: <span class=\"m\">30</span>.76 Mb\nFramework <span class=\"p\">&amp;</span> CUDA overhead: <span class=\"m\">423</span>.57 Mb\nTotal RAM usage: <span class=\"m\">454</span>.32 Mb\n------------------------------------------------------------------------------------------\nFloating Point Operations on forward: <span class=\"m\">5</span>.74 GFLOPs\nMultiply-Accumulations on forward: <span class=\"m\">2</span>.87 GMACs\nDirect memory accesses on forward: <span class=\"m\">2</span>.90 GDMAs\n__________________________________________________________________________________________\n</pre>\n<p>Results are aggregated to the selected depth for improved readability.</p>\n<p>For reference, here are explanations of a few acronyms:</p>\n<ul>\n<li><strong>FLOPs</strong>: floating-point operations (not to be confused with FLOPS which is FLOPs per second)</li>\n<li><strong>MACs</strong>: mutiply-accumulate operations (cf. <a href=\"https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation\" rel=\"nofollow\">wikipedia</a>)</li>\n<li><strong>DMAs</strong>: direct memory accesses (many argue that it is more relevant than FLOPs or MACs to compare model inference speeds cf. <a href=\"https://en.wikipedia.org/wiki/Direct_memory_access\" rel=\"nofollow\">wikipedia</a>)</li>\n</ul>\n<p>##\u00a0Benchmark</p>\n<p>Below are the results for classification models supported by <code>torchvision</code> for a single image with 3 color channels of size <code>224x224</code> (apart from  <code>inception_v3</code>   which uses <code>299x299</code>).</p>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Params (M)</th>\n<th>FLOPs (G)</th>\n<th>MACs (G)</th>\n<th>DMAs (G)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>alexnet</td>\n<td>61.1</td>\n<td>1.43</td>\n<td>0.71</td>\n<td>0.72</td>\n</tr>\n<tr>\n<td>googlenet</td>\n<td>6.62</td>\n<td>3.01</td>\n<td>1.51</td>\n<td>1.53</td>\n</tr>\n<tr>\n<td>vgg11</td>\n<td>132.86</td>\n<td>15.23</td>\n<td>7.61</td>\n<td>7.64</td>\n</tr>\n<tr>\n<td>vgg11_bn</td>\n<td>132.87</td>\n<td>15.26</td>\n<td>7.63</td>\n<td>7.66</td>\n</tr>\n<tr>\n<td>vgg13</td>\n<td>133.05</td>\n<td>22.63</td>\n<td>11.31</td>\n<td>11.35</td>\n</tr>\n<tr>\n<td>vgg13_bn</td>\n<td>133.05</td>\n<td>22.68</td>\n<td>11.33</td>\n<td>11.37</td>\n</tr>\n<tr>\n<td>vgg16</td>\n<td>138.36</td>\n<td>30.96</td>\n<td>15.47</td>\n<td>15.52</td>\n</tr>\n<tr>\n<td>vgg16_bn</td>\n<td>138.37</td>\n<td>31.01</td>\n<td>15.5</td>\n<td>15.55</td>\n</tr>\n<tr>\n<td>vgg19</td>\n<td>143.67</td>\n<td>39.28</td>\n<td>19.63</td>\n<td>19.69</td>\n</tr>\n<tr>\n<td>vgg19_bn</td>\n<td>143.68</td>\n<td>39.34</td>\n<td>19.66</td>\n<td>19.72</td>\n</tr>\n<tr>\n<td>resnet18</td>\n<td>11.69</td>\n<td>3.64</td>\n<td>1.82</td>\n<td>1.84</td>\n</tr>\n<tr>\n<td>resnet34</td>\n<td>21.8</td>\n<td>7.34</td>\n<td>3.67</td>\n<td>3.7</td>\n</tr>\n<tr>\n<td>resnet50</td>\n<td>25.56</td>\n<td>8.21</td>\n<td>4.11</td>\n<td>4.15</td>\n</tr>\n<tr>\n<td>resnet101</td>\n<td>44.55</td>\n<td>15.66</td>\n<td>7.83</td>\n<td>7.9</td>\n</tr>\n<tr>\n<td>resnet152</td>\n<td>60.19</td>\n<td>23.1</td>\n<td>11.56</td>\n<td>11.65</td>\n</tr>\n<tr>\n<td>inception_v3</td>\n<td>27.16</td>\n<td>11.45</td>\n<td>5.73</td>\n<td>5.76</td>\n</tr>\n<tr>\n<td>squeezenet1_0</td>\n<td>1.25</td>\n<td>1.64</td>\n<td>0.82</td>\n<td>0.83</td>\n</tr>\n<tr>\n<td>squeezenet1_1</td>\n<td>1.24</td>\n<td>0.7</td>\n<td>0.35</td>\n<td>0.36</td>\n</tr>\n<tr>\n<td>wide_resnet50_2</td>\n<td>68.88</td>\n<td>22.84</td>\n<td>11.43</td>\n<td>11.51</td>\n</tr>\n<tr>\n<td>wide_resnet101_2</td>\n<td>126.89</td>\n<td>45.58</td>\n<td>22.8</td>\n<td>22.95</td>\n</tr>\n<tr>\n<td>densenet121</td>\n<td>7.98</td>\n<td>5.74</td>\n<td>2.87</td>\n<td>2.9</td>\n</tr>\n<tr>\n<td>densenet161</td>\n<td>28.68</td>\n<td>15.59</td>\n<td>7.79</td>\n<td>7.86</td>\n</tr>\n<tr>\n<td>densenet169</td>\n<td>14.15</td>\n<td>6.81</td>\n<td>3.4</td>\n<td>3.44</td>\n</tr>\n<tr>\n<td>densenet201</td>\n<td>20.01</td>\n<td>8.7</td>\n<td>4.34</td>\n<td>4.39</td>\n</tr>\n<tr>\n<td>resnext50_32x4d</td>\n<td>25.03</td>\n<td>8.51</td>\n<td>4.26</td>\n<td>4.3</td>\n</tr>\n<tr>\n<td>resnext101_32x8d</td>\n<td>88.79</td>\n<td>32.93</td>\n<td>16.48</td>\n<td>16.61</td>\n</tr>\n<tr>\n<td>mobilenet_v2</td>\n<td>3.5</td>\n<td>0.63</td>\n<td>0.31</td>\n<td>0.32</td>\n</tr>\n<tr>\n<td>shufflenet_v2_x0_5</td>\n<td>1.37</td>\n<td>0.09</td>\n<td>0.04</td>\n<td>0.05</td>\n</tr>\n<tr>\n<td>shufflenet_v2_x1_0</td>\n<td>2.28</td>\n<td>0.3</td>\n<td>0.15</td>\n<td>0.15</td>\n</tr>\n<tr>\n<td>shufflenet_v2_x1_5</td>\n<td>3.5</td>\n<td>0.6</td>\n<td>0.3</td>\n<td>0.31</td>\n</tr>\n<tr>\n<td>shufflenet_v2_x2_0</td>\n<td>7.39</td>\n<td>1.18</td>\n<td>0.59</td>\n<td>0.6</td>\n</tr>\n<tr>\n<td>mnasnet0_5</td>\n<td>2.22</td>\n<td>0.22</td>\n<td>0.11</td>\n<td>0.12</td>\n</tr>\n<tr>\n<td>mnasnet0_75</td>\n<td>3.17</td>\n<td>0.45</td>\n<td>0.23</td>\n<td>0.24</td>\n</tr>\n<tr>\n<td>mnasnet1_0</td>\n<td>4.38</td>\n<td>0.65</td>\n<td>0.33</td>\n<td>0.34</td>\n</tr>\n<tr>\n<td>mnasnet1_3</td>\n<td>6.28</td>\n<td>1.08</td>\n<td>0.54</td>\n<td>0.56</td>\n</tr></tbody></table>\n<p>The above results were produced using the <code>scripts/benchmark.py</code> script.</p>\n<h2>Technical roadmap</h2>\n<p>The project is currently under development, here are the objectives for the next releases:</p>\n<ul>\n<li>\n<p>[x] Support of <code>torch.nn.Module</code> layers: ConvTranspose, Identity.</p>\n</li>\n<li>\n<p>[ ] Package distribution: add a conda package.</p>\n</li>\n<li>\n<p>[ ] Shared parameter support (cf. <a href=\"https://discuss.pytorch.org/t/repeated-model-layers-real-or-torchsummary-bug/26489\" rel=\"nofollow\">discussion</a>)</p>\n</li>\n<li>\n<p>[ ] Result exporting: add a csv export option.</p>\n</li>\n<li>\n<p>[ ] Forward pass stat support: RAM usage for each layer, on forward pass.</p>\n</li>\n<li>\n<p>[ ] Backward pass stat support: RAM usage for each layer, on backward pass.</p>\n</li>\n<li>\n<p>[ ] Support of <code>torch.nn.Module</code> layers: GroupNorm, Upsample, PixelShuffle, RNN, LSTM, GRU, Embedding, Transformer.</p>\n</li>\n<li>\n<p>[ ] Support of scripted modules</p>\n</li>\n<li>\n<p>[ ] Support of <code>torch.nn</code> functional API</p>\n</li>\n<li>\n<p>[ ] I/O handling: multiple inputs or outputs, non-tensor I/O</p>\n</li>\n<li>\n<p>[ ] Add computational graph (cf. <a href=\"https://github.com/szagoruyko/pytorchviz\" rel=\"nofollow\">pytorchviz</a>)</p>\n</li>\n</ul>\n<h2>Documentation</h2>\n<p>The full package documentation is available <a href=\"https://frgfm.github.io/torch-scan/\" rel=\"nofollow\">here</a> for detailed specifications. The documentation was built with <a href=\"sphinx-doc.org\" rel=\"nofollow\">Sphinx</a> using a <a href=\"github.com/readthedocs/sphinx_rtd_theme\" rel=\"nofollow\">theme</a> provided by <a href=\"readthedocs.org\" rel=\"nofollow\">Read the Docs</a>.</p>\n<h2>Contributing</h2>\n<p>Please refer to <code>CONTRIBUTING</code> if you wish to contribute to this project.</p>\n<h2>Credits</h2>\n<p>This project is developed and maintained by the repo owner, but the implementation was inspired or helped by the following contributions:</p>\n<ul>\n<li><a href=\"https://github.com/sksq96/pytorch-summary\" rel=\"nofollow\">Pytorch summary</a>: existing PyTorch porting of <code>tf.keras.Model.summary</code></li>\n<li><a href=\"https://github.com/Swall0w/torchstat\" rel=\"nofollow\">Torchstat</a>: another module inspection tool</li>\n<li><a href=\"https://github.com/sovrasov/flops-counter.pytorch\" rel=\"nofollow\">Flops counter Pytorch</a>: operation counter tool</li>\n<li><a href=\"https://github.com/Lyken17/pytorch-OpCounter\" rel=\"nofollow\">THOP</a>: PyTorch Op counter</li>\n<li>Number of operations and memory estimation articles by <a href=\"https://machinethink.net/blog/how-fast-is-my-model/\" rel=\"nofollow\">Matthijs Hollemans</a>, and <a href=\"https://www.sicara.ai/blog/2019-28-10-deep-learning-memory-usage-and-pytorch-optimization-tricks\" rel=\"nofollow\">Sicara</a></li>\n<li><a href=\"https://arxiv.org/abs/1611.06440\" rel=\"nofollow\">Pruning Convolutional Neural Networks for Resource Efficient Inference</a></li>\n</ul>\n<h2>License</h2>\n<p>Distributed under the MIT License. See <code>LICENSE</code> for more information.</p>\n\n          </div>"}, "last_serial": 6855679, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "5086d557027e1bfce9125ea665506caf", "sha256": "f92293133a629b192fa62084a241aac0b7c1c79541bf4b44c7874c64fd80774e"}, "downloads": -1, "filename": "torchscan-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "5086d557027e1bfce9125ea665506caf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 17745, "upload_time": "2020-03-21T14:34:03", "upload_time_iso_8601": "2020-03-21T14:34:03.805323Z", "url": "https://files.pythonhosted.org/packages/01/19/63627800c4cf1c463f1d3bdd264b6474d91afe0d865876efe35d09e62f5e/torchscan-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9f4057b99fe23825e8074ddcef9a7e0a", "sha256": "baf561e73cd7373c676d2c07e803ad4fdd070cac2320c370ba610cf7661f586e"}, "downloads": -1, "filename": "torchscan-0.1.0.tar.gz", "has_sig": false, "md5_digest": "9f4057b99fe23825e8074ddcef9a7e0a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 19739, "upload_time": "2020-03-21T14:34:05", "upload_time_iso_8601": "2020-03-21T14:34:05.956632Z", "url": "https://files.pythonhosted.org/packages/c6/1a/60b191ee8de9e01f3e4cbe76a76b534fc494ec32fff7e936e9b5d8d185d3/torchscan-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "5086d557027e1bfce9125ea665506caf", "sha256": "f92293133a629b192fa62084a241aac0b7c1c79541bf4b44c7874c64fd80774e"}, "downloads": -1, "filename": "torchscan-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "5086d557027e1bfce9125ea665506caf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6.0", "size": 17745, "upload_time": "2020-03-21T14:34:03", "upload_time_iso_8601": "2020-03-21T14:34:03.805323Z", "url": "https://files.pythonhosted.org/packages/01/19/63627800c4cf1c463f1d3bdd264b6474d91afe0d865876efe35d09e62f5e/torchscan-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9f4057b99fe23825e8074ddcef9a7e0a", "sha256": "baf561e73cd7373c676d2c07e803ad4fdd070cac2320c370ba610cf7661f586e"}, "downloads": -1, "filename": "torchscan-0.1.0.tar.gz", "has_sig": false, "md5_digest": "9f4057b99fe23825e8074ddcef9a7e0a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 19739, "upload_time": "2020-03-21T14:34:05", "upload_time_iso_8601": "2020-03-21T14:34:05.956632Z", "url": "https://files.pythonhosted.org/packages/c6/1a/60b191ee8de9e01f3e4cbe76a76b534fc494ec32fff7e936e9b5d8d185d3/torchscan-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:11 2020"}