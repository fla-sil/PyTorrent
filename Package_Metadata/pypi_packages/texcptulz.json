{"info": {"author": "Cisco Delgado", "author_email": "fdelgados@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Text Clean and Preprocess Tools (texcptulz)\n[![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/)\n![GitHub release (latest by date)](https://img.shields.io/github/v/release/fdelgados/Textools?style=flat)\n\ntextcptulz is a text preprocessing framework to transform raw ingested text into a form that is ready for computation and modeling.\n\n## Table of Contents\n* [Installations](#installations)\n* [Project Motivation](#project-motivation)\n* [Modules](#modules)\n* [Instructions](#instructions)\n* [License](#license)\n\n## Installations\n### Dependencies\n\ntextcptulz requires:\n\n* Python (>=3.5)\n* NumPy\n* scikit-learn\n* NLTK\n* spaCy\n* spacy-langdetect\n* gensim\n\n### Installation\n\nYou can install texcptulz using `pip`\n```\npip install texcptulz\n```\nThat easy.\n\n## Project Motivation\n\ntexcptulz aims to be a wrapper for all that processes involved in the wrangling part of an ETL pipeline for text analysis.\nIn addition, it includes other useful tools such as the detection of the language of a document. These are some of the \nsupported languages:\n\n* English\n* Spanish\n* French\n* German\n* Italian\n\n## Modules\n\nThe modules included in texcptulz are:\n\n### `normalizer`\nThe `normalizer` module includes `TextNormalizer` class that can perform the text normalization process, taking a text\nas input and returning a list of lemmatized tokens.\n\nSpecial characters, HTML tags, urls, etc. will be removed, using the `clean_text` function, then, the text will be \nconverted to lowercase and will be splitted into tokens and lemmatized. \n\n### `clusterer`\nThe `clusterer` module has only one class `KMeansClusters` that performs k-means clustering. The distance measure used \nis cosine distance. The `k` parameter is the number of clusters, the default value is 7.\n\n### `similarity`\nThe `similarity` module has only one class, `Similarity` that creates a `mxm` similarity matrix for a group of documents. \nIn this matrix, columns and rows represents the index of each document the elements of the matrix are similarity between \ndocuments.\n\nThe range of values will be between 0 and 1, where 0 means totally different and 1 means that the content is identical.\n\n### `vectorizer`\nThe `vectorizer` module has only one class, `OneHotVectorizer` that performs the text conversion into a vector \nrepresentations of categorical variables as binary vectors (one hot encoding)\n\n### `utils`\nThis module includes the `LangDetector` class to detect the language of a document\n\n## Instructions\n### Normalize text\n\n```python\nfrom txtools.normalizer import TextNormalizer\n\ntext = 'Python is a programming language that lets you work quickly' \\ \n' and integrate systems more effectively'\n\nnormalizer = TextNormalizer()\ntokens = normalizer.normalize(text, clean=True)\n\nprint(tokens)\n\n# output\n# ['python', 'programming', 'language', 'let', 'work', 'quickly', 'integrate', 'system', 'effectively']\n```\n`TextNormalizer` class implements the `Transformer` interface, this allows us to add this class into a scikit-learn pipeline.\n\n### Language detection\n`utils` module includes the `LangDetector` class intended to detect the language in which a document was written. The \ndetection is done with a certain confidence. You can provide a minimum percentage of confidence for a detection process:\n\n```python\nfrom txtools.utils import LangDetector\n\ndocuments = [\n    'El aprendizaje autom\u00e1tico tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito',\n    'Machine learning tasks are classified into several broad categories. In supervised learning, the algorithm builds a mathematical model from a set of data',\n    'L\\'apprendimento automatico viene impiegato in quei campi dell\\'informatica nei quali progettare e programmare algoritmi espliciti \u00e8 impraticabile',\n    'Selon les informations disponibles durant la phase d\\'apprentissage, l\\'apprentissage est qualifi\u00e9 de diff\u00e9rentes mani\u00e8res.'\n]\n\nlang_detector = LangDetector()\n\nfor idx, document in enumerate(documents):\n    print('Document {} is written in {}'.format(idx, lang_detector.lang(document)))\n\n# output\n# Document 0 is written in Spanish\n# Document 1 is written in English\n# Document 2 is written in Italian\n# Document 3 is written in French\n```\nAlso, you can get the ISO 639-1 code:\n\n```python\n\nfor idx, document in enumerate(documents):\n    print('ISO 639-1 code for document {} is {}'.format(idx, lang_detector.iso_639_1_code(document)))\n\n# output\n# ISO 639-1 lang code for document 0 is es\n# ISO 639-1 lang code for document 1 is en\n# ISO 639-1 lang code for document 2 is it\n# ISO 639-1 lang code for document 3 is fr\n```\n\n### Clustering documents\n\n```python\nfrom txtools.clusterer import KMeansClusters\n\ndocuments = [] # this must contains a lot of documents\n\nk_means = KMeansClusters(k=10)\n\nclusters = k_means.transform(documents)\n``` \n### Compute document similarity\n`Similarity` creates creates a `mxm` similarity matrix for a corpus where `m` is the number of documents.\n\n```python\nfrom txtools.similarity import Similarity\n\ndocuments = [\n    'Psycho is a 1960 American psychological horror film directed and produced by Alfred Hitchcock, and written by Joseph Stefano.',\n    'North by Northwest is a 1959 American thriller film directed by Alfred Hitchcock, starring Cary Grant, Eva Marie Saint and James Mason.',\n    'The Birds is a 1963 American horror-thriller film directed and produced by Alfred Hitchcock.',\n    'Rear Window is a 1954 American Technicolor mystery thriller film directed by Alfred Hitchcock and written by John Michael Hayes based on Cornell Woolrich\\'s 1942 short story \"It Had to Be Murder\".'\n]\n\nsimilarity = Similarity()\nsims = similarity.transform(documents)\n\nprint(sims)\n\n# output\n\n# [[0.99999994 0.         0.13075474 0.02665992]\n#  [0.         1.         0.00812627 0.00331377]\n#  [0.13075474 0.00812627 1.         0.0069054 ]\n#  [0.02665992 0.00331377 0.0069054  1.        ]]\n```\n\n### Using a scikit-learn pipeline\n`TextNormalizer`, `OneHotVectorizer`, `KMeansClusters` and `Similarity` implement the `Transformer` interface, so we can\nadd them to a scikit-learn pipeline.\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom txtools.normalizer import TextNormalizer\nfrom txtools.similarity import Similarity\n\ndocuments = [\n    'Psycho is a 1960 American psychological horror film directed and produced by Alfred Hitchcock, and written by Joseph Stefano.',\n    'North by Northwest is a 1959 American thriller film directed by Alfred Hitchcock, starring Cary Grant, Eva Marie Saint and James Mason.',\n    'The Birds is a 1963 American horror-thriller film directed and produced by Alfred Hitchcock.',\n    'Rear Window is a 1954 American Technicolor mystery thriller film directed by Alfred Hitchcock and written by John Michael Hayes based on Cornell Woolrich\\'s 1942 short story \"It Had to Be Murder\".'\n]\n\nmodel = Pipeline([\n    ('norm', TextNormalizer()),\n    ('sim', Similarity())\n])\n\nsims = model.fit_transform(documents)\n\n```\n\n### Cleaning text\nYou can clean text with the `clean_text` function included in the `normalizer` module\n\n```python\nfrom txtools.normalizer import clean_text\n\ntext = '<p><i><b>2001: A    Space Odyssey</b></i> \\n\\nis a 1968 <a href=\"/wiki/Epic_film\" title=\"Epic ' \\\n           'film\">epic</a> <a href=\"/wiki/Science_fiction_film\" title=\"Science fiction film\">science fiction film</a> ' \\\n           'produced and directed by\\t <a href=\"/wiki/Stanley_Kubrick\" title=\"Stanley Kubrick\">Stanley Kubrick</a>. ' \\\n           'The screenplay was written by Kubrick and <a href=\"/wiki/Arthur_C._Clarke\" title=\"Arthur C. ' \\\n           'Clarke\">Arthur C. Clarke</a>, and was inspired by Clarke\\'s short story \"\"<a href=\"/wiki/The_Sentinel_(' \\\n           'short_story)\" title=\"The Sentinel (short story)\">The Sentinel</a>\" and other short stories by Clarke. A ' \\\n           '<a href=\"/wiki/2001:_A_Space_Odyssey_(novel)\" title=\"2001: A Space Odyssey (novel)\">novelisation of the ' \\\n           'film</a> released after the film\\'s premiere was in part written concurrently with the screenplay. The ' \\\n           'film, which follows a voyage to <a href=\"/wiki/Jupiter\" title=\"Jupiter\">Jupiter</a> with the <a ' \\\n           'href=\"/wiki/Sentience\" title=\"Sentience\">sentient</a> computer <a href=\"/wiki/HAL_9000\" title=\"HAL ' \\\n           '9000\">HAL</a> after the discovery of a <a href=\"/wiki/Monolith_(Space_Odyssey)\" title=\"Monolith (Space ' \\\n           'Odyssey)\">featureless alien monolith</a> affecting human evolution, deals with themes of <a ' \\\n           'href=\"/wiki/Existentialism\" title=\"Existentialism\">existentialism</a>, <a href=\"/wiki/Human_evolution\" ' \\\n           'title=\"Human evolution\">human evolution</a>, technology, <a href=\"/wiki/Artificial_intelligence\" ' \\\n           'title=\"Artificial intelligence\">artificial intelligence</a>, and the possibility of <a ' \\\n           'href=\"/wiki/Extraterrestrial_life\" title=\"Extraterrestrial life\">extraterrestrial life</a>.</p> '\n\nprint(clean_text(text))\n\n# output\n\n# 2001: A Space Odyssey is a 1968 epic science fiction film produced and directed by Stanley Kubrick. The screenplay \n# was written by Kubrick and Arthur C. Clarke, and was inspired by Clarke's short story \"The Sentinel\" and other \n# short stories by Clarke. A novelisation of the film released after the film's premiere was in part written \n# concurrently with the screenplay. The film, which follows a voyage to Jupiter with the sentient computer HAL after \n# the discovery of a featureless alien monolith affecting human evolution, deals with themes of existentialism, \n# human evolution, technology, artificial intelligence, and the possibility of extraterrestrial life.\n\n```\n## License\n\nCopyright (c) 2019 Cisco Delgado\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/fdelgados/Textools.git", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "texcptulz", "package_url": "https://pypi.org/project/texcptulz/", "platform": "", "project_url": "https://pypi.org/project/texcptulz/", "project_urls": {"Homepage": "https://github.com/fdelgados/Textools.git"}, "release_url": "https://pypi.org/project/texcptulz/2.5.0/", "requires_dist": ["nltk", "scikit-learn", "spacy", "spacy-langdetect", "gensim", "numpy"], "requires_python": ">=3.5", "summary": "Tools for cleaning and preprocessing text", "version": "2.5.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Text Clean and Preprocess Tools (texcptulz)</h1>\n<p><a href=\"https://www.python.org/\" rel=\"nofollow\"><img alt=\"made-with-python\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/68a99107ffdf24c5fb2cc4bca38b7b662e501b97/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d616465253230776974682d507974686f6e2d3166343235662e737667\"></a>\n<img alt=\"GitHub release (latest by date)\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d9b04aff37f1db1bf9d2448219876d5131191392/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f6664656c6761646f732f546578746f6f6c733f7374796c653d666c6174\"></p>\n<p>textcptulz is a text preprocessing framework to transform raw ingested text into a form that is ready for computation and modeling.</p>\n<h2>Table of Contents</h2>\n<ul>\n<li><a href=\"#installations\" rel=\"nofollow\">Installations</a></li>\n<li><a href=\"#project-motivation\" rel=\"nofollow\">Project Motivation</a></li>\n<li><a href=\"#modules\" rel=\"nofollow\">Modules</a></li>\n<li><a href=\"#instructions\" rel=\"nofollow\">Instructions</a></li>\n<li><a href=\"#license\" rel=\"nofollow\">License</a></li>\n</ul>\n<h2>Installations</h2>\n<h3>Dependencies</h3>\n<p>textcptulz requires:</p>\n<ul>\n<li>Python (&gt;=3.5)</li>\n<li>NumPy</li>\n<li>scikit-learn</li>\n<li>NLTK</li>\n<li>spaCy</li>\n<li>spacy-langdetect</li>\n<li>gensim</li>\n</ul>\n<h3>Installation</h3>\n<p>You can install texcptulz using <code>pip</code></p>\n<pre><code>pip install texcptulz\n</code></pre>\n<p>That easy.</p>\n<h2>Project Motivation</h2>\n<p>texcptulz aims to be a wrapper for all that processes involved in the wrangling part of an ETL pipeline for text analysis.\nIn addition, it includes other useful tools such as the detection of the language of a document. These are some of the\nsupported languages:</p>\n<ul>\n<li>English</li>\n<li>Spanish</li>\n<li>French</li>\n<li>German</li>\n<li>Italian</li>\n</ul>\n<h2>Modules</h2>\n<p>The modules included in texcptulz are:</p>\n<h3><code>normalizer</code></h3>\n<p>The <code>normalizer</code> module includes <code>TextNormalizer</code> class that can perform the text normalization process, taking a text\nas input and returning a list of lemmatized tokens.</p>\n<p>Special characters, HTML tags, urls, etc. will be removed, using the <code>clean_text</code> function, then, the text will be\nconverted to lowercase and will be splitted into tokens and lemmatized.</p>\n<h3><code>clusterer</code></h3>\n<p>The <code>clusterer</code> module has only one class <code>KMeansClusters</code> that performs k-means clustering. The distance measure used\nis cosine distance. The <code>k</code> parameter is the number of clusters, the default value is 7.</p>\n<h3><code>similarity</code></h3>\n<p>The <code>similarity</code> module has only one class, <code>Similarity</code> that creates a <code>mxm</code> similarity matrix for a group of documents.\nIn this matrix, columns and rows represents the index of each document the elements of the matrix are similarity between\ndocuments.</p>\n<p>The range of values will be between 0 and 1, where 0 means totally different and 1 means that the content is identical.</p>\n<h3><code>vectorizer</code></h3>\n<p>The <code>vectorizer</code> module has only one class, <code>OneHotVectorizer</code> that performs the text conversion into a vector\nrepresentations of categorical variables as binary vectors (one hot encoding)</p>\n<h3><code>utils</code></h3>\n<p>This module includes the <code>LangDetector</code> class to detect the language of a document</p>\n<h2>Instructions</h2>\n<h3>Normalize text</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">txtools.normalizer</span> <span class=\"kn\">import</span> <span class=\"n\">TextNormalizer</span>\n\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s1\">'Python is a programming language that lets you work quickly'</span> \\ \n<span class=\"s1\">' and integrate systems more effectively'</span>\n\n<span class=\"n\">normalizer</span> <span class=\"o\">=</span> <span class=\"n\">TextNormalizer</span><span class=\"p\">()</span>\n<span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">normalizer</span><span class=\"o\">.</span><span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">clean</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tokens</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># output</span>\n<span class=\"c1\"># ['python', 'programming', 'language', 'let', 'work', 'quickly', 'integrate', 'system', 'effectively']</span>\n</pre>\n<p><code>TextNormalizer</code> class implements the <code>Transformer</code> interface, this allows us to add this class into a scikit-learn pipeline.</p>\n<h3>Language detection</h3>\n<p><code>utils</code> module includes the <code>LangDetector</code> class intended to detect the language in which a document was written. The\ndetection is done with a certain confidence. You can provide a minimum percentage of confidence for a detection process:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">txtools.utils</span> <span class=\"kn\">import</span> <span class=\"n\">LangDetector</span>\n\n<span class=\"n\">documents</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"s1\">'El aprendizaje autom\u00e1tico tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'Machine learning tasks are classified into several broad categories. In supervised learning, the algorithm builds a mathematical model from a set of data'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'L</span><span class=\"se\">\\'</span><span class=\"s1\">apprendimento automatico viene impiegato in quei campi dell</span><span class=\"se\">\\'</span><span class=\"s1\">informatica nei quali progettare e programmare algoritmi espliciti \u00e8 impraticabile'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'Selon les informations disponibles durant la phase d</span><span class=\"se\">\\'</span><span class=\"s1\">apprentissage, l</span><span class=\"se\">\\'</span><span class=\"s1\">apprentissage est qualifi\u00e9 de diff\u00e9rentes mani\u00e8res.'</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">lang_detector</span> <span class=\"o\">=</span> <span class=\"n\">LangDetector</span><span class=\"p\">()</span>\n\n<span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">document</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">documents</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'Document </span><span class=\"si\">{}</span><span class=\"s1\"> is written in </span><span class=\"si\">{}</span><span class=\"s1\">'</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">lang_detector</span><span class=\"o\">.</span><span class=\"n\">lang</span><span class=\"p\">(</span><span class=\"n\">document</span><span class=\"p\">)))</span>\n\n<span class=\"c1\"># output</span>\n<span class=\"c1\"># Document 0 is written in Spanish</span>\n<span class=\"c1\"># Document 1 is written in English</span>\n<span class=\"c1\"># Document 2 is written in Italian</span>\n<span class=\"c1\"># Document 3 is written in French</span>\n</pre>\n<p>Also, you can get the ISO 639-1 code:</p>\n<pre><span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">document</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">documents</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'ISO 639-1 code for document </span><span class=\"si\">{}</span><span class=\"s1\"> is </span><span class=\"si\">{}</span><span class=\"s1\">'</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">lang_detector</span><span class=\"o\">.</span><span class=\"n\">iso_639_1_code</span><span class=\"p\">(</span><span class=\"n\">document</span><span class=\"p\">)))</span>\n\n<span class=\"c1\"># output</span>\n<span class=\"c1\"># ISO 639-1 lang code for document 0 is es</span>\n<span class=\"c1\"># ISO 639-1 lang code for document 1 is en</span>\n<span class=\"c1\"># ISO 639-1 lang code for document 2 is it</span>\n<span class=\"c1\"># ISO 639-1 lang code for document 3 is fr</span>\n</pre>\n<h3>Clustering documents</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">txtools.clusterer</span> <span class=\"kn\">import</span> <span class=\"n\">KMeansClusters</span>\n\n<span class=\"n\">documents</span> <span class=\"o\">=</span> <span class=\"p\">[]</span> <span class=\"c1\"># this must contains a lot of documents</span>\n\n<span class=\"n\">k_means</span> <span class=\"o\">=</span> <span class=\"n\">KMeansClusters</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n<span class=\"n\">clusters</span> <span class=\"o\">=</span> <span class=\"n\">k_means</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">documents</span><span class=\"p\">)</span>\n</pre>\n<h3>Compute document similarity</h3>\n<p><code>Similarity</code> creates creates a <code>mxm</code> similarity matrix for a corpus where <code>m</code> is the number of documents.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">txtools.similarity</span> <span class=\"kn\">import</span> <span class=\"n\">Similarity</span>\n\n<span class=\"n\">documents</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"s1\">'Psycho is a 1960 American psychological horror film directed and produced by Alfred Hitchcock, and written by Joseph Stefano.'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'North by Northwest is a 1959 American thriller film directed by Alfred Hitchcock, starring Cary Grant, Eva Marie Saint and James Mason.'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'The Birds is a 1963 American horror-thriller film directed and produced by Alfred Hitchcock.'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'Rear Window is a 1954 American Technicolor mystery thriller film directed by Alfred Hitchcock and written by John Michael Hayes based on Cornell Woolrich</span><span class=\"se\">\\'</span><span class=\"s1\">s 1942 short story \"It Had to Be Murder\".'</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">similarity</span> <span class=\"o\">=</span> <span class=\"n\">Similarity</span><span class=\"p\">()</span>\n<span class=\"n\">sims</span> <span class=\"o\">=</span> <span class=\"n\">similarity</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">documents</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">sims</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># output</span>\n\n<span class=\"c1\"># [[0.99999994 0.         0.13075474 0.02665992]</span>\n<span class=\"c1\">#  [0.         1.         0.00812627 0.00331377]</span>\n<span class=\"c1\">#  [0.13075474 0.00812627 1.         0.0069054 ]</span>\n<span class=\"c1\">#  [0.02665992 0.00331377 0.0069054  1.        ]]</span>\n</pre>\n<h3>Using a scikit-learn pipeline</h3>\n<p><code>TextNormalizer</code>, <code>OneHotVectorizer</code>, <code>KMeansClusters</code> and <code>Similarity</code> implement the <code>Transformer</code> interface, so we can\nadd them to a scikit-learn pipeline.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">sklearn.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">Pipeline</span>\n<span class=\"kn\">from</span> <span class=\"nn\">txtools.normalizer</span> <span class=\"kn\">import</span> <span class=\"n\">TextNormalizer</span>\n<span class=\"kn\">from</span> <span class=\"nn\">txtools.similarity</span> <span class=\"kn\">import</span> <span class=\"n\">Similarity</span>\n\n<span class=\"n\">documents</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"s1\">'Psycho is a 1960 American psychological horror film directed and produced by Alfred Hitchcock, and written by Joseph Stefano.'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'North by Northwest is a 1959 American thriller film directed by Alfred Hitchcock, starring Cary Grant, Eva Marie Saint and James Mason.'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'The Birds is a 1963 American horror-thriller film directed and produced by Alfred Hitchcock.'</span><span class=\"p\">,</span>\n    <span class=\"s1\">'Rear Window is a 1954 American Technicolor mystery thriller film directed by Alfred Hitchcock and written by John Michael Hayes based on Cornell Woolrich</span><span class=\"se\">\\'</span><span class=\"s1\">s 1942 short story \"It Had to Be Murder\".'</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Pipeline</span><span class=\"p\">([</span>\n    <span class=\"p\">(</span><span class=\"s1\">'norm'</span><span class=\"p\">,</span> <span class=\"n\">TextNormalizer</span><span class=\"p\">()),</span>\n    <span class=\"p\">(</span><span class=\"s1\">'sim'</span><span class=\"p\">,</span> <span class=\"n\">Similarity</span><span class=\"p\">())</span>\n<span class=\"p\">])</span>\n\n<span class=\"n\">sims</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">documents</span><span class=\"p\">)</span>\n</pre>\n<h3>Cleaning text</h3>\n<p>You can clean text with the <code>clean_text</code> function included in the <code>normalizer</code> module</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">txtools.normalizer</span> <span class=\"kn\">import</span> <span class=\"n\">clean_text</span>\n\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"s1\">'&lt;p&gt;&lt;i&gt;&lt;b&gt;2001: A    Space Odyssey&lt;/b&gt;&lt;/i&gt; </span><span class=\"se\">\\n\\n</span><span class=\"s1\">is a 1968 &lt;a href=\"/wiki/Epic_film\" title=\"Epic '</span> \\\n           <span class=\"s1\">'film\"&gt;epic&lt;/a&gt; &lt;a href=\"/wiki/Science_fiction_film\" title=\"Science fiction film\"&gt;science fiction film&lt;/a&gt; '</span> \\\n           <span class=\"s1\">'produced and directed by</span><span class=\"se\">\\t</span><span class=\"s1\"> &lt;a href=\"/wiki/Stanley_Kubrick\" title=\"Stanley Kubrick\"&gt;Stanley Kubrick&lt;/a&gt;. '</span> \\\n           <span class=\"s1\">'The screenplay was written by Kubrick and &lt;a href=\"/wiki/Arthur_C._Clarke\" title=\"Arthur C. '</span> \\\n           <span class=\"s1\">'Clarke\"&gt;Arthur C. Clarke&lt;/a&gt;, and was inspired by Clarke</span><span class=\"se\">\\'</span><span class=\"s1\">s short story \"\"&lt;a href=\"/wiki/The_Sentinel_('</span> \\\n           <span class=\"s1\">'short_story)\" title=\"The Sentinel (short story)\"&gt;The Sentinel&lt;/a&gt;\" and other short stories by Clarke. A '</span> \\\n           <span class=\"s1\">'&lt;a href=\"/wiki/2001:_A_Space_Odyssey_(novel)\" title=\"2001: A Space Odyssey (novel)\"&gt;novelisation of the '</span> \\\n           <span class=\"s1\">'film&lt;/a&gt; released after the film</span><span class=\"se\">\\'</span><span class=\"s1\">s premiere was in part written concurrently with the screenplay. The '</span> \\\n           <span class=\"s1\">'film, which follows a voyage to &lt;a href=\"/wiki/Jupiter\" title=\"Jupiter\"&gt;Jupiter&lt;/a&gt; with the &lt;a '</span> \\\n           <span class=\"s1\">'href=\"/wiki/Sentience\" title=\"Sentience\"&gt;sentient&lt;/a&gt; computer &lt;a href=\"/wiki/HAL_9000\" title=\"HAL '</span> \\\n           <span class=\"s1\">'9000\"&gt;HAL&lt;/a&gt; after the discovery of a &lt;a href=\"/wiki/Monolith_(Space_Odyssey)\" title=\"Monolith (Space '</span> \\\n           <span class=\"s1\">'Odyssey)\"&gt;featureless alien monolith&lt;/a&gt; affecting human evolution, deals with themes of &lt;a '</span> \\\n           <span class=\"s1\">'href=\"/wiki/Existentialism\" title=\"Existentialism\"&gt;existentialism&lt;/a&gt;, &lt;a href=\"/wiki/Human_evolution\" '</span> \\\n           <span class=\"s1\">'title=\"Human evolution\"&gt;human evolution&lt;/a&gt;, technology, &lt;a href=\"/wiki/Artificial_intelligence\" '</span> \\\n           <span class=\"s1\">'title=\"Artificial intelligence\"&gt;artificial intelligence&lt;/a&gt;, and the possibility of &lt;a '</span> \\\n           <span class=\"s1\">'href=\"/wiki/Extraterrestrial_life\" title=\"Extraterrestrial life\"&gt;extraterrestrial life&lt;/a&gt;.&lt;/p&gt; '</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">clean_text</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># output</span>\n\n<span class=\"c1\"># 2001: A Space Odyssey is a 1968 epic science fiction film produced and directed by Stanley Kubrick. The screenplay </span>\n<span class=\"c1\"># was written by Kubrick and Arthur C. Clarke, and was inspired by Clarke's short story \"The Sentinel\" and other </span>\n<span class=\"c1\"># short stories by Clarke. A novelisation of the film released after the film's premiere was in part written </span>\n<span class=\"c1\"># concurrently with the screenplay. The film, which follows a voyage to Jupiter with the sentient computer HAL after </span>\n<span class=\"c1\"># the discovery of a featureless alien monolith affecting human evolution, deals with themes of existentialism, </span>\n<span class=\"c1\"># human evolution, technology, artificial intelligence, and the possibility of extraterrestrial life.</span>\n</pre>\n<h2>License</h2>\n<p>Copyright (c) 2019 Cisco Delgado</p>\n<p>Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:</p>\n<p>The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.</p>\n<p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.</p>\n\n          </div>"}, "last_serial": 6680331, "releases": {"0.5.0": [{"comment_text": "", "digests": {"md5": "6b5030816155b43fb7bc8bfa9bf90e77", "sha256": "743ae8d10510bbbe7b1e55343f40a732b33b958772354d29f63d880766b5c48c"}, "downloads": -1, "filename": "texcptulz-0.5.0-py3-none-any.whl", "has_sig": false, "md5_digest": "6b5030816155b43fb7bc8bfa9bf90e77", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 10891, "upload_time": "2019-12-22T19:30:38", "upload_time_iso_8601": "2019-12-22T19:30:38.647455Z", "url": "https://files.pythonhosted.org/packages/ab/a0/d7100e12179f474a2f5dda71d95cbcc73eed6ef3becf15e4246eb839d2f0/texcptulz-0.5.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fccee9dee6f031665d8de3b38aac8038", "sha256": "f329b28082766ff3c251e4e26bdde3a89dd044b0e14b3d082db69b172540c4f0"}, "downloads": -1, "filename": "texcptulz-0.5.0.tar.gz", "has_sig": false, "md5_digest": "fccee9dee6f031665d8de3b38aac8038", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 9420, "upload_time": "2019-12-22T19:30:44", "upload_time_iso_8601": "2019-12-22T19:30:44.690902Z", "url": "https://files.pythonhosted.org/packages/ed/1e/538f6f1fda51e4918dee86c03d841ac2771ecd54f26f1c6cbbfd5c821b73/texcptulz-0.5.0.tar.gz", "yanked": false}], "0.6.0": [{"comment_text": "", "digests": {"md5": "ea1deb68d7a492d046bb375921c4894c", "sha256": "7c7ec6b531fb39e500ff54d3b2fe7697485c5d713c369e5c2632f445c9d4fb77"}, "downloads": -1, "filename": "texcptulz-0.6.0-py3-none-any.whl", "has_sig": false, "md5_digest": "ea1deb68d7a492d046bb375921c4894c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 10890, "upload_time": "2019-12-22T19:30:42", "upload_time_iso_8601": "2019-12-22T19:30:42.429909Z", "url": "https://files.pythonhosted.org/packages/4d/d7/3a785be117c96850ffd9edb0ff6791686ee947219c23aebbc66813e3a75f/texcptulz-0.6.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2e87388bc5e4f8a2044d6a6db8068309", "sha256": "cf7f6c0062fa619d126352088c9c20823168b97cf7b71811a0a2fc8ccd21f805"}, "downloads": -1, "filename": "texcptulz-0.6.0.tar.gz", "has_sig": false, "md5_digest": "2e87388bc5e4f8a2044d6a6db8068309", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 9420, "upload_time": "2019-12-22T19:30:46", "upload_time_iso_8601": "2019-12-22T19:30:46.832626Z", "url": "https://files.pythonhosted.org/packages/b7/53/768a4797e2046b48a34be6a258ce3079943d1dc6a301c549cfa54c4bd8a7/texcptulz-0.6.0.tar.gz", "yanked": false}], "0.6.1": [{"comment_text": "", "digests": {"md5": "0339c108a7d342711732eb9d4f139cfa", "sha256": "374a01b866a9170ac480311e9f2366465c4bd21ab4d9af4e071f9e7652a4b5c9"}, "downloads": -1, "filename": "texcptulz-0.6.1-py3-none-any.whl", "has_sig": false, "md5_digest": "0339c108a7d342711732eb9d4f139cfa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 10941, "upload_time": "2019-12-22T20:20:00", "upload_time_iso_8601": "2019-12-22T20:20:00.279193Z", "url": "https://files.pythonhosted.org/packages/60/7c/0df2495173aae62a5d7497083150a4077bd2d63b87af7a8ed53ed0cd6a94/texcptulz-0.6.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "aadff71b770aaabc6727a6b2e9365e52", "sha256": "ba3298675d185e9e27ede3ff952bc53cc951ebfda224bbd2f07f8c714ae8a8d4"}, "downloads": -1, "filename": "texcptulz-0.6.1.tar.gz", "has_sig": false, "md5_digest": "aadff71b770aaabc6727a6b2e9365e52", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 9484, "upload_time": "2019-12-22T20:20:04", "upload_time_iso_8601": "2019-12-22T20:20:04.291729Z", "url": "https://files.pythonhosted.org/packages/71/e6/7144f6097ea5bc404e4dbe14a39bdfded7531b62eb8e25ee65d007eef14d/texcptulz-0.6.1.tar.gz", "yanked": false}], "0.6.2": [{"comment_text": "", "digests": {"md5": "d318f9d134b0d0ccb2dff67f9d46c711", "sha256": "d266db776f8cb8a3028397d54a5b9694b093a9328fce6d996de38f3d06d47703"}, "downloads": -1, "filename": "texcptulz-0.6.2-py3-none-any.whl", "has_sig": false, "md5_digest": "d318f9d134b0d0ccb2dff67f9d46c711", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 10945, "upload_time": "2019-12-23T12:10:21", "upload_time_iso_8601": "2019-12-23T12:10:21.545564Z", "url": "https://files.pythonhosted.org/packages/db/bc/8b8563e522cdf1804a86af6a2de3ecd7e09c9ca5873f8c9e606a77c2270c/texcptulz-0.6.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "722afc279e08d69521ef1e5c3a28dc0e", "sha256": "122ea93598636ab3660d5bf0f583c35cfe16200bb1e50939c0bf0d8c90fad321"}, "downloads": -1, "filename": "texcptulz-0.6.2.tar.gz", "has_sig": false, "md5_digest": "722afc279e08d69521ef1e5c3a28dc0e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 9507, "upload_time": "2019-12-23T12:10:26", "upload_time_iso_8601": "2019-12-23T12:10:26.859245Z", "url": "https://files.pythonhosted.org/packages/41/28/20739c08abf588cb4c3519aa260be17d874090ebc7096b13b25b6c231911/texcptulz-0.6.2.tar.gz", "yanked": false}], "0.6.3": [{"comment_text": "", "digests": {"md5": "e65e20d59a9f2720f19ed40817329e92", "sha256": "7f860f139b8d1b19e4578768881be939d7d43acf6d776c6b2c81716e3fc5c7ab"}, "downloads": -1, "filename": "texcptulz-0.6.3-py3-none-any.whl", "has_sig": false, "md5_digest": "e65e20d59a9f2720f19ed40817329e92", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 10992, "upload_time": "2019-12-27T11:20:52", "upload_time_iso_8601": "2019-12-27T11:20:52.116233Z", "url": "https://files.pythonhosted.org/packages/c5/69/f493fd44379476c74e24a13e03d0bc9509c26404b6fdceee96dd02722802/texcptulz-0.6.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ea586eea38d81870da9710245b5d85b3", "sha256": "fd8d1c2afb9ce40a4493f2946c120f1e0d9c2f02730db32d18c81a69b745e029"}, "downloads": -1, "filename": "texcptulz-0.6.3.tar.gz", "has_sig": false, "md5_digest": "ea586eea38d81870da9710245b5d85b3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 9516, "upload_time": "2019-12-27T11:20:57", "upload_time_iso_8601": "2019-12-27T11:20:57.526724Z", "url": "https://files.pythonhosted.org/packages/c9/86/d55aa636018e94556a052a6f0fdd394ed411de77173f20c7068f180b077b/texcptulz-0.6.3.tar.gz", "yanked": false}], "0.6.4": [{"comment_text": "", "digests": {"md5": "9f185ddf9d3a7f25ac67a542b3ab2600", "sha256": "eb8ef4590399189a5dcde580f7ca4849dac763c232fa15937e52208f943ed2cd"}, "downloads": -1, "filename": "texcptulz-0.6.4-py3-none-any.whl", "has_sig": false, "md5_digest": "9f185ddf9d3a7f25ac67a542b3ab2600", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11203, "upload_time": "2020-01-12T11:39:17", "upload_time_iso_8601": "2020-01-12T11:39:17.953269Z", "url": "https://files.pythonhosted.org/packages/e4/39/f505588ab440da9108d6289ce218bc39dbe03a53fcc137e75bf265deb68d/texcptulz-0.6.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f0a06cd34408e6636289b8c72c8e10c9", "sha256": "aa1360096b688910a7484336865bfc9a63604fda0ff1ecb3f72f0cf84e52caf7"}, "downloads": -1, "filename": "texcptulz-0.6.4.tar.gz", "has_sig": false, "md5_digest": "f0a06cd34408e6636289b8c72c8e10c9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 9715, "upload_time": "2020-01-12T11:39:24", "upload_time_iso_8601": "2020-01-12T11:39:24.616380Z", "url": "https://files.pythonhosted.org/packages/6f/c5/eee2b1bf4972905578ec5e6da0d15f68f0052db3049c30aa992ebb5f6dd5/texcptulz-0.6.4.tar.gz", "yanked": false}], "0.6.5": [{"comment_text": "", "digests": {"md5": "c5380156758b90ad905f839f703dead1", "sha256": "63a296634783c421062b61d240cfd08f71287149c57ca7cda929501e75ec9003"}, "downloads": -1, "filename": "texcptulz-0.6.5-py3-none-any.whl", "has_sig": false, "md5_digest": "c5380156758b90ad905f839f703dead1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11194, "upload_time": "2020-01-12T11:54:12", "upload_time_iso_8601": "2020-01-12T11:54:12.433443Z", "url": "https://files.pythonhosted.org/packages/84/cb/9fdcc53afa8d67172ea810f0d1f8ec32c0b0345d207ed488456cf82f64d4/texcptulz-0.6.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "be0a2f941b72e57af148a438bac414ea", "sha256": "496285c60fc65768956b50ed7c02d6dbb01d31b73cfa5ca497a27e6ce142d3c4"}, "downloads": -1, "filename": "texcptulz-0.6.5.tar.gz", "has_sig": false, "md5_digest": "be0a2f941b72e57af148a438bac414ea", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 9710, "upload_time": "2020-01-12T11:54:19", "upload_time_iso_8601": "2020-01-12T11:54:19.706539Z", "url": "https://files.pythonhosted.org/packages/ec/47/743f8f8c821d79aa4e0f0dd0bfba5531ee8a0f2f966c5fb2acd1e81008bd/texcptulz-0.6.5.tar.gz", "yanked": false}], "1.0": [{"comment_text": "", "digests": {"md5": "4d76c47247dbb0d17fa0ce596321cc3f", "sha256": "4a995a0b11d387b59a611b36945daed6a69e85d1d5e68d5111484ce37db97bbc"}, "downloads": -1, "filename": "texcptulz-1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "4d76c47247dbb0d17fa0ce596321cc3f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11563, "upload_time": "2020-02-20T18:16:25", "upload_time_iso_8601": "2020-02-20T18:16:25.821580Z", "url": "https://files.pythonhosted.org/packages/c5/5a/3014e617f1391a13632b698673e7965e0cb8a6336a26e21626c339db8dca/texcptulz-1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f1409b1722dfa3df4f9201993ee47b05", "sha256": "2a1244407adc3689ce920319ab6f831fb7d37f808229ee18543b1bb6aed4a7be"}, "downloads": -1, "filename": "texcptulz-1.0.tar.gz", "has_sig": false, "md5_digest": "f1409b1722dfa3df4f9201993ee47b05", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10052, "upload_time": "2020-02-20T18:16:32", "upload_time_iso_8601": "2020-02-20T18:16:32.879839Z", "url": "https://files.pythonhosted.org/packages/70/6d/a23eda62559e68300af5c0cc1fd964d1ceb3dcb3fb631544fc8bf24731f2/texcptulz-1.0.tar.gz", "yanked": false}], "2.0": [{"comment_text": "", "digests": {"md5": "5f3216956be39ac302d8c7d942f157eb", "sha256": "d432cab636c2aca4dc84b37ee632c1a2a67a34d307f0be4cd5d87636ca59cf16"}, "downloads": -1, "filename": "texcptulz-2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "5f3216956be39ac302d8c7d942f157eb", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11677, "upload_time": "2020-02-21T19:03:45", "upload_time_iso_8601": "2020-02-21T19:03:45.861917Z", "url": "https://files.pythonhosted.org/packages/74/5f/d0ebaba397e712d78620784bb4c018641ddadeee190c7054564a8172adf4/texcptulz-2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "073f321974fcd1c21b44d036fad11c0e", "sha256": "16c6ce7782b7a94b8cce0320911bb1f3a51bdd71121cfaa619243e040e685c03"}, "downloads": -1, "filename": "texcptulz-2.0.tar.gz", "has_sig": false, "md5_digest": "073f321974fcd1c21b44d036fad11c0e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10241, "upload_time": "2020-02-21T19:03:53", "upload_time_iso_8601": "2020-02-21T19:03:53.951870Z", "url": "https://files.pythonhosted.org/packages/97/30/7658ccde57a7157728f118a41853aeaddaf0930c7c7e0a4b7091ac92fd87/texcptulz-2.0.tar.gz", "yanked": false}], "2.0.1": [{"comment_text": "", "digests": {"md5": "3bedaa68066eb4037a2bd9bc893ca6bf", "sha256": "808b8e6b0fc5577ab34e4d2c3e7d142a0e35d64a958512f1a074d726e46f53ec"}, "downloads": -1, "filename": "texcptulz-2.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "3bedaa68066eb4037a2bd9bc893ca6bf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 11708, "upload_time": "2020-02-21T19:34:27", "upload_time_iso_8601": "2020-02-21T19:34:27.698076Z", "url": "https://files.pythonhosted.org/packages/c8/72/ae5e003a40dca8bb6faa0c63b04a3b1e8c2ad2ff9e36359215484dbc982d/texcptulz-2.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2ef018cbf9aa3595afc82d8d8d1f86c6", "sha256": "e71e822840777980d6158c473434ba6e6c084fc4fe597beb739bd29bc2f6c7bf"}, "downloads": -1, "filename": "texcptulz-2.0.1.tar.gz", "has_sig": false, "md5_digest": "2ef018cbf9aa3595afc82d8d8d1f86c6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10253, "upload_time": "2020-02-21T19:34:35", "upload_time_iso_8601": "2020-02-21T19:34:35.732125Z", "url": "https://files.pythonhosted.org/packages/e3/61/019b7e39e525e5a8f8d9537869b3ac62dc098d0fa1b13c01cef5d176532c/texcptulz-2.0.1.tar.gz", "yanked": false}], "2.5.0": [{"comment_text": "", "digests": {"md5": "a4cef7dc91559cf1de30b8a90c1a8bc3", "sha256": "2ef59f07c3aec0abbec8f4b0e7929943da2097eefd790176ebf1e0d05a59dec2"}, "downloads": -1, "filename": "texcptulz-2.5.0-py3-none-any.whl", "has_sig": false, "md5_digest": "a4cef7dc91559cf1de30b8a90c1a8bc3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 12386, "upload_time": "2020-02-22T12:46:47", "upload_time_iso_8601": "2020-02-22T12:46:47.104741Z", "url": "https://files.pythonhosted.org/packages/3e/d6/1f89ce05871654bcbd3b8136e0866814d86a7460369d7f5163d98da5e7ab/texcptulz-2.5.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b10717b85fda900a9901285a50c3d4fb", "sha256": "e550b1172f8bd7df9bb1fb50f9d443554613c70e1f253b65a4e50ffd8e6aa8aa"}, "downloads": -1, "filename": "texcptulz-2.5.0.tar.gz", "has_sig": false, "md5_digest": "b10717b85fda900a9901285a50c3d4fb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10877, "upload_time": "2020-02-22T12:46:56", "upload_time_iso_8601": "2020-02-22T12:46:56.341932Z", "url": "https://files.pythonhosted.org/packages/fe/c4/4e6ebf495cf9c2e19cddb588d17942783177ee761f6dbe1161ed20d90418/texcptulz-2.5.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a4cef7dc91559cf1de30b8a90c1a8bc3", "sha256": "2ef59f07c3aec0abbec8f4b0e7929943da2097eefd790176ebf1e0d05a59dec2"}, "downloads": -1, "filename": "texcptulz-2.5.0-py3-none-any.whl", "has_sig": false, "md5_digest": "a4cef7dc91559cf1de30b8a90c1a8bc3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 12386, "upload_time": "2020-02-22T12:46:47", "upload_time_iso_8601": "2020-02-22T12:46:47.104741Z", "url": "https://files.pythonhosted.org/packages/3e/d6/1f89ce05871654bcbd3b8136e0866814d86a7460369d7f5163d98da5e7ab/texcptulz-2.5.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b10717b85fda900a9901285a50c3d4fb", "sha256": "e550b1172f8bd7df9bb1fb50f9d443554613c70e1f253b65a4e50ffd8e6aa8aa"}, "downloads": -1, "filename": "texcptulz-2.5.0.tar.gz", "has_sig": false, "md5_digest": "b10717b85fda900a9901285a50c3d4fb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 10877, "upload_time": "2020-02-22T12:46:56", "upload_time_iso_8601": "2020-02-22T12:46:56.341932Z", "url": "https://files.pythonhosted.org/packages/fe/c4/4e6ebf495cf9c2e19cddb588d17942783177ee761f6dbe1161ed20d90418/texcptulz-2.5.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:55:10 2020"}