{"info": {"author": "Nimar Blume", "author_email": "l.nimar.b@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Science/Research", "License :: OSI Approved :: GNU General Public License (GPL)", "Operating System :: MacOS", "Operating System :: Microsoft :: Windows", "Operating System :: OS Independent", "Operating System :: POSIX", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Mathematics"], "description": "# Influence Functions for PyTorch\n\nThis is a PyTorch reimplementation of Influence Functions from the ICML2017 best paper:\n[Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730) by Pang Wei Koh and Percy Liang.\nThe reference implementation can be found here: [link](https://github.com/kohpangwei/influence-release).\n\n- [Why Use Influence Functions?](#why-use-influence-functions)\n- [Requirements](#requirements)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Background and Documentation](#background-and-documentation)\n  - [config](#config)\n    - [Misc parameters](#misc-parameters)\n    - [Calculation parameters](#calculation-parameters)\n      - [s_test](#stest)\n  - [Modes of computation](#modes-of-computation)\n  - [Output variables](#output-variables)\n    - [Influences](#influences)\n    - [Harmful](#harmful)\n    - [Helpful](#helpful)\n- [Roadmap](#roadmap)\n  - [v0.2](#v02)\n  - [v0.3](#v03)\n  - [v0.4](#v04)\n\n## Why Use Influence Functions?\n\nInfluence functions help you to debug the results of your deep learning model\nin terms of the dataset. When testing for a single test image, you can then\ncalculate which training images had the largest result on the classification\noutcome. Thus, you can easily find mislabeled images in your dataset, or\ncompress your dataset slightly to the most influential images important for\nyour individual test dataset. That can increase prediction accuracy, reduce\ntraining time, and reduce memory requirements. For more details please see\nthe original paper linked [here](https://arxiv.org/abs/1703.04730).\n\nInfluence functions can of course also be used for data other than images,\nas long as you have a supervised learning problem.\n\n## Requirements\n\n* Python 3.6 or later\n* PyTorch 1.0 or later\n* NumPy 1.12 or later\n\nTo run the tests, further requirements are:\n\n* torchvision 0.3 or later\n* PIL\n\n## Installation\n\nYou can either install this package directly through pip:\n\n```bash\npip3 install --user pytorch-influence-functions\n```\n\nOr you can clone the repo and import it as a package after it's in your `PATH`.\n\n## Usage\n\nCalculating the influence of the individual samples of your training dataset\non the final predictions is straight forward.\n\nThe most barebones way of getting the code to run is like this:\n\n```python\nimport pytorch_influence_functions as ptif\n\n# Supplied by the user:\nmodel = get_my_model()\ntrainloader, testloader = get_my_dataloaders()\n\nptif.init_logging()\nconfig = ptif.get_default_config()\n\ninfluences, harmful, helpful = ptif.calc_img_wise(config, model, trainloader, testloader)\n\n# do someting with influences/harmful/helpful\n```\n\nHere, `config` contains default values for the influence function calculation\nwhich can of course be changed. For details and examples, look [here](#config).\n\n## Background and Documentation\n\nThe precision of the output can be adjusted by using more iterations and/or\nmore recursions when approximating the influence.\n\n### config\n\n`config` is a dict which contains the parameters used to calculate the\ninfluences. You can get the default `config` by calling `ptif.get_default_config()`.\n\nI recommend you to change the following parameters to your liking. The list\nbelow is divided into parameters affecting the calculation and parameters\naffecting everything else.\n\n#### Misc parameters\n\n* `save_pth`: Default `None`, folder where to save `s_test` and `grad_z` files\n  if saving is desired\n* `outdir`: folder name to which the result json files are written\n* `log_filename`: Default `None`, if set the output will be logged to this\n  file in addition to `stdout`.\n\n#### Calculation parameters\n\n* `seed`: Default = 42, random seed for numpy, random, pytorch\n* `gpu`: Default = -1, `-1` for calculation on the CPU otherwise GPU id\n* `calc_method`: Default = img_wise, choose between the two calculation methods\n  outlined [here](#modes-of-computation).\n* `DataLoader` object for the desired dataset\n  * `train_loader` and `test_loader`\n* `test_sample_start_per_class`: Default = False, per class index from where to\n  start to calculate the influence function. If `False`, it will start from `0`.\n  This is useful if you want to calculate the influence function of a whole\n  test dataset and manually split the calculation up over multiple threads/\n  machines/gpus. Then, you can start at various points in the dataset.\n* `test_sample_num`: Default = False, number of samples per class\n  starting from the `test_sample_start_per_class` to calculate the influence\n  function for. E.g. if your dataset has 10 classes and you set this value to\n  `1`, then the influence functions will be calculated for `10 * 1` test\n  samples, one per class. If `False`, calculates the influence for all images.\n\n##### s_test\n\n* `recursion_depth`: Default = 5000, recursion depth for the `s_test` calculation.\n  Greater recursion depth improves precision.\n* `r`: Default = 1, number of `s_test` calculations to take the average of.\n  Greater r averaging improves precision.\n* Combined, the original paper suggests that `recursion_depth * r` should equal\n  the training dataset size, thus the above values of `r = 10` and\n  `recursion_depth = 5000` are valid for CIFAR-10 with a training dataset size\n  of 50000 items.\n* `damp`: Default = 0.01, damping factor during `s_test` calculation.\n* `scale`: Default = 25, scaling factor during `s_test` calculation.\n\n### Modes of computation\n\nThis packages offers two modes of computation to calculate the influence\nfunctions. The first mode is called `calc_img_wise`, during which the two\nvalues `s_test` and `grad_z` for each training image are computed on the fly\nwhen calculating the influence of that single image. The algorithm moves then\non to the next image. The second mode is called `calc_all_grad_then_test` and\ncalculates the `grad_z` values for all images first and saves them to disk.\nThen, it'll calculate all `s_test` values and save those to disk. Subsequently,\nthe algorithm will then calculate the influence functions for all images by\nreading both values from disk and calculating the influence base on them. This\ncan take significant amounts of disk space (100s of GBs) but with a fast SSD\ncan speed up the calculation significantly as no duplicate calculations take\nplace. This is the case because `grad_z` has to be calculated twice, once for\nthe first approximation in `s_test` and once to combine with the `s_test`\nvector to calculate the influence. Most importantnly however, `s_test` is only\ndependent on the test sample(s). While one `grad_z` is used to estimate the\ninitial value of the Hessian during the `s_test` calculation, this is\ninsignificant. `grad_z` on the other hand is only dependent on the training\nsample. Thus, in the `calc_img_wise` mode, we throw away all `grad_z`\ncalculations even if we could reuse them for all subsequent `s_test`\ncalculations, which could potentially be 10s of thousands. However, as stated\nabove, keeping the `grad_z`s only makes sense if they can be loaded faster/\nkept in RAM than calculating them on-the-fly.\n\n**TL;DR**: The recommended way is using `calc_img_wise` unless you have a crazy\nfast SSD, lots of free storage space, and want to calculate the influences on\nthe prediction outcomes of an entire dataset or even >1000 test samples.\n\n### Output variables\n\nVisualised, the output can look like this:\n\n![influences for ship on cifar10-resnet](figs/inf_resnet_basic_110_ship_1.png)\n\nThe test image on the top left is test image for which the influences were\ncalculated. To get the correct test outcome of _ship_, the Helpful images from\nthe training dataset were the most helpful, whereas the Harmful images were the\nmost harmful. Here, we used CIFAR-10 as dataset. The model was ResNet-110. The\nnumbers above the images show the actual influence value which was calculated.\n\nThe next figure shows the same but for a different model, DenseNet-100/12.\nThus, we can see that different models learn more from different images.\n\n![influences for ship on cifar10-densenet](figs/inf_densenet_BC_100_12_ship_1.png)\n\n#### Influences\n\nIs a dict/json containting the influences calculated of all training data\nsamples for each test data sample. The dict structure looks similiar to this:\n\n```python\n{\n    \"0\": {\n        \"label\": 3,\n        \"num_in_dataset\": 0,\n        \"time_calc_influence_s\": 129.6417362689972,\n        \"influence\": [\n            -0.00016939856868702918,\n            4.3426321099104825e-06,\n            -9.501376189291477e-05,\n            ...\n        ],\n        \"harmful\": [\n            31527,\n            5110,\n            47217,\n            ...\n        ],\n        \"helpful\": [\n            5287,\n            22736,\n            3598,\n            ...\n        ]\n    },\n    \"1\": {\n        \"label\": 8,\n        \"num_in_dataset\": 1,\n        \"time_calc_influence_s\": 121.8709237575531,\n        \"influence\": [\n            3.993639438704122e-06,\n            3.454859779594699e-06,\n            -3.5805194329441292e-06,\n            ...\n```\n\n#### Harmful\n\nHarmful is a list of numbers, which are the IDs of the training data samples\nordered by harmfulness. If the influence function is calculated for multiple\ntest images, the harmfulness is ordered by average harmfullness to the\nprediction outcome of the processed test samples.\n\n#### Helpful\n\nHelpful is a list of numbers, which are the IDs of the training data samples\nordered by helpfulness. If the influence function is calculated for multiple\ntest images, the helpfulness is ordered by average helpfulness to the\nprediction outcome of the processed test samples.\n\n## Roadmap\n\n### v0.2\n\n* [x] makes variable names etc. dataset independent\n* [x] remove all dataset name checks from the code\n* [ ] ability to disable shell output eg for `display_progress` from the config\n* [ ] add proper result plotting support\n* [ ] add a dataloader for training on the most influential samples only\n* [x] add some visualisation of the outcome\n* [ ] add recreation of some graphs of the original paper to verify\n  implementation\n\n### v0.3\n\n* [ ] make the config a class, so that it can readjust itself, for example\n  when the `r` and `recursion_depth` values can be lowered without big impact\n* [ ] check killing data augmentation!?\n* [ ] in `calc_influence_function.py` in `load_s_test`, `load_grad_z` don't\n  hard code the filenames\n\n### v0.4\n\n* [ ] integrate myPy type annotations (static type checking)\n* [ ] Use multiprocessing to calc the influence\n* [ ] use `r\"doc\"` docstrings like pytorch", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/nimarb/pytorch_influence_functions", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "pytorch-influence-functions", "package_url": "https://pypi.org/project/pytorch-influence-functions/", "platform": "", "project_url": "https://pypi.org/project/pytorch-influence-functions/", "project_urls": {"Homepage": "https://github.com/nimarb/pytorch_influence_functions"}, "release_url": "https://pypi.org/project/pytorch-influence-functions/0.1.1/", "requires_dist": null, "requires_python": ">=3.6", "summary": "This package is a plug-n-play PyTorch reimplementation of Influence Functions. Influence Functions were introduced in the paper Understanding Black-box Predictions via Influence Functions by Pang Wei Koh and Percy Liang (ICML2017).", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Influence Functions for PyTorch</h1>\n<p>This is a PyTorch reimplementation of Influence Functions from the ICML2017 best paper:\n<a href=\"https://arxiv.org/abs/1703.04730\" rel=\"nofollow\">Understanding Black-box Predictions via Influence Functions</a> by Pang Wei Koh and Percy Liang.\nThe reference implementation can be found here: <a href=\"https://github.com/kohpangwei/influence-release\" rel=\"nofollow\">link</a>.</p>\n<ul>\n<li><a href=\"#why-use-influence-functions\" rel=\"nofollow\">Why Use Influence Functions?</a></li>\n<li><a href=\"#requirements\" rel=\"nofollow\">Requirements</a></li>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a></li>\n<li><a href=\"#usage\" rel=\"nofollow\">Usage</a></li>\n<li><a href=\"#background-and-documentation\" rel=\"nofollow\">Background and Documentation</a>\n<ul>\n<li><a href=\"#config\" rel=\"nofollow\">config</a>\n<ul>\n<li><a href=\"#misc-parameters\" rel=\"nofollow\">Misc parameters</a></li>\n<li><a href=\"#calculation-parameters\" rel=\"nofollow\">Calculation parameters</a>\n<ul>\n<li><a href=\"#stest\" rel=\"nofollow\">s_test</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#modes-of-computation\" rel=\"nofollow\">Modes of computation</a></li>\n<li><a href=\"#output-variables\" rel=\"nofollow\">Output variables</a>\n<ul>\n<li><a href=\"#influences\" rel=\"nofollow\">Influences</a></li>\n<li><a href=\"#harmful\" rel=\"nofollow\">Harmful</a></li>\n<li><a href=\"#helpful\" rel=\"nofollow\">Helpful</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#roadmap\" rel=\"nofollow\">Roadmap</a>\n<ul>\n<li><a href=\"#v02\" rel=\"nofollow\">v0.2</a></li>\n<li><a href=\"#v03\" rel=\"nofollow\">v0.3</a></li>\n<li><a href=\"#v04\" rel=\"nofollow\">v0.4</a></li>\n</ul>\n</li>\n</ul>\n<h2>Why Use Influence Functions?</h2>\n<p>Influence functions help you to debug the results of your deep learning model\nin terms of the dataset. When testing for a single test image, you can then\ncalculate which training images had the largest result on the classification\noutcome. Thus, you can easily find mislabeled images in your dataset, or\ncompress your dataset slightly to the most influential images important for\nyour individual test dataset. That can increase prediction accuracy, reduce\ntraining time, and reduce memory requirements. For more details please see\nthe original paper linked <a href=\"https://arxiv.org/abs/1703.04730\" rel=\"nofollow\">here</a>.</p>\n<p>Influence functions can of course also be used for data other than images,\nas long as you have a supervised learning problem.</p>\n<h2>Requirements</h2>\n<ul>\n<li>Python 3.6 or later</li>\n<li>PyTorch 1.0 or later</li>\n<li>NumPy 1.12 or later</li>\n</ul>\n<p>To run the tests, further requirements are:</p>\n<ul>\n<li>torchvision 0.3 or later</li>\n<li>PIL</li>\n</ul>\n<h2>Installation</h2>\n<p>You can either install this package directly through pip:</p>\n<pre>pip3 install --user pytorch-influence-functions\n</pre>\n<p>Or you can clone the repo and import it as a package after it's in your <code>PATH</code>.</p>\n<h2>Usage</h2>\n<p>Calculating the influence of the individual samples of your training dataset\non the final predictions is straight forward.</p>\n<p>The most barebones way of getting the code to run is like this:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">pytorch_influence_functions</span> <span class=\"k\">as</span> <span class=\"nn\">ptif</span>\n\n<span class=\"c1\"># Supplied by the user:</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">get_my_model</span><span class=\"p\">()</span>\n<span class=\"n\">trainloader</span><span class=\"p\">,</span> <span class=\"n\">testloader</span> <span class=\"o\">=</span> <span class=\"n\">get_my_dataloaders</span><span class=\"p\">()</span>\n\n<span class=\"n\">ptif</span><span class=\"o\">.</span><span class=\"n\">init_logging</span><span class=\"p\">()</span>\n<span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"n\">ptif</span><span class=\"o\">.</span><span class=\"n\">get_default_config</span><span class=\"p\">()</span>\n\n<span class=\"n\">influences</span><span class=\"p\">,</span> <span class=\"n\">harmful</span><span class=\"p\">,</span> <span class=\"n\">helpful</span> <span class=\"o\">=</span> <span class=\"n\">ptif</span><span class=\"o\">.</span><span class=\"n\">calc_img_wise</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">trainloader</span><span class=\"p\">,</span> <span class=\"n\">testloader</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># do someting with influences/harmful/helpful</span>\n</pre>\n<p>Here, <code>config</code> contains default values for the influence function calculation\nwhich can of course be changed. For details and examples, look <a href=\"#config\" rel=\"nofollow\">here</a>.</p>\n<h2>Background and Documentation</h2>\n<p>The precision of the output can be adjusted by using more iterations and/or\nmore recursions when approximating the influence.</p>\n<h3>config</h3>\n<p><code>config</code> is a dict which contains the parameters used to calculate the\ninfluences. You can get the default <code>config</code> by calling <code>ptif.get_default_config()</code>.</p>\n<p>I recommend you to change the following parameters to your liking. The list\nbelow is divided into parameters affecting the calculation and parameters\naffecting everything else.</p>\n<h4>Misc parameters</h4>\n<ul>\n<li><code>save_pth</code>: Default <code>None</code>, folder where to save <code>s_test</code> and <code>grad_z</code> files\nif saving is desired</li>\n<li><code>outdir</code>: folder name to which the result json files are written</li>\n<li><code>log_filename</code>: Default <code>None</code>, if set the output will be logged to this\nfile in addition to <code>stdout</code>.</li>\n</ul>\n<h4>Calculation parameters</h4>\n<ul>\n<li><code>seed</code>: Default = 42, random seed for numpy, random, pytorch</li>\n<li><code>gpu</code>: Default = -1, <code>-1</code> for calculation on the CPU otherwise GPU id</li>\n<li><code>calc_method</code>: Default = img_wise, choose between the two calculation methods\noutlined <a href=\"#modes-of-computation\" rel=\"nofollow\">here</a>.</li>\n<li><code>DataLoader</code> object for the desired dataset\n<ul>\n<li><code>train_loader</code> and <code>test_loader</code></li>\n</ul>\n</li>\n<li><code>test_sample_start_per_class</code>: Default = False, per class index from where to\nstart to calculate the influence function. If <code>False</code>, it will start from <code>0</code>.\nThis is useful if you want to calculate the influence function of a whole\ntest dataset and manually split the calculation up over multiple threads/\nmachines/gpus. Then, you can start at various points in the dataset.</li>\n<li><code>test_sample_num</code>: Default = False, number of samples per class\nstarting from the <code>test_sample_start_per_class</code> to calculate the influence\nfunction for. E.g. if your dataset has 10 classes and you set this value to\n<code>1</code>, then the influence functions will be calculated for <code>10 * 1</code> test\nsamples, one per class. If <code>False</code>, calculates the influence for all images.</li>\n</ul>\n<h5>s_test</h5>\n<ul>\n<li><code>recursion_depth</code>: Default = 5000, recursion depth for the <code>s_test</code> calculation.\nGreater recursion depth improves precision.</li>\n<li><code>r</code>: Default = 1, number of <code>s_test</code> calculations to take the average of.\nGreater r averaging improves precision.</li>\n<li>Combined, the original paper suggests that <code>recursion_depth * r</code> should equal\nthe training dataset size, thus the above values of <code>r = 10</code> and\n<code>recursion_depth = 5000</code> are valid for CIFAR-10 with a training dataset size\nof 50000 items.</li>\n<li><code>damp</code>: Default = 0.01, damping factor during <code>s_test</code> calculation.</li>\n<li><code>scale</code>: Default = 25, scaling factor during <code>s_test</code> calculation.</li>\n</ul>\n<h3>Modes of computation</h3>\n<p>This packages offers two modes of computation to calculate the influence\nfunctions. The first mode is called <code>calc_img_wise</code>, during which the two\nvalues <code>s_test</code> and <code>grad_z</code> for each training image are computed on the fly\nwhen calculating the influence of that single image. The algorithm moves then\non to the next image. The second mode is called <code>calc_all_grad_then_test</code> and\ncalculates the <code>grad_z</code> values for all images first and saves them to disk.\nThen, it'll calculate all <code>s_test</code> values and save those to disk. Subsequently,\nthe algorithm will then calculate the influence functions for all images by\nreading both values from disk and calculating the influence base on them. This\ncan take significant amounts of disk space (100s of GBs) but with a fast SSD\ncan speed up the calculation significantly as no duplicate calculations take\nplace. This is the case because <code>grad_z</code> has to be calculated twice, once for\nthe first approximation in <code>s_test</code> and once to combine with the <code>s_test</code>\nvector to calculate the influence. Most importantnly however, <code>s_test</code> is only\ndependent on the test sample(s). While one <code>grad_z</code> is used to estimate the\ninitial value of the Hessian during the <code>s_test</code> calculation, this is\ninsignificant. <code>grad_z</code> on the other hand is only dependent on the training\nsample. Thus, in the <code>calc_img_wise</code> mode, we throw away all <code>grad_z</code>\ncalculations even if we could reuse them for all subsequent <code>s_test</code>\ncalculations, which could potentially be 10s of thousands. However, as stated\nabove, keeping the <code>grad_z</code>s only makes sense if they can be loaded faster/\nkept in RAM than calculating them on-the-fly.</p>\n<p><strong>TL;DR</strong>: The recommended way is using <code>calc_img_wise</code> unless you have a crazy\nfast SSD, lots of free storage space, and want to calculate the influences on\nthe prediction outcomes of an entire dataset or even &gt;1000 test samples.</p>\n<h3>Output variables</h3>\n<p>Visualised, the output can look like this:</p>\n<p><img alt=\"influences for ship on cifar10-resnet\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0158103a4a058d92609e53cf8449979abc0afa75/666967732f696e665f7265736e65745f62617369635f3131305f736869705f312e706e67\"></p>\n<p>The test image on the top left is test image for which the influences were\ncalculated. To get the correct test outcome of <em>ship</em>, the Helpful images from\nthe training dataset were the most helpful, whereas the Harmful images were the\nmost harmful. Here, we used CIFAR-10 as dataset. The model was ResNet-110. The\nnumbers above the images show the actual influence value which was calculated.</p>\n<p>The next figure shows the same but for a different model, DenseNet-100/12.\nThus, we can see that different models learn more from different images.</p>\n<p><img alt=\"influences for ship on cifar10-densenet\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/59ceaa56900240acfe63c5322828b07c7ef2a77c/666967732f696e665f64656e73656e65745f42435f3130305f31325f736869705f312e706e67\"></p>\n<h4>Influences</h4>\n<p>Is a dict/json containting the influences calculated of all training data\nsamples for each test data sample. The dict structure looks similiar to this:</p>\n<pre><span class=\"p\">{</span>\n    <span class=\"s2\">\"0\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">\"label\"</span><span class=\"p\">:</span> <span class=\"mi\">3</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"num_in_dataset\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"time_calc_influence_s\"</span><span class=\"p\">:</span> <span class=\"mf\">129.6417362689972</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"influence\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"o\">-</span><span class=\"mf\">0.00016939856868702918</span><span class=\"p\">,</span>\n            <span class=\"mf\">4.3426321099104825e-06</span><span class=\"p\">,</span>\n            <span class=\"o\">-</span><span class=\"mf\">9.501376189291477e-05</span><span class=\"p\">,</span>\n            <span class=\"o\">...</span>\n        <span class=\"p\">],</span>\n        <span class=\"s2\">\"harmful\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"mi\">31527</span><span class=\"p\">,</span>\n            <span class=\"mi\">5110</span><span class=\"p\">,</span>\n            <span class=\"mi\">47217</span><span class=\"p\">,</span>\n            <span class=\"o\">...</span>\n        <span class=\"p\">],</span>\n        <span class=\"s2\">\"helpful\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"mi\">5287</span><span class=\"p\">,</span>\n            <span class=\"mi\">22736</span><span class=\"p\">,</span>\n            <span class=\"mi\">3598</span><span class=\"p\">,</span>\n            <span class=\"o\">...</span>\n        <span class=\"p\">]</span>\n    <span class=\"p\">},</span>\n    <span class=\"s2\">\"1\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s2\">\"label\"</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"num_in_dataset\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"time_calc_influence_s\"</span><span class=\"p\">:</span> <span class=\"mf\">121.8709237575531</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"influence\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"mf\">3.993639438704122e-06</span><span class=\"p\">,</span>\n            <span class=\"mf\">3.454859779594699e-06</span><span class=\"p\">,</span>\n            <span class=\"o\">-</span><span class=\"mf\">3.5805194329441292e-06</span><span class=\"p\">,</span>\n            <span class=\"o\">...</span>\n</pre>\n<h4>Harmful</h4>\n<p>Harmful is a list of numbers, which are the IDs of the training data samples\nordered by harmfulness. If the influence function is calculated for multiple\ntest images, the harmfulness is ordered by average harmfullness to the\nprediction outcome of the processed test samples.</p>\n<h4>Helpful</h4>\n<p>Helpful is a list of numbers, which are the IDs of the training data samples\nordered by helpfulness. If the influence function is calculated for multiple\ntest images, the helpfulness is ordered by average helpfulness to the\nprediction outcome of the processed test samples.</p>\n<h2>Roadmap</h2>\n<h3>v0.2</h3>\n<ul>\n<li>[x] makes variable names etc. dataset independent</li>\n<li>[x] remove all dataset name checks from the code</li>\n<li>[ ] ability to disable shell output eg for <code>display_progress</code> from the config</li>\n<li>[ ] add proper result plotting support</li>\n<li>[ ] add a dataloader for training on the most influential samples only</li>\n<li>[x] add some visualisation of the outcome</li>\n<li>[ ] add recreation of some graphs of the original paper to verify\nimplementation</li>\n</ul>\n<h3>v0.3</h3>\n<ul>\n<li>[ ] make the config a class, so that it can readjust itself, for example\nwhen the <code>r</code> and <code>recursion_depth</code> values can be lowered without big impact</li>\n<li>[ ] check killing data augmentation!?</li>\n<li>[ ] in <code>calc_influence_function.py</code> in <code>load_s_test</code>, <code>load_grad_z</code> don't\nhard code the filenames</li>\n</ul>\n<h3>v0.4</h3>\n<ul>\n<li>[ ] integrate myPy type annotations (static type checking)</li>\n<li>[ ] Use multiprocessing to calc the influence</li>\n<li>[ ] use <code>r\"doc\"</code> docstrings like pytorch</li>\n</ul>\n\n          </div>"}, "last_serial": 6431006, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "5cfaea93d85c89a0160ae0a248a929a9", "sha256": "191d04854d2e835706fbb0f5164a6edf6644f9a762ff1696b9fe662637814375"}, "downloads": -1, "filename": "pytorch_influence_functions-0.1.0.tar.gz", "has_sig": false, "md5_digest": "5cfaea93d85c89a0160ae0a248a929a9", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 16246, "upload_time": "2019-11-30T15:37:35", "upload_time_iso_8601": "2019-11-30T15:37:35.658121Z", "url": "https://files.pythonhosted.org/packages/df/f2/95ab6f9bf72762c870de6ee8488cbb813a7ca90db370f9152be15a69b47b/pytorch_influence_functions-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "4630403a99cbf18634f5b4caae1fbad6", "sha256": "6ec4730ad2e0f38e5deb34d60979df5439f4c60b92a0e6390bc50f444f749670"}, "downloads": -1, "filename": "pytorch_influence_functions-0.1.1.tar.gz", "has_sig": false, "md5_digest": "4630403a99cbf18634f5b4caae1fbad6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 18486, "upload_time": "2020-01-10T15:12:10", "upload_time_iso_8601": "2020-01-10T15:12:10.137397Z", "url": "https://files.pythonhosted.org/packages/68/3e/b023b7c3fdbfbeced7399f65556b8cc2e07d5e37b915e939deade4ce7c9e/pytorch_influence_functions-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "4630403a99cbf18634f5b4caae1fbad6", "sha256": "6ec4730ad2e0f38e5deb34d60979df5439f4c60b92a0e6390bc50f444f749670"}, "downloads": -1, "filename": "pytorch_influence_functions-0.1.1.tar.gz", "has_sig": false, "md5_digest": "4630403a99cbf18634f5b4caae1fbad6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 18486, "upload_time": "2020-01-10T15:12:10", "upload_time_iso_8601": "2020-01-10T15:12:10.137397Z", "url": "https://files.pythonhosted.org/packages/68/3e/b023b7c3fdbfbeced7399f65556b8cc2e07d5e37b915e939deade4ce7c9e/pytorch_influence_functions-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:13:51 2020"}