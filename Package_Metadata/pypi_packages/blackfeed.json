{"info": {"author": "Ibragim Abubakarov", "author_email": "ibragim.ai95@gmail.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Internet :: File Transfer Protocol (FTP)", "Topic :: Internet :: WWW/HTTP"], "description": "# BlackFeed\n> BlackFeed is a micro python library that allows you download and upload files concurrently.\n> You can download your files locally but you can also upload them to your cloud without writing them to disk.\n\n### Packages required\n> Installed automatically with **pip**\n- requests\n- boto3\n\n## Install\n```bash\npip install blackfeed\n```\n\n## Usage\nDownload and upload files to AWS S3\n**For this to work, AWS CLI must be configured**\n```python\nfrom blackfeed.downloader import Downloader\nfrom blackfeed.adapter.s3 import S3Adapter\n\nqueue = [\n    {\n        'url': 'https://www.example.com/path/to/image.jpg', # Required\n        'destination': 'some/key/image.jpg' # S3 key - Required \n    },{\n        'url': 'https://www.example.com/path/to/image2.jpg',\n        'destination': 'some/key/image2.jpg' \n    }\n]\n\ndownloader = Downloader(\n    S3Adapter(bucket='bucketname'),\n    multi=True, # If true, uploads files to images to S3 with multithreading\n    stateless=False # If set to False, it generates and stores md5 hashes of files in a file\n    state_id='flux_states' # name of the file where hashes will be stored (states.txt) not required\n    bulksize=200 # Number of concurrent downloads\n)\ndownloader.process(queue)\nstats = downloader.get_stats() # Returns a dict with information about the process\n```\n\n### Download files with states\nLoading states can be useful if you don't want to re-download the same file twice.\n```python\nfrom blackfeed.downloader import Downloader\nfrom blackfeed.adapter.s3 import S3Adapter\n\nqueue = [\n...\n]\n\ndownloader = Downloader(\n    S3Adapter(bucket='bucketname'),\n    multi=True,\n    stateless=False,\n    state_id='filename'\n)\n\n# You can add a callback function if needed\n# This function will be called after each bulk is processed\ndef callback(responses):\n    # response: {\n    #    'destination': destination of the file can be local or can be S3 key,\n    #    'url': URL from where the file was downloaded,\n    #    'httpcode': HTTP code returned by the server,\n    #    'status': True|False,\n    #    'content-type': Mime type of the downloaded resource Example: image/jpeg\n    # }\n    # responses: response[]\n\n    pass # Your logic\n\ndownloader.set_callback(callback)\n\ndownloader.load_states('filename') # This will load states from \"filename.txt\"\ndownloader.process(queue)\nstats = downloader.get_stats() # Statistics \n```\n\n## ElasticDownloader\n> Let's you to download/retrieve files from FTP, SFTP and HTTP/S servers easily.\n\n### Examples\n#### Downloading file from FTP \n```python\nfrom blackfeed.elasticdownloader import ElasticDownloader\n\nuri = 'ftp://user:password@ftp.server.com/path/to/file.csv'\n\nretriever = ElasticDownloader()\nres = retriever.download(uri, localpath='/tmp/myfile.csv') # localfile is optional\n# .download() function returns False if there was an error or return the local path of the downloaded file if it was a success.\nprint(res)\n```\n```bash\n/tmp/myfile.csv\n```\n\n### Retrieving binary content of file from FTP\n```python\nfrom blackfeed.elasticdownloader import ElasticDownloader\n\nuri = 'ftp://user:password@ftp.server.com/path/to/file.csv'\n\nretriever = ElasticDownloader()\nres = retriever.retrieve(uri) # Return type: io.BytesIO | False\n\nwith open('/tmp/myfile.csv', 'wb') as f:\n    f.write(res.getvalue())\n```\n**ElasticDownloader** can handle FTP, SFTP and HTTP URIs automatically.\nUse the method **download** to download file locally and use the **retrieve** method to get the binary content of a file.\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ibragim64/blackfeed", "keywords": "", "license": "", "maintainer": "Ibragim Abubakarov", "maintainer_email": "ibragim.ai95@gmail.com", "name": "blackfeed", "package_url": "https://pypi.org/project/blackfeed/", "platform": "", "project_url": "https://pypi.org/project/blackfeed/", "project_urls": {"Homepage": "https://github.com/ibragim64/blackfeed"}, "release_url": "https://pypi.org/project/blackfeed/0.0.18/", "requires_dist": ["requests", "boto3", "pysftp"], "requires_python": "", "summary": "A python package that allows the download of thousands of files concurrently", "version": "0.0.18", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>BlackFeed</h1>\n<blockquote>\n<p>BlackFeed is a micro python library that allows you download and upload files concurrently.\nYou can download your files locally but you can also upload them to your cloud without writing them to disk.</p>\n</blockquote>\n<h3>Packages required</h3>\n<blockquote>\n<p>Installed automatically with <strong>pip</strong></p>\n</blockquote>\n<ul>\n<li>requests</li>\n<li>boto3</li>\n</ul>\n<h2>Install</h2>\n<pre>pip install blackfeed\n</pre>\n<h2>Usage</h2>\n<p>Download and upload files to AWS S3\n<strong>For this to work, AWS CLI must be configured</strong></p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">blackfeed.downloader</span> <span class=\"kn\">import</span> <span class=\"n\">Downloader</span>\n<span class=\"kn\">from</span> <span class=\"nn\">blackfeed.adapter.s3</span> <span class=\"kn\">import</span> <span class=\"n\">S3Adapter</span>\n\n<span class=\"n\">queue</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n        <span class=\"s1\">'url'</span><span class=\"p\">:</span> <span class=\"s1\">'https://www.example.com/path/to/image.jpg'</span><span class=\"p\">,</span> <span class=\"c1\"># Required</span>\n        <span class=\"s1\">'destination'</span><span class=\"p\">:</span> <span class=\"s1\">'some/key/image.jpg'</span> <span class=\"c1\"># S3 key - Required </span>\n    <span class=\"p\">},{</span>\n        <span class=\"s1\">'url'</span><span class=\"p\">:</span> <span class=\"s1\">'https://www.example.com/path/to/image2.jpg'</span><span class=\"p\">,</span>\n        <span class=\"s1\">'destination'</span><span class=\"p\">:</span> <span class=\"s1\">'some/key/image2.jpg'</span> \n    <span class=\"p\">}</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">downloader</span> <span class=\"o\">=</span> <span class=\"n\">Downloader</span><span class=\"p\">(</span>\n    <span class=\"n\">S3Adapter</span><span class=\"p\">(</span><span class=\"n\">bucket</span><span class=\"o\">=</span><span class=\"s1\">'bucketname'</span><span class=\"p\">),</span>\n    <span class=\"n\">multi</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"c1\"># If true, uploads files to images to S3 with multithreading</span>\n    <span class=\"n\">stateless</span><span class=\"o\">=</span><span class=\"kc\">False</span> <span class=\"c1\"># If set to False, it generates and stores md5 hashes of files in a file</span>\n    <span class=\"n\">state_id</span><span class=\"o\">=</span><span class=\"s1\">'flux_states'</span> <span class=\"c1\"># name of the file where hashes will be stored (states.txt) not required</span>\n    <span class=\"n\">bulksize</span><span class=\"o\">=</span><span class=\"mi\">200</span> <span class=\"c1\"># Number of concurrent downloads</span>\n<span class=\"p\">)</span>\n<span class=\"n\">downloader</span><span class=\"o\">.</span><span class=\"n\">process</span><span class=\"p\">(</span><span class=\"n\">queue</span><span class=\"p\">)</span>\n<span class=\"n\">stats</span> <span class=\"o\">=</span> <span class=\"n\">downloader</span><span class=\"o\">.</span><span class=\"n\">get_stats</span><span class=\"p\">()</span> <span class=\"c1\"># Returns a dict with information about the process</span>\n</pre>\n<h3>Download files with states</h3>\n<p>Loading states can be useful if you don't want to re-download the same file twice.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">blackfeed.downloader</span> <span class=\"kn\">import</span> <span class=\"n\">Downloader</span>\n<span class=\"kn\">from</span> <span class=\"nn\">blackfeed.adapter.s3</span> <span class=\"kn\">import</span> <span class=\"n\">S3Adapter</span>\n\n<span class=\"n\">queue</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n<span class=\"o\">...</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">downloader</span> <span class=\"o\">=</span> <span class=\"n\">Downloader</span><span class=\"p\">(</span>\n    <span class=\"n\">S3Adapter</span><span class=\"p\">(</span><span class=\"n\">bucket</span><span class=\"o\">=</span><span class=\"s1\">'bucketname'</span><span class=\"p\">),</span>\n    <span class=\"n\">multi</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">stateless</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"n\">state_id</span><span class=\"o\">=</span><span class=\"s1\">'filename'</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># You can add a callback function if needed</span>\n<span class=\"c1\"># This function will be called after each bulk is processed</span>\n<span class=\"k\">def</span> <span class=\"nf\">callback</span><span class=\"p\">(</span><span class=\"n\">responses</span><span class=\"p\">):</span>\n    <span class=\"c1\"># response: {</span>\n    <span class=\"c1\">#    'destination': destination of the file can be local or can be S3 key,</span>\n    <span class=\"c1\">#    'url': URL from where the file was downloaded,</span>\n    <span class=\"c1\">#    'httpcode': HTTP code returned by the server,</span>\n    <span class=\"c1\">#    'status': True|False,</span>\n    <span class=\"c1\">#    'content-type': Mime type of the downloaded resource Example: image/jpeg</span>\n    <span class=\"c1\"># }</span>\n    <span class=\"c1\"># responses: response[]</span>\n\n    <span class=\"k\">pass</span> <span class=\"c1\"># Your logic</span>\n\n<span class=\"n\">downloader</span><span class=\"o\">.</span><span class=\"n\">set_callback</span><span class=\"p\">(</span><span class=\"n\">callback</span><span class=\"p\">)</span>\n\n<span class=\"n\">downloader</span><span class=\"o\">.</span><span class=\"n\">load_states</span><span class=\"p\">(</span><span class=\"s1\">'filename'</span><span class=\"p\">)</span> <span class=\"c1\"># This will load states from \"filename.txt\"</span>\n<span class=\"n\">downloader</span><span class=\"o\">.</span><span class=\"n\">process</span><span class=\"p\">(</span><span class=\"n\">queue</span><span class=\"p\">)</span>\n<span class=\"n\">stats</span> <span class=\"o\">=</span> <span class=\"n\">downloader</span><span class=\"o\">.</span><span class=\"n\">get_stats</span><span class=\"p\">()</span> <span class=\"c1\"># Statistics </span>\n</pre>\n<h2>ElasticDownloader</h2>\n<blockquote>\n<p>Let's you to download/retrieve files from FTP, SFTP and HTTP/S servers easily.</p>\n</blockquote>\n<h3>Examples</h3>\n<h4>Downloading file from FTP</h4>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">blackfeed.elasticdownloader</span> <span class=\"kn\">import</span> <span class=\"n\">ElasticDownloader</span>\n\n<span class=\"n\">uri</span> <span class=\"o\">=</span> <span class=\"s1\">'ftp://user:password@ftp.server.com/path/to/file.csv'</span>\n\n<span class=\"n\">retriever</span> <span class=\"o\">=</span> <span class=\"n\">ElasticDownloader</span><span class=\"p\">()</span>\n<span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"n\">retriever</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">(</span><span class=\"n\">uri</span><span class=\"p\">,</span> <span class=\"n\">localpath</span><span class=\"o\">=</span><span class=\"s1\">'/tmp/myfile.csv'</span><span class=\"p\">)</span> <span class=\"c1\"># localfile is optional</span>\n<span class=\"c1\"># .download() function returns False if there was an error or return the local path of the downloaded file if it was a success.</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">res</span><span class=\"p\">)</span>\n</pre>\n<pre>/tmp/myfile.csv\n</pre>\n<h3>Retrieving binary content of file from FTP</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">blackfeed.elasticdownloader</span> <span class=\"kn\">import</span> <span class=\"n\">ElasticDownloader</span>\n\n<span class=\"n\">uri</span> <span class=\"o\">=</span> <span class=\"s1\">'ftp://user:password@ftp.server.com/path/to/file.csv'</span>\n\n<span class=\"n\">retriever</span> <span class=\"o\">=</span> <span class=\"n\">ElasticDownloader</span><span class=\"p\">()</span>\n<span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"n\">retriever</span><span class=\"o\">.</span><span class=\"n\">retrieve</span><span class=\"p\">(</span><span class=\"n\">uri</span><span class=\"p\">)</span> <span class=\"c1\"># Return type: io.BytesIO | False</span>\n\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">'/tmp/myfile.csv'</span><span class=\"p\">,</span> <span class=\"s1\">'wb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">res</span><span class=\"o\">.</span><span class=\"n\">getvalue</span><span class=\"p\">())</span>\n</pre>\n<p><strong>ElasticDownloader</strong> can handle FTP, SFTP and HTTP URIs automatically.\nUse the method <strong>download</strong> to download file locally and use the <strong>retrieve</strong> method to get the binary content of a file.</p>\n\n          </div>"}, "last_serial": 7001081, "releases": {"0.0.17": [{"comment_text": "", "digests": {"md5": "3913e964dc0e4d11da2407385c2bbd52", "sha256": "b79b611c631c123f183857bac0a592053ba8ded5330c9826d4177420c9f2fe67"}, "downloads": -1, "filename": "blackfeed-0.0.17-py3-none-any.whl", "has_sig": false, "md5_digest": "3913e964dc0e4d11da2407385c2bbd52", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 11148, "upload_time": "2020-04-11T11:07:50", "upload_time_iso_8601": "2020-04-11T11:07:50.602841Z", "url": "https://files.pythonhosted.org/packages/cf/42/f3afd1ced6050919ea0f4cf49681fb76ce9a95e9d53b2186595b6d78da06/blackfeed-0.0.17-py3-none-any.whl", "yanked": false}], "0.0.18": [{"comment_text": "", "digests": {"md5": "353c186c0323deb407a6a80a5bde4d9e", "sha256": "ae64ae26e34bc6c751c51897338abf98b9bf1c858cfa80efb4e17a88db93deb4"}, "downloads": -1, "filename": "blackfeed-0.0.18-py3-none-any.whl", "has_sig": false, "md5_digest": "353c186c0323deb407a6a80a5bde4d9e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 11193, "upload_time": "2020-04-11T18:40:43", "upload_time_iso_8601": "2020-04-11T18:40:43.907785Z", "url": "https://files.pythonhosted.org/packages/0f/e3/723d91c4e9b3fca432ea5032ddd928ccb041ddbda7d3ade87eb4d96f613d/blackfeed-0.0.18-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "353c186c0323deb407a6a80a5bde4d9e", "sha256": "ae64ae26e34bc6c751c51897338abf98b9bf1c858cfa80efb4e17a88db93deb4"}, "downloads": -1, "filename": "blackfeed-0.0.18-py3-none-any.whl", "has_sig": false, "md5_digest": "353c186c0323deb407a6a80a5bde4d9e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 11193, "upload_time": "2020-04-11T18:40:43", "upload_time_iso_8601": "2020-04-11T18:40:43.907785Z", "url": "https://files.pythonhosted.org/packages/0f/e3/723d91c4e9b3fca432ea5032ddd928ccb041ddbda7d3ade87eb4d96f613d/blackfeed-0.0.18-py3-none-any.whl", "yanked": false}], "timestamp": "Thu May  7 22:37:10 2020"}