{"info": {"author": "Davide Moro", "author_email": "davide.moro@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Framework :: Pytest", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: Implementation :: CPython", "Topic :: Software Development :: Testing"], "description": "pytest-play\n===========\n\n.. image:: https://travis-ci.org/pytest-dev/pytest-play.svg?branch=master\n    :target: https://travis-ci.org/pytest-dev/pytest-play\n    :alt: See Build Status on Travis CI\n\n.. image:: https://readthedocs.org/projects/pytest-play/badge/?version=latest\n    :target: http://pytest-play.readthedocs.io/en/latest/?badge=latest\n    :alt: Documentation Status\n\n.. image:: https://codecov.io/gh/pytest-dev/pytest-play/branch/master/graph/badge.svg\n    :target: https://codecov.io/gh/pytest-dev/pytest-play\n\n``pytest-play`` is a codeless, generic, pluggable and extensible **automation tool**,\nnot necessarily **test automation** only, based on the fantastic pytest_ test framework\nthat let you define and execute YAML_ files containing scripts or test scenarios\nthrough actions and assertions that can be implemented and managed even by **non technical users**:\n\n* automation (not necessarily test automation). You can build a set of actions on a single file (e.g,\n  call a JSON based API endpoint, perform an action if a condition matches) or a test automation\n  project with many test scenarios.\n\n  For example you can create always fresh test data on demand supporting\n  manual testing activities, build a live simulator and so on\n\n* codeless, or better almost codeless. If you have to write assertions against action results or some\n  conditional expressions you need a very basic knowledge of Python or Javascript expressions\n  with a smooth learning curve (something like ``variables['foo'] == 'bar'``)\n\n* generic. It is not yet again another automation tool for browser automation only, API only, etc.\n  You can drive a browser, perform some API calls, make database queries and/or make assertions\n  using the same tool for different technologies\n\n  So there are several free or not free testing frameworks or automation tools and many times\n  they address just one single area testing needs and they are not extensible: API testing only,\n  UI testing only and so on. It could be fine if you are testing a web\n  only application like a CMS but if you are dealing with a reactive IoT application you might something more,\n  make cross actions or cross checks against different systems or build something of more complex upon\n  ``pytest-play``\n\n* powerful. It is not yet again another test automation tool, it only extends the pytest_ framework\n  with another paradigm and inherits a lot of good stuff (test data decoupled by test implementation\n  that let you write once and executed many times the same scenario thanks to native parametrization\n  support, reporting, integration with test management tools, many useful command line options, browsers and\n  remote Selenium grids integration, etc)\n\n* pluggable and extensible. Let's say you need to interact with a system not yet supported by a ``pytest-play``\n  plugin, you can write by your own or pay someone for you. In addition there is a scaffolding tool that\n  let you implement your own command: https://github.com/davidemoro/cookiecutter-play-plugin\n  \n* easy to use. Why YAML? Easy to read, easy to write, simple and standard syntax, easy to be validated and\n  no parentheses hell. Despite there are no recording tools (not yet) for browser interaction or API calls, the\n  documentation based on very common patterns let you copy, paste and edit command by command with no pain\n\n* free software. It's an open source project based on the large and friendly pytest_ community\n\n* easy to install. The only prerequisite is Docker thanks to the ``davidemoro/pytest-play`` Docker Hub container.\n  Or better, with docker, no installation is required: you just need to type the following command\n  ``docker run -i --rm -v $(pwd):/src davidemoro/pytest-play`` inside your project folder\n  See https://hub.docker.com/r/davidemoro/pytest-play\n\nSee at the bottom of the page the third party plugins that extends ``pytest-play``:\n\n* `Third party pytest-play plugins`_\n\nHow it works\n------------\n\nDepending on your needs and skills you can choose to use pytest-play programmatically\nwriting some Python code or following a Python-less approach.\n\nAs said before with pytest-play_ you will be able to create codeless scripts or test scenarios\nwith no or very little Python knowledge: a file ``test_XXX.yml`` (e.g., ``test_something.yml``,\nwhere ``test_`` and ``.yml`` matter) will be automatically recognized and executed without having\nto touch any ``*.py`` module. \n\nYou can run a single scenario with ``pytest test_XXX.yml`` or running the entire suite filtering\nby name or keyword markers.\n\nDespite ``pytest-play`` was born with native support for JSON format, ``pytest-play``>=2.0 versions will support\nYAML only for improved usability.\n\nPython-less (pure YAML)\n=======================\n\nHere you can see the contents of a ``pytest-play`` project without any Python files inside\ncontaining a login scenario::\n\n  $ tree\n  .\n  \u251c\u2500\u2500 env-ALPHA.yml    (OPTIONAL)\n  \u2514\u2500\u2500 test_login.yml\n\nand you might have some global variables in a settings file specific for a target environment::  \n  \n  $ cat env-ALPHA.yml \n  pytest-play:\n    base_url: https://www.yoursite.com\n\nThe test scenario with action, assertions and optional metadata\n(play_selenium_ external plugin needed)::\n  \n  $ cat test_login.yml\n  ---\n  markers:\n    - login\n  test_data:\n    - username: siteadmin\n      password: siteadmin\n    - username: editor\n      password: editor\n    - username: reader\n      password: reader\n  ---\n  - comment: visit base url\n    type: get\n    provider: selenium\n    url: \"$base_url\"\n  - comment: click on login link\n    locator:\n      type: id\n      value: personaltools-login\n    type: clickElement\n    provider: selenium\n  - comment: provide a username\n    locator:\n      type: id\n      value: __ac_name\n    text: \"$username\"\n    type: setElementText\n    provider: selenium\n  - comment: provide a password\n    locator:\n      type: id\n      value: __ac_password\n    text: \"$password\"\n    type: setElementText\n    provider: selenium\n  - comment: click on login submit button\n    locator:\n      type: css\n      value: \".pattern-modal-buttons > input[name=submit]\"\n    type: clickElement\n    provider: selenium\n  - comment: wait for page loaded\n    locator:\n      type: css\n      value: \".icon-user\"\n    type: waitForElementVisible\n    provider: selenium\n\nThe first optional YAML document contains some metadata with keywords aka ``markers``\nso you can filter tests to be executed invoking pytest with marker expressions,\ndecoupled test data, etc.\n\nThe same ``test_login.yml`` scenario will be executed 3 times with different\ndecoupled test data ``test_data`` defined inside its first optional YAML\ndocument (the block between the 2 ``---`` lines).\n\nSo write once and execute many times with different test data!\n\nYou can see a hello world example here:\n\n* https://github.com/davidemoro/pytest-play-plone-example\n\nAs told before the metadata document is optional so you might have 1 or 2\ndocuments in your YAML file. You can find more info about `Metadata format`_.\n\nHere you can see the same example without the metadata section for sake of\ncompleteness::\n\n  ---\n  - comment: visit base url\n    type: get\n    provider: selenium\n    url: \"http://YOURSITE\"\n  - comment: click on login link\n    type: clickElement\n    provider: selenium\n    locator:\n      type: id\n      value: personaltools-login\n  - comment: provide a username\n    type: setElementText\n    provider: selenium\n    locator:\n      type: id\n      value: __ac_name\n    text: \"YOURUSERNAME\"\n  - comment: provide a password\n    type: setElementText\n    provider: selenium\n    locator:\n      type: id\n      value: __ac_password\n    text: \"YOURPASSWORD\"\n  - comment: click on login submit button\n    type: clickElement\n    provider: selenium\n    locator:\n      type: css\n      value: \".pattern-modal-buttons > input[name=submit]\"\n  - comment: wait for page loaded\n    type: waitForElementVisible\n    provider: selenium\n    locator:\n      type: css\n      value: \".icon-user\"\n\nProgrammatically\n================\n\nYou can invoke pytest-play programmatically too. \n\nYou can define a test ``test_login.py`` like this::\n\n  def test_login(play):\n      data = play.get_file_contents(\n          'my', 'path', 'etc', 'login.yml')\n      play.execute_raw(data, extra_variables={})\n\nOr this programmatical approach might be used if you are\nimplementing BDD based tests using ``pytest-bdd``.\n\nCore commands\n-------------\n\npytest-play_ provides some core commands that let you:\n\n* write simple Python assertions, expressions and variables\n\n* reuse steps including other test scenario scripts\n\n* provide a default command template for some particular providers\n  (eg: add by default HTTP authentication headers for all requests)\n\n* a generic wait until machinery. Useful for waiting for an\n  observable asynchronous event will complete its flow before\n  proceeding with the following commands that depends on the previous\n  step completion\n\nYou can write restricted Python expressions and assertions based on the ``RestrictedPython`` package.\n\nRestrictedPython_ is a tool that helps to define a subset of the Python\nlanguage which allows to provide a program input into a trusted environment.\nRestrictedPython is not a sandbox system or a secured environment, but it helps\nto define a trusted environment and execute untrusted code inside of it.\n\nSee:\n\n* https://github.com/zopefoundation/RestrictedPython\n\nHow to reuse steps\n==================\n\nYou can split your commands and reuse them using the ``include`` command avoiding\nduplication::\n\n    - provider: include\n      type: include\n      path: \"/some-path/included-scenario.yml\"\n\n\nYou can create a variable for the base folder where your test scripts live.\n\nDefault commands\n================\n\nSome commands require many verbose options you don't want to repeat (eg: authentication headers for play_requests_).\n\nInstead of replicating all the headers information you can initialize a ``pytest-play`` with the provider name as\nkey and as a value the default command you want to omit (this example neets the external plugin play_selenium_)::\n\n    - provider: python\n      type: store_variable\n      name: bearer\n      expression: \"'BEARER'\"\n    - provider: python\n      type: store_variable\n      name: play_requests\n      expression: \"{'parameters': {'headers': {'Authorization': '$bearer'}}}\"\n    - provider: play_requests\n      type: GET\n      comment: this is an authenticated request!\n      url: \"$base_url\"\n\n\nStore variables\n===============\n\nYou can store a pytest-play_ variables::\n\n    - provider: python\n      type: store_variable\n      expression: \"1+1\"\n      name: foo\n\nMake a Python assertion\n=======================\n\nYou can make an assertion based on a Python expression::\n\n    - provider: python\n      type: assert\n      expression: variables['foo'] == 2\n\nSleep\n=====\n\nSleep for a given amount of seconds::\n\n    - provider: python\n      type: sleep\n      seconds: 2\n\nExec a Python expresssion\n=========================\n\nYou can execute a Python expression::\n\n    - provider: python\n      type: exec\n      expression: \"1+1\"\n\nWhile condition and looping\n===========================\n\nIf you need to loop over a series of commands or wait something you can use\nthe ``while`` command. It will execute the sequence of sub commands, if any,\nwhile the resulting expression condition is true. Assuming you have a ``countdown``\nvariable containing a integer ``10``, the block of commands whill be executed 10 times::\n\n    ---\n    - provider: python\n      type: while\n      expression: variables['countdown'] >= 0\n      timeout: 2.3\n      poll: 0.1\n      sub_commands:\n      - provider: python\n        type: store_variable\n        name: countdown\n        expression: variables['countdown'] - 1\n\nThe ``while`` command supersedes the other legacy commands ``wait_until``\n(stops when the condition becomes true) or ``wait_until_not``.\ncommands.\n\n\nConditional commands (Python)\n=============================\n\nYou can skip any command evaluating a Python based skip condition\nlike the following::\n\n    - provider: include\n      type: include\n      path: \"/some-path/assertions.yml\"\n      skip_condition: variables['cassandra_assertions'] is True\n\nHow to assert commands elapsed time\n===================================\n\nThe engine updates a ``pytest-play`` variable called ``_elapsed``\nfor each executed command. So you can write something that::\n\n    ---\n    - type: GET\n      provider: play_requests\n      url: https://api.chucknorris.io/jokes/categories\n      expression: \"'dev' in response.json()\"\n    - type: assert\n      provider: python\n      expression: \"variables['_elapsed'] > 0\"\n\nGenerate a JUnit XML report\n===========================\n\nUse the ``--junit-xml`` command line option, e.g.::\n\n    --junit-xml results.xml\n\nYou'll get for each test case errors, commands executed in ``system-output`` (do not use ``-s`` or ``--capture=no`` otherwise you won't\nsee commands in ``system-output``) and execution timing metrics (global, per test case and per single command thanks to ``_elapsed`` property tracked on every executed command shown in ``system-output``).\n\nHere you can see a standard ``results.xml`` file::\n\n    <?xml version=\"1.0\" encoding=\"utf-8\"?><testsuite errors=\"0\" failures=\"0\" name=\"pytest\" skipped=\"0\" tests=\"1\" time=\"0.360\"><testcase classname=\"test_assertion.yml\" file=\"test_assertion.yml\" name=\"test_assertion.yml\" time=\"0.326\"><system-out>{&apos;expression&apos;: &apos;1 == 1&apos;, &apos;provider&apos;: &apos;python&apos;, &apos;type&apos;: &apos;assert&apos;, &apos;_elapsed&apos;: 0.0003077983856201172}\n    {&apos;expression&apos;: &apos;0 == 0&apos;, &apos;provider&apos;: &apos;python&apos;, &apos;type&apos;: &apos;assert&apos;, &apos;_elapsed&apos;: 0.0002529621124267578}\n    </system-out></testcase></testsuite>\n\nGenerate a custom JUnit XML report with custom properties and execution times metrics\n=====================================================================================\n\nYou can track execution time metrics for monitoring and measure\nwhat is important to you. For example you can track using a machine interpretable format:\n\n* response times (e.g., how much time is needed for returning a ``POST`` json payload)\n\n* time that occurs between the invocation of an API and a reactive web application update or some asynchronous data appearing on an event store\n\n* time that occurs between a user input on browser and results updated (e.g., a live search)\n\n* time that occurs between a login button and the page loaded an usable (e.g., how much time is needed after a browser action to click on a target button)\n\nTrack response time metric in JUnit XML report\n----------------------------------------------\n\nFor example, a ``test_categories.yml`` file executed with\nthe command line option ``--junit-xml report.xml`` (requires play_requests_ plugin)::\n\n    test_data:\n      - category: dev\n      - category: movie\n      - category: food\n    ---\n    - type: GET\n      provider: play_requests\n      url: https://api.chucknorris.io/jokes/categories\n      expression: \"'$category' in response.json()\"\n    - provider: metrics\n      type: record_elapsed\n      name: categories_time\n    - type: assert\n      provider: python\n      expression: \"variables['categories_time'] < 2.5\"\n      comment: you can make an assertion against the categories_time\n\nwill generate an extended ``report.xml`` file with custom properties like that::\n\n    <?xml version=\"1.0\" encoding=\"utf-8\"?><testsuite errors=\"0\" failures=\"0\" name=\"pytest\" skipped=\"0\" tests=\"3\" time=\"2.031\"><testcase classname=\"test_categories.yml\" file=\"test_categories.yml\" name=\"test_categories.yml0\" time=\"0.968\"><properties><property name=\"categories_time\" value=\"0.5829994678497314\"/></properties><system-out>{&apos;expression&apos;: &quot;&apos;dev&apos; in response.json()&quot;, &apos;provider&apos;: &apos;play_requests&apos;, &apos;type&apos;: &apos;GET&apos;, &apos;url&apos;: &apos;https://api.chucknorris.io/jokes/categories&apos;, &apos;_elapsed&apos;: 0.5829994678497314}\n    {&apos;name&apos;: &apos;categories_time&apos;, &apos;provider&apos;: &apos;metrics&apos;, &apos;type&apos;: &apos;record_elapsed&apos;, &apos;_elapsed&apos;: 3.3855438232421875e-05}\n    {&apos;comment&apos;: &apos;you can make an assertion against the categories_time&apos;, &apos;expression&apos;: &quot;variables[&apos;categories_time&apos;] &lt; 2.5&quot;, &apos;provider&apos;: &apos;python&apos;, &apos;type&apos;: &apos;assert&apos;, &apos;_elapsed&apos;: 0.0006382465362548828}\n    </system-out></testcase><testcase classname=\"test_categories.yml\" file=\"test_categories.yml\" name=\"test_categories.yml1\" time=\"0.481\"><properties><property name=\"categories_time\" value=\"0.4184422492980957\"/></properties><system-out>{&apos;expression&apos;: &quot;&apos;movie&apos; in response.json()&quot;, &apos;provider&apos;: &apos;play_requests&apos;, &apos;type&apos;: &apos;GET&apos;, &apos;url&apos;: &apos;https://api.chucknorris.io/jokes/categories&apos;, &apos;_elapsed&apos;: 0.4184422492980957}\n    {&apos;name&apos;: &apos;categories_time&apos;, &apos;provider&apos;: &apos;metrics&apos;, &apos;type&apos;: &apos;record_elapsed&apos;, &apos;_elapsed&apos;: 2.09808349609375e-05}\n    {&apos;comment&apos;: &apos;you can make an assertion against the categories_time&apos;, &apos;expression&apos;: &quot;variables[&apos;categories_time&apos;] &lt; 2.5&quot;, &apos;provider&apos;: &apos;python&apos;, &apos;type&apos;: &apos;assert&apos;, &apos;_elapsed&apos;: 0.000553131103515625}\n    </system-out></testcase><testcase classname=\"test_categories.yml\" file=\"test_categories.yml\" name=\"test_categories.yml2\" time=\"0.534\"><properties><property name=\"categories_time\" value=\"0.463592529296875\"/></properties><system-out>{&apos;expression&apos;: &quot;&apos;food&apos; in response.json()&quot;, &apos;provider&apos;: &apos;play_requests&apos;, &apos;type&apos;: &apos;GET&apos;, &apos;url&apos;: &apos;https://api.chucknorris.io/jokes/categories&apos;, &apos;_elapsed&apos;: 0.463592529296875}\n    {&apos;name&apos;: &apos;categories_time&apos;, &apos;provider&apos;: &apos;metrics&apos;, &apos;type&apos;: &apos;record_elapsed&apos;, &apos;_elapsed&apos;: 2.09808349609375e-05}\n    {&apos;comment&apos;: &apos;you can make an assertion against the categories_time&apos;, &apos;expression&apos;: &quot;variables[&apos;categories_time&apos;] &lt; 2.5&quot;, &apos;provider&apos;: &apos;python&apos;, &apos;type&apos;: &apos;assert&apos;, &apos;_elapsed&apos;: 0.00054931640625}\n    </system-out></testcase></testsuite>\n\nand the custom property ``categories_time`` will be tracked for each\ntest case execution, for example::\n\n    <properties>\n        <property name=\"categories_time\" value=\"0.5829994678497314\"/>\n    </properties>\n\nAdvanced metrics in JUnit XML report\n------------------------------------\n\nIn this example we want to measures how long it takes a page to become interactive\n(page responding to user interactions) and evaluate update time for a live search feature.\nLet's see the ``test_search.yml`` example (requires play_selenium_)::\n\n    ---\n    - provider: selenium\n      type: get\n      url: https://www.plone-demo.info/\n    - provider: metrics\n      type: record_elapsed_start\n      name: load_time\n    - provider: selenium\n      type: setElementText\n      text: plone 5\n      locator:\n        type: id\n        value: searchGadget\n    - provider: metrics\n      type: record_elapsed_stop\n      name: load_time\n    - provider: metrics\n      type: record_elapsed_start\n      name: live_search_time\n    - provider: selenium\n      type: waitForElementVisible\n      locator:\n        type: css\n        value: li[data-url$=\"https://www.plone-demo.info/front-page\"]\n    - provider: metrics\n      type: record_elapsed_stop\n      name: live_search_time\n\nIf you execute this scenario with the ``--junit-xml results.xml``\noption you'll get a ``results.xml`` file similar to this one::\n\n    <?xml version=\"1.0\" encoding=\"utf-8\"?><testsuite errors=\"0\" failures=\"0\" name=\"pytest\" skipped=\"0\" tests=\"1\" time=\"13.650\"><testcase classname=\"test_search.yml\" file=\"test_search.yml\" name=\"test_search.yml\" time=\"13.580\"><properties><property name=\"load_time\" value=\"1.1175920963287354\"/><property name=\"live_search_time\" value=\"1.0871295928955078\"/></properties><system-out>{&apos;provider&apos;: &apos;selenium&apos;, &apos;type&apos;: &apos;get&apos;, &apos;url&apos;: &apos;https://www.plone-demo.info/&apos;, &apos;_elapsed&apos;: 9.593282461166382}\n    {&apos;name&apos;: &apos;load_time&apos;, &apos;provider&apos;: &apos;metrics&apos;, &apos;type&apos;: &apos;record_elapsed_start&apos;, &apos;_elapsed&apos;: 1.1682510375976562e-05}\n    {&apos;locator&apos;: {&apos;type&apos;: &apos;id&apos;, &apos;value&apos;: &apos;searchGadget&apos;}, &apos;provider&apos;: &apos;selenium&apos;, &apos;text&apos;: &apos;plone 5&apos;, &apos;type&apos;: &apos;setElementText&apos;, &apos;_elapsed&apos;: 1.1019845008850098}\n    {&apos;name&apos;: &apos;load_time&apos;, &apos;provider&apos;: &apos;metrics&apos;, &apos;type&apos;: &apos;record_elapsed_stop&apos;, &apos;_elapsed&apos;: 1.9788742065429688e-05}\n    {&apos;name&apos;: &apos;live_search_time&apos;, &apos;provider&apos;: &apos;metrics&apos;, &apos;type&apos;: &apos;record_elapsed_start&apos;, &apos;_elapsed&apos;: 1.0013580322265625e-05}\n    {&apos;locator&apos;: {&apos;type&apos;: &apos;css&apos;, &apos;value&apos;: &apos;li[data-url$=&quot;https://www.plone-demo.info/front-page&quot;]&apos;}, &apos;provider&apos;: &apos;selenium&apos;, &apos;type&apos;: &apos;waitForElementVisible&apos;, &apos;_elapsed&apos;: 1.060795545578003}\n    {&apos;name&apos;: &apos;live_search_time&apos;, &apos;provider&apos;: &apos;metrics&apos;, &apos;type&apos;: &apos;record_elapsed_stop&apos;, &apos;_elapsed&apos;: 2.3603439331054688e-05}\n    </system-out></testcase></testsuite>\n\nand in this case you'll find out that the key metric ``load_time``\nwas ``1.11`` seconds and the ``live_search_time`` was ``1.09`` seconds as\nyou can see here::\n\n    <properties>\n        <property name=\"load_time\" value=\"1.1175920963287354\"/>\n        <property name=\"live_search_time\" value=\"1.0871295928955078\"/>\n    </properties>\n\nSo thanks to JUnit XML reporting you can track response times (not only browser based timings)\nusing a machine readable format to be ingested by third party systems with an acceptable approximation\nif you cannot track timings directly on the systems under test.\n\nTrack any property in JUnit XML reports using expressions\n---------------------------------------------------------\n\nLet's see a ``test_categories.yml`` (play_selenium_ required)::\n\n    test_data:\n      - category: dev\n      - category: movie\n      - category: food\n    ---\n    - type: GET\n      provider: play_requests\n      url: https://api.chucknorris.io/jokes/categories\n      expression: \"'$category' in response.json()\"\n    - provider: metrics\n      type: record_property\n      name: categories_time\n      expression: \"variables['_elapsed']*1000\"\n    - type: assert\n      provider: python\n      expression: \"variables['categories_time'] < 2500\"\n      comment: you can make an assertion against the categories_time\n\ngenerates some custom properties (``categories_time`` in milliseconds using a python expression)\nusing the ``--junit-xml results.xml`` cli option::\n\n    <?xml version=\"1.0\" encoding=\"utf-8\"?><testsuite errors=\"0\" failures=\"0\" name=\"pytest\" skipped=\"0\" tests=\"3\" time=\"2.312\"><testcase classname=\"test_categories.yml\" file=\"test_categories.yml\" name=\"test_categories.yml0\" time=\"1.034\"><properties><property name=\"categories_time\" value=\"610.3124618530273\"/></properties><system-out>{&apos;expression&apos;: &quot;&apos;dev&apos; in response.json()&quot;, &apos;provider&apos;: &apos;play_requests&apos;, &apos;type&apos;: &apos;GET&apos;, &apos;url&apos;: &apos;https://api.chucknorris.io/jokes/categories&apos;, &apos;_elapsed&apos;: 0.6103124618530273}\n    {&apos;expression&apos;: &quot;variables[&apos;_elapsed&apos;]*1000&quot;, &apos;provider&apos;: &apos;python&apos;, &apos;type&apos;: &apos;exec&apos;, &apos;_elapsed&apos;: 0.0006859302520751953}\n    {&apos;expression&apos;: &quot;variables[&apos;_elapsed&apos;]*1000&quot;, &apos;name&apos;: &apos;categories_time&apos;, &apos;provider&apos;: &apos;metrics&apos;, &apos;type&apos;: &apos;record_property&apos;, &apos;_elapsed&apos;: 0.006484270095825195}\n    {&apos;comment&apos;: &apos;you can make an assertion against the categories_time&apos;, &apos;expression&apos;: &quot;variables[&apos;categories_time&apos;] &lt; 2500&quot;, &apos;provider&apos;: &apos;python&apos;, &apos;type&apos;: &apos;assert&apos;, &apos;_elapsed&apos;: 0.0005526542663574219}\n    </system-out></testcase><testcase classname=\"test_categories.yml\" file=\"test_categories.yml\" name=\"test_categories.yml1\" time=\"0.550\"><properties><property name=\"categories_time\" value=\"443.72105598449707\"/></properties><system-out>{&apos;expression&apos;: &quot;&apos;movie&apos; in response.json()&quot;, &apos;provider&apos;: &apos;play_requests&apos;, &apos;type&apos;: &apos;GET&apos;, &apos;url&apos;: &apos;https://api.chucknorris.io/jokes/categories&apos;, &apos;_elapsed&apos;: 0.44372105598449707}\n    {&apos;expression&apos;: &quot;variables[&apos;_elapsed&apos;]*1000&quot;, &apos;provider&apos;: &apos;python&apos;, &apos;type&apos;: &apos;exec&apos;, &apos;_elapsed&apos;: 0.0009415149688720703}\n    {&apos;expression&apos;: &quot;variables[&apos;_elapsed&apos;]*1000&quot;, &apos;name&apos;: &apos;categories_time&apos;, &apos;provider&apos;: &apos;metrics&apos;, &apos;type&apos;: &apos;record_property&apos;, &apos;_elapsed&apos;: 0.01613616943359375}\n    {&apos;comment&apos;: &apos;you can make an assertion against the categories_time&apos;, &apos;expression&apos;: &quot;variables[&apos;categories_time&apos;] &lt; 2500&quot;, &apos;provider&apos;: &apos;python&apos;, &apos;type&apos;: &apos;assert&apos;, &apos;_elapsed&apos;: 0.0011241436004638672}\n    </system-out></testcase><testcase classname=\"test_categories.yml\" file=\"test_categories.yml\" name=\"test_categories.yml2\" time=\"0.676\"><properties><property name=\"categories_time\" value=\"576.5485763549805\"/></properties><system-out>{&apos;expression&apos;: &quot;&apos;food&apos; in response.json()&quot;, &apos;provider&apos;: &apos;play_requests&apos;, &apos;type&apos;: &apos;GET&apos;, &apos;url&apos;: &apos;https://api.chucknorris.io/jokes/categories&apos;, &apos;_elapsed&apos;: 0.5765485763549805}\n    {&apos;expression&apos;: &quot;variables[&apos;_elapsed&apos;]*1000&quot;, &apos;provider&apos;: &apos;python&apos;, &apos;type&apos;: &apos;exec&apos;, &apos;_elapsed&apos;: 0.0006375312805175781}\n    {&apos;expression&apos;: &quot;variables[&apos;_elapsed&apos;]*1000&quot;, &apos;name&apos;: &apos;categories_time&apos;, &apos;provider&apos;: &apos;metrics&apos;, &apos;type&apos;: &apos;record_property&apos;, &apos;_elapsed&apos;: 0.006584644317626953}\n    {&apos;comment&apos;: &apos;you can make an assertion against the categories_time&apos;, &apos;expression&apos;: &quot;variables[&apos;categories_time&apos;] &lt; 2500&quot;, &apos;provider&apos;: &apos;python&apos;, &apos;type&apos;: &apos;assert&apos;, &apos;_elapsed&apos;: 0.0005452632904052734}\n    </system-out></testcase></testsuite>\n\nobtaining the metrics you want to track for each execution, for example::\n\n    <properties><property name=\"categories_time\" value=\"610.3124618530273\"/></properties>\n\nso you might track the category as well for each test execution\nor whatever you want.\n\nMonitoring test metrics with statsd/graphite\n============================================\n\nIf you like the measure everything approach you can track and monitor interesting\ncustom test metrics from an end user perspective during normal test executions or\nheavy load/stress tests thanks to the statsd_/graphite_ integration.\n\nMeasuring important key metrics is important for many reasons:\n\n* compare performance between different versions under same conditions using past\n  tracked stats for the same metric (no more say the system *seems slower* today)\n\n* predict the system behaviour with many items on frontend (e.g., evaluate\n  the browser dealing with thousands and thousands of items managed by an infinite\n  scroll plugin)\n\n* predict the system behaviour under load\n\nYou can install ``statsd``/``graphite`` in minutes using Docker:\n\n* https://graphite.readthedocs.io/en/latest/install.html\n\nBasically you can track on ``statsd``/``graphite`` every **numeric** metric using\nthe same commands used for tracking metrics on JUnit XML reports as we will see.\n\nIn addition, but not required, installing the third party plugin called pytest-statsd_.\nyou can track on ``statsd``/``graphite``:\n\n* execution times\n* number of executed tests per status (pass, fail, error, etc)\n\nPrerequisites (you need to install the optional statsd client not installed by\ndefault):::\n\n    pip install pytest-play[statsd]\n\nUsage (cli options compatible with ``pytest-statsd``)::\n\n    --stats-d [--stats-prefix play --stats-host http://myserver.com --stats-port 3000]\n\nwhere:\n\n* ``--stats-d``, enable ``statsd``\n\n* ``--stats-prefix`` (optional), if you plan on having multiple projects sending\n  results to the same server.\n  For example if you provide ``play`` as prefix you'll get a time metric under\n  the ``stats.timers.play.YOURMETRIC.mean`` key (or instead of ``.mean`` you can use ``.upper``,\n  ``upper_90``, etc)\n\n* ``--stats-host``, by default ``localhost``\n\n* ``--stats-port``, by default ``8125``\n\nNow you can track timing metrics using the ``record_elapsed`` or\n``record_elapsed_start``/``record_elapsed_stop`` commands seen before (pytest-play will\nsend for you time values to ``statsd`` converted to ``milliseconds`` as requested by ``statsd``).\n\nIf you want to track custom metrics using the ``record_property`` command you have to provide\nan additional parameter called ``metric_type``. For example::\n\n    - provider: metrics\n      type: record_property\n      name: categories_time\n      expression: \"variables['_elapsed']*1000\"\n      metric_type: timing\n    - provider: metrics\n      type: record_property\n      name: fridge_temperature\n      expression: \"4\"\n      metric_type: gauge\n\nSome additional information regarding the ``record_property`` command:\n\n* if you don't provide the ``metric_type`` option in ``record_property`` commands values\n  will not be transmitted to ``statsd`` (eventually they will be tracked on JUnit XML report\n  if ``--junit-xml`` option was provided)\n\n* if you provide an allowed ``metric_type`` value (``timing`` or ``gauge``) non numeric values\n  will be considered as an error (``ValueError`` exception raised)\n\n* non allowed ``metric_type`` values will be considered as an error\n\n* if you provide ``timing`` as ``metric_type``, it's up to you providing a numeric value\n  expressed in ``milliseconds``\n\nMonitor HTTP response times\n---------------------------\n\nMonitor API response time (see https://github.com/pytest-dev/pytest-play/tree/features/examples/statsd_graphite_monitoring):\n\n.. image:: https://raw.githubusercontent.com/pytest-dev/pytest-play/features/docs/_static/statsd_graphite_monitoring.gif\n    :alt: Chuck Norris API response time\n\nBrowser metrics\n---------------\n\nMonitor browser metrics using Selenium from an end user perspective (see https://github.com/pytest-dev/pytest-play/tree/features/examples/statsd_graphite_monitoring_selenium):\n\n* from page load to page usable\n\n* live search responsiveness\n\n.. image:: https://raw.githubusercontent.com/pytest-dev/pytest-play/features/docs/_static/statsd_graphite_monitoring_selenium.gif\n    :alt: Time for first interaction after load and live search rendering timings\n\nRecord metrics programmatically\n-------------------------------\n\nIf you don't want to use ``pytest-play`` but you need to record test metrics\nyou can use ``pytest-play`` as a library:::\n\n    def test_programmatically(play):\n        play.execute_command({\n            'provider': 'metrics',\n            'type': 'record_property',\n            'name': 'oil_temperature',\n            'expression': '60',\n            'metric_type': 'gauge'})\n\nPerformance tests with pytest-play and bzt/Taurus (BlazeMeter)\n==============================================================\n\nYou can reuse all your pytest-play scenario and turn them to\nperformance tests using bzt/Taurus (so it is compatible with BlazeMeter_\ntoo and all its goodies).\n\nAdd a bzt/Taurus YAML file with no ``test_`` prefix like that (full example here in\nbzt_performance_)::\n\n    settings:\n      artifacts-dir: /tmp/%Y-%m-%d_%H-%M-%S.%f\n    \n    execution:\n    - executor: pytest\n      scenario: pytest-run\n      iterations: 1\n    \n    scenarios:\n      pytest-run:\n        # additional-args: --stats-d --stats-prefix play\n        script: scripts/\n    \n    services:\n    - module: shellexec\n      prepare:\n      - pip3 install -r https://raw.githubusercontent.com/davidemoro/pytest-play-docker/master/requirements.txt\n\nand run the following command::\n\n    docker run --rm -it -v $(pwd):/src --user root --entrypoint \"bzt\" davidemoro/pytest-play bzt.yml\n\nYou will see bzt up and running playing our scenarios:\n\n\n.. image:: https://raw.githubusercontent.com/pytest-dev/pytest-play/features/docs/_static/pytest_play_performance.png\n    :alt: Taurus/bzt running pytest-play scenarios\n\nYou can uncomment ``additional-args`` to pass other ``pytest`` command line options (e.g., enable ``statsd``\nfor key user metrics monitoring or any other cli option).\n\nMore info about bzt/Taurus here:\n\n* http://gettaurus.org/\n\nDynamic expressions in payloads without declaring variables\n===========================================================\n\nIf you have to send a certain payload to a REST endpoint or a MQTT message\ncontaining a dynamic value you can store a variable with ``store_variable``\nand use ``$variable_name`` in your payload when needed.\nStoring variables is cool if you will reuse later that value but if just have to\ngenerate a dynamic value, let's say a timestamp in milliseconds,\nyou can use the ``{! EXPRESSION !}`` format.\n\nFor example (play_mqtt_ plugin required):\n\n::\n\n    ---\n    - comment: python expressions in mqtt payload (without declaring variables)\n      provider: mqtt\n      type: publish\n      host: \"$mqtt_host\"\n      port: \"$mqtt_port\"\n      endpoint: \"$mqtt_endpoint/$device_serial_number\"\n      payload: '{\n            \"measure_id\":   [124],\n            \"obj_id_L\":     [0],\n            \"measureType\":  [\"float\"],\n            \"start_time\":   {! int(datetime.datetime.utcnow().timestamp()*1000) !},\n            \"bin_value\":    [1]\n        }'\n\nwhere instead of the expression::\n\n    {! int(datetime.datetime.utcnow().timestamp()*1000) !},\n\nwill be printed::\n\n    1553007973702\n\nBrowser based commands\n----------------------\n\nThe ``pytest-play`` core no more includes browser based commands. Moved to play_selenium_\nexternal plugin.\n\npytest-play is pluggable and extensible\n---------------------------------------\n\n``pytest-play`` has a pluggable architecture and you can extend it.\n\nFor example you might want to support your own commands, support non UI\ncommands like making raw POST/GET/etc calls, simulate IoT devices\nactivities, provide easy interaction with complex UI widgets like\ncalendar widgets, send commands to a device using the serial port implementing\na binary protocol and so on.\n\nHow to register a new command provider\n======================================\n\nLet's suppose you want to extend pytest-play with the following command::\n\n    command = {'type': 'print', 'provider': 'newprovider', 'message': 'Hello, World!'}\n\nYou just have to implement a command provider::\n\n    from pytest_play.providers import BaseProvider\n\n    class NewProvider(BaseProvider):\n\n        def this_is_not_a_command(self):\n            \"\"\" Commands should be command_ prefixed \"\"\"\n\n        def command_print(self, command):\n            print(command['message'])\n\n        def command_yetAnotherCommand(self, command):\n            print(command)\n\nand register your new provider in your ``setup.py`` adding an entrypoint::\n\n    entry_points={\n        'playcommands': [\n            'print = your_package.providers:NewProvider',\n        ],\n    },\n\nYou can define new providers also for non UI commands. For example publish MQTT\nmessages simulating IoT device activities for integration tests.\n\nIf you want you can generate a new command provider thanks to:\n\n* https://github.com/davidemoro/cookiecutter-play-plugin\n\nMetadata format\n---------------\n\nYou can also add some scenario metadata placing another YAML document on top of the scenario\ndefined on the ``test_XXX.yml`` with the following format::\n\n    ---\n    markers:\n      - marker1\n      - marker2\n    test_data:\n      - username: foo\n      - username: bar\n    ---\n    # omitted scenario steps in this example...\n\nOption details:\n\n* ``markers``, you can decorate your scenario with one or more markers. You can use them\n  in pytest command line for filtering scenarios to be executed thanks to marker\n  expressions like ``-m \"marker1 and not slow\"``\n\n* ``test_data``, enables parametrization of your decoupletd test data and let you execute\n  the same scenario many times. For example\n  the example above will be executed twice (one time with \"foo\" username and another time\n  with \"bar\")\n\nNew options will be added in the next feature (e.g., skip scenarios, xfail, xpass, etc).\n\nExamples\n--------\n\n* https://github.com/pytest-dev/pytest-play/tree/master/examples\n\n* https://github.com/davidemoro/pytest-play-docker/tree/master/tests\n\n* https://github.com/davidemoro/pytest-play-plone-example\n\n\nArticles and talks\n------------------\n\nArticles:\n\n* `Hello pytest-play!`_\n\n* `API/REST testing like Chuck Norris with pytest play using YAML`_\n\n* `pytest-play automated docker hub publishing workflow`_\n\n* `Test automation framework thoughts and examples with Python, pytest and Jenkins`_\n\n* `Testing metrics thoughts and examples: how to turn lights on and off through MQTT with pytest-play`_\n\nTalks:\n\n* `Serena Martinetti @ Pycon9 - Florence: Integration tests ready to use with pytest-play`_ \n\n* `Davide Moro @ STF2019 - Milan: Automazione e monitoraggio metriche di test in ambito IoT con pytest-play`_\n\nThird party pytest-play plugins\n-------------------------------\n\n* play_selenium_, ``pytest-play`` plugin driving browsers using Selenium/Splinter\n  under the hood. Selenium grid compatible and implicit auto wait actions\n  for more robust scenarios with less controls.\n\n* play_requests_, ``pytest-play`` plugin driving the famous Python ``requests``\n  library for making ``HTTP`` calls.\n\n* play_sql_, ``pytest-play`` support for SQL expressions and assertions\n\n* play_cassandra_, ``pytest-play`` support for Cassandra expressions and assertions\n\n* play_dynamodb_, ``pytest-play`` support for AWS DynamoDB queries and assertions\n\n* play_websocket_, ``pytest-play`` support for websockets\n\n* play_mqtt_, ``pytest-play`` plugin for MQTT support. Thanks to ``play_mqtt``\n  you can test the integration between a mocked IoT device that sends\n  commands on MQTT and a reactive web application with UI checks.\n\n  You can also build a simulator that generates messages for you.\n\n\nFeel free to add your own public plugins with a pull request!\n\n\nTwitter\n-------\n\n``pytest-play`` tweets happens here:\n\n* `@davidemoro`_\n \n\n.. _`pytest`: https://github.com/pytest-dev/pytest\n.. _`pypom_form`: http://pypom-form.readthedocs.io/en/latest/\n.. _`splinter`: https://splinter.readthedocs.io/en/latest/\n.. _`pypom`: http://pypom.readthedocs.io/en/latest/\n.. _`@davidemoro`: https://twitter.com/davidemoro\n.. _`cookiecutter-qa`: https://github.com/davidemoro/cookiecutter-qa\n.. _`play.yml`: https://github.com/davidemoro/cookiecutter-qa/blob/master/%7B%7Bcookiecutter.project_slug%7D%7D/%7B%7Bcookiecutter.project_slug%7D%7D/tests/functional/data/play.yml\n.. _`test_play.py`: https://github.com/davidemoro/cookiecutter-qa/blob/master/%7B%7Bcookiecutter.project_slug%7D%7D/%7B%7Bcookiecutter.project_slug%7D%7D/tests/functional/test_play.py\n.. _`play_mqtt`: https://github.com/davidemoro/play_mqtt\n.. _`play_selenium`: https://github.com/davidemoro/play_selenium\n.. _`play_requests`: https://github.com/davidemoro/play_requests\n.. _`play_sql`: https://github.com/davidemoro/play_sql\n.. _`play_cassandra`: https://github.com/davidemoro/play_cassandra\n.. _`play_dynamodb`: https://github.com/davidemoro/play_dynamodb\n.. _`play_websocket`: https://github.com/davidemoro/play_websocket\n.. _`RestrictedPython`: https://github.com/zopefoundation/RestrictedPython\n.. _`Serena Martinetti @ Pycon9 - Florence: Integration tests ready to use with pytest-play`: https://www.pycon.it/conference/talks/integration-tests-ready-to-use-with-pytest-play\n.. _`Davide Moro @ STF2019 - Milan: Automazione e monitoraggio metriche di test in ambito IoT con pytest-play`: https://speakerdeck.com/davidemoro/automazione-e-monitoraggio-metriche-di-test-in-ambito-iot-con-pytest-play\n.. _`Hello pytest-play!`: http://davidemoro.blogspot.it/2018/04/hello-pytest-play.html\n.. _`API/REST testing like Chuck Norris with pytest play using YAML`: https://davidemoro.blogspot.com/2019/02/api-rest-testing-pytest-play-yaml-chuck-norris.html\n.. _`YAML`: https://en.wikipedia.org/wiki/YAML\n.. _`pytest-play automated docker hub publishing workflow`: https://davidemoro.blogspot.com/2019/02/automated-docker-hub-push-travisci-pyup-python.html\n.. _`statsd`: https://github.com/statsd/statsd\n.. _`graphite`: https://github.com/graphite-project/graphite-web\n.. _`pytest-statsd`: https://github.com/jlane9/pytest-statsd\n.. _`Test automation framework thoughts and examples with Python, pytest and Jenkins`: https://davidemoro.blogspot.com/2018/03/test-automation-python-pytest-jenkins.html\n.. _`Testing metrics thoughts and examples: how to turn lights on and off through MQTT with pytest-play`: https://davidemoro.blogspot.com/2019/04/testing-metrics-thoughts-and-examples.html\n.. _`BlazeMeter`: https://www.blazemeter.com/\n.. _`bzt_performance`: https://github.com/pytest-dev/pytest-play/tree/features/examples/bzt_performance\n\nChangelog\n=========\n\n2.3.1 (2019-06-12)\n------------------\n\nBugfix:\n\n- fix compatibility with pytest 4.6. Rif #86\n\nDocumentation:\n\n- update media section (articles and talks)\n\n2.3.0 (2019-04-05)\n------------------\n\nFeatures and improvements:\n\n- ``wait_until`` and ``wait_until_not`` now accept commands with no ``sub_commands`` property\n\n- implement new ``while`` command in python provider (while expression is true)\n\n2.2.2 (2019-03-29)\n------------------\n\nMinor changes:\n\n- remove internal property parameter on engine\n\nBugfix:\n\n- add compatibility with ``pytest-repeat``'s ``--count`` command line option\n\nDocumentation:\n\n- mention how to generate dynamic values using ``{! expr !}`` expressions\n  (e.g., dynamic payloads in REST or MQTT without having to store variables\n  when not needed)\n\n\n2.2.1 (2019-03-19)\n------------------\n\nMinor changes:\n\n- add ``int`` and ``float`` builtins available in Python expressions\n\n- make python expressions more flexible for future improvements (internal change that doesn't\n  affect compatibility)\n\nBugfix:\n\n- fix ``--setup-plan`` invokation\n\nDocumentation:\n\n- add more examples (bzt/Taurus and performance tests using pytest-play)\n\n\n2.2.0 (2019-03-01)\n------------------\n\n- ``statsd`` integration (optional requirement) for advanced test metrics using statsd/graphite.\n  If you install pytest play with the optional statsd support with ``pytest-play[statsd]``\n  you will get the additional dependency ``statsd`` client and you can use the same cli\n  options defined by the ``pytest-statsd`` plugin (e.g.,\n  ``--stats-d [--stats-prefix myproject --stats-host http://myserver.com --stats-port 3000]``).\n\n  Note well: despite the above cli options are the same defined by the ``pytest-statsd`` plugin,\n  at this time of writing ``pytest-statsd`` is not a ``pytest-play`` dependency\n  so you won't get stats about number of failures, passing, etc but only stats tracked by\n  ``pytest-play``. If you need them you can install ``pytest-statsd`` (it plays well with ``pytest-play``)\n\n2.1.0 (2019-02-22)\n------------------\n\nFeatures:\n\n- support junit xml generation file with ``system-out`` element for\n  each test case execution (pytest ``--junit-xml`` option).\n  ``system-out`` will tracked by default in junit report unless you use\n  the ``--capture=no`` or its alias ``-s``\n\n- track ``_elapsed`` time for each executed command ``--junit-xml`` report\n  if ``system-out`` is enabled\n\n- track ``pytest`` custom properties in ``--junit-xml`` report for monitoring\n  and measure what is important to you. For example you can track as key metric\n  the time of the time occurred between the end of the previous action and\n  the completion of the following. Basically you can track under the ``property_name``\n  `load_login` key the time occurred between the click on the submit button\n  and the end of the current command (e.g., click on the menu or text input\n  being able to receive text) using a machine interpretable format.\n\n  The ``property_name`` value elapsed time will be available as standard ``pytest-play``\n  variable so that you can make additional assertions\n\n- after every command execution a ``pytest-play`` variable will be added/updated\n  reporting the elapsed time (accessible using ``variables['_elapsed']``).\n\n  So be aware that the ``_elapsed`` variable name should be considered as a special\n  variable and so you should not use this name for storing variables\n\n- improve debug in case of failed assertions or errored commands. Logged variables\n  dump in standard logs and ``system-out`` reporting if available\n\n- improve debuggability in case of assertion errors (log failing expression)\n\n- added a new ``metrics`` provider that let you track custom metrics in conjunction\n  with ``--junit-xml`` option. You can track in a machine readable format response\n  times, dynamic custom expressions, time that occurs between different commands\n  (e.g., measure the time needed after a login to interact with the page, time before\n  an asynchronous update happens and so on). Under the ``metrics`` provider you'll\n  find the ``record_property``, ``record_elapsed``, ``record_elapsed_start``  and\n  ``record_elapsed_stop`` commands\n\nDocumentation:\n\n- minor documentation changes\n\n- add more examples\n\n\n2.0.2 (2019-02-06)\n------------------\n\nDocumentation:\n\n- more examples\n\n- fix documentation bug on README (example based on selenium with missing ``provider: selenium``)\n\n\n2.0.1 (2019-01-30)\n------------------\n\nDocumentation:\n\n- Mention davidemoro/pytest-play docker container in README.\n  You can use pytest-play with a docker command like that now\n  ``docker run -i --rm -v $(pwd):/src davidemoro/pytest-play``\n\nBugfix:\n\n- Fix error locking pipenv due to pytest-play requirement\n  constraint not existing (RestrictedPython>=4.0.b2 -> RestrictedPython>=4.0b2)\n\n\n2.0.0 (2019-01-25)\n------------------\n\nBreaking changes:\n\n- Renamed fixture from `play_json` to `play` (#5)\n\n- Drop json support, adopt yaml only format for scenarios (#5)\n\n- Drop ``.ini`` file for metadata, if you need them you can add\n  a YAML document on top of the scenario ``.yml`` file. You no more\n  need multiple files for decorating your scenarios now (#65)\n\n- `play.execute` no more accepts raw data string), consumes a list of commands.\n  Introduced `play.execute_raw` accepting raw data string.\n\n- `play.execute_command` accepts a Python dictionary only now (not a string)\n\n- Selenium provider removed from ``pytest-play`` core, implemented on a\n  separate package ``play_selenium``. Starting from now you have to add\n  to your selenium commands ``provider: selenium``\n\n- engine's ``parametrizer_class`` attribute no more available (\n  use ``parametrizer.Parametrizer`` by default now)\n\nBug fix:\n\n- Fix invalid markup on PyPI (#55)\n\n- Fix invalid escape sequences (#62).\n\nDocumentation and trivial changes:\n\n- Add examples folder\n\n\n1.4.2 (2018-05-17)\n------------------\n\n- Configuration change on Github. Use the same branching policy adopted by\n  pytest (master becomes main branch, see #56)\n\n- Fixed skipped test and added new tests (deselect scenarios with keyword\n  and marker expressions)\n\n- Fix #58: you no more get a TypeError if you try to launch pytest-play\n  in autodiscovery mode\n\n- Fix #55: restructured text lint on README.rst (bad visualization on pypi)\n\n- Updated README (articles and talks links)\n\n- Added a ``DeprecationWarning`` for `play_json` fixture.\n  pytest-play will be based on yaml instead of json in version >=2.0.0.\n  See https://github.com/pytest-dev/pytest-play/issues/5\n\n\n1.4.1 (2018-04-06)\n------------------\n\n- Documentation improvements\n\n- Add bzt/Taurus/BlazeMeter compatibility\n\n\n1.4.0 (2018-04-05)\n------------------\n\n- Small documentation improvements\n\n- Now ``test_XXX.json`` files are automatically collected and executed\n\n- You can run a test scenario using the pytest CLI ``pytest test_YYY.json``\n\n- Introduced json test scenario ini file with markers definition. For a given\n  ``test_YYY.json`` scenario you can add a ``test_YYY.ini`` ini file::\n\n    [pytest]\n    markers =\n        marker1\n        marker2\n\n  and filter scenarios using marker expressions ``pytest -m marker1``\n\n- Enabled parametrization of arguments for a plain json scenario in scenario ini file::\n\n    [pytest]\n    test_data =\n       {\"username\": \"foo\"}\n       {\"username\": \"bar\"}\n\n  and your json scenario will be executed twice\n\n- ``pytest-play`` loads some variables based on the contents of the optional ``pytest-play``\n  section in your ``pytest-variables`` file now. So if your variables file contains the following\n  values::\n\n    pytest-play:\n      foo: bar\n      date_format: YYYYMMDD\n\n  you will be able to use expressions ``$foo``, ``$date_format``, ``variables['foo']`` or\n  ``variables['date_format']``\n\n\n1.3.2 (2018-02-05)\n------------------\n\n- Add ``sorted`` in python expressions\n\n\n1.3.1 (2018-01-31)\n------------------\n\n- Add more tests\n\n- Documentation update\n\n- play_json fixture no more assumes that you\n  have some pytest-variables settings.\n  No more mandatory\n\n- fix include scenario bug that occurs only\n  on Windows (slash vs backslash and\n  JSON decoding issues)\n\n\n1.3.0 (2018-01-22)\n------------------\n\n- documentation improvements\n\n- supports teardown callbacks\n\n\n1.2.0 (2018-01-22)\n------------------\n\n- implement python based commands in ``pytest-play`` and\n  deprecates ``play_python``.\n  So this feature is a drop-in replacement for the\n  ``play-python`` plugin.\n\n  You should no more install ``play_python`` since now.\n\n- update documentation\n\n- deprecate selenium commands (they will be implemented\n  on a separate plugin and dropped in\n  ``pytest-play`` >= 2.0.0). All your previous scripts\n  will work fine, this warning is just for people\n  directly importing the provider for some reason.\n\n- implement skip conditions. You can omit the execution of\n  any command evaluating a Python based skip condition\n\n\n1.1.0 (2018-01-16)\n------------------\n\n- Documentation updated (add new pytest play plugins)\n\n- Support default payloads for command providers. Useful\n  for HTTP authentication headers, common database settings\n\n\n1.0.0 (2018-01-10)\n------------------\n\n- execute command accepts kwargs now\n\n- execute command returns the command value now\n\n- complete refactor of ``include`` provider (no\n  backwards compatibility)\n\n- add ``play_json.get_file_contents`` and removed\n  ``data_getter`` fixture (no backwards compatibility)\n\n\n0.3.1 (2018-01-04)\n------------------\n\n- play engine now logs commands to be executed and errors\n\n\n0.3.0 (2018-01-04)\n------------------\n\n- you are able to update variables when executing commands\n\n- you can extend ``pytest-play`` with new pluggable commands coming\n  from third party packages thanks to setuptools entrypoints\n\n\n0.2.0 (2018-01-02)\n------------------\n\n- no more open browser by default\n  pytest-play is a generic test engine and it could be used for non UI tests too.\n\n  So there is no need to open the browser for non UI tests (eg: API tests)\n\n\n0.1.0 (2017-12-22)\n------------------\n\n- implement reusable steps (include scenario)\n\n- minor documentation changes\n\n0.0.1 (2017-12-20)\n------------------\n\n- First release", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/davidemoro/pytest-play", "keywords": "", "license": "Apache Software License 2.0", "maintainer": "Davide Moro", "maintainer_email": "davide.moro@gmail.com", "name": "pytest-play", "package_url": "https://pypi.org/project/pytest-play/", "platform": "", "project_url": "https://pypi.org/project/pytest-play/", "project_urls": {"Homepage": "https://github.com/davidemoro/pytest-play"}, "release_url": "https://pypi.org/project/pytest-play/2.3.1/", "requires_dist": null, "requires_python": "", "summary": "pytest plugin that let you automate actions and assertions with test metrics reporting executing plain YAML files", "version": "2.3.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"pytest-play\">\n<h2>pytest-play</h2>\n<a href=\"https://travis-ci.org/pytest-dev/pytest-play\" rel=\"nofollow\"><img alt=\"See Build Status on Travis CI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ea7e554ddaac52a3871282b8f2ab2d0b151c9e38/68747470733a2f2f7472617669732d63692e6f72672f7079746573742d6465762f7079746573742d706c61792e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"http://pytest-play.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6ad4452657c7e2caf9b476559e5670599608d779/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7079746573742d706c61792f62616467652f3f76657273696f6e3d6c6174657374\"></a>\n<a href=\"https://codecov.io/gh/pytest-dev/pytest-play\" rel=\"nofollow\"><img alt=\"https://codecov.io/gh/pytest-dev/pytest-play/branch/master/graph/badge.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/349d32bf069e94226bf2436a3a3ea3b0dfba6b39/68747470733a2f2f636f6465636f762e696f2f67682f7079746573742d6465762f7079746573742d706c61792f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a>\n<p><tt><span class=\"pre\">pytest-play</span></tt> is a codeless, generic, pluggable and extensible <strong>automation tool</strong>,\nnot necessarily <strong>test automation</strong> only, based on the fantastic <a href=\"https://github.com/pytest-dev/pytest\" rel=\"nofollow\">pytest</a> test framework\nthat let you define and execute <a href=\"https://en.wikipedia.org/wiki/YAML\" rel=\"nofollow\">YAML</a> files containing scripts or test scenarios\nthrough actions and assertions that can be implemented and managed even by <strong>non technical users</strong>:</p>\n<ul>\n<li><p>automation (not necessarily test automation). You can build a set of actions on a single file (e.g,\ncall a JSON based API endpoint, perform an action if a condition matches) or a test automation\nproject with many test scenarios.</p>\n<p>For example you can create always fresh test data on demand supporting\nmanual testing activities, build a live simulator and so on</p>\n</li>\n<li><p>codeless, or better almost codeless. If you have to write assertions against action results or some\nconditional expressions you need a very basic knowledge of Python or Javascript expressions\nwith a smooth learning curve (something like <tt><span class=\"pre\">variables['foo']</span> == 'bar'</tt>)</p>\n</li>\n<li><p>generic. It is not yet again another automation tool for browser automation only, API only, etc.\nYou can drive a browser, perform some API calls, make database queries and/or make assertions\nusing the same tool for different technologies</p>\n<p>So there are several free or not free testing frameworks or automation tools and many times\nthey address just one single area testing needs and they are not extensible: API testing only,\nUI testing only and so on. It could be fine if you are testing a web\nonly application like a CMS but if you are dealing with a reactive IoT application you might something more,\nmake cross actions or cross checks against different systems or build something of more complex upon\n<tt><span class=\"pre\">pytest-play</span></tt></p>\n</li>\n<li><p>powerful. It is not yet again another test automation tool, it only extends the <a href=\"https://github.com/pytest-dev/pytest\" rel=\"nofollow\">pytest</a> framework\nwith another paradigm and inherits a lot of good stuff (test data decoupled by test implementation\nthat let you write once and executed many times the same scenario thanks to native parametrization\nsupport, reporting, integration with test management tools, many useful command line options, browsers and\nremote Selenium grids integration, etc)</p>\n</li>\n<li><p>pluggable and extensible. Let\u2019s say you need to interact with a system not yet supported by a <tt><span class=\"pre\">pytest-play</span></tt>\nplugin, you can write by your own or pay someone for you. In addition there is a scaffolding tool that\nlet you implement your own command: <a href=\"https://github.com/davidemoro/cookiecutter-play-plugin\" rel=\"nofollow\">https://github.com/davidemoro/cookiecutter-play-plugin</a></p>\n</li>\n<li><p>easy to use. Why YAML? Easy to read, easy to write, simple and standard syntax, easy to be validated and\nno parentheses hell. Despite there are no recording tools (not yet) for browser interaction or API calls, the\ndocumentation based on very common patterns let you copy, paste and edit command by command with no pain</p>\n</li>\n<li><p>free software. It\u2019s an open source project based on the large and friendly <a href=\"https://github.com/pytest-dev/pytest\" rel=\"nofollow\">pytest</a> community</p>\n</li>\n<li><p>easy to install. The only prerequisite is Docker thanks to the <tt><span class=\"pre\">davidemoro/pytest-play</span></tt> Docker Hub container.\nOr better, with docker, no installation is required: you just need to type the following command\n<tt>docker run <span class=\"pre\">-i</span> <span class=\"pre\">--rm</span> <span class=\"pre\">-v</span> <span class=\"pre\">$(pwd):/src</span> <span class=\"pre\">davidemoro/pytest-play</span></tt> inside your project folder\nSee <a href=\"https://hub.docker.com/r/davidemoro/pytest-play\" rel=\"nofollow\">https://hub.docker.com/r/davidemoro/pytest-play</a></p>\n</li>\n</ul>\n<p>See at the bottom of the page the third party plugins that extends <tt><span class=\"pre\">pytest-play</span></tt>:</p>\n<ul>\n<li><a href=\"#third-party-pytest-play-plugins\" rel=\"nofollow\">Third party pytest-play plugins</a></li>\n</ul>\n<div id=\"how-it-works\">\n<h3>How it works</h3>\n<p>Depending on your needs and skills you can choose to use pytest-play programmatically\nwriting some Python code or following a Python-less approach.</p>\n<p>As said before with <a href=\"#pytest-play\" rel=\"nofollow\">pytest-play</a> you will be able to create codeless scripts or test scenarios\nwith no or very little Python knowledge: a file <tt>test_XXX.yml</tt> (e.g., <tt>test_something.yml</tt>,\nwhere <tt>test_</tt> and <tt>.yml</tt> matter) will be automatically recognized and executed without having\nto touch any <tt>*.py</tt> module.</p>\n<p>You can run a single scenario with <tt>pytest test_XXX.yml</tt> or running the entire suite filtering\nby name or keyword markers.</p>\n<p>Despite <tt><span class=\"pre\">pytest-play</span></tt> was born with native support for JSON format, <tt><span class=\"pre\">pytest-play</span></tt>&gt;=2.0 versions will support\nYAML only for improved usability.</p>\n</div>\n</div>\n<div id=\"python-less-pure-yaml\">\n<h2>Python-less (pure YAML)</h2>\n<p>Here you can see the contents of a <tt><span class=\"pre\">pytest-play</span></tt> project without any Python files inside\ncontaining a login scenario:</p>\n<pre>$ tree\n.\n\u251c\u2500\u2500 env-ALPHA.yml    (OPTIONAL)\n\u2514\u2500\u2500 test_login.yml\n</pre>\n<p>and you might have some global variables in a settings file specific for a target environment:</p>\n<pre>$ cat env-ALPHA.yml\npytest-play:\n  base_url: https://www.yoursite.com\n</pre>\n<p>The test scenario with action, assertions and optional metadata\n(<a href=\"https://github.com/davidemoro/play_selenium\" rel=\"nofollow\">play_selenium</a> external plugin needed):</p>\n<pre>$ cat test_login.yml\n---\nmarkers:\n  - login\ntest_data:\n  - username: siteadmin\n    password: siteadmin\n  - username: editor\n    password: editor\n  - username: reader\n    password: reader\n---\n- comment: visit base url\n  type: get\n  provider: selenium\n  url: \"$base_url\"\n- comment: click on login link\n  locator:\n    type: id\n    value: personaltools-login\n  type: clickElement\n  provider: selenium\n- comment: provide a username\n  locator:\n    type: id\n    value: __ac_name\n  text: \"$username\"\n  type: setElementText\n  provider: selenium\n- comment: provide a password\n  locator:\n    type: id\n    value: __ac_password\n  text: \"$password\"\n  type: setElementText\n  provider: selenium\n- comment: click on login submit button\n  locator:\n    type: css\n    value: \".pattern-modal-buttons &gt; input[name=submit]\"\n  type: clickElement\n  provider: selenium\n- comment: wait for page loaded\n  locator:\n    type: css\n    value: \".icon-user\"\n  type: waitForElementVisible\n  provider: selenium\n</pre>\n<p>The first optional YAML document contains some metadata with keywords aka <tt>markers</tt>\nso you can filter tests to be executed invoking pytest with marker expressions,\ndecoupled test data, etc.</p>\n<p>The same <tt>test_login.yml</tt> scenario will be executed 3 times with different\ndecoupled test data <tt>test_data</tt> defined inside its first optional YAML\ndocument (the block between the 2 <tt><span class=\"pre\">---</span></tt> lines).</p>\n<p>So write once and execute many times with different test data!</p>\n<p>You can see a hello world example here:</p>\n<ul>\n<li><a href=\"https://github.com/davidemoro/pytest-play-plone-example\" rel=\"nofollow\">https://github.com/davidemoro/pytest-play-plone-example</a></li>\n</ul>\n<p>As told before the metadata document is optional so you might have 1 or 2\ndocuments in your YAML file. You can find more info about <a href=\"#metadata-format\" rel=\"nofollow\">Metadata format</a>.</p>\n<p>Here you can see the same example without the metadata section for sake of\ncompleteness:</p>\n<pre>---\n- comment: visit base url\n  type: get\n  provider: selenium\n  url: \"http://YOURSITE\"\n- comment: click on login link\n  type: clickElement\n  provider: selenium\n  locator:\n    type: id\n    value: personaltools-login\n- comment: provide a username\n  type: setElementText\n  provider: selenium\n  locator:\n    type: id\n    value: __ac_name\n  text: \"YOURUSERNAME\"\n- comment: provide a password\n  type: setElementText\n  provider: selenium\n  locator:\n    type: id\n    value: __ac_password\n  text: \"YOURPASSWORD\"\n- comment: click on login submit button\n  type: clickElement\n  provider: selenium\n  locator:\n    type: css\n    value: \".pattern-modal-buttons &gt; input[name=submit]\"\n- comment: wait for page loaded\n  type: waitForElementVisible\n  provider: selenium\n  locator:\n    type: css\n    value: \".icon-user\"\n</pre>\n</div>\n<div id=\"programmatically\">\n<h2>Programmatically</h2>\n<p>You can invoke pytest-play programmatically too.</p>\n<p>You can define a test <tt>test_login.py</tt> like this:</p>\n<pre>def test_login(play):\n    data = play.get_file_contents(\n        'my', 'path', 'etc', 'login.yml')\n    play.execute_raw(data, extra_variables={})\n</pre>\n<p>Or this programmatical approach might be used if you are\nimplementing BDD based tests using <tt><span class=\"pre\">pytest-bdd</span></tt>.</p>\n<div id=\"core-commands\">\n<h3>Core commands</h3>\n<p><a href=\"#pytest-play\" rel=\"nofollow\">pytest-play</a> provides some core commands that let you:</p>\n<ul>\n<li>write simple Python assertions, expressions and variables</li>\n<li>reuse steps including other test scenario scripts</li>\n<li>provide a default command template for some particular providers\n(eg: add by default HTTP authentication headers for all requests)</li>\n<li>a generic wait until machinery. Useful for waiting for an\nobservable asynchronous event will complete its flow before\nproceeding with the following commands that depends on the previous\nstep completion</li>\n</ul>\n<p>You can write restricted Python expressions and assertions based on the <tt>RestrictedPython</tt> package.</p>\n<p><a href=\"https://github.com/zopefoundation/RestrictedPython\" rel=\"nofollow\">RestrictedPython</a> is a tool that helps to define a subset of the Python\nlanguage which allows to provide a program input into a trusted environment.\nRestrictedPython is not a sandbox system or a secured environment, but it helps\nto define a trusted environment and execute untrusted code inside of it.</p>\n<p>See:</p>\n<ul>\n<li><a href=\"https://github.com/zopefoundation/RestrictedPython\" rel=\"nofollow\">https://github.com/zopefoundation/RestrictedPython</a></li>\n</ul>\n</div>\n</div>\n<div id=\"how-to-reuse-steps\">\n<h2>How to reuse steps</h2>\n<p>You can split your commands and reuse them using the <tt>include</tt> command avoiding\nduplication:</p>\n<pre>- provider: include\n  type: include\n  path: \"/some-path/included-scenario.yml\"\n</pre>\n<p>You can create a variable for the base folder where your test scripts live.</p>\n</div>\n<div id=\"default-commands\">\n<h2>Default commands</h2>\n<p>Some commands require many verbose options you don\u2019t want to repeat (eg: authentication headers for <a href=\"https://github.com/davidemoro/play_requests\" rel=\"nofollow\">play_requests</a>).</p>\n<p>Instead of replicating all the headers information you can initialize a <tt><span class=\"pre\">pytest-play</span></tt> with the provider name as\nkey and as a value the default command you want to omit (this example neets the external plugin <a href=\"https://github.com/davidemoro/play_selenium\" rel=\"nofollow\">play_selenium</a>):</p>\n<pre>- provider: python\n  type: store_variable\n  name: bearer\n  expression: \"'BEARER'\"\n- provider: python\n  type: store_variable\n  name: play_requests\n  expression: \"{'parameters': {'headers': {'Authorization': '$bearer'}}}\"\n- provider: play_requests\n  type: GET\n  comment: this is an authenticated request!\n  url: \"$base_url\"\n</pre>\n</div>\n<div id=\"store-variables\">\n<h2>Store variables</h2>\n<p>You can store a <a href=\"#pytest-play\" rel=\"nofollow\">pytest-play</a> variables:</p>\n<pre>- provider: python\n  type: store_variable\n  expression: \"1+1\"\n  name: foo\n</pre>\n</div>\n<div id=\"make-a-python-assertion\">\n<h2>Make a Python assertion</h2>\n<p>You can make an assertion based on a Python expression:</p>\n<pre>- provider: python\n  type: assert\n  expression: variables['foo'] == 2\n</pre>\n</div>\n<div id=\"sleep\">\n<h2>Sleep</h2>\n<p>Sleep for a given amount of seconds:</p>\n<pre>- provider: python\n  type: sleep\n  seconds: 2\n</pre>\n</div>\n<div id=\"exec-a-python-expresssion\">\n<h2>Exec a Python expresssion</h2>\n<p>You can execute a Python expression:</p>\n<pre>- provider: python\n  type: exec\n  expression: \"1+1\"\n</pre>\n</div>\n<div id=\"while-condition-and-looping\">\n<h2>While condition and looping</h2>\n<p>If you need to loop over a series of commands or wait something you can use\nthe <tt>while</tt> command. It will execute the sequence of sub commands, if any,\nwhile the resulting expression condition is true. Assuming you have a <tt>countdown</tt>\nvariable containing a integer <tt>10</tt>, the block of commands whill be executed 10 times:</p>\n<pre>---\n- provider: python\n  type: while\n  expression: variables['countdown'] &gt;= 0\n  timeout: 2.3\n  poll: 0.1\n  sub_commands:\n  - provider: python\n    type: store_variable\n    name: countdown\n    expression: variables['countdown'] - 1\n</pre>\n<p>The <tt>while</tt> command supersedes the other legacy commands <tt>wait_until</tt>\n(stops when the condition becomes true) or <tt>wait_until_not</tt>.\ncommands.</p>\n</div>\n<div id=\"conditional-commands-python\">\n<h2>Conditional commands (Python)</h2>\n<p>You can skip any command evaluating a Python based skip condition\nlike the following:</p>\n<pre>- provider: include\n  type: include\n  path: \"/some-path/assertions.yml\"\n  skip_condition: variables['cassandra_assertions'] is True\n</pre>\n</div>\n<div id=\"how-to-assert-commands-elapsed-time\">\n<h2>How to assert commands elapsed time</h2>\n<p>The engine updates a <tt><span class=\"pre\">pytest-play</span></tt> variable called <tt>_elapsed</tt>\nfor each executed command. So you can write something that:</p>\n<pre>---\n- type: GET\n  provider: play_requests\n  url: https://api.chucknorris.io/jokes/categories\n  expression: \"'dev' in response.json()\"\n- type: assert\n  provider: python\n  expression: \"variables['_elapsed'] &gt; 0\"\n</pre>\n</div>\n<div id=\"generate-a-junit-xml-report\">\n<h2>Generate a JUnit XML report</h2>\n<p>Use the <tt><span class=\"pre\">--junit-xml</span></tt> command line option, e.g.:</p>\n<pre>--junit-xml results.xml\n</pre>\n<p>You\u2019ll get for each test case errors, commands executed in <tt><span class=\"pre\">system-output</span></tt> (do not use <tt><span class=\"pre\">-s</span></tt> or <tt><span class=\"pre\">--capture=no</span></tt> otherwise you won\u2019t\nsee commands in <tt><span class=\"pre\">system-output</span></tt>) and execution timing metrics (global, per test case and per single command thanks to <tt>_elapsed</tt> property tracked on every executed command shown in <tt><span class=\"pre\">system-output</span></tt>).</p>\n<p>Here you can see a standard <tt>results.xml</tt> file:</p>\n<pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;testsuite errors=\"0\" failures=\"0\" name=\"pytest\" skipped=\"0\" tests=\"1\" time=\"0.360\"&gt;&lt;testcase classname=\"test_assertion.yml\" file=\"test_assertion.yml\" name=\"test_assertion.yml\" time=\"0.326\"&gt;&lt;system-out&gt;{&amp;apos;expression&amp;apos;: &amp;apos;1 == 1&amp;apos;, &amp;apos;provider&amp;apos;: &amp;apos;python&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;assert&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.0003077983856201172}\n{&amp;apos;expression&amp;apos;: &amp;apos;0 == 0&amp;apos;, &amp;apos;provider&amp;apos;: &amp;apos;python&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;assert&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.0002529621124267578}\n&lt;/system-out&gt;&lt;/testcase&gt;&lt;/testsuite&gt;\n</pre>\n</div>\n<div id=\"generate-a-custom-junit-xml-report-with-custom-properties-and-execution-times-metrics\">\n<h2>Generate a custom JUnit XML report with custom properties and execution times metrics</h2>\n<p>You can track execution time metrics for monitoring and measure\nwhat is important to you. For example you can track using a machine interpretable format:</p>\n<ul>\n<li>response times (e.g., how much time is needed for returning a <tt>POST</tt> json payload)</li>\n<li>time that occurs between the invocation of an API and a reactive web application update or some asynchronous data appearing on an event store</li>\n<li>time that occurs between a user input on browser and results updated (e.g., a live search)</li>\n<li>time that occurs between a login button and the page loaded an usable (e.g., how much time is needed after a browser action to click on a target button)</li>\n</ul>\n<div id=\"track-response-time-metric-in-junit-xml-report\">\n<h3>Track response time metric in JUnit XML report</h3>\n<p>For example, a <tt>test_categories.yml</tt> file executed with\nthe command line option <tt><span class=\"pre\">--junit-xml</span> report.xml</tt> (requires <a href=\"https://github.com/davidemoro/play_requests\" rel=\"nofollow\">play_requests</a> plugin):</p>\n<pre>test_data:\n  - category: dev\n  - category: movie\n  - category: food\n---\n- type: GET\n  provider: play_requests\n  url: https://api.chucknorris.io/jokes/categories\n  expression: \"'$category' in response.json()\"\n- provider: metrics\n  type: record_elapsed\n  name: categories_time\n- type: assert\n  provider: python\n  expression: \"variables['categories_time'] &lt; 2.5\"\n  comment: you can make an assertion against the categories_time\n</pre>\n<p>will generate an extended <tt>report.xml</tt> file with custom properties like that:</p>\n<pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;testsuite errors=\"0\" failures=\"0\" name=\"pytest\" skipped=\"0\" tests=\"3\" time=\"2.031\"&gt;&lt;testcase classname=\"test_categories.yml\" file=\"test_categories.yml\" name=\"test_categories.yml0\" time=\"0.968\"&gt;&lt;properties&gt;&lt;property name=\"categories_time\" value=\"0.5829994678497314\"/&gt;&lt;/properties&gt;&lt;system-out&gt;{&amp;apos;expression&amp;apos;: &amp;quot;&amp;apos;dev&amp;apos; in response.json()&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;play_requests&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;GET&amp;apos;, &amp;apos;url&amp;apos;: &amp;apos;https://api.chucknorris.io/jokes/categories&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.5829994678497314}\n{&amp;apos;name&amp;apos;: &amp;apos;categories_time&amp;apos;, &amp;apos;provider&amp;apos;: &amp;apos;metrics&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;record_elapsed&amp;apos;, &amp;apos;_elapsed&amp;apos;: 3.3855438232421875e-05}\n{&amp;apos;comment&amp;apos;: &amp;apos;you can make an assertion against the categories_time&amp;apos;, &amp;apos;expression&amp;apos;: &amp;quot;variables[&amp;apos;categories_time&amp;apos;] &amp;lt; 2.5&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;python&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;assert&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.0006382465362548828}\n&lt;/system-out&gt;&lt;/testcase&gt;&lt;testcase classname=\"test_categories.yml\" file=\"test_categories.yml\" name=\"test_categories.yml1\" time=\"0.481\"&gt;&lt;properties&gt;&lt;property name=\"categories_time\" value=\"0.4184422492980957\"/&gt;&lt;/properties&gt;&lt;system-out&gt;{&amp;apos;expression&amp;apos;: &amp;quot;&amp;apos;movie&amp;apos; in response.json()&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;play_requests&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;GET&amp;apos;, &amp;apos;url&amp;apos;: &amp;apos;https://api.chucknorris.io/jokes/categories&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.4184422492980957}\n{&amp;apos;name&amp;apos;: &amp;apos;categories_time&amp;apos;, &amp;apos;provider&amp;apos;: &amp;apos;metrics&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;record_elapsed&amp;apos;, &amp;apos;_elapsed&amp;apos;: 2.09808349609375e-05}\n{&amp;apos;comment&amp;apos;: &amp;apos;you can make an assertion against the categories_time&amp;apos;, &amp;apos;expression&amp;apos;: &amp;quot;variables[&amp;apos;categories_time&amp;apos;] &amp;lt; 2.5&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;python&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;assert&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.000553131103515625}\n&lt;/system-out&gt;&lt;/testcase&gt;&lt;testcase classname=\"test_categories.yml\" file=\"test_categories.yml\" name=\"test_categories.yml2\" time=\"0.534\"&gt;&lt;properties&gt;&lt;property name=\"categories_time\" value=\"0.463592529296875\"/&gt;&lt;/properties&gt;&lt;system-out&gt;{&amp;apos;expression&amp;apos;: &amp;quot;&amp;apos;food&amp;apos; in response.json()&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;play_requests&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;GET&amp;apos;, &amp;apos;url&amp;apos;: &amp;apos;https://api.chucknorris.io/jokes/categories&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.463592529296875}\n{&amp;apos;name&amp;apos;: &amp;apos;categories_time&amp;apos;, &amp;apos;provider&amp;apos;: &amp;apos;metrics&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;record_elapsed&amp;apos;, &amp;apos;_elapsed&amp;apos;: 2.09808349609375e-05}\n{&amp;apos;comment&amp;apos;: &amp;apos;you can make an assertion against the categories_time&amp;apos;, &amp;apos;expression&amp;apos;: &amp;quot;variables[&amp;apos;categories_time&amp;apos;] &amp;lt; 2.5&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;python&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;assert&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.00054931640625}\n&lt;/system-out&gt;&lt;/testcase&gt;&lt;/testsuite&gt;\n</pre>\n<p>and the custom property <tt>categories_time</tt> will be tracked for each\ntest case execution, for example:</p>\n<pre>&lt;properties&gt;\n    &lt;property name=\"categories_time\" value=\"0.5829994678497314\"/&gt;\n&lt;/properties&gt;\n</pre>\n</div>\n<div id=\"advanced-metrics-in-junit-xml-report\">\n<h3>Advanced metrics in JUnit XML report</h3>\n<p>In this example we want to measures how long it takes a page to become interactive\n(page responding to user interactions) and evaluate update time for a live search feature.\nLet\u2019s see the <tt>test_search.yml</tt> example (requires <a href=\"https://github.com/davidemoro/play_selenium\" rel=\"nofollow\">play_selenium</a>):</p>\n<pre>---\n- provider: selenium\n  type: get\n  url: https://www.plone-demo.info/\n- provider: metrics\n  type: record_elapsed_start\n  name: load_time\n- provider: selenium\n  type: setElementText\n  text: plone 5\n  locator:\n    type: id\n    value: searchGadget\n- provider: metrics\n  type: record_elapsed_stop\n  name: load_time\n- provider: metrics\n  type: record_elapsed_start\n  name: live_search_time\n- provider: selenium\n  type: waitForElementVisible\n  locator:\n    type: css\n    value: li[data-url$=\"https://www.plone-demo.info/front-page\"]\n- provider: metrics\n  type: record_elapsed_stop\n  name: live_search_time\n</pre>\n<p>If you execute this scenario with the <tt><span class=\"pre\">--junit-xml</span> results.xml</tt>\noption you\u2019ll get a <tt>results.xml</tt> file similar to this one:</p>\n<pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;testsuite errors=\"0\" failures=\"0\" name=\"pytest\" skipped=\"0\" tests=\"1\" time=\"13.650\"&gt;&lt;testcase classname=\"test_search.yml\" file=\"test_search.yml\" name=\"test_search.yml\" time=\"13.580\"&gt;&lt;properties&gt;&lt;property name=\"load_time\" value=\"1.1175920963287354\"/&gt;&lt;property name=\"live_search_time\" value=\"1.0871295928955078\"/&gt;&lt;/properties&gt;&lt;system-out&gt;{&amp;apos;provider&amp;apos;: &amp;apos;selenium&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;get&amp;apos;, &amp;apos;url&amp;apos;: &amp;apos;https://www.plone-demo.info/&amp;apos;, &amp;apos;_elapsed&amp;apos;: 9.593282461166382}\n{&amp;apos;name&amp;apos;: &amp;apos;load_time&amp;apos;, &amp;apos;provider&amp;apos;: &amp;apos;metrics&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;record_elapsed_start&amp;apos;, &amp;apos;_elapsed&amp;apos;: 1.1682510375976562e-05}\n{&amp;apos;locator&amp;apos;: {&amp;apos;type&amp;apos;: &amp;apos;id&amp;apos;, &amp;apos;value&amp;apos;: &amp;apos;searchGadget&amp;apos;}, &amp;apos;provider&amp;apos;: &amp;apos;selenium&amp;apos;, &amp;apos;text&amp;apos;: &amp;apos;plone 5&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;setElementText&amp;apos;, &amp;apos;_elapsed&amp;apos;: 1.1019845008850098}\n{&amp;apos;name&amp;apos;: &amp;apos;load_time&amp;apos;, &amp;apos;provider&amp;apos;: &amp;apos;metrics&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;record_elapsed_stop&amp;apos;, &amp;apos;_elapsed&amp;apos;: 1.9788742065429688e-05}\n{&amp;apos;name&amp;apos;: &amp;apos;live_search_time&amp;apos;, &amp;apos;provider&amp;apos;: &amp;apos;metrics&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;record_elapsed_start&amp;apos;, &amp;apos;_elapsed&amp;apos;: 1.0013580322265625e-05}\n{&amp;apos;locator&amp;apos;: {&amp;apos;type&amp;apos;: &amp;apos;css&amp;apos;, &amp;apos;value&amp;apos;: &amp;apos;li[data-url$=&amp;quot;https://www.plone-demo.info/front-page&amp;quot;]&amp;apos;}, &amp;apos;provider&amp;apos;: &amp;apos;selenium&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;waitForElementVisible&amp;apos;, &amp;apos;_elapsed&amp;apos;: 1.060795545578003}\n{&amp;apos;name&amp;apos;: &amp;apos;live_search_time&amp;apos;, &amp;apos;provider&amp;apos;: &amp;apos;metrics&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;record_elapsed_stop&amp;apos;, &amp;apos;_elapsed&amp;apos;: 2.3603439331054688e-05}\n&lt;/system-out&gt;&lt;/testcase&gt;&lt;/testsuite&gt;\n</pre>\n<p>and in this case you\u2019ll find out that the key metric <tt>load_time</tt>\nwas <tt>1.11</tt> seconds and the <tt>live_search_time</tt> was <tt>1.09</tt> seconds as\nyou can see here:</p>\n<pre>&lt;properties&gt;\n    &lt;property name=\"load_time\" value=\"1.1175920963287354\"/&gt;\n    &lt;property name=\"live_search_time\" value=\"1.0871295928955078\"/&gt;\n&lt;/properties&gt;\n</pre>\n<p>So thanks to JUnit XML reporting you can track response times (not only browser based timings)\nusing a machine readable format to be ingested by third party systems with an acceptable approximation\nif you cannot track timings directly on the systems under test.</p>\n</div>\n<div id=\"track-any-property-in-junit-xml-reports-using-expressions\">\n<h3>Track any property in JUnit XML reports using expressions</h3>\n<p>Let\u2019s see a <tt>test_categories.yml</tt> (<a href=\"https://github.com/davidemoro/play_selenium\" rel=\"nofollow\">play_selenium</a> required):</p>\n<pre>test_data:\n  - category: dev\n  - category: movie\n  - category: food\n---\n- type: GET\n  provider: play_requests\n  url: https://api.chucknorris.io/jokes/categories\n  expression: \"'$category' in response.json()\"\n- provider: metrics\n  type: record_property\n  name: categories_time\n  expression: \"variables['_elapsed']*1000\"\n- type: assert\n  provider: python\n  expression: \"variables['categories_time'] &lt; 2500\"\n  comment: you can make an assertion against the categories_time\n</pre>\n<p>generates some custom properties (<tt>categories_time</tt> in milliseconds using a python expression)\nusing the <tt><span class=\"pre\">--junit-xml</span> results.xml</tt> cli option:</p>\n<pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;testsuite errors=\"0\" failures=\"0\" name=\"pytest\" skipped=\"0\" tests=\"3\" time=\"2.312\"&gt;&lt;testcase classname=\"test_categories.yml\" file=\"test_categories.yml\" name=\"test_categories.yml0\" time=\"1.034\"&gt;&lt;properties&gt;&lt;property name=\"categories_time\" value=\"610.3124618530273\"/&gt;&lt;/properties&gt;&lt;system-out&gt;{&amp;apos;expression&amp;apos;: &amp;quot;&amp;apos;dev&amp;apos; in response.json()&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;play_requests&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;GET&amp;apos;, &amp;apos;url&amp;apos;: &amp;apos;https://api.chucknorris.io/jokes/categories&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.6103124618530273}\n{&amp;apos;expression&amp;apos;: &amp;quot;variables[&amp;apos;_elapsed&amp;apos;]*1000&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;python&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;exec&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.0006859302520751953}\n{&amp;apos;expression&amp;apos;: &amp;quot;variables[&amp;apos;_elapsed&amp;apos;]*1000&amp;quot;, &amp;apos;name&amp;apos;: &amp;apos;categories_time&amp;apos;, &amp;apos;provider&amp;apos;: &amp;apos;metrics&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;record_property&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.006484270095825195}\n{&amp;apos;comment&amp;apos;: &amp;apos;you can make an assertion against the categories_time&amp;apos;, &amp;apos;expression&amp;apos;: &amp;quot;variables[&amp;apos;categories_time&amp;apos;] &amp;lt; 2500&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;python&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;assert&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.0005526542663574219}\n&lt;/system-out&gt;&lt;/testcase&gt;&lt;testcase classname=\"test_categories.yml\" file=\"test_categories.yml\" name=\"test_categories.yml1\" time=\"0.550\"&gt;&lt;properties&gt;&lt;property name=\"categories_time\" value=\"443.72105598449707\"/&gt;&lt;/properties&gt;&lt;system-out&gt;{&amp;apos;expression&amp;apos;: &amp;quot;&amp;apos;movie&amp;apos; in response.json()&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;play_requests&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;GET&amp;apos;, &amp;apos;url&amp;apos;: &amp;apos;https://api.chucknorris.io/jokes/categories&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.44372105598449707}\n{&amp;apos;expression&amp;apos;: &amp;quot;variables[&amp;apos;_elapsed&amp;apos;]*1000&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;python&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;exec&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.0009415149688720703}\n{&amp;apos;expression&amp;apos;: &amp;quot;variables[&amp;apos;_elapsed&amp;apos;]*1000&amp;quot;, &amp;apos;name&amp;apos;: &amp;apos;categories_time&amp;apos;, &amp;apos;provider&amp;apos;: &amp;apos;metrics&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;record_property&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.01613616943359375}\n{&amp;apos;comment&amp;apos;: &amp;apos;you can make an assertion against the categories_time&amp;apos;, &amp;apos;expression&amp;apos;: &amp;quot;variables[&amp;apos;categories_time&amp;apos;] &amp;lt; 2500&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;python&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;assert&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.0011241436004638672}\n&lt;/system-out&gt;&lt;/testcase&gt;&lt;testcase classname=\"test_categories.yml\" file=\"test_categories.yml\" name=\"test_categories.yml2\" time=\"0.676\"&gt;&lt;properties&gt;&lt;property name=\"categories_time\" value=\"576.5485763549805\"/&gt;&lt;/properties&gt;&lt;system-out&gt;{&amp;apos;expression&amp;apos;: &amp;quot;&amp;apos;food&amp;apos; in response.json()&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;play_requests&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;GET&amp;apos;, &amp;apos;url&amp;apos;: &amp;apos;https://api.chucknorris.io/jokes/categories&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.5765485763549805}\n{&amp;apos;expression&amp;apos;: &amp;quot;variables[&amp;apos;_elapsed&amp;apos;]*1000&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;python&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;exec&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.0006375312805175781}\n{&amp;apos;expression&amp;apos;: &amp;quot;variables[&amp;apos;_elapsed&amp;apos;]*1000&amp;quot;, &amp;apos;name&amp;apos;: &amp;apos;categories_time&amp;apos;, &amp;apos;provider&amp;apos;: &amp;apos;metrics&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;record_property&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.006584644317626953}\n{&amp;apos;comment&amp;apos;: &amp;apos;you can make an assertion against the categories_time&amp;apos;, &amp;apos;expression&amp;apos;: &amp;quot;variables[&amp;apos;categories_time&amp;apos;] &amp;lt; 2500&amp;quot;, &amp;apos;provider&amp;apos;: &amp;apos;python&amp;apos;, &amp;apos;type&amp;apos;: &amp;apos;assert&amp;apos;, &amp;apos;_elapsed&amp;apos;: 0.0005452632904052734}\n&lt;/system-out&gt;&lt;/testcase&gt;&lt;/testsuite&gt;\n</pre>\n<p>obtaining the metrics you want to track for each execution, for example:</p>\n<pre>&lt;properties&gt;&lt;property name=\"categories_time\" value=\"610.3124618530273\"/&gt;&lt;/properties&gt;\n</pre>\n<p>so you might track the category as well for each test execution\nor whatever you want.</p>\n</div>\n</div>\n<div id=\"monitoring-test-metrics-with-statsd-graphite\">\n<h2>Monitoring test metrics with statsd/graphite</h2>\n<p>If you like the measure everything approach you can track and monitor interesting\ncustom test metrics from an end user perspective during normal test executions or\nheavy load/stress tests thanks to the <a href=\"https://github.com/statsd/statsd\" rel=\"nofollow\">statsd</a>/<a href=\"https://github.com/graphite-project/graphite-web\" rel=\"nofollow\">graphite</a> integration.</p>\n<p>Measuring important key metrics is important for many reasons:</p>\n<ul>\n<li>compare performance between different versions under same conditions using past\ntracked stats for the same metric (no more say the system <em>seems slower</em> today)</li>\n<li>predict the system behaviour with many items on frontend (e.g., evaluate\nthe browser dealing with thousands and thousands of items managed by an infinite\nscroll plugin)</li>\n<li>predict the system behaviour under load</li>\n</ul>\n<p>You can install <tt>statsd</tt>/<tt>graphite</tt> in minutes using Docker:</p>\n<ul>\n<li><a href=\"https://graphite.readthedocs.io/en/latest/install.html\" rel=\"nofollow\">https://graphite.readthedocs.io/en/latest/install.html</a></li>\n</ul>\n<p>Basically you can track on <tt>statsd</tt>/<tt>graphite</tt> every <strong>numeric</strong> metric using\nthe same commands used for tracking metrics on JUnit XML reports as we will see.</p>\n<p>In addition, but not required, installing the third party plugin called <a href=\"https://github.com/jlane9/pytest-statsd\" rel=\"nofollow\">pytest-statsd</a>.\nyou can track on <tt>statsd</tt>/<tt>graphite</tt>:</p>\n<ul>\n<li>execution times</li>\n<li>number of executed tests per status (pass, fail, error, etc)</li>\n</ul>\n<p>Prerequisites (you need to install the optional statsd client not installed by\ndefault)::</p>\n<pre>pip install pytest-play[statsd]\n</pre>\n<p>Usage (cli options compatible with <tt><span class=\"pre\">pytest-statsd</span></tt>):</p>\n<pre>--stats-d [--stats-prefix play --stats-host http://myserver.com --stats-port 3000]\n</pre>\n<p>where:</p>\n<ul>\n<li><tt><span class=\"pre\">--stats-d</span></tt>, enable <tt>statsd</tt></li>\n<li><tt><span class=\"pre\">--stats-prefix</span></tt> (optional), if you plan on having multiple projects sending\nresults to the same server.\nFor example if you provide <tt>play</tt> as prefix you\u2019ll get a time metric under\nthe <tt>stats.timers.play.YOURMETRIC.mean</tt> key (or instead of <tt>.mean</tt> you can use <tt>.upper</tt>,\n<tt>upper_90</tt>, etc)</li>\n<li><tt><span class=\"pre\">--stats-host</span></tt>, by default <tt>localhost</tt></li>\n<li><tt><span class=\"pre\">--stats-port</span></tt>, by default <tt>8125</tt></li>\n</ul>\n<p>Now you can track timing metrics using the <tt>record_elapsed</tt> or\n<tt>record_elapsed_start</tt>/<tt>record_elapsed_stop</tt> commands seen before (pytest-play will\nsend for you time values to <tt>statsd</tt> converted to <tt>milliseconds</tt> as requested by <tt>statsd</tt>).</p>\n<p>If you want to track custom metrics using the <tt>record_property</tt> command you have to provide\nan additional parameter called <tt>metric_type</tt>. For example:</p>\n<pre>- provider: metrics\n  type: record_property\n  name: categories_time\n  expression: \"variables['_elapsed']*1000\"\n  metric_type: timing\n- provider: metrics\n  type: record_property\n  name: fridge_temperature\n  expression: \"4\"\n  metric_type: gauge\n</pre>\n<p>Some additional information regarding the <tt>record_property</tt> command:</p>\n<ul>\n<li>if you don\u2019t provide the <tt>metric_type</tt> option in <tt>record_property</tt> commands values\nwill not be transmitted to <tt>statsd</tt> (eventually they will be tracked on JUnit XML report\nif <tt><span class=\"pre\">--junit-xml</span></tt> option was provided)</li>\n<li>if you provide an allowed <tt>metric_type</tt> value (<tt>timing</tt> or <tt>gauge</tt>) non numeric values\nwill be considered as an error (<tt>ValueError</tt> exception raised)</li>\n<li>non allowed <tt>metric_type</tt> values will be considered as an error</li>\n<li>if you provide <tt>timing</tt> as <tt>metric_type</tt>, it\u2019s up to you providing a numeric value\nexpressed in <tt>milliseconds</tt></li>\n</ul>\n<div id=\"monitor-http-response-times\">\n<h3>Monitor HTTP response times</h3>\n<p>Monitor API response time (see <a href=\"https://github.com/pytest-dev/pytest-play/tree/features/examples/statsd_graphite_monitoring\" rel=\"nofollow\">https://github.com/pytest-dev/pytest-play/tree/features/examples/statsd_graphite_monitoring</a>):</p>\n<img alt=\"Chuck Norris API response time\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9d4e9a7524f7d4b8ab729202496d7aa92ba0d1a8/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f7079746573742d6465762f7079746573742d706c61792f66656174757265732f646f63732f5f7374617469632f7374617473645f67726170686974655f6d6f6e69746f72696e672e676966\">\n</div>\n<div id=\"browser-metrics\">\n<h3>Browser metrics</h3>\n<p>Monitor browser metrics using Selenium from an end user perspective (see <a href=\"https://github.com/pytest-dev/pytest-play/tree/features/examples/statsd_graphite_monitoring_selenium\" rel=\"nofollow\">https://github.com/pytest-dev/pytest-play/tree/features/examples/statsd_graphite_monitoring_selenium</a>):</p>\n<ul>\n<li>from page load to page usable</li>\n<li>live search responsiveness</li>\n</ul>\n<img alt=\"Time for first interaction after load and live search rendering timings\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/86deb5bab7a5b87e14e68d7b96429e1632c5bb48/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f7079746573742d6465762f7079746573742d706c61792f66656174757265732f646f63732f5f7374617469632f7374617473645f67726170686974655f6d6f6e69746f72696e675f73656c656e69756d2e676966\">\n</div>\n<div id=\"record-metrics-programmatically\">\n<h3>Record metrics programmatically</h3>\n<p>If you don\u2019t want to use <tt><span class=\"pre\">pytest-play</span></tt> but you need to record test metrics\nyou can use <tt><span class=\"pre\">pytest-play</span></tt> as a library::</p>\n<pre>def test_programmatically(play):\n    play.execute_command({\n        'provider': 'metrics',\n        'type': 'record_property',\n        'name': 'oil_temperature',\n        'expression': '60',\n        'metric_type': 'gauge'})\n</pre>\n</div>\n</div>\n<div id=\"performance-tests-with-pytest-play-and-bzt-taurus-blazemeter\">\n<h2>Performance tests with pytest-play and bzt/Taurus (BlazeMeter)</h2>\n<p>You can reuse all your pytest-play scenario and turn them to\nperformance tests using bzt/Taurus (so it is compatible with <a href=\"https://www.blazemeter.com/\" rel=\"nofollow\">BlazeMeter</a>\ntoo and all its goodies).</p>\n<p>Add a bzt/Taurus YAML file with no <tt>test_</tt> prefix like that (full example here in\n<a href=\"https://github.com/pytest-dev/pytest-play/tree/features/examples/bzt_performance\" rel=\"nofollow\">bzt_performance</a>):</p>\n<pre>settings:\n  artifacts-dir: /tmp/%Y-%m-%d_%H-%M-%S.%f\n\nexecution:\n- executor: pytest\n  scenario: pytest-run\n  iterations: 1\n\nscenarios:\n  pytest-run:\n    # additional-args: --stats-d --stats-prefix play\n    script: scripts/\n\nservices:\n- module: shellexec\n  prepare:\n  - pip3 install -r https://raw.githubusercontent.com/davidemoro/pytest-play-docker/master/requirements.txt\n</pre>\n<p>and run the following command:</p>\n<pre>docker run --rm -it -v $(pwd):/src --user root --entrypoint \"bzt\" davidemoro/pytest-play bzt.yml\n</pre>\n<p>You will see bzt up and running playing our scenarios:</p>\n<img alt=\"Taurus/bzt running pytest-play scenarios\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/294703a5d47e93b4d85c451878bf5262cbe0cf51/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f7079746573742d6465762f7079746573742d706c61792f66656174757265732f646f63732f5f7374617469632f7079746573745f706c61795f706572666f726d616e63652e706e67\">\n<p>You can uncomment <tt><span class=\"pre\">additional-args</span></tt> to pass other <tt>pytest</tt> command line options (e.g., enable <tt>statsd</tt>\nfor key user metrics monitoring or any other cli option).</p>\n<p>More info about bzt/Taurus here:</p>\n<ul>\n<li><a href=\"http://gettaurus.org/\" rel=\"nofollow\">http://gettaurus.org/</a></li>\n</ul>\n</div>\n<div id=\"dynamic-expressions-in-payloads-without-declaring-variables\">\n<h2>Dynamic expressions in payloads without declaring variables</h2>\n<p>If you have to send a certain payload to a REST endpoint or a MQTT message\ncontaining a dynamic value you can store a variable with <tt>store_variable</tt>\nand use <tt>$variable_name</tt> in your payload when needed.\nStoring variables is cool if you will reuse later that value but if just have to\ngenerate a dynamic value, let\u2019s say a timestamp in milliseconds,\nyou can use the <tt>{! EXPRESSION !}</tt> format.</p>\n<p>For example (<a href=\"https://github.com/davidemoro/play_mqtt\" rel=\"nofollow\">play_mqtt</a> plugin required):</p>\n<pre>---\n- comment: python expressions in mqtt payload (without declaring variables)\n  provider: mqtt\n  type: publish\n  host: \"$mqtt_host\"\n  port: \"$mqtt_port\"\n  endpoint: \"$mqtt_endpoint/$device_serial_number\"\n  payload: '{\n        \"measure_id\":   [124],\n        \"obj_id_L\":     [0],\n        \"measureType\":  [\"float\"],\n        \"start_time\":   {! int(datetime.datetime.utcnow().timestamp()*1000) !},\n        \"bin_value\":    [1]\n    }'\n</pre>\n<p>where instead of the expression:</p>\n<pre>{! int(datetime.datetime.utcnow().timestamp()*1000) !},\n</pre>\n<p>will be printed:</p>\n<pre>1553007973702\n</pre>\n<div id=\"browser-based-commands\">\n<h3>Browser based commands</h3>\n<p>The <tt><span class=\"pre\">pytest-play</span></tt> core no more includes browser based commands. Moved to <a href=\"https://github.com/davidemoro/play_selenium\" rel=\"nofollow\">play_selenium</a>\nexternal plugin.</p>\n</div>\n<div id=\"pytest-play-is-pluggable-and-extensible\">\n<h3>pytest-play is pluggable and extensible</h3>\n<p><tt><span class=\"pre\">pytest-play</span></tt> has a pluggable architecture and you can extend it.</p>\n<p>For example you might want to support your own commands, support non UI\ncommands like making raw POST/GET/etc calls, simulate IoT devices\nactivities, provide easy interaction with complex UI widgets like\ncalendar widgets, send commands to a device using the serial port implementing\na binary protocol and so on.</p>\n</div>\n</div>\n<div id=\"how-to-register-a-new-command-provider\">\n<h2>How to register a new command provider</h2>\n<p>Let\u2019s suppose you want to extend pytest-play with the following command:</p>\n<pre>command = {'type': 'print', 'provider': 'newprovider', 'message': 'Hello, World!'}\n</pre>\n<p>You just have to implement a command provider:</p>\n<pre>from pytest_play.providers import BaseProvider\n\nclass NewProvider(BaseProvider):\n\n    def this_is_not_a_command(self):\n        \"\"\" Commands should be command_ prefixed \"\"\"\n\n    def command_print(self, command):\n        print(command['message'])\n\n    def command_yetAnotherCommand(self, command):\n        print(command)\n</pre>\n<p>and register your new provider in your <tt>setup.py</tt> adding an entrypoint:</p>\n<pre>entry_points={\n    'playcommands': [\n        'print = your_package.providers:NewProvider',\n    ],\n},\n</pre>\n<p>You can define new providers also for non UI commands. For example publish MQTT\nmessages simulating IoT device activities for integration tests.</p>\n<p>If you want you can generate a new command provider thanks to:</p>\n<ul>\n<li><a href=\"https://github.com/davidemoro/cookiecutter-play-plugin\" rel=\"nofollow\">https://github.com/davidemoro/cookiecutter-play-plugin</a></li>\n</ul>\n<div id=\"metadata-format\">\n<h3>Metadata format</h3>\n<p>You can also add some scenario metadata placing another YAML document on top of the scenario\ndefined on the <tt>test_XXX.yml</tt> with the following format:</p>\n<pre>---\nmarkers:\n  - marker1\n  - marker2\ntest_data:\n  - username: foo\n  - username: bar\n---\n# omitted scenario steps in this example...\n</pre>\n<p>Option details:</p>\n<ul>\n<li><tt>markers</tt>, you can decorate your scenario with one or more markers. You can use them\nin pytest command line for filtering scenarios to be executed thanks to marker\nexpressions like <tt><span class=\"pre\">-m</span> \"marker1 and not slow\"</tt></li>\n<li><tt>test_data</tt>, enables parametrization of your decoupletd test data and let you execute\nthe same scenario many times. For example\nthe example above will be executed twice (one time with \u201cfoo\u201d username and another time\nwith \u201cbar\u201d)</li>\n</ul>\n<p>New options will be added in the next feature (e.g., skip scenarios, xfail, xpass, etc).</p>\n</div>\n<div id=\"examples\">\n<h3>Examples</h3>\n<ul>\n<li><a href=\"https://github.com/pytest-dev/pytest-play/tree/master/examples\" rel=\"nofollow\">https://github.com/pytest-dev/pytest-play/tree/master/examples</a></li>\n<li><a href=\"https://github.com/davidemoro/pytest-play-docker/tree/master/tests\" rel=\"nofollow\">https://github.com/davidemoro/pytest-play-docker/tree/master/tests</a></li>\n<li><a href=\"https://github.com/davidemoro/pytest-play-plone-example\" rel=\"nofollow\">https://github.com/davidemoro/pytest-play-plone-example</a></li>\n</ul>\n</div>\n<div id=\"articles-and-talks\">\n<h3>Articles and talks</h3>\n<p>Articles:</p>\n<ul>\n<li><a href=\"http://davidemoro.blogspot.it/2018/04/hello-pytest-play.html\" rel=\"nofollow\">Hello pytest-play!</a></li>\n<li><a href=\"https://davidemoro.blogspot.com/2019/02/api-rest-testing-pytest-play-yaml-chuck-norris.html\" rel=\"nofollow\">API/REST testing like Chuck Norris with pytest play using YAML</a></li>\n<li><a href=\"https://davidemoro.blogspot.com/2019/02/automated-docker-hub-push-travisci-pyup-python.html\" rel=\"nofollow\">pytest-play automated docker hub publishing workflow</a></li>\n<li><a href=\"https://davidemoro.blogspot.com/2018/03/test-automation-python-pytest-jenkins.html\" rel=\"nofollow\">Test automation framework thoughts and examples with Python, pytest and Jenkins</a></li>\n<li><a href=\"https://davidemoro.blogspot.com/2019/04/testing-metrics-thoughts-and-examples.html\" rel=\"nofollow\">Testing metrics thoughts and examples: how to turn lights on and off through MQTT with pytest-play</a></li>\n</ul>\n<p>Talks:</p>\n<ul>\n<li><a href=\"https://www.pycon.it/conference/talks/integration-tests-ready-to-use-with-pytest-play\" rel=\"nofollow\">Serena Martinetti @ Pycon9 - Florence: Integration tests ready to use with pytest-play</a></li>\n<li><a href=\"https://speakerdeck.com/davidemoro/automazione-e-monitoraggio-metriche-di-test-in-ambito-iot-con-pytest-play\" rel=\"nofollow\">Davide Moro @ STF2019 - Milan: Automazione e monitoraggio metriche di test in ambito IoT con pytest-play</a></li>\n</ul>\n</div>\n<div id=\"third-party-pytest-play-plugins\">\n<h3>Third party pytest-play plugins</h3>\n<ul>\n<li><p><a href=\"https://github.com/davidemoro/play_selenium\" rel=\"nofollow\">play_selenium</a>, <tt><span class=\"pre\">pytest-play</span></tt> plugin driving browsers using Selenium/Splinter\nunder the hood. Selenium grid compatible and implicit auto wait actions\nfor more robust scenarios with less controls.</p>\n</li>\n<li><p><a href=\"https://github.com/davidemoro/play_requests\" rel=\"nofollow\">play_requests</a>, <tt><span class=\"pre\">pytest-play</span></tt> plugin driving the famous Python <tt>requests</tt>\nlibrary for making <tt>HTTP</tt> calls.</p>\n</li>\n<li><p><a href=\"https://github.com/davidemoro/play_sql\" rel=\"nofollow\">play_sql</a>, <tt><span class=\"pre\">pytest-play</span></tt> support for SQL expressions and assertions</p>\n</li>\n<li><p><a href=\"https://github.com/davidemoro/play_cassandra\" rel=\"nofollow\">play_cassandra</a>, <tt><span class=\"pre\">pytest-play</span></tt> support for Cassandra expressions and assertions</p>\n</li>\n<li><p><a href=\"https://github.com/davidemoro/play_dynamodb\" rel=\"nofollow\">play_dynamodb</a>, <tt><span class=\"pre\">pytest-play</span></tt> support for AWS DynamoDB queries and assertions</p>\n</li>\n<li><p><a href=\"https://github.com/davidemoro/play_websocket\" rel=\"nofollow\">play_websocket</a>, <tt><span class=\"pre\">pytest-play</span></tt> support for websockets</p>\n</li>\n<li><p><a href=\"https://github.com/davidemoro/play_mqtt\" rel=\"nofollow\">play_mqtt</a>, <tt><span class=\"pre\">pytest-play</span></tt> plugin for MQTT support. Thanks to <tt>play_mqtt</tt>\nyou can test the integration between a mocked IoT device that sends\ncommands on MQTT and a reactive web application with UI checks.</p>\n<p>You can also build a simulator that generates messages for you.</p>\n</li>\n</ul>\n<p>Feel free to add your own public plugins with a pull request!</p>\n</div>\n<div id=\"twitter\">\n<h3>Twitter</h3>\n<p><tt><span class=\"pre\">pytest-play</span></tt> tweets happens here:</p>\n<ul>\n<li><a href=\"https://twitter.com/davidemoro\" rel=\"nofollow\">@davidemoro</a></li>\n</ul>\n</div>\n</div>\n<div id=\"changelog\">\n<h2>Changelog</h2>\n<div id=\"id1\">\n<h3>2.3.1 (2019-06-12)</h3>\n<p>Bugfix:</p>\n<ul>\n<li>fix compatibility with pytest 4.6. Rif #86</li>\n</ul>\n<p>Documentation:</p>\n<ul>\n<li>update media section (articles and talks)</li>\n</ul>\n</div>\n<div id=\"id2\">\n<h3>2.3.0 (2019-04-05)</h3>\n<p>Features and improvements:</p>\n<ul>\n<li><tt>wait_until</tt> and <tt>wait_until_not</tt> now accept commands with no <tt>sub_commands</tt> property</li>\n<li>implement new <tt>while</tt> command in python provider (while expression is true)</li>\n</ul>\n</div>\n<div id=\"id3\">\n<h3>2.2.2 (2019-03-29)</h3>\n<p>Minor changes:</p>\n<ul>\n<li>remove internal property parameter on engine</li>\n</ul>\n<p>Bugfix:</p>\n<ul>\n<li>add compatibility with <tt><span class=\"pre\">pytest-repeat</span></tt>\u2019s <tt><span class=\"pre\">--count</span></tt> command line option</li>\n</ul>\n<p>Documentation:</p>\n<ul>\n<li>mention how to generate dynamic values using <tt>{! expr !}</tt> expressions\n(e.g., dynamic payloads in REST or MQTT without having to store variables\nwhen not needed)</li>\n</ul>\n</div>\n<div id=\"id4\">\n<h3>2.2.1 (2019-03-19)</h3>\n<p>Minor changes:</p>\n<ul>\n<li>add <tt>int</tt> and <tt>float</tt> builtins available in Python expressions</li>\n<li>make python expressions more flexible for future improvements (internal change that doesn\u2019t\naffect compatibility)</li>\n</ul>\n<p>Bugfix:</p>\n<ul>\n<li>fix <tt><span class=\"pre\">--setup-plan</span></tt> invokation</li>\n</ul>\n<p>Documentation:</p>\n<ul>\n<li>add more examples (bzt/Taurus and performance tests using pytest-play)</li>\n</ul>\n</div>\n<div id=\"id5\">\n<h3>2.2.0 (2019-03-01)</h3>\n<ul>\n<li><p><tt>statsd</tt> integration (optional requirement) for advanced test metrics using statsd/graphite.\nIf you install pytest play with the optional statsd support with <tt><span class=\"pre\">pytest-play[statsd]</span></tt>\nyou will get the additional dependency <tt>statsd</tt> client and you can use the same cli\noptions defined by the <tt><span class=\"pre\">pytest-statsd</span></tt> plugin (e.g.,\n<tt><span class=\"pre\">--stats-d</span> <span class=\"pre\">[--stats-prefix</span> myproject <span class=\"pre\">--stats-host</span> <span class=\"pre\">http://myserver.com</span> <span class=\"pre\">--stats-port</span> 3000]</tt>).</p>\n<p>Note well: despite the above cli options are the same defined by the <tt><span class=\"pre\">pytest-statsd</span></tt> plugin,\nat this time of writing <tt><span class=\"pre\">pytest-statsd</span></tt> is not a <tt><span class=\"pre\">pytest-play</span></tt> dependency\nso you won\u2019t get stats about number of failures, passing, etc but only stats tracked by\n<tt><span class=\"pre\">pytest-play</span></tt>. If you need them you can install <tt><span class=\"pre\">pytest-statsd</span></tt> (it plays well with <tt><span class=\"pre\">pytest-play</span></tt>)</p>\n</li>\n</ul>\n</div>\n<div id=\"id6\">\n<h3>2.1.0 (2019-02-22)</h3>\n<p>Features:</p>\n<ul>\n<li><p>support junit xml generation file with <tt><span class=\"pre\">system-out</span></tt> element for\neach test case execution (pytest <tt><span class=\"pre\">--junit-xml</span></tt> option).\n<tt><span class=\"pre\">system-out</span></tt> will tracked by default in junit report unless you use\nthe <tt><span class=\"pre\">--capture=no</span></tt> or its alias <tt><span class=\"pre\">-s</span></tt></p>\n</li>\n<li><p>track <tt>_elapsed</tt> time for each executed command <tt><span class=\"pre\">--junit-xml</span></tt> report\nif <tt><span class=\"pre\">system-out</span></tt> is enabled</p>\n</li>\n<li><p>track <tt>pytest</tt> custom properties in <tt><span class=\"pre\">--junit-xml</span></tt> report for monitoring\nand measure what is important to you. For example you can track as key metric\nthe time of the time occurred between the end of the previous action and\nthe completion of the following. Basically you can track under the <tt>property_name</tt>\n<cite>load_login</cite> key the time occurred between the click on the submit button\nand the end of the current command (e.g., click on the menu or text input\nbeing able to receive text) using a machine interpretable format.</p>\n<p>The <tt>property_name</tt> value elapsed time will be available as standard <tt><span class=\"pre\">pytest-play</span></tt>\nvariable so that you can make additional assertions</p>\n</li>\n<li><p>after every command execution a <tt><span class=\"pre\">pytest-play</span></tt> variable will be added/updated\nreporting the elapsed time (accessible using <tt><span class=\"pre\">variables['_elapsed']</span></tt>).</p>\n<p>So be aware that the <tt>_elapsed</tt> variable name should be considered as a special\nvariable and so you should not use this name for storing variables</p>\n</li>\n<li><p>improve debug in case of failed assertions or errored commands. Logged variables\ndump in standard logs and <tt><span class=\"pre\">system-out</span></tt> reporting if available</p>\n</li>\n<li><p>improve debuggability in case of assertion errors (log failing expression)</p>\n</li>\n<li><p>added a new <tt>metrics</tt> provider that let you track custom metrics in conjunction\nwith <tt><span class=\"pre\">--junit-xml</span></tt> option. You can track in a machine readable format response\ntimes, dynamic custom expressions, time that occurs between different commands\n(e.g., measure the time needed after a login to interact with the page, time before\nan asynchronous update happens and so on). Under the <tt>metrics</tt> provider you\u2019ll\nfind the <tt>record_property</tt>, <tt>record_elapsed</tt>, <tt>record_elapsed_start</tt>  and\n<tt>record_elapsed_stop</tt> commands</p>\n</li>\n</ul>\n<p>Documentation:</p>\n<ul>\n<li>minor documentation changes</li>\n<li>add more examples</li>\n</ul>\n</div>\n<div id=\"id7\">\n<h3>2.0.2 (2019-02-06)</h3>\n<p>Documentation:</p>\n<ul>\n<li>more examples</li>\n<li>fix documentation bug on README (example based on selenium with missing <tt>provider: selenium</tt>)</li>\n</ul>\n</div>\n<div id=\"id8\">\n<h3>2.0.1 (2019-01-30)</h3>\n<p>Documentation:</p>\n<ul>\n<li>Mention davidemoro/pytest-play docker container in README.\nYou can use pytest-play with a docker command like that now\n<tt>docker run <span class=\"pre\">-i</span> <span class=\"pre\">--rm</span> <span class=\"pre\">-v</span> <span class=\"pre\">$(pwd):/src</span> <span class=\"pre\">davidemoro/pytest-play</span></tt></li>\n</ul>\n<p>Bugfix:</p>\n<ul>\n<li>Fix error locking pipenv due to pytest-play requirement\nconstraint not existing (RestrictedPython&gt;=4.0.b2 -&gt; RestrictedPython&gt;=4.0b2)</li>\n</ul>\n</div>\n<div id=\"id9\">\n<h3>2.0.0 (2019-01-25)</h3>\n<p>Breaking changes:</p>\n<ul>\n<li>Renamed fixture from <cite>play_json</cite> to <cite>play</cite> (#5)</li>\n<li>Drop json support, adopt yaml only format for scenarios (#5)</li>\n<li>Drop <tt>.ini</tt> file for metadata, if you need them you can add\na YAML document on top of the scenario <tt>.yml</tt> file. You no more\nneed multiple files for decorating your scenarios now (#65)</li>\n<li><cite>play.execute</cite> no more accepts raw data string), consumes a list of commands.\nIntroduced <cite>play.execute_raw</cite> accepting raw data string.</li>\n<li><cite>play.execute_command</cite> accepts a Python dictionary only now (not a string)</li>\n<li>Selenium provider removed from <tt><span class=\"pre\">pytest-play</span></tt> core, implemented on a\nseparate package <tt>play_selenium</tt>. Starting from now you have to add\nto your selenium commands <tt>provider: selenium</tt></li>\n<li>engine\u2019s <tt>parametrizer_class</tt> attribute no more available (\nuse <tt>parametrizer.Parametrizer</tt> by default now)</li>\n</ul>\n<p>Bug fix:</p>\n<ul>\n<li>Fix invalid markup on PyPI (#55)</li>\n<li>Fix invalid escape sequences (#62).</li>\n</ul>\n<p>Documentation and trivial changes:</p>\n<ul>\n<li>Add examples folder</li>\n</ul>\n</div>\n<div id=\"id10\">\n<h3>1.4.2 (2018-05-17)</h3>\n<ul>\n<li>Configuration change on Github. Use the same branching policy adopted by\npytest (master becomes main branch, see #56)</li>\n<li>Fixed skipped test and added new tests (deselect scenarios with keyword\nand marker expressions)</li>\n<li>Fix #58: you no more get a TypeError if you try to launch pytest-play\nin autodiscovery mode</li>\n<li>Fix #55: restructured text lint on README.rst (bad visualization on pypi)</li>\n<li>Updated README (articles and talks links)</li>\n<li>Added a <tt>DeprecationWarning</tt> for <cite>play_json</cite> fixture.\npytest-play will be based on yaml instead of json in version &gt;=2.0.0.\nSee <a href=\"https://github.com/pytest-dev/pytest-play/issues/5\" rel=\"nofollow\">https://github.com/pytest-dev/pytest-play/issues/5</a></li>\n</ul>\n</div>\n<div id=\"id11\">\n<h3>1.4.1 (2018-04-06)</h3>\n<ul>\n<li>Documentation improvements</li>\n<li>Add bzt/Taurus/BlazeMeter compatibility</li>\n</ul>\n</div>\n<div id=\"id12\">\n<h3>1.4.0 (2018-04-05)</h3>\n<ul>\n<li><p>Small documentation improvements</p>\n</li>\n<li><p>Now <tt>test_XXX.json</tt> files are automatically collected and executed</p>\n</li>\n<li><p>You can run a test scenario using the pytest CLI <tt>pytest test_YYY.json</tt></p>\n</li>\n<li><p>Introduced json test scenario ini file with markers definition. For a given\n<tt>test_YYY.json</tt> scenario you can add a <tt>test_YYY.ini</tt> ini file:</p>\n<pre>[pytest]\nmarkers =\n    marker1\n    marker2\n</pre>\n<p>and filter scenarios using marker expressions <tt>pytest <span class=\"pre\">-m</span> marker1</tt></p>\n</li>\n<li><p>Enabled parametrization of arguments for a plain json scenario in scenario ini file:</p>\n<pre>[pytest]\ntest_data =\n   {\"username\": \"foo\"}\n   {\"username\": \"bar\"}\n</pre>\n<p>and your json scenario will be executed twice</p>\n</li>\n<li><p><tt><span class=\"pre\">pytest-play</span></tt> loads some variables based on the contents of the optional <tt><span class=\"pre\">pytest-play</span></tt>\nsection in your <tt><span class=\"pre\">pytest-variables</span></tt> file now. So if your variables file contains the following\nvalues:</p>\n<pre>pytest-play:\n  foo: bar\n  date_format: YYYYMMDD\n</pre>\n<p>you will be able to use expressions <tt>$foo</tt>, <tt>$date_format</tt>, <tt><span class=\"pre\">variables['foo']</span></tt> or\n<tt><span class=\"pre\">variables['date_format']</span></tt></p>\n</li>\n</ul>\n</div>\n<div id=\"id13\">\n<h3>1.3.2 (2018-02-05)</h3>\n<ul>\n<li>Add <tt>sorted</tt> in python expressions</li>\n</ul>\n</div>\n<div id=\"id14\">\n<h3>1.3.1 (2018-01-31)</h3>\n<ul>\n<li>Add more tests</li>\n<li>Documentation update</li>\n<li>play_json fixture no more assumes that you\nhave some pytest-variables settings.\nNo more mandatory</li>\n<li>fix include scenario bug that occurs only\non Windows (slash vs backslash and\nJSON decoding issues)</li>\n</ul>\n</div>\n<div id=\"id15\">\n<h3>1.3.0 (2018-01-22)</h3>\n<ul>\n<li>documentation improvements</li>\n<li>supports teardown callbacks</li>\n</ul>\n</div>\n<div id=\"id16\">\n<h3>1.2.0 (2018-01-22)</h3>\n<ul>\n<li><p>implement python based commands in <tt><span class=\"pre\">pytest-play</span></tt> and\ndeprecates <tt>play_python</tt>.\nSo this feature is a drop-in replacement for the\n<tt><span class=\"pre\">play-python</span></tt> plugin.</p>\n<p>You should no more install <tt>play_python</tt> since now.</p>\n</li>\n<li><p>update documentation</p>\n</li>\n<li><p>deprecate selenium commands (they will be implemented\non a separate plugin and dropped in\n<tt><span class=\"pre\">pytest-play</span></tt> &gt;= 2.0.0). All your previous scripts\nwill work fine, this warning is just for people\ndirectly importing the provider for some reason.</p>\n</li>\n<li><p>implement skip conditions. You can omit the execution of\nany command evaluating a Python based skip condition</p>\n</li>\n</ul>\n</div>\n<div id=\"id17\">\n<h3>1.1.0 (2018-01-16)</h3>\n<ul>\n<li>Documentation updated (add new pytest play plugins)</li>\n<li>Support default payloads for command providers. Useful\nfor HTTP authentication headers, common database settings</li>\n</ul>\n</div>\n<div id=\"id18\">\n<h3>1.0.0 (2018-01-10)</h3>\n<ul>\n<li>execute command accepts kwargs now</li>\n<li>execute command returns the command value now</li>\n<li>complete refactor of <tt>include</tt> provider (no\nbackwards compatibility)</li>\n<li>add <tt>play_json.get_file_contents</tt> and removed\n<tt>data_getter</tt> fixture (no backwards compatibility)</li>\n</ul>\n</div>\n<div id=\"id19\">\n<h3>0.3.1 (2018-01-04)</h3>\n<ul>\n<li>play engine now logs commands to be executed and errors</li>\n</ul>\n</div>\n<div id=\"id20\">\n<h3>0.3.0 (2018-01-04)</h3>\n<ul>\n<li>you are able to update variables when executing commands</li>\n<li>you can extend <tt><span class=\"pre\">pytest-play</span></tt> with new pluggable commands coming\nfrom third party packages thanks to setuptools entrypoints</li>\n</ul>\n</div>\n<div id=\"id21\">\n<h3>0.2.0 (2018-01-02)</h3>\n<ul>\n<li><p>no more open browser by default\npytest-play is a generic test engine and it could be used for non UI tests too.</p>\n<p>So there is no need to open the browser for non UI tests (eg: API tests)</p>\n</li>\n</ul>\n</div>\n<div id=\"id22\">\n<h3>0.1.0 (2017-12-22)</h3>\n<ul>\n<li>implement reusable steps (include scenario)</li>\n<li>minor documentation changes</li>\n</ul>\n</div>\n<div id=\"id23\">\n<h3>0.0.1 (2017-12-20)</h3>\n<ul>\n<li>First release</li>\n</ul>\n</div>\n</div>\n\n          </div>"}, "last_serial": 5390830, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "1c66df8bff3c3f7c0466c3ac5b218a9a", "sha256": "22ac2ec8444f05010b3c6c10c5b0b39ca87d1629837615270385fd82878f86c7"}, "downloads": -1, "filename": "pytest-play-0.0.1.tar.gz", "has_sig": false, "md5_digest": "1c66df8bff3c3f7c0466c3ac5b218a9a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16162, "upload_time": "2017-12-20T14:50:01", "upload_time_iso_8601": "2017-12-20T14:50:01.470453Z", "url": "https://files.pythonhosted.org/packages/65/a9/0ca4fa619c3c7fca89d9ea24d30e3e0786c390450d74af0dbf8ca3bc9d79/pytest-play-0.0.1.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "8c5e3a3d5de35948565744e5e504e2a0", "sha256": "a415d8c45a76af6a4e5b5f1033f5f285777a9f8c6e7b8dc4912ecd236a2bea77"}, "downloads": -1, "filename": "pytest-play-0.1.0.tar.gz", "has_sig": false, "md5_digest": "8c5e3a3d5de35948565744e5e504e2a0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17254, "upload_time": "2017-12-22T11:58:15", "upload_time_iso_8601": "2017-12-22T11:58:15.968465Z", "url": "https://files.pythonhosted.org/packages/e4/f4/8e0ee21a85b179f22802be877bb92fe0eca34cb0fff2e44b4ec1d4e1ef9e/pytest-play-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "2f52aabf8177646a0da0412c02f7309a", "sha256": "bcfc0ad7d63f18ca535e689354a8cd0b2f7b20f6e4cd663f0d824e6fa469d6bc"}, "downloads": -1, "filename": "pytest-play-0.2.0.tar.gz", "has_sig": false, "md5_digest": "2f52aabf8177646a0da0412c02f7309a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17473, "upload_time": "2018-01-02T14:27:49", "upload_time_iso_8601": "2018-01-02T14:27:49.721230Z", "url": "https://files.pythonhosted.org/packages/a1/5b/40d36380f68e16ffcd085a0163e87d4dfb58c0272bfb94befa4191fd4eb3/pytest-play-0.2.0.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "1b131f6727487a49008ff019909cb04e", "sha256": "b322948a973a763d7555754bacb7d4d33fa1562db02b24015fbf47e7455a85d4"}, "downloads": -1, "filename": "pytest-play-0.3.0.tar.gz", "has_sig": false, "md5_digest": "1b131f6727487a49008ff019909cb04e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18333, "upload_time": "2018-01-04T09:40:07", "upload_time_iso_8601": "2018-01-04T09:40:07.980682Z", "url": "https://files.pythonhosted.org/packages/2f/71/68517f6c352481dd740978057a895f5ead33360c625947e08040350b6fb8/pytest-play-0.3.0.tar.gz", "yanked": false}], "0.3.1": [{"comment_text": "", "digests": {"md5": "e2d6847da3dbd057c9836775bbcd2953", "sha256": "99ff6b1add1d48f1a3c80e60a8b49eeb0db3983df07eed194771462e74f21f53"}, "downloads": -1, "filename": "pytest-play-0.3.1.tar.gz", "has_sig": false, "md5_digest": "e2d6847da3dbd057c9836775bbcd2953", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18440, "upload_time": "2018-01-04T14:03:12", "upload_time_iso_8601": "2018-01-04T14:03:12.253895Z", "url": "https://files.pythonhosted.org/packages/6d/e8/b429e0acdea75525e99973aaedc03baec1b262e9b926e77de4061833a886/pytest-play-0.3.1.tar.gz", "yanked": false}], "1.0.0": [{"comment_text": "", "digests": {"md5": "8edcdadba6a724af71f6181af427bb44", "sha256": "024f3d6ecc77ff757ad84df1a63ca11c5041dbd50e88a08c5f6ada81a8d3e349"}, "downloads": -1, "filename": "pytest-play-1.0.0.tar.gz", "has_sig": false, "md5_digest": "8edcdadba6a724af71f6181af427bb44", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18809, "upload_time": "2018-01-10T08:36:25", "upload_time_iso_8601": "2018-01-10T08:36:25.533081Z", "url": "https://files.pythonhosted.org/packages/a4/de/f7c74d227a9a31018c20c7c727af4354683e9ff141fd32c6cd534988f8db/pytest-play-1.0.0.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "464e39d132fdc4ff55476ca0247d0afd", "sha256": "4c5ac93e38c2e334917b18a4c41127a277ce6e6503b073d50e79777fe109cf07"}, "downloads": -1, "filename": "pytest-play-1.1.0.tar.gz", "has_sig": false, "md5_digest": "464e39d132fdc4ff55476ca0247d0afd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20153, "upload_time": "2018-01-16T14:03:29", "upload_time_iso_8601": "2018-01-16T14:03:29.235830Z", "url": "https://files.pythonhosted.org/packages/d3/ef/239ec9e2307cae584fdd9a9c3a1a0e9e7269d9dd092fb03a302eadc43d0f/pytest-play-1.1.0.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "b1c27616d96b061e4f250cdfafeb8efb", "sha256": "165815576ecf48919658a0fdb978d28a19f61600060bcd585c6cea72f103082c"}, "downloads": -1, "filename": "pytest-play-1.2.0.tar.gz", "has_sig": false, "md5_digest": "b1c27616d96b061e4f250cdfafeb8efb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24540, "upload_time": "2018-01-22T11:11:36", "upload_time_iso_8601": "2018-01-22T11:11:36.258458Z", "url": "https://files.pythonhosted.org/packages/db/38/57c54720e876c4150dc9303a9d992f12d8bb458db0cfea40ef3a81153799/pytest-play-1.2.0.tar.gz", "yanked": false}], "1.3.0": [{"comment_text": "", "digests": {"md5": "475669bb98dff1587a5f537730d49019", "sha256": "561c7307b3c5760e72611e4270ed7543677d750aa37028f15493d167ff68f1bc"}, "downloads": -1, "filename": "pytest-play-1.3.0.tar.gz", "has_sig": false, "md5_digest": "475669bb98dff1587a5f537730d49019", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24680, "upload_time": "2018-01-22T16:57:40", "upload_time_iso_8601": "2018-01-22T16:57:40.244494Z", "url": "https://files.pythonhosted.org/packages/ec/44/2e3e0c40e447e9e15204ee272d493207f36f603f08ff863a357b65f17a1c/pytest-play-1.3.0.tar.gz", "yanked": false}], "1.3.1": [{"comment_text": "", "digests": {"md5": "ec5615bff5b5e131a0e3341dd39fa780", "sha256": "253a570b14975568748c2d0ab07dcdb66ddc3bb9829efd9dc425e2215b472b4e"}, "downloads": -1, "filename": "pytest-play-1.3.1.tar.gz", "has_sig": false, "md5_digest": "ec5615bff5b5e131a0e3341dd39fa780", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25192, "upload_time": "2018-01-31T09:11:57", "upload_time_iso_8601": "2018-01-31T09:11:57.143128Z", "url": "https://files.pythonhosted.org/packages/33/ee/e6335df8e48940c990cdbf9051099d26d2a509c4c4df77eac3e3fa8143b5/pytest-play-1.3.1.tar.gz", "yanked": false}], "1.3.2": [{"comment_text": "", "digests": {"md5": "148fce124b4ebda2f394a007e4fa8392", "sha256": "76f110c930e4b7edfc3b2e7190d900daf192f9fbf2bf63f5c3fa608e961e1143"}, "downloads": -1, "filename": "pytest-play-1.3.2.tar.gz", "has_sig": false, "md5_digest": "148fce124b4ebda2f394a007e4fa8392", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25255, "upload_time": "2018-02-05T17:22:07", "upload_time_iso_8601": "2018-02-05T17:22:07.098793Z", "url": "https://files.pythonhosted.org/packages/ed/18/84aeea6f8a8264c27cf8220c74aeb94af40bbffd42baa525364fc9152e0c/pytest-play-1.3.2.tar.gz", "yanked": false}], "1.4.0": [{"comment_text": "", "digests": {"md5": "31168df37f28ce1af485c197c7066a99", "sha256": "0a1c108cced00f6a77d5709143b50260cbf4e37e057a967fb2979af9abcebe5e"}, "downloads": -1, "filename": "pytest-play-1.4.0.tar.gz", "has_sig": false, "md5_digest": "31168df37f28ce1af485c197c7066a99", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28509, "upload_time": "2018-04-04T22:14:27", "upload_time_iso_8601": "2018-04-04T22:14:27.921143Z", "url": "https://files.pythonhosted.org/packages/ee/d1/35867d069cab177a467ecab62f785799acc7f121cc87ee4542a1437d8c05/pytest-play-1.4.0.tar.gz", "yanked": false}], "1.4.1": [{"comment_text": "", "digests": {"md5": "03a9e37233da200f081d449381cfd09b", "sha256": "1e3e92c4d13017612b58764e8c06f201af97cb4b7f54f208c7657148b98ce959"}, "downloads": -1, "filename": "pytest-play-1.4.1.tar.gz", "has_sig": false, "md5_digest": "03a9e37233da200f081d449381cfd09b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31569, "upload_time": "2018-04-05T22:46:54", "upload_time_iso_8601": "2018-04-05T22:46:54.187739Z", "url": "https://files.pythonhosted.org/packages/f9/e8/49a489dfdbc2b9527492036f3abb963b00b8bf2bff883e6fdee313a206b2/pytest-play-1.4.1.tar.gz", "yanked": false}], "1.4.2": [{"comment_text": "", "digests": {"md5": "0653b0e36fc9c17f91f55d582656d1c3", "sha256": "326c705ccdf92900c5538a7ee429062f90e9f24ad6be86ecd8f0d5284328743f"}, "downloads": -1, "filename": "pytest-play-1.4.2.tar.gz", "has_sig": false, "md5_digest": "0653b0e36fc9c17f91f55d582656d1c3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32389, "upload_time": "2018-05-17T22:02:17", "upload_time_iso_8601": "2018-05-17T22:02:17.126756Z", "url": "https://files.pythonhosted.org/packages/4f/77/cad3b4f11a4991e1177032f5350523b64832f6a890637429bf19ec36ab75/pytest-play-1.4.2.tar.gz", "yanked": false}], "2.0.0": [{"comment_text": "", "digests": {"md5": "264aa38a983e838738542cb585d7bb94", "sha256": "b3a057a72104038372f823ef1b78ebc21287d8cb1a0d6b18d964c248fd947515"}, "downloads": -1, "filename": "pytest-play-2.0.0.tar.gz", "has_sig": false, "md5_digest": "264aa38a983e838738542cb585d7bb94", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28751, "upload_time": "2019-01-25T21:53:21", "upload_time_iso_8601": "2019-01-25T21:53:21.925425Z", "url": "https://files.pythonhosted.org/packages/0b/d2/9e9c32040ddf032f92eec61e9cc42321986706c94a008bbc349fadea6140/pytest-play-2.0.0.tar.gz", "yanked": false}], "2.0.1": [{"comment_text": "", "digests": {"md5": "d08ec5a2a3cc6965d2b767053ae57cbb", "sha256": "43f1390d79721468c96920f492a5a4ed29cbf436947e9cc1830ad8e7065014c5"}, "downloads": -1, "filename": "pytest-play-2.0.1.tar.gz", "has_sig": false, "md5_digest": "d08ec5a2a3cc6965d2b767053ae57cbb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29417, "upload_time": "2019-01-30T21:44:21", "upload_time_iso_8601": "2019-01-30T21:44:21.892515Z", "url": "https://files.pythonhosted.org/packages/39/f4/3854ae908bc95c89496b7c40ce5535a1de098073cc6d2a428946a4a827d6/pytest-play-2.0.1.tar.gz", "yanked": false}], "2.0.2": [{"comment_text": "", "digests": {"md5": "1be0ddc929a383f2f5a1103879fd10e0", "sha256": "8f4a16f1bec2e87310c3b4ecb7a51ded48d92eba1387c1243f7478a6c64e59e9"}, "downloads": -1, "filename": "pytest-play-2.0.2.tar.gz", "has_sig": false, "md5_digest": "1be0ddc929a383f2f5a1103879fd10e0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29822, "upload_time": "2019-02-06T23:10:59", "upload_time_iso_8601": "2019-02-06T23:10:59.997207Z", "url": "https://files.pythonhosted.org/packages/6b/20/41edbac39aee857e8dc9f0dfac73d667390b812af57fd6e7a10ab852501e/pytest-play-2.0.2.tar.gz", "yanked": false}], "2.1.0": [{"comment_text": "", "digests": {"md5": "09fb968054f8bea77b622cc3b507998e", "sha256": "4748537f4fc53a0bd739162a54c0dc6c540221766b85ec84e2f604cd998540e5"}, "downloads": -1, "filename": "pytest-play-2.1.0.tar.gz", "has_sig": false, "md5_digest": "09fb968054f8bea77b622cc3b507998e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 44500, "upload_time": "2019-02-22T23:16:23", "upload_time_iso_8601": "2019-02-22T23:16:23.202805Z", "url": "https://files.pythonhosted.org/packages/36/46/1ae847d3e17b972bb2220973f465f61dcc3dea96f3786e73395dc63c483a/pytest-play-2.1.0.tar.gz", "yanked": false}], "2.2.0": [{"comment_text": "", "digests": {"md5": "51672de55cd112aad12d8b2d727caa36", "sha256": "1d7830caca95707371b0c6396aa654a4dbb43275db00bb6724058e4ad9edd008"}, "downloads": -1, "filename": "pytest-play-2.2.0.tar.gz", "has_sig": false, "md5_digest": "51672de55cd112aad12d8b2d727caa36", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 50117, "upload_time": "2019-03-01T11:54:12", "upload_time_iso_8601": "2019-03-01T11:54:12.096257Z", "url": "https://files.pythonhosted.org/packages/21/81/7662b57ddc7401c69bcb4b3b3f2bcba6c82750c6d26b3c1cfcaf490c37bd/pytest-play-2.2.0.tar.gz", "yanked": false}], "2.2.1": [{"comment_text": "", "digests": {"md5": "02b113f41f6b14e11b099dfdae019aa2", "sha256": "d9f46c01cfe4db41677924a900fd5b21399f953e53c439d08ce78a8205bf8ea1"}, "downloads": -1, "filename": "pytest-play-2.2.1.tar.gz", "has_sig": false, "md5_digest": "02b113f41f6b14e11b099dfdae019aa2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 52166, "upload_time": "2019-03-19T13:04:48", "upload_time_iso_8601": "2019-03-19T13:04:48.974092Z", "url": "https://files.pythonhosted.org/packages/61/2c/1c79415f68ea9958686282c2cbeae8f5258e1c42fff29fe0a9a6a9f762fc/pytest-play-2.2.1.tar.gz", "yanked": false}], "2.2.2": [{"comment_text": "", "digests": {"md5": "fd58cc4e626adf4e41af754aea7a30df", "sha256": "a491f0f657e5869b8b4e3b88c6ab9fd34a2a759173ec123a56fda5b098e822bd"}, "downloads": -1, "filename": "pytest-play-2.2.2.tar.gz", "has_sig": false, "md5_digest": "fd58cc4e626adf4e41af754aea7a30df", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 54167, "upload_time": "2019-03-29T12:20:45", "upload_time_iso_8601": "2019-03-29T12:20:45.611748Z", "url": "https://files.pythonhosted.org/packages/32/53/03c5ede32ef99453e75e5c0df0dd4d67b2176baf05cfff14393f96d3d678/pytest-play-2.2.2.tar.gz", "yanked": false}], "2.3.0": [{"comment_text": "", "digests": {"md5": "05378ca35c0cb7e780cfa4dd9b247cac", "sha256": "48163c0bd889f09fb718f827b22cc7123cbdb0304f950ded3e33e0e45ab8f353"}, "downloads": -1, "filename": "pytest-play-2.3.0.tar.gz", "has_sig": false, "md5_digest": "05378ca35c0cb7e780cfa4dd9b247cac", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 52976, "upload_time": "2019-04-05T11:43:43", "upload_time_iso_8601": "2019-04-05T11:43:43.295622Z", "url": "https://files.pythonhosted.org/packages/ec/19/e526f837d78681ac9c336ebdb927c9ff574f26078c62ee1fd8355f7b424f/pytest-play-2.3.0.tar.gz", "yanked": false}], "2.3.1": [{"comment_text": "", "digests": {"md5": "69c1d2e2c00fd06bdeac9136ce6aa1b0", "sha256": "dd19138cdd1c8baa68671c404c8581eee64623e7ff02a4cc5f586fd2198936cd"}, "downloads": -1, "filename": "pytest-play-2.3.1.tar.gz", "has_sig": false, "md5_digest": "69c1d2e2c00fd06bdeac9136ce6aa1b0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 53504, "upload_time": "2019-06-12T11:59:29", "upload_time_iso_8601": "2019-06-12T11:59:29.654473Z", "url": "https://files.pythonhosted.org/packages/8c/ec/b9ab32993fbc2fb612cd828d58eba9b250dab591a380c7cef1d42f2e5fd0/pytest-play-2.3.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "69c1d2e2c00fd06bdeac9136ce6aa1b0", "sha256": "dd19138cdd1c8baa68671c404c8581eee64623e7ff02a4cc5f586fd2198936cd"}, "downloads": -1, "filename": "pytest-play-2.3.1.tar.gz", "has_sig": false, "md5_digest": "69c1d2e2c00fd06bdeac9136ce6aa1b0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 53504, "upload_time": "2019-06-12T11:59:29", "upload_time_iso_8601": "2019-06-12T11:59:29.654473Z", "url": "https://files.pythonhosted.org/packages/8c/ec/b9ab32993fbc2fb612cd828d58eba9b250dab591a380c7cef1d42f2e5fd0/pytest-play-2.3.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:54:52 2020"}