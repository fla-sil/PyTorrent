{"info": {"author": "Robin Andersson", "author_email": "robin.eric.andersson@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Topic :: Utilities"], "description": "============\nhops-util-py\n============\n\n|Downloads| |PypiStatus| |PythonVersions|\n\n`hops-util-py` is a helper library for Hops that facilitates development by hiding the complexity of running applications, discovering services and interacting with HopsFS.\n\nIt provides an Experiment API to run Python programs such as TensorFlow, Keras and PyTorch on a Hops Hadoop cluster. A TensorBoard will be started when an Experiment begins and the contents of the logdir saved in your Project.\n\nAn Experiment could be a single Python program, which we refer to as an *Experiment*. Grid search or genetic hyperparameter optimization such as differential evolution which runs several Experiments in parallel, which we refer to as *Parallel Experiment*. The library supports ParameterServerStrategy and CollectiveAllReduceStrategy, making multi-machine/multi-gpu training as simple as invoking a function for orchestration. This mode is referred to as *Distributed Training*.\n\nMoreover it provides an easy-to-use API for defining TLS-secured Kafka producers and consumers on the Hopsworks platform as well as an API for interacting with the Hopsworks Feature Store\n\n-----------\nQuick Start\n-----------\n\nTo Install:\n\n>>> pip install hops\n\nSample usage:\n\n>>> from hops import experiment\n>>> from hops import hdfs\n>>> notebook = hdfs.project_path() + \"Jupyter/Experiment/...\" #path to your notebook\n>>> # minimal_mnist is a function you defined\n>>> experiment.launch(minimal_mnist, #minimal_mnist is your training function\n>>>                   name='mnist estimator',\n>>>                   description='A minimal mnist example with two hidden layers',\n>>>                   versioned_resources=[notebook]\n\n\n------------------------\nDevelopment Instructions\n------------------------\n\nFor development details such as how to test and build docs, see this reference: Development_.\n\n.. _Development: ./Development.rst\n\n-------------\nDocumentation\n-------------\n\nAn overview of HopsML, a python-first ML pipeline is available here: hopsML_\n\nExample notebooks for doing deep learning and big data processing on Hops are available here: hops-examples_\n\nAPI documentation is available here: API-docs_\n\n\n.. _hops-examples: https://github.com/logicalclocks/hops-examples\n.. _hopsML: https://hops.readthedocs.io/en/latest/hopsml/hopsML.html\n.. _API-docs: http://hops-py.logicalclocks.com/\n\n\n\n------------------------------------\nQuick Start: Python with HopsML\n------------------------------------\n\nHops uses PySpark to distribute the execution of Python programs in a cluster. PySpark applications consist of two main components, a Driver and one to many Executors. The Driver and the Executors can be started on potentially any host in the cluster and use both the network and the HDFS filesystem to coordinate.\n\n\nRestructuring Python Programs as PySpark Programs\n--------------------------------------------------------------------\n\nIf you want to run a Python program, e.g.,  to train a neural network on a GPU on Hops, you will need to restructure your code. The reason for this is that your single Python process needs to be restructured as a PySpark program, see the figure below.\n\n.. _hopsml-pyspark.png: imgs/hopsml-pyspark.png\n.. figure:: imgs/hopsml-pyspark.png\n    :alt: HopsML Python Program\n    :target: `hopsml-pyspark.png`_\n    :align: center\n    :scale: 75 %\n    :figclass: align-center\n\nThe good news is that all you will need to do to get started is to move your code inside a function. In the code snippet below, the Executor code is on lines 1-3 (the *train* function) and the Driver code is on lines 5-7. For the Executor, you define a function (e.g., *train*, but the function can have any name).  The code in the function will get run on Executors (containers). To invoke the Executor function (*train*) from the Driver (the main part of your Python program), you use the Experiment API. Launch a single Executor with *experiment.launch(<fn_name>)*.  Launch many Executors with *experiment.grid_search(<fn_name>)* for hyperparameter optimization, and *experiment.collective_all_reduce(<fn_name>)* for distributed training.\n\n\n.. code-block:: python\n\n  def train():\n    import tensorflow as tf\n    # training code here\n\n  # Driver code starts here\n  from hops import experiment\n  experiment.launch(train)\n\n\n.. _driver.png: imgs/driver.png\n.. figure:: imgs/driver.png\n    :alt: HopsML Python Program\n    :target: `driver.png`_\n    :align: center\n    :scale: 90 %\n    :figclass: align-center\n\n\nLogging in the Driver\n---------------------------\nWhen you print to stdout and stderr in the Driver program, the output is printed in the Jupyter console.\n\n.. code-block:: python\n\n   # main scope of program or any non-Executor function\n   print('log message is printed to Jupyter cell output')\n\n\nLogging to stdout/stderr in the Executor\n------------------------------------------------------\n\nIf you execute print(\u2018...\u2019) in the executor, it will send the output to stdout and stderr on the executor. This will not be displayed in Jupyter console. You can, however, read output in the executors using the Spark UI. As soon as the Spark application has exited, these logs are cleaned up - they are no longer available.\n\n.. code-block:: python\n\n  train():\n    # This will write to stdout/stderr on the Spark Executors\n    # You can only view this log entry from the Spark UI while the application\n    # is running.\n    print(\"Executor log message - not visible in Jupyter, visible in Spark UI\")\n\n\nTo access the Spark executor logs, you will need 4 clicks on your mouse:\n1. Select the UI for the application you started running from Jupyter (click on the button inside the yellow highlighter in the image below):\n\n.. _executor-stderr1.png: imgs/executor-stderr1.png\n.. figure:: imgs/executor-stderr1.png\n    :alt: Stdout-err-1\n    :target: `executor-stderr1.png`_\n    :align: center\n    :scale: 75 %\n    :figclass: align-center\n\n\n2.  Select the \u201cExecutors\u201d tab from the Spark UI (click on the button inside the yellow highlighter):\n\n.. _executor-stderr2.png: imgs/executor-stderr2.png\n.. figure:: imgs/executor-stderr2.png\n    :alt: Stdout-err-2\n    :target: `executor-stderr2.png`_\n    :align: center\n    :scale: 75 %\n    :figclass: align-center\n\n\n3. Now you should see all the Executors that are running (active) or have finished running more than 90 seconds ago (dead). There will be stdout and stderr logs available for every Executor here - if you ran with 10 GPUs, with 1 GPU per Executor, there will be 10 different stdout and 10 different stderr log files available.. Click on the stderr or stdout log for the Executor you want to examine (yellow highlighted text below):\n\n.. _executor-stderr3.png: imgs/executor-stderr3.png\n.. figure:: imgs/executor-stderr3.png\n    :alt: Stdout-err-3\n    :target: `executor-stderr3.png`_\n    :align: center\n    :scale: 75 %\n    :figclass: align-center\n\n\n4. Now you can see the logs for that Executor on the screen:\n\n.. _executor-stderr4.png: imgs/executor-stderr4.png\n.. figure:: imgs/executor-stderr4.png\n    :alt: Stdout-err-4\n    :target: `executor-stderr4.png`_\n    :align: center\n    :scale: 75 %\n    :figclass: align-center\n\nLogging to file (HDFS) in the Executor or Driver\n---------------------------------------------------\n\nYou can also write log messages from either an Executor or Driver to the same logfile in HDFS.\n\n.. code-block:: python\n\n  train():\n    # This will write to your Experiments/ directory in your project\n    from hops import hdfs\n    hdfs.log(\"This is written to the logfile in the Experiments dataset, not output in Jupyter cell.\")\n\nYou can navigate to the log file created in the Datasets view in Hopsworks for your project, inside the Experiments dataset. The file created will be called \u201clogfile\u201d and if you right-click on it, you can preview its contents to see the first or last 1000 lines in the file. If you have the data-owner role in the project, you will also be allowed to download this file from here.\n\n.. _executor-hdfs-log.png: imgs/executor-hdfs-log.png\n.. figure:: imgs/executor-hdfs-log.png\n    :alt: hdfs-log\n    :target: `executor-hdfs-log.png`_\n    :align: center\n    :scale: 75 %\n    :figclass: align-center\n\nNote that the default log file is the same for all Executors. If many Executors write concurrently to the same file, this may have negative performance implications as Executors may block, waiting for write access to the file. In large-scale experiments, you can configure each Executors to write to its own log file (append a unique ID to the filename).\n\n\n\nInstalling Python Libraries in Hopsworks\n---------------------------------------------\n\nYou can use the \u2018Conda\u2019 and \u2018Pip\u2019 services in Hopsworks to install python libraries. In the \u2018Conda\u2019 service, you can change the conda repository by double-clicking on it and entering the URL for a new repo (or \u2018default\u2019 for the standard conda repository).\n\nNote: Pillow and matplotlib do not work from conda. Install using \u201cpip\u201d, instead.\n\n\nPlotting with Sparkmagic in Jupyter\n---------------------------------------------\n\nHopsworks supports both the Python kernel and Sparkmagic kernel. Plotting in the Python kernel is usually handled by libraries such as matplotlib and seaborne. These libraries can also be used in the Sparkmagic kernel, but require more work from the developer, as dataframes in Spark are distributed in the cluster and need to be localized to the Jupyter notebook server as Pandas dataframes, in order to be plotted.\nWhen you run a PySpark program with the Sparkmagic kernel in Jupyter, you will not need to initialize a Spark context, as it is done automatically for you (by Sparkmagic). However, as the PySpark application is not running on the same host as the Jupyter notebook server, plotting (with matplotlib) will not work as normal in a Python kernel. The main change you need to make is to use \u2018magics\u2019 in the sparkmagic kernel to get Spark or Pandas dataframes to be localized to the Jupyter notebook server, from where they can be visualized. More details are found in the reference notebook below. Information on the magics available in Sparkmagic are found in the link below.\n\n\nAdding Python modules to a Jupyter notebook\n---------------------------------------------\n\n.. _add-python-module.png: imgs/add-python-module.png\n.. figure:: imgs/add-python-module.png\n    :alt: add-python-module\n    :target: `add-python-module.png`_\n    :align: center\n    :scale: 75 %\n    :figclass: align-center\n\n\nAPI for the Hopsworks Feature Store\n--------------------------------------------------------------------\nHopsworks has a data management layer for machine learning, called a feature store.\nThe feature store enables simple and efficient versioning, sharing, governance and definition of features that can be used to both train machine learning models or to serve inference requests.\nThe featurestore serves as a natural interface between data engineering and data science.\n\n**Writing to the featurestore**:\n\n.. code-block:: python\n\n  raw_data = spark.read.format(\"csv\").load(filename)\n  polynomial_features = raw_data.map(lambda x: x^2)\n  from hops import featurestore\n  featurestore.insert_into_featuregroup(polynomial_features, \"polynomial_features\")\n\n**Reading from the featurestore**:\n\n.. code-block:: python\n\n  from hops import featurestore\n  features_df = featurestore.get_features([\"team_budget\", \"average_attendance\", \"average_player_age\"])\n\n**Integration with Sci-kit Learn**:\n\n.. code-block:: python\n\n  from hops import featurestore\n  train_df = featurestore.get_featuregroup(\"iris_features\", dataframe_type=\"pandas\")\n  x_df = train_df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n  y_df = train_df[[\"label\"]]\n  X = x_df.values\n  y = y_df.values.ravel()\n  iris_knn = KNeighborsClassifier()\n  iris_knn.fit(X, y)\n\n**Integration with Tensorflow**:\n\n.. code-block:: python\n\n  from hops import featurestore\n  features_df = featurestore.get_features(\n      [\"team_budget\", \"average_attendance\", \"average_player_age\",\n      \"team_position\", \"sum_attendance\",\n       \"average_player_rating\", \"average_player_worth\", \"sum_player_age\",\n       \"sum_player_rating\", \"sum_player_worth\", \"sum_position\",\n       \"average_position\"\n      ]\n  )\n  featurestore.create_training_dataset(features_df, \"team_position_prediction\", data_format=\"tfrecords\")\n\n  def create_tf_dataset():\n      dataset_dir = featurestore.get_training_dataset_path(\"team_position_prediction\")\n      input_files = tf.gfile.Glob(dataset_dir + \"/part-r-*\")\n      dataset = tf.data.TFRecordDataset(input_files)\n      tf_record_schema = featurestore.get_training_dataset_tf_record_schema(\"team_position_prediction\")\n      feature_names = [\"team_budget\", \"average_attendance\", \"average_player_age\", \"sum_attendance\",\n           \"average_player_rating\", \"average_player_worth\", \"sum_player_age\", \"sum_player_rating\", \"sum_player_worth\",\n           \"sum_position\", \"average_position\"\n          ]\n      label_name = \"team_position\"\n\n      def decode(example_proto):\n          example = tf.parse_single_example(example_proto, tf_record_schema)\n          x = []\n          for feature_name in feature_names:\n              x.append(example[feature_name])\n          y = [tf.cast(example[label_name], tf.float32)]\n          return x,y\n\n      dataset = dataset.map(decode).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat(NUM_EPOCHS)\n      return dataset\n  tf_dataset = create_tf_dataset()\n\n**Integration with PyTorch**:\n\n.. code-block:: python\n\n  from hops import featurestore\n  df_train=...\n  featurestore.create_training_dataset(df_train, \"MNIST_train_petastorm\", data_format=\"petastorm\")\n\n  from petastorm.pytorch import DataLoader\n  train_dataset_path = featurestore.get_training_dataset_path(\"MNIST_train_petastorm\")\n  device = torch.device('cuda' if use_cuda else 'cpu')\n  with DataLoader(make_reader(train_dataset_path, num_epochs=5, hdfs_driver='libhdfs', batch_size=64) as train_loader:\n          model.train()\n          for batch_idx, row in enumerate(train_loader):\n              data, target = row['image'].to(device), row['digit'].to(device)\n\n**Feature Visualizations**:\n\n.. _feature_plots1.png: imgs/feature_plots1.png\n.. figure:: imgs/feature_plots1.png\n    :alt: Visualizing feature distributions\n    :target: `feature_plots1.png`_\n    :align: center\n    :scale: 75 %\n    :figclass: align-center\n\n\n.. _feature_plots2.png: imgs/feature_plots2.png\n.. figure:: imgs/feature_plots2.png\n    :alt: Visualizing feature correlations\n    :target: `feature_plots2.png`_\n    :align: center\n    :scale: 75 %\n    :figclass: align-center\n\nModel Serving API\n--------------------------------------------------------------------\n\nIn the `serving` module you can find an API for creating/starting/stopping/updating models being served on Hopsworks as well as making inference requests.\n\n.. code-block:: python\n\n  from hops import serving\n  from hops import model\n\n  # Tensorflow\n  export_path = work_dir + '/model'\n  builder = tf.saved_model.builder.SavedModelBuilder(export_path\n  ... # tf specific export code\n  model.export(export_path, \"mnist\")\n  model_path=\"/Models/mnist/\"\n  SERVING_NAME=\"mnist\"\n  serving.create_or_update(model_path, \"mnist\", serving_type=\"TENSORFLOW\", model_version=1)\n  if serving.get_status(\"mnist\") == 'Stopped':\n      serving.start(\"mnist\")\n  data = {\"signature_name\": 'predict_images', \"instances\": [np.random.rand(784).tolist()]}\n  response = serving.make_inference_request(SERVING_NAME, data)\n\n   # SkLearn\n  script_path = \"Jupyter/Serving/sklearn/iris_flower_classifier.py\"\n  model.export(script_path, \"irisClassifier\")\n  if serving.exists(\"irisClassifier\"):\n      serving.delete(\"irisClassifier\")\n  serving.create_or_update(script_path, \"irisClassifier\", serving_type=\"SKLEARN\", model_version=1)\n  serving.start(\"irisClassifier\")\n  data = {\"inputs\" : [[random.uniform(1, 8) for i in range(NUM_FEATURES)]]}\n  response = serving.make_inference_request(SERVING_NAME, data)\n\nKafka API\n--------------------------------------------------------------------\n\nIn the `kafka` module you can find an API to interact with kafka topics in Hopsworks.\n\n.. code-block:: python\n\n  from hops import kafka, serving\n  from confluent_kafka import Producer, Consumer, KafkaError\n  TOPIC_NAME = serving.get_kafka_topic(SERVING_NAME) # get inference logs\n  config = kafka.get_kafka_default_config()\n  config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n  consumer = Consumer(config)\n  topics = [TOPIC_NAME]\n  consumer.subscribe(topics)\n  json_schema = kafka.get_schema(TOPIC_NAME)\n  avro_schema = kafka.convert_json_schema_to_avro(json_schema)\n  msg = consumer.poll(timeout=1.0)\n  value = msg.value()\n  event_dict = kafka.parse_avro_msg(value, avro_schema)\n\n\nHDFS API\n--------------------------------------------------------------------\n\nIn the `hdfs` module you can find a high-level API for interacting with the distributed file system\n\n.. code-block:: python\n\n  from hops import hdfs\n  hdfs.ls(\"Logs/\")\n  hdfs.cp(\"Resources/test.txt\", \"Logs/\")\n  hdfs.mkdir(\"Logs/test_dir\")\n  hdfs.rmr(\"Logs/test_dir\")\n  hdfs.move(\"Logs/README_dump_test.md\", \"Logs/README_dump_test2.md\")\n  hdfs.chmod(\"Logs/README.md\", 700)\n  hdfs.exists(\"Logs/\")\n  hdfs.copy_to_hdfs(\"test.txt\", \"Resources\", overwrite=True)\n  hdfs.copy_to_local(\"Resources/test.txt\", overwrite=True)\n\nExperiment API\n--------------------------------------------------------------------\n\nIn the `experiment` module you can find an API for launching reproducible machine learning experiments.\nStandalone experiments, distributed experiments, hyperparameter tuning and many more are supported.\n\n.. code-block:: python\n\n  from hops import experiment\n  log_dir, best_params = experiment.differential_evolution(\n      train_fn,\n      search_dict,\n      name='team_position_prediction_hyperparam_search',\n      description='Evolutionary search through the search space of hyperparameters with parallel executors to find the best parameters',\n      local_logdir=True,\n      population=4,\n      generations = 1\n  )\n\n\nReferences\n--------------\n\n- https://github.com/logicalclocks/hops-examples/blob/master/tensorflow/notebooks/Plotting/data_visualizations.ipynb\n- https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Magics%20in%20IPython%20Kernel.ipynb\n\n.. |Downloads| image:: https://pepy.tech/badge/hops\n   :target: https://pepy.tech/project/hops\n.. |PypiStatus| image:: https://img.shields.io/pypi/v/hops.svg\n    :target: https://pypi.org/project/hops\n.. |PythonVersions| image:: https://img.shields.io/pypi/pyversions/hops.svg\n    :target: https://travis-ci.org/hops", "description_content_type": "", "docs_url": null, "download_url": "http://snurran.sics.se/hops/hops-util-py/hops-1.2.0.6.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/logicalclocks/hops-util-py", "keywords": "Hops,Hadoop,TensorFlow,Spark", "license": "Apache License 2.0", "maintainer": "", "maintainer_email": "", "name": "hops", "package_url": "https://pypi.org/project/hops/", "platform": "", "project_url": "https://pypi.org/project/hops/", "project_urls": {"Download": "http://snurran.sics.se/hops/hops-util-py/hops-1.2.0.6.tar.gz", "Homepage": "https://github.com/logicalclocks/hops-util-py"}, "release_url": "https://pypi.org/project/hops/1.2.0.6/", "requires_dist": null, "requires_python": "", "summary": "Client library for interacting with Hopsworks, a full-stack platform for scale-out data science.", "version": "1.2.0.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://pepy.tech/project/hops\" rel=\"nofollow\"><img alt=\"Downloads\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4f48d1d0a24f5e38681b6fa34337632b2c4e0174/68747470733a2f2f706570792e746563682f62616467652f686f7073\"></a> <a href=\"https://pypi.org/project/hops\" rel=\"nofollow\"><img alt=\"PypiStatus\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8221adc5baead9cc07333e9f0cbcad4cb3f4c1c4/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f686f70732e737667\"></a> <a href=\"https://travis-ci.org/hops\" rel=\"nofollow\"><img alt=\"PythonVersions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/65e5aad6d7d59040928580e83d3b500d5f1f0f4e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f686f70732e737667\"></a></p>\n<p><cite>hops-util-py</cite> is a helper library for Hops that facilitates development by hiding the complexity of running applications, discovering services and interacting with HopsFS.</p>\n<p>It provides an Experiment API to run Python programs such as TensorFlow, Keras and PyTorch on a Hops Hadoop cluster. A TensorBoard will be started when an Experiment begins and the contents of the logdir saved in your Project.</p>\n<p>An Experiment could be a single Python program, which we refer to as an <em>Experiment</em>. Grid search or genetic hyperparameter optimization such as differential evolution which runs several Experiments in parallel, which we refer to as <em>Parallel Experiment</em>. The library supports ParameterServerStrategy and CollectiveAllReduceStrategy, making multi-machine/multi-gpu training as simple as invoking a function for orchestration. This mode is referred to as <em>Distributed Training</em>.</p>\n<p>Moreover it provides an easy-to-use API for defining TLS-secured Kafka producers and consumers on the Hopsworks platform as well as an API for interacting with the Hopsworks Feature Store</p>\n<div id=\"quick-start\">\n<h2>Quick Start</h2>\n<p>To Install:</p>\n<pre>&gt;&gt;&gt; pip install hops\n</pre>\n<p>Sample usage:</p>\n<pre>&gt;&gt;&gt; from hops import experiment\n&gt;&gt;&gt; from hops import hdfs\n&gt;&gt;&gt; notebook = hdfs.project_path() + \"Jupyter/Experiment/...\" #path to your notebook\n&gt;&gt;&gt; # minimal_mnist is a function you defined\n&gt;&gt;&gt; experiment.launch(minimal_mnist, #minimal_mnist is your training function\n&gt;&gt;&gt;                   name='mnist estimator',\n&gt;&gt;&gt;                   description='A minimal mnist example with two hidden layers',\n&gt;&gt;&gt;                   versioned_resources=[notebook]\n</pre>\n</div>\n<div id=\"development-instructions\">\n<h2>Development Instructions</h2>\n<p>For development details such as how to test and build docs, see this reference: <a href=\"./Development.rst\" rel=\"nofollow\">Development</a>.</p>\n</div>\n<div id=\"documentation\">\n<h2>Documentation</h2>\n<p>An overview of HopsML, a python-first ML pipeline is available here: <a href=\"https://hops.readthedocs.io/en/latest/hopsml/hopsML.html\" rel=\"nofollow\">hopsML</a></p>\n<p>Example notebooks for doing deep learning and big data processing on Hops are available here: <a href=\"https://github.com/logicalclocks/hops-examples\" rel=\"nofollow\">hops-examples</a></p>\n<p>API documentation is available here: <a href=\"http://hops-py.logicalclocks.com/\" rel=\"nofollow\">API-docs</a></p>\n</div>\n<div id=\"quick-start-python-with-hopsml\">\n<h2>Quick Start: Python with HopsML</h2>\n<p>Hops uses PySpark to distribute the execution of Python programs in a cluster. PySpark applications consist of two main components, a Driver and one to many Executors. The Driver and the Executors can be started on potentially any host in the cluster and use both the network and the HDFS filesystem to coordinate.</p>\n<div id=\"restructuring-python-programs-as-pyspark-programs\">\n<h3>Restructuring Python Programs as PySpark Programs</h3>\n<p>If you want to run a Python program, e.g.,  to train a neural network on a GPU on Hops, you will need to restructure your code. The reason for this is that your single Python process needs to be restructured as a PySpark program, see the figure below.</p>\n<div>\n<a href=\"imgs/hopsml-pyspark.png\" rel=\"nofollow\"><img alt=\"HopsML Python Program\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a1126ecbd19668269aa70607b02fd55b79af39ab/696d67732f686f70736d6c2d7079737061726b2e706e67\"></a>\n</div>\n<p>The good news is that all you will need to do to get started is to move your code inside a function. In the code snippet below, the Executor code is on lines 1-3 (the <em>train</em> function) and the Driver code is on lines 5-7. For the Executor, you define a function (e.g., <em>train</em>, but the function can have any name).  The code in the function will get run on Executors (containers). To invoke the Executor function (<em>train</em>) from the Driver (the main part of your Python program), you use the Experiment API. Launch a single Executor with <em>experiment.launch(&lt;fn_name&gt;)</em>.  Launch many Executors with <em>experiment.grid_search(&lt;fn_name&gt;)</em> for hyperparameter optimization, and <em>experiment.collective_all_reduce(&lt;fn_name&gt;)</em> for distributed training.</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">train</span><span class=\"p\">():</span>\n  <span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"nn\">tf</span>\n  <span class=\"c1\"># training code here</span>\n\n<span class=\"c1\"># Driver code starts here</span>\n<span class=\"kn\">from</span> <span class=\"nn\">hops</span> <span class=\"kn\">import</span> <span class=\"n\">experiment</span>\n<span class=\"n\">experiment</span><span class=\"o\">.</span><span class=\"n\">launch</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"p\">)</span>\n</pre>\n<div>\n<a href=\"imgs/driver.png\" rel=\"nofollow\"><img alt=\"HopsML Python Program\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f8e5e62ab7ae0f265e4ffd14218d2baf4278979a/696d67732f6472697665722e706e67\"></a>\n</div>\n</div>\n<div id=\"logging-in-the-driver\">\n<h3>Logging in the Driver</h3>\n<p>When you print to stdout and stderr in the Driver program, the output is printed in the Jupyter console.</p>\n<pre><span class=\"c1\"># main scope of program or any non-Executor function</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'log message is printed to Jupyter cell output'</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"logging-to-stdout-stderr-in-the-executor\">\n<h3>Logging to stdout/stderr in the Executor</h3>\n<p>If you execute print(\u2018\u2026\u2019) in the executor, it will send the output to stdout and stderr on the executor. This will not be displayed in Jupyter console. You can, however, read output in the executors using the Spark UI. As soon as the Spark application has exited, these logs are cleaned up - they are no longer available.</p>\n<pre><span class=\"n\">train</span><span class=\"p\">():</span>\n  <span class=\"c1\"># This will write to stdout/stderr on the Spark Executors</span>\n  <span class=\"c1\"># You can only view this log entry from the Spark UI while the application</span>\n  <span class=\"c1\"># is running.</span>\n  <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Executor log message - not visible in Jupyter, visible in Spark UI\"</span><span class=\"p\">)</span>\n</pre>\n<p>To access the Spark executor logs, you will need 4 clicks on your mouse:\n1. Select the UI for the application you started running from Jupyter (click on the button inside the yellow highlighter in the image below):</p>\n<div>\n<a href=\"imgs/executor-stderr1.png\" rel=\"nofollow\"><img alt=\"Stdout-err-1\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b36f58ecb5ba2983580445431ce35cf9b1f11995/696d67732f6578656375746f722d737464657272312e706e67\"></a>\n</div>\n<ol>\n<li>Select the \u201cExecutors\u201d tab from the Spark UI (click on the button inside the yellow highlighter):</li>\n</ol>\n<div>\n<a href=\"imgs/executor-stderr2.png\" rel=\"nofollow\"><img alt=\"Stdout-err-2\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5c966c56813c38fdd554cc683e007624ac3564d1/696d67732f6578656375746f722d737464657272322e706e67\"></a>\n</div>\n<ol>\n<li>Now you should see all the Executors that are running (active) or have finished running more than 90 seconds ago (dead). There will be stdout and stderr logs available for every Executor here - if you ran with 10 GPUs, with 1 GPU per Executor, there will be 10 different stdout and 10 different stderr log files available.. Click on the stderr or stdout log for the Executor you want to examine (yellow highlighted text below):</li>\n</ol>\n<div>\n<a href=\"imgs/executor-stderr3.png\" rel=\"nofollow\"><img alt=\"Stdout-err-3\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/660060740ceda84ab32af78a4d12cea05838e818/696d67732f6578656375746f722d737464657272332e706e67\"></a>\n</div>\n<ol>\n<li>Now you can see the logs for that Executor on the screen:</li>\n</ol>\n<div>\n<a href=\"imgs/executor-stderr4.png\" rel=\"nofollow\"><img alt=\"Stdout-err-4\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/07bb55a6f6e5c5f043b9afc583f8cce2d3e3d82e/696d67732f6578656375746f722d737464657272342e706e67\"></a>\n</div>\n</div>\n<div id=\"logging-to-file-hdfs-in-the-executor-or-driver\">\n<h3>Logging to file (HDFS) in the Executor or Driver</h3>\n<p>You can also write log messages from either an Executor or Driver to the same logfile in HDFS.</p>\n<pre><span class=\"n\">train</span><span class=\"p\">():</span>\n  <span class=\"c1\"># This will write to your Experiments/ directory in your project</span>\n  <span class=\"kn\">from</span> <span class=\"nn\">hops</span> <span class=\"kn\">import</span> <span class=\"n\">hdfs</span>\n  <span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"s2\">\"This is written to the logfile in the Experiments dataset, not output in Jupyter cell.\"</span><span class=\"p\">)</span>\n</pre>\n<p>You can navigate to the log file created in the Datasets view in Hopsworks for your project, inside the Experiments dataset. The file created will be called \u201clogfile\u201d and if you right-click on it, you can preview its contents to see the first or last 1000 lines in the file. If you have the data-owner role in the project, you will also be allowed to download this file from here.</p>\n<div>\n<a href=\"imgs/executor-hdfs-log.png\" rel=\"nofollow\"><img alt=\"hdfs-log\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0c66573067b4db8f4526b0aedc4083729074cabb/696d67732f6578656375746f722d686466732d6c6f672e706e67\"></a>\n</div>\n<p>Note that the default log file is the same for all Executors. If many Executors write concurrently to the same file, this may have negative performance implications as Executors may block, waiting for write access to the file. In large-scale experiments, you can configure each Executors to write to its own log file (append a unique ID to the filename).</p>\n</div>\n<div id=\"installing-python-libraries-in-hopsworks\">\n<h3>Installing Python Libraries in Hopsworks</h3>\n<p>You can use the \u2018Conda\u2019 and \u2018Pip\u2019 services in Hopsworks to install python libraries. In the \u2018Conda\u2019 service, you can change the conda repository by double-clicking on it and entering the URL for a new repo (or \u2018default\u2019 for the standard conda repository).</p>\n<p>Note: Pillow and matplotlib do not work from conda. Install using \u201cpip\u201d, instead.</p>\n</div>\n<div id=\"plotting-with-sparkmagic-in-jupyter\">\n<h3>Plotting with Sparkmagic in Jupyter</h3>\n<p>Hopsworks supports both the Python kernel and Sparkmagic kernel. Plotting in the Python kernel is usually handled by libraries such as matplotlib and seaborne. These libraries can also be used in the Sparkmagic kernel, but require more work from the developer, as dataframes in Spark are distributed in the cluster and need to be localized to the Jupyter notebook server as Pandas dataframes, in order to be plotted.\nWhen you run a PySpark program with the Sparkmagic kernel in Jupyter, you will not need to initialize a Spark context, as it is done automatically for you (by Sparkmagic). However, as the PySpark application is not running on the same host as the Jupyter notebook server, plotting (with matplotlib) will not work as normal in a Python kernel. The main change you need to make is to use \u2018magics\u2019 in the sparkmagic kernel to get Spark or Pandas dataframes to be localized to the Jupyter notebook server, from where they can be visualized. More details are found in the reference notebook below. Information on the magics available in Sparkmagic are found in the link below.</p>\n</div>\n<div id=\"adding-python-modules-to-a-jupyter-notebook\">\n<h3>Adding Python modules to a Jupyter notebook</h3>\n<div>\n<a href=\"imgs/add-python-module.png\" rel=\"nofollow\"><img alt=\"add-python-module\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3402791645a133032fbfd75ef23154ad42b19cd7/696d67732f6164642d707974686f6e2d6d6f64756c652e706e67\"></a>\n</div>\n</div>\n<div id=\"api-for-the-hopsworks-feature-store\">\n<h3>API for the Hopsworks Feature Store</h3>\n<p>Hopsworks has a data management layer for machine learning, called a feature store.\nThe feature store enables simple and efficient versioning, sharing, governance and definition of features that can be used to both train machine learning models or to serve inference requests.\nThe featurestore serves as a natural interface between data engineering and data science.</p>\n<p><strong>Writing to the featurestore</strong>:</p>\n<pre><span class=\"n\">raw_data</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"s2\">\"csv\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">filename</span><span class=\"p\">)</span>\n<span class=\"n\">polynomial_features</span> <span class=\"o\">=</span> <span class=\"n\">raw_data</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">x</span><span class=\"o\">^</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"kn\">from</span> <span class=\"nn\">hops</span> <span class=\"kn\">import</span> <span class=\"n\">featurestore</span>\n<span class=\"n\">featurestore</span><span class=\"o\">.</span><span class=\"n\">insert_into_featuregroup</span><span class=\"p\">(</span><span class=\"n\">polynomial_features</span><span class=\"p\">,</span> <span class=\"s2\">\"polynomial_features\"</span><span class=\"p\">)</span>\n</pre>\n<p><strong>Reading from the featurestore</strong>:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">hops</span> <span class=\"kn\">import</span> <span class=\"n\">featurestore</span>\n<span class=\"n\">features_df</span> <span class=\"o\">=</span> <span class=\"n\">featurestore</span><span class=\"o\">.</span><span class=\"n\">get_features</span><span class=\"p\">([</span><span class=\"s2\">\"team_budget\"</span><span class=\"p\">,</span> <span class=\"s2\">\"average_attendance\"</span><span class=\"p\">,</span> <span class=\"s2\">\"average_player_age\"</span><span class=\"p\">])</span>\n</pre>\n<p><strong>Integration with Sci-kit Learn</strong>:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">hops</span> <span class=\"kn\">import</span> <span class=\"n\">featurestore</span>\n<span class=\"n\">train_df</span> <span class=\"o\">=</span> <span class=\"n\">featurestore</span><span class=\"o\">.</span><span class=\"n\">get_featuregroup</span><span class=\"p\">(</span><span class=\"s2\">\"iris_features\"</span><span class=\"p\">,</span> <span class=\"n\">dataframe_type</span><span class=\"o\">=</span><span class=\"s2\">\"pandas\"</span><span class=\"p\">)</span>\n<span class=\"n\">x_df</span> <span class=\"o\">=</span> <span class=\"n\">train_df</span><span class=\"p\">[[</span><span class=\"s1\">'sepal_length'</span><span class=\"p\">,</span> <span class=\"s1\">'sepal_width'</span><span class=\"p\">,</span> <span class=\"s1\">'petal_length'</span><span class=\"p\">,</span> <span class=\"s1\">'petal_width'</span><span class=\"p\">]]</span>\n<span class=\"n\">y_df</span> <span class=\"o\">=</span> <span class=\"n\">train_df</span><span class=\"p\">[[</span><span class=\"s2\">\"label\"</span><span class=\"p\">]]</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">x_df</span><span class=\"o\">.</span><span class=\"n\">values</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y_df</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"o\">.</span><span class=\"n\">ravel</span><span class=\"p\">()</span>\n<span class=\"n\">iris_knn</span> <span class=\"o\">=</span> <span class=\"n\">KNeighborsClassifier</span><span class=\"p\">()</span>\n<span class=\"n\">iris_knn</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n</pre>\n<p><strong>Integration with Tensorflow</strong>:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">hops</span> <span class=\"kn\">import</span> <span class=\"n\">featurestore</span>\n<span class=\"n\">features_df</span> <span class=\"o\">=</span> <span class=\"n\">featurestore</span><span class=\"o\">.</span><span class=\"n\">get_features</span><span class=\"p\">(</span>\n    <span class=\"p\">[</span><span class=\"s2\">\"team_budget\"</span><span class=\"p\">,</span> <span class=\"s2\">\"average_attendance\"</span><span class=\"p\">,</span> <span class=\"s2\">\"average_player_age\"</span><span class=\"p\">,</span>\n    <span class=\"s2\">\"team_position\"</span><span class=\"p\">,</span> <span class=\"s2\">\"sum_attendance\"</span><span class=\"p\">,</span>\n     <span class=\"s2\">\"average_player_rating\"</span><span class=\"p\">,</span> <span class=\"s2\">\"average_player_worth\"</span><span class=\"p\">,</span> <span class=\"s2\">\"sum_player_age\"</span><span class=\"p\">,</span>\n     <span class=\"s2\">\"sum_player_rating\"</span><span class=\"p\">,</span> <span class=\"s2\">\"sum_player_worth\"</span><span class=\"p\">,</span> <span class=\"s2\">\"sum_position\"</span><span class=\"p\">,</span>\n     <span class=\"s2\">\"average_position\"</span>\n    <span class=\"p\">]</span>\n<span class=\"p\">)</span>\n<span class=\"n\">featurestore</span><span class=\"o\">.</span><span class=\"n\">create_training_dataset</span><span class=\"p\">(</span><span class=\"n\">features_df</span><span class=\"p\">,</span> <span class=\"s2\">\"team_position_prediction\"</span><span class=\"p\">,</span> <span class=\"n\">data_format</span><span class=\"o\">=</span><span class=\"s2\">\"tfrecords\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">create_tf_dataset</span><span class=\"p\">():</span>\n    <span class=\"n\">dataset_dir</span> <span class=\"o\">=</span> <span class=\"n\">featurestore</span><span class=\"o\">.</span><span class=\"n\">get_training_dataset_path</span><span class=\"p\">(</span><span class=\"s2\">\"team_position_prediction\"</span><span class=\"p\">)</span>\n    <span class=\"n\">input_files</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">gfile</span><span class=\"o\">.</span><span class=\"n\">Glob</span><span class=\"p\">(</span><span class=\"n\">dataset_dir</span> <span class=\"o\">+</span> <span class=\"s2\">\"/part-r-*\"</span><span class=\"p\">)</span>\n    <span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">TFRecordDataset</span><span class=\"p\">(</span><span class=\"n\">input_files</span><span class=\"p\">)</span>\n    <span class=\"n\">tf_record_schema</span> <span class=\"o\">=</span> <span class=\"n\">featurestore</span><span class=\"o\">.</span><span class=\"n\">get_training_dataset_tf_record_schema</span><span class=\"p\">(</span><span class=\"s2\">\"team_position_prediction\"</span><span class=\"p\">)</span>\n    <span class=\"n\">feature_names</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">\"team_budget\"</span><span class=\"p\">,</span> <span class=\"s2\">\"average_attendance\"</span><span class=\"p\">,</span> <span class=\"s2\">\"average_player_age\"</span><span class=\"p\">,</span> <span class=\"s2\">\"sum_attendance\"</span><span class=\"p\">,</span>\n         <span class=\"s2\">\"average_player_rating\"</span><span class=\"p\">,</span> <span class=\"s2\">\"average_player_worth\"</span><span class=\"p\">,</span> <span class=\"s2\">\"sum_player_age\"</span><span class=\"p\">,</span> <span class=\"s2\">\"sum_player_rating\"</span><span class=\"p\">,</span> <span class=\"s2\">\"sum_player_worth\"</span><span class=\"p\">,</span>\n         <span class=\"s2\">\"sum_position\"</span><span class=\"p\">,</span> <span class=\"s2\">\"average_position\"</span>\n        <span class=\"p\">]</span>\n    <span class=\"n\">label_name</span> <span class=\"o\">=</span> <span class=\"s2\">\"team_position\"</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"n\">example_proto</span><span class=\"p\">):</span>\n        <span class=\"n\">example</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">parse_single_example</span><span class=\"p\">(</span><span class=\"n\">example_proto</span><span class=\"p\">,</span> <span class=\"n\">tf_record_schema</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        <span class=\"k\">for</span> <span class=\"n\">feature_name</span> <span class=\"ow\">in</span> <span class=\"n\">feature_names</span><span class=\"p\">:</span>\n            <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">example</span><span class=\"p\">[</span><span class=\"n\">feature_name</span><span class=\"p\">])</span>\n        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">cast</span><span class=\"p\">(</span><span class=\"n\">example</span><span class=\"p\">[</span><span class=\"n\">label_name</span><span class=\"p\">],</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)]</span>\n        <span class=\"k\">return</span> <span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">y</span>\n\n    <span class=\"n\">dataset</span> <span class=\"o\">=</span> <span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span><span class=\"n\">decode</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"n\">SHUFFLE_BUFFER_SIZE</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">batch</span><span class=\"p\">(</span><span class=\"n\">BATCH_SIZE</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">repeat</span><span class=\"p\">(</span><span class=\"n\">NUM_EPOCHS</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">dataset</span>\n<span class=\"n\">tf_dataset</span> <span class=\"o\">=</span> <span class=\"n\">create_tf_dataset</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Integration with PyTorch</strong>:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">hops</span> <span class=\"kn\">import</span> <span class=\"n\">featurestore</span>\n<span class=\"n\">df_train</span><span class=\"o\">=...</span>\n<span class=\"n\">featurestore</span><span class=\"o\">.</span><span class=\"n\">create_training_dataset</span><span class=\"p\">(</span><span class=\"n\">df_train</span><span class=\"p\">,</span> <span class=\"s2\">\"MNIST_train_petastorm\"</span><span class=\"p\">,</span> <span class=\"n\">data_format</span><span class=\"o\">=</span><span class=\"s2\">\"petastorm\"</span><span class=\"p\">)</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">petastorm.pytorch</span> <span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n<span class=\"n\">train_dataset_path</span> <span class=\"o\">=</span> <span class=\"n\">featurestore</span><span class=\"o\">.</span><span class=\"n\">get_training_dataset_path</span><span class=\"p\">(</span><span class=\"s2\">\"MNIST_train_petastorm\"</span><span class=\"p\">)</span>\n<span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s1\">'cuda'</span> <span class=\"k\">if</span> <span class=\"n\">use_cuda</span> <span class=\"k\">else</span> <span class=\"s1\">'cpu'</span><span class=\"p\">)</span>\n<span class=\"k\">with</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">make_reader</span><span class=\"p\">(</span><span class=\"n\">train_dataset_path</span><span class=\"p\">,</span> <span class=\"n\">num_epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">hdfs_driver</span><span class=\"o\">=</span><span class=\"s1\">'libhdfs'</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">train_loader</span><span class=\"p\">:</span>\n        <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n        <span class=\"k\">for</span> <span class=\"n\">batch_idx</span><span class=\"p\">,</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">train_loader</span><span class=\"p\">):</span>\n            <span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"n\">row</span><span class=\"p\">[</span><span class=\"s1\">'image'</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">),</span> <span class=\"n\">row</span><span class=\"p\">[</span><span class=\"s1\">'digit'</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n</pre>\n<p><strong>Feature Visualizations</strong>:</p>\n<div>\n<a href=\"imgs/feature_plots1.png\" rel=\"nofollow\"><img alt=\"Visualizing feature distributions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a692b30716af73438b5ff20ef167cc36b8013a25/696d67732f666561747572655f706c6f7473312e706e67\"></a>\n</div>\n<div>\n<a href=\"imgs/feature_plots2.png\" rel=\"nofollow\"><img alt=\"Visualizing feature correlations\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ea8a8620cad47fe7d96caf0dedcd94dce66df563/696d67732f666561747572655f706c6f7473322e706e67\"></a>\n</div>\n</div>\n<div id=\"model-serving-api\">\n<h3>Model Serving API</h3>\n<p>In the <cite>serving</cite> module you can find an API for creating/starting/stopping/updating models being served on Hopsworks as well as making inference requests.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">hops</span> <span class=\"kn\">import</span> <span class=\"n\">serving</span>\n<span class=\"kn\">from</span> <span class=\"nn\">hops</span> <span class=\"kn\">import</span> <span class=\"n\">model</span>\n\n<span class=\"c1\"># Tensorflow</span>\n<span class=\"n\">export_path</span> <span class=\"o\">=</span> <span class=\"n\">work_dir</span> <span class=\"o\">+</span> <span class=\"s1\">'/model'</span>\n<span class=\"n\">builder</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">saved_model</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">SavedModelBuilder</span><span class=\"p\">(</span><span class=\"n\">export_path</span>\n<span class=\"o\">...</span> <span class=\"c1\"># tf specific export code</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">export</span><span class=\"p\">(</span><span class=\"n\">export_path</span><span class=\"p\">,</span> <span class=\"s2\">\"mnist\"</span><span class=\"p\">)</span>\n<span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s2\">\"/Models/mnist/\"</span>\n<span class=\"n\">SERVING_NAME</span><span class=\"o\">=</span><span class=\"s2\">\"mnist\"</span>\n<span class=\"n\">serving</span><span class=\"o\">.</span><span class=\"n\">create_or_update</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"p\">,</span> <span class=\"s2\">\"mnist\"</span><span class=\"p\">,</span> <span class=\"n\">serving_type</span><span class=\"o\">=</span><span class=\"s2\">\"TENSORFLOW\"</span><span class=\"p\">,</span> <span class=\"n\">model_version</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"k\">if</span> <span class=\"n\">serving</span><span class=\"o\">.</span><span class=\"n\">get_status</span><span class=\"p\">(</span><span class=\"s2\">\"mnist\"</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"s1\">'Stopped'</span><span class=\"p\">:</span>\n    <span class=\"n\">serving</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"p\">(</span><span class=\"s2\">\"mnist\"</span><span class=\"p\">)</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"signature_name\"</span><span class=\"p\">:</span> <span class=\"s1\">'predict_images'</span><span class=\"p\">,</span> <span class=\"s2\">\"instances\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">784</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()]}</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">serving</span><span class=\"o\">.</span><span class=\"n\">make_inference_request</span><span class=\"p\">(</span><span class=\"n\">SERVING_NAME</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">)</span>\n\n <span class=\"c1\"># SkLearn</span>\n<span class=\"n\">script_path</span> <span class=\"o\">=</span> <span class=\"s2\">\"Jupyter/Serving/sklearn/iris_flower_classifier.py\"</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">export</span><span class=\"p\">(</span><span class=\"n\">script_path</span><span class=\"p\">,</span> <span class=\"s2\">\"irisClassifier\"</span><span class=\"p\">)</span>\n<span class=\"k\">if</span> <span class=\"n\">serving</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">(</span><span class=\"s2\">\"irisClassifier\"</span><span class=\"p\">):</span>\n    <span class=\"n\">serving</span><span class=\"o\">.</span><span class=\"n\">delete</span><span class=\"p\">(</span><span class=\"s2\">\"irisClassifier\"</span><span class=\"p\">)</span>\n<span class=\"n\">serving</span><span class=\"o\">.</span><span class=\"n\">create_or_update</span><span class=\"p\">(</span><span class=\"n\">script_path</span><span class=\"p\">,</span> <span class=\"s2\">\"irisClassifier\"</span><span class=\"p\">,</span> <span class=\"n\">serving_type</span><span class=\"o\">=</span><span class=\"s2\">\"SKLEARN\"</span><span class=\"p\">,</span> <span class=\"n\">model_version</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">serving</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"p\">(</span><span class=\"s2\">\"irisClassifier\"</span><span class=\"p\">)</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"inputs\"</span> <span class=\"p\">:</span> <span class=\"p\">[[</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">uniform</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">NUM_FEATURES</span><span class=\"p\">)]]}</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">serving</span><span class=\"o\">.</span><span class=\"n\">make_inference_request</span><span class=\"p\">(</span><span class=\"n\">SERVING_NAME</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"kafka-api\">\n<h3>Kafka API</h3>\n<p>In the <cite>kafka</cite> module you can find an API to interact with kafka topics in Hopsworks.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">hops</span> <span class=\"kn\">import</span> <span class=\"n\">kafka</span><span class=\"p\">,</span> <span class=\"n\">serving</span>\n<span class=\"kn\">from</span> <span class=\"nn\">confluent_kafka</span> <span class=\"kn\">import</span> <span class=\"n\">Producer</span><span class=\"p\">,</span> <span class=\"n\">Consumer</span><span class=\"p\">,</span> <span class=\"n\">KafkaError</span>\n<span class=\"n\">TOPIC_NAME</span> <span class=\"o\">=</span> <span class=\"n\">serving</span><span class=\"o\">.</span><span class=\"n\">get_kafka_topic</span><span class=\"p\">(</span><span class=\"n\">SERVING_NAME</span><span class=\"p\">)</span> <span class=\"c1\"># get inference logs</span>\n<span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"n\">kafka</span><span class=\"o\">.</span><span class=\"n\">get_kafka_default_config</span><span class=\"p\">()</span>\n<span class=\"n\">config</span><span class=\"p\">[</span><span class=\"s1\">'default.topic.config'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">'auto.offset.reset'</span><span class=\"p\">:</span> <span class=\"s1\">'earliest'</span><span class=\"p\">}</span>\n<span class=\"n\">consumer</span> <span class=\"o\">=</span> <span class=\"n\">Consumer</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">)</span>\n<span class=\"n\">topics</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">TOPIC_NAME</span><span class=\"p\">]</span>\n<span class=\"n\">consumer</span><span class=\"o\">.</span><span class=\"n\">subscribe</span><span class=\"p\">(</span><span class=\"n\">topics</span><span class=\"p\">)</span>\n<span class=\"n\">json_schema</span> <span class=\"o\">=</span> <span class=\"n\">kafka</span><span class=\"o\">.</span><span class=\"n\">get_schema</span><span class=\"p\">(</span><span class=\"n\">TOPIC_NAME</span><span class=\"p\">)</span>\n<span class=\"n\">avro_schema</span> <span class=\"o\">=</span> <span class=\"n\">kafka</span><span class=\"o\">.</span><span class=\"n\">convert_json_schema_to_avro</span><span class=\"p\">(</span><span class=\"n\">json_schema</span><span class=\"p\">)</span>\n<span class=\"n\">msg</span> <span class=\"o\">=</span> <span class=\"n\">consumer</span><span class=\"o\">.</span><span class=\"n\">poll</span><span class=\"p\">(</span><span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">)</span>\n<span class=\"n\">value</span> <span class=\"o\">=</span> <span class=\"n\">msg</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"p\">()</span>\n<span class=\"n\">event_dict</span> <span class=\"o\">=</span> <span class=\"n\">kafka</span><span class=\"o\">.</span><span class=\"n\">parse_avro_msg</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"n\">avro_schema</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"hdfs-api\">\n<h3>HDFS API</h3>\n<p>In the <cite>hdfs</cite> module you can find a high-level API for interacting with the distributed file system</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">hops</span> <span class=\"kn\">import</span> <span class=\"n\">hdfs</span>\n<span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">ls</span><span class=\"p\">(</span><span class=\"s2\">\"Logs/\"</span><span class=\"p\">)</span>\n<span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">cp</span><span class=\"p\">(</span><span class=\"s2\">\"Resources/test.txt\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Logs/\"</span><span class=\"p\">)</span>\n<span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">mkdir</span><span class=\"p\">(</span><span class=\"s2\">\"Logs/test_dir\"</span><span class=\"p\">)</span>\n<span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">rmr</span><span class=\"p\">(</span><span class=\"s2\">\"Logs/test_dir\"</span><span class=\"p\">)</span>\n<span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">move</span><span class=\"p\">(</span><span class=\"s2\">\"Logs/README_dump_test.md\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Logs/README_dump_test2.md\"</span><span class=\"p\">)</span>\n<span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">chmod</span><span class=\"p\">(</span><span class=\"s2\">\"Logs/README.md\"</span><span class=\"p\">,</span> <span class=\"mi\">700</span><span class=\"p\">)</span>\n<span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">(</span><span class=\"s2\">\"Logs/\"</span><span class=\"p\">)</span>\n<span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">copy_to_hdfs</span><span class=\"p\">(</span><span class=\"s2\">\"test.txt\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Resources\"</span><span class=\"p\">,</span> <span class=\"n\">overwrite</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">copy_to_local</span><span class=\"p\">(</span><span class=\"s2\">\"Resources/test.txt\"</span><span class=\"p\">,</span> <span class=\"n\">overwrite</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"experiment-api\">\n<h3>Experiment API</h3>\n<p>In the <cite>experiment</cite> module you can find an API for launching reproducible machine learning experiments.\nStandalone experiments, distributed experiments, hyperparameter tuning and many more are supported.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">hops</span> <span class=\"kn\">import</span> <span class=\"n\">experiment</span>\n<span class=\"n\">log_dir</span><span class=\"p\">,</span> <span class=\"n\">best_params</span> <span class=\"o\">=</span> <span class=\"n\">experiment</span><span class=\"o\">.</span><span class=\"n\">differential_evolution</span><span class=\"p\">(</span>\n    <span class=\"n\">train_fn</span><span class=\"p\">,</span>\n    <span class=\"n\">search_dict</span><span class=\"p\">,</span>\n    <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'team_position_prediction_hyperparam_search'</span><span class=\"p\">,</span>\n    <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s1\">'Evolutionary search through the search space of hyperparameters with parallel executors to find the best parameters'</span><span class=\"p\">,</span>\n    <span class=\"n\">local_logdir</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">population</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span>\n    <span class=\"n\">generations</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n<span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"references\">\n<h3>References</h3>\n<ul>\n<li><a href=\"https://github.com/logicalclocks/hops-examples/blob/master/tensorflow/notebooks/Plotting/data_visualizations.ipynb\" rel=\"nofollow\">https://github.com/logicalclocks/hops-examples/blob/master/tensorflow/notebooks/Plotting/data_visualizations.ipynb</a></li>\n<li><a href=\"https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Magics%20in%20IPython%20Kernel.ipynb\" rel=\"nofollow\">https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Magics%20in%20IPython%20Kernel.ipynb</a></li>\n</ul>\n</div>\n</div>\n\n          </div>"}, "last_serial": 7136942, "releases": {"0.10.0.0": [{"comment_text": "", "digests": {"md5": "f07f91861767e474edf7fe063a347429", "sha256": "7b70509fb0542a725e05c3e3853b10f92bb6a65927a7282d0a471f8172cf3c95"}, "downloads": -1, "filename": "hops-0.10.0.0.tar.gz", "has_sig": false, "md5_digest": "f07f91861767e474edf7fe063a347429", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 100572, "upload_time": "2019-06-13T13:24:32", "upload_time_iso_8601": "2019-06-13T13:24:32.795624Z", "url": "https://files.pythonhosted.org/packages/c1/e2/6d564771ef4813a2597e9be9a888c0e257445fb3b3a0bcc227d7bcc5dc7d/hops-0.10.0.0.tar.gz", "yanked": false}], "0.10.0.1": [{"comment_text": "", "digests": {"md5": "97a209a67c7b0104c0cd84567c235225", "sha256": "b804956039c0bfdba8568c7d585f3c782412483a98c442500eb476b083af053b"}, "downloads": -1, "filename": "hops-0.10.0.1.tar.gz", "has_sig": false, "md5_digest": "97a209a67c7b0104c0cd84567c235225", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 100735, "upload_time": "2019-06-28T17:58:56", "upload_time_iso_8601": "2019-06-28T17:58:56.959313Z", "url": "https://files.pythonhosted.org/packages/27/11/4e6cbf6991087a75b1b42ee4264de862baaf648b523a03c850e3eaffdbaf/hops-0.10.0.1.tar.gz", "yanked": false}], "0.10.0.2": [{"comment_text": "", "digests": {"md5": "5df2e8d126d033a6bf21f21b0ae3924d", "sha256": "6f60995a76c860cce229d1916b0023178bb47b98e4a3c1a984f650fa794da08e"}, "downloads": -1, "filename": "hops-0.10.0.2.tar.gz", "has_sig": false, "md5_digest": "5df2e8d126d033a6bf21f21b0ae3924d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 100751, "upload_time": "2019-06-29T08:43:08", "upload_time_iso_8601": "2019-06-29T08:43:08.750067Z", "url": "https://files.pythonhosted.org/packages/8d/e5/b11795f331e707a2f18e1b9dfc016c6dc43be8b39f3eb2f14b324c83bf70/hops-0.10.0.2.tar.gz", "yanked": false}], "0.10.0.3": [{"comment_text": "", "digests": {"md5": "528729c68d92a3efec0e89e954cdfeb2", "sha256": "b950d7a6a2bd47e88b694068d6d3e73e8051612544003a2fd1dc694f88ae5f3a"}, "downloads": -1, "filename": "hops-0.10.0.3.tar.gz", "has_sig": false, "md5_digest": "528729c68d92a3efec0e89e954cdfeb2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 100772, "upload_time": "2019-07-09T14:25:12", "upload_time_iso_8601": "2019-07-09T14:25:12.869258Z", "url": "https://files.pythonhosted.org/packages/fd/bc/2ec29d07d15d11172ab36fdde2a91100977d3e18fd0f3b5cd55b53f72285/hops-0.10.0.3.tar.gz", "yanked": false}], "0.6.0.0": [{"comment_text": "", "digests": {"md5": "d506dda1ec5dfbf49f2cb4039c948559", "sha256": "829d48920380db4cf771885de0294730caebda3393172a70d2bb77cb0d3d5142"}, "downloads": -1, "filename": "hops-0.6.0.0.tar.gz", "has_sig": false, "md5_digest": "d506dda1ec5dfbf49f2cb4039c948559", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 35761, "upload_time": "2018-10-24T14:00:33", "upload_time_iso_8601": "2018-10-24T14:00:33.604551Z", "url": "https://files.pythonhosted.org/packages/7f/d6/9da2533ec0833ce4a1872cdcf79165e2b64c0f469c8c49c9e4e6b58834d2/hops-0.6.0.0.tar.gz", "yanked": false}], "0.6.0.1": [{"comment_text": "", "digests": {"md5": "e4176f70f2bfe3444a7f6d1e92831321", "sha256": "6bbf564b48769f56381ade0bfd37b7a7b3103bf4f808338a5a4e9ec4d1fcb5e4"}, "downloads": -1, "filename": "hops-0.6.0.1.tar.gz", "has_sig": false, "md5_digest": "e4176f70f2bfe3444a7f6d1e92831321", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 35825, "upload_time": "2018-11-06T19:52:24", "upload_time_iso_8601": "2018-11-06T19:52:24.151891Z", "url": "https://files.pythonhosted.org/packages/22/68/0c72e676fcbdf4d395364a9e26194f18ed89535d8a7622cd7e55730208cd/hops-0.6.0.1.tar.gz", "yanked": false}], "0.7.0.0": [{"comment_text": "", "digests": {"md5": "cf5e4039aaa46e5c8aa133a7d303765c", "sha256": "96373d6416386a22cb022a0f45b20cb0cb0868fdc687d1708ce9f87f1b468385"}, "downloads": -1, "filename": "hops-0.7.0.0.tar.gz", "has_sig": false, "md5_digest": "cf5e4039aaa46e5c8aa133a7d303765c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 37078, "upload_time": "2018-12-15T13:14:30", "upload_time_iso_8601": "2018-12-15T13:14:30.924413Z", "url": "https://files.pythonhosted.org/packages/5e/25/de6d9203199125fc231ed898facf36f8a6e3a92849d44a438018efa6e244/hops-0.7.0.0.tar.gz", "yanked": false}], "0.8.0.0": [{"comment_text": "", "digests": {"md5": "a7da57c7e9524191429ccdd0891b7d52", "sha256": "97e3fe8077d71cec24b82aeaf85271897936baa51e16debe6f7abe6e0d52674f"}, "downloads": -1, "filename": "hops-0.8.0.0.tar.gz", "has_sig": false, "md5_digest": "a7da57c7e9524191429ccdd0891b7d52", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 63907, "upload_time": "2019-01-05T20:36:23", "upload_time_iso_8601": "2019-01-05T20:36:23.765382Z", "url": "https://files.pythonhosted.org/packages/ef/66/d936b68e83620f50ddb5bf7f1368400103d5661058ac7ab79b6d706061e9/hops-0.8.0.0.tar.gz", "yanked": false}], "0.9.0.0": [{"comment_text": "", "digests": {"md5": "5c77204f6ffea844dbcdd04755ed49b5", "sha256": "f95e20ddd4e5678f2000f46916edcca326502ee8ab8f1cd2ecb783a31c3ecd04"}, "downloads": -1, "filename": "hops-0.9.0.0.tar.gz", "has_sig": false, "md5_digest": "5c77204f6ffea844dbcdd04755ed49b5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 64345, "upload_time": "2019-02-05T20:51:39", "upload_time_iso_8601": "2019-02-05T20:51:39.059252Z", "url": "https://files.pythonhosted.org/packages/ce/ab/21dd99c62e46686654a0df46698b552251675c2c6055a9b2e9159476621b/hops-0.9.0.0.tar.gz", "yanked": false}], "0.9.1.0": [{"comment_text": "", "digests": {"md5": "48c367ef22208538e8828d6d206c25b1", "sha256": "83ab747267dbb107482f3e4be338f1251eca4da52ed142ec9b6b44fd9d29ed54"}, "downloads": -1, "filename": "hops-0.9.1.0.tar.gz", "has_sig": false, "md5_digest": "48c367ef22208538e8828d6d206c25b1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 65620, "upload_time": "2019-03-25T16:48:46", "upload_time_iso_8601": "2019-03-25T16:48:46.898274Z", "url": "https://files.pythonhosted.org/packages/2e/39/9cacf396a1006f62407028a07ec991a76edb5635c2ad48a5e37184882197/hops-0.9.1.0.tar.gz", "yanked": false}], "0.9.1.1": [{"comment_text": "", "digests": {"md5": "43fb87512286a4c50773f188e7258060", "sha256": "0350ab3b2cb606e71b0ed10a5e74f900b00f99315979d2eb1f03f552acef8d92"}, "downloads": -1, "filename": "hops-0.9.1.1.tar.gz", "has_sig": false, "md5_digest": "43fb87512286a4c50773f188e7258060", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 65592, "upload_time": "2019-04-02T15:17:10", "upload_time_iso_8601": "2019-04-02T15:17:10.858531Z", "url": "https://files.pythonhosted.org/packages/7c/cb/68e0c33b2c980a7b2a48ffce1eff26b1502752b7982d137caa7195d55c26/hops-0.9.1.1.tar.gz", "yanked": false}], "0.9.1.2": [{"comment_text": "", "digests": {"md5": "c263bc51481cfc9d80919b00ba2292f5", "sha256": "bc2ba75d2d8be66badfb98007828d2fabcde7845959c9005363fdbb0f4b3633e"}, "downloads": -1, "filename": "hops-0.9.1.2.tar.gz", "has_sig": false, "md5_digest": "c263bc51481cfc9d80919b00ba2292f5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 67417, "upload_time": "2019-04-05T14:02:35", "upload_time_iso_8601": "2019-04-05T14:02:35.577541Z", "url": "https://files.pythonhosted.org/packages/40/ef/ac360586e53290404ea1735f49b614dbb5442b60152d397e220c279956ae/hops-0.9.1.2.tar.gz", "yanked": false}], "1.0.0.0": [{"comment_text": "", "digests": {"md5": "086b22216d1120298878f27d9d581383", "sha256": "7e2698dee41b5e77bc38f0d15a624c975ae4735dd0c19bba74ea99e2c7f22778"}, "downloads": -1, "filename": "hops-1.0.0.0.tar.gz", "has_sig": false, "md5_digest": "086b22216d1120298878f27d9d581383", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 125202, "upload_time": "2019-10-05T14:30:36", "upload_time_iso_8601": "2019-10-05T14:30:36.949626Z", "url": "https://files.pythonhosted.org/packages/64/dc/a939d04720543229a0b2f5ae7360980e0313339ab8f17e14ed453a95c7e4/hops-1.0.0.0.tar.gz", "yanked": false}], "1.0.0.1": [{"comment_text": "", "digests": {"md5": "1fdb258cea624122e6c78ead46535ec3", "sha256": "625451f2d5461dce151f1fa69eae237eb2d4361e118d39a1651d3399c33de414"}, "downloads": -1, "filename": "hops-1.0.0.1.tar.gz", "has_sig": false, "md5_digest": "1fdb258cea624122e6c78ead46535ec3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 125135, "upload_time": "2019-10-06T17:50:27", "upload_time_iso_8601": "2019-10-06T17:50:27.384919Z", "url": "https://files.pythonhosted.org/packages/35/93/20fbb1933228bfbb245772d34808b898fab609d25fd8c866bc91904d1ebb/hops-1.0.0.1.tar.gz", "yanked": false}], "1.0.0.2": [{"comment_text": "", "digests": {"md5": "33c9bf2bff8c437ba054dc414af16b50", "sha256": "ee992b252abfff1ba1c5d50d103bb1d9a329be75ffb9c357ff47fd1520425ce7"}, "downloads": -1, "filename": "hops-1.0.0.2.tar.gz", "has_sig": false, "md5_digest": "33c9bf2bff8c437ba054dc414af16b50", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 125130, "upload_time": "2019-10-07T11:26:24", "upload_time_iso_8601": "2019-10-07T11:26:24.903865Z", "url": "https://files.pythonhosted.org/packages/7c/5b/83520c7ae67b4158df47e0dbec345dafaaf5a93870b919a109893b4cc468/hops-1.0.0.2.tar.gz", "yanked": false}], "1.0.0.3": [{"comment_text": "", "digests": {"md5": "db552c9919489ec459f7b3717e023d42", "sha256": "6ee25dcfdc3682dafc7ae40f37e4ea0005ff5b3d0bd7a2ff538f79d0e6352c30"}, "downloads": -1, "filename": "hops-1.0.0.3.tar.gz", "has_sig": false, "md5_digest": "db552c9919489ec459f7b3717e023d42", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 125397, "upload_time": "2019-10-07T14:09:25", "upload_time_iso_8601": "2019-10-07T14:09:25.760329Z", "url": "https://files.pythonhosted.org/packages/f5/03/71f0bee5f64f84ea3d64d8273140f0c9aa4dcb3e1b7f8fab464576bd3084/hops-1.0.0.3.tar.gz", "yanked": false}], "1.0.0.4": [{"comment_text": "", "digests": {"md5": "f38675206ff9cc59bcde88a077d7d267", "sha256": "298638434e0327403c3babdd5c39a56156946cb8e1f82204f7dad42bbb94a7ca"}, "downloads": -1, "filename": "hops-1.0.0.4.tar.gz", "has_sig": false, "md5_digest": "f38675206ff9cc59bcde88a077d7d267", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 125624, "upload_time": "2019-10-15T11:44:09", "upload_time_iso_8601": "2019-10-15T11:44:09.898603Z", "url": "https://files.pythonhosted.org/packages/04/96/1abc17f94463c509bfc6e8d315c8605b6c7a6cbfd8fb67ce89ff60a89019/hops-1.0.0.4.tar.gz", "yanked": false}], "1.1.0.0": [{"comment_text": "", "digests": {"md5": "c9ada234a64a25e40886e8c79ec7c28b", "sha256": "40b5a029a4cadcb2655e1100324eb05fd3e783dce9c69e7cfdead3c9648f28ab"}, "downloads": -1, "filename": "hops-1.1.0.0.tar.gz", "has_sig": false, "md5_digest": "c9ada234a64a25e40886e8c79ec7c28b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 131634, "upload_time": "2019-12-20T11:41:11", "upload_time_iso_8601": "2019-12-20T11:41:11.607331Z", "url": "https://files.pythonhosted.org/packages/d1/e6/b0ad43935a90cbdcd9948d2b1bfda3229278871ed8c8d0d99e6d4c4c82b3/hops-1.1.0.0.tar.gz", "yanked": false}], "1.1.0.1": [{"comment_text": "", "digests": {"md5": "8ecd07d747757146d71c25d4ea86009c", "sha256": "6d487a52c7d8751eb5d1095189c56225cd48b27f53acbc2f194d536edd06cc75"}, "downloads": -1, "filename": "hops-1.1.0.1.tar.gz", "has_sig": false, "md5_digest": "8ecd07d747757146d71c25d4ea86009c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 131699, "upload_time": "2020-01-13T17:38:37", "upload_time_iso_8601": "2020-01-13T17:38:37.189642Z", "url": "https://files.pythonhosted.org/packages/a5/6a/644a7548c533e2d4cc9d01b4b8d45e73e4ad39c683b681585fbb09731316/hops-1.1.0.1.tar.gz", "yanked": false}], "1.1.0.2": [{"comment_text": "", "digests": {"md5": "e0f8cd244a793b4d47ba7c1dd8e1279e", "sha256": "2b154f2c29edb9babab7def26333c5e62cc3463b3060e23354d0d2f3917af7a7"}, "downloads": -1, "filename": "hops-1.1.0.2.tar.gz", "has_sig": false, "md5_digest": "e0f8cd244a793b4d47ba7c1dd8e1279e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 134153, "upload_time": "2020-03-03T13:04:50", "upload_time_iso_8601": "2020-03-03T13:04:50.092604Z", "url": "https://files.pythonhosted.org/packages/74/9a/48b79b5dd410e7e6f602472326e3b7d2f1e24655f244b400f8d449299fe1/hops-1.1.0.2.tar.gz", "yanked": false}], "1.2.0.0": [{"comment_text": "", "digests": {"md5": "5e15bc5f3828081adc64d76092a3901f", "sha256": "a93cd495382848e60d8fb5a5fd2ba2f5d83d25ab700e296d854dafd019a94088"}, "downloads": -1, "filename": "hops-1.2.0.0.tar.gz", "has_sig": false, "md5_digest": "5e15bc5f3828081adc64d76092a3901f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 133959, "upload_time": "2020-01-26T18:25:35", "upload_time_iso_8601": "2020-01-26T18:25:35.144074Z", "url": "https://files.pythonhosted.org/packages/15/67/65e0c887988a0b442c399177dcac8be6d906976ef4e2e27f3f40d63be455/hops-1.2.0.0.tar.gz", "yanked": false}], "1.2.0.1": [{"comment_text": "", "digests": {"md5": "e7cc81fb3c171871118d8687020460c5", "sha256": "d6d5a95f8f3f9206d9aaf2a9dd732fb8c35da020eb4ee89e30b1c8c0c4d9e01f"}, "downloads": -1, "filename": "hops-1.2.0.1.tar.gz", "has_sig": false, "md5_digest": "e7cc81fb3c171871118d8687020460c5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 133945, "upload_time": "2020-02-11T10:11:36", "upload_time_iso_8601": "2020-02-11T10:11:36.321043Z", "url": "https://files.pythonhosted.org/packages/c0/50/3ccc905fb799c58fc1c1f040b9733deb13002022040181914bee33cbd1eb/hops-1.2.0.1.tar.gz", "yanked": false}], "1.2.0.2": [{"comment_text": "", "digests": {"md5": "e107dc984dd06b100b298f9080fcfc83", "sha256": "7c6e243912c7ada3b4c5833e887746f8c69582ca19f8d67f6c050695c031b414"}, "downloads": -1, "filename": "hops-1.2.0.2.tar.gz", "has_sig": false, "md5_digest": "e107dc984dd06b100b298f9080fcfc83", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 134174, "upload_time": "2020-03-03T12:51:06", "upload_time_iso_8601": "2020-03-03T12:51:06.262776Z", "url": "https://files.pythonhosted.org/packages/87/3c/ca7fe913e9556a76a87a3b14941e89026576f48c175320b6020ca2d7816b/hops-1.2.0.2.tar.gz", "yanked": false}], "1.2.0.3": [{"comment_text": "", "digests": {"md5": "8c4307f9884e6a5a1abeb728de82c36a", "sha256": "5c2fdbd41f1dc699322403552ac8b0e03649847b2fb40dcb6eda704f74578cb9"}, "downloads": -1, "filename": "hops-1.2.0.3.tar.gz", "has_sig": false, "md5_digest": "8c4307f9884e6a5a1abeb728de82c36a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 134166, "upload_time": "2020-03-24T08:34:08", "upload_time_iso_8601": "2020-03-24T08:34:08.627447Z", "url": "https://files.pythonhosted.org/packages/96/f2/3d5a41b50df8346d0c1124c7f595e054b8fd564fa8bae221d2d644c561f1/hops-1.2.0.3.tar.gz", "yanked": false}], "1.2.0.4": [{"comment_text": "", "digests": {"md5": "5f43b2d13ea41a25cca81c6487fc6dd8", "sha256": "57420fa05d86b97601dcd7abd86d82ff547d2103ba77fcbf7f199302cc94d04a"}, "downloads": -1, "filename": "hops-1.2.0.4.tar.gz", "has_sig": false, "md5_digest": "5f43b2d13ea41a25cca81c6487fc6dd8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 134172, "upload_time": "2020-03-24T10:37:24", "upload_time_iso_8601": "2020-03-24T10:37:24.252102Z", "url": "https://files.pythonhosted.org/packages/0d/f8/9b63eeba9df5a8231562a44aab15df209fb199c692aa45f19b306b586366/hops-1.2.0.4.tar.gz", "yanked": false}], "1.2.0.5": [{"comment_text": "", "digests": {"md5": "6970b4e5bd39f1c2bae9967119a27bf2", "sha256": "45ddd1f3bc2a943701331a78ee48b5cbccfd8546b0fc6bbf863549aa40f36eaf"}, "downloads": -1, "filename": "hops-1.2.0.5.tar.gz", "has_sig": false, "md5_digest": "6970b4e5bd39f1c2bae9967119a27bf2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 134148, "upload_time": "2020-04-06T18:00:34", "upload_time_iso_8601": "2020-04-06T18:00:34.496901Z", "url": "https://files.pythonhosted.org/packages/14/01/34383618b29b20618942aec63176dc04d61fb806b0d26914200099b6cafb/hops-1.2.0.5.tar.gz", "yanked": false}], "1.2.0.6": [{"comment_text": "", "digests": {"md5": "cc47c6ec326f418c0922b4c56e0b1d1d", "sha256": "509af36d49502e30ec339e56b3deb1712479d1e4700ff15e6784baca611ee106"}, "downloads": -1, "filename": "hops-1.2.0.6.tar.gz", "has_sig": false, "md5_digest": "cc47c6ec326f418c0922b4c56e0b1d1d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 133966, "upload_time": "2020-04-30T12:28:22", "upload_time_iso_8601": "2020-04-30T12:28:22.628291Z", "url": "https://files.pythonhosted.org/packages/2b/ae/262860c617cf57186c582ce958929d52a8716b9296fa9f3b350ab48f28c7/hops-1.2.0.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "cc47c6ec326f418c0922b4c56e0b1d1d", "sha256": "509af36d49502e30ec339e56b3deb1712479d1e4700ff15e6784baca611ee106"}, "downloads": -1, "filename": "hops-1.2.0.6.tar.gz", "has_sig": false, "md5_digest": "cc47c6ec326f418c0922b4c56e0b1d1d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 133966, "upload_time": "2020-04-30T12:28:22", "upload_time_iso_8601": "2020-04-30T12:28:22.628291Z", "url": "https://files.pythonhosted.org/packages/2b/ae/262860c617cf57186c582ce958929d52a8716b9296fa9f3b350ab48f28c7/hops-1.2.0.6.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:38 2020"}