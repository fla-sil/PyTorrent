{"info": {"author": "blester125", "author_email": "blester125@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Environment :: Console", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 3 :: Only", "Topic :: Scientific/Engineering"], "description": "# Word Tokenizer", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/blester125/word_tokenizer/archive/0.0.3.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/blester125/word_tokenizer", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "word-tokenizer", "package_url": "https://pypi.org/project/word-tokenizer/", "platform": "", "project_url": "https://pypi.org/project/word-tokenizer/", "project_urls": {"Download": "https://github.com/blester125/word_tokenizer/archive/0.0.3.tar.gz", "Homepage": "https://github.com/blester125/word_tokenizer"}, "release_url": "https://pypi.org/project/word-tokenizer/0.0.3/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Word Tokenizer", "version": "0.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Word Tokenizer</h1>\n\n          </div>"}, "last_serial": 3855804, "releases": {"0.0.0": [{"comment_text": "", "digests": {"md5": "3b9a6a25ff3f54c026e6fc0789547177", "sha256": "a4b13077ba1e27fcd53ce5283e015797507dd88f31120b0a4d6134b11e7d0c71"}, "downloads": -1, "filename": "word_tokenizer-0.0.0.tar.gz", "has_sig": false, "md5_digest": "3b9a6a25ff3f54c026e6fc0789547177", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 909, "upload_time": "2018-04-18T18:35:36", "upload_time_iso_8601": "2018-04-18T18:35:36.393479Z", "url": "https://files.pythonhosted.org/packages/86/7e/061bdba3ce9d19582fdf503376d7286a3af83f7dbba19ee55914385e101c/word_tokenizer-0.0.0.tar.gz", "yanked": false}], "0.0.1": [{"comment_text": "", "digests": {"md5": "5fffd20110d3abcfee6755112f31b6d5", "sha256": "5c88729503f881447441573eb8b4eec7f305acbeede2d1d3b3d04e2f693de5a4"}, "downloads": -1, "filename": "word_tokenizer-0.0.1.tar.gz", "has_sig": false, "md5_digest": "5fffd20110d3abcfee6755112f31b6d5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1141, "upload_time": "2018-05-04T02:27:14", "upload_time_iso_8601": "2018-05-04T02:27:14.478013Z", "url": "https://files.pythonhosted.org/packages/79/b5/4865b8b25d7031f6ad3c977ef08bc26f1bd241f7729e1cc7a0056216fd4f/word_tokenizer-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "a75be5e64729a5b933f3520b3e36bf22", "sha256": "d53b45f9e7fb40f675d47eb9c9633954bc663a98eb15ee28f4e9755ed775420b"}, "downloads": -1, "filename": "word_tokenizer-0.0.2.tar.gz", "has_sig": false, "md5_digest": "a75be5e64729a5b933f3520b3e36bf22", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1149, "upload_time": "2018-05-04T21:41:42", "upload_time_iso_8601": "2018-05-04T21:41:42.861509Z", "url": "https://files.pythonhosted.org/packages/28/4a/b394c336de0d54972146aeb3216692c0487504d506a8a115302c5e60de3e/word_tokenizer-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "a531b4347751f368e1b3a610e391549d", "sha256": "f5e60daa5feb7eb07193e2006900fdea4505c20f91904c37c2edc289d199affb"}, "downloads": -1, "filename": "word_tokenizer-0.0.3.tar.gz", "has_sig": false, "md5_digest": "a531b4347751f368e1b3a610e391549d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 1583, "upload_time": "2018-05-12T00:35:04", "upload_time_iso_8601": "2018-05-12T00:35:04.450491Z", "url": "https://files.pythonhosted.org/packages/7a/f8/cbb2632d221f84df3f74e124df03c6e18766fa783a70cdc84f61bd5fef2b/word_tokenizer-0.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "a531b4347751f368e1b3a610e391549d", "sha256": "f5e60daa5feb7eb07193e2006900fdea4505c20f91904c37c2edc289d199affb"}, "downloads": -1, "filename": "word_tokenizer-0.0.3.tar.gz", "has_sig": false, "md5_digest": "a531b4347751f368e1b3a610e391549d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 1583, "upload_time": "2018-05-12T00:35:04", "upload_time_iso_8601": "2018-05-12T00:35:04.450491Z", "url": "https://files.pythonhosted.org/packages/7a/f8/cbb2632d221f84df3f74e124df03c6e18766fa783a70cdc84f61bd5fef2b/word_tokenizer-0.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:27:57 2020"}