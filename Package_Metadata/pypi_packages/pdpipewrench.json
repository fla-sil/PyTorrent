{"info": {"author": "Blake Naccarato", "author_email": "", "bugtrack_url": null, "classifiers": [], "description": "PdPipeWrench\n============\n\nYAML-configurable Pandas pipelines.\n\nThe `pdpipewrench` package reads input data, generates pipeline stages, and writes\noutput data entirely from the information supplied in a YAML configuration file. In\naddition, custom-made or module-specific functions may be wrapped into pipeline stages\nas specified in the YAML. Keyword arguments to such functions are also specified in\nYAML, which sidesteps the problem of hard coding parameters into numerous `*.py` files\nfor different datasets, each slightly different than the last.\n\nInstallation\n------------\n\n    pip install pdpipewrench\n\nRequirements\n------------\n\nThis package manages YAML configurations with `confuse`, which itself depends on\n`pyYAML`. Pipeline stages and pipelines are generated with `pdpipe`, and `engarde` is an\noptional dependency for `verify_all`-, `verify_any`-, and `engarde`-type stages.\n\nDetails\n-------\n\nAll aspects of a pipeline are defined in `config.yaml`. This file contains information\nabout `sources`, files from which the data is drawn, `pipelines` and their stages, and\nthe `sinks`, files to which the transformed data is written. Custom-made functions may\nbe defined in a standard `*.py` file/module, which must take a `pandas.DataFrame` as\ninput and return a `pandas.DataFrame` as output. Pipeline stages are generated from\nthese custom functions by specifying them and their keyword arguments in `config.yaml`.\n\nThe file `config.yaml` controls all aspects of the pipeline, from data discovery, to\npipeline stages, to data output. If the environment variable `PDPIPEWRENCHDIR` is not\nspecified, then then it will be set to the current working directory. The file\n`config.yaml` should be put in the `PDPIPEWRENCHDIR`, and data to be processed should be\nin that directory or its subdirectories.\n\nExample\n-------\n\nThe directory structure of this example is as follows:\n\n    example/\n        config.yaml\n        custom_functions.py\n        example.py\n        raw\n            products_storeA.csv\n            products_storeB.csv\n        output\n            products_storeA_processed.csv\n            products_storeB_processed.csv\n\nThe contents of `config.yaml` is as follows (paths are relative to the location of\n`config.yaml`, i.e. the `PDPIPEWRENCHDIR`):\n\n    sources:\n      example_source:\n        file: raw/products*.csv\n        kwargs:\n          usecols:\n            - items\n            - prices\n            - inventory\n        index_col: items\n\n    sinks:\n      example_sink:\n        file: output/*_processed.csv\n\n    pipelines:\n      example_pipeline:\n\n      - type: transform\n          function: add_to_col\n          kwargs:\n            col_name: prices\n            val: 1.5\n          staging:\n            desc: Adds $1.5 to column 'prices'\n            exmsg: Couldn't add to 'prices'.\n\n        - type: pdpipe\n          function: ColDrop\n          kwargs:\n            columns: inventory\n          staging:\n            exraise: false\n\n        - type: verify_all\n          check: high_enough\n          kwargs:\n            col_name: prices\n            val: 19\n          staging:\n            desc: Checks whether all 'prices' are over $19.\n\nThe module `custom_functions.py` contains:\n\n    custom_functions.py\n\n        def add_to_col(df, col_name, val):\n            df.loc[:, col_name] = df.loc[:, col_name] + val\n            return df\n\n        def high_enough(df, col_name, val):\n            return df.loc[:, col_name] > val\n\nFinally, the contents of the file `example.py`:\n\n    import custom_functions\n    import pdpipewrench as pdpw\n\n    src = pdpw.Source(\"example_source\")  # generate the source from `config.yaml`\n    snk = pdpw.Sink(\"example_sink\")  # generate the sink from `config.yaml`.\n\n    # generate the pipeline from `config.yaml`.\n    line = pdpw.Line(\"example_pipeline\", custom_functions)\n\n    # connect the source and sink to the pipeline, print what the pipeline will do, then run\n    # the pipeline, writing the output to disk. capture the input/output dataframes if desired.\n    pipeline = line.connect(src, snk)\n    print(pipeline)\n    (dfs_in, dfs_out) = line.run()\n\nRunning `example.py` generates `src`, `snk`, and `line` objects. Then, the `src` and\n`snk` are connected to an internal `pipeline`, which is a `pdpipe.PdPipeLine` object.\nWhen this pipeline is printed, the following output is displayed:\n\n    A pdpipe pipeline:\n    [ 0]  Adds $1.5 to column 'prices'\n    [ 1]  Drop columns inventory\n    [ 2]  Checks whether all 'prices' are over $19.\n\nThe function of this pipeline is apparent from the descriptions of each stage. Some\nstages have custom descriptions specified in the `desc` key of `config.yaml`. Stages\nof type `pdpipe` have their descriptions auto-generated from the keyword arguments.\n\nThe command `line.run()` pulls data from `src`, passes it through `pipeline`, and\ndrains it to `snk`. The returns `dfs_in` and `dfs_out` show that came in from `src`\nand what went to `snk`. In addition to `line.run()`, the first `n` stages of the\npipeline can be tested on file `m` from the source with `line.test(m,n)`.\n\nOutput from Example\n-------\n\nThis is  `.\\raw\\products_storeA.csv` before it is drawn into the source:\n\n| items   |   prices |   inventory | color |\n|:--------|---------:|------------:|------:|\n| foo     |       19 |           5 |   red |\n| bar     |       24 |           3 | green |\n| baz     |       22 |           7 |  blue |\n\nThis is  `.\\raw\\products_storeA.csv` after it is drawn into the source with the argument\n`usecols = [\"items\", \"prices\", \"inventory\"]` specified in `config.yaml`:\n\n| items   |   prices |   inventory |\n|:--------|---------:|------------:|\n| foo     |       19 |           5 |\n| bar     |       24 |           3 |\n| baz     |       22 |           7 |\n\nThe output from the pipeline is sent to `.\\products_storeA_processed.csv`. The arguments\nspecified by `config.yaml` have been applied. Namely, `prices` have been incremented by\n`1.5`, the `inventory` column has been dropped, and then a check has been made that all\n`prices` are over `19`.\n\n| items   |   prices |\n|:--------|---------:|\n| foo     |     20.5 |\n| bar     |     25.5 |\n| baz     |     23.5 |\n\nIf the `verify_all` step had failed, an exception would be raised, and the items that\ndid not pass the check would be returned in the exception message. Say, for example,\nthat the `val` argument was `21` instead of `19`:\n\n    AssertionError: ('high_enough not true for all',\n    prices  items        \n    foo      20.5)\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/blakeNaccarato/pdpipewrench", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "pdpipewrench", "package_url": "https://pypi.org/project/pdpipewrench/", "platform": "", "project_url": "https://pypi.org/project/pdpipewrench/", "project_urls": {"Homepage": "https://github.com/blakeNaccarato/pdpipewrench"}, "release_url": "https://pypi.org/project/pdpipewrench/0.2.0/", "requires_dist": ["pandas", "pdpipe", "engarde", "confuse", "setuptools ; extra == 'dev'", "wheel ; extra == 'dev'", "twine ; extra == 'dev'", "numpy ; extra == 'dev'", "scipy ; extra == 'dev'", "dtale ; extra == 'dev'", "doc8 ; extra == 'dev'", "jupyter ; extra == 'dev'", "black ; extra == 'dev'", "flake8 ; extra == 'dev'", "mypy ; extra == 'dev'", "pylint ; extra == 'dev'", "matplotlib ; extra == 'dev'", "PyQt5 ; extra == 'dev'", "rope ; extra == 'dev'"], "requires_python": ">=3.7", "summary": "YAML-configurable Pandas pipelines.", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>PdPipeWrench</h1>\n<p>YAML-configurable Pandas pipelines.</p>\n<p>The <code>pdpipewrench</code> package reads input data, generates pipeline stages, and writes\noutput data entirely from the information supplied in a YAML configuration file. In\naddition, custom-made or module-specific functions may be wrapped into pipeline stages\nas specified in the YAML. Keyword arguments to such functions are also specified in\nYAML, which sidesteps the problem of hard coding parameters into numerous <code>*.py</code> files\nfor different datasets, each slightly different than the last.</p>\n<h2>Installation</h2>\n<pre><code>pip install pdpipewrench\n</code></pre>\n<h2>Requirements</h2>\n<p>This package manages YAML configurations with <code>confuse</code>, which itself depends on\n<code>pyYAML</code>. Pipeline stages and pipelines are generated with <code>pdpipe</code>, and <code>engarde</code> is an\noptional dependency for <code>verify_all</code>-, <code>verify_any</code>-, and <code>engarde</code>-type stages.</p>\n<h2>Details</h2>\n<p>All aspects of a pipeline are defined in <code>config.yaml</code>. This file contains information\nabout <code>sources</code>, files from which the data is drawn, <code>pipelines</code> and their stages, and\nthe <code>sinks</code>, files to which the transformed data is written. Custom-made functions may\nbe defined in a standard <code>*.py</code> file/module, which must take a <code>pandas.DataFrame</code> as\ninput and return a <code>pandas.DataFrame</code> as output. Pipeline stages are generated from\nthese custom functions by specifying them and their keyword arguments in <code>config.yaml</code>.</p>\n<p>The file <code>config.yaml</code> controls all aspects of the pipeline, from data discovery, to\npipeline stages, to data output. If the environment variable <code>PDPIPEWRENCHDIR</code> is not\nspecified, then then it will be set to the current working directory. The file\n<code>config.yaml</code> should be put in the <code>PDPIPEWRENCHDIR</code>, and data to be processed should be\nin that directory or its subdirectories.</p>\n<h2>Example</h2>\n<p>The directory structure of this example is as follows:</p>\n<pre><code>example/\n    config.yaml\n    custom_functions.py\n    example.py\n    raw\n        products_storeA.csv\n        products_storeB.csv\n    output\n        products_storeA_processed.csv\n        products_storeB_processed.csv\n</code></pre>\n<p>The contents of <code>config.yaml</code> is as follows (paths are relative to the location of\n<code>config.yaml</code>, i.e. the <code>PDPIPEWRENCHDIR</code>):</p>\n<pre><code>sources:\n  example_source:\n    file: raw/products*.csv\n    kwargs:\n      usecols:\n        - items\n        - prices\n        - inventory\n    index_col: items\n\nsinks:\n  example_sink:\n    file: output/*_processed.csv\n\npipelines:\n  example_pipeline:\n\n  - type: transform\n      function: add_to_col\n      kwargs:\n        col_name: prices\n        val: 1.5\n      staging:\n        desc: Adds $1.5 to column 'prices'\n        exmsg: Couldn't add to 'prices'.\n\n    - type: pdpipe\n      function: ColDrop\n      kwargs:\n        columns: inventory\n      staging:\n        exraise: false\n\n    - type: verify_all\n      check: high_enough\n      kwargs:\n        col_name: prices\n        val: 19\n      staging:\n        desc: Checks whether all 'prices' are over $19.\n</code></pre>\n<p>The module <code>custom_functions.py</code> contains:</p>\n<pre><code>custom_functions.py\n\n    def add_to_col(df, col_name, val):\n        df.loc[:, col_name] = df.loc[:, col_name] + val\n        return df\n\n    def high_enough(df, col_name, val):\n        return df.loc[:, col_name] &gt; val\n</code></pre>\n<p>Finally, the contents of the file <code>example.py</code>:</p>\n<pre><code>import custom_functions\nimport pdpipewrench as pdpw\n\nsrc = pdpw.Source(\"example_source\")  # generate the source from `config.yaml`\nsnk = pdpw.Sink(\"example_sink\")  # generate the sink from `config.yaml`.\n\n# generate the pipeline from `config.yaml`.\nline = pdpw.Line(\"example_pipeline\", custom_functions)\n\n# connect the source and sink to the pipeline, print what the pipeline will do, then run\n# the pipeline, writing the output to disk. capture the input/output dataframes if desired.\npipeline = line.connect(src, snk)\nprint(pipeline)\n(dfs_in, dfs_out) = line.run()\n</code></pre>\n<p>Running <code>example.py</code> generates <code>src</code>, <code>snk</code>, and <code>line</code> objects. Then, the <code>src</code> and\n<code>snk</code> are connected to an internal <code>pipeline</code>, which is a <code>pdpipe.PdPipeLine</code> object.\nWhen this pipeline is printed, the following output is displayed:</p>\n<pre><code>A pdpipe pipeline:\n[ 0]  Adds $1.5 to column 'prices'\n[ 1]  Drop columns inventory\n[ 2]  Checks whether all 'prices' are over $19.\n</code></pre>\n<p>The function of this pipeline is apparent from the descriptions of each stage. Some\nstages have custom descriptions specified in the <code>desc</code> key of <code>config.yaml</code>. Stages\nof type <code>pdpipe</code> have their descriptions auto-generated from the keyword arguments.</p>\n<p>The command <code>line.run()</code> pulls data from <code>src</code>, passes it through <code>pipeline</code>, and\ndrains it to <code>snk</code>. The returns <code>dfs_in</code> and <code>dfs_out</code> show that came in from <code>src</code>\nand what went to <code>snk</code>. In addition to <code>line.run()</code>, the first <code>n</code> stages of the\npipeline can be tested on file <code>m</code> from the source with <code>line.test(m,n)</code>.</p>\n<h2>Output from Example</h2>\n<p>This is  <code>.\\raw\\products_storeA.csv</code> before it is drawn into the source:</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">items</th>\n<th align=\"right\">prices</th>\n<th align=\"right\">inventory</th>\n<th align=\"right\">color</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">foo</td>\n<td align=\"right\">19</td>\n<td align=\"right\">5</td>\n<td align=\"right\">red</td>\n</tr>\n<tr>\n<td align=\"left\">bar</td>\n<td align=\"right\">24</td>\n<td align=\"right\">3</td>\n<td align=\"right\">green</td>\n</tr>\n<tr>\n<td align=\"left\">baz</td>\n<td align=\"right\">22</td>\n<td align=\"right\">7</td>\n<td align=\"right\">blue</td>\n</tr></tbody></table>\n<p>This is  <code>.\\raw\\products_storeA.csv</code> after it is drawn into the source with the argument\n<code>usecols = [\"items\", \"prices\", \"inventory\"]</code> specified in <code>config.yaml</code>:</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">items</th>\n<th align=\"right\">prices</th>\n<th align=\"right\">inventory</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">foo</td>\n<td align=\"right\">19</td>\n<td align=\"right\">5</td>\n</tr>\n<tr>\n<td align=\"left\">bar</td>\n<td align=\"right\">24</td>\n<td align=\"right\">3</td>\n</tr>\n<tr>\n<td align=\"left\">baz</td>\n<td align=\"right\">22</td>\n<td align=\"right\">7</td>\n</tr></tbody></table>\n<p>The output from the pipeline is sent to <code>.\\products_storeA_processed.csv</code>. The arguments\nspecified by <code>config.yaml</code> have been applied. Namely, <code>prices</code> have been incremented by\n<code>1.5</code>, the <code>inventory</code> column has been dropped, and then a check has been made that all\n<code>prices</code> are over <code>19</code>.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">items</th>\n<th align=\"right\">prices</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"left\">foo</td>\n<td align=\"right\">20.5</td>\n</tr>\n<tr>\n<td align=\"left\">bar</td>\n<td align=\"right\">25.5</td>\n</tr>\n<tr>\n<td align=\"left\">baz</td>\n<td align=\"right\">23.5</td>\n</tr></tbody></table>\n<p>If the <code>verify_all</code> step had failed, an exception would be raised, and the items that\ndid not pass the check would be returned in the exception message. Say, for example,\nthat the <code>val</code> argument was <code>21</code> instead of <code>19</code>:</p>\n<pre><code>AssertionError: ('high_enough not true for all',\nprices  items        \nfoo      20.5)\n</code></pre>\n\n          </div>"}, "last_serial": 7146233, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "88e355c844603bb6b649e237632a69a6", "sha256": "56463a583304fb8c9f6355de93a7fb956e8f672d838637a4650ae757aed2ac87"}, "downloads": -1, "filename": "pdpipewrench-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "88e355c844603bb6b649e237632a69a6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 10952, "upload_time": "2020-04-22T17:27:30", "upload_time_iso_8601": "2020-04-22T17:27:30.572607Z", "url": "https://files.pythonhosted.org/packages/15/45/d096385e9605d19c7f65fe40f133ee104e3a0e0ab00bd380acc91a0710e5/pdpipewrench-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4d2a4493f6c2da653c74000162c5bfa4", "sha256": "4232c228cfde1c544702eee4014ff46efa51f2f04e833b4eaf9f112b49d20627"}, "downloads": -1, "filename": "pdpipewrench-0.1.0.tar.gz", "has_sig": false, "md5_digest": "4d2a4493f6c2da653c74000162c5bfa4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 10274, "upload_time": "2020-04-22T17:27:31", "upload_time_iso_8601": "2020-04-22T17:27:31.722618Z", "url": "https://files.pythonhosted.org/packages/22/5e/f07c0c8703d03204f9c9b64b56a15915fe47dbe93d13932138192e562b45/pdpipewrench-0.1.0.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "9462cad0e0b04e8a1835d0091a4650f5", "sha256": "412e8d7a880bf8a66aaa160066d7f3dccc30249b6d07c88187f9ff3c3305b919"}, "downloads": -1, "filename": "pdpipewrench-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "9462cad0e0b04e8a1835d0091a4650f5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 10911, "upload_time": "2020-05-01T16:28:12", "upload_time_iso_8601": "2020-05-01T16:28:12.327841Z", "url": "https://files.pythonhosted.org/packages/cf/5e/27241254b5f5296497a2520c30a8fabee4d0ea28907134800a0697d2be3c/pdpipewrench-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4aa7fa8e6156c085c49dee9ae01db15c", "sha256": "c57bb44f4ed13ad1ab317a26cd7341115c00e8c8c56100181cdaedd1ec9aeb33"}, "downloads": -1, "filename": "pdpipewrench-0.2.0.tar.gz", "has_sig": false, "md5_digest": "4aa7fa8e6156c085c49dee9ae01db15c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 10244, "upload_time": "2020-05-01T16:28:14", "upload_time_iso_8601": "2020-05-01T16:28:14.092930Z", "url": "https://files.pythonhosted.org/packages/c4/1d/b794836f640b17d5a9a4e8018321af132b5119431006fd3e09b1e56839c9/pdpipewrench-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "9462cad0e0b04e8a1835d0091a4650f5", "sha256": "412e8d7a880bf8a66aaa160066d7f3dccc30249b6d07c88187f9ff3c3305b919"}, "downloads": -1, "filename": "pdpipewrench-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "9462cad0e0b04e8a1835d0091a4650f5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 10911, "upload_time": "2020-05-01T16:28:12", "upload_time_iso_8601": "2020-05-01T16:28:12.327841Z", "url": "https://files.pythonhosted.org/packages/cf/5e/27241254b5f5296497a2520c30a8fabee4d0ea28907134800a0697d2be3c/pdpipewrench-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4aa7fa8e6156c085c49dee9ae01db15c", "sha256": "c57bb44f4ed13ad1ab317a26cd7341115c00e8c8c56100181cdaedd1ec9aeb33"}, "downloads": -1, "filename": "pdpipewrench-0.2.0.tar.gz", "has_sig": false, "md5_digest": "4aa7fa8e6156c085c49dee9ae01db15c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 10244, "upload_time": "2020-05-01T16:28:14", "upload_time_iso_8601": "2020-05-01T16:28:14.092930Z", "url": "https://files.pythonhosted.org/packages/c4/1d/b794836f640b17d5a9a4e8018321af132b5119431006fd3e09b1e56839c9/pdpipewrench-0.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:57:04 2020"}