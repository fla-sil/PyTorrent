{"info": {"author": "Thomas Breuel", "author_email": "tmbdev+removeme@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "License :: OSI Approved :: BSD License", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "# Torchmore\n\nThe `torchmore` library is a small library of layers and utilities\nfor writing PyTorch models for image recognition, OCR, and other applications.\n\n\n# Flex\n\n\nThe `flex` library performs simple size inference. It does so by wrapping up individual layers in a wrapper that instantiates the layer only when dimensional data is available. The wrappers can be removed later and the model turned into one with only completely standard modules. That looks like this:\n\n    from torch import nn\n    from torchmore import layers, flex\n\n    noutput = 10\n\n    model = nn.Sequential(\n        layers.Input(\"BDHW\"),\n        flex.Conv2d(100),\n        flex.BatchNorm(),\n        nn.ReLU(),\n        flex.Conv2d(100),\n        flex.BatchNorm(),\n        nn.ReLU(),\n        layers.Reshape([1, [2, 3, 4]]),\n        flex.Full(100),\n        flex.BatchNorm(),\n        nn.ReLU(),\n        flex.Full(noutput)\n    )\n\n    flex.shape_inference(model, (1, 1, 28, 28))\n\n\n\nThe `flex` library provides wrappers for the following layers right now:\n\n- `Linear`\n- `Conv1d`, `Conv2d`, `Conv3d`\n- `ConvTranspose1d`, `ConvTranspose2d`, `ConvTranspose3d`\n- `LSTM`, `BDL_LSTM`, `BDHW_LSTM`\n- `BatchNorm1d`, `BatchNorm2d`, `BatchNorm3d`\n- `BatchNorm`\n\nYou can use `Flex` directly. The following two layers are identical:\n\n    layer1 = flex.Conv2d(100)\n    layer2 = flex.Flex(lambda x: nn.Conv2d(x.size(1), 100))\n\nThat is, you can easily turn any layer into a `Flex` layer that way even if it isn't in the library.\n\n\n# Layers\n\n\n## layers.Input\n\nThe `Input` layer is a handy little layer that reorders input dimensions, checks size ranges and value ranges, and automatically transfers data to the current device on which the model runs.\n\nFor example, consider the following `Input` layer:\n\n        layers.Input(\"BHWD\", \"BDHW\", range=(0, 1), sizes=[None, 1, None, None]),\n\nThis says:\n\n- the input is in \"BHWD\" order and will get reordered to \"BDHW\"\n- input values must be in the interval $[0, 1]$\n- input tensors must have $D=1$\n- input tensors are transferred to the same device as weights for the model\n\n## The `.order` Attribute\n\nNote that if the input tensor has a `.order` attribute, that will be used to reorder the input dimensions into the desired dimensions. This allows the model to accept inputs in multiple orders. Consider\n\n    model = nn.Sequential(\n        layers.Input(\"BHWD\", \"BDHW\", range=(0, 1), sizes=[None, 1, None, None]),\n        ...\n    )\n    a = torch.rand((1, 100, 150, 1))\n    b = a.permute(0, 3, 1, 2)\n    b.order = \"BDHW\"\n\n    assert model(a) == model(b)\n\n\n\n# layers.Reorder\n\nThe `Reorder` layer reorders axes just like `Tensor.permute` does, but it does so in a way that documents better what is going on. Consider the following code fragment:\n\n        layers.Reorder(\"BDL\", \"LBD\"),\n        flex.LSTM(100, bidirectional=True),\n        layers.Reorder(\"LBD\", \"BDL\"),\n        flex.Conv1d(noutput, 1),\n        layers.Reorder(\"BDL\", \"BLD\")\n\nThe letters themselves are arbitrary, but common choices are \"BDLHW\". This is likely clearer than a sequence of permutations.\n\n\n## layers.Fun\n\nFor module-based networks, it's convenient to add functions. The `Fun` layer permits that, as in:\n\n        layers.Fun(\"lambda x: x.permute(2, 0, 1)\")\n\nNote that since functions are specified as strings, this can be pickled.\n\n\n# LSTM layers\n\n- `layers.LSTM`: a trivial LSTM layer that simply dicards the state output\n- `layers.BDL_LSTM`: an LSTM variant that is a drop-in replacement for a `Conv1d` layer\n- `layers.BDHW_LSTM`: an MDLSTM variant that is a drop-in replacement for a `Conv2d` layer\n- `layers.BDHW_LSTM_to_BDH`: a rowwise LSTM, reducing dimension by 1\n\n\n\n# Other Layers\n\nThese may be occasionally useful:\n\n- `layers.Info(info=\"\", every=1000000)`: prints info about the activations\n- `layers.CheckSizes(...)`: checks the sizes of tensors propagated through\n- `layers.CheckRange(...)`: checks the ranges of values\n- `layers.Permute(...)`: axis permutation (like x.permute)\n- `layers.Reshape(...)`: tensor reshaping, with the option of combining axes\n- `layers.View(...)`: equivalent of x.view\n- `layers.Parallel`: run two modules in parallel and stack the results\n- `layers.SimplePooling2d`: wrapped up max pooling/unpooling\n- `layers.AcrossPooling2d`: wrapped up max pooling/unpooling with convolution\n\n\n```python\n\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/tmbdev/torchmore", "keywords": "object store,client,deep learning", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "torchmore", "package_url": "https://pypi.org/project/torchmore/", "platform": "", "project_url": "https://pypi.org/project/torchmore/", "project_urls": {"Homepage": "http://github.com/tmbdev/torchmore"}, "release_url": "https://pypi.org/project/torchmore/0.1.0/", "requires_dist": ["Pillow", "braceexpand", "msgpack", "numpy", "pyyaml", "scipy", "simplejson", "torch", "torchvision"], "requires_python": ">=3.6", "summary": "Useful additional layers for PyTorch.", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Torchmore</h1>\n<p>The <code>torchmore</code> library is a small library of layers and utilities\nfor writing PyTorch models for image recognition, OCR, and other applications.</p>\n<h1>Flex</h1>\n<p>The <code>flex</code> library performs simple size inference. It does so by wrapping up individual layers in a wrapper that instantiates the layer only when dimensional data is available. The wrappers can be removed later and the model turned into one with only completely standard modules. That looks like this:</p>\n<pre><code>from torch import nn\nfrom torchmore import layers, flex\n\nnoutput = 10\n\nmodel = nn.Sequential(\n    layers.Input(\"BDHW\"),\n    flex.Conv2d(100),\n    flex.BatchNorm(),\n    nn.ReLU(),\n    flex.Conv2d(100),\n    flex.BatchNorm(),\n    nn.ReLU(),\n    layers.Reshape([1, [2, 3, 4]]),\n    flex.Full(100),\n    flex.BatchNorm(),\n    nn.ReLU(),\n    flex.Full(noutput)\n)\n\nflex.shape_inference(model, (1, 1, 28, 28))\n</code></pre>\n<p>The <code>flex</code> library provides wrappers for the following layers right now:</p>\n<ul>\n<li><code>Linear</code></li>\n<li><code>Conv1d</code>, <code>Conv2d</code>, <code>Conv3d</code></li>\n<li><code>ConvTranspose1d</code>, <code>ConvTranspose2d</code>, <code>ConvTranspose3d</code></li>\n<li><code>LSTM</code>, <code>BDL_LSTM</code>, <code>BDHW_LSTM</code></li>\n<li><code>BatchNorm1d</code>, <code>BatchNorm2d</code>, <code>BatchNorm3d</code></li>\n<li><code>BatchNorm</code></li>\n</ul>\n<p>You can use <code>Flex</code> directly. The following two layers are identical:</p>\n<pre><code>layer1 = flex.Conv2d(100)\nlayer2 = flex.Flex(lambda x: nn.Conv2d(x.size(1), 100))\n</code></pre>\n<p>That is, you can easily turn any layer into a <code>Flex</code> layer that way even if it isn't in the library.</p>\n<h1>Layers</h1>\n<h2>layers.Input</h2>\n<p>The <code>Input</code> layer is a handy little layer that reorders input dimensions, checks size ranges and value ranges, and automatically transfers data to the current device on which the model runs.</p>\n<p>For example, consider the following <code>Input</code> layer:</p>\n<pre><code>    layers.Input(\"BHWD\", \"BDHW\", range=(0, 1), sizes=[None, 1, None, None]),\n</code></pre>\n<p>This says:</p>\n<ul>\n<li>the input is in \"BHWD\" order and will get reordered to \"BDHW\"</li>\n<li>input values must be in the interval $[0, 1]$</li>\n<li>input tensors must have $D=1$</li>\n<li>input tensors are transferred to the same device as weights for the model</li>\n</ul>\n<h2>The <code>.order</code> Attribute</h2>\n<p>Note that if the input tensor has a <code>.order</code> attribute, that will be used to reorder the input dimensions into the desired dimensions. This allows the model to accept inputs in multiple orders. Consider</p>\n<pre><code>model = nn.Sequential(\n    layers.Input(\"BHWD\", \"BDHW\", range=(0, 1), sizes=[None, 1, None, None]),\n    ...\n)\na = torch.rand((1, 100, 150, 1))\nb = a.permute(0, 3, 1, 2)\nb.order = \"BDHW\"\n\nassert model(a) == model(b)\n</code></pre>\n<h1>layers.Reorder</h1>\n<p>The <code>Reorder</code> layer reorders axes just like <code>Tensor.permute</code> does, but it does so in a way that documents better what is going on. Consider the following code fragment:</p>\n<pre><code>    layers.Reorder(\"BDL\", \"LBD\"),\n    flex.LSTM(100, bidirectional=True),\n    layers.Reorder(\"LBD\", \"BDL\"),\n    flex.Conv1d(noutput, 1),\n    layers.Reorder(\"BDL\", \"BLD\")\n</code></pre>\n<p>The letters themselves are arbitrary, but common choices are \"BDLHW\". This is likely clearer than a sequence of permutations.</p>\n<h2>layers.Fun</h2>\n<p>For module-based networks, it's convenient to add functions. The <code>Fun</code> layer permits that, as in:</p>\n<pre><code>    layers.Fun(\"lambda x: x.permute(2, 0, 1)\")\n</code></pre>\n<p>Note that since functions are specified as strings, this can be pickled.</p>\n<h1>LSTM layers</h1>\n<ul>\n<li><code>layers.LSTM</code>: a trivial LSTM layer that simply dicards the state output</li>\n<li><code>layers.BDL_LSTM</code>: an LSTM variant that is a drop-in replacement for a <code>Conv1d</code> layer</li>\n<li><code>layers.BDHW_LSTM</code>: an MDLSTM variant that is a drop-in replacement for a <code>Conv2d</code> layer</li>\n<li><code>layers.BDHW_LSTM_to_BDH</code>: a rowwise LSTM, reducing dimension by 1</li>\n</ul>\n<h1>Other Layers</h1>\n<p>These may be occasionally useful:</p>\n<ul>\n<li><code>layers.Info(info=\"\", every=1000000)</code>: prints info about the activations</li>\n<li><code>layers.CheckSizes(...)</code>: checks the sizes of tensors propagated through</li>\n<li><code>layers.CheckRange(...)</code>: checks the ranges of values</li>\n<li><code>layers.Permute(...)</code>: axis permutation (like x.permute)</li>\n<li><code>layers.Reshape(...)</code>: tensor reshaping, with the option of combining axes</li>\n<li><code>layers.View(...)</code>: equivalent of x.view</li>\n<li><code>layers.Parallel</code>: run two modules in parallel and stack the results</li>\n<li><code>layers.SimplePooling2d</code>: wrapped up max pooling/unpooling</li>\n<li><code>layers.AcrossPooling2d</code>: wrapped up max pooling/unpooling with convolution</li>\n</ul>\n<pre></pre>\n\n          </div>"}, "last_serial": 6685706, "releases": {"0.0.0": [{"comment_text": "", "digests": {"md5": "f6af28fbbe19d02182000815195ccca9", "sha256": "062030ee71c33e33dbfcae4c93c0f007834c9fabc90d76a342741669021bd24a"}, "downloads": -1, "filename": "torchmore-0.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f6af28fbbe19d02182000815195ccca9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 13093, "upload_time": "2020-02-23T22:39:54", "upload_time_iso_8601": "2020-02-23T22:39:54.239608Z", "url": "https://files.pythonhosted.org/packages/15/3b/2e4e99e7650ee27170ad42809d64bcd65f8813bfed5b41ab68971dac7735/torchmore-0.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a23393b33d6a4b5e77919227f388c5c3", "sha256": "474d79d188cd6ba1fcf079ca97fde6dcf3be786aca6f0162ef9a9bd168c41d5c"}, "downloads": -1, "filename": "torchmore-0.0.0.tar.gz", "has_sig": false, "md5_digest": "a23393b33d6a4b5e77919227f388c5c3", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13420, "upload_time": "2020-02-23T22:39:56", "upload_time_iso_8601": "2020-02-23T22:39:56.541060Z", "url": "https://files.pythonhosted.org/packages/66/27/35ebdd07056c166e5f5c55f3d6f60d2e1733c1a6b9a77497c9fe405bcb42/torchmore-0.0.0.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "0b2f06db4613aa765002e010140a5623", "sha256": "6987868e92414b0d0b629a5cf46cce3f46a0d16feefe244a063c9d1385c8a3fc"}, "downloads": -1, "filename": "torchmore-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0b2f06db4613aa765002e010140a5623", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 13095, "upload_time": "2020-02-23T23:29:36", "upload_time_iso_8601": "2020-02-23T23:29:36.262255Z", "url": "https://files.pythonhosted.org/packages/9f/f9/4cf8d04eb675bc3b730e053ffdfc821fe70d58316de6763deb9110ff9e9e/torchmore-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e4e5818a26b597a4ff582d42c6270b33", "sha256": "e4f656693dd668747f39919de61be2ecbb849cde2c6a22adfb9530cf65e38654"}, "downloads": -1, "filename": "torchmore-0.1.0.tar.gz", "has_sig": false, "md5_digest": "e4e5818a26b597a4ff582d42c6270b33", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13421, "upload_time": "2020-02-23T23:29:37", "upload_time_iso_8601": "2020-02-23T23:29:37.779625Z", "url": "https://files.pythonhosted.org/packages/a4/a7/a245952774f7962d131e2ac89dbd34dc4955323c5978602b4b6a037c8d5b/torchmore-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "0b2f06db4613aa765002e010140a5623", "sha256": "6987868e92414b0d0b629a5cf46cce3f46a0d16feefe244a063c9d1385c8a3fc"}, "downloads": -1, "filename": "torchmore-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0b2f06db4613aa765002e010140a5623", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 13095, "upload_time": "2020-02-23T23:29:36", "upload_time_iso_8601": "2020-02-23T23:29:36.262255Z", "url": "https://files.pythonhosted.org/packages/9f/f9/4cf8d04eb675bc3b730e053ffdfc821fe70d58316de6763deb9110ff9e9e/torchmore-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "e4e5818a26b597a4ff582d42c6270b33", "sha256": "e4f656693dd668747f39919de61be2ecbb849cde2c6a22adfb9530cf65e38654"}, "downloads": -1, "filename": "torchmore-0.1.0.tar.gz", "has_sig": false, "md5_digest": "e4e5818a26b597a4ff582d42c6270b33", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 13421, "upload_time": "2020-02-23T23:29:37", "upload_time_iso_8601": "2020-02-23T23:29:37.779625Z", "url": "https://files.pythonhosted.org/packages/a4/a7/a245952774f7962d131e2ac89dbd34dc4955323c5978602b4b6a037c8d5b/torchmore-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:15 2020"}