{"info": {"author": "Randall Balestriero", "author_email": "randallbalestriero@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Operating System :: Unix", "Programming Language :: Python :: 3"], "description": "<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/RandallBalestriero/SymJAX/master/doc/img/logo.png\" alt=\"logo\"></img>\n</div>\n\n# SymJAX: symbolic CPU/GPU/TPU programming [![Test status](https://travis-ci.org/google/jax.svg?branch=master)](https://travis-ci.org/google/jax)\n\nThis is an under development research project, not an official product, expect bugs and sharp edges; please help by trying it out, reporting bugs.\n[**Reference docs**](https://symjax.readthedocs.io/en/latest/)\n\n\n## What is SymJAX ?\n\nSymJAX is a symbolic programming version of JAX simplifying graph input/output/updates and providing additional functionalities for general machine learning and deep learning applications. From an user perspective SymJAX apparents to Theano with fast graph optimization/compilation and broad hardware support, along with Lasagne-like deep learning functionalities\n\n## Examples\n\n```python\nimport sys\nimport symjax as sj\nimport symjax.tensor as T\n\n# create our variable to be optimized\nmu = T.Variable(T.random.normal((), seed=1))\n\n# create our cost\ncost = T.exp(-(mu-1)**2)\n\n# get the gradient, notice that it is itself a tensor that can then\n# be manipulated as well\ng = sj.gradients(cost, mu)\nprint(g)\n\n# (Tensor: shape=(), dtype=float32)\n\n# create the compield function that will compute the cost and apply\n# the update onto the variable\nf = sj.function(outputs=cost, updates={mu:mu-0.2*g})\n\nfor i in range(10):\n    print(f())\n\n# 0.008471076\n# 0.008201109\n# 0.007946267\n# ...\n```\n\n## Installation\n\nMake sure to install all the needed GPU drivers (for GPU support, not mandatory) and install JAX as follows (see [**guide**](https://github.com/google/jax/blob/master/README.md#installation)):\n\n    # install jaxlib\n    PYTHON_VERSION=cp37  # alternatives: cp35, cp36, cp37, cp38\n    CUDA_VERSION=cuda92  # alternatives: cuda92, cuda100, cuda101, cuda102\n    PLATFORM=linux_x86_64  # alternatives: linux_x86_64\n    BASE_URL='https://storage.googleapis.com/jax-releases'\n    pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.39-$PYTHON_VERSION-none-$PLATFORM.whl\n\n    pip install --upgrade jax  # install jax\n\nThen simply install SymJAX as follows:\n\n    pip install symjax\n\nonce this is done, to leverage the dataset please set up the environment variable\n\n    export DATASET_PATH=/path/to/default/location/\n\nthis path will be used as the default path where to download the various datasets in case no explicit path is given.\nAdditionally, the following options are standard to be set up to link with the CUDA library and deactivate the memory preallocation (example below for CUDA10.1, change for desired version)\n\n    export CUDA_DIR=\"/usr/local/cuda-10.1\"\n    export LD_LIBRARY_PATH=$CUDA_DIR/lib64:$LD_LIBRARY_PATH \n    export LIBRARY_PATH=$CUDA_DIR/lib64:$LIBRARY_PATH                               \n    export XLA_PYTHON_CLIENT_PREALLOCATE='false'                                \n    export XLA_FLAGS=\"--xla_gpu_cuda_data_dir=$CUDA_DIR\"             \n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/RandallBalestriero/SymJAX.git", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "symjax", "package_url": "https://pypi.org/project/symjax/", "platform": "", "project_url": "https://pypi.org/project/symjax/", "project_urls": {"Homepage": "https://github.com/RandallBalestriero/SymJAX.git"}, "release_url": "https://pypi.org/project/symjax/0.2/", "requires_dist": null, "requires_python": "", "summary": "A Symbolic JAX software", "version": "0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div>\n<img alt=\"logo\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/44daa01b731159895a460408dc4409b084f38f9a/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f52616e64616c6c42616c657374726965726f2f53796d4a41582f6d61737465722f646f632f696d672f6c6f676f2e706e67\">\n</div>\n<h1>SymJAX: symbolic CPU/GPU/TPU programming <a href=\"https://travis-ci.org/google/jax\" rel=\"nofollow\"><img alt=\"Test status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/51ea80b198f20d2aa034da1e23b876d727033233/68747470733a2f2f7472617669732d63692e6f72672f676f6f676c652f6a61782e7376673f6272616e63683d6d6173746572\"></a></h1>\n<p>This is an under development research project, not an official product, expect bugs and sharp edges; please help by trying it out, reporting bugs.\n<a href=\"https://symjax.readthedocs.io/en/latest/\" rel=\"nofollow\"><strong>Reference docs</strong></a></p>\n<h2>What is SymJAX ?</h2>\n<p>SymJAX is a symbolic programming version of JAX simplifying graph input/output/updates and providing additional functionalities for general machine learning and deep learning applications. From an user perspective SymJAX apparents to Theano with fast graph optimization/compilation and broad hardware support, along with Lasagne-like deep learning functionalities</p>\n<h2>Examples</h2>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">sys</span>\n<span class=\"kn\">import</span> <span class=\"nn\">symjax</span> <span class=\"k\">as</span> <span class=\"nn\">sj</span>\n<span class=\"kn\">import</span> <span class=\"nn\">symjax.tensor</span> <span class=\"k\">as</span> <span class=\"nn\">T</span>\n\n<span class=\"c1\"># create our variable to be optimized</span>\n<span class=\"n\">mu</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">((),</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># create our cost</span>\n<span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"p\">(</span><span class=\"n\">mu</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># get the gradient, notice that it is itself a tensor that can then</span>\n<span class=\"c1\"># be manipulated as well</span>\n<span class=\"n\">g</span> <span class=\"o\">=</span> <span class=\"n\">sj</span><span class=\"o\">.</span><span class=\"n\">gradients</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">,</span> <span class=\"n\">mu</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">g</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># (Tensor: shape=(), dtype=float32)</span>\n\n<span class=\"c1\"># create the compield function that will compute the cost and apply</span>\n<span class=\"c1\"># the update onto the variable</span>\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">sj</span><span class=\"o\">.</span><span class=\"n\">function</span><span class=\"p\">(</span><span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"n\">cost</span><span class=\"p\">,</span> <span class=\"n\">updates</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"n\">mu</span><span class=\"p\">:</span><span class=\"n\">mu</span><span class=\"o\">-</span><span class=\"mf\">0.2</span><span class=\"o\">*</span><span class=\"n\">g</span><span class=\"p\">})</span>\n\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">())</span>\n\n<span class=\"c1\"># 0.008471076</span>\n<span class=\"c1\"># 0.008201109</span>\n<span class=\"c1\"># 0.007946267</span>\n<span class=\"c1\"># ...</span>\n</pre>\n<h2>Installation</h2>\n<p>Make sure to install all the needed GPU drivers (for GPU support, not mandatory) and install JAX as follows (see <a href=\"https://github.com/google/jax/blob/master/README.md#installation\" rel=\"nofollow\"><strong>guide</strong></a>):</p>\n<pre><code># install jaxlib\nPYTHON_VERSION=cp37  # alternatives: cp35, cp36, cp37, cp38\nCUDA_VERSION=cuda92  # alternatives: cuda92, cuda100, cuda101, cuda102\nPLATFORM=linux_x86_64  # alternatives: linux_x86_64\nBASE_URL='https://storage.googleapis.com/jax-releases'\npip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.39-$PYTHON_VERSION-none-$PLATFORM.whl\n\npip install --upgrade jax  # install jax\n</code></pre>\n<p>Then simply install SymJAX as follows:</p>\n<pre><code>pip install symjax\n</code></pre>\n<p>once this is done, to leverage the dataset please set up the environment variable</p>\n<pre><code>export DATASET_PATH=/path/to/default/location/\n</code></pre>\n<p>this path will be used as the default path where to download the various datasets in case no explicit path is given.\nAdditionally, the following options are standard to be set up to link with the CUDA library and deactivate the memory preallocation (example below for CUDA10.1, change for desired version)</p>\n<pre><code>export CUDA_DIR=\"/usr/local/cuda-10.1\"\nexport LD_LIBRARY_PATH=$CUDA_DIR/lib64:$LD_LIBRARY_PATH \nexport LIBRARY_PATH=$CUDA_DIR/lib64:$LIBRARY_PATH                               \nexport XLA_PYTHON_CLIENT_PREALLOCATE='false'                                \nexport XLA_FLAGS=\"--xla_gpu_cuda_data_dir=$CUDA_DIR\"             \n</code></pre>\n\n          </div>"}, "last_serial": 6670000, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "1240ef3c46eb83b5a8e12dba132040c4", "sha256": "f8a74b88679900a654d43894264f015e2a33a323028a2668e4bea08f4a286232"}, "downloads": -1, "filename": "symjax-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "1240ef3c46eb83b5a8e12dba132040c4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 107306, "upload_time": "2020-02-10T01:59:42", "upload_time_iso_8601": "2020-02-10T01:59:42.553682Z", "url": "https://files.pythonhosted.org/packages/64/e3/acf5a4d60e8b092d6b89b132ea54330fdfd83037735ac7b4a2c576c75615/symjax-0.1-py3-none-any.whl", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "abc2df98240ca17e17f6985fe9cada90", "sha256": "bd1470f31e5b0791ee8c2571a01a38f1cec0ae2302d9637cee0131c6af142ad7"}, "downloads": -1, "filename": "symjax-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "abc2df98240ca17e17f6985fe9cada90", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 116347, "upload_time": "2020-02-20T18:53:39", "upload_time_iso_8601": "2020-02-20T18:53:39.568742Z", "url": "https://files.pythonhosted.org/packages/9b/c3/2efd3d413026f18366637cc927cbb419700ef944250f2bc5150d1de355e6/symjax-0.2-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "abc2df98240ca17e17f6985fe9cada90", "sha256": "bd1470f31e5b0791ee8c2571a01a38f1cec0ae2302d9637cee0131c6af142ad7"}, "downloads": -1, "filename": "symjax-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "abc2df98240ca17e17f6985fe9cada90", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 116347, "upload_time": "2020-02-20T18:53:39", "upload_time_iso_8601": "2020-02-20T18:53:39.568742Z", "url": "https://files.pythonhosted.org/packages/9b/c3/2efd3d413026f18366637cc927cbb419700ef944250f2bc5150d1de355e6/symjax-0.2-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 02:59:11 2020"}