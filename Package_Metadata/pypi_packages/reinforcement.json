{"info": {"author": "Bernhard Raml", "author_email": "pypi-reinforcment@googlegroups.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "[![Build Status](https://travis-ci.org/SwamyDev/reinforcement.svg?branch=master)](https://travis-ci.org/SwamyDev/reinforcement) [![Coverage Status](https://coveralls.io/repos/github/SwamyDev/reinforcement/badge.svg?branch=master)](https://coveralls.io/github/SwamyDev/reinforcement?branch=master) [![PyPI version](https://badge.fury.io/py/reinforcement.svg)](https://badge.fury.io/py/reinforcement)\n\n# Reinforcement\nThe reinforcement package aims to provide simple implementations for basic reinforcement learning algorithms, using Test Driven Development and other principles of Software Engineering in an attempt to minimize defects and improve reproducibility. \n\n## Installation\nThe library can be installed using pip:\n```bash\npip install reinforcement\n```\n\n## Example Implementation\nThis section demonstrates how to implement a REINFORCE agent and benchmark it on the 'CartPole' gym environment.\n\nYou can find the full implementation in [examples/reinforce.py](example/reinforce.py). The [example folder](example/) also contains some additional utility classes and functions that are used in the implementation.\n\n[embedmd]:# (example/reinforce.py python /def run_reinforce/ /env.close\\(\\)/)\n```python\ndef run_reinforce(config):\n    reporter, env, rewards = Reporter(config), gym.make('CartPole-v0'), []\n    with tf1.Session() as session:\n        agent = _make_agent(config, session, env)\n        for episode in range(1, config.episodes + 1):\n            reward = _run_episode(env, episode, agent, reporter)\n            rewards.append(reward)\n            if reporter.should_log(episode):\n                logger.info(reporter.report(episode, rewards))\n    env.close()\n```\nThis is the main function setting up the boiler plate code. It creates the tensorflow session, logs the progress, and creats the agent. The `Reporter` class is just a helper to make logging at a certain frequency more convenient\n\n[embedmd]:# (example/reinforce.py python /def _make_agent/ /return BatchAgent\\(alg\\)/)\n```python\ndef _make_agent(config, session, env):\n    p = ParameterizedPolicy(session, env.observation_space.shape[0], env.action_space.n, NoLog(), config.lr_policy)\n    b = ValueBaseline(session, env.observation_space.shape[0], NoLog(), config.lr_baseline)\n    alg = Reinforce(p, config.gamma, b, config.num_trajectories)\n    return BatchAgent(alg)\n```\n\nThe factory function `_make_agent` creates the REINFORCE agent object. It uses a parameterized policy and baseline to learn and estimate proper actions. In this case, both parameterizations are straightforward artificial neural networks with no hidden layer. Both have the same input layer, but the output layer of the policy is a softmax function, whereas the baseline outputs a single linear value. The `BatchAgent` type records trajectories (states, actions, rewards) which are then used to optimize the policy and the baseline. The `NoLog` class is a Null-Object implementing the TensorBoard `FileWriter` interface.\n\n[embedmd]:# (example/reinforce.py python /def _run_episode/ /return reward/)\n```python\ndef _run_episode(env, episode, agent, report):\n    obs = env.reset()\n    done, reward = False, 0\n    while not done:\n        if report.should_render(episode):\n            env.render()\n        obs, r, done, _ = env.step(agent.next_action(obs))\n        agent.signal(r)\n        reward += r\n\n    agent.train()\n    return reward\n```\n\nThis function performs a run through a single episode of the environment. Observations of the environment are passed to the agent's `next_action` interface function. The resulting estimated actions are passed again to the environment, leading to the next observation and a reward signal. The agent is then trained at the end of the episode because we want to train it on whole trajectories. It also contains a call to `env.render()` to visualize some runs. \n\n## Running an Example\nRunning the REINFORCE agent example with default settings:\n```bash\npython example/reinforce.py\n```\n\nAfter a few 1000 episodes it should get very close to the highest achievable reward:\n```\n...\nINFO:__main__:Episode 2800: reward=200.0; mean reward of last 100 episodes: 199.71\nINFO:__main__:Episode 2900: reward=200.0; mean reward of last 100 episodes: 199.36\nINFO:__main__:Episode 3000: reward=200.0; mean reward of last 100 episodes: 198.09\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/SwamyDev/reinforcement", "keywords": "AI reinforcement learning", "license": "", "maintainer": "", "maintainer_email": "", "name": "reinforcement", "package_url": "https://pypi.org/project/reinforcement/", "platform": "", "project_url": "https://pypi.org/project/reinforcement/", "project_urls": {"Bug Reports": "https://github.com/SwamyDev/reinforcement/issues", "Homepage": "https://github.com/SwamyDev/reinforcement", "Source": "https://github.com/SwamyDev/reinforcement/"}, "release_url": "https://pypi.org/project/reinforcement/1.2.0/", "requires_dist": ["numpy (<1.17) ; extra == 'test'", "tensorflow (==1.14) ; extra == 'test'", "pytest ; extra == 'test'", "pytest-rerunfailures ; extra == 'test'", "matplotlib ; extra == 'test'", "gym ; extra == 'test'", "numpy (<1.17) ; extra == 'tf_cpu'", "tensorflow (==1.14) ; extra == 'tf_cpu'", "numpy (<1.17) ; extra == 'tf_gpu'", "tensorflow-gpu (==1.14) ; extra == 'tf_gpu'"], "requires_python": "", "summary": "A reinforcement learning module", "version": "1.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://travis-ci.org/SwamyDev/reinforcement\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ef10071d0a6834b470a158f3a0565c603c33c86a/68747470733a2f2f7472617669732d63692e6f72672f5377616d794465762f7265696e666f7263656d656e742e7376673f6272616e63683d6d6173746572\"></a> <a href=\"https://coveralls.io/github/SwamyDev/reinforcement?branch=master\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/78ec51c8ea3a2639ede2fcf2a8c6be384e1a9a0b/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f5377616d794465762f7265696e666f7263656d656e742f62616467652e7376673f6272616e63683d6d6173746572\"></a> <a href=\"https://badge.fury.io/py/reinforcement\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4c955d75b549e7a6438cc0de89b35468c2d59106/68747470733a2f2f62616467652e667572792e696f2f70792f7265696e666f7263656d656e742e737667\"></a></p>\n<h1>Reinforcement</h1>\n<p>The reinforcement package aims to provide simple implementations for basic reinforcement learning algorithms, using Test Driven Development and other principles of Software Engineering in an attempt to minimize defects and improve reproducibility.</p>\n<h2>Installation</h2>\n<p>The library can be installed using pip:</p>\n<pre>pip install reinforcement\n</pre>\n<h2>Example Implementation</h2>\n<p>This section demonstrates how to implement a REINFORCE agent and benchmark it on the 'CartPole' gym environment.</p>\n<p>You can find the full implementation in <a href=\"example/reinforce.py\" rel=\"nofollow\">examples/reinforce.py</a>. The <a href=\"example/\" rel=\"nofollow\">example folder</a> also contains some additional utility classes and functions that are used in the implementation.</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">run_reinforce</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">):</span>\n    <span class=\"n\">reporter</span><span class=\"p\">,</span> <span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">rewards</span> <span class=\"o\">=</span> <span class=\"n\">Reporter</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">),</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s1\">'CartPole-v0'</span><span class=\"p\">),</span> <span class=\"p\">[]</span>\n    <span class=\"k\">with</span> <span class=\"n\">tf1</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">session</span><span class=\"p\">:</span>\n        <span class=\"n\">agent</span> <span class=\"o\">=</span> <span class=\"n\">_make_agent</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"n\">env</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">episode</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">episodes</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">):</span>\n            <span class=\"n\">reward</span> <span class=\"o\">=</span> <span class=\"n\">_run_episode</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">episode</span><span class=\"p\">,</span> <span class=\"n\">agent</span><span class=\"p\">,</span> <span class=\"n\">reporter</span><span class=\"p\">)</span>\n            <span class=\"n\">rewards</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">reward</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"n\">reporter</span><span class=\"o\">.</span><span class=\"n\">should_log</span><span class=\"p\">(</span><span class=\"n\">episode</span><span class=\"p\">):</span>\n                <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">reporter</span><span class=\"o\">.</span><span class=\"n\">report</span><span class=\"p\">(</span><span class=\"n\">episode</span><span class=\"p\">,</span> <span class=\"n\">rewards</span><span class=\"p\">))</span>\n    <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n</pre>\n<p>This is the main function setting up the boiler plate code. It creates the tensorflow session, logs the progress, and creats the agent. The <code>Reporter</code> class is just a helper to make logging at a certain frequency more convenient</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">_make_agent</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"n\">env</span><span class=\"p\">):</span>\n    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">ParameterizedPolicy</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">observation_space</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">action_space</span><span class=\"o\">.</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">NoLog</span><span class=\"p\">(),</span> <span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">lr_policy</span><span class=\"p\">)</span>\n    <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">ValueBaseline</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">observation_space</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">NoLog</span><span class=\"p\">(),</span> <span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">lr_baseline</span><span class=\"p\">)</span>\n    <span class=\"n\">alg</span> <span class=\"o\">=</span> <span class=\"n\">Reinforce</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">gamma</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">num_trajectories</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">BatchAgent</span><span class=\"p\">(</span><span class=\"n\">alg</span><span class=\"p\">)</span>\n</pre>\n<p>The factory function <code>_make_agent</code> creates the REINFORCE agent object. It uses a parameterized policy and baseline to learn and estimate proper actions. In this case, both parameterizations are straightforward artificial neural networks with no hidden layer. Both have the same input layer, but the output layer of the policy is a softmax function, whereas the baseline outputs a single linear value. The <code>BatchAgent</code> type records trajectories (states, actions, rewards) which are then used to optimize the policy and the baseline. The <code>NoLog</code> class is a Null-Object implementing the TensorBoard <code>FileWriter</code> interface.</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">_run_episode</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">episode</span><span class=\"p\">,</span> <span class=\"n\">agent</span><span class=\"p\">,</span> <span class=\"n\">report</span><span class=\"p\">):</span>\n    <span class=\"n\">obs</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n    <span class=\"n\">done</span><span class=\"p\">,</span> <span class=\"n\">reward</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"mi\">0</span>\n    <span class=\"k\">while</span> <span class=\"ow\">not</span> <span class=\"n\">done</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"n\">report</span><span class=\"o\">.</span><span class=\"n\">should_render</span><span class=\"p\">(</span><span class=\"n\">episode</span><span class=\"p\">):</span>\n            <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">render</span><span class=\"p\">()</span>\n        <span class=\"n\">obs</span><span class=\"p\">,</span> <span class=\"n\">r</span><span class=\"p\">,</span> <span class=\"n\">done</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">next_action</span><span class=\"p\">(</span><span class=\"n\">obs</span><span class=\"p\">))</span>\n        <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">signal</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"p\">)</span>\n        <span class=\"n\">reward</span> <span class=\"o\">+=</span> <span class=\"n\">r</span>\n\n    <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n    <span class=\"k\">return</span> <span class=\"n\">reward</span>\n</pre>\n<p>This function performs a run through a single episode of the environment. Observations of the environment are passed to the agent's <code>next_action</code> interface function. The resulting estimated actions are passed again to the environment, leading to the next observation and a reward signal. The agent is then trained at the end of the episode because we want to train it on whole trajectories. It also contains a call to <code>env.render()</code> to visualize some runs.</p>\n<h2>Running an Example</h2>\n<p>Running the REINFORCE agent example with default settings:</p>\n<pre>python example/reinforce.py\n</pre>\n<p>After a few 1000 episodes it should get very close to the highest achievable reward:</p>\n<pre><code>...\nINFO:__main__:Episode 2800: reward=200.0; mean reward of last 100 episodes: 199.71\nINFO:__main__:Episode 2900: reward=200.0; mean reward of last 100 episodes: 199.36\nINFO:__main__:Episode 3000: reward=200.0; mean reward of last 100 episodes: 198.09\n</code></pre>\n\n          </div>"}, "last_serial": 5905788, "releases": {"1.0.5": [{"comment_text": "", "digests": {"md5": "aa6cc5f3de432a4f687229dad0a13b90", "sha256": "20def6c426a5520fdda32e999fe443440029e581bebe584f50f40cda360e24b5"}, "downloads": -1, "filename": "reinforcement-1.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "aa6cc5f3de432a4f687229dad0a13b90", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 11076, "upload_time": "2018-04-07T16:31:20", "upload_time_iso_8601": "2018-04-07T16:31:20.031163Z", "url": "https://files.pythonhosted.org/packages/96/18/1c638eff2ea9a5afdb89de526b95620079bb6a7a43d7a4799c2e678f97bd/reinforcement-1.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "89ed01a3d5348df1b916e9df5df64b55", "sha256": "610f73516ed6ab9655a836bb27a70e7610212f96dc8a5187c519d6b82639fbaf"}, "downloads": -1, "filename": "reinforcement-1.0.5.tar.gz", "has_sig": false, "md5_digest": "89ed01a3d5348df1b916e9df5df64b55", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7538, "upload_time": "2018-04-07T16:31:20", "upload_time_iso_8601": "2018-04-07T16:31:20.970094Z", "url": "https://files.pythonhosted.org/packages/63/61/19d011b58b503c3e69b360e5437d594c7ce0d3eaa69995bc6ea65534a138/reinforcement-1.0.5.tar.gz", "yanked": false}], "1.0.6": [{"comment_text": "", "digests": {"md5": "dc45dbc4e94bdc74ad9723f22cdc3be0", "sha256": "67dec6b6a212dcfc6d502c386263e88d8a4861de86fd21983303368c93e664fe"}, "downloads": -1, "filename": "reinforcement-1.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "dc45dbc4e94bdc74ad9723f22cdc3be0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 11104, "upload_time": "2018-04-07T16:39:07", "upload_time_iso_8601": "2018-04-07T16:39:07.935922Z", "url": "https://files.pythonhosted.org/packages/ef/ae/aa0d951325e0b04082042307b4dd1bc509350af11b7ff497bb6229627350/reinforcement-1.0.6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "041cb8e4fc3c9adb907d9d5cf85125c3", "sha256": "9a5e00980723275414761027f90c655ef626fa65b2546a19bc5959b6221f1d52"}, "downloads": -1, "filename": "reinforcement-1.0.6.tar.gz", "has_sig": false, "md5_digest": "041cb8e4fc3c9adb907d9d5cf85125c3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7558, "upload_time": "2018-04-07T16:39:08", "upload_time_iso_8601": "2018-04-07T16:39:08.682494Z", "url": "https://files.pythonhosted.org/packages/0c/1c/131c6cf4cae50b3e1b4a49b261520434fa6b9c0616ed7ff5cf2bf56610f6/reinforcement-1.0.6.tar.gz", "yanked": false}], "1.1.2": [{"comment_text": "", "digests": {"md5": "a40c1886a435900bb4c2e01e43286e5f", "sha256": "2279ddf6af0cbe126330a36e2947f32f01876aaa07b9f5a35c8133eefc7272a9"}, "downloads": -1, "filename": "reinforcement-1.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "a40c1886a435900bb4c2e01e43286e5f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 11758, "upload_time": "2019-08-25T09:08:37", "upload_time_iso_8601": "2019-08-25T09:08:37.998309Z", "url": "https://files.pythonhosted.org/packages/6f/c8/0c5c261cafa0791729e62632e40cc72ca8716fe10ccae5a3a18bdb4a58df/reinforcement-1.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "50367a078e0b6723c995df2bc29db408", "sha256": "9e3189184c79a95bde5d642b1915c86fba817bbeed3287edfe88237a46fc2470"}, "downloads": -1, "filename": "reinforcement-1.1.2.tar.gz", "has_sig": false, "md5_digest": "50367a078e0b6723c995df2bc29db408", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7829, "upload_time": "2019-08-25T09:08:39", "upload_time_iso_8601": "2019-08-25T09:08:39.060922Z", "url": "https://files.pythonhosted.org/packages/1b/59/d205e8e15a3416f3201cba152f308047ed7723031c17970eea5536ff848e/reinforcement-1.1.2.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "0bde5b15d09e604eec05bb4d4669133a", "sha256": "f21e299b5467a5d73f7293d0c72ea75648671e3590bb284ff69699a96d41ecdc"}, "downloads": -1, "filename": "reinforcement-1.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0bde5b15d09e604eec05bb4d4669133a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23563, "upload_time": "2019-09-30T09:11:59", "upload_time_iso_8601": "2019-09-30T09:11:59.078783Z", "url": "https://files.pythonhosted.org/packages/8c/2d/dec9f01534257eb7edde9c51c44a62dfa329de5875c8aec965863ae0e2ec/reinforcement-1.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d5d627ca6caa55d0aa6c6b1d2f4a2698", "sha256": "ed2c9ff0a8b6f902abaa3bb39f589d0a8c17995cca0a01bf7a77f57497fcdbc8"}, "downloads": -1, "filename": "reinforcement-1.2.0.tar.gz", "has_sig": false, "md5_digest": "d5d627ca6caa55d0aa6c6b1d2f4a2698", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14410, "upload_time": "2019-09-30T09:12:01", "upload_time_iso_8601": "2019-09-30T09:12:01.047747Z", "url": "https://files.pythonhosted.org/packages/2d/00/9799ba20fb1276da5c5c3e1308f4ea2e33be95030868100c6dbc282dba44/reinforcement-1.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "0bde5b15d09e604eec05bb4d4669133a", "sha256": "f21e299b5467a5d73f7293d0c72ea75648671e3590bb284ff69699a96d41ecdc"}, "downloads": -1, "filename": "reinforcement-1.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0bde5b15d09e604eec05bb4d4669133a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23563, "upload_time": "2019-09-30T09:11:59", "upload_time_iso_8601": "2019-09-30T09:11:59.078783Z", "url": "https://files.pythonhosted.org/packages/8c/2d/dec9f01534257eb7edde9c51c44a62dfa329de5875c8aec965863ae0e2ec/reinforcement-1.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d5d627ca6caa55d0aa6c6b1d2f4a2698", "sha256": "ed2c9ff0a8b6f902abaa3bb39f589d0a8c17995cca0a01bf7a77f57497fcdbc8"}, "downloads": -1, "filename": "reinforcement-1.2.0.tar.gz", "has_sig": false, "md5_digest": "d5d627ca6caa55d0aa6c6b1d2f4a2698", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14410, "upload_time": "2019-09-30T09:12:01", "upload_time_iso_8601": "2019-09-30T09:12:01.047747Z", "url": "https://files.pythonhosted.org/packages/2d/00/9799ba20fb1276da5c5c3e1308f4ea2e33be95030868100c6dbc282dba44/reinforcement-1.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:05:07 2020"}