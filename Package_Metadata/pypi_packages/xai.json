{"info": {"author": "Alejandro Saucedo", "author_email": "a@ethical.institute", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Natural Language :: English", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "![GitHub](https://img.shields.io/badge/Release-ALPHA-yellow.svg)\n![GitHub](https://img.shields.io/badge/Version-0.0.4_ALPHA-lightgrey.svg)\n![GitHub](https://img.shields.io/badge/Python-3.5_|_3.6-blue.svg)\n![GitHub](https://img.shields.io/badge/License-MIT-lightgrey.svg)\n\n# XAI - An eXplainability toolbox for machine learning \n\nXAI is a Machine Learning library that is designed with AI explainability in its core. XAI contains various tools that enable for analysis and evaluation of data and models. The XAI library is maintained by [The Institute for Ethical AI & ML](http://ethical.institute/), and it was developed based on the [8 principles for Responsible Machine Learning](http://ethical.institute/principles.html).\n\nYou can find the documentation at [https://ethicalml.github.io/xai/index.html](https://ethicalml.github.io/xai/index.html). You can also check out our [talk at Tensorflow London](https://www.youtube.com/watch?v=GZpfBhQJ0H4) where the idea was first conceived - the talk also contains an insight on the definitions and principles in this library.\n\n## YouTube video showing how to use XAI to mitigate undesired biases\n\n<table>\n  <tr>\n    <td width=\"30%\">\n        This <a href=\"https://www.youtube.com/watch?v=rq95qznOZKw\">video of the talk presented at the PyCon By 2019 Conference </a> provides an overview on the motivations for machine learning explainability as well as techniques to introduce explainability and mitigate undesired biases using the XAI Library.\n    </td>\n    <td width=\"70%\">\n        <a href=\"https://www.youtube.com/watch?v=rq95qznOZKw\"><img src=\"images/video.jpg\"></a>\n    </td>\n  </tr>\n  <tr>\n    <td width=\"30%\">\n        Do you want to learn about more awesome machine learning explainability tools? Check out our community-built <a href=\"https://github.com/EthicalML/awesome-machine-learning-operations\">\"Awesome Machine Learning Production & Operations\"</a> list which contains an extensive list of tools for explainability, privacy, orchestration and beyond.\n    </td>\n    <td width=\"70%\">\n        <a href=\"https://github.com/EthicalML/awesome-machine-learning-operations\"><img src=\"images/mlops-link.png\"></a>\n    </td>\n  </tr>\n\n</table>\n\n# 0.0.4 - ALPHA Version\n\nThis library is currently in early stage developments and hence it will be quite unstable due to the fast updates. It is important to bare this in mind if using it in production. \n\nIf you want to see a fully functional demo in action clone this repo and run the <a href=\"https://github.com/EthicalML/xai/blob/master/examples/XAI%20Example%20Usage.ipynb\">Example Jupyter Notebook in the Examples folder</a>.\n\n## What do we mean by eXplainable AI?\n\nWe see the challenge of explainability as more than just an algorithmic challenge, which requires a combination of data science best practices with domain-specific knowledge. The XAI library is designed to empower machine learning engineers and relevant domain experts to analyse the end-to-end solution and identify discrepancies that may result in sub-optimal performance relative to the objectives required. More broadly, the XAI library is designed using the 3-steps of explainable machine learning, which involve 1) data analysis, 2) model evaluation, and 3) production monitoring. \n\nWe provide a visual overview of these three steps mentioned above in this diagram:\n\n<img width=\"100%\" src=\"images/bias.png\">\n\n# XAI Quickstart\n\n## Installation\n\nThe XAI package is on PyPI. To install you can run:\n\n```\npip install xai\n```\n\nAlternatively you can install from source by cloning the repo and running:\n\n```\npython setup.py install \n```\n\n## Usage\n\nYou can find example usage in the examples folder.\n\n### 1) Data Analysis\n\nWith XAI you can identify imbalances in the data. For this, we will load the census dataset from the XAI library.\n\n``` python\nimport xai.data\ndf = xai.data.load_census()\ndf.head()\n```\n<img width=\"100%\" src=\"images/readme-csv-head.jpg\">\n\n#### View class imbalances for all categories of one column\n``` python\nims = xai.imbalance_plot(df, \"gender\")\n```\n<img width=\"100%\" src=\"images/readme-imbalance-gender.jpg\">\n\n#### View imbalances for all categories across multiple columns\n``` python\nim = xai.show_imbalance(df, \"gender\", \"loan\")\n```\n<img width=\"100%\" src=\"images/readme-imbalance-multiple.jpg\">\n\n#### Balance classes using upsampling and/or downsampling\n``` python\nbal_df = xai.balance(df, \"gender\", \"loan\", upsample=0.8)\n```\n<img width=\"100%\" src=\"images/readme-balance-upsample.jpg\">\n\n#### Perform custom operations on groups\n``` python\ngroups = xai.group_by_columns(df, [\"gender\", \"loan\"])\nfor group, group_df in groups:    \n    print(group) \n    print(group_df[\"loan\"].head(), \"\\n\")\n```\n<img width=\"100%\" src=\"images/readme-groups.jpg\">\n\n#### Visualise correlations as a matrix\n``` python\n_ = xai.correlations(df, include_categorical=True, plot_type=\"matrix\")\n```\n<img width=\"100%\" src=\"images/readme-correlation-matrix.jpg\">\n\n#### Visualise correlations as a hierarchical dendogram\n``` python\n_ = xai.correlations(df, include_categorical=True)\n```\n<img width=\"100%\" src=\"images/readme-correlation-dendogram.jpg\">\n\n#### Create a balanced validation and training split dataset\n``` python\n# Balanced train-test split with minimum 300 examples of \n#     the cross of the target y and the column gender\nx_train, y_train, x_test, y_test, train_idx, test_idx = \\\n    xai.balanced_train_test_split(\n            x, y, \"gender\", \n            min_per_group=300,\n            max_per_group=300,\n            categorical_cols=categorical_cols)\n\nx_train_display = bal_df[train_idx]\nx_test_display = bal_df[test_idx]\n\nprint(\"Total number of examples: \", x_test.shape[0])\n\ndf_test = x_test_display.copy()\ndf_test[\"loan\"] = y_test\n\n_= xai.imbalance_plot(df_test, \"gender\", \"loan\", categorical_cols=categorical_cols)\n```\n<img width=\"100%\" src=\"images/readme-balance-split.jpg\">\n\n### 2) Model Evaluation\n\nWe are able to also analyse the interaction between inference results and input features. For this, we will train a single layer deep learning model.\n\n```\nmodel = build_model(proc_df.drop(\"loan\", axis=1))\n\nmodel.fit(f_in(x_train), y_train, epochs=50, batch_size=512)\n\nprobabilities = model.predict(f_in(x_test))\npredictions = list((probabilities >= 0.5).astype(int).T[0])\n```\n<img width=\"100%\" src=\"images/readme-15.png\">\n\n#### Visualise permutation feature importance\n``` python\ndef get_avg(x, y):\n    return model.evaluate(f_in(x), y, verbose=0)[1]\n\nimp = xai.feature_importance(x_test, y_test, get_avg)\n\nimp.head()\n```\n<img width=\"100%\" src=\"images/readme-6.png\">\n\n#### Identify metric imbalances against all test data\n``` python\n_= xai.metrics_plot(\n        y_test, \n        probabilities)\n```\n<img width=\"100%\" src=\"images/readme-metrics-plot.jpg\">\n\n#### Identify metric imbalances across a specific column\n``` python\n_ = xai.metrics_plot(\n    y_test, \n    probabilities, \n    df=x_test_display, \n    cross_cols=[\"gender\"],\n    categorical_cols=categorical_cols)\n```\n<img width=\"100%\" src=\"images/readme-metrics-column.jpg\">\n\n#### Identify metric imbalances across multiple columns\n``` python\n_ = xai.metrics_plot(\n    y_test, \n    probabilities, \n    df=x_test_display, \n    cross_cols=[\"gender\", \"ethnicity\"],\n    categorical_cols=categorical_cols)\n```\n<img width=\"100%\" src=\"images/readme-metrics-multiple.jpg\">\n\n#### Draw confusion matrix\n``` python\nxai.confusion_matrix_plot(y_test, pred)\n```\n<img width=\"100%\" src=\"images/readme-confusion-matrix.jpg\">\n\n#### Visualise the ROC curve against all test data\n``` python\n_ = xai.roc_plot(y_test, probabilities)\n```\n<img width=\"100%\" src=\"images/readme-9.png\">\n\n#### Visualise the ROC curves grouped by a protected column\n``` python\nprotected = [\"gender\", \"ethnicity\", \"age\"]\n_ = [xai.roc_plot(\n    y_test, \n    probabilities, \n    df=x_test_display, \n    cross_cols=[p],\n    categorical_cols=categorical_cols) for p in protected]\n```\n<img width=\"100%\" src=\"images/readme-10.png\">\n\n#### Visualise accuracy grouped by probability buckets\n``` python\nd = xai.smile_imbalance(\n    y_test, \n    probabilities)\n```\n<img width=\"100%\" src=\"images/readme-12.png\">\n\n#### Visualise statistical metrics grouped by probability buckets\n``` python\nd = xai.smile_imbalance(\n    y_test, \n    probabilities,\n    display_breakdown=True)\n```\n<img width=\"100%\" src=\"images/readme-13.png\">\n\n#### Visualise benefits of adding manual review on probability thresholds\n``` python\nd = xai.smile_imbalance(\n    y_test, \n    probabilities,\n    bins=9,\n    threshold=0.75,\n    manual_review=0.375,\n    display_breakdown=False)\n```\n<img width=\"100%\" src=\"images/readme-14.png\">\n\n\n\n\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/EthicalML/XAI", "keywords": "xai,machine learning,deep learning,explainability,bias evaluation", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "xai", "package_url": "https://pypi.org/project/xai/", "platform": "", "project_url": "https://pypi.org/project/xai/", "project_urls": {"Homepage": "https://github.com/EthicalML/XAI"}, "release_url": "https://pypi.org/project/xai/0.0.5/", "requires_dist": ["pandas (==0.23.4)", "matplotlib (==3.0.2)", "numpy (==1.15.4)", "scipy (==1.1.0)", "scikit-learn (==0.20.1)", "python-dateutil (==2.7.5)", "pytz (==2018.7)", "pyparsing (==2.3.0)", "cycler (==0.10.0)", "kiwisolver (==1.0.1)", "six (==1.12.0)"], "requires_python": "", "summary": "XAI - An industry-ready machine learning library that ensures explainable AI by design", "version": "0.0.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            ![GitHub](https://img.shields.io/badge/Release-ALPHA-yellow.svg)<br>![GitHub](https://img.shields.io/badge/Version-0.0.4_ALPHA-lightgrey.svg)<br>![GitHub](https://img.shields.io/badge/Python-3.5_|_3.6-blue.svg)<br>![GitHub](https://img.shields.io/badge/License-MIT-lightgrey.svg)<br><br># XAI - An eXplainability toolbox for machine learning <br><br>XAI is a Machine Learning library that is designed with AI explainability in its core. XAI contains various tools that enable for analysis and evaluation of data and models. The XAI library is maintained by [The Institute for Ethical AI &amp; ML](http://ethical.institute/), and it was developed based on the [8 principles for Responsible Machine Learning](http://ethical.institute/principles.html).<br><br>You can find the documentation at [https://ethicalml.github.io/xai/index.html](https://ethicalml.github.io/xai/index.html). You can also check out our [talk at Tensorflow London](https://www.youtube.com/watch?v=GZpfBhQJ0H4) where the idea was first conceived - the talk also contains an insight on the definitions and principles in this library.<br><br>## YouTube video showing how to use XAI to mitigate undesired biases<br><br>&lt;table&gt;<br>  &lt;tr&gt;<br>    &lt;td width=\"30%\"&gt;<br>        This &lt;a href=\"https://www.youtube.com/watch?v=rq95qznOZKw\"&gt;video of the talk presented at the PyCon By 2019 Conference &lt;/a&gt; provides an overview on the motivations for machine learning explainability as well as techniques to introduce explainability and mitigate undesired biases using the XAI Library.<br>    &lt;/td&gt;<br>    &lt;td width=\"70%\"&gt;<br>        &lt;a href=\"https://www.youtube.com/watch?v=rq95qznOZKw\"&gt;&lt;img src=\"images/video.jpg\"&gt;&lt;/a&gt;<br>    &lt;/td&gt;<br>  &lt;/tr&gt;<br>  &lt;tr&gt;<br>    &lt;td width=\"30%\"&gt;<br>        Do you want to learn about more awesome machine learning explainability tools? Check out our community-built &lt;a href=\"https://github.com/EthicalML/awesome-machine-learning-operations\"&gt;\"Awesome Machine Learning Production &amp; Operations\"&lt;/a&gt; list which contains an extensive list of tools for explainability, privacy, orchestration and beyond.<br>    &lt;/td&gt;<br>    &lt;td width=\"70%\"&gt;<br>        &lt;a href=\"https://github.com/EthicalML/awesome-machine-learning-operations\"&gt;&lt;img src=\"images/mlops-link.png\"&gt;&lt;/a&gt;<br>    &lt;/td&gt;<br>  &lt;/tr&gt;<br><br>&lt;/table&gt;<br><br># 0.0.4 - ALPHA Version<br><br>This library is currently in early stage developments and hence it will be quite unstable due to the fast updates. It is important to bare this in mind if using it in production. <br><br>If you want to see a fully functional demo in action clone this repo and run the &lt;a href=\"https://github.com/EthicalML/xai/blob/master/examples/XAI%20Example%20Usage.ipynb\"&gt;Example Jupyter Notebook in the Examples folder&lt;/a&gt;.<br><br>## What do we mean by eXplainable AI?<br><br>We see the challenge of explainability as more than just an algorithmic challenge, which requires a combination of data science best practices with domain-specific knowledge. The XAI library is designed to empower machine learning engineers and relevant domain experts to analyse the end-to-end solution and identify discrepancies that may result in sub-optimal performance relative to the objectives required. More broadly, the XAI library is designed using the 3-steps of explainable machine learning, which involve 1) data analysis, 2) model evaluation, and 3) production monitoring. <br><br>We provide a visual overview of these three steps mentioned above in this diagram:<br><br>&lt;img width=\"100%\" src=\"images/bias.png\"&gt;<br><br># XAI Quickstart<br><br>## Installation<br><br>The XAI package is on PyPI. To install you can run:<br><br>```<br>pip install xai<br>```<br><br>Alternatively you can install from source by cloning the repo and running:<br><br>```<br>python setup.py install <br>```<br><br>## Usage<br><br>You can find example usage in the examples folder.<br><br>### 1) Data Analysis<br><br>With XAI you can identify imbalances in the data. For this, we will load the census dataset from the XAI library.<br><br>``` python<br>import xai.data<br>df = xai.data.load_census()<br>df.head()<br>```<br>&lt;img width=\"100%\" src=\"images/readme-csv-head.jpg\"&gt;<br><br>#### View class imbalances for all categories of one column<br>``` python<br>ims = xai.imbalance_plot(df, \"gender\")<br>```<br>&lt;img width=\"100%\" src=\"images/readme-imbalance-gender.jpg\"&gt;<br><br>#### View imbalances for all categories across multiple columns<br>``` python<br>im = xai.show_imbalance(df, \"gender\", \"loan\")<br>```<br>&lt;img width=\"100%\" src=\"images/readme-imbalance-multiple.jpg\"&gt;<br><br>#### Balance classes using upsampling and/or downsampling<br>``` python<br>bal_df = xai.balance(df, \"gender\", \"loan\", upsample=0.8)<br>```<br>&lt;img width=\"100%\" src=\"images/readme-balance-upsample.jpg\"&gt;<br><br>#### Perform custom operations on groups<br>``` python<br>groups = xai.group_by_columns(df, [\"gender\", \"loan\"])<br>for group, group_df in groups:    <br>    print(group) <br>    print(group_df[\"loan\"].head(), \"\\n\")<br>```<br>&lt;img width=\"100%\" src=\"images/readme-groups.jpg\"&gt;<br><br>#### Visualise correlations as a matrix<br>``` python<br>_ = xai.correlations(df, include_categorical=True, plot_type=\"matrix\")<br>```<br>&lt;img width=\"100%\" src=\"images/readme-correlation-matrix.jpg\"&gt;<br><br>#### Visualise correlations as a hierarchical dendogram<br>``` python<br>_ = xai.correlations(df, include_categorical=True)<br>```<br>&lt;img width=\"100%\" src=\"images/readme-correlation-dendogram.jpg\"&gt;<br><br>#### Create a balanced validation and training split dataset<br>``` python<br># Balanced train-test split with minimum 300 examples of <br>#     the cross of the target y and the column gender<br>x_train, y_train, x_test, y_test, train_idx, test_idx = \\<br>    xai.balanced_train_test_split(<br>            x, y, \"gender\", <br>            min_per_group=300,<br>            max_per_group=300,<br>            categorical_cols=categorical_cols)<br><br>x_train_display = bal_df[train_idx]<br>x_test_display = bal_df[test_idx]<br><br>print(\"Total number of examples: \", x_test.shape[0])<br><br>df_test = x_test_display.copy()<br>df_test[\"loan\"] = y_test<br><br>_= xai.imbalance_plot(df_test, \"gender\", \"loan\", categorical_cols=categorical_cols)<br>```<br>&lt;img width=\"100%\" src=\"images/readme-balance-split.jpg\"&gt;<br><br>### 2) Model Evaluation<br><br>We are able to also analyse the interaction between inference results and input features. For this, we will train a single layer deep learning model.<br><br>```<br>model = build_model(proc_df.drop(\"loan\", axis=1))<br><br>model.fit(f_in(x_train), y_train, epochs=50, batch_size=512)<br><br>probabilities = model.predict(f_in(x_test))<br>predictions = list((probabilities &gt;= 0.5).astype(int).T[0])<br>```<br>&lt;img width=\"100%\" src=\"images/readme-15.png\"&gt;<br><br>#### Visualise permutation feature importance<br>``` python<br>def get_avg(x, y):<br>    return model.evaluate(f_in(x), y, verbose=0)[1]<br><br>imp = xai.feature_importance(x_test, y_test, get_avg)<br><br>imp.head()<br>```<br>&lt;img width=\"100%\" src=\"images/readme-6.png\"&gt;<br><br>#### Identify metric imbalances against all test data<br>``` python<br>_= xai.metrics_plot(<br>        y_test, <br>        probabilities)<br>```<br>&lt;img width=\"100%\" src=\"images/readme-metrics-plot.jpg\"&gt;<br><br>#### Identify metric imbalances across a specific column<br>``` python<br>_ = xai.metrics_plot(<br>    y_test, <br>    probabilities, <br>    df=x_test_display, <br>    cross_cols=[\"gender\"],<br>    categorical_cols=categorical_cols)<br>```<br>&lt;img width=\"100%\" src=\"images/readme-metrics-column.jpg\"&gt;<br><br>#### Identify metric imbalances across multiple columns<br>``` python<br>_ = xai.metrics_plot(<br>    y_test, <br>    probabilities, <br>    df=x_test_display, <br>    cross_cols=[\"gender\", \"ethnicity\"],<br>    categorical_cols=categorical_cols)<br>```<br>&lt;img width=\"100%\" src=\"images/readme-metrics-multiple.jpg\"&gt;<br><br>#### Draw confusion matrix<br>``` python<br>xai.confusion_matrix_plot(y_test, pred)<br>```<br>&lt;img width=\"100%\" src=\"images/readme-confusion-matrix.jpg\"&gt;<br><br>#### Visualise the ROC curve against all test data<br>``` python<br>_ = xai.roc_plot(y_test, probabilities)<br>```<br>&lt;img width=\"100%\" src=\"images/readme-9.png\"&gt;<br><br>#### Visualise the ROC curves grouped by a protected column<br>``` python<br>protected = [\"gender\", \"ethnicity\", \"age\"]<br>_ = [xai.roc_plot(<br>    y_test, <br>    probabilities, <br>    df=x_test_display, <br>    cross_cols=[p],<br>    categorical_cols=categorical_cols) for p in protected]<br>```<br>&lt;img width=\"100%\" src=\"images/readme-10.png\"&gt;<br><br>#### Visualise accuracy grouped by probability buckets<br>``` python<br>d = xai.smile_imbalance(<br>    y_test, <br>    probabilities)<br>```<br>&lt;img width=\"100%\" src=\"images/readme-12.png\"&gt;<br><br>#### Visualise statistical metrics grouped by probability buckets<br>``` python<br>d = xai.smile_imbalance(<br>    y_test, <br>    probabilities,<br>    display_breakdown=True)<br>```<br>&lt;img width=\"100%\" src=\"images/readme-13.png\"&gt;<br><br>#### Visualise benefits of adding manual review on probability thresholds<br>``` python<br>d = xai.smile_imbalance(<br>    y_test, <br>    probabilities,<br>    bins=9,<br>    threshold=0.75,<br>    manual_review=0.375,<br>    display_breakdown=False)<br>```<br>&lt;img width=\"100%\" src=\"images/readme-14.png\"&gt;<br><br><br><br><br><br><br>\n          </div>"}, "last_serial": 5170002, "releases": {"0.0.2": [{"comment_text": "", "digests": {"md5": "9deda062e026c4e8f4843acba37196b5", "sha256": "67a1acbb2b0083d20aaaab5b700b741dbfd184df6c85ec47d1cfdb8ce399e2f9"}, "downloads": -1, "filename": "xai-0.0.2.tar.gz", "has_sig": false, "md5_digest": "9deda062e026c4e8f4843acba37196b5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15583, "upload_time": "2019-01-12T20:46:33", "upload_time_iso_8601": "2019-01-12T20:46:33.310055Z", "url": "https://files.pythonhosted.org/packages/21/c3/afdac2eedfdc8180afee97a40e284ad48a8bdaea7b571ba281a3d98000ac/xai-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "3bb31ff27e0ed1576fedd796dd52cfa6", "sha256": "c2034db02c1f22527d6f0b66088c1e1f74ec8380e81885c6cb86c2a522d5304b"}, "downloads": -1, "filename": "xai-0.0.3.tar.gz", "has_sig": false, "md5_digest": "3bb31ff27e0ed1576fedd796dd52cfa6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15939, "upload_time": "2019-01-13T11:35:45", "upload_time_iso_8601": "2019-01-13T11:35:45.287518Z", "url": "https://files.pythonhosted.org/packages/9f/ba/c2c562b92bc4459b4847f42156ad995d0d279a0f7a401f875d75c0b95c90/xai-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "71dc8fd32dfe032a251468ce76025c45", "sha256": "51c35836fd7802a3d39e329dfbe3cf4b31fd78243fc6242ef8c887d0fc949852"}, "downloads": -1, "filename": "xai-0.0.4.tar.gz", "has_sig": false, "md5_digest": "71dc8fd32dfe032a251468ce76025c45", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 346255, "upload_time": "2019-01-13T21:24:01", "upload_time_iso_8601": "2019-01-13T21:24:01.955049Z", "url": "https://files.pythonhosted.org/packages/d3/d0/2942cb46a395a76ed95d97346c1922afb9b62ea372ba3dd03420957e13ea/xai-0.0.4.tar.gz", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "cf3a2c2018f697e341ee717e1904b93b", "sha256": "27b62ee90d1b6408e7a9e17fda304afc1af2cc822a05e1c2371d6c3db90b4682"}, "downloads": -1, "filename": "xai-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "cf3a2c2018f697e341ee717e1904b93b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 349130, "upload_time": "2019-04-21T14:53:12", "upload_time_iso_8601": "2019-04-21T14:53:12.247009Z", "url": "https://files.pythonhosted.org/packages/57/ff/cb5a2d94f4ce159f7abadc92c581749d6f45b027aaf4cc0823e948064b98/xai-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "df827dfccb4e6e2f7566a895bed82297", "sha256": "8b88864613065f0a1206ab171117cb628af1afdb12eb431668b8be887e061b44"}, "downloads": -1, "filename": "xai-0.0.5.tar.gz", "has_sig": false, "md5_digest": "df827dfccb4e6e2f7566a895bed82297", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1069236, "upload_time": "2019-04-21T14:53:14", "upload_time_iso_8601": "2019-04-21T14:53:14.543711Z", "url": "https://files.pythonhosted.org/packages/68/4e/11a09002c161072a7f24decc402f704e8f06dc89a39eacf7f55d52bb9209/xai-0.0.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "cf3a2c2018f697e341ee717e1904b93b", "sha256": "27b62ee90d1b6408e7a9e17fda304afc1af2cc822a05e1c2371d6c3db90b4682"}, "downloads": -1, "filename": "xai-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "cf3a2c2018f697e341ee717e1904b93b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 349130, "upload_time": "2019-04-21T14:53:12", "upload_time_iso_8601": "2019-04-21T14:53:12.247009Z", "url": "https://files.pythonhosted.org/packages/57/ff/cb5a2d94f4ce159f7abadc92c581749d6f45b027aaf4cc0823e948064b98/xai-0.0.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "df827dfccb4e6e2f7566a895bed82297", "sha256": "8b88864613065f0a1206ab171117cb628af1afdb12eb431668b8be887e061b44"}, "downloads": -1, "filename": "xai-0.0.5.tar.gz", "has_sig": false, "md5_digest": "df827dfccb4e6e2f7566a895bed82297", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1069236, "upload_time": "2019-04-21T14:53:14", "upload_time_iso_8601": "2019-04-21T14:53:14.543711Z", "url": "https://files.pythonhosted.org/packages/68/4e/11a09002c161072a7f24decc402f704e8f06dc89a39eacf7f55d52bb9209/xai-0.0.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:26:28 2020"}