{"info": {"author": "Peter Inglesby", "author_email": "peter.inglesby@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "http-crawler\n============\n\nhttp-crawler is a library for crawling websites.  It uses requests_ to speak HTTP.\n\n\nInstallation\n~~~~~~~~~~~~\n\nInstall with pip_:\n\n.. code-block:: console\n\n    $ pip install http-crawler\n\n\nUsage\n~~~~~\n\nThe ``http_crawler`` module provides one generator function, ``crawl``.\n\n``crawl`` is called with a URL, and yields instances of requests_'s |Response|_ class.\n\n``crawl`` will request the page at the given URL, and will extract all URLs from the response.  It will then make a request for each of those URLs, and will repeat the process until it has requested every URL linked to from pages on the original URL's domain.  It will not extract or process URLs from any page with a different domain to the original URL.\n\nFor instance, this is how you would use ``crawl`` to find and log any broken links on a site:\n\n.. code-block:: pycon\n\n    >>> from http_crawler import crawl\n    >>> for rsp in crawl('http://www.example.com'):\n    >>>     if rsp.status_code != 200:\n    >>>         print('Got {} at {}'.format(rsp.status_code, rsp.url))\n\n\n``crawl`` has a number of options:\n\n- ``follow_external_links`` (default ``True``)  If set, ``crawl`` will make a request for every URL it encounters, including ones with a different domain to the original URL.  If not set, ``crawl`` will ignore all URLs that have a different domain to the original URL.  In either case, ``crawl`` will not extract further URLs from a page with a different domain to the original URL.\n\n- ``ignore_fragments`` (default ``True``)  If set, ``crawl`` will ignore the fragment part of any URL.  This means that if ``crawl`` encounters ``http://domain/path#anchor``, it will make a request for ``http://domain/path``.  Moreover, it means that if ``crawl`` encounters ``http://domain/path#anchor1`` and ``http://domain/path#anchor2``, it will only make one request.\n\n- ``verify`` (default ``True``)  This option controls the behaviour of SSL certificate verification.  See the `requests documentation`_ for more details.\n\n\nMotivation\n~~~~~~~~~~\n\nWhy another crawling library?  There are certainly lots of Python tools for crawling websites, but all that I could find were either too complex, too simple, or had too many dependencies.\n\nhttp-crawler is designed to be a `library and not a framework`_, so it should be straightforward to use in applications or other libraries.\n\n\nContributing\n~~~~~~~~~~~~\n\nThere are a handful of enhancements on the `issue tracker`_ that would be suitable for somebody looking to contribute to Open Source for the first time.\n\nFor instructions about making Pull Requests, see `GitHub's guide`_.\n\nAll contributions should include tests with 100% code coverage, and should comply with `PEP 8`_.  The project uses tox_ for running tests and checking code quality metrics.\n\nTo run the tests:\n\n.. code-block:: console\n\n    $ tox\n\n\n.. _requests: http://docs.python-requests.org/en/master/\n.. _pip: https://pip.pypa.io/en/stable/\n.. |Response| replace:: ``Response``\n.. _Response: http://docs.python-requests.org/en/master/api/#requests.Response\n.. _`library and not a framework`: http://tomasp.net/blog/2015/library-frameworks/\n.. _`issue tracker`: https://github.com/inglesp/http-crawler/issues\n.. _`GitHub's guide`: https://help.github.com/articles/using-pull-requests/\n.. _`PEP 8`: https://www.python.org/dev/peps/pep-0008/\n.. _tox: https://tox.readthedocs.io/en/latest/\n.. _`requests documentation`: http://docs.python-requests.org/en/master/user/advanced/#ssl-cert-verification", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/inglesp/http-crawler", "keywords": "", "license": "License :: OSI Approved :: MIT License", "maintainer": "", "maintainer_email": "", "name": "http-crawler", "package_url": "https://pypi.org/project/http-crawler/", "platform": "", "project_url": "https://pypi.org/project/http-crawler/", "project_urls": {"Homepage": "http://github.com/inglesp/http-crawler"}, "release_url": "https://pypi.org/project/http-crawler/0.2.1/", "requires_dist": null, "requires_python": "", "summary": "A library for crawling websites", "version": "0.2.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>http-crawler is a library for crawling websites.  It uses <a href=\"http://docs.python-requests.org/en/master/\" rel=\"nofollow\">requests</a> to speak HTTP.</p>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>Install with <a href=\"https://pip.pypa.io/en/stable/\" rel=\"nofollow\">pip</a>:</p>\n<pre><span class=\"gp\">$</span> pip install http-crawler\n</pre>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>The <tt>http_crawler</tt> module provides one generator function, <tt>crawl</tt>.</p>\n<p><tt>crawl</tt> is called with a URL, and yields instances of <a href=\"http://docs.python-requests.org/en/master/\" rel=\"nofollow\">requests</a>\u2019s <a href=\"http://docs.python-requests.org/en/master/api/#requests.Response\" rel=\"nofollow\"><tt>Response</tt></a> class.</p>\n<p><tt>crawl</tt> will request the page at the given URL, and will extract all URLs from the response.  It will then make a request for each of those URLs, and will repeat the process until it has requested every URL linked to from pages on the original URL\u2019s domain.  It will not extract or process URLs from any page with a different domain to the original URL.</p>\n<p>For instance, this is how you would use <tt>crawl</tt> to find and log any broken links on a site:</p>\n<pre><span class=\"kn\"></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">http_crawler</span> <span class=\"kn\">import</span> <span class=\"n\">crawl</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">for</span> <span class=\"n\">rsp</span> <span class=\"ow\">in</span> <span class=\"n\">crawl</span><span class=\"p\">(</span><span class=\"s1\">'http://www.example.com'</span><span class=\"p\">):</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>    <span class=\"k\">if</span> <span class=\"n\">rsp</span><span class=\"o\">.</span><span class=\"n\">status_code</span> <span class=\"o\">!=</span> <span class=\"mi\">200</span><span class=\"p\">:</span>\n<span class=\"gp\">&gt;&gt;&gt; </span>        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">'Got </span><span class=\"si\">{}</span><span class=\"s1\"> at </span><span class=\"si\">{}</span><span class=\"s1\">'</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">rsp</span><span class=\"o\">.</span><span class=\"n\">status_code</span><span class=\"p\">,</span> <span class=\"n\">rsp</span><span class=\"o\">.</span><span class=\"n\">url</span><span class=\"p\">))</span>\n</pre>\n<p><tt>crawl</tt> has a number of options:</p>\n<ul>\n<li><tt>follow_external_links</tt> (default <tt>True</tt>)  If set, <tt>crawl</tt> will make a request for every URL it encounters, including ones with a different domain to the original URL.  If not set, <tt>crawl</tt> will ignore all URLs that have a different domain to the original URL.  In either case, <tt>crawl</tt> will not extract further URLs from a page with a different domain to the original URL.</li>\n<li><tt>ignore_fragments</tt> (default <tt>True</tt>)  If set, <tt>crawl</tt> will ignore the fragment part of any URL.  This means that if <tt>crawl</tt> encounters <tt><span class=\"pre\">http://domain/path#anchor</span></tt>, it will make a request for <tt><span class=\"pre\">http://domain/path</span></tt>.  Moreover, it means that if <tt>crawl</tt> encounters <tt><span class=\"pre\">http://domain/path#anchor1</span></tt> and <tt><span class=\"pre\">http://domain/path#anchor2</span></tt>, it will only make one request.</li>\n<li><tt>verify</tt> (default <tt>True</tt>)  This option controls the behaviour of SSL certificate verification.  See the <a href=\"http://docs.python-requests.org/en/master/user/advanced/#ssl-cert-verification\" rel=\"nofollow\">requests documentation</a> for more details.</li>\n</ul>\n</div>\n<div id=\"motivation\">\n<h2>Motivation</h2>\n<p>Why another crawling library?  There are certainly lots of Python tools for crawling websites, but all that I could find were either too complex, too simple, or had too many dependencies.</p>\n<p>http-crawler is designed to be a <a href=\"http://tomasp.net/blog/2015/library-frameworks/\" rel=\"nofollow\">library and not a framework</a>, so it should be straightforward to use in applications or other libraries.</p>\n</div>\n<div id=\"contributing\">\n<h2>Contributing</h2>\n<p>There are a handful of enhancements on the <a href=\"https://github.com/inglesp/http-crawler/issues\" rel=\"nofollow\">issue tracker</a> that would be suitable for somebody looking to contribute to Open Source for the first time.</p>\n<p>For instructions about making Pull Requests, see <a href=\"https://help.github.com/articles/using-pull-requests/\" rel=\"nofollow\">GitHub\u2019s guide</a>.</p>\n<p>All contributions should include tests with 100% code coverage, and should comply with <a href=\"https://www.python.org/dev/peps/pep-0008/\" rel=\"nofollow\">PEP 8</a>.  The project uses <a href=\"https://tox.readthedocs.io/en/latest/\" rel=\"nofollow\">tox</a> for running tests and checking code quality metrics.</p>\n<p>To run the tests:</p>\n<pre><span class=\"gp\">$</span> tox\n</pre>\n</div>\n\n          </div>"}, "last_serial": 4015581, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "604c53635cdeb0fffb7b0750fb36b25b", "sha256": "da52ebe4a26e88c6e835880f002c95ea24e6ca921eb8783c36165a73a165baf2"}, "downloads": -1, "filename": "http-crawler-0.0.1.tar.gz", "has_sig": false, "md5_digest": "604c53635cdeb0fffb7b0750fb36b25b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1252, "upload_time": "2016-06-01T20:36:43", "upload_time_iso_8601": "2016-06-01T20:36:43.334006Z", "url": "https://files.pythonhosted.org/packages/88/79/2bd604052d383a33491b5fe5fdac11302ba656dd2addeb5bf107736c5abb/http-crawler-0.0.1.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "e412b6b17eb8c57bbf612cac00df7362", "sha256": "636d1c15eb96c339a53aed294589a613baff04511baf65d6fc687805dd4755ef"}, "downloads": -1, "filename": "http-crawler-0.1.0.tar.gz", "has_sig": false, "md5_digest": "e412b6b17eb8c57bbf612cac00df7362", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3137, "upload_time": "2016-06-09T10:22:46", "upload_time_iso_8601": "2016-06-09T10:22:46.681212Z", "url": "https://files.pythonhosted.org/packages/92/7a/6134139d5b74d7065877e7e89eb2bedbf1630e12d8de28ce67b554a0df2e/http-crawler-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "d66499aeb246339ce2ac19fa58d67c8c", "sha256": "9e5642defc90707ee041c90c06071dbc3870cd810550c94ee347c77a26f1c99a"}, "downloads": -1, "filename": "http-crawler-0.1.1.tar.gz", "has_sig": false, "md5_digest": "d66499aeb246339ce2ac19fa58d67c8c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3175, "upload_time": "2016-07-07T18:23:00", "upload_time_iso_8601": "2016-07-07T18:23:00.049172Z", "url": "https://files.pythonhosted.org/packages/f2/2f/d28faa20ebf297116177ab372e49be44c13a43e5fd0602c4236e1d2d4086/http-crawler-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "15a21f306847231d020d24e2bda207b1", "sha256": "ed24a401505df931445d9bc33c7e2e4324357f842c4fab840aae2a0a4ef22781"}, "downloads": -1, "filename": "http-crawler-0.1.2.tar.gz", "has_sig": false, "md5_digest": "15a21f306847231d020d24e2bda207b1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3172, "upload_time": "2016-08-10T18:41:20", "upload_time_iso_8601": "2016-08-10T18:41:20.747205Z", "url": "https://files.pythonhosted.org/packages/1d/f1/ac79bc35e51cbbeb1d6b9cafa792bddcec61b2071bb852b6da96ac312d24/http-crawler-0.1.2.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "79ed7a9e3a588cf4f45f8f6524b9ba3d", "sha256": "fcdd4d0a3a17b370e726843933561248502651ca741fd4f6eadd3ab0e447cf00"}, "downloads": -1, "filename": "http-crawler-0.1.4.tar.gz", "has_sig": false, "md5_digest": "79ed7a9e3a588cf4f45f8f6524b9ba3d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3464, "upload_time": "2017-02-09T15:44:22", "upload_time_iso_8601": "2017-02-09T15:44:22.898365Z", "url": "https://files.pythonhosted.org/packages/b4/e2/5f48282f145cfede73f9bf90ac8ccd1f662d6a240e91a91bfb997f72b9b1/http-crawler-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "b68506254b572c86aa42d48f7c494f05", "sha256": "da889d6604a1ee2bf475eff6e8b848ef35ad4d5768db1eef1fa8912f9ce945e7"}, "downloads": -1, "filename": "http-crawler-0.1.5.tar.gz", "has_sig": false, "md5_digest": "b68506254b572c86aa42d48f7c494f05", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3455, "upload_time": "2017-02-09T16:49:59", "upload_time_iso_8601": "2017-02-09T16:49:59.905589Z", "url": "https://files.pythonhosted.org/packages/d9/50/b9e301e785080251a1f7bda652fa6129721caa2bef6c725613b902800ea4/http-crawler-0.1.5.tar.gz", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "72b9254862d73783d94cfccc0abf5980", "sha256": "61666a28b97df1021d626570171e1961f65ede87cc8b08f7a548df22e9dd49df"}, "downloads": -1, "filename": "http-crawler-0.1.6.tar.gz", "has_sig": false, "md5_digest": "72b9254862d73783d94cfccc0abf5980", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3574, "upload_time": "2017-08-19T12:27:20", "upload_time_iso_8601": "2017-08-19T12:27:20.256048Z", "url": "https://files.pythonhosted.org/packages/44/bb/c6aede6cbc4d0cd9af2aa943a25817817e0ba9b9031db95807812b8b78e1/http-crawler-0.1.6.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "cd6488b02118c3b34a54b68aed724116", "sha256": "9878cc120fe9e0191e15cfdbffff026676a8505bf144c1a76bff57d6be7ba24e"}, "downloads": -1, "filename": "http-crawler-0.2.0.tar.gz", "has_sig": false, "md5_digest": "cd6488b02118c3b34a54b68aed724116", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3574, "upload_time": "2017-10-09T20:35:25", "upload_time_iso_8601": "2017-10-09T20:35:25.087694Z", "url": "https://files.pythonhosted.org/packages/a3/e0/91ca2cf9b75ca8bb5a3de76a10292c74494b5f8ab05c6fdb1fec2ba980ff/http-crawler-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "fbc176eddb431c4dbf218d7bc9658f14", "sha256": "50f83d3ef82bb2ba5562aaad78a8ed812adaceec4b8633a5d949058be73c53c2"}, "downloads": -1, "filename": "http-crawler-0.2.1.tar.gz", "has_sig": false, "md5_digest": "fbc176eddb431c4dbf218d7bc9658f14", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3721, "upload_time": "2018-06-29T14:36:35", "upload_time_iso_8601": "2018-06-29T14:36:35.690740Z", "url": "https://files.pythonhosted.org/packages/fc/f4/da694dacc99fe444002ff587fddb240aa1055149fc420352ae9d53b66cfb/http-crawler-0.2.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "fbc176eddb431c4dbf218d7bc9658f14", "sha256": "50f83d3ef82bb2ba5562aaad78a8ed812adaceec4b8633a5d949058be73c53c2"}, "downloads": -1, "filename": "http-crawler-0.2.1.tar.gz", "has_sig": false, "md5_digest": "fbc176eddb431c4dbf218d7bc9658f14", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3721, "upload_time": "2018-06-29T14:36:35", "upload_time_iso_8601": "2018-06-29T14:36:35.690740Z", "url": "https://files.pythonhosted.org/packages/fc/f4/da694dacc99fe444002ff587fddb240aa1055149fc420352ae9d53b66cfb/http-crawler-0.2.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:09 2020"}