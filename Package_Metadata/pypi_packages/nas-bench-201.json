{"info": {"author": "Xuanyi Dong", "author_email": "dongxuanyi888@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python", "Topic :: Database", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# [NAS-BENCH-201: Extending the Scope of Reproducible Neural Architecture Search](https://openreview.net/forum?id=HJxyZkBKDr)\n\nWe propose an algorithm-agnostic NAS benchmark (NAS-Bench-201) with a fixed search space, which provides a unified benchmark for almost any up-to-date NAS algorithms.\nThe design of our search space is inspired by that used in the most popular cell-based searching algorithms, where a cell is represented as a directed acyclic graph.\nEach edge here is associated with an operation selected from a predefined operation set.\nFor it to be applicable for all NAS algorithms, the search space defined in NAS-Bench-201 includes 4 nodes and 5 associated operation options, which generates 15,625 neural cell candidates in total.\n\nIn this Markdown file, we provide:\n- [How to Use NAS-Bench-201](#how-to-use-nas-bench-201)\n- [Instruction to re-generate NAS-Bench-201](#instruction-to-re-generate-nas-bench-201)\n- [10 NAS algorithms evaluated in our paper](#to-reproduce-10-baseline-nas-algorithms-in-nas-bench-201)\n\nNote: please use `PyTorch >= 1.2.0` and `Python >= 3.6.0`.\n\nYou can simply type `pip install nas-bench-201` to install our api. Please see source codes of `nas-bench-201` module in [this repo](https://github.com/D-X-Y/NAS-Bench-201).\n\n**If you have any questions or issues, please post it at [here](https://github.com/D-X-Y/AutoDL-Projects/issues) or email me.**\n\n### Preparation and Download\n\n[deprecated] The benchmark file of NAS-Bench-201 can be downloaded from [Google Drive](https://drive.google.com/open?id=1SKW0Cu0u8-gb18zDpaAGi0f74UdXeGKs) or [Baidu-Wangpan (code:6u5d)](https://pan.baidu.com/s/1CiaNH6C12zuZf7q-Ilm09w).\n\n[recommended] The benchmark file of NAS-Bench-201 can be downloaded from [Google Drive](https://drive.google.com/open?id=1OOfVPpt-lA4u2HJrXbgrRd42IbfvJMyE). The files for model weight are too large (431G) and I need some time to upload it. Please be patient, thanks for your understanding.\n\nYou can move it to anywhere you want and send its path to our API for initialization.\n- [2020.02.25] APIv1.0/FILEv1.0: `NAS-Bench-201-v1_0-e61699.pth` (2.2G), where `e61699` is the last six digits for this file. It contains all information except for the trained weights of each trial.\n- [2020.02.25] APIv1.0/FILEv1.0: The full data of each architecture can be download from [\nNAS-BENCH-201-4-v1.0-archive.tar](https://drive.google.com/open?id=1X2i-JXaElsnVLuGgM4tP-yNwtsspXgdQ) (about 226GB). This compressed folder has 15625 files containing the the trained weights.\n- [2020.02.25] APIv1.0/FILEv1.0: Checkpoints for 3 runs of each baseline NAS algorithm are provided in [Google Drive](https://drive.google.com/open?id=1eAgLZQAViP3r6dA0_ZOOGG9zPLXhGwXi).\n- [2020.03.09] APIv1.2/FILEv1.0: More robust API with more functions and descriptions\n- [2020.03.16] APIv1.3/FILEv1.1: `NAS-Bench-201-v1_1-096897.pth` (4.7G), where `096897` is the last six digits for this file. It contains information of more trials compared to `NAS-Bench-201-v1_0-e61699.pth`, especially all models trained by 12 epochs on all datasets are avaliable.\n- [2020.06.01] APIv2.0/FILEv2.0: coming soon!\n\n\nThe training and evaluation data used in NAS-Bench-201 can be downloaded from [Google Drive](https://drive.google.com/open?id=1L0Lzq8rWpZLPfiQGd6QR8q5xLV88emU7) or [Baidu-Wangpan (code:4fg7)](https://pan.baidu.com/s/1XAzavPKq3zcat1yBA1L2tQ).\nIt is recommended to put these data into `$TORCH_HOME` (`~/.torch/` by default). If you want to generate NAS-Bench-201 or similar NAS datasets or training models by yourself, you need these data.\n\n## How to Use NAS-Bench-201\n\n1. Creating an API instance from a file:\n```\nfrom nas_201_api import NASBench201API as API\napi = API('$path_to_meta_nas_bench_file')\napi = API('NAS-Bench-201-v1_0-e61699.pth')\napi = API('{:}/{:}'.format(os.environ['TORCH_HOME'], 'NAS-Bench-201-v1_0-e61699.pth'))\n```\n\n2. Show the number of architectures `len(api)` and each architecture `api[i]`:\n```\nnum = len(api)\nfor i, arch_str in enumerate(api):\n  print ('{:5d}/{:5d} : {:}'.format(i, len(api), arch_str))\n```\n\n3. Show the results of all trials for a single architecture:\n```\n# show all information for a specific architecture\napi.show(1)\napi.show(2)\n\n# show the mean loss and accuracy of an architecture\ninfo = api.query_meta_info_by_index(1)  # This is an instance of `ArchResults`\nres_metrics = info.get_metrics('cifar10', 'train') # This is a dict with metric names as keys\ncost_metrics = info.get_comput_costs('cifar100') # This is a dict with metric names as keys, e.g., flops, params, latency\n\n# get the detailed information\nresults = api.query_by_index(1, 'cifar100') # a dict of all trials for 1st net on cifar100, where the key is the seed\nprint ('There are {:} trials for this architecture [{:}] on cifar100'.format(len(results), api[1]))\nprint ('Latency : {:}'.format(results[0].get_latency()))\nprint ('Train Info : {:}'.format(results[0].get_train()))\nprint ('Valid Info : {:}'.format(results[0].get_eval('x-valid')))\nprint ('Test  Info : {:}'.format(results[0].get_eval('x-test')))\n# for the metric after a specific epoch\nprint ('Train Info [10-th epoch] : {:}'.format(results[0].get_train(10)))\n```\n\n4. Query the index of an architecture by string\n```\nindex = api.query_index_by_arch('|nor_conv_3x3~0|+|nor_conv_3x3~0|avg_pool_3x3~1|+|skip_connect~0|nor_conv_3x3~1|skip_connect~2|')\napi.show(index)\n```\nThis string `|nor_conv_3x3~0|+|nor_conv_3x3~0|avg_pool_3x3~1|+|skip_connect~0|nor_conv_3x3~1|skip_connect~2|` means:\n```\nnode-0: the input tensor\nnode-1: conv-3x3( node-0 )\nnode-2: conv-3x3( node-0 ) + avg-pool-3x3( node-1 )\nnode-3: skip-connect( node-0 ) + conv-3x3( node-1 ) + skip-connect( node-2 )\n```\n\n5. Create the network from api:\n```\nconfig = api.get_net_config(123, 'cifar10') # obtain the network configuration for the 123-th architecture on the CIFAR-10 dataset\nfrom models import get_cell_based_tiny_net # this module is in AutoDL-Projects/lib/models\nnetwork = get_cell_based_tiny_net(config) # create the network from configurration\nprint(network) # show the structure of this architecture\n```\nIf you want to load the trained weights of this created network, you need to use `api.get_net_param(123, ...)` to obtain the weights and then load it to the network.\n\n6. `api.get_more_info(...)` can return the loss / accuracy / time on training / validation / test sets, which is very helpful. For more details, please look at the comments in the get_more_info function.\n\n7. For other usages, please see `lib/nas_201_api/api.py`. We provide some usage information in the comments for the corresponding functions. If what you want is not provided, please feel free to open an issue for discussion, and I am happy to answer any questions regarding NAS-Bench-201.\n\n\n### Detailed Instruction\n\nIn `nas_201_api`, we define three classes: `NASBench201API`, `ArchResults`, `ResultsCount`.\n\n`ResultsCount` maintains all information of a specific trial. One can instantiate ResultsCount and get the info via the following codes (`000157-FULL.pth` saves all information of all trials of 157-th architecture):\n```\nfrom nas_201_api import ResultsCount\nxdata  = torch.load('000157-FULL.pth')\nodata  = xdata['full']['all_results'][('cifar10-valid', 777)]\nresult = ResultsCount.create_from_state_dict( odata )\nprint(result) # print it\nprint(result.get_train())   # print the final training loss/accuracy/[optional:time-cost-of-a-training-epoch]\nprint(result.get_train(11)) # print the training info of the 11-th epoch\nprint(result.get_eval('x-valid'))     # print the final evaluation info on the validation set\nprint(result.get_eval('x-valid', 11)) # print the info on the validation set of the 11-th epoch\nprint(result.get_latency())           # print the evaluation latency [in batch]\nresult.get_net_param()                # the trained parameters of this trial\narch_config = result.get_config(CellStructure.str2structure) # create the network with params\nnet_config  = dict2config(arch_config, None)\nnetwork    = get_cell_based_tiny_net(net_config)\nnetwork.load_state_dict(result.get_net_param())\n```\n\n`ArchResults` maintains all information of all trials of an architecture. Please see the following usages:\n```\nfrom nas_201_api import ArchResults\nxdata   = torch.load('000157-FULL.pth')\narchRes = ArchResults.create_from_state_dict(xdata['less']) # load trials trained with  12 epochs\narchRes = ArchResults.create_from_state_dict(xdata['full']) # load trials trained with 200 epochs\n\nprint(archRes.arch_idx_str())      # print the index of this architecture \nprint(archRes.get_dataset_names()) # print the supported training data\nprint(archRes.get_comput_costs('cifar10-valid')) # print all computational info when training on cifar10-valid \nprint(archRes.get_metrics('cifar10-valid', 'x-valid', None, False)) # print the average loss/accuracy/time on all trials\nprint(archRes.get_metrics('cifar10-valid', 'x-valid', None,  True)) # print loss/accuracy/time of a randomly selected trial\n```\n\n`NASBench201API` is the topest level api. Please see the following usages:\n```\nfrom nas_201_api import NASBench201API as API\napi = API('NAS-Bench-201-v1_0-e61699.pth') # This will load all the information of NAS-Bench-201 except the trained weights\napi = API('{:}/{:}'.format(os.environ['TORCH_HOME'], 'NAS-Bench-201-v1_0-e61699.pth')) # The same as the above line while I usually save NAS-Bench-201-v1_0-e61699.pth in ~/.torch/.\napi.show(-1)  # show info of all architectures\napi.reload('{:}/{:}'.format(os.environ['TORCH_HOME'], 'NAS-BENCH-201-4-v1.0-archive'), 3) # This code will reload the information 3-th architecture with the trained weights\n\nweights = api.get_net_param(3, 'cifar10', None) # Obtaining the weights of all trials for the 3-th architecture on cifar10. It will returns a dict, where the key is the seed and the value is the trained weights.\n```\n\nTo obtain the training and evaluation information (please see the comments [here](https://github.com/D-X-Y/AutoDL-Projects/blob/master/lib/nas_201_api/api.py#L172)):\n```\napi.get_more_info(112, 'cifar10', None, False, True)\napi.get_more_info(112, 'ImageNet16-120', None, False, True) # the info of last training epoch for 112-th architecture (use 200-epoch-hyper-parameter and randomly select a trial)\n```\n\nPlease use the following script to show the best architectures on each dataset:\n```show the best architecture\npython exps/NAS-Bench-201/show-best.py\n```\n\n\n## Instruction to Re-Generate NAS-Bench-201\n\nThere are four steps to build NAS-Bench-201.\n\n1. generate the meta file for NAS-Bench-201 using the following script, where `NAS-BENCH-201` indicates the name and `4` indicates the maximum number of nodes in a cell.\n```\nbash scripts-search/NAS-Bench-201/meta-gen.sh NAS-BENCH-201 4\n```\n\n2. train earch architecture on a single GPU (see commands in `output/NAS-BENCH-201-4/BENCH-201-N4.opt-full.script`, which is automatically generated by step-1).\n```\nCUDA_VISIBLE_DEVICES=0 bash ./scripts-search/NAS-Bench-201/train-models.sh 0     0   389 -1 '777 888 999'\n```\nThis command will train 390 architectures (id from 0 to 389) using the following four kinds of splits with three random seeds (777, 888, 999).\n\n|     Dataset     |     Train     |      Eval    |\n|:---------------:|:-------------:|:------------:|\n| CIFAR-10        | train         | valid / test |\n| CIFAR-10        | train + valid | test         |\n| CIFAR-100       | train         | valid / test |\n| ImageNet-16-120 | train         | valid / test |\n\nNote that the above `train`, `valid`, and `test` indicate the proposed splits in our NAS-Bench-201, and they might be different with the original splits.\n\n3. calculate the latency, merge the results of all architectures, and simplify the results.\n(see commands in `output/NAS-BENCH-201-4/meta-node-4.cal-script.txt` which is automatically generated by step-1).\n```\nOMP_NUM_THREADS=6 CUDA_VISIBLE_DEVICES=0 python exps/NAS-Bench-201/statistics.py --mode cal --target_dir 000000-000389-C16-N5\n```\n\n4. merge all results into a single file for NAS-Bench-201-API.\n```\nOMP_NUM_THREADS=4 python exps/NAS-Bench-201/statistics.py --mode merge\n```\nThis command will generate a single file `output/NAS-BENCH-201-4/simplifies/C16-N5-final-infos.pth` contains all the data for NAS-Bench-201.\nThis generated file will serve as the input for our NAS-Bench-201 API.\n\n[option] train a single architecture on a single GPU.\n```\nCUDA_VISIBLE_DEVICES=0 bash ./scripts-search/NAS-Bench-201/train-a-net.sh resnet 16 5\nCUDA_VISIBLE_DEVICES=0 bash ./scripts-search/NAS-Bench-201/train-a-net.sh '|nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~1|+|skip_connect~0|skip_connect~1|skip_connect~2|' 16 5\n```\n\n\n## To Reproduce 10 Baseline NAS Algorithms in NAS-Bench-201\n\nWe have tried our best to implement each method. However, still, some algorithms might obtain non-optimal results since their hyper-parameters might not fit our NAS-Bench-201.\nIf researchers can provide better results with different hyper-parameters, we are happy to update results according to the new experimental results. We also welcome more NAS algorithms to test on our dataset and would include them accordingly.\n\n**Note that** you need to prepare the training and test data as described in [Preparation and Download](#preparation-and-download)\n\n- [1] `CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/algos/DARTS-V1.sh cifar10 1 -1`, where `cifar10` can be replaced with `cifar100` or `ImageNet16-120`.\n- [2] `CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/algos/DARTS-V2.sh cifar10 1 -1`\n- [3] `CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/algos/GDAS.sh     cifar10 1 -1`\n- [4] `CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/algos/SETN.sh     cifar10 1 -1`\n- [5] `CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/algos/ENAS.sh     cifar10 1 -1`\n- [6] `CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/algos/RANDOM-NAS.sh cifar10 1 -1`\n- [7] `bash ./scripts-search/algos/R-EA.sh cifar10 3 -1`\n- [8] `bash ./scripts-search/algos/Random.sh cifar10 -1`\n- [9] `bash ./scripts-search/algos/REINFORCE.sh cifar10 0.5 -1`\n- [10] `bash ./scripts-search/algos/BOHB.sh cifar10 -1`\n\nIn commands [1-6], the first args `cifar10` indicates the dataset name, the second args `1` indicates the behavior of BN, and the first args `-1` indicates the random seed.\n\n**Note that** since 2020 March 16, in these scripts, the default NAS-Bench-201 benchmark file has changed from `NAS-Bench-201-v1_0-e61699.pth` to `NAS-Bench-201-v1_1-096897.pth`, and thus the results could be slightly different from the original paper.\n\n\n# Citation\n\nIf you find that NAS-Bench-201 helps your research, please consider citing it:\n```\n@inproceedings{dong2020nasbench201,\n  title     = {NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search},\n  author    = {Dong, Xuanyi and Yang, Yi},\n  booktitle = {International Conference on Learning Representations (ICLR)},\n  url       = {https://openreview.net/forum?id=HJxyZkBKDr},\n  year      = {2020}\n}\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/D-X-Y/NAS-Bench-201", "keywords": "NAS Dataset API DeepLearning", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "nas-bench-201", "package_url": "https://pypi.org/project/nas-bench-201/", "platform": "", "project_url": "https://pypi.org/project/nas-bench-201/", "project_urls": {"Homepage": "https://github.com/D-X-Y/NAS-Bench-201"}, "release_url": "https://pypi.org/project/nas-bench-201/1.3/", "requires_dist": null, "requires_python": "", "summary": "API for NAS-Bench-201 (a benchmark for neural architecture search).", "version": "1.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1><a href=\"https://openreview.net/forum?id=HJxyZkBKDr\" rel=\"nofollow\">NAS-BENCH-201: Extending the Scope of Reproducible Neural Architecture Search</a></h1>\n<p>We propose an algorithm-agnostic NAS benchmark (NAS-Bench-201) with a fixed search space, which provides a unified benchmark for almost any up-to-date NAS algorithms.\nThe design of our search space is inspired by that used in the most popular cell-based searching algorithms, where a cell is represented as a directed acyclic graph.\nEach edge here is associated with an operation selected from a predefined operation set.\nFor it to be applicable for all NAS algorithms, the search space defined in NAS-Bench-201 includes 4 nodes and 5 associated operation options, which generates 15,625 neural cell candidates in total.</p>\n<p>In this Markdown file, we provide:</p>\n<ul>\n<li><a href=\"#how-to-use-nas-bench-201\" rel=\"nofollow\">How to Use NAS-Bench-201</a></li>\n<li><a href=\"#instruction-to-re-generate-nas-bench-201\" rel=\"nofollow\">Instruction to re-generate NAS-Bench-201</a></li>\n<li><a href=\"#to-reproduce-10-baseline-nas-algorithms-in-nas-bench-201\" rel=\"nofollow\">10 NAS algorithms evaluated in our paper</a></li>\n</ul>\n<p>Note: please use <code>PyTorch &gt;= 1.2.0</code> and <code>Python &gt;= 3.6.0</code>.</p>\n<p>You can simply type <code>pip install nas-bench-201</code> to install our api. Please see source codes of <code>nas-bench-201</code> module in <a href=\"https://github.com/D-X-Y/NAS-Bench-201\" rel=\"nofollow\">this repo</a>.</p>\n<p><strong>If you have any questions or issues, please post it at <a href=\"https://github.com/D-X-Y/AutoDL-Projects/issues\" rel=\"nofollow\">here</a> or email me.</strong></p>\n<h3>Preparation and Download</h3>\n<p>[deprecated] The benchmark file of NAS-Bench-201 can be downloaded from <a href=\"https://drive.google.com/open?id=1SKW0Cu0u8-gb18zDpaAGi0f74UdXeGKs\" rel=\"nofollow\">Google Drive</a> or <a href=\"https://pan.baidu.com/s/1CiaNH6C12zuZf7q-Ilm09w\" rel=\"nofollow\">Baidu-Wangpan (code:6u5d)</a>.</p>\n<p>[recommended] The benchmark file of NAS-Bench-201 can be downloaded from <a href=\"https://drive.google.com/open?id=1OOfVPpt-lA4u2HJrXbgrRd42IbfvJMyE\" rel=\"nofollow\">Google Drive</a>. The files for model weight are too large (431G) and I need some time to upload it. Please be patient, thanks for your understanding.</p>\n<p>You can move it to anywhere you want and send its path to our API for initialization.</p>\n<ul>\n<li>[2020.02.25] APIv1.0/FILEv1.0: <code>NAS-Bench-201-v1_0-e61699.pth</code> (2.2G), where <code>e61699</code> is the last six digits for this file. It contains all information except for the trained weights of each trial.</li>\n<li>[2020.02.25] APIv1.0/FILEv1.0: The full data of each architecture can be download from <a href=\"https://drive.google.com/open?id=1X2i-JXaElsnVLuGgM4tP-yNwtsspXgdQ\" rel=\"nofollow\">\nNAS-BENCH-201-4-v1.0-archive.tar</a> (about 226GB). This compressed folder has 15625 files containing the the trained weights.</li>\n<li>[2020.02.25] APIv1.0/FILEv1.0: Checkpoints for 3 runs of each baseline NAS algorithm are provided in <a href=\"https://drive.google.com/open?id=1eAgLZQAViP3r6dA0_ZOOGG9zPLXhGwXi\" rel=\"nofollow\">Google Drive</a>.</li>\n<li>[2020.03.09] APIv1.2/FILEv1.0: More robust API with more functions and descriptions</li>\n<li>[2020.03.16] APIv1.3/FILEv1.1: <code>NAS-Bench-201-v1_1-096897.pth</code> (4.7G), where <code>096897</code> is the last six digits for this file. It contains information of more trials compared to <code>NAS-Bench-201-v1_0-e61699.pth</code>, especially all models trained by 12 epochs on all datasets are avaliable.</li>\n<li>[2020.06.01] APIv2.0/FILEv2.0: coming soon!</li>\n</ul>\n<p>The training and evaluation data used in NAS-Bench-201 can be downloaded from <a href=\"https://drive.google.com/open?id=1L0Lzq8rWpZLPfiQGd6QR8q5xLV88emU7\" rel=\"nofollow\">Google Drive</a> or <a href=\"https://pan.baidu.com/s/1XAzavPKq3zcat1yBA1L2tQ\" rel=\"nofollow\">Baidu-Wangpan (code:4fg7)</a>.\nIt is recommended to put these data into <code>$TORCH_HOME</code> (<code>~/.torch/</code> by default). If you want to generate NAS-Bench-201 or similar NAS datasets or training models by yourself, you need these data.</p>\n<h2>How to Use NAS-Bench-201</h2>\n<ol>\n<li>Creating an API instance from a file:</li>\n</ol>\n<pre><code>from nas_201_api import NASBench201API as API\napi = API('$path_to_meta_nas_bench_file')\napi = API('NAS-Bench-201-v1_0-e61699.pth')\napi = API('{:}/{:}'.format(os.environ['TORCH_HOME'], 'NAS-Bench-201-v1_0-e61699.pth'))\n</code></pre>\n<ol>\n<li>Show the number of architectures <code>len(api)</code> and each architecture <code>api[i]</code>:</li>\n</ol>\n<pre><code>num = len(api)\nfor i, arch_str in enumerate(api):\n  print ('{:5d}/{:5d} : {:}'.format(i, len(api), arch_str))\n</code></pre>\n<ol>\n<li>Show the results of all trials for a single architecture:</li>\n</ol>\n<pre><code># show all information for a specific architecture\napi.show(1)\napi.show(2)\n\n# show the mean loss and accuracy of an architecture\ninfo = api.query_meta_info_by_index(1)  # This is an instance of `ArchResults`\nres_metrics = info.get_metrics('cifar10', 'train') # This is a dict with metric names as keys\ncost_metrics = info.get_comput_costs('cifar100') # This is a dict with metric names as keys, e.g., flops, params, latency\n\n# get the detailed information\nresults = api.query_by_index(1, 'cifar100') # a dict of all trials for 1st net on cifar100, where the key is the seed\nprint ('There are {:} trials for this architecture [{:}] on cifar100'.format(len(results), api[1]))\nprint ('Latency : {:}'.format(results[0].get_latency()))\nprint ('Train Info : {:}'.format(results[0].get_train()))\nprint ('Valid Info : {:}'.format(results[0].get_eval('x-valid')))\nprint ('Test  Info : {:}'.format(results[0].get_eval('x-test')))\n# for the metric after a specific epoch\nprint ('Train Info [10-th epoch] : {:}'.format(results[0].get_train(10)))\n</code></pre>\n<ol>\n<li>Query the index of an architecture by string</li>\n</ol>\n<pre><code>index = api.query_index_by_arch('|nor_conv_3x3~0|+|nor_conv_3x3~0|avg_pool_3x3~1|+|skip_connect~0|nor_conv_3x3~1|skip_connect~2|')\napi.show(index)\n</code></pre>\n<p>This string <code>|nor_conv_3x3~0|+|nor_conv_3x3~0|avg_pool_3x3~1|+|skip_connect~0|nor_conv_3x3~1|skip_connect~2|</code> means:</p>\n<pre><code>node-0: the input tensor\nnode-1: conv-3x3( node-0 )\nnode-2: conv-3x3( node-0 ) + avg-pool-3x3( node-1 )\nnode-3: skip-connect( node-0 ) + conv-3x3( node-1 ) + skip-connect( node-2 )\n</code></pre>\n<ol>\n<li>Create the network from api:</li>\n</ol>\n<pre><code>config = api.get_net_config(123, 'cifar10') # obtain the network configuration for the 123-th architecture on the CIFAR-10 dataset\nfrom models import get_cell_based_tiny_net # this module is in AutoDL-Projects/lib/models\nnetwork = get_cell_based_tiny_net(config) # create the network from configurration\nprint(network) # show the structure of this architecture\n</code></pre>\n<p>If you want to load the trained weights of this created network, you need to use <code>api.get_net_param(123, ...)</code> to obtain the weights and then load it to the network.</p>\n<ol>\n<li>\n<p><code>api.get_more_info(...)</code> can return the loss / accuracy / time on training / validation / test sets, which is very helpful. For more details, please look at the comments in the get_more_info function.</p>\n</li>\n<li>\n<p>For other usages, please see <code>lib/nas_201_api/api.py</code>. We provide some usage information in the comments for the corresponding functions. If what you want is not provided, please feel free to open an issue for discussion, and I am happy to answer any questions regarding NAS-Bench-201.</p>\n</li>\n</ol>\n<h3>Detailed Instruction</h3>\n<p>In <code>nas_201_api</code>, we define three classes: <code>NASBench201API</code>, <code>ArchResults</code>, <code>ResultsCount</code>.</p>\n<p><code>ResultsCount</code> maintains all information of a specific trial. One can instantiate ResultsCount and get the info via the following codes (<code>000157-FULL.pth</code> saves all information of all trials of 157-th architecture):</p>\n<pre><code>from nas_201_api import ResultsCount\nxdata  = torch.load('000157-FULL.pth')\nodata  = xdata['full']['all_results'][('cifar10-valid', 777)]\nresult = ResultsCount.create_from_state_dict( odata )\nprint(result) # print it\nprint(result.get_train())   # print the final training loss/accuracy/[optional:time-cost-of-a-training-epoch]\nprint(result.get_train(11)) # print the training info of the 11-th epoch\nprint(result.get_eval('x-valid'))     # print the final evaluation info on the validation set\nprint(result.get_eval('x-valid', 11)) # print the info on the validation set of the 11-th epoch\nprint(result.get_latency())           # print the evaluation latency [in batch]\nresult.get_net_param()                # the trained parameters of this trial\narch_config = result.get_config(CellStructure.str2structure) # create the network with params\nnet_config  = dict2config(arch_config, None)\nnetwork    = get_cell_based_tiny_net(net_config)\nnetwork.load_state_dict(result.get_net_param())\n</code></pre>\n<p><code>ArchResults</code> maintains all information of all trials of an architecture. Please see the following usages:</p>\n<pre><code>from nas_201_api import ArchResults\nxdata   = torch.load('000157-FULL.pth')\narchRes = ArchResults.create_from_state_dict(xdata['less']) # load trials trained with  12 epochs\narchRes = ArchResults.create_from_state_dict(xdata['full']) # load trials trained with 200 epochs\n\nprint(archRes.arch_idx_str())      # print the index of this architecture \nprint(archRes.get_dataset_names()) # print the supported training data\nprint(archRes.get_comput_costs('cifar10-valid')) # print all computational info when training on cifar10-valid \nprint(archRes.get_metrics('cifar10-valid', 'x-valid', None, False)) # print the average loss/accuracy/time on all trials\nprint(archRes.get_metrics('cifar10-valid', 'x-valid', None,  True)) # print loss/accuracy/time of a randomly selected trial\n</code></pre>\n<p><code>NASBench201API</code> is the topest level api. Please see the following usages:</p>\n<pre><code>from nas_201_api import NASBench201API as API\napi = API('NAS-Bench-201-v1_0-e61699.pth') # This will load all the information of NAS-Bench-201 except the trained weights\napi = API('{:}/{:}'.format(os.environ['TORCH_HOME'], 'NAS-Bench-201-v1_0-e61699.pth')) # The same as the above line while I usually save NAS-Bench-201-v1_0-e61699.pth in ~/.torch/.\napi.show(-1)  # show info of all architectures\napi.reload('{:}/{:}'.format(os.environ['TORCH_HOME'], 'NAS-BENCH-201-4-v1.0-archive'), 3) # This code will reload the information 3-th architecture with the trained weights\n\nweights = api.get_net_param(3, 'cifar10', None) # Obtaining the weights of all trials for the 3-th architecture on cifar10. It will returns a dict, where the key is the seed and the value is the trained weights.\n</code></pre>\n<p>To obtain the training and evaluation information (please see the comments <a href=\"https://github.com/D-X-Y/AutoDL-Projects/blob/master/lib/nas_201_api/api.py#L172\" rel=\"nofollow\">here</a>):</p>\n<pre><code>api.get_more_info(112, 'cifar10', None, False, True)\napi.get_more_info(112, 'ImageNet16-120', None, False, True) # the info of last training epoch for 112-th architecture (use 200-epoch-hyper-parameter and randomly select a trial)\n</code></pre>\n<p>Please use the following script to show the best architectures on each dataset:</p>\n<pre>python exps/NAS-Bench-201/show-best.py\n</pre>\n<h2>Instruction to Re-Generate NAS-Bench-201</h2>\n<p>There are four steps to build NAS-Bench-201.</p>\n<ol>\n<li>generate the meta file for NAS-Bench-201 using the following script, where <code>NAS-BENCH-201</code> indicates the name and <code>4</code> indicates the maximum number of nodes in a cell.</li>\n</ol>\n<pre><code>bash scripts-search/NAS-Bench-201/meta-gen.sh NAS-BENCH-201 4\n</code></pre>\n<ol>\n<li>train earch architecture on a single GPU (see commands in <code>output/NAS-BENCH-201-4/BENCH-201-N4.opt-full.script</code>, which is automatically generated by step-1).</li>\n</ol>\n<pre><code>CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/NAS-Bench-201/train-models.sh 0     0   389 -1 '777 888 999'\n</code></pre>\n<p>This command will train 390 architectures (id from 0 to 389) using the following four kinds of splits with three random seeds (777, 888, 999).</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Dataset</th>\n<th align=\"center\">Train</th>\n<th align=\"center\">Eval</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">CIFAR-10</td>\n<td align=\"center\">train</td>\n<td align=\"center\">valid / test</td>\n</tr>\n<tr>\n<td align=\"center\">CIFAR-10</td>\n<td align=\"center\">train + valid</td>\n<td align=\"center\">test</td>\n</tr>\n<tr>\n<td align=\"center\">CIFAR-100</td>\n<td align=\"center\">train</td>\n<td align=\"center\">valid / test</td>\n</tr>\n<tr>\n<td align=\"center\">ImageNet-16-120</td>\n<td align=\"center\">train</td>\n<td align=\"center\">valid / test</td>\n</tr></tbody></table>\n<p>Note that the above <code>train</code>, <code>valid</code>, and <code>test</code> indicate the proposed splits in our NAS-Bench-201, and they might be different with the original splits.</p>\n<ol>\n<li>calculate the latency, merge the results of all architectures, and simplify the results.\n(see commands in <code>output/NAS-BENCH-201-4/meta-node-4.cal-script.txt</code> which is automatically generated by step-1).</li>\n</ol>\n<pre><code>OMP_NUM_THREADS=6 CUDA_VISIBLE_DEVICES=0 python exps/NAS-Bench-201/statistics.py --mode cal --target_dir 000000-000389-C16-N5\n</code></pre>\n<ol>\n<li>merge all results into a single file for NAS-Bench-201-API.</li>\n</ol>\n<pre><code>OMP_NUM_THREADS=4 python exps/NAS-Bench-201/statistics.py --mode merge\n</code></pre>\n<p>This command will generate a single file <code>output/NAS-BENCH-201-4/simplifies/C16-N5-final-infos.pth</code> contains all the data for NAS-Bench-201.\nThis generated file will serve as the input for our NAS-Bench-201 API.</p>\n<p>[option] train a single architecture on a single GPU.</p>\n<pre><code>CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/NAS-Bench-201/train-a-net.sh resnet 16 5\nCUDA_VISIBLE_DEVICES=0 bash ./scripts-search/NAS-Bench-201/train-a-net.sh '|nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~1|+|skip_connect~0|skip_connect~1|skip_connect~2|' 16 5\n</code></pre>\n<h2>To Reproduce 10 Baseline NAS Algorithms in NAS-Bench-201</h2>\n<p>We have tried our best to implement each method. However, still, some algorithms might obtain non-optimal results since their hyper-parameters might not fit our NAS-Bench-201.\nIf researchers can provide better results with different hyper-parameters, we are happy to update results according to the new experimental results. We also welcome more NAS algorithms to test on our dataset and would include them accordingly.</p>\n<p><strong>Note that</strong> you need to prepare the training and test data as described in <a href=\"#preparation-and-download\" rel=\"nofollow\">Preparation and Download</a></p>\n<ul>\n<li>[1] <code>CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/algos/DARTS-V1.sh cifar10 1 -1</code>, where <code>cifar10</code> can be replaced with <code>cifar100</code> or <code>ImageNet16-120</code>.</li>\n<li>[2] <code>CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/algos/DARTS-V2.sh cifar10 1 -1</code></li>\n<li>[3] <code>CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/algos/GDAS.sh cifar10 1 -1</code></li>\n<li>[4] <code>CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/algos/SETN.sh cifar10 1 -1</code></li>\n<li>[5] <code>CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/algos/ENAS.sh cifar10 1 -1</code></li>\n<li>[6] <code>CUDA_VISIBLE_DEVICES=0 bash ./scripts-search/algos/RANDOM-NAS.sh cifar10 1 -1</code></li>\n<li>[7] <code>bash ./scripts-search/algos/R-EA.sh cifar10 3 -1</code></li>\n<li>[8] <code>bash ./scripts-search/algos/Random.sh cifar10 -1</code></li>\n<li>[9] <code>bash ./scripts-search/algos/REINFORCE.sh cifar10 0.5 -1</code></li>\n<li>[10] <code>bash ./scripts-search/algos/BOHB.sh cifar10 -1</code></li>\n</ul>\n<p>In commands [1-6], the first args <code>cifar10</code> indicates the dataset name, the second args <code>1</code> indicates the behavior of BN, and the first args <code>-1</code> indicates the random seed.</p>\n<p><strong>Note that</strong> since 2020 March 16, in these scripts, the default NAS-Bench-201 benchmark file has changed from <code>NAS-Bench-201-v1_0-e61699.pth</code> to <code>NAS-Bench-201-v1_1-096897.pth</code>, and thus the results could be slightly different from the original paper.</p>\n<h1>Citation</h1>\n<p>If you find that NAS-Bench-201 helps your research, please consider citing it:</p>\n<pre><code>@inproceedings{dong2020nasbench201,\n  title     = {NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search},\n  author    = {Dong, Xuanyi and Yang, Yi},\n  booktitle = {International Conference on Learning Representations (ICLR)},\n  url       = {https://openreview.net/forum?id=HJxyZkBKDr},\n  year      = {2020}\n}\n</code></pre>\n\n          </div>"}, "last_serial": 6854763, "releases": {"1.0": [{"comment_text": "", "digests": {"md5": "604a8fccd3524a4ba6650136d12e5cc7", "sha256": "7467cf31e4690043b51053cd2bf7ceb9128afe0fbfabf7f464c7d1d40fb1e1fb"}, "downloads": -1, "filename": "nas_bench_201-1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "604a8fccd3524a4ba6650136d12e5cc7", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10841, "upload_time": "2020-01-14T14:39:12", "upload_time_iso_8601": "2020-01-14T14:39:12.145593Z", "url": "https://files.pythonhosted.org/packages/6e/06/9f0e4d8286b38ddc21dc1c8ad1087edda1ce5d597b7bffafdc571658ceba/nas_bench_201-1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ba21397584a3ed2d819b1590412241f9", "sha256": "b3d6f6474ede8fc69eaac6913cbd1b7a102c4d2deb5c27d51bdebe935fedfd02"}, "downloads": -1, "filename": "nas_bench_201-1.0.tar.gz", "has_sig": false, "md5_digest": "ba21397584a3ed2d819b1590412241f9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12734, "upload_time": "2020-01-14T14:39:14", "upload_time_iso_8601": "2020-01-14T14:39:14.777033Z", "url": "https://files.pythonhosted.org/packages/61/47/af01725d9c559d72d965aeeeb631542adccaf9c9c371a2f3424705b3d3ad/nas_bench_201-1.0.tar.gz", "yanked": false}], "1.1": [{"comment_text": "", "digests": {"md5": "37e61298496c4af2c49bb7631ed741db", "sha256": "f6944c677133ffc7db14503c31215b0c1c8f135798748ab280bc7cd276918c2e"}, "downloads": -1, "filename": "nas_bench_201-1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "37e61298496c4af2c49bb7631ed741db", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 12407, "upload_time": "2020-01-22T01:16:16", "upload_time_iso_8601": "2020-01-22T01:16:16.259471Z", "url": "https://files.pythonhosted.org/packages/d2/ea/60978852424ef02d178dfa4a0867574dd3131a6f0122834673f288720346/nas_bench_201-1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6d0c8e2da2bf17cff8586ae93a6bbaf4", "sha256": "24529c7c8e4ad40ea4627e8706d7823fdc0d7700528f82f42fe27f0969a4af31"}, "downloads": -1, "filename": "nas_bench_201-1.1.tar.gz", "has_sig": false, "md5_digest": "6d0c8e2da2bf17cff8586ae93a6bbaf4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14121, "upload_time": "2020-01-22T01:16:18", "upload_time_iso_8601": "2020-01-22T01:16:18.017775Z", "url": "https://files.pythonhosted.org/packages/66/11/1f2fa922bdbcca1d99dec1af05cd3e03eeb13bc3dac1e0f47a894521b36e/nas_bench_201-1.1.tar.gz", "yanked": false}], "1.2": [{"comment_text": "", "digests": {"md5": "24333fc8126eb5aef8506eafc7c63a8f", "sha256": "8b5c55169a810407c690bef4787266752e116d32d66f39850228cb63f9acb659"}, "downloads": -1, "filename": "nas_bench_201-1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "24333fc8126eb5aef8506eafc7c63a8f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18403, "upload_time": "2020-03-21T09:26:43", "upload_time_iso_8601": "2020-03-21T09:26:43.716191Z", "url": "https://files.pythonhosted.org/packages/61/fe/ff6adc99f4bf333a32559aa101c0fc8869555b153364526373f40a5034bd/nas_bench_201-1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8cd2b1c9dc61b6a54570a09b51061b67", "sha256": "7fbd80804fa56fba3b5172f23ad611bfa5f45461888723f8771354e5cc63d641"}, "downloads": -1, "filename": "nas_bench_201-1.2.tar.gz", "has_sig": false, "md5_digest": "8cd2b1c9dc61b6a54570a09b51061b67", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22414, "upload_time": "2020-03-21T09:26:45", "upload_time_iso_8601": "2020-03-21T09:26:45.818407Z", "url": "https://files.pythonhosted.org/packages/07/8a/7bf309a7816b53ab497f8b4544813bd601ee1c4344a6bbb902ef2a08bbfd/nas_bench_201-1.2.tar.gz", "yanked": false}], "1.3": [{"comment_text": "", "digests": {"md5": "c1a45cdcb844a7c11b1015895de44e5f", "sha256": "38661db70ea44ba30995c5b4bd8ed9dc1325173cbdb037f15042406ce4d77d33"}, "downloads": -1, "filename": "nas_bench_201-1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "c1a45cdcb844a7c11b1015895de44e5f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18406, "upload_time": "2020-03-21T09:30:50", "upload_time_iso_8601": "2020-03-21T09:30:50.621532Z", "url": "https://files.pythonhosted.org/packages/c7/b8/e978062e240fe631e6dd010a9faf6fb05fa8cab4ede84b58016c4edab04d/nas_bench_201-1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "92d013fa21ffaf79a7d1b287465c1b33", "sha256": "b2eeb4ae3b3c5c71e217adb3fea0b9448d83845e94eb16df2ddc878adfae301c"}, "downloads": -1, "filename": "nas_bench_201-1.3.tar.gz", "has_sig": false, "md5_digest": "92d013fa21ffaf79a7d1b287465c1b33", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22441, "upload_time": "2020-03-21T09:30:52", "upload_time_iso_8601": "2020-03-21T09:30:52.112296Z", "url": "https://files.pythonhosted.org/packages/d2/33/89e0daec6da6cf5690d5d3c28bb12a349d7ced82e491375ca53f232ee006/nas_bench_201-1.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c1a45cdcb844a7c11b1015895de44e5f", "sha256": "38661db70ea44ba30995c5b4bd8ed9dc1325173cbdb037f15042406ce4d77d33"}, "downloads": -1, "filename": "nas_bench_201-1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "c1a45cdcb844a7c11b1015895de44e5f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 18406, "upload_time": "2020-03-21T09:30:50", "upload_time_iso_8601": "2020-03-21T09:30:50.621532Z", "url": "https://files.pythonhosted.org/packages/c7/b8/e978062e240fe631e6dd010a9faf6fb05fa8cab4ede84b58016c4edab04d/nas_bench_201-1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "92d013fa21ffaf79a7d1b287465c1b33", "sha256": "b2eeb4ae3b3c5c71e217adb3fea0b9448d83845e94eb16df2ddc878adfae301c"}, "downloads": -1, "filename": "nas_bench_201-1.3.tar.gz", "has_sig": false, "md5_digest": "92d013fa21ffaf79a7d1b287465c1b33", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22441, "upload_time": "2020-03-21T09:30:52", "upload_time_iso_8601": "2020-03-21T09:30:52.112296Z", "url": "https://files.pythonhosted.org/packages/d2/33/89e0daec6da6cf5690d5d3c28bb12a349d7ced82e491375ca53f232ee006/nas_bench_201-1.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:47:26 2020"}