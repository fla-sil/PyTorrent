{"info": {"author": "Takenori Yamamoto", "author_email": "yamamoto.takenory@gmail.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# A PyTorch Extension for Learning Rate Warmup\n\nThis library contains PyTorch implementations of the warmup schedules described in [On the adequacy of untuned warmup for adaptive optimization](https://arxiv.org/abs/1910.04209).\n\n<p align=\"center\"><img src=\"https://github.com/Tony-Y/pytorch_warmup/raw/master/examples/plots/figs/warmup_schedule.png\" alt=\"Warmup schedule\" width=\"400\"/></p>\n\n![Python package](https://github.com/Tony-Y/pytorch_warmup/workflows/Python%20package/badge.svg)\n[![PyPI version shields.io](https://img.shields.io/pypi/v/pytorch-warmup.svg)](https://pypi.python.org/pypi/pytorch-warmup/)\n[![PyPI license](https://img.shields.io/pypi/l/pytorch-warmup.svg)](https://pypi.python.org/pypi/pytorch-warmup/)\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/pytorch-warmup.svg)](https://pypi.python.org/pypi/pytorch-warmup/)\n\n## Installation\n\nMake sure you have Python 3.6+ and PyTorch 1.1+. Then, run the following command:\n\n```\npython setup.py install\n```\n\nor\n\n```\npip install -U pytorch_warmup\n```\n\n## Usage\n\n### Sample Codes\n\nThe scheduled learning rate is dampened by the multiplication of the warmup factor:\n\n<p align=\"center\"><img src=\"https://github.com/Tony-Y/pytorch_warmup/raw/master/examples/emnist/figs/learning_rate.png\" alt=\"Learning rate\" width=\"400\"/></p>\n\n#### Approach 1\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Tony-Y/colab-notebooks/blob/master/PyTorch_Warmup_Approach1.ipynb)\n\nWhen the learning rate schedule uses the global iteration number, the untuned linear warmup can be used as follows:\n\n```python\nimport torch\nimport pytorch_warmup as warmup\n\noptimizer = torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), weight_decay=0.01)\nnum_steps = len(dataloader) * num_epochs\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\nwarmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\nfor epoch in range(1,num_epochs+1):\n    for batch in dataloader:\n        optimizer.zero_grad()\n        loss = ...\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        warmup_scheduler.dampen()\n```\n\n#### Approach 2\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Tony-Y/colab-notebooks/blob/master/PyTorch_Warmup_Approach2.ipynb)\n\nWhen the learning rate schedule uses the epoch number, the warmup schedule can be used as follows (for PyTorch 1.2 or above):\n\n```python\nlr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[num_epochs//3], gamma=0.1)\nwarmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\nwarmup_scheduler.last_step = -1 # initialize the step counter\nfor epoch in range(1,num_epochs+1):\n    for batch in dataloader:\n        lr_scheduler.step(epoch-1)\n        warmup_scheduler.dampen()\n        optimizer.zero_grad()\n        loss = ...\n        loss.backward()\n        optimizer.step()\n```\n\nThe user warning about calling `lr_scheduler.step()` before `optimizer.step()` may be ignored.\n\n### Warmup Schedules\n\n#### Manual Warmup\n\nThe warmup factor `w(t)` depends on the warmup period, which must manually be specified, for `LinearWarmup` and `ExponentialWarmup`.\n\n##### Linear\n\n`w(t) = min(1, t / warmup_period)`\n\n```python\nwarmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period=2000)\n```\n\n##### Exponential\n\n`w(t) = 1 - exp(-t / warmup_period)`\n\n```python\nwarmup_scheduler = warmup.ExponentialWarmup(optimizer, warmup_period=1000)\n```\n\n#### Untuned Warmup\n\nThe warmup period is given by a function of Adam's `beta2` parameter for `UntunedLinearWarmup` and `UntunedExponentialWarmup`.\n\n##### Linear\n\n`warmup_period = 2 / (1 - beta2)`\n\n```python\nwarmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n```\n\n##### Exponential\n\n`warmup_period = 1 / (1 - beta2)`\n\n```python\nwarmup_scheduler = warmup.UntunedExponentialWarmup(optimizer)\n```\n\n#### RAdam Warmup\n\nThe warmup factor depends on Adam's `beta2` parameter for `RAdamWarmup`. Please see the original paper for the details.\n\n```python\nwarmup_scheduler = warmup.RAdamWarmup(optimizer)\n```\n\n### Apex's Adam\n\nThe Apex library provides an Adam optimizer tuned for CUDA devices, [FusedAdam](https://nvidia.github.io/apex/optimizers.html#apex.optimizers.FusedAdam). The FusedAdam optimizer can be used with the warmup schedulers. For example:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Tony-Y/colab-notebooks/blob/master/PyTorch_Warmup_FusedAdam.ipynb)\n\n```python\noptimizer = apex.optimizers.FusedAdam(params, lr=0.001, betas=(0.9, 0.999), weight_decay=0.01)\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\nwarmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n```\n\n\n## License\n\nMIT License\n\nCopyright (c) 2019 Takenori Yamamoto\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Tony-Y/pytorch_warmup", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "pytorch-warmup", "package_url": "https://pypi.org/project/pytorch-warmup/", "platform": "", "project_url": "https://pypi.org/project/pytorch-warmup/", "project_urls": {"Homepage": "https://github.com/Tony-Y/pytorch_warmup"}, "release_url": "https://pypi.org/project/pytorch-warmup/0.0.4/", "requires_dist": ["torch (>=1.1)"], "requires_python": ">=3.6", "summary": "A PyTorch Extension for Learning Rate Warmup", "version": "0.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>A PyTorch Extension for Learning Rate Warmup</h1>\n<p>This library contains PyTorch implementations of the warmup schedules described in <a href=\"https://arxiv.org/abs/1910.04209\" rel=\"nofollow\">On the adequacy of untuned warmup for adaptive optimization</a>.</p>\n<p align=\"center\"><img alt=\"Warmup schedule\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1120c6af1fab9b30db7469ebc114e1936ef55ccc/68747470733a2f2f6769746875622e636f6d2f546f6e792d592f7079746f7263685f7761726d75702f7261772f6d61737465722f6578616d706c65732f706c6f74732f666967732f7761726d75705f7363686564756c652e706e67\" width=\"400\"></p>\n<p><img alt=\"Python package\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8b6674083e0f4b8875230c3027bc806c7b326e11/68747470733a2f2f6769746875622e636f6d2f546f6e792d592f7079746f7263685f7761726d75702f776f726b666c6f77732f507974686f6e2532307061636b6167652f62616467652e737667\">\n<a href=\"https://pypi.python.org/pypi/pytorch-warmup/\" rel=\"nofollow\"><img alt=\"PyPI version shields.io\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5f8bff30678b49a677a9530b13149dd8f52689ae/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7079746f7263682d7761726d75702e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/pytorch-warmup/\" rel=\"nofollow\"><img alt=\"PyPI license\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/81a94d18266d2aff7a4a61348df4c17a280dd774/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f7079746f7263682d7761726d75702e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/pytorch-warmup/\" rel=\"nofollow\"><img alt=\"PyPI pyversions\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bcbe9c4ebf4644953afeb83764dbcea0866b3910/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7079746f7263682d7761726d75702e737667\"></a></p>\n<h2>Installation</h2>\n<p>Make sure you have Python 3.6+ and PyTorch 1.1+. Then, run the following command:</p>\n<pre><code>python setup.py install\n</code></pre>\n<p>or</p>\n<pre><code>pip install -U pytorch_warmup\n</code></pre>\n<h2>Usage</h2>\n<h3>Sample Codes</h3>\n<p>The scheduled learning rate is dampened by the multiplication of the warmup factor:</p>\n<p align=\"center\"><img alt=\"Learning rate\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/cdb62053a353c9f40ef307fac45ad0a14689d05a/68747470733a2f2f6769746875622e636f6d2f546f6e792d592f7079746f7263685f7761726d75702f7261772f6d61737465722f6578616d706c65732f656d6e6973742f666967732f6c6561726e696e675f726174652e706e67\" width=\"400\"></p>\n<h4>Approach 1</h4>\n<p><a href=\"https://colab.research.google.com/github/Tony-Y/colab-notebooks/blob/master/PyTorch_Warmup_Approach1.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></p>\n<p>When the learning rate schedule uses the global iteration number, the untuned linear warmup can be used as follows:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pytorch_warmup</span> <span class=\"k\">as</span> <span class=\"nn\">warmup</span>\n\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">AdamW</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">,</span> <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">),</span> <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">)</span>\n<span class=\"n\">num_steps</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">num_epochs</span>\n<span class=\"n\">lr_scheduler</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">lr_scheduler</span><span class=\"o\">.</span><span class=\"n\">CosineAnnealingLR</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">T_max</span><span class=\"o\">=</span><span class=\"n\">num_steps</span><span class=\"p\">)</span>\n<span class=\"n\">warmup_scheduler</span> <span class=\"o\">=</span> <span class=\"n\">warmup</span><span class=\"o\">.</span><span class=\"n\">UntunedLinearWarmup</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">num_epochs</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">dataloader</span><span class=\"p\">:</span>\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n        <span class=\"n\">lr_scheduler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n        <span class=\"n\">warmup_scheduler</span><span class=\"o\">.</span><span class=\"n\">dampen</span><span class=\"p\">()</span>\n</pre>\n<h4>Approach 2</h4>\n<p><a href=\"https://colab.research.google.com/github/Tony-Y/colab-notebooks/blob/master/PyTorch_Warmup_Approach2.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></p>\n<p>When the learning rate schedule uses the epoch number, the warmup schedule can be used as follows (for PyTorch 1.2 or above):</p>\n<pre><span class=\"n\">lr_scheduler</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">lr_scheduler</span><span class=\"o\">.</span><span class=\"n\">MultiStepLR</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">milestones</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">num_epochs</span><span class=\"o\">//</span><span class=\"mi\">3</span><span class=\"p\">],</span> <span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n<span class=\"n\">warmup_scheduler</span> <span class=\"o\">=</span> <span class=\"n\">warmup</span><span class=\"o\">.</span><span class=\"n\">UntunedLinearWarmup</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n<span class=\"n\">warmup_scheduler</span><span class=\"o\">.</span><span class=\"n\">last_step</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">1</span> <span class=\"c1\"># initialize the step counter</span>\n<span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">num_epochs</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">dataloader</span><span class=\"p\">:</span>\n        <span class=\"n\">lr_scheduler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"n\">warmup_scheduler</span><span class=\"o\">.</span><span class=\"n\">dampen</span><span class=\"p\">()</span>\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p>The user warning about calling <code>lr_scheduler.step()</code> before <code>optimizer.step()</code> may be ignored.</p>\n<h3>Warmup Schedules</h3>\n<h4>Manual Warmup</h4>\n<p>The warmup factor <code>w(t)</code> depends on the warmup period, which must manually be specified, for <code>LinearWarmup</code> and <code>ExponentialWarmup</code>.</p>\n<h5>Linear</h5>\n<p><code>w(t) = min(1, t / warmup_period)</code></p>\n<pre><span class=\"n\">warmup_scheduler</span> <span class=\"o\">=</span> <span class=\"n\">warmup</span><span class=\"o\">.</span><span class=\"n\">LinearWarmup</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">warmup_period</span><span class=\"o\">=</span><span class=\"mi\">2000</span><span class=\"p\">)</span>\n</pre>\n<h5>Exponential</h5>\n<p><code>w(t) = 1 - exp(-t / warmup_period)</code></p>\n<pre><span class=\"n\">warmup_scheduler</span> <span class=\"o\">=</span> <span class=\"n\">warmup</span><span class=\"o\">.</span><span class=\"n\">ExponentialWarmup</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">warmup_period</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">)</span>\n</pre>\n<h4>Untuned Warmup</h4>\n<p>The warmup period is given by a function of Adam's <code>beta2</code> parameter for <code>UntunedLinearWarmup</code> and <code>UntunedExponentialWarmup</code>.</p>\n<h5>Linear</h5>\n<p><code>warmup_period = 2 / (1 - beta2)</code></p>\n<pre><span class=\"n\">warmup_scheduler</span> <span class=\"o\">=</span> <span class=\"n\">warmup</span><span class=\"o\">.</span><span class=\"n\">UntunedLinearWarmup</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n</pre>\n<h5>Exponential</h5>\n<p><code>warmup_period = 1 / (1 - beta2)</code></p>\n<pre><span class=\"n\">warmup_scheduler</span> <span class=\"o\">=</span> <span class=\"n\">warmup</span><span class=\"o\">.</span><span class=\"n\">UntunedExponentialWarmup</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n</pre>\n<h4>RAdam Warmup</h4>\n<p>The warmup factor depends on Adam's <code>beta2</code> parameter for <code>RAdamWarmup</code>. Please see the original paper for the details.</p>\n<pre><span class=\"n\">warmup_scheduler</span> <span class=\"o\">=</span> <span class=\"n\">warmup</span><span class=\"o\">.</span><span class=\"n\">RAdamWarmup</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n</pre>\n<h3>Apex's Adam</h3>\n<p>The Apex library provides an Adam optimizer tuned for CUDA devices, <a href=\"https://nvidia.github.io/apex/optimizers.html#apex.optimizers.FusedAdam\" rel=\"nofollow\">FusedAdam</a>. The FusedAdam optimizer can be used with the warmup schedulers. For example:</p>\n<p><a href=\"https://colab.research.google.com/github/Tony-Y/colab-notebooks/blob/master/PyTorch_Warmup_FusedAdam.ipynb\" rel=\"nofollow\"><img alt=\"Open In Colab\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/74d996556a82b2f1dd5252d2fd8bead60f9e9d21/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a></p>\n<pre><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">apex</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">FusedAdam</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">,</span> <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">),</span> <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">)</span>\n<span class=\"n\">lr_scheduler</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">lr_scheduler</span><span class=\"o\">.</span><span class=\"n\">CosineAnnealingLR</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">T_max</span><span class=\"o\">=</span><span class=\"n\">num_steps</span><span class=\"p\">)</span>\n<span class=\"n\">warmup_scheduler</span> <span class=\"o\">=</span> <span class=\"n\">warmup</span><span class=\"o\">.</span><span class=\"n\">UntunedLinearWarmup</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n</pre>\n<h2>License</h2>\n<p>MIT License</p>\n<p>Copyright (c) 2019 Takenori Yamamoto</p>\n\n          </div>"}, "last_serial": 6135448, "releases": {"0.0.3": [{"comment_text": "", "digests": {"md5": "4873556236e2195b56a25d91ec16144d", "sha256": "d10c8704b1e2adf3efa84480cc15572b70b05117b2417d4a8897524c011f4d28"}, "downloads": -1, "filename": "pytorch_warmup-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "4873556236e2195b56a25d91ec16144d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 4952, "upload_time": "2019-10-31T11:48:28", "upload_time_iso_8601": "2019-10-31T11:48:28.824167Z", "url": "https://files.pythonhosted.org/packages/39/8a/792ddf9bb209c71dc7ea753d3a2990d924a6c07eb2cc30de4cfacf8d3645/pytorch_warmup-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b1cbd80e5a5e1d85de3c95da2477ee4f", "sha256": "64c992276d454d8267bdfdf5060028c8aae121df4f45ed7241653c78e0aadc4f"}, "downloads": -1, "filename": "pytorch-warmup-0.0.3.tar.gz", "has_sig": false, "md5_digest": "b1cbd80e5a5e1d85de3c95da2477ee4f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 310448, "upload_time": "2019-10-31T11:48:32", "upload_time_iso_8601": "2019-10-31T11:48:32.807322Z", "url": "https://files.pythonhosted.org/packages/8c/ba/995c87140dc058ca664fa36df875c3596364c31b84193ffe7011089cef26/pytorch-warmup-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "7912af946575fdcf191ce1de924af2d8", "sha256": "38110440a51840120732844e1e312227bb787a2e675589c241484b95a6d515bc"}, "downloads": -1, "filename": "pytorch_warmup-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "7912af946575fdcf191ce1de924af2d8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 6478, "upload_time": "2019-11-14T08:51:10", "upload_time_iso_8601": "2019-11-14T08:51:10.763884Z", "url": "https://files.pythonhosted.org/packages/7a/22/2fb600a06a1d1b493d54ac8fa6c41e96870985992fc504104e0620bc2ea4/pytorch_warmup-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fa618189316894e7d780f028621b9bbb", "sha256": "9c630ac77a76f594492ffad8ccd5015aa5c946aee71525f867a86c88aaacd922"}, "downloads": -1, "filename": "pytorch-warmup-0.0.4.tar.gz", "has_sig": false, "md5_digest": "fa618189316894e7d780f028621b9bbb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 312831, "upload_time": "2019-11-14T08:51:14", "upload_time_iso_8601": "2019-11-14T08:51:14.250736Z", "url": "https://files.pythonhosted.org/packages/da/32/3a662cf200368441d4f8c34b6da4c2037fa767a5c6ab9a19b136899482a5/pytorch-warmup-0.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "7912af946575fdcf191ce1de924af2d8", "sha256": "38110440a51840120732844e1e312227bb787a2e675589c241484b95a6d515bc"}, "downloads": -1, "filename": "pytorch_warmup-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "7912af946575fdcf191ce1de924af2d8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 6478, "upload_time": "2019-11-14T08:51:10", "upload_time_iso_8601": "2019-11-14T08:51:10.763884Z", "url": "https://files.pythonhosted.org/packages/7a/22/2fb600a06a1d1b493d54ac8fa6c41e96870985992fc504104e0620bc2ea4/pytorch_warmup-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fa618189316894e7d780f028621b9bbb", "sha256": "9c630ac77a76f594492ffad8ccd5015aa5c946aee71525f867a86c88aaacd922"}, "downloads": -1, "filename": "pytorch-warmup-0.0.4.tar.gz", "has_sig": false, "md5_digest": "fa618189316894e7d780f028621b9bbb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 312831, "upload_time": "2019-11-14T08:51:14", "upload_time_iso_8601": "2019-11-14T08:51:14.250736Z", "url": "https://files.pythonhosted.org/packages/da/32/3a662cf200368441d4f8c34b6da4c2037fa767a5c6ab9a19b136899482a5/pytorch-warmup-0.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:13:30 2020"}