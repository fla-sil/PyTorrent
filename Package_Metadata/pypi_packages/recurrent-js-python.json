{"info": {"author": "Jonathan Raiman", "author_email": "jraiman at mit dot edu", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "Operating System :: OS Independent", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.3", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Mathematics"], "description": "# RecurrentJS in Python\n\nFollowing in the footsteps of [Andrej Kaparthy](http://cs.stanford.edu/people/karpathy/), here is a  Python implementation of [recurrentJS](http://cs.stanford.edu/people/karpathy/recurrentjs/) ([Github](https://github.com/karpathy/recurrentjs)).\n\n### Why ?\n\nWhile Python has great automatic differentiation libraries, a no-compile version is lacking. In particular recurrentJS makes great use of callbacks and garbage collection to enable backprop through time. In this implementation the goal is to reduce reliance on these abstractions and have a simple backprop step class. Finally if this is easily feasible, then ultimately we will implement a C++ version that will make the majority of the computation steps fast except for some intermediary allocations of backprop steps, but that do not make up the bulk of the computation, and keep the API and the syntax clear.\n\n### Planned extensions to Javascript version\n\nIn this implementation the goal is simple:\n\n* Move away from callbacks\n* Enable batch processing (through masked losses, tensors, and advanced indexing when plucking rows from matrices)\n* Form a baseline for implementation in non-scripting languages\n\n\n### Usage\n\nBelow we follow the same steps as in the character generation demo, and we import the same text for character model learning. Perplexity drops quickly to around 7-8, (mirroring the behavior found in the Javascript version).\n\n    from recurrentjs import *\n\n    input_size  = -1\n    output_size = -1\n    epoch_size  = -1\n    letter_size = 5\n    letterToIndex = {}\n    indexToLetter = {}\n    hidden_sizes = [20,20]\n    generator = \"lstm\"\n    vocab = []\n    regc = 0.000001 # L2 regularization strength\n    learning_rate = 0.01 # learning rate\n    clipval = 5.0\n\n    solver = Solver()\n\n    def initVocab(sents, count_threshold):\n        global input_size\n        global output_size\n        global epoch_size\n        global vocab\n        global letterToIndex\n        global indexToLetter\n        # count up all characters\n        d = {}\n        for sent in sents:\n            for c in sent:\n                if c in d:\n                    d[c] += 1\n                else:\n                    d[c] = 1\n\n        # filter by count threshold and create pointers\n        letterToIndex = {}\n        indexToLetter = {}\n        vocab = []\n        # NOTE: start at one because we will have START and END tokens!\n        # that is, START token will be index 0 in model letter vectors\n        # and END token will be index 0 in the next character softmax\n        q = 1\n        for ch in d.keys():\n            if d[ch] >= count_threshold:\n                # add character to vocab\n                letterToIndex[ch] = q\n                indexToLetter[q] = ch\n                vocab.append(ch)\n                q += 1\n        # globals written: indexToLetter, letterToIndex, vocab (list), and:\n        input_size  = len(vocab) + 1;\n        output_size = len(vocab) + 1;\n        epoch_size  = len(sents)\n\n    def forwardIndex(G, model, ix, prev):\n        x = G.row_pluck(model['Wil'], ix)\n        # forward prop the sequence learner\n        if generator == \"rnn\":\n            out_struct = forwardRNN(G, model, hidden_sizes, x, prev)\n        else:\n            out_struct = forwardLSTM(G, model, hidden_sizes, x, prev)   \n        return out_struct\n\n    def initModel():\n        model = {}\n        lstm = initLSTM(letter_size, hidden_sizes, output_size) if generator == \"lstm\" else initRNN(letter_size, hidden_sizes, output_size)\n        model['Wil'] = RandMat(input_size, letter_size , 0.08)\n        model.update(lstm)\n\n        return model\n\n    def costfun(model, sent):\n        # takes a model and a sentence and\n        # calculates the loss. Also returns the Graph\n        # object which can be used to do backprop\n        n = len(sent)\n        G = Graph()\n        log2ppl = 0.0;\n        cost = 0.0;\n        prev = None\n        for i in range(-1, n):\n            # start and end tokens are zeros\n            ix_source = 0 if i == -1 else letterToIndex[sent[i]] # first step: start with START token\n            ix_target = 0 if i == n-1 else letterToIndex[sent[i+1]] # last step: end with END token\n\n            lh = forwardIndex(G, model, ix_source, prev)\n            prev = lh\n\n            # set gradients into logprobabilities\n            logprobs = lh.output # interpret output as logprobs\n            probs = softmax(logprobs) # compute the softmax probabilities\n\n            log2ppl += -np.log(probs.w[ix_target,0]) # accumulate base 2 log prob and do smoothing\n            cost += -np.log(probs.w[ix_target,0])\n\n            # write gradients into log probabilities\n            logprobs.dw = probs.w\n            logprobs.dw[ix_target] -= 1\n\n        ppl = np.power(2, log2ppl / (n - 1))\n\n        return G, ppl, cost\n\n    text_data = open(\"paulgraham_text.txt\", \"rt\").readlines()\n    initVocab(text_data, 1)\n    model = initModel()\n    ppl_list = []\n    median_ppl = []\n    tick_iter = 0\n\n    def tick():\n        global tick_iter\n        global ppl_list\n        global median_ppl\n        sentix = np.random.randint(0, len(text_data))\n        sent = text_data[sentix]\n        G, ppl, cost = costfun(model, sent)\n        G.backward()\n        solver.step(model, learning_rate, regc, clipval)\n        ppl_list.append(ppl)\n        tick_iter += 1\n\n        if tick_iter % 100 == 0:\n            median = np.median(ppl_list)\n            ppl_list = []\n            median_ppl.append(median)\n            \n  \nAnd the training loop (no fancy prediction and sampling implemented here, but fairly straightforward conversion from the javascript code)\n  \n    for i in range(1000):\n        tick()", "description_content_type": null, "docs_url": null, "download_url": "https://github.com/JonathanRaiman/recurrentjs", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/JonathanRaiman/recurrentjs", "keywords": "Gradient Descent,LSTM,neural networks,automatic differentiation", "license": "MIT", "maintainer": null, "maintainer_email": null, "name": "recurrent-js-python", "package_url": "https://pypi.org/project/recurrent-js-python/", "platform": "any", "project_url": "https://pypi.org/project/recurrent-js-python/", "project_urls": {"Download": "https://github.com/JonathanRaiman/recurrentjs", "Homepage": "https://github.com/JonathanRaiman/recurrentjs"}, "release_url": "https://pypi.org/project/recurrent-js-python/0.0.1/", "requires_dist": null, "requires_python": null, "summary": "Python implementation of Andrej Kaparthy's recurrentjs library", "version": "0.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            # RecurrentJS in Python<br><br>Following in the footsteps of [Andrej Kaparthy](http://cs.stanford.edu/people/karpathy/), here is a  Python implementation of [recurrentJS](http://cs.stanford.edu/people/karpathy/recurrentjs/) ([Github](https://github.com/karpathy/recurrentjs)).<br><br>### Why ?<br><br>While Python has great automatic differentiation libraries, a no-compile version is lacking. In particular recurrentJS makes great use of callbacks and garbage collection to enable backprop through time. In this implementation the goal is to reduce reliance on these abstractions and have a simple backprop step class. Finally if this is easily feasible, then ultimately we will implement a C++ version that will make the majority of the computation steps fast except for some intermediary allocations of backprop steps, but that do not make up the bulk of the computation, and keep the API and the syntax clear.<br><br>### Planned extensions to Javascript version<br><br>In this implementation the goal is simple:<br><br>* Move away from callbacks<br>* Enable batch processing (through masked losses, tensors, and advanced indexing when plucking rows from matrices)<br>* Form a baseline for implementation in non-scripting languages<br><br><br>### Usage<br><br>Below we follow the same steps as in the character generation demo, and we import the same text for character model learning. Perplexity drops quickly to around 7-8, (mirroring the behavior found in the Javascript version).<br><br>    from recurrentjs import *<br><br>    input_size  = -1<br>    output_size = -1<br>    epoch_size  = -1<br>    letter_size = 5<br>    letterToIndex = {}<br>    indexToLetter = {}<br>    hidden_sizes = [20,20]<br>    generator = \"lstm\"<br>    vocab = []<br>    regc = 0.000001 # L2 regularization strength<br>    learning_rate = 0.01 # learning rate<br>    clipval = 5.0<br><br>    solver = Solver()<br><br>    def initVocab(sents, count_threshold):<br>        global input_size<br>        global output_size<br>        global epoch_size<br>        global vocab<br>        global letterToIndex<br>        global indexToLetter<br>        # count up all characters<br>        d = {}<br>        for sent in sents:<br>            for c in sent:<br>                if c in d:<br>                    d[c] += 1<br>                else:<br>                    d[c] = 1<br><br>        # filter by count threshold and create pointers<br>        letterToIndex = {}<br>        indexToLetter = {}<br>        vocab = []<br>        # NOTE: start at one because we will have START and END tokens!<br>        # that is, START token will be index 0 in model letter vectors<br>        # and END token will be index 0 in the next character softmax<br>        q = 1<br>        for ch in d.keys():<br>            if d[ch] &gt;= count_threshold:<br>                # add character to vocab<br>                letterToIndex[ch] = q<br>                indexToLetter[q] = ch<br>                vocab.append(ch)<br>                q += 1<br>        # globals written: indexToLetter, letterToIndex, vocab (list), and:<br>        input_size  = len(vocab) + 1;<br>        output_size = len(vocab) + 1;<br>        epoch_size  = len(sents)<br><br>    def forwardIndex(G, model, ix, prev):<br>        x = G.row_pluck(model['Wil'], ix)<br>        # forward prop the sequence learner<br>        if generator == \"rnn\":<br>            out_struct = forwardRNN(G, model, hidden_sizes, x, prev)<br>        else:<br>            out_struct = forwardLSTM(G, model, hidden_sizes, x, prev)   <br>        return out_struct<br><br>    def initModel():<br>        model = {}<br>        lstm = initLSTM(letter_size, hidden_sizes, output_size) if generator == \"lstm\" else initRNN(letter_size, hidden_sizes, output_size)<br>        model['Wil'] = RandMat(input_size, letter_size , 0.08)<br>        model.update(lstm)<br><br>        return model<br><br>    def costfun(model, sent):<br>        # takes a model and a sentence and<br>        # calculates the loss. Also returns the Graph<br>        # object which can be used to do backprop<br>        n = len(sent)<br>        G = Graph()<br>        log2ppl = 0.0;<br>        cost = 0.0;<br>        prev = None<br>        for i in range(-1, n):<br>            # start and end tokens are zeros<br>            ix_source = 0 if i == -1 else letterToIndex[sent[i]] # first step: start with START token<br>            ix_target = 0 if i == n-1 else letterToIndex[sent[i+1]] # last step: end with END token<br><br>            lh = forwardIndex(G, model, ix_source, prev)<br>            prev = lh<br><br>            # set gradients into logprobabilities<br>            logprobs = lh.output # interpret output as logprobs<br>            probs = softmax(logprobs) # compute the softmax probabilities<br><br>            log2ppl += -np.log(probs.w[ix_target,0]) # accumulate base 2 log prob and do smoothing<br>            cost += -np.log(probs.w[ix_target,0])<br><br>            # write gradients into log probabilities<br>            logprobs.dw = probs.w<br>            logprobs.dw[ix_target] -= 1<br><br>        ppl = np.power(2, log2ppl / (n - 1))<br><br>        return G, ppl, cost<br><br>    text_data = open(\"paulgraham_text.txt\", \"rt\").readlines()<br>    initVocab(text_data, 1)<br>    model = initModel()<br>    ppl_list = []<br>    median_ppl = []<br>    tick_iter = 0<br><br>    def tick():<br>        global tick_iter<br>        global ppl_list<br>        global median_ppl<br>        sentix = np.random.randint(0, len(text_data))<br>        sent = text_data[sentix]<br>        G, ppl, cost = costfun(model, sent)<br>        G.backward()<br>        solver.step(model, learning_rate, regc, clipval)<br>        ppl_list.append(ppl)<br>        tick_iter += 1<br><br>        if tick_iter % 100 == 0:<br>            median = np.median(ppl_list)<br>            ppl_list = []<br>            median_ppl.append(median)<br>            <br>  <br>And the training loop (no fancy prediction and sampling implemented here, but fairly straightforward conversion from the javascript code)<br>  <br>    for i in range(1000):<br>        tick()\n          </div>"}, "last_serial": 1380923, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "34f503818e7835aaeb622bc40d56883f", "sha256": "982390e084fa44672f1f9c0117f132203b58d76ff146a88531c31046339004db"}, "downloads": -1, "filename": "recurrent-js-python-0.0.1.tar.gz", "has_sig": false, "md5_digest": "34f503818e7835aaeb622bc40d56883f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39529, "upload_time": "2015-01-13T15:22:19", "upload_time_iso_8601": "2015-01-13T15:22:19.778614Z", "url": "https://files.pythonhosted.org/packages/69/c8/e2c971cd06d4ffa982932c4641ad5270c6bb6f32c5782be03c8ddadc867e/recurrent-js-python-0.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "34f503818e7835aaeb622bc40d56883f", "sha256": "982390e084fa44672f1f9c0117f132203b58d76ff146a88531c31046339004db"}, "downloads": -1, "filename": "recurrent-js-python-0.0.1.tar.gz", "has_sig": false, "md5_digest": "34f503818e7835aaeb622bc40d56883f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39529, "upload_time": "2015-01-13T15:22:19", "upload_time_iso_8601": "2015-01-13T15:22:19.778614Z", "url": "https://files.pythonhosted.org/packages/69/c8/e2c971cd06d4ffa982932c4641ad5270c6bb6f32c5782be03c8ddadc867e/recurrent-js-python-0.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:06:05 2020"}