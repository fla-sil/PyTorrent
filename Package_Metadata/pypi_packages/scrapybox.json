{"info": {"author": "Steven Almeroth", "author_email": "sroth77@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Environment :: Console", "Environment :: Web Environment", "Framework :: Scrapy", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Topic :: Internet :: WWW/HTTP", "Topic :: Internet :: WWW/HTTP :: Browsers", "Topic :: Internet :: WWW/HTTP :: HTTP Servers", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Application Frameworks", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "README\n\nScrapybox - a Scrapy GUI\n------------------------\n\nA RESTful async Python web server that runs arbitrary code within Scrapy spiders\nvia an HTML webapge interface.\n\n1. Server receives POST request:\n\n[26/Mar/2016:21:50:38 +0000] \"POST /api/eval HTTP/1.1\" 200 3172 \"-\" \"Mozilla/5.0 (X11; Linux...\"\n\n2. Server uses Scrapy to crawl site:\n\n2016-03-26 14:50:34 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats', 'scrapy.extensions.corestats.CoreStats']\n2016-03-26 14:50:34 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-03-26 14:50:34 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-03-26 14:50:34 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-03-26 14:50:34 [scrapy] INFO: Spider opened\n2016-03-26 14:50:34 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-03-26 14:50:37 [scrapy] DEBUG: Crawled (404) <GET http://scrapy.org/robots.txt> (referer: None)\n2016-03-26 14:50:38 [scrapy] DEBUG: Crawled (200) <GET http://scrapy.org> (referer: None)\n2016-03-26 14:50:38 [scrapy] DEBUG: Scraped from <200 http://scrapy.org>\n{'request': {'encoding': 'utf-8', 'cookies': {}, 'headers': {'User-Agent': ['Mozilla/5.0 (X11; Linux...\n2016-03-26 14:50:38 [scrapy] INFO: Closing spider (finished)\n2016-03-26 14:50:38 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 494,\n 'downloader/request_count': 2,\n 'downloader/request_method_count/GET': 2,\n 'downloader/response_bytes': 10719,\n 'downloader/response_count': 2,\n 'downloader/response_status_count/200': 1,\n 'downloader/response_status_count/404': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 3, 26, 21, 50, 38, 238364),\n 'item_scraped_count': 1,\n 'log_count/DEBUG': 3,\n 'log_count/INFO': 7,\n 'response_received_count': 2,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2016, 3, 26, 21, 50, 34, 479901)}\n2016-03-26 14:50:38 [scrapy] INFO: Spider closed (finished)\n2016-03-26 14:50:38 [aiohttp] INFO: 127.0.0.1 - [26/Mar/2016:21:50:38] \"POST /api/eval HTTP/1.1\" 200...\"\n\n\n3. JSON response is sent back to user:\n\n{\n    items: [\n        {\n            request: {\n                encoding: \"utf-8\",\n                cookies: { },\n                headers: {\n                    User-Agent: [\n                        \"Mozilla/5.0 (X11; Linux x86_64) Scrapybox/0.1 Scrapy/1.2 Python/3.5\"\n                    ],\n                    Accept: [\n                        \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n                    ],\n                    Accept-Language: [\n                        \"en\"\n                    ],\n                    Accept-Encoding: [\n                        \"gzip,deflate\"\n                    ]\n                },\n                url: \"http://scrapy.org\",\n                method: \"GET\",\n                meta: {\n                    download_slot: \"scrapy.org\",\n                    download_latency: 0.6338846683502197,\n                    download_timeout: 180,\n                    depth: 0\n                }\n            },\n            response: {\n                url: \"http://scrapy.org\",\n                flags: [ ],\n                headers: {\n                    Content-Type: [\n                        \"text/html; charset=utf-8\"\n                    ],\n                    Cache-Control: [\n                        \"max-age=600\"\n                    ],\n                    Last-Modified: [\n                        \"Thu, 03 Mar 2016 10:37:55 GMT\"\n                    ],\n                    X-Github-Request-Id: [\n                        \"BDADB31E:7CE4:240796FC:56F7042B\"\n                    ],\n                    Server: [\n                        \"GitHub.com\"\n                    ],\n                    Expires: [\n                        \"Sat, 26 Mar 2016 22:00:37 GMT\"\n                    ],\n                    Date: [\n                        \"Sat, 26 Mar 2016 21:50:37 GMT\"\n                    ],\n                    Access-Control-Allow-Origin: [\n                        \"*\"\n                    ]\n                },\n                body: {\n                    title: \"Scrapy | A Fast and Powerful Scraping and Web Crawling Framework\",\n                    head: \"<head> <meta charset=\"utf-8\"> <title>Scrapy | A Fast and Powerful Scraping...\"\n                },\n                meta: {\n                    download_slot: \"scrapy.org\",\n                    download_latency: 0.6338846683502197,\n                    download_timeout: 180,\n                    depth: 0\n                },\n                status: 200\n            }\n        }\n    ],\n    scrapybox: {\n        request: {\n            cookies: { },\n            version: \"HttpVersion(major=1, minor=1)\",\n            headers: {\n                CONTENT-LENGTH: \"170\",\n                HOST: \"localhost:8080\",\n                ORIGIN: \"null\",\n                CACHE-CONTROL: \"max-age=0\",\n                UPGRADE-INSECURE-REQUESTS: \"1\",\n                ACCEPT-LANGUAGE: \"en-US,en;q=0.8\",\n                ACCEPT-ENCODING: \"gzip, deflate\",\n                CONTENT-TYPE: \"application/x-www-form-urlencoded\",\n                ACCEPT: \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n                CONNECTION: \"keep-alive\",\n                USER-AGENT: \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)...\"\n            },\n            path: \"/api/eval\",\n            method: \"POST\",\n            scheme: \"http\",\n            POST: {\n                settings.ROBOTSTXT_OBEY: \"on\",\n                settings.USER_AGENT: \"Mozilla/5.0 (X11; Linux x86_64) Scrapybox/0.1 Scrapy/1.2 Python/3.5\",\n                parse: \"\",\n                yield_item: \"on\",\n                start_url: \"scrapy.org\"\n            },\n            payload: [ ],\n            has_body: true,\n            query: \"\",\n            content: [ ],\n            host: \"localhost:8080\"\n        }\n    }\n}\n\n\nCommand line interface\n----------------------\n\n$ curl \"http://127.0.0.1:8080/api/eval/response.status\"\n200\n\n$ curl \"http://127.0.0.1:8080/api/eval/response.headers\"\n{b'Last-Modified': [b'Fri, 09 Aug 2013 23:54:35 GMT'], b'Etag': [b'\"359670651+gzip\"'], b'X-Cache': [b'HIT'],...}\n\n$ curl \"http://127.0.0.1:8080/api/eval/response\"\n<200 http://www.example.com/>\n\n$ curl \"http://127.0.0.1:8080/api/eval/self\"\n<Spider 'parse' at 0x7f3a3af2fb38>\n\n$ curl \"http://127.0.0.1:8080/api/eval/self.__dict__\"\n{'settings': <scrapy.settings.Settings object at 0x7f3a3af47e10>, 'result': {...}, 'crawler': <scrapy.crawler...>}\n\n$ curl localhost:8080/api/eval -d start_urls=[\\\"http://scrapinghub.com\\\",\\\"http://scrapy.org\\\"]\nCrawled responses: [<200 http://scrapinghub.com/>, <200 http://scrapy.org>]\n\n\nRequirements\n------------\n\nPython 3.5.0+\n\n.. seealso:: requirements.txt\n\nLicense\n-------\n\nBSD licensed", "description_content_type": null, "docs_url": null, "download_url": "https://github.com/stav/scrapybox/archive/master.zip", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/stav/scrapybox", "keywords": "scrapy,async,server", "license": "BSD", "maintainer": null, "maintainer_email": null, "name": "scrapybox", "package_url": "https://pypi.org/project/scrapybox/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/scrapybox/", "project_urls": {"Download": "https://github.com/stav/scrapybox/archive/master.zip", "Homepage": "https://github.com/stav/scrapybox"}, "release_url": "https://pypi.org/project/scrapybox/0.1/", "requires_dist": null, "requires_python": null, "summary": "A Scrapy GUI", "version": "0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            README<br><br>Scrapybox - a Scrapy GUI<br>------------------------<br><br>A RESTful async Python web server that runs arbitrary code within Scrapy spiders<br>via an HTML webapge interface.<br><br>1. Server receives POST request:<br><br>[26/Mar/2016:21:50:38 +0000] \"POST /api/eval HTTP/1.1\" 200 3172 \"-\" \"Mozilla/5.0 (X11; Linux...\"<br><br>2. Server uses Scrapy to crawl site:<br><br>2016-03-26 14:50:34 [scrapy] INFO: Enabled extensions:<br>['scrapy.extensions.logstats.LogStats', 'scrapy.extensions.corestats.CoreStats']<br>2016-03-26 14:50:34 [scrapy] INFO: Enabled downloader middlewares:<br>['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',<br> 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',<br> 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',<br> 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',<br> 'scrapy.downloadermiddlewares.retry.RetryMiddleware',<br> 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',<br> 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',<br> 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',<br> 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',<br> 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',<br> 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',<br> 'scrapy.downloadermiddlewares.stats.DownloaderStats']<br>2016-03-26 14:50:34 [scrapy] INFO: Enabled spider middlewares:<br>['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',<br> 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',<br> 'scrapy.spidermiddlewares.referer.RefererMiddleware',<br> 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',<br> 'scrapy.spidermiddlewares.depth.DepthMiddleware']<br>2016-03-26 14:50:34 [scrapy] INFO: Enabled item pipelines:<br>[]<br>2016-03-26 14:50:34 [scrapy] INFO: Spider opened<br>2016-03-26 14:50:34 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)<br>2016-03-26 14:50:37 [scrapy] DEBUG: Crawled (404) &lt;GET http://scrapy.org/robots.txt&gt; (referer: None)<br>2016-03-26 14:50:38 [scrapy] DEBUG: Crawled (200) &lt;GET http://scrapy.org&gt; (referer: None)<br>2016-03-26 14:50:38 [scrapy] DEBUG: Scraped from &lt;200 http://scrapy.org&gt;<br>{'request': {'encoding': 'utf-8', 'cookies': {}, 'headers': {'User-Agent': ['Mozilla/5.0 (X11; Linux...<br>2016-03-26 14:50:38 [scrapy] INFO: Closing spider (finished)<br>2016-03-26 14:50:38 [scrapy] INFO: Dumping Scrapy stats:<br>{'downloader/request_bytes': 494,<br> 'downloader/request_count': 2,<br> 'downloader/request_method_count/GET': 2,<br> 'downloader/response_bytes': 10719,<br> 'downloader/response_count': 2,<br> 'downloader/response_status_count/200': 1,<br> 'downloader/response_status_count/404': 1,<br> 'finish_reason': 'finished',<br> 'finish_time': datetime.datetime(2016, 3, 26, 21, 50, 38, 238364),<br> 'item_scraped_count': 1,<br> 'log_count/DEBUG': 3,<br> 'log_count/INFO': 7,<br> 'response_received_count': 2,<br> 'scheduler/dequeued': 1,<br> 'scheduler/dequeued/memory': 1,<br> 'scheduler/enqueued': 1,<br> 'scheduler/enqueued/memory': 1,<br> 'start_time': datetime.datetime(2016, 3, 26, 21, 50, 34, 479901)}<br>2016-03-26 14:50:38 [scrapy] INFO: Spider closed (finished)<br>2016-03-26 14:50:38 [aiohttp] INFO: 127.0.0.1 - [26/Mar/2016:21:50:38] \"POST /api/eval HTTP/1.1\" 200...\"<br><br><br>3. JSON response is sent back to user:<br><br>{<br>    items: [<br>        {<br>            request: {<br>                encoding: \"utf-8\",<br>                cookies: { },<br>                headers: {<br>                    User-Agent: [<br>                        \"Mozilla/5.0 (X11; Linux x86_64) Scrapybox/0.1 Scrapy/1.2 Python/3.5\"<br>                    ],<br>                    Accept: [<br>                        \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"<br>                    ],<br>                    Accept-Language: [<br>                        \"en\"<br>                    ],<br>                    Accept-Encoding: [<br>                        \"gzip,deflate\"<br>                    ]<br>                },<br>                url: \"http://scrapy.org\",<br>                method: \"GET\",<br>                meta: {<br>                    download_slot: \"scrapy.org\",<br>                    download_latency: 0.6338846683502197,<br>                    download_timeout: 180,<br>                    depth: 0<br>                }<br>            },<br>            response: {<br>                url: \"http://scrapy.org\",<br>                flags: [ ],<br>                headers: {<br>                    Content-Type: [<br>                        \"text/html; charset=utf-8\"<br>                    ],<br>                    Cache-Control: [<br>                        \"max-age=600\"<br>                    ],<br>                    Last-Modified: [<br>                        \"Thu, 03 Mar 2016 10:37:55 GMT\"<br>                    ],<br>                    X-Github-Request-Id: [<br>                        \"BDADB31E:7CE4:240796FC:56F7042B\"<br>                    ],<br>                    Server: [<br>                        \"GitHub.com\"<br>                    ],<br>                    Expires: [<br>                        \"Sat, 26 Mar 2016 22:00:37 GMT\"<br>                    ],<br>                    Date: [<br>                        \"Sat, 26 Mar 2016 21:50:37 GMT\"<br>                    ],<br>                    Access-Control-Allow-Origin: [<br>                        \"*\"<br>                    ]<br>                },<br>                body: {<br>                    title: \"Scrapy | A Fast and Powerful Scraping and Web Crawling Framework\",<br>                    head: \"&lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;Scrapy | A Fast and Powerful Scraping...\"<br>                },<br>                meta: {<br>                    download_slot: \"scrapy.org\",<br>                    download_latency: 0.6338846683502197,<br>                    download_timeout: 180,<br>                    depth: 0<br>                },<br>                status: 200<br>            }<br>        }<br>    ],<br>    scrapybox: {<br>        request: {<br>            cookies: { },<br>            version: \"HttpVersion(major=1, minor=1)\",<br>            headers: {<br>                CONTENT-LENGTH: \"170\",<br>                HOST: \"localhost:8080\",<br>                ORIGIN: \"null\",<br>                CACHE-CONTROL: \"max-age=0\",<br>                UPGRADE-INSECURE-REQUESTS: \"1\",<br>                ACCEPT-LANGUAGE: \"en-US,en;q=0.8\",<br>                ACCEPT-ENCODING: \"gzip, deflate\",<br>                CONTENT-TYPE: \"application/x-www-form-urlencoded\",<br>                ACCEPT: \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",<br>                CONNECTION: \"keep-alive\",<br>                USER-AGENT: \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)...\"<br>            },<br>            path: \"/api/eval\",<br>            method: \"POST\",<br>            scheme: \"http\",<br>            POST: {<br>                settings.ROBOTSTXT_OBEY: \"on\",<br>                settings.USER_AGENT: \"Mozilla/5.0 (X11; Linux x86_64) Scrapybox/0.1 Scrapy/1.2 Python/3.5\",<br>                parse: \"\",<br>                yield_item: \"on\",<br>                start_url: \"scrapy.org\"<br>            },<br>            payload: [ ],<br>            has_body: true,<br>            query: \"\",<br>            content: [ ],<br>            host: \"localhost:8080\"<br>        }<br>    }<br>}<br><br><br>Command line interface<br>----------------------<br><br>$ curl \"http://127.0.0.1:8080/api/eval/response.status\"<br>200<br><br>$ curl \"http://127.0.0.1:8080/api/eval/response.headers\"<br>{b'Last-Modified': [b'Fri, 09 Aug 2013 23:54:35 GMT'], b'Etag': [b'\"359670651+gzip\"'], b'X-Cache': [b'HIT'],...}<br><br>$ curl \"http://127.0.0.1:8080/api/eval/response\"<br>&lt;200 http://www.example.com/&gt;<br><br>$ curl \"http://127.0.0.1:8080/api/eval/self\"<br>&lt;Spider 'parse' at 0x7f3a3af2fb38&gt;<br><br>$ curl \"http://127.0.0.1:8080/api/eval/self.__dict__\"<br>{'settings': &lt;scrapy.settings.Settings object at 0x7f3a3af47e10&gt;, 'result': {...}, 'crawler': &lt;scrapy.crawler...&gt;}<br><br>$ curl localhost:8080/api/eval -d start_urls=[\\\"http://scrapinghub.com\\\",\\\"http://scrapy.org\\\"]<br>Crawled responses: [&lt;200 http://scrapinghub.com/&gt;, &lt;200 http://scrapy.org&gt;]<br><br><br>Requirements<br>------------<br><br>Python 3.5.0+<br><br>.. seealso:: requirements.txt<br><br>License<br>-------<br><br>BSD licensed\n          </div>"}, "last_serial": 2032349, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "b52c56ae482532b93cf2b77680ba11dd", "sha256": "89f3125b5dcb57032aa9d79a757c5feffb0bca50ea4091b17b59dabfa4c33390"}, "downloads": -1, "filename": "scrapybox-0.1.tar.gz", "has_sig": false, "md5_digest": "b52c56ae482532b93cf2b77680ba11dd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12025, "upload_time": "2016-03-28T18:08:36", "upload_time_iso_8601": "2016-03-28T18:08:36.449583Z", "url": "https://files.pythonhosted.org/packages/42/46/06af4abd325a896ef0e2666286e6e03d72480ebfe5f9a9f2f9e2b5aca678/scrapybox-0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b52c56ae482532b93cf2b77680ba11dd", "sha256": "89f3125b5dcb57032aa9d79a757c5feffb0bca50ea4091b17b59dabfa4c33390"}, "downloads": -1, "filename": "scrapybox-0.1.tar.gz", "has_sig": false, "md5_digest": "b52c56ae482532b93cf2b77680ba11dd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12025, "upload_time": "2016-03-28T18:08:36", "upload_time_iso_8601": "2016-03-28T18:08:36.449583Z", "url": "https://files.pythonhosted.org/packages/42/46/06af4abd325a896ef0e2666286e6e03d72480ebfe5f9a9f2f9e2b5aca678/scrapybox-0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:49 2020"}