{"info": {"author": "Neal Wong", "author_email": "qwang16@olivetuniversity.edu", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: Implementation :: CPython", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Utilities"], "description": "Proxy Scrape\n============\n\n.. image:: https://img.shields.io/travis/JaredLGillespie/proxyscrape.svg\n    :alt: Travis\n    :target: https://travis-ci.org/JaredLGillespie/proxyscrape\n.. image:: https://img.shields.io/coveralls/github/JaredLGillespie/proxyscrape.svg\n    :alt: Coveralls github\n    :target: https://coveralls.io/github/JaredLGillespie/proxyscrape\n.. image:: https://img.shields.io/pypi/v/proxyscrape.svg\n    :alt: PyPI\n    :target: https://pypi.org/project/proxyscrape/\n.. image:: https://img.shields.io/pypi/wheel/proxyscrape.svg\n    :alt: PyPI - Wheel\n    :target: https://pypi.org/project/proxyscrape/\n.. image:: https://img.shields.io/pypi/pyversions/proxyscrape.svg\n    :alt: PyPI - Python Version\n    :target: https://pypi.org/project/proxyscrape/\n.. image:: https://img.shields.io/pypi/l/proxyscrape.svg\n    :alt: PyPI - License\n    :target: https://pypi.org/project/proxyscrape/\n\nA library for retrieving free proxies (HTTP, HTTPS, SOCKS4, SOCKS5).\n\n*NOTE: This library isn't designed for production use. It's advised to use your own proxies or purchase a service which\nprovides an API. These are merely free ones that are retrieved from sites and should only be used for development\nor testing purposes.*\n\n.. code-block:: python\n\n    import proxyscrape\n\n    collector = proxyscrape.create_collector('default', 'http')  # Create a collector for http resources\n    proxy = collector.get_proxy({'country': 'united states'})  # Retrieve a united states proxy\n\nInstallation\n------------\n\nThe latest version of proxyscrape is available via ``pip``:\n\n.. code-block:: bash\n\n    $ pip install proxyscrape23\n\nAlternatively, you can download and install from source:\n\n.. code-block:: bash\n\n    $ python setup.py install\n\nProvided Proxies\n----------------\nCurrent proxies provided are scraped from various sites which offer free HTTP, HTTPS, SOCKS4, and SOCKS5 proxies; and\ndon't require headless browsers or selenium to retrieve. The list of sites proxies retrieved are shown below.\n\n+--------------------+----------------+--------------------------------------------------+\n| resource           | resource type  | url                                              |\n+====================+================+==================================================+\n| anonymous-proxy    | http, https    | https://free-proxy-list.net/anonymous-proxy.html |\n+--------------------+----------------+--------------------------------------------------+\n| free-proxy-list    | http, https    | https://free-proxy-list.net                      |\n+--------------------+----------------+--------------------------------------------------+\n| proxy-daily-http   | http           | http://www.proxy-daily.com                       |\n| proxy-daily-socks4 | socks4         |                                                  |\n| proxy-daily-socks5 | socks5         |                                                  |\n+--------------------+----------------+--------------------------------------------------+\n| socks-proxy        | socks4, socks5 | https://www.socks-proxy.net                      |\n+--------------------+----------------+--------------------------------------------------+\n| ssl-proxy          | https          | https://www.sslproxies.org                       |\n+--------------------+----------------+--------------------------------------------------+\n| uk-proxy           | http, https    | https://free-proxy-list.net/uk-proxy.html        |\n+--------------------+----------------+--------------------------------------------------+\n| us-proxy           | http, https    | https://www.us-proxy.org                         |\n+--------------------+----------------+--------------------------------------------------+\n\nGetting Started\n---------------\n\nProxy Scrape is a library aimed at providing an efficient an easy means of retrieving proxies for web-scraping\npurposes. The proxies retrieved are available from sites providing free proxies. The proxies provided, as shown in the\nabove table, can be of one of the following types (referred to as a `resource type`): http, https, socks4, and socks5.\n\nCollectors\n^^^^^^^^^^\nCollectors serve as the interface to retrieving proxies. They are instantiating at module-level and can be retrieved\nand re-used in different parts of the application (similar to the Python `logging` library). Collectors can be created\nand retrieved via the `create_collector(...)` and `get_collector(...)` functions.\n\n.. code-block:: python\n\n    from proxyscrape import create_collector, get_collector\n\n    collector = create_collector('my-collector', ['socks4', 'socks5'])\n\n    # Some other section of code\n    collector = get_collector('my-collector')\n\nEach collector should have a unique name and be initialized only once. Typically, only a single collector of a given\nresource type should be utilized. Filters can then be applied to the proxies if specific criteria is desired.\n\nWhen given one or more resources, the collector will use those to retrieve proxies. If one or more resource types\nare given, the resources for each of the types will be used to retrieve proxies.\n\nOnce created, proxies can be retrieved via the `get_proxy(...)` function. This optionally takes a `filter_opts`\nparameter which can filter by the following:\n- ``code`` (us, ca, ...)\n- ``country`` (united states, canada, ...)\n- ``anonymous`` (True, False)\n- ``type`` (http, https, socks4, socks5, ...)\n\n.. code-block:: python\n\n    from proxyscrape import create_collector\n\n    collector = create_collector('my-collector', 'http')\n\n    # Retrieve any http proxy\n    proxy = collector.get_proxy()\n\n    # Retrieve only 'us' proxies\n    proxy = collector.get_proxy({'code': 'us'})\n\n    # Retrieve only anonymous 'uk' or 'us' proxies\n    proxy = collector.get_proxy({'code': ('us', 'uk'), 'anonymous': True})\n\nFilters can be applied to every proxy retrieval from the collector via `apply_filter(...)`. This is useful when the same\nfilter is expected for any proxy retrieved.\n\n.. code-block:: python\n\n    from proxyscrape import create_collector\n\n    collector = create_collector('my-collector', 'http')\n\n    # Only retrieve 'uk' and 'us' proxies\n    collector.apply_filter({'code': 'us'})\n\n    # Filtered proxies\n    proxy = collector.get_proxy()\n\n    # Clear filter\n    collector.clear_filter()\n\nNote that some filters may instead use specific resources to achieve the same results (i.e. 'us-proxy' or 'uk-proxy' for\n'us' and 'uk' proxies).\n\nBlacklists can be applied to a collector to prevent specific proxies from being retrieved. They accept one or more Proxy\nobjects and won't allow retrieval of matching proxies.\n\n.. code-block:: python\n\n    from proxyscrape import create_collector\n\n    collector = create_collector('my-collector', 'http')\n\n    # Add proxy to blacklist\n    collector.blacklist_proxy(Proxy('192.168.1.1', '80', None, None, None, 'http', 'my-resource'))\n\n    # Blacklisted proxies won't be included\n    proxy = get_proxy()\n\n    # Clear blacklist\n    collector.clear_blacklist()\n\nInstead of permanently blacklisting a particular proxies, a proxy can instead be removed from internal memory. This\nallows it to be re-added to the pool upon a subsequent refresh.\n\n.. code-block:: python\n\n    from proxyscrape import create_collector\n\n    collector = create_collector('my-collector', 'http')\n\n    # Remove proxy from internal pool\n    collector.remove_proxy(Proxy('192.168.1.1', '80', None, None, 'http', 'my-resource'))\n\n\nApart from automatic refreshes when retrieving proxies, they can also be forcefully refreshed via the\n`refresh_proxies(...)` function.\n\n.. code-block:: python\n\n    from proxyscrape import create_collector\n\n    collector = create_collector('my-collector', 'http')\n\n    # Forcefully refresh\n    collector.refresh_proxies(force=True)\n\n    # Refresh only if proxies not refreshed within `refresh_interval`\n    collector.refresh_proxies(force=False)\n\nResources\n^^^^^^^^^\nResources refer to a specific function that retrieves a set of proxies; the currently implemented proxies are all\nretrieves from scraping a particular web site.\n\nAdditional user-defined resources can be added to the pool of proxy retrieval functions via the `add_resource(...)`\nfunction. Resources can belong to multiple resource types.\n\n.. code-block:: python\n\n    from proxyscrape import add_resource\n\n    def func():\n        return {Proxy('192.168.1.1', '80', 'us', 'united states', False, 'http', 'my-resource'), }\n\n    add_resource('my-resource', func, 'http')\n\nAs shown above, a resource doesn't necessarily have to scrape proxies from a web site. It can be return a hard-coded\nlist of proxies, make a call to an api, read from a file, etc.\n\nThe set of library- and user-defined resources can be retrieved via the `get_resources(...)` function.\n\n.. code-block:: python\n\n    from proxyscrape import get_resources\n    resources = get_resources()\n\nResource Types\n^^^^^^^^^^^^^^\nResource types are groupings of resources that can be specified when defining a collector (opposed to giving a\ncollection of resources.\n\nAdditional user-defined resource types can be added via the `add_resource_type(...)` function. Resources can optionally\nbe added to a resource type when defining it.\n\n.. code-block:: python\n\n    from proxyscrape import add_resource_type\n    add_resource_type('my-resource-type')\n    add_resource_type('my-other-resource-type', 'my-resource')  # Define resources for resource type\n\nThe set of library- and user-defined resource types can be retrieved via the `get_resource_types(...)` function.\n\n.. code-block:: python\n\n    from proxyscrape import get_resource_types\n    resources = get_resource_types()\n\nContribution\n------------\n\nContributions or suggestions are welcome! Feel free to `open an issue`_ if a bug is found or an enhancement is desired,\nor even a `pull request`_.\n\n.. _open an issue: https://github.com/jaredlgillespie/proxyscrape/issues\n.. _pull request: https://github.com/jaredlgillespie/proxyscrape/compare\n\nChangelog\n---------\n\nAll changes and versioning information can be found in the `CHANGELOG`_.\n\n.. _CHANGELOG: https://github.com/JaredLGillespie/proxyscrape/blob/master/CHANGELOG.rst\n\nLicense\n-------\n\nCopyright (c) 2018 Jared Gillespie. See `LICENSE`_ for details.\n\n.. _LICENSE: https://github.com/JaredLGillespie/proxyscrape/blob/master/LICENSE.txt\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/hyan15/proxyscrape", "keywords": "proxyscrape proxy scrape scraper", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "proxyscrape23", "package_url": "https://pypi.org/project/proxyscrape23/", "platform": "", "project_url": "https://pypi.org/project/proxyscrape23/", "project_urls": {"Homepage": "https://github.com/hyan15/proxyscrape"}, "release_url": "https://pypi.org/project/proxyscrape23/0.1.1/", "requires_dist": ["BeautifulSoup4", "requests"], "requires_python": "", "summary": "A library for retrieving free proxies (HTTP, HTTPS, SOCKS4, SOCKS5).", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://travis-ci.org/JaredLGillespie/proxyscrape\" rel=\"nofollow\"><img alt=\"Travis\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/cf2377d43e820a713e7764a3cb0f418590ea03f1/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f4a617265644c47696c6c65737069652f70726f78797363726170652e737667\"></a>\n<a href=\"https://coveralls.io/github/JaredLGillespie/proxyscrape\" rel=\"nofollow\"><img alt=\"Coveralls github\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5a0085bb717979240c1750f345bf9aaefe6e3f0b/68747470733a2f2f696d672e736869656c64732e696f2f636f766572616c6c732f6769746875622f4a617265644c47696c6c65737069652f70726f78797363726170652e737667\"></a>\n<a href=\"https://pypi.org/project/proxyscrape/\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4b7396a044a48b7b0f6ae8649f085a2fc565082e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f70726f78797363726170652e737667\"></a>\n<a href=\"https://pypi.org/project/proxyscrape/\" rel=\"nofollow\"><img alt=\"PyPI - Wheel\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/94a346605974b4de16fe80d09635e3446f98e1c8/68747470733a2f2f696d672e736869656c64732e696f2f707970692f776865656c2f70726f78797363726170652e737667\"></a>\n<a href=\"https://pypi.org/project/proxyscrape/\" rel=\"nofollow\"><img alt=\"PyPI - Python Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b13809a3a1146215c5ed51997ad866f7ce09600c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f70726f78797363726170652e737667\"></a>\n<a href=\"https://pypi.org/project/proxyscrape/\" rel=\"nofollow\"><img alt=\"PyPI - License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/176deb4fdab2f0601d30e2f38c9a9d598100680e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f70726f78797363726170652e737667\"></a>\n<p>A library for retrieving free proxies (HTTP, HTTPS, SOCKS4, SOCKS5).</p>\n<p><em>NOTE: This library isn\u2019t designed for production use. It\u2019s advised to use your own proxies or purchase a service which\nprovides an API. These are merely free ones that are retrieved from sites and should only be used for development\nor testing purposes.</em></p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">proxyscrape</span>\n\n<span class=\"n\">collector</span> <span class=\"o\">=</span> <span class=\"n\">proxyscrape</span><span class=\"o\">.</span><span class=\"n\">create_collector</span><span class=\"p\">(</span><span class=\"s1\">'default'</span><span class=\"p\">,</span> <span class=\"s1\">'http'</span><span class=\"p\">)</span>  <span class=\"c1\"># Create a collector for http resources</span>\n<span class=\"n\">proxy</span> <span class=\"o\">=</span> <span class=\"n\">collector</span><span class=\"o\">.</span><span class=\"n\">get_proxy</span><span class=\"p\">({</span><span class=\"s1\">'country'</span><span class=\"p\">:</span> <span class=\"s1\">'united states'</span><span class=\"p\">})</span>  <span class=\"c1\"># Retrieve a united states proxy</span>\n</pre>\n<div id=\"installation\">\n<h2>Installation</h2>\n<p>The latest version of proxyscrape is available via <tt>pip</tt>:</p>\n<pre>$ pip install proxyscrape23\n</pre>\n<p>Alternatively, you can download and install from source:</p>\n<pre>$ python setup.py install\n</pre>\n</div>\n<div id=\"provided-proxies\">\n<h2>Provided Proxies</h2>\n<p>Current proxies provided are scraped from various sites which offer free HTTP, HTTPS, SOCKS4, and SOCKS5 proxies; and\ndon\u2019t require headless browsers or selenium to retrieve. The list of sites proxies retrieved are shown below.</p>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>resource</th>\n<th>resource type</th>\n<th>url</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>anonymous-proxy</td>\n<td>http, https</td>\n<td><a href=\"https://free-proxy-list.net/anonymous-proxy.html\" rel=\"nofollow\">https://free-proxy-list.net/anonymous-proxy.html</a></td>\n</tr>\n<tr><td>free-proxy-list</td>\n<td>http, https</td>\n<td><a href=\"https://free-proxy-list.net\" rel=\"nofollow\">https://free-proxy-list.net</a></td>\n</tr>\n<tr><td>proxy-daily-http\nproxy-daily-socks4\nproxy-daily-socks5</td>\n<td>http\nsocks4\nsocks5</td>\n<td><a href=\"http://www.proxy-daily.com\" rel=\"nofollow\">http://www.proxy-daily.com</a></td>\n</tr>\n<tr><td>socks-proxy</td>\n<td>socks4, socks5</td>\n<td><a href=\"https://www.socks-proxy.net\" rel=\"nofollow\">https://www.socks-proxy.net</a></td>\n</tr>\n<tr><td>ssl-proxy</td>\n<td>https</td>\n<td><a href=\"https://www.sslproxies.org\" rel=\"nofollow\">https://www.sslproxies.org</a></td>\n</tr>\n<tr><td>uk-proxy</td>\n<td>http, https</td>\n<td><a href=\"https://free-proxy-list.net/uk-proxy.html\" rel=\"nofollow\">https://free-proxy-list.net/uk-proxy.html</a></td>\n</tr>\n<tr><td>us-proxy</td>\n<td>http, https</td>\n<td><a href=\"https://www.us-proxy.org\" rel=\"nofollow\">https://www.us-proxy.org</a></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div id=\"getting-started\">\n<h2>Getting Started</h2>\n<p>Proxy Scrape is a library aimed at providing an efficient an easy means of retrieving proxies for web-scraping\npurposes. The proxies retrieved are available from sites providing free proxies. The proxies provided, as shown in the\nabove table, can be of one of the following types (referred to as a <cite>resource type</cite>): http, https, socks4, and socks5.</p>\n<div id=\"collectors\">\n<h3>Collectors</h3>\n<p>Collectors serve as the interface to retrieving proxies. They are instantiating at module-level and can be retrieved\nand re-used in different parts of the application (similar to the Python <cite>logging</cite> library). Collectors can be created\nand retrieved via the <cite>create_collector(\u2026)</cite> and <cite>get_collector(\u2026)</cite> functions.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">proxyscrape</span> <span class=\"kn\">import</span> <span class=\"n\">create_collector</span><span class=\"p\">,</span> <span class=\"n\">get_collector</span>\n\n<span class=\"n\">collector</span> <span class=\"o\">=</span> <span class=\"n\">create_collector</span><span class=\"p\">(</span><span class=\"s1\">'my-collector'</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s1\">'socks4'</span><span class=\"p\">,</span> <span class=\"s1\">'socks5'</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Some other section of code</span>\n<span class=\"n\">collector</span> <span class=\"o\">=</span> <span class=\"n\">get_collector</span><span class=\"p\">(</span><span class=\"s1\">'my-collector'</span><span class=\"p\">)</span>\n</pre>\n<p>Each collector should have a unique name and be initialized only once. Typically, only a single collector of a given\nresource type should be utilized. Filters can then be applied to the proxies if specific criteria is desired.</p>\n<p>When given one or more resources, the collector will use those to retrieve proxies. If one or more resource types\nare given, the resources for each of the types will be used to retrieve proxies.</p>\n<p>Once created, proxies can be retrieved via the <cite>get_proxy(\u2026)</cite> function. This optionally takes a <cite>filter_opts</cite>\nparameter which can filter by the following:\n- <tt>code</tt> (us, ca, \u2026)\n- <tt>country</tt> (united states, canada, \u2026)\n- <tt>anonymous</tt> (True, False)\n- <tt>type</tt> (http, https, socks4, socks5, \u2026)</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">proxyscrape</span> <span class=\"kn\">import</span> <span class=\"n\">create_collector</span>\n\n<span class=\"n\">collector</span> <span class=\"o\">=</span> <span class=\"n\">create_collector</span><span class=\"p\">(</span><span class=\"s1\">'my-collector'</span><span class=\"p\">,</span> <span class=\"s1\">'http'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Retrieve any http proxy</span>\n<span class=\"n\">proxy</span> <span class=\"o\">=</span> <span class=\"n\">collector</span><span class=\"o\">.</span><span class=\"n\">get_proxy</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Retrieve only 'us' proxies</span>\n<span class=\"n\">proxy</span> <span class=\"o\">=</span> <span class=\"n\">collector</span><span class=\"o\">.</span><span class=\"n\">get_proxy</span><span class=\"p\">({</span><span class=\"s1\">'code'</span><span class=\"p\">:</span> <span class=\"s1\">'us'</span><span class=\"p\">})</span>\n\n<span class=\"c1\"># Retrieve only anonymous 'uk' or 'us' proxies</span>\n<span class=\"n\">proxy</span> <span class=\"o\">=</span> <span class=\"n\">collector</span><span class=\"o\">.</span><span class=\"n\">get_proxy</span><span class=\"p\">({</span><span class=\"s1\">'code'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"s1\">'us'</span><span class=\"p\">,</span> <span class=\"s1\">'uk'</span><span class=\"p\">),</span> <span class=\"s1\">'anonymous'</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">})</span>\n</pre>\n<p>Filters can be applied to every proxy retrieval from the collector via <cite>apply_filter(\u2026)</cite>. This is useful when the same\nfilter is expected for any proxy retrieved.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">proxyscrape</span> <span class=\"kn\">import</span> <span class=\"n\">create_collector</span>\n\n<span class=\"n\">collector</span> <span class=\"o\">=</span> <span class=\"n\">create_collector</span><span class=\"p\">(</span><span class=\"s1\">'my-collector'</span><span class=\"p\">,</span> <span class=\"s1\">'http'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Only retrieve 'uk' and 'us' proxies</span>\n<span class=\"n\">collector</span><span class=\"o\">.</span><span class=\"n\">apply_filter</span><span class=\"p\">({</span><span class=\"s1\">'code'</span><span class=\"p\">:</span> <span class=\"s1\">'us'</span><span class=\"p\">})</span>\n\n<span class=\"c1\"># Filtered proxies</span>\n<span class=\"n\">proxy</span> <span class=\"o\">=</span> <span class=\"n\">collector</span><span class=\"o\">.</span><span class=\"n\">get_proxy</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Clear filter</span>\n<span class=\"n\">collector</span><span class=\"o\">.</span><span class=\"n\">clear_filter</span><span class=\"p\">()</span>\n</pre>\n<p>Note that some filters may instead use specific resources to achieve the same results (i.e. \u2018us-proxy\u2019 or \u2018uk-proxy\u2019 for\n\u2018us\u2019 and \u2018uk\u2019 proxies).</p>\n<p>Blacklists can be applied to a collector to prevent specific proxies from being retrieved. They accept one or more Proxy\nobjects and won\u2019t allow retrieval of matching proxies.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">proxyscrape</span> <span class=\"kn\">import</span> <span class=\"n\">create_collector</span>\n\n<span class=\"n\">collector</span> <span class=\"o\">=</span> <span class=\"n\">create_collector</span><span class=\"p\">(</span><span class=\"s1\">'my-collector'</span><span class=\"p\">,</span> <span class=\"s1\">'http'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Add proxy to blacklist</span>\n<span class=\"n\">collector</span><span class=\"o\">.</span><span class=\"n\">blacklist_proxy</span><span class=\"p\">(</span><span class=\"n\">Proxy</span><span class=\"p\">(</span><span class=\"s1\">'192.168.1.1'</span><span class=\"p\">,</span> <span class=\"s1\">'80'</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"s1\">'http'</span><span class=\"p\">,</span> <span class=\"s1\">'my-resource'</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Blacklisted proxies won't be included</span>\n<span class=\"n\">proxy</span> <span class=\"o\">=</span> <span class=\"n\">get_proxy</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Clear blacklist</span>\n<span class=\"n\">collector</span><span class=\"o\">.</span><span class=\"n\">clear_blacklist</span><span class=\"p\">()</span>\n</pre>\n<p>Instead of permanently blacklisting a particular proxies, a proxy can instead be removed from internal memory. This\nallows it to be re-added to the pool upon a subsequent refresh.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">proxyscrape</span> <span class=\"kn\">import</span> <span class=\"n\">create_collector</span>\n\n<span class=\"n\">collector</span> <span class=\"o\">=</span> <span class=\"n\">create_collector</span><span class=\"p\">(</span><span class=\"s1\">'my-collector'</span><span class=\"p\">,</span> <span class=\"s1\">'http'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Remove proxy from internal pool</span>\n<span class=\"n\">collector</span><span class=\"o\">.</span><span class=\"n\">remove_proxy</span><span class=\"p\">(</span><span class=\"n\">Proxy</span><span class=\"p\">(</span><span class=\"s1\">'192.168.1.1'</span><span class=\"p\">,</span> <span class=\"s1\">'80'</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"s1\">'http'</span><span class=\"p\">,</span> <span class=\"s1\">'my-resource'</span><span class=\"p\">))</span>\n</pre>\n<p>Apart from automatic refreshes when retrieving proxies, they can also be forcefully refreshed via the\n<cite>refresh_proxies(\u2026)</cite> function.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">proxyscrape</span> <span class=\"kn\">import</span> <span class=\"n\">create_collector</span>\n\n<span class=\"n\">collector</span> <span class=\"o\">=</span> <span class=\"n\">create_collector</span><span class=\"p\">(</span><span class=\"s1\">'my-collector'</span><span class=\"p\">,</span> <span class=\"s1\">'http'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Forcefully refresh</span>\n<span class=\"n\">collector</span><span class=\"o\">.</span><span class=\"n\">refresh_proxies</span><span class=\"p\">(</span><span class=\"n\">force</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Refresh only if proxies not refreshed within `refresh_interval`</span>\n<span class=\"n\">collector</span><span class=\"o\">.</span><span class=\"n\">refresh_proxies</span><span class=\"p\">(</span><span class=\"n\">force</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"resources\">\n<h3>Resources</h3>\n<p>Resources refer to a specific function that retrieves a set of proxies; the currently implemented proxies are all\nretrieves from scraping a particular web site.</p>\n<p>Additional user-defined resources can be added to the pool of proxy retrieval functions via the <cite>add_resource(\u2026)</cite>\nfunction. Resources can belong to multiple resource types.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">proxyscrape</span> <span class=\"kn\">import</span> <span class=\"n\">add_resource</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">func</span><span class=\"p\">():</span>\n    <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"n\">Proxy</span><span class=\"p\">(</span><span class=\"s1\">'192.168.1.1'</span><span class=\"p\">,</span> <span class=\"s1\">'80'</span><span class=\"p\">,</span> <span class=\"s1\">'us'</span><span class=\"p\">,</span> <span class=\"s1\">'united states'</span><span class=\"p\">,</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"s1\">'http'</span><span class=\"p\">,</span> <span class=\"s1\">'my-resource'</span><span class=\"p\">),</span> <span class=\"p\">}</span>\n\n<span class=\"n\">add_resource</span><span class=\"p\">(</span><span class=\"s1\">'my-resource'</span><span class=\"p\">,</span> <span class=\"n\">func</span><span class=\"p\">,</span> <span class=\"s1\">'http'</span><span class=\"p\">)</span>\n</pre>\n<p>As shown above, a resource doesn\u2019t necessarily have to scrape proxies from a web site. It can be return a hard-coded\nlist of proxies, make a call to an api, read from a file, etc.</p>\n<p>The set of library- and user-defined resources can be retrieved via the <cite>get_resources(\u2026)</cite> function.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">proxyscrape</span> <span class=\"kn\">import</span> <span class=\"n\">get_resources</span>\n<span class=\"n\">resources</span> <span class=\"o\">=</span> <span class=\"n\">get_resources</span><span class=\"p\">()</span>\n</pre>\n</div>\n<div id=\"resource-types\">\n<h3>Resource Types</h3>\n<p>Resource types are groupings of resources that can be specified when defining a collector (opposed to giving a\ncollection of resources.</p>\n<p>Additional user-defined resource types can be added via the <cite>add_resource_type(\u2026)</cite> function. Resources can optionally\nbe added to a resource type when defining it.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">proxyscrape</span> <span class=\"kn\">import</span> <span class=\"n\">add_resource_type</span>\n<span class=\"n\">add_resource_type</span><span class=\"p\">(</span><span class=\"s1\">'my-resource-type'</span><span class=\"p\">)</span>\n<span class=\"n\">add_resource_type</span><span class=\"p\">(</span><span class=\"s1\">'my-other-resource-type'</span><span class=\"p\">,</span> <span class=\"s1\">'my-resource'</span><span class=\"p\">)</span>  <span class=\"c1\"># Define resources for resource type</span>\n</pre>\n<p>The set of library- and user-defined resource types can be retrieved via the <cite>get_resource_types(\u2026)</cite> function.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">proxyscrape</span> <span class=\"kn\">import</span> <span class=\"n\">get_resource_types</span>\n<span class=\"n\">resources</span> <span class=\"o\">=</span> <span class=\"n\">get_resource_types</span><span class=\"p\">()</span>\n</pre>\n</div>\n</div>\n<div id=\"contribution\">\n<h2>Contribution</h2>\n<p>Contributions or suggestions are welcome! Feel free to <a href=\"https://github.com/jaredlgillespie/proxyscrape/issues\" rel=\"nofollow\">open an issue</a> if a bug is found or an enhancement is desired,\nor even a <a href=\"https://github.com/jaredlgillespie/proxyscrape/compare\" rel=\"nofollow\">pull request</a>.</p>\n</div>\n<div id=\"changelog\">\n<h2>Changelog</h2>\n<p>All changes and versioning information can be found in the <a href=\"https://github.com/JaredLGillespie/proxyscrape/blob/master/CHANGELOG.rst\" rel=\"nofollow\">CHANGELOG</a>.</p>\n</div>\n<div id=\"license\">\n<h2>License</h2>\n<p>Copyright (c) 2018 Jared Gillespie. See <a href=\"https://github.com/JaredLGillespie/proxyscrape/blob/master/LICENSE.txt\" rel=\"nofollow\">LICENSE</a> for details.</p>\n</div>\n\n          </div>"}, "last_serial": 4513696, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "c34cb813f46a87749a25f94e1f4ed819", "sha256": "89a4eca3c2c259ff0e272cf57844f73e11a39556ae708cc8c639c652c7ad9d62"}, "downloads": -1, "filename": "proxyscrape23-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c34cb813f46a87749a25f94e1f4ed819", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 15910, "upload_time": "2018-11-21T17:56:14", "upload_time_iso_8601": "2018-11-21T17:56:14.822342Z", "url": "https://files.pythonhosted.org/packages/45/cb/45100429e7222ebc8495513e806d7365815ead32d1dd7e49cc06324b938c/proxyscrape23-0.1.1-py2.py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c34cb813f46a87749a25f94e1f4ed819", "sha256": "89a4eca3c2c259ff0e272cf57844f73e11a39556ae708cc8c639c652c7ad9d62"}, "downloads": -1, "filename": "proxyscrape23-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c34cb813f46a87749a25f94e1f4ed819", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 15910, "upload_time": "2018-11-21T17:56:14", "upload_time_iso_8601": "2018-11-21T17:56:14.822342Z", "url": "https://files.pythonhosted.org/packages/45/cb/45100429e7222ebc8495513e806d7365815ead32d1dd7e49cc06324b938c/proxyscrape23-0.1.1-py2.py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 03:16:16 2020"}