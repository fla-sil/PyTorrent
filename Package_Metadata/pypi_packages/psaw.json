{"info": {"author": "David Marx", "author_email": "david.marx84@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Environment :: Console", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Natural Language :: English", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3 :: Only", "Programming Language :: Python :: Implementation :: CPython", "Topic :: Utilities"], "description": "Python Pushshift.io API Wrapper (for comment/submission search)\n===============================================================\n\nDetailed documentation for PSAW is available at: https://psaw.readthedocs.io/en/latest/\n\n.. _installation:\n\nInstallation\n------------\n\n.. code-block:: bash\n\n    pip install psaw\n\nDescription\n-----------\n\nA minimalist wrapper for searching public reddit comments/submissions via the pushshift.io API.\n\nPushshift is an extremely useful resource, but the API is poorly documented. As such, this API wrapper\nis currently designed to make it easy to pass pretty much any search parameter the user wants to try.\n\nAlthough it is not necessarily reflective of the current status of the API, you should\nattempt to familiarize yourself with the Pushshift API documentation to better understand\nwhat search arguments are likely to work.\n\n* `API Documentation on github <https://github.com/pushshift/api>`_\n* `Endpoints and parameter descriptions <https://pushshift.io/api-parameters/>`_\n* `/r/pushshift <https://www.reddit.com/r/pushshift/>`_\n\n\nFeatures\n--------\n\n* Handles rate limiting and exponential backoff subject to maximum retries and\n  maximum backoff limits. A minimum rate limit of 1 request per second is used\n  as a default per consultation with Pushshift's maintainer,\n  `/u/Stuck_in_the_matrix <https://www.reddit.com/u/Stuck_in_the_matrix>`_.\n* Handles paging of results. Returns all historical results for a given query by default.\n* Optionally handles incorporation of ``praw`` to fetch objects after getting ids from pushshift\n* If not using ``praw``, returns results in ``comment`` and ``submission`` objects whose\n  API is similar to the corresponding ``praw`` objects. Additionally, result objects have\n  an additional ``.d_`` attribute that offers dict access to the associated data attributes.\n* Optionally adds a ``created`` attribute which converts a comment/submission's ``created_utc``\n  timestamp to the user's local time. (may raise exceptions for users with certain timezone\n  settings).\n* Simple interface to pass query arguments to the API. The API is sparsely documented,\n  so it's often fruitful to just try an argument and see if it works.\n* A ``stop_condition`` argument to make it simple to stop yielding results given arbitrary user-defined criteria\n* Commandline interface (CLI) for simplified usage outside of python environment.\n\nWARNINGS\n--------\n\n* Using non-default sort may result in unexpected behavior.\n* Default behavior is to continuously hit the pushshift api. If a query is taking\n  longer than expected to return results, it's possible that psaw is pulling more data\n  than you may want or is caught in some kind of loop.\n* I strongly recommend prototyping queries by printing to stdout to ensure you're getting the\n  desired behavior.\n\nDemo usage (python)\n-------------------\n\n.. code-block:: python\n\n    from psaw import PushshiftAPI\n\n    api = PushshiftAPI()\n\nOr to use pushshift search to fetch ids and then use praw to fetch objects:\n\n.. code-block:: python\n\n    import praw\n    from psaw import PushshiftAPI\n\n    r = praw.Reddit(...)\n    api = PushshiftAPI(r)\n\n100 most recent submissions\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: python\n\n    # The `search_comments` and `search_submissions` methods return generator objects\n    gen = api.search_submissions(limit=100)\n    results = list(gen)\n\nFirst 10 submissions to /r/politics in 2017, filtering results to url/author/title/subreddit fields.\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe ``created_utc`` field will be added automatically (it's used for paging).\n\n.. code-block:: python\n\n    import datetime as dt\n\n    start_epoch=int(dt.datetime(2017, 1, 1).timestamp())\n\n    list(api.search_submissions(after=start_epoch,\n                                subreddit='politics',\n                                filter=['url','author', 'title', 'subreddit'],\n                                limit=10))\n\nTrying a search argument that doesn't actually work\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAccording to the pushshift.io API documentation, we should be able to search submissions by url,\nbut (at the time of this writing) this doesn't actually work in practice.\nThe API should still respect the ``limit`` argument and possibly other supported arguments,\nbut no guarantees. If you find that an argument you have passed is not supported by the API,\nbest thing is to just remove it from the query and modify your api call to only utilize\nsupported arguments to mitigate risks from of unexpected behavior.\n\n.. code-block:: python\n\n    url = 'http://www.politico.com/story/2017/02/mike-flynn-russia-ties-investigation-235272'\n    url_results = list(api.search_submissions(url=url, limit=500))\n\n    len(url_results), any(r.url == url for r in url_results)\n    # 500, False\n\nAll AskReddit comments containing the text \"OP\"\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nUse the ``q`` parameter to search text. Omitting the ``limit`` parameter does a full\nhistorical search. Requests are performed in batches of size specified by the\n``max_results_per_request`` parameter (default=500). Omitting the \"max_reponse_cache\"\ntest in the demo below will return all results. Otherwise, this demo will perform two\nAPI requests returning 500 comments each. Alternatively, the generator can be queried for additional results.\n\n.. code-block:: python\n\n    gen = api.search_comments(q='OP', subreddit='askreddit')\n\n    max_response_cache = 1000\n    cache = []\n\n    for c in gen:\n        cache.append(c)\n\n        # Omit this test to actually return all results. Wouldn't recommend it though: could take a while, but you do you.\n        if len(cache) >= max_response_cache:\n            break\n\n    # If you really want to: pick up where we left off to get the rest of the results.\n    if False:\n        for c in gen:\n            cache.append(c)\n\nUsing the ``aggs`` argument to summarize search results\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nWhen an aggs parameter is provided to a search method, the first result yielded by the generator\nwill contain the aggs result.\n\n.. code-block:: python\n\n    api = PushshiftAPI()\n    gen = api.search_comments(author='nasa', aggs='subreddit')\n    next(gen)\n    #  {'subreddit': [\n    #    {'doc_count': 300, 'key': 'IAmA'},\n    #    {'doc_count': 6, 'key': 'space'},\n    #    {'doc_count': 1, 'key': 'ExposurePorn'},\n    #    {'doc_count': 1, 'key': 'Mars'},\n    #    {'doc_count': 1, 'key': 'OldSchoolCool'},\n    #    {'doc_count': 1, 'key': 'news'},\n    #    {'doc_count': 1, 'key': 'pics'},\n    #    {'doc_count': 1, 'key': 'reddit.com'}]}\n    len(list(gen)) # 312\n\nUsing the ``redditor_subreddit_activity`` convenience method\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIf you want to profile a redditors activity as in the ``aggs`` example, the\n``redditor_subreddit_activity`` provides a simple shorthand for profiling a user by the subreddits\nin which they are active, counting comments and submissions separately in a single call,\nand returning Counter objects for commenting and posting activity, respectively.\n\n.. code-block:: python\n\n    api = PushshiftAPI()\n    result = api.redditor_subreddit_activity('nasa')\n    result\n    #{'comment':\n    #   Counter({\n    #      'ExposurePorn': 1,\n    #      'IAmA': 300,\n    #      'Mars': 1,\n    #      'OldSchoolCool': 1,\n    #      'news': 1,\n    #      'pics': 1,\n    #      'reddit.com': 1,\n    #      'space': 6}),\n    # 'submission':\n    #   Counter({\n    #      'IAmA': 3,\n    #      'ISS': 1,\n    #      'Mars': 1,\n    #      'space': 3,\n    #      'u_nasa': 86})}\n\nUsing the ``stop_condition`` argument to get the most recent submission by a bot account\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: python\n\n    gen = api.search_submissions(stop_condition=lambda x: 'bot' in x.author)\n\n    for subm in gen:\n        pass\n\n    print(subm.author)\n\nCollecting results in a ``pandas.DataFrame`` for analysis\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: python\n\n    import pandas as pd\n\n    df = pd.Dataframe([thing.d_ for thing in gen])\n\n\nSpecial Convenience Attributes\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nConsider the following simple query:\n\n.. code-block:: python\n\n    gen = api.search_submissions(subreddit='pushshift')\n    thing = next(gen)\n\nSpecial attributes:\n\n* ``thing.d_`` a dict containing all of the data attributes attached to the thing (which otherwise would be accessed via dot notation). One specific convenience this enables is simplifying pushing results into a pandas dataframe (above).\n* ``api.metadata_`` The metadata data provided by pushshift (if any) from the most recent successful request. The most useful metadata attributes, IMHO, are:\n\n  * ``api.metadata_.get('shards')`` - For checking if any shards are down, which can impact the result cardinality.\n  * ``api.metadata_.get('total_results')`` - The database-side count of how many total items were found in the query and should be returned after paging through all results. Users have encountered rare edge cases that don't return all expected results, probably due to more than 500 items sharing the same timestamp in a result range. See `issue #47 <https://github.com/dmarx/psaw/issues/47/>`_ for progress resolving this behavior.\n\nDemo usage (CLI)\n----------------\n\nFor CLI documentation, run\n\n.. code-block::\n\n    psaw --help\n\nLicense\n-------\n\nPSAW's source is provided under the `Simplified BSD License\n<https://github.com/dmarx/psaw/master/LICENSE>`_.\n\n* Copyright (c), 2018, David Marx\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/dmarx/psaw", "keywords": "reddit api wrapper pushshift", "license": "Simplified BSD License", "maintainer": "", "maintainer_email": "", "name": "psaw", "package_url": "https://pypi.org/project/psaw/", "platform": "", "project_url": "https://pypi.org/project/psaw/", "project_urls": {"Homepage": "http://github.com/dmarx/psaw"}, "release_url": "https://pypi.org/project/psaw/0.0.12/", "requires_dist": ["requests", "Click"], "requires_python": ">=3", "summary": "Pushshift.io API Wrapper for reddit.com public comment/submission search", "version": "0.0.12", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Detailed documentation for PSAW is available at: <a href=\"https://psaw.readthedocs.io/en/latest/\" rel=\"nofollow\">https://psaw.readthedocs.io/en/latest/</a></p>\n<div id=\"id1\">\n<span id=\"installation\"></span><h2>Installation</h2>\n<pre>pip install psaw\n</pre>\n</div>\n<div id=\"description\">\n<h2>Description</h2>\n<p>A minimalist wrapper for searching public reddit comments/submissions via the pushshift.io API.</p>\n<p>Pushshift is an extremely useful resource, but the API is poorly documented. As such, this API wrapper\nis currently designed to make it easy to pass pretty much any search parameter the user wants to try.</p>\n<p>Although it is not necessarily reflective of the current status of the API, you should\nattempt to familiarize yourself with the Pushshift API documentation to better understand\nwhat search arguments are likely to work.</p>\n<ul>\n<li><a href=\"https://github.com/pushshift/api\" rel=\"nofollow\">API Documentation on github</a></li>\n<li><a href=\"https://pushshift.io/api-parameters/\" rel=\"nofollow\">Endpoints and parameter descriptions</a></li>\n<li><a href=\"https://www.reddit.com/r/pushshift/\" rel=\"nofollow\">/r/pushshift</a></li>\n</ul>\n</div>\n<div id=\"features\">\n<h2>Features</h2>\n<ul>\n<li>Handles rate limiting and exponential backoff subject to maximum retries and\nmaximum backoff limits. A minimum rate limit of 1 request per second is used\nas a default per consultation with Pushshift\u2019s maintainer,\n<a href=\"https://www.reddit.com/u/Stuck_in_the_matrix\" rel=\"nofollow\">/u/Stuck_in_the_matrix</a>.</li>\n<li>Handles paging of results. Returns all historical results for a given query by default.</li>\n<li>Optionally handles incorporation of <tt>praw</tt> to fetch objects after getting ids from pushshift</li>\n<li>If not using <tt>praw</tt>, returns results in <tt>comment</tt> and <tt>submission</tt> objects whose\nAPI is similar to the corresponding <tt>praw</tt> objects. Additionally, result objects have\nan additional <tt>.d_</tt> attribute that offers dict access to the associated data attributes.</li>\n<li>Optionally adds a <tt>created</tt> attribute which converts a comment/submission\u2019s <tt>created_utc</tt>\ntimestamp to the user\u2019s local time. (may raise exceptions for users with certain timezone\nsettings).</li>\n<li>Simple interface to pass query arguments to the API. The API is sparsely documented,\nso it\u2019s often fruitful to just try an argument and see if it works.</li>\n<li>A <tt>stop_condition</tt> argument to make it simple to stop yielding results given arbitrary user-defined criteria</li>\n<li>Commandline interface (CLI) for simplified usage outside of python environment.</li>\n</ul>\n</div>\n<div id=\"warnings\">\n<h2>WARNINGS</h2>\n<ul>\n<li>Using non-default sort may result in unexpected behavior.</li>\n<li>Default behavior is to continuously hit the pushshift api. If a query is taking\nlonger than expected to return results, it\u2019s possible that psaw is pulling more data\nthan you may want or is caught in some kind of loop.</li>\n<li>I strongly recommend prototyping queries by printing to stdout to ensure you\u2019re getting the\ndesired behavior.</li>\n</ul>\n</div>\n<div id=\"demo-usage-python\">\n<h2>Demo usage (python)</h2>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">psaw</span> <span class=\"kn\">import</span> <span class=\"n\">PushshiftAPI</span>\n\n<span class=\"n\">api</span> <span class=\"o\">=</span> <span class=\"n\">PushshiftAPI</span><span class=\"p\">()</span>\n</pre>\n<p>Or to use pushshift search to fetch ids and then use praw to fetch objects:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">praw</span>\n<span class=\"kn\">from</span> <span class=\"nn\">psaw</span> <span class=\"kn\">import</span> <span class=\"n\">PushshiftAPI</span>\n\n<span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">praw</span><span class=\"o\">.</span><span class=\"n\">Reddit</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">)</span>\n<span class=\"n\">api</span> <span class=\"o\">=</span> <span class=\"n\">PushshiftAPI</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"p\">)</span>\n</pre>\n<div id=\"most-recent-submissions\">\n<h3>100 most recent submissions</h3>\n<pre><span class=\"c1\"># The `search_comments` and `search_submissions` methods return generator objects</span>\n<span class=\"n\">gen</span> <span class=\"o\">=</span> <span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">search_submissions</span><span class=\"p\">(</span><span class=\"n\">limit</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">gen</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"first-10-submissions-to-r-politics-in-2017-filtering-results-to-url-author-title-subreddit-fields\">\n<h3>First 10 submissions to /r/politics in 2017, filtering results to url/author/title/subreddit fields.</h3>\n<p>The <tt>created_utc</tt> field will be added automatically (it\u2019s used for paging).</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">datetime</span> <span class=\"k\">as</span> <span class=\"nn\">dt</span>\n\n<span class=\"n\">start_epoch</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">dt</span><span class=\"o\">.</span><span class=\"n\">datetime</span><span class=\"p\">(</span><span class=\"mi\">2017</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">timestamp</span><span class=\"p\">())</span>\n\n<span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">search_submissions</span><span class=\"p\">(</span><span class=\"n\">after</span><span class=\"o\">=</span><span class=\"n\">start_epoch</span><span class=\"p\">,</span>\n                            <span class=\"n\">subreddit</span><span class=\"o\">=</span><span class=\"s1\">'politics'</span><span class=\"p\">,</span>\n                            <span class=\"nb\">filter</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'url'</span><span class=\"p\">,</span><span class=\"s1\">'author'</span><span class=\"p\">,</span> <span class=\"s1\">'title'</span><span class=\"p\">,</span> <span class=\"s1\">'subreddit'</span><span class=\"p\">],</span>\n                            <span class=\"n\">limit</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">))</span>\n</pre>\n</div>\n<div id=\"trying-a-search-argument-that-doesn-t-actually-work\">\n<h3>Trying a search argument that doesn\u2019t actually work</h3>\n<p>According to the pushshift.io API documentation, we should be able to search submissions by url,\nbut (at the time of this writing) this doesn\u2019t actually work in practice.\nThe API should still respect the <tt>limit</tt> argument and possibly other supported arguments,\nbut no guarantees. If you find that an argument you have passed is not supported by the API,\nbest thing is to just remove it from the query and modify your api call to only utilize\nsupported arguments to mitigate risks from of unexpected behavior.</p>\n<pre><span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s1\">'http://www.politico.com/story/2017/02/mike-flynn-russia-ties-investigation-235272'</span>\n<span class=\"n\">url_results</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">search_submissions</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"o\">=</span><span class=\"n\">url</span><span class=\"p\">,</span> <span class=\"n\">limit</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">))</span>\n\n<span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">url_results</span><span class=\"p\">),</span> <span class=\"nb\">any</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">url</span> <span class=\"o\">==</span> <span class=\"n\">url</span> <span class=\"k\">for</span> <span class=\"n\">r</span> <span class=\"ow\">in</span> <span class=\"n\">url_results</span><span class=\"p\">)</span>\n<span class=\"c1\"># 500, False</span>\n</pre>\n</div>\n<div id=\"all-askreddit-comments-containing-the-text-op\">\n<h3>All AskReddit comments containing the text \u201cOP\u201d</h3>\n<p>Use the <tt>q</tt> parameter to search text. Omitting the <tt>limit</tt> parameter does a full\nhistorical search. Requests are performed in batches of size specified by the\n<tt>max_results_per_request</tt> parameter (default=500). Omitting the \u201cmax_reponse_cache\u201d\ntest in the demo below will return all results. Otherwise, this demo will perform two\nAPI requests returning 500 comments each. Alternatively, the generator can be queried for additional results.</p>\n<pre><span class=\"n\">gen</span> <span class=\"o\">=</span> <span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">search_comments</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"o\">=</span><span class=\"s1\">'OP'</span><span class=\"p\">,</span> <span class=\"n\">subreddit</span><span class=\"o\">=</span><span class=\"s1\">'askreddit'</span><span class=\"p\">)</span>\n\n<span class=\"n\">max_response_cache</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span>\n<span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"n\">gen</span><span class=\"p\">:</span>\n    <span class=\"n\">cache</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">c</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Omit this test to actually return all results. Wouldn't recommend it though: could take a while, but you do you.</span>\n    <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">cache</span><span class=\"p\">)</span> <span class=\"o\">&gt;=</span> <span class=\"n\">max_response_cache</span><span class=\"p\">:</span>\n        <span class=\"k\">break</span>\n\n<span class=\"c1\"># If you really want to: pick up where we left off to get the rest of the results.</span>\n<span class=\"k\">if</span> <span class=\"kc\">False</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"n\">gen</span><span class=\"p\">:</span>\n        <span class=\"n\">cache</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">c</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"using-the-aggs-argument-to-summarize-search-results\">\n<h3>Using the <tt>aggs</tt> argument to summarize search results</h3>\n<p>When an aggs parameter is provided to a search method, the first result yielded by the generator\nwill contain the aggs result.</p>\n<pre><span class=\"n\">api</span> <span class=\"o\">=</span> <span class=\"n\">PushshiftAPI</span><span class=\"p\">()</span>\n<span class=\"n\">gen</span> <span class=\"o\">=</span> <span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">search_comments</span><span class=\"p\">(</span><span class=\"n\">author</span><span class=\"o\">=</span><span class=\"s1\">'nasa'</span><span class=\"p\">,</span> <span class=\"n\">aggs</span><span class=\"o\">=</span><span class=\"s1\">'subreddit'</span><span class=\"p\">)</span>\n<span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"n\">gen</span><span class=\"p\">)</span>\n<span class=\"c1\">#  {'subreddit': [</span>\n<span class=\"c1\">#    {'doc_count': 300, 'key': 'IAmA'},</span>\n<span class=\"c1\">#    {'doc_count': 6, 'key': 'space'},</span>\n<span class=\"c1\">#    {'doc_count': 1, 'key': 'ExposurePorn'},</span>\n<span class=\"c1\">#    {'doc_count': 1, 'key': 'Mars'},</span>\n<span class=\"c1\">#    {'doc_count': 1, 'key': 'OldSchoolCool'},</span>\n<span class=\"c1\">#    {'doc_count': 1, 'key': 'news'},</span>\n<span class=\"c1\">#    {'doc_count': 1, 'key': 'pics'},</span>\n<span class=\"c1\">#    {'doc_count': 1, 'key': 'reddit.com'}]}</span>\n<span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">gen</span><span class=\"p\">))</span> <span class=\"c1\"># 312</span>\n</pre>\n</div>\n<div id=\"using-the-redditor-subreddit-activity-convenience-method\">\n<h3>Using the <tt>redditor_subreddit_activity</tt> convenience method</h3>\n<p>If you want to profile a redditors activity as in the <tt>aggs</tt> example, the\n<tt>redditor_subreddit_activity</tt> provides a simple shorthand for profiling a user by the subreddits\nin which they are active, counting comments and submissions separately in a single call,\nand returning Counter objects for commenting and posting activity, respectively.</p>\n<pre><span class=\"n\">api</span> <span class=\"o\">=</span> <span class=\"n\">PushshiftAPI</span><span class=\"p\">()</span>\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">redditor_subreddit_activity</span><span class=\"p\">(</span><span class=\"s1\">'nasa'</span><span class=\"p\">)</span>\n<span class=\"n\">result</span>\n<span class=\"c1\">#{'comment':</span>\n<span class=\"c1\">#   Counter({</span>\n<span class=\"c1\">#      'ExposurePorn': 1,</span>\n<span class=\"c1\">#      'IAmA': 300,</span>\n<span class=\"c1\">#      'Mars': 1,</span>\n<span class=\"c1\">#      'OldSchoolCool': 1,</span>\n<span class=\"c1\">#      'news': 1,</span>\n<span class=\"c1\">#      'pics': 1,</span>\n<span class=\"c1\">#      'reddit.com': 1,</span>\n<span class=\"c1\">#      'space': 6}),</span>\n<span class=\"c1\"># 'submission':</span>\n<span class=\"c1\">#   Counter({</span>\n<span class=\"c1\">#      'IAmA': 3,</span>\n<span class=\"c1\">#      'ISS': 1,</span>\n<span class=\"c1\">#      'Mars': 1,</span>\n<span class=\"c1\">#      'space': 3,</span>\n<span class=\"c1\">#      'u_nasa': 86})}</span>\n</pre>\n</div>\n<div id=\"using-the-stop-condition-argument-to-get-the-most-recent-submission-by-a-bot-account\">\n<h3>Using the <tt>stop_condition</tt> argument to get the most recent submission by a bot account</h3>\n<pre><span class=\"n\">gen</span> <span class=\"o\">=</span> <span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">search_submissions</span><span class=\"p\">(</span><span class=\"n\">stop_condition</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"s1\">'bot'</span> <span class=\"ow\">in</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">author</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">subm</span> <span class=\"ow\">in</span> <span class=\"n\">gen</span><span class=\"p\">:</span>\n    <span class=\"k\">pass</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">subm</span><span class=\"o\">.</span><span class=\"n\">author</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"collecting-results-in-a-pandas-dataframe-for-analysis\">\n<h3>Collecting results in a <tt>pandas.DataFrame</tt> for analysis</h3>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">Dataframe</span><span class=\"p\">([</span><span class=\"n\">thing</span><span class=\"o\">.</span><span class=\"n\">d_</span> <span class=\"k\">for</span> <span class=\"n\">thing</span> <span class=\"ow\">in</span> <span class=\"n\">gen</span><span class=\"p\">])</span>\n</pre>\n</div>\n<div id=\"special-convenience-attributes\">\n<h3>Special Convenience Attributes</h3>\n<p>Consider the following simple query:</p>\n<pre><span class=\"n\">gen</span> <span class=\"o\">=</span> <span class=\"n\">api</span><span class=\"o\">.</span><span class=\"n\">search_submissions</span><span class=\"p\">(</span><span class=\"n\">subreddit</span><span class=\"o\">=</span><span class=\"s1\">'pushshift'</span><span class=\"p\">)</span>\n<span class=\"n\">thing</span> <span class=\"o\">=</span> <span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"n\">gen</span><span class=\"p\">)</span>\n</pre>\n<p>Special attributes:</p>\n<ul>\n<li><tt>thing.d_</tt> a dict containing all of the data attributes attached to the thing (which otherwise would be accessed via dot notation). One specific convenience this enables is simplifying pushing results into a pandas dataframe (above).</li>\n<li><tt>api.metadata_</tt> The metadata data provided by pushshift (if any) from the most recent successful request. The most useful metadata attributes, IMHO, are:<ul>\n<li><tt><span class=\"pre\">api.metadata_.get('shards')</span></tt> - For checking if any shards are down, which can impact the result cardinality.</li>\n<li><tt><span class=\"pre\">api.metadata_.get('total_results')</span></tt> - The database-side count of how many total items were found in the query and should be returned after paging through all results. Users have encountered rare edge cases that don\u2019t return all expected results, probably due to more than 500 items sharing the same timestamp in a result range. See <a href=\"https://github.com/dmarx/psaw/issues/47/\" rel=\"nofollow\">issue #47</a> for progress resolving this behavior.</li>\n</ul>\n</li>\n</ul>\n</div>\n</div>\n<div id=\"demo-usage-cli\">\n<h2>Demo usage (CLI)</h2>\n<p>For CLI documentation, run</p>\n<pre>psaw --help\n</pre>\n</div>\n<div id=\"license\">\n<h2>License</h2>\n<p>PSAW\u2019s source is provided under the <a href=\"https://github.com/dmarx/psaw/master/LICENSE\" rel=\"nofollow\">Simplified BSD License</a>.</p>\n<ul>\n<li>Copyright (c), 2018, David Marx</li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 6838217, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "a459d8cc25dc8e885d4a6f29b72e04ee", "sha256": "4a1a07549b59f5ef88da1b2f4826bfe3cd91b0413fb6295744a9ab6006ab3166"}, "downloads": -1, "filename": "psaw-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "a459d8cc25dc8e885d4a6f29b72e04ee", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 9351, "upload_time": "2018-04-15T03:14:53", "upload_time_iso_8601": "2018-04-15T03:14:53.769832Z", "url": "https://files.pythonhosted.org/packages/c3/17/8cc9aca8e7b2cb5a9daf9d72de5209719e7f72b660a2872d3592e658981c/psaw-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "15342c95c92bdad3356cb14e3a3aa6ae", "sha256": "f56e94ef009cf7e86319b18ffb5233f79ad6f302c0d31dce7495e560a5270a03"}, "downloads": -1, "filename": "psaw-0.0.1.tar.gz", "has_sig": false, "md5_digest": "15342c95c92bdad3356cb14e3a3aa6ae", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 5949, "upload_time": "2018-04-15T03:14:55", "upload_time_iso_8601": "2018-04-15T03:14:55.215966Z", "url": "https://files.pythonhosted.org/packages/59/07/db0980b92edd8a1d84259fa98848be7f3bf325cfcd8827ff6d96549686fd/psaw-0.0.1.tar.gz", "yanked": false}], "0.0.10": [{"comment_text": "", "digests": {"md5": "b7e5dade46e0719531092e47e59d1f96", "sha256": "09d4e20b91055c1b345a727a0d618ef76f9f57057b02ed230e38f0fb43db9881"}, "downloads": -1, "filename": "psaw-0.0.10.tar.gz", "has_sig": false, "md5_digest": "b7e5dade46e0719531092e47e59d1f96", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 17547, "upload_time": "2020-03-10T22:32:08", "upload_time_iso_8601": "2020-03-10T22:32:08.133832Z", "url": "https://files.pythonhosted.org/packages/76/d6/b1ef9af3bbae506e329b959770241ad92b5da47ce6d19c3d65d346335771/psaw-0.0.10.tar.gz", "yanked": false}], "0.0.11": [{"comment_text": "", "digests": {"md5": "8055c6be335d59741f10e7aee082f72b", "sha256": "ab6257b9fb18a90cf0b96b9f179cc566435530e38c89d34191f1802e5667d8ec"}, "downloads": -1, "filename": "psaw-0.0.11-py3-none-any.whl", "has_sig": false, "md5_digest": "8055c6be335d59741f10e7aee082f72b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 15574, "upload_time": "2020-03-18T17:31:38", "upload_time_iso_8601": "2020-03-18T17:31:38.970866Z", "url": "https://files.pythonhosted.org/packages/8a/d2/7e22e49e0c5b766ee6f2c36402f1d59fb3c97026d05fad9cb7cb4e222051/psaw-0.0.11-py3-none-any.whl", "yanked": false}], "0.0.12": [{"comment_text": "", "digests": {"md5": "cc782167fcd855eac14835f33e1dd711", "sha256": "cfbfdf1953ee5f31ca9d4ec6d471873ace960da9fbc7234b9d19a059d5cfa2d6"}, "downloads": -1, "filename": "psaw-0.0.12-py3-none-any.whl", "has_sig": false, "md5_digest": "cc782167fcd855eac14835f33e1dd711", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 15575, "upload_time": "2020-03-18T18:23:38", "upload_time_iso_8601": "2020-03-18T18:23:38.469342Z", "url": "https://files.pythonhosted.org/packages/a2/ad/ba88c5004a6e3434c032a605685d90eb3ad3de355d7ee26869124bf3be73/psaw-0.0.12-py3-none-any.whl", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "afee2019191e7e280b45f212aedc077f", "sha256": "4a6ec6a854d773423895a839189937b538c875a37319a846677c871f8cb756f3"}, "downloads": -1, "filename": "psaw-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "afee2019191e7e280b45f212aedc077f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 9439, "upload_time": "2018-05-13T04:21:19", "upload_time_iso_8601": "2018-05-13T04:21:19.456179Z", "url": "https://files.pythonhosted.org/packages/59/9e/927149c3e59a8d0aaa73d3aa0c3b68f5ebbb792dd6936dc62c75dd99d792/psaw-0.0.2-py3-none-any.whl", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "a7d48c37e3158967d97ac5ef59d0370e", "sha256": "923d5c7f0ea3846f67f53a4864cfb7d20bb891c9c3afc710fadefc8587ee1c79"}, "downloads": -1, "filename": "psaw-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "a7d48c37e3158967d97ac5ef59d0370e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 10317, "upload_time": "2018-05-18T08:08:28", "upload_time_iso_8601": "2018-05-18T08:08:28.977462Z", "url": "https://files.pythonhosted.org/packages/e9/81/665728901aa1cc0b3dc98c325074b3ffcefc7622a89676a9c3000d937904/psaw-0.0.3-py3-none-any.whl", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "d87fb1b9014959d09b58ae6052ea82bd", "sha256": "7116815b6c5b40af85cb12b7038425ea6925a5b2ad30f05427848e6f8ed2a90f"}, "downloads": -1, "filename": "psaw-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "d87fb1b9014959d09b58ae6052ea82bd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 10753, "upload_time": "2018-05-18T08:34:27", "upload_time_iso_8601": "2018-05-18T08:34:27.994473Z", "url": "https://files.pythonhosted.org/packages/09/21/0507e22ddaba307ef363b5d69b54c9da2320ca46c5cb3e12a3d815c7477f/psaw-0.0.4-py3-none-any.whl", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "966c836e7e18ea091bda259215aeaa94", "sha256": "ae878073a1fed3304397bce6d242bacdae67eea2060f125169039cf3d03535b2"}, "downloads": -1, "filename": "psaw-0.0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "966c836e7e18ea091bda259215aeaa94", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 10836, "upload_time": "2018-08-06T07:06:20", "upload_time_iso_8601": "2018-08-06T07:06:20.476970Z", "url": "https://files.pythonhosted.org/packages/ec/62/0e20f81d7199d5e418a2efe637e39174256adc79a8b11a36bee6956f464b/psaw-0.0.5-py3-none-any.whl", "yanked": false}], "0.0.6": [{"comment_text": "", "digests": {"md5": "34d7244e658263b179b5d3310da1a31a", "sha256": "63a2cf916fbe036069906c983495c42dbbf0d8e941c9af8a208be8f7c8523684"}, "downloads": -1, "filename": "psaw-0.0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "34d7244e658263b179b5d3310da1a31a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 10800, "upload_time": "2018-08-06T09:23:59", "upload_time_iso_8601": "2018-08-06T09:23:59.483119Z", "url": "https://files.pythonhosted.org/packages/56/5e/6620335cffda65387d6ec8dee091216565735f439c88d56e6a6490d02362/psaw-0.0.6-py3-none-any.whl", "yanked": false}], "0.0.7": [{"comment_text": "", "digests": {"md5": "85b8127701e8552dee7727d406a39b19", "sha256": "4c8c45a6f80e0a1f3436a1fd6685497f401248b8a119fe4a8531ad5fdb75244e"}, "downloads": -1, "filename": "psaw-0.0.7-py3-none-any.whl", "has_sig": false, "md5_digest": "85b8127701e8552dee7727d406a39b19", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 11551, "upload_time": "2018-08-14T04:44:52", "upload_time_iso_8601": "2018-08-14T04:44:52.255300Z", "url": "https://files.pythonhosted.org/packages/60/b7/6724defc12bdcc45470e2b1fc1b978367f3d183ec6c6baa2770a0b083fc7/psaw-0.0.7-py3-none-any.whl", "yanked": false}], "0.0.8": [{"comment_text": "", "digests": {"md5": "9f7b434aff89ea39e5e0265e7a52d954", "sha256": "1aca9e5bd8e391decd285cc13e85d9c0fbd6b3d721dcbb1b2c51faff78cdc773"}, "downloads": -1, "filename": "psaw-0.0.8-py3-none-any.whl", "has_sig": false, "md5_digest": "9f7b434aff89ea39e5e0265e7a52d954", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 8569, "upload_time": "2020-02-29T06:51:40", "upload_time_iso_8601": "2020-02-29T06:51:40.749136Z", "url": "https://files.pythonhosted.org/packages/be/70/d6b867d6743983314e20dfdaac1ced60f25a3389ca42ed64c8cade6347e3/psaw-0.0.8-py3-none-any.whl", "yanked": false}], "0.0.9": [{"comment_text": "", "digests": {"md5": "cbaaa1d833bfe16d347962b54d1b43d4", "sha256": "57f018f37069446f7f101623229403f5ab9ca77fa7b002f32ea76c272f92242e"}, "downloads": -1, "filename": "psaw-0.0.9-py3-none-any.whl", "has_sig": false, "md5_digest": "cbaaa1d833bfe16d347962b54d1b43d4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 13612, "upload_time": "2020-03-09T00:05:56", "upload_time_iso_8601": "2020-03-09T00:05:56.064382Z", "url": "https://files.pythonhosted.org/packages/c0/87/83c644aa417028bcee46d035035d6b553feaee70bdeb26a488f03618ca46/psaw-0.0.9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d5437aa2f0cddae6f796ac0ff872c711", "sha256": "d0b6deb0d3b70ad2858e510586d165a4db1551b384670d2d413489e51c01438d"}, "downloads": -1, "filename": "psaw-0.0.9.tar.gz", "has_sig": false, "md5_digest": "d5437aa2f0cddae6f796ac0ff872c711", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 14920, "upload_time": "2020-03-09T00:05:57", "upload_time_iso_8601": "2020-03-09T00:05:57.348420Z", "url": "https://files.pythonhosted.org/packages/34/19/c867ba6873514322b9a793423760ed934322ed43a8c50cc08667f8ba2005/psaw-0.0.9.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "cc782167fcd855eac14835f33e1dd711", "sha256": "cfbfdf1953ee5f31ca9d4ec6d471873ace960da9fbc7234b9d19a059d5cfa2d6"}, "downloads": -1, "filename": "psaw-0.0.12-py3-none-any.whl", "has_sig": false, "md5_digest": "cc782167fcd855eac14835f33e1dd711", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 15575, "upload_time": "2020-03-18T18:23:38", "upload_time_iso_8601": "2020-03-18T18:23:38.469342Z", "url": "https://files.pythonhosted.org/packages/a2/ad/ba88c5004a6e3434c032a605685d90eb3ad3de355d7ee26869124bf3be73/psaw-0.0.12-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 03:16:07 2020"}