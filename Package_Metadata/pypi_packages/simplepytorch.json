{"info": {"author": "Alex Gaudio", "author_email": "", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.8"], "description": "Configure and train deep feedforward PyTorch models with a lot of the\ndetails already or partially implemented.\n\n**DISCLAIMER:** At the moment, this repo is used for my research.\nNew versions are not necessarily backwards compatible.  The API is\nsubject to change at a moment's notice.  If you happen to use it in your\nresearch or work, make sure in your requirements.txt to pin the version\nor reference the specific commit you used so you don't suffer unwanted\nsurprises.\n\nMotivation and useful features:\n===\n\n- **Clarity:** Much research using PyTorch mixes tedious boiler plate\n  code (like argparse configuration, standard training loop code,\n  logging) with the contribution of your work (ie a new enhancement\n  method, model or training style).  By design, this repo tries to force\n  you as a programmer to better separate the standard PyTorch code from\n  your research contribution.\n- **Simplicity from Command-line:** All key parameters should be\n  automatically exposed on the command-line.  This library converts all\n  public class variables in your Model Config class into an organized\n  list of command-line arguments.  This enables reproducible and\n  highly configurable experiments.\n- **Reproducibility:** The logging infrastructure organizes all results,\n  logs and model checkpoints for a particular experiment, identified by\n  *run_id* into a dedicated directory.  All configuration for your model\n  can be defined at command-line.\n- **Easy to get started:** There can be a dizzying array of little\n  details to implement when training a PyTorch model.  Forgetting these\n  details often leads to bugs and experiments with missing or incorrect\n  results.  The library (specifically the FeedForward class) gives a\n  straightforward recipe and list of functions to implement.\n- **Datasets:** PyTorch Dataset implementations for data I use in my\n  research.  Mostly retinal fundus image datasets.  You must download\n  and unzip the datasets yourself.  A download link is usually in the\n  class docstring.\n\nInstall\n===\n\n```\npip install --upgrade simplepytorch\n```\n\nQuick Start\n===\n\n\nTrain (or evaluate) your model\n```\n#\n# set up a project\n#\n# --> create a directory for your project\nmkdir -p ./myproject/data\n# --> copy the examples directory (from this repo)\ncp -rf ./examples ./myproject/\n# --> link your pre-trained torch models into ./data if you want.\nln -sr ~/.torch ./myproject/data/torch\n# --> now go download the RITE dataset and unzip it into ./myproject/data/RITE\nls ./myproject/data/RITE\n# ls output: AV_groundTruth.zip  introduction.txt  read_me.txt  test  training\n\ncd ./myproject\n# --> ask Python to register the code in ./examples as a package\nexport PYTHONPATH=.:$PYTHONPATH\n\n#\n# train the model\n#\nsimplepytorch ./examples/ -h\nsimplepytorch ./examples/ LetsTrainSomething -h\nsimplepytorch ./examples/ LetsTrainSomething --run-id experimentA --epochs 3\nrun_id=experimentB epochs=3 simplepytorch ./examples/ LetsTrainSomething\n\n# --> debug your model with IPython\nsimplepytorch_debug ./examples/ LetsTrainSomething --run-id experimentA --epochs a\n# --> now you can type %debug to drop into a PDB debugger.  Move around by typing `up` and `down`\n\n# check the results\nls ./data/results/experimentA\ntail -f ./data/results/experimentA/perf.csv \n# --> plot results for all experiments matching a regex\nsimplepytorch_plot 'experiment.*' --ns\n```\n\n\nCheck the examples directory for a simple getting started template.  You\ncan train a model to perform vessel segmentation on the RITE dataset in\nabout 70 lines of code.\n\n[examples/](examples/)\n\n<!-- TODO -->\n<!-- You can also look at prior work using this library.  If you would\nlike to add your (preferably published and) reproducible work to this list,\nplease make a PR and update the README! -->\n\n<!-- - [Pixel Color Amplification (ICIAR 2020)]() -->\n<!-- - [O-MedAL (Wiley DMKD 2020)](https://github.com/adgaudio/o-medal) Early version of this library developed mostly here, so perhaps not a great example. -->\n\nAs a next step, you can copy the examples directory, rename it to\nwhatever your project name is and start from there.  You will find, as\nmentioned in `examples/my_feedforward_model_config.py` that\nthe api.FeedForward class typically lists everything needed.  Assuming\nyou want to use the FeedForward class, just implement or override its\nmethods.  If something isn't obvious or clear, create a GitHub issue.  I\nwill support you to the extent that I can.\n\n\n**Datasets:**\n\nTo use the pre-defined dataset classes, you must download the data and\nunzip it yourself.  Consult Dataset class docstring if necessary.\n\nFor example, some datasets I use have the following structure:\n\n```\n $ ls data/{arsn_qualdr,eyepacs,messidor,IDRiD_segmentation,RITE}\ndata/IDRiD_segmentation:\n'1. Original Images'  '2. All Segmentation Groundtruths'   CC-BY-4.0.txt   LICENSE.txt\n\ndata/RITE:\nAV_groundTruth.zip  introduction.txt  read_me.txt  test  training\n\ndata/arsn_qualdr:\nREADME.md  annotations  annotations.zip  imgs1  imgs1.zip  imgs2  imgs2.zip\n\ndata/eyepacs:\nREADME.md                 test          test.zip.003  test.zip.006  train.zip.001  train.zip.004\nsample.zip                test.zip.001  test.zip.004  test.zip.007  train.zip.002  train.zip.005\nsampleSubmission.csv.zip  test.zip.002  test.zip.005  train         train.zip.003  trainLabels.csv.zip\n\ndata/messidor:\nAnnotation_Base11.csv  Annotation_Base21.csv  Annotation_Base31.csv  Base11  Base21  Base31\nAnnotation_Base12.csv  Annotation_Base22.csv  Annotation_Base32.csv  Base12  Base22  Base32\nAnnotation_Base13.csv  Annotation_Base23.csv  Annotation_Base33.csv  Base13  Base23  Base33\nAnnotation_Base14.csv  Annotation_Base24.csv  Annotation_Base34.csv  Base14  Base24  Base34\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/adgaudio/simplepytorch", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "simplepytorch", "package_url": "https://pypi.org/project/simplepytorch/", "platform": "", "project_url": "https://pypi.org/project/simplepytorch/", "project_urls": {"Homepage": "https://github.com/adgaudio/simplepytorch"}, "release_url": "https://pypi.org/project/simplepytorch/0.0.4/", "requires_dist": ["torchvision", "torch", "pyjq", "pretrainedmodels", "pandas", "numpy", "configargparse", "pillow", "scikit-learn", "matplotlib", "efficientnet-pytorch"], "requires_python": "", "summary": "Setup and train deep nets with PyTorch. Opinionated and Simple.", "version": "0.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Configure and train deep feedforward PyTorch models with a lot of the\ndetails already or partially implemented.</p>\n<p><strong>DISCLAIMER:</strong> At the moment, this repo is used for my research.\nNew versions are not necessarily backwards compatible.  The API is\nsubject to change at a moment's notice.  If you happen to use it in your\nresearch or work, make sure in your requirements.txt to pin the version\nor reference the specific commit you used so you don't suffer unwanted\nsurprises.</p>\n<h1>Motivation and useful features:</h1>\n<ul>\n<li><strong>Clarity:</strong> Much research using PyTorch mixes tedious boiler plate\ncode (like argparse configuration, standard training loop code,\nlogging) with the contribution of your work (ie a new enhancement\nmethod, model or training style).  By design, this repo tries to force\nyou as a programmer to better separate the standard PyTorch code from\nyour research contribution.</li>\n<li><strong>Simplicity from Command-line:</strong> All key parameters should be\nautomatically exposed on the command-line.  This library converts all\npublic class variables in your Model Config class into an organized\nlist of command-line arguments.  This enables reproducible and\nhighly configurable experiments.</li>\n<li><strong>Reproducibility:</strong> The logging infrastructure organizes all results,\nlogs and model checkpoints for a particular experiment, identified by\n<em>run_id</em> into a dedicated directory.  All configuration for your model\ncan be defined at command-line.</li>\n<li><strong>Easy to get started:</strong> There can be a dizzying array of little\ndetails to implement when training a PyTorch model.  Forgetting these\ndetails often leads to bugs and experiments with missing or incorrect\nresults.  The library (specifically the FeedForward class) gives a\nstraightforward recipe and list of functions to implement.</li>\n<li><strong>Datasets:</strong> PyTorch Dataset implementations for data I use in my\nresearch.  Mostly retinal fundus image datasets.  You must download\nand unzip the datasets yourself.  A download link is usually in the\nclass docstring.</li>\n</ul>\n<h1>Install</h1>\n<pre><code>pip install --upgrade simplepytorch\n</code></pre>\n<h1>Quick Start</h1>\n<p>Train (or evaluate) your model</p>\n<pre><code>#\n# set up a project\n#\n# --&gt; create a directory for your project\nmkdir -p ./myproject/data\n# --&gt; copy the examples directory (from this repo)\ncp -rf ./examples ./myproject/\n# --&gt; link your pre-trained torch models into ./data if you want.\nln -sr ~/.torch ./myproject/data/torch\n# --&gt; now go download the RITE dataset and unzip it into ./myproject/data/RITE\nls ./myproject/data/RITE\n# ls output: AV_groundTruth.zip  introduction.txt  read_me.txt  test  training\n\ncd ./myproject\n# --&gt; ask Python to register the code in ./examples as a package\nexport PYTHONPATH=.:$PYTHONPATH\n\n#\n# train the model\n#\nsimplepytorch ./examples/ -h\nsimplepytorch ./examples/ LetsTrainSomething -h\nsimplepytorch ./examples/ LetsTrainSomething --run-id experimentA --epochs 3\nrun_id=experimentB epochs=3 simplepytorch ./examples/ LetsTrainSomething\n\n# --&gt; debug your model with IPython\nsimplepytorch_debug ./examples/ LetsTrainSomething --run-id experimentA --epochs a\n# --&gt; now you can type %debug to drop into a PDB debugger.  Move around by typing `up` and `down`\n\n# check the results\nls ./data/results/experimentA\ntail -f ./data/results/experimentA/perf.csv \n# --&gt; plot results for all experiments matching a regex\nsimplepytorch_plot 'experiment.*' --ns\n</code></pre>\n<p>Check the examples directory for a simple getting started template.  You\ncan train a model to perform vessel segmentation on the RITE dataset in\nabout 70 lines of code.</p>\n<p><a href=\"examples/\" rel=\"nofollow\">examples/</a></p>\n\n\n\n\n<p>As a next step, you can copy the examples directory, rename it to\nwhatever your project name is and start from there.  You will find, as\nmentioned in <code>examples/my_feedforward_model_config.py</code> that\nthe api.FeedForward class typically lists everything needed.  Assuming\nyou want to use the FeedForward class, just implement or override its\nmethods.  If something isn't obvious or clear, create a GitHub issue.  I\nwill support you to the extent that I can.</p>\n<p><strong>Datasets:</strong></p>\n<p>To use the pre-defined dataset classes, you must download the data and\nunzip it yourself.  Consult Dataset class docstring if necessary.</p>\n<p>For example, some datasets I use have the following structure:</p>\n<pre><code> $ ls data/{arsn_qualdr,eyepacs,messidor,IDRiD_segmentation,RITE}\ndata/IDRiD_segmentation:\n'1. Original Images'  '2. All Segmentation Groundtruths'   CC-BY-4.0.txt   LICENSE.txt\n\ndata/RITE:\nAV_groundTruth.zip  introduction.txt  read_me.txt  test  training\n\ndata/arsn_qualdr:\nREADME.md  annotations  annotations.zip  imgs1  imgs1.zip  imgs2  imgs2.zip\n\ndata/eyepacs:\nREADME.md                 test          test.zip.003  test.zip.006  train.zip.001  train.zip.004\nsample.zip                test.zip.001  test.zip.004  test.zip.007  train.zip.002  train.zip.005\nsampleSubmission.csv.zip  test.zip.002  test.zip.005  train         train.zip.003  trainLabels.csv.zip\n\ndata/messidor:\nAnnotation_Base11.csv  Annotation_Base21.csv  Annotation_Base31.csv  Base11  Base21  Base31\nAnnotation_Base12.csv  Annotation_Base22.csv  Annotation_Base32.csv  Base12  Base22  Base32\nAnnotation_Base13.csv  Annotation_Base23.csv  Annotation_Base33.csv  Base13  Base23  Base33\nAnnotation_Base14.csv  Annotation_Base24.csv  Annotation_Base34.csv  Base14  Base24  Base34\n</code></pre>\n\n          </div>"}, "last_serial": 6664376, "releases": {"0.0.2": [{"comment_text": "", "digests": {"md5": "ceb05abca8e4a3090c081eeea7bd94c8", "sha256": "f287e6b53735bf74adcf27195a071d58f48a6e9c6cb8a188075d058b5ae25df6"}, "downloads": -1, "filename": "simplepytorch-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "ceb05abca8e4a3090c081eeea7bd94c8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 21214, "upload_time": "2020-02-19T22:24:42", "upload_time_iso_8601": "2020-02-19T22:24:42.949154Z", "url": "https://files.pythonhosted.org/packages/12/db/384315a1a962218b83bb74c9ade4a250ea8f4df937f6e75fcb9fc88238f0/simplepytorch-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8ca959934827db14e559cc4c0a93ea6b", "sha256": "8cddb7fb6e2acdee2fcc83e90e05ed829959f59bd322e62ade652d8e82466749"}, "downloads": -1, "filename": "simplepytorch-0.0.2.tar.gz", "has_sig": false, "md5_digest": "8ca959934827db14e559cc4c0a93ea6b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20237, "upload_time": "2020-02-19T22:24:45", "upload_time_iso_8601": "2020-02-19T22:24:45.368549Z", "url": "https://files.pythonhosted.org/packages/2d/b6/4537f08aeed66f618452e53b4f4782b4843fe54d432d493e35dddaccd201/simplepytorch-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "1df2fabaf05efc8f01091141f0c86629", "sha256": "18f76228202bbb08322e434c2251980ceed10c603b092ac77a9fd2fda9022f57"}, "downloads": -1, "filename": "simplepytorch-0.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "1df2fabaf05efc8f01091141f0c86629", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 21214, "upload_time": "2020-02-19T22:28:38", "upload_time_iso_8601": "2020-02-19T22:28:38.222783Z", "url": "https://files.pythonhosted.org/packages/e9/a4/43c61365d67a63bfefd437ffba0d7ca95c991da050c44e9aefc32c1caff3/simplepytorch-0.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a33a14ea04fd04efefce852f126b4e22", "sha256": "2761078929cd7a756426870778cc064a6f50d6e281a729155f13edae62a40e75"}, "downloads": -1, "filename": "simplepytorch-0.0.3.tar.gz", "has_sig": false, "md5_digest": "a33a14ea04fd04efefce852f126b4e22", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20241, "upload_time": "2020-02-19T22:28:39", "upload_time_iso_8601": "2020-02-19T22:28:39.802782Z", "url": "https://files.pythonhosted.org/packages/54/ba/1bc80a393a0018790e1b75675b1299ce89ff1f358292d4e9044a9a664573/simplepytorch-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "2304d79060dacc4abf7a9bb4e1feb2c9", "sha256": "a9e910ded22fabef0d2edd6a8432c266f813cc7e48a2ab53479b95752a00877d"}, "downloads": -1, "filename": "simplepytorch-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "2304d79060dacc4abf7a9bb4e1feb2c9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 33835, "upload_time": "2020-02-19T22:44:45", "upload_time_iso_8601": "2020-02-19T22:44:45.491720Z", "url": "https://files.pythonhosted.org/packages/46/55/07dbd69df1563a3be9c45d4c40b89c3c78a365dee855aa72c70b9103bed4/simplepytorch-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ca99aaf3cf9c5a587414d0578ea8bc6f", "sha256": "56269cbab44cd9968a21c9996c3595ae15904f148985c2a04f593cc2a7081a63"}, "downloads": -1, "filename": "simplepytorch-0.0.4.tar.gz", "has_sig": false, "md5_digest": "ca99aaf3cf9c5a587414d0578ea8bc6f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28453, "upload_time": "2020-02-19T22:44:46", "upload_time_iso_8601": "2020-02-19T22:44:46.915674Z", "url": "https://files.pythonhosted.org/packages/41/8a/79cc141486de658cb3cd528f50afb68e63b233061d8d40df72cc3e86ba6b/simplepytorch-0.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2304d79060dacc4abf7a9bb4e1feb2c9", "sha256": "a9e910ded22fabef0d2edd6a8432c266f813cc7e48a2ab53479b95752a00877d"}, "downloads": -1, "filename": "simplepytorch-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "2304d79060dacc4abf7a9bb4e1feb2c9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 33835, "upload_time": "2020-02-19T22:44:45", "upload_time_iso_8601": "2020-02-19T22:44:45.491720Z", "url": "https://files.pythonhosted.org/packages/46/55/07dbd69df1563a3be9c45d4c40b89c3c78a365dee855aa72c70b9103bed4/simplepytorch-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ca99aaf3cf9c5a587414d0578ea8bc6f", "sha256": "56269cbab44cd9968a21c9996c3595ae15904f148985c2a04f593cc2a7081a63"}, "downloads": -1, "filename": "simplepytorch-0.0.4.tar.gz", "has_sig": false, "md5_digest": "ca99aaf3cf9c5a587414d0578ea8bc6f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28453, "upload_time": "2020-02-19T22:44:46", "upload_time_iso_8601": "2020-02-19T22:44:46.915674Z", "url": "https://files.pythonhosted.org/packages/41/8a/79cc141486de658cb3cd528f50afb68e63b233061d8d40df72cc3e86ba6b/simplepytorch-0.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:10:19 2020"}