{"info": {"author": "Veli \u00d6zcan Budak", "author_email": "veliozcanbudak@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: GNU Lesser General Public License v2 or later (LGPLv2+)", "Operating System :: OS Independent", "Programming Language :: Python :: 3.7"], "description": "# Information Retrieval (IR) Effectiveness Evaluation Library for Python\n\nThis library was created in order to evaluate the effectiveness of any kind of algorithm used in IR systems and analyze how well they perform. For this purpose,\n14 different effectiveness measurements have been put together. All of these measurements consist of mostly used ones in the literature. They are as follow:\n\n* Average Precision @n (AP@n)\n* Mean Average Precision (MAP)\n* Geometric Mean Average Precision (GMAP)\n* Eleven Point - Interpolated Average Precision (IAP)\n* R-Precision\n* F-Measure\n* Cumulative Gain (CG)\n* Normalized Cumulative Gain (NCG)\n* Discounted Cumulative Gain (DCG)\n* Normalized Discounted Cumulative Gain (NDCG)\n* Mean Reciprocal Rank (MRR)\n* Rank-Biased Precision (RBP)\n* Expected Reciprocal Rank (ERR)\n* BPref\n\nThis library has also 5 datasets, which were organized for learning and testing each method with their different parameters.\nEven though this library was dynamically used on an online IR system with real user data, it can be used for static datasets as well.\n\n## Before Starting:\n### Some explanations about datasets\nShared datasets are in the format of txt. Each row in the datasets was separated by a pipeline. Even though these datasets have similar attributes, they were created in different formats and have been used in different measurements together or separately for showing how the methods work and what kind of attributes(parameters) these methods need.\nThe attributes used in the datasets are as follow: \n\n* **id**: this is just the id of an interaction\n* **query_id**: this could be thought as a session id\n* **total_result**: how many results are returned to a query\n* **related_document_id**: this is the id of a document which is already known as related to a query. For better understanding, it might be thought that as if a specialist has determined the related documents corresponding to a query before\n* **visited_or_related_document_id**: when thinking of an online system, the id of a clicked page on the result list could be assumed as a related document id corresponding to a query. On the other hand, this case could also be counted as the same for static datasets. For example, let's say an algorithm is tested and a query was submitted. The id\u00e2\u20ac\u2122s of some documents in the result list according to related_document_id attribute could be assumed as a related document id\n* **order_number_of_the_document**: the order number of the clicked/related document in the result list\n* **assessment_of_the_document**: the assessment, which belongs to the clicked/related document. This attribute is in the range of numeric values for this library (1-5) and has to be the same format for different studies, even if the range limits change (for example: 0-5, 1-3, 0-3, 1-7, eg.)\n* **judgement_of_the_document**: the judgement, which belongs to the clicked/related document. This attribute has boolean values 0 or 1 which indicate whether a page/document is related to a query or not\n\n### The parameters used in the methods\nThere are 5 different parameters used in the methods. While some of them are used in common, some of them are used separately. All parameters and their explanations are as follow:\n* **data**: this parameter consists of assessments (as a range) of a user or a specialist, judgements (as boolean) of a user or a specialist and user interactions and is used in every method.\n* **boundaries (Default: ['all'])**: this parameter means cut-off points on total result. In this way, we can analyze performance based on cut-off points. Numeric values such as 5, 10, eg. or string values such as 'all' can be assigned to it in an array format. If a cut-off point is numeric and lower than total result, the processed row is going to be added to the total calculation. On the other hand, in a case that a cut-off point is not numeric, all the rows in the datasets are used in the calculation.\n* **constant (Default: '0.3')** : this parameter is just used in the calculation of GMAP in order to avoid zero values during the calculation.\n* **persistence (or probability) levels (Default: [0.5, 0.8, 0.95])**: this parameter is just used in the calculation of RBP.\n* **max_grade_level (Default: '5')**: this parameter is just used in the calculation of ERR parameter. The minimum level is also used in the calculation but does not need to be included as a parameter for this measurement.\n\n## Installing\n\nThe package can be installed using the code below:\n\n```\npip ir_evaluation_py install\n```\n\n## Importing to a study\nAfter installing the package, the code below is used:\n```\nfrom ir_evaluation.effectiveness import effectiveness\nimport ir_evaluation.datasets as datasets\n\nir=effectiveness() # --> an object, which we can use all methods in it, is created\n```\n\nThe example datasets might not be preferred to include in case of that different dataset will be used.\n\n## Viewing the datasets\nThe example below is for viewing the dataset 1.\n\n```\nfor row in datasets.dataset1:\n    row = row.strip().split('|')\n    print(row)\n```\n\nThe other datasets can be viewed by just changing the number between 1 and 5 at the end of the term **dataset1**.\n\n## Usage of the methods\nAs mentioned before, each method uses some of the datasets together or separately. In this direction, the methods, which need the same datasets, have been explained together. \n\n#### Precision based methods: AP@n, MAP, GMAP, IAP, R-Precision and F-Measure \nThese methods use dataset3 and dataset4 together. Before calling the methods, a variable named **`interactions`** is created as follow:\n\n```\ninteractions = {}\n\n# dataset3 formation: id|query_id|related_document_id\n\nfor row in datasets.dataset3:\n    row = row.strip().split('|')\n\n    if interactions.get(row[1]) is None:\n        interactions[row[1]] = {}\n\n    if interactions[row[1]].get('related_documents') is None:\n        interactions[row[1]]['related_documents'] = set()\n    interactions[row[1]]['related_documents'].add(row[2])\n\n# dataset4 formation: id|query_id|total_result|visited_or_related_document_id|order_number_of_the_document\n\nfor row in datasets.dataset4:\n    row = row.strip().split('|')\n    interactions[row[1]]['total_result'] = row[2]\n\n    if interactions[row[1]].get('visited_documents') is None:\n        interactions[row[1]]['visited_documents'] = []\n    interactions[row[1]]['visited_documents'].append(row[3])\n\n    if interactions[row[1]].get('visited_documents_orders') is None:\n        interactions[row[1]]['visited_documents_orders'] = {}\n    interactions[row[1]]['visited_documents_orders'][row[3]] = row[4]\n```\n\nAfter creating interactions variable, the usage of the related methods is below:\n\n```\n########################################################################################\n# parameters => (data, boundaries)\n\nprint (\"Average Precision@n:\")\nap_at_n = ir.ap_at_n(interactions,[5,10,15,20,'all'])\nprint(ap_at_n)\n\nprint(\"\\n\")\n\nprint (\"R-Precision@n:\")\nrprecision = ir.rprecision(interactions,[5,10,15,20,'all'])\nprint(rprecision)\n\nprint(\"\\n\")\n\nprint (\"Mean Average Precision:\")\nmean_ap = ir.mean_ap(interactions,[5,10,15,20,'all'])\nprint(mean_ap)\n\nprint(\"\\n\")\n\nprint (\"F-Measure:\")\nfmeasure = ir.fmeasure(interactions,[5,10,15,20,'all'])\nprint(fmeasure)\n\nprint(\"\\n\")\n########################################################################################\n# parameters -> (data, constant, boundaries)\n\nprint (\"Geometric Mean Average Precision:\")\ngmap = ir.gmap(interactions,0.3,[5,10,15,20,'all'])\nprint(gmap)\n\nprint(\"\\n\")\n########################################################################################\n# parameters -> (data)\n\nprint (\"Eleven Point - Interpolated Average Precision:\")\nprint (\"Recall => Precision\")\niap = ir.iap(interactions)\nprint(iap)\n```\n#### Gain based methods: CG, NCG, DCG and NDCG\nThese methods use just dataset1. Before calling the methods, a variable named interactions is created as follow:\n\n```\ninteractions = {}\n\n# dataset1 formation: id|query_id|total_result|visited_or_related_document_id|order_number_of_the_document|assessment_of_the_document\n# assessment_of_the_document: assessment is between 1 and 5 for this example\n\nfor row in datasets.dataset1:\n    row = row.strip().split('|')\n\n    if interactions.get(row[1]) is None:\n        interactions[row[1]] = {}\n\n    interactions[row[1]]['total_result'] = row[2]\n\n    if interactions[row[1]].get('assessed_documents') is None:\n        interactions[row[1]]['assessed_documents'] = {}\n    interactions[row[1]]['assessed_documents'][row[3]] = [row[4], row[5]]\n```\n\nAfter creating interactions variable, the usage of the related methods is below:\n\n```\n# parameters => (data, boundaries)\n\nprint (\"Cumulative Gain:\")\ncgain = ir.cgain(interactions,[5,10,15,20,'all'])\nprint(cgain)\n\nprint(\"\\n\")\n\nprint (\"Normalized Cumulative Gain:\")\nncgain = ir.ncgain(interactions,[5,10,15,20])\nprint(ncgain)\n\nprint(\"\\n\")\n\nprint (\"Discounted Cumulative Gain:\")\ndcgain = ir.dcgain(interactions,[5,10,15,20])\nprint(dcgain)\n\nprint(\"\\n\")\n\nprint (\"Normalized Discounted Cumulative Gain:\")\nndcgain = ir.ndcgain(interactions,[5,10,15,20,'all'])\nprint(ndcgain)\n```\n\n#### Mean Reciprocal Rank (MRR) \nThis method use just dataset2. Before calling the method, a variable named interactions is created as follow:\n\n```\ninteractions = {}\n\n# dataset2 formation: id|query_id|visited_or_related_document_id|order_number_of_the_document\n\nfor row in datasets.dataset2:\n    row = row.strip().split('|')\n\n    if interactions.get(row[1]) is None:\n        interactions[row[1]] = {}\n\n    if interactions[row[1]].get('visited_documents_orders') is None:\n        interactions[row[1]]['visited_documents_orders'] = []\n    interactions[row[1]]['visited_documents_orders'].append(row[3])\n```\n\nAfter creating interactions variable, the usage of the method is below:\n\n```\n# parameters => (data)\n\nprint (\"Mean Reciprocal Rank:\")\nmrr = ir.mrr(interactions)\nprint(mrr)\n```\n\n#### Rank-Biased Precision (RBP)\nThis method use just dataset4. Before calling the method, a variable named interactions is created as follow:\n\n```\ninteractions = {}\n\n# dataset4 formation: id|query_id|total_result|visited_or_related_document_id|order_number_of_the_document\n\nfor row in datasets.dataset4:\n    row = row.strip().split('|')\n    if interactions.get(row[1]) is None:\n        interactions[row[1]] = {}\n\n    interactions[row[1]]['total_result'] = row[2]\n\n    if interactions[row[1]].get('visited_documents') is None:\n        interactions[row[1]]['visited_documents'] = []\n    interactions[row[1]]['visited_documents'].append(row[3])\n\n    if interactions[row[1]].get('visited_documents_orders') is None:\n        interactions[row[1]]['visited_documents_orders'] = {}\n    interactions[row[1]]['visited_documents_orders'][row[3]] = row[4]\n```\n\nAfter creating interactions variable, the usage of the method is below:\n\n```\n# parameters => (data, persistence (or probability) levels, boundaries)\n\nprint (\"Rank-Biased Precision:\")\nrbprecision = ir.rbprecision(interactions, [0.5, 0.8, 0.95], [5, 10, 15, 20, 'all'])\nprint(rbprecision)\n```\n\n#### Expected Reciprocal Rank (ERR)\nThis method use just dataset1. Before calling the method, a variable named interactions is created as follow:\n\n```\ninteractions = {}\n\n# dataset1 formation: id|query_id|total_result|visited_or_related_document_id|order_number_of_the_document|assessment_of_the_document\n# assessment_of_the_document: assessment is between 1 and 5 for this example\n\nfor row in datasets.dataset1:\n    row = row.strip().split('|')\n\n    if interactions.get(row[1]) is None:\n        interactions[row[1]] = {}\n\n    interactions[row[1]]['total_result'] = row[2]\n\n    if interactions[row[1]].get('assessed_documents') is None:\n        interactions[row[1]]['assessed_documents'] = {}\n    interactions[row[1]]['assessed_documents'][row[3]] = [row[4], row[5]]\n```\n\nAfter creating interactions variable, the usage of the method is below:\n\n```\n# parameters => (data, max_grade_level, boundaries)\n\nprint (\"Expected Reciprocal Rank:\")\nerr = ir.err(interactions, 5, [5,10,15,20,'all'])\nprint(err)\n```\n\n#### BPref\nThis method use just dataset5. Before calling the method, a variable named interactions is created as follow:\n\n```\ninteractions = {}\n\n# dataset5 just consists of judged documents. Similar to dataset1, but last column has 2 different (boolean) values: 1: related, 0: unrelated\n# data, which belong to unjudged documents, do not need to be inside of the dataset\n# dataset5 formation: id|query_id|total_result|visited_or_related_document_id|order_number_of_the_document|judgement_of_the_document\n\nfor row in datasets.dataset5:\n    row = row.strip().split('|')\n\n    if interactions.get(row[1]) is None:\n        interactions[row[1]] = {}\n\n    interactions[row[1]]['total_result'] = row[2]\n\n    if interactions[row[1]].get('assessed_documents') is None:\n        interactions[row[1]]['assessed_documents'] = {}\n    interactions[row[1]]['assessed_documents'][row[3]] = [row[4], row[5]]\n```\n\nAfter creating interactions variable, the usage of the method is below:\n\n```\n# parameters => (data, boundaries)\n\nprint (\"BPref:\")\nbpref = ir.bpref(interactions,[5,10,15,20,'all'])\nprint(bpref)\n```\n\n## How the analysis result is shown\nIf the method has boundaries parameter, the results are shown for every cut-off point separately. For example:\n\n```\nMean Average Precision:\n{\n    5: {'count': 27, 'value': 0.33497942386831275},\n    10: {'count': 19, 'value': 0.3966374269005848},\n    15: {'count': 9, 'value': 0.4420940170940171},\n    20: {'count': 3, 'value': 0.6923076923076922},\n    'all': {'count': 28, 'value': 0.3850509113901969}\n}\n\nfor MAP@5: 27 row records were used in total for calculation and the result has been found as the value (0.33497942386831275).\nIn this calculation, if the query has a result equal to 5 or higher than 5, the processed row data is added to the calculation process.\n```\n\nIf the method has boundaries and persistence (or probability) levels parameters, the results are shown for every cut-off point and persistence (or probability) levels separately. For example:\n\n```\nRank Biased Precision:\n{\n    5: {\n        '0.5': {'count': 27, 'value': 0.1724537037037037},\n        '0.8': {'count': 27, 'value': 0.10601481481481481},\n        '0.95': {'count': 27, 'value': 0.0339625578703704}\n        },\n    10: {\n        '0.5': {'count': 19, 'value': 0.18045847039473684},\n        '0.8': {'count': 19, 'value': 0.11351753566315788},\n        '0.95': {'count': 19, 'value': 0.042275296240743256}\n        },\n    15: {\n        '0.5': {'count': 9, 'value': 0.2048882378472222},\n        '0.8': {'count': 9, 'value': 0.12131197674382221},\n        '0.95': {'count': 9, 'value': 0.042236674585140445}\n        },\n    20: {\n        '0.5': {'count': 3, 'value': 0.3333740234375},\n        '0.8': {'count': 3, 'value': 0.13791463178239996},\n        '0.95': {'count': 3, 'value': 0.04233933479437731}\n        },\n    'all': {\n        '0.5': {'count': 28, 'value': 0.17031478881835938},\n        '0.8': {'count': 28, 'value': 0.1224766315254345},\n        '0.95': {'count': 28, 'value': 0.04952452518068968}\n        }\n}\n```\n\nIf the method has just data parameter, the results are shown as a single value except for IAP method. For example:\n\n```\nMean Reciprocal Rank:\n0.3965163308913308\n\n##############################################################\n\nEleven Point - Interpolated Average Precision:\nRecall => Precision\n{\n    '0.0': 0.40527483429269134,\n    '0.1': 0.40527483429269134,\n    '0.2': 0.40527483429269134,\n    '0.3': 0.40527483429269134,\n    '0.4': 0.40527483429269134,\n    '0.5': 0.40527483429269134,\n    '0.6': 0.3731319771498342,\n    '0.7': 0.3731319771498342,\n    '0.8': 0.3731319771498342,\n    '0.9': 0.3731319771498342,\n    '1.0': 0.3731319771498342\n}\n```\n\n## License\nThis library is distributed under the [LGPL 2.1 license](https://www.gnu.org/licenses/old-licenses/lgpl-2.1.html). Please read LICENSE for information on the library availability and distribution.\n\n## For citation this library\nThis section is going to be updated.\n\n## For further reading about the measurements\n**Average Precision @n, Mean Average Precision (MAP), R-Precision**: \n\nRicardo Baeza-Yates and Berthier Ribeiro-Neto. 2011. Modern Information Retrieval: The concepts and technology behind search (2nd. ed.). Addison-Wesley Publishing Company, USA.\n\n**Geometric Mean Average Precision:**\nhttps://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf\n\n**Eleven Point - Interpolated Average Precision (IAP):** \n\nBruce Croft, Donald Metzler, and Trevor Strohman. 2009. Search Engines: Information Retrieval in Practice (1st. ed.). Addison-Wesley Publishing Company, USA.\n\n**F-Measure:** \n\nC. J. Van Rijsbergen. 1979. Information Retrieval (2nd. ed.). Butterworth-Heinemann, USA.\n\n**Cumulative Gain, Normalized Cumulative Gain, Discounted Cumulative Gain, Normalized Discounted Cumulative Gain:**\n\nKalervo J\u00c3\u00a4rvelin and Jaana Kek\u00c3\u00a4l\u00c3\u00a4inen. 2000. IR evaluation methods for retrieving highly relevant documents. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR \u00e2\u20ac\u212200). Association for Computing Machinery, New York, NY, USA, 41\u00e2\u20ac\u201c48. DOI:https://doi.org/10.1145/345508.345545\n\nKalervo J\u00c3\u00a4rvelin and Jaana Kek\u00c3\u00a4l\u00c3\u00a4inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst. 20, 4 (October 2002), 422\u00e2\u20ac\u201c446. DOI:https://doi.org/10.1145/582415.582418\n\n**Mean Reciprocal Rank:**\n\nEllen Voorhees. 1999. The TREC-8 Question Answering Track Report. Proceedings of the 8th Text Retrieval Conference. 77-82.\n\n**Rank-Biased Precision (RBP):**\n\nAlistair Moffat and Justin Zobel. 2008. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst. 27, 1, Article 2 (December 2008), 27 pages. DOI:https://doi.org/10.1145/1416950.1416952\n\n**Expected Reciprocal Rank:**\n\nOlivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM conference on Information and knowledge management (CIKM \u00e2\u20ac\u212209). Association for Computing Machinery, New York, NY, USA, 621\u00e2\u20ac\u201c630. DOI:https://doi.org/10.1145/1645953.1646033\n\n**Bpref:**\n\nChris Buckley and Ellen M. Voorhees. 2004. Retrieval evaluation with incomplete information. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR \u00e2\u20ac\u212204). Association for Computing Machinery, New York, NY, USA, 25\u00e2\u20ac\u201c32. DOI:https://doi.org/10.1145/1008992.1009000\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ozcan39/ir_evaluation_py", "keywords": "information retrieval effectiveness evaluation", "license": "", "maintainer": "", "maintainer_email": "", "name": "ir-evaluation-py", "package_url": "https://pypi.org/project/ir-evaluation-py/", "platform": "", "project_url": "https://pypi.org/project/ir-evaluation-py/", "project_urls": {"Homepage": "https://github.com/ozcan39/ir_evaluation_py"}, "release_url": "https://pypi.org/project/ir-evaluation-py/1.0.0/", "requires_dist": null, "requires_python": ">=3.7", "summary": "This library was created in order to evaluate the effectiveness of any kind of algorithm used in IR systems and analyze how well they perform.", "version": "1.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Information Retrieval (IR) Effectiveness Evaluation Library for Python</h1>\n<p>This library was created in order to evaluate the effectiveness of any kind of algorithm used in IR systems and analyze how well they perform. For this purpose,\n14 different effectiveness measurements have been put together. All of these measurements consist of mostly used ones in the literature. They are as follow:</p>\n<ul>\n<li>Average Precision @n (AP@n)</li>\n<li>Mean Average Precision (MAP)</li>\n<li>Geometric Mean Average Precision (GMAP)</li>\n<li>Eleven Point - Interpolated Average Precision (IAP)</li>\n<li>R-Precision</li>\n<li>F-Measure</li>\n<li>Cumulative Gain (CG)</li>\n<li>Normalized Cumulative Gain (NCG)</li>\n<li>Discounted Cumulative Gain (DCG)</li>\n<li>Normalized Discounted Cumulative Gain (NDCG)</li>\n<li>Mean Reciprocal Rank (MRR)</li>\n<li>Rank-Biased Precision (RBP)</li>\n<li>Expected Reciprocal Rank (ERR)</li>\n<li>BPref</li>\n</ul>\n<p>This library has also 5 datasets, which were organized for learning and testing each method with their different parameters.\nEven though this library was dynamically used on an online IR system with real user data, it can be used for static datasets as well.</p>\n<h2>Before Starting:</h2>\n<h3>Some explanations about datasets</h3>\n<p>Shared datasets are in the format of txt. Each row in the datasets was separated by a pipeline. Even though these datasets have similar attributes, they were created in different formats and have been used in different measurements together or separately for showing how the methods work and what kind of attributes(parameters) these methods need.\nThe attributes used in the datasets are as follow:</p>\n<ul>\n<li><strong>id</strong>: this is just the id of an interaction</li>\n<li><strong>query_id</strong>: this could be thought as a session id</li>\n<li><strong>total_result</strong>: how many results are returned to a query</li>\n<li><strong>related_document_id</strong>: this is the id of a document which is already known as related to a query. For better understanding, it might be thought that as if a specialist has determined the related documents corresponding to a query before</li>\n<li><strong>visited_or_related_document_id</strong>: when thinking of an online system, the id of a clicked page on the result list could be assumed as a related document id corresponding to a query. On the other hand, this case could also be counted as the same for static datasets. For example, let's say an algorithm is tested and a query was submitted. The id\u00e2\u20ac\u2122s of some documents in the result list according to related_document_id attribute could be assumed as a related document id</li>\n<li><strong>order_number_of_the_document</strong>: the order number of the clicked/related document in the result list</li>\n<li><strong>assessment_of_the_document</strong>: the assessment, which belongs to the clicked/related document. This attribute is in the range of numeric values for this library (1-5) and has to be the same format for different studies, even if the range limits change (for example: 0-5, 1-3, 0-3, 1-7, eg.)</li>\n<li><strong>judgement_of_the_document</strong>: the judgement, which belongs to the clicked/related document. This attribute has boolean values 0 or 1 which indicate whether a page/document is related to a query or not</li>\n</ul>\n<h3>The parameters used in the methods</h3>\n<p>There are 5 different parameters used in the methods. While some of them are used in common, some of them are used separately. All parameters and their explanations are as follow:</p>\n<ul>\n<li><strong>data</strong>: this parameter consists of assessments (as a range) of a user or a specialist, judgements (as boolean) of a user or a specialist and user interactions and is used in every method.</li>\n<li><strong>boundaries (Default: ['all'])</strong>: this parameter means cut-off points on total result. In this way, we can analyze performance based on cut-off points. Numeric values such as 5, 10, eg. or string values such as 'all' can be assigned to it in an array format. If a cut-off point is numeric and lower than total result, the processed row is going to be added to the total calculation. On the other hand, in a case that a cut-off point is not numeric, all the rows in the datasets are used in the calculation.</li>\n<li><strong>constant (Default: '0.3')</strong> : this parameter is just used in the calculation of GMAP in order to avoid zero values during the calculation.</li>\n<li><strong>persistence (or probability) levels (Default: [0.5, 0.8, 0.95])</strong>: this parameter is just used in the calculation of RBP.</li>\n<li><strong>max_grade_level (Default: '5')</strong>: this parameter is just used in the calculation of ERR parameter. The minimum level is also used in the calculation but does not need to be included as a parameter for this measurement.</li>\n</ul>\n<h2>Installing</h2>\n<p>The package can be installed using the code below:</p>\n<pre><code>pip ir_evaluation_py install\n</code></pre>\n<h2>Importing to a study</h2>\n<p>After installing the package, the code below is used:</p>\n<pre><code>from ir_evaluation.effectiveness import effectiveness\nimport ir_evaluation.datasets as datasets\n\nir=effectiveness() # --&gt; an object, which we can use all methods in it, is created\n</code></pre>\n<p>The example datasets might not be preferred to include in case of that different dataset will be used.</p>\n<h2>Viewing the datasets</h2>\n<p>The example below is for viewing the dataset 1.</p>\n<pre><code>for row in datasets.dataset1:\n    row = row.strip().split('|')\n    print(row)\n</code></pre>\n<p>The other datasets can be viewed by just changing the number between 1 and 5 at the end of the term <strong>dataset1</strong>.</p>\n<h2>Usage of the methods</h2>\n<p>As mentioned before, each method uses some of the datasets together or separately. In this direction, the methods, which need the same datasets, have been explained together.</p>\n<h4>Precision based methods: AP@n, MAP, GMAP, IAP, R-Precision and F-Measure</h4>\n<p>These methods use dataset3 and dataset4 together. Before calling the methods, a variable named <strong><code>interactions</code></strong> is created as follow:</p>\n<pre><code>interactions = {}\n\n# dataset3 formation: id|query_id|related_document_id\n\nfor row in datasets.dataset3:\n    row = row.strip().split('|')\n\n    if interactions.get(row[1]) is None:\n        interactions[row[1]] = {}\n\n    if interactions[row[1]].get('related_documents') is None:\n        interactions[row[1]]['related_documents'] = set()\n    interactions[row[1]]['related_documents'].add(row[2])\n\n# dataset4 formation: id|query_id|total_result|visited_or_related_document_id|order_number_of_the_document\n\nfor row in datasets.dataset4:\n    row = row.strip().split('|')\n    interactions[row[1]]['total_result'] = row[2]\n\n    if interactions[row[1]].get('visited_documents') is None:\n        interactions[row[1]]['visited_documents'] = []\n    interactions[row[1]]['visited_documents'].append(row[3])\n\n    if interactions[row[1]].get('visited_documents_orders') is None:\n        interactions[row[1]]['visited_documents_orders'] = {}\n    interactions[row[1]]['visited_documents_orders'][row[3]] = row[4]\n</code></pre>\n<p>After creating interactions variable, the usage of the related methods is below:</p>\n<pre><code>########################################################################################\n# parameters =&gt; (data, boundaries)\n\nprint (\"Average Precision@n:\")\nap_at_n = ir.ap_at_n(interactions,[5,10,15,20,'all'])\nprint(ap_at_n)\n\nprint(\"\\n\")\n\nprint (\"R-Precision@n:\")\nrprecision = ir.rprecision(interactions,[5,10,15,20,'all'])\nprint(rprecision)\n\nprint(\"\\n\")\n\nprint (\"Mean Average Precision:\")\nmean_ap = ir.mean_ap(interactions,[5,10,15,20,'all'])\nprint(mean_ap)\n\nprint(\"\\n\")\n\nprint (\"F-Measure:\")\nfmeasure = ir.fmeasure(interactions,[5,10,15,20,'all'])\nprint(fmeasure)\n\nprint(\"\\n\")\n########################################################################################\n# parameters -&gt; (data, constant, boundaries)\n\nprint (\"Geometric Mean Average Precision:\")\ngmap = ir.gmap(interactions,0.3,[5,10,15,20,'all'])\nprint(gmap)\n\nprint(\"\\n\")\n########################################################################################\n# parameters -&gt; (data)\n\nprint (\"Eleven Point - Interpolated Average Precision:\")\nprint (\"Recall =&gt; Precision\")\niap = ir.iap(interactions)\nprint(iap)\n</code></pre>\n<h4>Gain based methods: CG, NCG, DCG and NDCG</h4>\n<p>These methods use just dataset1. Before calling the methods, a variable named interactions is created as follow:</p>\n<pre><code>interactions = {}\n\n# dataset1 formation: id|query_id|total_result|visited_or_related_document_id|order_number_of_the_document|assessment_of_the_document\n# assessment_of_the_document: assessment is between 1 and 5 for this example\n\nfor row in datasets.dataset1:\n    row = row.strip().split('|')\n\n    if interactions.get(row[1]) is None:\n        interactions[row[1]] = {}\n\n    interactions[row[1]]['total_result'] = row[2]\n\n    if interactions[row[1]].get('assessed_documents') is None:\n        interactions[row[1]]['assessed_documents'] = {}\n    interactions[row[1]]['assessed_documents'][row[3]] = [row[4], row[5]]\n</code></pre>\n<p>After creating interactions variable, the usage of the related methods is below:</p>\n<pre><code># parameters =&gt; (data, boundaries)\n\nprint (\"Cumulative Gain:\")\ncgain = ir.cgain(interactions,[5,10,15,20,'all'])\nprint(cgain)\n\nprint(\"\\n\")\n\nprint (\"Normalized Cumulative Gain:\")\nncgain = ir.ncgain(interactions,[5,10,15,20])\nprint(ncgain)\n\nprint(\"\\n\")\n\nprint (\"Discounted Cumulative Gain:\")\ndcgain = ir.dcgain(interactions,[5,10,15,20])\nprint(dcgain)\n\nprint(\"\\n\")\n\nprint (\"Normalized Discounted Cumulative Gain:\")\nndcgain = ir.ndcgain(interactions,[5,10,15,20,'all'])\nprint(ndcgain)\n</code></pre>\n<h4>Mean Reciprocal Rank (MRR)</h4>\n<p>This method use just dataset2. Before calling the method, a variable named interactions is created as follow:</p>\n<pre><code>interactions = {}\n\n# dataset2 formation: id|query_id|visited_or_related_document_id|order_number_of_the_document\n\nfor row in datasets.dataset2:\n    row = row.strip().split('|')\n\n    if interactions.get(row[1]) is None:\n        interactions[row[1]] = {}\n\n    if interactions[row[1]].get('visited_documents_orders') is None:\n        interactions[row[1]]['visited_documents_orders'] = []\n    interactions[row[1]]['visited_documents_orders'].append(row[3])\n</code></pre>\n<p>After creating interactions variable, the usage of the method is below:</p>\n<pre><code># parameters =&gt; (data)\n\nprint (\"Mean Reciprocal Rank:\")\nmrr = ir.mrr(interactions)\nprint(mrr)\n</code></pre>\n<h4>Rank-Biased Precision (RBP)</h4>\n<p>This method use just dataset4. Before calling the method, a variable named interactions is created as follow:</p>\n<pre><code>interactions = {}\n\n# dataset4 formation: id|query_id|total_result|visited_or_related_document_id|order_number_of_the_document\n\nfor row in datasets.dataset4:\n    row = row.strip().split('|')\n    if interactions.get(row[1]) is None:\n        interactions[row[1]] = {}\n\n    interactions[row[1]]['total_result'] = row[2]\n\n    if interactions[row[1]].get('visited_documents') is None:\n        interactions[row[1]]['visited_documents'] = []\n    interactions[row[1]]['visited_documents'].append(row[3])\n\n    if interactions[row[1]].get('visited_documents_orders') is None:\n        interactions[row[1]]['visited_documents_orders'] = {}\n    interactions[row[1]]['visited_documents_orders'][row[3]] = row[4]\n</code></pre>\n<p>After creating interactions variable, the usage of the method is below:</p>\n<pre><code># parameters =&gt; (data, persistence (or probability) levels, boundaries)\n\nprint (\"Rank-Biased Precision:\")\nrbprecision = ir.rbprecision(interactions, [0.5, 0.8, 0.95], [5, 10, 15, 20, 'all'])\nprint(rbprecision)\n</code></pre>\n<h4>Expected Reciprocal Rank (ERR)</h4>\n<p>This method use just dataset1. Before calling the method, a variable named interactions is created as follow:</p>\n<pre><code>interactions = {}\n\n# dataset1 formation: id|query_id|total_result|visited_or_related_document_id|order_number_of_the_document|assessment_of_the_document\n# assessment_of_the_document: assessment is between 1 and 5 for this example\n\nfor row in datasets.dataset1:\n    row = row.strip().split('|')\n\n    if interactions.get(row[1]) is None:\n        interactions[row[1]] = {}\n\n    interactions[row[1]]['total_result'] = row[2]\n\n    if interactions[row[1]].get('assessed_documents') is None:\n        interactions[row[1]]['assessed_documents'] = {}\n    interactions[row[1]]['assessed_documents'][row[3]] = [row[4], row[5]]\n</code></pre>\n<p>After creating interactions variable, the usage of the method is below:</p>\n<pre><code># parameters =&gt; (data, max_grade_level, boundaries)\n\nprint (\"Expected Reciprocal Rank:\")\nerr = ir.err(interactions, 5, [5,10,15,20,'all'])\nprint(err)\n</code></pre>\n<h4>BPref</h4>\n<p>This method use just dataset5. Before calling the method, a variable named interactions is created as follow:</p>\n<pre><code>interactions = {}\n\n# dataset5 just consists of judged documents. Similar to dataset1, but last column has 2 different (boolean) values: 1: related, 0: unrelated\n# data, which belong to unjudged documents, do not need to be inside of the dataset\n# dataset5 formation: id|query_id|total_result|visited_or_related_document_id|order_number_of_the_document|judgement_of_the_document\n\nfor row in datasets.dataset5:\n    row = row.strip().split('|')\n\n    if interactions.get(row[1]) is None:\n        interactions[row[1]] = {}\n\n    interactions[row[1]]['total_result'] = row[2]\n\n    if interactions[row[1]].get('assessed_documents') is None:\n        interactions[row[1]]['assessed_documents'] = {}\n    interactions[row[1]]['assessed_documents'][row[3]] = [row[4], row[5]]\n</code></pre>\n<p>After creating interactions variable, the usage of the method is below:</p>\n<pre><code># parameters =&gt; (data, boundaries)\n\nprint (\"BPref:\")\nbpref = ir.bpref(interactions,[5,10,15,20,'all'])\nprint(bpref)\n</code></pre>\n<h2>How the analysis result is shown</h2>\n<p>If the method has boundaries parameter, the results are shown for every cut-off point separately. For example:</p>\n<pre><code>Mean Average Precision:\n{\n    5: {'count': 27, 'value': 0.33497942386831275},\n    10: {'count': 19, 'value': 0.3966374269005848},\n    15: {'count': 9, 'value': 0.4420940170940171},\n    20: {'count': 3, 'value': 0.6923076923076922},\n    'all': {'count': 28, 'value': 0.3850509113901969}\n}\n\nfor MAP@5: 27 row records were used in total for calculation and the result has been found as the value (0.33497942386831275).\nIn this calculation, if the query has a result equal to 5 or higher than 5, the processed row data is added to the calculation process.\n</code></pre>\n<p>If the method has boundaries and persistence (or probability) levels parameters, the results are shown for every cut-off point and persistence (or probability) levels separately. For example:</p>\n<pre><code>Rank Biased Precision:\n{\n    5: {\n        '0.5': {'count': 27, 'value': 0.1724537037037037},\n        '0.8': {'count': 27, 'value': 0.10601481481481481},\n        '0.95': {'count': 27, 'value': 0.0339625578703704}\n        },\n    10: {\n        '0.5': {'count': 19, 'value': 0.18045847039473684},\n        '0.8': {'count': 19, 'value': 0.11351753566315788},\n        '0.95': {'count': 19, 'value': 0.042275296240743256}\n        },\n    15: {\n        '0.5': {'count': 9, 'value': 0.2048882378472222},\n        '0.8': {'count': 9, 'value': 0.12131197674382221},\n        '0.95': {'count': 9, 'value': 0.042236674585140445}\n        },\n    20: {\n        '0.5': {'count': 3, 'value': 0.3333740234375},\n        '0.8': {'count': 3, 'value': 0.13791463178239996},\n        '0.95': {'count': 3, 'value': 0.04233933479437731}\n        },\n    'all': {\n        '0.5': {'count': 28, 'value': 0.17031478881835938},\n        '0.8': {'count': 28, 'value': 0.1224766315254345},\n        '0.95': {'count': 28, 'value': 0.04952452518068968}\n        }\n}\n</code></pre>\n<p>If the method has just data parameter, the results are shown as a single value except for IAP method. For example:</p>\n<pre><code>Mean Reciprocal Rank:\n0.3965163308913308\n\n##############################################################\n\nEleven Point - Interpolated Average Precision:\nRecall =&gt; Precision\n{\n    '0.0': 0.40527483429269134,\n    '0.1': 0.40527483429269134,\n    '0.2': 0.40527483429269134,\n    '0.3': 0.40527483429269134,\n    '0.4': 0.40527483429269134,\n    '0.5': 0.40527483429269134,\n    '0.6': 0.3731319771498342,\n    '0.7': 0.3731319771498342,\n    '0.8': 0.3731319771498342,\n    '0.9': 0.3731319771498342,\n    '1.0': 0.3731319771498342\n}\n</code></pre>\n<h2>License</h2>\n<p>This library is distributed under the <a href=\"https://www.gnu.org/licenses/old-licenses/lgpl-2.1.html\" rel=\"nofollow\">LGPL 2.1 license</a>. Please read LICENSE for information on the library availability and distribution.</p>\n<h2>For citation this library</h2>\n<p>This section is going to be updated.</p>\n<h2>For further reading about the measurements</h2>\n<p><strong>Average Precision @n, Mean Average Precision (MAP), R-Precision</strong>:</p>\n<p>Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 2011. Modern Information Retrieval: The concepts and technology behind search (2nd. ed.). Addison-Wesley Publishing Company, USA.</p>\n<p><strong>Geometric Mean Average Precision:</strong>\n<a href=\"https://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf\" rel=\"nofollow\">https://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf</a></p>\n<p><strong>Eleven Point - Interpolated Average Precision (IAP):</strong></p>\n<p>Bruce Croft, Donald Metzler, and Trevor Strohman. 2009. Search Engines: Information Retrieval in Practice (1st. ed.). Addison-Wesley Publishing Company, USA.</p>\n<p><strong>F-Measure:</strong></p>\n<p>C. J. Van Rijsbergen. 1979. Information Retrieval (2nd. ed.). Butterworth-Heinemann, USA.</p>\n<p><strong>Cumulative Gain, Normalized Cumulative Gain, Discounted Cumulative Gain, Normalized Discounted Cumulative Gain:</strong></p>\n<p>Kalervo J\u00c3\u00a4rvelin and Jaana Kek\u00c3\u00a4l\u00c3\u00a4inen. 2000. IR evaluation methods for retrieving highly relevant documents. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR \u00e2\u20ac\u212200). Association for Computing Machinery, New York, NY, USA, 41\u00e2\u20ac\u201c48. DOI:<a href=\"https://doi.org/10.1145/345508.345545\" rel=\"nofollow\">https://doi.org/10.1145/345508.345545</a></p>\n<p>Kalervo J\u00c3\u00a4rvelin and Jaana Kek\u00c3\u00a4l\u00c3\u00a4inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst. 20, 4 (October 2002), 422\u00e2\u20ac\u201c446. DOI:<a href=\"https://doi.org/10.1145/582415.582418\" rel=\"nofollow\">https://doi.org/10.1145/582415.582418</a></p>\n<p><strong>Mean Reciprocal Rank:</strong></p>\n<p>Ellen Voorhees. 1999. The TREC-8 Question Answering Track Report. Proceedings of the 8th Text Retrieval Conference. 77-82.</p>\n<p><strong>Rank-Biased Precision (RBP):</strong></p>\n<p>Alistair Moffat and Justin Zobel. 2008. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst. 27, 1, Article 2 (December 2008), 27 pages. DOI:<a href=\"https://doi.org/10.1145/1416950.1416952\" rel=\"nofollow\">https://doi.org/10.1145/1416950.1416952</a></p>\n<p><strong>Expected Reciprocal Rank:</strong></p>\n<p>Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM conference on Information and knowledge management (CIKM \u00e2\u20ac\u212209). Association for Computing Machinery, New York, NY, USA, 621\u00e2\u20ac\u201c630. DOI:<a href=\"https://doi.org/10.1145/1645953.1646033\" rel=\"nofollow\">https://doi.org/10.1145/1645953.1646033</a></p>\n<p><strong>Bpref:</strong></p>\n<p>Chris Buckley and Ellen M. Voorhees. 2004. Retrieval evaluation with incomplete information. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR \u00e2\u20ac\u212204). Association for Computing Machinery, New York, NY, USA, 25\u00e2\u20ac\u201c32. DOI:<a href=\"https://doi.org/10.1145/1008992.1009000\" rel=\"nofollow\">https://doi.org/10.1145/1008992.1009000</a></p>\n\n          </div>"}, "last_serial": 6859125, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "efe56bdb6ee52f415b2040a180b108ab", "sha256": "963cc8a4b2e6d5ccbbd663eb774e316a0ed8535cde3380178c3aca5ff5bee221"}, "downloads": -1, "filename": "ir_evaluation_py-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "efe56bdb6ee52f415b2040a180b108ab", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 22786, "upload_time": "2020-03-22T09:24:52", "upload_time_iso_8601": "2020-03-22T09:24:52.403618Z", "url": "https://files.pythonhosted.org/packages/de/1c/328cba6438769c044466e9e543fc4ff3d77f6c616cc1b71c7a523090780e/ir_evaluation_py-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c22ffb1d51176ec859bd43aeed0737ab", "sha256": "c886a5288a75d6a953fd4e390aee8c714ce5f50a1b5378a68d59a3e9cb577814"}, "downloads": -1, "filename": "ir_evaluation_py-1.0.0.tar.gz", "has_sig": false, "md5_digest": "c22ffb1d51176ec859bd43aeed0737ab", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 17706, "upload_time": "2020-03-22T09:24:54", "upload_time_iso_8601": "2020-03-22T09:24:54.921105Z", "url": "https://files.pythonhosted.org/packages/a8/aa/fa5d82a7c9017ec6b033ddc1b5d01ac693048c8726075617b4b651ba5459/ir_evaluation_py-1.0.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "efe56bdb6ee52f415b2040a180b108ab", "sha256": "963cc8a4b2e6d5ccbbd663eb774e316a0ed8535cde3380178c3aca5ff5bee221"}, "downloads": -1, "filename": "ir_evaluation_py-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "efe56bdb6ee52f415b2040a180b108ab", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 22786, "upload_time": "2020-03-22T09:24:52", "upload_time_iso_8601": "2020-03-22T09:24:52.403618Z", "url": "https://files.pythonhosted.org/packages/de/1c/328cba6438769c044466e9e543fc4ff3d77f6c616cc1b71c7a523090780e/ir_evaluation_py-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c22ffb1d51176ec859bd43aeed0737ab", "sha256": "c886a5288a75d6a953fd4e390aee8c714ce5f50a1b5378a68d59a3e9cb577814"}, "downloads": -1, "filename": "ir_evaluation_py-1.0.0.tar.gz", "has_sig": false, "md5_digest": "c22ffb1d51176ec859bd43aeed0737ab", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 17706, "upload_time": "2020-03-22T09:24:54", "upload_time_iso_8601": "2020-03-22T09:24:54.921105Z", "url": "https://files.pythonhosted.org/packages/a8/aa/fa5d82a7c9017ec6b033ddc1b5d01ac693048c8726075617b4b651ba5459/ir_evaluation_py-1.0.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:54:17 2020"}