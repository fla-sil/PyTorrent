{"info": {"author": "Ed Medvedev", "author_email": "edward.medvedev@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# Attention-based OCR\n\nVisual attention-based OCR model for image recognition with additional tools for creating TFRecords datasets and exporting the trained model with weights as a [SavedModel](https://www.tensorflow.org/api_docs/python/tf/saved_model) or a frozen graph.\n\n## Acknowledgements\n\nThis project is based on a model by [Qi Guo](http://qiguo.ml) and [Yuntian Deng](https://github.com/da03). You can find the original model in the [da03/Attention-OCR](https://github.com/da03/Attention-OCR) repository.\n\n## The model\n\nAuthors: [Qi Guo](http://qiguo.ml) and [Yuntian Deng](https://github.com/da03).\n\nThe model first runs a sliding CNN on the image (images are resized to height 32 while preserving aspect ratio). Then an LSTM is stacked on top of the CNN. Finally, an attention model is used as a decoder for producing the final outputs.\n\n![OCR example](http://cs.cmu.edu/~yuntiand/OCR-2.jpg)\n\n## Installation\n\n```\npip install aocr\n```\n\nNote: Tensorflow and Numpy will be installed as dependencies. Additional dependencies are `PIL`/`Pillow`, `distance`, and `six`.\n\n## Usage\n\n### Create a dataset\n\nTo build a TFRecords dataset, you need a collection of images and an annotation file with their respective labels.\n\n```\naocr dataset ./datasets/annotations-training.txt ./datasets/training.tfrecords\naocr dataset ./datasets/annotations-testing.txt ./datasets/testing.tfrecords\n```\n\nAnnotations are simple text files containing the image paths (either absolute or relative to your working dir) and their corresponding labels:\n\n```\ndatasets/images/hello.jpg hello\ndatasets/images/world.jpg world\n```\n\n### Train\n\n```\naocr train ./datasets/training.tfrecords\n```\n\nA new model will be created, and the training will start. Note that it takes quite a long time to reach convergence, since we are training the CNN and attention model simultaneously.\n\nThe `--steps-per-checkpoint` parameter determines how often the model checkpoints will be saved (the default output dir is `checkpoints/`).\n\n**Important:** there is a lot of available training options. See the CLI help or the `parameters` section of this README.\n\n### Test and visualize\n\n```\naocr test ./datasets/testing.tfrecords\n```\n\nAdditionally, you can visualize the attention results during testing (saved to `out/` by default):\n\n```\naocr test --visualize ./datasets/testing.tfrecords\n```\n\nExample output images in `results/correct`:\n\nImage 0 (j/j):\n\n![example image 0](http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_0.jpg)\n\nImage 1 (u/u):\n\n![example image 1](http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_1.jpg)\n\nImage 2 (n/n):\n\n![example image 2](http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_2.jpg)\n\nImage 3 (g/g):\n\n![example image 3](http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_3.jpg)\n\nImage 4 (l/l):\n\n![example image 4](http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_4.jpg)\n\nImage 5 (e/e):\n\n![example image 5](http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_5.jpg)\n\n### Export\n\nAfter the model is trained and a checkpoint is available, it can be exported as either a frozen graph or a SavedModel.\n\n```bash\n\n# SavedModel (default):\naocr export ./exported-model\n\n# Frozen graph:\naocr export --format=frozengraph ./exported-model\n\n```\n\nLoad weights from the latest checkpoints and export the model into the `./exported-model` directory.\n\n### Serving\n\nExported SavedModel can be served as a HTTP REST API using [Tensorflow Serving](https://github.com/tensorflow/serving). You can start the server by running following command:\n\n```\ntensorflow_model_server --port=9000 --rest_api_port=9001 --model_name=yourmodelname --model_base_path=./exported-model\n```\n\n**Note**: tensorflow_model_server requires a sub-directory with the version number to be present and inside it the files exported in the previous step. So you need to manually move contents of `exported-model` into `exported-model/1`.\n\nNow you can send a prediction request to the running server, for example:\n\n```\ncurl -X POST \\\n  http://localhost:9001/v1/models/aocr:predict \\\n  -H 'cache-control: no-cache' \\\n  -H 'content-type: application/json' \\\n  -d '{\n  \"signature_name\": \"serving_default\",\n  \"inputs\": {\n     \t\"input\": { \"b64\": \"<your image encoded as base64>\" }\n  }\n}'\n```\n\nREST API requires binary inputs to be encoded as Base64 and wrapped in an object containing `b64` key. [See 'Encoding binary values' in Tensorflow Serving documentation](https://www.tensorflow.org/serving/api_rest#encoding_binary_values)\n\n\n\n## Google Cloud ML Engine\n\nTo train the model in the [Google Cloud Machine Learning Engine](https://cloud.google.com/ml-engine/), upload the training dataset into a Google Cloud Storage bucket and start a training job with the `gcloud` tool.\n\n1. Set the environment variables:\n\n```\n# Prefix for the job name.\nexport JOB_PREFIX=\"aocr\"\n\n# Region to launch the training job in.\n# Should be the same as the storage bucket region.\nexport REGION=\"us-central1\"\n\n# Your storage bucket.\nexport GS_BUCKET=\"gs://aocr-bucket\"\n\n# Path to store your training dataset in the bucket.\nexport DATASET_UPLOAD_PATH=\"training.tfrecords\"\n```\n\n2. Upload the training dataset:\n\n```\ngsutil cp ./datasets/training.tfrecords $GS_BUCKET/$DATASET_UPLOAD_PATH\n```\n\n3. Launch the ML Engine job:\n\n```\nexport NOW=$(date +\"%Y%m%d_%H%M%S\")\nexport JOB_NAME=\"$JOB_PREFIX$NOW\"\nexport JOB_DIR=\"$GS_BUCKET/$JOB_NAME\"\n\ngcloud ml-engine jobs submit training $JOB_NAME \\\n    --job-dir=$JOB_DIR \\\n    --module-name=aocr \\\n    --package-path=aocr \\\n    --region=$REGION \\\n    --scale-tier=BASIC_GPU \\\n    --runtime-version=1.2 \\\n    -- \\\n    train $GS_BUCKET/$DATASET_UPLOAD_PATH \\\n    --steps-per-checkpoint=500 \\\n    --batch-size=512 \\\n    --num-epoch=20\n```\n\n## Parameters\n\n### Global\n\n* `log-path`: Path for the log file.\n\n### Testing\n\n* `visualize`: Output the attention maps on the original image.\n\n### Exporting\n\n* `format`: Format for the export (either `savedmodel` or `frozengraph`).\n\n### Training\n\n* `steps-per-checkpoint`: Checkpointing (print perplexity, save model) per how many steps\n* `num-epoch`: The number of whole data passes.\n* `batch-size`: Batch size.\n* `initial-learning-rate`: Initial learning rate, note the we use AdaDelta, so the initial value does not matter much.\n* `target-embedding-size`: Embedding dimension for each target.\n* `attn-num-hidden`: Number of hidden units in attention decoder cell.\n* `attn-num-layers`: Number of layers in attention decoder cell. (Encoder number of hidden units will be `attn-num-hidden`*`attn-num-layers`).\n* `no-resume`: Create new weights even if there are checkpoints present.\n* `max-gradient-norm`: Clip gradients to this norm.\n* `no-gradient-clipping`: Do not perform gradient clipping.\n* `gpu-id`: GPU to use.\n* `use-gru`: Use GRU cells instead of LSTM.\n* `max-width`: Maximum width for the input images. WARNING: images with the width higher than maximum will be discarded.\n* `max-height`: Maximum height for the input images.\n* `max-prediction`: Maximum length of the predicted word/phrase.\n\n## References\n\n[Convert a formula to its LaTex source](https://github.com/harvardnlp/im2markup)\n\n[What You Get Is What You See: A Visual Markup Decompiler](https://arxiv.org/pdf/1609.04938.pdf)\n\n[Torch attention OCR](https://github.com/da03/torch-Attention-OCR)\n", "description_content_type": "", "docs_url": null, "download_url": "https://github.com/emedvedev/attention-ocr/archive/0.7.6.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/emedvedev/attention-ocr", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "aocr", "package_url": "https://pypi.org/project/aocr/", "platform": "", "project_url": "https://pypi.org/project/aocr/", "project_urls": {"Download": "https://github.com/emedvedev/attention-ocr/archive/0.7.6.tar.gz", "Homepage": "https://github.com/emedvedev/attention-ocr"}, "release_url": "https://pypi.org/project/aocr/0.7.6/", "requires_dist": null, "requires_python": "", "summary": "Optical character recognition model for Tensorflow based on Visual Attention.", "version": "0.7.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            # Attention-based OCR<br><br>Visual attention-based OCR model for image recognition with additional tools for creating TFRecords datasets and exporting the trained model with weights as a [SavedModel](https://www.tensorflow.org/api_docs/python/tf/saved_model) or a frozen graph.<br><br>## Acknowledgements<br><br>This project is based on a model by [Qi Guo](http://qiguo.ml) and [Yuntian Deng](https://github.com/da03). You can find the original model in the [da03/Attention-OCR](https://github.com/da03/Attention-OCR) repository.<br><br>## The model<br><br>Authors: [Qi Guo](http://qiguo.ml) and [Yuntian Deng](https://github.com/da03).<br><br>The model first runs a sliding CNN on the image (images are resized to height 32 while preserving aspect ratio). Then an LSTM is stacked on top of the CNN. Finally, an attention model is used as a decoder for producing the final outputs.<br><br>![OCR example](http://cs.cmu.edu/~yuntiand/OCR-2.jpg)<br><br>## Installation<br><br>```<br>pip install aocr<br>```<br><br>Note: Tensorflow and Numpy will be installed as dependencies. Additional dependencies are `PIL`/`Pillow`, `distance`, and `six`.<br><br>## Usage<br><br>### Create a dataset<br><br>To build a TFRecords dataset, you need a collection of images and an annotation file with their respective labels.<br><br>```<br>aocr dataset ./datasets/annotations-training.txt ./datasets/training.tfrecords<br>aocr dataset ./datasets/annotations-testing.txt ./datasets/testing.tfrecords<br>```<br><br>Annotations are simple text files containing the image paths (either absolute or relative to your working dir) and their corresponding labels:<br><br>```<br>datasets/images/hello.jpg hello<br>datasets/images/world.jpg world<br>```<br><br>### Train<br><br>```<br>aocr train ./datasets/training.tfrecords<br>```<br><br>A new model will be created, and the training will start. Note that it takes quite a long time to reach convergence, since we are training the CNN and attention model simultaneously.<br><br>The `--steps-per-checkpoint` parameter determines how often the model checkpoints will be saved (the default output dir is `checkpoints/`).<br><br>**Important:** there is a lot of available training options. See the CLI help or the `parameters` section of this README.<br><br>### Test and visualize<br><br>```<br>aocr test ./datasets/testing.tfrecords<br>```<br><br>Additionally, you can visualize the attention results during testing (saved to `out/` by default):<br><br>```<br>aocr test --visualize ./datasets/testing.tfrecords<br>```<br><br>Example output images in `results/correct`:<br><br>Image 0 (j/j):<br><br>![example image 0](http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_0.jpg)<br><br>Image 1 (u/u):<br><br>![example image 1](http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_1.jpg)<br><br>Image 2 (n/n):<br><br>![example image 2](http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_2.jpg)<br><br>Image 3 (g/g):<br><br>![example image 3](http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_3.jpg)<br><br>Image 4 (l/l):<br><br>![example image 4](http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_4.jpg)<br><br>Image 5 (e/e):<br><br>![example image 5](http://cs.cmu.edu/~yuntiand/2evaluation_data_icdar13_images_word_370.png/image_5.jpg)<br><br>### Export<br><br>After the model is trained and a checkpoint is available, it can be exported as either a frozen graph or a SavedModel.<br><br>```bash<br><br># SavedModel (default):<br>aocr export ./exported-model<br><br># Frozen graph:<br>aocr export --format=frozengraph ./exported-model<br><br>```<br><br>Load weights from the latest checkpoints and export the model into the `./exported-model` directory.<br><br>### Serving<br><br>Exported SavedModel can be served as a HTTP REST API using [Tensorflow Serving](https://github.com/tensorflow/serving). You can start the server by running following command:<br><br>```<br>tensorflow_model_server --port=9000 --rest_api_port=9001 --model_name=yourmodelname --model_base_path=./exported-model<br>```<br><br>**Note**: tensorflow_model_server requires a sub-directory with the version number to be present and inside it the files exported in the previous step. So you need to manually move contents of `exported-model` into `exported-model/1`.<br><br>Now you can send a prediction request to the running server, for example:<br><br>```<br>curl -X POST \\<br>  http://localhost:9001/v1/models/aocr:predict \\<br>  -H 'cache-control: no-cache' \\<br>  -H 'content-type: application/json' \\<br>  -d '{<br>  \"signature_name\": \"serving_default\",<br>  \"inputs\": {<br>     \t\"input\": { \"b64\": \"&lt;your image encoded as base64&gt;\" }<br>  }<br>}'<br>```<br><br>REST API requires binary inputs to be encoded as Base64 and wrapped in an object containing `b64` key. [See 'Encoding binary values' in Tensorflow Serving documentation](https://www.tensorflow.org/serving/api_rest#encoding_binary_values)<br><br><br><br>## Google Cloud ML Engine<br><br>To train the model in the [Google Cloud Machine Learning Engine](https://cloud.google.com/ml-engine/), upload the training dataset into a Google Cloud Storage bucket and start a training job with the `gcloud` tool.<br><br>1. Set the environment variables:<br><br>```<br># Prefix for the job name.<br>export JOB_PREFIX=\"aocr\"<br><br># Region to launch the training job in.<br># Should be the same as the storage bucket region.<br>export REGION=\"us-central1\"<br><br># Your storage bucket.<br>export GS_BUCKET=\"gs://aocr-bucket\"<br><br># Path to store your training dataset in the bucket.<br>export DATASET_UPLOAD_PATH=\"training.tfrecords\"<br>```<br><br>2. Upload the training dataset:<br><br>```<br>gsutil cp ./datasets/training.tfrecords $GS_BUCKET/$DATASET_UPLOAD_PATH<br>```<br><br>3. Launch the ML Engine job:<br><br>```<br>export NOW=$(date +\"%Y%m%d_%H%M%S\")<br>export JOB_NAME=\"$JOB_PREFIX$NOW\"<br>export JOB_DIR=\"$GS_BUCKET/$JOB_NAME\"<br><br>gcloud ml-engine jobs submit training $JOB_NAME \\<br>    --job-dir=$JOB_DIR \\<br>    --module-name=aocr \\<br>    --package-path=aocr \\<br>    --region=$REGION \\<br>    --scale-tier=BASIC_GPU \\<br>    --runtime-version=1.2 \\<br>    -- \\<br>    train $GS_BUCKET/$DATASET_UPLOAD_PATH \\<br>    --steps-per-checkpoint=500 \\<br>    --batch-size=512 \\<br>    --num-epoch=20<br>```<br><br>## Parameters<br><br>### Global<br><br>* `log-path`: Path for the log file.<br><br>### Testing<br><br>* `visualize`: Output the attention maps on the original image.<br><br>### Exporting<br><br>* `format`: Format for the export (either `savedmodel` or `frozengraph`).<br><br>### Training<br><br>* `steps-per-checkpoint`: Checkpointing (print perplexity, save model) per how many steps<br>* `num-epoch`: The number of whole data passes.<br>* `batch-size`: Batch size.<br>* `initial-learning-rate`: Initial learning rate, note the we use AdaDelta, so the initial value does not matter much.<br>* `target-embedding-size`: Embedding dimension for each target.<br>* `attn-num-hidden`: Number of hidden units in attention decoder cell.<br>* `attn-num-layers`: Number of layers in attention decoder cell. (Encoder number of hidden units will be `attn-num-hidden`*`attn-num-layers`).<br>* `no-resume`: Create new weights even if there are checkpoints present.<br>* `max-gradient-norm`: Clip gradients to this norm.<br>* `no-gradient-clipping`: Do not perform gradient clipping.<br>* `gpu-id`: GPU to use.<br>* `use-gru`: Use GRU cells instead of LSTM.<br>* `max-width`: Maximum width for the input images. WARNING: images with the width higher than maximum will be discarded.<br>* `max-height`: Maximum height for the input images.<br>* `max-prediction`: Maximum length of the predicted word/phrase.<br><br>## References<br><br>[Convert a formula to its LaTex source](https://github.com/harvardnlp/im2markup)<br><br>[What You Get Is What You See: A Visual Markup Decompiler](https://arxiv.org/pdf/1609.04938.pdf)<br><br>[Torch attention OCR](https://github.com/da03/torch-Attention-OCR)<br>\n          </div>"}, "last_serial": 5163404, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "7d832dbb0caa5aae9e4f8ba15bf2361d", "sha256": "0fefb96c65bc3f6ca2524005c122740c52e919036354f3608e0be4571bfbd0c6"}, "downloads": -1, "filename": "aocr-0.0.1.tar.gz", "has_sig": false, "md5_digest": "7d832dbb0caa5aae9e4f8ba15bf2361d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28909, "upload_time": "2017-07-21T19:21:16", "upload_time_iso_8601": "2017-07-21T19:21:16.039914Z", "url": "https://files.pythonhosted.org/packages/c1/ea/3c58d216ab6e7cfcd9c3882ac577d8f6040506271d1222dcee4c45929e26/aocr-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "5c56df2353d9877d71f077c504ee92b2", "sha256": "6fa5cfd3fa170aba44b34fc02d8058e92d84f440caa34981fe049e068d8099af"}, "downloads": -1, "filename": "aocr-0.0.2.tar.gz", "has_sig": false, "md5_digest": "5c56df2353d9877d71f077c504ee92b2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28669, "upload_time": "2017-07-21T19:36:31", "upload_time_iso_8601": "2017-07-21T19:36:31.119171Z", "url": "https://files.pythonhosted.org/packages/df/fd/cc669956579e6d4d7cb9e5fe046f04c0d93455dd276d091aac762059adf1/aocr-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "7217af105da0c5f687e4aa0bcdfb7962", "sha256": "5eff55bf8ef390a59b4dec461a199bf396059b4613ed9ae46514e315afefe00e"}, "downloads": -1, "filename": "aocr-0.0.3.tar.gz", "has_sig": false, "md5_digest": "7217af105da0c5f687e4aa0bcdfb7962", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29539, "upload_time": "2017-07-24T04:42:13", "upload_time_iso_8601": "2017-07-24T04:42:13.943360Z", "url": "https://files.pythonhosted.org/packages/67/2e/cf86ad88b59926c84040502bda6582d7c7085a53e39171f092ac3ad2b4b4/aocr-0.0.3.tar.gz", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "2c9a1eb6204a750f6e11c2c171343f18", "sha256": "1c966ddeb5c3f9567fed4d79edfc32ad45726a6f208e905e9340d27f470dfb70"}, "downloads": -1, "filename": "aocr-0.1.0.tar.gz", "has_sig": false, "md5_digest": "2c9a1eb6204a750f6e11c2c171343f18", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29362, "upload_time": "2017-08-04T23:50:48", "upload_time_iso_8601": "2017-08-04T23:50:48.600510Z", "url": "https://files.pythonhosted.org/packages/ea/e5/3640f291f9d838a377894226f3d5e93bfd5ae64ad1b1e6f6b513907769e8/aocr-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "0eb71a8efc80116d248264424653f723", "sha256": "84569ed6b2fa1fc0b8e6582f11a7d2dc639c25d9aace878e6a35412d6ee8e9dd"}, "downloads": -1, "filename": "aocr-0.1.1.tar.gz", "has_sig": false, "md5_digest": "0eb71a8efc80116d248264424653f723", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29492, "upload_time": "2017-08-04T23:59:31", "upload_time_iso_8601": "2017-08-04T23:59:31.262115Z", "url": "https://files.pythonhosted.org/packages/06/9e/987391c73b650ba3151be17cd4823b4f98813cf6c5c5a7e129bda2cdc5a2/aocr-0.1.1.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "30672b8b461e80c0bda27cb0db6e3ac1", "sha256": "8de0197167ab776f8f3d8781533673accf2bfe52944c997fb5b14375ac9968dd"}, "downloads": -1, "filename": "aocr-0.2.0.tar.gz", "has_sig": false, "md5_digest": "30672b8b461e80c0bda27cb0db6e3ac1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29136, "upload_time": "2017-08-11T09:08:30", "upload_time_iso_8601": "2017-08-11T09:08:30.222760Z", "url": "https://files.pythonhosted.org/packages/e0/92/236e93de0907752b71500dc56f8c32cba1c95853865f837506fef521d854/aocr-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "4de6b706d5509805b11e5dba9db4bfea", "sha256": "c712b892954a06c8d60009589a58224f314299c6a8595cd6f3d50a57066c76c9"}, "downloads": -1, "filename": "aocr-0.2.1.tar.gz", "has_sig": false, "md5_digest": "4de6b706d5509805b11e5dba9db4bfea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26788, "upload_time": "2017-08-13T11:29:07", "upload_time_iso_8601": "2017-08-13T11:29:07.775865Z", "url": "https://files.pythonhosted.org/packages/a6/70/a08e7b2d2eed95003481aba4d2a7edf452f082f0090638dea676f8e5219d/aocr-0.2.1.tar.gz", "yanked": false}], "0.2.2": [{"comment_text": "", "digests": {"md5": "2e0ed4b37b153ee4096bcf21a7287c4f", "sha256": "8eab7ceefa39f5ff0cea7ba642240b198fb9f291e93c39698ed4d5a0a906a382"}, "downloads": -1, "filename": "aocr-0.2.2.tar.gz", "has_sig": false, "md5_digest": "2e0ed4b37b153ee4096bcf21a7287c4f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26772, "upload_time": "2017-09-28T20:30:07", "upload_time_iso_8601": "2017-09-28T20:30:07.361643Z", "url": "https://files.pythonhosted.org/packages/23/12/194e0aad8fd83422e0b6fb72fc0f1ca2eef6f9f5d8b3b8112ac97fc54574/aocr-0.2.2.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "ed70d22a021005f4a8c0a6974123469d", "sha256": "126e318ca4d8bc2d893103c9d471358785185641451bd4ad5bec011f1bd2f0df"}, "downloads": -1, "filename": "aocr-0.3.0.tar.gz", "has_sig": false, "md5_digest": "ed70d22a021005f4a8c0a6974123469d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29989, "upload_time": "2017-10-06T23:00:56", "upload_time_iso_8601": "2017-10-06T23:00:56.335984Z", "url": "https://files.pythonhosted.org/packages/71/3e/87bff87e8ad53fc0ad81953f4d22da95af8b63ea776130b56ffb363a581c/aocr-0.3.0.tar.gz", "yanked": false}], "0.5.0": [{"comment_text": "", "digests": {"md5": "48aa579ebf90a16356ee0e0db93dfd3c", "sha256": "03c9348a070f37b66a36572a088659421003f394781aea581884ea1c44e45011"}, "downloads": -1, "filename": "aocr-0.5.0.tar.gz", "has_sig": false, "md5_digest": "48aa579ebf90a16356ee0e0db93dfd3c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31186, "upload_time": "2018-01-04T06:01:41", "upload_time_iso_8601": "2018-01-04T06:01:41.065111Z", "url": "https://files.pythonhosted.org/packages/4f/09/4215d54f1d34db7ea09a2386853a8685a07fb9ccca7d24d07c1e83d33ba8/aocr-0.5.0.tar.gz", "yanked": false}], "0.6.0": [{"comment_text": "", "digests": {"md5": "751c798f122402eb0b524241506f5a95", "sha256": "288c31245bb7eb056a2efd6d069e84911fd2a0ce9a17962cd3a7d1ceee248598"}, "downloads": -1, "filename": "aocr-0.6.0.tar.gz", "has_sig": false, "md5_digest": "751c798f122402eb0b524241506f5a95", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31122, "upload_time": "2018-01-25T08:39:19", "upload_time_iso_8601": "2018-01-25T08:39:19.088939Z", "url": "https://files.pythonhosted.org/packages/3a/45/73825adb3bfc4f079f526d29b8c52994e32ebae1d5a187e4571f9d02c394/aocr-0.6.0.tar.gz", "yanked": false}], "0.6.1": [{"comment_text": "", "digests": {"md5": "e962ff827d2379732b103e8d9afc861b", "sha256": "1cc78107200f966f683fa54622e221abcc63103633e014a0098d7dff041e4ff2"}, "downloads": -1, "filename": "aocr-0.6.1.tar.gz", "has_sig": false, "md5_digest": "e962ff827d2379732b103e8d9afc861b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31154, "upload_time": "2018-02-01T04:20:55", "upload_time_iso_8601": "2018-02-01T04:20:55.671137Z", "url": "https://files.pythonhosted.org/packages/41/97/297fbebd1e4a4d100097056f563e4dc5d2cb7dea85c5ff4a3987276c124a/aocr-0.6.1.tar.gz", "yanked": false}], "0.6.2": [{"comment_text": "", "digests": {"md5": "7a3bfd4f55b0b57f9b5032ba6310e4ae", "sha256": "9ded959999f2524212c2ac7607a1841edd0efd41973dbbc8e48dde928a94dccf"}, "downloads": -1, "filename": "aocr-0.6.2.tar.gz", "has_sig": false, "md5_digest": "7a3bfd4f55b0b57f9b5032ba6310e4ae", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31163, "upload_time": "2018-02-01T10:29:44", "upload_time_iso_8601": "2018-02-01T10:29:44.585344Z", "url": "https://files.pythonhosted.org/packages/d4/87/d5560b874f4e9c4bcf2b2b65a93118355fa65d3147a1f477a5154e9b576d/aocr-0.6.2.tar.gz", "yanked": false}], "0.6.3": [{"comment_text": "", "digests": {"md5": "c100d75ebbdd80d3dcfeb97c00fe3393", "sha256": "f8d79c7f3868da86492971d8d31cb2a99dc1dfaf6e8f5bd7d3a4c064615b37a4"}, "downloads": -1, "filename": "aocr-0.6.3.tar.gz", "has_sig": false, "md5_digest": "c100d75ebbdd80d3dcfeb97c00fe3393", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31169, "upload_time": "2018-02-05T12:19:55", "upload_time_iso_8601": "2018-02-05T12:19:55.180423Z", "url": "https://files.pythonhosted.org/packages/1d/ab/db7bbb2ce0be2ecb86a0f1403a39ec96b5dbf64061f0068fd892b6ef55c1/aocr-0.6.3.tar.gz", "yanked": false}], "0.7.0": [{"comment_text": "", "digests": {"md5": "7045a2ae7d86e64482cfa7c68b6fdfb0", "sha256": "d9066a96bfa1517c54110c6702e3f3520c66f359c689ecc415e8a39f80f59e27"}, "downloads": -1, "filename": "aocr-0.7.0.tar.gz", "has_sig": false, "md5_digest": "7045a2ae7d86e64482cfa7c68b6fdfb0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31179, "upload_time": "2018-02-21T11:00:40", "upload_time_iso_8601": "2018-02-21T11:00:40.298355Z", "url": "https://files.pythonhosted.org/packages/f5/f4/7f85565a3e049263e8e386247f3803c50a75262bb1fe3ecd9f4a9a5f1288/aocr-0.7.0.tar.gz", "yanked": false}], "0.7.1": [{"comment_text": "", "digests": {"md5": "1a7a34907efeb7df69b879c6e3b030da", "sha256": "28a118ecd7c4a1cea1f6ec2621078724228130931f4e14bbb5a134f881504d43"}, "downloads": -1, "filename": "aocr-0.7.1.tar.gz", "has_sig": false, "md5_digest": "1a7a34907efeb7df69b879c6e3b030da", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32298, "upload_time": "2018-03-23T17:47:13", "upload_time_iso_8601": "2018-03-23T17:47:13.396570Z", "url": "https://files.pythonhosted.org/packages/39/61/7345ab10a9835c9f8880315c84690d0f79e1cdf71f13b90fe0457340b159/aocr-0.7.1.tar.gz", "yanked": false}], "0.7.2": [{"comment_text": "", "digests": {"md5": "37a4e6d835e46e0827e69d39b003fb89", "sha256": "0bc1df3322e17913ee3b14775d9d559bf4d616363efcb06033d590543b6fde12"}, "downloads": -1, "filename": "aocr-0.7.2.tar.gz", "has_sig": false, "md5_digest": "37a4e6d835e46e0827e69d39b003fb89", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32303, "upload_time": "2018-03-31T17:41:50", "upload_time_iso_8601": "2018-03-31T17:41:50.038001Z", "url": "https://files.pythonhosted.org/packages/25/2e/c6d7aa7b62cd9d2fee786581518a6d1604cedb8cab62595d3b49bd5ac899/aocr-0.7.2.tar.gz", "yanked": false}], "0.7.3": [{"comment_text": "", "digests": {"md5": "e045d2446a2a5e18b94429e87951c55f", "sha256": "bb9b26c23f48dc469f1608fb43c61fc47e535539519ede18417cd659b1bcb105"}, "downloads": -1, "filename": "aocr-0.7.3.tar.gz", "has_sig": false, "md5_digest": "e045d2446a2a5e18b94429e87951c55f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32292, "upload_time": "2018-04-11T06:28:26", "upload_time_iso_8601": "2018-04-11T06:28:26.884262Z", "url": "https://files.pythonhosted.org/packages/d2/25/6b5176466e69905575a003faeb12d75b6f1c8cee1c9309a171e69a84c5ac/aocr-0.7.3.tar.gz", "yanked": false}], "0.7.4": [{"comment_text": "", "digests": {"md5": "f09fa8ff2d237685a5dd59238314769a", "sha256": "657e4e15bc0a2e06b9db061a60f1a392fbfdb971bdb0339f066b6416154b72d7"}, "downloads": -1, "filename": "aocr-0.7.4.tar.gz", "has_sig": false, "md5_digest": "f09fa8ff2d237685a5dd59238314769a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32325, "upload_time": "2018-05-02T03:34:44", "upload_time_iso_8601": "2018-05-02T03:34:44.376931Z", "url": "https://files.pythonhosted.org/packages/5c/01/67b6484865316da8d5eb2fdbda8405291f160a5b6f09f1be2f805f7ae972/aocr-0.7.4.tar.gz", "yanked": false}], "0.7.5": [{"comment_text": "", "digests": {"md5": "477a2fcf5b370d0acd273fd0399627f6", "sha256": "a96ad8e457b65f941750d605dbecea81e946bd79b3e387273f49e214a5521dce"}, "downloads": -1, "filename": "aocr-0.7.5.tar.gz", "has_sig": false, "md5_digest": "477a2fcf5b370d0acd273fd0399627f6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31081, "upload_time": "2018-07-01T09:16:15", "upload_time_iso_8601": "2018-07-01T09:16:15.686134Z", "url": "https://files.pythonhosted.org/packages/35/9d/525814e57a600c5bb3a20d221c32b927ae0f6a4dc2e55ba2cf62a938566d/aocr-0.7.5.tar.gz", "yanked": false}], "0.7.6": [{"comment_text": "", "digests": {"md5": "c15892b0e32208b3f1e45a9e1c4cd42c", "sha256": "8f46a713194a63b238e7e99ba10b49207e9b113e634eaf9f1c61405337ad0f73"}, "downloads": -1, "filename": "aocr-0.7.6.tar.gz", "has_sig": false, "md5_digest": "c15892b0e32208b3f1e45a9e1c4cd42c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31080, "upload_time": "2019-04-19T05:28:27", "upload_time_iso_8601": "2019-04-19T05:28:27.856522Z", "url": "https://files.pythonhosted.org/packages/23/8b/0c566b2c43eade33fad729e898b3288db728c886572bcb0b86039155dc8a/aocr-0.7.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c15892b0e32208b3f1e45a9e1c4cd42c", "sha256": "8f46a713194a63b238e7e99ba10b49207e9b113e634eaf9f1c61405337ad0f73"}, "downloads": -1, "filename": "aocr-0.7.6.tar.gz", "has_sig": false, "md5_digest": "c15892b0e32208b3f1e45a9e1c4cd42c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31080, "upload_time": "2019-04-19T05:28:27", "upload_time_iso_8601": "2019-04-19T05:28:27.856522Z", "url": "https://files.pythonhosted.org/packages/23/8b/0c566b2c43eade33fad729e898b3288db728c886572bcb0b86039155dc8a/aocr-0.7.6.tar.gz", "yanked": false}], "timestamp": "Thu May  7 18:17:55 2020"}