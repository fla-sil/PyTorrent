{"info": {"author": "Amazon", "author_email": "tdiethe@amazon.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering"], "description": "# Automatic Discovery of Privacy-Utility Pareto Fronts\n\nThis repo contains the underlying code for all the experiments from the paper: \"Automatic Discovery of Privacy-Utility Pareto Fronts\" (https://arxiv.org/abs/1905.10862).\n\nTo cite this article:\n\n```\n@article{avent2019automatic,\n  title={Automatic Discovery of Privacy-Utility Pareto Fronts},\n  author={Avent, Brendan and Gonzalez, Javier and Diethe, Tom and Paleyes, Andrei and Balle, Borja},\n  journal={arXiv preprint arXiv:1905.10862},\n  year={2019}\n}\n```\n\n## Installation\n\nYou can install the library via pypi, for example in a clean conda environment:\n\n```\nconda create -n dpareto python=3.7\nconda activate dpareto\npip install dpareto\n```\n\nCurrently supported Python versions: 3.5, 3.6, 3.7\n\nNote: If you get a message like `Direct url requirement (like gpflow@ git+https://github.com/GPflow/GPflow.git@ce5ad7ea75687fb0bf178b25f62855fc861eb10f) are not allowed for dependencies`, then you need to upgrade your pip version:\n\n```\npip install -U pip\n```\n\n\n\nThis project is built in layers, so we'll give a bottom-up explanation for each layer and how to run them.\nFirst, we'll go through the dependencies and what needs to be done to run the code.\n\n\n## Major dependencies\n Various portions of the code make use of multiprocessing capabilities of CPU(s) for parallelism, and GPU(s) for efficient model-training. \n \n - mxnet-cu92mkl (Versions close to this will probably work as well, but be sure to install with the CUDA 'cu' option; MKL isn't necessary, it just helps with performance.)\n - autodp (Version 0.1 is the only one released at the time of writing this.)\n - multiprocessing\n - psutil\n - gpflowopt\n\n**Note: All code will require the root of this project to be in your PATH (and also possibly PYTHONPATH), or set as the \"working directory\" in your IDE, etc. so that Python can find the dpareto module.**\n\n We'll explain what needs to be done to get/install autodp and gpflowopt, and assume that you have (or can easily get) any of the other dependencies.\n\n\n### autodp\n\nautodp is an open-source moments accountant implementation in Python, used for computing privacy loss.\n\nIt can be installed with pip, and the code is available here:  https://github.com/yuxiangw/autodp\n\n\n### gpflowopt\n\nGPFlowOpt is used for the Pareto front computation and optimization. It in turn relies on an old version of GPFlow.\n\nThe following instructions (also provided in their GitHub repo) will install GPFlowOpt as well as the compatible version of GPFlow and Tensorflow. Note that these instructions may require pip version <= 18.1.\n\n 1) Clone this repo somewhere:  https://github.com/GPflow/GPflowOpt.git\n 2) In the repo, execute:  pip install . --process-dependency-links\n\nAlso note that this may install an old version of Tensorflow-GPU that may require a different version of CUDA. If executing the code in this project yields CUDA-related crashes, check that you have the proper version of CUDA and cuDNN installed for whatever the Tensorflow version is (and remember that multiple versions of CUDA can coexist on the same system). \n\n## Project structure\n\n### Data\n\nThis project uses a processed version of the [Adult dataset](https://www.csie.ntu.edu.tw/%7Ecjlin/libsvmtools/datasets/binary.html) as well as the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). The Adult dataset is downloaded and processed by running the downloader.py script in the `data/adult/` directory from the project root:\n```\npython data/adult/downloader.py\n```\nThe MNIST dataset is imported automatically using the MXNet Gluon API. \n\n### Feedforward Neural Net and Optimizer\n\nThe implementations (and usages) of the feedforward neural net code and the parameter optimizer are tightly coupled, so we'll discuss them together.\n\n`dp_optimizer.py` is an abstract class providing most of the implementation of differentially-private SGD (essentially as detailed in Abadi et al's \"Deep Learning with Differential Privacy\", with some minor modifications).\n`dp_sgd.py` completes the implementation, and `dp_adam.py` extends and completes the implementation to create a differentially private variant of the ADAM optimizer.\n\n`dp_feedforward_net.py` provides the abstract class and implementations of most methods necessary for building a feedforward net.\nIts child classes reside in mnist and adult directories each as `base.py`, providing the concrete structure of the network (MLP and SLP respectively) and the dataset-specific functionality for their respective problems.\nThe `base.py` files in each directory are still abstract classes. The adult directory's `base.py` is then extended to implement logistic regression and SVM models to be trained on the Adult dataset with both the DP SGD and ADAM optimizers. The mnist directory's `base.py` is similarly extended to be trained on the MNIST dataset with the DP SGD and ADAM optimizers.\n\n`dp_feedforward_net.py` specifies one of the concrete DP optimizers.\nThe reason that the `dp_feedforward_net.py` code and optimization code are tightly coupled is purely for performance reasons: we have manually created our network and we are manually specifying how batch computations are done (for both the actual results as well as the gradients of the parameters).\n\n#### How to run\nEach of the concrete child classes (e.g., `dpareto/models/adult/lr/dp_sgd.py`) contain main functions which can be run (primarily for testing/debugging purposes).\nAt a high level, the input is a set of hyperparameters and the output is a tuple (privacy, utility) (where privacy is the epsilon differential privacy value and utility is classification accuracy between 0 and 1).\nSee the main function in those files to get a better idea.\n\nMake sure to run experiments from root of the repo, so that relative paths to Adult dataset work.\n\n\n### Random Sampling of Hyperparameters\n\n`random_sampling/harness.py` provides the abstract class and implementations of most methods necessary for performing random sampling on one of these concrete dp_feedforward_net problems.\n\nFor a simple example of how this can be used, see `examples/random_sampling.py`.\n\nThe primary inputs here are:\n - The _distribution_ of hyperparameters to sample from for the specific problem.\n - The number of instances; i.e., how many hyperparameter samples to take.\n\nThe output is the full set of results -- that is: a list of tuples where the first item in each is the input (a single random setting of hyperparameters), and the second and third items respectively are the privacy and utility that resulted from an execution of that setting of hyperparameters.\n\nSince these instances are fully independent, we (optionally) take advantage of parallel processing by also specifying the number of workers to use to compute all the instances.\nWe can also provide command line arguments to override the passed-in number of instances, number of workers, and device type (CPU or GPU).\nSee the main function in either child class file to get a better idea.\n\n### Grid Search of Hyperparameters\n\n`grid_search/harness.py` provides the abstract class and implementations of most methods necessary for performing random sampling on one of these concrete dp_feedforward_net problems.\n\nEverything here is analogous to the random sampling discussed above, and a simple example of its use is in `examples/grid_search.py`.\n\n### Computing and Optimizing the Pareto Front\n\n`hypervolume_improvement/harness.py` provides the abstract class and implementations of most methods necessary for computing and optimizing the pareto front.\n\nA simple example of its use is in `examples/hypervolume_improvement.py`.\n\nThere are several options for input here currently, but the primary idea is:\n 1) Pass in some initial points to kick-start the GP models for privacy and accuracy. This requires at least one run of random sampling. In our experiments we used 16 initial points.\n 2) Provide the \"anti-ideal\" point -- i.e., the reference point from which to compute the hypervolume from (think: top right point, and we want to optimize towards the bottom left). In our experiments we used [10, 0.999] based on the ranges of the output domain.\n 3) Provide the optimization domain -- i.e., the range of valid values for each hyperparameter that the optimizer should consider.\n 4) The number of points that the Bayesian optimizer should test. Note: this is the time-consuming portion, particularly because we can't currently do it in parallel.\nThe output is saved to disk, as:\n - The set of hyperparameters suggested by the BO framework at each iteration\n - Pareto front plots (the plot from the initial data, as well as the new plot for each point tested by the Bayesian optimizer).\n - The full state of the hypervolume improvement class object at each iteration, to allow extraction of any results at a later time.\n   - Due to a known bug related to the unpickling of an object which contains use of the multiprocessing package, extra steps must be taken to unpickle these object-state results. `experiments/scripts/picky_unpickler.py` was created to enable the extraction of specific results from these objects -- look through that file to see what is done and how.\n\n### Paper Experiments\nThe runnable code for all the experiments in the paper is located in the experiments/ directory.\nThe output_perturbation/ subdirectory contains the relevant code for the \"illustrative example\" of training a logistic regression model using output perturbation.\nThe svt/ subdirectory contains the relevant code for the \"illustrative example\" of the sparse vector technique algorithm.\nThe adult/ and mnist/ subdirectories contain the code for the experiments run on the various models trained on the Adult and MNIST datasets.\n\n\n#### Example\nLet's review an example of running a full experiment for an already provided algorithm. We will use Adult dataset/Logistic Regression/Adam optimizer. Switch to project's root folder, make sure dependencies above are installed, and update PATH and PYTHONPATH variables as needed. For example, this verifies that we have gpflowopt installed:\n\n```\npip freeze | grep -i gpflowopt\n```\n\nAnd this adds project's root folder to PATH, assuming we are already in that folder:\n```\nexport PATH=$PATH:$(pwd)\n```\n\nAs noted above, having at least one run of random sampling is required, so that we have some initial data to bootstrap the GP model. Let's do this:\n```\npython experiments/adult/dp_adult_lr_adam/random_sampling.py --workers 4 --instances 128\n```\nThis says we will use 4 CPUs concurrently on the machine, and that we would like to collect 128 random points.\n\nOnce this finishes, we can run the Bayesian optimization routine. It requires only one parameter, the path to the random sampling results, serialized with Python's pickle module. You can find it by browsing \"experiments/adult/dp_adult_lr_adam/results\" folder. Note that failed runs can also create results folders, so make sure to pick the correct file. Result folders are named according to corresponding timestamps.\n\n```\npython experiments/adult/dp_adult_lr_adam/hypervolume_improvement.py --init-data-file experiments/adult/dp_adult_lr_adam/results/random_sampling_results/1578475241956/full_results.pkl\n```\n\nAlternatively we could just edit the executed file directly, by adding this line:\n```\ninitial_data_options['filename'] = current_dir + '/results/random_sampling_results/1578475241956/full_results.pkl'\n```\n\nOnce this run finishes, results will be available in \"experiments/adult/dp_adult_lr_adam/results\" folder as well.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/amzn/differential-privacy-bayesian-optimization/archive/v_01.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/amzn/differential-privacy-bayesian-optimization", "keywords": "Differential privacy,Bayesian optimization", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "dpareto", "package_url": "https://pypi.org/project/dpareto/", "platform": "", "project_url": "https://pypi.org/project/dpareto/", "project_urls": {"Download": "https://github.com/amzn/differential-privacy-bayesian-optimization/archive/v_01.tar.gz", "Homepage": "https://github.com/amzn/differential-privacy-bayesian-optimization"}, "release_url": "https://pypi.org/project/dpareto/0.1.4/", "requires_dist": null, "requires_python": "", "summary": "Automatic Discovery of Privacy-Utility Pareto Fronts", "version": "0.1.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Automatic Discovery of Privacy-Utility Pareto Fronts</h1>\n<p>This repo contains the underlying code for all the experiments from the paper: \"Automatic Discovery of Privacy-Utility Pareto Fronts\" (<a href=\"https://arxiv.org/abs/1905.10862\" rel=\"nofollow\">https://arxiv.org/abs/1905.10862</a>).</p>\n<p>To cite this article:</p>\n<pre><code>@article{avent2019automatic,\n  title={Automatic Discovery of Privacy-Utility Pareto Fronts},\n  author={Avent, Brendan and Gonzalez, Javier and Diethe, Tom and Paleyes, Andrei and Balle, Borja},\n  journal={arXiv preprint arXiv:1905.10862},\n  year={2019}\n}\n</code></pre>\n<h2>Installation</h2>\n<p>You can install the library via pypi, for example in a clean conda environment:</p>\n<pre><code>conda create -n dpareto python=3.7\nconda activate dpareto\npip install dpareto\n</code></pre>\n<p>Currently supported Python versions: 3.5, 3.6, 3.7</p>\n<p>Note: If you get a message like <code>Direct url requirement (like gpflow@ git+https://github.com/GPflow/GPflow.git@ce5ad7ea75687fb0bf178b25f62855fc861eb10f) are not allowed for dependencies</code>, then you need to upgrade your pip version:</p>\n<pre><code>pip install -U pip\n</code></pre>\n<p>This project is built in layers, so we'll give a bottom-up explanation for each layer and how to run them.\nFirst, we'll go through the dependencies and what needs to be done to run the code.</p>\n<h2>Major dependencies</h2>\n<p>Various portions of the code make use of multiprocessing capabilities of CPU(s) for parallelism, and GPU(s) for efficient model-training.</p>\n<ul>\n<li>mxnet-cu92mkl (Versions close to this will probably work as well, but be sure to install with the CUDA 'cu' option; MKL isn't necessary, it just helps with performance.)</li>\n<li>autodp (Version 0.1 is the only one released at the time of writing this.)</li>\n<li>multiprocessing</li>\n<li>psutil</li>\n<li>gpflowopt</li>\n</ul>\n<p><strong>Note: All code will require the root of this project to be in your PATH (and also possibly PYTHONPATH), or set as the \"working directory\" in your IDE, etc. so that Python can find the dpareto module.</strong></p>\n<p>We'll explain what needs to be done to get/install autodp and gpflowopt, and assume that you have (or can easily get) any of the other dependencies.</p>\n<h3>autodp</h3>\n<p>autodp is an open-source moments accountant implementation in Python, used for computing privacy loss.</p>\n<p>It can be installed with pip, and the code is available here:  <a href=\"https://github.com/yuxiangw/autodp\" rel=\"nofollow\">https://github.com/yuxiangw/autodp</a></p>\n<h3>gpflowopt</h3>\n<p>GPFlowOpt is used for the Pareto front computation and optimization. It in turn relies on an old version of GPFlow.</p>\n<p>The following instructions (also provided in their GitHub repo) will install GPFlowOpt as well as the compatible version of GPFlow and Tensorflow. Note that these instructions may require pip version &lt;= 18.1.</p>\n<ol>\n<li>Clone this repo somewhere:  <a href=\"https://github.com/GPflow/GPflowOpt.git\" rel=\"nofollow\">https://github.com/GPflow/GPflowOpt.git</a></li>\n<li>In the repo, execute:  pip install . --process-dependency-links</li>\n</ol>\n<p>Also note that this may install an old version of Tensorflow-GPU that may require a different version of CUDA. If executing the code in this project yields CUDA-related crashes, check that you have the proper version of CUDA and cuDNN installed for whatever the Tensorflow version is (and remember that multiple versions of CUDA can coexist on the same system).</p>\n<h2>Project structure</h2>\n<h3>Data</h3>\n<p>This project uses a processed version of the <a href=\"https://www.csie.ntu.edu.tw/%7Ecjlin/libsvmtools/datasets/binary.html\" rel=\"nofollow\">Adult dataset</a> as well as the <a href=\"http://yann.lecun.com/exdb/mnist/\" rel=\"nofollow\">MNIST dataset</a>. The Adult dataset is downloaded and processed by running the downloader.py script in the <code>data/adult/</code> directory from the project root:</p>\n<pre><code>python data/adult/downloader.py\n</code></pre>\n<p>The MNIST dataset is imported automatically using the MXNet Gluon API.</p>\n<h3>Feedforward Neural Net and Optimizer</h3>\n<p>The implementations (and usages) of the feedforward neural net code and the parameter optimizer are tightly coupled, so we'll discuss them together.</p>\n<p><code>dp_optimizer.py</code> is an abstract class providing most of the implementation of differentially-private SGD (essentially as detailed in Abadi et al's \"Deep Learning with Differential Privacy\", with some minor modifications).\n<code>dp_sgd.py</code> completes the implementation, and <code>dp_adam.py</code> extends and completes the implementation to create a differentially private variant of the ADAM optimizer.</p>\n<p><code>dp_feedforward_net.py</code> provides the abstract class and implementations of most methods necessary for building a feedforward net.\nIts child classes reside in mnist and adult directories each as <code>base.py</code>, providing the concrete structure of the network (MLP and SLP respectively) and the dataset-specific functionality for their respective problems.\nThe <code>base.py</code> files in each directory are still abstract classes. The adult directory's <code>base.py</code> is then extended to implement logistic regression and SVM models to be trained on the Adult dataset with both the DP SGD and ADAM optimizers. The mnist directory's <code>base.py</code> is similarly extended to be trained on the MNIST dataset with the DP SGD and ADAM optimizers.</p>\n<p><code>dp_feedforward_net.py</code> specifies one of the concrete DP optimizers.\nThe reason that the <code>dp_feedforward_net.py</code> code and optimization code are tightly coupled is purely for performance reasons: we have manually created our network and we are manually specifying how batch computations are done (for both the actual results as well as the gradients of the parameters).</p>\n<h4>How to run</h4>\n<p>Each of the concrete child classes (e.g., <code>dpareto/models/adult/lr/dp_sgd.py</code>) contain main functions which can be run (primarily for testing/debugging purposes).\nAt a high level, the input is a set of hyperparameters and the output is a tuple (privacy, utility) (where privacy is the epsilon differential privacy value and utility is classification accuracy between 0 and 1).\nSee the main function in those files to get a better idea.</p>\n<p>Make sure to run experiments from root of the repo, so that relative paths to Adult dataset work.</p>\n<h3>Random Sampling of Hyperparameters</h3>\n<p><code>random_sampling/harness.py</code> provides the abstract class and implementations of most methods necessary for performing random sampling on one of these concrete dp_feedforward_net problems.</p>\n<p>For a simple example of how this can be used, see <code>examples/random_sampling.py</code>.</p>\n<p>The primary inputs here are:</p>\n<ul>\n<li>The <em>distribution</em> of hyperparameters to sample from for the specific problem.</li>\n<li>The number of instances; i.e., how many hyperparameter samples to take.</li>\n</ul>\n<p>The output is the full set of results -- that is: a list of tuples where the first item in each is the input (a single random setting of hyperparameters), and the second and third items respectively are the privacy and utility that resulted from an execution of that setting of hyperparameters.</p>\n<p>Since these instances are fully independent, we (optionally) take advantage of parallel processing by also specifying the number of workers to use to compute all the instances.\nWe can also provide command line arguments to override the passed-in number of instances, number of workers, and device type (CPU or GPU).\nSee the main function in either child class file to get a better idea.</p>\n<h3>Grid Search of Hyperparameters</h3>\n<p><code>grid_search/harness.py</code> provides the abstract class and implementations of most methods necessary for performing random sampling on one of these concrete dp_feedforward_net problems.</p>\n<p>Everything here is analogous to the random sampling discussed above, and a simple example of its use is in <code>examples/grid_search.py</code>.</p>\n<h3>Computing and Optimizing the Pareto Front</h3>\n<p><code>hypervolume_improvement/harness.py</code> provides the abstract class and implementations of most methods necessary for computing and optimizing the pareto front.</p>\n<p>A simple example of its use is in <code>examples/hypervolume_improvement.py</code>.</p>\n<p>There are several options for input here currently, but the primary idea is:</p>\n<ol>\n<li>Pass in some initial points to kick-start the GP models for privacy and accuracy. This requires at least one run of random sampling. In our experiments we used 16 initial points.</li>\n<li>Provide the \"anti-ideal\" point -- i.e., the reference point from which to compute the hypervolume from (think: top right point, and we want to optimize towards the bottom left). In our experiments we used [10, 0.999] based on the ranges of the output domain.</li>\n<li>Provide the optimization domain -- i.e., the range of valid values for each hyperparameter that the optimizer should consider.</li>\n<li>The number of points that the Bayesian optimizer should test. Note: this is the time-consuming portion, particularly because we can't currently do it in parallel.\nThe output is saved to disk, as:</li>\n</ol>\n<ul>\n<li>The set of hyperparameters suggested by the BO framework at each iteration</li>\n<li>Pareto front plots (the plot from the initial data, as well as the new plot for each point tested by the Bayesian optimizer).</li>\n<li>The full state of the hypervolume improvement class object at each iteration, to allow extraction of any results at a later time.\n<ul>\n<li>Due to a known bug related to the unpickling of an object which contains use of the multiprocessing package, extra steps must be taken to unpickle these object-state results. <code>experiments/scripts/picky_unpickler.py</code> was created to enable the extraction of specific results from these objects -- look through that file to see what is done and how.</li>\n</ul>\n</li>\n</ul>\n<h3>Paper Experiments</h3>\n<p>The runnable code for all the experiments in the paper is located in the experiments/ directory.\nThe output_perturbation/ subdirectory contains the relevant code for the \"illustrative example\" of training a logistic regression model using output perturbation.\nThe svt/ subdirectory contains the relevant code for the \"illustrative example\" of the sparse vector technique algorithm.\nThe adult/ and mnist/ subdirectories contain the code for the experiments run on the various models trained on the Adult and MNIST datasets.</p>\n<h4>Example</h4>\n<p>Let's review an example of running a full experiment for an already provided algorithm. We will use Adult dataset/Logistic Regression/Adam optimizer. Switch to project's root folder, make sure dependencies above are installed, and update PATH and PYTHONPATH variables as needed. For example, this verifies that we have gpflowopt installed:</p>\n<pre><code>pip freeze | grep -i gpflowopt\n</code></pre>\n<p>And this adds project's root folder to PATH, assuming we are already in that folder:</p>\n<pre><code>export PATH=$PATH:$(pwd)\n</code></pre>\n<p>As noted above, having at least one run of random sampling is required, so that we have some initial data to bootstrap the GP model. Let's do this:</p>\n<pre><code>python experiments/adult/dp_adult_lr_adam/random_sampling.py --workers 4 --instances 128\n</code></pre>\n<p>This says we will use 4 CPUs concurrently on the machine, and that we would like to collect 128 random points.</p>\n<p>Once this finishes, we can run the Bayesian optimization routine. It requires only one parameter, the path to the random sampling results, serialized with Python's pickle module. You can find it by browsing \"experiments/adult/dp_adult_lr_adam/results\" folder. Note that failed runs can also create results folders, so make sure to pick the correct file. Result folders are named according to corresponding timestamps.</p>\n<pre><code>python experiments/adult/dp_adult_lr_adam/hypervolume_improvement.py --init-data-file experiments/adult/dp_adult_lr_adam/results/random_sampling_results/1578475241956/full_results.pkl\n</code></pre>\n<p>Alternatively we could just edit the executed file directly, by adding this line:</p>\n<pre><code>initial_data_options['filename'] = current_dir + '/results/random_sampling_results/1578475241956/full_results.pkl'\n</code></pre>\n<p>Once this run finishes, results will be available in \"experiments/adult/dp_adult_lr_adam/results\" folder as well.</p>\n\n          </div>"}, "last_serial": 7172482, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "c61c2890a2b055ddfcc187031a98571c", "sha256": "edcd2856ce63c663af8fc5cd182cb23efcc64ba601201a2d5e7f4c01f14c73c6"}, "downloads": -1, "filename": "dpareto-0.1.tar.gz", "has_sig": false, "md5_digest": "c61c2890a2b055ddfcc187031a98571c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 1100, "upload_time": "2020-05-04T22:31:07", "upload_time_iso_8601": "2020-05-04T22:31:07.729742Z", "url": "https://files.pythonhosted.org/packages/17/61/f22fb7551ee594bc2fce41a80c268c922eb37f4fd931addc37fae63bbf1d/dpareto-0.1.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "2947c175ef04725a64badf909c827f7c", "sha256": "0bcd623eb3394184072069122712d586de0fec278c64b08cd39c5b1a80f6f082"}, "downloads": -1, "filename": "dpareto-0.1.1.tar.gz", "has_sig": false, "md5_digest": "2947c175ef04725a64badf909c827f7c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 30670, "upload_time": "2020-05-04T23:20:45", "upload_time_iso_8601": "2020-05-04T23:20:45.481761Z", "url": "https://files.pythonhosted.org/packages/ff/0a/4227cc86cd56c31ee73991634f8866f26277cde804ccb92b9a74dc7e9fbd/dpareto-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "1fc58537bcfc4ffda22a43587f780e79", "sha256": "95fcf65c238f9fcab3b00e16f4d7c3fc75f451285097eccb7be64ba6350caafa"}, "downloads": -1, "filename": "dpareto-0.1.2.tar.gz", "has_sig": false, "md5_digest": "1fc58537bcfc4ffda22a43587f780e79", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 30671, "upload_time": "2020-05-04T23:22:34", "upload_time_iso_8601": "2020-05-04T23:22:34.569403Z", "url": "https://files.pythonhosted.org/packages/65/12/280a054206404ac098b3c40f3c3ffb559d0ef1a18d2d6151c379587fdb17/dpareto-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "d011bc86719af081db43bd3e59442faa", "sha256": "b647e55750206f413fad7d8d00e5b274c92732a914f67def0453dac5fbe0fe63"}, "downloads": -1, "filename": "dpareto-0.1.3.tar.gz", "has_sig": false, "md5_digest": "d011bc86719af081db43bd3e59442faa", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31027, "upload_time": "2020-05-05T14:28:10", "upload_time_iso_8601": "2020-05-05T14:28:10.111187Z", "url": "https://files.pythonhosted.org/packages/86/21/0856a00b2ac046ca1999061e917c775ba0bc5314742a393e61d2804065b1/dpareto-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "88824fbc523003dec100356b0518a534", "sha256": "acd80bf54d9136fb468e9df3176d331320ce37967a74f7aec80337ca1aa09f93"}, "downloads": -1, "filename": "dpareto-0.1.4.tar.gz", "has_sig": false, "md5_digest": "88824fbc523003dec100356b0518a534", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31022, "upload_time": "2020-05-05T14:49:29", "upload_time_iso_8601": "2020-05-05T14:49:29.678090Z", "url": "https://files.pythonhosted.org/packages/00/08/1001514a2007cfb34b90cd4cdc326e8c759685076344760cec67ad5ab5df/dpareto-0.1.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "88824fbc523003dec100356b0518a534", "sha256": "acd80bf54d9136fb468e9df3176d331320ce37967a74f7aec80337ca1aa09f93"}, "downloads": -1, "filename": "dpareto-0.1.4.tar.gz", "has_sig": false, "md5_digest": "88824fbc523003dec100356b0518a534", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 31022, "upload_time": "2020-05-05T14:49:29", "upload_time_iso_8601": "2020-05-05T14:49:29.678090Z", "url": "https://files.pythonhosted.org/packages/00/08/1001514a2007cfb34b90cd4cdc326e8c759685076344760cec67ad5ab5df/dpareto-0.1.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:50:03 2020"}