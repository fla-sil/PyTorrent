{"info": {"author": "Xiang Gao", "author_email": "qasdfgtyuiop@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# TorchSnooper\n\nStatus:\n\n![PyPI](https://img.shields.io/pypi/v/TorchSnooper.svg)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/TorchSnooper.svg)\n[![Actions Status](https://github.com/zasdfgbnm/TorchSnooper/workflows/tests/badge.svg)](https://github.com/zasdfgbnm/TorchSnooper/actions)\n[![Actions Status](https://github.com/zasdfgbnm/TorchSnooper/workflows/deploy-test-pypi/badge.svg)](https://github.com/zasdfgbnm/TorchSnooper/actions)\n\nDeploy (only run on release):\n\n[![Actions Status](https://github.com/zasdfgbnm/TorchSnooper/workflows/deploy-pypi/badge.svg)](https://github.com/zasdfgbnm/TorchSnooper/actions)\n\nDo you want to look at the shape/dtype/etc. of every step of you model, but tired of manually writing prints?\n\nAre you bothered by errors like `RuntimeError: Expected object of scalar type Double but got scalar type Float`, and want to quickly figure out the problem?\n\nTorchSnooper is a [PySnooper](https://github.com/cool-RR/PySnooper) extension that helps you debugging these errors.\n\nTo use TorchSnooper, you just use it like using PySnooper. Remember to replace the `pysnooper.snoop` with `torchsnooper.snoop` in your code.\n\nTo install:\n\n```\npip install torchsnooper\n```\n\nTorchSnooper also support [snoop](https://github.com/alexmojaki/snoop). To use TorchSnooper with snoop, simply execute:\n```python\ntorchsnooper.register_snoop()\n```\nor\n```python\ntorchsnooper.register_snoop(verbose=True)\n```\nat the beginning, and use snoop normally.\n\n# Example 1: Monitoring device and dtype\n\nWe're writing a simple function:\n\n```python\ndef myfunc(mask, x):\n    y = torch.zeros(6)\n    y.masked_scatter_(mask, x)\n    return y\n```\n\nand use it like below\n\n```python\nmask = torch.tensor([0, 1, 0, 1, 1, 0], device='cuda')\nsource = torch.tensor([1.0, 2.0, 3.0], device='cuda')\ny = myfunc(mask, source)\n```\n\nThe above code seems to be correct, but unfortunately, we are getting the following error:\n\n```\nRuntimeError: Expected object of backend CPU but got backend CUDA for argument #2 'mask'\n```\n\nWhat is the problem? Let's snoop it! Decorate our function with `torchsnooper.snoop()`:\n\n```python\nimport torch\nimport torchsnooper\n\n@torchsnooper.snoop()\ndef myfunc(mask, x):\n    y = torch.zeros(6)\n    y.masked_scatter_(mask, x)\n    return y\n\nmask = torch.tensor([0, 1, 0, 1, 1, 0], device='cuda')\nsource = torch.tensor([1.0, 2.0, 3.0], device='cuda')\ny = myfunc(mask, source)\n```\n\nRun our script, and we will see:\n\n```\nStarting var:.. mask = tensor<(6,), int64, cuda:0>\nStarting var:.. x = tensor<(3,), float32, cuda:0>\n21:41:42.941668 call         5 def myfunc(mask, x):\n21:41:42.941834 line         6     y = torch.zeros(6)\nNew var:....... y = tensor<(6,), float32, cpu>\n21:41:42.943443 line         7     y.masked_scatter_(mask, x)\n21:41:42.944404 exception    7     y.masked_scatter_(mask, x)\n```\n\nNow pay attention to the devices of tensors, we notice\n```\nNew var:....... y = tensor<(6,), float32, cpu>\n```\n\nNow, it's clear that, the problem is because `y` is a tensor on CPU, that is,\nwe forget to specify the device on `y = torch.zeros(6)`. Changing it to\n`y = torch.zeros(6, device='cuda')`, this problem is solved.\n\nBut when running the script again we are getting another error:\n\n```\nRuntimeError: Expected object of scalar type Byte but got scalar type Long for argument #2 'mask'\n```\n\nLook at the trace above again, pay attention to the dtype of variables, we notice\n\n```\nStarting var:.. mask = tensor<(6,), int64, cuda:0>\n```\n\nOK, the problem is that, we didn't make the `mask` in the input a byte tensor. Changing the line into\n```\nmask = torch.tensor([0, 1, 0, 1, 1, 0], device='cuda', dtype=torch.uint8)\n```\nProblem solved.\n\n# Example 2: Monitoring shape\n\nWe are building a linear model\n\n```python\nclass Model(torch.nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(2, 1)\n\n    def forward(self, x):\n        return self.layer(x)\n```\n\nand we want to fit `y = x1 + 2 * x2 + 3`, so we create a dataset:\n\n```python\nx = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\ny = torch.tensor([3.0, 5.0, 4.0, 6.0])\n```\n\nWe train our model on this dataset using SGD optimizer:\n\n```python\nmodel = Model()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\nfor _ in range(10):\n    optimizer.zero_grad()\n    pred = model(x)\n    squared_diff = (y - pred) ** 2\n    loss = squared_diff.mean()\n    print(loss.item())\n    loss.backward()\n    optimizer.step()\n```\n\nBut unfortunately, the loss does not go down to a low enough number.\n\nWhat's wrong? Let's snoop it! Putting the training loop inside snoop:\n\n```python\nwith torchsnooper.snoop():\n    for _ in range(100):\n        optimizer.zero_grad()\n        pred = model(x)\n        squared_diff = (y - pred) ** 2\n        loss = squared_diff.mean()\n        print(loss.item())\n        loss.backward()\n        optimizer.step()\n```\n\nPart of the trace looks like:\n\n```\nNew var:....... x = tensor<(4, 2), float32, cpu>\nNew var:....... y = tensor<(4,), float32, cpu>\nNew var:....... model = Model(  (layer): Linear(in_features=2, out_features=1, bias=True))\nNew var:....... optimizer = SGD (Parameter Group 0    dampening: 0    lr: 0....omentum: 0    nesterov: False    weight_decay: 0)\n22:27:01.024233 line        21     for _ in range(100):\nNew var:....... _ = 0\n22:27:01.024439 line        22         optimizer.zero_grad()\n22:27:01.024574 line        23         pred = model(x)\nNew var:....... pred = tensor<(4, 1), float32, cpu, grad>\n22:27:01.026442 line        24         squared_diff = (y - pred) ** 2\nNew var:....... squared_diff = tensor<(4, 4), float32, cpu, grad>\n22:27:01.027369 line        25         loss = squared_diff.mean()\nNew var:....... loss = tensor<(), float32, cpu, grad>\n22:27:01.027616 line        26         print(loss.item())\n22:27:01.027793 line        27         loss.backward()\n22:27:01.050189 line        28         optimizer.step()\n```\n\nWe notice that, `y` has shape `(4,)`, but `pred` has shape `(4, 1)`. As a result, `squared_diff` has shape `(4, 4)` due to broadcasting!\n\nThis is not the expected behavior, let's fix it: `pred = model(x).squeeze()`, now everything looks good:\n\n```\nNew var:....... x = tensor<(4, 2), float32, cpu>\nNew var:....... y = tensor<(4,), float32, cpu>\nNew var:....... model = Model(  (layer): Linear(in_features=2, out_features=1, bias=True))\nNew var:....... optimizer = SGD (Parameter Group 0    dampening: 0    lr: 0....omentum: 0    nesterov: False    weight_decay: 0)\n22:28:19.778089 line        21     for _ in range(100):\nNew var:....... _ = 0\n22:28:19.778293 line        22         optimizer.zero_grad()\n22:28:19.778436 line        23         pred = model(x).squeeze()\nNew var:....... pred = tensor<(4,), float32, cpu, grad>\n22:28:19.780250 line        24         squared_diff = (y - pred) ** 2\nNew var:....... squared_diff = tensor<(4,), float32, cpu, grad>\n22:28:19.781099 line        25         loss = squared_diff.mean()\nNew var:....... loss = tensor<(), float32, cpu, grad>\n22:28:19.781361 line        26         print(loss.item())\n22:28:19.781537 line        27         loss.backward()\n22:28:19.798983 line        28         optimizer.step()\n```\n\nAnd the final model converge to the desired values.\n\n# Example 3: Monitoring nan and inf\n\nLet's say we have a model that output the likelihood of something. For this example, we will just use a mock:\n\n```python\nclass MockModel(torch.nn.Module):\n\n    def __init__(self):\n        super(MockModel, self).__init__()\n        self.unused = torch.nn.Linear(6, 4)\n\n    def forward(self, x):\n        return torch.tensor([0.0, 0.25, 0.9, 0.75]) + self.unused(x) * 0.0\n\nmodel = MockModel()\n```\n\nDuring training, we want to minimize the negative log likelihood, we have code:\n\n```python\nfor epoch in range(100):\n    batch_input = torch.randn(6, 6)\n    likelihood = model(batch_input)\n    log_likelihood = likelihood.log()\n    target = -log_likelihood.mean()\n    print(target.item())\n\n    optimizer.zero_grad()\n    target.backward()\n    optimizer.step()\n```\n\nUnfortunately, we first get `inf` then `nan` for our target during training. What's wrong? Let's snoop it:\n\n```python\nwith torchsnooper.snoop():\n    for epoch in range(100):\n        batch_input = torch.randn(6, 6)\n        likelihood = model(batch_input)\n        log_likelihood = likelihood.log()\n        target = -log_likelihood.mean()\n        print(target.item())\n\n        optimizer.zero_grad()\n        target.backward()\n        optimizer.step()\n```\n\nWe will see the part of the output of the snoop looks like:\n\n```\n19:30:20.928316 line        18     for epoch in range(100):\nNew var:....... epoch = 0\n19:30:20.928575 line        19         batch_input = torch.randn(6, 6)\nNew var:....... batch_input = tensor<(6, 6), float32, cpu>\n19:30:20.929671 line        20         likelihood = model(batch_input)\nNew var:....... likelihood = tensor<(6, 4), float32, cpu, grad>\n19:30:20.930284 line        21         log_likelihood = likelihood.log()\nNew var:....... log_likelihood = tensor<(6, 4), float32, cpu, grad, has_inf>\n19:30:20.930672 line        22         target = -log_likelihood.mean()\nNew var:....... target = tensor<(), float32, cpu, grad, has_inf>\n19:30:20.931136 line        23         print(target.item())\n19:30:20.931508 line        25         optimizer.zero_grad()\n19:30:20.931871 line        26         target.backward()\ninf\n19:30:20.960028 line        27         optimizer.step()\n19:30:20.960673 line        18     for epoch in range(100):\nModified var:.. epoch = 1\n19:30:20.961043 line        19         batch_input = torch.randn(6, 6)\n19:30:20.961423 line        20         likelihood = model(batch_input)\nModified var:.. likelihood = tensor<(6, 4), float32, cpu, grad, has_nan>\n19:30:20.961910 line        21         log_likelihood = likelihood.log()\nModified var:.. log_likelihood = tensor<(6, 4), float32, cpu, grad, has_nan>\n19:30:20.962302 line        22         target = -log_likelihood.mean()\nModified var:.. target = tensor<(), float32, cpu, grad, has_nan>\n19:30:20.962715 line        23         print(target.item())\n19:30:20.963089 line        25         optimizer.zero_grad()\n19:30:20.963464 line        26         target.backward()\n19:30:20.964051 line        27         optimizer.step()\n```\n\nReading the output, we find that, at the first epoch (`epoch = 0`), the `log_likelihood` has `has_inf` flag.\nThe `has_inf` flag means, your tensor contains `inf` in its value. The same flag appears for `target`.\nAnd at the second epoch, starting from `likelihood`, tensors all have a `has_nan` flag.\n\nFrom our experience with deep learning, we would guess this is because the first epoch has `inf`, which causes\nthe gradient to be `nan`, and when parameters are updated, these `nan` propagate to parameters and causing all\nfuture steps to have `nan` result.\n\nTaking a deeper look, we figure out that the `likelihood` contains 0 in it, which leads to `log(0) = -inf`. Changing\nthe line\n```python\nlog_likelihood = likelihood.log()\n```\ninto\n```python\nlog_likelihood = likelihood.clamp(min=1e-8).log()\n```\nProblem solved.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/zasdfgbnm/TorchSnooper", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "TorchSnooper", "package_url": "https://pypi.org/project/TorchSnooper/", "platform": "", "project_url": "https://pypi.org/project/TorchSnooper/", "project_urls": {"Homepage": "https://github.com/zasdfgbnm/TorchSnooper"}, "release_url": "https://pypi.org/project/TorchSnooper/0.8/", "requires_dist": ["pysnooper (>=0.1.0)", "numpy"], "requires_python": "", "summary": "Debug PyTorch code using PySnooper.", "version": "0.8", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>TorchSnooper</h1>\n<p>Status:</p>\n<p><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d8fc05b9c2ca4a80dd3772e779592a859f713d02/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f546f726368536e6f6f7065722e737667\">\n<img alt=\"PyPI - Downloads\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2a9f926205004c9a240fbf6a789a4b0d295ace7a/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f546f726368536e6f6f7065722e737667\">\n<a href=\"https://github.com/zasdfgbnm/TorchSnooper/actions\" rel=\"nofollow\"><img alt=\"Actions Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/781a16bbe70957a6d3ef6175002be2a1beb42b36/68747470733a2f2f6769746875622e636f6d2f7a6173646667626e6d2f546f726368536e6f6f7065722f776f726b666c6f77732f74657374732f62616467652e737667\"></a>\n<a href=\"https://github.com/zasdfgbnm/TorchSnooper/actions\" rel=\"nofollow\"><img alt=\"Actions Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7778979014f7d030c9d797698797bdbfdf730488/68747470733a2f2f6769746875622e636f6d2f7a6173646667626e6d2f546f726368536e6f6f7065722f776f726b666c6f77732f6465706c6f792d746573742d707970692f62616467652e737667\"></a></p>\n<p>Deploy (only run on release):</p>\n<p><a href=\"https://github.com/zasdfgbnm/TorchSnooper/actions\" rel=\"nofollow\"><img alt=\"Actions Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5cde5537bd418918c399ab72fbc3496772ce2173/68747470733a2f2f6769746875622e636f6d2f7a6173646667626e6d2f546f726368536e6f6f7065722f776f726b666c6f77732f6465706c6f792d707970692f62616467652e737667\"></a></p>\n<p>Do you want to look at the shape/dtype/etc. of every step of you model, but tired of manually writing prints?</p>\n<p>Are you bothered by errors like <code>RuntimeError: Expected object of scalar type Double but got scalar type Float</code>, and want to quickly figure out the problem?</p>\n<p>TorchSnooper is a <a href=\"https://github.com/cool-RR/PySnooper\" rel=\"nofollow\">PySnooper</a> extension that helps you debugging these errors.</p>\n<p>To use TorchSnooper, you just use it like using PySnooper. Remember to replace the <code>pysnooper.snoop</code> with <code>torchsnooper.snoop</code> in your code.</p>\n<p>To install:</p>\n<pre><code>pip install torchsnooper\n</code></pre>\n<p>TorchSnooper also support <a href=\"https://github.com/alexmojaki/snoop\" rel=\"nofollow\">snoop</a>. To use TorchSnooper with snoop, simply execute:</p>\n<pre><span class=\"n\">torchsnooper</span><span class=\"o\">.</span><span class=\"n\">register_snoop</span><span class=\"p\">()</span>\n</pre>\n<p>or</p>\n<pre><span class=\"n\">torchsnooper</span><span class=\"o\">.</span><span class=\"n\">register_snoop</span><span class=\"p\">(</span><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<p>at the beginning, and use snoop normally.</p>\n<h1>Example 1: Monitoring device and dtype</h1>\n<p>We're writing a simple function:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">myfunc</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">)</span>\n    <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">masked_scatter_</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">y</span>\n</pre>\n<p>and use it like below</p>\n<pre><span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">'cuda'</span><span class=\"p\">)</span>\n<span class=\"n\">source</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"mf\">2.0</span><span class=\"p\">,</span> <span class=\"mf\">3.0</span><span class=\"p\">],</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">'cuda'</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">myfunc</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">source</span><span class=\"p\">)</span>\n</pre>\n<p>The above code seems to be correct, but unfortunately, we are getting the following error:</p>\n<pre><code>RuntimeError: Expected object of backend CPU but got backend CUDA for argument #2 'mask'\n</code></pre>\n<p>What is the problem? Let's snoop it! Decorate our function with <code>torchsnooper.snoop()</code>:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchsnooper</span>\n\n<span class=\"nd\">@torchsnooper</span><span class=\"o\">.</span><span class=\"n\">snoop</span><span class=\"p\">()</span>\n<span class=\"k\">def</span> <span class=\"nf\">myfunc</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">)</span>\n    <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">masked_scatter_</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">y</span>\n\n<span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">'cuda'</span><span class=\"p\">)</span>\n<span class=\"n\">source</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"mf\">2.0</span><span class=\"p\">,</span> <span class=\"mf\">3.0</span><span class=\"p\">],</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">'cuda'</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">myfunc</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">source</span><span class=\"p\">)</span>\n</pre>\n<p>Run our script, and we will see:</p>\n<pre><code>Starting var:.. mask = tensor&lt;(6,), int64, cuda:0&gt;\nStarting var:.. x = tensor&lt;(3,), float32, cuda:0&gt;\n21:41:42.941668 call         5 def myfunc(mask, x):\n21:41:42.941834 line         6     y = torch.zeros(6)\nNew var:....... y = tensor&lt;(6,), float32, cpu&gt;\n21:41:42.943443 line         7     y.masked_scatter_(mask, x)\n21:41:42.944404 exception    7     y.masked_scatter_(mask, x)\n</code></pre>\n<p>Now pay attention to the devices of tensors, we notice</p>\n<pre><code>New var:....... y = tensor&lt;(6,), float32, cpu&gt;\n</code></pre>\n<p>Now, it's clear that, the problem is because <code>y</code> is a tensor on CPU, that is,\nwe forget to specify the device on <code>y = torch.zeros(6)</code>. Changing it to\n<code>y = torch.zeros(6, device='cuda')</code>, this problem is solved.</p>\n<p>But when running the script again we are getting another error:</p>\n<pre><code>RuntimeError: Expected object of scalar type Byte but got scalar type Long for argument #2 'mask'\n</code></pre>\n<p>Look at the trace above again, pay attention to the dtype of variables, we notice</p>\n<pre><code>Starting var:.. mask = tensor&lt;(6,), int64, cuda:0&gt;\n</code></pre>\n<p>OK, the problem is that, we didn't make the <code>mask</code> in the input a byte tensor. Changing the line into</p>\n<pre><code>mask = torch.tensor([0, 1, 0, 1, 1, 0], device='cuda', dtype=torch.uint8)\n</code></pre>\n<p>Problem solved.</p>\n<h1>Example 2: Monitoring shape</h1>\n<p>We are building a linear model</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">Model</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n</pre>\n<p>and we want to fit <code>y = x1 + 2 * x2 + 3</code>, so we create a dataset:</p>\n<pre><span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">([[</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">]])</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"mf\">3.0</span><span class=\"p\">,</span> <span class=\"mf\">5.0</span><span class=\"p\">,</span> <span class=\"mf\">4.0</span><span class=\"p\">,</span> <span class=\"mf\">6.0</span><span class=\"p\">])</span>\n</pre>\n<p>We train our model on this dataset using SGD optimizer:</p>\n<pre><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">()</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">SGD</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">):</span>\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n    <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"n\">squared_diff</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">y</span> <span class=\"o\">-</span> <span class=\"n\">pred</span><span class=\"p\">)</span> <span class=\"o\">**</span> <span class=\"mi\">2</span>\n    <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">squared_diff</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">())</span>\n    <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p>But unfortunately, the loss does not go down to a low enough number.</p>\n<p>What's wrong? Let's snoop it! Putting the training loop inside snoop:</p>\n<pre><span class=\"k\">with</span> <span class=\"n\">torchsnooper</span><span class=\"o\">.</span><span class=\"n\">snoop</span><span class=\"p\">():</span>\n    <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">squared_diff</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">y</span> <span class=\"o\">-</span> <span class=\"n\">pred</span><span class=\"p\">)</span> <span class=\"o\">**</span> <span class=\"mi\">2</span>\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">squared_diff</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">())</span>\n        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p>Part of the trace looks like:</p>\n<pre><code>New var:....... x = tensor&lt;(4, 2), float32, cpu&gt;\nNew var:....... y = tensor&lt;(4,), float32, cpu&gt;\nNew var:....... model = Model(  (layer): Linear(in_features=2, out_features=1, bias=True))\nNew var:....... optimizer = SGD (Parameter Group 0    dampening: 0    lr: 0....omentum: 0    nesterov: False    weight_decay: 0)\n22:27:01.024233 line        21     for _ in range(100):\nNew var:....... _ = 0\n22:27:01.024439 line        22         optimizer.zero_grad()\n22:27:01.024574 line        23         pred = model(x)\nNew var:....... pred = tensor&lt;(4, 1), float32, cpu, grad&gt;\n22:27:01.026442 line        24         squared_diff = (y - pred) ** 2\nNew var:....... squared_diff = tensor&lt;(4, 4), float32, cpu, grad&gt;\n22:27:01.027369 line        25         loss = squared_diff.mean()\nNew var:....... loss = tensor&lt;(), float32, cpu, grad&gt;\n22:27:01.027616 line        26         print(loss.item())\n22:27:01.027793 line        27         loss.backward()\n22:27:01.050189 line        28         optimizer.step()\n</code></pre>\n<p>We notice that, <code>y</code> has shape <code>(4,)</code>, but <code>pred</code> has shape <code>(4, 1)</code>. As a result, <code>squared_diff</code> has shape <code>(4, 4)</code> due to broadcasting!</p>\n<p>This is not the expected behavior, let's fix it: <code>pred = model(x).squeeze()</code>, now everything looks good:</p>\n<pre><code>New var:....... x = tensor&lt;(4, 2), float32, cpu&gt;\nNew var:....... y = tensor&lt;(4,), float32, cpu&gt;\nNew var:....... model = Model(  (layer): Linear(in_features=2, out_features=1, bias=True))\nNew var:....... optimizer = SGD (Parameter Group 0    dampening: 0    lr: 0....omentum: 0    nesterov: False    weight_decay: 0)\n22:28:19.778089 line        21     for _ in range(100):\nNew var:....... _ = 0\n22:28:19.778293 line        22         optimizer.zero_grad()\n22:28:19.778436 line        23         pred = model(x).squeeze()\nNew var:....... pred = tensor&lt;(4,), float32, cpu, grad&gt;\n22:28:19.780250 line        24         squared_diff = (y - pred) ** 2\nNew var:....... squared_diff = tensor&lt;(4,), float32, cpu, grad&gt;\n22:28:19.781099 line        25         loss = squared_diff.mean()\nNew var:....... loss = tensor&lt;(), float32, cpu, grad&gt;\n22:28:19.781361 line        26         print(loss.item())\n22:28:19.781537 line        27         loss.backward()\n22:28:19.798983 line        28         optimizer.step()\n</code></pre>\n<p>And the final model converge to the desired values.</p>\n<h1>Example 3: Monitoring nan and inf</h1>\n<p>Let's say we have a model that output the likelihood of something. For this example, we will just use a mock:</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">MockModel</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">MockModel</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">unused</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">([</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.25</span><span class=\"p\">,</span> <span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.75</span><span class=\"p\">])</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">unused</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mf\">0.0</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">MockModel</span><span class=\"p\">()</span>\n</pre>\n<p>During training, we want to minimize the negative log likelihood, we have code:</p>\n<pre><span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n    <span class=\"n\">batch_input</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">)</span>\n    <span class=\"n\">likelihood</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">batch_input</span><span class=\"p\">)</span>\n    <span class=\"n\">log_likelihood</span> <span class=\"o\">=</span> <span class=\"n\">likelihood</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">()</span>\n    <span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">log_likelihood</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">())</span>\n\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n    <span class=\"n\">target</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p>Unfortunately, we first get <code>inf</code> then <code>nan</code> for our target during training. What's wrong? Let's snoop it:</p>\n<pre><span class=\"k\">with</span> <span class=\"n\">torchsnooper</span><span class=\"o\">.</span><span class=\"n\">snoop</span><span class=\"p\">():</span>\n    <span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n        <span class=\"n\">batch_input</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">)</span>\n        <span class=\"n\">likelihood</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">batch_input</span><span class=\"p\">)</span>\n        <span class=\"n\">log_likelihood</span> <span class=\"o\">=</span> <span class=\"n\">likelihood</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">()</span>\n        <span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">log_likelihood</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">())</span>\n\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n        <span class=\"n\">target</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p>We will see the part of the output of the snoop looks like:</p>\n<pre><code>19:30:20.928316 line        18     for epoch in range(100):\nNew var:....... epoch = 0\n19:30:20.928575 line        19         batch_input = torch.randn(6, 6)\nNew var:....... batch_input = tensor&lt;(6, 6), float32, cpu&gt;\n19:30:20.929671 line        20         likelihood = model(batch_input)\nNew var:....... likelihood = tensor&lt;(6, 4), float32, cpu, grad&gt;\n19:30:20.930284 line        21         log_likelihood = likelihood.log()\nNew var:....... log_likelihood = tensor&lt;(6, 4), float32, cpu, grad, has_inf&gt;\n19:30:20.930672 line        22         target = -log_likelihood.mean()\nNew var:....... target = tensor&lt;(), float32, cpu, grad, has_inf&gt;\n19:30:20.931136 line        23         print(target.item())\n19:30:20.931508 line        25         optimizer.zero_grad()\n19:30:20.931871 line        26         target.backward()\ninf\n19:30:20.960028 line        27         optimizer.step()\n19:30:20.960673 line        18     for epoch in range(100):\nModified var:.. epoch = 1\n19:30:20.961043 line        19         batch_input = torch.randn(6, 6)\n19:30:20.961423 line        20         likelihood = model(batch_input)\nModified var:.. likelihood = tensor&lt;(6, 4), float32, cpu, grad, has_nan&gt;\n19:30:20.961910 line        21         log_likelihood = likelihood.log()\nModified var:.. log_likelihood = tensor&lt;(6, 4), float32, cpu, grad, has_nan&gt;\n19:30:20.962302 line        22         target = -log_likelihood.mean()\nModified var:.. target = tensor&lt;(), float32, cpu, grad, has_nan&gt;\n19:30:20.962715 line        23         print(target.item())\n19:30:20.963089 line        25         optimizer.zero_grad()\n19:30:20.963464 line        26         target.backward()\n19:30:20.964051 line        27         optimizer.step()\n</code></pre>\n<p>Reading the output, we find that, at the first epoch (<code>epoch = 0</code>), the <code>log_likelihood</code> has <code>has_inf</code> flag.\nThe <code>has_inf</code> flag means, your tensor contains <code>inf</code> in its value. The same flag appears for <code>target</code>.\nAnd at the second epoch, starting from <code>likelihood</code>, tensors all have a <code>has_nan</code> flag.</p>\n<p>From our experience with deep learning, we would guess this is because the first epoch has <code>inf</code>, which causes\nthe gradient to be <code>nan</code>, and when parameters are updated, these <code>nan</code> propagate to parameters and causing all\nfuture steps to have <code>nan</code> result.</p>\n<p>Taking a deeper look, we figure out that the <code>likelihood</code> contains 0 in it, which leads to <code>log(0) = -inf</code>. Changing\nthe line</p>\n<pre><span class=\"n\">log_likelihood</span> <span class=\"o\">=</span> <span class=\"n\">likelihood</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">()</span>\n</pre>\n<p>into</p>\n<pre><span class=\"n\">log_likelihood</span> <span class=\"o\">=</span> <span class=\"n\">likelihood</span><span class=\"o\">.</span><span class=\"n\">clamp</span><span class=\"p\">(</span><span class=\"nb\">min</span><span class=\"o\">=</span><span class=\"mf\">1e-8</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">()</span>\n</pre>\n<p>Problem solved.</p>\n\n          </div>"}, "last_serial": 6378062, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "48474abdfb938be6bda97554d4d78c8f", "sha256": "ea10791acc776b67e46916f169628565782a72a67aa97ee41f351262d27a2215"}, "downloads": -1, "filename": "TorchSnooper-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "48474abdfb938be6bda97554d4d78c8f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 2714, "upload_time": "2019-05-29T01:13:22", "upload_time_iso_8601": "2019-05-29T01:13:22.278187Z", "url": "https://files.pythonhosted.org/packages/52/1a/f1bfff0832922eb66155eaa0b1a17e0d66565237a84ec3fa95c305068c68/TorchSnooper-0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "39ca12fbd2c348260211605b1c878162", "sha256": "6ff62a860e87c2b6aa908b8d9b058ef705e143712411e1799179bfb64ea1c1ce"}, "downloads": -1, "filename": "TorchSnooper-0.1.tar.gz", "has_sig": false, "md5_digest": "39ca12fbd2c348260211605b1c878162", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5483, "upload_time": "2019-05-29T01:14:00", "upload_time_iso_8601": "2019-05-29T01:14:00.489231Z", "url": "https://files.pythonhosted.org/packages/18/ad/4ae99c9becaeddf55e5d9e48e85a42618c9b7235dd2ed7b18583bb7fdd96/TorchSnooper-0.1.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "1c0daf04aef95fcd413622af5a44710d", "sha256": "abb50d9d2948fd31eee6fca2b9152067831da91a465db7f5ebaa63aaf208883e"}, "downloads": -1, "filename": "TorchSnooper-0.1.3.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "1c0daf04aef95fcd413622af5a44710d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4819, "upload_time": "2019-05-29T03:30:44", "upload_time_iso_8601": "2019-05-29T03:30:44.909006Z", "url": "https://files.pythonhosted.org/packages/e5/e3/29b0e47c68cde3b8b7dc6dea0cff8cc3abd4a0776ce779f919b3c433ce90/TorchSnooper-0.1.3.linux-x86_64.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "30de607365ad183025aeb5cade8fc331", "sha256": "071070ac0e6d2b24a04fb4452ba249dea1e8ef2f44bf73d4fff5200d55af0934"}, "downloads": -1, "filename": "TorchSnooper-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "30de607365ad183025aeb5cade8fc331", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 2689, "upload_time": "2019-05-29T03:30:43", "upload_time_iso_8601": "2019-05-29T03:30:43.687937Z", "url": "https://files.pythonhosted.org/packages/3d/14/26394429e9de04f316946ba4a6dc464744e7f700c5be0e26ef7b6145e46f/TorchSnooper-0.1.3-py3-none-any.whl", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "6c9e7ac76eb38ee9eb116a629d72cfa8", "sha256": "ec0f466a2e39feb7f4e159f95476acb1c753a6dacd61934a04032e047cad2c75"}, "downloads": -1, "filename": "TorchSnooper-0.2.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "6c9e7ac76eb38ee9eb116a629d72cfa8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4836, "upload_time": "2019-05-30T06:32:46", "upload_time_iso_8601": "2019-05-30T06:32:46.671414Z", "url": "https://files.pythonhosted.org/packages/8f/1e/a4d9c3739940b56b22c9ab1ce87c62d0a3281677771159edc311202c5134/TorchSnooper-0.2.linux-x86_64.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "534935f659f130f6b0778210dbe3b29e", "sha256": "b8431c6e8b95da33fce2ca0f76603a2c5102e69c17760f5235e917311afb9e43"}, "downloads": -1, "filename": "TorchSnooper-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "534935f659f130f6b0778210dbe3b29e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 3490, "upload_time": "2019-05-30T06:32:45", "upload_time_iso_8601": "2019-05-30T06:32:45.454170Z", "url": "https://files.pythonhosted.org/packages/e9/5c/302069d01b531162f0be29d258461c007392863b745a6bd57d3063062c0f/TorchSnooper-0.2-py3-none-any.whl", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "42fe5867b8a7cb2b1511799e5c926432", "sha256": "3ffb5c4507f24118de1a3ba51ae065652499fa65fbd0d55e47f73bd71f3d5453"}, "downloads": -1, "filename": "TorchSnooper-0.3.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "42fe5867b8a7cb2b1511799e5c926432", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4372, "upload_time": "2019-06-28T18:55:48", "upload_time_iso_8601": "2019-06-28T18:55:48.133181Z", "url": "https://files.pythonhosted.org/packages/b0/51/af8d268c6cc0fbe14268cec35c1642ce5191b98a2e06cde2b73764c1d04d/TorchSnooper-0.3.linux-x86_64.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "0f11c877afcb2302dee19e5a88a06ad2", "sha256": "0b1a945c6fb860f2165fd8f33694187e3ff34b2a676218674a6884d3f5f057ae"}, "downloads": -1, "filename": "TorchSnooper-0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "0f11c877afcb2302dee19e5a88a06ad2", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 3387, "upload_time": "2019-06-28T18:55:46", "upload_time_iso_8601": "2019-06-28T18:55:46.660885Z", "url": "https://files.pythonhosted.org/packages/34/21/7aad0aa419b14a5b8ecd7c087876ae7d082fc53aa0afe780be781157e837/TorchSnooper-0.3-py3-none-any.whl", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "66477e95b6ec5cbb226906c043ee6701", "sha256": "41059eaae11e620e648f4546fc998ff0713b72739ace381d364c148f8cd63d7f"}, "downloads": -1, "filename": "TorchSnooper-0.4.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "66477e95b6ec5cbb226906c043ee6701", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4611, "upload_time": "2019-07-01T23:08:17", "upload_time_iso_8601": "2019-07-01T23:08:17.300349Z", "url": "https://files.pythonhosted.org/packages/c1/41/8546e55d19f02a9c709439170b1078201746e93251a5f5aa2e1f5f70f733/TorchSnooper-0.4.linux-x86_64.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "f965de504b352a6c371c8ad418ae1554", "sha256": "3d876854140f04b337ea855da22d311aab03fbbe5437902839b9cf03adbfaefe"}, "downloads": -1, "filename": "TorchSnooper-0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "f965de504b352a6c371c8ad418ae1554", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 3475, "upload_time": "2019-07-01T23:08:15", "upload_time_iso_8601": "2019-07-01T23:08:15.905490Z", "url": "https://files.pythonhosted.org/packages/cd/6d/7f14a6b3521e294e36aae70422f9c39b1001d81553214fe8a36d6ec319b0/TorchSnooper-0.4-py3-none-any.whl", "yanked": false}], "0.4.1": [{"comment_text": "", "digests": {"md5": "596f855ea38106c26b8847e330121d26", "sha256": "ca20917a03d2dca23c4474d28a635981ec5917d80d0d34d5e4b72327ab8d40f5"}, "downloads": -1, "filename": "TorchSnooper-0.4.1.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "596f855ea38106c26b8847e330121d26", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6817, "upload_time": "2019-07-02T00:07:10", "upload_time_iso_8601": "2019-07-02T00:07:10.425963Z", "url": "https://files.pythonhosted.org/packages/b2/b5/754db71878deeb8bb591b3becaa19ae19df86a4e3efdaf26c684302373a3/TorchSnooper-0.4.1.linux-x86_64.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "47950d50797b57227e74ab677b8a5fdc", "sha256": "696e54b09cbc95d07b7c040f206fb0812308a37528c0dd77c1edb5cdd4c12480"}, "downloads": -1, "filename": "TorchSnooper-0.4.1-py3-none-any.whl", "has_sig": false, "md5_digest": "47950d50797b57227e74ab677b8a5fdc", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 5627, "upload_time": "2019-07-02T00:07:08", "upload_time_iso_8601": "2019-07-02T00:07:08.942195Z", "url": "https://files.pythonhosted.org/packages/e1/62/bd535c0b1533be9e573cfd753a4b89c2e643c6716871798d44425d234885/TorchSnooper-0.4.1-py3-none-any.whl", "yanked": false}], "0.5": [{"comment_text": "", "digests": {"md5": "cfb6a28d1c926b85315eb9be7b9852d7", "sha256": "5d67cb3a4abda7bf5758d77ff4a3d7c5ecf951bf6aa03a64d321d351f8b6f05e"}, "downloads": -1, "filename": "TorchSnooper-0.5.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "cfb6a28d1c926b85315eb9be7b9852d7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8084, "upload_time": "2019-07-05T23:49:30", "upload_time_iso_8601": "2019-07-05T23:49:30.034871Z", "url": "https://files.pythonhosted.org/packages/fd/5e/5b348b3d35bf8cfa8ffac6c32b933900a00d6e4e07f62bdb598386f681b7/TorchSnooper-0.5.linux-x86_64.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "4f65a531a6ba536cb1c2b012f13edf37", "sha256": "2f4261d222843084ec1757cd2bee80e29fd741d7b631e480bfa57350ccfc7b77"}, "downloads": -1, "filename": "TorchSnooper-0.5-py3-none-any.whl", "has_sig": false, "md5_digest": "4f65a531a6ba536cb1c2b012f13edf37", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 6662, "upload_time": "2019-07-05T23:49:28", "upload_time_iso_8601": "2019-07-05T23:49:28.561923Z", "url": "https://files.pythonhosted.org/packages/df/bb/147923a1274155034189a3a622ac99781009a15928101b7f47afd1892cdb/TorchSnooper-0.5-py3-none-any.whl", "yanked": false}], "0.6": [{"comment_text": "", "digests": {"md5": "bbeb04f359669e5af2d12e73180de7e9", "sha256": "1fe1468ffac899b4b53392d6a374d796745e986917f8a943dc445491b54d9e30"}, "downloads": -1, "filename": "TorchSnooper-0.6.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "bbeb04f359669e5af2d12e73180de7e9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8925, "upload_time": "2019-07-10T01:57:31", "upload_time_iso_8601": "2019-07-10T01:57:31.783509Z", "url": "https://files.pythonhosted.org/packages/e9/ac/5c3a244284d6db22d3a84929d4b7e9dd9f6ee48d0b9d705a3dfcdd712c21/TorchSnooper-0.6.linux-x86_64.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "8a3d63b832d63bde76ef33bd215aa385", "sha256": "e136cb90420ce39cbd6527524046582286633cc1a270447e420a82ac0a2aa8a2"}, "downloads": -1, "filename": "TorchSnooper-0.6-py3-none-any.whl", "has_sig": false, "md5_digest": "8a3d63b832d63bde76ef33bd215aa385", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7004, "upload_time": "2019-07-10T01:57:30", "upload_time_iso_8601": "2019-07-10T01:57:30.113114Z", "url": "https://files.pythonhosted.org/packages/79/98/4eaf43200a6a4ea31eae1163527db3cf0d2587f449a5cefa81931b4dcede/TorchSnooper-0.6-py3-none-any.whl", "yanked": false}], "0.6.1": [{"comment_text": "", "digests": {"md5": "034522fa84e34fbd13b6af3e276bd28b", "sha256": "48a4363745454ad051e228d1cfd6339fe1be9e9655d9601789846f36fa0bf516"}, "downloads": -1, "filename": "TorchSnooper-0.6.1.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "034522fa84e34fbd13b6af3e276bd28b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8974, "upload_time": "2019-07-10T05:28:11", "upload_time_iso_8601": "2019-07-10T05:28:11.557757Z", "url": "https://files.pythonhosted.org/packages/ed/9f/ce053efdb9ccb6890d7d87b2257ba10a72d7d19cae3e341f8704104a6d92/TorchSnooper-0.6.1.linux-x86_64.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "08957137dc3060f247e290549a3c183c", "sha256": "161036f9190fcff7ccc2aac8816eec8d5d3a0c29e5e612ce4a6a86bf8f59fd11"}, "downloads": -1, "filename": "TorchSnooper-0.6.1-py3-none-any.whl", "has_sig": false, "md5_digest": "08957137dc3060f247e290549a3c183c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7031, "upload_time": "2019-07-10T05:28:10", "upload_time_iso_8601": "2019-07-10T05:28:10.012871Z", "url": "https://files.pythonhosted.org/packages/de/54/ea99bdc44a773cafc8cbd29c9c4d5fe98d84244cf5849e1a1aadb48aec14/TorchSnooper-0.6.1-py3-none-any.whl", "yanked": false}], "0.7": [{"comment_text": "", "digests": {"md5": "2c5e2bad1a21fd5a6022295ddb4ca7fb", "sha256": "a967b627b40f20019cd1e1517ed5aac7bb3154d591401e1976531d7d40987e86"}, "downloads": -1, "filename": "TorchSnooper-0.7.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "2c5e2bad1a21fd5a6022295ddb4ca7fb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9210, "upload_time": "2019-08-08T18:58:40", "upload_time_iso_8601": "2019-08-08T18:58:40.055084Z", "url": "https://files.pythonhosted.org/packages/56/ef/6904da7e8b7bb1d985b7c2aeb103a5763bd1f9d64416bc6d17d500ae3050/TorchSnooper-0.7.linux-x86_64.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "37993306713a0bb55fa92b89a72c2157", "sha256": "beb31bae9fa65c923c48f2de877a489dba1e10bcbc65d0e8f18af5e4c898c5e0"}, "downloads": -1, "filename": "TorchSnooper-0.7-py3-none-any.whl", "has_sig": false, "md5_digest": "37993306713a0bb55fa92b89a72c2157", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7142, "upload_time": "2019-08-08T18:58:38", "upload_time_iso_8601": "2019-08-08T18:58:38.381998Z", "url": "https://files.pythonhosted.org/packages/7b/67/1e30f88a689d423f46fb1752bc26001678bad506cc915bbea735ecdb962d/TorchSnooper-0.7-py3-none-any.whl", "yanked": false}], "0.7.1": [{"comment_text": "", "digests": {"md5": "ac0020fcd65fe90195121c5a161f9744", "sha256": "45290ee3e963e21961f188beab76f24a5137932bc085314715f1fe21ef9aa09d"}, "downloads": -1, "filename": "TorchSnooper-0.7.1.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "ac0020fcd65fe90195121c5a161f9744", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9214, "upload_time": "2019-09-09T02:58:37", "upload_time_iso_8601": "2019-09-09T02:58:37.861432Z", "url": "https://files.pythonhosted.org/packages/fa/1b/429a26cee50ef7bd31bcdc8ecf5585ec6402dbd9eb64f3b8b8f24a853b25/TorchSnooper-0.7.1.linux-x86_64.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "52570036546acaaf55f3053fc7d4caae", "sha256": "f362d3156e2130f7e46abfff2824321eff9270dab1e48a3fbbed1c86a8f97e76"}, "downloads": -1, "filename": "TorchSnooper-0.7.1-py3-none-any.whl", "has_sig": false, "md5_digest": "52570036546acaaf55f3053fc7d4caae", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7171, "upload_time": "2019-09-09T02:58:36", "upload_time_iso_8601": "2019-09-09T02:58:36.571655Z", "url": "https://files.pythonhosted.org/packages/33/52/91ad263db398cf1fb5689c562bab82d35184501dc7ad62805d535fd27fa7/TorchSnooper-0.7.1-py3-none-any.whl", "yanked": false}], "0.8": [{"comment_text": "", "digests": {"md5": "b80acd6ae1342b1a351d45c2ac59cab2", "sha256": "baa441da053f9ce943f2862c3fcf983f90b23f0006ed7cafe7403606e5067de2"}, "downloads": -1, "filename": "TorchSnooper-0.8.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "b80acd6ae1342b1a351d45c2ac59cab2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9516, "upload_time": "2019-12-31T05:12:56", "upload_time_iso_8601": "2019-12-31T05:12:56.324350Z", "url": "https://files.pythonhosted.org/packages/4f/78/7a8da20a9bb5883825ba3ce0bd4f646dd1d7268e9d0a8fc9f2b3488508a1/TorchSnooper-0.8.linux-x86_64.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "bf923c0e21a17bf8001ab80a62953616", "sha256": "5b5c7a8cd6952c4b9dada9a9502baa9ef28410747c97e6702eef1330b715115a"}, "downloads": -1, "filename": "TorchSnooper-0.8-py3-none-any.whl", "has_sig": false, "md5_digest": "bf923c0e21a17bf8001ab80a62953616", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7163, "upload_time": "2019-12-31T05:12:54", "upload_time_iso_8601": "2019-12-31T05:12:54.505452Z", "url": "https://files.pythonhosted.org/packages/8c/31/7ce9965aa20d00394733dc3614b929065f402fcbcbee6fba300d0df3b061/TorchSnooper-0.8-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "b80acd6ae1342b1a351d45c2ac59cab2", "sha256": "baa441da053f9ce943f2862c3fcf983f90b23f0006ed7cafe7403606e5067de2"}, "downloads": -1, "filename": "TorchSnooper-0.8.linux-x86_64.tar.gz", "has_sig": false, "md5_digest": "b80acd6ae1342b1a351d45c2ac59cab2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9516, "upload_time": "2019-12-31T05:12:56", "upload_time_iso_8601": "2019-12-31T05:12:56.324350Z", "url": "https://files.pythonhosted.org/packages/4f/78/7a8da20a9bb5883825ba3ce0bd4f646dd1d7268e9d0a8fc9f2b3488508a1/TorchSnooper-0.8.linux-x86_64.tar.gz", "yanked": false}, {"comment_text": "", "digests": {"md5": "bf923c0e21a17bf8001ab80a62953616", "sha256": "5b5c7a8cd6952c4b9dada9a9502baa9ef28410747c97e6702eef1330b715115a"}, "downloads": -1, "filename": "TorchSnooper-0.8-py3-none-any.whl", "has_sig": false, "md5_digest": "bf923c0e21a17bf8001ab80a62953616", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7163, "upload_time": "2019-12-31T05:12:54", "upload_time_iso_8601": "2019-12-31T05:12:54.505452Z", "url": "https://files.pythonhosted.org/packages/8c/31/7ce9965aa20d00394733dc3614b929065f402fcbcbee6fba300d0df3b061/TorchSnooper-0.8-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 03:50:12 2020"}