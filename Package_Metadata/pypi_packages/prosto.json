{"info": {"author": "Prosto Data", "author_email": "52108119+prostodata@users.noreply.github.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering", "Topic :: Software Development"], "description": "```\n ____                _        \n|  _ \\ _ __ ___  ___| |_ ___   ___________________________________________\n| |_) | '__/ _ \\/ __| __/ _ \\ \n|  __/| | | (_) \\__ \\ || (_) | Data Processing Toolkit - noSql-noMapReduce\n|_|   |_|  \\___/|___/\\__\\___/  ___________________________________________\n```\n[![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://github.com/prostodata/prosto/blob/master/LICENSE)\n[![Python 3.6](https://img.shields.io/badge/python-3.6-brightgreen.svg)](https://www.python.org/downloads/release/python-370/)\n[![PyPI version](https://badge.fury.io/py/prosto.svg)](https://badge.fury.io/py/prosto)\n\n* [What is Prosto?](#what-is-prosto)\n* [Why Prosto?](#why-prosto)\n* [Getting started with Prosto](#getting-started-with-prosto)\n* [Concepts](#concepts)\n* [Prosto operations](#prosto-operations)\n* [How to use](#how-to-use)\n\n# What is Prosto?\n\n`Prosto` is a data processing toolkit which significantly simplifies data processing and analysis. \n\nConceptually, it is an alternative to *set-oriented* approaches to data processing like map-reduce, relational algebra, SQL or data-frame-based tools like Python `pandas`.\n\n`Prosto` radically changes the way data is processed by relying on a novel data processing paradigm which treats columns (mathematical functions) as first-class elements of the data processing pipeline having the same rights as tables. Accordingly, a `Prosto` workflow consists of two categories of operations:\n\n* *Table operations* produce (populate) new tables from existing tables. A table is an implementation of a mathematical *set* which is a collection of tuples.\n* *Column operations* produce (evaluate) new columns from existing columns. A column is an implementation of a mathematical *function* which is a mapping of values from one set to another set.\n\n# Why Prosto?\n\nProsto provides the following unique features and benefits:\n\n* *Processing data in multiple tables.* We can easily implement calculate columns (as demonstrated in examples) using `apply` method of `pandas`. However, we cannot use this technique in the case of multiple tables. `Prosto` is intended for and makes it easy to process data stored in many tables by relying on `link` columns which are also evaluated from the data.\n\n* *Getting rid of joins.* Data in multiple tables can be processed using the relational join operation. However, it is tedious, error prone and requires high expertise especially in the case of many tables. `Prosto` does not use joins. Instead, it relies on `link` columns which also have definitions and are part of one workflow.\n\n* *Getting rid of group-by.* Data aggregation is typically performed using some kind of group-by operation. `Prosto` does not use this relational operation by providing column operations for that purpose which are simpler and more natural especially in describing complex analytical workflows.\n\n* *Flexibility via user-defined functions.* `Prosto` is very flexible in defining how data will be processed because it relies on user-defined functions which are its minimal units of data processing. They provide the logic of processing at the level of individual values while the logic of looping through the sets is implemented by the system according to the type of operation applied. User-defined functions can be as simple as format conversion and as complex as as a machine learning algorithm.\n\n* In future, `Prosto` will implement such features as *incremental evaluation* for processing only what has changed, *model training* for training models as part of the workflow, data/model persistence and other data processing and analysis operations.\n\n# Getting started with Prosto\n\n## Importing Prosto\n\n`Prosto` is a toolkit and it is intended to be used from another (Python) application. Before its data processing functions can be used, the module has to be imported:\n\n```python\nimport prosto as pr\n```\n\n## Defining a workflow\n\nA workflow contains *definitions* of data elements (tables and columns) as well as *operations* for data generation. Before data processing operations can be defined, a `Prosto` workflow has to be created:\n\n```python\nprosto = pr.Prosto(\"My Prosto Workflow\")\n```\n\n`Prosto` provides two types of operations which can be used in a workflow:\n\n* A *table population operation* adds new records to the table given records from one or more input tables\n* A *column evaluation operation* generates values of the column given values of one or more input columns\n\n## Defining a table\n\nEach table has some structure which is defined by its *attributes*. Table data is defined by the tuples it consists of and each tuple is a combination of some attribute values.\n\nThere exist many different ways to populate a table with tuples (attribute values). One of the simplest one is a table `population` operation. It relies on a *user-defined function* which is supposed to \"know\" how to populate this table by returning a `pandas` data frame with the data:\n\n```python\nsales_data = {\n    \"product_name\": [\"beer\", \"chips\", \"chips\", \"beer\", \"chips\"],\n    \"quantity\": [1, 2, 3, 2, 1],\n    \"price\": [10.0, 5.0, 6.0, 15.0, 4.0]\n}\n\nsales = prosto.populate(\n    # Table definition consists of a name and a list of attributes\n    table_name=\"Sales\", attributes=[\"product_name\", \"quantity\", \"price\"],\n\n    # Table operation is an UDF, list of input tables and model (parameters for UDF)\n    func=lambda **m: pd.DataFrame(sales_data), tables=[], model={},\n\n    # This parameter says that UDF returns a complete data frame (not one row)\n    input_length=\"table\"\n)\n```\n\nThe user-defined function in this example returns a `pandas` data frame with in-memory sales data. In a more realistic case, the data could be loaded from a CSV file or database. This data frame has to contain all attributes declared for this table.\n\nOther table operations like `project`, `product` and `filter` allow for processing table data from already existing input tables which in turn could be populated using other operations.\n\n## Defining a column\n\nA column is formally interpreted as a mathematical function which maps tuples (defined by table attributes) of this table to tuples in another table.\n\nThere exist many different ways to compute a mapping form one table to another table. One of the simplest column operations is a `calculate` column which *computes* output values of the mapping using the values of the specified input columns of the same table:\n\n```python\ncalc_column = prosto.calculate(\n    # Column definition consists of a name and table it belongs to\n    name=\"amount\", table=sales.id,\n\n    # Column operation is UDF, list of input columns and model (parameters for UDF)\n    func=lambda x: x[\"quantity\"] * x[\"price\"], columns=[\"quantity\", \"price\"], model=None,\n\n    # This parameter says that the UDF returns one value (not a whole column)\n    input_length=\"value\"\n)\n```\n\nThis new column will store the amount computed for each record as a product of quantity and price. Note that the input columns could be also derived columns computed from some other data in this or other tables.\n\nOther column operations like `link`, `aggregate` or `roll` allow for producing link columns referencing records in other tables and aggregate data.\n\n## Executing a workflow\n\nWhen a workflow is defined it is not executed - it stores only operation definitions. In order to really process data, the workflow has to be executed:\n\n```python\nprosto.run()\n```\n\n`Prosto` translates a workflow into a graph of operations (topology) taking into account their dependencies and then executes each operation: table operations will populate tables and column operations will evaluate columns.\n\nNow we can explore the result by reading data form the table along with the calculate column:\n\n```python\ndf = table.get_data()\nprint(df)\n```\n\n```\n   product_name quantity price amount\n0  beer         1        10.0  10.0\n1  chips        2        5.0   10.0\n2  chips        3        6.0   18.0\n3  beer         2        15.0  30.0\n4  chips        1        4.0   4.0\n```\n\nAlthough it looks like a normal table, the last column was derived from the data in other columns. In more realistic cases, column data and table data will be derived from columns in other tables.\n\n# Concepts\n\n## Matrixes vs. sets\n\nIt is important to understand the following crucial difference between matrixes and sets expressed in terms of multidimensional spaces:\n\n> A cell of a matrix is a point in the multidimensional space defined by the matrix axes - the space has as many dimensions as the matrix has axes. Values are defined for all points of the space.\n> A tuple of a set is a point in the space defined by the table columns - the space has as many dimensions as the table has column. Values are defined only for a subset of all points of the space.\n\nIt is summarized in the table:\n\n| Property          | Matrix                | Set                    |\n| ---               | ---                   | ---                    |\n| Dimension         | Axis                  | Attribute              |\n| Point coordinates | Cell axes values      | Tuple attribute values |\n| Dimensionality    | Number of axes        | Number of attributes   |\n| Represents        | Distribution          | Predicate              |\n| Point             | Value of distribution | True of false          |\n\nThe both structure can represent some distribution over a multidimensional space but do it in different ways. Obviously, these differences make it extremely difficult to combine these two semantics in one framework.\n\n`Prosto` is an implementation of the set-oriented approach where a table represents a set and its rows represent tuples. Note however that `Prosto` supports an extended version of the set-oriented approach which includes also functions as first-class elements of the model.\n\n## Sets vs. functions\n\n*Tuples* are a formal representation of data values. A tuple has structure declared by its *attributes*.\n\nA *set* is a collection of *tuples*. A set is a formal representation of a collection of values. Tuples (data values) can be only added to or removed from a set. In `Prosto`, sets are implemented via table objects. \n\nA *function* is a mapping from an input set to an output set. Given an input value, the output value can be read from the function or set for the function. In `Prosto`, functions are implemented via column objects.\n\n## Attributes vs. columns\n\nAttributes define the structure of tuples and they are not evaluated. Attribute values are set by the table population procedure.\n\nColumns implement functions (mappings between sets) and their values are computed by the column evaluation procedure.\n\n## `Pandas` vs. `Prosto`\n\n`Pandas` is a very powerful toolkit which relies on the notion of matrix for data representation. In other words, matrix is the main unit of data representation in `pandas`. Yet, `pandas` supports not only matrix operations (in this case, having `numpy` would be enough) but also set operations, relational operations, map-reduce, multidimensional and OLAP as well as some other conceptions. In this sense, `pandas` is a quite eclectic toolkit. \n\nIn contrast, `Prosto` is based on only one theoretical basis: the concept-oriented model of data. For simplicity, it can be viewed as a purely set-oriented model (not the relational model) along with a function-oriented model. Yet, `Prosto` relies on `pandas` in its implementation just because `pandas` provides a powerful set of various highly optimized operations with data.\n\n# Prosto operations\n\n## List of operations\n\n`Prosto` currently supports the following operations:\n\n* Column operations\n\n  * `calculate` - new column values are computed from other values in the same table and row\n  * `link` - new column values uniquely represent rows from another table\n  * `merge` - new columns values are copied from a linked column in another table\n  * `roll` - new column values are computed from the subset of rows in the same table\n  * `aggregate` - new column values are computed from a subset of row in another table\n\n* Table operations\n\n  * `populate` - rows of the new table are populated by the specified procedure\n  * `product` - rows of the new table are combinations of rows from other tables\n  * `filter` - rows of the new table are a subset of rows from another table\n  * `project` - rows of the new table are all unique sub-tuples from another table\n\nExamples of these operations can be found in unit tests or Jupyter notebooks in the `notebooks` project folder.\n\n## Column operations\n\n### Calculate column (instead of map operation)\n\nProbably the simplest and most frequent operation in `Prosto` is computing a new column of the table which is done by defining a `calculate` column. The main computational part of the definition is a (Python) function which returns a single value computed from one or more input values in its arguments. \n\nThis function will be evaluated for each row of the table and its outputs will be stored as a new column. \n\nIt is precisely how `apply` works in `pandas` (and actually it relies on it in its implementation) but it is different from how `map` operation works because a calculated column does not add any new table while `map` computes a new collection (which makes computations less efficient). \n\nThe `Prosto` approach is somewhat similar to spreadsheets with the difference that new columns depend on only one coordinate - other columns - while cells in spreadsheets depend on two coordinates - row and column addresses. The both however are equally simple and natural.   \n\nCheck out the `calculate.ipynb` notebook for a working example of the `calculate` operaiton.\n\n### Link column (instead of join)\n\nWe can define and evaluate new columns only in individual tables but we cannot define a new column which depends on the data in another table. Link columns solve this problem. A link column stores values which uniquely represent rows of a target (linked) table. In this sense, it is a normal column with some values which are computed using some definition. The difference is how these values are computed and their semantics. They do not have a domain-specific semantics but rather they are understood only by the system. More specifically, each value of a link column is a reference to a row in the linked table or None in the case it does not reference anything. \n\nThe main part of the definition is a criterion for finding a target row which matching this row. The most wide spread criterion is based on equality of some values in two rows and the definition includes lists of the columns which have to be equal in order for this row to reference the target row.\n\nLink columns have several major uses:\n* Data in other (linked) tables can be accessed when doing something in this table, say, when defining its calculate columns\n* Data can be grouped using linked rows interpreted as groups, that is, all rows of this table referencing the same row of the target table are interpreted as one group \n* Link columns are used when defining aggregate columns\n\nThere could be other criteria for matching rows and defining link columns which will be implemented in future versions.\n\nCheck out the `link.ipynb` notebook for a working example of the `link` operaiton.\n\n### Merge column (instead of join)\n\nOnce we have defined link columns and interlinked our (initially isolated) set of tables, the question is how we can use these links? Currently, the only way is to move data between table by copying linked columns performed by the merge operation. It copies a column from the target linked table into this table. In this sense, it simply copies data between tables. Its definition is very simple: we need to specify only the link column and the target column. \n\nThe copied (merged) columns can be then used in other operations like calculate columns or aggregate columns.   \n\nNote that the merge operation (as an explicit operation) is planned to become obsolete in future versions (but can still be used behind the scenes). Yet, currently it is the only way to access data in other tables using link columns. \n\n### Rolling aggregation column (instead of over-partition)\n\nThis column will aggregate data located in \"neighbor\" rows of this same table which are selected using criteria in the window objects. For example, we can specify how many previous rows to select. \n\nCurrently, its logic is equivalent to that of the rolling aggregation in `pandas` with the difference that the result column is immediately added to the table and this operation is part of the whole workflow.\n\nCheck out the `roll.ipynb` notebook for a working example of rolling aggregation.\n\n### Aggregate column (instead of groupby)\n\nThis column aggregates data in groups of rows selected from another table. The selection is performed by specifying an existing link column which links the fact table with this (group) table. The new column is added to this (group) table. \n\nCurrently, its logic is equivalent to that of the groupby in `pandas` with the difference that the result column is added to the existing table and the two tables must be linked beforehand.\n\nCheck out the `aggregate.ipynb` notebook for a working example of aggregation.\n\n## Table operations\n\n### Filter table (instead of select-where)\n\nIt is one of the most frequently used operations. The main difference form conventional implementations is that the result never includes the source table columns. Instead, the result (filtered) table references the selected source rows using an automatically created link column. If it is necessary to use the source table data (and it is almost always the case) then they are accessible via the created link column. \n\n### Project table (instead of select-distinct)\n\nThis operation has these important uses:\n* Creating a table with group elements for aggregation because (in contrast to other approaches) it must exist\n* Creating a dimension table for multi-dimensional analysis in the case it does not exist\n\nCheck out the `project.ipynb` notebook for a working example of the `project` operaiton.\n\n### Product of tables (instead of join)\n\nThis table is intended to produce all combinations of rows in other tables. Its main difference from the relational model is that the result table stores links to the rows of the source tables rather than copies of its rows. The result table has as many attributes as it has source tables in its definition. (In contrast, the number of attributes in a relational product is equal to the sum of attributes in all source tables.)\n\nUses:\n* Creating a cube table from dimension tables for multi-dimensional analysis. It is typically followed by aggregating data in the fact table. \n\n# How to use\n\n## Install from source code\n\nCheck out the source code and execute this command in the project directory (where `setup.py` is located):\n\n```console\n$ pip install .\n```\n\nOr alternatively:\n\n```console\n$ python setup.py install\n```\n\n## Install from PYPI\n\nThis command will install the latest release of `Prosto` from PYPI:\n\n```console\n$ pip install prosto\n```\n\n## How to test\n\nRun tests from the project root:\n\n```console\n$ python -m unittest discover -s ./tests\n```\n\nor\n\n```console\n$ python setup.py test\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/prostodata/prosto", "keywords": "data processing,analytics,data science,pandas,map-reduce,feature engineering,business intelligence", "license": "MIT License", "maintainer": "", "maintainer_email": "", "name": "prosto", "package_url": "https://pypi.org/project/prosto/", "platform": "", "project_url": "https://pypi.org/project/prosto/", "project_urls": {"Homepage": "https://github.com/prostodata/prosto"}, "release_url": "https://pypi.org/project/prosto/0.2.0/", "requires_dist": ["numpy", "pandas"], "requires_python": "", "summary": "Data processing toolkit radically changing the way data is processed", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <pre><code> ____                _        \n|  _ \\ _ __ ___  ___| |_ ___   ___________________________________________\n| |_) | '__/ _ \\/ __| __/ _ \\ \n|  __/| | | (_) \\__ \\ || (_) | Data Processing Toolkit - noSql-noMapReduce\n|_|   |_|  \\___/|___/\\__\\___/  ___________________________________________\n</code></pre>\n<p><a href=\"https://github.com/prostodata/prosto/blob/master/LICENSE\" rel=\"nofollow\"><img alt=\"License: MIT\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/13402995be86cde517cc34ba2cc02d3de74b86c4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d627269676874677265656e2e737667\"></a>\n<a href=\"https://www.python.org/downloads/release/python-370/\" rel=\"nofollow\"><img alt=\"Python 3.6\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b4d126068744eac14a7a6f738762a928f4a8fd38/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d627269676874677265656e2e737667\"></a>\n<a href=\"https://badge.fury.io/py/prosto\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/317b1709ef8a77bbd3b31280920ffee756f2cb3f/68747470733a2f2f62616467652e667572792e696f2f70792f70726f73746f2e737667\"></a></p>\n<ul>\n<li><a href=\"#what-is-prosto\" rel=\"nofollow\">What is Prosto?</a></li>\n<li><a href=\"#why-prosto\" rel=\"nofollow\">Why Prosto?</a></li>\n<li><a href=\"#getting-started-with-prosto\" rel=\"nofollow\">Getting started with Prosto</a></li>\n<li><a href=\"#concepts\" rel=\"nofollow\">Concepts</a></li>\n<li><a href=\"#prosto-operations\" rel=\"nofollow\">Prosto operations</a></li>\n<li><a href=\"#how-to-use\" rel=\"nofollow\">How to use</a></li>\n</ul>\n<h1>What is Prosto?</h1>\n<p><code>Prosto</code> is a data processing toolkit which significantly simplifies data processing and analysis.</p>\n<p>Conceptually, it is an alternative to <em>set-oriented</em> approaches to data processing like map-reduce, relational algebra, SQL or data-frame-based tools like Python <code>pandas</code>.</p>\n<p><code>Prosto</code> radically changes the way data is processed by relying on a novel data processing paradigm which treats columns (mathematical functions) as first-class elements of the data processing pipeline having the same rights as tables. Accordingly, a <code>Prosto</code> workflow consists of two categories of operations:</p>\n<ul>\n<li><em>Table operations</em> produce (populate) new tables from existing tables. A table is an implementation of a mathematical <em>set</em> which is a collection of tuples.</li>\n<li><em>Column operations</em> produce (evaluate) new columns from existing columns. A column is an implementation of a mathematical <em>function</em> which is a mapping of values from one set to another set.</li>\n</ul>\n<h1>Why Prosto?</h1>\n<p>Prosto provides the following unique features and benefits:</p>\n<ul>\n<li>\n<p><em>Processing data in multiple tables.</em> We can easily implement calculate columns (as demonstrated in examples) using <code>apply</code> method of <code>pandas</code>. However, we cannot use this technique in the case of multiple tables. <code>Prosto</code> is intended for and makes it easy to process data stored in many tables by relying on <code>link</code> columns which are also evaluated from the data.</p>\n</li>\n<li>\n<p><em>Getting rid of joins.</em> Data in multiple tables can be processed using the relational join operation. However, it is tedious, error prone and requires high expertise especially in the case of many tables. <code>Prosto</code> does not use joins. Instead, it relies on <code>link</code> columns which also have definitions and are part of one workflow.</p>\n</li>\n<li>\n<p><em>Getting rid of group-by.</em> Data aggregation is typically performed using some kind of group-by operation. <code>Prosto</code> does not use this relational operation by providing column operations for that purpose which are simpler and more natural especially in describing complex analytical workflows.</p>\n</li>\n<li>\n<p><em>Flexibility via user-defined functions.</em> <code>Prosto</code> is very flexible in defining how data will be processed because it relies on user-defined functions which are its minimal units of data processing. They provide the logic of processing at the level of individual values while the logic of looping through the sets is implemented by the system according to the type of operation applied. User-defined functions can be as simple as format conversion and as complex as as a machine learning algorithm.</p>\n</li>\n<li>\n<p>In future, <code>Prosto</code> will implement such features as <em>incremental evaluation</em> for processing only what has changed, <em>model training</em> for training models as part of the workflow, data/model persistence and other data processing and analysis operations.</p>\n</li>\n</ul>\n<h1>Getting started with Prosto</h1>\n<h2>Importing Prosto</h2>\n<p><code>Prosto</code> is a toolkit and it is intended to be used from another (Python) application. Before its data processing functions can be used, the module has to be imported:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">prosto</span> <span class=\"k\">as</span> <span class=\"nn\">pr</span>\n</pre>\n<h2>Defining a workflow</h2>\n<p>A workflow contains <em>definitions</em> of data elements (tables and columns) as well as <em>operations</em> for data generation. Before data processing operations can be defined, a <code>Prosto</code> workflow has to be created:</p>\n<pre><span class=\"n\">prosto</span> <span class=\"o\">=</span> <span class=\"n\">pr</span><span class=\"o\">.</span><span class=\"n\">Prosto</span><span class=\"p\">(</span><span class=\"s2\">\"My Prosto Workflow\"</span><span class=\"p\">)</span>\n</pre>\n<p><code>Prosto</code> provides two types of operations which can be used in a workflow:</p>\n<ul>\n<li>A <em>table population operation</em> adds new records to the table given records from one or more input tables</li>\n<li>A <em>column evaluation operation</em> generates values of the column given values of one or more input columns</li>\n</ul>\n<h2>Defining a table</h2>\n<p>Each table has some structure which is defined by its <em>attributes</em>. Table data is defined by the tuples it consists of and each tuple is a combination of some attribute values.</p>\n<p>There exist many different ways to populate a table with tuples (attribute values). One of the simplest one is a table <code>population</code> operation. It relies on a <em>user-defined function</em> which is supposed to \"know\" how to populate this table by returning a <code>pandas</code> data frame with the data:</p>\n<pre><span class=\"n\">sales_data</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">\"product_name\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"beer\"</span><span class=\"p\">,</span> <span class=\"s2\">\"chips\"</span><span class=\"p\">,</span> <span class=\"s2\">\"chips\"</span><span class=\"p\">,</span> <span class=\"s2\">\"beer\"</span><span class=\"p\">,</span> <span class=\"s2\">\"chips\"</span><span class=\"p\">],</span>\n    <span class=\"s2\">\"quantity\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n    <span class=\"s2\">\"price\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">10.0</span><span class=\"p\">,</span> <span class=\"mf\">5.0</span><span class=\"p\">,</span> <span class=\"mf\">6.0</span><span class=\"p\">,</span> <span class=\"mf\">15.0</span><span class=\"p\">,</span> <span class=\"mf\">4.0</span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">sales</span> <span class=\"o\">=</span> <span class=\"n\">prosto</span><span class=\"o\">.</span><span class=\"n\">populate</span><span class=\"p\">(</span>\n    <span class=\"c1\"># Table definition consists of a name and a list of attributes</span>\n    <span class=\"n\">table_name</span><span class=\"o\">=</span><span class=\"s2\">\"Sales\"</span><span class=\"p\">,</span> <span class=\"n\">attributes</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"product_name\"</span><span class=\"p\">,</span> <span class=\"s2\">\"quantity\"</span><span class=\"p\">,</span> <span class=\"s2\">\"price\"</span><span class=\"p\">],</span>\n\n    <span class=\"c1\"># Table operation is an UDF, list of input tables and model (parameters for UDF)</span>\n    <span class=\"n\">func</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"o\">**</span><span class=\"n\">m</span><span class=\"p\">:</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">sales_data</span><span class=\"p\">),</span> <span class=\"n\">tables</span><span class=\"o\">=</span><span class=\"p\">[],</span> <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"p\">{},</span>\n\n    <span class=\"c1\"># This parameter says that UDF returns a complete data frame (not one row)</span>\n    <span class=\"n\">input_length</span><span class=\"o\">=</span><span class=\"s2\">\"table\"</span>\n<span class=\"p\">)</span>\n</pre>\n<p>The user-defined function in this example returns a <code>pandas</code> data frame with in-memory sales data. In a more realistic case, the data could be loaded from a CSV file or database. This data frame has to contain all attributes declared for this table.</p>\n<p>Other table operations like <code>project</code>, <code>product</code> and <code>filter</code> allow for processing table data from already existing input tables which in turn could be populated using other operations.</p>\n<h2>Defining a column</h2>\n<p>A column is formally interpreted as a mathematical function which maps tuples (defined by table attributes) of this table to tuples in another table.</p>\n<p>There exist many different ways to compute a mapping form one table to another table. One of the simplest column operations is a <code>calculate</code> column which <em>computes</em> output values of the mapping using the values of the specified input columns of the same table:</p>\n<pre><span class=\"n\">calc_column</span> <span class=\"o\">=</span> <span class=\"n\">prosto</span><span class=\"o\">.</span><span class=\"n\">calculate</span><span class=\"p\">(</span>\n    <span class=\"c1\"># Column definition consists of a name and table it belongs to</span>\n    <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">\"amount\"</span><span class=\"p\">,</span> <span class=\"n\">table</span><span class=\"o\">=</span><span class=\"n\">sales</span><span class=\"o\">.</span><span class=\"n\">id</span><span class=\"p\">,</span>\n\n    <span class=\"c1\"># Column operation is UDF, list of input columns and model (parameters for UDF)</span>\n    <span class=\"n\">func</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"s2\">\"quantity\"</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"s2\">\"price\"</span><span class=\"p\">],</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"quantity\"</span><span class=\"p\">,</span> <span class=\"s2\">\"price\"</span><span class=\"p\">],</span> <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n\n    <span class=\"c1\"># This parameter says that the UDF returns one value (not a whole column)</span>\n    <span class=\"n\">input_length</span><span class=\"o\">=</span><span class=\"s2\">\"value\"</span>\n<span class=\"p\">)</span>\n</pre>\n<p>This new column will store the amount computed for each record as a product of quantity and price. Note that the input columns could be also derived columns computed from some other data in this or other tables.</p>\n<p>Other column operations like <code>link</code>, <code>aggregate</code> or <code>roll</code> allow for producing link columns referencing records in other tables and aggregate data.</p>\n<h2>Executing a workflow</h2>\n<p>When a workflow is defined it is not executed - it stores only operation definitions. In order to really process data, the workflow has to be executed:</p>\n<pre><span class=\"n\">prosto</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n</pre>\n<p><code>Prosto</code> translates a workflow into a graph of operations (topology) taking into account their dependencies and then executes each operation: table operations will populate tables and column operations will evaluate columns.</p>\n<p>Now we can explore the result by reading data form the table along with the calculate column:</p>\n<pre><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">table</span><span class=\"o\">.</span><span class=\"n\">get_data</span><span class=\"p\">()</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n</pre>\n<pre><code>   product_name quantity price amount\n0  beer         1        10.0  10.0\n1  chips        2        5.0   10.0\n2  chips        3        6.0   18.0\n3  beer         2        15.0  30.0\n4  chips        1        4.0   4.0\n</code></pre>\n<p>Although it looks like a normal table, the last column was derived from the data in other columns. In more realistic cases, column data and table data will be derived from columns in other tables.</p>\n<h1>Concepts</h1>\n<h2>Matrixes vs. sets</h2>\n<p>It is important to understand the following crucial difference between matrixes and sets expressed in terms of multidimensional spaces:</p>\n<blockquote>\n<p>A cell of a matrix is a point in the multidimensional space defined by the matrix axes - the space has as many dimensions as the matrix has axes. Values are defined for all points of the space.\nA tuple of a set is a point in the space defined by the table columns - the space has as many dimensions as the table has column. Values are defined only for a subset of all points of the space.</p>\n</blockquote>\n<p>It is summarized in the table:</p>\n<table>\n<thead>\n<tr>\n<th>Property</th>\n<th>Matrix</th>\n<th>Set</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Dimension</td>\n<td>Axis</td>\n<td>Attribute</td>\n</tr>\n<tr>\n<td>Point coordinates</td>\n<td>Cell axes values</td>\n<td>Tuple attribute values</td>\n</tr>\n<tr>\n<td>Dimensionality</td>\n<td>Number of axes</td>\n<td>Number of attributes</td>\n</tr>\n<tr>\n<td>Represents</td>\n<td>Distribution</td>\n<td>Predicate</td>\n</tr>\n<tr>\n<td>Point</td>\n<td>Value of distribution</td>\n<td>True of false</td>\n</tr></tbody></table>\n<p>The both structure can represent some distribution over a multidimensional space but do it in different ways. Obviously, these differences make it extremely difficult to combine these two semantics in one framework.</p>\n<p><code>Prosto</code> is an implementation of the set-oriented approach where a table represents a set and its rows represent tuples. Note however that <code>Prosto</code> supports an extended version of the set-oriented approach which includes also functions as first-class elements of the model.</p>\n<h2>Sets vs. functions</h2>\n<p><em>Tuples</em> are a formal representation of data values. A tuple has structure declared by its <em>attributes</em>.</p>\n<p>A <em>set</em> is a collection of <em>tuples</em>. A set is a formal representation of a collection of values. Tuples (data values) can be only added to or removed from a set. In <code>Prosto</code>, sets are implemented via table objects.</p>\n<p>A <em>function</em> is a mapping from an input set to an output set. Given an input value, the output value can be read from the function or set for the function. In <code>Prosto</code>, functions are implemented via column objects.</p>\n<h2>Attributes vs. columns</h2>\n<p>Attributes define the structure of tuples and they are not evaluated. Attribute values are set by the table population procedure.</p>\n<p>Columns implement functions (mappings between sets) and their values are computed by the column evaluation procedure.</p>\n<h2><code>Pandas</code> vs. <code>Prosto</code></h2>\n<p><code>Pandas</code> is a very powerful toolkit which relies on the notion of matrix for data representation. In other words, matrix is the main unit of data representation in <code>pandas</code>. Yet, <code>pandas</code> supports not only matrix operations (in this case, having <code>numpy</code> would be enough) but also set operations, relational operations, map-reduce, multidimensional and OLAP as well as some other conceptions. In this sense, <code>pandas</code> is a quite eclectic toolkit.</p>\n<p>In contrast, <code>Prosto</code> is based on only one theoretical basis: the concept-oriented model of data. For simplicity, it can be viewed as a purely set-oriented model (not the relational model) along with a function-oriented model. Yet, <code>Prosto</code> relies on <code>pandas</code> in its implementation just because <code>pandas</code> provides a powerful set of various highly optimized operations with data.</p>\n<h1>Prosto operations</h1>\n<h2>List of operations</h2>\n<p><code>Prosto</code> currently supports the following operations:</p>\n<ul>\n<li>\n<p>Column operations</p>\n<ul>\n<li><code>calculate</code> - new column values are computed from other values in the same table and row</li>\n<li><code>link</code> - new column values uniquely represent rows from another table</li>\n<li><code>merge</code> - new columns values are copied from a linked column in another table</li>\n<li><code>roll</code> - new column values are computed from the subset of rows in the same table</li>\n<li><code>aggregate</code> - new column values are computed from a subset of row in another table</li>\n</ul>\n</li>\n<li>\n<p>Table operations</p>\n<ul>\n<li><code>populate</code> - rows of the new table are populated by the specified procedure</li>\n<li><code>product</code> - rows of the new table are combinations of rows from other tables</li>\n<li><code>filter</code> - rows of the new table are a subset of rows from another table</li>\n<li><code>project</code> - rows of the new table are all unique sub-tuples from another table</li>\n</ul>\n</li>\n</ul>\n<p>Examples of these operations can be found in unit tests or Jupyter notebooks in the <code>notebooks</code> project folder.</p>\n<h2>Column operations</h2>\n<h3>Calculate column (instead of map operation)</h3>\n<p>Probably the simplest and most frequent operation in <code>Prosto</code> is computing a new column of the table which is done by defining a <code>calculate</code> column. The main computational part of the definition is a (Python) function which returns a single value computed from one or more input values in its arguments.</p>\n<p>This function will be evaluated for each row of the table and its outputs will be stored as a new column.</p>\n<p>It is precisely how <code>apply</code> works in <code>pandas</code> (and actually it relies on it in its implementation) but it is different from how <code>map</code> operation works because a calculated column does not add any new table while <code>map</code> computes a new collection (which makes computations less efficient).</p>\n<p>The <code>Prosto</code> approach is somewhat similar to spreadsheets with the difference that new columns depend on only one coordinate - other columns - while cells in spreadsheets depend on two coordinates - row and column addresses. The both however are equally simple and natural.</p>\n<p>Check out the <code>calculate.ipynb</code> notebook for a working example of the <code>calculate</code> operaiton.</p>\n<h3>Link column (instead of join)</h3>\n<p>We can define and evaluate new columns only in individual tables but we cannot define a new column which depends on the data in another table. Link columns solve this problem. A link column stores values which uniquely represent rows of a target (linked) table. In this sense, it is a normal column with some values which are computed using some definition. The difference is how these values are computed and their semantics. They do not have a domain-specific semantics but rather they are understood only by the system. More specifically, each value of a link column is a reference to a row in the linked table or None in the case it does not reference anything.</p>\n<p>The main part of the definition is a criterion for finding a target row which matching this row. The most wide spread criterion is based on equality of some values in two rows and the definition includes lists of the columns which have to be equal in order for this row to reference the target row.</p>\n<p>Link columns have several major uses:</p>\n<ul>\n<li>Data in other (linked) tables can be accessed when doing something in this table, say, when defining its calculate columns</li>\n<li>Data can be grouped using linked rows interpreted as groups, that is, all rows of this table referencing the same row of the target table are interpreted as one group</li>\n<li>Link columns are used when defining aggregate columns</li>\n</ul>\n<p>There could be other criteria for matching rows and defining link columns which will be implemented in future versions.</p>\n<p>Check out the <code>link.ipynb</code> notebook for a working example of the <code>link</code> operaiton.</p>\n<h3>Merge column (instead of join)</h3>\n<p>Once we have defined link columns and interlinked our (initially isolated) set of tables, the question is how we can use these links? Currently, the only way is to move data between table by copying linked columns performed by the merge operation. It copies a column from the target linked table into this table. In this sense, it simply copies data between tables. Its definition is very simple: we need to specify only the link column and the target column.</p>\n<p>The copied (merged) columns can be then used in other operations like calculate columns or aggregate columns.</p>\n<p>Note that the merge operation (as an explicit operation) is planned to become obsolete in future versions (but can still be used behind the scenes). Yet, currently it is the only way to access data in other tables using link columns.</p>\n<h3>Rolling aggregation column (instead of over-partition)</h3>\n<p>This column will aggregate data located in \"neighbor\" rows of this same table which are selected using criteria in the window objects. For example, we can specify how many previous rows to select.</p>\n<p>Currently, its logic is equivalent to that of the rolling aggregation in <code>pandas</code> with the difference that the result column is immediately added to the table and this operation is part of the whole workflow.</p>\n<p>Check out the <code>roll.ipynb</code> notebook for a working example of rolling aggregation.</p>\n<h3>Aggregate column (instead of groupby)</h3>\n<p>This column aggregates data in groups of rows selected from another table. The selection is performed by specifying an existing link column which links the fact table with this (group) table. The new column is added to this (group) table.</p>\n<p>Currently, its logic is equivalent to that of the groupby in <code>pandas</code> with the difference that the result column is added to the existing table and the two tables must be linked beforehand.</p>\n<p>Check out the <code>aggregate.ipynb</code> notebook for a working example of aggregation.</p>\n<h2>Table operations</h2>\n<h3>Filter table (instead of select-where)</h3>\n<p>It is one of the most frequently used operations. The main difference form conventional implementations is that the result never includes the source table columns. Instead, the result (filtered) table references the selected source rows using an automatically created link column. If it is necessary to use the source table data (and it is almost always the case) then they are accessible via the created link column.</p>\n<h3>Project table (instead of select-distinct)</h3>\n<p>This operation has these important uses:</p>\n<ul>\n<li>Creating a table with group elements for aggregation because (in contrast to other approaches) it must exist</li>\n<li>Creating a dimension table for multi-dimensional analysis in the case it does not exist</li>\n</ul>\n<p>Check out the <code>project.ipynb</code> notebook for a working example of the <code>project</code> operaiton.</p>\n<h3>Product of tables (instead of join)</h3>\n<p>This table is intended to produce all combinations of rows in other tables. Its main difference from the relational model is that the result table stores links to the rows of the source tables rather than copies of its rows. The result table has as many attributes as it has source tables in its definition. (In contrast, the number of attributes in a relational product is equal to the sum of attributes in all source tables.)</p>\n<p>Uses:</p>\n<ul>\n<li>Creating a cube table from dimension tables for multi-dimensional analysis. It is typically followed by aggregating data in the fact table.</li>\n</ul>\n<h1>How to use</h1>\n<h2>Install from source code</h2>\n<p>Check out the source code and execute this command in the project directory (where <code>setup.py</code> is located):</p>\n<pre><span class=\"gp\">$</span> pip install .\n</pre>\n<p>Or alternatively:</p>\n<pre><span class=\"gp\">$</span> python setup.py install\n</pre>\n<h2>Install from PYPI</h2>\n<p>This command will install the latest release of <code>Prosto</code> from PYPI:</p>\n<pre><span class=\"gp\">$</span> pip install prosto\n</pre>\n<h2>How to test</h2>\n<p>Run tests from the project root:</p>\n<pre><span class=\"gp\">$</span> python -m unittest discover -s ./tests\n</pre>\n<p>or</p>\n<pre><span class=\"gp\">$</span> python setup.py <span class=\"nb\">test</span>\n</pre>\n\n          </div>"}, "last_serial": 7011587, "releases": {"0.2.0": [{"comment_text": "", "digests": {"md5": "7824990a1a8b171af7f86acd284bfae4", "sha256": "3bb3d6fccf61a8f136871cce1935ec5f6b48c121f4f321d76ad986657d87aa72"}, "downloads": -1, "filename": "prosto-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "7824990a1a8b171af7f86acd284bfae4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 31513, "upload_time": "2020-04-13T17:43:19", "upload_time_iso_8601": "2020-04-13T17:43:19.526784Z", "url": "https://files.pythonhosted.org/packages/d3/1d/2cc054329b31d7bed6f32fa26e9daf44519ea018dff3a7badad72443bfe7/prosto-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "071db5c546a4e42764778a3f25e9fcc6", "sha256": "faa66ec219cc73fa9ae78c44b3413892a258bdc068f88536e198e67b51687294"}, "downloads": -1, "filename": "prosto-0.2.0.tar.gz", "has_sig": false, "md5_digest": "071db5c546a4e42764778a3f25e9fcc6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 33318, "upload_time": "2020-04-13T17:43:21", "upload_time_iso_8601": "2020-04-13T17:43:21.562587Z", "url": "https://files.pythonhosted.org/packages/b7/63/8252343c6f6b129b261a952f7b57106595aff063d7164684a48c4abd9b1f/prosto-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "7824990a1a8b171af7f86acd284bfae4", "sha256": "3bb3d6fccf61a8f136871cce1935ec5f6b48c121f4f321d76ad986657d87aa72"}, "downloads": -1, "filename": "prosto-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "7824990a1a8b171af7f86acd284bfae4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 31513, "upload_time": "2020-04-13T17:43:19", "upload_time_iso_8601": "2020-04-13T17:43:19.526784Z", "url": "https://files.pythonhosted.org/packages/d3/1d/2cc054329b31d7bed6f32fa26e9daf44519ea018dff3a7badad72443bfe7/prosto-0.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "071db5c546a4e42764778a3f25e9fcc6", "sha256": "faa66ec219cc73fa9ae78c44b3413892a258bdc068f88536e198e67b51687294"}, "downloads": -1, "filename": "prosto-0.2.0.tar.gz", "has_sig": false, "md5_digest": "071db5c546a4e42764778a3f25e9fcc6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 33318, "upload_time": "2020-04-13T17:43:21", "upload_time_iso_8601": "2020-04-13T17:43:21.562587Z", "url": "https://files.pythonhosted.org/packages/b7/63/8252343c6f6b129b261a952f7b57106595aff063d7164684a48c4abd9b1f/prosto-0.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:16:45 2020"}