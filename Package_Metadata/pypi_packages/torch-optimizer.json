{"info": {"author": "Nikolay Novik", "author_email": "nickolainovik@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "torch-optimizer\n===============\n.. image:: https://travis-ci.com/jettify/pytorch-optimizer.svg?branch=master\n    :target: https://travis-ci.com/jettify/pytorch-optimizer\n.. image:: https://codecov.io/gh/jettify/pytorch-optimizer/branch/master/graph/badge.svg\n    :target: https://codecov.io/gh/jettify/pytorch-optimizer\n.. image:: https://img.shields.io/pypi/pyversions/torch-optimizer.svg\n    :target: https://pypi.org/project/torch-optimizer\n.. image:: https://img.shields.io/pypi/v/torch-optimizer.svg\n    :target: https://pypi.python.org/pypi/torch-optimizer\n.. image:: https://static.deepsource.io/deepsource-badge-light-mini.svg\n    :target: https://deepsource.io/gh/jettify/pytorch-optimizer/?ref=repository-badge\n\n\n**torch-optimizer** -- collection of optimizers for PyTorch_.\n\n\n\nSimple example\n--------------\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.DiffGrad(model.parameters(), lr=0.001)\n    optimizer.step()\n\n\nInstallation\n------------\nInstallation process is simple, just::\n\n    $ pip install torch_optimizer\n\n\nSupported Optimizers\n====================\n\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `AccSGD`_   | https://arxiv.org/abs/1803.05591                                              |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `AdaBound`_ | https://arxiv.org/abs/1902.09843                                              |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `AdaMod`_   | https://arxiv.org/abs/1910.12249                                              |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `DiffGrad`_ | https://arxiv.org/abs/1909.11015                                              |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `Lamb`_     | https://arxiv.org/abs/1904.00962                                              |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `NovoGrad`_ | https://arxiv.org/abs/1905.11286                                              |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `PID`_      | https://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR18_PID.pdf                 |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `QHAdam`_   | https://arxiv.org/abs/1810.06801                                              |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `QHM`_      | https://arxiv.org/abs/1810.06801                                              |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `RAdam`_    | https://arxiv.org/abs/1908.03265                                              |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `Ranger`_   | https://arxiv.org/abs/1908.00700v2                                            |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `RangerQH`_ | https://arxiv.org/abs/1908.00700v2                                            |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `RangerVA`_ | https://arxiv.org/abs/1908.00700v2                                            |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `SGDW`_     | https://arxiv.org/abs/1608.03983                                              |\n+-------------+-------------------------------------------------------------------------------+\n|             |                                                                               |\n| `Yogi`_     | https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization |\n+-------------+-------------------------------------------------------------------------------+\n\n\nVisualizations\n--------------\nVisualizations help us to see how different algorithms deals with simple\nsituations like: saddle points, local minima, valleys etc, and may provide\ninteresting insights into inner workings of algorithm. Rosenbrock_ and Rastrigin_\nbenchmark_ functions was selected, because:\n\n* Rosenbrock_ (also known as banana function), is non-convex function that has\n  one global minima  `(1.0. 1.0)`. The global minimum is inside a long,\n  narrow, parabolic shaped flat valley. To find the valley is trivial. To\n  converge to the global minima, however, is difficult. Optimization\n  algorithms might pay a lot of attention to one coordinate, and have\n  problems to follow valley which is relatively flat.\n\n .. image::  https://upload.wikimedia.org/wikipedia/commons/3/32/Rosenbrock_function.svg\n\n* Rastrigin_ function is a non-convex and has one global minima in `(0.0, 0.0)`.\n  Finding the minimum of this function is a fairly difficult problem due to\n  its large search space and its large number of local minima.\n\n  .. image::  https://upload.wikimedia.org/wikipedia/commons/8/8b/Rastrigin_function.png\n\nEach optimizer performs `501` optimization steps. Learning rate is best one found\nby hyper parameter search algorithm, rest of tuning parameters are default. It\nis very easy to extend script and tune other optimizer parameters.\n\n\n.. code::\n\n    python examples/viz_optimizers.py\n\n\nAccSGD\n------\n\n+-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_AccSGD.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_AccSGD.png  |\n+-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.AccSGD(\n        model.parameters(),\n        lr=1e-3,\n        kappa=1000.0,\n        xi=10.0,\n        small_const=0.7,\n        weight_decay=0\n    )\n    optimizer.step()\n\n\n**Paper**: *On the insufficiency of existing momentum schemes for Stochastic Optimization* (2019) [https://arxiv.org/abs/1803.05591]\n\n**Reference Code**: https://github.com/rahulkidambi/AccSGD\n\nAdaBound\n--------\n\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_AdaBound.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_AdaBound.png |\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.AdaBound(\n        m.parameters(),\n        lr= 1e-3,\n        betas= (0.9, 0.999),\n        final_lr = 0.1,\n        gamma=1e-3,\n        eps= 1e-8,\n        weight_decay=0,\n        amsbound=False,\n    )\n    optimizer.step()\n\n\n**Paper**: *Adaptive Gradient Methods with Dynamic Bound of Learning Rate* (2019) [https://arxiv.org/abs/1902.09843]\n\n**Reference Code**: https://github.com/Luolc/AdaBound\n\nAdaMod\n------\nAdaMod method restricts the adaptive learning rates with adaptive and momental\nupper bounds. The dynamic learning rate bounds are based on the exponential\nmoving averages of the adaptive learning rates themselves, which smooth out\nunexpected large learning rates and stabilize the training of deep neural networks.\n\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_AdaMod.png    |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_AdaMod.png   |\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.AdaMod(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        beta3=0.999,\n        eps=1e-8,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n**Paper**: *An Adaptive and Momental Bound Method for Stochastic Learning.* (2019) [https://arxiv.org/abs/1910.12249]\n\n**Reference Code**: https://github.com/lancopku/AdaMod\n\nDiffGrad\n--------\nOptimizer based on the difference between the present and the immediate past\ngradient, the step size is adjusted for each parameter in such\na way that it should have a larger step size for faster gradient changing\nparameters and a lower step size for lower gradient changing parameters.\n\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_DiffGrad.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_DiffGrad.png  |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.DiffGrad(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n\n**Paper**: *diffGrad: An Optimization Method for Convolutional Neural Networks.* (2019) [https://arxiv.org/abs/1909.11015]\n\n**Reference Code**: https://github.com/shivram1987/diffGrad\n\nLamb\n----\n\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Lamb.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Lamb.png  |\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.Lamb(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n\n**Paper**: *Large Batch Optimization for Deep Learning: Training BERT in 76 minutes* (2019) [https://arxiv.org/abs/1904.00962]\n\n**Reference Code**: https://github.com/cybertronai/pytorch-lamb\n\n\nNovoGrad\n--------\n\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_NovoGrad.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_NovoGrad.png  |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.NovoGrad(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n        grad_averaging=False,\n        amsgrad=False,\n    )\n    optimizer.step()\n\n\n**Paper**: *Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks* (2019) [https://arxiv.org/abs/1905.11286]\n\n**Reference Code**: https://github.com/NVIDIA/DeepLearningExamples/\n\n\nPID\n---\n\n+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_PID.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_PID.png  |\n+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.PID(\n        m.parameters(),\n        lr=1e-3,\n        momentum=0,\n        dampening=0,\n        weight_decay=1e-2,\n        integral=5.0,\n        derivative=10.0,\n    )\n    optimizer.step()\n\n\n**Paper**: *A PID Controller Approach for Stochastic Optimization of Deep Networks* (2018) [http://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR18_PID.pdf]\n\n**Reference Code**: https://github.com/tensorboy/PIDOptimizer\n\n\nQHAdam\n------\n\n+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_QHAdam.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_QHAdam.png  |\n+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.QHAdam(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        nus=(1.0, 1.0),\n        weight_decay=0,\n        decouple_weight_decay=False,\n        eps=1e-8,\n    )\n    optimizer.step()\n\n\n**Paper**: *Quasi-hyperbolic momentum and Adam for deep learning* (2019) [https://arxiv.org/abs/1810.06801]\n\n**Reference Code**: https://github.com/facebookresearch/qhoptim\n\n\nQHM\n---\n\n+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_QHM.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_QHM.png  |\n+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.QHM(\n        m.parameters(),\n        lr=1e-3,\n        momentum=0,\n        nu=0.7,\n        weight_decay=1e-2,\n        weight_decay_type='grad',\n    )\n    optimizer.step()\n\n\n**Paper**: *Quasi-hyperbolic momentum and Adam for deep learning* (2019) [https://arxiv.org/abs/1810.06801]\n\n**Reference Code**: https://github.com/facebookresearch/qhoptim\n\n\nRAdam\n-----\n\n+---------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_RAdam.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_RAdam.png  |\n+---------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.RAdam(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n\n**Paper**: *On the Variance of the Adaptive Learning Rate and Beyond* (2019) [https://arxiv.org/abs/1908.03265]\n\n**Reference Code**: https://github.com/LiyuanLucasLiu/RAdam\n\n\nRanger\n------\n\n+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Ranger.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Ranger.png  |\n+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.Ranger(\n        m.parameters(),\n        lr=1e-3,\n        alpha=0.5,\n        k=6,\n        N_sma_threshhold=5,\n        betas=(.95, 0.999),\n        eps=1e-5,\n        weight_decay=0\n    )\n    optimizer.step()\n\n\n**Paper**: *Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM* (2019) [https://arxiv.org/abs/1908.00700v2]\n\n**Reference Code**: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n\n\nRangerQH\n--------\n\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_RangerQH.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_RangerQH.png  |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.RangerQH(\n        m.parameters(),\n        lr=1e-3,\n        betas=(0.9, 0.999),\n        nus=(.7, 1.0),\n        weight_decay=0.0,\n        k=6,\n        alpha=.5,\n        decouple_weight_decay=False,\n        eps=1e-8,\n    )\n    optimizer.step()\n\n\n**Paper**: *Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM* (2019) [https://arxiv.org/abs/1908.00700v2]\n\n**Reference Code**: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n\n\nRangerVA\n--------\n\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_RangerVA.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_RangerVA.png  |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.RangerVA(\n        m.parameters(),\n        lr=1e-3,\n        alpha=0.5,\n        k=6,\n        n_sma_threshhold=5,\n        betas=(.95, 0.999),\n        eps=1e-5,\n        weight_decay=0,\n        amsgrad=True,\n        transformer='softplus',\n        smooth=50,\n        grad_transformer='square'\n    )\n    optimizer.step()\n\n\n**Paper**: *Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM* (2019) [https://arxiv.org/abs/1908.00700v2]\n\n**Reference Code**: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n\n\nSGDW\n----\n\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_SGDW.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_SGDW.png  |\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.SGDW(\n        m.parameters(),\n        lr= 1e-3,\n        momentum=0,\n        dampening=0,\n        weight_decay=1e-2,\n        nesterov=False,\n    )\n    optimizer.step()\n\n\n**Paper**: *SGDR: Stochastic Gradient Descent with Warm Restarts* (2017) [https://arxiv.org/abs/1608.03983]\n\n**Reference Code**: https://github.com/pytorch/pytorch/pull/22466\n\nYogi\n----\n\nYogi is optimization algorithm based on ADAM with more fine grained effective\nlearning rate control, and has similar theoretical guarantees on convergence as ADAM.\n\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Yogi.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Yogi.png  |\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.Yogi(\n        m.parameters(),\n        lr= 1e-2,\n        betas=(0.9, 0.999),\n        eps=1e-3,\n        initial_accumulator=1e-6,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n\n**Paper**: *Adaptive Methods for Nonconvex Optimization* (2018) [https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization]\n\n**Reference Code**: https://github.com/4rtemi5/Yogi-Optimizer_Keras\n\n\nAdam (PyTorch built-in)\n-----------------------\n\n+---------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Adam.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Adam.png  |\n+---------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\nSGD (PyTorch built-in)\n----------------------\n\n+--------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_SGD.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_SGD.png  |\n+--------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n\n.. _Python: https://www.python.org\n.. _PyTorch: https://github.com/pytorch/pytorch\n.. _Rastrigin: https://en.wikipedia.org/wiki/Rastrigin_function\n.. _Rosenbrock: https://en.wikipedia.org/wiki/Rosenbrock_function\n.. _benchmark: https://en.wikipedia.org/wiki/Test_functions_for_optimization\n\nChanges\n-------\n\n0.0.1 (YYYY-MM-DD)\n------------------\n* Initial release.\n\n", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "https://pypi.org/project/torch-optimizer/", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/jettify/pytorch-optimizer", "keywords": "torch-optimizer,pytorch,accsgd,adabound,adamod,diffgrad,lamb,lookahead,novograd,pid,qhadam,qhm,radam,sgdw,yogi,ranger", "license": "Apache 2", "maintainer": "", "maintainer_email": "", "name": "torch-optimizer", "package_url": "https://pypi.org/project/torch-optimizer/", "platform": "POSIX", "project_url": "https://pypi.org/project/torch-optimizer/", "project_urls": {"Download": "https://pypi.org/project/torch-optimizer/", "Homepage": "https://github.com/jettify/pytorch-optimizer"}, "release_url": "https://pypi.org/project/torch-optimizer/0.0.1a12/", "requires_dist": ["torch (>=1.1.0)", "pytorch-ranger (>=0.1.1)"], "requires_python": "", "summary": "pytorch-optimizer", "version": "0.0.1a12", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"torch-optimizer\">\n<h2>torch-optimizer</h2>\n<a href=\"https://travis-ci.com/jettify/pytorch-optimizer\" rel=\"nofollow\"><img alt=\"https://travis-ci.com/jettify/pytorch-optimizer.svg?branch=master\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0fc92762bd59ceaf4a71a30c77a77cbdb3b09283/68747470733a2f2f7472617669732d63692e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://codecov.io/gh/jettify/pytorch-optimizer\" rel=\"nofollow\"><img alt=\"https://codecov.io/gh/jettify/pytorch-optimizer/branch/master/graph/badge.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4e6f12b725347cbea93071eb50fcc06aba305f2f/68747470733a2f2f636f6465636f762e696f2f67682f6a6574746966792f7079746f7263682d6f7074696d697a65722f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a>\n<a href=\"https://pypi.org/project/torch-optimizer\" rel=\"nofollow\"><img alt=\"https://img.shields.io/pypi/pyversions/torch-optimizer.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/31e469419c76f0ec53a5ded066854e8ac3e5f6bf/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f746f7263682d6f7074696d697a65722e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/torch-optimizer\" rel=\"nofollow\"><img alt=\"https://img.shields.io/pypi/v/torch-optimizer.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5a48537229c550084a2a5e0303f320650e7d3665/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f746f7263682d6f7074696d697a65722e737667\"></a>\n<a href=\"https://deepsource.io/gh/jettify/pytorch-optimizer/?ref=repository-badge\" rel=\"nofollow\"><img alt=\"https://static.deepsource.io/deepsource-badge-light-mini.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6837f002aa8f2f99f307c9febf81cee78d869751/68747470733a2f2f7374617469632e64656570736f757263652e696f2f64656570736f757263652d62616467652d6c696768742d6d696e692e737667\"></a>\n<p><strong>torch-optimizer</strong> \u2013 collection of optimizers for <a href=\"https://github.com/pytorch/pytorch\" rel=\"nofollow\">PyTorch</a>.</p>\n<div id=\"simple-example\">\n<h3>Simple example</h3>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">DiffGrad</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n</div>\n<div id=\"installation\">\n<h3>Installation</h3>\n<p>Installation process is simple, just:</p>\n<pre>$ pip install torch_optimizer\n</pre>\n</div>\n</div>\n<div id=\"supported-optimizers\">\n<h2>Supported Optimizers</h2>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><a href=\"#accsgd\" rel=\"nofollow\">AccSGD</a></td>\n<td><a href=\"https://arxiv.org/abs/1803.05591\" rel=\"nofollow\">https://arxiv.org/abs/1803.05591</a></td>\n</tr>\n<tr><td><a href=\"#adabound\" rel=\"nofollow\">AdaBound</a></td>\n<td><a href=\"https://arxiv.org/abs/1902.09843\" rel=\"nofollow\">https://arxiv.org/abs/1902.09843</a></td>\n</tr>\n<tr><td><a href=\"#adamod\" rel=\"nofollow\">AdaMod</a></td>\n<td><a href=\"https://arxiv.org/abs/1910.12249\" rel=\"nofollow\">https://arxiv.org/abs/1910.12249</a></td>\n</tr>\n<tr><td><a href=\"#diffgrad\" rel=\"nofollow\">DiffGrad</a></td>\n<td><a href=\"https://arxiv.org/abs/1909.11015\" rel=\"nofollow\">https://arxiv.org/abs/1909.11015</a></td>\n</tr>\n<tr><td><a href=\"#lamb\" rel=\"nofollow\">Lamb</a></td>\n<td><a href=\"https://arxiv.org/abs/1904.00962\" rel=\"nofollow\">https://arxiv.org/abs/1904.00962</a></td>\n</tr>\n<tr><td><a href=\"#novograd\" rel=\"nofollow\">NovoGrad</a></td>\n<td><a href=\"https://arxiv.org/abs/1905.11286\" rel=\"nofollow\">https://arxiv.org/abs/1905.11286</a></td>\n</tr>\n<tr><td><a href=\"#pid\" rel=\"nofollow\">PID</a></td>\n<td><a href=\"https://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR18_PID.pdf\" rel=\"nofollow\">https://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR18_PID.pdf</a></td>\n</tr>\n<tr><td><a href=\"#qhadam\" rel=\"nofollow\">QHAdam</a></td>\n<td><a href=\"https://arxiv.org/abs/1810.06801\" rel=\"nofollow\">https://arxiv.org/abs/1810.06801</a></td>\n</tr>\n<tr><td><a href=\"#qhm\" rel=\"nofollow\">QHM</a></td>\n<td><a href=\"https://arxiv.org/abs/1810.06801\" rel=\"nofollow\">https://arxiv.org/abs/1810.06801</a></td>\n</tr>\n<tr><td><a href=\"#radam\" rel=\"nofollow\">RAdam</a></td>\n<td><a href=\"https://arxiv.org/abs/1908.03265\" rel=\"nofollow\">https://arxiv.org/abs/1908.03265</a></td>\n</tr>\n<tr><td><a href=\"#ranger\" rel=\"nofollow\">Ranger</a></td>\n<td><a href=\"https://arxiv.org/abs/1908.00700v2\" rel=\"nofollow\">https://arxiv.org/abs/1908.00700v2</a></td>\n</tr>\n<tr><td><a href=\"#rangerqh\" rel=\"nofollow\">RangerQH</a></td>\n<td><a href=\"https://arxiv.org/abs/1908.00700v2\" rel=\"nofollow\">https://arxiv.org/abs/1908.00700v2</a></td>\n</tr>\n<tr><td><a href=\"#rangerva\" rel=\"nofollow\">RangerVA</a></td>\n<td><a href=\"https://arxiv.org/abs/1908.00700v2\" rel=\"nofollow\">https://arxiv.org/abs/1908.00700v2</a></td>\n</tr>\n<tr><td><a href=\"#sgdw\" rel=\"nofollow\">SGDW</a></td>\n<td><a href=\"https://arxiv.org/abs/1608.03983\" rel=\"nofollow\">https://arxiv.org/abs/1608.03983</a></td>\n</tr>\n<tr><td><a href=\"#yogi\" rel=\"nofollow\">Yogi</a></td>\n<td><a href=\"https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization\" rel=\"nofollow\">https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization</a></td>\n</tr>\n</tbody>\n</table>\n<div id=\"visualizations\">\n<h3>Visualizations</h3>\n<p>Visualizations help us to see how different algorithms deals with simple\nsituations like: saddle points, local minima, valleys etc, and may provide\ninteresting insights into inner workings of algorithm. <a href=\"https://en.wikipedia.org/wiki/Rosenbrock_function\" rel=\"nofollow\">Rosenbrock</a> and <a href=\"https://en.wikipedia.org/wiki/Rastrigin_function\" rel=\"nofollow\">Rastrigin</a>\n<a href=\"https://en.wikipedia.org/wiki/Test_functions_for_optimization\" rel=\"nofollow\">benchmark</a> functions was selected, because:</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Rosenbrock_function\" rel=\"nofollow\">Rosenbrock</a> (also known as banana function), is non-convex function that has\none global minima  <cite>(1.0. 1.0)</cite>. The global minimum is inside a long,\nnarrow, parabolic shaped flat valley. To find the valley is trivial. To\nconverge to the global minima, however, is difficult. Optimization\nalgorithms might pay a lot of attention to one coordinate, and have\nproblems to follow valley which is relatively flat.</li>\n</ul>\n<blockquote>\n<img alt=\"https://upload.wikimedia.org/wikipedia/commons/3/32/Rosenbrock_function.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bd2b4374f1a0a11e347bccadd18b6503c48fb09b/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f332f33322f526f73656e62726f636b5f66756e6374696f6e2e737667\">\n</blockquote>\n<ul>\n<li><p><a href=\"https://en.wikipedia.org/wiki/Rastrigin_function\" rel=\"nofollow\">Rastrigin</a> function is a non-convex and has one global minima in <cite>(0.0, 0.0)</cite>.\nFinding the minimum of this function is a fairly difficult problem due to\nits large search space and its large number of local minima.</p>\n<img alt=\"https://upload.wikimedia.org/wikipedia/commons/8/8b/Rastrigin_function.png\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1b840dcb46004ac04769616b56790c31dce8b485/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f382f38622f52617374726967696e5f66756e6374696f6e2e706e67\">\n</li>\n</ul>\n<p>Each optimizer performs <cite>501</cite> optimization steps. Learning rate is best one found\nby hyper parameter search algorithm, rest of tuning parameters are default. It\nis very easy to extend script and tune other optimizer parameters.</p>\n<pre>python examples/viz_optimizers.py\n</pre>\n</div>\n<div id=\"accsgd\">\n<h3>AccSGD</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_AccSGD.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/63d5aaa2072f0da4bdd9b4c6d2b0759df4f6cd19/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f4163635347442e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_AccSGD.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b90f0c282de4b305cefe5cfb590228aec9445f95/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f4163635347442e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">AccSGD</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">kappa</span><span class=\"o\">=</span><span class=\"mf\">1000.0</span><span class=\"p\">,</span>\n    <span class=\"n\">xi</span><span class=\"o\">=</span><span class=\"mf\">10.0</span><span class=\"p\">,</span>\n    <span class=\"n\">small_const</span><span class=\"o\">=</span><span class=\"mf\">0.7</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mi\">0</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>On the insufficiency of existing momentum schemes for Stochastic Optimization</em> (2019) [<a href=\"https://arxiv.org/abs/1803.05591\" rel=\"nofollow\">https://arxiv.org/abs/1803.05591</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/rahulkidambi/AccSGD\" rel=\"nofollow\">https://github.com/rahulkidambi/AccSGD</a></p>\n</div>\n<div id=\"adabound\">\n<h3>AdaBound</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_AdaBound.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6215553dba268cb790ee7217cc44dc84c810b85d/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f416461426f756e642e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_AdaBound.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bcf50ee7c46ef9fba9d9aec6c9a8c370d894b49d/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f416461426f756e642e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">AdaBound</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span> <span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">betas</span><span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">),</span>\n    <span class=\"n\">final_lr</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span>\n    <span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">eps</span><span class=\"o\">=</span> <span class=\"mf\">1e-8</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">amsbound</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>Adaptive Gradient Methods with Dynamic Bound of Learning Rate</em> (2019) [<a href=\"https://arxiv.org/abs/1902.09843\" rel=\"nofollow\">https://arxiv.org/abs/1902.09843</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/Luolc/AdaBound\" rel=\"nofollow\">https://github.com/Luolc/AdaBound</a></p>\n</div>\n<div id=\"adamod\">\n<h3>AdaMod</h3>\n<p>AdaMod method restricts the adaptive learning rates with adaptive and momental\nupper bounds. The dynamic learning rate bounds are based on the exponential\nmoving averages of the adaptive learning rates themselves, which smooth out\nunexpected large learning rates and stabilize the training of deep neural networks.</p>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_AdaMod.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1123c0729973575393c827671cb03df4374dda0d/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f4164614d6f642e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_AdaMod.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9c715f95ded5820dfa2cbba71797ce8126c9fd56/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f4164614d6f642e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">AdaMod</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span> <span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">),</span>\n    <span class=\"n\">beta3</span><span class=\"o\">=</span><span class=\"mf\">0.999</span><span class=\"p\">,</span>\n    <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-8</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>An Adaptive and Momental Bound Method for Stochastic Learning.</em> (2019) [<a href=\"https://arxiv.org/abs/1910.12249\" rel=\"nofollow\">https://arxiv.org/abs/1910.12249</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/lancopku/AdaMod\" rel=\"nofollow\">https://github.com/lancopku/AdaMod</a></p>\n</div>\n<div id=\"diffgrad\">\n<h3>DiffGrad</h3>\n<p>Optimizer based on the difference between the present and the immediate past\ngradient, the step size is adjusted for each parameter in such\na way that it should have a larger step size for faster gradient changing\nparameters and a lower step size for lower gradient changing parameters.</p>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_DiffGrad.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/af44188dd6261594034f1dc2929ba96cd6a38457/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f44696666477261642e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_DiffGrad.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/db858032d1f11560a8d183be20fcfb1032311b21/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f44696666477261642e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">DiffGrad</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span> <span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">),</span>\n    <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-8</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>diffGrad: An Optimization Method for Convolutional Neural Networks.</em> (2019) [<a href=\"https://arxiv.org/abs/1909.11015\" rel=\"nofollow\">https://arxiv.org/abs/1909.11015</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/shivram1987/diffGrad\" rel=\"nofollow\">https://github.com/shivram1987/diffGrad</a></p>\n</div>\n<div id=\"lamb\">\n<h3>Lamb</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Lamb.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/cc69d895c9de2b3c122b2757bfcee997e5b05f90/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f4c616d622e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Lamb.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2721153e8983a3d11cf12bb7bfdc0d9c87e3f2a4/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f4c616d622e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Lamb</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span> <span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">),</span>\n    <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-8</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</em> (2019) [<a href=\"https://arxiv.org/abs/1904.00962\" rel=\"nofollow\">https://arxiv.org/abs/1904.00962</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/cybertronai/pytorch-lamb\" rel=\"nofollow\">https://github.com/cybertronai/pytorch-lamb</a></p>\n</div>\n<div id=\"novograd\">\n<h3>NovoGrad</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_NovoGrad.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/227c39b0dd4becd9be45b9ebec6889f06207025c/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f4e6f766f477261642e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_NovoGrad.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fec140222394facdfb20ca5c400c0872fadb3e23/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f4e6f766f477261642e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">NovoGrad</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span> <span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">),</span>\n    <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-8</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">grad_averaging</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"n\">amsgrad</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks</em> (2019) [<a href=\"https://arxiv.org/abs/1905.11286\" rel=\"nofollow\">https://arxiv.org/abs/1905.11286</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/NVIDIA/DeepLearningExamples/\" rel=\"nofollow\">https://github.com/NVIDIA/DeepLearningExamples/</a></p>\n</div>\n<div id=\"pid\">\n<h3>PID</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_PID.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/44d50cea767fdd155017a182a17f88e961a92f9f/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f5049442e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_PID.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/06165d2cec95c3db3d0b1d710a35b1c18930f107/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f5049442e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">PID</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">dampening</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span>\n    <span class=\"n\">integral</span><span class=\"o\">=</span><span class=\"mf\">5.0</span><span class=\"p\">,</span>\n    <span class=\"n\">derivative</span><span class=\"o\">=</span><span class=\"mf\">10.0</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>A PID Controller Approach for Stochastic Optimization of Deep Networks</em> (2018) [<a href=\"http://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR18_PID.pdf\" rel=\"nofollow\">http://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR18_PID.pdf</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/tensorboy/PIDOptimizer\" rel=\"nofollow\">https://github.com/tensorboy/PIDOptimizer</a></p>\n</div>\n<div id=\"qhadam\">\n<h3>QHAdam</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_QHAdam.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/da24824fe24ad876e8c6002ecb19e4693672f4c0/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f51484164616d2e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_QHAdam.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/289c62858abeed0e70a4682550662a5d79f3f823/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f51484164616d2e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">QHAdam</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span> <span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">),</span>\n    <span class=\"n\">nus</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">),</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">decouple_weight_decay</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-8</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>Quasi-hyperbolic momentum and Adam for deep learning</em> (2019) [<a href=\"https://arxiv.org/abs/1810.06801\" rel=\"nofollow\">https://arxiv.org/abs/1810.06801</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/facebookresearch/qhoptim\" rel=\"nofollow\">https://github.com/facebookresearch/qhoptim</a></p>\n</div>\n<div id=\"qhm\">\n<h3>QHM</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_QHM.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b7e8efcfb773810fea3355a00d8b667343865d22/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f51484d2e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_QHM.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/565cb025f30dadc663c1a2f16f5fc6785ff3192e/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f51484d2e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">QHM</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">nu</span><span class=\"o\">=</span><span class=\"mf\">0.7</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay_type</span><span class=\"o\">=</span><span class=\"s1\">'grad'</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>Quasi-hyperbolic momentum and Adam for deep learning</em> (2019) [<a href=\"https://arxiv.org/abs/1810.06801\" rel=\"nofollow\">https://arxiv.org/abs/1810.06801</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/facebookresearch/qhoptim\" rel=\"nofollow\">https://github.com/facebookresearch/qhoptim</a></p>\n</div>\n<div id=\"radam\">\n<h3>RAdam</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_RAdam.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/42c87e7c409369f1b0136b346bae92291f97c592/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f524164616d2e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_RAdam.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7eb43b1a0ecad08510da90a71ff194fa7cdd9dcc/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f524164616d2e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">RAdam</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span> <span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">),</span>\n    <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-8</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>On the Variance of the Adaptive Learning Rate and Beyond</em> (2019) [<a href=\"https://arxiv.org/abs/1908.03265\" rel=\"nofollow\">https://arxiv.org/abs/1908.03265</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/LiyuanLucasLiu/RAdam\" rel=\"nofollow\">https://github.com/LiyuanLucasLiu/RAdam</a></p>\n</div>\n<div id=\"ranger\">\n<h3>Ranger</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Ranger.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e1f2e2a368a098ff6b40f76f833ab3d59ff73f8f/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f52616e6765722e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Ranger.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bd8d653dca66cd352089393481db3f888ac69757/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f52616e6765722e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Ranger</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span>\n    <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">,</span>\n    <span class=\"n\">N_sma_threshhold</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"o\">.</span><span class=\"mi\">95</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">),</span>\n    <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-5</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mi\">0</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM</em> (2019) [<a href=\"https://arxiv.org/abs/1908.00700v2\" rel=\"nofollow\">https://arxiv.org/abs/1908.00700v2</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\" rel=\"nofollow\">https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer</a></p>\n</div>\n<div id=\"rangerqh\">\n<h3>RangerQH</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_RangerQH.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/608a67858adf77e123527b7347649e226b013dae/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f52616e67657251482e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_RangerQH.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0e1e2cf81eb5cf86863c3afa4754400b14cd63e4/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f52616e67657251482e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">RangerQH</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">),</span>\n    <span class=\"n\">nus</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"o\">.</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">),</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">,</span>\n    <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">,</span>\n    <span class=\"n\">alpha</span><span class=\"o\">=.</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"n\">decouple_weight_decay</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n    <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-8</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM</em> (2019) [<a href=\"https://arxiv.org/abs/1908.00700v2\" rel=\"nofollow\">https://arxiv.org/abs/1908.00700v2</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\" rel=\"nofollow\">https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer</a></p>\n</div>\n<div id=\"rangerva\">\n<h3>RangerVA</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_RangerVA.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/43c990f3c2d55707f46502d7f0f25a9567096bd6/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f52616e67657256412e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_RangerVA.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e1456e7895708ea8985f6dd94fc0a8119c3d0139/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f52616e67657256412e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">RangerVA</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span>\n    <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">6</span><span class=\"p\">,</span>\n    <span class=\"n\">n_sma_threshhold</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"o\">.</span><span class=\"mi\">95</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">),</span>\n    <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-5</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">amsgrad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">transformer</span><span class=\"o\">=</span><span class=\"s1\">'softplus'</span><span class=\"p\">,</span>\n    <span class=\"n\">smooth</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">,</span>\n    <span class=\"n\">grad_transformer</span><span class=\"o\">=</span><span class=\"s1\">'square'</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM</em> (2019) [<a href=\"https://arxiv.org/abs/1908.00700v2\" rel=\"nofollow\">https://arxiv.org/abs/1908.00700v2</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\" rel=\"nofollow\">https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer</a></p>\n</div>\n<div id=\"sgdw\">\n<h3>SGDW</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_SGDW.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/03e788ebd87b9ea36df8e3f723133b70ea0821a0/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f534744572e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_SGDW.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2f4a8cc56a7c0bb22c327639651c2212e85ce047/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f534744572e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">SGDW</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span> <span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">dampening</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span>\n    <span class=\"n\">nesterov</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>SGDR: Stochastic Gradient Descent with Warm Restarts</em> (2017) [<a href=\"https://arxiv.org/abs/1608.03983\" rel=\"nofollow\">https://arxiv.org/abs/1608.03983</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/pytorch/pytorch/pull/22466\" rel=\"nofollow\">https://github.com/pytorch/pytorch/pull/22466</a></p>\n</div>\n<div id=\"yogi\">\n<h3>Yogi</h3>\n<p>Yogi is optimization algorithm based on ADAM with more fine grained effective\nlearning rate control, and has similar theoretical guarantees on convergence as ADAM.</p>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Yogi.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6e2a485565b4af0d09adab81f402a0e3b1d28ef2/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f596f67692e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Yogi.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/18f8a75fcb2de9f4b280395319073d2289c30d6c/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f596f67692e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch_optimizer</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n\n<span class=\"c1\"># model = ...</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Yogi</span><span class=\"p\">(</span>\n    <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span>\n    <span class=\"n\">lr</span><span class=\"o\">=</span> <span class=\"mf\">1e-2</span><span class=\"p\">,</span>\n    <span class=\"n\">betas</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.999</span><span class=\"p\">),</span>\n    <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span>\n    <span class=\"n\">initial_accumulator</span><span class=\"o\">=</span><span class=\"mf\">1e-6</span><span class=\"p\">,</span>\n    <span class=\"n\">weight_decay</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</pre>\n<p><strong>Paper</strong>: <em>Adaptive Methods for Nonconvex Optimization</em> (2018) [<a href=\"https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization\" rel=\"nofollow\">https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization</a>]</p>\n<p><strong>Reference Code</strong>: <a href=\"https://github.com/4rtemi5/Yogi-Optimizer_Keras\" rel=\"nofollow\">https://github.com/4rtemi5/Yogi-Optimizer_Keras</a></p>\n</div>\n<div id=\"adam-pytorch-built-in\">\n<h3>Adam (PyTorch built-in)</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Adam.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5531b59124635fcdf2ce4f2d25700f8d5f4f6674/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f4164616d2e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Adam.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9f670ed6f6134402a91048069f90d2a02590d642/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f4164616d2e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div id=\"sgd-pytorch-built-in\">\n<h3>SGD (PyTorch built-in)</h3>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<tbody>\n<tr><td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_SGD.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ada02088061325b74e346d69e0beed56d7d27340/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f72617374726967696e5f5347442e706e67\">\n</td>\n<td><img alt=\"https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_SGD.png\" class=\"first last\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0c7f016ebe4a622ec40724e23706a725ee276f97/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a6574746966792f7079746f7263682d6f7074696d697a65722f6d61737465722f646f63732f726f73656e62726f636b5f5347442e706e67\">\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div id=\"changes\">\n<h3>Changes</h3>\n</div>\n<div id=\"yyyy-mm-dd\">\n<h3>0.0.1 (YYYY-MM-DD)</h3>\n<ul>\n<li>Initial release.</li>\n</ul>\n</div>\n</div>\n\n          </div>"}, "last_serial": 7106180, "releases": {"0.0.1a0": [{"comment_text": "", "digests": {"md5": "3d338a31dfac5f77d730883a37a99a68", "sha256": "0d1778111fa9e1a54698ec0cd62c6689a7ae91d3688e50e91cb56865d1885996"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a0.tar.gz", "has_sig": false, "md5_digest": "3d338a31dfac5f77d730883a37a99a68", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10119, "upload_time": "2020-01-05T01:36:30", "upload_time_iso_8601": "2020-01-05T01:36:30.307831Z", "url": "https://files.pythonhosted.org/packages/eb/69/a72a7f4560258b1dfe32fa813202e06fe2e9311df4b878680848f129e609/torch-optimizer-0.0.1a0.tar.gz", "yanked": false}], "0.0.1a1": [{"comment_text": "", "digests": {"md5": "f88cf06a8068397f41785780c3ebdfec", "sha256": "ffab7bce904e618171a88fdb7614d9abf5e08db54df4a6cccc7960f6ecd49959"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a1.tar.gz", "has_sig": false, "md5_digest": "f88cf06a8068397f41785780c3ebdfec", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16812, "upload_time": "2020-01-22T01:59:44", "upload_time_iso_8601": "2020-01-22T01:59:44.336241Z", "url": "https://files.pythonhosted.org/packages/73/e4/d490cd10deb415c4cb232690a0e4668ed53a399cb42c6bb57b12f0d5ce2c/torch-optimizer-0.0.1a1.tar.gz", "yanked": false}], "0.0.1a10": [{"comment_text": "", "digests": {"md5": "f9feffd605ea065fef4a5a1beff04427", "sha256": "23e4800ca1e54fee5cd439b88e741eaf829bd52539b36f40ef4bf8c45b9841ee"}, "downloads": -1, "filename": "torch_optimizer-0.0.1a10-py3-none-any.whl", "has_sig": false, "md5_digest": "f9feffd605ea065fef4a5a1beff04427", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 30056, "upload_time": "2020-03-15T21:25:58", "upload_time_iso_8601": "2020-03-15T21:25:58.652483Z", "url": "https://files.pythonhosted.org/packages/d7/c8/62c4c2c350b0993dc88eef34ba5548983f4f6367f2e441727a0f60ab91cf/torch_optimizer-0.0.1a10-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "29916f14229aeb51be08ea24f80ee36f", "sha256": "ae4a73870fcee05a60901e51a04bb58a1495be9ca792eee888bd73e943fd0e79"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a10.tar.gz", "has_sig": false, "md5_digest": "29916f14229aeb51be08ea24f80ee36f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 26948, "upload_time": "2020-03-15T21:26:00", "upload_time_iso_8601": "2020-03-15T21:26:00.644078Z", "url": "https://files.pythonhosted.org/packages/91/70/24efab24e5f44e279cf9c3ffe0cbd65c2fc6d5dd7b0b5d72f9378f12b190/torch-optimizer-0.0.1a10.tar.gz", "yanked": false}], "0.0.1a11": [{"comment_text": "", "digests": {"md5": "fd9f491b7f7cea7822468eec386d3253", "sha256": "14b767da66327eb188b5b08ecb3de6b5a2f84c43e0bf57c9f050df89782b7fa0"}, "downloads": -1, "filename": "torch_optimizer-0.0.1a11-py3-none-any.whl", "has_sig": false, "md5_digest": "fd9f491b7f7cea7822468eec386d3253", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 32961, "upload_time": "2020-04-05T18:47:18", "upload_time_iso_8601": "2020-04-05T18:47:18.801321Z", "url": "https://files.pythonhosted.org/packages/38/8f/8c9fdfb199a8f7e06a16c2315d076aa1505057f2496b7a7f2c73ece66215/torch_optimizer-0.0.1a11-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "ed0ee74f32db5ff6f2c466f65715b4d5", "sha256": "1f7d37f795a997634151807fa615785046621668b3410924da7ea342c36f93bc"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a11.tar.gz", "has_sig": false, "md5_digest": "ed0ee74f32db5ff6f2c466f65715b4d5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29470, "upload_time": "2020-04-05T18:47:20", "upload_time_iso_8601": "2020-04-05T18:47:20.181406Z", "url": "https://files.pythonhosted.org/packages/6b/79/b4b0bb8eac2c192e93a04c9a5313354e22c38d93d4bb434970006e257c27/torch-optimizer-0.0.1a11.tar.gz", "yanked": false}], "0.0.1a12": [{"comment_text": "", "digests": {"md5": "6c27a9dce0197e399dd26a46036d7cd5", "sha256": "3c8db4263407ccf8413e5a14e4dcb0f912c0a06081b9fa32c9e607390fee2169"}, "downloads": -1, "filename": "torch_optimizer-0.0.1a12-py3-none-any.whl", "has_sig": false, "md5_digest": "6c27a9dce0197e399dd26a46036d7cd5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 33121, "upload_time": "2020-04-26T18:11:17", "upload_time_iso_8601": "2020-04-26T18:11:17.355735Z", "url": "https://files.pythonhosted.org/packages/33/d3/4ff0ce01ccbedf3a3c86e7882f428097a37a9b9a55eca73548e657da5518/torch_optimizer-0.0.1a12-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3bdfc896f8f2d6694e89ce2f42b85f2e", "sha256": "5012c0162f0f45c74c9e7cd4c5be0a5b480b8d6fe54787117b1e9e1bee2e2196"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a12.tar.gz", "has_sig": false, "md5_digest": "3bdfc896f8f2d6694e89ce2f42b85f2e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29579, "upload_time": "2020-04-26T18:11:19", "upload_time_iso_8601": "2020-04-26T18:11:19.075456Z", "url": "https://files.pythonhosted.org/packages/94/8c/7bd8fa05b00e9b1de63841cab0fb071366811422afa014a7487c343569c0/torch-optimizer-0.0.1a12.tar.gz", "yanked": false}], "0.0.1a2": [{"comment_text": "", "digests": {"md5": "1d652568f8f5c840d08297bd88a4d106", "sha256": "bd164de32045e1aaa50e1f0d69c42521b29730c92abd16b10d46fd5f7ed8af5d"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a2.tar.gz", "has_sig": false, "md5_digest": "1d652568f8f5c840d08297bd88a4d106", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 18126, "upload_time": "2020-02-03T02:00:48", "upload_time_iso_8601": "2020-02-03T02:00:48.904587Z", "url": "https://files.pythonhosted.org/packages/2d/78/d5237b1ed914768135e70f014b4c2a01b16176048d1fe1ae57db4701eaf7/torch-optimizer-0.0.1a2.tar.gz", "yanked": false}], "0.0.1a3": [{"comment_text": "", "digests": {"md5": "c4cfffcabca14eb608414457f8970bdb", "sha256": "1289d3295dcbb61cb9ff030bfd89cb0eeae518cbaf9c0139173805c786a91039"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a3.tar.gz", "has_sig": false, "md5_digest": "c4cfffcabca14eb608414457f8970bdb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19669, "upload_time": "2020-02-09T03:42:57", "upload_time_iso_8601": "2020-02-09T03:42:57.281686Z", "url": "https://files.pythonhosted.org/packages/51/c4/2e3535a133b6041c9d5b2acc197ed83de91112844b727057797034c0c428/torch-optimizer-0.0.1a3.tar.gz", "yanked": false}], "0.0.1a4": [{"comment_text": "", "digests": {"md5": "66b76b26fbb5835ba95da9f36e477921", "sha256": "839601e18e95383b5c277474f9a96349875064df00de79b50f00e6b8ba327f18"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a4.tar.gz", "has_sig": false, "md5_digest": "66b76b26fbb5835ba95da9f36e477921", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19475, "upload_time": "2020-02-11T03:05:35", "upload_time_iso_8601": "2020-02-11T03:05:35.683279Z", "url": "https://files.pythonhosted.org/packages/63/b5/23ad42b267c4c834cde6e8dd9b02f23f72f2d88b6c5952d97c585e346b33/torch-optimizer-0.0.1a4.tar.gz", "yanked": false}], "0.0.1a5": [{"comment_text": "", "digests": {"md5": "b026a52fb17462585c699bd3e99f9640", "sha256": "9b723b751cdd8849dc12ab486d20534c9a76dc5a57117d6c56955943e291af20"}, "downloads": -1, "filename": "torch_optimizer-0.0.1a5-py3-none-any.whl", "has_sig": false, "md5_digest": "b026a52fb17462585c699bd3e99f9640", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 21904, "upload_time": "2020-02-15T23:34:44", "upload_time_iso_8601": "2020-02-15T23:34:44.583190Z", "url": "https://files.pythonhosted.org/packages/a6/22/45f1ebab7fd773370d9758fcb85bded3e3e4bd10159b948bed00d3bc4e35/torch_optimizer-0.0.1a5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "822ada9b126a35265d898dd7be763349", "sha256": "84cdc5150764c799dbf2a2b0558950c50871a85b93e7577e9efbc544d16b565b"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a5.tar.gz", "has_sig": false, "md5_digest": "822ada9b126a35265d898dd7be763349", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20816, "upload_time": "2020-02-15T23:34:46", "upload_time_iso_8601": "2020-02-15T23:34:46.295851Z", "url": "https://files.pythonhosted.org/packages/41/bd/64e4363c65c1b55d5e9c67d5225f6b26e171fa9d4625d3ed7c25385e3d50/torch-optimizer-0.0.1a5.tar.gz", "yanked": false}], "0.0.1a6": [{"comment_text": "", "digests": {"md5": "669088b858e19a6e91e523848c05421b", "sha256": "9afdd966c7177699c2937013949fe5f08f3efb65bd08d59a3fdc20e7f041f891"}, "downloads": -1, "filename": "torch_optimizer-0.0.1a6-py3-none-any.whl", "has_sig": false, "md5_digest": "669088b858e19a6e91e523848c05421b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23963, "upload_time": "2020-02-22T03:50:18", "upload_time_iso_8601": "2020-02-22T03:50:18.337031Z", "url": "https://files.pythonhosted.org/packages/5f/82/168a55e13ed8098c4dce4507a53bd20031a1ef8be261ac793758902e19bd/torch_optimizer-0.0.1a6-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c430e6ff0faa2f95b6edcbef888ec686", "sha256": "2b3a6bc1947983977c84001714291de43a9203648f54124275d72b78071c8671"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a6.tar.gz", "has_sig": false, "md5_digest": "c430e6ff0faa2f95b6edcbef888ec686", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 21653, "upload_time": "2020-02-22T03:50:19", "upload_time_iso_8601": "2020-02-22T03:50:19.718910Z", "url": "https://files.pythonhosted.org/packages/1a/31/d630f2c051324c5088021333541ac9095f42d13a81a0fbfb0eb07762a44e/torch-optimizer-0.0.1a6.tar.gz", "yanked": false}], "0.0.1a7": [{"comment_text": "", "digests": {"md5": "8490bfcbc700a2b504f7ecefccaed5ce", "sha256": "7a14470fb7ffd780df48b42ad3bd3e3743c506febc861dd6054deddf9434c046"}, "downloads": -1, "filename": "torch_optimizer-0.0.1a7-py3-none-any.whl", "has_sig": false, "md5_digest": "8490bfcbc700a2b504f7ecefccaed5ce", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 25636, "upload_time": "2020-02-27T00:33:47", "upload_time_iso_8601": "2020-02-27T00:33:47.692423Z", "url": "https://files.pythonhosted.org/packages/54/cc/7c390224710db1c4a3a32d8c42ae2bf69d826c81b566deeca7cec8cf69b6/torch_optimizer-0.0.1a7-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "708645bdd21958b4ea21c43165fe11b2", "sha256": "e08998303d385efb74a4f16fe02308a2f3d9ebbf63be66056ef2d0074686f1db"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a7.tar.gz", "has_sig": false, "md5_digest": "708645bdd21958b4ea21c43165fe11b2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22908, "upload_time": "2020-02-27T00:33:49", "upload_time_iso_8601": "2020-02-27T00:33:49.486410Z", "url": "https://files.pythonhosted.org/packages/97/ee/e39de19ddc9f9747e3165fc17c765c76c3354db5dcf9394743b8b48e3423/torch-optimizer-0.0.1a7.tar.gz", "yanked": false}], "0.0.1a8": [{"comment_text": "", "digests": {"md5": "49d52957238733912b072227b69e2900", "sha256": "6fa5bf2c08b16c1e55391417f29bf3d620d8d93a285d1ca019ac821168099096"}, "downloads": -1, "filename": "torch_optimizer-0.0.1a8-py3-none-any.whl", "has_sig": false, "md5_digest": "49d52957238733912b072227b69e2900", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 26168, "upload_time": "2020-03-02T02:10:41", "upload_time_iso_8601": "2020-03-02T02:10:41.219476Z", "url": "https://files.pythonhosted.org/packages/d7/84/822966979c1fcc95272f46f62b20d09bc81295c17ef6d456ded5b8baeccd/torch_optimizer-0.0.1a8-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "788cf4d715b49b212f4153a1f821ca02", "sha256": "429e006a90a97627f57eb4625dd39ab137b737e16297ffde7adef29bfb4750d8"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a8.tar.gz", "has_sig": false, "md5_digest": "788cf4d715b49b212f4153a1f821ca02", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 24334, "upload_time": "2020-03-02T02:10:42", "upload_time_iso_8601": "2020-03-02T02:10:42.880115Z", "url": "https://files.pythonhosted.org/packages/1d/ee/0cec414f5a4bba1a8a2deb1acc14c89dd47d19a3b0ca4a80bdc16ca2e404/torch-optimizer-0.0.1a8.tar.gz", "yanked": false}], "0.0.1a9": [{"comment_text": "", "digests": {"md5": "85263c6b0bbf64934deca7180c1807c0", "sha256": "ca1f1e2625dc0ffacd18c0b891af7b9440391bcda00051d7f0b40d8a525e34ed"}, "downloads": -1, "filename": "torch_optimizer-0.0.1a9-py3-none-any.whl", "has_sig": false, "md5_digest": "85263c6b0bbf64934deca7180c1807c0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 27800, "upload_time": "2020-03-04T03:02:52", "upload_time_iso_8601": "2020-03-04T03:02:52.562636Z", "url": "https://files.pythonhosted.org/packages/74/e3/23bec2c68a74820505d15c43c4b858ba7d8845fe101cef04b16334484633/torch_optimizer-0.0.1a9-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d2e130565201f0326dae96050aadb125", "sha256": "d036edab9d76f2b983986b97ab6287b50e8209a1f0b95f1e1021f77396269e78"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a9.tar.gz", "has_sig": false, "md5_digest": "d2e130565201f0326dae96050aadb125", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25234, "upload_time": "2020-03-04T03:02:54", "upload_time_iso_8601": "2020-03-04T03:02:54.209417Z", "url": "https://files.pythonhosted.org/packages/30/7b/fd6721d6c38b9dd4bf2636e023d9177ee73d29e77685f997e9f415def4c9/torch-optimizer-0.0.1a9.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "6c27a9dce0197e399dd26a46036d7cd5", "sha256": "3c8db4263407ccf8413e5a14e4dcb0f912c0a06081b9fa32c9e607390fee2169"}, "downloads": -1, "filename": "torch_optimizer-0.0.1a12-py3-none-any.whl", "has_sig": false, "md5_digest": "6c27a9dce0197e399dd26a46036d7cd5", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 33121, "upload_time": "2020-04-26T18:11:17", "upload_time_iso_8601": "2020-04-26T18:11:17.355735Z", "url": "https://files.pythonhosted.org/packages/33/d3/4ff0ce01ccbedf3a3c86e7882f428097a37a9b9a55eca73548e657da5518/torch_optimizer-0.0.1a12-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3bdfc896f8f2d6694e89ce2f42b85f2e", "sha256": "5012c0162f0f45c74c9e7cd4c5be0a5b480b8d6fe54787117b1e9e1bee2e2196"}, "downloads": -1, "filename": "torch-optimizer-0.0.1a12.tar.gz", "has_sig": false, "md5_digest": "3bdfc896f8f2d6694e89ce2f42b85f2e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29579, "upload_time": "2020-04-26T18:11:19", "upload_time_iso_8601": "2020-04-26T18:11:19.075456Z", "url": "https://files.pythonhosted.org/packages/94/8c/7bd8fa05b00e9b1de63841cab0fb071366811422afa014a7487c343569c0/torch-optimizer-0.0.1a12.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:15 2020"}