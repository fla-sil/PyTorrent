{"info": {"author": "Sebastian H\u00f6rl", "author_email": "hoerl.sebastian@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Environment :: Console", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering"], "description": "# Synthetic Population Pipeline (synpp)\n\n[![Build Status](https://travis-ci.org/eqasim-org/synpp.svg?branch=develop)](https://travis-ci.org/eqasim-org/synpp)\n\nThe *synpp* module is a tool to chain different stages of a (population)\nsynthesis pipeline. This means that self-contained pieces of code can be\nrun, which are dependent on the outputs of other self-contained pieces\nof code. Those pieces, or steps, are called *stages* in this module.\n\nThe following will describe the components of the pipeline and how it can\nbe set up and configured. Scroll to the bottom to find a full example of such\na pipeline which automatically downloads NYC taxi data sets, merges them together\nand calculates the average vehicle occupancy during a predefined period.\n\n## Installation\n\nThe `synpp` package releases can be installed via `pip`:\n\n```sh\npip install synpp\n```\n\nCurrently, version `1.2.1` is the active release version. Alternatively, you can\nclone the `develop` branch of this repository to use the development version `1.2.1-dev`.\nIt can be installed by calling\n\n```\npip install .\n```\n\ninside of the repository directoy.\n\n## Concepts\n\nA typical chain of stages could, for instance, be: **(C1)** load raw census data,\n**(C2)** clean raw census data *(dependent on C1)*, **(H1)** load raw household travel survey data,\n**(H2)** clean survey data *(dependent on C2)*, **(P1)** merge census *(C1)* and survey *(H2)* data,\n**(P2)** generate a synthetic population from merged data *(P1)*.\n\nIn *synpp* each *stage* is defined by:\n\n* A *descriptor*, which can be a Python module, a class or a class instance, or a string referencing a module or class.\n* *Configuration options* that parameterize each *stage*.\n\nThe most common form of a *stage* is a Python module. A full stage would look\nlike this:\n\n```python\ndef configure(context):\n  pass\n\ndef execute(context):\n  pass\n\ndef validate(context):\n  pass\n```\n\n### Configuration and Parameterization\n\nWhenever the pipeline explores a stage, *configure* is called first. Note that\nin the example above we use a Python module, but the same procedure would work\nanalogously with a class. In *configure* one can the pipeline what the *stage*\nexpects in terms of other input *stages* and in terms of\n*configuration options*:\n\n```python\ndef configure(context):\n  # Expect an output directory\n  value = context.config(\"output_path\")\n\n  # Expect a random seed\n  value = context.config(\"random_seed\")\n\n  # Expect a certain stage (no return value)\n  context.stage(\"my.pipeline.raw_data\")\n```\n\nWe could add this stage (let's call it `my.pipeline.raw_data`)\nas a dependency to another one. However, as we did not define a default\nvalue with the `config` method, we need to explicitly set one, like so:\n\n```python\ndef configure(context):\n  context.stage(\"my.pipeline.raw_data\", { \"random_seed\": 1234 })\n```\n\nNote that it is even possible to build recursive chains of stages using only\none stage definition:\n\n```python\ndef configure(context):\n  i = context.config(\"i\")\n\n  if i > 0:\n    context.stage(\"this.stage\", { \"i\": i - 1 })\n```\n\nConfiguration options can also be defined globally in the pipeline. In case\nno default value is given for an option in `configure` and in case that no\nspecific value is passed to the stage, a global configuration that is specific\nto the pipeline will be used to look up the value.\n\n### Execution\n\nThe requested configuration values and stages are afterwards available\nto the `execute` step of a *stage*. There those values can be used to do the\n\"heavy work\" of the stage. As the `configure` step already defined what kind\nof values to expect, we can be sure that those values and dependencies are\npresent once `execute` is called.\n\n```python\ndef execute(context):\n  # Load some data from another stage\n  df = context.stage(\"my.pipeline.census.raw\")\n\n  df = df.dropna()\n  df[\"age\"] = df[\"age\"].astype(int)\n\n  # We could access some values if we wanted\n  value = context.config(\"...\")\n\n  return df\n```\n\nNote that the `execute` step returns a value. This value will be *pickled* (see\n*pickle* package of Python) and cached on the hard drive. This means that whenever\nthe output of this stage is requested by another stage, it doesn't need to be\nrun again. The pipeline can simply load the cached result from hard drive.\n\nIf one has a very complex pipeline with many stages this means that changes in\none stage will likely not lead to a situation where one needs to re-run the\nwhole pipeline, but only a fraction. The *synpp* framework has intelligent\nexplorations algorithms included which figure out automatically, which\n*stages* need to be re-run.\n\n### Running a pipeline\n\nA pipeline can be started using the `synpp.run` method. A typical run would\nlook like this:\n\n```python\nconfig = { \"random_seed\": 1234 }\nworking_directory = \"~/pipeline/cache\"\n\nsynpp.run([\n    { \"descriptor\": \"my.pipeline.final_population\" },\n    { \"descriptor\": \"my.pipeline.paper_analysis\", \"config\": { \"font_size\": 12 } }\n], config = config, working_directory = working_directory)\n```\n\nHere we call the *stage* defined by the module `my.pipeline.final_population`\nwhich should be available in the Python path. And we also want to run the\n`my.pipeline.paper_analysis` path with a font size parameter of `12`. Note that\nin both cases we could also have based the bare Python module objects instead\nof strings.\n\nThe pipeline will now figure out how to run those *stages*. Probably they have\ndependencies and the analysis *stage* may even depend on the other one. Therefore,\n*synpp* explores the tree of dependencies as follows:\n\n* Consider the requested stages (two in this case)\n* Step by step, go through the dependencies of those stages\n* Then again, go through the dependencies of all added stages, and so on\n\nBy that the pipeline traverses the whole tree of dependencies as they are defined\nby the `configure` steps of all stages. At the same time it collects information\nabout which configuration options and parameters are required by each stage. Note\nthat a stage can occur twice in this dependency tree if it has different\nparameters.\n\nAfter constructing a tree of *stages*, *synpp* devalidates some of them according\nto the following scheme. A *stage* is devalidated if ...\n\n- ... it is requested by the `run` call\n- ... it is new (no meta data from a previous call is present)\n- ... if at least one of the requested configuration options has changed\n- ... if at least one dependency has been re-run since the last run of the stage\n- ... if list of dependencies has changed\n- ... if manual *validation* of the stage has failed (see below)\n- ... if any ascendant of a stage has been devalidated\n\nThis list of conditions makes sure that in almost any case of pipeline\nmodification we end up in a consistent situation (though we cannot prove it).\nThe only measure that may be important to enforce 'by convention' is to\n*always run a stage after the code has been modified*. Though even this can\nbe automated.\n\n### Validation\n\nEach *stage* has an additional `validate` step, which also receives the\nconfiguration options and the parameters. Its purpose is to return a hash\nvalue that represents the environment of the *stage*. To learn about the concept\nin general, search for \"md5 hash\", for instance. The idea is the following:\nAfter the `execute` step, the `validate` step is called and\n it will return a certain value. Next time the pipeline\nis resolved the `validate` step is called during devalidation, i.e. before\nthe stage is actually *executed*. If the return value of `validate` now differs\nfrom what it was before, the stage will be devalidated.\n\nThis is useful to check the integrity of data that is not generated inside of\nthe pipeline but comes from the outside, for instance:\n\n```python\ndef configure(context):\n  context.config(\"input_path\")\n\ndef validate(context):\n  path = context.config(\"input_path\")\n  filesize = get_filesize(path)\n\n  # If the file size has changed, the file must have changed,\n  # hence we want to run the stage again.\n  return filesize\n\ndef execute(context):\n  pass # Do something with the file\n```\n\n### Cache paths\n\nSometimes, results of a *stage* are not easily representable in Python. Even\nmore, stages may call Java or Shell scripts which simply generate an output\nfile. For these cases each stage has its own *cache path*. It can be accessed\nthrough the stage context:\n\n```python\ndef execute(context):\n  # In this case we write a file to the cache path of the current stage\n  with open(\"%s/myfile.txt\" % context.path()) as f:\n    f.write(\"my content\")\n\n  # In this case we read a file from the cache path of another stage\n  with open(\"%s/otherfile.txt\" % context.path(\"my.other.stage\")) as f:\n    value = f.read()\n```\n\nAs the example shows, we can also access cache paths of other stages. The pipeline\nwill make sure that you only have access to the cache path of stages that\nhave been defined as dependencies before. Note that the pipeline cannot enforce\nthat one stage is not corrupting the cache path of another stage. Therefore,\nby convention, a stage should never *write* to the cache path of another stage.\n\n### Parallel execution\n\nThe *synpp* package comes with some simplified ways of parallelizing code,\nwhich are built on top of the `multiprocessing` package. To set up a parallel\nroutine, one can follow the following pattern:\n\n```python\ndef run_parallel(context, x):\n  return x**2 + context.data(\"y\")\n\ndef execute(context):\n  data = { \"y\": 5 }\n\n  with context.parallel(data) as parallel:\n    result = parallel.map(run_parallel, [1, 2, 3, 4, 5])\n```\n\nThis approach looks similar to the `Pool` object of `multiprocessing` but has\nsome simplifications. First, the first argument of the parallel routine is a\ncontext object, which provides configuration and parameters. Furthermore, it\nprovides data, which has been passed before in the `execute` function. This\nsimplifies passing data to all parallel threads considerably to the more\nflexible approach in `multiprocessing`. Otherwise, the `parallel` object\nprovides most of the functionality of `Pool`, like, `map`, `async_map`,\n`imap`, and `unordered_imap`.\n\n### Info\n\nWhile running the pipeline a lot of additional information may be interesting,\nlike how many samples of a data set have been discarded in a certain stage. However,\nthey often would only be used at the very end of the pipeline when maybe a paper,\na report or some explanatory graphics are generated. For that, the pipeline\nprovides the `set_info` method:\n\n```python\ndef execute(context):\n  # ...\n  context.set_info(\"dropped_samples\", number_of_dropped_samples)\n  # ...\n```\n\nThe information can later be retrieved from another stage (which has the\nstage in question as a dependency):\n\n```python\ndef execute(context):\n  # ...\n  value = context.get_info(\"my.other.stage\", \"dropped_samples\")\n  # ...\n```\n\nNote that the *info* functionality should only be used for light-weight\ninformation like integers, short strings, etc.\n\n### Progress\n\nThe *synpp* package provides functionality to show the progress of a stage\nsimilar to `tqdm`. However, `tqdm` tends to spam the console output which is\nespecially undesired if pipelines have long runtimes and run, for instance, in\nContinuous Integration environments. Therefore, *synpp* provides its own\nfunctionality, although `tqdm` could still be used:\n\n```python\ndef execute(context):\n  # As a\n  with context.progress(label = \"My progress...\", total = 100) as progress:\n    i = 0\n\n    while i < 100:\n      progress.update()\n      i += 1\n\n  for i in context.progress(range(100)):\n    pass\n```\n\n### Command-line tool\n\nThe `synpp` pipeline comes with a command line tool, which can be called like\n\n```sh\npython3 -m synpp [config_path]\n```\n\nIf not config path is given, it will assume `config.yml`. This file should\ncontain everything to run a pipeline. A simple version would look like this:\n\n```yaml\n# General pipeline settings\nworking_directory: /path/to/my/working_directory\n\n# Requested stages\nrun:\n  - my_first_module.my_first_stage\n  - my_first_parameterized_stage:\n    param1: 123\n    param2: 345\n\n# These are configuration options that are used in the pipeline\nconfig:\n  my_option: 123\n```\n\nIt receives the working directory, a list of stages (which may be parameterized)\nand all configuration options. The stages listed above should be available\nas Python modules or classes.\n\n## NYC Taxi Example\n\nThis repository contains an example of the pipline. To run it, you will need\n`pandas` as an additional Python dependency. For testing, you can clone this\nrepository to any directory on your machine. Inside the repository directory\nyou can find the `example` directory. If you did not install `synpp` yet,\nyou can do this by executing\n\n```sh\npip install .\n```\n\ninside of the repository directory. Afterwards, open `examples/config.yml`\nand adjust the `working_directory` path. This is a path that should exist on\nyour machine and it should be empty. The best is if you simply create a new\nfolder and add the path in `config.yml`.\n\nYou can now go to `examples` and call the pipeline code:\n\n```sh\ncd examples\npython3 -m synpp\n```\n\nIt will automatically discover `config.yml` (but you could path a different\nconfig file path manually as a command line argument). It will then download\nthe NYC taxi data for January, February and March 2018 (see configuration\noptions in `config.yml`). Note that this is happening in one stage for which\nyou can find the code in `nyc_taxi.download`. It is parameterized by a month\nand a year to download the respective data set. These data sets then go into\n`nyc_taxi.aggregate`, where they are merged together. Finally, an average\noccupancy value is printed out in `nyc_taxi.print_occupancy`. So the dependency\nstructure is as follows:\n\n```\nnyc_taxi.aggregate depends on multiple nyc_taxi.download(year, month)\nnyc_taxi.print_occupancy depends on nyc_taxi.aggregate\n```\n\nAfter one successful run of the pipeline you can start it again. You will notice\nthat the pipeline does *not* download the data again, because nothing has changed\nfor those stages. However, if you would change the requested months in `config.yml`\nthe pipeline may download the additional data sets.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/eqasim-org/synpp", "keywords": "pipeline automation synthetic population dependency management transport", "license": "", "maintainer": "", "maintainer_email": "", "name": "synpp", "package_url": "https://pypi.org/project/synpp/", "platform": "", "project_url": "https://pypi.org/project/synpp/", "project_urls": {"Homepage": "https://github.com/eqasim-org/synpp"}, "release_url": "https://pypi.org/project/synpp/1.2.1/", "requires_dist": ["networkx (>=2.4)", "PyYAML (>=5.1.2)", "pyzmq (>=18.1.0)", "pandas (>=0.25.3) ; extra == 'example'", "pytest (>=5.3.1) ; extra == 'test'"], "requires_python": ">=3.0", "summary": "Synthetic population pipeline package for eqasim", "version": "1.2.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Synthetic Population Pipeline (synpp)</h1>\n<p><a href=\"https://travis-ci.org/eqasim-org/synpp\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/386f623b4b655ee69d5a4b8ac303c2c55deacc7f/68747470733a2f2f7472617669732d63692e6f72672f65716173696d2d6f72672f73796e70702e7376673f6272616e63683d646576656c6f70\"></a></p>\n<p>The <em>synpp</em> module is a tool to chain different stages of a (population)\nsynthesis pipeline. This means that self-contained pieces of code can be\nrun, which are dependent on the outputs of other self-contained pieces\nof code. Those pieces, or steps, are called <em>stages</em> in this module.</p>\n<p>The following will describe the components of the pipeline and how it can\nbe set up and configured. Scroll to the bottom to find a full example of such\na pipeline which automatically downloads NYC taxi data sets, merges them together\nand calculates the average vehicle occupancy during a predefined period.</p>\n<h2>Installation</h2>\n<p>The <code>synpp</code> package releases can be installed via <code>pip</code>:</p>\n<pre>pip install synpp\n</pre>\n<p>Currently, version <code>1.2.1</code> is the active release version. Alternatively, you can\nclone the <code>develop</code> branch of this repository to use the development version <code>1.2.1-dev</code>.\nIt can be installed by calling</p>\n<pre><code>pip install .\n</code></pre>\n<p>inside of the repository directoy.</p>\n<h2>Concepts</h2>\n<p>A typical chain of stages could, for instance, be: <strong>(C1)</strong> load raw census data,\n<strong>(C2)</strong> clean raw census data <em>(dependent on C1)</em>, <strong>(H1)</strong> load raw household travel survey data,\n<strong>(H2)</strong> clean survey data <em>(dependent on C2)</em>, <strong>(P1)</strong> merge census <em>(C1)</em> and survey <em>(H2)</em> data,\n<strong>(P2)</strong> generate a synthetic population from merged data <em>(P1)</em>.</p>\n<p>In <em>synpp</em> each <em>stage</em> is defined by:</p>\n<ul>\n<li>A <em>descriptor</em>, which can be a Python module, a class or a class instance, or a string referencing a module or class.</li>\n<li><em>Configuration options</em> that parameterize each <em>stage</em>.</li>\n</ul>\n<p>The most common form of a <em>stage</em> is a Python module. A full stage would look\nlike this:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">configure</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"k\">pass</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"k\">pass</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">validate</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"k\">pass</span>\n</pre>\n<h3>Configuration and Parameterization</h3>\n<p>Whenever the pipeline explores a stage, <em>configure</em> is called first. Note that\nin the example above we use a Python module, but the same procedure would work\nanalogously with a class. In <em>configure</em> one can the pipeline what the <em>stage</em>\nexpects in terms of other input <em>stages</em> and in terms of\n<em>configuration options</em>:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">configure</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"c1\"># Expect an output directory</span>\n  <span class=\"n\">value</span> <span class=\"o\">=</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"p\">(</span><span class=\"s2\">\"output_path\"</span><span class=\"p\">)</span>\n\n  <span class=\"c1\"># Expect a random seed</span>\n  <span class=\"n\">value</span> <span class=\"o\">=</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"p\">(</span><span class=\"s2\">\"random_seed\"</span><span class=\"p\">)</span>\n\n  <span class=\"c1\"># Expect a certain stage (no return value)</span>\n  <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">stage</span><span class=\"p\">(</span><span class=\"s2\">\"my.pipeline.raw_data\"</span><span class=\"p\">)</span>\n</pre>\n<p>We could add this stage (let's call it <code>my.pipeline.raw_data</code>)\nas a dependency to another one. However, as we did not define a default\nvalue with the <code>config</code> method, we need to explicitly set one, like so:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">configure</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">stage</span><span class=\"p\">(</span><span class=\"s2\">\"my.pipeline.raw_data\"</span><span class=\"p\">,</span> <span class=\"p\">{</span> <span class=\"s2\">\"random_seed\"</span><span class=\"p\">:</span> <span class=\"mi\">1234</span> <span class=\"p\">})</span>\n</pre>\n<p>Note that it is even possible to build recursive chains of stages using only\none stage definition:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">configure</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"n\">i</span> <span class=\"o\">=</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"p\">(</span><span class=\"s2\">\"i\"</span><span class=\"p\">)</span>\n\n  <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n    <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">stage</span><span class=\"p\">(</span><span class=\"s2\">\"this.stage\"</span><span class=\"p\">,</span> <span class=\"p\">{</span> <span class=\"s2\">\"i\"</span><span class=\"p\">:</span> <span class=\"n\">i</span> <span class=\"o\">-</span> <span class=\"mi\">1</span> <span class=\"p\">})</span>\n</pre>\n<p>Configuration options can also be defined globally in the pipeline. In case\nno default value is given for an option in <code>configure</code> and in case that no\nspecific value is passed to the stage, a global configuration that is specific\nto the pipeline will be used to look up the value.</p>\n<h3>Execution</h3>\n<p>The requested configuration values and stages are afterwards available\nto the <code>execute</code> step of a <em>stage</em>. There those values can be used to do the\n\"heavy work\" of the stage. As the <code>configure</code> step already defined what kind\nof values to expect, we can be sure that those values and dependencies are\npresent once <code>execute</code> is called.</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"c1\"># Load some data from another stage</span>\n  <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">stage</span><span class=\"p\">(</span><span class=\"s2\">\"my.pipeline.census.raw\"</span><span class=\"p\">)</span>\n\n  <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">dropna</span><span class=\"p\">()</span>\n  <span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s2\">\"age\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s2\">\"age\"</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">)</span>\n\n  <span class=\"c1\"># We could access some values if we wanted</span>\n  <span class=\"n\">value</span> <span class=\"o\">=</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"p\">(</span><span class=\"s2\">\"...\"</span><span class=\"p\">)</span>\n\n  <span class=\"k\">return</span> <span class=\"n\">df</span>\n</pre>\n<p>Note that the <code>execute</code> step returns a value. This value will be <em>pickled</em> (see\n<em>pickle</em> package of Python) and cached on the hard drive. This means that whenever\nthe output of this stage is requested by another stage, it doesn't need to be\nrun again. The pipeline can simply load the cached result from hard drive.</p>\n<p>If one has a very complex pipeline with many stages this means that changes in\none stage will likely not lead to a situation where one needs to re-run the\nwhole pipeline, but only a fraction. The <em>synpp</em> framework has intelligent\nexplorations algorithms included which figure out automatically, which\n<em>stages</em> need to be re-run.</p>\n<h3>Running a pipeline</h3>\n<p>A pipeline can be started using the <code>synpp.run</code> method. A typical run would\nlook like this:</p>\n<pre><span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"p\">{</span> <span class=\"s2\">\"random_seed\"</span><span class=\"p\">:</span> <span class=\"mi\">1234</span> <span class=\"p\">}</span>\n<span class=\"n\">working_directory</span> <span class=\"o\">=</span> <span class=\"s2\">\"~/pipeline/cache\"</span>\n\n<span class=\"n\">synpp</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span>\n    <span class=\"p\">{</span> <span class=\"s2\">\"descriptor\"</span><span class=\"p\">:</span> <span class=\"s2\">\"my.pipeline.final_population\"</span> <span class=\"p\">},</span>\n    <span class=\"p\">{</span> <span class=\"s2\">\"descriptor\"</span><span class=\"p\">:</span> <span class=\"s2\">\"my.pipeline.paper_analysis\"</span><span class=\"p\">,</span> <span class=\"s2\">\"config\"</span><span class=\"p\">:</span> <span class=\"p\">{</span> <span class=\"s2\">\"font_size\"</span><span class=\"p\">:</span> <span class=\"mi\">12</span> <span class=\"p\">}</span> <span class=\"p\">}</span>\n<span class=\"p\">],</span> <span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">working_directory</span> <span class=\"o\">=</span> <span class=\"n\">working_directory</span><span class=\"p\">)</span>\n</pre>\n<p>Here we call the <em>stage</em> defined by the module <code>my.pipeline.final_population</code>\nwhich should be available in the Python path. And we also want to run the\n<code>my.pipeline.paper_analysis</code> path with a font size parameter of <code>12</code>. Note that\nin both cases we could also have based the bare Python module objects instead\nof strings.</p>\n<p>The pipeline will now figure out how to run those <em>stages</em>. Probably they have\ndependencies and the analysis <em>stage</em> may even depend on the other one. Therefore,\n<em>synpp</em> explores the tree of dependencies as follows:</p>\n<ul>\n<li>Consider the requested stages (two in this case)</li>\n<li>Step by step, go through the dependencies of those stages</li>\n<li>Then again, go through the dependencies of all added stages, and so on</li>\n</ul>\n<p>By that the pipeline traverses the whole tree of dependencies as they are defined\nby the <code>configure</code> steps of all stages. At the same time it collects information\nabout which configuration options and parameters are required by each stage. Note\nthat a stage can occur twice in this dependency tree if it has different\nparameters.</p>\n<p>After constructing a tree of <em>stages</em>, <em>synpp</em> devalidates some of them according\nto the following scheme. A <em>stage</em> is devalidated if ...</p>\n<ul>\n<li>... it is requested by the <code>run</code> call</li>\n<li>... it is new (no meta data from a previous call is present)</li>\n<li>... if at least one of the requested configuration options has changed</li>\n<li>... if at least one dependency has been re-run since the last run of the stage</li>\n<li>... if list of dependencies has changed</li>\n<li>... if manual <em>validation</em> of the stage has failed (see below)</li>\n<li>... if any ascendant of a stage has been devalidated</li>\n</ul>\n<p>This list of conditions makes sure that in almost any case of pipeline\nmodification we end up in a consistent situation (though we cannot prove it).\nThe only measure that may be important to enforce 'by convention' is to\n<em>always run a stage after the code has been modified</em>. Though even this can\nbe automated.</p>\n<h3>Validation</h3>\n<p>Each <em>stage</em> has an additional <code>validate</code> step, which also receives the\nconfiguration options and the parameters. Its purpose is to return a hash\nvalue that represents the environment of the <em>stage</em>. To learn about the concept\nin general, search for \"md5 hash\", for instance. The idea is the following:\nAfter the <code>execute</code> step, the <code>validate</code> step is called and\nit will return a certain value. Next time the pipeline\nis resolved the <code>validate</code> step is called during devalidation, i.e. before\nthe stage is actually <em>executed</em>. If the return value of <code>validate</code> now differs\nfrom what it was before, the stage will be devalidated.</p>\n<p>This is useful to check the integrity of data that is not generated inside of\nthe pipeline but comes from the outside, for instance:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">configure</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"p\">(</span><span class=\"s2\">\"input_path\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">validate</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"n\">path</span> <span class=\"o\">=</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"p\">(</span><span class=\"s2\">\"input_path\"</span><span class=\"p\">)</span>\n  <span class=\"n\">filesize</span> <span class=\"o\">=</span> <span class=\"n\">get_filesize</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">)</span>\n\n  <span class=\"c1\"># If the file size has changed, the file must have changed,</span>\n  <span class=\"c1\"># hence we want to run the stage again.</span>\n  <span class=\"k\">return</span> <span class=\"n\">filesize</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"k\">pass</span> <span class=\"c1\"># Do something with the file</span>\n</pre>\n<h3>Cache paths</h3>\n<p>Sometimes, results of a <em>stage</em> are not easily representable in Python. Even\nmore, stages may call Java or Shell scripts which simply generate an output\nfile. For these cases each stage has its own <em>cache path</em>. It can be accessed\nthrough the stage context:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"c1\"># In this case we write a file to the cache path of the current stage</span>\n  <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">\"</span><span class=\"si\">%s</span><span class=\"s2\">/myfile.txt\"</span> <span class=\"o\">%</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"p\">())</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"s2\">\"my content\"</span><span class=\"p\">)</span>\n\n  <span class=\"c1\"># In this case we read a file from the cache path of another stage</span>\n  <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">\"</span><span class=\"si\">%s</span><span class=\"s2\">/otherfile.txt\"</span> <span class=\"o\">%</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"p\">(</span><span class=\"s2\">\"my.other.stage\"</span><span class=\"p\">))</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">value</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">()</span>\n</pre>\n<p>As the example shows, we can also access cache paths of other stages. The pipeline\nwill make sure that you only have access to the cache path of stages that\nhave been defined as dependencies before. Note that the pipeline cannot enforce\nthat one stage is not corrupting the cache path of another stage. Therefore,\nby convention, a stage should never <em>write</em> to the cache path of another stage.</p>\n<h3>Parallel execution</h3>\n<p>The <em>synpp</em> package comes with some simplified ways of parallelizing code,\nwhich are built on top of the <code>multiprocessing</code> package. To set up a parallel\nroutine, one can follow the following pattern:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">run_parallel</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n  <span class=\"k\">return</span> <span class=\"n\">x</span><span class=\"o\">**</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">(</span><span class=\"s2\">\"y\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span> <span class=\"s2\">\"y\"</span><span class=\"p\">:</span> <span class=\"mi\">5</span> <span class=\"p\">}</span>\n\n  <span class=\"k\">with</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">parallel</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">parallel</span><span class=\"p\">:</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">parallel</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span><span class=\"n\">run_parallel</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">])</span>\n</pre>\n<p>This approach looks similar to the <code>Pool</code> object of <code>multiprocessing</code> but has\nsome simplifications. First, the first argument of the parallel routine is a\ncontext object, which provides configuration and parameters. Furthermore, it\nprovides data, which has been passed before in the <code>execute</code> function. This\nsimplifies passing data to all parallel threads considerably to the more\nflexible approach in <code>multiprocessing</code>. Otherwise, the <code>parallel</code> object\nprovides most of the functionality of <code>Pool</code>, like, <code>map</code>, <code>async_map</code>,\n<code>imap</code>, and <code>unordered_imap</code>.</p>\n<h3>Info</h3>\n<p>While running the pipeline a lot of additional information may be interesting,\nlike how many samples of a data set have been discarded in a certain stage. However,\nthey often would only be used at the very end of the pipeline when maybe a paper,\na report or some explanatory graphics are generated. For that, the pipeline\nprovides the <code>set_info</code> method:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"c1\"># ...</span>\n  <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">set_info</span><span class=\"p\">(</span><span class=\"s2\">\"dropped_samples\"</span><span class=\"p\">,</span> <span class=\"n\">number_of_dropped_samples</span><span class=\"p\">)</span>\n  <span class=\"c1\"># ...</span>\n</pre>\n<p>The information can later be retrieved from another stage (which has the\nstage in question as a dependency):</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"c1\"># ...</span>\n  <span class=\"n\">value</span> <span class=\"o\">=</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">get_info</span><span class=\"p\">(</span><span class=\"s2\">\"my.other.stage\"</span><span class=\"p\">,</span> <span class=\"s2\">\"dropped_samples\"</span><span class=\"p\">)</span>\n  <span class=\"c1\"># ...</span>\n</pre>\n<p>Note that the <em>info</em> functionality should only be used for light-weight\ninformation like integers, short strings, etc.</p>\n<h3>Progress</h3>\n<p>The <em>synpp</em> package provides functionality to show the progress of a stage\nsimilar to <code>tqdm</code>. However, <code>tqdm</code> tends to spam the console output which is\nespecially undesired if pipelines have long runtimes and run, for instance, in\nContinuous Integration environments. Therefore, <em>synpp</em> provides its own\nfunctionality, although <code>tqdm</code> could still be used:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n  <span class=\"c1\"># As a</span>\n  <span class=\"k\">with</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">progress</span><span class=\"p\">(</span><span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s2\">\"My progress...\"</span><span class=\"p\">,</span> <span class=\"n\">total</span> <span class=\"o\">=</span> <span class=\"mi\">100</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">progress</span><span class=\"p\">:</span>\n    <span class=\"n\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n\n    <span class=\"k\">while</span> <span class=\"n\">i</span> <span class=\"o\">&lt;</span> <span class=\"mi\">100</span><span class=\"p\">:</span>\n      <span class=\"n\">progress</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n      <span class=\"n\">i</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n\n  <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">progress</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)):</span>\n    <span class=\"k\">pass</span>\n</pre>\n<h3>Command-line tool</h3>\n<p>The <code>synpp</code> pipeline comes with a command line tool, which can be called like</p>\n<pre>python3 -m synpp <span class=\"o\">[</span>config_path<span class=\"o\">]</span>\n</pre>\n<p>If not config path is given, it will assume <code>config.yml</code>. This file should\ncontain everything to run a pipeline. A simple version would look like this:</p>\n<pre><span class=\"c1\"># General pipeline settings</span>\n<span class=\"nt\">working_directory</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">/path/to/my/working_directory</span>\n\n<span class=\"c1\"># Requested stages</span>\n<span class=\"nt\">run</span><span class=\"p\">:</span>\n  <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">my_first_module.my_first_stage</span>\n  <span class=\"p p-Indicator\">-</span> <span class=\"nt\">my_first_parameterized_stage</span><span class=\"p\">:</span>\n    <span class=\"nt\">param1</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">123</span>\n    <span class=\"nt\">param2</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">345</span>\n\n<span class=\"c1\"># These are configuration options that are used in the pipeline</span>\n<span class=\"nt\">config</span><span class=\"p\">:</span>\n  <span class=\"nt\">my_option</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">123</span>\n</pre>\n<p>It receives the working directory, a list of stages (which may be parameterized)\nand all configuration options. The stages listed above should be available\nas Python modules or classes.</p>\n<h2>NYC Taxi Example</h2>\n<p>This repository contains an example of the pipline. To run it, you will need\n<code>pandas</code> as an additional Python dependency. For testing, you can clone this\nrepository to any directory on your machine. Inside the repository directory\nyou can find the <code>example</code> directory. If you did not install <code>synpp</code> yet,\nyou can do this by executing</p>\n<pre>pip install .\n</pre>\n<p>inside of the repository directory. Afterwards, open <code>examples/config.yml</code>\nand adjust the <code>working_directory</code> path. This is a path that should exist on\nyour machine and it should be empty. The best is if you simply create a new\nfolder and add the path in <code>config.yml</code>.</p>\n<p>You can now go to <code>examples</code> and call the pipeline code:</p>\n<pre><span class=\"nb\">cd</span> examples\npython3 -m synpp\n</pre>\n<p>It will automatically discover <code>config.yml</code> (but you could path a different\nconfig file path manually as a command line argument). It will then download\nthe NYC taxi data for January, February and March 2018 (see configuration\noptions in <code>config.yml</code>). Note that this is happening in one stage for which\nyou can find the code in <code>nyc_taxi.download</code>. It is parameterized by a month\nand a year to download the respective data set. These data sets then go into\n<code>nyc_taxi.aggregate</code>, where they are merged together. Finally, an average\noccupancy value is printed out in <code>nyc_taxi.print_occupancy</code>. So the dependency\nstructure is as follows:</p>\n<pre><code>nyc_taxi.aggregate depends on multiple nyc_taxi.download(year, month)\nnyc_taxi.print_occupancy depends on nyc_taxi.aggregate\n</code></pre>\n<p>After one successful run of the pipeline you can start it again. You will notice\nthat the pipeline does <em>not</em> download the data again, because nothing has changed\nfor those stages. However, if you would change the requested months in <code>config.yml</code>\nthe pipeline may download the additional data sets.</p>\n\n          </div>"}, "last_serial": 7107019, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "619a82e11fc3f2337e9c70be434e5896", "sha256": "c9aebedad5f5dad1fbfeb7b21f55670e24f56eb7ba9b2377ef78594cb6428f0e"}, "downloads": -1, "filename": "synpp-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "619a82e11fc3f2337e9c70be434e5896", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 14518, "upload_time": "2019-12-12T08:57:26", "upload_time_iso_8601": "2019-12-12T08:57:26.694394Z", "url": "https://files.pythonhosted.org/packages/7b/26/fdc8abd3c9610b15bb2295d675aebf80b594ef4b1b9023eed1517ea6dd71/synpp-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1ce00394249006b78b657552b66d8780", "sha256": "9dffc475936b37af7505177d39a46b8c3d11b08104341bd37c7be8a849be2c3b"}, "downloads": -1, "filename": "synpp-1.0.0.tar.gz", "has_sig": false, "md5_digest": "1ce00394249006b78b657552b66d8780", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 18939, "upload_time": "2019-12-12T08:57:30", "upload_time_iso_8601": "2019-12-12T08:57:30.200467Z", "url": "https://files.pythonhosted.org/packages/aa/3c/a535d1766acf553de827139f9f8d718ad71629666e8716f8a9f017f86a2f/synpp-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "7872f2b2b1bfe5678d5bed3d280be685", "sha256": "46cc9bbbc86e9d554f5c7e72e13b9d62ae61a1478af527b1f22bba8029ad7af6"}, "downloads": -1, "filename": "synpp-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "7872f2b2b1bfe5678d5bed3d280be685", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 15201, "upload_time": "2019-12-22T10:54:26", "upload_time_iso_8601": "2019-12-22T10:54:26.018452Z", "url": "https://files.pythonhosted.org/packages/58/b1/71444cb9fa95f47c1a72dcc984cf1ef2b93a4994fa096931cdee8d2cc67f/synpp-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "478be593b501283cec1e19e81c166959", "sha256": "0ddf009070c0c92c3c918b27db6e8dd58facab4a1444a25a253f97e212335603"}, "downloads": -1, "filename": "synpp-1.0.1.tar.gz", "has_sig": false, "md5_digest": "478be593b501283cec1e19e81c166959", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 19844, "upload_time": "2019-12-22T10:54:27", "upload_time_iso_8601": "2019-12-22T10:54:27.932448Z", "url": "https://files.pythonhosted.org/packages/5e/85/17dc126586dc87e6210ef891a0a463cfd301bab85c14bc0acc1cd7db4403/synpp-1.0.1.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "8a1e1320cbb8ed2750044dc283c84502", "sha256": "8f22caf31b5115073b3865003996a65d77815285e72e7ba3343bc56234130581"}, "downloads": -1, "filename": "synpp-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "8a1e1320cbb8ed2750044dc283c84502", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 15078, "upload_time": "2019-12-22T12:24:46", "upload_time_iso_8601": "2019-12-22T12:24:46.926125Z", "url": "https://files.pythonhosted.org/packages/2d/f0/e5b933ae19a05144f3f6ed423dba07ea2a9f0cd6ac75626cc4c4ebe1a677/synpp-1.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5fbd1a711be6dff964a35070208bae92", "sha256": "b0e98cc347c2a1e4d644b688b2ee8424ee00b4db732579712161fbeceeb60539"}, "downloads": -1, "filename": "synpp-1.1.0.tar.gz", "has_sig": false, "md5_digest": "5fbd1a711be6dff964a35070208bae92", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 19726, "upload_time": "2019-12-22T12:24:49", "upload_time_iso_8601": "2019-12-22T12:24:49.088012Z", "url": "https://files.pythonhosted.org/packages/52/a6/a97a9a956fc8fd5b4dbb8eb07d33ddba227f4057761c6fe5b240e0bdfe48/synpp-1.1.0.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "db6320d8a466f7473325a96ebf16b997", "sha256": "cb11a1b4f2a1acef491de60789be8538750bb8c368e4689f548b010129799681"}, "downloads": -1, "filename": "synpp-1.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "db6320d8a466f7473325a96ebf16b997", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 16747, "upload_time": "2020-04-26T12:15:30", "upload_time_iso_8601": "2020-04-26T12:15:30.516231Z", "url": "https://files.pythonhosted.org/packages/0f/6f/8c01cf721e8ee4327b5fa5d6efe533466899d9fab596a6e3e974c5676ecf/synpp-1.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6d317dcf85972b584188f3a1f10f0f78", "sha256": "576b9e97dd73bb445d05b2453e91ebc96385f394e0a9a12050251886d60d0f4f"}, "downloads": -1, "filename": "synpp-1.2.0.tar.gz", "has_sig": false, "md5_digest": "6d317dcf85972b584188f3a1f10f0f78", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 21414, "upload_time": "2020-04-26T12:15:35", "upload_time_iso_8601": "2020-04-26T12:15:35.815498Z", "url": "https://files.pythonhosted.org/packages/10/5f/c805cfbfb146bf5bf86fe0dfc33043b22d4cd0d7aa813cbbb53c5856f6cc/synpp-1.2.0.tar.gz", "yanked": false}], "1.2.1": [{"comment_text": "", "digests": {"md5": "d0e1b818bb4110d841c3046ab001ff86", "sha256": "e610f1d21dfe2df3c3c546fd460637dd0c3432b3cea879e2ab416b1d287657f6"}, "downloads": -1, "filename": "synpp-1.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "d0e1b818bb4110d841c3046ab001ff86", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 16756, "upload_time": "2020-04-26T21:27:56", "upload_time_iso_8601": "2020-04-26T21:27:56.028486Z", "url": "https://files.pythonhosted.org/packages/cd/64/d1df38e55127ee864fd3c2684f22997b3698cd73100d5d355430f8704505/synpp-1.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "41e7851f14e121ba87f283ae5736e5ea", "sha256": "8715ad0fee040ef057101a9f980ca1b4805fcfeb715cf14aca20ad6b51418aa2"}, "downloads": -1, "filename": "synpp-1.2.1.tar.gz", "has_sig": false, "md5_digest": "41e7851f14e121ba87f283ae5736e5ea", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 21446, "upload_time": "2020-04-26T21:28:01", "upload_time_iso_8601": "2020-04-26T21:28:01.346713Z", "url": "https://files.pythonhosted.org/packages/9e/4b/84d3029e87cd283a7c6dfb9e6f2cd0202bf9c3f1299a73fc5cfce02f3768/synpp-1.2.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d0e1b818bb4110d841c3046ab001ff86", "sha256": "e610f1d21dfe2df3c3c546fd460637dd0c3432b3cea879e2ab416b1d287657f6"}, "downloads": -1, "filename": "synpp-1.2.1-py3-none-any.whl", "has_sig": false, "md5_digest": "d0e1b818bb4110d841c3046ab001ff86", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.0", "size": 16756, "upload_time": "2020-04-26T21:27:56", "upload_time_iso_8601": "2020-04-26T21:27:56.028486Z", "url": "https://files.pythonhosted.org/packages/cd/64/d1df38e55127ee864fd3c2684f22997b3698cd73100d5d355430f8704505/synpp-1.2.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "41e7851f14e121ba87f283ae5736e5ea", "sha256": "8715ad0fee040ef057101a9f980ca1b4805fcfeb715cf14aca20ad6b51418aa2"}, "downloads": -1, "filename": "synpp-1.2.1.tar.gz", "has_sig": false, "md5_digest": "41e7851f14e121ba87f283ae5736e5ea", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0", "size": 21446, "upload_time": "2020-04-26T21:28:01", "upload_time_iso_8601": "2020-04-26T21:28:01.346713Z", "url": "https://files.pythonhosted.org/packages/9e/4b/84d3029e87cd283a7c6dfb9e6f2cd0202bf9c3f1299a73fc5cfce02f3768/synpp-1.2.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:59:00 2020"}