{"info": {"author": "Russell Power", "author_email": "power@cs.nyu.edu", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: System Administrators", "License :: OSI Approved :: BSD License", "Operating System :: POSIX", "Programming Language :: Python :: 2.5", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.0", "Programming Language :: Python :: 3.1", "Programming Language :: Python :: 3.2", "Topic :: Software Development :: Libraries", "Topic :: System :: Clustering", "Topic :: System :: Distributed Computing"], "description": "MyCloud\n=======\n\nLeverage small clusters of machines to increase your productivity.\n\nMyCloud requires no prior setup; if you can SSH to your machines, then\nit will work out of the box. MyCloud currently exports a simple\nmapreduce API with several common input formats; adding support for your\nown is easy as well.\n\nUsage\n=====\n\nStarting your cluster:\n\n::\n\n    import mycloud\n\n    cluster = mycloud.Cluster(['machine1', 'machine2'])\n\n    # or use defaults from ~/.config/mycloud\n    # cluster = mycloud.Cluster()\n\nMap over a list:\n\n::\n\n    result = cluster.map(compute_factors, range(1000))\n\nClientFS makes accessing local files seamless!\n\n::\n\n    def my_worker(filename):\n      do_work(mycloud.fs.FS.open(filename, 'r'))\n\n    cluster.map(['client:///my/local/file'], my_worker)\n\nUse the MapReduce interface to easily handle processing of larger\ndatasets:\n\n::\n\n    from mycloud.mapreduce import MapReduce, group\n    from mycloud.resource import CSV  \n    input_desc = [CSV('client:///path/to/my_input_%d.csv') % i for i in range(100)]\n    output_desc = [CSV('client:///path/to/my_output_file.csv')]\n\n    def map_identity(kv_iter, output):\n      for k, v in kv_iter:\n        output(k, int(v[0]))\n\n    def reduce_sum(kv_iter, output): \n      for k, values in group(kv_iter):\n        output(k, sum(values))\n\n    mr = MapReduce(cluster, map_identity, reduce_sum, input_desc, output_desc)\n\n    result = mr.run()\n\n    for k, v in result[0].reader():\n      print k, v\n\nPerformance\n===========\n\nIt is, keep in mind, written entirely in Python.\n\nSome simple operations I've used it for (6 machines, 96 cores):\n\n-  Sorting a billion numbers: ~5m\n-  Preprocessing 1.3 million images (resizing and SIFT feature\n   extraction): ~1 hour\n\nInput formats\n=============\n\nMycloud has builtin support for processing the following file types:\n\n-  LevelDB\n-  CSV\n-  Text (lines)\n-  Zip\n\nAdding support for your own is simple - just write a resource class\ndescribing how to get a reader and writer. (see resource.py for\ndetails).\n\nWhy?!?\n======\n\nSometimes you're developing something in Python (because that's what you\ndo), and you decide you'd like it to be parallelized. Our current\noptions are multiprocessing (limiting us to a single machine) and Hadoop\nstreaming (limiting us to strings and Hadoop's input formats).\n\nAlso, because I could.\n\nCredits\n=======\n\nMyCloud builds on the phenomonally useful\n`cloud <http://pypi.python.org/pypi/cloud/>`_ serialization,\n`SSH/Paramiko <http://pypi.python.org/pypi/paramiko/1.9.0>`_, and\n`LevelDB <http://pypi.python.org/pypi/leveldb>`_ libraries.", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://github.com/rjpower/mycloud", "keywords": null, "license": "BSD", "maintainer": null, "maintainer_email": null, "name": "mycloud", "package_url": "https://pypi.org/project/mycloud/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/mycloud/", "project_urls": {"Download": "UNKNOWN", "Homepage": "http://github.com/rjpower/mycloud"}, "release_url": "https://pypi.org/project/mycloud/0.51/", "requires_dist": null, "requires_python": null, "summary": "Work distribution for small clusters.", "version": "0.51", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"mycloud\">\n<h2>MyCloud</h2>\n<p>Leverage small clusters of machines to increase your productivity.</p>\n<p>MyCloud requires no prior setup; if you can SSH to your machines, then\nit will work out of the box. MyCloud currently exports a simple\nmapreduce API with several common input formats; adding support for your\nown is easy as well.</p>\n</div>\n<div id=\"usage\">\n<h2>Usage</h2>\n<p>Starting your cluster:</p>\n<pre>import mycloud\n\ncluster = mycloud.Cluster(['machine1', 'machine2'])\n\n# or use defaults from ~/.config/mycloud\n# cluster = mycloud.Cluster()\n</pre>\n<p>Map over a list:</p>\n<pre>result = cluster.map(compute_factors, range(1000))\n</pre>\n<p>ClientFS makes accessing local files seamless!</p>\n<pre>def my_worker(filename):\n  do_work(mycloud.fs.FS.open(filename, 'r'))\n\ncluster.map(['client:///my/local/file'], my_worker)\n</pre>\n<p>Use the MapReduce interface to easily handle processing of larger\ndatasets:</p>\n<pre>from mycloud.mapreduce import MapReduce, group\nfrom mycloud.resource import CSV\ninput_desc = [CSV('client:///path/to/my_input_%d.csv') % i for i in range(100)]\noutput_desc = [CSV('client:///path/to/my_output_file.csv')]\n\ndef map_identity(kv_iter, output):\n  for k, v in kv_iter:\n    output(k, int(v[0]))\n\ndef reduce_sum(kv_iter, output):\n  for k, values in group(kv_iter):\n    output(k, sum(values))\n\nmr = MapReduce(cluster, map_identity, reduce_sum, input_desc, output_desc)\n\nresult = mr.run()\n\nfor k, v in result[0].reader():\n  print k, v\n</pre>\n</div>\n<div id=\"performance\">\n<h2>Performance</h2>\n<p>It is, keep in mind, written entirely in Python.</p>\n<p>Some simple operations I\u2019ve used it for (6 machines, 96 cores):</p>\n<ul>\n<li>Sorting a billion numbers: ~5m</li>\n<li>Preprocessing 1.3 million images (resizing and SIFT feature\nextraction): ~1 hour</li>\n</ul>\n</div>\n<div id=\"input-formats\">\n<h2>Input formats</h2>\n<p>Mycloud has builtin support for processing the following file types:</p>\n<ul>\n<li>LevelDB</li>\n<li>CSV</li>\n<li>Text (lines)</li>\n<li>Zip</li>\n</ul>\n<p>Adding support for your own is simple - just write a resource class\ndescribing how to get a reader and writer. (see resource.py for\ndetails).</p>\n</div>\n<div id=\"why\">\n<h2>Why?!?</h2>\n<p>Sometimes you\u2019re developing something in Python (because that\u2019s what you\ndo), and you decide you\u2019d like it to be parallelized. Our current\noptions are multiprocessing (limiting us to a single machine) and Hadoop\nstreaming (limiting us to strings and Hadoop\u2019s input formats).</p>\n<p>Also, because I could.</p>\n</div>\n<div id=\"credits\">\n<h2>Credits</h2>\n<p>MyCloud builds on the phenomonally useful\n<a href=\"http://pypi.python.org/pypi/cloud/\" rel=\"nofollow\">cloud</a> serialization,\n<a href=\"http://pypi.python.org/pypi/paramiko/1.9.0\" rel=\"nofollow\">SSH/Paramiko</a>, and\n<a href=\"http://pypi.python.org/pypi/leveldb\" rel=\"nofollow\">LevelDB</a> libraries.</p>\n</div>\n\n          </div>"}, "last_serial": 795140, "releases": {"0.48": [{"comment_text": "", "digests": {"md5": "8f08de28d4589083ed444c4791e29766", "sha256": "95769a952cc764b4dbfc6e29f4457255bcb4f3657c82664a37e11ed7d0c51b74"}, "downloads": -1, "filename": "mycloud-0.48.tar.gz", "has_sig": false, "md5_digest": "8f08de28d4589083ed444c4791e29766", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11965, "upload_time": "2012-11-16T22:10:52", "upload_time_iso_8601": "2012-11-16T22:10:52.515879Z", "url": "https://files.pythonhosted.org/packages/f4/dc/dba714db7a221524f8607d343711add6fae2718a85d43aaf5e4aab745b08/mycloud-0.48.tar.gz", "yanked": false}], "0.49": [{"comment_text": "", "digests": {"md5": "a9a274c6662d4e9b46ebf11b423d3da2", "sha256": "11e76d45df445da82fd7d035e400765ce92f776cb6a43500f0384c86e75e93ce"}, "downloads": -1, "filename": "mycloud-0.49.tar.gz", "has_sig": false, "md5_digest": "a9a274c6662d4e9b46ebf11b423d3da2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 11864, "upload_time": "2013-01-01T22:45:45", "upload_time_iso_8601": "2013-01-01T22:45:45.933218Z", "url": "https://files.pythonhosted.org/packages/0f/31/ee241621babf75d33414855b9ea469b43febe1da1c4996d1a96985b1463c/mycloud-0.49.tar.gz", "yanked": false}], "0.51": [{"comment_text": "", "digests": {"md5": "16cdbe7c0c8c03c32e4a6f1ad0a93870", "sha256": "397326a15bef6b4f80d689939c44018eb0444807ee9b95cf46b48a01100d5f25"}, "downloads": -1, "filename": "mycloud-0.51.tar.gz", "has_sig": false, "md5_digest": "16cdbe7c0c8c03c32e4a6f1ad0a93870", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12009, "upload_time": "2013-01-15T22:33:25", "upload_time_iso_8601": "2013-01-15T22:33:25.522408Z", "url": "https://files.pythonhosted.org/packages/9b/74/101e9410700f5c252e0c21ec28ae9fa65fabdf2dd8722f1cf0acc508a0a5/mycloud-0.51.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "16cdbe7c0c8c03c32e4a6f1ad0a93870", "sha256": "397326a15bef6b4f80d689939c44018eb0444807ee9b95cf46b48a01100d5f25"}, "downloads": -1, "filename": "mycloud-0.51.tar.gz", "has_sig": false, "md5_digest": "16cdbe7c0c8c03c32e4a6f1ad0a93870", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12009, "upload_time": "2013-01-15T22:33:25", "upload_time_iso_8601": "2013-01-15T22:33:25.522408Z", "url": "https://files.pythonhosted.org/packages/9b/74/101e9410700f5c252e0c21ec28ae9fa65fabdf2dd8722f1cf0acc508a0a5/mycloud-0.51.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:49:36 2020"}