{"info": {"author": "Artem Gavrilov", "author_email": "info@teamfnd.ru", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)", "Programming Language :: Python", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "PyPascalTokenizer\n=================\n\nModule for Python 3. It does tokenizing of Pascal code (it's planned to support full Delphi and FreePascal syntax).\n\nAPI\n===\n\nToken struct\n------------\n\nToken is saved as 4-tuple (text, begin, end, final):\n\n* text: string with token text\n* begin: position of token start, tuple (y, x), where y - 0-based line index and x - 0-based character index in line\n* end: position after token end, tuple (y, x)\n* final: bool, True if it was last token\n\nClass PasTokenizer\n------------------\n\n* __init__(lines): params: list of source code strings\n* get_next(): get next token and change end pos\n* read_next(): get next token, but don't change end pos\n* is_ended(): check if text ended\n\nClass PasTokenizerStack\n-----------------------\n\n* __init__(lines, comments=True): params: list of source code strings; \"comments\" allows to get also comment-tokens (otherwise tokenizer skips them)\n* push(s): push token to stack\n* pop(): pop (get and delete) token from stack\n* read_last(): read (get but don't delete) top token from stack\n* is_ended(): check if stack ended\n\nClass PasTokenizerParallelStack\n-------------------------------\n\nDescendant of PasTokenizerStack, which uses thread(s) for parsing entire file. Before destroying it you mast call stop().\n\n* __init__(lines, comments=True, qlong=1000): additonal param: size of internal queue buffer\n\nUtils\n-----\n\nHelper functions to analyze token text.\n\n* is_name(s): Check for valid identifier (can be reserved word too).\n* is_comment(s): Check for valid comment.\n* is_string(s): Check for string constant.\n\n\nAuthor\n=====\nArtem Gavrilov (@Artem3213212 at Github)", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "pascal-tokenizer", "package_url": "https://pypi.org/project/pascal-tokenizer/", "platform": "", "project_url": "https://pypi.org/project/pascal-tokenizer/", "project_urls": null, "release_url": "https://pypi.org/project/pascal-tokenizer/1.0.4/", "requires_dist": null, "requires_python": "", "summary": "", "version": "1.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            PyPascalTokenizer<br>=================<br><br>Module for Python 3. It does tokenizing of Pascal code (it's planned to support full Delphi and FreePascal syntax).<br><br>API<br>===<br><br>Token struct<br>------------<br><br>Token is saved as 4-tuple (text, begin, end, final):<br><br>* text: string with token text<br>* begin: position of token start, tuple (y, x), where y - 0-based line index and x - 0-based character index in line<br>* end: position after token end, tuple (y, x)<br>* final: bool, True if it was last token<br><br>Class PasTokenizer<br>------------------<br><br>* __init__(lines): params: list of source code strings<br>* get_next(): get next token and change end pos<br>* read_next(): get next token, but don't change end pos<br>* is_ended(): check if text ended<br><br>Class PasTokenizerStack<br>-----------------------<br><br>* __init__(lines, comments=True): params: list of source code strings; \"comments\" allows to get also comment-tokens (otherwise tokenizer skips them)<br>* push(s): push token to stack<br>* pop(): pop (get and delete) token from stack<br>* read_last(): read (get but don't delete) top token from stack<br>* is_ended(): check if stack ended<br><br>Class PasTokenizerParallelStack<br>-------------------------------<br><br>Descendant of PasTokenizerStack, which uses thread(s) for parsing entire file. Before destroying it you mast call stop().<br><br>* __init__(lines, comments=True, qlong=1000): additonal param: size of internal queue buffer<br><br>Utils<br>-----<br><br>Helper functions to analyze token text.<br><br>* is_name(s): Check for valid identifier (can be reserved word too).<br>* is_comment(s): Check for valid comment.<br>* is_string(s): Check for string constant.<br><br><br>Author<br>=====<br>Artem Gavrilov (@Artem3213212 at Github)\n          </div>"}, "last_serial": 4219028, "releases": {"1.0.3": [{"comment_text": "", "digests": {"md5": "00838346137144ad136320e9f484f7ee", "sha256": "c5a2068cde20696271b98d8883f7de59e7ec6f21cada87ef58fa7f2edb7f6950"}, "downloads": -1, "filename": "pascal_tokenizer-1.0.3.tar.gz", "has_sig": false, "md5_digest": "00838346137144ad136320e9f484f7ee", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3475, "upload_time": "2018-08-14T14:30:20", "upload_time_iso_8601": "2018-08-14T14:30:20.579235Z", "url": "https://files.pythonhosted.org/packages/f3/ff/7543334239438c902c4658c7b9502617fc460955a0920a090a514c447109/pascal_tokenizer-1.0.3.tar.gz", "yanked": false}], "1.0.4": [{"comment_text": "", "digests": {"md5": "1ac308e8e83119ea295a3cc0ea13532d", "sha256": "99774ff72534a3d9de76008fe0cf2d0a4aa6c39d352414a0335e44817ec434be"}, "downloads": -1, "filename": "pascal_tokenizer-1.0.4.tar.gz", "has_sig": false, "md5_digest": "1ac308e8e83119ea295a3cc0ea13532d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3487, "upload_time": "2018-08-29T14:03:33", "upload_time_iso_8601": "2018-08-29T14:03:33.069555Z", "url": "https://files.pythonhosted.org/packages/95/a0/29cc235c86238f53573dbf8d6ed36ca5e6da05bacdfd41cb8f009b86ca6a/pascal_tokenizer-1.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1ac308e8e83119ea295a3cc0ea13532d", "sha256": "99774ff72534a3d9de76008fe0cf2d0a4aa6c39d352414a0335e44817ec434be"}, "downloads": -1, "filename": "pascal_tokenizer-1.0.4.tar.gz", "has_sig": false, "md5_digest": "1ac308e8e83119ea295a3cc0ea13532d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3487, "upload_time": "2018-08-29T14:03:33", "upload_time_iso_8601": "2018-08-29T14:03:33.069555Z", "url": "https://files.pythonhosted.org/packages/95/a0/29cc235c86238f53573dbf8d6ed36ca5e6da05bacdfd41cb8f009b86ca6a/pascal_tokenizer-1.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:58:10 2020"}