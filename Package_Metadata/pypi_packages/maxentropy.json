{"info": {"author": "Ed Schofield", "author_email": "ed@pythoncharmers.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: BSD License", "Operating System :: MacOS", "Operating System :: Microsoft :: Windows", "Operating System :: POSIX", "Operating System :: Unix", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering", "Topic :: Software Development"], "description": "# maxentropy: Maximum entropy and minimum divergence models in Python\n\n## Purpose\n\nThis package helps you to construct a probability distribution\n(Bayesian prior) from prior information that you encode as\ngeneralized moment constraints.\n\nYou can use it to either:\n\n1. find the flattest distribution that meets your constraints, using the\n   maximum entropy principle (discrete distributions only)\n\n2. or find the \"closest\" model to a given prior model (in a KL divergence\n   sense) that also satisfies your additional constraints.\n\n## Background\n\nThe maximum entropy principle has been shown [Cox 1982, Jaynes 2003] to be the unique consistent approach to\nconstructing a discrete probability distribution from prior information that is available as \"testable information\".\n\nIf the constraints have the form of linear moment constraints, then\nthe principle gives rise to a unique probability distribution of\n**exponential form**. Most well-known probability distributions are\nspecial cases of maximum entropy distributions. This includes\nuniform, geometric, exponential, Pareto, normal, von Mises, Cauchy,\nand others: see\n[here](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution).\n\n## Examples: constructing a prior subject to known constraints\n\nSee the [notebooks folder](https://github.com/PythonCharmers/maxentropy/tree/master/notebooks).\n\n### Quickstart guide\nThis is a good place to start: [Loaded die example (scikit-learn estimator API)](https://github.com/PythonCharmers/maxentropy/blob/master/notebooks/Loaded%20die%20example%20-%20skmaxent.ipynb)\n\n## History\nThis package previously lived in SciPy \n(http://scipy.org) as ``scipy.maxentropy`` from versions v0.5 to v0.10.\nIt was under-maintained and removed from SciPy v0.11. It has since been\nresurrected and refactored to use the scikit-learn Estimator inteface.\n\n## Copyright\n(c) Ed Schofield, 2003-2019", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/PythonCharmers/maxentropy.git", "keywords": "maximum-entropy minimum-divergence kullback-leibler-divergence KL-divergence bayesian-inference bayes scikit-learn sklearn prior prior-distribution", "license": "BSD", "maintainer": "", "maintainer_email": "", "name": "maxentropy", "package_url": "https://pypi.org/project/maxentropy/", "platform": "", "project_url": "https://pypi.org/project/maxentropy/", "project_urls": {"Homepage": "https://github.com/PythonCharmers/maxentropy.git"}, "release_url": "https://pypi.org/project/maxentropy/0.3.0/", "requires_dist": null, "requires_python": ">=3.3", "summary": "Maximum entropy and minimum divergence models in Python", "version": "0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>maxentropy: Maximum entropy and minimum divergence models in Python</h1>\n<h2>Purpose</h2>\n<p>This package helps you to construct a probability distribution\n(Bayesian prior) from prior information that you encode as\ngeneralized moment constraints.</p>\n<p>You can use it to either:</p>\n<ol>\n<li>\n<p>find the flattest distribution that meets your constraints, using the\nmaximum entropy principle (discrete distributions only)</p>\n</li>\n<li>\n<p>or find the \"closest\" model to a given prior model (in a KL divergence\nsense) that also satisfies your additional constraints.</p>\n</li>\n</ol>\n<h2>Background</h2>\n<p>The maximum entropy principle has been shown [Cox 1982, Jaynes 2003] to be the unique consistent approach to\nconstructing a discrete probability distribution from prior information that is available as \"testable information\".</p>\n<p>If the constraints have the form of linear moment constraints, then\nthe principle gives rise to a unique probability distribution of\n<strong>exponential form</strong>. Most well-known probability distributions are\nspecial cases of maximum entropy distributions. This includes\nuniform, geometric, exponential, Pareto, normal, von Mises, Cauchy,\nand others: see\n<a href=\"https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution\" rel=\"nofollow\">here</a>.</p>\n<h2>Examples: constructing a prior subject to known constraints</h2>\n<p>See the <a href=\"https://github.com/PythonCharmers/maxentropy/tree/master/notebooks\" rel=\"nofollow\">notebooks folder</a>.</p>\n<h3>Quickstart guide</h3>\n<p>This is a good place to start: <a href=\"https://github.com/PythonCharmers/maxentropy/blob/master/notebooks/Loaded%20die%20example%20-%20skmaxent.ipynb\" rel=\"nofollow\">Loaded die example (scikit-learn estimator API)</a></p>\n<h2>History</h2>\n<p>This package previously lived in SciPy\n(<a href=\"http://scipy.org\" rel=\"nofollow\">http://scipy.org</a>) as <code>scipy.maxentropy</code> from versions v0.5 to v0.10.\nIt was under-maintained and removed from SciPy v0.11. It has since been\nresurrected and refactored to use the scikit-learn Estimator inteface.</p>\n<h2>Copyright</h2>\n<p>(c) Ed Schofield, 2003-2019</p>\n\n          </div>"}, "last_serial": 5648228, "releases": {"0.2.3": [{"comment_text": "", "digests": {"md5": "8e299e5a55e720340987fbb7c5f92c00", "sha256": "775283cd9ed2f15b8b7b27c1d95e6170937024513e1a078e293b1ecb55ff6ef7"}, "downloads": -1, "filename": "maxentropy-0.2.3.tar.gz", "has_sig": false, "md5_digest": "8e299e5a55e720340987fbb7c5f92c00", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 60718, "upload_time": "2017-08-01T11:15:11", "upload_time_iso_8601": "2017-08-01T11:15:11.677187Z", "url": "https://files.pythonhosted.org/packages/7f/43/c1cb7bc7a957fc3ed8d795ada1f80848ab43e50392a645631fe60d766dff/maxentropy-0.2.3.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "fec218a23e423ab2496e4b3b45b87dd5", "sha256": "ac00af89312eb910e19e0911cdd2922aee84b2a46c55ea16e64f0c64ed14fb9c"}, "downloads": -1, "filename": "maxentropy-0.3.0.tar.gz", "has_sig": false, "md5_digest": "fec218a23e423ab2496e4b3b45b87dd5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.3", "size": 47585, "upload_time": "2019-08-08T05:10:19", "upload_time_iso_8601": "2019-08-08T05:10:19.907957Z", "url": "https://files.pythonhosted.org/packages/27/65/aba65609f961a6e6cef14c9d2cc81445c973c9c51317aa5cee2c4213cc09/maxentropy-0.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "fec218a23e423ab2496e4b3b45b87dd5", "sha256": "ac00af89312eb910e19e0911cdd2922aee84b2a46c55ea16e64f0c64ed14fb9c"}, "downloads": -1, "filename": "maxentropy-0.3.0.tar.gz", "has_sig": false, "md5_digest": "fec218a23e423ab2496e4b3b45b87dd5", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.3", "size": 47585, "upload_time": "2019-08-08T05:10:19", "upload_time_iso_8601": "2019-08-08T05:10:19.907957Z", "url": "https://files.pythonhosted.org/packages/27/65/aba65609f961a6e6cef14c9d2cc81445c973c9c51317aa5cee2c4213cc09/maxentropy-0.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:57:08 2020"}