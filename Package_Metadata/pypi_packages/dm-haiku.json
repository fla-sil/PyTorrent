{"info": {"author": "DeepMind", "author_email": "haiku-dev-os@google.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Mathematics", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# Haiku: [Sonnet](https://github.com/deepmind/sonnet) for [JAX](https://github.com/google/jax)\n\n[**Overview**](#overview)\n| [**Why Haiku?**](#why-haiku)\n| [**Quickstart**](#quickstart)\n| [**Installation**](#installation)\n| [**Examples**](https://github.com/deepmind/dm-haiku/tree/master/examples/)\n| [**User manual**](#user-manual)\n| [**Documentation**](https://dm-haiku.readthedocs.io/)\n| [**Citing Haiku**](#citing-haiku)\n\n![pytest](https://github.com/deepmind/dm-haiku/workflows/pytest/badge.svg)\n\n## What is Haiku?\n\nHaiku is a simple neural network library for\n[JAX](https://github.com/google/jax) developed by some of the\nauthors of [Sonnet](https://github.com/deepmind/sonnet), a neural network\nlibrary for [TensorFlow](https://github.com/tensorflow/tensorflow).\n\n**Disambiguation:** if you are looking for Haiku the operating system then\nplease see https://haiku-os.org/.\n\nNOTE: Haiku is currently **beta**. A number of researchers have tested Haiku\nfor several months and have reproduced a number of experiments at scale. Please\nfeel free to use Haiku, and\n[let us know](https://github.com/deepmind/dm-haiku/issues) if you have issues!\n\n## Overview\n\n[JAX](https://github.com/google/jax) is a numerical computing library that\ncombines NumPy, automatic differentiation, and first-class GPU/TPU support.\n\nHaiku is a simple neural network library for JAX that enables users to use\nfamiliar **object-oriented programming models** while allowing full access to\nJAX's pure function transformations.\n\nHaiku provides two core tools: a module abstraction, `hk.Module`, and a simple\nfunction transformation, `hk.transform`.\n\n`hk.Module`s are Python objects that hold references to their own parameters,\nother modules, and methods that apply functions on user inputs.\n\n`hk.transform` turns functions that use these object-oriented, functionally\n\"impure\" modules into pure functions that can be used with `jax.jit`,\n`jax.grad`, `jax.pmap`, etc.\n\n## Why Haiku?\n\nThere are a number of neural network libraries for JAX. Why should you choose\nHaiku?\n\n### Haiku has been tested by researchers at DeepMind at scale.\n\n- DeepMind has reproduced a number of experiments in Haiku and JAX with relative\n  ease. These include large-scale results in image and language processing,\n  generative models, and reinforcement learning.\n\n### Haiku is a library, not a framework.\n\n- Haiku is designed to make specific things simpler: managing model parameters\n  and other model state.\n- Haiku can be expected to compose with other libraries and work well with the\n  rest of JAX.\n- Haiku otherwise is designed to get out of your way - it does not define custom\n  optimizers, checkpointing formats, or replication APIs.\n\n### Haiku does not reinvent the wheel.\n\n- Haiku builds on the programming model and APIs of Sonnet, a neural network\n  library with near universal adoption at DeepMind. It preserves Sonnet's\n  `Module`-based programming model for state management while retaining access\n  to JAX's function transformations.\n- Haiku APIs and abstractions are as close as reasonable to Sonnet. Many users\n  have found Sonnet to be a productive programming model in TensorFlow; Haiku\n  enables the same experience in JAX.\n\n### Transitioning to Haiku is easy.\n\n- By design, transitioning from TensorFlow and Sonnet to JAX and Haiku is easy.\n- Outside of new features (e.g. `hk.transform`), Haiku aims to match the API of\n  Sonnet 2. Modules, methods, argument names, defaults, and initialization\n  schemes should match.\n\n### Haiku makes other aspects of JAX simpler.\n\n- Haiku offers a trivial model for working with random numbers. Within a\n  transformed function, `hk.next_rng_key()` returns a unique rng key.\n- These unique keys are deterministically derived from an initial random key\n  passed into the top-level transformed function, and are thus safe to use with\n  JAX program transformations.\n\n## Quickstart\n\nLet's take a look at an example neural network and loss function.\n\n```python\nimport haiku as hk\nimport jax.numpy as jnp\n\ndef softmax_cross_entropy(logits, labels):\n  one_hot = hk.one_hot(labels, logits.shape[-1])\n  return -jnp.sum(jax.nn.log_softmax(logits) * one_hot, axis=-1)\n\ndef loss_fn(images, labels):\n  mlp = hk.Sequential([\n      hk.Linear(300), jax.nn.relu,\n      hk.Linear(100), jax.nn.relu,\n      hk.Linear(10),\n  ])\n  logits = mlp(images)\n  return jnp.mean(softmax_cross_entropy(logits, labels))\n\nloss_obj = hk.transform(loss_fn)\n```\n\n`hk.transform` allows us to turn this function into a pair of pure functions:\n`init` and `apply`. All JAX transformations (e.g. `jax.grad`) require you to pass\nin a pure function for correct behaviour. Haiku makes it easy to write them.\n\nThe `init` function returned by `hk.transform` allows you to **collect** the\ninitial value of any parameters in the network. Haiku does this by running your\nfunction, keeping track of any parameters requested through `hk.get_parameter`\nand returning them to you:\n\n```python\n# Initial parameter values are typically random. In JAX you need a key in order\n# to generate random numbers and so Haiku requires you to pass one in.\nrng = jax.random.PRNGKey(42)\n\n# `init` runs your function, as such we need an example input. Typically you can\n# pass \"dummy\" inputs (e.g. ones of the same shape and dtype) since initialization\n# is not usually data dependent.\nimages, labels = next(input_dataset)\n\n# The result of `init` is a nested data structure of all the parameters in your\n# network. You can pass this into `apply`.\nparams = loss_obj.init(rng, images, labels)\n```\n\nThe `params` object is designed for you to inspect and manipulate. It is a\nmapping of module name to module parameters, where a module parameter is a mapping\nof parameter name to parameter value. For example:\n\n```\n{'linear': {'b': ndarray(..., shape=(1000,), dtype=float32),\n            'w': ndarray(..., shape=(28, 1000), dtype=float32)},\n 'linear_1': {'b': ndarray(..., shape=(100,), dtype=float32),\n              'w': ndarray(..., shape=(1000, 100), dtype=float32)},\n 'linear_2': {'b': ndarray(..., shape=(10,), dtype=float32),\n              'w': ndarray(..., shape=(100, 10), dtype=float32)}}\n```\n\nThe `apply` function allows you to **inject** parameter values into your\nfunction. Whenever `hk.get_parameter` is called the value returned will come\nfrom the `params` you provide as input to `apply`:\n\n```python\nloss = loss_obj.apply(params, images, labels)\n```\n\nSince `apply` is a pure function we can pass it to `jax.grad` (or any of JAX's\nother transforms):\n\n```python\ngrads = jax.grad(loss_obj.apply)(params, images, labels)\n```\n\nFinally, we put this all together into a simple training loop:\n\n```python\ndef sgd(param, update):\n  return param - 0.01 * update\n\nfor images, labels in input_dataset:\n  grads = jax.grad(loss_obj.apply)(params, images, labels)\n  params = jax.tree_multimap(sgd, params, grads)\n```\n\nHere we used `jax.tree_multimap` to apply the `sgd` function across all matching\nentries in `params` and `grads`. The result has the same structure as the previous\n`params` and can again be used with `apply`.\n\nFor more, see our\n[examples directory](https://github.com/deepmind/dm-haiku/tree/master/examples/).\nThe\n[MNIST example](https://github.com/deepmind/dm-haiku/tree/master/examples/mnist.py)\nis a good place to start.\n\n## Installation\n\nHaiku is written in pure Python, but depends on C++ code via JAX.\n\nBecause JAX installation is different depending on your CUDA version, Haiku does\nnot list JAX as a dependency in `requirements.txt`.\n\nFirst, follow [these instructions](https://github.com/google/jax#installation)\nto install JAX with the relevant accelerator support.\n\nThen, install Haiku using pip:\n\n```bash\n$ pip install git+https://github.com/deepmind/dm-haiku\n```\n\nOur examples rely on additional libraries (e.g. [bsuite](https://github.com/deepmind/bsuite)). You can install the full set of additional requirements using pip:\n\n```bash\n$ pip install -r examples/requirements.txt\n```\n\n## User manual\n\n### Writing your own modules\n\nIn Haiku, all modules are a subclass of `hk.Module`. You can implement any\nmethod you like (nothing is special-cased), but typically modules implement\n`__init__` and `__call__`.\n\nLet's work through implementing a linear layer:\n\n```python\nclass MyLinear(hk.Module):\n\n  def __init__(self, output_size, name=None):\n    super(MyLinear, self).__init__(name=name)\n    self.output_size = output_size\n\n  def __call__(self, x):\n    j, k = x.shape[-1], self.output_size\n    w_init = hk.initializers.TruncatedNormal(1. / np.sqrt(j))\n    w = hk.get_parameter(\"w\", shape=[j, k], dtype=x.dtype, init=w_init)\n    b = hk.get_parameter(\"b\", shape=[k], dtype=x.dtype, init=jnp.zeros)\n    return jnp.dot(x, w) + b\n```\n\nAll modules have a name. When no `name` argument is passed to the module, its\nname is inferred from the name of the Python class (for example `MyLinear`\nbecomes `my_linear`). Modules can have named parameters that are accessed\nusing `hk.get_parameter(param_name, ...)`. We use this API (rather than just\nusing object properties) so that we can convert your code into a pure function\nusing `hk.transform`.\n\nWhen using modules you need to define functions and transform them into a pair\nof pure functions using `hk.transform`. See our [quickstart](#quickstart) for\nmore details about the functions returned from `transform`:\n\n```python\ndef forward_fn(x):\n  model = MyLinear(10)\n  return model(x)\n\n# Turn `forward_fn` into an object with `init` and `apply` methods.\nforward = hk.transform(forward_fn)\n\nx = jnp.ones([1, 1])\n\n# When we run `forward.init`, Haiku will run `forward(x)` and collect initial\n# parameter values. Haiku requires you pass a RNG key to `init`, since parameters\n# are typically initialized randomly:\nkey = hk.PRNGSequence(42)\nparams = forward.init(next(key), x)\n\n# When we run `forward.apply`, Haiku will run `forward(x)` and inject parameter\n# values from the `params` that are passed as the first argument. We do not require\n# an RNG key by default since models are deterministic. You can (of course!) change\n# this using `hk.transform(f, apply_rng=True)` if you prefer:\ny = forward.apply(params, x)\n```\n\n### Working with stochastic models\n\nSome models may require random sampling as part of the computation.\nFor example, in variational autoencoders with the reparametrization trick,\na random sample from the standard normal distribution is needed.\nThe main hurdle in making this work with JAX is in management of PRNG keys.\n\nIn Haiku we provide a simple API for maintaining a PRNG key sequence associated\nwith modules: `hk.next_rng_key()` (or `next_rng_keys()` for multiple keys).\nIn order to use this functionality you need to specify `apply_rng=True`\nargument on the `hk.transform` call:\n\n```python\nclass Dropout(hk.Module):\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    rng_key = hk.next_rng_key()\n    p = jax.random.bernoulli(rng_key, 1.0 - self.rate, shape=x.shape)\n    return x * p / (1.0 - self.rate)\n\nforward = hk.transform(lambda x: VAE()(x), apply_rng=True)\n\nrng_key1, rng_key2 = jax.random.split(jax.random.PRNGKey(42), 2)\n\nparams = forward.init(rng_key1, x)\nprediction = forward.apply(params, rng_key2, x)\n```\n\nFor a more complete look at working with stochastic models, please see our\n[VAE example](https://github.com/deepmind/dm-haiku/tree/master/examples/vae.py).\n\n**Note:** `hk.next_rng_key()` is not functionally pure which means you should\navoid using it alongside JAX transformations which are inside `hk.transform`.\nFor more information and possible workarounds, please consult the docs on\n[Haiku transforms](https://dm-haiku.readthedocs.io/en/latest/transforms.html).\n\n### Working with non-trainable state\n\nSome models may want to maintain some internal, mutable state. For example, in\nbatch normalization a moving average of values encountered during training is\nmaintained.\n\nIn Haiku we provide a simple API for maintaining mutable state that is\nassociated with modules: `hk.set_state` and `hk.get_state`. When using these\nfunctions you need to transform your function using `hk.transform_with_state`\nsince the signature of the returned pair of functions is different:\n\n```python\ndef forward(x, is_training):\n  net = hk.nets.ResNet50(1000)\n  return net(x, is_training)\n\nforward = hk.transform_with_state(forward)\n\n# The `init` function now returns parameters **and** state. State contains\n# anything that was created using `hk.set_state`. The structure is the same as\n# params (e.g. it is a per-module mapping of named values).\nparams, state = forward.init(rng, x, is_training=True)\n\n# The apply function now takes both params **and** state. Additionally it will\n# return updated values for state. In the resnet example this will be the\n# updated values for moving averages used in the batch norm layers.\nlogits, state = forward.apply(params, state, rng, x, is_training=True)\n```\n\nIf you forget to use `hk.transform_with_state` don't worry, we will print a\nclear error pointing you to `hk.transform_with_state` rather than silently\ndropping your state.\n\n### Distributed training with `jax.pmap`\n\nThe pure functions returned from `hk.transform` (or `hk.transform_with_state`)\nare fully compatible with `jax.pmap`. For more details on SPMD programming with\n`jax.pmap`,\n[look here](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap).\n\nOne common use of `jax.pmap` with Haiku is for data-parallel training on many\naccelerators, potentially across multiple hosts. With Haiku, that might look\nlike this:\n\n```python\ndef loss_fn(inputs, labels):\n  logits = hk.nets.MLP([8, 4, 2])(x)\n  return jnp.mean(softmax_cross_entropy(logits, labels))\n\nloss_obj = hk.transform(loss_fn)\n\n# Initialize the model on a single device.\nrng = jax.random.PRNGKey(428)\nsample_image, sample_label = next(input_dataset)\nparams = loss_obj.init(rng, sample_image, sample_label)\n\n# Replicate params onto all devices.\nnum_devices = jax.local_device_count()\nparams = jax.tree_util.tree_map(lambda x: np.stack([x] * num_devices), params)\n\ndef make_superbatch():\n  \"\"\"Constructs a superbatch, i.e. one batch of data per device.\"\"\"\n  # Get N batches, then split into list-of-images and list-of-labels.\n  superbatch = [next(input_dataset) for _ in range(num_devices)]\n  superbatch_images, superbatch_labels = zip(*superbatch)\n  # Stack the superbatches to be one array with a leading dimension, rather than\n  # a python list. This is what `jax.pmap` expects as input.\n  superbatch_images = np.stack(superbatch_images)\n  superbatch_labels = np.stack(superbatch_labels)\n  return superbatch_images, superbatch_labels\n\ndef update(params, inputs, labels, axis_name='i'):\n  \"\"\"Updates params based on performance on inputs and labels.\"\"\"\n  grads = jax.grad(loss_obj.apply)(params, inputs, labels)\n  # Take the mean of the gradients across all data-parallel replicas.\n  grads = jax.lax.pmean(grads, axis_name)\n  # Update parameters using SGD or Adam or ...\n  new_params = my_update_rule(params, grads)\n  return new_params\n\n# Run several training updates.\nfor _ in range(10):\n  superbatch_images, superbatch_labels = make_superbatch()\n  params = jax.pmap(update, axis_name='i')(params, superbatch_images,\n                                           superbatch_labels)\n```\n\nFor a more complete look at distributed Haiku training, take a look at our\n[ResNet-50 on ImageNet example](https://github.com/deepmind/dm-haiku/tree/master/examples/imagenet/).\n\n## Citing Haiku\n\nTo cite this repository:\n\n```\n@software{haiku2020github,\n  author = {Tom Hennigan and Trevor Cai and Tamara Norman and Igor Babuschkin},\n  title = {{H}aiku: {S}onnet for {JAX}},\n  url = {http://github.com/deepmind/dm-haiku},\n  version = {0.0.1b0},\n  year = {2020},\n}\n```\n\nIn this bibtex entry, the version number is intended to be from\n[`haiku/__init__.py`](https://github.com/deepmind/dm-haiku/blob/master/haiku/__init__.py),\nand the year corresponds to the project's open-source release.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/deepmind/dm-haiku", "keywords": "", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "dm-haiku", "package_url": "https://pypi.org/project/dm-haiku/", "platform": "", "project_url": "https://pypi.org/project/dm-haiku/", "project_urls": {"Homepage": "https://github.com/deepmind/dm-haiku"}, "release_url": "https://pypi.org/project/dm-haiku/0.0.1b0/", "requires_dist": ["absl-py (>=0.7.1)", "numpy (>=1.18.0)", "six (>=1.12.0)", "dm-tree (>=0.1.1)", "wrapt (>=1.11.1)", "tabulate (>=0.7.5)", "jax (>=0.1.55) ; extra == 'jax'", "jaxlib (>=0.1.37) ; extra == 'jaxlib'"], "requires_python": "", "summary": "Haiku is a library for building neural networks in JAX.", "version": "0.0.1b0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Haiku: <a href=\"https://github.com/deepmind/sonnet\" rel=\"nofollow\">Sonnet</a> for <a href=\"https://github.com/google/jax\" rel=\"nofollow\">JAX</a></h1>\n<p><a href=\"#overview\" rel=\"nofollow\"><strong>Overview</strong></a>\n| <a href=\"#why-haiku\" rel=\"nofollow\"><strong>Why Haiku?</strong></a>\n| <a href=\"#quickstart\" rel=\"nofollow\"><strong>Quickstart</strong></a>\n| <a href=\"#installation\" rel=\"nofollow\"><strong>Installation</strong></a>\n| <a href=\"https://github.com/deepmind/dm-haiku/tree/master/examples/\" rel=\"nofollow\"><strong>Examples</strong></a>\n| <a href=\"#user-manual\" rel=\"nofollow\"><strong>User manual</strong></a>\n| <a href=\"https://dm-haiku.readthedocs.io/\" rel=\"nofollow\"><strong>Documentation</strong></a>\n| <a href=\"#citing-haiku\" rel=\"nofollow\"><strong>Citing Haiku</strong></a></p>\n<p><img alt=\"pytest\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/057acd16e5e24ba2a5fd951377467d394e9c3547/68747470733a2f2f6769746875622e636f6d2f646565706d696e642f646d2d6861696b752f776f726b666c6f77732f7079746573742f62616467652e737667\"></p>\n<h2>What is Haiku?</h2>\n<p>Haiku is a simple neural network library for\n<a href=\"https://github.com/google/jax\" rel=\"nofollow\">JAX</a> developed by some of the\nauthors of <a href=\"https://github.com/deepmind/sonnet\" rel=\"nofollow\">Sonnet</a>, a neural network\nlibrary for <a href=\"https://github.com/tensorflow/tensorflow\" rel=\"nofollow\">TensorFlow</a>.</p>\n<p><strong>Disambiguation:</strong> if you are looking for Haiku the operating system then\nplease see <a href=\"https://haiku-os.org/\" rel=\"nofollow\">https://haiku-os.org/</a>.</p>\n<p>NOTE: Haiku is currently <strong>beta</strong>. A number of researchers have tested Haiku\nfor several months and have reproduced a number of experiments at scale. Please\nfeel free to use Haiku, and\n<a href=\"https://github.com/deepmind/dm-haiku/issues\" rel=\"nofollow\">let us know</a> if you have issues!</p>\n<h2>Overview</h2>\n<p><a href=\"https://github.com/google/jax\" rel=\"nofollow\">JAX</a> is a numerical computing library that\ncombines NumPy, automatic differentiation, and first-class GPU/TPU support.</p>\n<p>Haiku is a simple neural network library for JAX that enables users to use\nfamiliar <strong>object-oriented programming models</strong> while allowing full access to\nJAX's pure function transformations.</p>\n<p>Haiku provides two core tools: a module abstraction, <code>hk.Module</code>, and a simple\nfunction transformation, <code>hk.transform</code>.</p>\n<p><code>hk.Module</code>s are Python objects that hold references to their own parameters,\nother modules, and methods that apply functions on user inputs.</p>\n<p><code>hk.transform</code> turns functions that use these object-oriented, functionally\n\"impure\" modules into pure functions that can be used with <code>jax.jit</code>,\n<code>jax.grad</code>, <code>jax.pmap</code>, etc.</p>\n<h2>Why Haiku?</h2>\n<p>There are a number of neural network libraries for JAX. Why should you choose\nHaiku?</p>\n<h3>Haiku has been tested by researchers at DeepMind at scale.</h3>\n<ul>\n<li>DeepMind has reproduced a number of experiments in Haiku and JAX with relative\nease. These include large-scale results in image and language processing,\ngenerative models, and reinforcement learning.</li>\n</ul>\n<h3>Haiku is a library, not a framework.</h3>\n<ul>\n<li>Haiku is designed to make specific things simpler: managing model parameters\nand other model state.</li>\n<li>Haiku can be expected to compose with other libraries and work well with the\nrest of JAX.</li>\n<li>Haiku otherwise is designed to get out of your way - it does not define custom\noptimizers, checkpointing formats, or replication APIs.</li>\n</ul>\n<h3>Haiku does not reinvent the wheel.</h3>\n<ul>\n<li>Haiku builds on the programming model and APIs of Sonnet, a neural network\nlibrary with near universal adoption at DeepMind. It preserves Sonnet's\n<code>Module</code>-based programming model for state management while retaining access\nto JAX's function transformations.</li>\n<li>Haiku APIs and abstractions are as close as reasonable to Sonnet. Many users\nhave found Sonnet to be a productive programming model in TensorFlow; Haiku\nenables the same experience in JAX.</li>\n</ul>\n<h3>Transitioning to Haiku is easy.</h3>\n<ul>\n<li>By design, transitioning from TensorFlow and Sonnet to JAX and Haiku is easy.</li>\n<li>Outside of new features (e.g. <code>hk.transform</code>), Haiku aims to match the API of\nSonnet 2. Modules, methods, argument names, defaults, and initialization\nschemes should match.</li>\n</ul>\n<h3>Haiku makes other aspects of JAX simpler.</h3>\n<ul>\n<li>Haiku offers a trivial model for working with random numbers. Within a\ntransformed function, <code>hk.next_rng_key()</code> returns a unique rng key.</li>\n<li>These unique keys are deterministically derived from an initial random key\npassed into the top-level transformed function, and are thus safe to use with\nJAX program transformations.</li>\n</ul>\n<h2>Quickstart</h2>\n<p>Let's take a look at an example neural network and loss function.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">haiku</span> <span class=\"k\">as</span> <span class=\"nn\">hk</span>\n<span class=\"kn\">import</span> <span class=\"nn\">jax.numpy</span> <span class=\"k\">as</span> <span class=\"nn\">jnp</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">softmax_cross_entropy</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">):</span>\n  <span class=\"n\">one_hot</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">one_hot</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">logits</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n  <span class=\"k\">return</span> <span class=\"o\">-</span><span class=\"n\">jnp</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">log_softmax</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">one_hot</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">):</span>\n  <span class=\"n\">mlp</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">([</span>\n      <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">300</span><span class=\"p\">),</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">,</span>\n      <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">,</span>\n      <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">),</span>\n  <span class=\"p\">])</span>\n  <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">mlp</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span> <span class=\"n\">jnp</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">softmax_cross_entropy</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">))</span>\n\n<span class=\"n\">loss_obj</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">loss_fn</span><span class=\"p\">)</span>\n</pre>\n<p><code>hk.transform</code> allows us to turn this function into a pair of pure functions:\n<code>init</code> and <code>apply</code>. All JAX transformations (e.g. <code>jax.grad</code>) require you to pass\nin a pure function for correct behaviour. Haiku makes it easy to write them.</p>\n<p>The <code>init</code> function returned by <code>hk.transform</code> allows you to <strong>collect</strong> the\ninitial value of any parameters in the network. Haiku does this by running your\nfunction, keeping track of any parameters requested through <code>hk.get_parameter</code>\nand returning them to you:</p>\n<pre><span class=\"c1\"># Initial parameter values are typically random. In JAX you need a key in order</span>\n<span class=\"c1\"># to generate random numbers and so Haiku requires you to pass one in.</span>\n<span class=\"n\">rng</span> <span class=\"o\">=</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">PRNGKey</span><span class=\"p\">(</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># `init` runs your function, as such we need an example input. Typically you can</span>\n<span class=\"c1\"># pass \"dummy\" inputs (e.g. ones of the same shape and dtype) since initialization</span>\n<span class=\"c1\"># is not usually data dependent.</span>\n<span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"n\">input_dataset</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># The result of `init` is a nested data structure of all the parameters in your</span>\n<span class=\"c1\"># network. You can pass this into `apply`.</span>\n<span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">loss_obj</span><span class=\"o\">.</span><span class=\"n\">init</span><span class=\"p\">(</span><span class=\"n\">rng</span><span class=\"p\">,</span> <span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">)</span>\n</pre>\n<p>The <code>params</code> object is designed for you to inspect and manipulate. It is a\nmapping of module name to module parameters, where a module parameter is a mapping\nof parameter name to parameter value. For example:</p>\n<pre><code>{'linear': {'b': ndarray(..., shape=(1000,), dtype=float32),\n            'w': ndarray(..., shape=(28, 1000), dtype=float32)},\n 'linear_1': {'b': ndarray(..., shape=(100,), dtype=float32),\n              'w': ndarray(..., shape=(1000, 100), dtype=float32)},\n 'linear_2': {'b': ndarray(..., shape=(10,), dtype=float32),\n              'w': ndarray(..., shape=(100, 10), dtype=float32)}}\n</code></pre>\n<p>The <code>apply</code> function allows you to <strong>inject</strong> parameter values into your\nfunction. Whenever <code>hk.get_parameter</code> is called the value returned will come\nfrom the <code>params</code> you provide as input to <code>apply</code>:</p>\n<pre><span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">loss_obj</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">)</span>\n</pre>\n<p>Since <code>apply</code> is a pure function we can pass it to <code>jax.grad</code> (or any of JAX's\nother transforms):</p>\n<pre><span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"p\">(</span><span class=\"n\">loss_obj</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">)(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">)</span>\n</pre>\n<p>Finally, we put this all together into a simple training loop:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">sgd</span><span class=\"p\">(</span><span class=\"n\">param</span><span class=\"p\">,</span> <span class=\"n\">update</span><span class=\"p\">):</span>\n  <span class=\"k\">return</span> <span class=\"n\">param</span> <span class=\"o\">-</span> <span class=\"mf\">0.01</span> <span class=\"o\">*</span> <span class=\"n\">update</span>\n\n<span class=\"k\">for</span> <span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span> <span class=\"ow\">in</span> <span class=\"n\">input_dataset</span><span class=\"p\">:</span>\n  <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"p\">(</span><span class=\"n\">loss_obj</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">)(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">)</span>\n  <span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">tree_multimap</span><span class=\"p\">(</span><span class=\"n\">sgd</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">)</span>\n</pre>\n<p>Here we used <code>jax.tree_multimap</code> to apply the <code>sgd</code> function across all matching\nentries in <code>params</code> and <code>grads</code>. The result has the same structure as the previous\n<code>params</code> and can again be used with <code>apply</code>.</p>\n<p>For more, see our\n<a href=\"https://github.com/deepmind/dm-haiku/tree/master/examples/\" rel=\"nofollow\">examples directory</a>.\nThe\n<a href=\"https://github.com/deepmind/dm-haiku/tree/master/examples/mnist.py\" rel=\"nofollow\">MNIST example</a>\nis a good place to start.</p>\n<h2>Installation</h2>\n<p>Haiku is written in pure Python, but depends on C++ code via JAX.</p>\n<p>Because JAX installation is different depending on your CUDA version, Haiku does\nnot list JAX as a dependency in <code>requirements.txt</code>.</p>\n<p>First, follow <a href=\"https://github.com/google/jax#installation\" rel=\"nofollow\">these instructions</a>\nto install JAX with the relevant accelerator support.</p>\n<p>Then, install Haiku using pip:</p>\n<pre>$ pip install git+https://github.com/deepmind/dm-haiku\n</pre>\n<p>Our examples rely on additional libraries (e.g. <a href=\"https://github.com/deepmind/bsuite\" rel=\"nofollow\">bsuite</a>). You can install the full set of additional requirements using pip:</p>\n<pre>$ pip install -r examples/requirements.txt\n</pre>\n<h2>User manual</h2>\n<h3>Writing your own modules</h3>\n<p>In Haiku, all modules are a subclass of <code>hk.Module</code>. You can implement any\nmethod you like (nothing is special-cased), but typically modules implement\n<code>__init__</code> and <code>__call__</code>.</p>\n<p>Let's work through implementing a linear layer:</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">MyLinear</span><span class=\"p\">(</span><span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n\n  <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">output_size</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n    <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">MyLinear</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"n\">name</span><span class=\"p\">)</span>\n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">output_size</span> <span class=\"o\">=</span> <span class=\"n\">output_size</span>\n\n  <span class=\"k\">def</span> <span class=\"fm\">__call__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"n\">j</span><span class=\"p\">,</span> <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">output_size</span>\n    <span class=\"n\">w_init</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">initializers</span><span class=\"o\">.</span><span class=\"n\">TruncatedNormal</span><span class=\"p\">(</span><span class=\"mf\">1.</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">j</span><span class=\"p\">))</span>\n    <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">get_parameter</span><span class=\"p\">(</span><span class=\"s2\">\"w\"</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">init</span><span class=\"o\">=</span><span class=\"n\">w_init</span><span class=\"p\">)</span>\n    <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">get_parameter</span><span class=\"p\">(</span><span class=\"s2\">\"b\"</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">init</span><span class=\"o\">=</span><span class=\"n\">jnp</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">jnp</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n</pre>\n<p>All modules have a name. When no <code>name</code> argument is passed to the module, its\nname is inferred from the name of the Python class (for example <code>MyLinear</code>\nbecomes <code>my_linear</code>). Modules can have named parameters that are accessed\nusing <code>hk.get_parameter(param_name, ...)</code>. We use this API (rather than just\nusing object properties) so that we can convert your code into a pure function\nusing <code>hk.transform</code>.</p>\n<p>When using modules you need to define functions and transform them into a pair\nof pure functions using <code>hk.transform</code>. See our <a href=\"#quickstart\" rel=\"nofollow\">quickstart</a> for\nmore details about the functions returned from <code>transform</code>:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">forward_fn</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n  <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">MyLinear</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Turn `forward_fn` into an object with `init` and `apply` methods.</span>\n<span class=\"n\">forward</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">forward_fn</span><span class=\"p\">)</span>\n\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">jnp</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># When we run `forward.init`, Haiku will run `forward(x)` and collect initial</span>\n<span class=\"c1\"># parameter values. Haiku requires you pass a RNG key to `init`, since parameters</span>\n<span class=\"c1\"># are typically initialized randomly:</span>\n<span class=\"n\">key</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">PRNGSequence</span><span class=\"p\">(</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n<span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">forward</span><span class=\"o\">.</span><span class=\"n\">init</span><span class=\"p\">(</span><span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">),</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># When we run `forward.apply`, Haiku will run `forward(x)` and inject parameter</span>\n<span class=\"c1\"># values from the `params` that are passed as the first argument. We do not require</span>\n<span class=\"c1\"># an RNG key by default since models are deterministic. You can (of course!) change</span>\n<span class=\"c1\"># this using `hk.transform(f, apply_rng=True)` if you prefer:</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">forward</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n</pre>\n<h3>Working with stochastic models</h3>\n<p>Some models may require random sampling as part of the computation.\nFor example, in variational autoencoders with the reparametrization trick,\na random sample from the standard normal distribution is needed.\nThe main hurdle in making this work with JAX is in management of PRNG keys.</p>\n<p>In Haiku we provide a simple API for maintaining a PRNG key sequence associated\nwith modules: <code>hk.next_rng_key()</code> (or <code>next_rng_keys()</code> for multiple keys).\nIn order to use this functionality you need to specify <code>apply_rng=True</code>\nargument on the <code>hk.transform</code> call:</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">Dropout</span><span class=\"p\">(</span><span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n  <span class=\"k\">def</span> <span class=\"fm\">__call__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">jnp</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">jnp</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">:</span>\n    <span class=\"n\">rng_key</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">next_rng_key</span><span class=\"p\">()</span>\n    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">bernoulli</span><span class=\"p\">(</span><span class=\"n\">rng_key</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span> <span class=\"o\">-</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rate</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">x</span> <span class=\"o\">*</span> <span class=\"n\">p</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mf\">1.0</span> <span class=\"o\">-</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rate</span><span class=\"p\">)</span>\n\n<span class=\"n\">forward</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">VAE</span><span class=\"p\">()(</span><span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"n\">apply_rng</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"n\">rng_key1</span><span class=\"p\">,</span> <span class=\"n\">rng_key2</span> <span class=\"o\">=</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">PRNGKey</span><span class=\"p\">(</span><span class=\"mi\">42</span><span class=\"p\">),</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">forward</span><span class=\"o\">.</span><span class=\"n\">init</span><span class=\"p\">(</span><span class=\"n\">rng_key1</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"n\">prediction</span> <span class=\"o\">=</span> <span class=\"n\">forward</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">rng_key2</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n</pre>\n<p>For a more complete look at working with stochastic models, please see our\n<a href=\"https://github.com/deepmind/dm-haiku/tree/master/examples/vae.py\" rel=\"nofollow\">VAE example</a>.</p>\n<p><strong>Note:</strong> <code>hk.next_rng_key()</code> is not functionally pure which means you should\navoid using it alongside JAX transformations which are inside <code>hk.transform</code>.\nFor more information and possible workarounds, please consult the docs on\n<a href=\"https://dm-haiku.readthedocs.io/en/latest/transforms.html\" rel=\"nofollow\">Haiku transforms</a>.</p>\n<h3>Working with non-trainable state</h3>\n<p>Some models may want to maintain some internal, mutable state. For example, in\nbatch normalization a moving average of values encountered during training is\nmaintained.</p>\n<p>In Haiku we provide a simple API for maintaining mutable state that is\nassociated with modules: <code>hk.set_state</code> and <code>hk.get_state</code>. When using these\nfunctions you need to transform your function using <code>hk.transform_with_state</code>\nsince the signature of the returned pair of functions is different:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">is_training</span><span class=\"p\">):</span>\n  <span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">nets</span><span class=\"o\">.</span><span class=\"n\">ResNet50</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span> <span class=\"n\">net</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">is_training</span><span class=\"p\">)</span>\n\n<span class=\"n\">forward</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">transform_with_state</span><span class=\"p\">(</span><span class=\"n\">forward</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># The `init` function now returns parameters **and** state. State contains</span>\n<span class=\"c1\"># anything that was created using `hk.set_state`. The structure is the same as</span>\n<span class=\"c1\"># params (e.g. it is a per-module mapping of named values).</span>\n<span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"n\">forward</span><span class=\"o\">.</span><span class=\"n\">init</span><span class=\"p\">(</span><span class=\"n\">rng</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">is_training</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># The apply function now takes both params **and** state. Additionally it will</span>\n<span class=\"c1\"># return updated values for state. In the resnet example this will be the</span>\n<span class=\"c1\"># updated values for moving averages used in the batch norm layers.</span>\n<span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"n\">forward</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">state</span><span class=\"p\">,</span> <span class=\"n\">rng</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">is_training</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre>\n<p>If you forget to use <code>hk.transform_with_state</code> don't worry, we will print a\nclear error pointing you to <code>hk.transform_with_state</code> rather than silently\ndropping your state.</p>\n<h3>Distributed training with <code>jax.pmap</code></h3>\n<p>The pure functions returned from <code>hk.transform</code> (or <code>hk.transform_with_state</code>)\nare fully compatible with <code>jax.pmap</code>. For more details on SPMD programming with\n<code>jax.pmap</code>,\n<a href=\"https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap\" rel=\"nofollow\">look here</a>.</p>\n<p>One common use of <code>jax.pmap</code> with Haiku is for data-parallel training on many\naccelerators, potentially across multiple hosts. With Haiku, that might look\nlike this:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">):</span>\n  <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">nets</span><span class=\"o\">.</span><span class=\"n\">MLP</span><span class=\"p\">([</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">])(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span> <span class=\"n\">jnp</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">softmax_cross_entropy</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">))</span>\n\n<span class=\"n\">loss_obj</span> <span class=\"o\">=</span> <span class=\"n\">hk</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">loss_fn</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Initialize the model on a single device.</span>\n<span class=\"n\">rng</span> <span class=\"o\">=</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">PRNGKey</span><span class=\"p\">(</span><span class=\"mi\">428</span><span class=\"p\">)</span>\n<span class=\"n\">sample_image</span><span class=\"p\">,</span> <span class=\"n\">sample_label</span> <span class=\"o\">=</span> <span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"n\">input_dataset</span><span class=\"p\">)</span>\n<span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">loss_obj</span><span class=\"o\">.</span><span class=\"n\">init</span><span class=\"p\">(</span><span class=\"n\">rng</span><span class=\"p\">,</span> <span class=\"n\">sample_image</span><span class=\"p\">,</span> <span class=\"n\">sample_label</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Replicate params onto all devices.</span>\n<span class=\"n\">num_devices</span> <span class=\"o\">=</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">local_device_count</span><span class=\"p\">()</span>\n<span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">tree_util</span><span class=\"o\">.</span><span class=\"n\">tree_map</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">([</span><span class=\"n\">x</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">num_devices</span><span class=\"p\">),</span> <span class=\"n\">params</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">make_superbatch</span><span class=\"p\">():</span>\n  <span class=\"sd\">\"\"\"Constructs a superbatch, i.e. one batch of data per device.\"\"\"</span>\n  <span class=\"c1\"># Get N batches, then split into list-of-images and list-of-labels.</span>\n  <span class=\"n\">superbatch</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"n\">input_dataset</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_devices</span><span class=\"p\">)]</span>\n  <span class=\"n\">superbatch_images</span><span class=\"p\">,</span> <span class=\"n\">superbatch_labels</span> <span class=\"o\">=</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">superbatch</span><span class=\"p\">)</span>\n  <span class=\"c1\"># Stack the superbatches to be one array with a leading dimension, rather than</span>\n  <span class=\"c1\"># a python list. This is what `jax.pmap` expects as input.</span>\n  <span class=\"n\">superbatch_images</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">(</span><span class=\"n\">superbatch_images</span><span class=\"p\">)</span>\n  <span class=\"n\">superbatch_labels</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">(</span><span class=\"n\">superbatch_labels</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span> <span class=\"n\">superbatch_images</span><span class=\"p\">,</span> <span class=\"n\">superbatch_labels</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">update</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">axis_name</span><span class=\"o\">=</span><span class=\"s1\">'i'</span><span class=\"p\">):</span>\n  <span class=\"sd\">\"\"\"Updates params based on performance on inputs and labels.\"\"\"</span>\n  <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"p\">(</span><span class=\"n\">loss_obj</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">)(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">)</span>\n  <span class=\"c1\"># Take the mean of the gradients across all data-parallel replicas.</span>\n  <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">lax</span><span class=\"o\">.</span><span class=\"n\">pmean</span><span class=\"p\">(</span><span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">axis_name</span><span class=\"p\">)</span>\n  <span class=\"c1\"># Update parameters using SGD or Adam or ...</span>\n  <span class=\"n\">new_params</span> <span class=\"o\">=</span> <span class=\"n\">my_update_rule</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span> <span class=\"n\">new_params</span>\n\n<span class=\"c1\"># Run several training updates.</span>\n<span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">):</span>\n  <span class=\"n\">superbatch_images</span><span class=\"p\">,</span> <span class=\"n\">superbatch_labels</span> <span class=\"o\">=</span> <span class=\"n\">make_superbatch</span><span class=\"p\">()</span>\n  <span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">jax</span><span class=\"o\">.</span><span class=\"n\">pmap</span><span class=\"p\">(</span><span class=\"n\">update</span><span class=\"p\">,</span> <span class=\"n\">axis_name</span><span class=\"o\">=</span><span class=\"s1\">'i'</span><span class=\"p\">)(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">superbatch_images</span><span class=\"p\">,</span>\n                                           <span class=\"n\">superbatch_labels</span><span class=\"p\">)</span>\n</pre>\n<p>For a more complete look at distributed Haiku training, take a look at our\n<a href=\"https://github.com/deepmind/dm-haiku/tree/master/examples/imagenet/\" rel=\"nofollow\">ResNet-50 on ImageNet example</a>.</p>\n<h2>Citing Haiku</h2>\n<p>To cite this repository:</p>\n<pre><code>@software{haiku2020github,\n  author = {Tom Hennigan and Trevor Cai and Tamara Norman and Igor Babuschkin},\n  title = {{H}aiku: {S}onnet for {JAX}},\n  url = {http://github.com/deepmind/dm-haiku},\n  version = {0.0.1b0},\n  year = {2020},\n}\n</code></pre>\n<p>In this bibtex entry, the version number is intended to be from\n<a href=\"https://github.com/deepmind/dm-haiku/blob/master/haiku/__init__.py\" rel=\"nofollow\"><code>haiku/__init__.py</code></a>,\nand the year corresponds to the project's open-source release.</p>\n\n          </div>"}, "last_serial": 6894773, "releases": {"0.0.1a0": [{"comment_text": "", "digests": {"md5": "d13166e4d3a16fa85b880364ca354ffb", "sha256": "5ab917a8eee9f8fb328b22e7a89eeda4b7ea351579b5a258a986b787c05e6d4d"}, "downloads": -1, "filename": "dm_haiku-0.0.1a0-py3-none-any.whl", "has_sig": false, "md5_digest": "d13166e4d3a16fa85b880364ca354ffb", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 310752, "upload_time": "2020-02-20T11:16:30", "upload_time_iso_8601": "2020-02-20T11:16:30.526464Z", "url": "https://files.pythonhosted.org/packages/b9/e1/7b879679b07a2040bba8224043abfb644f2003f381a8f5ee460cf1ba6341/dm_haiku-0.0.1a0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "68ee5ce12ca7008601c335126fea0f7f", "sha256": "bc334188ce6d3594f2a59bda454bf822879c2f354e59290936a70dd3cc556047"}, "downloads": -1, "filename": "dm-haiku-0.0.1a0.tar.gz", "has_sig": false, "md5_digest": "68ee5ce12ca7008601c335126fea0f7f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 99894, "upload_time": "2020-02-20T11:16:33", "upload_time_iso_8601": "2020-02-20T11:16:33.612105Z", "url": "https://files.pythonhosted.org/packages/53/b3/7d743ef61d24f51a63b797da79d439dfca2e270150973b87e4badeba4a3d/dm-haiku-0.0.1a0.tar.gz", "yanked": false}], "0.0.1b0": [{"comment_text": "", "digests": {"md5": "272ede580cbf52e0f63a6af0e5b2f97d", "sha256": "3ed2d7c240bedce744e13a409e90819b88285db6b0e0150ee55fecbe021c2cd7"}, "downloads": -1, "filename": "dm_haiku-0.0.1b0-py3-none-any.whl", "has_sig": false, "md5_digest": "272ede580cbf52e0f63a6af0e5b2f97d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 193544, "upload_time": "2020-03-27T09:13:22", "upload_time_iso_8601": "2020-03-27T09:13:22.178782Z", "url": "https://files.pythonhosted.org/packages/ed/da/53e83e19c447d64da8f420104b1340f35e5213842abe81fee3860b0bc619/dm_haiku-0.0.1b0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2d23ff1bfbd2bfb48e3b5ef1c5395112", "sha256": "f19fdaf8281b7fb0048493adfa9ac0e75f494fef6b62d66bc00e7f6f2e1b0f16"}, "downloads": -1, "filename": "dm-haiku-0.0.1b0.tar.gz", "has_sig": false, "md5_digest": "2d23ff1bfbd2bfb48e3b5ef1c5395112", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 121496, "upload_time": "2020-03-27T09:13:23", "upload_time_iso_8601": "2020-03-27T09:13:23.883431Z", "url": "https://files.pythonhosted.org/packages/92/44/7d03a718dc5e29826fa9be0346138e0d9baf2ff72c0bd93f7405925acb36/dm-haiku-0.0.1b0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "272ede580cbf52e0f63a6af0e5b2f97d", "sha256": "3ed2d7c240bedce744e13a409e90819b88285db6b0e0150ee55fecbe021c2cd7"}, "downloads": -1, "filename": "dm_haiku-0.0.1b0-py3-none-any.whl", "has_sig": false, "md5_digest": "272ede580cbf52e0f63a6af0e5b2f97d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 193544, "upload_time": "2020-03-27T09:13:22", "upload_time_iso_8601": "2020-03-27T09:13:22.178782Z", "url": "https://files.pythonhosted.org/packages/ed/da/53e83e19c447d64da8f420104b1340f35e5213842abe81fee3860b0bc619/dm_haiku-0.0.1b0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2d23ff1bfbd2bfb48e3b5ef1c5395112", "sha256": "f19fdaf8281b7fb0048493adfa9ac0e75f494fef6b62d66bc00e7f6f2e1b0f16"}, "downloads": -1, "filename": "dm-haiku-0.0.1b0.tar.gz", "has_sig": false, "md5_digest": "2d23ff1bfbd2bfb48e3b5ef1c5395112", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 121496, "upload_time": "2020-03-27T09:13:23", "upload_time_iso_8601": "2020-03-27T09:13:23.883431Z", "url": "https://files.pythonhosted.org/packages/92/44/7d03a718dc5e29826fa9be0346138e0d9baf2ff72c0bd93f7405925acb36/dm-haiku-0.0.1b0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:51:14 2020"}