{"info": {"author": "cgsdfc", "author_email": "cgsdfc@126.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Text Processing :: Linguistic"], "description": "# ROUGE\nThis is a pure Python implementation of the ROUGE metrics family in the automatic summarization fields\nfollowing the paper *ROUGE: A Package for Automatic Evaluation of Summaries Chin-Yew Lin et al.*.\nIt is an attempt to implement these metrics correctly and elegantly in total Python. It provides the following features:\n- ROUGE-N, ROUGE-L and ROUGE-W are currently supported.\n- Flexible input. For each metrics supported, a sentence level and a summary level variants are provided, which means you\ncan use them in a machine translation context with sentence pairs.\n- Correctness. All the claimed implemented metrics are tested against a non-trivial amount of data, using the plain old perl script\nas a baseline.\n- Self-contained. The total implementation is *one single script*  in *one single package*.\nNo dependency except a Python-3 is required. *No perl script involved.*\n- Fast. At least faster the perl script wrappers.\n- Preprocessing is a total freedom of the client. We establish API on the concept of *sentence*, which is list of tokens\nand *sentences*, which is a list of *sentence*. Preprocessing like *stopword-removal*, *stemming* and *tokenization* is\nleft to client totally.\n- Well documented. Every function is `doctest` powered.\n- Procedural style API. You don't need to instantiate an object. Just call the function that does the right job.\n\n# Usage\nAn example use of calculating `ROUGE-2` on sentence level is provided:\n```python\nfrom rouge.rouge import rouge_n_sentence_level\n\nsummary_sentence = 'the capital of China is Beijing'.split()\nreference_sentence = 'Beijing is the capital of China'.split()\n\n# Calculate ROUGE-2.\nrecall, precision, rouge = rouge_n_sentence_level(summary_sentence, reference_sentence, 2)\nprint('ROUGE-2-R', recall)\nprint('ROUGE-2-P', precision)\nprint('ROUGE-2-F', rouge)\n\n# If you just want the F-measure you can do this:\n*_, rouge = rouge_n_sentence_level(summary_sentence, reference_sentence, 2)  # Requires a Python-3 to use *_.\nprint('ROUGE-2-R', recall)\n\n```\nFor more usage examples, please refer to `example.py`.\n\n# Install\nCurrently not uploaded to PyPi...\n```bash\ngit clone https://github.com/neural-dialogue-metrics/rouge.git\npython ./setup.py install\n```\n\n# Dependencies\nThe code is *only* tested on `python==3.6.2` but it should work with higher version of Python.\nIf you want to run the tests locally, you need to install [Pythonrouge](https://github.com/tagucci/pythonrouge.git), which is a wrapper on the original\nperl script. To install it:\n```bash\n# not using pip\ngit clone https://github.com/tagucci/pythonrouge.git\npython setup.py install\n\n# using pip\npip install git+https://github.com/tagucci/pythonrouge.git\n```\nThen go to the project root directory, run:\n```bash\n# Run doctests.\npython -m doctest ./rouge/*.py -v\n\n# Run unittests.\npython -m unittest -v\n```\nSince the python wrapper is generally slow, the tests takes more than a few minutes to finish.\n\n# Rationals\nIn this section we talk about the rationals for reinventing the wheels or reimplementing the ROUGE metrics in a few words.\n\n## The Complexity\nROUGE is *not* a trivial metric. First of all, it has a lot of variants, `ROUGE-N`, `ROUGE-L`, `ROUGE-W`, `ROUGE-S`, `ROUGE-SU`, `ROUGE-BE`.\nThat's why the author called it *a package*. To implement each of them correctly is not a trivial job.\nSecond, ROUGE has two completely different signatures. By *signature* I mean the shape of the input data. It has both *sentence level* and\n*summary level* defined, sometimes in varying forms. Third, ROUGE can handle multiple references and multiple summaries altogether.\nIt handles multiple references by fixing the summary to a single one and calculating a list of values given a list of references. It then\nuses a *jackknifing procedure* to reduce those values to a scalar. It handles multiple summaries by using a *A-or-B* scheme to reduce the list\nof values produced by the list of summaries to a scalar. For *A-or-B*, `A` means taking average and `B` means taking the best one.\nIf you feel calm after learning these, ROUGE can handle a great deal of preprocessing on the input data and each of them can affect the final score.\nFew projects on Github implement all these functionalities correctly. Some of them don't even realize the problems of signatures.\n\n## The Plain Old Perl Script\nWhile the plain old perl script `ROUGE-1.5.5.pl` implements all these things correctly, its interface is quite discouraging.\nIt has a long array of single-character options with some options affecting the others!\nIt takes the input from a fixed directory format and requires a configure file in XML, making it less usable in the context of\nrapid prototyping and development. Worse still, it *don't* provide an application programmer interface even in perl. You have no way\nto use it programmatically except lauching a process, which is very expensive.\n\n## Our Usage Scenario\nAlthough ROUGE is originated from automatic summarization, we want to use it in a sentence-oriented style.\nThat's why the sentence level API is emphasized in the project. We want to evaluate the average ROUGE score on a large\nnumber of `response-ground_truth` pairs quickly. The wrapper way is not taken since its inefficiency.\n\n## Simplification of the Problem\nThe first simplification we made is to throw away any preprocessing and stick to a general representation of *sentence*.\nA sentence is just a list of strings or tokens. Since then we don't have to implement any tokenizer. In fact we don't\n*need* to because there are a lot of libraries can do that nicely, like `nltk` and `spicy`.\n\nThe second decision is that we don't care about *multiple references and multiple summaries*.\nWe *only* care about *a single reference and a single summary*. Unlike BLEU, multiple references means all the same\nto every variant of ROUGE (almost always Jackknifing). And how to combine scores from different instance of summary is not our interest.\nThey can be implemented in the project however, only as extensions to the core metrics.\n\nAfter making the two decisions, our code can be implemented in a clear and reusable way.\nHopefully, it will also be *extensible* because right now we haven't implement *all* of the ROUGE metrics.\nAnd you know, a ROUGE can never have too many metrics!\n\n# Acknowledgement\nThe test data is taken literally from [sumeval](https://github.com/chakki-works/sumeval.git).\nThe code borrows ideas from both `sumeval` and the `rouge.py` script of [tensorflow/nmt](https://github.com/tensorflow/nmt.git).\nThe API style closely follows that of `nltk.translate.bleu_score`.\n\n# References\n[1] ROUGE: A Package for Automatic Evaluation of Summaries.\n\n[2] ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks.\n\n[3] Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics.\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/neural-dialogue-metrics/rouge.git", "keywords": "NL,CL,MT,natural language processing,computational linguistics,automatic summarization", "license": "LICENCE.txt", "maintainer": "", "maintainer_email": "", "name": "easy-rouge", "package_url": "https://pypi.org/project/easy-rouge/", "platform": "", "project_url": "https://pypi.org/project/easy-rouge/", "project_urls": {"Homepage": "https://github.com/neural-dialogue-metrics/rouge.git"}, "release_url": "https://pypi.org/project/easy-rouge/0.2.2/", "requires_dist": null, "requires_python": "", "summary": "An implementation of ROUGE family metrics for automatic summarization", "version": "0.2.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            # ROUGE<br>This is a pure Python implementation of the ROUGE metrics family in the automatic summarization fields<br>following the paper *ROUGE: A Package for Automatic Evaluation of Summaries Chin-Yew Lin et al.*.<br>It is an attempt to implement these metrics correctly and elegantly in total Python. It provides the following features:<br>- ROUGE-N, ROUGE-L and ROUGE-W are currently supported.<br>- Flexible input. For each metrics supported, a sentence level and a summary level variants are provided, which means you<br>can use them in a machine translation context with sentence pairs.<br>- Correctness. All the claimed implemented metrics are tested against a non-trivial amount of data, using the plain old perl script<br>as a baseline.<br>- Self-contained. The total implementation is *one single script*  in *one single package*.<br>No dependency except a Python-3 is required. *No perl script involved.*<br>- Fast. At least faster the perl script wrappers.<br>- Preprocessing is a total freedom of the client. We establish API on the concept of *sentence*, which is list of tokens<br>and *sentences*, which is a list of *sentence*. Preprocessing like *stopword-removal*, *stemming* and *tokenization* is<br>left to client totally.<br>- Well documented. Every function is `doctest` powered.<br>- Procedural style API. You don't need to instantiate an object. Just call the function that does the right job.<br><br># Usage<br>An example use of calculating `ROUGE-2` on sentence level is provided:<br>```python<br>from rouge.rouge import rouge_n_sentence_level<br><br>summary_sentence = 'the capital of China is Beijing'.split()<br>reference_sentence = 'Beijing is the capital of China'.split()<br><br># Calculate ROUGE-2.<br>recall, precision, rouge = rouge_n_sentence_level(summary_sentence, reference_sentence, 2)<br>print('ROUGE-2-R', recall)<br>print('ROUGE-2-P', precision)<br>print('ROUGE-2-F', rouge)<br><br># If you just want the F-measure you can do this:<br>*_, rouge = rouge_n_sentence_level(summary_sentence, reference_sentence, 2)  # Requires a Python-3 to use *_.<br>print('ROUGE-2-R', recall)<br><br>```<br>For more usage examples, please refer to `example.py`.<br><br># Install<br>Currently not uploaded to PyPi...<br>```bash<br>git clone https://github.com/neural-dialogue-metrics/rouge.git<br>python ./setup.py install<br>```<br><br># Dependencies<br>The code is *only* tested on `python==3.6.2` but it should work with higher version of Python.<br>If you want to run the tests locally, you need to install [Pythonrouge](https://github.com/tagucci/pythonrouge.git), which is a wrapper on the original<br>perl script. To install it:<br>```bash<br># not using pip<br>git clone https://github.com/tagucci/pythonrouge.git<br>python setup.py install<br><br># using pip<br>pip install git+https://github.com/tagucci/pythonrouge.git<br>```<br>Then go to the project root directory, run:<br>```bash<br># Run doctests.<br>python -m doctest ./rouge/*.py -v<br><br># Run unittests.<br>python -m unittest -v<br>```<br>Since the python wrapper is generally slow, the tests takes more than a few minutes to finish.<br><br># Rationals<br>In this section we talk about the rationals for reinventing the wheels or reimplementing the ROUGE metrics in a few words.<br><br>## The Complexity<br>ROUGE is *not* a trivial metric. First of all, it has a lot of variants, `ROUGE-N`, `ROUGE-L`, `ROUGE-W`, `ROUGE-S`, `ROUGE-SU`, `ROUGE-BE`.<br>That's why the author called it *a package*. To implement each of them correctly is not a trivial job.<br>Second, ROUGE has two completely different signatures. By *signature* I mean the shape of the input data. It has both *sentence level* and<br>*summary level* defined, sometimes in varying forms. Third, ROUGE can handle multiple references and multiple summaries altogether.<br>It handles multiple references by fixing the summary to a single one and calculating a list of values given a list of references. It then<br>uses a *jackknifing procedure* to reduce those values to a scalar. It handles multiple summaries by using a *A-or-B* scheme to reduce the list<br>of values produced by the list of summaries to a scalar. For *A-or-B*, `A` means taking average and `B` means taking the best one.<br>If you feel calm after learning these, ROUGE can handle a great deal of preprocessing on the input data and each of them can affect the final score.<br>Few projects on Github implement all these functionalities correctly. Some of them don't even realize the problems of signatures.<br><br>## The Plain Old Perl Script<br>While the plain old perl script `ROUGE-1.5.5.pl` implements all these things correctly, its interface is quite discouraging.<br>It has a long array of single-character options with some options affecting the others!<br>It takes the input from a fixed directory format and requires a configure file in XML, making it less usable in the context of<br>rapid prototyping and development. Worse still, it *don't* provide an application programmer interface even in perl. You have no way<br>to use it programmatically except lauching a process, which is very expensive.<br><br>## Our Usage Scenario<br>Although ROUGE is originated from automatic summarization, we want to use it in a sentence-oriented style.<br>That's why the sentence level API is emphasized in the project. We want to evaluate the average ROUGE score on a large<br>number of `response-ground_truth` pairs quickly. The wrapper way is not taken since its inefficiency.<br><br>## Simplification of the Problem<br>The first simplification we made is to throw away any preprocessing and stick to a general representation of *sentence*.<br>A sentence is just a list of strings or tokens. Since then we don't have to implement any tokenizer. In fact we don't<br>*need* to because there are a lot of libraries can do that nicely, like `nltk` and `spicy`.<br><br>The second decision is that we don't care about *multiple references and multiple summaries*.<br>We *only* care about *a single reference and a single summary*. Unlike BLEU, multiple references means all the same<br>to every variant of ROUGE (almost always Jackknifing). And how to combine scores from different instance of summary is not our interest.<br>They can be implemented in the project however, only as extensions to the core metrics.<br><br>After making the two decisions, our code can be implemented in a clear and reusable way.<br>Hopefully, it will also be *extensible* because right now we haven't implement *all* of the ROUGE metrics.<br>And you know, a ROUGE can never have too many metrics!<br><br># Acknowledgement<br>The test data is taken literally from [sumeval](https://github.com/chakki-works/sumeval.git).<br>The code borrows ideas from both `sumeval` and the `rouge.py` script of [tensorflow/nmt](https://github.com/tensorflow/nmt.git).<br>The API style closely follows that of `nltk.translate.bleu_score`.<br><br># References<br>[1] ROUGE: A Package for Automatic Evaluation of Summaries.<br><br>[2] ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks.<br><br>[3] Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics.<br><br><br>\n          </div>"}, "last_serial": 4968110, "releases": {"0.2.2": [{"comment_text": "", "digests": {"md5": "6d65fa004589c136aa3a35ba75314b35", "sha256": "a5a90125d8f5a5f4d089a812e8721f00a42443cc54abedb394134d8a98cf1c9e"}, "downloads": -1, "filename": "easy_rouge-0.2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "6d65fa004589c136aa3a35ba75314b35", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 22056, "upload_time": "2019-03-21T13:35:18", "upload_time_iso_8601": "2019-03-21T13:35:18.915026Z", "url": "https://files.pythonhosted.org/packages/0c/0a/b7ebb887dac3ece27fffc65bbc7dc0abcf991f2ccce8073126329ce4be8f/easy_rouge-0.2.2-py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "6d65fa004589c136aa3a35ba75314b35", "sha256": "a5a90125d8f5a5f4d089a812e8721f00a42443cc54abedb394134d8a98cf1c9e"}, "downloads": -1, "filename": "easy_rouge-0.2.2-py3-none-any.whl", "has_sig": false, "md5_digest": "6d65fa004589c136aa3a35ba75314b35", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 22056, "upload_time": "2019-03-21T13:35:18", "upload_time_iso_8601": "2019-03-21T13:35:18.915026Z", "url": "https://files.pythonhosted.org/packages/0c/0a/b7ebb887dac3ece27fffc65bbc7dc0abcf991f2ccce8073126329ce4be8f/easy_rouge-0.2.2-py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 00:48:16 2020"}