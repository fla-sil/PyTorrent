{"info": {"author": "nofdev", "author_email": "jiasir@icloud.com", "bugtrack_url": null, "classifiers": [], "description": "# FastForward [![GoDoc](https://godoc.org/github.com/nofdev/fastforward?status.svg)](https://godoc.org/github.com/nofdev/fastforward) [![Build Status](https://travis-ci.org/nofdev/fastforward.svg?branch=master)](https://travis-ci.org/nofdev/fastforward)\nFastForward is an ultimate DevOps platform.\n\n\n## Quick-start provision OpenStack(mitaka) on xenil or trusty\n\n### Requirements\n\n* The OpenStack bare metal hosts are in MAAS environment(recommend)\n* All hosts are two NICs at least(external and internal)\n* We assume that you have ceph installed, the cinder bachend default using ceph, the running instace default to using ceph as it's local storage. About ceph please visit: http://docs.ceph.com/docs/master/rbd/rbd-openstack/ or see the (Option)Ceph Guide below.\n* For resize instance nova user can be login to each compute node via ssh passwordless(include sudo), and all compute nodes need to restart libvirt-bin to enable live migration\n* The FastForward node is the same as ceph-deploy node where can be login to each openstack node passwordless and sudo-passwordless\n* The FastForward node default using ~/.ssh/id_rsa ssh private key to logon remote server\n* You need to restart the `nova-compute`, `cinder-volume` and `glance-api` services to finalize the installation if you have selected the ceph as that backend\n* FastForward support consistency groups for future use but the default LVM and Ceph driver does not support consistency groups yet because the consistency technology is not available at that storage level\n\n### Install FastForward\n\nInstall FastForward on FASTFORWARD-NODE\n\n    pip install fastforward\n\n### Prepare environment\n\nPrepare the OpenStack environment.\n(NOTE) DO NOT setup eth1 in /etc/network/interfaces\n\n    ff --user ubuntu --hosts \\\n    HAPROXY1,\\\n    HAPROXY2,\\\n    CONTROLLER1,\\\n    CONTROLLER2,\\\n    COMPUTE1,\\\n    COMPUTE2,\\\n    COMPUTE3,\\\n    COMPUTE4,\\\n    COMPUTE5,\\\n    COMPUTE6,\\\n    COMPUTE7,\\\n    COMPUTE8,\\\n    COMPUTE9,\\\n    COMPUTE10 \\\n    environment \\\n    prepare-host --public-interface eth1\n\n### MySQL HA\n\nDeploy to CONTROLLER1\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack mysql install\n    ff --user ubuntu --hosts CONTROLLER1 openstack mysql config --wsrep-cluster-address \"gcomm://CONTROLLER1,CONTROLLER2\" --wsrep-node-name=\"galera1\" --wsrep-node-address=\"CONTROLLER1\"\n\nDeploy to CONTROLLER2\n\n    ff --user ubuntu --hosts CONTROLLER2 openstack mysql install\n    ff --user ubuntu --hosts CONTROLLER2 openstack mysql config --wsrep-cluster-address \"gcomm://CONTROLLER1,CONTROLLER2\" --wsrep-node-name=\"galera2\" --wsrep-node-address=\"CONTROLLER2\"\n\nStart the cluster\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack mysql manage --wsrep-new-cluster\n    ff --user ubuntu --hosts CONTROLLER2 openstack mysql manage --start\n    ff --user ubuntu --hosts CONTROLLER1 openstack mysql manage --change-root-password changeme\n\nShow the cluster status\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack mysql manage --show-cluster-status --root-db-pass changeme\n\n\n### HAProxy HA\n\nDeploy to HAPROXY1\n\n    ff --user ubuntu --hosts HAPROXY1 openstack haproxy install\n\nDeploy to HAPROXY2\n\n    ff --user ubuntu --hosts HAPROXY2 openstack haproxy install\n\nGenerate the HAProxy configuration and upload to target hosts(Do not forget to edit the generated configuration)\n\n    ff openstack haproxygen-conf\n    ff --user ubuntu --hosts HAPROXY1,HAPROXY2 openstack haproxy config --upload-conf haproxy.cfg\n\nConfigure Keepalived\n\n    ff --user ubuntu --hosts HAPROXY1 openstack haproxy config --configure-keepalived --router_id lb1 --priority 150 --state MASTER --interface eth0 --vip CONTROLLER_VIP\n    ff --user ubuntu --hosts HAPROXY2 openstack haproxy config --configure-keepalived --router_id lb2 --priority 100 --state SLAVE --interface eth0 --vip CONTROLLER_VIP\n\n### RabbitMQ HA\n\nDeploy to CONTROLLER1 and CONTROLLER2\n\n    ff --user ubuntu --hosts CONTROLLER1,CONTROLLER2 openstack rabbitmq install --erlang-cookie changemechangeme --rabbit-user openstack --rabbit-pass changeme\n\nCreate cluster(Ensure CONTROLLER2 can access CONTROLLER1 via hostname)\n\n    ff --user ubuntu --hosts CONTROLLER2 openstack rabbitmq join-cluster --name rabbit@CONTROLLER1\n\n### Keystone HA\n\nCreate keystone database\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack keystone create-keystone-db --root-db-pass changeme --keystone-db-pass changeme\n\nInstall keystone on CONTROLLER1 and CONTROLLER2\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack keystone install --admin-token changeme --connection mysql+pymysql://keystone:changeme@CONTROLLER_VIP/keystone --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --populate\n    ff --user ubuntu --hosts CONTROLLER2 openstack keystone install --admin-token changeme --connection mysql+pymysql://keystone:changeme@CONTROLLER_VIP/keystone --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211\n\nCreate the service entity and API endpoints\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack keystone create-entity-and-endpoint --os-token changeme --os-url http://CONTROLLER_VIP:35357/v3 --public-endpoint http://CONTROLLER_VIP:5000/v3 --internal-endpoint http://CONTROLLER_VIP:5000/v3 --admin-endpoint http://CONTROLLER_vip:35357/v3\n\nCreate projects, users, and roles\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack keystone create-projects-users-roles --os-token changeme --os-url http://CONTROLLER_VIP:35357/v3 --admin-pass changeme --demo-pass changeme\n\n(OPTION) you will need to create OpenStack client environment scripts\nadmin-openrc.sh\n\n    export OS_PROJECT_DOMAIN_NAME=default\n    export OS_USER_DOMAIN_NAME=default\n    export OS_PROJECT_NAME=admin\n    export OS_TENANT_NAME=admin\n    export OS_USERNAME=admin\n    export OS_PASSWORD=changeme\n    export OS_AUTH_URL=http://CONTROLLER_VIP:35357/v3\n    export OS_IDENTITY_API_VERSION=3\n    export OS_IMAGE_API_VERSION=2\n    export OS_AUTH_VERSION=3\n\ndemo-openrc.sh\n\n    export OS_PROJECT_DOMAIN_NAME=default\n    export OS_USER_DOMAIN_NAME=default\n    export OS_PROJECT_NAME=demo\n    export OS_TENANT_NAME=demo\n    export OS_USERNAME=demo\n    export OS_PASSWORD=changeme\n    export OS_AUTH_URL=http://CONTROLLER_VIP:5000/v3\n    export OS_IDENTITY_API_VERSION=3\n    export OS_IMAGE_API_VERSION=2\n    export OS_AUTH_VERSION=3\n\n### Glance HA\n\nCreate glance database\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack glance create-glance-db --root-db-pass changeme --glance-db-pass changeme\n\nCreate service credentials\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack glance create-service-credentials --os-password changeme --os-auth-url http://CONTROLLER_VIP:35357/v3 --glance-pass changeme --public-endpoint http://CONTROLLER_VIP:9292 --internal-endpoint http://CONTROLLER_VIP:9292 --admin-endpoint http://CONTROLLER_VIP:9292\n\nInstall glance on CONTROLLER1 and CONTROLLER2\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack glance install --connection mysql+pymysql://glance:GLANCE_PASS@CONTROLLER_VIP/glance --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --glance-pass changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --populate\n    ff --user ubuntu --hosts CONTROLLER2 openstack glance install --connection mysql+pymysql://glance:GLANCE_PASS@CONTROLLER_VIP/glance --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --glance-pass changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211\n\n### Nova HA\n\nCreate nova database\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack nova create-nova-db --root-db-pass changeme --nova-db-pass changeme\n\nCreate service credentials\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack nova create-service-credentials --os-password changeme --os-auth-url http://CONTROLLER_VIP:35357/v3 --nova-pass changeme --public-endpoint 'http://CONTROLLER_VIP:8774/v2.1/%\\(tenant_id\\)s' --internal-endpoint 'http://CONTROLLER_VIP:8774/v2.1/%\\(tenant_id\\)s' --admin-endpoint 'http://CONTROLLER_VIP:8774/v2.1/%\\(tenant_id\\)s'\n\nInstall nova on CONTROLLER1\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack nova install --connection mysql+pymysql://nova:NOVA_PASS@CONTROLLER_VIP/nova --api-connection mysql+pymysql://nova:NOVA_PASS@CONTROLLER_VIP/nova_api --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --nova-pass changeme --my-ip MANAGEMENT_IP --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --glance-api-servers http://CONTROLLER_VIP:9292 --neutron-endpoint http://CONTROLLER_VIP:9696 --neutron-pass changeme --metadata-proxy-shared-secret changeme --populate\n\nInstall nova on CONTROLLER2\n\n    ff --user ubuntu --hosts CONTROLLER2 openstack nova install --connection mysql+pymysql://nova:NOVA_PASS@CONTROLLER_VIP/nova --api-connection mysql+pymysql://nova:NOVA_PASS@CONTROLLER_VIP/nova_api --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --nova-pass changeme --my-ip MANAGEMENT_IP --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --glance-api-servers http://CONTROLLER_VIP:9292 --neutron-endpoint http://CONTROLLER_VIP:9696 --neutron-pass changeme --metadata-proxy-shared-secret changeme\n\n### Nova Compute\n\nAdd nova computes(use `uuidgen` to generate the ceph uuid)\n\n    ff --user ubuntu --hosts COMPUTE1 openstack nova-compute install --my-ip MANAGEMENT_IP --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --nova-pass changeme --novncproxy-base-url http://CONTROLLER_VIP:6080/vnc_auto.html --glance-api-servers http://CONTROLLER_VIP:9292 --neutron-endpoint http://CONTROLLER_VIP:9696 --neutron-pass changeme --rbd-secret-uuid changeme-changeme-changeme-changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211\n    ff --user ubuntu --hosts COMPUTE2 openstack nova-compute install --my-ip MANAGEMENT_IP --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --nova-pass changeme --novncproxy-base-url http://CONTROLLER_VIP:6080/vnc_auto.html --glance-api-servers http://CONTROLLER_VIP:9292 --neutron-endpoint http://CONTROLLER_VIP:9696 --neutron-pass changeme --rbd-secret-uuid changeme-changeme-changeme-changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211\n\nThe libvirt defaults to using ceph as shared storage, the ceph pool for running instance is vms. if you do not using ceph as it's bachend, you must remove the following param:\n\n    images_type = rbd\n    images_rbd_pool = vms\n    images_rbd_ceph_conf = /etc/ceph/ceph.conf\n    rbd_user = cinder\n    rbd_secret_uuid = changeme-changeme-changeme-changeme\n    disk_cachemodes=\"network=writeback\"\n    live_migration_flag=\"VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED\"\n\n\n### Neutron HA\n\nCreate nova database\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack neutron create-neutron-db --root-db-pass changeme --neutron-db-pass changeme\n\nCreate service credentials\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack neutron create-service-credentials --os-password changeme --os-auth-url http://CONTROLLER_VIP:35357/v3 --neutron-pass changeme --public-endpoint http://CONTROLLER_VIP:9696 --internal-endpoint http://CONTROLLER_VIP:9696 --admin-endpoint http://CONTROLLER_VIP:9696\n\nInstall Neutron for self-service\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack neutron install --connection mysql+pymysql://neutron:NEUTRON_PASS@CONTROLLER_VIP/neutron --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --neutron-pass changeme --nova-url http://CONTROLLER_VIP:8774/v2.1 --nova-pass changeme --public-interface eth1 --local-ip MANAGEMENT_INTERFACE_IP --nova-metadata-ip CONTROLLER_VIP --metadata-proxy-shared-secret changeme-changeme-changeme-changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --populate\n    ff --user ubuntu --hosts CONTROLLER2 openstack neutron install --connection mysql+pymysql://neutron:NEUTRON_PASS@CONTROLLER_VIP/neutron --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --neutron-pass changeme --nova-url http://CONTROLLER_VIP:8774/v2.1 --nova-pass changeme --public-interface eth1 --local-ip MANAGEMENT_INTERFACE_IP --nova-metadata-ip CONTROLLER_VIP --metadata-proxy-shared-secret changeme-changeme-changeme-changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211\n\n\n### Neutron Agent\n\nInstall neutron agent on compute nodes\n\n    ff --user ubuntu --hosts COMPUTE1 openstack neutron-agent install --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --neutron-pass changeme --public-interface eth1 --local-ip MANAGEMENT_INTERFACE_IP --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211\n    ff --user ubuntu --hosts COMPUTE2 openstack neutron-agent install --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --neutron-pass changeme --public-interface eth1 --local-ip MANAGEMENT_INTERFACE_IP --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211\n\n\n### Horizon HA\n\nInstall horizon on controller nodes\n\n    ff --user ubuntu --hosts CONTROLLER1,CONTROLLER2 openstack horizon install --openstack-host CONTROLLER_VIP  --memcached-servers CONTROLLER1:11211 --time-zone Asia/Shanghai\n\n\n### Cinder HA\n\nCreate cinder database\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack cinder create-cinder-db --root-db-pass changeme --cinder-db-pass changeme\n\nCreate cinder service creadentials\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack cinder create-service-credentials --os-password changeme --os-auth-url http://CONTROLLER_VIP:35357/v3 --cinder-pass changeme --public-endpoint-v1 'http://CONTROLLER_VIP:8776/v1/%\\(tenant_id\\)s' --internal-endpoint-v1 'http://CONTROLLER_VIP:8776/v1/%\\(tenant_id\\)s' --admin-endpoint-v1 'http://CONTROLLER_VIP:8776/v1/%\\(tenant_id\\)s' --public-endpoint-v2 'http://CONTROLLER_VIP:8776/v2/%\\(tenant_id\\)s' --internal-endpoint-v2 'http://CONTROLLER_VIP:8776/v2/%\\(tenant_id\\)s' --admin-endpoint-v2 'http://CONTROLLER_VIP:8776/v2/%\\(tenant_id\\)s'\n\nInstall cinder-api and cinder-volume on controller nodes, the volume backend defaults to ceph (you must have ceph installed)\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack cinder install --connection mysql+pymysql://cinder:CINDER_PASS@CONTROLLER_VIP/cinder --rabbit-user openstack --rabbit-pass changeme --rabbit-hosts CONTROLLER1,CONTROLLER2 --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --cinder-pass changeme --my-ip MANAGEMENT_INTERFACE_IP --glance-api-servers http://CONTROLLER_VIP:9292 --rbd-secret-uuid changeme-changeme-changeme-changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --populate\n    ff --user ubuntu --hosts CONTROLLER2 openstack cinder install --connection mysql+pymysql://cinder:CINDER_PASS@CONTROLLER_VIP/cinder --rabbit-user openstack --rabbit-pass changeme --rabbit-hosts CONTROLLER1,CONTROLLER2 --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --cinder-pass changeme --my-ip MANAGEMENT_INTERFACE_IP --glance-api-servers http://CONTROLLER_VIP:9292 --rbd-secret-uuid changeme-changeme-changeme-changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211\n\n### Swift proxy HA\n\nCreate the Identity service credentials\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift create-service-credentials --os-password changeme --os-auth-url http://CONTROLLER_VIP:35357/v3 --swift-pass changeme --public-endpoint 'http://CONTROLLER_VIP:8080/v1/AUTH_%\\(tenant_id\\)s' --internal-endpoint 'http://CONTROLLER_VIP:8080/v1/AUTH_%\\(tenant_id\\)s' --admin-endpoint http://CONTROLLER_VIP:8080/v1\n\nInstall swift proxy\n\n    ff --user ubuntu --hosts CONTROLLER1,CONTROLLER2 openstack swift install --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --swift-pass changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211\n\n\n### Swift storage\n\nPrepare disks on storage node\n\n    ff --user ubuntu --hosts OBJECT1,OBJECT2 openstack swift-storage prepare-disks --name sdb,sdc,sdd,sde\n\nInstall swift storage on storage node\n\n    ff --user ubuntu --hosts OBJECT1 openstack swift-storage install --address MANAGEMENT_INTERFACE_IP --bind-ip MANAGEMENT_INTERFACE_IP\n    ff --user ubuntu --hosts OBJECT2 openstack swift-storage install --address MANAGEMENT_INTERFACE_IP --bind-ip MANAGEMENT_INTERFACE_IP\n\nCreate account ring on controller node\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage create-account-builder-file --partitions 10 --replicas 3 --moving 1\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdb --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdc --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdd --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sde --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdb --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdc --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdd --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sde --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-rebalance\n\nCreate container ring on controller node\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage create-container-builder-file --partitions 10 --replicas 3 --moving 1\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdb --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdc --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdd --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sde --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdb --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdc --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdd --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sde --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-rebalance\n\nCreate object ring on controller node\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage create-object-builder-file --partitions 10 --replicas 3 --moving 1\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdb --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdc --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdd --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sde --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdb --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdc --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdd --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sde --weight 100\n    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-rebalance\n\n Sync the builder file from controller node to each storage node and other any proxy node\n\n    ff --user ubuntu --host CONTROLLER1 openstack swift-storage sync-builder-file --to CONTROLLER2,OBJECT1,OBJECT2\n\nFinalize installation on all nodes\n\n    ff --user ubuntu --hosts CONTROLLER1,CONTROLLER2,OBJECT1,OBJECT2 openstack swift finalize-install --swift-hash-path-suffix changeme --swift-hash-path-prefix changeme\n\n### Ceph Guide\n\nFor more information about ceph backend visit:\n\n[preflight](http://docs.ceph.com/docs/jewel/start/quick-start-preflight/)\n\n[Cinder and Glance driver](http://docs.ceph.com/docs/jewel/rbd/rbd-openstack/)\n\nOn Xenial please using ceph-deploy version 1.5.34\n\nInstall ceph-deploy(1.5.34)\n\n    wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -\n    echo deb http://download.ceph.com/debian-jewel/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list\n    sudo apt-get update && sudo apt-get install ceph-deploy\n\nCreate ceph cluster directory\n\n    mkdir ceph-cluster\n    cd ceph-cluster\n\nCreate cluster and add initial monitor(s) to the ceph.conf\n\n    ceph-deploy new  CONTROLLER1 CONTROLLER2 COMPUTE1 COMPUTE2 BLOCK1 BLOCK2\n    echo \"osd pool default size = 2\" | tee -a ceph.conf\n\nInstall ceph client(Optionaly you can use `--release jewel` to install jewel version, the ceph-deploy 1.5.34 default release is jewel) and you can use `--repo-url http://your-local-repo.example.org/mirror/download.ceph.com/debian-jewel` to specify the local repository.\n\n    ceph-deploy install PLAYBACK-NODE CONTROLLER1 CONTROLLER2 COMPUTE1 COMPUTE2 BLOCK1 BLOCK2\n\nAdd the initial monitor(s) and gather the keys\n\n    ceph-deploy mon create-initial\n\nIf you want to add additional monitors, do that\n\n    ceph-deploy mon add {additional-monitor}\n\nAdd ceph osd(s)\n\n    ceph-deploy osd create --zap-disk BLOCK1:/dev/sdb\n    ceph-deploy osd create --zap-disk BLOCK1:/dev/sdc\n    ceph-deploy osd create --zap-disk BLOCK2:/dev/sdb\n    ceph-deploy osd create --zap-disk BLOCK2:/dev/sdc\n\nSync admin key\n\n    ceph-deploy admin PLAYBACK-NODE CONTROLLER1 CONTROLLER2 COMPUTE1 COMPUTE2 BLOCK1 BLOCK2\n    sudo chmod +r /etc/ceph/ceph.client.admin.keyring # On all ceph clients node\n\nCreate osd pool for cinder and running instance\n\n    ceph osd pool create volumes 512\n    ceph osd pool create vms 512\n    ceph osd pool create images 512\n\nSetup ceph client authentication\n\n    ceph auth get-or-create client.cinder mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rx pool=images'\n    ceph auth get-or-create client.glance mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images'\n\nAdd the keyrings for `client.cinder` and `client.glance` to appropriate nodes and change their ownership\n\n    ceph auth get-or-create client.cinder | sudo tee /etc/ceph/ceph.client.cinder.keyring # On all cinder-volume nodes\n    sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring\" # On all cinder-volume nodes\n\n    ceph auth get-or-create client.glance | sudo tee /etc/ceph/ceph.client.glance.keyring # On all glance-api nodes\n    sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring\" # On all glance-api nodes\n\nNodes running `nova-compute` need the keyring file for the `nova-compute` process\n\n    ceph auth get-or-create client.cinder | sudo tee /etc/ceph/ceph.client.cinder.keyring # On all nova-compute nodes\n\nThey also need to store the secret key of the `client.cinder user` in `libvirt`. The libvirt process needs it to access the cluster while attaching a block device from Cinder.\nCreate a temporary copy of the secret key on the nodes running `nova-compute`\n\n    ceph auth get-key client.cinder | tee client.cinder.key # On all nova-compute nodes\n\nThen, on the `compute nodes`, add the secret key to `libvirt` and remove the temporary copy of the key(the uuid is the same as your --rbd-secret-uuid option, you have to save the uuid for later)\n\n    uuidgen\n    457eb676-33da-42ec-9a8c-9293d545c337\n\n    # The following steps on all nova-compute nodes\n    cat > secret.xml <<EOF\n    <secret ephemeral='no' private='no'>\n      <uuid>457eb676-33da-42ec-9a8c-9293d545c337</uuid>\n      <usage type='ceph'>\n        <name>client.cinder secret</name>\n      </usage>\n    </secret>\n    EOF\n    sudo virsh secret-define --file secret.xml\n    Secret 457eb676-33da-42ec-9a8c-9293d545c337 created\n    sudo virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 --base64 $(cat client.cinder.key) && rm client.cinder.key secret.xml\n\n(optional)Now on every compute nodes edit your Ceph configuration file, add the client section\n\n    [client]\n    rbd cache = true\n    rbd cache writethrough until flush = true\n    rbd concurrent management ops = 20\n\n    [client.cinder]\n    keyring = /etc/ceph/ceph.client.cinder.keyring\n\n(optional)On every glance-api nodes edit your Ceph configuration file, add the client section\n\n    [client.glance]\n    keyring= /etc/ceph/ceph.client.glance.keyring\n\n(optional)If you want to remove osd\n\n    sudo stop ceph-mon-all && sudo stop ceph-osd-all # On osd node\n    ceph osd out {OSD-NUM}\n    ceph osd crush remove osd.{OSD-NUM}\n    ceph auth del osd.{OSD-NUM}\n    ceph osd rm {OSD-NUM}\n    ceph osd crush remove {HOST}\n\n(optional)If you want to remove monitor\n\n    ceph mon remove {MON-ID}\n\nNotes: you need to restart the `nova-compute`, `cinder-volume` and `glance-api` services to finalize the installation.\n\n### Shared File Systems service\n\nCreate manila database and service credentials\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack manila create-manila-db --root-db-pass CHANGEME --manila-db-pass CHANGEME\n    ff --user ubuntu --hosts CONTROLLER1 openstack manila create-service-credentials --os-password CHANGEME --os-auth-url http://CONTROLLER_VIP:35357/v3 --manila-pass CHANGEME --public-endpoint-v1 \"http://CONTROLLER_VIP:8786/v1/%\\(tenant_id\\)s\" --internal-endpoint-v1 \"http://CONTROLLER_VIP:8786/v1/%\\(tenant_id\\)s\" --admin-endpoint-v1 \"http://CONTROLLER_VIP:8786/v1/%\\(tenant_id\\)s\" --public-endpoint-v2 \"http://CONTROLLER_VIP:8786/v2/%\\(tenant_id\\)s\" --internal-endpoint-v2 \"http://CONTROLLER_VIP:8786/v2/%\\(tenant_id\\)s\" --admin-endpoint-v2 \"http://CONTROLLER_VIP:8786/v2/%\\(tenant_id\\)s\"\n\nInstall manila on CONTROLLER1 and CONTROLLER2\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack manila install --connection mysql+pymysql://manila:CHANGEME@CONTROLLER_VIP/manila --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --manila-pass CHANGEME --my-ip CONTROLLER1 --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass CHANGEME --populate\n    ff --user ubuntu --hosts CONTROLLER2 openstack manila install --connection mysql+pymysql://manila:CHANGEME@CONTROLLER_VIP/manila --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --manila-pass CHANGEME --my-ip CONTROLLER2 --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass CHANGEME\n\nInstall manila share on CONTROLLER1 and CONTROLLER2\n\n    ff --user ubuntu --hosts CONTROLLER1 openstack manila-share install --connection mysql+pymysql://manila:CHANGEME@CONTROLLER_VIP/manila --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --manila-pass CHANGEME --my-ip CONTROLLER1 --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass CHANGEME --neutron-endpoint http://CONTROLLER_VIP:9696 --neutron-pass CHANGEME --nova-pass CHANGEME --cinder-pass CHANGEME\n    ff --user ubuntu --hosts CONTROLLER2 openstack manila-share install --connection mysql+pymysql://manila:CHANGEME@CONTROLLER_VIP/manila --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --manila-pass CHANGEME --my-ip CONTROLLER2 --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass CHANGEME --neutron-endpoint http://CONTROLLER_VIP:9696 --neutron-pass CHANGEME --nova-pass CHANGEME --cinder-pass CHANGEME\n\nCreate the service image for manila\n\nhttp://docs.openstack.org/mitaka/install-guide-ubuntu/launch-instance-manila.html\n\nCreate shares with share servers management support\n\nhttp://docs.openstack.org/mitaka/install-guide-ubuntu/launch-instance-manila-dhss-true-option2.html", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/nofdev/fastforward", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "fastforward", "package_url": "https://pypi.org/project/fastforward/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/fastforward/", "project_urls": {"Homepage": "https://github.com/nofdev/fastforward"}, "release_url": "https://pypi.org/project/fastforward/0.0.10/", "requires_dist": ["playback (==0.3.8)"], "requires_python": "", "summary": "FastForward is a DevOps automate platform", "version": "0.0.10", "yanked": false, "html_description": "<div class=\"project-description\">\n            # FastForward [![GoDoc](https://godoc.org/github.com/nofdev/fastforward?status.svg)](https://godoc.org/github.com/nofdev/fastforward) [![Build Status](https://travis-ci.org/nofdev/fastforward.svg?branch=master)](https://travis-ci.org/nofdev/fastforward)<br>FastForward is an ultimate DevOps platform.<br><br><br>## Quick-start provision OpenStack(mitaka) on xenil or trusty<br><br>### Requirements<br><br>* The OpenStack bare metal hosts are in MAAS environment(recommend)<br>* All hosts are two NICs at least(external and internal)<br>* We assume that you have ceph installed, the cinder bachend default using ceph, the running instace default to using ceph as it's local storage. About ceph please visit: http://docs.ceph.com/docs/master/rbd/rbd-openstack/ or see the (Option)Ceph Guide below.<br>* For resize instance nova user can be login to each compute node via ssh passwordless(include sudo), and all compute nodes need to restart libvirt-bin to enable live migration<br>* The FastForward node is the same as ceph-deploy node where can be login to each openstack node passwordless and sudo-passwordless<br>* The FastForward node default using ~/.ssh/id_rsa ssh private key to logon remote server<br>* You need to restart the `nova-compute`, `cinder-volume` and `glance-api` services to finalize the installation if you have selected the ceph as that backend<br>* FastForward support consistency groups for future use but the default LVM and Ceph driver does not support consistency groups yet because the consistency technology is not available at that storage level<br><br>### Install FastForward<br><br>Install FastForward on FASTFORWARD-NODE<br><br>    pip install fastforward<br><br>### Prepare environment<br><br>Prepare the OpenStack environment.<br>(NOTE) DO NOT setup eth1 in /etc/network/interfaces<br><br>    ff --user ubuntu --hosts \\<br>    HAPROXY1,\\<br>    HAPROXY2,\\<br>    CONTROLLER1,\\<br>    CONTROLLER2,\\<br>    COMPUTE1,\\<br>    COMPUTE2,\\<br>    COMPUTE3,\\<br>    COMPUTE4,\\<br>    COMPUTE5,\\<br>    COMPUTE6,\\<br>    COMPUTE7,\\<br>    COMPUTE8,\\<br>    COMPUTE9,\\<br>    COMPUTE10 \\<br>    environment \\<br>    prepare-host --public-interface eth1<br><br>### MySQL HA<br><br>Deploy to CONTROLLER1<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack mysql install<br>    ff --user ubuntu --hosts CONTROLLER1 openstack mysql config --wsrep-cluster-address \"gcomm://CONTROLLER1,CONTROLLER2\" --wsrep-node-name=\"galera1\" --wsrep-node-address=\"CONTROLLER1\"<br><br>Deploy to CONTROLLER2<br><br>    ff --user ubuntu --hosts CONTROLLER2 openstack mysql install<br>    ff --user ubuntu --hosts CONTROLLER2 openstack mysql config --wsrep-cluster-address \"gcomm://CONTROLLER1,CONTROLLER2\" --wsrep-node-name=\"galera2\" --wsrep-node-address=\"CONTROLLER2\"<br><br>Start the cluster<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack mysql manage --wsrep-new-cluster<br>    ff --user ubuntu --hosts CONTROLLER2 openstack mysql manage --start<br>    ff --user ubuntu --hosts CONTROLLER1 openstack mysql manage --change-root-password changeme<br><br>Show the cluster status<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack mysql manage --show-cluster-status --root-db-pass changeme<br><br><br>### HAProxy HA<br><br>Deploy to HAPROXY1<br><br>    ff --user ubuntu --hosts HAPROXY1 openstack haproxy install<br><br>Deploy to HAPROXY2<br><br>    ff --user ubuntu --hosts HAPROXY2 openstack haproxy install<br><br>Generate the HAProxy configuration and upload to target hosts(Do not forget to edit the generated configuration)<br><br>    ff openstack haproxygen-conf<br>    ff --user ubuntu --hosts HAPROXY1,HAPROXY2 openstack haproxy config --upload-conf haproxy.cfg<br><br>Configure Keepalived<br><br>    ff --user ubuntu --hosts HAPROXY1 openstack haproxy config --configure-keepalived --router_id lb1 --priority 150 --state MASTER --interface eth0 --vip CONTROLLER_VIP<br>    ff --user ubuntu --hosts HAPROXY2 openstack haproxy config --configure-keepalived --router_id lb2 --priority 100 --state SLAVE --interface eth0 --vip CONTROLLER_VIP<br><br>### RabbitMQ HA<br><br>Deploy to CONTROLLER1 and CONTROLLER2<br><br>    ff --user ubuntu --hosts CONTROLLER1,CONTROLLER2 openstack rabbitmq install --erlang-cookie changemechangeme --rabbit-user openstack --rabbit-pass changeme<br><br>Create cluster(Ensure CONTROLLER2 can access CONTROLLER1 via hostname)<br><br>    ff --user ubuntu --hosts CONTROLLER2 openstack rabbitmq join-cluster --name rabbit@CONTROLLER1<br><br>### Keystone HA<br><br>Create keystone database<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack keystone create-keystone-db --root-db-pass changeme --keystone-db-pass changeme<br><br>Install keystone on CONTROLLER1 and CONTROLLER2<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack keystone install --admin-token changeme --connection mysql+pymysql://keystone:changeme@CONTROLLER_VIP/keystone --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --populate<br>    ff --user ubuntu --hosts CONTROLLER2 openstack keystone install --admin-token changeme --connection mysql+pymysql://keystone:changeme@CONTROLLER_VIP/keystone --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211<br><br>Create the service entity and API endpoints<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack keystone create-entity-and-endpoint --os-token changeme --os-url http://CONTROLLER_VIP:35357/v3 --public-endpoint http://CONTROLLER_VIP:5000/v3 --internal-endpoint http://CONTROLLER_VIP:5000/v3 --admin-endpoint http://CONTROLLER_vip:35357/v3<br><br>Create projects, users, and roles<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack keystone create-projects-users-roles --os-token changeme --os-url http://CONTROLLER_VIP:35357/v3 --admin-pass changeme --demo-pass changeme<br><br>(OPTION) you will need to create OpenStack client environment scripts<br>admin-openrc.sh<br><br>    export OS_PROJECT_DOMAIN_NAME=default<br>    export OS_USER_DOMAIN_NAME=default<br>    export OS_PROJECT_NAME=admin<br>    export OS_TENANT_NAME=admin<br>    export OS_USERNAME=admin<br>    export OS_PASSWORD=changeme<br>    export OS_AUTH_URL=http://CONTROLLER_VIP:35357/v3<br>    export OS_IDENTITY_API_VERSION=3<br>    export OS_IMAGE_API_VERSION=2<br>    export OS_AUTH_VERSION=3<br><br>demo-openrc.sh<br><br>    export OS_PROJECT_DOMAIN_NAME=default<br>    export OS_USER_DOMAIN_NAME=default<br>    export OS_PROJECT_NAME=demo<br>    export OS_TENANT_NAME=demo<br>    export OS_USERNAME=demo<br>    export OS_PASSWORD=changeme<br>    export OS_AUTH_URL=http://CONTROLLER_VIP:5000/v3<br>    export OS_IDENTITY_API_VERSION=3<br>    export OS_IMAGE_API_VERSION=2<br>    export OS_AUTH_VERSION=3<br><br>### Glance HA<br><br>Create glance database<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack glance create-glance-db --root-db-pass changeme --glance-db-pass changeme<br><br>Create service credentials<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack glance create-service-credentials --os-password changeme --os-auth-url http://CONTROLLER_VIP:35357/v3 --glance-pass changeme --public-endpoint http://CONTROLLER_VIP:9292 --internal-endpoint http://CONTROLLER_VIP:9292 --admin-endpoint http://CONTROLLER_VIP:9292<br><br>Install glance on CONTROLLER1 and CONTROLLER2<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack glance install --connection mysql+pymysql://glance:GLANCE_PASS@CONTROLLER_VIP/glance --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --glance-pass changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --populate<br>    ff --user ubuntu --hosts CONTROLLER2 openstack glance install --connection mysql+pymysql://glance:GLANCE_PASS@CONTROLLER_VIP/glance --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --glance-pass changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211<br><br>### Nova HA<br><br>Create nova database<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack nova create-nova-db --root-db-pass changeme --nova-db-pass changeme<br><br>Create service credentials<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack nova create-service-credentials --os-password changeme --os-auth-url http://CONTROLLER_VIP:35357/v3 --nova-pass changeme --public-endpoint 'http://CONTROLLER_VIP:8774/v2.1/%\\(tenant_id\\)s' --internal-endpoint 'http://CONTROLLER_VIP:8774/v2.1/%\\(tenant_id\\)s' --admin-endpoint 'http://CONTROLLER_VIP:8774/v2.1/%\\(tenant_id\\)s'<br><br>Install nova on CONTROLLER1<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack nova install --connection mysql+pymysql://nova:NOVA_PASS@CONTROLLER_VIP/nova --api-connection mysql+pymysql://nova:NOVA_PASS@CONTROLLER_VIP/nova_api --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --nova-pass changeme --my-ip MANAGEMENT_IP --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --glance-api-servers http://CONTROLLER_VIP:9292 --neutron-endpoint http://CONTROLLER_VIP:9696 --neutron-pass changeme --metadata-proxy-shared-secret changeme --populate<br><br>Install nova on CONTROLLER2<br><br>    ff --user ubuntu --hosts CONTROLLER2 openstack nova install --connection mysql+pymysql://nova:NOVA_PASS@CONTROLLER_VIP/nova --api-connection mysql+pymysql://nova:NOVA_PASS@CONTROLLER_VIP/nova_api --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --nova-pass changeme --my-ip MANAGEMENT_IP --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --glance-api-servers http://CONTROLLER_VIP:9292 --neutron-endpoint http://CONTROLLER_VIP:9696 --neutron-pass changeme --metadata-proxy-shared-secret changeme<br><br>### Nova Compute<br><br>Add nova computes(use `uuidgen` to generate the ceph uuid)<br><br>    ff --user ubuntu --hosts COMPUTE1 openstack nova-compute install --my-ip MANAGEMENT_IP --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --nova-pass changeme --novncproxy-base-url http://CONTROLLER_VIP:6080/vnc_auto.html --glance-api-servers http://CONTROLLER_VIP:9292 --neutron-endpoint http://CONTROLLER_VIP:9696 --neutron-pass changeme --rbd-secret-uuid changeme-changeme-changeme-changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211<br>    ff --user ubuntu --hosts COMPUTE2 openstack nova-compute install --my-ip MANAGEMENT_IP --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --nova-pass changeme --novncproxy-base-url http://CONTROLLER_VIP:6080/vnc_auto.html --glance-api-servers http://CONTROLLER_VIP:9292 --neutron-endpoint http://CONTROLLER_VIP:9696 --neutron-pass changeme --rbd-secret-uuid changeme-changeme-changeme-changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211<br><br>The libvirt defaults to using ceph as shared storage, the ceph pool for running instance is vms. if you do not using ceph as it's bachend, you must remove the following param:<br><br>    images_type = rbd<br>    images_rbd_pool = vms<br>    images_rbd_ceph_conf = /etc/ceph/ceph.conf<br>    rbd_user = cinder<br>    rbd_secret_uuid = changeme-changeme-changeme-changeme<br>    disk_cachemodes=\"network=writeback\"<br>    live_migration_flag=\"VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED\"<br><br><br>### Neutron HA<br><br>Create nova database<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack neutron create-neutron-db --root-db-pass changeme --neutron-db-pass changeme<br><br>Create service credentials<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack neutron create-service-credentials --os-password changeme --os-auth-url http://CONTROLLER_VIP:35357/v3 --neutron-pass changeme --public-endpoint http://CONTROLLER_VIP:9696 --internal-endpoint http://CONTROLLER_VIP:9696 --admin-endpoint http://CONTROLLER_VIP:9696<br><br>Install Neutron for self-service<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack neutron install --connection mysql+pymysql://neutron:NEUTRON_PASS@CONTROLLER_VIP/neutron --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --neutron-pass changeme --nova-url http://CONTROLLER_VIP:8774/v2.1 --nova-pass changeme --public-interface eth1 --local-ip MANAGEMENT_INTERFACE_IP --nova-metadata-ip CONTROLLER_VIP --metadata-proxy-shared-secret changeme-changeme-changeme-changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --populate<br>    ff --user ubuntu --hosts CONTROLLER2 openstack neutron install --connection mysql+pymysql://neutron:NEUTRON_PASS@CONTROLLER_VIP/neutron --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --neutron-pass changeme --nova-url http://CONTROLLER_VIP:8774/v2.1 --nova-pass changeme --public-interface eth1 --local-ip MANAGEMENT_INTERFACE_IP --nova-metadata-ip CONTROLLER_VIP --metadata-proxy-shared-secret changeme-changeme-changeme-changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211<br><br><br>### Neutron Agent<br><br>Install neutron agent on compute nodes<br><br>    ff --user ubuntu --hosts COMPUTE1 openstack neutron-agent install --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --neutron-pass changeme --public-interface eth1 --local-ip MANAGEMENT_INTERFACE_IP --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211<br>    ff --user ubuntu --hosts COMPUTE2 openstack neutron-agent install --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass changeme --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --neutron-pass changeme --public-interface eth1 --local-ip MANAGEMENT_INTERFACE_IP --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211<br><br><br>### Horizon HA<br><br>Install horizon on controller nodes<br><br>    ff --user ubuntu --hosts CONTROLLER1,CONTROLLER2 openstack horizon install --openstack-host CONTROLLER_VIP  --memcached-servers CONTROLLER1:11211 --time-zone Asia/Shanghai<br><br><br>### Cinder HA<br><br>Create cinder database<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack cinder create-cinder-db --root-db-pass changeme --cinder-db-pass changeme<br><br>Create cinder service creadentials<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack cinder create-service-credentials --os-password changeme --os-auth-url http://CONTROLLER_VIP:35357/v3 --cinder-pass changeme --public-endpoint-v1 'http://CONTROLLER_VIP:8776/v1/%\\(tenant_id\\)s' --internal-endpoint-v1 'http://CONTROLLER_VIP:8776/v1/%\\(tenant_id\\)s' --admin-endpoint-v1 'http://CONTROLLER_VIP:8776/v1/%\\(tenant_id\\)s' --public-endpoint-v2 'http://CONTROLLER_VIP:8776/v2/%\\(tenant_id\\)s' --internal-endpoint-v2 'http://CONTROLLER_VIP:8776/v2/%\\(tenant_id\\)s' --admin-endpoint-v2 'http://CONTROLLER_VIP:8776/v2/%\\(tenant_id\\)s'<br><br>Install cinder-api and cinder-volume on controller nodes, the volume backend defaults to ceph (you must have ceph installed)<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack cinder install --connection mysql+pymysql://cinder:CINDER_PASS@CONTROLLER_VIP/cinder --rabbit-user openstack --rabbit-pass changeme --rabbit-hosts CONTROLLER1,CONTROLLER2 --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --cinder-pass changeme --my-ip MANAGEMENT_INTERFACE_IP --glance-api-servers http://CONTROLLER_VIP:9292 --rbd-secret-uuid changeme-changeme-changeme-changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --populate<br>    ff --user ubuntu --hosts CONTROLLER2 openstack cinder install --connection mysql+pymysql://cinder:CINDER_PASS@CONTROLLER_VIP/cinder --rabbit-user openstack --rabbit-pass changeme --rabbit-hosts CONTROLLER1,CONTROLLER2 --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --cinder-pass changeme --my-ip MANAGEMENT_INTERFACE_IP --glance-api-servers http://CONTROLLER_VIP:9292 --rbd-secret-uuid changeme-changeme-changeme-changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211<br><br>### Swift proxy HA<br><br>Create the Identity service credentials<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift create-service-credentials --os-password changeme --os-auth-url http://CONTROLLER_VIP:35357/v3 --swift-pass changeme --public-endpoint 'http://CONTROLLER_VIP:8080/v1/AUTH_%\\(tenant_id\\)s' --internal-endpoint 'http://CONTROLLER_VIP:8080/v1/AUTH_%\\(tenant_id\\)s' --admin-endpoint http://CONTROLLER_VIP:8080/v1<br><br>Install swift proxy<br><br>    ff --user ubuntu --hosts CONTROLLER1,CONTROLLER2 openstack swift install --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --swift-pass changeme --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211<br><br><br>### Swift storage<br><br>Prepare disks on storage node<br><br>    ff --user ubuntu --hosts OBJECT1,OBJECT2 openstack swift-storage prepare-disks --name sdb,sdc,sdd,sde<br><br>Install swift storage on storage node<br><br>    ff --user ubuntu --hosts OBJECT1 openstack swift-storage install --address MANAGEMENT_INTERFACE_IP --bind-ip MANAGEMENT_INTERFACE_IP<br>    ff --user ubuntu --hosts OBJECT2 openstack swift-storage install --address MANAGEMENT_INTERFACE_IP --bind-ip MANAGEMENT_INTERFACE_IP<br><br>Create account ring on controller node<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage create-account-builder-file --partitions 10 --replicas 3 --moving 1<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdb --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdc --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdd --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sde --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdb --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdc --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdd --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sde --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage account-builder-rebalance<br><br>Create container ring on controller node<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage create-container-builder-file --partitions 10 --replicas 3 --moving 1<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdb --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdc --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdd --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sde --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdb --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdc --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdd --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sde --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage container-builder-rebalance<br><br>Create object ring on controller node<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage create-object-builder-file --partitions 10 --replicas 3 --moving 1<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdb --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdc --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sdd --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT1_MANAGEMENT_IP --device sde --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdb --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdc --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sdd --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-add --region 1 --zone 1 --ip OBJECT2_MANAGEMENT_IP --device sde --weight 100<br>    ff --user ubuntu --hosts CONTROLLER1 openstack swift-storage object-builder-rebalance<br><br> Sync the builder file from controller node to each storage node and other any proxy node<br><br>    ff --user ubuntu --host CONTROLLER1 openstack swift-storage sync-builder-file --to CONTROLLER2,OBJECT1,OBJECT2<br><br>Finalize installation on all nodes<br><br>    ff --user ubuntu --hosts CONTROLLER1,CONTROLLER2,OBJECT1,OBJECT2 openstack swift finalize-install --swift-hash-path-suffix changeme --swift-hash-path-prefix changeme<br><br>### Ceph Guide<br><br>For more information about ceph backend visit:<br><br>[preflight](http://docs.ceph.com/docs/jewel/start/quick-start-preflight/)<br><br>[Cinder and Glance driver](http://docs.ceph.com/docs/jewel/rbd/rbd-openstack/)<br><br>On Xenial please using ceph-deploy version 1.5.34<br><br>Install ceph-deploy(1.5.34)<br><br>    wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -<br>    echo deb http://download.ceph.com/debian-jewel/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list<br>    sudo apt-get update &amp;&amp; sudo apt-get install ceph-deploy<br><br>Create ceph cluster directory<br><br>    mkdir ceph-cluster<br>    cd ceph-cluster<br><br>Create cluster and add initial monitor(s) to the ceph.conf<br><br>    ceph-deploy new  CONTROLLER1 CONTROLLER2 COMPUTE1 COMPUTE2 BLOCK1 BLOCK2<br>    echo \"osd pool default size = 2\" | tee -a ceph.conf<br><br>Install ceph client(Optionaly you can use `--release jewel` to install jewel version, the ceph-deploy 1.5.34 default release is jewel) and you can use `--repo-url http://your-local-repo.example.org/mirror/download.ceph.com/debian-jewel` to specify the local repository.<br><br>    ceph-deploy install PLAYBACK-NODE CONTROLLER1 CONTROLLER2 COMPUTE1 COMPUTE2 BLOCK1 BLOCK2<br><br>Add the initial monitor(s) and gather the keys<br><br>    ceph-deploy mon create-initial<br><br>If you want to add additional monitors, do that<br><br>    ceph-deploy mon add {additional-monitor}<br><br>Add ceph osd(s)<br><br>    ceph-deploy osd create --zap-disk BLOCK1:/dev/sdb<br>    ceph-deploy osd create --zap-disk BLOCK1:/dev/sdc<br>    ceph-deploy osd create --zap-disk BLOCK2:/dev/sdb<br>    ceph-deploy osd create --zap-disk BLOCK2:/dev/sdc<br><br>Sync admin key<br><br>    ceph-deploy admin PLAYBACK-NODE CONTROLLER1 CONTROLLER2 COMPUTE1 COMPUTE2 BLOCK1 BLOCK2<br>    sudo chmod +r /etc/ceph/ceph.client.admin.keyring # On all ceph clients node<br><br>Create osd pool for cinder and running instance<br><br>    ceph osd pool create volumes 512<br>    ceph osd pool create vms 512<br>    ceph osd pool create images 512<br><br>Setup ceph client authentication<br><br>    ceph auth get-or-create client.cinder mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rx pool=images'<br>    ceph auth get-or-create client.glance mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images'<br><br>Add the keyrings for `client.cinder` and `client.glance` to appropriate nodes and change their ownership<br><br>    ceph auth get-or-create client.cinder | sudo tee /etc/ceph/ceph.client.cinder.keyring # On all cinder-volume nodes<br>    sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring\" # On all cinder-volume nodes<br><br>    ceph auth get-or-create client.glance | sudo tee /etc/ceph/ceph.client.glance.keyring # On all glance-api nodes<br>    sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring\" # On all glance-api nodes<br><br>Nodes running `nova-compute` need the keyring file for the `nova-compute` process<br><br>    ceph auth get-or-create client.cinder | sudo tee /etc/ceph/ceph.client.cinder.keyring # On all nova-compute nodes<br><br>They also need to store the secret key of the `client.cinder user` in `libvirt`. The libvirt process needs it to access the cluster while attaching a block device from Cinder.<br>Create a temporary copy of the secret key on the nodes running `nova-compute`<br><br>    ceph auth get-key client.cinder | tee client.cinder.key # On all nova-compute nodes<br><br>Then, on the `compute nodes`, add the secret key to `libvirt` and remove the temporary copy of the key(the uuid is the same as your --rbd-secret-uuid option, you have to save the uuid for later)<br><br>    uuidgen<br>    457eb676-33da-42ec-9a8c-9293d545c337<br><br>    # The following steps on all nova-compute nodes<br>    cat &gt; secret.xml &lt;&lt;EOF<br>    &lt;secret ephemeral='no' private='no'&gt;<br>      &lt;uuid&gt;457eb676-33da-42ec-9a8c-9293d545c337&lt;/uuid&gt;<br>      &lt;usage type='ceph'&gt;<br>        &lt;name&gt;client.cinder secret&lt;/name&gt;<br>      &lt;/usage&gt;<br>    &lt;/secret&gt;<br>    EOF<br>    sudo virsh secret-define --file secret.xml<br>    Secret 457eb676-33da-42ec-9a8c-9293d545c337 created<br>    sudo virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 --base64 $(cat client.cinder.key) &amp;&amp; rm client.cinder.key secret.xml<br><br>(optional)Now on every compute nodes edit your Ceph configuration file, add the client section<br><br>    [client]<br>    rbd cache = true<br>    rbd cache writethrough until flush = true<br>    rbd concurrent management ops = 20<br><br>    [client.cinder]<br>    keyring = /etc/ceph/ceph.client.cinder.keyring<br><br>(optional)On every glance-api nodes edit your Ceph configuration file, add the client section<br><br>    [client.glance]<br>    keyring= /etc/ceph/ceph.client.glance.keyring<br><br>(optional)If you want to remove osd<br><br>    sudo stop ceph-mon-all &amp;&amp; sudo stop ceph-osd-all # On osd node<br>    ceph osd out {OSD-NUM}<br>    ceph osd crush remove osd.{OSD-NUM}<br>    ceph auth del osd.{OSD-NUM}<br>    ceph osd rm {OSD-NUM}<br>    ceph osd crush remove {HOST}<br><br>(optional)If you want to remove monitor<br><br>    ceph mon remove {MON-ID}<br><br>Notes: you need to restart the `nova-compute`, `cinder-volume` and `glance-api` services to finalize the installation.<br><br>### Shared File Systems service<br><br>Create manila database and service credentials<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack manila create-manila-db --root-db-pass CHANGEME --manila-db-pass CHANGEME<br>    ff --user ubuntu --hosts CONTROLLER1 openstack manila create-service-credentials --os-password CHANGEME --os-auth-url http://CONTROLLER_VIP:35357/v3 --manila-pass CHANGEME --public-endpoint-v1 \"http://CONTROLLER_VIP:8786/v1/%\\(tenant_id\\)s\" --internal-endpoint-v1 \"http://CONTROLLER_VIP:8786/v1/%\\(tenant_id\\)s\" --admin-endpoint-v1 \"http://CONTROLLER_VIP:8786/v1/%\\(tenant_id\\)s\" --public-endpoint-v2 \"http://CONTROLLER_VIP:8786/v2/%\\(tenant_id\\)s\" --internal-endpoint-v2 \"http://CONTROLLER_VIP:8786/v2/%\\(tenant_id\\)s\" --admin-endpoint-v2 \"http://CONTROLLER_VIP:8786/v2/%\\(tenant_id\\)s\"<br><br>Install manila on CONTROLLER1 and CONTROLLER2<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack manila install --connection mysql+pymysql://manila:CHANGEME@CONTROLLER_VIP/manila --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --manila-pass CHANGEME --my-ip CONTROLLER1 --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass CHANGEME --populate<br>    ff --user ubuntu --hosts CONTROLLER2 openstack manila install --connection mysql+pymysql://manila:CHANGEME@CONTROLLER_VIP/manila --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --manila-pass CHANGEME --my-ip CONTROLLER2 --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass CHANGEME<br><br>Install manila share on CONTROLLER1 and CONTROLLER2<br><br>    ff --user ubuntu --hosts CONTROLLER1 openstack manila-share install --connection mysql+pymysql://manila:CHANGEME@CONTROLLER_VIP/manila --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --manila-pass CHANGEME --my-ip CONTROLLER1 --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass CHANGEME --neutron-endpoint http://CONTROLLER_VIP:9696 --neutron-pass CHANGEME --nova-pass CHANGEME --cinder-pass CHANGEME<br>    ff --user ubuntu --hosts CONTROLLER2 openstack manila-share install --connection mysql+pymysql://manila:CHANGEME@CONTROLLER_VIP/manila --auth-uri http://CONTROLLER_VIP:5000 --auth-url http://CONTROLLER_VIP:35357 --manila-pass CHANGEME --my-ip CONTROLLER2 --memcached-servers CONTROLLER1:11211,CONTROLLER2:11211 --rabbit-hosts CONTROLLER1,CONTROLLER2 --rabbit-user openstack --rabbit-pass CHANGEME --neutron-endpoint http://CONTROLLER_VIP:9696 --neutron-pass CHANGEME --nova-pass CHANGEME --cinder-pass CHANGEME<br><br>Create the service image for manila<br><br>http://docs.openstack.org/mitaka/install-guide-ubuntu/launch-instance-manila.html<br><br>Create shares with share servers management support<br><br>http://docs.openstack.org/mitaka/install-guide-ubuntu/launch-instance-manila-dhss-true-option2.html\n          </div>"}, "last_serial": 2250737, "releases": {"0.0.10": [{"comment_text": "", "digests": {"md5": "5d5d9fc3fd2e51a1408a78a358730403", "sha256": "183b30fff7eef1b8a7df242591368a3768d3da0babc9b5172fcbcd3805852ec9"}, "downloads": -1, "filename": "fastforward-0.0.10-py2-none-any.whl", "has_sig": false, "md5_digest": "5d5d9fc3fd2e51a1408a78a358730403", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 36066, "upload_time": "2016-07-29T06:28:05", "upload_time_iso_8601": "2016-07-29T06:28:05.712471Z", "url": "https://files.pythonhosted.org/packages/1d/79/e93b6180df48f4ebdfde7ab54dee25c710c86252fbb8c0adea3b0baae298/fastforward-0.0.10-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "928e3db95d6f8b9e1700d490b58fb864", "sha256": "949f9a09223b13681f25274d73c0e64c228029a8c4f67b6151798f3fa6dc8777"}, "downloads": -1, "filename": "fastforward-0.0.10.tar.gz", "has_sig": false, "md5_digest": "928e3db95d6f8b9e1700d490b58fb864", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22195, "upload_time": "2016-07-21T07:15:56", "upload_time_iso_8601": "2016-07-21T07:15:56.806589Z", "url": "https://files.pythonhosted.org/packages/1d/65/f15281ee13a77fefe913a85f2f1258e64abbbb7b7a32caf233e69e416f44/fastforward-0.0.10.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "ed8fdef3e8ca6c8f881c6dd24162c251", "sha256": "7535c6cc215694dc5a72306754bb0c1a134608085ce4fb379b7b791c16df7c14"}, "downloads": -1, "filename": "fastforward-0.0.4.zip", "has_sig": false, "md5_digest": "ed8fdef3e8ca6c8f881c6dd24162c251", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5439, "upload_time": "2016-03-30T03:43:07", "upload_time_iso_8601": "2016-03-30T03:43:07.109787Z", "url": "https://files.pythonhosted.org/packages/e3/bc/fc12d81f0164793bb4cc72811398bea3c55ff0f47aa12ff8b9ea4965f522/fastforward-0.0.4.zip", "yanked": false}], "0.0.5": [{"comment_text": "", "digests": {"md5": "92ca7e7cba3aef46f6e111ba01be09b8", "sha256": "37eae310afabd31d8806f2ba05b6d5d749f010d42b687d01dbf77249887e81cd"}, "downloads": -1, "filename": "fastforward-0.0.5.tar.gz", "has_sig": false, "md5_digest": "92ca7e7cba3aef46f6e111ba01be09b8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2842, "upload_time": "2016-06-24T10:32:26", "upload_time_iso_8601": "2016-06-24T10:32:26.101115Z", "url": "https://files.pythonhosted.org/packages/b5/0d/a5b8b816ad3eee1ca7ad01ba5a6bc2f20d6a6cf68da36a2a951b7494c038/fastforward-0.0.5.tar.gz", "yanked": false}], "0.0.6": [{"comment_text": "", "digests": {"md5": "09d491893b68183b4d2de7a5952a8c69", "sha256": "62cfdbd6bb39989e35bb7335c923e26800d1174aa773d0ea1993372f761f0b82"}, "downloads": -1, "filename": "fastforward-0.0.6.tar.gz", "has_sig": false, "md5_digest": "09d491893b68183b4d2de7a5952a8c69", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2843, "upload_time": "2016-07-13T07:33:00", "upload_time_iso_8601": "2016-07-13T07:33:00.654292Z", "url": "https://files.pythonhosted.org/packages/24/ce/02a29720ee83ca752ab7a4e59cfd2d0d94bdc94b1776da56031f839f4bb8/fastforward-0.0.6.tar.gz", "yanked": false}], "0.0.7": [{"comment_text": "", "digests": {"md5": "0aa3ad412827b2e9940eae7959fde854", "sha256": "e85bd253e1b5c810d0edad8cd888bf5076cb78a71abcfd7e3c742c661c305bc9"}, "downloads": -1, "filename": "fastforward-0.0.7.tar.gz", "has_sig": false, "md5_digest": "0aa3ad412827b2e9940eae7959fde854", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22130, "upload_time": "2016-07-15T11:17:02", "upload_time_iso_8601": "2016-07-15T11:17:02.840979Z", "url": "https://files.pythonhosted.org/packages/cf/4e/5bb8a06a08d353265d0661bee3441ace2d65f857bb93cd8e8d0ca96e17b5/fastforward-0.0.7.tar.gz", "yanked": false}], "0.0.8": [{"comment_text": "", "digests": {"md5": "70a2f36a37c05c7053f7b4530fd582b2", "sha256": "1f368dbbed13e932794a6b1d6c4569f6034ff8631be1ff222a006e93dcccaa21"}, "downloads": -1, "filename": "fastforward-0.0.8.tar.gz", "has_sig": false, "md5_digest": "70a2f36a37c05c7053f7b4530fd582b2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22186, "upload_time": "2016-07-18T09:51:05", "upload_time_iso_8601": "2016-07-18T09:51:05.130555Z", "url": "https://files.pythonhosted.org/packages/5e/d0/c7c11d3cc7d65b3a894fffbbf9cf818fa8eeee8eed8213abdeda43e42cac/fastforward-0.0.8.tar.gz", "yanked": false}], "0.0.9": [{"comment_text": "", "digests": {"md5": "4c72e010f770f9845835314d21f96898", "sha256": "3388bad81c386277b7cadfff3c2d0b3e0b4c4ef22295fe6d2e7bff5241e0a5dc"}, "downloads": -1, "filename": "fastforward-0.0.9.tar.gz", "has_sig": false, "md5_digest": "4c72e010f770f9845835314d21f96898", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22173, "upload_time": "2016-07-18T10:14:40", "upload_time_iso_8601": "2016-07-18T10:14:40.991902Z", "url": "https://files.pythonhosted.org/packages/8d/da/2a3086caeeec6175946b5359f37cd37c9e776d57c3bf9d5c7c3e98a267c8/fastforward-0.0.9.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "5d5d9fc3fd2e51a1408a78a358730403", "sha256": "183b30fff7eef1b8a7df242591368a3768d3da0babc9b5172fcbcd3805852ec9"}, "downloads": -1, "filename": "fastforward-0.0.10-py2-none-any.whl", "has_sig": false, "md5_digest": "5d5d9fc3fd2e51a1408a78a358730403", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 36066, "upload_time": "2016-07-29T06:28:05", "upload_time_iso_8601": "2016-07-29T06:28:05.712471Z", "url": "https://files.pythonhosted.org/packages/1d/79/e93b6180df48f4ebdfde7ab54dee25c710c86252fbb8c0adea3b0baae298/fastforward-0.0.10-py2-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "928e3db95d6f8b9e1700d490b58fb864", "sha256": "949f9a09223b13681f25274d73c0e64c228029a8c4f67b6151798f3fa6dc8777"}, "downloads": -1, "filename": "fastforward-0.0.10.tar.gz", "has_sig": false, "md5_digest": "928e3db95d6f8b9e1700d490b58fb864", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22195, "upload_time": "2016-07-21T07:15:56", "upload_time_iso_8601": "2016-07-21T07:15:56.806589Z", "url": "https://files.pythonhosted.org/packages/1d/65/f15281ee13a77fefe913a85f2f1258e64abbbb7b7a32caf233e69e416f44/fastforward-0.0.10.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:32 2020"}