{"info": {"author": "Thomas Tumiel", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Interpretable Deep Learning\n\n![Class Visualisations](./class_vis.png)\n\nA simple to use PyTorch library for interpreting your deep learning results.\n\n**Note: Repo under construction**\n\n## Installation\n\nCurrently, install from GitHub:\n\n```bash\npip install git+https://github.com/ttumiel/interpret\n```\n\n### Dependencies\n\n`interpret` requires a working installation of PyTorch.\n\n## Usage\n\n`interpret` can be used for both visualisation and attribution. Here an example using a pretrained network is shown.\n\n### Visualisation\n\n```python\nfrom interpret import OptVis, ImageParam, denorm\nimport torchvision\n\n# Get the PyTorch neural network\nnetwork = torchvision.models.vgg11(pretrained=True)\n\n# Select a layer from the network. Use get_layer_names()\n# to see a list of layer names and sizes.\nlayer = 'classifier/6'\nneuron = 5\n\n# Create an OptVis object from a PyTorch model\noptvis = OptVis.from_layer(network, layer=layer, neuron=neuron)\n\n# Parameterise input noise in colour decorrelated Fourier domain\nimg_param = ImageParam(224, fft=True, decorrelate=True)\n\n# Create visualisation\noptvis.vis(img_param, thresh=(250, 500), transform=True, lr=0.05, wd=0.9)\n\n# Denormalise and return the final image\ndenorm(img_param())\n```\n\n### Attribution\n\n```python\nfrom interpret import gradcam, norm\nfrom PIL import Image\nimport torchvision\n\nnetwork = torchvision.models.vgg11(pretrained=True)\ninput_img = Image.open('image.jpg')\n\n# Normalise the input image and turn it into a tensor\ninput_data = norm(input_img)\n\n# Select the class that we are attributing to\nclass_number = 207\n\n# Choose a layer for Grad-CAM\nlayer = 'features/20'\n\n# Generate a Grad-CAM attribution map\nsaliency_map = gradcam(network, input_data, im_class=class_number, layer=layer)\n```\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ttumiel/deep-interp", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "interpret-pytorch", "package_url": "https://pypi.org/project/interpret-pytorch/", "platform": "", "project_url": "https://pypi.org/project/interpret-pytorch/", "project_urls": {"Homepage": "https://github.com/ttumiel/deep-interp"}, "release_url": "https://pypi.org/project/interpret-pytorch/0.1.0/", "requires_dist": ["numpy", "matplotlib", "pandas", "Pillow", "tqdm", "torchvision", "torch", "coverage ; extra == 'test'", "pytest ; extra == 'test'"], "requires_python": ">=3.6", "summary": "Interpreting deep learning models in PyTorch.", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Interpretable Deep Learning</h1>\n<p><img alt=\"Class Visualisations\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/bb4f6d421d2d2e74cf64b2ef59f810f8b1314fe8/2e2f636c6173735f7669732e706e67\"></p>\n<p>A simple to use PyTorch library for interpreting your deep learning results.</p>\n<p><strong>Note: Repo under construction</strong></p>\n<h2>Installation</h2>\n<p>Currently, install from GitHub:</p>\n<pre>pip install git+https://github.com/ttumiel/interpret\n</pre>\n<h3>Dependencies</h3>\n<p><code>interpret</code> requires a working installation of PyTorch.</p>\n<h2>Usage</h2>\n<p><code>interpret</code> can be used for both visualisation and attribution. Here an example using a pretrained network is shown.</p>\n<h3>Visualisation</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">interpret</span> <span class=\"kn\">import</span> <span class=\"n\">OptVis</span><span class=\"p\">,</span> <span class=\"n\">ImageParam</span><span class=\"p\">,</span> <span class=\"n\">denorm</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision</span>\n\n<span class=\"c1\"># Get the PyTorch neural network</span>\n<span class=\"n\">network</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">vgg11</span><span class=\"p\">(</span><span class=\"n\">pretrained</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Select a layer from the network. Use get_layer_names()</span>\n<span class=\"c1\"># to see a list of layer names and sizes.</span>\n<span class=\"n\">layer</span> <span class=\"o\">=</span> <span class=\"s1\">'classifier/6'</span>\n<span class=\"n\">neuron</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n\n<span class=\"c1\"># Create an OptVis object from a PyTorch model</span>\n<span class=\"n\">optvis</span> <span class=\"o\">=</span> <span class=\"n\">OptVis</span><span class=\"o\">.</span><span class=\"n\">from_layer</span><span class=\"p\">(</span><span class=\"n\">network</span><span class=\"p\">,</span> <span class=\"n\">layer</span><span class=\"o\">=</span><span class=\"n\">layer</span><span class=\"p\">,</span> <span class=\"n\">neuron</span><span class=\"o\">=</span><span class=\"n\">neuron</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Parameterise input noise in colour decorrelated Fourier domain</span>\n<span class=\"n\">img_param</span> <span class=\"o\">=</span> <span class=\"n\">ImageParam</span><span class=\"p\">(</span><span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"n\">fft</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">decorrelate</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create visualisation</span>\n<span class=\"n\">optvis</span><span class=\"o\">.</span><span class=\"n\">vis</span><span class=\"p\">(</span><span class=\"n\">img_param</span><span class=\"p\">,</span> <span class=\"n\">thresh</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">250</span><span class=\"p\">,</span> <span class=\"mi\">500</span><span class=\"p\">),</span> <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.05</span><span class=\"p\">,</span> <span class=\"n\">wd</span><span class=\"o\">=</span><span class=\"mf\">0.9</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Denormalise and return the final image</span>\n<span class=\"n\">denorm</span><span class=\"p\">(</span><span class=\"n\">img_param</span><span class=\"p\">())</span>\n</pre>\n<h3>Attribution</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">interpret</span> <span class=\"kn\">import</span> <span class=\"n\">gradcam</span><span class=\"p\">,</span> <span class=\"n\">norm</span>\n<span class=\"kn\">from</span> <span class=\"nn\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision</span>\n\n<span class=\"n\">network</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">vgg11</span><span class=\"p\">(</span><span class=\"n\">pretrained</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">input_img</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"s1\">'image.jpg'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Normalise the input image and turn it into a tensor</span>\n<span class=\"n\">input_data</span> <span class=\"o\">=</span> <span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">input_img</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Select the class that we are attributing to</span>\n<span class=\"n\">class_number</span> <span class=\"o\">=</span> <span class=\"mi\">207</span>\n\n<span class=\"c1\"># Choose a layer for Grad-CAM</span>\n<span class=\"n\">layer</span> <span class=\"o\">=</span> <span class=\"s1\">'features/20'</span>\n\n<span class=\"c1\"># Generate a Grad-CAM attribution map</span>\n<span class=\"n\">saliency_map</span> <span class=\"o\">=</span> <span class=\"n\">gradcam</span><span class=\"p\">(</span><span class=\"n\">network</span><span class=\"p\">,</span> <span class=\"n\">input_data</span><span class=\"p\">,</span> <span class=\"n\">im_class</span><span class=\"o\">=</span><span class=\"n\">class_number</span><span class=\"p\">,</span> <span class=\"n\">layer</span><span class=\"o\">=</span><span class=\"n\">layer</span><span class=\"p\">)</span>\n</pre>\n\n          </div>"}, "last_serial": 6058267, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "3e2c635c73810bbe5c43da81400f2162", "sha256": "848923fa5e17771cdcf7bad5a851f1e891427e85c48fb57c003c87bea6758407"}, "downloads": -1, "filename": "interpret_pytorch-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3e2c635c73810bbe5c43da81400f2162", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 36803, "upload_time": "2019-10-31T13:07:23", "upload_time_iso_8601": "2019-10-31T13:07:23.226847Z", "url": "https://files.pythonhosted.org/packages/3b/1c/c7d932983bccafddc1a0a2f05ad2f2e9d4a5ab4b215ce4ed1460ff070d9e/interpret_pytorch-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a2de971d2dedeff0820839b4b0d4f2e6", "sha256": "6df75fa0e4609a8e27bf6fe67550e2d253b6c7b744f8c019fe23176a66795cf0"}, "downloads": -1, "filename": "interpret-pytorch-0.1.0.tar.gz", "has_sig": false, "md5_digest": "a2de971d2dedeff0820839b4b0d4f2e6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 33277, "upload_time": "2019-10-31T13:07:25", "upload_time_iso_8601": "2019-10-31T13:07:25.812234Z", "url": "https://files.pythonhosted.org/packages/32/30/2ebc24b59bc72c86b7e9a21706f39643470329854aeb18e3e4bd1c9822bb/interpret-pytorch-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3e2c635c73810bbe5c43da81400f2162", "sha256": "848923fa5e17771cdcf7bad5a851f1e891427e85c48fb57c003c87bea6758407"}, "downloads": -1, "filename": "interpret_pytorch-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3e2c635c73810bbe5c43da81400f2162", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 36803, "upload_time": "2019-10-31T13:07:23", "upload_time_iso_8601": "2019-10-31T13:07:23.226847Z", "url": "https://files.pythonhosted.org/packages/3b/1c/c7d932983bccafddc1a0a2f05ad2f2e9d4a5ab4b215ce4ed1460ff070d9e/interpret_pytorch-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a2de971d2dedeff0820839b4b0d4f2e6", "sha256": "6df75fa0e4609a8e27bf6fe67550e2d253b6c7b744f8c019fe23176a66795cf0"}, "downloads": -1, "filename": "interpret-pytorch-0.1.0.tar.gz", "has_sig": false, "md5_digest": "a2de971d2dedeff0820839b4b0d4f2e6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 33277, "upload_time": "2019-10-31T13:07:25", "upload_time_iso_8601": "2019-10-31T13:07:25.812234Z", "url": "https://files.pythonhosted.org/packages/32/30/2ebc24b59bc72c86b7e9a21706f39643470329854aeb18e3e4bd1c9822bb/interpret-pytorch-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:55:26 2020"}