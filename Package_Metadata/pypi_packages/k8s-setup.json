{"info": {"author": "gprossliner", "author_email": "createissue@github.com", "bugtrack_url": null, "classifiers": [], "description": "# General\n\nk8s-setup provides instant access to a configured Kubernetes cluster, based\non [kubeadm](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/).\nIt allows to provision the cluster to a local machine as well as to production.\nIt is made with the help from [Ansible](https://www.ansible.com/), and \n[Vagrant](https://www.vagrantup.com/) for local virtual setup in \n[VirtualBox](https://www.virtualbox.org/).\n\nNote: Currently only Linux is supported for local VM deployment, but it is\ndesigned in a way, that Windows support is possible. It just needs to be implemented.\n\n# How to use\n\n## Install by pip\n\nEvery released version will have a wheel package pushed to PyPI. You can install\nit by `pip install k8s-setup`\n\n## Install by git\n\n1. Clone the repository\n2. Install python (>=2.7) and pip\n3. Install the package editable by `pip install GitPython && pip install --editable .`\n4. [Vagrant](https://www.vagrantup.com/intro/getting-started/install.html) (tested against Vagrant 2.2.4)\n5. VirtualBox (tested against VirtualBox 6.0.12)\n\nThe steps 4. and 5. are only required, if you want to setup a local virtual cluster.\n\n## Provide the configuration\n\n### Local Deployment\n\nFor local vm deployments, the default 'vagrant.yml' file should work.\nYou don't need to provide a custom configuration, if you just want a vm cluster with a single control plane and worker node.\n\nTo access the cluster from your machine, you should have an host record for the\napiserver. The IP is configured by the `k8s_apiserver_vip` configuration setting.\nThe hostname is constructed by the `k8s_apiserver_hostname` and `k8s_cluster_dnsname` settings.\n\nTo generate the correct /etc/hosts file, you can run `k8s-setup generate hostsfile --merge`.\nThe --merge flag instructs the generator to merge the current /etc/hosts file with\nthe generated records.\n\nNOTE: Because write-access to /etc/hosts needs root permissions, you can't just\nsimply redirect the output to /etc/hosts. I used a temporary file, with a move\noperation: `k8s-setup generate hostsfile --merge > /tmp/hosts && sudo mv /tmp/hosts /etc/hosts`\nFirst you should run the generator before running the provisioner, because it needs\na 'apiserver' host. After provisioning is done, run it again, so that the ingress hosts are included.\n\n### Production Deployment\n\nFor production deployments, you need to:\n\n1. Create an Ansible inventory file, with the machines in it.\nYou need to assign the host to these groups:\n    * lnxclp: All Linux control plane nodes\n    * lnxclp_setup: One of the Linux control plane nodes, which will be the first\n    control plane instance\n    * lnxwrk: All Linux worker nodes\n    * winwrk: All Windows worker nodes\n\n2. Create a .yml file, representing variables of your environment.\nYou can check the provided files in '/conf'. The 'default.yml' contains the \nsystem default settings. You can override them in your custom configuration file.\n\n3. Register a custom configuration by executing `k8s-setup config set --file <path>`. \nThe path can be absolute, or relative to the repository root. By default the\n`./conf/vagrant.yml` is selected.\n\nThis Information is stored in `~/k8s-setup/current-config`. It is persistent, so normally you only have to execute it once.\n4. You may verify if everything is ok by running `k8s-setup info`\n\n## Provide the configuration in an own repository\n\nk8s-setup doesn't care, where the config file is coming from. Just clone the\nrepository containing your configuration file, and register it by `k8s-setup config <path>`.\n\n## Running the provisioners\n\nBy executing `k8s-setup provision` you start the provisioning process.\n\nBecause provisioning is idempotent, you can always use provision 'all'. The ability\nto select a scope explicitly is just a time-saver, when you know what has hanged.\nThis is basically to cut wait-time when developing and testing k8s-setup.\nIf you don't know what has changed exacly, always provision the 'all' scope.\n\nThe following steps will be performed in the 'vagrant' mode:\n1. The configuration is validated.\n2. Vagrant only: The relevant configuration settings are reflected \nin environment variables.\n3. Vagrant only: The `./lib/vagrant/Vagrantfile` is used to start the VMs, \ndepending on the reflected environmet variables.\n4. Vagrant only: The Vagrantfile declares the following provisioners:\n    * Host: Updates the `/etc/hosts` files on each machine, because we have no\n    DNS server in the network.\n    * Ansible: Runs the `./lib/ansible/vagrant.yml` playbook. This playbook only\n    performs connectivity tests. The provisioning playbooks are launched later.\n    The Ansible provisioner also generates an inventory file, which is used in \n    the next step.\n5. Depending on the scope, the following playbooks are executed:\n    * all (default): Runs hosts, cluster and incluster playbooks.\n    * hosts: Provisions the machines so that everything is installed and OS level\n    configuration is applied, but no kubeadm operation to deploy the cluster was\n    performed.\n    * cluster: Provisions by kubeadm operations like `kubeadm init` or \n    `kubeadm join` to initialize the cluster, or add new nodes.\n    * incluster: Provisions all kubernetes objects in an existing cluster.\n\n\n## Get Information\n\nBy using the `k8s-setup info` command, you get some metadata of the k8s-setup \nstate and configuration.\n\n```\n$ k8s-setup info\nconfig-files:\n- conf/defaults.yml\n- localpath/k8s-setup/conf/vagrant.yml\nconfiguration:\n  ansible_inventory_filepath: lib/vagrant/.vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory\n  cluster-dns-name: k8stest.local\n  k8s-version: 1.16.3\n  lnxclp-nodes: 1\n  lnxwrk-nodes: 1\n  mode: vagrant\n  winwrk-nodes: 0\nversion: v0.1-alpha-base-7-gd978c740a5c526+dirty\n```\n\nWhen running the 'cluster' provisioning scope, a ConfigMap named 'k8s-setup-info'\nis created in the 'default' namespace. It contains this information in the 'setup-info' field.\nThere is an additional field named 'sys-info', containing information like the\ndate, the user of the name of the provisioning host.\n\nPlease note that the provisioning logic don't read this data. It only serves\ninformational purposes.\n\n## Enable Shell Completion\n\nWith the help of the wonderfull [click](https://click.palletsprojects.com/) library\nk8s-setup has buildin completion for bash and zsh.\n\nYou need to activate this, like:\n\n```sh\n\n# bash (you may put this in .bashrc)\neval \"$(_K8S_SETUP_COMPLETE=source k8s-setup)\"\n\n# zsh (you may put this in .zshrc)\neval \"$(_K8S_SETUP_COMPLETE=source_zsh k8s-setup)\"\n```\n\n## Output debug Messages\n\nYou can enable Debug level logging, by setting the `K8S_SETUP_DEBUG` environment\nvariable to '1'.\n\nYou can also use the --debug command line option.\n\n# Vagrant Development Environment\n\n## Networking\n\nk8s-setup uses a default IP plan to setup the cluster network:\nThere is a configurable /24 network which is used for the Vagrant boxes.\n\n```yaml\n# conf/defaults.yml\nglobal_vagrant_hosts_network: 10.0.0.*\n```\n\nYou only need to change this settings, if you have conflicting IP addresses\nin your LAN.\n\nThe following addresses are used:\n\n* 10.0.0.1: Reserver (Gateway)\n* 10.0.0.2: Virtual IP (keepalived) for apiserver\n* 10.0.0.10-19: Control plane nodes\n* 10.0.0.20-29: Linux worker nodes\n* 10.0.0.30-39: Windows worker nodes\n* 10.0.0.40-49: None-cluster nodes (like test-clients or AD server)\n\nAfter the hosts are provisioned, you get a route by virtualbox, like:\n\n```bash\n$ ip route | grep 10.0.0.0\n10.0.0.0/24 dev vboxnet3 proto kernel scope link src 10.0.0.1 \n```\n\nSo you can just access the network from you host. You may add an entry in your\n/etc/hosts file, like `10.0.0.2 apiserver.k8stest.local`\n\n\nhttps://setuptools.readthedocs.io/en/latest/setuptools.html#automatic-resource-extraction\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/world-direct/k8s-setup", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "k8s-setup", "package_url": "https://pypi.org/project/k8s-setup/", "platform": "", "project_url": "https://pypi.org/project/k8s-setup/", "project_urls": {"Homepage": "https://github.com/world-direct/k8s-setup"}, "release_url": "https://pypi.org/project/k8s-setup/0.2.2.dev0/", "requires_dist": ["click (>=7.0)", "ansible (>=2.8.6)", "kubernetes (>=10.0)", "colorlog (>=4.1.0)", "pyyaml (>=5.2)"], "requires_python": "", "summary": "Setup local and production hybrid clusters, using Ansible and Vagrant", "version": "0.2.2.dev0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>General</h1>\n<p>k8s-setup provides instant access to a configured Kubernetes cluster, based\non <a href=\"https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/\" rel=\"nofollow\">kubeadm</a>.\nIt allows to provision the cluster to a local machine as well as to production.\nIt is made with the help from <a href=\"https://www.ansible.com/\" rel=\"nofollow\">Ansible</a>, and\n<a href=\"https://www.vagrantup.com/\" rel=\"nofollow\">Vagrant</a> for local virtual setup in\n<a href=\"https://www.virtualbox.org/\" rel=\"nofollow\">VirtualBox</a>.</p>\n<p>Note: Currently only Linux is supported for local VM deployment, but it is\ndesigned in a way, that Windows support is possible. It just needs to be implemented.</p>\n<h1>How to use</h1>\n<h2>Install by pip</h2>\n<p>Every released version will have a wheel package pushed to PyPI. You can install\nit by <code>pip install k8s-setup</code></p>\n<h2>Install by git</h2>\n<ol>\n<li>Clone the repository</li>\n<li>Install python (&gt;=2.7) and pip</li>\n<li>Install the package editable by <code>pip install GitPython &amp;&amp; pip install --editable .</code></li>\n<li><a href=\"https://www.vagrantup.com/intro/getting-started/install.html\" rel=\"nofollow\">Vagrant</a> (tested against Vagrant 2.2.4)</li>\n<li>VirtualBox (tested against VirtualBox 6.0.12)</li>\n</ol>\n<p>The steps 4. and 5. are only required, if you want to setup a local virtual cluster.</p>\n<h2>Provide the configuration</h2>\n<h3>Local Deployment</h3>\n<p>For local vm deployments, the default 'vagrant.yml' file should work.\nYou don't need to provide a custom configuration, if you just want a vm cluster with a single control plane and worker node.</p>\n<p>To access the cluster from your machine, you should have an host record for the\napiserver. The IP is configured by the <code>k8s_apiserver_vip</code> configuration setting.\nThe hostname is constructed by the <code>k8s_apiserver_hostname</code> and <code>k8s_cluster_dnsname</code> settings.</p>\n<p>To generate the correct /etc/hosts file, you can run <code>k8s-setup generate hostsfile --merge</code>.\nThe --merge flag instructs the generator to merge the current /etc/hosts file with\nthe generated records.</p>\n<p>NOTE: Because write-access to /etc/hosts needs root permissions, you can't just\nsimply redirect the output to /etc/hosts. I used a temporary file, with a move\noperation: <code>k8s-setup generate hostsfile --merge &gt; /tmp/hosts &amp;&amp; sudo mv /tmp/hosts /etc/hosts</code>\nFirst you should run the generator before running the provisioner, because it needs\na 'apiserver' host. After provisioning is done, run it again, so that the ingress hosts are included.</p>\n<h3>Production Deployment</h3>\n<p>For production deployments, you need to:</p>\n<ol>\n<li>\n<p>Create an Ansible inventory file, with the machines in it.\nYou need to assign the host to these groups:</p>\n<ul>\n<li>lnxclp: All Linux control plane nodes</li>\n<li>lnxclp_setup: One of the Linux control plane nodes, which will be the first\ncontrol plane instance</li>\n<li>lnxwrk: All Linux worker nodes</li>\n<li>winwrk: All Windows worker nodes</li>\n</ul>\n</li>\n<li>\n<p>Create a .yml file, representing variables of your environment.\nYou can check the provided files in '/conf'. The 'default.yml' contains the\nsystem default settings. You can override them in your custom configuration file.</p>\n</li>\n<li>\n<p>Register a custom configuration by executing <code>k8s-setup config set --file &lt;path&gt;</code>.\nThe path can be absolute, or relative to the repository root. By default the\n<code>./conf/vagrant.yml</code> is selected.</p>\n</li>\n</ol>\n<p>This Information is stored in <code>~/k8s-setup/current-config</code>. It is persistent, so normally you only have to execute it once.\n4. You may verify if everything is ok by running <code>k8s-setup info</code></p>\n<h2>Provide the configuration in an own repository</h2>\n<p>k8s-setup doesn't care, where the config file is coming from. Just clone the\nrepository containing your configuration file, and register it by <code>k8s-setup config &lt;path&gt;</code>.</p>\n<h2>Running the provisioners</h2>\n<p>By executing <code>k8s-setup provision</code> you start the provisioning process.</p>\n<p>Because provisioning is idempotent, you can always use provision 'all'. The ability\nto select a scope explicitly is just a time-saver, when you know what has hanged.\nThis is basically to cut wait-time when developing and testing k8s-setup.\nIf you don't know what has changed exacly, always provision the 'all' scope.</p>\n<p>The following steps will be performed in the 'vagrant' mode:</p>\n<ol>\n<li>The configuration is validated.</li>\n<li>Vagrant only: The relevant configuration settings are reflected\nin environment variables.</li>\n<li>Vagrant only: The <code>./lib/vagrant/Vagrantfile</code> is used to start the VMs,\ndepending on the reflected environmet variables.</li>\n<li>Vagrant only: The Vagrantfile declares the following provisioners:\n<ul>\n<li>Host: Updates the <code>/etc/hosts</code> files on each machine, because we have no\nDNS server in the network.</li>\n<li>Ansible: Runs the <code>./lib/ansible/vagrant.yml</code> playbook. This playbook only\nperforms connectivity tests. The provisioning playbooks are launched later.\nThe Ansible provisioner also generates an inventory file, which is used in\nthe next step.</li>\n</ul>\n</li>\n<li>Depending on the scope, the following playbooks are executed:\n<ul>\n<li>all (default): Runs hosts, cluster and incluster playbooks.</li>\n<li>hosts: Provisions the machines so that everything is installed and OS level\nconfiguration is applied, but no kubeadm operation to deploy the cluster was\nperformed.</li>\n<li>cluster: Provisions by kubeadm operations like <code>kubeadm init</code> or\n<code>kubeadm join</code> to initialize the cluster, or add new nodes.</li>\n<li>incluster: Provisions all kubernetes objects in an existing cluster.</li>\n</ul>\n</li>\n</ol>\n<h2>Get Information</h2>\n<p>By using the <code>k8s-setup info</code> command, you get some metadata of the k8s-setup\nstate and configuration.</p>\n<pre><code>$ k8s-setup info\nconfig-files:\n- conf/defaults.yml\n- localpath/k8s-setup/conf/vagrant.yml\nconfiguration:\n  ansible_inventory_filepath: lib/vagrant/.vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory\n  cluster-dns-name: k8stest.local\n  k8s-version: 1.16.3\n  lnxclp-nodes: 1\n  lnxwrk-nodes: 1\n  mode: vagrant\n  winwrk-nodes: 0\nversion: v0.1-alpha-base-7-gd978c740a5c526+dirty\n</code></pre>\n<p>When running the 'cluster' provisioning scope, a ConfigMap named 'k8s-setup-info'\nis created in the 'default' namespace. It contains this information in the 'setup-info' field.\nThere is an additional field named 'sys-info', containing information like the\ndate, the user of the name of the provisioning host.</p>\n<p>Please note that the provisioning logic don't read this data. It only serves\ninformational purposes.</p>\n<h2>Enable Shell Completion</h2>\n<p>With the help of the wonderfull <a href=\"https://click.palletsprojects.com/\" rel=\"nofollow\">click</a> library\nk8s-setup has buildin completion for bash and zsh.</p>\n<p>You need to activate this, like:</p>\n<pre><span class=\"c1\"># bash (you may put this in .bashrc)</span>\n<span class=\"nb\">eval</span> <span class=\"s2\">\"</span><span class=\"k\">$(</span><span class=\"nv\">_K8S_SETUP_COMPLETE</span><span class=\"o\">=</span><span class=\"nb\">source</span> k8s-setup<span class=\"k\">)</span><span class=\"s2\">\"</span>\n\n<span class=\"c1\"># zsh (you may put this in .zshrc)</span>\n<span class=\"nb\">eval</span> <span class=\"s2\">\"</span><span class=\"k\">$(</span><span class=\"nv\">_K8S_SETUP_COMPLETE</span><span class=\"o\">=</span>source_zsh k8s-setup<span class=\"k\">)</span><span class=\"s2\">\"</span>\n</pre>\n<h2>Output debug Messages</h2>\n<p>You can enable Debug level logging, by setting the <code>K8S_SETUP_DEBUG</code> environment\nvariable to '1'.</p>\n<p>You can also use the --debug command line option.</p>\n<h1>Vagrant Development Environment</h1>\n<h2>Networking</h2>\n<p>k8s-setup uses a default IP plan to setup the cluster network:\nThere is a configurable /24 network which is used for the Vagrant boxes.</p>\n<pre><span class=\"c1\"># conf/defaults.yml</span>\n<span class=\"nt\">global_vagrant_hosts_network</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">10.0.0.*</span>\n</pre>\n<p>You only need to change this settings, if you have conflicting IP addresses\nin your LAN.</p>\n<p>The following addresses are used:</p>\n<ul>\n<li>10.0.0.1: Reserver (Gateway)</li>\n<li>10.0.0.2: Virtual IP (keepalived) for apiserver</li>\n<li>10.0.0.10-19: Control plane nodes</li>\n<li>10.0.0.20-29: Linux worker nodes</li>\n<li>10.0.0.30-39: Windows worker nodes</li>\n<li>10.0.0.40-49: None-cluster nodes (like test-clients or AD server)</li>\n</ul>\n<p>After the hosts are provisioned, you get a route by virtualbox, like:</p>\n<pre>$ ip route <span class=\"p\">|</span> grep <span class=\"m\">10</span>.0.0.0\n<span class=\"m\">10</span>.0.0.0/24 dev vboxnet3 proto kernel scope link src <span class=\"m\">10</span>.0.0.1 \n</pre>\n<p>So you can just access the network from you host. You may add an entry in your\n/etc/hosts file, like <code>10.0.0.2 apiserver.k8stest.local</code></p>\n<p><a href=\"https://setuptools.readthedocs.io/en/latest/setuptools.html#automatic-resource-extraction\" rel=\"nofollow\">https://setuptools.readthedocs.io/en/latest/setuptools.html#automatic-resource-extraction</a></p>\n\n          </div>"}, "last_serial": 6537838, "releases": {"0.2.1.dev0": [{"comment_text": "", "digests": {"md5": "7c5d2ef190f09d8f6b6138025cc11472", "sha256": "93d6f6680062f69ca5deacd4f4c338c5e74219afe94b7682ee088461c46de1b9"}, "downloads": -1, "filename": "k8s_setup-0.2.1.dev0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "7c5d2ef190f09d8f6b6138025cc11472", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 218414, "upload_time": "2020-01-28T16:08:13", "upload_time_iso_8601": "2020-01-28T16:08:13.726054Z", "url": "https://files.pythonhosted.org/packages/b4/a6/913b46f3709e78f713f9f0b455411676445ae5d1aabcfb03aa01f586af9e/k8s_setup-0.2.1.dev0-py2.py3-none-any.whl", "yanked": false}], "0.2.2.dev0": [{"comment_text": "", "digests": {"md5": "495e987842740dc1e3ec40b3b495abd6", "sha256": "f5f2abcad1c5dc6461c0543a5b80a66431b69143a977d9343df410362cc63782"}, "downloads": -1, "filename": "k8s_setup-0.2.2.dev0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "495e987842740dc1e3ec40b3b495abd6", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 218341, "upload_time": "2020-01-29T13:20:25", "upload_time_iso_8601": "2020-01-29T13:20:25.012988Z", "url": "https://files.pythonhosted.org/packages/f9/f3/090c2f34878aee4742cf845ea94d18dcfc52e28f931d0eadbb60dc109072/k8s_setup-0.2.2.dev0-py2.py3-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "495e987842740dc1e3ec40b3b495abd6", "sha256": "f5f2abcad1c5dc6461c0543a5b80a66431b69143a977d9343df410362cc63782"}, "downloads": -1, "filename": "k8s_setup-0.2.2.dev0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "495e987842740dc1e3ec40b3b495abd6", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 218341, "upload_time": "2020-01-29T13:20:25", "upload_time_iso_8601": "2020-01-29T13:20:25.012988Z", "url": "https://files.pythonhosted.org/packages/f9/f3/090c2f34878aee4742cf845ea94d18dcfc52e28f931d0eadbb60dc109072/k8s_setup-0.2.2.dev0-py2.py3-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 00:50:54 2020"}