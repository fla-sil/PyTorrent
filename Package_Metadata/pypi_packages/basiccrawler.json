{"info": {"author": "Learning Equality", "author_email": "ivan@learningequalty.org", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "BasicCrawler\n============\n\nSemi-automated crawling bot with special features for extracting website\nstructure automatically.\n\n::\n\n    from basiccrawler.crawler import BasicCrawler\n\n    SOURCE_DOMAIN='http://learningequality.org'\n    start_page = 'http://learningequality.org/kolibri/'\n\n    class LECrawler(BasicCrawler):\n        pass\n\n    crawler = LECrawler(main_source_domain=SOURCE_DOMAIN,\n                        start_page=start_page)\n\n    web_resource_tree = crawler.crawl()\n\nThe crawler concludes will summary of the findings (according to crude\nheuristics).\n\n::\n\n    # CRAWLER RECOMMENDATIONS BASED ON URLS ENCOUNTERED:\n    ################################################################################\n    1. These URLs are very common and look like global navigation links:\n      -  http://learningequality.org/about/team/\n      -  http://learningequality.org/about/board/\n      -  http://learningequality.org/about/supporters/\n      -  ...\n    2. These are common path fragments found in URLs paths, so could correspond to site struture:\n      -  ...\n    ################################################################################\n\nThe web resource tree contains the information about the site structure\nat a high level (``print_depth=3``) or in full detail\n(``print_depth=100``). For example:\n\n::\n\n    crawler.print_tree(web_resource_tree, print_depth=4)\n\n         - path: /kolibri/  (PageWebResource) \n           children:\n            - path: /  (PageWebResource) \n              children:\n               - path: /media/Rapport-Etude-Cameroun_KL_ENG.pdf  (MediaWebResource) \n            - path: /about/  (PageWebResource) \n              children:\n               - path: /ka-lite/map/  (PageWebResource) \n            - path: /about/values/  (PageWebResource) \n            - path: /about/team/  (PageWebResource) \n            - path: /about/board/  (PageWebResource) \n            - path: /about/supporters/  (PageWebResource) \n            - path: /about/press/  (PageWebResource) \n            - path: /about/jobs/  (PageWebResource) \n            - path: /about/internships/  (PageWebResource) \n              children:\n               - path: https://learningequality.org/about/jobs/?gh_jid=533166  (PageWebResource) \n            - path: /download/  (PageWebResource) \n            - path: /documentation/  (PageWebResource) \n            - path: /hardware_grant/  (PageWebResource) \n            - path: /ka-lite/  (PageWebResource) \n              children:\n               - path: /ka-lite/infographic/  (PageWebResource) \n            - path: /translate/  (PageWebResource) \n            - path: https://blog.learningequality.org/?gi=2589e076ea04  (PageWebResource) \n            - path: /ka-lite/map/add/  (PageWebResource) \n            - path: /donate/  (PageWebResource) \n              children:\n               - path: /static/doc/learning_equality_irs_determination_letter.pdf  (MediaWebResource) \n            - path: /cdn-cgi/l/email-protection  (PageWebResource) \n\nFor this crawl, we didn\u2019t find too many educational materials\n(docs/videos/audio/webapps), but at least we get some idea of the links\non that page. Try it on another website.\n\nExample usage\n-------------\n\nhttps://github.com/learningequality/sushi-chef-tessa/blob/master/tessa_cralwer.py#L229\n\nTODO\n----\n\n-  Update examples + notebooks\n-  path to url / vice versa (and possibly elsewhere): consider\n   ``urllib.urlparse``? [e.g. ``url.startwith(source_domain)`` could be\n   ``source_domain in url.domain`` to make it more flexible with\n   subdomains\n\n   -  Additional valid domains can be specified but ``url_to_path_list``\n      assumes adding CHANNEL_ROOT_DOMAIN [we may wish to expand all\n      links based on parent URL]\n   -  refactor and remove need for MAIN_SOURCE_DOMAIN and use only\n      SOURCE_DOMAINS instead\n\nFuture feature ideas\n--------------------\n\n-  Asynchronous download (not necessary but might be good for\n   performance on large sites)\n\n   -  don\u2019t block for HTTP\n   -  allow multiple workers getting from queue\n\n-  content_selector hints for default ``on_page`` handler to follow\n   links only within a certain subset of the HTML tree. Can have:\n\n   -  site-wide selector at class level\n   -  pass in additional ``content_selector`` from referring page via\n      context dict\n\n-  Automatically detect standard embed tags (audio, video, pdfs) and add\n   links to web resource tree in default ``on_page`` handler.\n\nCrawler API\n-----------\n\nThe goal of the ``BasicCrawler`` class is to help with the initial\nexploration of the source website. It is your responsibility to write a\nsubclass that uses the HTML, URL structure, and content to guide the\ncrawling and produce the web resource tree.\n\nYour crawler should inherit from ``BasicCrawler`` and define:\n\n1. What site we\u2019re crawling and where to start:\n\n   -  set the following attributes\n\n      -  ``MAIN_SOURCE_DOMAIN`` e.g. ``'https://learningequality.org'``\n         or pass as arg ``main_source_domain`` at creation time.\n      -  ``START_PAGE`` e.g. ``'https://learningequality.org/'`` or pass\n         at creation time as ``start_page``.\n\n   -  ``IGNORE_URLS=[]``: crawler will ignore these URLs (can be\n      specified as str, re, or callable)\n   -  ``CRAWLING_STAGE_OUTPUT='chefdata/trees/web_resource_tree.json'``:\n      where the output of the crawling will be stored\n\n2. Run for the first time by calling ``crawler.crawl()`` or as a command\n   line script\n\n-  The BasicCrawler has logic for visiting pages and will print out on\n   the a summary of the auto inferred site stricture findings and\n   recommendations based on the URL structure observed during the\n   initial crawl.\n-  Based on the number of times a link appears on different pages of the\n   site the crawler will suggest to you candidates for global navigation\n   links. Most websites have an /about page, /contact us, and other such\n   non-content-containing pages, which we do not want to include in the\n   web resource tree. You should inspect these suggestions and decide\n   which should be ignored (i.e.\u00a0not crawled or included in the\n   web_resource_tree output). To ignore URLs you can edit the\n   attributes:\n\n   -  ``IGNORE_URLS``: crawler will ignore these URLs Edit your crawler\n      subclass\u2019 code and append to ``IGNORE_URLS`` the URLs you want to\n      skip (anything that is not likely to contain content).\n\n3. Run the crawler again, this time there should be less noise in the\n   output.\n\n-  Note the suggestion for different paths that you might want to handle\n   specially (e.g. ``/course``, ``/lesson``, ``/content``, etc.) You can\n   define class methods to handle each of these URL types:\n\n   ::\n\n        def on_course(self, url, page, context):\n            # what do you want the crawler to do when it visits the  course with `url`\n            # in the `context` (used for extra metadata; contains reference to parent)\n            # The BeautifulSoup parsed contents of the `url` are provided as `page`.\n\n        def on_lesson(self, url, page, context):\n            # what do you want the crawler to do when it visits the lesson\n\n        def on_content(self, url, page, context):\n            # what do you want the crawler to do when it visits the content url\n\nCheck out the default ``on_page`` method so see how a web resource tree\nis constructed:\nhttps://github.com/learningequality/BasicCrawler/blob/master/basiccrawler/crawler.py#L212\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/learningequality/BasicCrawler", "keywords": "basiccrawler", "license": "MIT license", "maintainer": "", "maintainer_email": "", "name": "basiccrawler", "package_url": "https://pypi.org/project/basiccrawler/", "platform": "", "project_url": "https://pypi.org/project/basiccrawler/", "project_urls": {"Homepage": "https://github.com/learningequality/BasicCrawler"}, "release_url": "https://pypi.org/project/basiccrawler/0.2.0/", "requires_dist": null, "requires_python": "", "summary": "Basic web crawler that automates website exploration and producing web resource trees.", "version": "0.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Semi-automated crawling bot with special features for extracting website\nstructure automatically.</p>\n<pre>from basiccrawler.crawler import BasicCrawler\n\nSOURCE_DOMAIN='http://learningequality.org'\nstart_page = 'http://learningequality.org/kolibri/'\n\nclass LECrawler(BasicCrawler):\n    pass\n\ncrawler = LECrawler(main_source_domain=SOURCE_DOMAIN,\n                    start_page=start_page)\n\nweb_resource_tree = crawler.crawl()\n</pre>\n<p>The crawler concludes will summary of the findings (according to crude\nheuristics).</p>\n<pre># CRAWLER RECOMMENDATIONS BASED ON URLS ENCOUNTERED:\n################################################################################\n1. These URLs are very common and look like global navigation links:\n  -  http://learningequality.org/about/team/\n  -  http://learningequality.org/about/board/\n  -  http://learningequality.org/about/supporters/\n  -  ...\n2. These are common path fragments found in URLs paths, so could correspond to site struture:\n  -  ...\n################################################################################\n</pre>\n<p>The web resource tree contains the information about the site structure\nat a high level (<tt>print_depth=3</tt>) or in full detail\n(<tt>print_depth=100</tt>). For example:</p>\n<pre>crawler.print_tree(web_resource_tree, print_depth=4)\n\n     - path: /kolibri/  (PageWebResource)\n       children:\n        - path: /  (PageWebResource)\n          children:\n           - path: /media/Rapport-Etude-Cameroun_KL_ENG.pdf  (MediaWebResource)\n        - path: /about/  (PageWebResource)\n          children:\n           - path: /ka-lite/map/  (PageWebResource)\n        - path: /about/values/  (PageWebResource)\n        - path: /about/team/  (PageWebResource)\n        - path: /about/board/  (PageWebResource)\n        - path: /about/supporters/  (PageWebResource)\n        - path: /about/press/  (PageWebResource)\n        - path: /about/jobs/  (PageWebResource)\n        - path: /about/internships/  (PageWebResource)\n          children:\n           - path: https://learningequality.org/about/jobs/?gh_jid=533166  (PageWebResource)\n        - path: /download/  (PageWebResource)\n        - path: /documentation/  (PageWebResource)\n        - path: /hardware_grant/  (PageWebResource)\n        - path: /ka-lite/  (PageWebResource)\n          children:\n           - path: /ka-lite/infographic/  (PageWebResource)\n        - path: /translate/  (PageWebResource)\n        - path: https://blog.learningequality.org/?gi=2589e076ea04  (PageWebResource)\n        - path: /ka-lite/map/add/  (PageWebResource)\n        - path: /donate/  (PageWebResource)\n          children:\n           - path: /static/doc/learning_equality_irs_determination_letter.pdf  (MediaWebResource)\n        - path: /cdn-cgi/l/email-protection  (PageWebResource)\n</pre>\n<p>For this crawl, we didn\u2019t find too many educational materials\n(docs/videos/audio/webapps), but at least we get some idea of the links\non that page. Try it on another website.</p>\n<div id=\"example-usage\">\n<h2>Example usage</h2>\n<p><a href=\"https://github.com/learningequality/sushi-chef-tessa/blob/master/tessa_cralwer.py#L229\" rel=\"nofollow\">https://github.com/learningequality/sushi-chef-tessa/blob/master/tessa_cralwer.py#L229</a></p>\n</div>\n<div id=\"todo\">\n<h2>TODO</h2>\n<ul>\n<li>Update examples + notebooks</li>\n<li>path to url / vice versa (and possibly elsewhere): consider\n<tt>urllib.urlparse</tt>? [e.g. <tt>url.startwith(source_domain)</tt> could be\n<tt>source_domain in url.domain</tt> to make it more flexible with\nsubdomains<ul>\n<li>Additional valid domains can be specified but <tt>url_to_path_list</tt>\nassumes adding CHANNEL_ROOT_DOMAIN [we may wish to expand all\nlinks based on parent URL]</li>\n<li>refactor and remove need for MAIN_SOURCE_DOMAIN and use only\nSOURCE_DOMAINS instead</li>\n</ul>\n</li>\n</ul>\n</div>\n<div id=\"future-feature-ideas\">\n<h2>Future feature ideas</h2>\n<ul>\n<li>Asynchronous download (not necessary but might be good for\nperformance on large sites)<ul>\n<li>don\u2019t block for HTTP</li>\n<li>allow multiple workers getting from queue</li>\n</ul>\n</li>\n<li>content_selector hints for default <tt>on_page</tt> handler to follow\nlinks only within a certain subset of the HTML tree. Can have:<ul>\n<li>site-wide selector at class level</li>\n<li>pass in additional <tt>content_selector</tt> from referring page via\ncontext dict</li>\n</ul>\n</li>\n<li>Automatically detect standard embed tags (audio, video, pdfs) and add\nlinks to web resource tree in default <tt>on_page</tt> handler.</li>\n</ul>\n</div>\n<div id=\"crawler-api\">\n<h2>Crawler API</h2>\n<p>The goal of the <tt>BasicCrawler</tt> class is to help with the initial\nexploration of the source website. It is your responsibility to write a\nsubclass that uses the HTML, URL structure, and content to guide the\ncrawling and produce the web resource tree.</p>\n<p>Your crawler should inherit from <tt>BasicCrawler</tt> and define:</p>\n<ol>\n<li>What site we\u2019re crawling and where to start:<ul>\n<li>set the following attributes<ul>\n<li><tt>MAIN_SOURCE_DOMAIN</tt> e.g. <tt><span class=\"pre\">'https://learningequality.org'</span></tt>\nor pass as arg <tt>main_source_domain</tt> at creation time.</li>\n<li><tt>START_PAGE</tt> e.g. <tt><span class=\"pre\">'https://learningequality.org/'</span></tt> or pass\nat creation time as <tt>start_page</tt>.</li>\n</ul>\n</li>\n<li><tt><span class=\"pre\">IGNORE_URLS=[]</span></tt>: crawler will ignore these URLs (can be\nspecified as str, re, or callable)</li>\n<li><tt><span class=\"pre\">CRAWLING_STAGE_OUTPUT='chefdata/trees/web_resource_tree.json'</span></tt>:\nwhere the output of the crawling will be stored</li>\n</ul>\n</li>\n<li>Run for the first time by calling <tt>crawler.crawl()</tt> or as a command\nline script</li>\n</ol>\n<ul>\n<li>The BasicCrawler has logic for visiting pages and will print out on\nthe a summary of the auto inferred site stricture findings and\nrecommendations based on the URL structure observed during the\ninitial crawl.</li>\n<li>Based on the number of times a link appears on different pages of the\nsite the crawler will suggest to you candidates for global navigation\nlinks. Most websites have an /about page, /contact us, and other such\nnon-content-containing pages, which we do not want to include in the\nweb resource tree. You should inspect these suggestions and decide\nwhich should be ignored (i.e.\u00a0not crawled or included in the\nweb_resource_tree output). To ignore URLs you can edit the\nattributes:<ul>\n<li><tt>IGNORE_URLS</tt>: crawler will ignore these URLs Edit your crawler\nsubclass\u2019 code and append to <tt>IGNORE_URLS</tt> the URLs you want to\nskip (anything that is not likely to contain content).</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li>Run the crawler again, this time there should be less noise in the\noutput.</li>\n</ol>\n<ul>\n<li><p>Note the suggestion for different paths that you might want to handle\nspecially (e.g. <tt>/course</tt>, <tt>/lesson</tt>, <tt>/content</tt>, etc.) You can\ndefine class methods to handle each of these URL types:</p>\n<pre>def on_course(self, url, page, context):\n    # what do you want the crawler to do when it visits the  course with `url`\n    # in the `context` (used for extra metadata; contains reference to parent)\n    # The BeautifulSoup parsed contents of the `url` are provided as `page`.\n\ndef on_lesson(self, url, page, context):\n    # what do you want the crawler to do when it visits the lesson\n\ndef on_content(self, url, page, context):\n    # what do you want the crawler to do when it visits the content url\n</pre>\n</li>\n</ul>\n<p>Check out the default <tt>on_page</tt> method so see how a web resource tree\nis constructed:\n<a href=\"https://github.com/learningequality/BasicCrawler/blob/master/basiccrawler/crawler.py#L212\" rel=\"nofollow\">https://github.com/learningequality/BasicCrawler/blob/master/basiccrawler/crawler.py#L212</a></p>\n</div>\n\n          </div>"}, "last_serial": 6614095, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "a76b1ae29a26f5e7c9611ada4c62ec7d", "sha256": "46200abbd594894915756291548c34cd57f2983ce2ee55d6709d3e6138510cc5"}, "downloads": -1, "filename": "basiccrawler-0.1.0.tar.gz", "has_sig": false, "md5_digest": "a76b1ae29a26f5e7c9611ada4c62ec7d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14731, "upload_time": "2017-11-21T17:18:09", "upload_time_iso_8601": "2017-11-21T17:18:09.349256Z", "url": "https://files.pythonhosted.org/packages/67/a0/6d159230c76de46e082c09b6c0731c518cdec035baf940dde5321319f1f8/basiccrawler-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "d25fb1960374dc569e35dbebf70c1f7a", "sha256": "510328ef70a219e0d50a45fa82ae620c2bace84031f3cf61334724d74f3e041f"}, "downloads": -1, "filename": "basiccrawler-0.1.1.tar.gz", "has_sig": false, "md5_digest": "d25fb1960374dc569e35dbebf70c1f7a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 14782, "upload_time": "2017-11-21T20:55:36", "upload_time_iso_8601": "2017-11-21T20:55:36.425960Z", "url": "https://files.pythonhosted.org/packages/02/78/80a3b9a853c95128da4dd0db7ca71385b1db17427f7c1552cbb7d650185a/basiccrawler-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "e264c49cd719b72fdd2cac5f1b549182", "sha256": "060968468028acece57a29372513f8699353c8b30d3f16e6dfa6fd72bf1d5214"}, "downloads": -1, "filename": "basiccrawler-0.1.2.tar.gz", "has_sig": false, "md5_digest": "e264c49cd719b72fdd2cac5f1b549182", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15728, "upload_time": "2017-11-27T16:32:00", "upload_time_iso_8601": "2017-11-27T16:32:00.839413Z", "url": "https://files.pythonhosted.org/packages/53/2f/e31ac0f0037541af9e3398f6884090dd96fad479efc1a2c753b68f3fc22b/basiccrawler-0.1.2.tar.gz", "yanked": false}], "0.2.0": [{"comment_text": "", "digests": {"md5": "667080df22febbe6139df3e0a5898ad6", "sha256": "a7a2774aa87b4e1d6eb4bb8c1e5eb98c7a39a772d8c82db1c43d9955bffa50a9"}, "downloads": -1, "filename": "basiccrawler-0.2.0.tar.gz", "has_sig": false, "md5_digest": "667080df22febbe6139df3e0a5898ad6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19628, "upload_time": "2020-02-12T04:36:12", "upload_time_iso_8601": "2020-02-12T04:36:12.621296Z", "url": "https://files.pythonhosted.org/packages/77/65/068fbdd01e0b033d4e4c17eb167e43a9c9d3a68f650578724bd71b60f36b/basiccrawler-0.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "667080df22febbe6139df3e0a5898ad6", "sha256": "a7a2774aa87b4e1d6eb4bb8c1e5eb98c7a39a772d8c82db1c43d9955bffa50a9"}, "downloads": -1, "filename": "basiccrawler-0.2.0.tar.gz", "has_sig": false, "md5_digest": "667080df22febbe6139df3e0a5898ad6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 19628, "upload_time": "2020-02-12T04:36:12", "upload_time_iso_8601": "2020-02-12T04:36:12.621296Z", "url": "https://files.pythonhosted.org/packages/77/65/068fbdd01e0b033d4e4c17eb167e43a9c9d3a68f650578724bd71b60f36b/basiccrawler-0.2.0.tar.gz", "yanked": false}], "timestamp": "Thu May  7 18:14:47 2020"}