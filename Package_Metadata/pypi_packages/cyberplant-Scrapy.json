{"info": {"author": "Pablo Hoffman", "author_email": "pablo@pablohoffman.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Console", "Framework :: Scrapy", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Topic :: Internet :: WWW/HTTP", "Topic :: Software Development :: Libraries :: Application Frameworks", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "======\nScrapy\n======\n\n.. image:: https://img.shields.io/pypi/v/Scrapy.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: PyPI Version\n\n.. image:: https://img.shields.io/pypi/dm/Scrapy.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: PyPI Monthly downloads\n\n.. image:: https://img.shields.io/travis/scrapy/scrapy/master.svg\n   :target: http://travis-ci.org/scrapy/scrapy\n   :alt: Build Status\n\n.. image:: https://img.shields.io/badge/wheel-yes-brightgreen.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: Wheel Status\n   \n.. image:: http://static.scrapy.org/py3progress/badge.svg\n   :target: https://github.com/scrapy/scrapy/wiki/Python-3-Porting\n   :alt: Python 3 Porting Status\n\n.. image:: https://img.shields.io/codecov/c/github/scrapy/scrapy/master.svg\n   :target: http://codecov.io/github/scrapy/scrapy?branch=master\n   :alt: Coverage report\n\n\nOverview\n========\n\nScrapy is a fast high-level web crawling and web scraping framework, used to\ncrawl websites and extract structured data from their pages. It can be used for\na wide range of purposes, from data mining to monitoring and automated testing.\n\nFor more information including a list of features check the Scrapy homepage at:\nhttp://scrapy.org\n\nRequirements\n============\n\n* Python 2.7 or Python 3.3+\n* Works on Linux, Windows, Mac OSX, BSD\n\nInstall\n=======\n\nThe quick way::\n\n    pip install scrapy\n\nFor more details see the install section in the documentation:\nhttp://doc.scrapy.org/en/latest/intro/install.html\n\nReleases\n========\n\nYou can download the latest stable and development releases from:\nhttp://scrapy.org/download/\n\nDocumentation\n=============\n\nDocumentation is available online at http://doc.scrapy.org/ and in the ``docs``\ndirectory.\n\nCommunity (blog, twitter, mail list, IRC)\n=========================================\n\nSee http://scrapy.org/community/\n\nContributing\n============\n\nPlease note that this project is released with a Contributor Code of Conduct\n(see https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md).\n\nBy participating in this project you agree to abide by its terms.\nPlease report unacceptable behavior to opensource@scrapinghub.com.\n\nSee http://doc.scrapy.org/en/master/contributing.html\n\nCompanies using Scrapy\n======================\n\nSee http://scrapy.org/companies/\n\nCommercial Support\n==================\n\nSee http://scrapy.org/support/", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "http://scrapy.org", "keywords": null, "license": "BSD", "maintainer": null, "maintainer_email": null, "name": "cyberplant-Scrapy", "package_url": "https://pypi.org/project/cyberplant-Scrapy/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/cyberplant-Scrapy/", "project_urls": {"Download": "UNKNOWN", "Homepage": "http://scrapy.org"}, "release_url": "https://pypi.org/project/cyberplant-Scrapy/1.2.0.dev2/", "requires_dist": null, "requires_python": null, "summary": "A high-level Web Crawling and Web Scraping framework", "version": "1.2.0.dev2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://pypi.python.org/pypi/Scrapy\" rel=\"nofollow\"><img alt=\"PyPI Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c32f7e728d29da096100985c2ef0de5c416b3ba2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f5363726170792e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/Scrapy\" rel=\"nofollow\"><img alt=\"PyPI Monthly downloads\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4c6f91492dd0bafd7e21e2974401009c017e6d79/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f5363726170792e737667\"></a>\n<a href=\"http://travis-ci.org/scrapy/scrapy\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/912110d08cc8b03a9003feaaf9f94825513b4e0e/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f7363726170792f7363726170792f6d61737465722e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/Scrapy\" rel=\"nofollow\"><img alt=\"Wheel Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/36f0977ff012856a4c1d90260124b61d42daea02/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f776865656c2d7965732d627269676874677265656e2e737667\"></a>\n<a href=\"https://github.com/scrapy/scrapy/wiki/Python-3-Porting\" rel=\"nofollow\"><img alt=\"Python 3 Porting Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fde4a84c2346a51e2cae5501ebae5ce4cfab084c/687474703a2f2f7374617469632e7363726170792e6f72672f70793370726f67726573732f62616467652e737667\"></a>\n<a href=\"http://codecov.io/github/scrapy/scrapy?branch=master\" rel=\"nofollow\"><img alt=\"Coverage report\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/29794fdbb42dbb17c1705fce84779e0ae5a53224/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f7363726170792f7363726170792f6d61737465722e737667\"></a>\n<div id=\"overview\">\n<h2>Overview</h2>\n<p>Scrapy is a fast high-level web crawling and web scraping framework, used to\ncrawl websites and extract structured data from their pages. It can be used for\na wide range of purposes, from data mining to monitoring and automated testing.</p>\n<p>For more information including a list of features check the Scrapy homepage at:\n<a href=\"http://scrapy.org\" rel=\"nofollow\">http://scrapy.org</a></p>\n</div>\n<div id=\"requirements\">\n<h2>Requirements</h2>\n<ul>\n<li>Python 2.7 or Python 3.3+</li>\n<li>Works on Linux, Windows, Mac OSX, BSD</li>\n</ul>\n</div>\n<div id=\"install\">\n<h2>Install</h2>\n<p>The quick way:</p>\n<pre>pip install scrapy\n</pre>\n<p>For more details see the install section in the documentation:\n<a href=\"http://doc.scrapy.org/en/latest/intro/install.html\" rel=\"nofollow\">http://doc.scrapy.org/en/latest/intro/install.html</a></p>\n</div>\n<div id=\"releases\">\n<h2>Releases</h2>\n<p>You can download the latest stable and development releases from:\n<a href=\"http://scrapy.org/download/\" rel=\"nofollow\">http://scrapy.org/download/</a></p>\n</div>\n<div id=\"documentation\">\n<h2>Documentation</h2>\n<p>Documentation is available online at <a href=\"http://doc.scrapy.org/\" rel=\"nofollow\">http://doc.scrapy.org/</a> and in the <tt>docs</tt>\ndirectory.</p>\n</div>\n<div id=\"community-blog-twitter-mail-list-irc\">\n<h2>Community (blog, twitter, mail list, IRC)</h2>\n<p>See <a href=\"http://scrapy.org/community/\" rel=\"nofollow\">http://scrapy.org/community/</a></p>\n</div>\n<div id=\"contributing\">\n<h2>Contributing</h2>\n<p>Please note that this project is released with a Contributor Code of Conduct\n(see <a href=\"https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\" rel=\"nofollow\">https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md</a>).</p>\n<p>By participating in this project you agree to abide by its terms.\nPlease report unacceptable behavior to <a href=\"mailto:opensource%40scrapinghub.com\">opensource<span>@</span>scrapinghub<span>.</span>com</a>.</p>\n<p>See <a href=\"http://doc.scrapy.org/en/master/contributing.html\" rel=\"nofollow\">http://doc.scrapy.org/en/master/contributing.html</a></p>\n</div>\n<div id=\"companies-using-scrapy\">\n<h2>Companies using Scrapy</h2>\n<p>See <a href=\"http://scrapy.org/companies/\" rel=\"nofollow\">http://scrapy.org/companies/</a></p>\n</div>\n<div id=\"commercial-support\">\n<h2>Commercial Support</h2>\n<p>See <a href=\"http://scrapy.org/support/\" rel=\"nofollow\">http://scrapy.org/support/</a></p>\n</div>\n\n          </div>"}, "last_serial": 2172419, "releases": {"1.2.0.dev2": [{"comment_text": "", "digests": {"md5": "29819d0e3f51745d1667a66dfa4dde23", "sha256": "41393a25df0bb371a2c7a05ead2e64b8d2df1c263109e3fac2971ca0f35974cb"}, "downloads": -1, "filename": "cyberplant_Scrapy-1.2.0.dev2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "29819d0e3f51745d1667a66dfa4dde23", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 294573, "upload_time": "2016-06-17T05:10:45", "upload_time_iso_8601": "2016-06-17T05:10:45.536380Z", "url": "https://files.pythonhosted.org/packages/42/41/98d4ed775d4122ccea52e09fb1b5f4471686edd678c39896f51437720856/cyberplant_Scrapy-1.2.0.dev2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "18bd13f36466d2596d6e82389ddebb7d", "sha256": "1c2e64205ea728fa177147eb8179c94aebbe6a919b5f56c38f75b558bb0bfb46"}, "downloads": -1, "filename": "cyberplant-Scrapy-1.2.0.dev2.tar.gz", "has_sig": false, "md5_digest": "18bd13f36466d2596d6e82389ddebb7d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 821356, "upload_time": "2016-06-17T05:10:50", "upload_time_iso_8601": "2016-06-17T05:10:50.546491Z", "url": "https://files.pythonhosted.org/packages/dc/44/88b02cbc6b924e6e4239befbe5e29c5ed3325640efb1c7c3a60754c97e2a/cyberplant-Scrapy-1.2.0.dev2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "29819d0e3f51745d1667a66dfa4dde23", "sha256": "41393a25df0bb371a2c7a05ead2e64b8d2df1c263109e3fac2971ca0f35974cb"}, "downloads": -1, "filename": "cyberplant_Scrapy-1.2.0.dev2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "29819d0e3f51745d1667a66dfa4dde23", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 294573, "upload_time": "2016-06-17T05:10:45", "upload_time_iso_8601": "2016-06-17T05:10:45.536380Z", "url": "https://files.pythonhosted.org/packages/42/41/98d4ed775d4122ccea52e09fb1b5f4471686edd678c39896f51437720856/cyberplant_Scrapy-1.2.0.dev2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "18bd13f36466d2596d6e82389ddebb7d", "sha256": "1c2e64205ea728fa177147eb8179c94aebbe6a919b5f56c38f75b558bb0bfb46"}, "downloads": -1, "filename": "cyberplant-Scrapy-1.2.0.dev2.tar.gz", "has_sig": false, "md5_digest": "18bd13f36466d2596d6e82389ddebb7d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 821356, "upload_time": "2016-06-17T05:10:50", "upload_time_iso_8601": "2016-06-17T05:10:50.546491Z", "url": "https://files.pythonhosted.org/packages/dc/44/88b02cbc6b924e6e4239befbe5e29c5ed3325640efb1c7c3a60754c97e2a/cyberplant-Scrapy-1.2.0.dev2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:41:08 2020"}