{"info": {"author": "Robert Sachunsky", "author_email": "sachunsky@informatik.uni-leipzig.de", "bugtrack_url": null, "classifiers": [], "description": "# cor-asv-ann\n    OCR post-correction with encoder-attention-decoder LSTMs\n\n[![CircleCI](https://circleci.com/gh/ASVLeipzig/cor-asv-ann.svg?style=svg)](https://circleci.com/gh/ASVLeipzig/cor-asv-ann)\n\n## Introduction\n\nThis is a tool for automatic OCR _post-correction_ (reducing optical character recognition errors) with recurrent neural networks. It uses sequence-to-sequence transduction on the _character level_ with a model architecture akin to neural machine translation, i.e. a stacked **encoder-decoder** network with attention mechanism. \n\n### Architecture\n\nThe **attention model** always applies to full lines (in a _local, monotonic_ configuration), and uses a linear _additive_ alignment model. (This transfers information between the encoder and decoder hidden layer states, and calculates a _soft alignment_ between input and output characters. It is imperative for character-level processing, because with a simple final-initial transfer, models tend to start \"forgetting\" the input altogether at some point in the line and behave like unconditional LM generators. Local alignment is necessary to prevent snapping back to earlier states during long sequences.)\n\nThe **architecture** is as follows: \n0. The input characters are represented as unit vectors (or as a probability distribution in case of uncertainty and ambiguity). These enter a dense projection layer to be picked up by the encoder.\n1. The bottom hidden layer of the encoder is a bi-directional LSTM. \n2. The next encoder layers are forward LSTMs stacked on top of each other. \n3. The outputs of the top layer enter the attention model as constants (both in raw form to be weighted with the decoder state recurrently, and in a pre-calculated dense projection).\n4. The hidden layers of the decoder are forward LSTMs stacked on top of each other.\n5. The top hidden layer of the decoder has double width and contains the attention model:\n   - It reads the attention constants from 3. and uses the alignment as attention state (to be input as initial and output as final state). \n   - The attention model masks a window around the center of the previous alignment plus 1 character, calculates a new alignment between encoder outputs and current decoder state, and superimposes this with the encoder outputs to yield a context vector.\n   - The context vector is concatenated to the previous layers output and enters the LSTM.\n6. The decoder outputs enter a dense projection and get normalized to a probability distribution (softmax) for each character. (The output projection weights are the transpose of the input projection weights in 0. \u2013 weight tying.)\n7. Depending on the decoder mode, the decoder output is fed back directly (greedy) or indirectly (beamed) into the decoder input. (The first position is fed with a start symbol. Decoding ends on receiving a stop symbol.)\n8. The result is the character sequences corresponding to the argmax probabilities of the decoder outputs.\n\nHL depth and width, as well as many other topology and training options can be configured:\n- residual connections between layers in encoder and decoder?\n- deep bidirectional encoder (with fw/bw cross-summarization)?\n- LM loss/prediction as secondary output (multi-task learning, dual scoring)?\n\n### Multi-OCR input\n\nnot yet!\n\n### Decoder feedback\n\nOne important empirical finding is that the softmax output (full probability distribution) of the decoder can carry important information for the next state when input directly. This greatly improves the accuracy of both alignments and predictions. (This is in part attributable to exposure bias.) Therefore, instead of following the usual convention of feeding back argmax unit vectors, this implementation feeds back the softmax output directly.\n\nThis can even be done for beam search (which normally splits up the full distribution into a few select explicit candidates, represented as unit vectors) by simply resetting maximum outputs for lower-scoring candidates successively.\n\n### Decoder modes\n\nWhile the _encoder_ can always be run in parallel over a batch of lines and by passing the full sequence of characters in one tensor (padded to the longest line in the batch), which is very efficient with Keras backends like Tensorflow, a **beam-search** _decoder_ requires passing initial/final states character-by-character, with parallelism employed to capture multiple history hypotheses of a single line. However, one can also **greedily** use the best output only for each position (without beam search). This latter option also allows to run in parallel over lines, which is much faster \u2013 consuming up to ten times less CPU time.\n\nThererfore, the backend function `lib.Sequence2Sequence.correct_lines` can operate the decoder network in either of the following modes:\n\n#### _fast_\n\nDecode greedily, but feeding back the full softmax distribution in batch mode (lines-parallel).\n\n#### _greedy_\n\nDecode greedily, but feeding back the full softmax distribution for each line separately.\n\n#### _default_\n\nDecode beamed, selecting the best output candidates of the best history hypotheses for each line and feeding back their (successively reset) partial softmax distributions in batch mode (hypotheses-parallel). More specifically:\n\n> Start decoder with start-of-sequence, then keep decoding until\n> end-of-sequence is found or output length is way off, repeatedly.\n> Decode by using the best predicted output characters and several next-best\n> alternatives (up to some degradation threshold) as next input.\n> Follow-up on the N best overall candidates (estimated by accumulated\n> score, normalized by length and prospective cost), i.e. do A*-like\n> breadth-first search, with N equal `batch_size`.\n> Pass decoder initial/final states from character to character,\n> for each candidate respectively.\n\n### Rejection\n\nDuring beam search (default decoder mode), whenever the input and output is in good alignment (i.e. the attention model yields an alignment approximately 1 character after their predecessor's alignment on average), it is possible to estimate the current position in the source string. This input character's predicted output score, when smaller than a given (i.e. variable) probability threshold can be clipped to that minimum. This effectively adds a candidate which _rejects_ correction at that position (keeping the input unchanged).\n\n### Underspecification and gap\n\nInput characters that have not been seen during training must be well-behaved at inference time: They must be represented by a reserved index, and should behave like **neutral/unknown** characters instead of spoiling HL states and predictions in a longer follow-up context. This is achieved by dedicated leave-one-out training and regularization to optimize for interpolation of all known characters. At runtime, the encoder merely shows a warning of the previously unseen character.\n\nThe same device is useful to fill a known **gap** in the input (the only difference being that no warning is shown).\n\n### Training\n\nPossibilities:\n- incremental training and pretraining (on clean-only text)\n- scheduled sampling (mixed teacher forcing and decoder feedback)\n- LM transfer (initialization of the decoder weights from a language model of the same topology)\n- shallow transfer (initialization of encoder/decoder weights from a model of lesser depth)\n\n### Processing PAGE annotations\n\nWhen applied on PAGE-XML (as OCR-D workspace processor), this component also allows processing below the `TextLine` hierarchy level, i.e. on `Word` or `Glyph` level. For that it uses the soft alignment scores to calculate an optimal hard alignment path for characters, and thereby distributes the transduction onto the lower level elements (keeping their coordinates and other meta-data), while changing Word segmentation if necessary.\n\n...\n\n### Evaluation\n\nText lines can be compared (by aligning and computing a distance under some metric) across multiple inputs. (This would typically be GT and OCR vs post-correction.) This can be done both on plain text files (`cor-asv-ann-eval`) and PAGE-XML annotations (`ocrd-cor-asv-ann-evaluate`). \n\nDistances are accumulated (as micro-averages) as character error rate (CER) mean and stddev, but only on the character level.\n\nThere are a number of distance metrics available (all operating on grapheme clusters, not mere codepoints):\n- `Levenshtein`:  \n  simple unweighted edit distance (fastest, standard; GT level 3)\n- `NFC`:  \n  like `Levenshtein`, but apply Unicode normal form with canonical composition before (i.e. less than GT level 2)\n- `NFKC`:  \n  like `Levenshtein`, but apply Unicode normal form with compatibility composition before (i.e. less than GT level 2, except for `\u017f`, which is already normalized to `s`)\n- `historic_latin`:  \n  like `Levenshtein`, but decomposing non-vocalic ligatures before and treating as equivalent (i.e. zero distances) confusions of certain semantically close characters often found in historic texts (e.g. umlauts with combining letter `e` as in `Wu\u0364\u017fte` instead of  to `W\u00fc\u017fte`, `\u017f` vs `s`, or quotation/citation marks; GT level 1)\n\n...perplexity measurement...\n\n## Installation\n\nRequired Ubuntu packages:\n\n* Python (``python`` or ``python3``)\n* pip (``python-pip`` or ``python3-pip``)\n* venv (``python-venv`` or ``python3-venv``)\n\nCreate and activate a virtual environment as usual.\n\nTo install Python dependencies:\n```shell\nmake deps\n```\nWhich is the equivalent of:\n```shell\npip install -r requirements.txt\n```\n\nTo install this module, then do:\n```shell\nmake install\n```\nWhich is the equivalent of:\n```shell\npip install .\n```\n\nThe module can use CUDA-enabled GPUs (when sufficiently installed), but can also run on CPU only. Models are always interchangable.\n\n## Usage\n\nThis packages has the following user interfaces:\n\n### command line interface `cor-asv-ann-train`\n\nTo be used with string arguments and plain-text files.\n\n...\n\n### command line interface `cor-asv-ann-eval`\n\nTo be used with string arguments and plain-text files.\n\n...\n\n### command line interface `cor-asv-ann-repl`\n\ninteractive, visualization\n\n...\n\n### [OCR-D processor](https://ocr-d.github.io/cli) interface `ocrd-cor-asv-ann-process`\n\nTo be used with [PageXML](https://github.com/PRImA-Research-Lab/PAGE-XML) documents in an [OCR-D](https://ocr-d.github.io/) annotation workflow. \n\nInput could be anything with a textual annotation (`TextEquiv` on the given `textequiv_level`). \n\nPretrained model files are contained in the [models subrepository](models/README.md). At runtime, you can use both absolute and relative paths for model files. The latter are searched for in the installation directory, and under the path in the environment variable `CORASVANN_DATA` (if given).\n\n\n```json\n    \"ocrd-cor-asv-ann-process\": {\n      \"executable\": \"ocrd-cor-asv-ann-process\",\n      \"categories\": [\n        \"Text recognition and optimization\"\n      ],\n      \"steps\": [\n        \"recognition/post-correction\"\n      ],\n      \"description\": \"Improve text annotation by character-level encoder-attention-decoder ANN model\",\n      \"input_file_grp\": [\n        \"OCR-D-OCR-TESS\",\n        \"OCR-D-OCR-KRAK\",\n        \"OCR-D-OCR-OCRO\",\n        \"OCR-D-OCR-CALA\",\n        \"OCR-D-OCR-ANY\"\n      ],\n      \"output_file_grp\": [\n        \"OCR-D-COR-ASV\"\n      ],\n      \"parameters\": {\n        \"model_file\": {\n          \"type\": \"string\",\n          \"format\": \"uri\",\n          \"content-type\": \"application/x-hdf;subtype=bag\",\n          \"description\": \"path of h5py weight/config file for model trained with cor-asv-ann-train\",\n          \"required\": true,\n          \"cacheable\": true\n        },\n        \"textequiv_level\": {\n          \"type\": \"string\",\n          \"enum\": [\"line\", \"word\", \"glyph\"],\n          \"default\": \"glyph\",\n          \"description\": \"PAGE XML hierarchy level to read/write TextEquiv input/output on\"\n        },\n        \"rejection_threshold\": {\n          \"type\": \"number\",\n          \"format\": \"float\",\n          \"default\": 0.5,\n          \"description\": \"minimum probability of the candidate corresponding to the input character in each hypothesis during beam search, helps balance precision/recall trade-off; set to 0 to disable rejection (max recall) or 1 to disable correction (max precision)\"\n        },\n        \"relative_beam_width\": {\n          \"type\": \"number\",\n          \"format\": \"float\",\n          \"default\": 0.2,\n          \"description\": \"minimum fraction of the best candidate's probability required to enter the beam in each hypothesis; controls the quality/performance trade-off\"\n        },\n        \"fixed_beam_width\": {\n          \"type\": \"number\",\n          \"format\": \"integer\",\n          \"default\": 15,\n          \"description\": \"maximum number of candidates allowed to enter the beam in each hypothesis; controls the quality/performance trade-off\"\n        },\n        \"fast_mode\": {\n          \"type\": \"boolean\",\n          \"default\": false,\n          \"description\": \"decode greedy instead of beamed, with batches of parallel lines instead of parallel alternatives; also disables rejection and beam parameters; enable if performance is far more important than quality\"\n        }\n      }\n   }\n```\n\n...\n\n### [OCR-D processor](https://ocr-d.github.io/cli) interface `ocrd-cor-asv-ann-evaluate`\n\nTo be used with [PageXML](https://github.com/PRImA-Research-Lab/PAGE-XML) documents in an [OCR-D](https://ocr-d.github.io/) annotation workflow.\n\nInputs could be anything with a textual annotation (`TextEquiv` on the line level), but at least 2. The first in the list of input file groups will be regarded as reference/GT.\n\nThere are various evaluation [metrics](#Evaluation) available.\n\nThe tool can also aggregate and show the most frequent character confusions.\n\n```json\n    \"ocrd-cor-asv-ann-evaluate\": {\n      \"executable\": \"ocrd-cor-asv-ann-evaluate\",\n      \"categories\": [\n        \"Text recognition and optimization\"\n      ],\n      \"steps\": [\n        \"recognition/evaluation\"\n      ],\n      \"description\": \"Align different textline annotations and compute distance\",\n      \"input_file_grp\": [\n        \"OCR-D-GT-SEG-LINE\",\n        \"OCR-D-OCR-TESS\",\n        \"OCR-D-OCR-KRAK\",\n        \"OCR-D-OCR-OCRO\",\n        \"OCR-D-OCR-CALA\",\n        \"OCR-D-OCR-ANY\",\n        \"OCR-D-COR-ASV\"\n      ],\n      \"parameters\": {\n        \"metric\": {\n          \"type\": \"string\",\n          \"enum\": [\"Levenshtein\", \"NFC\", \"NFKC\", \"historic_latin\"],\n          \"default\": \"Levenshtein\",\n          \"description\": \"Distance metric to calculate and aggregate: historic_latin for GT level 1, NFKC for GT level 2 (except \u017f-s), Levenshtein for GT level 3\"\n        },\n        \"confusion\": {\n          \"type\": \"number\",\n          \"format\": \"integer\",\n          \"minimum\": 0,\n          \"default\": 0,\n          \"description\": \"Count edits and show that number of most frequent confusions (non-identity) in the end.\"\n        }\n      }\n    }\n```\n\n...\n\n## Testing\n\nnot yet!\n...\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ASVLeipzig/cor-asv-ann", "keywords": "", "license": "Apache License 2.0", "maintainer": "", "maintainer_email": "", "name": "ocrd-cor-asv-ann", "package_url": "https://pypi.org/project/ocrd-cor-asv-ann/", "platform": "", "project_url": "https://pypi.org/project/ocrd-cor-asv-ann/", "project_urls": {"Homepage": "https://github.com/ASVLeipzig/cor-asv-ann"}, "release_url": "https://pypi.org/project/ocrd-cor-asv-ann/0.1.2/", "requires_dist": ["ocrd (>=2.0)", "click", "keras (>=2.3.1)", "numpy", "tensorflow (==1.15.2)", "h5py", "editdistance", "matplotlib"], "requires_python": "", "summary": "sequence-to-sequence translator for noisy channel error correction", "version": "0.1.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>cor-asv-ann</h1>\n<pre><code>OCR post-correction with encoder-attention-decoder LSTMs\n</code></pre>\n<p><a href=\"https://circleci.com/gh/ASVLeipzig/cor-asv-ann\" rel=\"nofollow\"><img alt=\"CircleCI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/585fc9b1d479ed7176507a64b8702220d4435a90/68747470733a2f2f636972636c6563692e636f6d2f67682f4153564c6569707a69672f636f722d6173762d616e6e2e7376673f7374796c653d737667\"></a></p>\n<h2>Introduction</h2>\n<p>This is a tool for automatic OCR <em>post-correction</em> (reducing optical character recognition errors) with recurrent neural networks. It uses sequence-to-sequence transduction on the <em>character level</em> with a model architecture akin to neural machine translation, i.e. a stacked <strong>encoder-decoder</strong> network with attention mechanism.</p>\n<h3>Architecture</h3>\n<p>The <strong>attention model</strong> always applies to full lines (in a <em>local, monotonic</em> configuration), and uses a linear <em>additive</em> alignment model. (This transfers information between the encoder and decoder hidden layer states, and calculates a <em>soft alignment</em> between input and output characters. It is imperative for character-level processing, because with a simple final-initial transfer, models tend to start \"forgetting\" the input altogether at some point in the line and behave like unconditional LM generators. Local alignment is necessary to prevent snapping back to earlier states during long sequences.)</p>\n<p>The <strong>architecture</strong> is as follows:\n0. The input characters are represented as unit vectors (or as a probability distribution in case of uncertainty and ambiguity). These enter a dense projection layer to be picked up by the encoder.</p>\n<ol>\n<li>The bottom hidden layer of the encoder is a bi-directional LSTM.</li>\n<li>The next encoder layers are forward LSTMs stacked on top of each other.</li>\n<li>The outputs of the top layer enter the attention model as constants (both in raw form to be weighted with the decoder state recurrently, and in a pre-calculated dense projection).</li>\n<li>The hidden layers of the decoder are forward LSTMs stacked on top of each other.</li>\n<li>The top hidden layer of the decoder has double width and contains the attention model:\n<ul>\n<li>It reads the attention constants from 3. and uses the alignment as attention state (to be input as initial and output as final state).</li>\n<li>The attention model masks a window around the center of the previous alignment plus 1 character, calculates a new alignment between encoder outputs and current decoder state, and superimposes this with the encoder outputs to yield a context vector.</li>\n<li>The context vector is concatenated to the previous layers output and enters the LSTM.</li>\n</ul>\n</li>\n<li>The decoder outputs enter a dense projection and get normalized to a probability distribution (softmax) for each character. (The output projection weights are the transpose of the input projection weights in 0. \u2013 weight tying.)</li>\n<li>Depending on the decoder mode, the decoder output is fed back directly (greedy) or indirectly (beamed) into the decoder input. (The first position is fed with a start symbol. Decoding ends on receiving a stop symbol.)</li>\n<li>The result is the character sequences corresponding to the argmax probabilities of the decoder outputs.</li>\n</ol>\n<p>HL depth and width, as well as many other topology and training options can be configured:</p>\n<ul>\n<li>residual connections between layers in encoder and decoder?</li>\n<li>deep bidirectional encoder (with fw/bw cross-summarization)?</li>\n<li>LM loss/prediction as secondary output (multi-task learning, dual scoring)?</li>\n</ul>\n<h3>Multi-OCR input</h3>\n<p>not yet!</p>\n<h3>Decoder feedback</h3>\n<p>One important empirical finding is that the softmax output (full probability distribution) of the decoder can carry important information for the next state when input directly. This greatly improves the accuracy of both alignments and predictions. (This is in part attributable to exposure bias.) Therefore, instead of following the usual convention of feeding back argmax unit vectors, this implementation feeds back the softmax output directly.</p>\n<p>This can even be done for beam search (which normally splits up the full distribution into a few select explicit candidates, represented as unit vectors) by simply resetting maximum outputs for lower-scoring candidates successively.</p>\n<h3>Decoder modes</h3>\n<p>While the <em>encoder</em> can always be run in parallel over a batch of lines and by passing the full sequence of characters in one tensor (padded to the longest line in the batch), which is very efficient with Keras backends like Tensorflow, a <strong>beam-search</strong> <em>decoder</em> requires passing initial/final states character-by-character, with parallelism employed to capture multiple history hypotheses of a single line. However, one can also <strong>greedily</strong> use the best output only for each position (without beam search). This latter option also allows to run in parallel over lines, which is much faster \u2013 consuming up to ten times less CPU time.</p>\n<p>Thererfore, the backend function <code>lib.Sequence2Sequence.correct_lines</code> can operate the decoder network in either of the following modes:</p>\n<h4><em>fast</em></h4>\n<p>Decode greedily, but feeding back the full softmax distribution in batch mode (lines-parallel).</p>\n<h4><em>greedy</em></h4>\n<p>Decode greedily, but feeding back the full softmax distribution for each line separately.</p>\n<h4><em>default</em></h4>\n<p>Decode beamed, selecting the best output candidates of the best history hypotheses for each line and feeding back their (successively reset) partial softmax distributions in batch mode (hypotheses-parallel). More specifically:</p>\n<blockquote>\n<p>Start decoder with start-of-sequence, then keep decoding until\nend-of-sequence is found or output length is way off, repeatedly.\nDecode by using the best predicted output characters and several next-best\nalternatives (up to some degradation threshold) as next input.\nFollow-up on the N best overall candidates (estimated by accumulated\nscore, normalized by length and prospective cost), i.e. do A*-like\nbreadth-first search, with N equal <code>batch_size</code>.\nPass decoder initial/final states from character to character,\nfor each candidate respectively.</p>\n</blockquote>\n<h3>Rejection</h3>\n<p>During beam search (default decoder mode), whenever the input and output is in good alignment (i.e. the attention model yields an alignment approximately 1 character after their predecessor's alignment on average), it is possible to estimate the current position in the source string. This input character's predicted output score, when smaller than a given (i.e. variable) probability threshold can be clipped to that minimum. This effectively adds a candidate which <em>rejects</em> correction at that position (keeping the input unchanged).</p>\n<h3>Underspecification and gap</h3>\n<p>Input characters that have not been seen during training must be well-behaved at inference time: They must be represented by a reserved index, and should behave like <strong>neutral/unknown</strong> characters instead of spoiling HL states and predictions in a longer follow-up context. This is achieved by dedicated leave-one-out training and regularization to optimize for interpolation of all known characters. At runtime, the encoder merely shows a warning of the previously unseen character.</p>\n<p>The same device is useful to fill a known <strong>gap</strong> in the input (the only difference being that no warning is shown).</p>\n<h3>Training</h3>\n<p>Possibilities:</p>\n<ul>\n<li>incremental training and pretraining (on clean-only text)</li>\n<li>scheduled sampling (mixed teacher forcing and decoder feedback)</li>\n<li>LM transfer (initialization of the decoder weights from a language model of the same topology)</li>\n<li>shallow transfer (initialization of encoder/decoder weights from a model of lesser depth)</li>\n</ul>\n<h3>Processing PAGE annotations</h3>\n<p>When applied on PAGE-XML (as OCR-D workspace processor), this component also allows processing below the <code>TextLine</code> hierarchy level, i.e. on <code>Word</code> or <code>Glyph</code> level. For that it uses the soft alignment scores to calculate an optimal hard alignment path for characters, and thereby distributes the transduction onto the lower level elements (keeping their coordinates and other meta-data), while changing Word segmentation if necessary.</p>\n<p>...</p>\n<h3>Evaluation</h3>\n<p>Text lines can be compared (by aligning and computing a distance under some metric) across multiple inputs. (This would typically be GT and OCR vs post-correction.) This can be done both on plain text files (<code>cor-asv-ann-eval</code>) and PAGE-XML annotations (<code>ocrd-cor-asv-ann-evaluate</code>).</p>\n<p>Distances are accumulated (as micro-averages) as character error rate (CER) mean and stddev, but only on the character level.</p>\n<p>There are a number of distance metrics available (all operating on grapheme clusters, not mere codepoints):</p>\n<ul>\n<li><code>Levenshtein</code>:<br>\nsimple unweighted edit distance (fastest, standard; GT level 3)</li>\n<li><code>NFC</code>:<br>\nlike <code>Levenshtein</code>, but apply Unicode normal form with canonical composition before (i.e. less than GT level 2)</li>\n<li><code>NFKC</code>:<br>\nlike <code>Levenshtein</code>, but apply Unicode normal form with compatibility composition before (i.e. less than GT level 2, except for <code>\u017f</code>, which is already normalized to <code>s</code>)</li>\n<li><code>historic_latin</code>:<br>\nlike <code>Levenshtein</code>, but decomposing non-vocalic ligatures before and treating as equivalent (i.e. zero distances) confusions of certain semantically close characters often found in historic texts (e.g. umlauts with combining letter <code>e</code> as in <code>Wu\u0364\u017fte</code> instead of  to <code>W\u00fc\u017fte</code>, <code>\u017f</code> vs <code>s</code>, or quotation/citation marks; GT level 1)</li>\n</ul>\n<p>...perplexity measurement...</p>\n<h2>Installation</h2>\n<p>Required Ubuntu packages:</p>\n<ul>\n<li>Python (<code>python</code> or <code>python3</code>)</li>\n<li>pip (<code>python-pip</code> or <code>python3-pip</code>)</li>\n<li>venv (<code>python-venv</code> or <code>python3-venv</code>)</li>\n</ul>\n<p>Create and activate a virtual environment as usual.</p>\n<p>To install Python dependencies:</p>\n<pre>make deps\n</pre>\n<p>Which is the equivalent of:</p>\n<pre>pip install -r requirements.txt\n</pre>\n<p>To install this module, then do:</p>\n<pre>make install\n</pre>\n<p>Which is the equivalent of:</p>\n<pre>pip install .\n</pre>\n<p>The module can use CUDA-enabled GPUs (when sufficiently installed), but can also run on CPU only. Models are always interchangable.</p>\n<h2>Usage</h2>\n<p>This packages has the following user interfaces:</p>\n<h3>command line interface <code>cor-asv-ann-train</code></h3>\n<p>To be used with string arguments and plain-text files.</p>\n<p>...</p>\n<h3>command line interface <code>cor-asv-ann-eval</code></h3>\n<p>To be used with string arguments and plain-text files.</p>\n<p>...</p>\n<h3>command line interface <code>cor-asv-ann-repl</code></h3>\n<p>interactive, visualization</p>\n<p>...</p>\n<h3><a href=\"https://ocr-d.github.io/cli\" rel=\"nofollow\">OCR-D processor</a> interface <code>ocrd-cor-asv-ann-process</code></h3>\n<p>To be used with <a href=\"https://github.com/PRImA-Research-Lab/PAGE-XML\" rel=\"nofollow\">PageXML</a> documents in an <a href=\"https://ocr-d.github.io/\" rel=\"nofollow\">OCR-D</a> annotation workflow.</p>\n<p>Input could be anything with a textual annotation (<code>TextEquiv</code> on the given <code>textequiv_level</code>).</p>\n<p>Pretrained model files are contained in the <a href=\"models/README.md\" rel=\"nofollow\">models subrepository</a>. At runtime, you can use both absolute and relative paths for model files. The latter are searched for in the installation directory, and under the path in the environment variable <code>CORASVANN_DATA</code> (if given).</p>\n<pre>    <span class=\"s2\">\"ocrd-cor-asv-ann-process\"</span><span class=\"err\">:</span> <span class=\"p\">{</span>\n      <span class=\"nt\">\"executable\"</span><span class=\"p\">:</span> <span class=\"s2\">\"ocrd-cor-asv-ann-process\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"categories\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"Text recognition and optimization\"</span>\n      <span class=\"p\">],</span>\n      <span class=\"nt\">\"steps\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"recognition/post-correction\"</span>\n      <span class=\"p\">],</span>\n      <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Improve text annotation by character-level encoder-attention-decoder ANN model\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"input_file_grp\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"OCR-D-OCR-TESS\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-OCR-KRAK\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-OCR-OCRO\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-OCR-CALA\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-OCR-ANY\"</span>\n      <span class=\"p\">],</span>\n      <span class=\"nt\">\"output_file_grp\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"OCR-D-COR-ASV\"</span>\n      <span class=\"p\">],</span>\n      <span class=\"nt\">\"parameters\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"model_file\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"string\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"format\"</span><span class=\"p\">:</span> <span class=\"s2\">\"uri\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"content-type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"application/x-hdf;subtype=bag\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"path of h5py weight/config file for model trained with cor-asv-ann-train\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"required\"</span><span class=\"p\">:</span> <span class=\"kc\">true</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"cacheable\"</span><span class=\"p\">:</span> <span class=\"kc\">true</span>\n        <span class=\"p\">},</span>\n        <span class=\"nt\">\"textequiv_level\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"string\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"enum\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"line\"</span><span class=\"p\">,</span> <span class=\"s2\">\"word\"</span><span class=\"p\">,</span> <span class=\"s2\">\"glyph\"</span><span class=\"p\">],</span>\n          <span class=\"nt\">\"default\"</span><span class=\"p\">:</span> <span class=\"s2\">\"glyph\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"PAGE XML hierarchy level to read/write TextEquiv input/output on\"</span>\n        <span class=\"p\">},</span>\n        <span class=\"nt\">\"rejection_threshold\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"number\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"format\"</span><span class=\"p\">:</span> <span class=\"s2\">\"float\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"default\"</span><span class=\"p\">:</span> <span class=\"mf\">0.5</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"minimum probability of the candidate corresponding to the input character in each hypothesis during beam search, helps balance precision/recall trade-off; set to 0 to disable rejection (max recall) or 1 to disable correction (max precision)\"</span>\n        <span class=\"p\">},</span>\n        <span class=\"nt\">\"relative_beam_width\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"number\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"format\"</span><span class=\"p\">:</span> <span class=\"s2\">\"float\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"default\"</span><span class=\"p\">:</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"minimum fraction of the best candidate's probability required to enter the beam in each hypothesis; controls the quality/performance trade-off\"</span>\n        <span class=\"p\">},</span>\n        <span class=\"nt\">\"fixed_beam_width\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"number\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"format\"</span><span class=\"p\">:</span> <span class=\"s2\">\"integer\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"default\"</span><span class=\"p\">:</span> <span class=\"mi\">15</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"maximum number of candidates allowed to enter the beam in each hypothesis; controls the quality/performance trade-off\"</span>\n        <span class=\"p\">},</span>\n        <span class=\"nt\">\"fast_mode\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"boolean\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"default\"</span><span class=\"p\">:</span> <span class=\"kc\">false</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"decode greedy instead of beamed, with batches of parallel lines instead of parallel alternatives; also disables rejection and beam parameters; enable if performance is far more important than quality\"</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">}</span>\n   <span class=\"p\">}</span>\n</pre>\n<p>...</p>\n<h3><a href=\"https://ocr-d.github.io/cli\" rel=\"nofollow\">OCR-D processor</a> interface <code>ocrd-cor-asv-ann-evaluate</code></h3>\n<p>To be used with <a href=\"https://github.com/PRImA-Research-Lab/PAGE-XML\" rel=\"nofollow\">PageXML</a> documents in an <a href=\"https://ocr-d.github.io/\" rel=\"nofollow\">OCR-D</a> annotation workflow.</p>\n<p>Inputs could be anything with a textual annotation (<code>TextEquiv</code> on the line level), but at least 2. The first in the list of input file groups will be regarded as reference/GT.</p>\n<p>There are various evaluation <a href=\"#Evaluation\" rel=\"nofollow\">metrics</a> available.</p>\n<p>The tool can also aggregate and show the most frequent character confusions.</p>\n<pre>    <span class=\"s2\">\"ocrd-cor-asv-ann-evaluate\"</span><span class=\"err\">:</span> <span class=\"p\">{</span>\n      <span class=\"nt\">\"executable\"</span><span class=\"p\">:</span> <span class=\"s2\">\"ocrd-cor-asv-ann-evaluate\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"categories\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"Text recognition and optimization\"</span>\n      <span class=\"p\">],</span>\n      <span class=\"nt\">\"steps\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"recognition/evaluation\"</span>\n      <span class=\"p\">],</span>\n      <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Align different textline annotations and compute distance\"</span><span class=\"p\">,</span>\n      <span class=\"nt\">\"input_file_grp\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"s2\">\"OCR-D-GT-SEG-LINE\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-OCR-TESS\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-OCR-KRAK\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-OCR-OCRO\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-OCR-CALA\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-OCR-ANY\"</span><span class=\"p\">,</span>\n        <span class=\"s2\">\"OCR-D-COR-ASV\"</span>\n      <span class=\"p\">],</span>\n      <span class=\"nt\">\"parameters\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"metric\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"string\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"enum\"</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"Levenshtein\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NFC\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NFKC\"</span><span class=\"p\">,</span> <span class=\"s2\">\"historic_latin\"</span><span class=\"p\">],</span>\n          <span class=\"nt\">\"default\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Levenshtein\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Distance metric to calculate and aggregate: historic_latin for GT level 1, NFKC for GT level 2 (except \u017f-s), Levenshtein for GT level 3\"</span>\n        <span class=\"p\">},</span>\n        <span class=\"nt\">\"confusion\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"number\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"format\"</span><span class=\"p\">:</span> <span class=\"s2\">\"integer\"</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"minimum\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"default\"</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n          <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Count edits and show that number of most frequent confusions (non-identity) in the end.\"</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n</pre>\n<p>...</p>\n<h2>Testing</h2>\n<p>not yet!\n...</p>\n\n          </div>"}, "last_serial": 6606039, "releases": {"0.1.2": [{"comment_text": "", "digests": {"md5": "af3ad3baf28e72b896b83470a267fef0", "sha256": "4dc1d1aa49be07730ca788f2ce060f43f9e266badf4f30dbd2faa08f608fc912"}, "downloads": -1, "filename": "ocrd_cor_asv_ann-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "af3ad3baf28e72b896b83470a267fef0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 74568, "upload_time": "2020-02-10T23:42:06", "upload_time_iso_8601": "2020-02-10T23:42:06.515439Z", "url": "https://files.pythonhosted.org/packages/65/92/3c23e6ec93846cb3c7da6c789ebf4f861dfe7c7a5c75b98d55f02ece3443/ocrd_cor_asv_ann-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "76f75131ba098c9807c65ea5c5a69200", "sha256": "c10bbd76c64d4f2e155b220a8ecd58dbed9fd8e040b8763d176d139215e02569"}, "downloads": -1, "filename": "ocrd_cor_asv_ann-0.1.2.tar.gz", "has_sig": false, "md5_digest": "76f75131ba098c9807c65ea5c5a69200", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 66983, "upload_time": "2020-02-10T23:42:09", "upload_time_iso_8601": "2020-02-10T23:42:09.311626Z", "url": "https://files.pythonhosted.org/packages/8e/28/3db473a5a1ad65930633f7fb05e9519ca0fc971035b7488ddec1f5787837/ocrd_cor_asv_ann-0.1.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "af3ad3baf28e72b896b83470a267fef0", "sha256": "4dc1d1aa49be07730ca788f2ce060f43f9e266badf4f30dbd2faa08f608fc912"}, "downloads": -1, "filename": "ocrd_cor_asv_ann-0.1.2-py3-none-any.whl", "has_sig": false, "md5_digest": "af3ad3baf28e72b896b83470a267fef0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 74568, "upload_time": "2020-02-10T23:42:06", "upload_time_iso_8601": "2020-02-10T23:42:06.515439Z", "url": "https://files.pythonhosted.org/packages/65/92/3c23e6ec93846cb3c7da6c789ebf4f861dfe7c7a5c75b98d55f02ece3443/ocrd_cor_asv_ann-0.1.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "76f75131ba098c9807c65ea5c5a69200", "sha256": "c10bbd76c64d4f2e155b220a8ecd58dbed9fd8e040b8763d176d139215e02569"}, "downloads": -1, "filename": "ocrd_cor_asv_ann-0.1.2.tar.gz", "has_sig": false, "md5_digest": "76f75131ba098c9807c65ea5c5a69200", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 66983, "upload_time": "2020-02-10T23:42:09", "upload_time_iso_8601": "2020-02-10T23:42:09.311626Z", "url": "https://files.pythonhosted.org/packages/8e/28/3db473a5a1ad65930633f7fb05e9519ca0fc971035b7488ddec1f5787837/ocrd_cor_asv_ann-0.1.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:40 2020"}