{"info": {"author": "Gorka Eguileor", "author_email": "gorka@eguileor.com", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7"], "description": "Cinderlib CSI driver\n====================\n\n.. image:: https://img.shields.io/pypi/v/cinderlib_csi.svg\n   :target: https://pypi.python.org/pypi/cinderlib_csi\n\n.. image:: https://img.shields.io/pypi/pyversions/cinderlib_csi.svg\n   :target: https://pypi.python.org/pypi/cinderlib_csi\n\n.. image:: https://pyup.io/repos/github/akrog/cinderlib_csi/shield.svg\n     :target: https://pyup.io/repos/github/akrog/cinderlib_csi/\n     :alt: Updates\n\n.. image:: https://img.shields.io/:license-apache-blue.svg\n   :target: http://www.apache.org/licenses/LICENSE-2.0\n\n\nCSI Python driver that leverages all Cinder drivers to provide block volumes\nwithout needing to run any additional service, such as RabbitMQ, MariaDB,\nCinder-API, Cinder-Scheduler, or Cinder-Volume.\n\nCurrent code is is a **Proof of Concept** only compatible with Cinder\nOSP-12/Pike release.\n\n* Free software: Apache Software License 2.0\n* Documentation: Pending\n\n\nFeatures\n--------\n\nThis CSI driver is up to date with latest CSI specs including the `new\nsnapshots feature\n<https://github.com/container-storage-interface/spec/pull/224>`_ recently\nintroduced.\n\nCurrently supported features are:\n\n- Create block volume\n- Creating snapshots\n- Creating a block volume from a snapshot\n- Delete block volume\n- Deleting snapshots\n- Listing volumes with pagination\n- Listing snapshots with pagination\n- Attaching volumes\n- Detaching volumes\n- Reporting storage capacity\n- Probing the node\n- Retrieving the plugin info\n\n\nRuntime Dependencies\n--------------------\n\nThis driver requires that Cinder v11.0 (OSP-12/Pike) is already installed in\nthe system, how this is accomplished is left to the installer, as there are\nmultiple ways this can be accomplished:\n\n- From OSP repositories\n- From RDO repositories\n- From github\n- From other repositories\n\nAny other basic requirement is already handled by `cinderlib-csi` when\ninstalling from PyPi.\n\nBesides the basic dependencies there are also some drivers that have additional\nrequirements that must be met for proper operation of the driver and/or\nattachment/detachment operations, just like in Cinder.\n\nSome of these Python dependencies for the Controller servicer are:\n\n- DRBD: dbus and drbdmanage\n- HPE 3PAR: python-3parclient\n- Kaminario: krest\n- Pure: purestorage\n- Dell EMC VMAX, IBM DS8K: pyOpenSSL\n- HPE Lefthad: python-lefthandclient\n- Fujitsu Eternus DX: pywbem\n- IBM XIV: pyxcli\n- RBD: rados and rbd\n- Dell EMC VNX: storops\n- Violin: vmemclient\n- INFINIDAT: infinisdk, capacity, infy.dtypes.wwn, infi.dtypes.iqn\n\nOther backends may also require additional packages, for example LVM on\nCentOS/RHEL requires the `targetcli` package, so please check with your\nhardware vendor.\n\nBesides the Controller requirements there are usually requirements for the\nNode servicer needed to handle the attaching and detaching of volumes to the\nnode based on the connection used to access the storage.  For example:\n\n- iSCSI: iscsi-initiator-tools and device-mapper-multipath\n- RBD/Ceph: ceph-common package\n\n\nInstallation\n------------\n\nFirst we need to install the Cinder Python package, for example to install from\nRDO on CentOS:\n\n.. code-block:: shell\n\n    $ sudo yum install -y centos-release-openstack-pike\n    $ sudo yum install -y openstack-cinder python-pip\n\n\nThen we just need to install the `cinderlib-csi` package:\n\n.. code-block:: shell\n\n    $ sudo pip install cinderlib-csi\n\nNow we should install any additional package required by our backend.\n\nFor iSCSI backends we'll want to install:\n\n.. code-block:: shell\n\n    $ sudo yum install iscsi-initiator-utils\n    $ sudo yum install device-mapper-multipath\n    $ sudo mpathconf --enable --with_multipathd y --user_friendly_names n --find_multipaths y\n\nFor RBD we'll also need a specific package:\n\n.. code-block:: shell\n\n    $ sudo yum install ceph-common\n\n\nConfiguration\n-------------\n\nThe CSI driver is configured via environmental variables, any value that\ndoesn't have a default is a required value.\n\n+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Name                       | Role       | Description                                                   | Default                                                                                                  | Example                                                                                                                                                                                                                                 |\n+============================+============+===============================================================+==========================================================================================================+=========================================================================================================================================================================================================================================+\n| `CSI_ENDPOINT`             | all        | IP and port to bind the service                               | [::]:50051                                                                                               | 192.168.1.22:50050                                                                                                                                                                                                                      |\n+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| `CSI_MODE`                 | controller | Role the service should perform: controller, node, all        | all                                                                                                      | controller                                                                                                                                                                                                                              |\n+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| `X_CSI_STORAGE_NW_IP`      | node       | IP address in the Node used to connect to the storage         | IP resolved from Node's fqdn                                                                             | 192.168.1.22                                                                                                                                                                                                                            |\n+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| `X_CSI_NODE_ID`            | node       | ID used by this node to identify itself to the controller     | Node's fqdn                                                                                              | csi_test_node                                                                                                                                                                                                                           |\n+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| `X_CSI_PERSISTENCE_CONFIG` | all        | Configuration of the `cinderlib` metadata persistence plugin. | {'storage': 'db', 'connection': 'sqlite:///db.sqlite'}                                                   | {'storage': 'db', 'connection': 'mysql+pymysql://root:stackdb@192.168.1.1/cinder?charset=utf8'}                                                                                                                                         |\n+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| `X_CSI_CINDERLIB_CONFIG`   | controller | Global `cinderlib` configuration                              | {'project_id': 'com.redhat.cinderlib-csi', 'user_id': 'com.redhat.cinderlib-csi', 'root_helper': 'sudo'} | {\"project_id\":\"com.redhat.cinderlib-csi\",\"user_id\":\"com.redhat.cinderlib-csi\",\"root_helper\":\"sudo\"}                                                                                                                                     |\n+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| `X_CSI_BACKEND_CONFIG`     | controller | Driver configuration                                          |                                                                                                          | {\"volume_backend_name\": \"rbd\", \"volume_driver\": \"cinder.volume.drivers.rbd.RBDDriver\", \"rbd_user\": \"cinder\", \"rbd_pool\": \"volumes\", \"rbd_ceph_conf\": \"/etc/ceph/ceph.conf\", \"rbd_keyring_conf\": \"/etc/ceph/ceph.client.cinder.keyring\"} |\n+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\nThe only role that has been tested at the moment is the default one, where\nController and Node servicer are executed in the same service (`CSI_MODE=all`),\nand other modes are expected to have issues at the moment.\n\n\nStaring the plugin\n------------------\n\nOnce we have installed `cinderlib-csi` and required dependencies (for the\nbackend and for the connection type) we just have to run the `cinderlib-csi`\nservice with a user that can do passwordless sudo.\n\n.. code-block:: shell\n\n    $ cinderlib-csi\n\n\nTesting the plugin\n------------------\n\nThere are several examples of running the CSI cinderlib driver in the\n`examples` directory both for a baremetal deployment and a containerized\nversion of the driver.\n\nIn all cases we have to run the plugin first before we can test it, and for\nthat we have to check the configuration provided as a test before starting the\nplugin.  By default all examples run the service on port 50051.\n\n\nBaremetal\n~~~~~~~~~\n\nFor example to test with the LVM driver on our development environment we can\njust run the following commands from the root of the `cinderlib-csi` project:\n\n.. code-block:: shell\n\n    $ cd tmp\n    $ sudo dd if=/dev/zero of=cinder-volumes bs=1048576 seek=22527 count=1\n    $ lodevice=`sudo losetup --show -f ./cinder-volumes`\n    $ sudo pvcreate $lodevice\n    $ sudo vgcreate cinder-volumes $lodevice\n    $ sudo vgscan --cache\n    $ cd ../examples/baremetal\n    $ ./run.sh lvm\n    py27 develop-inst-nodeps: /home/geguileo/code/reuse-cinder-drivers/cinderlib-csi\n    py27 installed: ...\n    ___ summary ___\n      py27: skipped tests\n      congratulations :)\n    Starting cinderlib CSI v0.0.1 (cinderlib: 0.1.0, cinder: 11.1.1.dev41)\n    Running backend LVMVolumeDriver v3.0.0\n    Now serving on [::]:50051...\n\n\nThere is also an example of testing a Ceph cluster using a user called \"cinder\"\nand the \"volumes\" pool.  For the Ceph/RBD backend, due to a limitation in\nCinder, we need to have both the credentials and the configuration in\n`/etc/ceph` for it to work.\n\n.. code-block:: shell\n\n    $ cd examples/baremetal\n    $ ./run.sh rbd\n    Starting cinderlib CSI v0.0.1 (cinderlib: 0.1.0, cinder: 11.1.0)\n    Running backend RBDDriver v1.2.0\n    Now serving on [::]:50051...\n\n\nThere is also an XtremIO example that only requires the iSCSI connection\npackages.\n\n\nContainerized\n~~~~~~~~~~~~~\n\nThere is a sample `Dockerfile` included in the project that has been used to\ncreate the `akrog/cinderlib-csi` container available in the docker hub.\n\nThere are two bash scripts, one for each example, that will run the CSI driver\non a container, be aware that the container needs to run as privileged to mount\nthe volumes.\n\nFor the RBD example we need to copy our \"ceph.conf\" and\n\"ceph.client.cinder.keyring\" files, assuming we are using the \"cinder\" user\ninto the example/docker directory replacing the existing ones.\n\n.. code-block:: shell\n\n    $ cd examples/docker\n    $ ./rbd.sh\n    Starting cinderlib CSI v0.0.1 (cinderlib: 0.1.0, cinder: 11.1.0)\n    Running backend RBDDriver v1.2.0\n    Now serving on [::]:50051...\n\nCSC\n~~~\n\nNow that we have the service running we can use the `CSC tool\n<https://github.com/rexray/gocsi/tree/master/csc>`_ to run\ncommands simulating the Container Orchestration system.\n\nDue to the recent changes in the CSI spec not all commands are available yet,\nso you won't be able to test the snapshot commands.\n\nChecking the plugin info:\n\n.. code-block:: shell\n\n    $ csc identity plugin-info -e tcp://127.0.0.1:50051\n    \"com.redhat.cinderlib-csi\"      \"0.0.1\" \"cinder-driver\"=\"RBDDriver\"     \"cinder-driver-supported\"=\"True\"        \"cinder-driver-version\"=\"1.2.0\" \"cinder-version\"=\"11.1.0\"       \"cinderlib-version\"=\"0.1.0\"     \"persistence\"=\"DBPersistence\"\n\nChecking the node id:\n\n.. code-block:: shell\n\n    $ csc node get-id -e tcp://127.0.0.1:50051\n    localhost.localdomain\n\n    $ hostname -f\n    localhost.localdomain\n\nChecking the current backend capacity:\n\n.. code-block:: shell\n\n    $ csc controller get-capacity -e tcp://127.0.0.1:50051\n    24202140712\n\nCreating a volume:\n\n.. code-block:: shell\n\n    $ csc controller create-volume --cap SINGLE_NODE_WRITER,block --req-bytes 2147483648 disk -e tcp://127.0.0.1:50051\n    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  2147483648\n\n\nListing volumes:\n\n.. code-block:: shell\n\n   $ csc controller list-volumes -e tcp://127.0.0.1:50051\n   \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  2147483648\n\nStore the volume id for all the following calls:\n\n.. code-block:: shell\n\n   $ vol_id=`csc controller list-volumes -e tcp://127.0.0.1:50051|awk '{ print gensub(\"\\\"\",\"\",\"g\",$1)}'\n\nAttaching the volume to `tmp/mnt/publish` on baremetal:\n\n.. code-block:: shell\n\n    $ touch ../../tmp/mnt/{staging,publish}\n\n    $ csc controller publish --cap SINGLE_NODE_WRITER,block --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  \"connection_info\"=\"{\\\"connector\\\": {\\\"initiator\\\": \\\"iqn.1994-05.com.redhat:aa532823bac9\\\", \\\"ip\\\": \\\"127.0.0.1\\\", \\\"platform\\\": \\\"x86_64\\\", \\\"host\\\": \\\"localhost.localdomain\\\", \\\"do_local_attach\\\": false, \\\"os_type\\\": \\\"linux2\\\", \\\"multipath\\\": false}, \\\"conn\\\": {\\\"driver_volume_type\\\": \\\"rbd\\\", \\\"data\\\": {\\\"secret_uuid\\\": null, \\\"volume_id\\\": \\\"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"auth_username\\\": \\\"cinder\\\", \\\"secret_type\\\": \\\"ceph\\\", \\\"name\\\": \\\"volumes/volume-5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"discard\\\": true, \\\"keyring\\\": \\\"[client.cinder]\\\\n\\\\tkey = AQCQPetaof03IxAAoHZJD6kGxiMQfLdn3QzdlQ==\\\\n\\\", \\\"cluster_name\\\": \\\"ceph\\\", \\\"hosts\\\": [\\\"192.168.1.22\\\"], \\\"auth_enabled\\\": true, \\\"ports\\\": [\\\"6789\\\"]}}}\"\n\n    $ csc node stage --pub-info connection_info=\"irrelevant\" --cap SINGLE_NODE_WRITER,block --staging-target-path `realpath ../../tmp/mnt/staging` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node publish --cap SINGLE_NODE_WRITER,block --pub-info connection_info=\"irrelevant\" --staging-target-path `realpath ../../tmp/mnt/staging` --target-path `realpath ../../tmp/mnt/publish` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\nAttaching the volume to `tmp/mnt/publish` on container:\n\n.. code-block:: shell\n\n    $ touch ../../tmp/mnt/{staging,publish}\n\n    $ csc controller publish --cap SINGLE_NODE_WRITER,block --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  \"connection_info\"=\"{\\\"connector\\\": {\\\"initiator\\\": \\\"iqn.1994-05.com.redhat:aa532823bac9\\\", \\\"ip\\\": \\\"127.0.0.1\\\", \\\"platform\\\": \\\"x86_64\\\", \\\"host\\\": \\\"localhost.localdomain\\\", \\\"do_local_attach\\\": false, \\\"os_type\\\": \\\"linux2\\\", \\\"multipath\\\": false}, \\\"conn\\\": {\\\"driver_volume_type\\\": \\\"rbd\\\", \\\"data\\\": {\\\"secret_uuid\\\": null, \\\"volume_id\\\": \\\"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"auth_username\\\": \\\"cinder\\\", \\\"secret_type\\\": \\\"ceph\\\", \\\"name\\\": \\\"volumes/volume-5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"discard\\\": true, \\\"keyring\\\": \\\"[client.cinder]\\\\n\\\\tkey = AQCQPetaof03IxAAoHZJD6kGxiMQfLdn3QzdlQ==\\\\n\\\", \\\"cluster_name\\\": \\\"ceph\\\", \\\"hosts\\\": [\\\"192.168.1.22\\\"], \\\"auth_enabled\\\": true, \\\"ports\\\": [\\\"6789\\\"]}}}\"\n\n    $ csc node stage --pub-info connection_info=\"irrelevant\" --cap SINGLE_NODE_WRITER,block --staging-target-path /mnt/staging $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node publish --cap SINGLE_NODE_WRITER,block --pub-info connection_info=\"irrelevant\" --staging-target-path /mnt/staging --target-path /mnt/publish $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n\nDetaching the volume on baremetal:\n\n.. code-block:: shell\n\n    $ csc node unpublish --target-path `realpath ../../tmp/mnt/publish` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node unstage --staging-target-path `realpath ../../tmp/mnt/staging` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc controller unpublish --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\nDetaching the volume on container:\n\n.. code-block:: shell\n\n    $ csc node unpublish --target-path /mnt/publish $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node unstage --staging-target-path /tmp/mnt/staging $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc controller unpublish --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\nDeleting the volume:\n\n.. code-block:: shell\n\n    $ csc controller delete-volume $vol_id -e tcp://127.0.0.1:50051\n\n\nCapable operational modes\n-------------------------\n\nThe CSI spec defines a set of `AccessModes` that CSI drivers can support, such\nas single writer, single reader, multiple writers, single writer and multiple\nreaders.\n\nThis CSI driver currently only supports `SINGLE_MODE_WRITER`, although it will\nalso succeed with the `SINGLE_MODE_READER_ONLY` mode and mount it as\nread/write.\n\n\nSupport\n-------\n\nFor any questions or concerns please file an issue with the\n`cinderlib-csi <https://github.com/akrog/cinderlib-csi/issues>`_\nproject or ping me on IRC (my handle is geguileo and I hang on the\n#openstack-cinder channel in Freenode).\n\n\nTODO\n----\n\nThere are many things that need to be done in this POC driver, and here's a non\nexhaustive list:\n\n- Support for NFS volumes\n- Support for mount filesystems\n- Support for Kubernetes CRDs as the persistence storage\n- Unit tests\n- Functional tests\n- Improve received parameters checking\n- Make driver more resilient\n- Test driver in Kubernetes\n- Review some of the returned error codes\n- Support volume attributes via cinder volume types\n- Look into multi-attaching\n- Support read-only mode\n- Report capacity based on over provisioning values\n- Configure the private data location\n\n\n=======\nHistory\n=======\n\n0.0.2 (2018-06-19)\n------------------\n\n* Use cinderlib v0.2.1 instead of github branch\n\n0.0.1 (2018-05-18)\n------------------\n\n* First release on PyPI.\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/akrog/cinderlib-csi", "keywords": "cinderlib_csi", "license": "Apache Software License 2.0", "maintainer": "", "maintainer_email": "", "name": "cinderlib-csi", "package_url": "https://pypi.org/project/cinderlib-csi/", "platform": "", "project_url": "https://pypi.org/project/cinderlib-csi/", "project_urls": {"Homepage": "https://github.com/akrog/cinderlib-csi"}, "release_url": "https://pypi.org/project/cinderlib-csi/0.0.2/", "requires_dist": null, "requires_python": "", "summary": "CSI driver supporting all Cinder drivers without needing to run any additional services like RabbitMQ, MariaDB, or Cinder service", "version": "0.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            Cinderlib CSI driver<br>====================<br><br>.. image:: https://img.shields.io/pypi/v/cinderlib_csi.svg<br>   :target: https://pypi.python.org/pypi/cinderlib_csi<br><br>.. image:: https://img.shields.io/pypi/pyversions/cinderlib_csi.svg<br>   :target: https://pypi.python.org/pypi/cinderlib_csi<br><br>.. image:: https://pyup.io/repos/github/akrog/cinderlib_csi/shield.svg<br>     :target: https://pyup.io/repos/github/akrog/cinderlib_csi/<br>     :alt: Updates<br><br>.. image:: https://img.shields.io/:license-apache-blue.svg<br>   :target: http://www.apache.org/licenses/LICENSE-2.0<br><br><br>CSI Python driver that leverages all Cinder drivers to provide block volumes<br>without needing to run any additional service, such as RabbitMQ, MariaDB,<br>Cinder-API, Cinder-Scheduler, or Cinder-Volume.<br><br>Current code is is a **Proof of Concept** only compatible with Cinder<br>OSP-12/Pike release.<br><br>* Free software: Apache Software License 2.0<br>* Documentation: Pending<br><br><br>Features<br>--------<br><br>This CSI driver is up to date with latest CSI specs including the `new<br>snapshots feature<br>&lt;https://github.com/container-storage-interface/spec/pull/224&gt;`_ recently<br>introduced.<br><br>Currently supported features are:<br><br>- Create block volume<br>- Creating snapshots<br>- Creating a block volume from a snapshot<br>- Delete block volume<br>- Deleting snapshots<br>- Listing volumes with pagination<br>- Listing snapshots with pagination<br>- Attaching volumes<br>- Detaching volumes<br>- Reporting storage capacity<br>- Probing the node<br>- Retrieving the plugin info<br><br><br>Runtime Dependencies<br>--------------------<br><br>This driver requires that Cinder v11.0 (OSP-12/Pike) is already installed in<br>the system, how this is accomplished is left to the installer, as there are<br>multiple ways this can be accomplished:<br><br>- From OSP repositories<br>- From RDO repositories<br>- From github<br>- From other repositories<br><br>Any other basic requirement is already handled by `cinderlib-csi` when<br>installing from PyPi.<br><br>Besides the basic dependencies there are also some drivers that have additional<br>requirements that must be met for proper operation of the driver and/or<br>attachment/detachment operations, just like in Cinder.<br><br>Some of these Python dependencies for the Controller servicer are:<br><br>- DRBD: dbus and drbdmanage<br>- HPE 3PAR: python-3parclient<br>- Kaminario: krest<br>- Pure: purestorage<br>- Dell EMC VMAX, IBM DS8K: pyOpenSSL<br>- HPE Lefthad: python-lefthandclient<br>- Fujitsu Eternus DX: pywbem<br>- IBM XIV: pyxcli<br>- RBD: rados and rbd<br>- Dell EMC VNX: storops<br>- Violin: vmemclient<br>- INFINIDAT: infinisdk, capacity, infy.dtypes.wwn, infi.dtypes.iqn<br><br>Other backends may also require additional packages, for example LVM on<br>CentOS/RHEL requires the `targetcli` package, so please check with your<br>hardware vendor.<br><br>Besides the Controller requirements there are usually requirements for the<br>Node servicer needed to handle the attaching and detaching of volumes to the<br>node based on the connection used to access the storage.  For example:<br><br>- iSCSI: iscsi-initiator-tools and device-mapper-multipath<br>- RBD/Ceph: ceph-common package<br><br><br>Installation<br>------------<br><br>First we need to install the Cinder Python package, for example to install from<br>RDO on CentOS:<br><br>.. code-block:: shell<br><br>    $ sudo yum install -y centos-release-openstack-pike<br>    $ sudo yum install -y openstack-cinder python-pip<br><br><br>Then we just need to install the `cinderlib-csi` package:<br><br>.. code-block:: shell<br><br>    $ sudo pip install cinderlib-csi<br><br>Now we should install any additional package required by our backend.<br><br>For iSCSI backends we'll want to install:<br><br>.. code-block:: shell<br><br>    $ sudo yum install iscsi-initiator-utils<br>    $ sudo yum install device-mapper-multipath<br>    $ sudo mpathconf --enable --with_multipathd y --user_friendly_names n --find_multipaths y<br><br>For RBD we'll also need a specific package:<br><br>.. code-block:: shell<br><br>    $ sudo yum install ceph-common<br><br><br>Configuration<br>-------------<br><br>The CSI driver is configured via environmental variables, any value that<br>doesn't have a default is a required value.<br><br>+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+<br>| Name                       | Role       | Description                                                   | Default                                                                                                  | Example                                                                                                                                                                                                                                 |<br>+============================+============+===============================================================+==========================================================================================================+=========================================================================================================================================================================================================================================+<br>| `CSI_ENDPOINT`             | all        | IP and port to bind the service                               | [::]:50051                                                                                               | 192.168.1.22:50050                                                                                                                                                                                                                      |<br>+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+<br>| `CSI_MODE`                 | controller | Role the service should perform: controller, node, all        | all                                                                                                      | controller                                                                                                                                                                                                                              |<br>+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+<br>| `X_CSI_STORAGE_NW_IP`      | node       | IP address in the Node used to connect to the storage         | IP resolved from Node's fqdn                                                                             | 192.168.1.22                                                                                                                                                                                                                            |<br>+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+<br>| `X_CSI_NODE_ID`            | node       | ID used by this node to identify itself to the controller     | Node's fqdn                                                                                              | csi_test_node                                                                                                                                                                                                                           |<br>+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+<br>| `X_CSI_PERSISTENCE_CONFIG` | all        | Configuration of the `cinderlib` metadata persistence plugin. | {'storage': 'db', 'connection': 'sqlite:///db.sqlite'}                                                   | {'storage': 'db', 'connection': 'mysql+pymysql://root:stackdb@192.168.1.1/cinder?charset=utf8'}                                                                                                                                         |<br>+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+<br>| `X_CSI_CINDERLIB_CONFIG`   | controller | Global `cinderlib` configuration                              | {'project_id': 'com.redhat.cinderlib-csi', 'user_id': 'com.redhat.cinderlib-csi', 'root_helper': 'sudo'} | {\"project_id\":\"com.redhat.cinderlib-csi\",\"user_id\":\"com.redhat.cinderlib-csi\",\"root_helper\":\"sudo\"}                                                                                                                                     |<br>+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+<br>| `X_CSI_BACKEND_CONFIG`     | controller | Driver configuration                                          |                                                                                                          | {\"volume_backend_name\": \"rbd\", \"volume_driver\": \"cinder.volume.drivers.rbd.RBDDriver\", \"rbd_user\": \"cinder\", \"rbd_pool\": \"volumes\", \"rbd_ceph_conf\": \"/etc/ceph/ceph.conf\", \"rbd_keyring_conf\": \"/etc/ceph/ceph.client.cinder.keyring\"} |<br>+----------------------------+------------+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+<br><br>The only role that has been tested at the moment is the default one, where<br>Controller and Node servicer are executed in the same service (`CSI_MODE=all`),<br>and other modes are expected to have issues at the moment.<br><br><br>Staring the plugin<br>------------------<br><br>Once we have installed `cinderlib-csi` and required dependencies (for the<br>backend and for the connection type) we just have to run the `cinderlib-csi`<br>service with a user that can do passwordless sudo.<br><br>.. code-block:: shell<br><br>    $ cinderlib-csi<br><br><br>Testing the plugin<br>------------------<br><br>There are several examples of running the CSI cinderlib driver in the<br>`examples` directory both for a baremetal deployment and a containerized<br>version of the driver.<br><br>In all cases we have to run the plugin first before we can test it, and for<br>that we have to check the configuration provided as a test before starting the<br>plugin.  By default all examples run the service on port 50051.<br><br><br>Baremetal<br>~~~~~~~~~<br><br>For example to test with the LVM driver on our development environment we can<br>just run the following commands from the root of the `cinderlib-csi` project:<br><br>.. code-block:: shell<br><br>    $ cd tmp<br>    $ sudo dd if=/dev/zero of=cinder-volumes bs=1048576 seek=22527 count=1<br>    $ lodevice=`sudo losetup --show -f ./cinder-volumes`<br>    $ sudo pvcreate $lodevice<br>    $ sudo vgcreate cinder-volumes $lodevice<br>    $ sudo vgscan --cache<br>    $ cd ../examples/baremetal<br>    $ ./run.sh lvm<br>    py27 develop-inst-nodeps: /home/geguileo/code/reuse-cinder-drivers/cinderlib-csi<br>    py27 installed: ...<br>    ___ summary ___<br>      py27: skipped tests<br>      congratulations :)<br>    Starting cinderlib CSI v0.0.1 (cinderlib: 0.1.0, cinder: 11.1.1.dev41)<br>    Running backend LVMVolumeDriver v3.0.0<br>    Now serving on [::]:50051...<br><br><br>There is also an example of testing a Ceph cluster using a user called \"cinder\"<br>and the \"volumes\" pool.  For the Ceph/RBD backend, due to a limitation in<br>Cinder, we need to have both the credentials and the configuration in<br>`/etc/ceph` for it to work.<br><br>.. code-block:: shell<br><br>    $ cd examples/baremetal<br>    $ ./run.sh rbd<br>    Starting cinderlib CSI v0.0.1 (cinderlib: 0.1.0, cinder: 11.1.0)<br>    Running backend RBDDriver v1.2.0<br>    Now serving on [::]:50051...<br><br><br>There is also an XtremIO example that only requires the iSCSI connection<br>packages.<br><br><br>Containerized<br>~~~~~~~~~~~~~<br><br>There is a sample `Dockerfile` included in the project that has been used to<br>create the `akrog/cinderlib-csi` container available in the docker hub.<br><br>There are two bash scripts, one for each example, that will run the CSI driver<br>on a container, be aware that the container needs to run as privileged to mount<br>the volumes.<br><br>For the RBD example we need to copy our \"ceph.conf\" and<br>\"ceph.client.cinder.keyring\" files, assuming we are using the \"cinder\" user<br>into the example/docker directory replacing the existing ones.<br><br>.. code-block:: shell<br><br>    $ cd examples/docker<br>    $ ./rbd.sh<br>    Starting cinderlib CSI v0.0.1 (cinderlib: 0.1.0, cinder: 11.1.0)<br>    Running backend RBDDriver v1.2.0<br>    Now serving on [::]:50051...<br><br>CSC<br>~~~<br><br>Now that we have the service running we can use the `CSC tool<br>&lt;https://github.com/rexray/gocsi/tree/master/csc&gt;`_ to run<br>commands simulating the Container Orchestration system.<br><br>Due to the recent changes in the CSI spec not all commands are available yet,<br>so you won't be able to test the snapshot commands.<br><br>Checking the plugin info:<br><br>.. code-block:: shell<br><br>    $ csc identity plugin-info -e tcp://127.0.0.1:50051<br>    \"com.redhat.cinderlib-csi\"      \"0.0.1\" \"cinder-driver\"=\"RBDDriver\"     \"cinder-driver-supported\"=\"True\"        \"cinder-driver-version\"=\"1.2.0\" \"cinder-version\"=\"11.1.0\"       \"cinderlib-version\"=\"0.1.0\"     \"persistence\"=\"DBPersistence\"<br><br>Checking the node id:<br><br>.. code-block:: shell<br><br>    $ csc node get-id -e tcp://127.0.0.1:50051<br>    localhost.localdomain<br><br>    $ hostname -f<br>    localhost.localdomain<br><br>Checking the current backend capacity:<br><br>.. code-block:: shell<br><br>    $ csc controller get-capacity -e tcp://127.0.0.1:50051<br>    24202140712<br><br>Creating a volume:<br><br>.. code-block:: shell<br><br>    $ csc controller create-volume --cap SINGLE_NODE_WRITER,block --req-bytes 2147483648 disk -e tcp://127.0.0.1:50051<br>    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  2147483648<br><br><br>Listing volumes:<br><br>.. code-block:: shell<br><br>   $ csc controller list-volumes -e tcp://127.0.0.1:50051<br>   \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  2147483648<br><br>Store the volume id for all the following calls:<br><br>.. code-block:: shell<br><br>   $ vol_id=`csc controller list-volumes -e tcp://127.0.0.1:50051|awk '{ print gensub(\"\\\"\",\"\",\"g\",$1)}'<br><br>Attaching the volume to `tmp/mnt/publish` on baremetal:<br><br>.. code-block:: shell<br><br>    $ touch ../../tmp/mnt/{staging,publish}<br><br>    $ csc controller publish --cap SINGLE_NODE_WRITER,block --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051<br>    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  \"connection_info\"=\"{\\\"connector\\\": {\\\"initiator\\\": \\\"iqn.1994-05.com.redhat:aa532823bac9\\\", \\\"ip\\\": \\\"127.0.0.1\\\", \\\"platform\\\": \\\"x86_64\\\", \\\"host\\\": \\\"localhost.localdomain\\\", \\\"do_local_attach\\\": false, \\\"os_type\\\": \\\"linux2\\\", \\\"multipath\\\": false}, \\\"conn\\\": {\\\"driver_volume_type\\\": \\\"rbd\\\", \\\"data\\\": {\\\"secret_uuid\\\": null, \\\"volume_id\\\": \\\"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"auth_username\\\": \\\"cinder\\\", \\\"secret_type\\\": \\\"ceph\\\", \\\"name\\\": \\\"volumes/volume-5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"discard\\\": true, \\\"keyring\\\": \\\"[client.cinder]\\\\n\\\\tkey = AQCQPetaof03IxAAoHZJD6kGxiMQfLdn3QzdlQ==\\\\n\\\", \\\"cluster_name\\\": \\\"ceph\\\", \\\"hosts\\\": [\\\"192.168.1.22\\\"], \\\"auth_enabled\\\": true, \\\"ports\\\": [\\\"6789\\\"]}}}\"<br><br>    $ csc node stage --pub-info connection_info=\"irrelevant\" --cap SINGLE_NODE_WRITER,block --staging-target-path `realpath ../../tmp/mnt/staging` $vol_id -e tcp://127.0.0.1:50051<br>    5ee5fd7c-45cd-44cf-af7b-06081f680f2c<br><br>    $ csc node publish --cap SINGLE_NODE_WRITER,block --pub-info connection_info=\"irrelevant\" --staging-target-path `realpath ../../tmp/mnt/staging` --target-path `realpath ../../tmp/mnt/publish` $vol_id -e tcp://127.0.0.1:50051<br>    5ee5fd7c-45cd-44cf-af7b-06081f680f2c<br><br>Attaching the volume to `tmp/mnt/publish` on container:<br><br>.. code-block:: shell<br><br>    $ touch ../../tmp/mnt/{staging,publish}<br><br>    $ csc controller publish --cap SINGLE_NODE_WRITER,block --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051<br>    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  \"connection_info\"=\"{\\\"connector\\\": {\\\"initiator\\\": \\\"iqn.1994-05.com.redhat:aa532823bac9\\\", \\\"ip\\\": \\\"127.0.0.1\\\", \\\"platform\\\": \\\"x86_64\\\", \\\"host\\\": \\\"localhost.localdomain\\\", \\\"do_local_attach\\\": false, \\\"os_type\\\": \\\"linux2\\\", \\\"multipath\\\": false}, \\\"conn\\\": {\\\"driver_volume_type\\\": \\\"rbd\\\", \\\"data\\\": {\\\"secret_uuid\\\": null, \\\"volume_id\\\": \\\"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"auth_username\\\": \\\"cinder\\\", \\\"secret_type\\\": \\\"ceph\\\", \\\"name\\\": \\\"volumes/volume-5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"discard\\\": true, \\\"keyring\\\": \\\"[client.cinder]\\\\n\\\\tkey = AQCQPetaof03IxAAoHZJD6kGxiMQfLdn3QzdlQ==\\\\n\\\", \\\"cluster_name\\\": \\\"ceph\\\", \\\"hosts\\\": [\\\"192.168.1.22\\\"], \\\"auth_enabled\\\": true, \\\"ports\\\": [\\\"6789\\\"]}}}\"<br><br>    $ csc node stage --pub-info connection_info=\"irrelevant\" --cap SINGLE_NODE_WRITER,block --staging-target-path /mnt/staging $vol_id -e tcp://127.0.0.1:50051<br>    5ee5fd7c-45cd-44cf-af7b-06081f680f2c<br><br>    $ csc node publish --cap SINGLE_NODE_WRITER,block --pub-info connection_info=\"irrelevant\" --staging-target-path /mnt/staging --target-path /mnt/publish $vol_id -e tcp://127.0.0.1:50051<br>    5ee5fd7c-45cd-44cf-af7b-06081f680f2c<br><br><br>Detaching the volume on baremetal:<br><br>.. code-block:: shell<br><br>    $ csc node unpublish --target-path `realpath ../../tmp/mnt/publish` $vol_id -e tcp://127.0.0.1:50051<br>    5ee5fd7c-45cd-44cf-af7b-06081f680f2c<br><br>    $ csc node unstage --staging-target-path `realpath ../../tmp/mnt/staging` $vol_id -e tcp://127.0.0.1:50051<br>    5ee5fd7c-45cd-44cf-af7b-06081f680f2c<br><br>    $ csc controller unpublish --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051<br>    5ee5fd7c-45cd-44cf-af7b-06081f680f2c<br><br>Detaching the volume on container:<br><br>.. code-block:: shell<br><br>    $ csc node unpublish --target-path /mnt/publish $vol_id -e tcp://127.0.0.1:50051<br>    5ee5fd7c-45cd-44cf-af7b-06081f680f2c<br><br>    $ csc node unstage --staging-target-path /tmp/mnt/staging $vol_id -e tcp://127.0.0.1:50051<br>    5ee5fd7c-45cd-44cf-af7b-06081f680f2c<br><br>    $ csc controller unpublish --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051<br>    5ee5fd7c-45cd-44cf-af7b-06081f680f2c<br><br>Deleting the volume:<br><br>.. code-block:: shell<br><br>    $ csc controller delete-volume $vol_id -e tcp://127.0.0.1:50051<br><br><br>Capable operational modes<br>-------------------------<br><br>The CSI spec defines a set of `AccessModes` that CSI drivers can support, such<br>as single writer, single reader, multiple writers, single writer and multiple<br>readers.<br><br>This CSI driver currently only supports `SINGLE_MODE_WRITER`, although it will<br>also succeed with the `SINGLE_MODE_READER_ONLY` mode and mount it as<br>read/write.<br><br><br>Support<br>-------<br><br>For any questions or concerns please file an issue with the<br>`cinderlib-csi &lt;https://github.com/akrog/cinderlib-csi/issues&gt;`_<br>project or ping me on IRC (my handle is geguileo and I hang on the<br>#openstack-cinder channel in Freenode).<br><br><br>TODO<br>----<br><br>There are many things that need to be done in this POC driver, and here's a non<br>exhaustive list:<br><br>- Support for NFS volumes<br>- Support for mount filesystems<br>- Support for Kubernetes CRDs as the persistence storage<br>- Unit tests<br>- Functional tests<br>- Improve received parameters checking<br>- Make driver more resilient<br>- Test driver in Kubernetes<br>- Review some of the returned error codes<br>- Support volume attributes via cinder volume types<br>- Look into multi-attaching<br>- Support read-only mode<br>- Report capacity based on over provisioning values<br>- Configure the private data location<br><br><br>=======<br>History<br>=======<br><br>0.0.2 (2018-06-19)<br>------------------<br><br>* Use cinderlib v0.2.1 instead of github branch<br><br>0.0.1 (2018-05-18)<br>------------------<br><br>* First release on PyPI.<br>\n          </div>"}, "last_serial": 3977510, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "a12e464024b7054458163bc3ee4a7991", "sha256": "c1d9b61347900ba1055ae425fe302bc561f31e45a7526eaad0d45a00d7354499"}, "downloads": -1, "filename": "cinderlib-csi-0.0.1.tar.gz", "has_sig": false, "md5_digest": "a12e464024b7054458163bc3ee4a7991", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 46571, "upload_time": "2018-05-18T20:14:33", "upload_time_iso_8601": "2018-05-18T20:14:33.763726Z", "url": "https://files.pythonhosted.org/packages/ef/5e/2b42b6a93228b47a0e1e1decfff65636802a856e703e876d9dc1d737f6a9/cinderlib-csi-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "efc1ed686fa0c1532d2c1fb9dfbc1737", "sha256": "770ca50f8a374e87e2df0c7de116ed57484657b88260a60e4d0bba17163818c8"}, "downloads": -1, "filename": "cinderlib_csi-0.0.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "efc1ed686fa0c1532d2c1fb9dfbc1737", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 35562, "upload_time": "2018-06-19T13:42:55", "upload_time_iso_8601": "2018-06-19T13:42:55.498055Z", "url": "https://files.pythonhosted.org/packages/d5/16/862be1434288c0bdcd7a2674376f15f5c9b9f80d8cf0bbc21b850e1ebaae/cinderlib_csi-0.0.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "816177ee8f52950ec96182e1c712d92b", "sha256": "5154414977ad156a7ccfd519510da1968ed5238256a5f609c532b15dfa7a4cfc"}, "downloads": -1, "filename": "cinderlib-csi-0.0.2.tar.gz", "has_sig": false, "md5_digest": "816177ee8f52950ec96182e1c712d92b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 46885, "upload_time": "2018-06-19T13:42:53", "upload_time_iso_8601": "2018-06-19T13:42:53.659943Z", "url": "https://files.pythonhosted.org/packages/e6/f6/ebd8d653b7df4c0954574338e28a2710b2ddc604179dc79b42f2b30fe61e/cinderlib-csi-0.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "efc1ed686fa0c1532d2c1fb9dfbc1737", "sha256": "770ca50f8a374e87e2df0c7de116ed57484657b88260a60e4d0bba17163818c8"}, "downloads": -1, "filename": "cinderlib_csi-0.0.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "efc1ed686fa0c1532d2c1fb9dfbc1737", "packagetype": "bdist_wheel", "python_version": "2.7", "requires_python": null, "size": 35562, "upload_time": "2018-06-19T13:42:55", "upload_time_iso_8601": "2018-06-19T13:42:55.498055Z", "url": "https://files.pythonhosted.org/packages/d5/16/862be1434288c0bdcd7a2674376f15f5c9b9f80d8cf0bbc21b850e1ebaae/cinderlib_csi-0.0.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "816177ee8f52950ec96182e1c712d92b", "sha256": "5154414977ad156a7ccfd519510da1968ed5238256a5f609c532b15dfa7a4cfc"}, "downloads": -1, "filename": "cinderlib-csi-0.0.2.tar.gz", "has_sig": false, "md5_digest": "816177ee8f52950ec96182e1c712d92b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 46885, "upload_time": "2018-06-19T13:42:53", "upload_time_iso_8601": "2018-06-19T13:42:53.659943Z", "url": "https://files.pythonhosted.org/packages/e6/f6/ebd8d653b7df4c0954574338e28a2710b2ddc604179dc79b42f2b30fe61e/cinderlib-csi-0.0.2.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:19:22 2020"}