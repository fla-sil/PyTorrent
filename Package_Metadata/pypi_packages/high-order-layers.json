{"info": {"author": "John Loverich", "author_email": "john.loverich@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "[![Build Status](https://travis-ci.org/jloveric/piecewise-polynomial-layers.svg?branch=master)](https://travis-ci.org/jloveric/piecewise-polynomial-layers)\n[![Zenodo](https://zenodo.org/badge/DOI/10.5281/zenodo.3628932.svg)](https://zenodo.org/record/3628932#.Xi-RAd-YXRY)\n# Piecewise Polynomial and Fourier Series Layers for Tensorflow\nTensorflow layers using piecewise Lagrange polynomials with Gauss Lobatto nodes (I'm also adding truncated fourier series and other orthogonal functions).  This is a technique commonly used in finite element\nanalysis and means that the weight assigned to each node is exactly the function value at that node.  Long ago I wrote a c++ code that explored higher \norder polynomials in the synapse of a standard neural network [here](https://www.researchgate.net/publication/276923198_Discontinuous_Piecewise_Polynomial_Neural_Networks) .  Here I'm implementing some of that capability in Tensorflow.\n\n## Idea\n\nThe idea is extremely simple - instead of a single weight at the synapse, use n-weights.  The n-weights describe a piecewise polynomial and each of the n-weights can be updated independently.  A Lagrange polynomial and Gauss Lobatto points are used to minimize oscillations of the polynomial.  The same approach can be applied to any \"functional\" synapse, and I also have Fourier series synapses in this repo as well.  This can be implemented as construction of a polynomial or Fourier kernel followed by a standard tensorflow layer where a linear activation is used.\n\n## Why\n\nUsing higher order polynomial representations might allow networks with much fewer total weights. In physics, higher order methods\ncan be much more efficient, (while being more complex to implement). Spectral and discontinuous galerkin methods are examples of this.  Note that a standard neural network with relu activations is piecewise linear.  Here there are no bias weights and the \"non-linearity\" is in the synapse. \n\nIn addition, it's well known that the dendrites are also computational units in neurons, for example [Dendritic action potentials and computation in human layer 2/3 cortical neurons](https://science.sciencemag.org/content/367/6473/83) and this is a simple way to add more computational power into the artificial neural network model.\n\n# Installation\n\n```bash\npip install high-order-layers\n```\n\n# Use\n\n```python\nimport tensorflow as tf\nimport high_order_layers.PolynomialLayers as poly\nfrom tensorflow.keras.layers import *\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = (x_train / 128.0-1.0), (x_test / 128.0-1.0)\n\nunits = 20\n\nbasis = poly.b3\n\nmodel = tf.keras.models.Sequential([\n  Flatten(input_shape=(28, 28)),\n  poly.Polynomial(units, basis=basis, shift=0.0),\n  LayerNormalization(),\n  poly.Polynomial(units, basis=basis, shift=0.0),\n  LayerNormalization(),\n  poly.Polynomial(units, basis=basis, shift=0.0),\n  LayerNormalization(),\n  poly.Polynomial(units, basis=basis, shift=0.0),\n  LayerNormalization(),\n  Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=20, batch_size=10)\nmodel.evaluate(x_test, y_test)\n```\n\n# Examples\n\nRun the example from the main directory.  For the functionExample case run\n```bash\npython functionExample.py\n```\n\n1. [invariant mnist resnet](invariantMnistExample.py)\n2. [convolutional neural network mnist](mnistCNNExample.py)\n3. [fitting a sin wave](functionExample.py)\n4. [cifar10 convolutional neural network](cifar10CNNExample.py)\n5. [invariant cifar10 resnet](invariantCIFAR10Example.py)\n6. [reinforcement learning inverted pendulum Fourier series](inverted_pendulum_rl.py)\n\n# Fitting a function\n\nThe examples below are super simple - just fit a shifted sin wave.  Using the Lagrange Polynomial layers here a single input and ouput unit with no hidden layers is sufficient to fit the sin wave (as demonstrated below).  I'm hoping this helps illustrate exactly what is going on and why one might want to use a technique like this.  A comparison with a standard ReLU network with 1 and 2 hidden layers is provided for comparison.\n\n## Example - Simple Polynomial\n\nSolution is for a linear, cubic and 5th order polynomial used in the synapse - there are 6 weights in the 5th order polynomial and 2 units total (1 input and 1 output).\n\n![](images/sin5p.png)\n\n## Example 2 - Piecewise Discontinuous Polynomial (2 pieces)\n\nSame problem, but comparison between 1st, 2nd and 5th order piecewise discontinuous polynomial synapse.  This could be useful in problems that\nhave discontinuties such as many problems in physics.\n\n![](images/sin5d.png)\n\n## Example 3 - Piecewise Continuous Polynomial (2 pieces)\n\nSame problem, but comparison between 1st, 2nd and 5th order piecewise continuous polynomial synapse.\n\n![](images/sin5c.png)\n\n## Example 4 - Fourier series layer up to 5 frequencies\n\nSame problem, but comparison between 1, 2 and 5 and 5 frequency fourier series.\n\n![](images/sin5f.png)\n\n## Comparison with ReLU layer\n\nReLU network for comparison.\n![1 hidden layer with given number of units in each layer](images/sinRelu1.png)\nAdding a second layer and we get the result we expect.  However, at the cost of a massive increase in the total number of weights.  Since we are using a dense layer in the case of 5 units per layer we have a total of 35 weights.  At 10 units per layer we have 120 weights + bias weights.  5th order polynomial pair has a total of 12 weights in the discontinuous case and 11 in the continuous case.  So by moving to high order polynomials, it's possible the number of weights required decreases by as much as an order of magnitude - more research necessary, however this is inline with results from other fields.\n![2 hidden layers with given number of units in each layer](images/sinRelu2.png)\n\n## Available polynomial orders\n\n```python\nimport high_order_layers.PolynomialLayers as poly\n\n#Non piecewise polynomials\npoly.b1 #linear\npoly.b2 #quadratic\npoly.b3 #3rd order\nboly.b4 #4th order\npoly.b5 #5th order\n\n## Discontinous piecewise polynomials, 2 pieces\npoly.b1D #linear (discontinuous pair)\npoly.b2D #quadratic (discontinuous pair)\npoly.b3D #dubic (discontinuous pair)\npoly.b4D #quartic (discontinuous pair)\npoly.b5D #5th order (discontinuous pair)\n\n## Continuous piecewise polynomials, 2 pieces\npoly.b1C #linear (continuous pair)\npoly.b2C #quadratic (continuous pair)\npoly.b3C #cubic (continuous pair)\npoly.b4C #quartic (continuous pair)\npoly.b5C #5th order (continuous pair)\n```\nThe layer inside tensorflow is then called (see mnist example above)\n```\npoly.Polynomial(units, input, basis=basis),\n```\nwhere units is the number of units and input is the size of the input and basis would be 'poly.b3' for example.\n\n## Fourier Series Layer\nIn addition there is a fourier series layer\n```python\nimport snovalleyai_piecewise_polynomial_layers.FourierLayers as fourier\n...\nlayer = fourier.Fourier(units, frequencies=10, length=2.0, shift=0.0)\n```\nwhere 'units' is the number of units, 'frequencies' is the number of frequencies to include and 'length' is the wavelength of the longest wave.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/jloveric/high-order-layers", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "high-order-layers", "package_url": "https://pypi.org/project/high-order-layers/", "platform": "", "project_url": "https://pypi.org/project/high-order-layers/", "project_urls": {"Homepage": "https://github.com/jloveric/high-order-layers"}, "release_url": "https://pypi.org/project/high-order-layers/1.1.5/", "requires_dist": null, "requires_python": "", "summary": "Polynomial, piecewise polynomial, fourier series layers for tensorflow", "version": "1.1.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://travis-ci.org/jloveric/piecewise-polynomial-layers\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4147e3009df49c2971ad78317e70f00e27696a9c/68747470733a2f2f7472617669732d63692e6f72672f6a6c6f76657269632f7069656365776973652d706f6c796e6f6d69616c2d6c61796572732e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://zenodo.org/record/3628932#.Xi-RAd-YXRY\" rel=\"nofollow\"><img alt=\"Zenodo\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/86828fc4447ba2f559204ebcc3196389eec80d93/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333632383933322e737667\"></a></p>\n<h1>Piecewise Polynomial and Fourier Series Layers for Tensorflow</h1>\n<p>Tensorflow layers using piecewise Lagrange polynomials with Gauss Lobatto nodes (I'm also adding truncated fourier series and other orthogonal functions).  This is a technique commonly used in finite element\nanalysis and means that the weight assigned to each node is exactly the function value at that node.  Long ago I wrote a c++ code that explored higher\norder polynomials in the synapse of a standard neural network <a href=\"https://www.researchgate.net/publication/276923198_Discontinuous_Piecewise_Polynomial_Neural_Networks\" rel=\"nofollow\">here</a> .  Here I'm implementing some of that capability in Tensorflow.</p>\n<h2>Idea</h2>\n<p>The idea is extremely simple - instead of a single weight at the synapse, use n-weights.  The n-weights describe a piecewise polynomial and each of the n-weights can be updated independently.  A Lagrange polynomial and Gauss Lobatto points are used to minimize oscillations of the polynomial.  The same approach can be applied to any \"functional\" synapse, and I also have Fourier series synapses in this repo as well.  This can be implemented as construction of a polynomial or Fourier kernel followed by a standard tensorflow layer where a linear activation is used.</p>\n<h2>Why</h2>\n<p>Using higher order polynomial representations might allow networks with much fewer total weights. In physics, higher order methods\ncan be much more efficient, (while being more complex to implement). Spectral and discontinuous galerkin methods are examples of this.  Note that a standard neural network with relu activations is piecewise linear.  Here there are no bias weights and the \"non-linearity\" is in the synapse.</p>\n<p>In addition, it's well known that the dendrites are also computational units in neurons, for example <a href=\"https://science.sciencemag.org/content/367/6473/83\" rel=\"nofollow\">Dendritic action potentials and computation in human layer 2/3 cortical neurons</a> and this is a simple way to add more computational power into the artificial neural network model.</p>\n<h1>Installation</h1>\n<pre>pip install high-order-layers\n</pre>\n<h1>Use</h1>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"nn\">tf</span>\n<span class=\"kn\">import</span> <span class=\"nn\">high_order_layers.PolynomialLayers</span> <span class=\"k\">as</span> <span class=\"nn\">poly</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.layers</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n<span class=\"n\">mnist</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">mnist</span>\n\n<span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">),(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">mnist</span><span class=\"o\">.</span><span class=\"n\">load_data</span><span class=\"p\">()</span>\n<span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x_train</span> <span class=\"o\">/</span> <span class=\"mf\">128.0</span><span class=\"o\">-</span><span class=\"mf\">1.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">x_test</span> <span class=\"o\">/</span> <span class=\"mf\">128.0</span><span class=\"o\">-</span><span class=\"mf\">1.0</span><span class=\"p\">)</span>\n\n<span class=\"n\">units</span> <span class=\"o\">=</span> <span class=\"mi\">20</span>\n\n<span class=\"n\">basis</span> <span class=\"o\">=</span> <span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b3</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">([</span>\n  <span class=\"n\">Flatten</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">)),</span>\n  <span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">Polynomial</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"p\">,</span> <span class=\"n\">basis</span><span class=\"o\">=</span><span class=\"n\">basis</span><span class=\"p\">,</span> <span class=\"n\">shift</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">),</span>\n  <span class=\"n\">LayerNormalization</span><span class=\"p\">(),</span>\n  <span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">Polynomial</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"p\">,</span> <span class=\"n\">basis</span><span class=\"o\">=</span><span class=\"n\">basis</span><span class=\"p\">,</span> <span class=\"n\">shift</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">),</span>\n  <span class=\"n\">LayerNormalization</span><span class=\"p\">(),</span>\n  <span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">Polynomial</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"p\">,</span> <span class=\"n\">basis</span><span class=\"o\">=</span><span class=\"n\">basis</span><span class=\"p\">,</span> <span class=\"n\">shift</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">),</span>\n  <span class=\"n\">LayerNormalization</span><span class=\"p\">(),</span>\n  <span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">Polynomial</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"p\">,</span> <span class=\"n\">basis</span><span class=\"o\">=</span><span class=\"n\">basis</span><span class=\"p\">,</span> <span class=\"n\">shift</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">),</span>\n  <span class=\"n\">LayerNormalization</span><span class=\"p\">(),</span>\n  <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'softmax'</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s1\">'adam'</span><span class=\"p\">,</span>\n              <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s1\">'sparse_categorical_crossentropy'</span><span class=\"p\">,</span>\n              <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'accuracy'</span><span class=\"p\">])</span>\n\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">evaluate</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">)</span>\n</pre>\n<h1>Examples</h1>\n<p>Run the example from the main directory.  For the functionExample case run</p>\n<pre>python functionExample.py\n</pre>\n<ol>\n<li><a href=\"invariantMnistExample.py\" rel=\"nofollow\">invariant mnist resnet</a></li>\n<li><a href=\"mnistCNNExample.py\" rel=\"nofollow\">convolutional neural network mnist</a></li>\n<li><a href=\"functionExample.py\" rel=\"nofollow\">fitting a sin wave</a></li>\n<li><a href=\"cifar10CNNExample.py\" rel=\"nofollow\">cifar10 convolutional neural network</a></li>\n<li><a href=\"invariantCIFAR10Example.py\" rel=\"nofollow\">invariant cifar10 resnet</a></li>\n<li><a href=\"inverted_pendulum_rl.py\" rel=\"nofollow\">reinforcement learning inverted pendulum Fourier series</a></li>\n</ol>\n<h1>Fitting a function</h1>\n<p>The examples below are super simple - just fit a shifted sin wave.  Using the Lagrange Polynomial layers here a single input and ouput unit with no hidden layers is sufficient to fit the sin wave (as demonstrated below).  I'm hoping this helps illustrate exactly what is going on and why one might want to use a technique like this.  A comparison with a standard ReLU network with 1 and 2 hidden layers is provided for comparison.</p>\n<h2>Example - Simple Polynomial</h2>\n<p>Solution is for a linear, cubic and 5th order polynomial used in the synapse - there are 6 weights in the 5th order polynomial and 2 units total (1 input and 1 output).</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/097152745a49534487274a46158493a437c1b16e/696d616765732f73696e35702e706e67\"></p>\n<h2>Example 2 - Piecewise Discontinuous Polynomial (2 pieces)</h2>\n<p>Same problem, but comparison between 1st, 2nd and 5th order piecewise discontinuous polynomial synapse.  This could be useful in problems that\nhave discontinuties such as many problems in physics.</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/972048c30620ca3028a9af8cbcf8573960575663/696d616765732f73696e35642e706e67\"></p>\n<h2>Example 3 - Piecewise Continuous Polynomial (2 pieces)</h2>\n<p>Same problem, but comparison between 1st, 2nd and 5th order piecewise continuous polynomial synapse.</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ab81a4fbf0acbb26882c66b1f5055817480aa68b/696d616765732f73696e35632e706e67\"></p>\n<h2>Example 4 - Fourier series layer up to 5 frequencies</h2>\n<p>Same problem, but comparison between 1, 2 and 5 and 5 frequency fourier series.</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b705805c7a292e06d8620fdfdd86e29160766573/696d616765732f73696e35662e706e67\"></p>\n<h2>Comparison with ReLU layer</h2>\n<p>ReLU network for comparison.\n<img alt=\"1 hidden layer with given number of units in each layer\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f471d6550e8e9737bf9e4ef03d62d0dd2a9576b1/696d616765732f73696e52656c75312e706e67\">\nAdding a second layer and we get the result we expect.  However, at the cost of a massive increase in the total number of weights.  Since we are using a dense layer in the case of 5 units per layer we have a total of 35 weights.  At 10 units per layer we have 120 weights + bias weights.  5th order polynomial pair has a total of 12 weights in the discontinuous case and 11 in the continuous case.  So by moving to high order polynomials, it's possible the number of weights required decreases by as much as an order of magnitude - more research necessary, however this is inline with results from other fields.\n<img alt=\"2 hidden layers with given number of units in each layer\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/33c1e479648390985f1bfd081776dfaf7be7ca93/696d616765732f73696e52656c75322e706e67\"></p>\n<h2>Available polynomial orders</h2>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">high_order_layers.PolynomialLayers</span> <span class=\"k\">as</span> <span class=\"nn\">poly</span>\n\n<span class=\"c1\">#Non piecewise polynomials</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b1</span> <span class=\"c1\">#linear</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b2</span> <span class=\"c1\">#quadratic</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b3</span> <span class=\"c1\">#3rd order</span>\n<span class=\"n\">boly</span><span class=\"o\">.</span><span class=\"n\">b4</span> <span class=\"c1\">#4th order</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b5</span> <span class=\"c1\">#5th order</span>\n\n<span class=\"c1\">## Discontinous piecewise polynomials, 2 pieces</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b1D</span> <span class=\"c1\">#linear (discontinuous pair)</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b2D</span> <span class=\"c1\">#quadratic (discontinuous pair)</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b3D</span> <span class=\"c1\">#dubic (discontinuous pair)</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b4D</span> <span class=\"c1\">#quartic (discontinuous pair)</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b5D</span> <span class=\"c1\">#5th order (discontinuous pair)</span>\n\n<span class=\"c1\">## Continuous piecewise polynomials, 2 pieces</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b1C</span> <span class=\"c1\">#linear (continuous pair)</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b2C</span> <span class=\"c1\">#quadratic (continuous pair)</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b3C</span> <span class=\"c1\">#cubic (continuous pair)</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b4C</span> <span class=\"c1\">#quartic (continuous pair)</span>\n<span class=\"n\">poly</span><span class=\"o\">.</span><span class=\"n\">b5C</span> <span class=\"c1\">#5th order (continuous pair)</span>\n</pre>\n<p>The layer inside tensorflow is then called (see mnist example above)</p>\n<pre><code>poly.Polynomial(units, input, basis=basis),\n</code></pre>\n<p>where units is the number of units and input is the size of the input and basis would be 'poly.b3' for example.</p>\n<h2>Fourier Series Layer</h2>\n<p>In addition there is a fourier series layer</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">snovalleyai_piecewise_polynomial_layers.FourierLayers</span> <span class=\"k\">as</span> <span class=\"nn\">fourier</span>\n<span class=\"o\">...</span>\n<span class=\"n\">layer</span> <span class=\"o\">=</span> <span class=\"n\">fourier</span><span class=\"o\">.</span><span class=\"n\">Fourier</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"p\">,</span> <span class=\"n\">frequencies</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">length</span><span class=\"o\">=</span><span class=\"mf\">2.0</span><span class=\"p\">,</span> <span class=\"n\">shift</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">)</span>\n</pre>\n<p>where 'units' is the number of units, 'frequencies' is the number of frequencies to include and 'length' is the wavelength of the longest wave.</p>\n\n          </div>"}, "last_serial": 6579690, "releases": {"1.1.4": [{"comment_text": "", "digests": {"md5": "4cb7cdb05c0b6d24786da51c751838cc", "sha256": "c66b63dd62688e948beef3f71a295231bdf35e30eb1d6e36d4263333ab99e764"}, "downloads": -1, "filename": "high_order_layers-1.1.4.tar.gz", "has_sig": false, "md5_digest": "4cb7cdb05c0b6d24786da51c751838cc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7267, "upload_time": "2020-02-04T15:58:46", "upload_time_iso_8601": "2020-02-04T15:58:46.358736Z", "url": "https://files.pythonhosted.org/packages/6e/c5/1f6375a25ef228f71a1381c99d4588118adeb2c6b4eb8f0f2cf14458756a/high_order_layers-1.1.4.tar.gz", "yanked": false}], "1.1.5": [{"comment_text": "", "digests": {"md5": "03f0ef1da4b9535732ed899e0dee02e2", "sha256": "a700033a46ebdb84ec06a9e4bfb7029ead3f6e119f7d798bb6cafc6d39e0dfcc"}, "downloads": -1, "filename": "high_order_layers-1.1.5.tar.gz", "has_sig": false, "md5_digest": "03f0ef1da4b9535732ed899e0dee02e2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8091, "upload_time": "2020-02-06T00:57:18", "upload_time_iso_8601": "2020-02-06T00:57:18.713711Z", "url": "https://files.pythonhosted.org/packages/81/9c/957b6882d34295cc271156348e5a5e60e89add9745887cabfe487be21973/high_order_layers-1.1.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "03f0ef1da4b9535732ed899e0dee02e2", "sha256": "a700033a46ebdb84ec06a9e4bfb7029ead3f6e119f7d798bb6cafc6d39e0dfcc"}, "downloads": -1, "filename": "high_order_layers-1.1.5.tar.gz", "has_sig": false, "md5_digest": "03f0ef1da4b9535732ed899e0dee02e2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 8091, "upload_time": "2020-02-06T00:57:18", "upload_time_iso_8601": "2020-02-06T00:57:18.713711Z", "url": "https://files.pythonhosted.org/packages/81/9c/957b6882d34295cc271156348e5a5e60e89add9745887cabfe487be21973/high_order_layers-1.1.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:51:14 2020"}