{"info": {"author": "Anastasis Germanidis", "author_email": "agermanidis@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "<img width=800 src=\"resources/preview.jpg\"></img>\n\n## Thingscoop: Utility for searching and filtering videos based on their content\n\n### Description\n\nThingscoop is a command-line utility for analyzing videos semantically - that means searching, filtering, and describing videos based on objects, places, and other things that appear in them.\n\nWhen you first run thingscoop on a video file, it uses a [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) to create an \"index\" of what's contained in the every second of the input by repeatedly performing image classification on a frame-by-frame basis. Once an index for a video file has been created, you can search (i.e. get the start and end times of the regions in the video matching the query) and filter (i.e. create a [supercut](https://en.wikipedia.org/wiki/Supercut) of the matching regions) the input using arbitrary queries. Thingscoop uses a very basic query language that lets you to compose queries that test for the presence or absence of labels with the logical operators `!` (not), `||` (or) and `&&` (and). For example, to search a video the presence of the sky *and* the absence of the ocean: `thingscoop search 'sky && !ocean' <file>`.\n\nRight now two models are supported by thingscoop: `vgg_imagenet` uses the architecture described in [\"Very Deep Convolutional Networks for Large-Scale Image Recognition\"](http://arxiv.org/abs/1409.1556) to recognize objects from the [ImageNet](http://www.image-net.org/) database, and `googlenet_places` uses the architecture described in [\"Going Deeper with Convolutions\"](http://arxiv.org/abs/1409.4842) to recognize settings and places from the [MIT Places](http://places.csail.mit.edu/) database. You can specify which model you'd like to use by running `thingscoop models use <model>`, where `<model>` is either `vgg_imagenet` or `googlenet_places`. More models will be added soon.\n\nThingscoop is based on [Caffe](http://caffe.berkeleyvision.org/), an open-source deep learning framework.\n\n### Installation\n\n1. Install ffmpeg, imagemagick, and ghostscript: `brew install ffmpeg imagemagick ghostscript` (Mac OS X) or `apt-get install ffmpeg imagemagick ghostscript` (Ubuntu).\n1. Follow the installation instructions on the [Caffe Installation page](http://caffe.berkeleyvision.org/installation.html). \n2. Make sure you build the Python bindings by running `make pycaffe` (on Caffe's directory).\n3. Set the environment variable CAFFE_ROOT to point to Caffe's directory: `export CAFFE_ROOT=[Caffe's directory]`.\n4. Install thingscoop: `easy_install thingscoop` or `pip install thingscoop`.\n\n### Usage\n\n#### `thingscoop search <query> <files...>`\n\nPrint the start and end times (in seconds) of the regions in `<files>` that match `<query>`. Creates an index for `<file>` using the current model if it does not exist.\n\nExample output:\n\n```\n$ thingscoop search violin waking_life.mp4\n/Users/anastasis/Downloads/waking_life.mp4 148.000000 162.000000\n/Users/anastasis/Downloads/waking_life.mp4 176.000000 179.000000\n/Users/anastasis/Downloads/waking_life.mp4 180.000000 186.000000\n/Users/anastasis/Downloads/waking_life.mp4 189.000000 190.000000\n/Users/anastasis/Downloads/waking_life.mp4 192.000000 200.000000\n/Users/anastasis/Downloads/waking_life.mp4 211.000000 212.000000\n/Users/anastasis/Downloads/waking_life.mp4 222.000000 223.000000\n/Users/anastasis/Downloads/waking_life.mp4 235.000000 243.000000\n/Users/anastasis/Downloads/waking_life.mp4 247.000000 249.000000\n/Users/anastasis/Downloads/waking_life.mp4 251.000000 253.000000\n/Users/anastasis/Downloads/waking_life.mp4 254.000000 258.000000\n```\n\n####`thingscoop filter <query> <files...>`\n\nGenerate a video compilation of the regions in the `<files>` that match `<query>`. Creates index for `<file>` using the current model if it does not exist.\n\nExample output:\n\n<a href=\"https://www.youtube.com/watch?v=qe9GjrUJipY\"><img width=600 src=\"resources/filter.png\"></img></a>\n\n#### `thingscoop sort <file>`\n\nCreate a compilation video showing examples for every label recognized in the video (in alphabetic order). Creates an index for `<file>` using the current model if it does not exist.\n\nExample output:\n\n<a href=\"https://www.youtube.com/watch?v=o0VoyJgPgJE><img width=600 src=\"resources/clockwork_orange.png\"></img></a>\n\n#### `thingscoop describe <file>`\n\nPrint every label that appears in `<file>` along with the number of times it appears. Creates an index for `<file>` using the current model if it does not exist.\n\n#### `thingscoop preview <file>`\n\nCreate a window that plays the input video `<file>` while also displaying the labels the model recognizes on every frame.\n\n```\n$ thingscoop describe koyaanisqatsi.mp4 -m googlenet_places\nsky 405\nskyscraper 363\ncanyon 141\noffice_building 130\nhighway 78\nlighthouse 66\nhospital 64\ndesert 59\nshower 49\nvolcano 45\nunderwater 44\nairport_terminal 43\nfountain 39\nrunway 36\nassembly_line 35\naquarium 34\nfire_escape 34\nmusic_studio 32\nbar 28\namusement_park 28\nstage 26\nwheat_field 25\nbutchers_shop 25\nengine_room 24\nslum 20\nbutte 20\nigloo 20\n...etc\n```\n\n#### `thingscoop index <file>`\n\nCreate an index for `<file>` using the current model if it does not exist.\n\n#### `thingscoop models list`\n\nList all models currently available in Thingscoop.\n\n```\n$ thingscoop models list\ngooglenet_imagenet            Model described in the paper \"Going Deeper with Convolutions\" trained on the ImageNet database\ngooglenet_places              Model described in the paper \"Going Deeper with Convolutions\" trained on the MIT Places database\nvgg_imagenet                  16-layer model described in the paper \"Return of the Devil in the Details: Delving Deep into Convolutional Nets\" trained on the ImageNet database\n```\n\n#### `thingscoop models info <model>`\n\nPrint more detailed information about `<model>`.\n\n```\n$ thingscoop models info googlenet_places\nName: googlenet_places\nDescription: Model described in the paper \"Going Deeper with Convolutions\" trained on the MIT Places database\nDataset: MIT Places\n```\n\n#### `thingscoop models freeze`\n\nList all models that have already been downloaded.\n\n```\n$ thingscoop models freeze\ngooglenet_places\nvgg_imagenet\n```\n\n#### `thingscoop models current`\n\nPrint the model that is currently in use.\n\n```\n$ thingscoop models current\ngooglenet_places\n```\n\n#### `thingscoop models use <model>`\n\nSet the current model to `<model>`. Downloads that model locally if it hasn't been downloaded already.\n\n#### `thingscoop models download <model>`\n\nDownload the model `<model>` locally.\n\n#### `thingscoop models remove <model>`\n\nRemove the model `<model>` locally.\n\n#### `thingscoop models clear`\n\nRemove all models stored locally.\n\n#### `thingscoop labels list`\n\nPrint all the labels used by the current model.\n\n```\n$ thingscoop labels list\nabacus\nabaya\nabstraction\nacademic gown\naccessory\naccordion\nacorn\nacorn squash\nacoustic guitar\nact\nactinic radiation\naction\nactivity\nadhesive bandage\nadjudicator\nadministrative district\nadmiral\nadornment\nadventurer\nadvocate\n...\n```\n\n#### `thingscoop labels search <regexp>`\n\nPrint all the labels supported by the current model that match the regular expression `<regexp>`.\n\n```\n$ thingscoop labels search instrument$\nbeating-reed instrument\nbowed stringed instrument\ndouble-reed instrument\nfree-reed instrument\ninstrument\nkeyboard instrument\nmeasuring instrument\nmedical instrument\nmusical instrument\nnavigational instrument\nnegotiable instrument\noptical instrument\npercussion instrument\nscientific instrument\nstringed instrument\nsurveying instrument\nwind instrument\n...\n\n```\n\n### Full usage options\n\n```\nthingscoop - Command-line utility for searching and filtering videos based on their content\n\nUsage:\n  thingscoop filter <query> <files>... [-o <output_path>] [-m <model>] [-s <sr>] [-c <mc>] [--recreate-index] [--gpu-mode] [--open]\n  thingscoop search <query> <files>... [-o <output_path>] [-m <model>] [-s <sr>] [-c <mc>] [--recreate-index] [--gpu-mode] \n  thingscoop describe <file> [-n <words>] [-m <model>] [--recreate-index] [--gpu-mode] [-c <mc>]\n  thingscoop index <files> [-m <model>] [-s <sr>] [-c <mc>] [-r <ocr>] [--recreate-index] [--gpu-mode] \n  thingscoop sort <file> [-m <model>] [--gpu-mode] [--min-confidence <ct>] [--max-section-length <ms>] [-i <ignore>] [--open]\n  thingscoop preview <file> [-m <model>] [--gpu-mode] [--min-confidence <ct>]\n  thingscoop labels list [-m <model>]\n  thingscoop labels search <regexp> [-m <model>]\n  thingscoop models list\n  thingscoop models info <model>\n  thingscoop models freeze\n  thingscoop models current\n  thingscoop models use <model>\n  thingscoop models download <model>\n  thingscoop models remove <model>\n  thingscoop models clear\n\nOptions:\n  --version                       Show version.\n  -h --help                       Show this screen.\n  -o --output <dst>               Output file for supercut\n  -s --sample-rate <sr>           How many frames to classify per second (default = 1)\n  -c --min-confidence <mc>        Minimum prediction confidence required to consider a label (default depends on model)\n  -m --model <model>              Model to use (use 'thingscoop models list' to see all available models)\n  -n --number-of-words <words>    Number of words to describe the video with (default = 5)\n  -t --max-section-length <ms>    Max number of seconds to show examples of a label in the sorted video (default = 5)\n  -r --min-occurrences <ocr>      Minimum number of occurrences of a label in video required for it to be shown in the sorted video (default = 2)\n  -i --ignore-labels <labels>     Labels to ignore when creating the sorted video video\n  --title <title>                 Title to show at the beginning of the video (sort mode only)\n  --gpu-mode                      Enable GPU mode\n  --recreate-index                Recreate object index for file if it already exists\n  --open                          Open filtered video after creating it (OS X only)\n```\n\n### CHANGELOG\n\n#### 0.2 (8/16/2015)\n\n* Added `sort` option for creating a video compilation of all labels appearing in a video\n* Now using JSON for the index files\n\n#### 0.1 (8/5/2015)\n\n* Conception\n\n### License\n\nMIT\n", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/agermanidis/thingscoop", "keywords": null, "license": "MIT", "maintainer": null, "maintainer_email": null, "name": "thingscoop", "package_url": "https://pypi.org/project/thingscoop/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/thingscoop/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/agermanidis/thingscoop"}, "release_url": "https://pypi.org/project/thingscoop/0.2/", "requires_dist": null, "requires_python": null, "summary": "Command-line utility for searching and filtering videos based on objects that appear in them using convolutional neural networks", "version": "0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            &lt;img width=800 src=\"resources/preview.jpg\"&gt;&lt;/img&gt;<br><br>## Thingscoop: Utility for searching and filtering videos based on their content<br><br>### Description<br><br>Thingscoop is a command-line utility for analyzing videos semantically - that means searching, filtering, and describing videos based on objects, places, and other things that appear in them.<br><br>When you first run thingscoop on a video file, it uses a [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) to create an \"index\" of what's contained in the every second of the input by repeatedly performing image classification on a frame-by-frame basis. Once an index for a video file has been created, you can search (i.e. get the start and end times of the regions in the video matching the query) and filter (i.e. create a [supercut](https://en.wikipedia.org/wiki/Supercut) of the matching regions) the input using arbitrary queries. Thingscoop uses a very basic query language that lets you to compose queries that test for the presence or absence of labels with the logical operators `!` (not), `||` (or) and `&amp;&amp;` (and). For example, to search a video the presence of the sky *and* the absence of the ocean: `thingscoop search 'sky &amp;&amp; !ocean' &lt;file&gt;`.<br><br>Right now two models are supported by thingscoop: `vgg_imagenet` uses the architecture described in [\"Very Deep Convolutional Networks for Large-Scale Image Recognition\"](http://arxiv.org/abs/1409.1556) to recognize objects from the [ImageNet](http://www.image-net.org/) database, and `googlenet_places` uses the architecture described in [\"Going Deeper with Convolutions\"](http://arxiv.org/abs/1409.4842) to recognize settings and places from the [MIT Places](http://places.csail.mit.edu/) database. You can specify which model you'd like to use by running `thingscoop models use &lt;model&gt;`, where `&lt;model&gt;` is either `vgg_imagenet` or `googlenet_places`. More models will be added soon.<br><br>Thingscoop is based on [Caffe](http://caffe.berkeleyvision.org/), an open-source deep learning framework.<br><br>### Installation<br><br>1. Install ffmpeg, imagemagick, and ghostscript: `brew install ffmpeg imagemagick ghostscript` (Mac OS X) or `apt-get install ffmpeg imagemagick ghostscript` (Ubuntu).<br>1. Follow the installation instructions on the [Caffe Installation page](http://caffe.berkeleyvision.org/installation.html). <br>2. Make sure you build the Python bindings by running `make pycaffe` (on Caffe's directory).<br>3. Set the environment variable CAFFE_ROOT to point to Caffe's directory: `export CAFFE_ROOT=[Caffe's directory]`.<br>4. Install thingscoop: `easy_install thingscoop` or `pip install thingscoop`.<br><br>### Usage<br><br>#### `thingscoop search &lt;query&gt; &lt;files...&gt;`<br><br>Print the start and end times (in seconds) of the regions in `&lt;files&gt;` that match `&lt;query&gt;`. Creates an index for `&lt;file&gt;` using the current model if it does not exist.<br><br>Example output:<br><br>```<br>$ thingscoop search violin waking_life.mp4<br>/Users/anastasis/Downloads/waking_life.mp4 148.000000 162.000000<br>/Users/anastasis/Downloads/waking_life.mp4 176.000000 179.000000<br>/Users/anastasis/Downloads/waking_life.mp4 180.000000 186.000000<br>/Users/anastasis/Downloads/waking_life.mp4 189.000000 190.000000<br>/Users/anastasis/Downloads/waking_life.mp4 192.000000 200.000000<br>/Users/anastasis/Downloads/waking_life.mp4 211.000000 212.000000<br>/Users/anastasis/Downloads/waking_life.mp4 222.000000 223.000000<br>/Users/anastasis/Downloads/waking_life.mp4 235.000000 243.000000<br>/Users/anastasis/Downloads/waking_life.mp4 247.000000 249.000000<br>/Users/anastasis/Downloads/waking_life.mp4 251.000000 253.000000<br>/Users/anastasis/Downloads/waking_life.mp4 254.000000 258.000000<br>```<br><br>####`thingscoop filter &lt;query&gt; &lt;files...&gt;`<br><br>Generate a video compilation of the regions in the `&lt;files&gt;` that match `&lt;query&gt;`. Creates index for `&lt;file&gt;` using the current model if it does not exist.<br><br>Example output:<br><br>&lt;a href=\"https://www.youtube.com/watch?v=qe9GjrUJipY\"&gt;&lt;img width=600 src=\"resources/filter.png\"&gt;&lt;/img&gt;&lt;/a&gt;<br><br>#### `thingscoop sort &lt;file&gt;`<br><br>Create a compilation video showing examples for every label recognized in the video (in alphabetic order). Creates an index for `&lt;file&gt;` using the current model if it does not exist.<br><br>Example output:<br><br>&lt;a href=\"https://www.youtube.com/watch?v=o0VoyJgPgJE&gt;&lt;img width=600 src=\"resources/clockwork_orange.png\"&gt;&lt;/img&gt;&lt;/a&gt;<br><br>#### `thingscoop describe &lt;file&gt;`<br><br>Print every label that appears in `&lt;file&gt;` along with the number of times it appears. Creates an index for `&lt;file&gt;` using the current model if it does not exist.<br><br>#### `thingscoop preview &lt;file&gt;`<br><br>Create a window that plays the input video `&lt;file&gt;` while also displaying the labels the model recognizes on every frame.<br><br>```<br>$ thingscoop describe koyaanisqatsi.mp4 -m googlenet_places<br>sky 405<br>skyscraper 363<br>canyon 141<br>office_building 130<br>highway 78<br>lighthouse 66<br>hospital 64<br>desert 59<br>shower 49<br>volcano 45<br>underwater 44<br>airport_terminal 43<br>fountain 39<br>runway 36<br>assembly_line 35<br>aquarium 34<br>fire_escape 34<br>music_studio 32<br>bar 28<br>amusement_park 28<br>stage 26<br>wheat_field 25<br>butchers_shop 25<br>engine_room 24<br>slum 20<br>butte 20<br>igloo 20<br>...etc<br>```<br><br>#### `thingscoop index &lt;file&gt;`<br><br>Create an index for `&lt;file&gt;` using the current model if it does not exist.<br><br>#### `thingscoop models list`<br><br>List all models currently available in Thingscoop.<br><br>```<br>$ thingscoop models list<br>googlenet_imagenet            Model described in the paper \"Going Deeper with Convolutions\" trained on the ImageNet database<br>googlenet_places              Model described in the paper \"Going Deeper with Convolutions\" trained on the MIT Places database<br>vgg_imagenet                  16-layer model described in the paper \"Return of the Devil in the Details: Delving Deep into Convolutional Nets\" trained on the ImageNet database<br>```<br><br>#### `thingscoop models info &lt;model&gt;`<br><br>Print more detailed information about `&lt;model&gt;`.<br><br>```<br>$ thingscoop models info googlenet_places<br>Name: googlenet_places<br>Description: Model described in the paper \"Going Deeper with Convolutions\" trained on the MIT Places database<br>Dataset: MIT Places<br>```<br><br>#### `thingscoop models freeze`<br><br>List all models that have already been downloaded.<br><br>```<br>$ thingscoop models freeze<br>googlenet_places<br>vgg_imagenet<br>```<br><br>#### `thingscoop models current`<br><br>Print the model that is currently in use.<br><br>```<br>$ thingscoop models current<br>googlenet_places<br>```<br><br>#### `thingscoop models use &lt;model&gt;`<br><br>Set the current model to `&lt;model&gt;`. Downloads that model locally if it hasn't been downloaded already.<br><br>#### `thingscoop models download &lt;model&gt;`<br><br>Download the model `&lt;model&gt;` locally.<br><br>#### `thingscoop models remove &lt;model&gt;`<br><br>Remove the model `&lt;model&gt;` locally.<br><br>#### `thingscoop models clear`<br><br>Remove all models stored locally.<br><br>#### `thingscoop labels list`<br><br>Print all the labels used by the current model.<br><br>```<br>$ thingscoop labels list<br>abacus<br>abaya<br>abstraction<br>academic gown<br>accessory<br>accordion<br>acorn<br>acorn squash<br>acoustic guitar<br>act<br>actinic radiation<br>action<br>activity<br>adhesive bandage<br>adjudicator<br>administrative district<br>admiral<br>adornment<br>adventurer<br>advocate<br>...<br>```<br><br>#### `thingscoop labels search &lt;regexp&gt;`<br><br>Print all the labels supported by the current model that match the regular expression `&lt;regexp&gt;`.<br><br>```<br>$ thingscoop labels search instrument$<br>beating-reed instrument<br>bowed stringed instrument<br>double-reed instrument<br>free-reed instrument<br>instrument<br>keyboard instrument<br>measuring instrument<br>medical instrument<br>musical instrument<br>navigational instrument<br>negotiable instrument<br>optical instrument<br>percussion instrument<br>scientific instrument<br>stringed instrument<br>surveying instrument<br>wind instrument<br>...<br><br>```<br><br>### Full usage options<br><br>```<br>thingscoop - Command-line utility for searching and filtering videos based on their content<br><br>Usage:<br>  thingscoop filter &lt;query&gt; &lt;files&gt;... [-o &lt;output_path&gt;] [-m &lt;model&gt;] [-s &lt;sr&gt;] [-c &lt;mc&gt;] [--recreate-index] [--gpu-mode] [--open]<br>  thingscoop search &lt;query&gt; &lt;files&gt;... [-o &lt;output_path&gt;] [-m &lt;model&gt;] [-s &lt;sr&gt;] [-c &lt;mc&gt;] [--recreate-index] [--gpu-mode] <br>  thingscoop describe &lt;file&gt; [-n &lt;words&gt;] [-m &lt;model&gt;] [--recreate-index] [--gpu-mode] [-c &lt;mc&gt;]<br>  thingscoop index &lt;files&gt; [-m &lt;model&gt;] [-s &lt;sr&gt;] [-c &lt;mc&gt;] [-r &lt;ocr&gt;] [--recreate-index] [--gpu-mode] <br>  thingscoop sort &lt;file&gt; [-m &lt;model&gt;] [--gpu-mode] [--min-confidence &lt;ct&gt;] [--max-section-length &lt;ms&gt;] [-i &lt;ignore&gt;] [--open]<br>  thingscoop preview &lt;file&gt; [-m &lt;model&gt;] [--gpu-mode] [--min-confidence &lt;ct&gt;]<br>  thingscoop labels list [-m &lt;model&gt;]<br>  thingscoop labels search &lt;regexp&gt; [-m &lt;model&gt;]<br>  thingscoop models list<br>  thingscoop models info &lt;model&gt;<br>  thingscoop models freeze<br>  thingscoop models current<br>  thingscoop models use &lt;model&gt;<br>  thingscoop models download &lt;model&gt;<br>  thingscoop models remove &lt;model&gt;<br>  thingscoop models clear<br><br>Options:<br>  --version                       Show version.<br>  -h --help                       Show this screen.<br>  -o --output &lt;dst&gt;               Output file for supercut<br>  -s --sample-rate &lt;sr&gt;           How many frames to classify per second (default = 1)<br>  -c --min-confidence &lt;mc&gt;        Minimum prediction confidence required to consider a label (default depends on model)<br>  -m --model &lt;model&gt;              Model to use (use 'thingscoop models list' to see all available models)<br>  -n --number-of-words &lt;words&gt;    Number of words to describe the video with (default = 5)<br>  -t --max-section-length &lt;ms&gt;    Max number of seconds to show examples of a label in the sorted video (default = 5)<br>  -r --min-occurrences &lt;ocr&gt;      Minimum number of occurrences of a label in video required for it to be shown in the sorted video (default = 2)<br>  -i --ignore-labels &lt;labels&gt;     Labels to ignore when creating the sorted video video<br>  --title &lt;title&gt;                 Title to show at the beginning of the video (sort mode only)<br>  --gpu-mode                      Enable GPU mode<br>  --recreate-index                Recreate object index for file if it already exists<br>  --open                          Open filtered video after creating it (OS X only)<br>```<br><br>### CHANGELOG<br><br>#### 0.2 (8/16/2015)<br><br>* Added `sort` option for creating a video compilation of all labels appearing in a video<br>* Now using JSON for the index files<br><br>#### 0.1 (8/5/2015)<br><br>* Conception<br><br>### License<br><br>MIT<br>\n          </div>"}, "last_serial": 1679777, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "b7885941f613296372cbd04bfed450bb", "sha256": "902936ba35ef16451583b5c3181506ff3c7334bb33c866000687242aacb079c8"}, "downloads": -1, "filename": "thingscoop-0.1.tar.gz", "has_sig": false, "md5_digest": "b7885941f613296372cbd04bfed450bb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10987, "upload_time": "2015-08-05T00:54:29", "upload_time_iso_8601": "2015-08-05T00:54:29.156532Z", "url": "https://files.pythonhosted.org/packages/88/ce/2aaa9bfa87ac4e10681a9563d07b007ef652f17a19a4f9f14d39b7704f62/thingscoop-0.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "83d58e0c4c125616b53daa4a3fa0500f", "sha256": "fc5549647adc491caf704c72f71435dee01250703c30c283669e8174b22c1d92"}, "downloads": -1, "filename": "thingscoop-0.2.tar.gz", "has_sig": false, "md5_digest": "83d58e0c4c125616b53daa4a3fa0500f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12503, "upload_time": "2015-08-16T22:56:40", "upload_time_iso_8601": "2015-08-16T22:56:40.457635Z", "url": "https://files.pythonhosted.org/packages/f3/53/33d5edb3c029aa2b8e599dcefc6ab7a8bad7b21c982159b2abdd4633ce92/thingscoop-0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "83d58e0c4c125616b53daa4a3fa0500f", "sha256": "fc5549647adc491caf704c72f71435dee01250703c30c283669e8174b22c1d92"}, "downloads": -1, "filename": "thingscoop-0.2.tar.gz", "has_sig": false, "md5_digest": "83d58e0c4c125616b53daa4a3fa0500f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12503, "upload_time": "2015-08-16T22:56:40", "upload_time_iso_8601": "2015-08-16T22:56:40.457635Z", "url": "https://files.pythonhosted.org/packages/f3/53/33d5edb3c029aa2b8e599dcefc6ab7a8bad7b21c982159b2abdd4633ce92/thingscoop-0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:53:56 2020"}