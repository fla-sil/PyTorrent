{"info": {"author": "Karik Isichei", "author_email": "karik.isichei@digital.justice.gov.uk", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7"], "description": "# etl_manager\n\n[![Actions Status](https://github.com/moj-analytical-services/etl_manager/workflows/ETL%20Manager/badge.svg)](https://github.com/moj-analytical-services/etl_manager/actions)\n\nA python package that manages our data engineering framework and implements them on AWS Glue.\n\nThe main functionality of this package is to interact with AWS Glue to create meta data catalogues and run Glue jobs.\n\nTo install:\n\n```bash\npip install git+git://github.com/moj-analytical-services/etl_manager.git#egg=etl_manager\n```\n\nIf you do not have it installed already you will also need to install boto3.\n\n## Meta Data\n\nLet's say I have a single table (a csv file) and I want to query it using Amazon athena. My csv file is in the following S3 path: `s3://my-bucket/my-table-folder/file.csv`.\n\nfile.csv is a table that looks like this:\n\n| col1 | col2 |\n|------|------|\n| a    | 1    |\n| b    | 12   |\n| c    | 42   |\n\nAs you can see col1 is a string and col2 is a integer.\n\n> **Notes:**\n> - for Athena to work your table should not contain a header. So before file.csv is uploaded to S3 you should make sure it has no header.\n> - Tables must be in a folder. I.e. the location of your table (`table.location`) should be the parent folder of where you data exists. See example below.\n\nTo create a schema for your data to be queried by Athena you can use the following code:\n\n```python\nfrom etl_manager.meta import DatabaseMeta, TableMeta\n\n# Create database meta object\ndb = DatabaseMeta(name = 'my_database', bucket='my-bucket')\n\n# Create table meta object\ntab = TableMeta(name = 'my_table', location = 'my-table-folder')\n\n# Add column defintions to the table\ntab.add_column(name = 'col1', 'character', description = 'column contains a letter')\ntab.add_column(name = 'col2', 'int', description = 'column contains a number')\n\n# Add table to the database\ndb.add_table(tab)\n\n# Create the table on AWS glue\ndb.create_glue_database()\n```\n\nNow the table can be queried via SQL e.g. `SELECT * FROM my_database.my_table`\n\n### Meta data structure\n\nCurrently at very simple level. Assume you have the following folder structure from example code:\n\n```\nmeta_data/\n--- database.json\n--- teams.json\n--- employees.json\n```\n\ndatabase.json is a special json file that holds the meta data for the database. In our example it looks like this:\n\n```json\n{\n    \"description\": \"Example database\",\n    \"name\": \"workforce\",\n    \"bucket\": \"my-bucket\",\n    \"base_folder\": \"database/database1\"\n}\n```\n\nWhen you create your database it will have this `name` and `description`. The `bucket` key specifies where the database exists (and therefore where the tables exist) in S3. The `base_folder` is is the initial path to where the tables exist. If your tables are in folders directly in the bucket (e.g. `s3://my-bucket/table1/`) then you can leave base_folder as an empty string (`\"\"`).\n\nThe employees table has an ID for each employee their name and dob. The table meta looks like this:\n\n```json\n{\n    \"$schema\" : \"https://moj-analytical-services.github.io/metadata_schema/table/v1.0.0.json\",\n    \"name\": \"employees\",\n    \"description\": \"table containing employee information\",\n    \"data_format\": \"parquet\",\n    \"location\": \"employees/\",\n    \"columns\": [\n        {\n            \"name\": \"employee_id\",\n            \"type\": \"int\",\n            \"description\": \"an ID for each employee\"\n        },\n        {\n            \"name\": \"employee_name\",\n            \"type\": \"character\",\n            \"description\": \"name of the employee\"\n        },\n        {\n            \"name\": \"employee_dob\",\n            \"type\": \"date\",\n            \"description\": \"date of birth for the employee\"\n        }\n    ]\n}\n```\n\n**Currently supported data types for your columns currently are:**\n\n`character | int | long | float | double | date | datetime |  boolean`\n\nThis is a standard layout for a table metadata json. `$schema` points to another json that validates the structure of out table metadata files. Your table will have this `name` and `description` _(Note: It is strongly suggested that the name of your table matches the name of the metadata json file)_ when the database is created. The `location` is the relative folder path to where your table exists. This path is relative to your database `base_folder`. This means that the full path your table is `s3://<database.bucket>/<database.base_folder>/<table.folder>/`. So in this example the table employees should be in the s3 path `s3://my-bucket/database/database1/employees`. The `data_format` specifies what type of data the table is. Finally your columns is an array of objects. Where each object is a column definition specifying the `name`, `description` and `type` (data type) of the column. Each column can have optional arguments `pattern`, `enum` and `nullable` (see [table_schema.json](https://moj-analytical-services.github.io/metadata_schema/table/v1.0.0.json) for definition).\n\n**Note:** that the order of the columns listed here should be the order of the columns in the table _(remember that data for a table should not have a header so the data will be queried wrong if the column order does not match up with the actual data)_.\n\nHere is another table in the database called teams. The teams table is a list of employee IDs for each team. Showing which employees are in each team. This table is taken each month (so you can see which employee was in which team each month). Therefore this table is partitioned by each monthly snapshot.\n\n```json\n{\n    \"$schema\" : \"https://moj-analytical-services.github.io/metadata_schema/table/v1.0.0.json\",\n    \"name\": \"teams\",\n    \"description\": \"month snapshot of which employee with working in what team\",\n    \"data_format\": \"parquet\",\n    \"location\": \"teams/\",\n    \"columns\": [\n        {\n            \"name\": \"team_id\",\n            \"type\": \"int\",\n            \"description\": \"ID given to each team\",\n            \"nullable\" : false\n        },\n        {\n            \"name\": \"team_name\",\n            \"type\": \"character\",\n            \"description\": \"name of the team\"\n        },\n        {\n            \"name\": \"employee_id\",\n            \"type\": \"int\",\n            \"description\": \"primary key for each employee in the employees table\",\n            \"pattern\" : \"\\\\d+\"\n        },\n        {\n            \"name\": \"snapshot_year\",\n            \"type\": \"int\",\n            \"description\": \"year at which snapshot of workforce was taken\"\n        },\n        {\n            \"name\": \"snapshot_month\",\n            \"type\": \"int\",\n            \"description\": \"month at which snapshot of workforce was taken\",\n            \"enum\" : [1,2,3,4,5,6,7,8,9,10,11,12]\n        }\n    ],\n    \"partitions\" : [\"snapshot_year\", \"snapshot_month\"]\n}\n```\n\nFrom the above you can see this has additional properties `enum`, `pattern`, `nullable` and a `partitions` property:\n\n- **enum:** What values the column can take (does not have to include nulls - should use nullable property)\n- **pattern:** Values in this column should match this regex string in the pattern property\n- **nullable:** Specifies if this column should accept `NULL` values.\n- **partitions:** Specifies if any of the columns in the table are file partitions rather than columns in the data. `etl_manager` will force your meta data json files to have columns that are partitions at the end of your data's list of columns.\n\n> **Note:** etl_manager does not enforce information provided by `enum`, `pattern` and `nullable`. It is just there to provide information to other tools or functions that could use this information to validate your data. Also the information in `pattern`, `enum` and `nullable` can conflict etl_manager does not check for conflicts. For example a column with an enum of `[0,1]` and a pattern of `[A-Za-z]` is allowed.\n\n### Examples using the DatabaseMeta Class\n\nThe easiest way to create a database is to run the code below. It reads a database schema based on the json files in a folder and creates this database meta in the glue catalogue. Allowing you to query the data using SQL using Athena.\n\n```python\nfrom etl_manager.meta import read_database_folder\ndb = read_database_folder('example_meta_data/')\ndb.create_glue_database()\n```\n\nThe code snippet below creates a database meta object that allows you to manipulate the database and the tables that exist in it\n\n```python\nfrom etl_manager.meta import read_database_folder\n\ndb = read_database_folder('example_meta_data/')\n\n# Database has callable objects\n\ndb.name # workforce\n\ndb.table_names # [employees, teams]\n\n# Each table in the database is an object from the TableMeta Class which can be callable from the database meta object\n\ndb.table('employees').columns # returns all columns in employees table\n\n# The db and table object properties can also be altered and updated\n\ndb.name = 'new_db_name'\ndb.name # 'new_db_name\n\ndb.table('employees').name = 'new_name'\n\ndb.table_names # [new_name, teams]\n\ndb.remove_table('new_name')\n\ndb.name # workforce_dev (note as default the package adds _dev if a db_suffix is not provided in DatabaseMeta)\n\n# Set all table types to parquet and create database schema in glue\nfor t in db_table_names :\n    db.table(t).data_format = 'parquet'\ndb.create_glue_database()\n```\n\n## Using the GlueJob Class\n\nThe GlueJob class can be used to run pyspark jobs on AWS Glue. It is worth keeping up to date with AWS release notes and general guidance on running Glue jobs. This class is a wrapper function to simplify running glue jobs by using a structured format.\n\n```python\nfrom etl_manager.etl import GlueJob\n\nmy_role = 'aws_role'\nbucket = 'bucket-to-store-temp-glue-job-in'\n\njob = GlueJob('glue_jobs/simple_etl_job/', bucket=bucket, job_role=my_role, job_arguments={\"--test_arg\" : 'some_string'})\njob.run_job()\n\nprint(job.job_status)\n```\n\n### Glue Job Folder Structure\n\nGlue jobs have the prescribed folder format as follows:\n\n```\n\u251c\u2500\u2500 glue_jobs/\n|   |\n\u2502   \u251c\u2500\u2500 job1/\n|   |   \u251c\u2500\u2500 job.py\n|   |   \u251c\u2500\u2500 glue_resources/\n|   |   |   \u2514\u2500\u2500 my_lookup_table.csv\n|   |   \u2514\u2500\u2500 glue_py_resources/\n|   |       \u251c\u2500\u2500 my_python_functions.zip\n|   |       \u2514\u2500\u2500 github_zip_urls.txt\n|   |   \u2514\u2500\u2500 glue_jars/\n|   |       \u2514\u2500\u2500 my_jar.jar\n|   |\n\u2502   \u251c\u2500\u2500 job2/\n\u2502   |   \u251c\u2500\u2500 job.py\n\u2502   |   \u251c\u2500\u2500 glue_resources/\n\u2502   |   \u2514\u2500\u2500 glue_py_resources/\n|   |\n|   \u251c\u2500\u2500 shared_job_resources/\n\u2502   |   \u251c\u2500\u2500 glue_resources/\n|   |   |   \u2514\u2500\u2500 meta_data_dictionary.json\n\u2502   |   \u2514\u2500\u2500 glue_py_resources/\n|   |   \u2514\u2500\u2500 glue_jars/\n|   |       \u2514\u2500\u2500 my_other_jar.jar\n```\n\nEvery glue job folder must have a `job.py` script in that folder. That is the only required file everything else is optional. When you want to create a glue job object you point the GlueJob class to the parent folder of the `job.py` script you want to run. There are two additional folders you can add to this parent folder :\n\n#### glue_resources folder\n\nAny files in this folder are uploaded to the working directory of the glue job. This means in your `job.py` script you can get the path to these files by:\n\n```python\npath_to_file = os.path.join(os.getcwd(), 'file_in_folder.txt')\n```\n\nThe GlueJob class will only upload files with extensions (.csv, .sql, .json, .txt) to S3 for the glue job to access.\n\n#### glue_py_resources\n\nThese are python scripts you can import in your `job.py` script. e.g. if I had a `utils.py` script in my glue_py_resources folder. I could import that script normally e.g.\n\n```python\nfrom utils import *\n```\n\nYou can also supply zip file which is a group of python functions in the standard python package structure. You can then reference this package as you would normally in python. For example if I had a package zipped as `my_package.zip` in the glue_py_resources folder then you could access that package normally in your job script like:\n\n```python\nfrom my_package.utils import *\n```\n\nYou can also supply a text file with the special name `github_zip_urls.txt`. This is a text file where each line is a path to a github zip ball. The GlueJob class will download the github package rezip it and send it to S3. This github python package can then be accessed in the same way you would the local zip packages. For example if the `github_zip_urls.txt` file had a single line `https://github.com/moj-analytical-services/gluejobutils/archive/master.zip`. The package `gluejobutils` would be accessible in the `job.py` script:\n\n```python\nfrom gluejobutils.s3 import read_json_from_s3\n```\n\n#### shared_job_resources folder\n\nThis a specific folder (must have the name `shared_job_resources`). This folder has the same structure and restrictions as a normal glue job folder but does not have a `job.py` file. Instead anything in the `glue_resources` or `glue_py_resources` folders will also be used (and therefore uploaded to S3) by any other glue job. Take the example below:\n\n```\n\u251c\u2500\u2500 glue_jobs/\n\u2502   \u251c\u2500\u2500 job1/\n\u2502   |   \u251c\u2500\u2500 job.py\n\u2502   |   \u251c\u2500\u2500 glue_resources/\n|   |   |   \u2514\u2500\u2500 lookup_table.csv\n\u2502   |   \u2514\u2500\u2500 glue_py_resources/\n|   |       \u2514\u2500\u2500 job1_specific_functions.py\n|   |\n|   \u251c\u2500\u2500 shared_job_resources/\n\u2502   |   \u251c\u2500\u2500 glue_resources/\n|   |   |   \u2514\u2500\u2500 some_global_config.json\n\u2502   |   \u2514\u2500\u2500 glue_py_resources/\n|   |       \u2514\u2500\u2500 utils.py\n```\n\nRunning the glue job `job1` i.e.\n\n```python\njob = GlueJob('glue_jobs/job1/', bucket, job_role)\njob.run_job()\n```\n\nThis glue job would not only have access the the python script `job1_specific_functions.py` and file `lookup_table.csv` but also have access to the python script `utils.py` and file `some_global_config.json`. This is because the latter two files are in the `shared_job_resources` folder and accessible to all job folders (in their `glue_jobs` parent folder).\n\n**Note:** Users should make sure there is no naming conflicts between filenames that are uploaded to S3 as they are sent to the same working folder.\n\n### Using the Glue Job class\n\nReturning to the initial example:\n\n```python\nfrom etl_manager.etl import GlueJob\n\nmy_role = 'aws_role'\nbucket = 'bucket-to-store-temp-glue-job-in'\n\njob = GlueJob('glue_jobs/simple_etl_job/', bucket=bucket, job_role=my_role)\n```\n\nAllows you to create a job object. The GlueJob class will have a `job_name` which is defaulted to the folder name you pointed it to i.e. in this instance the job is called `simple_etl_job`. To change the job name:\n\n```python\njob.job_name = 'new_job_name'\n```\n\nIn AWS you can only have unique job names.\n\nOther useful function and properties:\n\n```python\n# Increase the number of workers on a glue job (default is 2)\njob.allocated_capacity = 5\n\n# Set job arguments these are input params that can be accessed by the job.py script\njob.job_arguments = {\"--test_arg\" : 'some_string', \"--enable-metrics\" : \"\"}\n```\n\n####\u00a0job_arguments\n\nThese are strings that can be passed to the glue job script. Below is an example of how these are accessed in the `job.py` script. This code snippit is taken from the `simple_etl_job` found in the `example` folder of this repo.\n\n```python\n# Example job tests access to all files passed to the job runner class\nimport sys\nimport os\n\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nfrom gluejobutils.s3 import read_json_from_s3\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME', 'metadata_base_path', 'test_arg'])\n\nprint \"JOB SPECS...\"\nprint \"JOB_NAME: \", args[\"JOB_NAME\"]\nprint \"test argument: \", args[\"test_arg\"]\n\n# Read in meta data json\nmeta_employees = read_json_from_s3(os.path.join(args['metadata_base_path'], \"employees.json\"))\n\n### etc\n```\n\n>**Notes:**\n> - The `test_arg` does not have two dashes in front of it. When specifying job_arguments with the GlueJob class it must be suffixed with `--` but you should remove these when accessing the args in the `job.py` script.\n> - `metadata_base_path` is a special parameter that is set by the GlueJob class. It is the S3 path to where the `meta_data` folder is in S3 so that you can read in your agnostic metadata files if you want to use them in your glue job. Note that the [gluejobutils](https://github.com/moj-analytical-services/gluejobutils) package has a lot of functionality with integrating our metadata jsons with spark.\n> - The GlueJob argument `--enable-metrics` is also a special parameter that enables you to see metrics of your glue job. [See here for more details on enabling metrics](https://docs.aws.amazon.com/en_us/glue/latest/dg/monitor-profile-glue-job-cloudwatch-metrics.html).\n> - Note that `JOB_NAME` is a special parameter that is not set in GlueJob but automatically passed to the AWS Glue when running `job.py`. [See here for more on special parameters](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-arguments.html).\n\nExample of full `glue_job` and `meta_data` structures and code can be found [here](https://github.com/moj-analytical-services/etl_manager/tree/master/example).\n\n# Unit Tests\n\nThis package has [unit tests](https://github.com/moj-analytical-services/etl_manager/blob/master/tests/test_tests.py) which can also be used to see functionality.\n\nUnit tests can be ran by:\n\n```python\npython -m unittest tests.test_tests -v\n```\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "", "keywords": "", "license": "MIT", "maintainer": "Karik Isichei", "maintainer_email": "karik.isichei@digital.justice.gov.uk", "name": "etl-manager", "package_url": "https://pypi.org/project/etl-manager/", "platform": "", "project_url": "https://pypi.org/project/etl-manager/", "project_urls": null, "release_url": "https://pypi.org/project/etl-manager/7.0.3/", "requires_dist": ["boto3 (>=1.9.205)", "jsonschema (>=3.0.0)", "parameterized (>=0.7.0,<0.8.0)", "regex (>=2019.6.0,<2019.7.0)"], "requires_python": ">=3.6,<4.0", "summary": "A python package to manage etl processes on AWS", "version": "7.0.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>etl_manager</h1>\n<p><a href=\"https://github.com/moj-analytical-services/etl_manager/actions\" rel=\"nofollow\"><img alt=\"Actions Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c6d02c7e120f6d15b4cf2b9dfb55765ed4f38fe4/68747470733a2f2f6769746875622e636f6d2f6d6f6a2d616e616c79746963616c2d73657276696365732f65746c5f6d616e616765722f776f726b666c6f77732f45544c2532304d616e616765722f62616467652e737667\"></a></p>\n<p>A python package that manages our data engineering framework and implements them on AWS Glue.</p>\n<p>The main functionality of this package is to interact with AWS Glue to create meta data catalogues and run Glue jobs.</p>\n<p>To install:</p>\n<pre>pip install git+git://github.com/moj-analytical-services/etl_manager.git#egg<span class=\"o\">=</span>etl_manager\n</pre>\n<p>If you do not have it installed already you will also need to install boto3.</p>\n<h2>Meta Data</h2>\n<p>Let's say I have a single table (a csv file) and I want to query it using Amazon athena. My csv file is in the following S3 path: <code>s3://my-bucket/my-table-folder/file.csv</code>.</p>\n<p>file.csv is a table that looks like this:</p>\n<table>\n<thead>\n<tr>\n<th>col1</th>\n<th>col2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>1</td>\n</tr>\n<tr>\n<td>b</td>\n<td>12</td>\n</tr>\n<tr>\n<td>c</td>\n<td>42</td>\n</tr></tbody></table>\n<p>As you can see col1 is a string and col2 is a integer.</p>\n<blockquote>\n<p><strong>Notes:</strong></p>\n<ul>\n<li>for Athena to work your table should not contain a header. So before file.csv is uploaded to S3 you should make sure it has no header.</li>\n<li>Tables must be in a folder. I.e. the location of your table (<code>table.location</code>) should be the parent folder of where you data exists. See example below.</li>\n</ul>\n</blockquote>\n<p>To create a schema for your data to be queried by Athena you can use the following code:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">etl_manager.meta</span> <span class=\"kn\">import</span> <span class=\"n\">DatabaseMeta</span><span class=\"p\">,</span> <span class=\"n\">TableMeta</span>\n\n<span class=\"c1\"># Create database meta object</span>\n<span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">DatabaseMeta</span><span class=\"p\">(</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s1\">'my_database'</span><span class=\"p\">,</span> <span class=\"n\">bucket</span><span class=\"o\">=</span><span class=\"s1\">'my-bucket'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create table meta object</span>\n<span class=\"n\">tab</span> <span class=\"o\">=</span> <span class=\"n\">TableMeta</span><span class=\"p\">(</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s1\">'my_table'</span><span class=\"p\">,</span> <span class=\"n\">location</span> <span class=\"o\">=</span> <span class=\"s1\">'my-table-folder'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Add column defintions to the table</span>\n<span class=\"n\">tab</span><span class=\"o\">.</span><span class=\"n\">add_column</span><span class=\"p\">(</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s1\">'col1'</span><span class=\"p\">,</span> <span class=\"s1\">'character'</span><span class=\"p\">,</span> <span class=\"n\">description</span> <span class=\"o\">=</span> <span class=\"s1\">'column contains a letter'</span><span class=\"p\">)</span>\n<span class=\"n\">tab</span><span class=\"o\">.</span><span class=\"n\">add_column</span><span class=\"p\">(</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s1\">'col2'</span><span class=\"p\">,</span> <span class=\"s1\">'int'</span><span class=\"p\">,</span> <span class=\"n\">description</span> <span class=\"o\">=</span> <span class=\"s1\">'column contains a number'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Add table to the database</span>\n<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">add_table</span><span class=\"p\">(</span><span class=\"n\">tab</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create the table on AWS glue</span>\n<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">create_glue_database</span><span class=\"p\">()</span>\n</pre>\n<p>Now the table can be queried via SQL e.g. <code>SELECT * FROM my_database.my_table</code></p>\n<h3>Meta data structure</h3>\n<p>Currently at very simple level. Assume you have the following folder structure from example code:</p>\n<pre><code>meta_data/\n--- database.json\n--- teams.json\n--- employees.json\n</code></pre>\n<p>database.json is a special json file that holds the meta data for the database. In our example it looks like this:</p>\n<pre><span class=\"p\">{</span>\n    <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Example database\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"workforce\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"bucket\"</span><span class=\"p\">:</span> <span class=\"s2\">\"my-bucket\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"base_folder\"</span><span class=\"p\">:</span> <span class=\"s2\">\"database/database1\"</span>\n<span class=\"p\">}</span>\n</pre>\n<p>When you create your database it will have this <code>name</code> and <code>description</code>. The <code>bucket</code> key specifies where the database exists (and therefore where the tables exist) in S3. The <code>base_folder</code> is is the initial path to where the tables exist. If your tables are in folders directly in the bucket (e.g. <code>s3://my-bucket/table1/</code>) then you can leave base_folder as an empty string (<code>\"\"</code>).</p>\n<p>The employees table has an ID for each employee their name and dob. The table meta looks like this:</p>\n<pre><span class=\"p\">{</span>\n    <span class=\"nt\">\"$schema\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"https://moj-analytical-services.github.io/metadata_schema/table/v1.0.0.json\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"employees\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"table containing employee information\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"data_format\"</span><span class=\"p\">:</span> <span class=\"s2\">\"parquet\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"location\"</span><span class=\"p\">:</span> <span class=\"s2\">\"employees/\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"columns\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"employee_id\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"int\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"an ID for each employee\"</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"employee_name\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"character\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"name of the employee\"</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"employee_dob\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"date\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"date of birth for the employee\"</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</pre>\n<p><strong>Currently supported data types for your columns currently are:</strong></p>\n<p><code>character | int | long | float | double | date | datetime | boolean</code></p>\n<p>This is a standard layout for a table metadata json. <code>$schema</code> points to another json that validates the structure of out table metadata files. Your table will have this <code>name</code> and <code>description</code> <em>(Note: It is strongly suggested that the name of your table matches the name of the metadata json file)</em> when the database is created. The <code>location</code> is the relative folder path to where your table exists. This path is relative to your database <code>base_folder</code>. This means that the full path your table is <code>s3://&lt;database.bucket&gt;/&lt;database.base_folder&gt;/&lt;table.folder&gt;/</code>. So in this example the table employees should be in the s3 path <code>s3://my-bucket/database/database1/employees</code>. The <code>data_format</code> specifies what type of data the table is. Finally your columns is an array of objects. Where each object is a column definition specifying the <code>name</code>, <code>description</code> and <code>type</code> (data type) of the column. Each column can have optional arguments <code>pattern</code>, <code>enum</code> and <code>nullable</code> (see <a href=\"https://moj-analytical-services.github.io/metadata_schema/table/v1.0.0.json\" rel=\"nofollow\">table_schema.json</a> for definition).</p>\n<p><strong>Note:</strong> that the order of the columns listed here should be the order of the columns in the table <em>(remember that data for a table should not have a header so the data will be queried wrong if the column order does not match up with the actual data)</em>.</p>\n<p>Here is another table in the database called teams. The teams table is a list of employee IDs for each team. Showing which employees are in each team. This table is taken each month (so you can see which employee was in which team each month). Therefore this table is partitioned by each monthly snapshot.</p>\n<pre><span class=\"p\">{</span>\n    <span class=\"nt\">\"$schema\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"https://moj-analytical-services.github.io/metadata_schema/table/v1.0.0.json\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"teams\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"month snapshot of which employee with working in what team\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"data_format\"</span><span class=\"p\">:</span> <span class=\"s2\">\"parquet\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"location\"</span><span class=\"p\">:</span> <span class=\"s2\">\"teams/\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"columns\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"team_id\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"int\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"ID given to each team\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"nullable\"</span> <span class=\"p\">:</span> <span class=\"kc\">false</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"team_name\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"character\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"name of the team\"</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"employee_id\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"int\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"primary key for each employee in the employees table\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"pattern\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"\\\\d+\"</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"snapshot_year\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"int\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"year at which snapshot of workforce was taken\"</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"snapshot_month\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"type\"</span><span class=\"p\">:</span> <span class=\"s2\">\"int\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"description\"</span><span class=\"p\">:</span> <span class=\"s2\">\"month at which snapshot of workforce was taken\"</span><span class=\"p\">,</span>\n            <span class=\"nt\">\"enum\"</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"mi\">6</span><span class=\"p\">,</span><span class=\"mi\">7</span><span class=\"p\">,</span><span class=\"mi\">8</span><span class=\"p\">,</span><span class=\"mi\">9</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">11</span><span class=\"p\">,</span><span class=\"mi\">12</span><span class=\"p\">]</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">],</span>\n    <span class=\"nt\">\"partitions\"</span> <span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s2\">\"snapshot_year\"</span><span class=\"p\">,</span> <span class=\"s2\">\"snapshot_month\"</span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</pre>\n<p>From the above you can see this has additional properties <code>enum</code>, <code>pattern</code>, <code>nullable</code> and a <code>partitions</code> property:</p>\n<ul>\n<li><strong>enum:</strong> What values the column can take (does not have to include nulls - should use nullable property)</li>\n<li><strong>pattern:</strong> Values in this column should match this regex string in the pattern property</li>\n<li><strong>nullable:</strong> Specifies if this column should accept <code>NULL</code> values.</li>\n<li><strong>partitions:</strong> Specifies if any of the columns in the table are file partitions rather than columns in the data. <code>etl_manager</code> will force your meta data json files to have columns that are partitions at the end of your data's list of columns.</li>\n</ul>\n<blockquote>\n<p><strong>Note:</strong> etl_manager does not enforce information provided by <code>enum</code>, <code>pattern</code> and <code>nullable</code>. It is just there to provide information to other tools or functions that could use this information to validate your data. Also the information in <code>pattern</code>, <code>enum</code> and <code>nullable</code> can conflict etl_manager does not check for conflicts. For example a column with an enum of <code>[0,1]</code> and a pattern of <code>[A-Za-z]</code> is allowed.</p>\n</blockquote>\n<h3>Examples using the DatabaseMeta Class</h3>\n<p>The easiest way to create a database is to run the code below. It reads a database schema based on the json files in a folder and creates this database meta in the glue catalogue. Allowing you to query the data using SQL using Athena.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">etl_manager.meta</span> <span class=\"kn\">import</span> <span class=\"n\">read_database_folder</span>\n<span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">read_database_folder</span><span class=\"p\">(</span><span class=\"s1\">'example_meta_data/'</span><span class=\"p\">)</span>\n<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">create_glue_database</span><span class=\"p\">()</span>\n</pre>\n<p>The code snippet below creates a database meta object that allows you to manipulate the database and the tables that exist in it</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">etl_manager.meta</span> <span class=\"kn\">import</span> <span class=\"n\">read_database_folder</span>\n\n<span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">read_database_folder</span><span class=\"p\">(</span><span class=\"s1\">'example_meta_data/'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Database has callable objects</span>\n\n<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">name</span> <span class=\"c1\"># workforce</span>\n\n<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">table_names</span> <span class=\"c1\"># [employees, teams]</span>\n\n<span class=\"c1\"># Each table in the database is an object from the TableMeta Class which can be callable from the database meta object</span>\n\n<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">table</span><span class=\"p\">(</span><span class=\"s1\">'employees'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">columns</span> <span class=\"c1\"># returns all columns in employees table</span>\n\n<span class=\"c1\"># The db and table object properties can also be altered and updated</span>\n\n<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s1\">'new_db_name'</span>\n<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">name</span> <span class=\"c1\"># 'new_db_name</span>\n\n<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">table</span><span class=\"p\">(</span><span class=\"s1\">'employees'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s1\">'new_name'</span>\n\n<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">table_names</span> <span class=\"c1\"># [new_name, teams]</span>\n\n<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">remove_table</span><span class=\"p\">(</span><span class=\"s1\">'new_name'</span><span class=\"p\">)</span>\n\n<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">name</span> <span class=\"c1\"># workforce_dev (note as default the package adds _dev if a db_suffix is not provided in DatabaseMeta)</span>\n\n<span class=\"c1\"># Set all table types to parquet and create database schema in glue</span>\n<span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">db_table_names</span> <span class=\"p\">:</span>\n    <span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">table</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">data_format</span> <span class=\"o\">=</span> <span class=\"s1\">'parquet'</span>\n<span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">create_glue_database</span><span class=\"p\">()</span>\n</pre>\n<h2>Using the GlueJob Class</h2>\n<p>The GlueJob class can be used to run pyspark jobs on AWS Glue. It is worth keeping up to date with AWS release notes and general guidance on running Glue jobs. This class is a wrapper function to simplify running glue jobs by using a structured format.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">etl_manager.etl</span> <span class=\"kn\">import</span> <span class=\"n\">GlueJob</span>\n\n<span class=\"n\">my_role</span> <span class=\"o\">=</span> <span class=\"s1\">'aws_role'</span>\n<span class=\"n\">bucket</span> <span class=\"o\">=</span> <span class=\"s1\">'bucket-to-store-temp-glue-job-in'</span>\n\n<span class=\"n\">job</span> <span class=\"o\">=</span> <span class=\"n\">GlueJob</span><span class=\"p\">(</span><span class=\"s1\">'glue_jobs/simple_etl_job/'</span><span class=\"p\">,</span> <span class=\"n\">bucket</span><span class=\"o\">=</span><span class=\"n\">bucket</span><span class=\"p\">,</span> <span class=\"n\">job_role</span><span class=\"o\">=</span><span class=\"n\">my_role</span><span class=\"p\">,</span> <span class=\"n\">job_arguments</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"--test_arg\"</span> <span class=\"p\">:</span> <span class=\"s1\">'some_string'</span><span class=\"p\">})</span>\n<span class=\"n\">job</span><span class=\"o\">.</span><span class=\"n\">run_job</span><span class=\"p\">()</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">job</span><span class=\"o\">.</span><span class=\"n\">job_status</span><span class=\"p\">)</span>\n</pre>\n<h3>Glue Job Folder Structure</h3>\n<p>Glue jobs have the prescribed folder format as follows:</p>\n<pre><code>\u251c\u2500\u2500 glue_jobs/\n|   |\n\u2502   \u251c\u2500\u2500 job1/\n|   |   \u251c\u2500\u2500 job.py\n|   |   \u251c\u2500\u2500 glue_resources/\n|   |   |   \u2514\u2500\u2500 my_lookup_table.csv\n|   |   \u2514\u2500\u2500 glue_py_resources/\n|   |       \u251c\u2500\u2500 my_python_functions.zip\n|   |       \u2514\u2500\u2500 github_zip_urls.txt\n|   |   \u2514\u2500\u2500 glue_jars/\n|   |       \u2514\u2500\u2500 my_jar.jar\n|   |\n\u2502   \u251c\u2500\u2500 job2/\n\u2502   |   \u251c\u2500\u2500 job.py\n\u2502   |   \u251c\u2500\u2500 glue_resources/\n\u2502   |   \u2514\u2500\u2500 glue_py_resources/\n|   |\n|   \u251c\u2500\u2500 shared_job_resources/\n\u2502   |   \u251c\u2500\u2500 glue_resources/\n|   |   |   \u2514\u2500\u2500 meta_data_dictionary.json\n\u2502   |   \u2514\u2500\u2500 glue_py_resources/\n|   |   \u2514\u2500\u2500 glue_jars/\n|   |       \u2514\u2500\u2500 my_other_jar.jar\n</code></pre>\n<p>Every glue job folder must have a <code>job.py</code> script in that folder. That is the only required file everything else is optional. When you want to create a glue job object you point the GlueJob class to the parent folder of the <code>job.py</code> script you want to run. There are two additional folders you can add to this parent folder :</p>\n<h4>glue_resources folder</h4>\n<p>Any files in this folder are uploaded to the working directory of the glue job. This means in your <code>job.py</code> script you can get the path to these files by:</p>\n<pre><span class=\"n\">path_to_file</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">getcwd</span><span class=\"p\">(),</span> <span class=\"s1\">'file_in_folder.txt'</span><span class=\"p\">)</span>\n</pre>\n<p>The GlueJob class will only upload files with extensions (.csv, .sql, .json, .txt) to S3 for the glue job to access.</p>\n<h4>glue_py_resources</h4>\n<p>These are python scripts you can import in your <code>job.py</code> script. e.g. if I had a <code>utils.py</code> script in my glue_py_resources folder. I could import that script normally e.g.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">utils</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n</pre>\n<p>You can also supply zip file which is a group of python functions in the standard python package structure. You can then reference this package as you would normally in python. For example if I had a package zipped as <code>my_package.zip</code> in the glue_py_resources folder then you could access that package normally in your job script like:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">my_package.utils</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>\n</pre>\n<p>You can also supply a text file with the special name <code>github_zip_urls.txt</code>. This is a text file where each line is a path to a github zip ball. The GlueJob class will download the github package rezip it and send it to S3. This github python package can then be accessed in the same way you would the local zip packages. For example if the <code>github_zip_urls.txt</code> file had a single line <code>https://github.com/moj-analytical-services/gluejobutils/archive/master.zip</code>. The package <code>gluejobutils</code> would be accessible in the <code>job.py</code> script:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">gluejobutils.s3</span> <span class=\"kn\">import</span> <span class=\"n\">read_json_from_s3</span>\n</pre>\n<h4>shared_job_resources folder</h4>\n<p>This a specific folder (must have the name <code>shared_job_resources</code>). This folder has the same structure and restrictions as a normal glue job folder but does not have a <code>job.py</code> file. Instead anything in the <code>glue_resources</code> or <code>glue_py_resources</code> folders will also be used (and therefore uploaded to S3) by any other glue job. Take the example below:</p>\n<pre><code>\u251c\u2500\u2500 glue_jobs/\n\u2502   \u251c\u2500\u2500 job1/\n\u2502   |   \u251c\u2500\u2500 job.py\n\u2502   |   \u251c\u2500\u2500 glue_resources/\n|   |   |   \u2514\u2500\u2500 lookup_table.csv\n\u2502   |   \u2514\u2500\u2500 glue_py_resources/\n|   |       \u2514\u2500\u2500 job1_specific_functions.py\n|   |\n|   \u251c\u2500\u2500 shared_job_resources/\n\u2502   |   \u251c\u2500\u2500 glue_resources/\n|   |   |   \u2514\u2500\u2500 some_global_config.json\n\u2502   |   \u2514\u2500\u2500 glue_py_resources/\n|   |       \u2514\u2500\u2500 utils.py\n</code></pre>\n<p>Running the glue job <code>job1</code> i.e.</p>\n<pre><span class=\"n\">job</span> <span class=\"o\">=</span> <span class=\"n\">GlueJob</span><span class=\"p\">(</span><span class=\"s1\">'glue_jobs/job1/'</span><span class=\"p\">,</span> <span class=\"n\">bucket</span><span class=\"p\">,</span> <span class=\"n\">job_role</span><span class=\"p\">)</span>\n<span class=\"n\">job</span><span class=\"o\">.</span><span class=\"n\">run_job</span><span class=\"p\">()</span>\n</pre>\n<p>This glue job would not only have access the the python script <code>job1_specific_functions.py</code> and file <code>lookup_table.csv</code> but also have access to the python script <code>utils.py</code> and file <code>some_global_config.json</code>. This is because the latter two files are in the <code>shared_job_resources</code> folder and accessible to all job folders (in their <code>glue_jobs</code> parent folder).</p>\n<p><strong>Note:</strong> Users should make sure there is no naming conflicts between filenames that are uploaded to S3 as they are sent to the same working folder.</p>\n<h3>Using the Glue Job class</h3>\n<p>Returning to the initial example:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">etl_manager.etl</span> <span class=\"kn\">import</span> <span class=\"n\">GlueJob</span>\n\n<span class=\"n\">my_role</span> <span class=\"o\">=</span> <span class=\"s1\">'aws_role'</span>\n<span class=\"n\">bucket</span> <span class=\"o\">=</span> <span class=\"s1\">'bucket-to-store-temp-glue-job-in'</span>\n\n<span class=\"n\">job</span> <span class=\"o\">=</span> <span class=\"n\">GlueJob</span><span class=\"p\">(</span><span class=\"s1\">'glue_jobs/simple_etl_job/'</span><span class=\"p\">,</span> <span class=\"n\">bucket</span><span class=\"o\">=</span><span class=\"n\">bucket</span><span class=\"p\">,</span> <span class=\"n\">job_role</span><span class=\"o\">=</span><span class=\"n\">my_role</span><span class=\"p\">)</span>\n</pre>\n<p>Allows you to create a job object. The GlueJob class will have a <code>job_name</code> which is defaulted to the folder name you pointed it to i.e. in this instance the job is called <code>simple_etl_job</code>. To change the job name:</p>\n<pre><span class=\"n\">job</span><span class=\"o\">.</span><span class=\"n\">job_name</span> <span class=\"o\">=</span> <span class=\"s1\">'new_job_name'</span>\n</pre>\n<p>In AWS you can only have unique job names.</p>\n<p>Other useful function and properties:</p>\n<pre><span class=\"c1\"># Increase the number of workers on a glue job (default is 2)</span>\n<span class=\"n\">job</span><span class=\"o\">.</span><span class=\"n\">allocated_capacity</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n\n<span class=\"c1\"># Set job arguments these are input params that can be accessed by the job.py script</span>\n<span class=\"n\">job</span><span class=\"o\">.</span><span class=\"n\">job_arguments</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"--test_arg\"</span> <span class=\"p\">:</span> <span class=\"s1\">'some_string'</span><span class=\"p\">,</span> <span class=\"s2\">\"--enable-metrics\"</span> <span class=\"p\">:</span> <span class=\"s2\">\"\"</span><span class=\"p\">}</span>\n</pre>\n<p>####\u00a0job_arguments</p>\n<p>These are strings that can be passed to the glue job script. Below is an example of how these are accessed in the <code>job.py</code> script. This code snippit is taken from the <code>simple_etl_job</code> found in the <code>example</code> folder of this repo.</p>\n<pre><span class=\"c1\"># Example job tests access to all files passed to the job runner class</span>\n<span class=\"kn\">import</span> <span class=\"nn\">sys</span>\n<span class=\"kn\">import</span> <span class=\"nn\">os</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">awsglue.utils</span> <span class=\"kn\">import</span> <span class=\"n\">getResolvedOptions</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pyspark.context</span> <span class=\"kn\">import</span> <span class=\"n\">SparkContext</span>\n<span class=\"kn\">from</span> <span class=\"nn\">awsglue.context</span> <span class=\"kn\">import</span> <span class=\"n\">GlueContext</span>\n<span class=\"kn\">from</span> <span class=\"nn\">awsglue.job</span> <span class=\"kn\">import</span> <span class=\"n\">Job</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">gluejobutils.s3</span> <span class=\"kn\">import</span> <span class=\"n\">read_json_from_s3</span>\n\n<span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"n\">getResolvedOptions</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">argv</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s1\">'JOB_NAME'</span><span class=\"p\">,</span> <span class=\"s1\">'metadata_base_path'</span><span class=\"p\">,</span> <span class=\"s1\">'test_arg'</span><span class=\"p\">])</span>\n\n<span class=\"nb\">print</span> <span class=\"s2\">\"JOB SPECS...\"</span>\n<span class=\"nb\">print</span> <span class=\"s2\">\"JOB_NAME: \"</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">[</span><span class=\"s2\">\"JOB_NAME\"</span><span class=\"p\">]</span>\n<span class=\"nb\">print</span> <span class=\"s2\">\"test argument: \"</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">[</span><span class=\"s2\">\"test_arg\"</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># Read in meta data json</span>\n<span class=\"n\">meta_employees</span> <span class=\"o\">=</span> <span class=\"n\">read_json_from_s3</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">[</span><span class=\"s1\">'metadata_base_path'</span><span class=\"p\">],</span> <span class=\"s2\">\"employees.json\"</span><span class=\"p\">))</span>\n\n<span class=\"c1\">### etc</span>\n</pre>\n<blockquote>\n<p><strong>Notes:</strong></p>\n<ul>\n<li>The <code>test_arg</code> does not have two dashes in front of it. When specifying job_arguments with the GlueJob class it must be suffixed with <code>--</code> but you should remove these when accessing the args in the <code>job.py</code> script.</li>\n<li><code>metadata_base_path</code> is a special parameter that is set by the GlueJob class. It is the S3 path to where the <code>meta_data</code> folder is in S3 so that you can read in your agnostic metadata files if you want to use them in your glue job. Note that the <a href=\"https://github.com/moj-analytical-services/gluejobutils\" rel=\"nofollow\">gluejobutils</a> package has a lot of functionality with integrating our metadata jsons with spark.</li>\n<li>The GlueJob argument <code>--enable-metrics</code> is also a special parameter that enables you to see metrics of your glue job. <a href=\"https://docs.aws.amazon.com/en_us/glue/latest/dg/monitor-profile-glue-job-cloudwatch-metrics.html\" rel=\"nofollow\">See here for more details on enabling metrics</a>.</li>\n<li>Note that <code>JOB_NAME</code> is a special parameter that is not set in GlueJob but automatically passed to the AWS Glue when running <code>job.py</code>. <a href=\"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-arguments.html\" rel=\"nofollow\">See here for more on special parameters</a>.</li>\n</ul>\n</blockquote>\n<p>Example of full <code>glue_job</code> and <code>meta_data</code> structures and code can be found <a href=\"https://github.com/moj-analytical-services/etl_manager/tree/master/example\" rel=\"nofollow\">here</a>.</p>\n<h1>Unit Tests</h1>\n<p>This package has <a href=\"https://github.com/moj-analytical-services/etl_manager/blob/master/tests/test_tests.py\" rel=\"nofollow\">unit tests</a> which can also be used to see functionality.</p>\n<p>Unit tests can be ran by:</p>\n<pre><span class=\"n\">python</span> <span class=\"o\">-</span><span class=\"n\">m</span> <span class=\"n\">unittest</span> <span class=\"n\">tests</span><span class=\"o\">.</span><span class=\"n\">test_tests</span> <span class=\"o\">-</span><span class=\"n\">v</span>\n</pre>\n\n          </div>"}, "last_serial": 6452814, "releases": {"7.0.1": [{"comment_text": "", "digests": {"md5": "be3e952b3b447b0a82b01f53196ad135", "sha256": "258df6ee2ac7a2f8c8c0c11d1ce680d45c3a8551ba4de248c648f256170d2b69"}, "downloads": -1, "filename": "etl_manager-7.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "be3e952b3b447b0a82b01f53196ad135", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4.0", "size": 25886, "upload_time": "2019-10-29T14:07:17", "upload_time_iso_8601": "2019-10-29T14:07:17.447010Z", "url": "https://files.pythonhosted.org/packages/cd/c9/0670cee93ddde63493ee31ed2faab11b868206a96a1642fced2d56d12656/etl_manager-7.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fe25af6096fe2321d03be194f29ddb7d", "sha256": "744387e90462d3647745baa4949f2c19e34c992ed365d5a9221d4140e3e0c19d"}, "downloads": -1, "filename": "etl_manager-7.0.1.tar.gz", "has_sig": false, "md5_digest": "fe25af6096fe2321d03be194f29ddb7d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4.0", "size": 27824, "upload_time": "2019-10-29T14:07:19", "upload_time_iso_8601": "2019-10-29T14:07:19.665961Z", "url": "https://files.pythonhosted.org/packages/7b/04/87729a5c67207745db6c44c2dec5ea29aa35cecc85d321afb0e0476525a0/etl_manager-7.0.1.tar.gz", "yanked": false}], "7.0.2": [{"comment_text": "", "digests": {"md5": "d35d6cf6da12bf3f7d190161ade1b39d", "sha256": "ed1b1086a24309c30084db7e094773906aead1d996cb6c016d6f37b5e88bbafe"}, "downloads": -1, "filename": "etl_manager-7.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "d35d6cf6da12bf3f7d190161ade1b39d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4.0", "size": 26134, "upload_time": "2019-12-18T17:42:43", "upload_time_iso_8601": "2019-12-18T17:42:43.360864Z", "url": "https://files.pythonhosted.org/packages/58/c5/060d3bb6dacf58dca0cf1588fcdd255a87b1bbb687a01bfd0521ad8e5083/etl_manager-7.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "14360d5c4f4cf459c2d0194044547350", "sha256": "cf384614a5f71596bab7d9f82d6f8ecacd7f59bca67e63b2138a766ce755e126"}, "downloads": -1, "filename": "etl_manager-7.0.2.tar.gz", "has_sig": false, "md5_digest": "14360d5c4f4cf459c2d0194044547350", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4.0", "size": 28021, "upload_time": "2019-12-18T17:42:45", "upload_time_iso_8601": "2019-12-18T17:42:45.378253Z", "url": "https://files.pythonhosted.org/packages/83/c1/b072603f6dfc4fdd47c0e389d9dade82685fe44b24b78011816d2b975e71/etl_manager-7.0.2.tar.gz", "yanked": false}], "7.0.3": [{"comment_text": "", "digests": {"md5": "92467bc3fdd3737a71a42f9a7d09e440", "sha256": "34b1d3a78c615b9e8e40741033b7ff65d83d5a7baa03d8004be479e6a8113e79"}, "downloads": -1, "filename": "etl_manager-7.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "92467bc3fdd3737a71a42f9a7d09e440", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4.0", "size": 26251, "upload_time": "2020-01-14T16:50:30", "upload_time_iso_8601": "2020-01-14T16:50:30.132221Z", "url": "https://files.pythonhosted.org/packages/b0/d2/b16ed2d81d1e256255a9f37bcb5fc4cd1cbbf0859ff899348ad501414d6b/etl_manager-7.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "38ac936437efb773708b6e8978b0f417", "sha256": "9ca249e8a43f5dea924011564be31f3da11b9d498d209371631312f70d193096"}, "downloads": -1, "filename": "etl_manager-7.0.3.tar.gz", "has_sig": false, "md5_digest": "38ac936437efb773708b6e8978b0f417", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4.0", "size": 28306, "upload_time": "2020-01-14T16:50:32", "upload_time_iso_8601": "2020-01-14T16:50:32.544407Z", "url": "https://files.pythonhosted.org/packages/12/19/a333c002fffed67cbf0929a88df9b2d1095d99735ae3e3d2584da5686893/etl_manager-7.0.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "92467bc3fdd3737a71a42f9a7d09e440", "sha256": "34b1d3a78c615b9e8e40741033b7ff65d83d5a7baa03d8004be479e6a8113e79"}, "downloads": -1, "filename": "etl_manager-7.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "92467bc3fdd3737a71a42f9a7d09e440", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4.0", "size": 26251, "upload_time": "2020-01-14T16:50:30", "upload_time_iso_8601": "2020-01-14T16:50:30.132221Z", "url": "https://files.pythonhosted.org/packages/b0/d2/b16ed2d81d1e256255a9f37bcb5fc4cd1cbbf0859ff899348ad501414d6b/etl_manager-7.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "38ac936437efb773708b6e8978b0f417", "sha256": "9ca249e8a43f5dea924011564be31f3da11b9d498d209371631312f70d193096"}, "downloads": -1, "filename": "etl_manager-7.0.3.tar.gz", "has_sig": false, "md5_digest": "38ac936437efb773708b6e8978b0f417", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4.0", "size": 28306, "upload_time": "2020-01-14T16:50:32", "upload_time_iso_8601": "2020-01-14T16:50:32.544407Z", "url": "https://files.pythonhosted.org/packages/12/19/a333c002fffed67cbf0929a88df9b2d1095d99735ae3e3d2584da5686893/etl_manager-7.0.3.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:45:24 2020"}