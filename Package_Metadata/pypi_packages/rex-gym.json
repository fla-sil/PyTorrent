{"info": {"author": "Nicola Russo", "author_email": "dott.nicolarusso@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Framework :: Robot Framework :: Library", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# Rex: an open-source domestic robot\nThe goal of this project is to train an open-source 3D printed quadruped robot exploring \n`Reinforcement Learning` and `OpenAI Gym`. The aim is to let the robot learns domestic and generic tasks in the simulations and then \nsuccessfully transfer the knowledge  (`Control Policies`) on the real robot without any other manual tuning.\n\nThis project is mostly inspired by the incredible works done by Boston Dynamics.\n\n# Rex-gym: OpenAI Gym environments and tools\nThis repository contains different `OpenAI Gym Environments` used to train Rex, the Rex URDF model, \nthe learning agent and some scripts to start the training session and visualise the learned `Control Polices`.\n\n## Installation\nCreate a `Python 3.7` virtual environment, e.g. using `Anaconda`\n```\nconda create -n rex python=3.7 anaconda\nconda activate rex\n```\n\n### PyPI package\nInstall the public `rex-gym` package:\n```\npip install rex_gym\n```\n\n### Install from source\nAlternately, clone this repository and run from the root of the project:\n```\npip install .\n```\n\n# Run a pre-trained agent\nTo start a pre-trained agent:\n```\npython -m rex_gym.playground.policy_player --env ENV-NAME-HERE \n```\n\n| Environment |     Flag      |\n| ----------- | ------------- |\n| Run         | rex_galloping |\n| Walk        | rex_walk      |\n| Turn        | rex_turn      |\n| Stand up    | rex_standup   |\n\n# Run single training simulation\nTo start a training simulation test (`agents=1`, `render=True`):\n```\npython -m rex_gym.playground.training_player --config rex_reactive --logdir YOUR_LOG_DIR_PATH\n```\nWhere `YOUR_LOG_DIR_PATH` is the output path. \n\nSet the `Environment` with the `--config` flag:\n\n| Environment |     Flag      |\n| ----------- | ------------- |\n| Run         | rex_galloping |\n| Walk        | rex_walk      |\n| Turn        | rex_turn      |\n| Stand up    | rex_standup   |\n\n# Start a new batch training simulation\nTo start a new batch training session:\n```\npython -m rex_gym.agents.scripts.train --config rex_reactive --logdir YOUR_LOG_DIR_PATH \n```\nWhere `YOUR_LOG_DIR_PATH` is the output path. \n\nSet the `Environment` with the `--config` flag:\n\n| Environment |     Flag      |\n| ----------- | ------------- |\n| Run         | rex_galloping |\n| Walk        | rex_walk      |\n| Turn        | rex_turn      |\n| Stand up    | rex_standup   |\n\n## PPO Agent configuration\nYou may want to edit the PPO agent's default configuration, especially the number of parallel agents launched during \nthe simulation.  \n\nEdit the `num_agents` variable in the `agents/scripts/configs.py` script:\n\n```\ndef default():\n    \"\"\"Default configuration for PPO.\"\"\"\n    # General\n    ...\n    num_agents = 20\n```\nInstall rex_gym from source. This configuration will launch 20 agents (threads) in parallel to train your model.\n\n# Robot platform\nThe robot used for this experiment is the [Spotmicro](https://www.thingiverse.com/thing:3445283) made by [Deok-yeon Kim](https://www.thingiverse.com/KDY0523/about).\n\n[<img src=\"https://thingiverse-production-new.s3.amazonaws.com/assets/bf/af/74/db/83/complete_4.jpg\">](https://www.thingiverse.com/thing:3445283)\n\nI've printed the components using a Creality Ender3 3D printer, with PLA and TPU+.\n\nThe idea is to extend the robot adding components like a robotic arm on the top of the rack and a LiDAR sensor.\n\n## Simulation model\nRex is a 12 joints robot with 3 motors (`Shoulder`, `Leg` and `Foot`) for each leg. \nThe `poses signals` (see ```/model/rex.py```) set the 12 motor angles and allow Rex to stand up.\n\nThe robot model is imported in `pyBullet` using an [URDF file](rex_gym/util/pybullet_data/assets/urdf/rex.urdf). \n\n![rex bullet](rex_gym/util/images/rex.png)\n\n# Tasks\nThis is the list of tasks this experiment will cover:\n\n1. Basic controls\n    1. Run/Walk straight on - forward/backward\n    2. Turn left/right on the spot\n    3. Stand up/Sit down\n    4. Steer - Run/Walk\n    5. Side swipe\n2. Fall recovery\n3. Reach a specific point in a map\n5. Grab an object\n\n## Basic Controls: Run\nGoal: how to run straight on. \n### Gym Environment\nThere is a good number of papers on quadrupeds locomotion, some of them with sample code. Probably, the most complete collection \nof examples is the [Minitaur folder](https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/gym/pybullet_envs/minitaur) in the Bullet3 repository. \nFor this task, the ```Minitaur Reactive Environment``` explained in the paper [Sim-to-Real: Learning Agile Locomotion For Quadruped Robots](https://arxiv.org/pdf/1804.10332.pdf)\nis a great example.\n\n#### Galloping gait - from scratch\nIn this very first experiment, I let the system learn from scratch: giving the feedback component large output bounds `[\u22120.6,0.6]` radians.\nThe `leg model` (see ```galloping_env.py```) forces legs and foots movements (positive or negative direction, depending on the leg) influencing the learning \nscore and time. In this first version, the `leg model` holds the Shoulder motors in the start position (0 degrees).  \n\nAs in the Minitaur example, I'm using the Proximal Policy Optimization (PPO). \n\n![](rex_gym/util/images/run.gif)\n\nThe emerged galloping gait shows the chassis tilled up and some unusual positions/movements (especially starting from the initial pose) during the locomotion. The `leg model` needs improvements. \n\n#### Galloping gait - bounded feedback\nTo improve the gait, in this second simulation, I've worked on the `leg model`:\n\n![](rex_gym/util/images/leg_model_bounds.png) \n\nI set bounds for both `Leg` and `Foot` angles, keeping the `Shoulder` in the initial position.\n\n![](rex_gym/util/images/galloping.gif)\n\nThe emerged gait now looks more clear.\n\n#### Galloping gait - balanced feedback\nAnother test was made using a balanced feedback:\n\n![](rex_gym/util/images/leg_model_improved.png) \n\nThe Action Space dimension is equals to 4, the same angle is assigned to both the front legs and a different one to the rear ones.\nThe very same was done for the foot angles.\n\nThe simulation score is massively improved (about 10x) as the learning time while the emerged gait is very similar to the `bounded feedback` model. \nThe Tensorflow score with this model, after ~500k attempts, is the same after ~4M attempts using any other models.\n\n## Basic Controls: Walk\nGoal: how to walk straight on.\n### Gym Environment\nStarting from  [Minitaur Alternating Leg](https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/minitaur/envs/minitaur_alternating_legs_env.py)\nenvironment, I've used a sinusoidal signal as `leg_model` alternating the Rex legs during the locomotion. The feedback component has small \nbounds [-0.1,0.1] as in the original script. \n\n![](rex_gym/util/images/walk.gif)\n\n## Basic Controls: Turn left/right\nGoal: How to reach a certain orientation turning on the spot.\n### Gym Environment\nIn this environment the `leg_model` applies a 'steer-on-the-spot' gait, allowing Rex to moving towards a specific orientation. \nThe reward function takes the chassis position/orientation and compares it with a fixed target position/orientation. \nWhen this difference is less than 0.1 radiant, the `leg_model` is set to the stand up. In order to make the learning more robust, \nthe Rex starting orientation is randomly chosen (every 'Reset' step).\n\n![](rex_gym/util/images/turn.gif)\n\n## Basic Controls: Stand up\nGoal: Reach the base standing position starting from the rest position\n### Gym Environment\nThis environment introduces the `rest_postion`, ideally the position assumed when Rex is in stand-by. \nThe `leg_model` is the `stand_low` position, while the `signal` function applies a 'brake' forcing Rex to assume an halfway position \nbefore completing the movement.\n\n![](rex_gym/util/images/standup.gif)\n\n# Credits\n[Sim-to-Real: Learning Agile Locomotion For Quadruped Robots](https://arxiv.org/pdf/1804.10332.pdf) and all the related papers. Google Brain, Google X, Google DeepMind - Minitaur Ghost Robotics.\n\n[Deok-yeon Kim](https://www.thingiverse.com/KDY0523/about) creator of SpotMini.\n\nThe great work in rendering the robot platform done by the [SpotMicroAI](https://github.com/FlorianWilk/SpotMicroAI) community.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/nicrusso7/rex-gym/archive/master.zip", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/nicrusso7/rex-gym", "keywords": "openai,gym,robot,quadruped,pybullet,ai,reinforcement learning,machine learning,RL,ML,tensorflow,spotmicro,rex", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "rex-gym", "package_url": "https://pypi.org/project/rex-gym/", "platform": "", "project_url": "https://pypi.org/project/rex-gym/", "project_urls": {"Download": "https://github.com/nicrusso7/rex-gym/archive/master.zip", "Homepage": "https://github.com/nicrusso7/rex-gym"}, "release_url": "https://pypi.org/project/rex-gym/0.1.8/", "requires_dist": null, "requires_python": "", "summary": "", "version": "0.1.8", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Rex: an open-source domestic robot</h1>\n<p>The goal of this project is to train an open-source 3D printed quadruped robot exploring\n<code>Reinforcement Learning</code> and <code>OpenAI Gym</code>. The aim is to let the robot learns domestic and generic tasks in the simulations and then\nsuccessfully transfer the knowledge  (<code>Control Policies</code>) on the real robot without any other manual tuning.</p>\n<p>This project is mostly inspired by the incredible works done by Boston Dynamics.</p>\n<h1>Rex-gym: OpenAI Gym environments and tools</h1>\n<p>This repository contains different <code>OpenAI Gym Environments</code> used to train Rex, the Rex URDF model,\nthe learning agent and some scripts to start the training session and visualise the learned <code>Control Polices</code>.</p>\n<h2>Installation</h2>\n<p>Create a <code>Python 3.7</code> virtual environment, e.g. using <code>Anaconda</code></p>\n<pre><code>conda create -n rex python=3.7 anaconda\nconda activate rex\n</code></pre>\n<h3>PyPI package</h3>\n<p>Install the public <code>rex-gym</code> package:</p>\n<pre><code>pip install rex_gym\n</code></pre>\n<h3>Install from source</h3>\n<p>Alternately, clone this repository and run from the root of the project:</p>\n<pre><code>pip install .\n</code></pre>\n<h1>Run a pre-trained agent</h1>\n<p>To start a pre-trained agent:</p>\n<pre><code>python -m rex_gym.playground.policy_player --env ENV-NAME-HERE \n</code></pre>\n<table>\n<thead>\n<tr>\n<th>Environment</th>\n<th>Flag</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Run</td>\n<td>rex_galloping</td>\n</tr>\n<tr>\n<td>Walk</td>\n<td>rex_walk</td>\n</tr>\n<tr>\n<td>Turn</td>\n<td>rex_turn</td>\n</tr>\n<tr>\n<td>Stand up</td>\n<td>rex_standup</td>\n</tr></tbody></table>\n<h1>Run single training simulation</h1>\n<p>To start a training simulation test (<code>agents=1</code>, <code>render=True</code>):</p>\n<pre><code>python -m rex_gym.playground.training_player --config rex_reactive --logdir YOUR_LOG_DIR_PATH\n</code></pre>\n<p>Where <code>YOUR_LOG_DIR_PATH</code> is the output path.</p>\n<p>Set the <code>Environment</code> with the <code>--config</code> flag:</p>\n<table>\n<thead>\n<tr>\n<th>Environment</th>\n<th>Flag</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Run</td>\n<td>rex_galloping</td>\n</tr>\n<tr>\n<td>Walk</td>\n<td>rex_walk</td>\n</tr>\n<tr>\n<td>Turn</td>\n<td>rex_turn</td>\n</tr>\n<tr>\n<td>Stand up</td>\n<td>rex_standup</td>\n</tr></tbody></table>\n<h1>Start a new batch training simulation</h1>\n<p>To start a new batch training session:</p>\n<pre><code>python -m rex_gym.agents.scripts.train --config rex_reactive --logdir YOUR_LOG_DIR_PATH \n</code></pre>\n<p>Where <code>YOUR_LOG_DIR_PATH</code> is the output path.</p>\n<p>Set the <code>Environment</code> with the <code>--config</code> flag:</p>\n<table>\n<thead>\n<tr>\n<th>Environment</th>\n<th>Flag</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Run</td>\n<td>rex_galloping</td>\n</tr>\n<tr>\n<td>Walk</td>\n<td>rex_walk</td>\n</tr>\n<tr>\n<td>Turn</td>\n<td>rex_turn</td>\n</tr>\n<tr>\n<td>Stand up</td>\n<td>rex_standup</td>\n</tr></tbody></table>\n<h2>PPO Agent configuration</h2>\n<p>You may want to edit the PPO agent's default configuration, especially the number of parallel agents launched during\nthe simulation.</p>\n<p>Edit the <code>num_agents</code> variable in the <code>agents/scripts/configs.py</code> script:</p>\n<pre><code>def default():\n    \"\"\"Default configuration for PPO.\"\"\"\n    # General\n    ...\n    num_agents = 20\n</code></pre>\n<p>Install rex_gym from source. This configuration will launch 20 agents (threads) in parallel to train your model.</p>\n<h1>Robot platform</h1>\n<p>The robot used for this experiment is the <a href=\"https://www.thingiverse.com/thing:3445283\" rel=\"nofollow\">Spotmicro</a> made by <a href=\"https://www.thingiverse.com/KDY0523/about\" rel=\"nofollow\">Deok-yeon Kim</a>.</p>\n<p><a href=\"https://www.thingiverse.com/thing:3445283\" rel=\"nofollow\"><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ba61332510c537b7d60b8e8aaab863f100e811f4/68747470733a2f2f7468696e676976657273652d70726f64756374696f6e2d6e65772e73332e616d617a6f6e6177732e636f6d2f6173736574732f62662f61662f37342f64622f38332f636f6d706c6574655f342e6a7067\"></a></p>\n<p>I've printed the components using a Creality Ender3 3D printer, with PLA and TPU+.</p>\n<p>The idea is to extend the robot adding components like a robotic arm on the top of the rack and a LiDAR sensor.</p>\n<h2>Simulation model</h2>\n<p>Rex is a 12 joints robot with 3 motors (<code>Shoulder</code>, <code>Leg</code> and <code>Foot</code>) for each leg.\nThe <code>poses signals</code> (see <code>/model/rex.py</code>) set the 12 motor angles and allow Rex to stand up.</p>\n<p>The robot model is imported in <code>pyBullet</code> using an <a href=\"rex_gym/util/pybullet_data/assets/urdf/rex.urdf\" rel=\"nofollow\">URDF file</a>.</p>\n<p><img alt=\"rex bullet\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/db8df835cd59253f5a4d5f3a0890b2a9c0cbe4ed/7265785f67796d2f7574696c2f696d616765732f7265782e706e67\"></p>\n<h1>Tasks</h1>\n<p>This is the list of tasks this experiment will cover:</p>\n<ol>\n<li>Basic controls\n<ol>\n<li>Run/Walk straight on - forward/backward</li>\n<li>Turn left/right on the spot</li>\n<li>Stand up/Sit down</li>\n<li>Steer - Run/Walk</li>\n<li>Side swipe</li>\n</ol>\n</li>\n<li>Fall recovery</li>\n<li>Reach a specific point in a map</li>\n<li>Grab an object</li>\n</ol>\n<h2>Basic Controls: Run</h2>\n<p>Goal: how to run straight on.</p>\n<h3>Gym Environment</h3>\n<p>There is a good number of papers on quadrupeds locomotion, some of them with sample code. Probably, the most complete collection\nof examples is the <a href=\"https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/gym/pybullet_envs/minitaur\" rel=\"nofollow\">Minitaur folder</a> in the Bullet3 repository.\nFor this task, the <code>Minitaur Reactive Environment</code> explained in the paper <a href=\"https://arxiv.org/pdf/1804.10332.pdf\" rel=\"nofollow\">Sim-to-Real: Learning Agile Locomotion For Quadruped Robots</a>\nis a great example.</p>\n<h4>Galloping gait - from scratch</h4>\n<p>In this very first experiment, I let the system learn from scratch: giving the feedback component large output bounds <code>[\u22120.6,0.6]</code> radians.\nThe <code>leg model</code> (see <code>galloping_env.py</code>) forces legs and foots movements (positive or negative direction, depending on the leg) influencing the learning\nscore and time. In this first version, the <code>leg model</code> holds the Shoulder motors in the start position (0 degrees).</p>\n<p>As in the Minitaur example, I'm using the Proximal Policy Optimization (PPO).</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fb6a969ae81b1640bb990750fadfbd0b8ba598de/7265785f67796d2f7574696c2f696d616765732f72756e2e676966\"></p>\n<p>The emerged galloping gait shows the chassis tilled up and some unusual positions/movements (especially starting from the initial pose) during the locomotion. The <code>leg model</code> needs improvements.</p>\n<h4>Galloping gait - bounded feedback</h4>\n<p>To improve the gait, in this second simulation, I've worked on the <code>leg model</code>:</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b3bc72b8fc70c10b89399bbd8c32662e3694f7f9/7265785f67796d2f7574696c2f696d616765732f6c65675f6d6f64656c5f626f756e64732e706e67\"></p>\n<p>I set bounds for both <code>Leg</code> and <code>Foot</code> angles, keeping the <code>Shoulder</code> in the initial position.</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5553ff544c334c1ebebc1c87469c6a5b98ba789c/7265785f67796d2f7574696c2f696d616765732f67616c6c6f70696e672e676966\"></p>\n<p>The emerged gait now looks more clear.</p>\n<h4>Galloping gait - balanced feedback</h4>\n<p>Another test was made using a balanced feedback:</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/139017248e724eb91688dc0783ef522839feed8d/7265785f67796d2f7574696c2f696d616765732f6c65675f6d6f64656c5f696d70726f7665642e706e67\"></p>\n<p>The Action Space dimension is equals to 4, the same angle is assigned to both the front legs and a different one to the rear ones.\nThe very same was done for the foot angles.</p>\n<p>The simulation score is massively improved (about 10x) as the learning time while the emerged gait is very similar to the <code>bounded feedback</code> model.\nThe Tensorflow score with this model, after ~500k attempts, is the same after ~4M attempts using any other models.</p>\n<h2>Basic Controls: Walk</h2>\n<p>Goal: how to walk straight on.</p>\n<h3>Gym Environment</h3>\n<p>Starting from  <a href=\"https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/minitaur/envs/minitaur_alternating_legs_env.py\" rel=\"nofollow\">Minitaur Alternating Leg</a>\nenvironment, I've used a sinusoidal signal as <code>leg_model</code> alternating the Rex legs during the locomotion. The feedback component has small\nbounds [-0.1,0.1] as in the original script.</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9023ada811b37d4dad32d29c91e0b598343a9c2a/7265785f67796d2f7574696c2f696d616765732f77616c6b2e676966\"></p>\n<h2>Basic Controls: Turn left/right</h2>\n<p>Goal: How to reach a certain orientation turning on the spot.</p>\n<h3>Gym Environment</h3>\n<p>In this environment the <code>leg_model</code> applies a 'steer-on-the-spot' gait, allowing Rex to moving towards a specific orientation.\nThe reward function takes the chassis position/orientation and compares it with a fixed target position/orientation.\nWhen this difference is less than 0.1 radiant, the <code>leg_model</code> is set to the stand up. In order to make the learning more robust,\nthe Rex starting orientation is randomly chosen (every 'Reset' step).</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c81efbcbeff6457abb35329a75701495f90541d1/7265785f67796d2f7574696c2f696d616765732f7475726e2e676966\"></p>\n<h2>Basic Controls: Stand up</h2>\n<p>Goal: Reach the base standing position starting from the rest position</p>\n<h3>Gym Environment</h3>\n<p>This environment introduces the <code>rest_postion</code>, ideally the position assumed when Rex is in stand-by.\nThe <code>leg_model</code> is the <code>stand_low</code> position, while the <code>signal</code> function applies a 'brake' forcing Rex to assume an halfway position\nbefore completing the movement.</p>\n<p><img alt=\"\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/371200c80b7f0c149421d8445c18e0e41cd26b04/7265785f67796d2f7574696c2f696d616765732f7374616e6475702e676966\"></p>\n<h1>Credits</h1>\n<p><a href=\"https://arxiv.org/pdf/1804.10332.pdf\" rel=\"nofollow\">Sim-to-Real: Learning Agile Locomotion For Quadruped Robots</a> and all the related papers. Google Brain, Google X, Google DeepMind - Minitaur Ghost Robotics.</p>\n<p><a href=\"https://www.thingiverse.com/KDY0523/about\" rel=\"nofollow\">Deok-yeon Kim</a> creator of SpotMini.</p>\n<p>The great work in rendering the robot platform done by the <a href=\"https://github.com/FlorianWilk/SpotMicroAI\" rel=\"nofollow\">SpotMicroAI</a> community.</p>\n\n          </div>"}, "last_serial": 6521908, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "2694c8c4c7a1629d5777c08b7bef019a", "sha256": "9351aabcac3a968941c9ba77d6517ab8b0c4ded0d36f09462adbff167bf918a7"}, "downloads": -1, "filename": "rex_gym-0.1.tar.gz", "has_sig": false, "md5_digest": "2694c8c4c7a1629d5777c08b7bef019a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32689575, "upload_time": "2019-11-15T23:43:08", "upload_time_iso_8601": "2019-11-15T23:43:08.541517Z", "url": "https://files.pythonhosted.org/packages/ff/97/a6063af710acb817edb1c54c4f96de6860a3a10812c7ee7352c0086bfaa0/rex_gym-0.1.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "56abaa8b62d37638eb1b0dbbe622fa4a", "sha256": "be0916e1c9931d4c5ca034dbe16a683e73a592802e820cf8c132b15cff7ed753"}, "downloads": -1, "filename": "rex_gym-0.1.1.tar.gz", "has_sig": false, "md5_digest": "56abaa8b62d37638eb1b0dbbe622fa4a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32692295, "upload_time": "2019-11-15T23:57:53", "upload_time_iso_8601": "2019-11-15T23:57:53.375352Z", "url": "https://files.pythonhosted.org/packages/0e/74/4237d74e94d4328004293f650ffa60367679f7313b10788db98b9e4432da/rex_gym-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "3fa9c3ce50779cd81619454472e98a53", "sha256": "250d1d0470ceadc804edcc2e5143c683a1bd7ad12709b8b897e1b52b550ad089"}, "downloads": -1, "filename": "rex_gym-0.1.2.tar.gz", "has_sig": false, "md5_digest": "3fa9c3ce50779cd81619454472e98a53", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32692258, "upload_time": "2019-11-16T00:19:04", "upload_time_iso_8601": "2019-11-16T00:19:04.379006Z", "url": "https://files.pythonhosted.org/packages/d9/d9/10d7bf1fcc4897de9e6b90d51375136be70c2499122c8406130d3528a6e8/rex_gym-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "7077edf6a7be86a6d5b094ca873be0a1", "sha256": "6a0dd28ec83462ab6116901cf7d3ee681672a18e68e3da521d78df6771203408"}, "downloads": -1, "filename": "rex_gym-0.1.3.tar.gz", "has_sig": false, "md5_digest": "7077edf6a7be86a6d5b094ca873be0a1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32692257, "upload_time": "2019-11-16T00:28:36", "upload_time_iso_8601": "2019-11-16T00:28:36.099732Z", "url": "https://files.pythonhosted.org/packages/4f/27/f4521e6e04435993a73910dd0034f9708191054aacd5f4046013a8df5c1c/rex_gym-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "95c24b4a44e165a3a6430c619db760f6", "sha256": "48e92b58eb50640393b7ae7f69c7b13bfbb7be451e7f930cf5dc067a5c5db6e6"}, "downloads": -1, "filename": "rex_gym-0.1.4.tar.gz", "has_sig": false, "md5_digest": "95c24b4a44e165a3a6430c619db760f6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 41956159, "upload_time": "2019-11-16T00:58:50", "upload_time_iso_8601": "2019-11-16T00:58:50.366766Z", "url": "https://files.pythonhosted.org/packages/27/fa/26ece38e6fb3b308d943e512cd7cf86fe474a6aea0695dbe5b89830a365f/rex_gym-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "03312002ea1a9244d9ff80cc395c933a", "sha256": "778737772b5d1ed8d7d271171805c4d09e3c8a7c2b0c2f02b26a1843485eacc8"}, "downloads": -1, "filename": "rex_gym-0.1.5.tar.gz", "has_sig": false, "md5_digest": "03312002ea1a9244d9ff80cc395c933a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 41956189, "upload_time": "2019-11-16T01:15:51", "upload_time_iso_8601": "2019-11-16T01:15:51.891528Z", "url": "https://files.pythonhosted.org/packages/7d/ce/e702bb2e74f90a6b6099380b61c1f4bc387f31c20a786ba49e87e93a2c1b/rex_gym-0.1.5.tar.gz", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "79ac5784c27e98dd2d8e7851374343f1", "sha256": "c25434f283a251ef81d908db04b8d8b8d8ef01dddd8e661a480a3d95ef05ef0c"}, "downloads": -1, "filename": "rex_gym-0.1.6.tar.gz", "has_sig": false, "md5_digest": "79ac5784c27e98dd2d8e7851374343f1", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 25147201, "upload_time": "2019-12-07T16:26:33", "upload_time_iso_8601": "2019-12-07T16:26:33.641727Z", "url": "https://files.pythonhosted.org/packages/e7/df/489b953e27cb416fc36f67442624e4b0ee032fd2d2e6e28f59d4fc3d8ddc/rex_gym-0.1.6.tar.gz", "yanked": false}], "0.1.7": [{"comment_text": "", "digests": {"md5": "b969823f283b5ab421b7af3215467877", "sha256": "796c8f6b7225809c65a1abe4693ccff81dda183c3d507e194144fcbe428de73a"}, "downloads": -1, "filename": "rex_gym-0.1.7.tar.gz", "has_sig": false, "md5_digest": "b969823f283b5ab421b7af3215467877", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17245514, "upload_time": "2019-12-29T00:53:10", "upload_time_iso_8601": "2019-12-29T00:53:10.512660Z", "url": "https://files.pythonhosted.org/packages/98/1c/6d979037d598d3ddd410908eba0bbcf76b464ebd22143a8d9682c3a9af26/rex_gym-0.1.7.tar.gz", "yanked": false}], "0.1.8": [{"comment_text": "", "digests": {"md5": "eceb06f97087a1860f2d2fead84ad87b", "sha256": "4abf61c8c5407ea6bcee70328653a014a0764906dd43f27ba94d3c0d74a19f36"}, "downloads": -1, "filename": "rex_gym-0.1.8.tar.gz", "has_sig": false, "md5_digest": "eceb06f97087a1860f2d2fead84ad87b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23536093, "upload_time": "2020-01-26T13:35:57", "upload_time_iso_8601": "2020-01-26T13:35:57.325327Z", "url": "https://files.pythonhosted.org/packages/30/7d/21a5ea74ce640c73af4318e711a3058caa9117fd21ac85891f210af5a58d/rex_gym-0.1.8.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "eceb06f97087a1860f2d2fead84ad87b", "sha256": "4abf61c8c5407ea6bcee70328653a014a0764906dd43f27ba94d3c0d74a19f36"}, "downloads": -1, "filename": "rex_gym-0.1.8.tar.gz", "has_sig": false, "md5_digest": "eceb06f97087a1860f2d2fead84ad87b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23536093, "upload_time": "2020-01-26T13:35:57", "upload_time_iso_8601": "2020-01-26T13:35:57.325327Z", "url": "https://files.pythonhosted.org/packages/30/7d/21a5ea74ce640c73af4318e711a3058caa9117fd21ac85891f210af5a58d/rex_gym-0.1.8.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:03:13 2020"}