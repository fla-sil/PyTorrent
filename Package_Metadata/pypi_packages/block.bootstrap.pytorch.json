{"info": {"author": "Remi Cadene", "author_email": "remi.cadene@icloud.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.7", "Topic :: Software Development :: Build Tools"], "description": "# BLOCK: Bilinear Superdiagonal Fusion for VQA and VRD\n\n\n<a href=\"https://travis-ci.org/Cadene/block.bootstrap.pytorch\"><img src=\"https://travis-ci.org/Cadene/block.bootstrap.pytorch.svg?branch=master\"/></a>\n\n\nIn Machine Learning, an important question is \"How to fuse two modalities in a same space\".\nFor instance, in Visual Question Answering, one must fuse the image and the question embeddings in a same bi-modal space. This multimodal embedding is latter classified to provide the answer.\n\n<p align=\"center\">\n    <img src=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/assets/VQA_block.png?raw=true\" width=\"600\"/>\n</p>\n\nWe introduce a novel module (BLOCK) to fuse two representations together. First, we experimentaly demonstrate that it is better than any available fusion for our tasks. Secondly, we provide a theoritical-grounded analysis around the notion of tensor complexity. For further details, please see [our AAAI 2019 paper](http://remicadene.com/pdfs/paper_aaai2019.pdf) and [poster](http://remicadene.com/pdfs/poster_aaai2019.pdf).\n\nIn this repo, we make our BLOCK fusion available via pip install including several powerful fusions from the state-of-the-art (MLB, MUTAN, MCB, MFB, MFH, etc.). Also, we provide pretrained models and all the code needed to reproduce our experiments.\n\n\n#### Summary\n\n* [Installation](#installation)\n    * [Python 3 & Anaconda](#1-python-3--anaconda)\n    * [As a standalone project](#2-as-standalone-project)\n    * [Download datasets](#3-download-datasets)\n    * [As a python library](#2-as-a-python-library)\n* [Quick start](#quick-start)\n    * [Train a model](#train-a-model)\n    * [Evaluate a model](#evaluate-a-model)\n* [Reproduce results](#reproduce-results)\n    * [VRD](#vrd-dataset)\n    * [VQA2](#vqa2-dataset)\n    * [TDIUC](#tdiuc-dataset)\n* [Pretrained models](#pretrained-models)\n* [Fusions](#fusions)\n    * [Block](#block)\n    * [LinearSum](#linearsum)\n    * [ConcatMLP](#concatMLP)\n    * [MLB](#mlb)\n    * [Tucker](#tucker)\n    * [Mutan](#mutan)\n    * [BlockTucker](#blocktucker)\n    * [MFB](#mfb)\n    * [MFH](#mfh)\n    * [MCB](#mcb)\n* [Useful commands](#useful-commands)\n* [Citation](#citation)\n* [Poster](#poster)\n* [Authors](#authors)\n* [Acknowledgment](#acknowledgment)\n\n\n## Installation\n\n### 1. Python 3 & Anaconda\n\nWe don't provide support for python 2. We advise you to install python 3 with [Anaconda](https://www.continuum.io/downloads). Then, you can create an environment.\n\n### 2. As standalone project\n\n```\nconda create --name block python=3\nsource activate block\ngit clone --recursive https://github.com/Cadene/block.bootstrap.pytorch.git\ncd block.bootstrap.pytorch\npip install -r requirements.txt\n```\n\n### 3. Download datasets\n\nDownload annotations, images and features for VRD experiments:\n```\nbash block/datasets/scripts/download_vrd.sh\n```\n\nDownload annotations, images and features for VQA experiments:\n```\nbash block/datasets/scripts/download_vqa2.sh\nbash block/datasets/scripts/download_vgenome.sh\nbash block/datasets/scripts/download_tdiuc.sh\n```\n\n**Note:** The features have been extracted from a pretrained Faster-RCNN with caffe. We don't provide the code for pretraining or extracting features for now.\n\n### (2. As a python library)\n\nBy importing the `block` python module, you can access every fusions, datasets and models in a simple way:\n```python\nimport torch\nfrom block import fusions\nmm = fusions.Block([100,100], 300)\ninputs = [torch.randn(10,100), torch.randn(10,100)]\nout = mm(inputs) # torch.Size([10,300])\n# ...\nfusions.LinearSum\nfusions.ConcatMLP\nfusions.MLB\nfusions.Mutan\nfusions.Tucker\nfusions.BlockTucker\nfusions.MFB\nfusions.MFH\nfusions.MCB\n# ...\nfrom block.datasets.vqa2 import VQA2\nfrom block.datasets.tdiuc import TDIUC\nfrom block.datasets.vg import VG\nfrom block.datasets.vrd import VRD\n# ...\nfrom block.models.networks.vqa_net import VQANet\nfrom block.models.networks.vrd_net import VRDNet\n# ...\n```\n\nTo be able to do so, you can use pip:\n```\npip install block.bootstrap.pytorch\n```\n\nOr install from source:\n```\ngit clone https://github.com/Cadene/block.bootstrap.pytorch.git\npython setup.py install\n```\n\n\n## Quick start\n\n### Train a model\n\nThe [boostrap/run.py](https://github.com/Cadene/bootstrap.pytorch/blob/master/bootstrap/run.py) file load the options contained in a yaml file, create the corresponding experiment directory and start the training procedure. For instance, you can train our best model on VRD by running:\n```\npython -m bootstrap.run -o block/options/vrd/block.yaml\n```\nThen, several files are going to be created in `logs/vrd/block`:\n- [options.yaml](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/assets/logs/vrd/block/options.yaml) (copy of options)\n- [logs.txt](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/assets/logs/vrd/block/logs.txt) (history of print)\n- [logs.json](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/assets/logs/vrd/block/logs.json) (batchs and epochs statistics)\n- [view.html](http://htmlpreview.github.io/?https://raw.githubusercontent.com/Cadene/block.bootstrap.pytorch/master/assets/logs/vrd/block/view.html?token=AEdvLlDSYaSn3Hsr7gO5sDBxeyuKNQhEks5cTF6-wA%3D%3D) (learning curves)\n- ckpt_last_engine.pth.tar (checkpoints of last epoch)\n- ckpt_last_model.pth.tar\n- ckpt_last_optimizer.pth.tar\n- ckpt_best_eval_epoch.predicate.R_50_engine.pth.tar (checkpoints of best epoch)\n- ckpt_best_eval_epoch.predicate.R_50_model.pth.tar\n- ckpt_best_eval_epoch.predicate.R_50_optimizer.pth.tar\n\nMany options are available in the [options directory](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/options).\n\n### Evaluate a model\n\nAt the end of the training procedure, you can evaluate your model on the testing set. In this example, [boostrap/run.py](https://github.com/Cadene/bootstrap.pytorch/blob/master/bootstrap/run.py) load the options from your experiment directory, resume the best checkpoint on the validation set and start an evaluation on the testing set instead of the validation set while skipping the training set (train_split is empty). Thanks to `--misc.logs_name`, the logs will be written in the new `logs_predicate.txt` and `logs_predicate.json` files, instead of being appended to the `logs.txt` and `logs.json` files.\n```\npython -m bootstrap.run \\\n-o logs/vrd/block/options.yaml \\\n--exp.resume best_eval_epoch.predicate.R_50 \\\n--dataset.train_split \\\n--dataset.eval_split test \\\n--misc.logs_name predicate\n```\n\n## Reproduce results\n\n### VRD dataset\n\n#### Train and evaluate on VRD\n\n1. Train block on trainset with early stopping on valset\n2. Evaluate the best checkpoint on testset (Predicate Prediction)\n3. Evaluate the best checkpoint on testset (Relationship and Phrase Detection)\n\n```\npython -m bootstrap.run \\\n-o block/options/vrd/block.yaml \\\n--exp.dir logs/vrd/block\n\npython -m bootstrap.run \\\n-o logs/vrd/block/options.yaml \\\n--dataset.train_split \\\n--dataset.eval_split test \\\n--exp.resume best_eval_epoch.predicate.R_50 \\\n--misc.logs_name predicate\n\npython -m bootstrap.run \\\n-o logs/vrd/block/options.yaml \\\n--dataset.train_split \\\n--dataset.eval_split test \\\n--dataset.mode rel_phrase \\\n--model.metric.name vrd_rel_phrase \\\n--exp.resume best_eval_epoch.predicate.R_50 \\\n--misc.logs_name rel_phrase\n```\n\n**Note:** You can copy past the three commands at once in the terminal to run one after each other seamlessly.\n\n**Note:** Block is not the only option available. You can find several others [here](https://github.com/Cadene/block.bootstrap.pytorch/tree/master/block/options/vrd).\n\n**Note:** Learning curves can be viewed in the experiment directy (`logs/vrd/block/view.html`). An example is available [here](http://htmlpreview.github.io/?https://raw.githubusercontent.com/Cadene/block.bootstrap.pytorch/master/assets/logs/vrd/block/view.html?token=AEdvLlDSYaSn3Hsr7gO5sDBxeyuKNQhEks5cTF6-wA%3D%3D).\n\n**Note:** In our article, we report result for a negative sampling ratio of 0.5. Better results in *Predicate Prediction* can be achieve with a ratio of 0.0. Better results in *Phrase Detection* and *Relationship Detection* can be achieve with a ratio of 0.8. You can change the ratio by doing so:\n```\npython -m bootstrap.run \\\n-o block/options/vrd/block.yaml \\\n--exp.dir logs/vrd/block_ratio,0.0 \\\n--dataset.neg_ratio 0.0\n```\n\n#### Compare experiments on VRD\n\nFinally you can compare experiments on the valset or testset metrics:\n```\npython -m block.compare_vrd_val -d \\\nlogs/vrd/block \\\nlogs/vrd/block_tucker \\\nlogs/vrd/mutan \\\nlogs/vrd/mfh \\\nlogs/vrd/mlb\n\npython -m block.compare_vrd_test -d \\\nlogs/vrd/block \\\nlogs/vrd/block_tucker\n```\n\nExample:\n```\n## eval_epoch.predicate.R_50\n  Place  Method          Score    Epoch\n-------  ------------  -------  -------\n      1  block         86.3708       13\n      2  block_tucker  86.2529        9\n\n## eval_epoch.predicate.R_100\n  Place  Method          Score    Epoch\n-------  ------------  -------  -------\n      1  block         92.4588       13\n      2  block_tucker  91.5816        9\n\n## eval_epoch.phrase.R_50\n  Place  Method          Score    Epoch\n-------  ------------  -------  -------\n      1  block         25.4779       13\n      2  block_tucker  23.7759        9\n\n## eval_epoch.phrase.R_100\n  Place  Method          Score    Epoch\n-------  ------------  -------  -------\n      1  block         29.7198       13\n      2  block_tucker  27.9131        9\n\n## eval_epoch.rel.R_50\n  Place  Method          Score    Epoch\n-------  ------------  -------  -------\n      1  block         18.0806       13\n      2  block_tucker  17.0856        9\n\n## eval_epoch.rel.R_100\n  Place  Method          Score    Epoch\n-------  ------------  -------  -------\n      1  block         21.1181       13\n      2  block_tucker  19.7565        9\n```\n\n### VQA2 dataset\n\n#### Training and evaluation (train/val)\n\nWe use this simple setup to tune our hyperparameters on the valset.\n\n```\npython -m bootstrap.run \\\n-o block/options/vqa2/block.yaml \\\n--exp.dir logs/vqa2/block\n```\n\n#### Training and evaluation (train+val/val/test)\n\nThis heavier setup allows us to train a model on 95% of the concatenation of train and val sets, and to evaluate it on the 5% rest. Then we extract the predictions of our best checkpoint on the testset. Finally, we submit a json file on the EvalAI web site.\n\n```\npython -m bootstrap.run \\\n-o block/options/vqa2/block.yaml \\\n--exp.dir logs/vqa2/block_trainval \\\n--dataset.proc_split trainval\n\npython -m bootstrap.run \\\n-o logs/vqa2/block_trainval/options.yaml \\\n--exp.resume best_eval_epoch.accuracy_top1 \\\n--dataset.train_split \\\n--dataset.eval_split test \\\n--misc.logs_name test\n```\n\n#### Training and evaluation (train+val+vg/val/test)\n\nSame, but we add pairs from the VisualGenome dataset.\n\n```\npython -m bootstrap.run \\\n-o block/options/vqa2/block.yaml \\\n--exp.dir logs/vqa2/block_trainval_vg \\\n--dataset.proc_split trainval \\\n--dataset.vg True\n\npython -m bootstrap.run \\\n-o logs/vqa2/block_trainval_vg/options.yaml \\\n--exp.resume best_eval_epoch.accuracy_top1 \\\n--dataset.train_split \\\n--dataset.eval_split test \\\n--misc.logs_name test\n```\n\n#### Compare experiments on valset\n\nYou can compare experiments by displaying their best metrics on the valset.\n\n```\npython -m block.compare_vqa_val -d logs/vqa2/block logs/vqa2/mutan\n```\n\n#### Submit predictions on EvalAI\n\nIt is not possible to automaticaly compute the accuracies on the testset. You need to submit a json file on the [EvalAI platform](http://evalai.cloudcv.org/web/challenges/challenge-page/80/my-submission). The evaluation step on the testset creates the json file that contains the prediction of your model on the full testset. For instance: `logs/vqa2/block_trainval_vg/results/test/epoch,19/OpenEnded_mscoco_test2015_model_results.json`. To get the accuracies on testdev or test sets, you must submit this file.\n\n\n### TDIUC dataset\n\n#### Training and evaluation (train/val/test)\n\nThe full training set is split into a trainset and a valset. At the end of the training, we evaluate our best checkpoint on the testset. The TDIUC metrics are computed and displayed at the end of each epoch. They are also stored in `logs.json` and `logs_test.json`.\n\n```\npython -m bootstrap.run \\\n-o block/options/tdiuc/block.yaml \\\n--exp.dir logs/tdiuc/block\n\npython -m bootstrap.run \\\n-o logs/tdiuc/block/options.yaml \\\n--exp.resume best_eval_epoch.accuracy_top1 \\\n--dataset.train_split \\\n--dataset.eval_split test \\\n--misc.logs_name test\n```\n\n#### Compare experiments\n\nYou can compare experiments by displaying their best metrics on the valset or testset.\n\n```\npython -m block.compare_tdiuc_val -d logs/tdiuc/block logs/tdiuc/mutan\npython -m block.compare_tdiuc_test -d logs/tdiuc/block logs/tdiuc/mutan\n```\n\n## Pretrained models\n\n**Note:** These pretrained models have been trained using the Pytorch 1.0 to make sure that our results are reproducible in this version. We also used a more efficient learning rate scheduling strategy which turned out to give slightly better results.\n\n### VRD\n\nDownload **Block**:\n```\nmkdir -p logs/vrd\ncd logs/vrd\nwget http://data.lip6.fr/cadene/block/vrd/block.tar.gz\ntar -xzvf block.tar.gz\n```\n\nResults `python -m block.compare_vrd_test -d logs/vrd/block`:\n- predicate.R_50: 86.3708\n- predicate.R_100: 92.4588\n- phrase.R_50: 25.4779\n- phrase.R_100: 29.7198\n- rel.R_50: 18.0806\n- rel.R_100: 21.1181\n\n### VQA2\n\nDownload **Block train/val**:\n```\nmkdir -p logs/vqa2\ncd logs/vqa2\nwget http://data.lip6.fr/cadene/block/vqa2/block.tar.gz\ntar -xzvf block.tar.gz\n```\n\nResults val (`python -m block.compare_vqa2_val -d logs/vqa2/block`):\n- overall (oe): 63.6\n- accuracy_top1: 54.4254\n\n\nDownload **Block train+val/val/test**:\n```\nmkdir -p logs/vqa2\ncd logs/vqa2\nwget http://data.lip6.fr/cadene/block/vqa2/block_trainval.tar.gz\ntar -xzvf block_trainval.tar.gz\n```\n\nResults test-dev (EvalAI):\n- overall: 66.74\n- yes/no: 83.73\n- number: 46.51\n- other: 56.84\n\n\nDownload **Block train+val+vg/val/test**:\n```\nmkdir -p logs/vqa2\ncd logs/vqa2\nwget http://data.lip6.fr/cadene/block/vqa2/block_trainval_vg.tar.gz\ntar -xzvf block_trainval_vg.tar.gz\n```\n\nResults test-dev (EvalAI):\n- overall: 67.41\n- yes/no: 83.89\n- number: 46.22\n- other: 58.18\n\n\n### TDIUC\n\nDownload **Block train+val/val/test**:\n```\nmkdir -p logs/tdiuc\ncd logs/tdiuc\nwget http://data.lip6.fr/cadene/block/tdiuc/block_trainval.tar.gz\ntar -xzvf block_trainval.tar.gz\n```\n\nResults val (`python -m block.compare_tdiuc_val -d logs/tdiuc/block`):\n- accuracy_top1: 88.0195\n- acc_mpt_a: 72.2555\n- acc_mpt_h: 59.9484\n- acc_mpt_a_norm: 60.9635\n- acc_mpt_h_norm: 44.7724\n\nResults test (`python -m block.compare_tdiuc_test -d logs/tdiuc/block`):\n- accuracy_top1: 86.3242\n- acc_mpt_a: 72.4447\n- acc_mpt_h: 66.15\n- acc_mpt_a_norm: 58.5728\n- acc_mpt_h_norm: 38.8279\n\n\n## Fusions\n\n### Block\n\n<img src=\"http://latex2png.com/output//latex_cc316d74deb08bb2c635b77fc2473639.png\" width=\"300\"/>\n<img src=\"http://latex2png.com/output//latex_a12becd74b38910f6200e359e1d5f0f4.png\" width=\"300\"/>\n<img src=\"http://latex2png.com/output//latex_a3751b02fddbb25e7d207122c7b286ec.png\" width=\"300\"/>\n\n\n`fusion = fusions.Block([100,100], 300)`\n\nParameters:\n\n- *input_dims*: list containing the dimensions of each input vector\n- *output_dim*: desired output dimension\n- *mm_dim*:  dimension of the multi-modal space. Here, it is equal the sum of output dimensions of all the D_c tensors (default: `1600`)\n- *chunks*: number of blocks in the block-diagonal tensor. Equal to C in the previous equations (default: `20`)\n- *rank*: upper-bound of the rank of mode-3 slice matrices of D_c tensors (default: `15`)\n- *shared*: boolean that specifies if we want to share the values of input mono-modal projections (default: `False`)\n- *dropout_input*: dropout rate right after the input projections (default: `0.`)\n- *dropout_pre_lin*: dropout rate just before the output linear (default: `0.`)\n- *dropout_output*: dropout rate right after the output linear (default: `0.`)\n- *pos_norm*: string that specifies if the signed-square root - l2 normalization should be done on every chunk outputs or on the concatenations of every outputs. Accepted values: `'before_cat' and 'after_cat'`. (default: `'before_cat'`)\n\nReference: [BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection](https://arxiv.org/abs/1902.00038.pdf), *Hedi Ben-younes, R\u00e9mi Cadene, Nicolas Thome, Matthieu Cord *\n\n[code](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L30)\n\n<!-- <img src=\"http://latex2png.com/output//latex_fae4ddee815f7e0a6a1ffadae34b463e.png\" />\n -->\n\n### LinearSum\n\n<img src=\"http://latex2png.com/output//latex_229da98874b0e361343dfd9f8803a0c5.png\" width=\"300\"/>\n\n`fusion = fusions.LinearSum([100, 100], 300)`\n\nParameters:\n\n- *input_dims*: list containing the dimensions of each input vector\n- *output_dim*: desired output dimension\n- *mm_dim*: dimension of the multi-modal space (default: `1200`)\n- *activ_input*: name of the activation function that follows mono-modal projections, before the sum (default: `relu`)\n- *activ_output*: name of the activation function that follows output projection (default: `relu`)\n- *normalize*: boolean that specifies whether or not we want to apply the signed square root - l2 normalization (default: `False`)\n- *dropout_input*: dropout rate right after the *activ_input* (default: `0.`)\n- *dropout_pre_lin*: dropout rate just before the output linear (default: `0.`)\n- *dropout_output*: dropout rate right after the *activ_output* (default: `0.`)\n\n\n[code](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L545)\n\n### ConcatMLP\n\n<img src=\"http://latex2png.com/output//latex_be4d96be3dc7d8a80f68df6d67174d58.png\" width=\"300\"/>\n\n`fusion = fusions.ConcatMLP([100, 100], 300)`\n\nParameters:\n\n- *input_dims*: list containing the dimensions of each input vector\n- *output_dim*: desired output dimension\n- *dimensions*: list of hidden dimensions (default: `[500,500]`)\n- *activation*: stringname of the activation function of the network, applied at each layer but the last (default: `'relu'`)\n- *dropout*: dropout rate, applied at each layer but the last (default: `0.`)\n\n[code](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L604)\n\n### MLB\n\n<img src=\"http://latex2png.com/output//latex_26dea72b86d0ae692b2fd0f6cd60d535.png\" width=\"300\"/>\n\n`fusion = fusions.MLB([100,100], 300)`\n\nParameters:\n\n- *input_dims*: list containing the dimensions of each input vector\n- *output_dim*: desired output dimension\n- *mm_dim*: dimension of the multi-modal space (default: `1200`)\n- *activ_input*: name of the activation function that follows mono-modal projections, before the element-wise product (default: `'relu'`)\n- *activ_output*: name of the activation function that follows output projection (default: `'relu'`)\n- *normalize*: boolean that specifies whether or not we want to apply the signed square root - l2 normalization (default: `False`)\n- *dropout_input*: dropout rate right after the *activ_input* (default: `0.`)\n- *dropout_pre_lin*: dropout rate just before the output linear (default: `0.`)\n- *dropout_output*: dropout rate right after the *activ_output* (default: `0.`)\n\nReference: [Hadamard Product for Low-rank Bilinear Pooling](https://arxiv.org/abs/1610.04325), *Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang*\n\n[code](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L298)\n\n### Mutan\n\n<img src=\"http://latex2png.com/output//latex_cc316d74deb08bb2c635b77fc2473639.png\" width=\"300\"/>\n<img src=\"http://latex2png.com/output//latex_5bfae86f6f252a1cc80ea591caea4cd0.png\" width=\"300\"/>\n<img src=\"http://latex2png.com/output//latex_0af95aba6c082d5f10f835e12183d67b.png\" width=\"300\"/>\n\n\n`fusion = fusions.Mutan([100, 100], 300)`\n\nParameters:\n\n- *input_dims*: list containing the dimensions of each input vector\n- *output_dim*: desired output dimension\n- *mm_dim*:  dimension of the multi-modal space. Here, it is equal to the output dimensions of the D tensor (default: `1600`)\n- *rank*: upper-bound of the rank of mode-3 slice matrices of the D tensor (default: `15`)\n- *shared*: boolean that specifies if we want to share the values of input mono-modal projections (default: `False`)\n- *normalize*: boolean that specifies whether or not we want to apply the signed square root - l2 normalization (default: `False`)\n- *dropout_input*: dropout rate right after the input projections (default: `0.`)\n- *dropout_pre_lin*: dropout rate just before the output linear (default: `0.`)\n- *dropout_output*: dropout rate right after the output linear (default: `0.`)\n\nReference: [MUTAN: Multimodal Tucker Fusion for Visual Question Answering](https://arxiv.org/abs/1705.06676), *Hedi Ben-younes\\*, R\u00e9mi Cadene\\*, Nicolas Thome, Matthieu Cord*\n\n[code](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L182)\n\n### Tucker\n\n<img src=\"http://latex2png.com/output//latex_cc316d74deb08bb2c635b77fc2473639.png\" width=\"300\"/>\n<img src=\"http://latex2png.com/output//latex_5bfae86f6f252a1cc80ea591caea4cd0.png\" width=\"300\"/>\n\nThis module correponds to `Mutan` without the low-rank constraint on third-mode slices of the D tensor.\n\n`fusion = fusions.Tucker([100, 100], 300)`\n\nParameters:\n\n- *input_dims*: list containing the dimensions of each input vector\n- *output_dim*: desired output dimension\n- *mm_dim*:  dimension of the multi-modal space. Here, it is equal to the output dimensions of the D tensor (default: `1600`)\n- *shared*: boolean that specifies if we want to share the values of input mono-modal projections (default: `False`)\n- *normalize*: boolean that specifies whether or not we want to apply the signed square root - l2 normalization (default: `False`)\n- *dropout_input*: dropout rate right after the input projections (default: `0.`)\n- *dropout_pre_lin*: dropout rate just before the output linear (default: `0.`)\n- *dropout_output*: dropout rate right after the output linear (default: `0.`)\n\nReference: [MUTAN: Multimodal Tucker Fusion for Visual Question Answering](https://arxiv.org/abs/1705.06676), *Hedi Ben-younes\\*, R\u00e9mi Cadene\\*, Nicolas Thome, Matthieu Cord*\n\n[code](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L243)\n\n### BlockTucker\n\n<img src=\"http://latex2png.com/output//latex_cc316d74deb08bb2c635b77fc2473639.png\" width=\"300\"/>\n<img src=\"http://latex2png.com/output//latex_a12becd74b38910f6200e359e1d5f0f4.png\" width=\"300\"/>\n\nThis module correponds to `Block` without the low-rank constraint on third-mode slices of D_c tensors\n\n`fusion = fusions.BlockTucker([100,100], 300)`\n\nParameters:\n\n- *input_dims*: list containing the dimensions of each input vector\n- *output_dim*: desired output dimension\n- *mm_dim*:  dimension of the multi-modal space. Here, it is equal the sum of output dimensions of all the D_c tensors (default: `1600`)\n- *chunks*: number of blocks in the block-diagonal tensor. Equal to C in the previous equations (default: `20`)\n- *shared*: boolean that specifies if we want to share the values of input mono-modal projections (default: `False`)\n- *dropout_input*: dropout rate right after the input projections (default: `0.`)\n- *dropout_pre_lin*: dropout rate just before the output linear (default: `0.`)\n- *dropout_output*: dropout rate right after the output linear (default: `0.`)\n- *pos_norm*: string that specifies if the signed-square root - l2 normalization should be done on every chunk outputs or on the concatenations of every outputs. Accepted values: `'before_cat' and 'after_cat'`. (default: `'before_cat'`)\n\nReference: [BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection](https://arxiv.org/abs/1902.00038.pdf), *Hedi Ben-younes, R\u00e9mi Cadene, Nicolas Thome, Matthieu Cord *\n\n[code](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L111)\n\n### MFB\n\n<img src=\"http://latex2png.com/output//latex_df17fad925ed42f5ff17bdcc5f7848f7.png\" width=\"300\"/>\n\n`fusion = fusions.MFB([100,100], 300)`\n\nParameters:\n\n- *input_dims*: list containing the dimensions of each input vector\n- *output_dim*: desired output dimension\n- *mm_dim*:  dimension of the multi-modal space. Here, it is equal to the output dimension of the MFB layer (default: `1200`)\n- *factor*: MFB factor (default: `2`)\n- *activ_input*: name of the activation function that follows mono-modal projections, before the element-wise product (default: `'relu'`)\n- *activ_output*: name of the activation function that follows output projection (default: `'relu'`)\n- *normalize*: boolean that specifies whether or not we want to apply the signed square root - l2 normalization (default: `False`)\n- *dropout_input*: dropout rate right after the *activ_input* (default: `0.`)\n- *dropout_pre_lin*: dropout rate just before the output linear (default: `0.`)\n- *dropout_output*: dropout rate right after the *activ_output* (default: `0.`)\n\nReference: [Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering](https://arxiv.org/abs/1708.01471), *Zhou Yu, Jun Yu, Jianping Fan, Dacheng Tao *\n\n[code](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L357)\n\n### MFH\n\n<img src=\"http://latex2png.com/output//latex_3aba419f0c08eb35891a3681795e2091.png\" width=\"300\"/>\n\n`fusion = fusions.MFH([100,100], 300)`\n\nParameters:\n\n- *input_dims*: list containing the dimensions of each input vector\n- *output_dim*: desired output dimension\n- *mm_dim*:  dimension of the multi-modal space. Here, it is equal to the output dimension of the MFH layer (default: `1200`)\n- *factor*: MFB factor (default: `2`)\n- *activ_input*: name of the activation function that follows mono-modal projections, before the element-wise product (default: `'relu'`)\n- *activ_output*: name of the activation function that follows output projection (default: `'relu'`)\n- *normalize*: boolean that specifies whether or not we want to apply the signed square root - l2 normalization (default: `False`)\n- *dropout_input*: dropout rate right after the *activ_input* (default: `0.`)\n- *dropout_pre_lin*: dropout rate just before the output linear (default: `0.`)\n- *dropout_output*: dropout rate right after the *activ_output* (default: `0.`)\n\nReference: [Beyond Bilinear: Generalized Multi-modal Factorized High-order Pooling for Visual Question Answering](https://arxiv.org/abs/1708.03619), *Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, Dacheng Tao*\n\n[code](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L421)\n\n### MCB\n\n/!\\ Not available in pytorch 1.0 - Avaiable in pytorch 0.3 and 0.4\n\n<img src=\"http://latex2png.com/output//latex_b38152dc4885205d7cd9778132d6a87e.png\" width=\"300\"/>\n\n`fusion = fusions.MCB([100,100], 300)`\n\nParameters:\n\n- *input_dims*: list containing the dimensions of each input vector\n- *output_dim*: desired output dimension\n- *mm_dim*:  dimension of the multi-modal space. Here, it is equal to the output dimension of the MCB layer (default: `16000`)\n- *activ_output*: name of the activation function that follows output projection (default: `'relu'`)\n- *dropout_output*: dropout rate right after the *activ_output* (default: `0.`)\n\nReference: [Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding](https://arxiv.org/abs/1708.03619), *Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach*\n\n[code](https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L514)\n\n\n\n## Useful commands\n\n### Use tensorboard instead of plotly\n\nInstead of creating a `view.html` file, a tensorboard file will be created:\n```\npython -m bootstrap.run -o block/options/vqa2/block.yaml \\\n--view.name tensorboard\n```\n\n```\ntensorboard --logdir=logs/vqa2\n```\n\nYou can use plotly and tensorboard at the same time by updating the yaml file like [this one](https://github.com/Cadene/bootstrap.pytorch/blob/master/bootstrap/options/mnist_plotly_tensorboard.yaml#L38).\n\n### Use a specific GPU\n\nFor a specific experiment:\n```\nCUDA_VISIBLE_DEVICES=0 python -m boostrap.run -o block/options/vqa2/block.yaml\n```\n\nFor the current terminal session:\n```\nexport CUDA_VISIBLE_DEVICES=0\n```\n\n### Overwrite an option\n\nThe boostrap.pytorch framework makes it easy to overwrite a hyperparameter. In this example, we run an experiment with a non-default learning rate. Thus, I also overwrite the experiment directory path:\n```\npython -m bootstrap.run -o block/options/vqa2/block.yaml \\\n--optimizer.lr 0.0003 \\\n--exp.dir logs/vqa2/block_lr,0.0003\n```\n\n### Resume training\n\nIf a problem occurs, it is easy to resume the last epoch by specifying the options file from the experiment directory while overwritting the `exp.resume` option (default is None):\n```\npython -m bootstrap.run -o logs/vqa2/block/options.yaml \\\n--exp.resume last\n```\n\n### Web API\n\n```\nTODO\n```\n\n### Extract your own image features\n\n```\nTODO\n```\n\n\n## Citation\n\n```\n@InProceedings{BenYounes_2019_AAAI,\n    author = {Ben-Younes, Hedi and Cadene, Remi and Thome, Nicolas and Cord, Matthieu},\n    title = {BLOCK: {B}ilinear {S}uperdiagonal {F}usion for {V}isual {Q}uestion {A}nswering and {V}isual {R}elationship {D}etection},\n    booktitle = {The Thirty-Third AAAI Conference on Artificial Intelligence},\n    year = {2019},\n    url = {http://remicadene.com/pdfs/paper_aaai2019.pdf}\n}\n```\n\n## Poster\n\n<p align=\"center\">\n    <a href=\"http://remicadene.com/pdfs/poster_aaai2019.pdf\"><img src=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/assets/poster_aaai2019.png?raw=true\" width=\"300\"/></a>\n</p>\n\n## Authors\n\nThis code was made available by [Hedi Ben-Younes](https://twitter.com/labegne) (Sorbonne-Heuritech), [Remi Cadene](http://remicadene.com) (Sorbonne), [Matthieu Cord](http://webia.lip6.fr/~cord) (Sorbonne) and [Nicolas Thome](http://cedric.cnam.fr/~thomen/) (CNAM).\n\n## Acknowledgment\n\nSpecial thanks to the authors of [VQA2](TODO), [TDIUC](TODO), [VisualGenome](TODO) and [VRD](TODO), the datasets used in this research project.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/cadene/block.bootstrap.pytorch", "keywords": "pytorch block vqa vrd visual question answering visual relationship detection relation bootstrap deep learning aaai", "license": "", "maintainer": "", "maintainer_email": "", "name": "block.bootstrap.pytorch", "package_url": "https://pypi.org/project/block.bootstrap.pytorch/", "platform": "", "project_url": "https://pypi.org/project/block.bootstrap.pytorch/", "project_urls": {"Homepage": "https://github.com/cadene/block.bootstrap.pytorch"}, "release_url": "https://pypi.org/project/block.bootstrap.pytorch/0.1.6/", "requires_dist": null, "requires_python": "", "summary": "BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection", "version": "0.1.6", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>BLOCK: Bilinear Superdiagonal Fusion for VQA and VRD</h1>\n<p><a href=\"https://travis-ci.org/Cadene/block.bootstrap.pytorch\" rel=\"nofollow\"><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/532b1d37d29426bd3c59ac915b4117fe06b59d7f/68747470733a2f2f7472617669732d63692e6f72672f436164656e652f626c6f636b2e626f6f7473747261702e7079746f7263682e7376673f6272616e63683d6d6173746572\"></a></p>\n<p>In Machine Learning, an important question is \"How to fuse two modalities in a same space\".\nFor instance, in Visual Question Answering, one must fuse the image and the question embeddings in a same bi-modal space. This multimodal embedding is latter classified to provide the answer.</p>\n<p align=\"center\">\n    <img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/123dfa1c0f6f1ec5310dd5a44e77b35f64f802c0/68747470733a2f2f6769746875622e636f6d2f436164656e652f626c6f636b2e626f6f7473747261702e7079746f7263682f626c6f622f6d61737465722f6173736574732f5651415f626c6f636b2e706e673f7261773d74727565\" width=\"600\">\n</p>\n<p>We introduce a novel module (BLOCK) to fuse two representations together. First, we experimentaly demonstrate that it is better than any available fusion for our tasks. Secondly, we provide a theoritical-grounded analysis around the notion of tensor complexity. For further details, please see <a href=\"http://remicadene.com/pdfs/paper_aaai2019.pdf\" rel=\"nofollow\">our AAAI 2019 paper</a> and <a href=\"http://remicadene.com/pdfs/poster_aaai2019.pdf\" rel=\"nofollow\">poster</a>.</p>\n<p>In this repo, we make our BLOCK fusion available via pip install including several powerful fusions from the state-of-the-art (MLB, MUTAN, MCB, MFB, MFH, etc.). Also, we provide pretrained models and all the code needed to reproduce our experiments.</p>\n<h4>Summary</h4>\n<ul>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a>\n<ul>\n<li><a href=\"#1-python-3--anaconda\" rel=\"nofollow\">Python 3 &amp; Anaconda</a></li>\n<li><a href=\"#2-as-standalone-project\" rel=\"nofollow\">As a standalone project</a></li>\n<li><a href=\"#3-download-datasets\" rel=\"nofollow\">Download datasets</a></li>\n<li><a href=\"#2-as-a-python-library\" rel=\"nofollow\">As a python library</a></li>\n</ul>\n</li>\n<li><a href=\"#quick-start\" rel=\"nofollow\">Quick start</a>\n<ul>\n<li><a href=\"#train-a-model\" rel=\"nofollow\">Train a model</a></li>\n<li><a href=\"#evaluate-a-model\" rel=\"nofollow\">Evaluate a model</a></li>\n</ul>\n</li>\n<li><a href=\"#reproduce-results\" rel=\"nofollow\">Reproduce results</a>\n<ul>\n<li><a href=\"#vrd-dataset\" rel=\"nofollow\">VRD</a></li>\n<li><a href=\"#vqa2-dataset\" rel=\"nofollow\">VQA2</a></li>\n<li><a href=\"#tdiuc-dataset\" rel=\"nofollow\">TDIUC</a></li>\n</ul>\n</li>\n<li><a href=\"#pretrained-models\" rel=\"nofollow\">Pretrained models</a></li>\n<li><a href=\"#fusions\" rel=\"nofollow\">Fusions</a>\n<ul>\n<li><a href=\"#block\" rel=\"nofollow\">Block</a></li>\n<li><a href=\"#linearsum\" rel=\"nofollow\">LinearSum</a></li>\n<li><a href=\"#concatMLP\" rel=\"nofollow\">ConcatMLP</a></li>\n<li><a href=\"#mlb\" rel=\"nofollow\">MLB</a></li>\n<li><a href=\"#tucker\" rel=\"nofollow\">Tucker</a></li>\n<li><a href=\"#mutan\" rel=\"nofollow\">Mutan</a></li>\n<li><a href=\"#blocktucker\" rel=\"nofollow\">BlockTucker</a></li>\n<li><a href=\"#mfb\" rel=\"nofollow\">MFB</a></li>\n<li><a href=\"#mfh\" rel=\"nofollow\">MFH</a></li>\n<li><a href=\"#mcb\" rel=\"nofollow\">MCB</a></li>\n</ul>\n</li>\n<li><a href=\"#useful-commands\" rel=\"nofollow\">Useful commands</a></li>\n<li><a href=\"#citation\" rel=\"nofollow\">Citation</a></li>\n<li><a href=\"#poster\" rel=\"nofollow\">Poster</a></li>\n<li><a href=\"#authors\" rel=\"nofollow\">Authors</a></li>\n<li><a href=\"#acknowledgment\" rel=\"nofollow\">Acknowledgment</a></li>\n</ul>\n<h2>Installation</h2>\n<h3>1. Python 3 &amp; Anaconda</h3>\n<p>We don't provide support for python 2. We advise you to install python 3 with <a href=\"https://www.continuum.io/downloads\" rel=\"nofollow\">Anaconda</a>. Then, you can create an environment.</p>\n<h3>2. As standalone project</h3>\n<pre><code>conda create --name block python=3\nsource activate block\ngit clone --recursive https://github.com/Cadene/block.bootstrap.pytorch.git\ncd block.bootstrap.pytorch\npip install -r requirements.txt\n</code></pre>\n<h3>3. Download datasets</h3>\n<p>Download annotations, images and features for VRD experiments:</p>\n<pre><code>bash block/datasets/scripts/download_vrd.sh\n</code></pre>\n<p>Download annotations, images and features for VQA experiments:</p>\n<pre><code>bash block/datasets/scripts/download_vqa2.sh\nbash block/datasets/scripts/download_vgenome.sh\nbash block/datasets/scripts/download_tdiuc.sh\n</code></pre>\n<p><strong>Note:</strong> The features have been extracted from a pretrained Faster-RCNN with caffe. We don't provide the code for pretraining or extracting features for now.</p>\n<h3>(2. As a python library)</h3>\n<p>By importing the <code>block</code> python module, you can access every fusions, datasets and models in a simple way:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">block</span> <span class=\"kn\">import</span> <span class=\"n\">fusions</span>\n<span class=\"n\">mm</span> <span class=\"o\">=</span> <span class=\"n\">fusions</span><span class=\"o\">.</span><span class=\"n\">Block</span><span class=\"p\">([</span><span class=\"mi\">100</span><span class=\"p\">,</span><span class=\"mi\">100</span><span class=\"p\">],</span> <span class=\"mi\">300</span><span class=\"p\">)</span>\n<span class=\"n\">inputs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">100</span><span class=\"p\">),</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">100</span><span class=\"p\">)]</span>\n<span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"n\">mm</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">)</span> <span class=\"c1\"># torch.Size([10,300])</span>\n<span class=\"c1\"># ...</span>\n<span class=\"n\">fusions</span><span class=\"o\">.</span><span class=\"n\">LinearSum</span>\n<span class=\"n\">fusions</span><span class=\"o\">.</span><span class=\"n\">ConcatMLP</span>\n<span class=\"n\">fusions</span><span class=\"o\">.</span><span class=\"n\">MLB</span>\n<span class=\"n\">fusions</span><span class=\"o\">.</span><span class=\"n\">Mutan</span>\n<span class=\"n\">fusions</span><span class=\"o\">.</span><span class=\"n\">Tucker</span>\n<span class=\"n\">fusions</span><span class=\"o\">.</span><span class=\"n\">BlockTucker</span>\n<span class=\"n\">fusions</span><span class=\"o\">.</span><span class=\"n\">MFB</span>\n<span class=\"n\">fusions</span><span class=\"o\">.</span><span class=\"n\">MFH</span>\n<span class=\"n\">fusions</span><span class=\"o\">.</span><span class=\"n\">MCB</span>\n<span class=\"c1\"># ...</span>\n<span class=\"kn\">from</span> <span class=\"nn\">block.datasets.vqa2</span> <span class=\"kn\">import</span> <span class=\"n\">VQA2</span>\n<span class=\"kn\">from</span> <span class=\"nn\">block.datasets.tdiuc</span> <span class=\"kn\">import</span> <span class=\"n\">TDIUC</span>\n<span class=\"kn\">from</span> <span class=\"nn\">block.datasets.vg</span> <span class=\"kn\">import</span> <span class=\"n\">VG</span>\n<span class=\"kn\">from</span> <span class=\"nn\">block.datasets.vrd</span> <span class=\"kn\">import</span> <span class=\"n\">VRD</span>\n<span class=\"c1\"># ...</span>\n<span class=\"kn\">from</span> <span class=\"nn\">block.models.networks.vqa_net</span> <span class=\"kn\">import</span> <span class=\"n\">VQANet</span>\n<span class=\"kn\">from</span> <span class=\"nn\">block.models.networks.vrd_net</span> <span class=\"kn\">import</span> <span class=\"n\">VRDNet</span>\n<span class=\"c1\"># ...</span>\n</pre>\n<p>To be able to do so, you can use pip:</p>\n<pre><code>pip install block.bootstrap.pytorch\n</code></pre>\n<p>Or install from source:</p>\n<pre><code>git clone https://github.com/Cadene/block.bootstrap.pytorch.git\npython setup.py install\n</code></pre>\n<h2>Quick start</h2>\n<h3>Train a model</h3>\n<p>The <a href=\"https://github.com/Cadene/bootstrap.pytorch/blob/master/bootstrap/run.py\" rel=\"nofollow\">boostrap/run.py</a> file load the options contained in a yaml file, create the corresponding experiment directory and start the training procedure. For instance, you can train our best model on VRD by running:</p>\n<pre><code>python -m bootstrap.run -o block/options/vrd/block.yaml\n</code></pre>\n<p>Then, several files are going to be created in <code>logs/vrd/block</code>:</p>\n<ul>\n<li><a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/assets/logs/vrd/block/options.yaml\" rel=\"nofollow\">options.yaml</a> (copy of options)</li>\n<li><a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/assets/logs/vrd/block/logs.txt\" rel=\"nofollow\">logs.txt</a> (history of print)</li>\n<li><a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/assets/logs/vrd/block/logs.json\" rel=\"nofollow\">logs.json</a> (batchs and epochs statistics)</li>\n<li><a href=\"http://htmlpreview.github.io/?https://raw.githubusercontent.com/Cadene/block.bootstrap.pytorch/master/assets/logs/vrd/block/view.html?token=AEdvLlDSYaSn3Hsr7gO5sDBxeyuKNQhEks5cTF6-wA%3D%3D\" rel=\"nofollow\">view.html</a> (learning curves)</li>\n<li>ckpt_last_engine.pth.tar (checkpoints of last epoch)</li>\n<li>ckpt_last_model.pth.tar</li>\n<li>ckpt_last_optimizer.pth.tar</li>\n<li>ckpt_best_eval_epoch.predicate.R_50_engine.pth.tar (checkpoints of best epoch)</li>\n<li>ckpt_best_eval_epoch.predicate.R_50_model.pth.tar</li>\n<li>ckpt_best_eval_epoch.predicate.R_50_optimizer.pth.tar</li>\n</ul>\n<p>Many options are available in the <a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/options\" rel=\"nofollow\">options directory</a>.</p>\n<h3>Evaluate a model</h3>\n<p>At the end of the training procedure, you can evaluate your model on the testing set. In this example, <a href=\"https://github.com/Cadene/bootstrap.pytorch/blob/master/bootstrap/run.py\" rel=\"nofollow\">boostrap/run.py</a> load the options from your experiment directory, resume the best checkpoint on the validation set and start an evaluation on the testing set instead of the validation set while skipping the training set (train_split is empty). Thanks to <code>--misc.logs_name</code>, the logs will be written in the new <code>logs_predicate.txt</code> and <code>logs_predicate.json</code> files, instead of being appended to the <code>logs.txt</code> and <code>logs.json</code> files.</p>\n<pre><code>python -m bootstrap.run \\\n-o logs/vrd/block/options.yaml \\\n--exp.resume best_eval_epoch.predicate.R_50 \\\n--dataset.train_split \\\n--dataset.eval_split test \\\n--misc.logs_name predicate\n</code></pre>\n<h2>Reproduce results</h2>\n<h3>VRD dataset</h3>\n<h4>Train and evaluate on VRD</h4>\n<ol>\n<li>Train block on trainset with early stopping on valset</li>\n<li>Evaluate the best checkpoint on testset (Predicate Prediction)</li>\n<li>Evaluate the best checkpoint on testset (Relationship and Phrase Detection)</li>\n</ol>\n<pre><code>python -m bootstrap.run \\\n-o block/options/vrd/block.yaml \\\n--exp.dir logs/vrd/block\n\npython -m bootstrap.run \\\n-o logs/vrd/block/options.yaml \\\n--dataset.train_split \\\n--dataset.eval_split test \\\n--exp.resume best_eval_epoch.predicate.R_50 \\\n--misc.logs_name predicate\n\npython -m bootstrap.run \\\n-o logs/vrd/block/options.yaml \\\n--dataset.train_split \\\n--dataset.eval_split test \\\n--dataset.mode rel_phrase \\\n--model.metric.name vrd_rel_phrase \\\n--exp.resume best_eval_epoch.predicate.R_50 \\\n--misc.logs_name rel_phrase\n</code></pre>\n<p><strong>Note:</strong> You can copy past the three commands at once in the terminal to run one after each other seamlessly.</p>\n<p><strong>Note:</strong> Block is not the only option available. You can find several others <a href=\"https://github.com/Cadene/block.bootstrap.pytorch/tree/master/block/options/vrd\" rel=\"nofollow\">here</a>.</p>\n<p><strong>Note:</strong> Learning curves can be viewed in the experiment directy (<code>logs/vrd/block/view.html</code>). An example is available <a href=\"http://htmlpreview.github.io/?https://raw.githubusercontent.com/Cadene/block.bootstrap.pytorch/master/assets/logs/vrd/block/view.html?token=AEdvLlDSYaSn3Hsr7gO5sDBxeyuKNQhEks5cTF6-wA%3D%3D\" rel=\"nofollow\">here</a>.</p>\n<p><strong>Note:</strong> In our article, we report result for a negative sampling ratio of 0.5. Better results in <em>Predicate Prediction</em> can be achieve with a ratio of 0.0. Better results in <em>Phrase Detection</em> and <em>Relationship Detection</em> can be achieve with a ratio of 0.8. You can change the ratio by doing so:</p>\n<pre><code>python -m bootstrap.run \\\n-o block/options/vrd/block.yaml \\\n--exp.dir logs/vrd/block_ratio,0.0 \\\n--dataset.neg_ratio 0.0\n</code></pre>\n<h4>Compare experiments on VRD</h4>\n<p>Finally you can compare experiments on the valset or testset metrics:</p>\n<pre><code>python -m block.compare_vrd_val -d \\\nlogs/vrd/block \\\nlogs/vrd/block_tucker \\\nlogs/vrd/mutan \\\nlogs/vrd/mfh \\\nlogs/vrd/mlb\n\npython -m block.compare_vrd_test -d \\\nlogs/vrd/block \\\nlogs/vrd/block_tucker\n</code></pre>\n<p>Example:</p>\n<pre><code>## eval_epoch.predicate.R_50\n  Place  Method          Score    Epoch\n-------  ------------  -------  -------\n      1  block         86.3708       13\n      2  block_tucker  86.2529        9\n\n## eval_epoch.predicate.R_100\n  Place  Method          Score    Epoch\n-------  ------------  -------  -------\n      1  block         92.4588       13\n      2  block_tucker  91.5816        9\n\n## eval_epoch.phrase.R_50\n  Place  Method          Score    Epoch\n-------  ------------  -------  -------\n      1  block         25.4779       13\n      2  block_tucker  23.7759        9\n\n## eval_epoch.phrase.R_100\n  Place  Method          Score    Epoch\n-------  ------------  -------  -------\n      1  block         29.7198       13\n      2  block_tucker  27.9131        9\n\n## eval_epoch.rel.R_50\n  Place  Method          Score    Epoch\n-------  ------------  -------  -------\n      1  block         18.0806       13\n      2  block_tucker  17.0856        9\n\n## eval_epoch.rel.R_100\n  Place  Method          Score    Epoch\n-------  ------------  -------  -------\n      1  block         21.1181       13\n      2  block_tucker  19.7565        9\n</code></pre>\n<h3>VQA2 dataset</h3>\n<h4>Training and evaluation (train/val)</h4>\n<p>We use this simple setup to tune our hyperparameters on the valset.</p>\n<pre><code>python -m bootstrap.run \\\n-o block/options/vqa2/block.yaml \\\n--exp.dir logs/vqa2/block\n</code></pre>\n<h4>Training and evaluation (train+val/val/test)</h4>\n<p>This heavier setup allows us to train a model on 95% of the concatenation of train and val sets, and to evaluate it on the 5% rest. Then we extract the predictions of our best checkpoint on the testset. Finally, we submit a json file on the EvalAI web site.</p>\n<pre><code>python -m bootstrap.run \\\n-o block/options/vqa2/block.yaml \\\n--exp.dir logs/vqa2/block_trainval \\\n--dataset.proc_split trainval\n\npython -m bootstrap.run \\\n-o logs/vqa2/block_trainval/options.yaml \\\n--exp.resume best_eval_epoch.accuracy_top1 \\\n--dataset.train_split \\\n--dataset.eval_split test \\\n--misc.logs_name test\n</code></pre>\n<h4>Training and evaluation (train+val+vg/val/test)</h4>\n<p>Same, but we add pairs from the VisualGenome dataset.</p>\n<pre><code>python -m bootstrap.run \\\n-o block/options/vqa2/block.yaml \\\n--exp.dir logs/vqa2/block_trainval_vg \\\n--dataset.proc_split trainval \\\n--dataset.vg True\n\npython -m bootstrap.run \\\n-o logs/vqa2/block_trainval_vg/options.yaml \\\n--exp.resume best_eval_epoch.accuracy_top1 \\\n--dataset.train_split \\\n--dataset.eval_split test \\\n--misc.logs_name test\n</code></pre>\n<h4>Compare experiments on valset</h4>\n<p>You can compare experiments by displaying their best metrics on the valset.</p>\n<pre><code>python -m block.compare_vqa_val -d logs/vqa2/block logs/vqa2/mutan\n</code></pre>\n<h4>Submit predictions on EvalAI</h4>\n<p>It is not possible to automaticaly compute the accuracies on the testset. You need to submit a json file on the <a href=\"http://evalai.cloudcv.org/web/challenges/challenge-page/80/my-submission\" rel=\"nofollow\">EvalAI platform</a>. The evaluation step on the testset creates the json file that contains the prediction of your model on the full testset. For instance: <code>logs/vqa2/block_trainval_vg/results/test/epoch,19/OpenEnded_mscoco_test2015_model_results.json</code>. To get the accuracies on testdev or test sets, you must submit this file.</p>\n<h3>TDIUC dataset</h3>\n<h4>Training and evaluation (train/val/test)</h4>\n<p>The full training set is split into a trainset and a valset. At the end of the training, we evaluate our best checkpoint on the testset. The TDIUC metrics are computed and displayed at the end of each epoch. They are also stored in <code>logs.json</code> and <code>logs_test.json</code>.</p>\n<pre><code>python -m bootstrap.run \\\n-o block/options/tdiuc/block.yaml \\\n--exp.dir logs/tdiuc/block\n\npython -m bootstrap.run \\\n-o logs/tdiuc/block/options.yaml \\\n--exp.resume best_eval_epoch.accuracy_top1 \\\n--dataset.train_split \\\n--dataset.eval_split test \\\n--misc.logs_name test\n</code></pre>\n<h4>Compare experiments</h4>\n<p>You can compare experiments by displaying their best metrics on the valset or testset.</p>\n<pre><code>python -m block.compare_tdiuc_val -d logs/tdiuc/block logs/tdiuc/mutan\npython -m block.compare_tdiuc_test -d logs/tdiuc/block logs/tdiuc/mutan\n</code></pre>\n<h2>Pretrained models</h2>\n<p><strong>Note:</strong> These pretrained models have been trained using the Pytorch 1.0 to make sure that our results are reproducible in this version. We also used a more efficient learning rate scheduling strategy which turned out to give slightly better results.</p>\n<h3>VRD</h3>\n<p>Download <strong>Block</strong>:</p>\n<pre><code>mkdir -p logs/vrd\ncd logs/vrd\nwget http://data.lip6.fr/cadene/block/vrd/block.tar.gz\ntar -xzvf block.tar.gz\n</code></pre>\n<p>Results <code>python -m block.compare_vrd_test -d logs/vrd/block</code>:</p>\n<ul>\n<li>predicate.R_50: 86.3708</li>\n<li>predicate.R_100: 92.4588</li>\n<li>phrase.R_50: 25.4779</li>\n<li>phrase.R_100: 29.7198</li>\n<li>rel.R_50: 18.0806</li>\n<li>rel.R_100: 21.1181</li>\n</ul>\n<h3>VQA2</h3>\n<p>Download <strong>Block train/val</strong>:</p>\n<pre><code>mkdir -p logs/vqa2\ncd logs/vqa2\nwget http://data.lip6.fr/cadene/block/vqa2/block.tar.gz\ntar -xzvf block.tar.gz\n</code></pre>\n<p>Results val (<code>python -m block.compare_vqa2_val -d logs/vqa2/block</code>):</p>\n<ul>\n<li>overall (oe): 63.6</li>\n<li>accuracy_top1: 54.4254</li>\n</ul>\n<p>Download <strong>Block train+val/val/test</strong>:</p>\n<pre><code>mkdir -p logs/vqa2\ncd logs/vqa2\nwget http://data.lip6.fr/cadene/block/vqa2/block_trainval.tar.gz\ntar -xzvf block_trainval.tar.gz\n</code></pre>\n<p>Results test-dev (EvalAI):</p>\n<ul>\n<li>overall: 66.74</li>\n<li>yes/no: 83.73</li>\n<li>number: 46.51</li>\n<li>other: 56.84</li>\n</ul>\n<p>Download <strong>Block train+val+vg/val/test</strong>:</p>\n<pre><code>mkdir -p logs/vqa2\ncd logs/vqa2\nwget http://data.lip6.fr/cadene/block/vqa2/block_trainval_vg.tar.gz\ntar -xzvf block_trainval_vg.tar.gz\n</code></pre>\n<p>Results test-dev (EvalAI):</p>\n<ul>\n<li>overall: 67.41</li>\n<li>yes/no: 83.89</li>\n<li>number: 46.22</li>\n<li>other: 58.18</li>\n</ul>\n<h3>TDIUC</h3>\n<p>Download <strong>Block train+val/val/test</strong>:</p>\n<pre><code>mkdir -p logs/tdiuc\ncd logs/tdiuc\nwget http://data.lip6.fr/cadene/block/tdiuc/block_trainval.tar.gz\ntar -xzvf block_trainval.tar.gz\n</code></pre>\n<p>Results val (<code>python -m block.compare_tdiuc_val -d logs/tdiuc/block</code>):</p>\n<ul>\n<li>accuracy_top1: 88.0195</li>\n<li>acc_mpt_a: 72.2555</li>\n<li>acc_mpt_h: 59.9484</li>\n<li>acc_mpt_a_norm: 60.9635</li>\n<li>acc_mpt_h_norm: 44.7724</li>\n</ul>\n<p>Results test (<code>python -m block.compare_tdiuc_test -d logs/tdiuc/block</code>):</p>\n<ul>\n<li>accuracy_top1: 86.3242</li>\n<li>acc_mpt_a: 72.4447</li>\n<li>acc_mpt_h: 66.15</li>\n<li>acc_mpt_a_norm: 58.5728</li>\n<li>acc_mpt_h_norm: 38.8279</li>\n</ul>\n<h2>Fusions</h2>\n<h3>Block</h3>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/609248259be09b17f21e24e77b6c00daf39cc20b/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f63633331366437346465623038626232633633356237376663323437333633392e706e67\" width=\"300\">\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5bcf7b4bc589f4533d5f64940cff92393fac0ea3/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f61313262656364373462333839313066363230306533353965316435663066342e706e67\" width=\"300\">\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c0af8535f01667dbbb36efdfb6d2ca27e516e0aa/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f61333735316230326664646262323565376432303731323263376232383665632e706e67\" width=\"300\">\n<p><code>fusion = fusions.Block([100,100], 300)</code></p>\n<p>Parameters:</p>\n<ul>\n<li><em>input_dims</em>: list containing the dimensions of each input vector</li>\n<li><em>output_dim</em>: desired output dimension</li>\n<li><em>mm_dim</em>:  dimension of the multi-modal space. Here, it is equal the sum of output dimensions of all the D_c tensors (default: <code>1600</code>)</li>\n<li><em>chunks</em>: number of blocks in the block-diagonal tensor. Equal to C in the previous equations (default: <code>20</code>)</li>\n<li><em>rank</em>: upper-bound of the rank of mode-3 slice matrices of D_c tensors (default: <code>15</code>)</li>\n<li><em>shared</em>: boolean that specifies if we want to share the values of input mono-modal projections (default: <code>False</code>)</li>\n<li><em>dropout_input</em>: dropout rate right after the input projections (default: <code>0.</code>)</li>\n<li><em>dropout_pre_lin</em>: dropout rate just before the output linear (default: <code>0.</code>)</li>\n<li><em>dropout_output</em>: dropout rate right after the output linear (default: <code>0.</code>)</li>\n<li><em>pos_norm</em>: string that specifies if the signed-square root - l2 normalization should be done on every chunk outputs or on the concatenations of every outputs. Accepted values: <code>'before_cat' and 'after_cat'</code>. (default: <code>'before_cat'</code>)</li>\n</ul>\n<p>Reference: <a href=\"https://arxiv.org/abs/1902.00038.pdf\" rel=\"nofollow\">BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection</a>, *Hedi Ben-younes, R\u00e9mi Cadene, Nicolas Thome, Matthieu Cord *</p>\n<p><a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L30\" rel=\"nofollow\">code</a></p>\n\n<h3>LinearSum</h3>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b0bde3223b960dc3603681d101e3318d22022fac/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f32323964613938383734623065333631333433646664396638383033613063352e706e67\" width=\"300\">\n<p><code>fusion = fusions.LinearSum([100, 100], 300)</code></p>\n<p>Parameters:</p>\n<ul>\n<li><em>input_dims</em>: list containing the dimensions of each input vector</li>\n<li><em>output_dim</em>: desired output dimension</li>\n<li><em>mm_dim</em>: dimension of the multi-modal space (default: <code>1200</code>)</li>\n<li><em>activ_input</em>: name of the activation function that follows mono-modal projections, before the sum (default: <code>relu</code>)</li>\n<li><em>activ_output</em>: name of the activation function that follows output projection (default: <code>relu</code>)</li>\n<li><em>normalize</em>: boolean that specifies whether or not we want to apply the signed square root - l2 normalization (default: <code>False</code>)</li>\n<li><em>dropout_input</em>: dropout rate right after the <em>activ_input</em> (default: <code>0.</code>)</li>\n<li><em>dropout_pre_lin</em>: dropout rate just before the output linear (default: <code>0.</code>)</li>\n<li><em>dropout_output</em>: dropout rate right after the <em>activ_output</em> (default: <code>0.</code>)</li>\n</ul>\n<p><a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L545\" rel=\"nofollow\">code</a></p>\n<h3>ConcatMLP</h3>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6c4559a2430ec1af3db751a23b9c46619b4465e8/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f62653464393662653364633764386138306636386466366436373137346435382e706e67\" width=\"300\">\n<p><code>fusion = fusions.ConcatMLP([100, 100], 300)</code></p>\n<p>Parameters:</p>\n<ul>\n<li><em>input_dims</em>: list containing the dimensions of each input vector</li>\n<li><em>output_dim</em>: desired output dimension</li>\n<li><em>dimensions</em>: list of hidden dimensions (default: <code>[500,500]</code>)</li>\n<li><em>activation</em>: stringname of the activation function of the network, applied at each layer but the last (default: <code>'relu'</code>)</li>\n<li><em>dropout</em>: dropout rate, applied at each layer but the last (default: <code>0.</code>)</li>\n</ul>\n<p><a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L604\" rel=\"nofollow\">code</a></p>\n<h3>MLB</h3>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/31aa23478b7f8984df9e9e8ef1ff317ab7efde4f/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f32366465613732623836643061653639326232666430663663643630643533352e706e67\" width=\"300\">\n<p><code>fusion = fusions.MLB([100,100], 300)</code></p>\n<p>Parameters:</p>\n<ul>\n<li><em>input_dims</em>: list containing the dimensions of each input vector</li>\n<li><em>output_dim</em>: desired output dimension</li>\n<li><em>mm_dim</em>: dimension of the multi-modal space (default: <code>1200</code>)</li>\n<li><em>activ_input</em>: name of the activation function that follows mono-modal projections, before the element-wise product (default: <code>'relu'</code>)</li>\n<li><em>activ_output</em>: name of the activation function that follows output projection (default: <code>'relu'</code>)</li>\n<li><em>normalize</em>: boolean that specifies whether or not we want to apply the signed square root - l2 normalization (default: <code>False</code>)</li>\n<li><em>dropout_input</em>: dropout rate right after the <em>activ_input</em> (default: <code>0.</code>)</li>\n<li><em>dropout_pre_lin</em>: dropout rate just before the output linear (default: <code>0.</code>)</li>\n<li><em>dropout_output</em>: dropout rate right after the <em>activ_output</em> (default: <code>0.</code>)</li>\n</ul>\n<p>Reference: <a href=\"https://arxiv.org/abs/1610.04325\" rel=\"nofollow\">Hadamard Product for Low-rank Bilinear Pooling</a>, <em>Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang</em></p>\n<p><a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L298\" rel=\"nofollow\">code</a></p>\n<h3>Mutan</h3>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/609248259be09b17f21e24e77b6c00daf39cc20b/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f63633331366437346465623038626232633633356237376663323437333633392e706e67\" width=\"300\">\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e6f0d88bc6b6bb7e4b860486f43f963f222cf2bd/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f35626661653836663666323532613163633830656135393163616561346364302e706e67\" width=\"300\">\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6b5baf50e30b2d4a64fe58c3913b92780faaec6d/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f30616639356162613663303832643566313066383335653132313833643637622e706e67\" width=\"300\">\n<p><code>fusion = fusions.Mutan([100, 100], 300)</code></p>\n<p>Parameters:</p>\n<ul>\n<li><em>input_dims</em>: list containing the dimensions of each input vector</li>\n<li><em>output_dim</em>: desired output dimension</li>\n<li><em>mm_dim</em>:  dimension of the multi-modal space. Here, it is equal to the output dimensions of the D tensor (default: <code>1600</code>)</li>\n<li><em>rank</em>: upper-bound of the rank of mode-3 slice matrices of the D tensor (default: <code>15</code>)</li>\n<li><em>shared</em>: boolean that specifies if we want to share the values of input mono-modal projections (default: <code>False</code>)</li>\n<li><em>normalize</em>: boolean that specifies whether or not we want to apply the signed square root - l2 normalization (default: <code>False</code>)</li>\n<li><em>dropout_input</em>: dropout rate right after the input projections (default: <code>0.</code>)</li>\n<li><em>dropout_pre_lin</em>: dropout rate just before the output linear (default: <code>0.</code>)</li>\n<li><em>dropout_output</em>: dropout rate right after the output linear (default: <code>0.</code>)</li>\n</ul>\n<p>Reference: <a href=\"https://arxiv.org/abs/1705.06676\" rel=\"nofollow\">MUTAN: Multimodal Tucker Fusion for Visual Question Answering</a>, <em>Hedi Ben-younes*, R\u00e9mi Cadene*, Nicolas Thome, Matthieu Cord</em></p>\n<p><a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L182\" rel=\"nofollow\">code</a></p>\n<h3>Tucker</h3>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/609248259be09b17f21e24e77b6c00daf39cc20b/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f63633331366437346465623038626232633633356237376663323437333633392e706e67\" width=\"300\">\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e6f0d88bc6b6bb7e4b860486f43f963f222cf2bd/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f35626661653836663666323532613163633830656135393163616561346364302e706e67\" width=\"300\">\n<p>This module correponds to <code>Mutan</code> without the low-rank constraint on third-mode slices of the D tensor.</p>\n<p><code>fusion = fusions.Tucker([100, 100], 300)</code></p>\n<p>Parameters:</p>\n<ul>\n<li><em>input_dims</em>: list containing the dimensions of each input vector</li>\n<li><em>output_dim</em>: desired output dimension</li>\n<li><em>mm_dim</em>:  dimension of the multi-modal space. Here, it is equal to the output dimensions of the D tensor (default: <code>1600</code>)</li>\n<li><em>shared</em>: boolean that specifies if we want to share the values of input mono-modal projections (default: <code>False</code>)</li>\n<li><em>normalize</em>: boolean that specifies whether or not we want to apply the signed square root - l2 normalization (default: <code>False</code>)</li>\n<li><em>dropout_input</em>: dropout rate right after the input projections (default: <code>0.</code>)</li>\n<li><em>dropout_pre_lin</em>: dropout rate just before the output linear (default: <code>0.</code>)</li>\n<li><em>dropout_output</em>: dropout rate right after the output linear (default: <code>0.</code>)</li>\n</ul>\n<p>Reference: <a href=\"https://arxiv.org/abs/1705.06676\" rel=\"nofollow\">MUTAN: Multimodal Tucker Fusion for Visual Question Answering</a>, <em>Hedi Ben-younes*, R\u00e9mi Cadene*, Nicolas Thome, Matthieu Cord</em></p>\n<p><a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L243\" rel=\"nofollow\">code</a></p>\n<h3>BlockTucker</h3>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/609248259be09b17f21e24e77b6c00daf39cc20b/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f63633331366437346465623038626232633633356237376663323437333633392e706e67\" width=\"300\">\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5bcf7b4bc589f4533d5f64940cff92393fac0ea3/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f61313262656364373462333839313066363230306533353965316435663066342e706e67\" width=\"300\">\n<p>This module correponds to <code>Block</code> without the low-rank constraint on third-mode slices of D_c tensors</p>\n<p><code>fusion = fusions.BlockTucker([100,100], 300)</code></p>\n<p>Parameters:</p>\n<ul>\n<li><em>input_dims</em>: list containing the dimensions of each input vector</li>\n<li><em>output_dim</em>: desired output dimension</li>\n<li><em>mm_dim</em>:  dimension of the multi-modal space. Here, it is equal the sum of output dimensions of all the D_c tensors (default: <code>1600</code>)</li>\n<li><em>chunks</em>: number of blocks in the block-diagonal tensor. Equal to C in the previous equations (default: <code>20</code>)</li>\n<li><em>shared</em>: boolean that specifies if we want to share the values of input mono-modal projections (default: <code>False</code>)</li>\n<li><em>dropout_input</em>: dropout rate right after the input projections (default: <code>0.</code>)</li>\n<li><em>dropout_pre_lin</em>: dropout rate just before the output linear (default: <code>0.</code>)</li>\n<li><em>dropout_output</em>: dropout rate right after the output linear (default: <code>0.</code>)</li>\n<li><em>pos_norm</em>: string that specifies if the signed-square root - l2 normalization should be done on every chunk outputs or on the concatenations of every outputs. Accepted values: <code>'before_cat' and 'after_cat'</code>. (default: <code>'before_cat'</code>)</li>\n</ul>\n<p>Reference: <a href=\"https://arxiv.org/abs/1902.00038.pdf\" rel=\"nofollow\">BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection</a>, *Hedi Ben-younes, R\u00e9mi Cadene, Nicolas Thome, Matthieu Cord *</p>\n<p><a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L111\" rel=\"nofollow\">code</a></p>\n<h3>MFB</h3>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7f40b2df38a0389fc4540e2c6ea90bce759c82ed/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f64663137666164393235656434326635666631376264636335663738343866372e706e67\" width=\"300\">\n<p><code>fusion = fusions.MFB([100,100], 300)</code></p>\n<p>Parameters:</p>\n<ul>\n<li><em>input_dims</em>: list containing the dimensions of each input vector</li>\n<li><em>output_dim</em>: desired output dimension</li>\n<li><em>mm_dim</em>:  dimension of the multi-modal space. Here, it is equal to the output dimension of the MFB layer (default: <code>1200</code>)</li>\n<li><em>factor</em>: MFB factor (default: <code>2</code>)</li>\n<li><em>activ_input</em>: name of the activation function that follows mono-modal projections, before the element-wise product (default: <code>'relu'</code>)</li>\n<li><em>activ_output</em>: name of the activation function that follows output projection (default: <code>'relu'</code>)</li>\n<li><em>normalize</em>: boolean that specifies whether or not we want to apply the signed square root - l2 normalization (default: <code>False</code>)</li>\n<li><em>dropout_input</em>: dropout rate right after the <em>activ_input</em> (default: <code>0.</code>)</li>\n<li><em>dropout_pre_lin</em>: dropout rate just before the output linear (default: <code>0.</code>)</li>\n<li><em>dropout_output</em>: dropout rate right after the <em>activ_output</em> (default: <code>0.</code>)</li>\n</ul>\n<p>Reference: <a href=\"https://arxiv.org/abs/1708.01471\" rel=\"nofollow\">Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering</a>, *Zhou Yu, Jun Yu, Jianping Fan, Dacheng Tao *</p>\n<p><a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L357\" rel=\"nofollow\">code</a></p>\n<h3>MFH</h3>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/237acfcb17e37dee6d9e3eb1fc5ef71bbba79447/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f33616261343139663063303865623335383931613336383137393565323039312e706e67\" width=\"300\">\n<p><code>fusion = fusions.MFH([100,100], 300)</code></p>\n<p>Parameters:</p>\n<ul>\n<li><em>input_dims</em>: list containing the dimensions of each input vector</li>\n<li><em>output_dim</em>: desired output dimension</li>\n<li><em>mm_dim</em>:  dimension of the multi-modal space. Here, it is equal to the output dimension of the MFH layer (default: <code>1200</code>)</li>\n<li><em>factor</em>: MFB factor (default: <code>2</code>)</li>\n<li><em>activ_input</em>: name of the activation function that follows mono-modal projections, before the element-wise product (default: <code>'relu'</code>)</li>\n<li><em>activ_output</em>: name of the activation function that follows output projection (default: <code>'relu'</code>)</li>\n<li><em>normalize</em>: boolean that specifies whether or not we want to apply the signed square root - l2 normalization (default: <code>False</code>)</li>\n<li><em>dropout_input</em>: dropout rate right after the <em>activ_input</em> (default: <code>0.</code>)</li>\n<li><em>dropout_pre_lin</em>: dropout rate just before the output linear (default: <code>0.</code>)</li>\n<li><em>dropout_output</em>: dropout rate right after the <em>activ_output</em> (default: <code>0.</code>)</li>\n</ul>\n<p>Reference: <a href=\"https://arxiv.org/abs/1708.03619\" rel=\"nofollow\">Beyond Bilinear: Generalized Multi-modal Factorized High-order Pooling for Visual Question Answering</a>, <em>Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, Dacheng Tao</em></p>\n<p><a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L421\" rel=\"nofollow\">code</a></p>\n<h3>MCB</h3>\n<p>/!\\ Not available in pytorch 1.0 - Avaiable in pytorch 0.3 and 0.4</p>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a1ef6ef901ee7b1c883c02edc8281c7ceb354fb4/687474703a2f2f6c6174657832706e672e636f6d2f6f75747075742f2f6c617465785f62333831353264633438383532303564376364393737383133326436613837652e706e67\" width=\"300\">\n<p><code>fusion = fusions.MCB([100,100], 300)</code></p>\n<p>Parameters:</p>\n<ul>\n<li><em>input_dims</em>: list containing the dimensions of each input vector</li>\n<li><em>output_dim</em>: desired output dimension</li>\n<li><em>mm_dim</em>:  dimension of the multi-modal space. Here, it is equal to the output dimension of the MCB layer (default: <code>16000</code>)</li>\n<li><em>activ_output</em>: name of the activation function that follows output projection (default: <code>'relu'</code>)</li>\n<li><em>dropout_output</em>: dropout rate right after the <em>activ_output</em> (default: <code>0.</code>)</li>\n</ul>\n<p>Reference: <a href=\"https://arxiv.org/abs/1708.03619\" rel=\"nofollow\">Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</a>, <em>Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach</em></p>\n<p><a href=\"https://github.com/Cadene/block.bootstrap.pytorch/blob/master/block/models/networks/fusions/fusions.py#L514\" rel=\"nofollow\">code</a></p>\n<h2>Useful commands</h2>\n<h3>Use tensorboard instead of plotly</h3>\n<p>Instead of creating a <code>view.html</code> file, a tensorboard file will be created:</p>\n<pre><code>python -m bootstrap.run -o block/options/vqa2/block.yaml \\\n--view.name tensorboard\n</code></pre>\n<pre><code>tensorboard --logdir=logs/vqa2\n</code></pre>\n<p>You can use plotly and tensorboard at the same time by updating the yaml file like <a href=\"https://github.com/Cadene/bootstrap.pytorch/blob/master/bootstrap/options/mnist_plotly_tensorboard.yaml#L38\" rel=\"nofollow\">this one</a>.</p>\n<h3>Use a specific GPU</h3>\n<p>For a specific experiment:</p>\n<pre><code>CUDA_VISIBLE_DEVICES=0 python -m boostrap.run -o block/options/vqa2/block.yaml\n</code></pre>\n<p>For the current terminal session:</p>\n<pre><code>export CUDA_VISIBLE_DEVICES=0\n</code></pre>\n<h3>Overwrite an option</h3>\n<p>The boostrap.pytorch framework makes it easy to overwrite a hyperparameter. In this example, we run an experiment with a non-default learning rate. Thus, I also overwrite the experiment directory path:</p>\n<pre><code>python -m bootstrap.run -o block/options/vqa2/block.yaml \\\n--optimizer.lr 0.0003 \\\n--exp.dir logs/vqa2/block_lr,0.0003\n</code></pre>\n<h3>Resume training</h3>\n<p>If a problem occurs, it is easy to resume the last epoch by specifying the options file from the experiment directory while overwritting the <code>exp.resume</code> option (default is None):</p>\n<pre><code>python -m bootstrap.run -o logs/vqa2/block/options.yaml \\\n--exp.resume last\n</code></pre>\n<h3>Web API</h3>\n<pre><code>TODO\n</code></pre>\n<h3>Extract your own image features</h3>\n<pre><code>TODO\n</code></pre>\n<h2>Citation</h2>\n<pre><code>@InProceedings{BenYounes_2019_AAAI,\n    author = {Ben-Younes, Hedi and Cadene, Remi and Thome, Nicolas and Cord, Matthieu},\n    title = {BLOCK: {B}ilinear {S}uperdiagonal {F}usion for {V}isual {Q}uestion {A}nswering and {V}isual {R}elationship {D}etection},\n    booktitle = {The Thirty-Third AAAI Conference on Artificial Intelligence},\n    year = {2019},\n    url = {http://remicadene.com/pdfs/paper_aaai2019.pdf}\n}\n</code></pre>\n<h2>Poster</h2>\n<p align=\"center\">\n    <a href=\"http://remicadene.com/pdfs/poster_aaai2019.pdf\" rel=\"nofollow\"><img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e18eff3a0750424f82bc0eba6983b694a2f03073/68747470733a2f2f6769746875622e636f6d2f436164656e652f626c6f636b2e626f6f7473747261702e7079746f7263682f626c6f622f6d61737465722f6173736574732f706f737465725f61616169323031392e706e673f7261773d74727565\" width=\"300\"></a>\n</p>\n<h2>Authors</h2>\n<p>This code was made available by <a href=\"https://twitter.com/labegne\" rel=\"nofollow\">Hedi Ben-Younes</a> (Sorbonne-Heuritech), <a href=\"http://remicadene.com\" rel=\"nofollow\">Remi Cadene</a> (Sorbonne), <a href=\"http://webia.lip6.fr/%7Ecord\" rel=\"nofollow\">Matthieu Cord</a> (Sorbonne) and <a href=\"http://cedric.cnam.fr/%7Ethomen/\" rel=\"nofollow\">Nicolas Thome</a> (CNAM).</p>\n<h2>Acknowledgment</h2>\n<p>Special thanks to the authors of <a href=\"TODO\" rel=\"nofollow\">VQA2</a>, <a href=\"TODO\" rel=\"nofollow\">TDIUC</a>, <a href=\"TODO\" rel=\"nofollow\">VisualGenome</a> and <a href=\"TODO\" rel=\"nofollow\">VRD</a>, the datasets used in this research project.</p>\n\n          </div>"}, "last_serial": 6240770, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "79653af02d5e8d4b179f6af7f7ff2701", "sha256": "f996d83b811c3f122082583b065e88a7718990cabe93a0797fe000c3aa359d8f"}, "downloads": -1, "filename": "block.bootstrap.pytorch-0.0.1-py3.7.egg", "has_sig": false, "md5_digest": "79653af02d5e8d4b179f6af7f7ff2701", "packagetype": "bdist_egg", "python_version": "3.7", "requires_python": null, "size": 85221, "upload_time": "2019-01-31T07:08:14", "upload_time_iso_8601": "2019-01-31T07:08:14.323025Z", "url": "https://files.pythonhosted.org/packages/b6/d3/92773e89a52d0e6afb7c178adb4b55dbb299dfc23d121185ab1fdb35ca7a/block.bootstrap.pytorch-0.0.1-py3.7.egg", "yanked": false}], "0.1.0": [{"comment_text": "", "digests": {"md5": "3dddcd7c5e8ffdb973b9f737f8af0ab2", "sha256": "de3804672802bb1414139018b9121a98529a4ac50083ec81b9b5a4ba433f5eb0"}, "downloads": -1, "filename": "block.bootstrap.pytorch-0.1.0.tar.gz", "has_sig": false, "md5_digest": "3dddcd7c5e8ffdb973b9f737f8af0ab2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 44152, "upload_time": "2019-01-31T07:08:16", "upload_time_iso_8601": "2019-01-31T07:08:16.710867Z", "url": "https://files.pythonhosted.org/packages/58/6f/933cab05f0738e081a5b42ef77a21af8ab1f6d16a018b9b4e3f90b11ec52/block.bootstrap.pytorch-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "af65e75cd58ea89078644ba93100638b", "sha256": "efea6f318e2ae2b91de292ebe0214bd1c51e4689f6127ced5d10cd816416be20"}, "downloads": -1, "filename": "block.bootstrap.pytorch-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "af65e75cd58ea89078644ba93100638b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 84708, "upload_time": "2019-02-12T14:13:39", "upload_time_iso_8601": "2019-02-12T14:13:39.575597Z", "url": "https://files.pythonhosted.org/packages/b5/25/7c262053454692cbb12cbe1fdbb0eeb3f90863bb46ca329ef7432eb26375/block.bootstrap.pytorch-0.1.1-py3-none-any.whl", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "f86fa1b17476b0af3e67e6eb8b017cb4", "sha256": "b60ca28487b7c0370919e2d1980bf87b91007b8927986d416d58faaf9da5fed4"}, "downloads": -1, "filename": "block.bootstrap.pytorch-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "f86fa1b17476b0af3e67e6eb8b017cb4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 83873, "upload_time": "2019-02-12T14:30:52", "upload_time_iso_8601": "2019-02-12T14:30:52.211036Z", "url": "https://files.pythonhosted.org/packages/c1/ec/e99d3defe598007aa2a919e50bd97c2eac425400dff7122643505daabfa9/block.bootstrap.pytorch-0.1.3-py3-none-any.whl", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "43f9030854354bed6be8c12b4da63f95", "sha256": "8f924715a32b463d00905da99a43e9daafd14a321cb34f0b8d7520ac419ed7f2"}, "downloads": -1, "filename": "block.bootstrap.pytorch-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "43f9030854354bed6be8c12b4da63f95", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 92816, "upload_time": "2019-03-18T19:05:48", "upload_time_iso_8601": "2019-03-18T19:05:48.880543Z", "url": "https://files.pythonhosted.org/packages/9d/29/aeed493a21c860ac12452a2817d06951b89b67d065cffb67983f73c7fd1a/block.bootstrap.pytorch-0.1.4-py3-none-any.whl", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "3f46557fc5c1ad1af4939540e708114b", "sha256": "e42395d4d297e4b269a141e96e479835e3851e6b721d11451d1968057806db20"}, "downloads": -1, "filename": "block.bootstrap.pytorch-0.1.5-py3-none-any.whl", "has_sig": false, "md5_digest": "3f46557fc5c1ad1af4939540e708114b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 51548, "upload_time": "2019-06-26T22:48:55", "upload_time_iso_8601": "2019-06-26T22:48:55.300699Z", "url": "https://files.pythonhosted.org/packages/bd/a9/3485173981514cbd99b21ec7a39e47383ebe1d6985b6a8e442117d860bfd/block.bootstrap.pytorch-0.1.5-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4018b34e27d4d42056948eb2c9697fc2", "sha256": "9183aae7edccb52b971b369f783ae54f6a38bb528fd648849a8d97cdb8071dee"}, "downloads": -1, "filename": "block.bootstrap.pytorch-0.1.5.tar.gz", "has_sig": false, "md5_digest": "4018b34e27d4d42056948eb2c9697fc2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 51999, "upload_time": "2019-06-26T22:48:57", "upload_time_iso_8601": "2019-06-26T22:48:57.563891Z", "url": "https://files.pythonhosted.org/packages/42/d7/3a827cfd44f6a8ff747aabad7b3d518e1e216a1151e8a4fc3ab06b9a3467/block.bootstrap.pytorch-0.1.5.tar.gz", "yanked": false}], "0.1.6": [{"comment_text": "", "digests": {"md5": "980d254d839f41e410035ac72d2020af", "sha256": "91d137248c23970e058195ae20531e83257d3cbca83278e21faafac96f4000fb"}, "downloads": -1, "filename": "block.bootstrap.pytorch-0.1.6.tar.gz", "has_sig": false, "md5_digest": "980d254d839f41e410035ac72d2020af", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 51939, "upload_time": "2019-12-04T10:02:13", "upload_time_iso_8601": "2019-12-04T10:02:13.822294Z", "url": "https://files.pythonhosted.org/packages/3a/b3/45138c54b7263f460e758f191632c40483fff1d47aa34ed2eb390abb8532/block.bootstrap.pytorch-0.1.6.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "980d254d839f41e410035ac72d2020af", "sha256": "91d137248c23970e058195ae20531e83257d3cbca83278e21faafac96f4000fb"}, "downloads": -1, "filename": "block.bootstrap.pytorch-0.1.6.tar.gz", "has_sig": false, "md5_digest": "980d254d839f41e410035ac72d2020af", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 51939, "upload_time": "2019-12-04T10:02:13", "upload_time_iso_8601": "2019-12-04T10:02:13.822294Z", "url": "https://files.pythonhosted.org/packages/3a/b3/45138c54b7263f460e758f191632c40483fff1d47aa34ed2eb390abb8532/block.bootstrap.pytorch-0.1.6.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:37:03 2020"}