{"info": {"author": "Kartik Ramasubramanian", "author_email": "r.kartik@berkeley.edu", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6"], "description": "Project Background\n==================\n\npython package that helps data engineers and data scientists accelerate data-pipeline development\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe goal of this python project is to build a bunch of wrappers that can\nbe reused for building data pipelines from - - Relational databases:\npostgres, mysql, greenplum, redshift, etc. - NOSQL databases: hive,\nmongo, etc. - messaging sources and caches: kafka, redis, rabbitmq, etc.\n- cloud service providers: salesforce, mixpanel, jira, google-drive,\ndelighted, wootric, etc.\n\nInstallation\n------------\n\nThere are 3 ways to install dattasa package -\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n1) Easiest way is to install from pypi using pip\n\n::\n\n    pip install dattasa\n\n2) Download from github and build from scratch\n\n::\n\n    git clone git@github.com:kartikra/dattasa.git\n    cd dattasa\n    python setup.py build\n    python setup.py clean\n    python setup.py install\n\n3) Download from github and install using pip\n\n::\n\n    git clone git@github.com:kartikra/dattasa.git\n    cd dattasa\n    pip install -e .\n    pip install -U -e . (if upgrading)\n\nConfig Files\n------------\n\nBy default dattasa expects the config files to be in the mode directory\nof user. These can be overridden. See links to sample code in README\nfile below to find out more. There are 2 yaml config files -\ndatabase.yaml - conists of database credentials and api keys needed for\nmaking connection. see `sample database\nconfig <documentation/database.yaml>`__ - ftpsites.yaml - Needed for\nperforming sftp transfers. see `sample ftpsites\nconfig <documentation/ftpsites.yaml>`__\n\nEnvironment Variables\n---------------------\n\ndattasa package relies on the following environment variables. Make sure\nto set these in your bash profile - GPLOAD_HOME: Path to gpload package\n(needed only if using gpload utilities for greenplum or redshift) -\nPROJECT_HOME: Path to python project directory -\nPROJECT_HOME/python_bash_scripts: python scripts to invoke gpload\n(needed only if using gpload utilities for greenplum or redshift) -\nSQL_DIR: Place to keep all sql scripts - TEMP_DIR: All temp files\ncreated in this folder - LOG_DIR: All log files are created in this\nfolder\n\nDescription of classes\n----------------------\n\nv1.0 of the package comprises of the following classes. Please see link\nto sample code for details on how to use each of them.\n\n+----------------+----------------------------------+------------------+\n| class          | Description                      | Sample Code      |\n+================+==================================+==================+\n| environment    | Lets you source all the os       | `see first row   |\n|                | environment variables            | in mongo         |\n|                |                                  | example <documen |\n|                |                                  | tation/mongo_exa |\n|                |                                  | mple.ipynb>`__   |\n+----------------+----------------------------------+------------------+\n| postgres_clien | Lets you use psql and gpload     | `sample postgres |\n| t              | utilities provided by `pivotal   | code <documentat |\n|                | greenplum <https://gpdb.docs.piv | ion/postgres_cli |\n|                | otal.io/4350/common/client-docs- | ent.ipynb>`__    |\n|                | unix.html>`__.                   |                  |\n|                | Make connections to postgres /   |                  |\n|                | greenplum database using         |                  |\n|                | pyscopg2 or sqlalchemy.Use the   |                  |\n|                | connections to interact with     |                  |\n|                | database in interactive program  |                  |\n|                | or run queries from a sql file   |                  |\n|                | using the connection             |                  |\n+----------------+----------------------------------+------------------+\n| greenplum_clie | Lets you use psql and gpload     | `sample          |\n| nt             | utilities provided by `pivotal   | greenplum        |\n| (inherits      | greenplum <https://gpdb.docs.piv | code <documentat |\n| postgres_clien | otal.io/4350/common/client-docs- | ion/greenplum_cl |\n| t)             | unix.html>`__.                   | ient.ipynb>`__   |\n|                | Make connections to postgres /   |                  |\n|                | greenplum database using         |                  |\n|                | pyscopg2 or sqlalchemy.Use the   |                  |\n|                | connections to interact with     |                  |\n|                | database in interactive program  |                  |\n|                | or run queries from a sql file   |                  |\n|                | using the connection             |                  |\n+----------------+----------------------------------+------------------+\n| mysql_client   | Lets you use mysql and other     | `sample mysql    |\n|                | methods provided by PyMySQL      | code <documentat |\n|                | Package                          | ion/mysql_client |\n|                |                                  | .ipynb>`__       |\n+----------------+----------------------------------+------------------+\n| file_processor | Create sftp connection using     | `see file        |\n|                | `paramiko <https://github.com/pa | processing       |\n|                | ramiko/paramiko.git>`__          | example <documen |\n|                | package. Other file              | tation/file_proc |\n|                | manipulations like row_count,    | essing.ipynb>`__ |\n|                | encryption, archive (File Class) |                  |\n+----------------+----------------------------------+------------------+\n| notification   | Send email notifications         |                  |\n+----------------+----------------------------------+------------------+\n| mongo_client   | Load data to mongodb using bulk  | `see mongo       |\n|                | load. Run java script queries    | example <documen |\n|                |                                  | tation/mongo_exa |\n|                |                                  | mple.ipynb>`__   |\n+----------------+----------------------------------+------------------+\n| redis_client   | Read data from a redis cache or  | `see redis       |\n|                | load a redis cache               | example <documen |\n|                |                                  | tation/redis_exa |\n|                |                                  | mple.ipynb>`__   |\n+----------------+----------------------------------+------------------+\n| kafka_system   | Currently allows Publisher and   | `see kafka       |\n|                | Consumer to use kafka in batch   | example <documen |\n|                | mode                             | tation/kafka_exa |\n|                |                                  | mple.ipynb>`__   |\n+----------------+----------------------------------+------------------+\n| rabbitmq_syste | Currently has Publisher to       |                  |\n| m              | publish messages in rabbitmq     |                  |\n+----------------+----------------------------------+------------------+\n| mixpanel_clien | Connect to mixpanel api and      | `see mixpnael    |\n| t              | fetch data using jql or export   | section in api   |\n|                | raw events data. `mixpanel api   | example <documen |\n|                | documentation <https://mixpanel. | tation/api_examp |\n|                | com/help/reference/jql/api-refer | les.ipynb>`__    |\n|                | ence>`__                         |                  |\n+----------------+----------------------------------+------------------+\n| salesforce_cli | Create a connection to           | `see salesforce  |\n| ent            | salesforce using                 | section in api   |\n|                | `simple_salesforce <https://gith | example <documen |\n|                | ub.com/simple-salesforce/simple- | tation/api_examp |\n|                | salesforce>`__                   | les.ipynb>`__    |\n|                | package                          |                  |\n+----------------+----------------------------------+------------------+\n| delighted_clie | Get nps scores and survey        | `see delighted   |\n| nt             | responses from delighted.\\ `api  | section in api   |\n|                | documentation <https://delighted | example <documen |\n|                | .com/docs/api/>`__               | tation/api_examp |\n|                |                                  | les.ipynb>`__    |\n+----------------+----------------------------------+------------------+\n| wootric_client | Gets nps scores and survey       | `see wootric     |\n|                | responses from wootric.\\ `api    | section in api   |\n|                | documentation <http://docs.wootr | example <documen |\n|                | ic.com/api>`__                   | tation/api_examp |\n|                |                                  | les.ipynb>`__    |\n+----------------+----------------------------------+------------------+\n| dag_controller | Functions needed to integrate    |                  |\n|                | this package within an airflow   |                  |\n|                | dag. `airflow                    |                  |\n|                | documentation <https://airflow.a |                  |\n|                | pache.org/>`__                   |                  |\n|                | and `github                      |                  |\n|                | project <https://github.com/apac |                  |\n|                | he/incubator-airflow>`__         |                  |\n+----------------+----------------------------------+------------------+\n\ndata_pipeline class\n~~~~~~~~~~~~~~~~~~~\n\nThis is the main class that\u2019s accessible to other projects. The data\npipeline consists of data from components and API. Each object of\ndata-processor can use individual data streams and process them\ndata_pipeline decides which modules to call based on type of database\n(as defined in config file). data_pipeline comprises of 3 classes -\nDataComponent : Each database connection is considered to be\ndata-component object.See examples for postgres, mysql, greenplum, etc\nabove - APICall : Each api call is an apicall object. See examples for\nmixpanel, delighted, salesforce and wootric above - DataProcessor :\ntransfers and loads data between data components. `see\nexamples <documentation/data_processor.ipynb>`__\n\nAdding ipython notebook files to github\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nUse git lfs See\n`documentation <https://git-lfs.github.com/?utm_source=github_site&utm_medium=jupyter_blog_link&utm_campaign=gitlfs>`__\n\n-  if using mac install git-lfs using brew ``brew install git-lfs``\n-  install lfs ``git lfs install``\n-  track ipynb files in your project. go to the project folder and do\n   ``git lfs track \"*.psd\"``\n-  add ``.*ipynb_checkpoints/`` to .gitignore file\n-  Finally add .gitattributes file ``git add .gitatttributes``\n\nDeploying code in pypi\n~~~~~~~~~~~~~~~~~~~~~~\n\n-  build the code:\n   ``python setup.py build && python setup.py clean && python setup.py install``\n-  push to pypitest : ``python setup.py sdist upload -r pypitest``\n-  push to pypi prod : ``python setup.py sdist upload -r pypi``\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/kartikra/dattasa", "keywords": "greenplum postgres mysql mongodb kafka redis rabbitmq salesforce mixpanel delighted wootric data pipeline", "license": "GNU", "maintainer": "", "maintainer_email": "", "name": "dattasa", "package_url": "https://pypi.org/project/dattasa/", "platform": "", "project_url": "https://pypi.org/project/dattasa/", "project_urls": {"Homepage": "https://github.com/kartikra/dattasa"}, "release_url": "https://pypi.org/project/dattasa/1.1/", "requires_dist": null, "requires_python": "", "summary": "Python wrapper for connecting to postgres/greenplum, mysql, mongodb, kafka, redis, mixpanel and salesforce.        Also included are modules for performing secure file transfer and sourcing environment variables.", "version": "1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"python-package-that-helps-data-engineers-and-data-scientists-accelerate-data-pipeline-development\">\n<h2>python package that helps data engineers and data scientists accelerate data-pipeline development</h2>\n<p>The goal of this python project is to build a bunch of wrappers that can\nbe reused for building data pipelines from - - Relational databases:\npostgres, mysql, greenplum, redshift, etc. - NOSQL databases: hive,\nmongo, etc. - messaging sources and caches: kafka, redis, rabbitmq, etc.\n- cloud service providers: salesforce, mixpanel, jira, google-drive,\ndelighted, wootric, etc.</p>\n<div id=\"installation\">\n<h3>Installation</h3>\n</div>\n</div>\n<div id=\"there-are-3-ways-to-install-dattasa-package\">\n<h2>There are 3 ways to install dattasa package -</h2>\n<ol>\n<li>Easiest way is to install from pypi using pip</li>\n</ol>\n<pre>pip install dattasa\n</pre>\n<ol>\n<li>Download from github and build from scratch</li>\n</ol>\n<pre>git clone git@github.com:kartikra/dattasa.git\ncd dattasa\npython setup.py build\npython setup.py clean\npython setup.py install\n</pre>\n<ol>\n<li>Download from github and install using pip</li>\n</ol>\n<pre>git clone git@github.com:kartikra/dattasa.git\ncd dattasa\npip install -e .\npip install -U -e . (if upgrading)\n</pre>\n<div id=\"config-files\">\n<h3>Config Files</h3>\n<p>By default dattasa expects the config files to be in the mode directory\nof user. These can be overridden. See links to sample code in README\nfile below to find out more. There are 2 yaml config files -\ndatabase.yaml - conists of database credentials and api keys needed for\nmaking connection. see <a href=\"documentation/database.yaml\" rel=\"nofollow\">sample database\nconfig</a> - ftpsites.yaml - Needed for\nperforming sftp transfers. see <a href=\"documentation/ftpsites.yaml\" rel=\"nofollow\">sample ftpsites\nconfig</a></p>\n</div>\n<div id=\"environment-variables\">\n<h3>Environment Variables</h3>\n<p>dattasa package relies on the following environment variables. Make sure\nto set these in your bash profile - GPLOAD_HOME: Path to gpload package\n(needed only if using gpload utilities for greenplum or redshift) -\nPROJECT_HOME: Path to python project directory -\nPROJECT_HOME/python_bash_scripts: python scripts to invoke gpload\n(needed only if using gpload utilities for greenplum or redshift) -\nSQL_DIR: Place to keep all sql scripts - TEMP_DIR: All temp files\ncreated in this folder - LOG_DIR: All log files are created in this\nfolder</p>\n</div>\n<div id=\"description-of-classes\">\n<h3>Description of classes</h3>\n<p>v1.0 of the package comprises of the following classes. Please see link\nto sample code for details on how to use each of them.</p>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>class</th>\n<th>Description</th>\n<th>Sample Code</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>environment</td>\n<td>Lets you source all the os\nenvironment variables</td>\n<td><a href=\"documentation/mongo_example.ipynb\" rel=\"nofollow\">see first row\nin mongo\nexample</a></td>\n</tr>\n<tr><td>postgres_clien\nt</td>\n<td>Lets you use psql and gpload\nutilities provided by <a href=\"https://gpdb.docs.pivotal.io/4350/common/client-docs-unix.html\" rel=\"nofollow\">pivotal\ngreenplum</a>.\nMake connections to postgres /\ngreenplum database using\npyscopg2 or sqlalchemy.Use the\nconnections to interact with\ndatabase in interactive program\nor run queries from a sql file\nusing the connection</td>\n<td><a href=\"documentation/postgres_client.ipynb\" rel=\"nofollow\">sample postgres\ncode</a></td>\n</tr>\n<tr><td>greenplum_clie\nnt\n(inherits\npostgres_clien\nt)</td>\n<td>Lets you use psql and gpload\nutilities provided by <a href=\"https://gpdb.docs.pivotal.io/4350/common/client-docs-unix.html\" rel=\"nofollow\">pivotal\ngreenplum</a>.\nMake connections to postgres /\ngreenplum database using\npyscopg2 or sqlalchemy.Use the\nconnections to interact with\ndatabase in interactive program\nor run queries from a sql file\nusing the connection</td>\n<td><a href=\"documentation/greenplum_client.ipynb\" rel=\"nofollow\">sample\ngreenplum\ncode</a></td>\n</tr>\n<tr><td>mysql_client</td>\n<td>Lets you use mysql and other\nmethods provided by PyMySQL\nPackage</td>\n<td><a href=\"documentation/mysql_client.ipynb\" rel=\"nofollow\">sample mysql\ncode</a></td>\n</tr>\n<tr><td>file_processor</td>\n<td>Create sftp connection using\n<a href=\"https://github.com/paramiko/paramiko.git\" rel=\"nofollow\">paramiko</a>\npackage. Other file\nmanipulations like row_count,\nencryption, archive (File Class)</td>\n<td><a href=\"documentation/file_processing.ipynb\" rel=\"nofollow\">see file\nprocessing\nexample</a></td>\n</tr>\n<tr><td>notification</td>\n<td>Send email notifications</td>\n<td>\u00a0</td>\n</tr>\n<tr><td>mongo_client</td>\n<td>Load data to mongodb using bulk\nload. Run java script queries</td>\n<td><a href=\"documentation/mongo_example.ipynb\" rel=\"nofollow\">see mongo\nexample</a></td>\n</tr>\n<tr><td>redis_client</td>\n<td>Read data from a redis cache or\nload a redis cache</td>\n<td><a href=\"documentation/redis_example.ipynb\" rel=\"nofollow\">see redis\nexample</a></td>\n</tr>\n<tr><td>kafka_system</td>\n<td>Currently allows Publisher and\nConsumer to use kafka in batch\nmode</td>\n<td><a href=\"documentation/kafka_example.ipynb\" rel=\"nofollow\">see kafka\nexample</a></td>\n</tr>\n<tr><td>rabbitmq_syste\nm</td>\n<td>Currently has Publisher to\npublish messages in rabbitmq</td>\n<td>\u00a0</td>\n</tr>\n<tr><td>mixpanel_clien\nt</td>\n<td>Connect to mixpanel api and\nfetch data using jql or export\nraw events data. <a href=\"https://mixpanel.com/help/reference/jql/api-reference\" rel=\"nofollow\">mixpanel api\ndocumentation</a></td>\n<td><a href=\"documentation/api_examples.ipynb\" rel=\"nofollow\">see mixpnael\nsection in api\nexample</a></td>\n</tr>\n<tr><td>salesforce_cli\nent</td>\n<td>Create a connection to\nsalesforce using\n<a href=\"https://github.com/simple-salesforce/simple-salesforce\" rel=\"nofollow\">simple_salesforce</a>\npackage</td>\n<td><a href=\"documentation/api_examples.ipynb\" rel=\"nofollow\">see salesforce\nsection in api\nexample</a></td>\n</tr>\n<tr><td>delighted_clie\nnt</td>\n<td>Get nps scores and survey\nresponses from delighted.<a href=\"https://delighted.com/docs/api/\" rel=\"nofollow\">api\ndocumentation</a></td>\n<td><a href=\"documentation/api_examples.ipynb\" rel=\"nofollow\">see delighted\nsection in api\nexample</a></td>\n</tr>\n<tr><td>wootric_client</td>\n<td>Gets nps scores and survey\nresponses from wootric.<a href=\"http://docs.wootric.com/api\" rel=\"nofollow\">api\ndocumentation</a></td>\n<td><a href=\"documentation/api_examples.ipynb\" rel=\"nofollow\">see wootric\nsection in api\nexample</a></td>\n</tr>\n<tr><td>dag_controller</td>\n<td>Functions needed to integrate\nthis package within an airflow\ndag. <a href=\"https://airflow.apache.org/\" rel=\"nofollow\">airflow\ndocumentation</a>\nand <a href=\"https://github.com/apache/incubator-airflow\" rel=\"nofollow\">github\nproject</a></td>\n<td>\u00a0</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<div id=\"data-pipeline-class\">\n<h2>data_pipeline class</h2>\n<p>This is the main class that\u2019s accessible to other projects. The data\npipeline consists of data from components and API. Each object of\ndata-processor can use individual data streams and process them\ndata_pipeline decides which modules to call based on type of database\n(as defined in config file). data_pipeline comprises of 3 classes -\nDataComponent : Each database connection is considered to be\ndata-component object.See examples for postgres, mysql, greenplum, etc\nabove - APICall : Each api call is an apicall object. See examples for\nmixpanel, delighted, salesforce and wootric above - DataProcessor :\ntransfers and loads data between data components. <a href=\"documentation/data_processor.ipynb\" rel=\"nofollow\">see\nexamples</a></p>\n</div>\n<div id=\"adding-ipython-notebook-files-to-github\">\n<h2>Adding ipython notebook files to github</h2>\n<p>Use git lfs See\n<a href=\"https://git-lfs.github.com/?utm_source=github_site&amp;utm_medium=jupyter_blog_link&amp;utm_campaign=gitlfs\" rel=\"nofollow\">documentation</a></p>\n<ul>\n<li>if using mac install git-lfs using brew <tt>brew install <span class=\"pre\">git-lfs</span></tt></li>\n<li>install lfs <tt>git lfs install</tt></li>\n<li>track ipynb files in your project. go to the project folder and do\n<tt>git lfs track <span class=\"pre\">\"*.psd\"</span></tt></li>\n<li>add <tt>.*ipynb_checkpoints/</tt> to .gitignore file</li>\n<li>Finally add .gitattributes file <tt>git add .gitatttributes</tt></li>\n</ul>\n</div>\n<div id=\"deploying-code-in-pypi\">\n<h2>Deploying code in pypi</h2>\n<ul>\n<li>build the code:\n<tt>python setup.py build &amp;&amp; python setup.py clean &amp;&amp; python setup.py install</tt></li>\n<li>push to pypitest : <tt>python setup.py sdist upload <span class=\"pre\">-r</span> pypitest</tt></li>\n<li>push to pypi prod : <tt>python setup.py sdist upload <span class=\"pre\">-r</span> pypi</tt></li>\n</ul>\n</div>\n\n          </div>"}, "last_serial": 3571594, "releases": {"1.1": [{"comment_text": "", "digests": {"md5": "cea52ebbddd232bda6bb7024660764b0", "sha256": "2ca8d0b5c3156c3d46b249eaefb18024e8da958c452f51497bdbe719a2efb9c9"}, "downloads": -1, "filename": "dattasa-1.1.tar.gz", "has_sig": false, "md5_digest": "cea52ebbddd232bda6bb7024660764b0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29469, "upload_time": "2018-02-11T07:41:32", "upload_time_iso_8601": "2018-02-11T07:41:32.632983Z", "url": "https://files.pythonhosted.org/packages/71/82/73224de19e0397d973eec1ab20c3346b95783ffe41e2d8a4c74a88e94970/dattasa-1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "cea52ebbddd232bda6bb7024660764b0", "sha256": "2ca8d0b5c3156c3d46b249eaefb18024e8da958c452f51497bdbe719a2efb9c9"}, "downloads": -1, "filename": "dattasa-1.1.tar.gz", "has_sig": false, "md5_digest": "cea52ebbddd232bda6bb7024660764b0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 29469, "upload_time": "2018-02-11T07:41:32", "upload_time_iso_8601": "2018-02-11T07:41:32.632983Z", "url": "https://files.pythonhosted.org/packages/71/82/73224de19e0397d973eec1ab20c3346b95783ffe41e2d8a4c74a88e94970/dattasa-1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:40:03 2020"}