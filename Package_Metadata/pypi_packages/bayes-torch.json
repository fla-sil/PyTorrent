{"info": {"author": "yiyuezhuo", "author_email": "616271835@163.com", "bugtrack_url": null, "classifiers": [], "description": "        # Bayes-torch: A light weight bayes inference framework\n\n        \n\n        Though there're a lot of bayes inference modeling lib/language\n\n        such as [stan](https://github.com/stan-dev/stan), \n\n        [edward](https://github.com/blei-lab/edward) (tensorflow) \n\n        [pymc](https://github.com/pymc-devs/pymc3) (theano), \n\n        [pyro](https://github.com/uber/pyro) (pytorch) the relation between\n\n        their computation ground and absract high API is awkward.\n\n        \n\n        So the project is found to implment stan-like API on the flexible\n\n        autograd library, pytorch. It's a light-weight framework, you will\n\n        directly write joint likelihood function to run inference instead of\n\n        fake sampling statment in stan, pymc or ugly style in Edward, \n\n        weired namebinding in pyro.\n\n        \n\n        ## Example\n\n        \n\n        We can implement following stan model as such:\n\n        \n\n        ```\n\n        data {\n\n            int<lower=1> N;\n\n            real y[N];\n\n        }\n\n        parameters {\n\n            real mu;\n\n        }\n\n        model {\n\n            y ~ normal(mu, 1);\n\n        }\n\n        ```\n\n        \n\n        torch-bayes model code:\n\n        \n\n        ```python\n\n        mu = Parameter(0.0) # optimizing/vb/sampling init value\n\n        sigma = Data(1.0)\n\n        X = Data(_X)\n\n        \n\n        def target():\n\n            target = Normal(mu,sigma).log_prob(X).sum(0)\n\n            return target\n\n        ```\n\n        \n\n        Full code comparing two framework:\n\n        \n\n        ```python\n\n        from bayestorch import Parameter,Data,optimizing,vb,sampling,reset\n\n        import torch\n\n        from torch.distributions import Normal\n\n        \n\n        _X = torch.arange(10)\n\n        print(_X.mean())\n\n        \n\n        # torch-bayes model\n\n        \n\n        mu = Parameter(0.0)\n\n        sigma = Data(1.0)\n\n        X = Data(_X)\n\n        \n\n        def target():\n\n            target = Normal(mu,sigma).log_prob(X).sum(0)\n\n            return target\n\n        \n\n        optimizing(target)\n\n        print(f'optimizing: mu={mu.data}')\n\n        \n\n        res = vb(target)\n\n        q_mu = res.params['mu']\n\n        print(f'vb mu={q_mu[\"loc\"]} omega={q_mu[\"omega\"]} sigma={torch.exp(q_mu[\"omega\"])}')\n\n        \n\n        reset()\n\n        \n\n        mu = Parameter(0.0)\n\n        \n\n        res = vb(target, q_size = 10, n_epoch=200)\n\n        q_mu = res.params['mu']\n\n        print(f'vb mu={q_mu[\"loc\"]} omega={q_mu[\"omega\"]} sigma={torch.exp(q_mu[\"omega\"])}')\n\n        \n\n        trace = sampling(target,trace_length=300)\n\n        mu_trace = torch.tensor([t['mu'].item() for t in trace])\n\n        print('sampling: mu={} sigma={}'.format(torch.mean(mu_trace), torch.std(mu_trace)))\n\n        \n\n        \n\n        # stan model\n\n        \n\n        \n\n        import numpy as np\n\n        import pystan\n\n        stan_code = '''\n\n        data {\n\n            int<lower=1> N;\n\n            real y[N];\n\n        }\n\n        parameters {\n\n            real mu;\n\n        }\n\n        model {\n\n            y ~ normal(mu, 1);\n\n        }\n\n        '''\n\n        sm = pystan.StanModel(model_code = stan_code)\n\n        \n\n        _X = _X.numpy()\n\n        res2 = sm.optimizing(data = dict(N = len(_X), y = _X))\n\n        print(f'optimizing(stan): mu={res2[\"mu\"]}')\n\n        res3 = sm.vb(data = dict(N = len(_X), y = _X))\n\n        res3a=np.array(res3['sampler_params'])\n\n        print(f'vb(stan): mu={res3a[0,:].mean()} sigma={res3a[0,:].std()}')\n\n        \n\n        ```\n\n        \n\n        Enemy location detecting example:\n\n        \n\n        ```python\n\n        # model\n\n        friend = Data(friend_point)\n\n        battle = Data(battle_point)\n\n        enemy = Parameter(enemy_point) # set real value as init value, though maybe a randomed init is more proper\n\n        \n\n        logPC = Data(_logPC)\n\n        \n\n        conflict_threshold = 0.2\n\n        distance_threshold = 1.0\n\n        tense = 10.0\n\n        alpha = 5.0\n\n        prior_threshold = 5.0\n\n        prior_tense = 5.0\n\n        \n\n        def target():\n\n            friend_enemy = torch.cat((friend, enemy),0)\n\n            \n\n            distance = cdist(battle, friend_enemy).min(dim=1)[0]\n\n            \n\n            \n\n            mu = torch.stack([friend.mean(0),enemy.mean(0)],0)\n\n            sd = torch.stack([friend.std(0),enemy.std(0)],0)\n\n            \n\n            conflict = torch.exp(norm_naive_bayes_predict(battle, mu, sd, logPC)).prod(1)\n\n            p = soft_cut_ge(conflict,conflict_threshold, tense = tense) *  \\\n\n                soft_cut_le(distance, distance_threshold, tense = tense)\n\n            \n\n            target= torch.log(p).sum(0)\n\n            return target\n\n        \n\n        def target2():\n\n            target1 = target()\n\n            # location prior\n\n            target2 = target1 + enemy.sum(0).sum(0)\n\n            return target2\n\n        ```\n\n        \n\n        <img src=\"images/example.png\">\n\n        \n\n        ## Basic principle\n\n        \n\n        bayes-torch(BT) use `target_f.__globals__` to access and change variables labeled `Parameter` or `Data`. \n\n        It implies some limit to way to introduce parameters, for example you can't define a list of `Parameter`\n\n        and expect BT can find it.\n\n        \n\n        In `optimizing`, BT run standard SGD algorithm. \n\n        In `sampling`, BT will frequently replace variable with another(same shape) using HMC. \n\n        In `vb`, BT will map a variable to a normal variational distribution object, that contain variational\n\n        parameters mu and omega (`omega = log(sigma)`). The last dimention will be used by `vb` innerly.\n\n        So code such as `sum(-1)` or `sum()`(reduce to scaler) will raise error.\n\n        \n\n        However, the thin implementation(`core.py`) only consists of 300- lines, \n\n        you can always check origin code to figure out what happen anyway.\n\n        \n\n        ## Reference\n\n        \n\n        Automatic Differentiation Variational Inference\n\n        \n\n        Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M\n\n        \n\n        https://arxiv.org/abs/1603.00788\n\n        \n\n        Fully automatic variational inference of differentiable probability models\n\n        \n\n        Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David\n\n        \n\n        http://andrewgelman.com/wp-content/uploads/2014/12/pp_workshop_nips2014.pdf\n\n        \n\n        The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.\n\n        \n\n        Hoffman, Matthew D and Gelman, Andrew\n\n        \n\n        https://arxiv.org/abs/1111.4246\nKeywords: bayes statistic scientific\nPlatform: UNKNOWN\nDescription-Content-Type: text/markdown\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/yiyuezhuo/bayes-torch", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "bayes-torch", "package_url": "https://pypi.org/project/bayes-torch/", "platform": "", "project_url": "https://pypi.org/project/bayes-torch/", "project_urls": {"Bug Reports": "https://github.com/yiyuezhuo/bayes-torch/issues", "Homepage": "https://github.com/yiyuezhuo/bayes-torch", "Source": "https://github.com/yiyuezhuo/bayes-torch"}, "release_url": "https://pypi.org/project/bayes-torch/0.0.4/", "requires_dist": null, "requires_python": "", "summary": "A light weight bayes inference framework based on pytorch.", "version": "0.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            # Bayes-torch: A light weight bayes inference framework<br><br>        <br><br>        Though there're a lot of bayes inference modeling lib/language<br><br>        such as [stan](https://github.com/stan-dev/stan), <br><br>        [edward](https://github.com/blei-lab/edward) (tensorflow) <br><br>        [pymc](https://github.com/pymc-devs/pymc3) (theano), <br><br>        [pyro](https://github.com/uber/pyro) (pytorch) the relation between<br><br>        their computation ground and absract high API is awkward.<br><br>        <br><br>        So the project is found to implment stan-like API on the flexible<br><br>        autograd library, pytorch. It's a light-weight framework, you will<br><br>        directly write joint likelihood function to run inference instead of<br><br>        fake sampling statment in stan, pymc or ugly style in Edward, <br><br>        weired namebinding in pyro.<br><br>        <br><br>        ## Example<br><br>        <br><br>        We can implement following stan model as such:<br><br>        <br><br>        ```<br><br>        data {<br><br>            int&lt;lower=1&gt; N;<br><br>            real y[N];<br><br>        }<br><br>        parameters {<br><br>            real mu;<br><br>        }<br><br>        model {<br><br>            y ~ normal(mu, 1);<br><br>        }<br><br>        ```<br><br>        <br><br>        torch-bayes model code:<br><br>        <br><br>        ```python<br><br>        mu = Parameter(0.0) # optimizing/vb/sampling init value<br><br>        sigma = Data(1.0)<br><br>        X = Data(_X)<br><br>        <br><br>        def target():<br><br>            target = Normal(mu,sigma).log_prob(X).sum(0)<br><br>            return target<br><br>        ```<br><br>        <br><br>        Full code comparing two framework:<br><br>        <br><br>        ```python<br><br>        from bayestorch import Parameter,Data,optimizing,vb,sampling,reset<br><br>        import torch<br><br>        from torch.distributions import Normal<br><br>        <br><br>        _X = torch.arange(10)<br><br>        print(_X.mean())<br><br>        <br><br>        # torch-bayes model<br><br>        <br><br>        mu = Parameter(0.0)<br><br>        sigma = Data(1.0)<br><br>        X = Data(_X)<br><br>        <br><br>        def target():<br><br>            target = Normal(mu,sigma).log_prob(X).sum(0)<br><br>            return target<br><br>        <br><br>        optimizing(target)<br><br>        print(f'optimizing: mu={mu.data}')<br><br>        <br><br>        res = vb(target)<br><br>        q_mu = res.params['mu']<br><br>        print(f'vb mu={q_mu[\"loc\"]} omega={q_mu[\"omega\"]} sigma={torch.exp(q_mu[\"omega\"])}')<br><br>        <br><br>        reset()<br><br>        <br><br>        mu = Parameter(0.0)<br><br>        <br><br>        res = vb(target, q_size = 10, n_epoch=200)<br><br>        q_mu = res.params['mu']<br><br>        print(f'vb mu={q_mu[\"loc\"]} omega={q_mu[\"omega\"]} sigma={torch.exp(q_mu[\"omega\"])}')<br><br>        <br><br>        trace = sampling(target,trace_length=300)<br><br>        mu_trace = torch.tensor([t['mu'].item() for t in trace])<br><br>        print('sampling: mu={} sigma={}'.format(torch.mean(mu_trace), torch.std(mu_trace)))<br><br>        <br><br>        <br><br>        # stan model<br><br>        <br><br>        <br><br>        import numpy as np<br><br>        import pystan<br><br>        stan_code = '''<br><br>        data {<br><br>            int&lt;lower=1&gt; N;<br><br>            real y[N];<br><br>        }<br><br>        parameters {<br><br>            real mu;<br><br>        }<br><br>        model {<br><br>            y ~ normal(mu, 1);<br><br>        }<br><br>        '''<br><br>        sm = pystan.StanModel(model_code = stan_code)<br><br>        <br><br>        _X = _X.numpy()<br><br>        res2 = sm.optimizing(data = dict(N = len(_X), y = _X))<br><br>        print(f'optimizing(stan): mu={res2[\"mu\"]}')<br><br>        res3 = sm.vb(data = dict(N = len(_X), y = _X))<br><br>        res3a=np.array(res3['sampler_params'])<br><br>        print(f'vb(stan): mu={res3a[0,:].mean()} sigma={res3a[0,:].std()}')<br><br>        <br><br>        ```<br><br>        <br><br>        Enemy location detecting example:<br><br>        <br><br>        ```python<br><br>        # model<br><br>        friend = Data(friend_point)<br><br>        battle = Data(battle_point)<br><br>        enemy = Parameter(enemy_point) # set real value as init value, though maybe a randomed init is more proper<br><br>        <br><br>        logPC = Data(_logPC)<br><br>        <br><br>        conflict_threshold = 0.2<br><br>        distance_threshold = 1.0<br><br>        tense = 10.0<br><br>        alpha = 5.0<br><br>        prior_threshold = 5.0<br><br>        prior_tense = 5.0<br><br>        <br><br>        def target():<br><br>            friend_enemy = torch.cat((friend, enemy),0)<br><br>            <br><br>            distance = cdist(battle, friend_enemy).min(dim=1)[0]<br><br>            <br><br>            <br><br>            mu = torch.stack([friend.mean(0),enemy.mean(0)],0)<br><br>            sd = torch.stack([friend.std(0),enemy.std(0)],0)<br><br>            <br><br>            conflict = torch.exp(norm_naive_bayes_predict(battle, mu, sd, logPC)).prod(1)<br><br>            p = soft_cut_ge(conflict,conflict_threshold, tense = tense) *  \\<br><br>                soft_cut_le(distance, distance_threshold, tense = tense)<br><br>            <br><br>            target= torch.log(p).sum(0)<br><br>            return target<br><br>        <br><br>        def target2():<br><br>            target1 = target()<br><br>            # location prior<br><br>            target2 = target1 + enemy.sum(0).sum(0)<br><br>            return target2<br><br>        ```<br><br>        <br><br>        &lt;img src=\"images/example.png\"&gt;<br><br>        <br><br>        ## Basic principle<br><br>        <br><br>        bayes-torch(BT) use `target_f.__globals__` to access and change variables labeled `Parameter` or `Data`. <br><br>        It implies some limit to way to introduce parameters, for example you can't define a list of `Parameter`<br><br>        and expect BT can find it.<br><br>        <br><br>        In `optimizing`, BT run standard SGD algorithm. <br><br>        In `sampling`, BT will frequently replace variable with another(same shape) using HMC. <br><br>        In `vb`, BT will map a variable to a normal variational distribution object, that contain variational<br><br>        parameters mu and omega (`omega = log(sigma)`). The last dimention will be used by `vb` innerly.<br><br>        So code such as `sum(-1)` or `sum()`(reduce to scaler) will raise error.<br><br>        <br><br>        However, the thin implementation(`core.py`) only consists of 300- lines, <br><br>        you can always check origin code to figure out what happen anyway.<br><br>        <br><br>        ## Reference<br><br>        <br><br>        Automatic Differentiation Variational Inference<br><br>        <br><br>        Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M<br><br>        <br><br>        https://arxiv.org/abs/1603.00788<br><br>        <br><br>        Fully automatic variational inference of differentiable probability models<br><br>        <br><br>        Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David<br><br>        <br><br>        http://andrewgelman.com/wp-content/uploads/2014/12/pp_workshop_nips2014.pdf<br><br>        <br><br>        The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.<br><br>        <br><br>        Hoffman, Matthew D and Gelman, Andrew<br><br>        <br><br>        https://arxiv.org/abs/1111.4246<br>Keywords: bayes statistic scientific<br>Platform: UNKNOWN<br>Description-Content-Type: text/markdown<br>\n          </div>"}, "last_serial": 3981860, "releases": {"0.0.3": [{"comment_text": "", "digests": {"md5": "93a205f1dca3e33dc211eb5c80f4cfa5", "sha256": "7809e9ddb7adcd67df1df2c79cea0fea2102430b0ec4099f6084658e43d64abb"}, "downloads": -1, "filename": "bayes-torch-0.0.3.tar.gz", "has_sig": false, "md5_digest": "93a205f1dca3e33dc211eb5c80f4cfa5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6605, "upload_time": "2018-04-22T11:59:39", "upload_time_iso_8601": "2018-04-22T11:59:39.638569Z", "url": "https://files.pythonhosted.org/packages/e1/c5/f5c9c8d7f98cb961ce68d11d1f76bebe082c2bfb61a4d35605b6820e3b8d/bayes-torch-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "6b22f257a52ba6589a5f8072fae9b521", "sha256": "275f21f1edc29bf1312612810ee24d3f5fcb32fb7c277e7f574fd8251fd5329e"}, "downloads": -1, "filename": "bayes_torch-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "6b22f257a52ba6589a5f8072fae9b521", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 4140, "upload_time": "2018-06-20T15:13:19", "upload_time_iso_8601": "2018-06-20T15:13:19.135404Z", "url": "https://files.pythonhosted.org/packages/9c/e9/1ca7ad3b79f2fcbde56bd7cd1d0fe5d1afa958b79428038700e54c2cee61/bayes_torch-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "315090fdf5d8cc7356f6aa68248bcba4", "sha256": "1538db5f468fb7636450f5d4a20132fd3166a5a7b304d6ba624e8c3c7ecdbce3"}, "downloads": -1, "filename": "bayes-torch-0.0.4.tar.gz", "has_sig": false, "md5_digest": "315090fdf5d8cc7356f6aa68248bcba4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6072, "upload_time": "2018-06-20T15:13:20", "upload_time_iso_8601": "2018-06-20T15:13:20.544210Z", "url": "https://files.pythonhosted.org/packages/60/ae/39528f5d1e8fef4602b3bf01d69694fc8192700185129a4cc0c9fa7a6761/bayes-torch-0.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "6b22f257a52ba6589a5f8072fae9b521", "sha256": "275f21f1edc29bf1312612810ee24d3f5fcb32fb7c277e7f574fd8251fd5329e"}, "downloads": -1, "filename": "bayes_torch-0.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "6b22f257a52ba6589a5f8072fae9b521", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 4140, "upload_time": "2018-06-20T15:13:19", "upload_time_iso_8601": "2018-06-20T15:13:19.135404Z", "url": "https://files.pythonhosted.org/packages/9c/e9/1ca7ad3b79f2fcbde56bd7cd1d0fe5d1afa958b79428038700e54c2cee61/bayes_torch-0.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "315090fdf5d8cc7356f6aa68248bcba4", "sha256": "1538db5f468fb7636450f5d4a20132fd3166a5a7b304d6ba624e8c3c7ecdbce3"}, "downloads": -1, "filename": "bayes-torch-0.0.4.tar.gz", "has_sig": false, "md5_digest": "315090fdf5d8cc7356f6aa68248bcba4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6072, "upload_time": "2018-06-20T15:13:20", "upload_time_iso_8601": "2018-06-20T15:13:20.544210Z", "url": "https://files.pythonhosted.org/packages/60/ae/39528f5d1e8fef4602b3bf01d69694fc8192700185129a4cc0c9fa7a6761/bayes-torch-0.0.4.tar.gz", "yanked": false}], "timestamp": "Thu May  7 18:14:41 2020"}