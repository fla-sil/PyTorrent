{"info": {"author": "Ines Montani", "author_email": "ines@explosion.ai", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "<a href=\"https://explosion.ai\"><img src=\"https://explosion.ai/assets/img/logo.svg\" width=\"125\" height=\"125\" align=\"right\" /></a>\n\n# spaCy + Stanza (formerly StanfordNLP)\n\nThis package wraps the [Stanza](https://github.com/stanfordnlp/stanza)\n(formerly StanfordNLP) library, so you can use Stanford's models as a\n[spaCy](https://spacy.io) pipeline. The Stanford models achieved top accuracy in\nthe CoNLL 2017 and 2018 shared task, which involves tokenization,\npart-of-speech tagging, morphological analysis, lemmatization and labelled\ndependency parsing in 58 languages. As of v1.0, Stanza also supports named\nentity recognition for selected languages.\n\n> \u26a0\ufe0f Previous version of this package were available as\n> [`spacy-stanfordnlp`](https://pypi.python.org/pypi/spacy-stanfordnlp).\n\n[![Azure Pipelines](https://img.shields.io/azure-devops/build/explosion-ai/public/17/master.svg?logo=azure-pipelines&style=flat-square)](https://dev.azure.com/explosion-ai/public/_build?definitionId=17)\n[![PyPi](https://img.shields.io/pypi/v/spacy-stanza.svg?style=flat-square)](https://pypi.python.org/pypi/spacy-stanza)\n[![GitHub](https://img.shields.io/github/release/explosion/spacy-stanza/all.svg?style=flat-square)](https://github.com/explosion/spacy-stanza)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/ambv/black)\n\nUsing this wrapper, you'll be able to use the following annotations, computed by\nyour pretrained `stanza` model:\n\n- Statistical tokenization (reflected in the `Doc` and its tokens)\n- Lemmatization (`token.lemma` and `token.lemma_`)\n- Part-of-speech tagging (`token.tag`, `token.tag_`, `token.pos`, `token.pos_`)\n- Dependency parsing (`token.dep`, `token.dep_`, `token.head`)\n- Named entity recognition (`doc.ents`, `token.ent_type`, `token.ent_type_`, `token.ent_iob`, `token.ent_iob_`)\n- Sentence segmentation (`doc.sents`)\n\n## \ufe0f\ufe0f\ufe0f\u231b\ufe0f Installation\n\n```bash\npip install spacy-stanza\n```\n\nMake sure to also install one of the\n[pre-trained Stanza models](https://stanfordnlp.github.io/stanza/models.html).\n\n## \ud83d\udcd6 Usage & Examples\n\nThe `StanzaLanguage` class can be initialized with a loaded Stanza\npipeline and returns a spaCy [`Language` object](https://spacy.io/api/language),\ni.e. the `nlp` object you can use to process text and create a\n[`Doc` object](https://spacy.io/api/doc).\n\n```python\nimport stanza\nfrom spacy_stanza import StanzaLanguage\n\nsnlp = stanza.Pipeline(lang=\"en\")\nnlp = StanzaLanguage(snlp)\n\ndoc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\nfor token in doc:\n    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\nprint(doc.ents)\n```\n\nIf language data for the given language is available in spaCy, the respective\nlanguage class will be used as the base for the `nlp` object \u2013 for example,\n`English()`. This lets you use spaCy's lexical attributes like `is_stop` or\n`like_num`. The `nlp` object follows the same API as any other spaCy `Language`\nclass \u2013 so you can visualize the `Doc` objects with displaCy, add custom\ncomponents to the pipeline, use the rule-based matcher and do pretty much\nanything else you'd normally do in spaCy.\n\n```python\n# Access spaCy's lexical attributes\nprint([token.is_stop for token in doc])\nprint([token.like_num for token in doc])\n\n# Visualize dependencies\nfrom spacy import displacy\ndisplacy.serve(doc)  # or displacy.render if you're in a Jupyter notebook\n\n# Efficient processing with nlp.pipe\nfor doc in nlp.pipe([\"Lots of texts\", \"Even more texts\", \"...\"]):\n    print(doc.text)\n\n# Combine with your own custom pipeline components\ndef custom_component(doc):\n    # Do something to the doc here\n    return doc\n\nnlp.add_pipe(custom_component)\n\n# Serialize it to a numpy array\nnp_array = doc.to_array(['ORTH', 'LEMMA', 'POS'])\n```\n\n### Experimental: Mixing and matching pipeline components\n\nBy default, the `nlp` object's pipeline will be empty, because all attributes\nare computed once and set in the custom [`Tokenizer`](spacy_stanza/language.py).\nBut since it's a regular `nlp` object, you can add your own components to the\npipeline. For example, you could add and train\n[your own custom text classification component](https://spacy.io/usage/training#textcat)\nand use `nlp.add_pipe` to add it to the pipeline, or augment the named\nentities with your own rule-based patterns using the\n[`EntityRuler` component](https://spacy.io/usage/rule-based-matching#entityruler).\n\n### Advanced: serialization and entry points\n\nThe spaCy `nlp` object created by `StanzaLanguage` exposes its language as\n`stanza_xx`.\n\n```python\nfrom spacy.util import get_lang_class\nlang_cls = get_lang_class(\"stanza_en\")\n```\n\nNormally, the above would fail because spaCy doesn't include a language class\n`stanza_en`. But because this package exposes a `spacy_languages` entry\npoint in its [`setup.py`](setup.py) that points to `StanzaLanguage`, spaCy\nknows how to initialize it.\n\nThis means that saving to and loading from disk works:\n\n```python\nsnlp = stanza.Pipeline(lang=\"en\")\nnlp = StanzaLanguage(snlp)\nnlp.to_disk(\"./stanza-spacy-model\")\n```\n\nAdditional arguments on `spacy.load` are automatically passed down to the\nlanguage class and pipeline components. So when loading the saved model, you can\npass in the `snlp` argument:\n\n```python\nsnlp = stanza.Pipeline(lang=\"en\")\nnlp = spacy.load(\"./stanza-spacy-model\", snlp=snlp)\n```\n\nNote that this **will not save any model data by default**. The Stanza\nmodels are very large, so for now, this package expects that you load them\nseparately.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://explosion.ai", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "spacy-stanza", "package_url": "https://pypi.org/project/spacy-stanza/", "platform": "", "project_url": "https://pypi.org/project/spacy-stanza/", "project_urls": {"Homepage": "https://explosion.ai"}, "release_url": "https://pypi.org/project/spacy-stanza/0.2.1/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Use the latest Stanza (StanfordNLP) research models directly in spaCy", "version": "0.2.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><a href=\"https://explosion.ai\" rel=\"nofollow\"><img align=\"right\" height=\"125\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/657dbf4006ad44b146bd59acc7f61d67177af1a5/68747470733a2f2f6578706c6f73696f6e2e61692f6173736574732f696d672f6c6f676f2e737667\" width=\"125\"></a></p>\n<h1>spaCy + Stanza (formerly StanfordNLP)</h1>\n<p>This package wraps the <a href=\"https://github.com/stanfordnlp/stanza\" rel=\"nofollow\">Stanza</a>\n(formerly StanfordNLP) library, so you can use Stanford's models as a\n<a href=\"https://spacy.io\" rel=\"nofollow\">spaCy</a> pipeline. The Stanford models achieved top accuracy in\nthe CoNLL 2017 and 2018 shared task, which involves tokenization,\npart-of-speech tagging, morphological analysis, lemmatization and labelled\ndependency parsing in 58 languages. As of v1.0, Stanza also supports named\nentity recognition for selected languages.</p>\n<blockquote>\n<p>\u26a0\ufe0f Previous version of this package were available as\n<a href=\"https://pypi.python.org/pypi/spacy-stanfordnlp\" rel=\"nofollow\"><code>spacy-stanfordnlp</code></a>.</p>\n</blockquote>\n<p><a href=\"https://dev.azure.com/explosion-ai/public/_build?definitionId=17\" rel=\"nofollow\"><img alt=\"Azure Pipelines\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ce4468a69e95f6ce8fdd0793cb4925de29b880f9/68747470733a2f2f696d672e736869656c64732e696f2f617a7572652d6465766f70732f6275696c642f6578706c6f73696f6e2d61692f7075626c69632f31372f6d61737465722e7376673f6c6f676f3d617a7572652d706970656c696e6573267374796c653d666c61742d737175617265\"></a>\n<a href=\"https://pypi.python.org/pypi/spacy-stanza\" rel=\"nofollow\"><img alt=\"PyPi\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/75703e332e170f4c89fb74c45bc4a3a1b444b8eb/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f73706163792d7374616e7a612e7376673f7374796c653d666c61742d737175617265\"></a>\n<a href=\"https://github.com/explosion/spacy-stanza\" rel=\"nofollow\"><img alt=\"GitHub\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/9895b4189b027ae0e0992cea930740ec521cc649/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6578706c6f73696f6e2f73706163792d7374616e7a612f616c6c2e7376673f7374796c653d666c61742d737175617265\"></a>\n<a href=\"https://github.com/ambv/black\" rel=\"nofollow\"><img alt=\"Code style: black\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1c326c58e924b9f3508f32a8ac6b3ee91f40b090/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e7376673f7374796c653d666c61742d737175617265\"></a></p>\n<p>Using this wrapper, you'll be able to use the following annotations, computed by\nyour pretrained <code>stanza</code> model:</p>\n<ul>\n<li>Statistical tokenization (reflected in the <code>Doc</code> and its tokens)</li>\n<li>Lemmatization (<code>token.lemma</code> and <code>token.lemma_</code>)</li>\n<li>Part-of-speech tagging (<code>token.tag</code>, <code>token.tag_</code>, <code>token.pos</code>, <code>token.pos_</code>)</li>\n<li>Dependency parsing (<code>token.dep</code>, <code>token.dep_</code>, <code>token.head</code>)</li>\n<li>Named entity recognition (<code>doc.ents</code>, <code>token.ent_type</code>, <code>token.ent_type_</code>, <code>token.ent_iob</code>, <code>token.ent_iob_</code>)</li>\n<li>Sentence segmentation (<code>doc.sents</code>)</li>\n</ul>\n<h2>\ufe0f\ufe0f\ufe0f\u231b\ufe0f Installation</h2>\n<pre>pip install spacy-stanza\n</pre>\n<p>Make sure to also install one of the\n<a href=\"https://stanfordnlp.github.io/stanza/models.html\" rel=\"nofollow\">pre-trained Stanza models</a>.</p>\n<h2>\ud83d\udcd6 Usage &amp; Examples</h2>\n<p>The <code>StanzaLanguage</code> class can be initialized with a loaded Stanza\npipeline and returns a spaCy <a href=\"https://spacy.io/api/language\" rel=\"nofollow\"><code>Language</code> object</a>,\ni.e. the <code>nlp</code> object you can use to process text and create a\n<a href=\"https://spacy.io/api/doc\" rel=\"nofollow\"><code>Doc</code> object</a>.</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">stanza</span>\n<span class=\"kn\">from</span> <span class=\"nn\">spacy_stanza</span> <span class=\"kn\">import</span> <span class=\"n\">StanzaLanguage</span>\n\n<span class=\"n\">snlp</span> <span class=\"o\">=</span> <span class=\"n\">stanza</span><span class=\"o\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">lang</span><span class=\"o\">=</span><span class=\"s2\">\"en\"</span><span class=\"p\">)</span>\n<span class=\"n\">nlp</span> <span class=\"o\">=</span> <span class=\"n\">StanzaLanguage</span><span class=\"p\">(</span><span class=\"n\">snlp</span><span class=\"p\">)</span>\n\n<span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"n\">nlp</span><span class=\"p\">(</span><span class=\"s2\">\"Barack Obama was born in Hawaii. He was elected president in 2008.\"</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">token</span> <span class=\"ow\">in</span> <span class=\"n\">doc</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">lemma_</span><span class=\"p\">,</span> <span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">pos_</span><span class=\"p\">,</span> <span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">dep_</span><span class=\"p\">,</span> <span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">ent_type_</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">ents</span><span class=\"p\">)</span>\n</pre>\n<p>If language data for the given language is available in spaCy, the respective\nlanguage class will be used as the base for the <code>nlp</code> object \u2013 for example,\n<code>English()</code>. This lets you use spaCy's lexical attributes like <code>is_stop</code> or\n<code>like_num</code>. The <code>nlp</code> object follows the same API as any other spaCy <code>Language</code>\nclass \u2013 so you can visualize the <code>Doc</code> objects with displaCy, add custom\ncomponents to the pipeline, use the rule-based matcher and do pretty much\nanything else you'd normally do in spaCy.</p>\n<pre><span class=\"c1\"># Access spaCy's lexical attributes</span>\n<span class=\"nb\">print</span><span class=\"p\">([</span><span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">is_stop</span> <span class=\"k\">for</span> <span class=\"n\">token</span> <span class=\"ow\">in</span> <span class=\"n\">doc</span><span class=\"p\">])</span>\n<span class=\"nb\">print</span><span class=\"p\">([</span><span class=\"n\">token</span><span class=\"o\">.</span><span class=\"n\">like_num</span> <span class=\"k\">for</span> <span class=\"n\">token</span> <span class=\"ow\">in</span> <span class=\"n\">doc</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Visualize dependencies</span>\n<span class=\"kn\">from</span> <span class=\"nn\">spacy</span> <span class=\"kn\">import</span> <span class=\"n\">displacy</span>\n<span class=\"n\">displacy</span><span class=\"o\">.</span><span class=\"n\">serve</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>  <span class=\"c1\"># or displacy.render if you're in a Jupyter notebook</span>\n\n<span class=\"c1\"># Efficient processing with nlp.pipe</span>\n<span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">pipe</span><span class=\"p\">([</span><span class=\"s2\">\"Lots of texts\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Even more texts\"</span><span class=\"p\">,</span> <span class=\"s2\">\"...\"</span><span class=\"p\">]):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Combine with your own custom pipeline components</span>\n<span class=\"k\">def</span> <span class=\"nf\">custom_component</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">):</span>\n    <span class=\"c1\"># Do something to the doc here</span>\n    <span class=\"k\">return</span> <span class=\"n\">doc</span>\n\n<span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">add_pipe</span><span class=\"p\">(</span><span class=\"n\">custom_component</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Serialize it to a numpy array</span>\n<span class=\"n\">np_array</span> <span class=\"o\">=</span> <span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">to_array</span><span class=\"p\">([</span><span class=\"s1\">'ORTH'</span><span class=\"p\">,</span> <span class=\"s1\">'LEMMA'</span><span class=\"p\">,</span> <span class=\"s1\">'POS'</span><span class=\"p\">])</span>\n</pre>\n<h3>Experimental: Mixing and matching pipeline components</h3>\n<p>By default, the <code>nlp</code> object's pipeline will be empty, because all attributes\nare computed once and set in the custom <a href=\"spacy_stanza/language.py\" rel=\"nofollow\"><code>Tokenizer</code></a>.\nBut since it's a regular <code>nlp</code> object, you can add your own components to the\npipeline. For example, you could add and train\n<a href=\"https://spacy.io/usage/training#textcat\" rel=\"nofollow\">your own custom text classification component</a>\nand use <code>nlp.add_pipe</code> to add it to the pipeline, or augment the named\nentities with your own rule-based patterns using the\n<a href=\"https://spacy.io/usage/rule-based-matching#entityruler\" rel=\"nofollow\"><code>EntityRuler</code> component</a>.</p>\n<h3>Advanced: serialization and entry points</h3>\n<p>The spaCy <code>nlp</code> object created by <code>StanzaLanguage</code> exposes its language as\n<code>stanza_xx</code>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">spacy.util</span> <span class=\"kn\">import</span> <span class=\"n\">get_lang_class</span>\n<span class=\"n\">lang_cls</span> <span class=\"o\">=</span> <span class=\"n\">get_lang_class</span><span class=\"p\">(</span><span class=\"s2\">\"stanza_en\"</span><span class=\"p\">)</span>\n</pre>\n<p>Normally, the above would fail because spaCy doesn't include a language class\n<code>stanza_en</code>. But because this package exposes a <code>spacy_languages</code> entry\npoint in its <a href=\"setup.py\" rel=\"nofollow\"><code>setup.py</code></a> that points to <code>StanzaLanguage</code>, spaCy\nknows how to initialize it.</p>\n<p>This means that saving to and loading from disk works:</p>\n<pre><span class=\"n\">snlp</span> <span class=\"o\">=</span> <span class=\"n\">stanza</span><span class=\"o\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">lang</span><span class=\"o\">=</span><span class=\"s2\">\"en\"</span><span class=\"p\">)</span>\n<span class=\"n\">nlp</span> <span class=\"o\">=</span> <span class=\"n\">StanzaLanguage</span><span class=\"p\">(</span><span class=\"n\">snlp</span><span class=\"p\">)</span>\n<span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">to_disk</span><span class=\"p\">(</span><span class=\"s2\">\"./stanza-spacy-model\"</span><span class=\"p\">)</span>\n</pre>\n<p>Additional arguments on <code>spacy.load</code> are automatically passed down to the\nlanguage class and pipeline components. So when loading the saved model, you can\npass in the <code>snlp</code> argument:</p>\n<pre><span class=\"n\">snlp</span> <span class=\"o\">=</span> <span class=\"n\">stanza</span><span class=\"o\">.</span><span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">lang</span><span class=\"o\">=</span><span class=\"s2\">\"en\"</span><span class=\"p\">)</span>\n<span class=\"n\">nlp</span> <span class=\"o\">=</span> <span class=\"n\">spacy</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s2\">\"./stanza-spacy-model\"</span><span class=\"p\">,</span> <span class=\"n\">snlp</span><span class=\"o\">=</span><span class=\"n\">snlp</span><span class=\"p\">)</span>\n</pre>\n<p>Note that this <strong>will not save any model data by default</strong>. The Stanza\nmodels are very large, so for now, this package expects that you load them\nseparately.</p>\n\n          </div>"}, "last_serial": 6842407, "releases": {"0.2.0": [{"comment_text": "", "digests": {"md5": "0518f231feeb0a8b15c14f21a4fadf7c", "sha256": "346a85876154a4a07a30cd0947afec478f949ad053b721e5f8d9f657fa9fb236"}, "downloads": -1, "filename": "spacy-stanza-0.2.0.tar.gz", "has_sig": false, "md5_digest": "0518f231feeb0a8b15c14f21a4fadf7c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 7252, "upload_time": "2020-03-17T18:11:20", "upload_time_iso_8601": "2020-03-17T18:11:20.768132Z", "url": "https://files.pythonhosted.org/packages/bb/c1/5f9a7b5e2152e6c84776f43713dcb9169c2e02cc435985ea77fd79559004/spacy-stanza-0.2.0.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "3adbfdad677d7926c7e0fd2ec2c9a610", "sha256": "5e6022d019df131e68988a02ee4496b902a81334e2040672cef19b89dca068e3"}, "downloads": -1, "filename": "spacy-stanza-0.2.1.tar.gz", "has_sig": false, "md5_digest": "3adbfdad677d7926c7e0fd2ec2c9a610", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 7390, "upload_time": "2020-03-19T10:28:20", "upload_time_iso_8601": "2020-03-19T10:28:20.630961Z", "url": "https://files.pythonhosted.org/packages/7f/c0/efa20a93b933602fd21651e38379cc6b419f2e2353caee267d4fa2dcbf71/spacy-stanza-0.2.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3adbfdad677d7926c7e0fd2ec2c9a610", "sha256": "5e6022d019df131e68988a02ee4496b902a81334e2040672cef19b89dca068e3"}, "downloads": -1, "filename": "spacy-stanza-0.2.1.tar.gz", "has_sig": false, "md5_digest": "3adbfdad677d7926c7e0fd2ec2c9a610", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 7390, "upload_time": "2020-03-19T10:28:20", "upload_time_iso_8601": "2020-03-19T10:28:20.630961Z", "url": "https://files.pythonhosted.org/packages/7f/c0/efa20a93b933602fd21651e38379cc6b419f2e2353caee267d4fa2dcbf71/spacy-stanza-0.2.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:06:03 2020"}