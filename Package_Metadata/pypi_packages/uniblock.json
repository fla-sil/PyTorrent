{"info": {"author": "Ringo", "author_email": "", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Text Processing"], "description": "# uniblock\n\nTired of scripting rules to remove **special punctuation marks, full-width characters, emojis, mathematical symbols, Latin characters, currency symbols, scientific notations, ...** for your NLP model?\n\nThis repository contains code for the paper [uniblock: Scoring and Filtering Corpus with Unicode Block Information](https://arxiv.org/abs/1908.09716) in [EMNLP-IJCNLP 2019](https://www.emnlp-ijcnlp2019.org/).\n\nUsing [Unicode](https://home.unicode.org/) [Block](https://en.wikipedia.org/wiki/Unicode_block) information (and more) as features, a [Bayesian Gaussian Mixture Model (BGM model)](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html) is trained to score and filter corpus. This method provides an alternative to the traditional rule-based approaches for \"illegal\" character filtering. It is statistical, extendable, simple and effective.\n\n# Getting Started\n\n## installation\n\nTo install the package, please use the following command (uniblock uses [f-strings](https://www.python.org/dev/peps/pep-0498/) and requires python 3.6+):\n```\npip install uniblock\n```\nAfter correct installation, the \"uniblock\" command should be added to the PATH:\n```\n$ uniblock -h\nusage: uniblock.py <command> [<args>]\n\nuniblock, scoring and filtering corpus with Unicode block information (and\nmore).\n\npositional arguments:\n  {setup,train,score,filter}\n                        command to execute\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n```\n\n## getting help\n\nuniblock uses a git-like \"command \\<subcommand\\> [\\<args\\>]\" interface. To get help about available subcommands and details about each subcommand, please use the following commands:\n```\nuniblock --help\nuniblock setup --help\nuniblock train --help\nuniblock score --help\nuniblock filter --help\n```\n\n## an example run\n\nThe best way to get started with uniblock is to do an example run to score and filter some text. Following the steps below takes about 20 minutes and you should be comfortable using uniblock for your own purposes afterwards.\n\n### 1. get data\n\nFor this run, we will use some Machine Translation (MT) data prepared from [WMT19](http://www.statmt.org/wmt19/). The language pair is Chinese-English, and training data is subsampled down to 2M sentence pairs for a quick and realistic run of uniblock. To download the data:\n\n[Google Drive](https://drive.google.com/drive/folders/1TK28o9ShMsub4PM7e1lL58C4BlLsezI5?usp=sharing) or [OneDrive](https://1drv.ms/u/s!AqfzE52eeEFjg0ExRoJl6N4mrk0T?e=dhYApM)\n\nAfter download and decompression, you should see 4 data files: \"train.2M.en\", \"train.2M.zh\", \"valid.en\" and \"valid.zh\". Take a look at the data and get a feeling of the quality:\n```\nless train.2M.en\nless train.2M.zh\nless valid.en\nless valid.zh\n```\nWe will use \"valid.xx\" to train our BGM model and score and filter \"train.2M.xx\".\n\n### 2. create folders\n\nCreate a folder called \"uniblock-an-example-run\" and subfolders called \"zh\" and \"en\":\n```\nmkdir uniblock-an-example-run\nmkdir uniblock-an-example-run/zh\nmkdir uniblock-an-example-run/en\n```\n\n### 3. setup features\nWe will filter sentences by their lengths and character Unicode distributions. It is also possible to discriminate the ASCII digits and ASCII punctuation and symbols from other ASCII characters, which is achieved by adding \"pseudo-blocks\".\n```\nuniblock setup --help\nuniblock setup --exp-path uniblock-an-example-run/zh --char-count --word-count --pseudo-blocks '0030..0039; ASCII digits' '0020..002F 003A..0040 005B..0060 007B..007E; ASCII punctuation and symbols'\nuniblock setup --exp-path uniblock-an-example-run/en --char-count --word-count --pseudo-blocks '0030..0039; ASCII digits' '0020..002F 003A..0040 005B..0060 007B..007E; ASCII punctuation and symbols'\n```\n\"uniblock-an-example/xx/uniblock\" folders are created to store uniblock-related files and \"uniblock-an-example/xx/uniblock/feature.dump\" are dumped for fast conversion from sentences to feature vectors.\n\n### 4. train zh and en BGM models\nIn this step we train BGM models, one for \"zh\" and one for \"en\". With the \"--verbose\" flag, we can see that with the default hyperparameters the EM training converged.\n```\nuniblock train --help\nuniblock train uniblock-data/valid.zh --exp-path uniblock-an-example-run/zh/ --verbose\nuniblock train uniblock-data/valid.en --exp-path uniblock-an-example-run/en/ --verbose\n```\nIt is also possible to examine the score files (replace \"xx\" with \"zh\" or \"en\"):\n```\nsort -g uniblock-an-example-run/xx/uniblock/valid.xx.score | less\n```\nand score statistics (replace \"xx\" with \"zh\" or \"en\"):\n```\nless uniblock-an-example-run/xx/uniblock/valid.xx.stat\n```\n\n### 5. score corpus\nNLP corpus can often go up to over 100M lines. Therefore in this step we split our 2M line corpus files into subfiles each containing 1M lines to simulate a more realistic run.\nFirst split the corpus files:\n```\nsplit uniblock-data/train.2M.zh -l 1000000 uniblock-data/train.2M.zh.split.\nsplit uniblock-data/train.2M.en -l 1000000 uniblock-data/train.2M.en.split.\n```\nThen score the subfiles with uniblock:\n```\nuniblock score --help\nuniblock score uniblock-data/train.2M.zh.split.aa --exp-path uniblock-an-example-run/zh/\nuniblock score uniblock-data/train.2M.zh.split.ab --exp-path uniblock-an-example-run/zh/\nuniblock score uniblock-data/train.2M.en.split.aa --exp-path uniblock-an-example-run/en/\nuniblock score uniblock-data/train.2M.en.split.ab --exp-path uniblock-an-example-run/en/\n```\nWhen the original corpus is large, there may be many subfiles. In this case, you can use your favorite job submitting software for your cluster and easilly parallelize this step.\nFinally, merge the scores:\n```\ncat uniblock-an-example-run/zh/uniblock/train.2M.zh.split.*.score > uniblock-an-example-run/zh/uniblock/train.2M.zh.score\ncat uniblock-an-example-run/en/uniblock/train.2M.en.split.*.score > uniblock-an-example-run/en/uniblock/train.2M.en.score\n```\nOptionally, check the results qualitatively:\n```\nsort -g uniblock-an-example-run/zh/uniblock/train.2M.zh.score | less\nsort -g uniblock-an-example-run/en/uniblock/train.2M.en.score | less\n```\nAt this point, you should see sentences with low qualities (really short ones, really long ones and ones with illegal characters) are assigned with low scores. Scrolling down to the end of the sorted score files (\"shift + G\"), you should also see high quality sentences with high scores.\n\n### 6. filter corpus\nuniblock supports several ways to filter the corpus with scores. For example, an absolute threshold (inclusive) or a relative threshold ([0, 1]) could be used. Alternatively, one could use the lowest score seen during the training of the BGM model and use it as an absolute threshold. uniblock also supports parallel corpus filtering. In this case, a reduction (the --combine flag) method needs to be specified to combine the scores across the parallel score files (different languages in the case of MT) and then apply the threshold. To see the usage of these options:\n```\nuniblock filter --help\n```\nTo filter with an absolute threshold of 10.0:\n```\n$ uniblock filter uniblock-an-example-run/zh/uniblock/train.2M.zh.score uniblock-an-example-run/en/uniblock/train.2M.en.score --thres-abs 10.0\n@ 59830 / 2000000 \u2248 2.99% lines filtered out\n```\nTo filter with a relative threshold of 20%:\n```\n$ uniblock filter uniblock-an-example-run/zh/uniblock/train.2M.zh.score uniblock-an-example-run/en/uniblock/train.2M.en.score --thres-rel 0.2\n@ 400000 / 2000000 \u2248 20.00% lines filtered out\n```\nTo filter with min scores seen during the training of the BGM model:\n```\n$ uniblock filter uniblock-an-example-run/zh/uniblock/train.2M.zh.score uniblock-an-example-run/en/uniblock/train.2M.en.score --combine none --stat-paths uniblock-an-example-run/zh/uniblock/valid.zh.stat uniblock-an-example-run/en/uniblock/valid.en.stat \n@ 106350 / 2000000 \u2248 5.32% lines filtered out\n```\nTo filter by linearly combining the scores for different languages:\n```\n$ uniblock filter uniblock-an-example-run/zh/uniblock/train.2M.zh.score uniblock-an-example-run/en/uniblock/train.2M.en.score --combine weighted_sum --lambdas 0.9 0.1 --thres-rel 0.3\n@ 600000 / 2000000 \u2248 30.00% lines filtered out\n```\nAfter filtering, it is important to check the \".filter\" files and get a feeling for the cleaned corpus:\n```\nless uniblock-an-example-run/zh/uniblock/train.2M.zh.score.filter\nless uniblock-an-example-run/en/uniblock/train.2M.en.score.filter\n```\nThese two files are the cleaned version of the original \"uniblock-data/train.2M.zh\" \"uniblock-data/train.2M.en\" files.\n\n### 7. next steps\nFollowing the previous steps, you successfully scored and filtered a training corpus for MT. For other tasks that work with monolingual data, it is easy to adapt the steps and obtain \"clean\" training data. As in the paper, we have experimented with Sentiment Analysis, Language Modeling and Machine Translation to show that uniblocks is robust and works quite well across different tasks and languages. As next steps, you should train your NLP model with the uniblock-cleaned data, and see if there is expected improvement.\n\n# Cite This Work\nTo cite this work, please use the following .bib:\n```\n@InProceedings{gao19:uniblock,\n\tauthor={Gao, Yingbo and Wang, Weiyue and Ney, Hermann},  \t\n\ttitle={uniblock: Scoring and Filtering Corpus with Unicode Block Information},  \n\tbooktitle={Conference on Empirical Methods in Natural Language Processing},\n\tyear=2019,  \n\taddress={Hong Kong, China},  \n\tmonth=nov,  \n\tbooktitlelink={https://www.emnlp-ijcnlp2019.org/},\n}\n```\n\n> Written with [StackEdit](https://stackedit.io/).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ringoreality/uniblock", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "uniblock", "package_url": "https://pypi.org/project/uniblock/", "platform": "", "project_url": "https://pypi.org/project/uniblock/", "project_urls": {"Homepage": "https://github.com/ringoreality/uniblock"}, "release_url": "https://pypi.org/project/uniblock/1.0.4/", "requires_dist": ["numpy", "tqdm", "joblib", "sklearn"], "requires_python": ">=3.6", "summary": "uniblock, scoring and filtering corpus with Unicode block information (and more)", "version": "1.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>uniblock</h1>\n<p>Tired of scripting rules to remove <strong>special punctuation marks, full-width characters, emojis, mathematical symbols, Latin characters, currency symbols, scientific notations, ...</strong> for your NLP model?</p>\n<p>This repository contains code for the paper <a href=\"https://arxiv.org/abs/1908.09716\" rel=\"nofollow\">uniblock: Scoring and Filtering Corpus with Unicode Block Information</a> in <a href=\"https://www.emnlp-ijcnlp2019.org/\" rel=\"nofollow\">EMNLP-IJCNLP 2019</a>.</p>\n<p>Using <a href=\"https://home.unicode.org/\" rel=\"nofollow\">Unicode</a> <a href=\"https://en.wikipedia.org/wiki/Unicode_block\" rel=\"nofollow\">Block</a> information (and more) as features, a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html\" rel=\"nofollow\">Bayesian Gaussian Mixture Model (BGM model)</a> is trained to score and filter corpus. This method provides an alternative to the traditional rule-based approaches for \"illegal\" character filtering. It is statistical, extendable, simple and effective.</p>\n<h1>Getting Started</h1>\n<h2>installation</h2>\n<p>To install the package, please use the following command (uniblock uses <a href=\"https://www.python.org/dev/peps/pep-0498/\" rel=\"nofollow\">f-strings</a> and requires python 3.6+):</p>\n<pre><code>pip install uniblock\n</code></pre>\n<p>After correct installation, the \"uniblock\" command should be added to the PATH:</p>\n<pre><code>$ uniblock -h\nusage: uniblock.py &lt;command&gt; [&lt;args&gt;]\n\nuniblock, scoring and filtering corpus with Unicode block information (and\nmore).\n\npositional arguments:\n  {setup,train,score,filter}\n                        command to execute\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n</code></pre>\n<h2>getting help</h2>\n<p>uniblock uses a git-like \"command &lt;subcommand&gt; [&lt;args&gt;]\" interface. To get help about available subcommands and details about each subcommand, please use the following commands:</p>\n<pre><code>uniblock --help\nuniblock setup --help\nuniblock train --help\nuniblock score --help\nuniblock filter --help\n</code></pre>\n<h2>an example run</h2>\n<p>The best way to get started with uniblock is to do an example run to score and filter some text. Following the steps below takes about 20 minutes and you should be comfortable using uniblock for your own purposes afterwards.</p>\n<h3>1. get data</h3>\n<p>For this run, we will use some Machine Translation (MT) data prepared from <a href=\"http://www.statmt.org/wmt19/\" rel=\"nofollow\">WMT19</a>. The language pair is Chinese-English, and training data is subsampled down to 2M sentence pairs for a quick and realistic run of uniblock. To download the data:</p>\n<p><a href=\"https://drive.google.com/drive/folders/1TK28o9ShMsub4PM7e1lL58C4BlLsezI5?usp=sharing\" rel=\"nofollow\">Google Drive</a> or <a href=\"https://1drv.ms/u/s!AqfzE52eeEFjg0ExRoJl6N4mrk0T?e=dhYApM\" rel=\"nofollow\">OneDrive</a></p>\n<p>After download and decompression, you should see 4 data files: \"train.2M.en\", \"train.2M.zh\", \"valid.en\" and \"valid.zh\". Take a look at the data and get a feeling of the quality:</p>\n<pre><code>less train.2M.en\nless train.2M.zh\nless valid.en\nless valid.zh\n</code></pre>\n<p>We will use \"valid.xx\" to train our BGM model and score and filter \"train.2M.xx\".</p>\n<h3>2. create folders</h3>\n<p>Create a folder called \"uniblock-an-example-run\" and subfolders called \"zh\" and \"en\":</p>\n<pre><code>mkdir uniblock-an-example-run\nmkdir uniblock-an-example-run/zh\nmkdir uniblock-an-example-run/en\n</code></pre>\n<h3>3. setup features</h3>\n<p>We will filter sentences by their lengths and character Unicode distributions. It is also possible to discriminate the ASCII digits and ASCII punctuation and symbols from other ASCII characters, which is achieved by adding \"pseudo-blocks\".</p>\n<pre><code>uniblock setup --help\nuniblock setup --exp-path uniblock-an-example-run/zh --char-count --word-count --pseudo-blocks '0030..0039; ASCII digits' '0020..002F 003A..0040 005B..0060 007B..007E; ASCII punctuation and symbols'\nuniblock setup --exp-path uniblock-an-example-run/en --char-count --word-count --pseudo-blocks '0030..0039; ASCII digits' '0020..002F 003A..0040 005B..0060 007B..007E; ASCII punctuation and symbols'\n</code></pre>\n<p>\"uniblock-an-example/xx/uniblock\" folders are created to store uniblock-related files and \"uniblock-an-example/xx/uniblock/feature.dump\" are dumped for fast conversion from sentences to feature vectors.</p>\n<h3>4. train zh and en BGM models</h3>\n<p>In this step we train BGM models, one for \"zh\" and one for \"en\". With the \"--verbose\" flag, we can see that with the default hyperparameters the EM training converged.</p>\n<pre><code>uniblock train --help\nuniblock train uniblock-data/valid.zh --exp-path uniblock-an-example-run/zh/ --verbose\nuniblock train uniblock-data/valid.en --exp-path uniblock-an-example-run/en/ --verbose\n</code></pre>\n<p>It is also possible to examine the score files (replace \"xx\" with \"zh\" or \"en\"):</p>\n<pre><code>sort -g uniblock-an-example-run/xx/uniblock/valid.xx.score | less\n</code></pre>\n<p>and score statistics (replace \"xx\" with \"zh\" or \"en\"):</p>\n<pre><code>less uniblock-an-example-run/xx/uniblock/valid.xx.stat\n</code></pre>\n<h3>5. score corpus</h3>\n<p>NLP corpus can often go up to over 100M lines. Therefore in this step we split our 2M line corpus files into subfiles each containing 1M lines to simulate a more realistic run.\nFirst split the corpus files:</p>\n<pre><code>split uniblock-data/train.2M.zh -l 1000000 uniblock-data/train.2M.zh.split.\nsplit uniblock-data/train.2M.en -l 1000000 uniblock-data/train.2M.en.split.\n</code></pre>\n<p>Then score the subfiles with uniblock:</p>\n<pre><code>uniblock score --help\nuniblock score uniblock-data/train.2M.zh.split.aa --exp-path uniblock-an-example-run/zh/\nuniblock score uniblock-data/train.2M.zh.split.ab --exp-path uniblock-an-example-run/zh/\nuniblock score uniblock-data/train.2M.en.split.aa --exp-path uniblock-an-example-run/en/\nuniblock score uniblock-data/train.2M.en.split.ab --exp-path uniblock-an-example-run/en/\n</code></pre>\n<p>When the original corpus is large, there may be many subfiles. In this case, you can use your favorite job submitting software for your cluster and easilly parallelize this step.\nFinally, merge the scores:</p>\n<pre><code>cat uniblock-an-example-run/zh/uniblock/train.2M.zh.split.*.score &gt; uniblock-an-example-run/zh/uniblock/train.2M.zh.score\ncat uniblock-an-example-run/en/uniblock/train.2M.en.split.*.score &gt; uniblock-an-example-run/en/uniblock/train.2M.en.score\n</code></pre>\n<p>Optionally, check the results qualitatively:</p>\n<pre><code>sort -g uniblock-an-example-run/zh/uniblock/train.2M.zh.score | less\nsort -g uniblock-an-example-run/en/uniblock/train.2M.en.score | less\n</code></pre>\n<p>At this point, you should see sentences with low qualities (really short ones, really long ones and ones with illegal characters) are assigned with low scores. Scrolling down to the end of the sorted score files (\"shift + G\"), you should also see high quality sentences with high scores.</p>\n<h3>6. filter corpus</h3>\n<p>uniblock supports several ways to filter the corpus with scores. For example, an absolute threshold (inclusive) or a relative threshold ([0, 1]) could be used. Alternatively, one could use the lowest score seen during the training of the BGM model and use it as an absolute threshold. uniblock also supports parallel corpus filtering. In this case, a reduction (the --combine flag) method needs to be specified to combine the scores across the parallel score files (different languages in the case of MT) and then apply the threshold. To see the usage of these options:</p>\n<pre><code>uniblock filter --help\n</code></pre>\n<p>To filter with an absolute threshold of 10.0:</p>\n<pre><code>$ uniblock filter uniblock-an-example-run/zh/uniblock/train.2M.zh.score uniblock-an-example-run/en/uniblock/train.2M.en.score --thres-abs 10.0\n@ 59830 / 2000000 \u2248 2.99% lines filtered out\n</code></pre>\n<p>To filter with a relative threshold of 20%:</p>\n<pre><code>$ uniblock filter uniblock-an-example-run/zh/uniblock/train.2M.zh.score uniblock-an-example-run/en/uniblock/train.2M.en.score --thres-rel 0.2\n@ 400000 / 2000000 \u2248 20.00% lines filtered out\n</code></pre>\n<p>To filter with min scores seen during the training of the BGM model:</p>\n<pre><code>$ uniblock filter uniblock-an-example-run/zh/uniblock/train.2M.zh.score uniblock-an-example-run/en/uniblock/train.2M.en.score --combine none --stat-paths uniblock-an-example-run/zh/uniblock/valid.zh.stat uniblock-an-example-run/en/uniblock/valid.en.stat \n@ 106350 / 2000000 \u2248 5.32% lines filtered out\n</code></pre>\n<p>To filter by linearly combining the scores for different languages:</p>\n<pre><code>$ uniblock filter uniblock-an-example-run/zh/uniblock/train.2M.zh.score uniblock-an-example-run/en/uniblock/train.2M.en.score --combine weighted_sum --lambdas 0.9 0.1 --thres-rel 0.3\n@ 600000 / 2000000 \u2248 30.00% lines filtered out\n</code></pre>\n<p>After filtering, it is important to check the \".filter\" files and get a feeling for the cleaned corpus:</p>\n<pre><code>less uniblock-an-example-run/zh/uniblock/train.2M.zh.score.filter\nless uniblock-an-example-run/en/uniblock/train.2M.en.score.filter\n</code></pre>\n<p>These two files are the cleaned version of the original \"uniblock-data/train.2M.zh\" \"uniblock-data/train.2M.en\" files.</p>\n<h3>7. next steps</h3>\n<p>Following the previous steps, you successfully scored and filtered a training corpus for MT. For other tasks that work with monolingual data, it is easy to adapt the steps and obtain \"clean\" training data. As in the paper, we have experimented with Sentiment Analysis, Language Modeling and Machine Translation to show that uniblocks is robust and works quite well across different tasks and languages. As next steps, you should train your NLP model with the uniblock-cleaned data, and see if there is expected improvement.</p>\n<h1>Cite This Work</h1>\n<p>To cite this work, please use the following .bib:</p>\n<pre><code>@InProceedings{gao19:uniblock,\n\tauthor={Gao, Yingbo and Wang, Weiyue and Ney, Hermann},  \t\n\ttitle={uniblock: Scoring and Filtering Corpus with Unicode Block Information},  \n\tbooktitle={Conference on Empirical Methods in Natural Language Processing},\n\tyear=2019,  \n\taddress={Hong Kong, China},  \n\tmonth=nov,  \n\tbooktitlelink={https://www.emnlp-ijcnlp2019.org/},\n}\n</code></pre>\n<blockquote>\n<p>Written with <a href=\"https://stackedit.io/\" rel=\"nofollow\">StackEdit</a>.</p>\n</blockquote>\n\n          </div>"}, "last_serial": 5736400, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "f120eea7fc1e89dd7214c731d3be9962", "sha256": "90e357eb7ed9f07338835a41681855eda75190b399d96ddfd99fa239c00451e9"}, "downloads": -1, "filename": "uniblock-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "f120eea7fc1e89dd7214c731d3be9962", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 7129, "upload_time": "2019-08-22T15:13:05", "upload_time_iso_8601": "2019-08-22T15:13:05.492979Z", "url": "https://files.pythonhosted.org/packages/73/42/4a69aa9da7118ea35a03440c49ad7e4f237135a63bf9876e91ccd770bd25/uniblock-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6d8a97860f8912ea67a8f9754dbbcdc3", "sha256": "2d753a8d0eafd7b05500e45191684d703811a45e0f7235cddba306212d06ee9c"}, "downloads": -1, "filename": "uniblock-1.0.0.tar.gz", "has_sig": false, "md5_digest": "6d8a97860f8912ea67a8f9754dbbcdc3", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5668, "upload_time": "2019-08-22T15:13:07", "upload_time_iso_8601": "2019-08-22T15:13:07.927195Z", "url": "https://files.pythonhosted.org/packages/42/45/6edd9a349e7824212e51d0fa085fcb9fd2526f53482163c3a88b28c799bb/uniblock-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "90041c5bd39f60d7e5f5a52b048bd986", "sha256": "b2649ca37360197ab322013dd43827207503472acc3d14695e797323e2e7145a"}, "downloads": -1, "filename": "uniblock-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "90041c5bd39f60d7e5f5a52b048bd986", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 7138, "upload_time": "2019-08-23T08:50:52", "upload_time_iso_8601": "2019-08-23T08:50:52.556939Z", "url": "https://files.pythonhosted.org/packages/36/9f/1be9527d17b1f314e2195a78504ea67ea88af36a17f5129584b06d2f544e/uniblock-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "c664f06034d7f8ffffb777845ec9b3af", "sha256": "250e724dd92f3985b64fcb22e5e0580c51eb89df79bdee66faf3eafcc0f1ce5d"}, "downloads": -1, "filename": "uniblock-1.0.1.tar.gz", "has_sig": false, "md5_digest": "c664f06034d7f8ffffb777845ec9b3af", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 5705, "upload_time": "2019-08-23T08:50:54", "upload_time_iso_8601": "2019-08-23T08:50:54.135150Z", "url": "https://files.pythonhosted.org/packages/c1/2a/ac7c8e26400de6ea404fe2a654553fb6db3fec47c455670353f9431cfc9c/uniblock-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "1c9ae735eb892974210aa032314b566b", "sha256": "262647a5d8a966de2d3d9da5960dab9dc805fa8e4385ee60ff0433ec6de8bc7d"}, "downloads": -1, "filename": "uniblock-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "1c9ae735eb892974210aa032314b566b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 11505, "upload_time": "2019-08-23T09:41:36", "upload_time_iso_8601": "2019-08-23T09:41:36.719488Z", "url": "https://files.pythonhosted.org/packages/8e/3d/ab85cb79425bf50af8dd80662122ef191eea88b92157b134e8ac50cc4d1a/uniblock-1.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "dee0dbb1774b9990b9bb899dc26ca78e", "sha256": "42228e42b265fbd6020de8521dbfa4d3c8a3d795d2fbb84c1fcb4ab264105e4f"}, "downloads": -1, "filename": "uniblock-1.0.2.tar.gz", "has_sig": false, "md5_digest": "dee0dbb1774b9990b9bb899dc26ca78e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 10393, "upload_time": "2019-08-23T09:41:38", "upload_time_iso_8601": "2019-08-23T09:41:38.310796Z", "url": "https://files.pythonhosted.org/packages/0a/f7/a5f7a92db596579adbe73ea275ff2432922f83abc6d9a6cc7d024643a088/uniblock-1.0.2.tar.gz", "yanked": false}], "1.0.3": [{"comment_text": "", "digests": {"md5": "b99e2fa990bcb5dae8b90220797f8707", "sha256": "abe97c57178b2f6b00551952e9396928f786a35a82912992e37b609d7d4c7069"}, "downloads": -1, "filename": "uniblock-1.0.3-py3-none-any.whl", "has_sig": false, "md5_digest": "b99e2fa990bcb5dae8b90220797f8707", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 14801, "upload_time": "2019-08-23T12:17:39", "upload_time_iso_8601": "2019-08-23T12:17:39.940215Z", "url": "https://files.pythonhosted.org/packages/72/b7/bd0fdc563d87bf117f170a6bc172cb6940a4ce351c38b026f655d767e4cc/uniblock-1.0.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "3e76a49422b034eefdc4e3805220572b", "sha256": "609ad95dda774f1beff3b1edbd405eb109719b8e671c442675b96d5122f7da7e"}, "downloads": -1, "filename": "uniblock-1.0.3.tar.gz", "has_sig": false, "md5_digest": "3e76a49422b034eefdc4e3805220572b", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17220, "upload_time": "2019-08-23T12:17:41", "upload_time_iso_8601": "2019-08-23T12:17:41.737850Z", "url": "https://files.pythonhosted.org/packages/b1/f5/f3c135697a522f9e3d89237bf12d39e2f701c4caad6981fb234d69626da3/uniblock-1.0.3.tar.gz", "yanked": false}], "1.0.4": [{"comment_text": "", "digests": {"md5": "e2e212a7d7200af75dcaed0dde645d9f", "sha256": "9fcbc52d57d2ff069f96b095c50ebbbc26fdca408b61722eb6371dcbd7afb980"}, "downloads": -1, "filename": "uniblock-1.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "e2e212a7d7200af75dcaed0dde645d9f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 14846, "upload_time": "2019-08-27T12:07:01", "upload_time_iso_8601": "2019-08-27T12:07:01.242302Z", "url": "https://files.pythonhosted.org/packages/ef/09/84cdcd84209c1fad6af235eca209bb40914e828d10c98f9298963a6b8311/uniblock-1.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "25276154f319c8ec81521d6033cc4d40", "sha256": "259ef2b51b0fa6f612a6d2f368bb92b006a81db130d2edf7d3268dffa58c67ca"}, "downloads": -1, "filename": "uniblock-1.0.4.tar.gz", "has_sig": false, "md5_digest": "25276154f319c8ec81521d6033cc4d40", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17318, "upload_time": "2019-08-27T12:07:02", "upload_time_iso_8601": "2019-08-27T12:07:02.812620Z", "url": "https://files.pythonhosted.org/packages/7d/b0/22f63e44de62fdd91b7cdb2f7eb726c207b0fdc3b36be943aeae2b249dc7/uniblock-1.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e2e212a7d7200af75dcaed0dde645d9f", "sha256": "9fcbc52d57d2ff069f96b095c50ebbbc26fdca408b61722eb6371dcbd7afb980"}, "downloads": -1, "filename": "uniblock-1.0.4-py3-none-any.whl", "has_sig": false, "md5_digest": "e2e212a7d7200af75dcaed0dde645d9f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 14846, "upload_time": "2019-08-27T12:07:01", "upload_time_iso_8601": "2019-08-27T12:07:01.242302Z", "url": "https://files.pythonhosted.org/packages/ef/09/84cdcd84209c1fad6af235eca209bb40914e828d10c98f9298963a6b8311/uniblock-1.0.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "25276154f319c8ec81521d6033cc4d40", "sha256": "259ef2b51b0fa6f612a6d2f368bb92b006a81db130d2edf7d3268dffa58c67ca"}, "downloads": -1, "filename": "uniblock-1.0.4.tar.gz", "has_sig": false, "md5_digest": "25276154f319c8ec81521d6033cc4d40", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 17318, "upload_time": "2019-08-27T12:07:02", "upload_time_iso_8601": "2019-08-27T12:07:02.812620Z", "url": "https://files.pythonhosted.org/packages/7d/b0/22f63e44de62fdd91b7cdb2f7eb726c207b0fdc3b36be943aeae2b249dc7/uniblock-1.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:40:48 2020"}