{"info": {"author": "Syrus Akbary, The AIODataLoader contributors", "author_email": "me@syrusakbary.com, zinsouloris@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Software Development :: Libraries"], "description": "AIODataLoader-next\n==================\n\nThis a fork from the\n`asyncio DataLoader <https://github.com/syrusakbary/aiodataloader/>`__\nincluding community fixes and Python 3.7+ compatibility.\n\nDataLoader is a generic utility to be used as part of your application's\ndata fetching layer to provide a simplified and consistent API over\nvarious remote data sources such as databases or web services via\nbatching and caching.\n\n|Build Status|\n\nA port of the \"Loader\" API originally developed by [@schrockn][] at\nFacebook in 2010 as a simplifying force to coalesce the sundry key-value\nstore back-end APIs which existed at the time. At Facebook, \"Loader\"\nbecame one of the implementation details of the \"Ent\" framework, a\nprivacy-aware data entity loading and caching layer within web server\nproduct code. This ultimately became the underpinning for Facebook's\nGraphQL server implementation and type definitions.\n\nDataLoader is a simplified version of this original idea implemented in\nPython for AsyncIO services. DataLoader is often used when implementing\na `graphene <https://github.com/graphql-python/graphene>`__ service,\nthough it is also broadly useful in other situations.\n\nDataLoader is provided so that it may be useful not just to build\nGraphQL services with AsyncIO but also as a publicly available reference\nimplementation of this concept in the hopes that it can be ported to\nother languages. If you port DataLoader to another language, please open\nan issue to include a link from this repository.\n\nGetting Started\n---------------\n\nFirst, install DataLoader using pip.\n\n.. code:: sh\n\n    pip install aiodataloader-next\n\nTo get started, create a ``DataLoader``. Each ``DataLoader`` instance\nrepresents a unique cache. Typically instances are created per request\nwhen used within a web-server like\n`Sanic <https://sanic.readthedocs.io/en/latest/>`__ if different users\ncan see different things.\n\n    Note: DataLoader assumes a AsyncIO environment with ``async/await``\n    available only in Python 3.5+.\n\nBatching\n--------\n\nBatching is not an advanced feature, it's DataLoader's primary feature.\nCreate loaders by providing a batch loading function.\n\n.. code:: python\n\n    from aiodataloader import DataLoader\n\n    class UserLoader(DataLoader):\n        async def batch_load_fn(self, keys):\n            return await my_batch_get_users(keys)\n\n    user_loader = UserLoader()\n\nA batch loading function accepts a Iterable of keys, and returns a\nPromise which resolves to a List of values\\ `\\* <#batch-function>`__.\n\nThen load individual values from the loader. DataLoader will coalesce\nall individual loads which occur within a single frame of execution (a\nsingle tick of the event loop) and then call your batch function with\nall requested keys.\n\n.. code:: python\n\n    user1_future = user_loader.load(1)\n    user2_future = user_loader.load(2)\n\n    user1 = await user1_future\n    user2 = await user2_future\n\n    user1_invitedby = user_loader.load(user1.invited_by_id)\n    user2_invitedby = user_loader.load(user2.invited_by_id)\n\n    print(\"User 1 was invited by\", await user1_invitedby)\n    print(\"User 2 was invited by\", await user2_invitedby)\n\nA naive application may have issued four round-trips to a backend for\nthe required information, but with DataLoader this application will make\nat most two.\n\nDataLoader allows you to decouple unrelated parts of your application\nwithout sacrificing the performance of batch data-loading. While the\nloader presents an API that loads individual values, all concurrent\nrequests will be coalesced and presented to your batch loading function.\nThis allows your application to safely distribute data fetching\nrequirements throughout your application and maintain minimal outgoing\ndata requests.\n\nBatch Function\n~~~~~~~~~~~~~~\n\nA batch loading function accepts an List of keys, and returns a Future\nwhich resolves to an List of values. There are a few constraints that\nmust be upheld:\n\n-  The List of values must be the same length as the List of keys.\n-  Each index in the List of values must correspond to the same index in\n   the List of keys.\n\nFor example, if your batch function was provided the List of keys:\n``[ 2, 9, 6, 1 ]``, and loading from a back-end service returned the\nvalues:\n\n.. code:: python\n\n    { 'id': 9, 'name': 'Chicago' }\n    { 'id': 1, 'name': 'New York' }\n    { 'id': 2, 'name': 'San Francisco' }\n\nOur back-end service returned results in a different order than we\nrequested, likely because it was more efficient for it to do so. Also,\nit omitted a result for key ``6``, which we can interpret as no value\nexisting for that key.\n\nTo uphold the constraints of the batch function, it must return an List\nof values the same length as the List of keys, and re-order them to\nensure each index aligns with the original keys ``[ 2, 9, 6, 1 ]``:\n\n.. code:: python\n\n    [\n      { 'id': 2, 'name': 'San Francisco' },\n      { 'id': 9, 'name': 'Chicago' },\n      None,\n      { 'id': 1, 'name': 'New York' }\n    ]\n\nCaching\n-------\n\nDataLoader provides a memoization cache for all loads which occur in a\nsingle request to your application. After ``.load()`` is called once\nwith a given key, the resulting value is cached to eliminate redundant\nloads.\n\nIn addition to relieving pressure on your data storage, caching results\nper-request also creates fewer objects which may relieve memory pressure\non your application:\n\n.. code:: python\n\n    user_future1 = user_loader.load(1)\n    user_future2 = user_loader.load(1)\n\n    assert user_future1 == user_future2\n\nCaching per-Request\n~~~~~~~~~~~~~~~~~~~\n\nDataLoader caching *does not* replace Redis, Memcache, or any other\nshared application-level cache. DataLoader is first and foremost a data\nloading mechanism, and its cache only serves the purpose of not\nrepeatedly loading the same data in the context of a single request to\nyour Application. To do this, it maintains a simple in-memory\nmemoization cache (more accurately: ``.load()`` is a memoized function).\n\nAvoid multiple requests from different users using the DataLoader\ninstance, which could result in cached data incorrectly appearing in\neach request. Typically, DataLoader instances are created when a Request\nbegins, and are not used once the Request ends.\n\nFor example, when using with\n`Sanic <https://sanic.readthedocs.io/en/latest/>`__:\n\n.. code:: python\n\n    def create_loaders(auth_token) {\n        return {\n          'users': user_loader,\n        }\n    }\n\n\n    app = Sanic(__name__)\n\n    @app.route(\"/\")\n    async def test(request):\n        auth_token = authenticate_user(request)\n        loaders = create_loaders(auth_token)\n        return render_page(request, loaders)\n\nClearing Cache\n~~~~~~~~~~~~~~\n\nIn certain uncommon cases, clearing the request cache may be necessary.\n\nThe most common example when clearing the loader's cache is necessary is\nafter a mutation or update within the same request, when a cached value\ncould be out of date and future loads should not use any possibly cached\nvalue.\n\nHere's a simple example using SQL UPDATE to illustrate.\n\n.. code:: python\n\n    # Request begins...\n    user_loader = ...\n\n    # And a value happens to be loaded (and cached).\n    user4 = await user_loader.load(4)\n\n    # A mutation occurs, invalidating what might be in cache.\n    await sql_run('UPDATE users WHERE id=4 SET username=\"zuck\"')\n    user_loader.clear(4)\n\n    # Later the value load is loaded again so the mutated data appears.\n    user4 = await user_loader.load(4)\n\n    # Request completes.\n\nCaching Exceptions\n~~~~~~~~~~~~~~~~~~\n\nIf a batch load fails (that is, a batch function throws or returns a\nrejected Promise), then the requested values will not be cached. However\nif a batch function returns an ``Exception`` instance for an individual\nvalue, that ``Exception`` will be cached to avoid frequently loading the\nsame ``Exception``.\n\nIn some circumstances you may wish to clear the cache for these\nindividual Errors:\n\n.. code:: python\n\n    try:\n        user_loader.load(1)\n    except Exception as e:\n        user_loader.clear(1)\n        raise\n\nDisabling Cache\n~~~~~~~~~~~~~~~\n\nIn certain uncommon cases, a DataLoader which *does not* cache may be\ndesirable. Calling ``DataLoader(batch_fn, cache=false)`` will ensure\nthat every call to ``.load()`` will produce a *new* Future, and\nrequested keys will not be saved in memory.\n\nHowever, when the memoization cache is disabled, your batch function\nwill receive an array of keys which may contain duplicates! Each key\nwill be associated with each call to ``.load()``. Your batch loader\nshould provide a value for each instance of the requested key.\n\nFor example:\n\n.. code:: python\n\n    class MyLoader(DataLoader):\n        cache = False\n        async def batch_load_fn(self, keys):\n            print(keys)\n            return keys\n\n    my_loader = MyLoader()\n\n    my_loader.load('A')\n    my_loader.load('B')\n    my_loader.load('A')\n\n    # > [ 'A', 'B', 'A' ]\n\nMore complex cache behavior can be achieved by calling ``.clear()`` or\n``.clear_all()`` rather than disabling the cache completely. For\nexample, this DataLoader will provide unique keys to a batch function\ndue to the memoization cache being enabled, but will immediately clear\nits cache when the batch function is called so later requests will load\nnew values.\n\n.. code:: python\n\n    class MyLoader(DataLoader):\n        cache = False\n        async def batch_load_fn(self, keys):\n            self.clear_all()\n            return keys\n\nAPI\n---\n\nclass DataLoader\n~~~~~~~~~~~~~~~~\n\nDataLoader creates a public API for loading data from a particular data\nback-end with unique keys such as the ``id`` column of a SQL table or\ndocument name in a MongoDB database, given a batch loading function.\n\nEach ``DataLoader`` instance contains a unique memoized cache. Use\ncaution when used in long-lived applications or those which serve many\nusers with different access permissions and consider creating a new\ninstance per web request.\n\n``new DataLoader(batch_load_fn, **options)``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nCreate a new ``DataLoader`` given a batch loading function and options.\n\n-  *batch\\_load\\_fn*: An async function (coroutine) which accepts an\n   List of keys and returns a Future which resolves to an List of\n   values.\n\n-  *options*:\n\n-  *batch*: Default ``True``. Set to ``False`` to disable batching,\n   instead immediately invoking ``batch_load_fn`` with a single load\n   key.\n\n-  *max\\_batch\\_size*: Default ``Infinity``. Limits the number of items\n   that get passed in to the ``batch_load_fn``.\n\n-  *cache*: Default ``True``. Set to ``False`` to disable memoization\n   caching, instead creating a new Promise and new key in the\n   ``batch_load_fn`` for every load of the same key.\n\n-  *cache\\_key\\_fn*: A function to produce a cache key for a given load\n   key. Defaults to ``key => key``. Useful to provide when Python\n   objects are keys and two similarly shaped objects should be\n   considered equivalent.\n\n-  *cache\\_map*: An instance of\n   `dict <https://docs.python.org/3/tutorial/datastructures.html#dictionaries>`__\n   (or an object with a similar API) to be used as the underlying cache\n   for this loader. Default ``{}``.\n\n``load(key)``\n^^^^^^^^^^^^^\n\nLoads a key, returning a ``Future`` for the value represented by that\nkey.\n\n-  *key*: An key value to load.\n\n``load_many(keys)``\n^^^^^^^^^^^^^^^^^^^\n\nLoads multiple keys, promising an array of values:\n\n.. code:: python\n\n    a, b = await my_loader.load_many([ 'a', 'b' ]);\n\nThis is equivalent to the more verbose:\n\n.. code:: python\n\n    from asyncio import gather\n    a, b = await gather(\n        my_loader.load('a'),\n        my_loader.load('b')\n    )\n\n-  *keys*: A list of key values to load.\n\n``clear(key)``\n^^^^^^^^^^^^^^\n\nClears the value at ``key`` from the cache, if it exists. Returns itself\nfor method chaining.\n\n-  *key*: An key value to clear.\n\n``clear_all()``\n^^^^^^^^^^^^^^^\n\nClears the entire cache. To be used when some event results in unknown\ninvalidations across this particular ``DataLoader``. Returns itself for\nmethod chaining.\n\n``prime(key, value)``\n^^^^^^^^^^^^^^^^^^^^^\n\nPrimes the cache with the provided key and value. If the key already\nexists, no change is made. (To forcefully prime the cache, clear the key\nfirst with ``loader.clear(key).prime(key, value)``.) Returns itself for\nmethod chaining.\n\nUsing with GraphQL\n------------------\n\nDataLoader pairs nicely well with\n`GraphQL <https://github.com/graphql-python/graphene>`__. GraphQL fields\nare designed to be stand-alone functions. Without a caching or batching\nmechanism, it's easy for a naive GraphQL server to issue new database\nrequests each time a field is resolved.\n\nConsider the following GraphQL request:\n\n::\n\n    {\n      me {\n        name\n        bestFriend {\n          name\n        }\n        friends(first: 5) {\n          name\n          bestFriend {\n            name\n          }\n        }\n      }\n    }\n\nNaively, if ``me``, ``bestFriend`` and ``friends`` each need to request\nthe backend, there could be at most 13 database requests!\n\nWhen using DataLoader, we could define the ``User`` type using the\n`SQLite <examples/SQL.md>`__ example with clearer code and at most 4\ndatabase requests, and possibly fewer if there are cache hits.\n\n.. code:: python\n\n    class User(graphene.ObjectType):\n        name = graphene.String()\n        best_friend = graphene.Field(lambda: User)\n        friends = graphene.List(lambda: User)\n\n        def resolve_best_friend(self, args, context, info):\n            return user_loader.load(self.best_friend_id)\n\n        def resolve_friends(self, args, context, info):\n            return user_loader.load_many(self.friend_ids)\n\nCommon Patterns\n---------------\n\nCreating a new DataLoader per request.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn many applications, a web server using DataLoader serves requests to\nmany different users with different access permissions. It may be\ndangerous to use one cache across many users, and is encouraged to\ncreate a new DataLoader per request:\n\n.. code:: python\n\n    def create_loaders(auth_token):\n      return {\n        'users': DataLoader(lambda ids: gen_users(auth_token, ids)),\n        'cdn_urls': DataLoader(lambda raw_urls: gen_cdn_urls(auth_token, raw_urls)),\n        'stories': DataLoader(lambda keys: gen_stories(auth_token, keys)),\n      }\n    }\n\n    # When handling an incoming web request:\n    loaders = create_loaders(request.query.auth_token)\n\n    # Then, within application logic:\n    user = await loaders.users.load(4)\n    pic = await loaders.cdn_urls.load(user.raw_pic_url)\n\nCreating an object where each key is a ``DataLoader`` is one common\npattern which provides a single value to pass around to code which needs\nto perform data loading, such as part of the ``root_value`` in a\n[graphql][] request.\n\nLoading by alternative keys.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nOccasionally, some kind of value can be accessed in multiple ways. For\nexample, perhaps a \"User\" type can be loaded not only by an \"id\" but\nalso by a \"username\" value. If the same user is loaded by both keys,\nthen it may be useful to fill both caches when a user is loaded from\neither source:\n\n.. code:: python\n\n    async def user_by_id_batch_fn(ids):\n        users = await gen_users_by_id(ids)\n        for user in users:\n            username_loader.prime(user.username, user)\n        return users\n\n    user_by_id_loader = DataLoader(user_by_id_batch_fn)\n\n    async def username_batch_fn(names):\n        users = await gen_usernames(names)\n        for user in users:\n            user_by_id_loader.prime(user.id, user)\n        return users\n\n    username_loader = DataLoader(username_batch_fn)\n\nCustom Caches\n-------------\n\nDataLoader can optionaly be provided a custom dict instance to use as\nits memoization cache. More specifically, any object that implements the\nmethods ``get()``, ``set()``, ``delete()`` and ``clear()`` can be\nprovided. This allows for custom dicts which implement various `cache\nalgorithms <https://en.wikipedia.org/wiki/Cache_algorithms>`__ to be\nprovided. By default, DataLoader uses the standard\n`dict <https://docs.python.org/3/tutorial/datastructures.html#dictionaries>`__\nwhich simply grows until the DataLoader is released. The default is\nappropriate when requests to your application are short-lived.\n\nVideo Source Code Walkthrough\n-----------------------------\n\n**DataLoader Source Code Walkthrough (YouTube):**\n\n.. |Build Status| image:: https://travis-ci.org/syrusakbary/aiodataloader.svg\n   :target: https://travis-ci.org/syrusakbary/aiodataloader\n.. |Coverage Status| image:: https://coveralls.io/repos/syrusakbary/aiodataloader/badge.svg?branch=master&service=github\n   :target: https://coveralls.io/github/syrusakbary/aiodataloader?branch=master\n\n\n", "description_content_type": "", "docs_url": null, "download_url": "https://github.com/syrusakbary/aiodataloader/releases", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/syrusakbary/aiodataloader", "keywords": "concurrent future deferred aiodataloader", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "aiodataloader-next", "package_url": "https://pypi.org/project/aiodataloader-next/", "platform": "", "project_url": "https://pypi.org/project/aiodataloader-next/", "project_urls": {"Download": "https://github.com/syrusakbary/aiodataloader/releases", "Homepage": "https://github.com/syrusakbary/aiodataloader"}, "release_url": "https://pypi.org/project/aiodataloader-next/0.1.4/", "requires_dist": ["pytest (>=3.6) ; extra == 'test'", "pytest-cov ; extra == 'test'", "coveralls ; extra == 'test'", "mock ; extra == 'test'", "pytest-asyncio ; extra == 'test'"], "requires_python": "", "summary": "Asyncio DataLoader implementation for Python", "version": "0.1.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>This a fork from the\n<a href=\"https://github.com/syrusakbary/aiodataloader/\" rel=\"nofollow\">asyncio DataLoader</a>\nincluding community fixes and Python 3.7+ compatibility.</p>\n<p>DataLoader is a generic utility to be used as part of your application\u2019s\ndata fetching layer to provide a simplified and consistent API over\nvarious remote data sources such as databases or web services via\nbatching and caching.</p>\n<p><a href=\"https://travis-ci.org/syrusakbary/aiodataloader\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/4f968a7169f315f6286d477170afbad1b8abcfe0/68747470733a2f2f7472617669732d63692e6f72672f7379727573616b626172792f61696f646174616c6f616465722e737667\"></a></p>\n<p>A port of the \u201cLoader\u201d API originally developed by [@schrockn][] at\nFacebook in 2010 as a simplifying force to coalesce the sundry key-value\nstore back-end APIs which existed at the time. At Facebook, \u201cLoader\u201d\nbecame one of the implementation details of the \u201cEnt\u201d framework, a\nprivacy-aware data entity loading and caching layer within web server\nproduct code. This ultimately became the underpinning for Facebook\u2019s\nGraphQL server implementation and type definitions.</p>\n<p>DataLoader is a simplified version of this original idea implemented in\nPython for AsyncIO services. DataLoader is often used when implementing\na <a href=\"https://github.com/graphql-python/graphene\" rel=\"nofollow\">graphene</a> service,\nthough it is also broadly useful in other situations.</p>\n<p>DataLoader is provided so that it may be useful not just to build\nGraphQL services with AsyncIO but also as a publicly available reference\nimplementation of this concept in the hopes that it can be ported to\nother languages. If you port DataLoader to another language, please open\nan issue to include a link from this repository.</p>\n<div id=\"getting-started\">\n<h2>Getting Started</h2>\n<p>First, install DataLoader using pip.</p>\n<pre>pip install aiodataloader-next\n</pre>\n<p>To get started, create a <tt>DataLoader</tt>. Each <tt>DataLoader</tt> instance\nrepresents a unique cache. Typically instances are created per request\nwhen used within a web-server like\n<a href=\"https://sanic.readthedocs.io/en/latest/\" rel=\"nofollow\">Sanic</a> if different users\ncan see different things.</p>\n<blockquote>\nNote: DataLoader assumes a AsyncIO environment with <tt>async/await</tt>\navailable only in Python 3.5+.</blockquote>\n</div>\n<div id=\"batching\">\n<h2>Batching</h2>\n<p>Batching is not an advanced feature, it\u2019s DataLoader\u2019s primary feature.\nCreate loaders by providing a batch loading function.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">aiodataloader</span> <span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">UserLoader</span><span class=\"p\">(</span><span class=\"n\">DataLoader</span><span class=\"p\">):</span>\n    <span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">batch_load_fn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">keys</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"k\">await</span> <span class=\"n\">my_batch_get_users</span><span class=\"p\">(</span><span class=\"n\">keys</span><span class=\"p\">)</span>\n\n<span class=\"n\">user_loader</span> <span class=\"o\">=</span> <span class=\"n\">UserLoader</span><span class=\"p\">()</span>\n</pre>\n<p>A batch loading function accepts a Iterable of keys, and returns a\nPromise which resolves to a List of values<a href=\"#batch-function\" rel=\"nofollow\">*</a>.</p>\n<p>Then load individual values from the loader. DataLoader will coalesce\nall individual loads which occur within a single frame of execution (a\nsingle tick of the event loop) and then call your batch function with\nall requested keys.</p>\n<pre><span class=\"n\">user1_future</span> <span class=\"o\">=</span> <span class=\"n\">user_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">user2_future</span> <span class=\"o\">=</span> <span class=\"n\">user_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"n\">user1</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">user1_future</span>\n<span class=\"n\">user2</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">user2_future</span>\n\n<span class=\"n\">user1_invitedby</span> <span class=\"o\">=</span> <span class=\"n\">user_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">user1</span><span class=\"o\">.</span><span class=\"n\">invited_by_id</span><span class=\"p\">)</span>\n<span class=\"n\">user2_invitedby</span> <span class=\"o\">=</span> <span class=\"n\">user_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">user2</span><span class=\"o\">.</span><span class=\"n\">invited_by_id</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"User 1 was invited by\"</span><span class=\"p\">,</span> <span class=\"k\">await</span> <span class=\"n\">user1_invitedby</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"User 2 was invited by\"</span><span class=\"p\">,</span> <span class=\"k\">await</span> <span class=\"n\">user2_invitedby</span><span class=\"p\">)</span>\n</pre>\n<p>A naive application may have issued four round-trips to a backend for\nthe required information, but with DataLoader this application will make\nat most two.</p>\n<p>DataLoader allows you to decouple unrelated parts of your application\nwithout sacrificing the performance of batch data-loading. While the\nloader presents an API that loads individual values, all concurrent\nrequests will be coalesced and presented to your batch loading function.\nThis allows your application to safely distribute data fetching\nrequirements throughout your application and maintain minimal outgoing\ndata requests.</p>\n<div id=\"batch-function\">\n<h3>Batch Function</h3>\n<p>A batch loading function accepts an List of keys, and returns a Future\nwhich resolves to an List of values. There are a few constraints that\nmust be upheld:</p>\n<ul>\n<li>The List of values must be the same length as the List of keys.</li>\n<li>Each index in the List of values must correspond to the same index in\nthe List of keys.</li>\n</ul>\n<p>For example, if your batch function was provided the List of keys:\n<tt>[ 2, 9, 6, 1 ]</tt>, and loading from a back-end service returned the\nvalues:</p>\n<pre><span class=\"p\">{</span> <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"s1\">'name'</span><span class=\"p\">:</span> <span class=\"s1\">'Chicago'</span> <span class=\"p\">}</span>\n<span class=\"p\">{</span> <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'name'</span><span class=\"p\">:</span> <span class=\"s1\">'New York'</span> <span class=\"p\">}</span>\n<span class=\"p\">{</span> <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s1\">'name'</span><span class=\"p\">:</span> <span class=\"s1\">'San Francisco'</span> <span class=\"p\">}</span>\n</pre>\n<p>Our back-end service returned results in a different order than we\nrequested, likely because it was more efficient for it to do so. Also,\nit omitted a result for key <tt>6</tt>, which we can interpret as no value\nexisting for that key.</p>\n<p>To uphold the constraints of the batch function, it must return an List\nof values the same length as the List of keys, and re-order them to\nensure each index aligns with the original keys <tt>[ 2, 9, 6, 1 ]</tt>:</p>\n<pre><span class=\"p\">[</span>\n  <span class=\"p\">{</span> <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s1\">'name'</span><span class=\"p\">:</span> <span class=\"s1\">'San Francisco'</span> <span class=\"p\">},</span>\n  <span class=\"p\">{</span> <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"s1\">'name'</span><span class=\"p\">:</span> <span class=\"s1\">'Chicago'</span> <span class=\"p\">},</span>\n  <span class=\"kc\">None</span><span class=\"p\">,</span>\n  <span class=\"p\">{</span> <span class=\"s1\">'id'</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'name'</span><span class=\"p\">:</span> <span class=\"s1\">'New York'</span> <span class=\"p\">}</span>\n<span class=\"p\">]</span>\n</pre>\n</div>\n</div>\n<div id=\"caching\">\n<h2>Caching</h2>\n<p>DataLoader provides a memoization cache for all loads which occur in a\nsingle request to your application. After <tt>.load()</tt> is called once\nwith a given key, the resulting value is cached to eliminate redundant\nloads.</p>\n<p>In addition to relieving pressure on your data storage, caching results\nper-request also creates fewer objects which may relieve memory pressure\non your application:</p>\n<pre><span class=\"n\">user_future1</span> <span class=\"o\">=</span> <span class=\"n\">user_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">user_future2</span> <span class=\"o\">=</span> <span class=\"n\">user_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"k\">assert</span> <span class=\"n\">user_future1</span> <span class=\"o\">==</span> <span class=\"n\">user_future2</span>\n</pre>\n<div id=\"caching-per-request\">\n<h3>Caching per-Request</h3>\n<p>DataLoader caching <em>does not</em> replace Redis, Memcache, or any other\nshared application-level cache. DataLoader is first and foremost a data\nloading mechanism, and its cache only serves the purpose of not\nrepeatedly loading the same data in the context of a single request to\nyour Application. To do this, it maintains a simple in-memory\nmemoization cache (more accurately: <tt>.load()</tt> is a memoized function).</p>\n<p>Avoid multiple requests from different users using the DataLoader\ninstance, which could result in cached data incorrectly appearing in\neach request. Typically, DataLoader instances are created when a Request\nbegins, and are not used once the Request ends.</p>\n<p>For example, when using with\n<a href=\"https://sanic.readthedocs.io/en/latest/\" rel=\"nofollow\">Sanic</a>:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">create_loaders</span><span class=\"p\">(</span><span class=\"n\">auth_token</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"p\">{</span>\n      <span class=\"s1\">'users'</span><span class=\"p\">:</span> <span class=\"n\">user_loader</span><span class=\"p\">,</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n\n<span class=\"n\">app</span> <span class=\"o\">=</span> <span class=\"n\">Sanic</span><span class=\"p\">(</span><span class=\"vm\">__name__</span><span class=\"p\">)</span>\n\n<span class=\"nd\">@app</span><span class=\"o\">.</span><span class=\"n\">route</span><span class=\"p\">(</span><span class=\"s2\">\"/\"</span><span class=\"p\">)</span>\n<span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">test</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"p\">):</span>\n    <span class=\"n\">auth_token</span> <span class=\"o\">=</span> <span class=\"n\">authenticate_user</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"p\">)</span>\n    <span class=\"n\">loaders</span> <span class=\"o\">=</span> <span class=\"n\">create_loaders</span><span class=\"p\">(</span><span class=\"n\">auth_token</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">render_page</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"p\">,</span> <span class=\"n\">loaders</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"clearing-cache\">\n<h3>Clearing Cache</h3>\n<p>In certain uncommon cases, clearing the request cache may be necessary.</p>\n<p>The most common example when clearing the loader\u2019s cache is necessary is\nafter a mutation or update within the same request, when a cached value\ncould be out of date and future loads should not use any possibly cached\nvalue.</p>\n<p>Here\u2019s a simple example using SQL UPDATE to illustrate.</p>\n<pre><span class=\"c1\"># Request begins...</span>\n<span class=\"n\">user_loader</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n\n<span class=\"c1\"># And a value happens to be loaded (and cached).</span>\n<span class=\"n\">user4</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">user_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># A mutation occurs, invalidating what might be in cache.</span>\n<span class=\"k\">await</span> <span class=\"n\">sql_run</span><span class=\"p\">(</span><span class=\"s1\">'UPDATE users WHERE id=4 SET username=\"zuck\"'</span><span class=\"p\">)</span>\n<span class=\"n\">user_loader</span><span class=\"o\">.</span><span class=\"n\">clear</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Later the value load is loaded again so the mutated data appears.</span>\n<span class=\"n\">user4</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">user_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Request completes.</span>\n</pre>\n</div>\n<div id=\"caching-exceptions\">\n<h3>Caching Exceptions</h3>\n<p>If a batch load fails (that is, a batch function throws or returns a\nrejected Promise), then the requested values will not be cached. However\nif a batch function returns an <tt>Exception</tt> instance for an individual\nvalue, that <tt>Exception</tt> will be cached to avoid frequently loading the\nsame <tt>Exception</tt>.</p>\n<p>In some circumstances you may wish to clear the cache for these\nindividual Errors:</p>\n<pre><span class=\"k\">try</span><span class=\"p\">:</span>\n    <span class=\"n\">user_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"k\">except</span> <span class=\"ne\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n    <span class=\"n\">user_loader</span><span class=\"o\">.</span><span class=\"n\">clear</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"k\">raise</span>\n</pre>\n</div>\n<div id=\"disabling-cache\">\n<h3>Disabling Cache</h3>\n<p>In certain uncommon cases, a DataLoader which <em>does not</em> cache may be\ndesirable. Calling <tt>DataLoader(batch_fn, cache=false)</tt> will ensure\nthat every call to <tt>.load()</tt> will produce a <em>new</em> Future, and\nrequested keys will not be saved in memory.</p>\n<p>However, when the memoization cache is disabled, your batch function\nwill receive an array of keys which may contain duplicates! Each key\nwill be associated with each call to <tt>.load()</tt>. Your batch loader\nshould provide a value for each instance of the requested key.</p>\n<p>For example:</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">MyLoader</span><span class=\"p\">(</span><span class=\"n\">DataLoader</span><span class=\"p\">):</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n    <span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">batch_load_fn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">keys</span><span class=\"p\">):</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">keys</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">keys</span>\n\n<span class=\"n\">my_loader</span> <span class=\"o\">=</span> <span class=\"n\">MyLoader</span><span class=\"p\">()</span>\n\n<span class=\"n\">my_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">'A'</span><span class=\"p\">)</span>\n<span class=\"n\">my_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">'B'</span><span class=\"p\">)</span>\n<span class=\"n\">my_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">'A'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># &gt; [ 'A', 'B', 'A' ]</span>\n</pre>\n<p>More complex cache behavior can be achieved by calling <tt>.clear()</tt> or\n<tt>.clear_all()</tt> rather than disabling the cache completely. For\nexample, this DataLoader will provide unique keys to a batch function\ndue to the memoization cache being enabled, but will immediately clear\nits cache when the batch function is called so later requests will load\nnew values.</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">MyLoader</span><span class=\"p\">(</span><span class=\"n\">DataLoader</span><span class=\"p\">):</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n    <span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">batch_load_fn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">keys</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">clear_all</span><span class=\"p\">()</span>\n        <span class=\"k\">return</span> <span class=\"n\">keys</span>\n</pre>\n</div>\n</div>\n<div id=\"api\">\n<h2>API</h2>\n<h2 id=\"class-dataloader\"><span class=\"section-subtitle\">class DataLoader</span></h2>\n<p>DataLoader creates a public API for loading data from a particular data\nback-end with unique keys such as the <tt>id</tt> column of a SQL table or\ndocument name in a MongoDB database, given a batch loading function.</p>\n<p>Each <tt>DataLoader</tt> instance contains a unique memoized cache. Use\ncaution when used in long-lived applications or those which serve many\nusers with different access permissions and consider creating a new\ninstance per web request.</p>\n<div id=\"new-dataloader-batch-load-fn-options\">\n<h3><tt>new DataLoader(batch_load_fn, **options)</tt></h3>\n<p>Create a new <tt>DataLoader</tt> given a batch loading function and options.</p>\n<ul>\n<li><em>batch_load_fn</em>: An async function (coroutine) which accepts an\nList of keys and returns a Future which resolves to an List of\nvalues.</li>\n<li><em>options</em>:</li>\n<li><em>batch</em>: Default <tt>True</tt>. Set to <tt>False</tt> to disable batching,\ninstead immediately invoking <tt>batch_load_fn</tt> with a single load\nkey.</li>\n<li><em>max_batch_size</em>: Default <tt>Infinity</tt>. Limits the number of items\nthat get passed in to the <tt>batch_load_fn</tt>.</li>\n<li><em>cache</em>: Default <tt>True</tt>. Set to <tt>False</tt> to disable memoization\ncaching, instead creating a new Promise and new key in the\n<tt>batch_load_fn</tt> for every load of the same key.</li>\n<li><em>cache_key_fn</em>: A function to produce a cache key for a given load\nkey. Defaults to <tt>key =&gt; key</tt>. Useful to provide when Python\nobjects are keys and two similarly shaped objects should be\nconsidered equivalent.</li>\n<li><em>cache_map</em>: An instance of\n<a href=\"https://docs.python.org/3/tutorial/datastructures.html#dictionaries\" rel=\"nofollow\">dict</a>\n(or an object with a similar API) to be used as the underlying cache\nfor this loader. Default <tt>{}</tt>.</li>\n</ul>\n</div>\n<div id=\"load-key\">\n<h3><tt>load(key)</tt></h3>\n<p>Loads a key, returning a <tt>Future</tt> for the value represented by that\nkey.</p>\n<ul>\n<li><em>key</em>: An key value to load.</li>\n</ul>\n</div>\n<div id=\"load-many-keys\">\n<h3><tt>load_many(keys)</tt></h3>\n<p>Loads multiple keys, promising an array of values:</p>\n<pre><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">my_loader</span><span class=\"o\">.</span><span class=\"n\">load_many</span><span class=\"p\">([</span> <span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'b'</span> <span class=\"p\">]);</span>\n</pre>\n<p>This is equivalent to the more verbose:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">asyncio</span> <span class=\"kn\">import</span> <span class=\"n\">gather</span>\n<span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">gather</span><span class=\"p\">(</span>\n    <span class=\"n\">my_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">),</span>\n    <span class=\"n\">my_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">'b'</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n</pre>\n<ul>\n<li><em>keys</em>: A list of key values to load.</li>\n</ul>\n</div>\n<div id=\"clear-key\">\n<h3><tt>clear(key)</tt></h3>\n<p>Clears the value at <tt>key</tt> from the cache, if it exists. Returns itself\nfor method chaining.</p>\n<ul>\n<li><em>key</em>: An key value to clear.</li>\n</ul>\n</div>\n<div id=\"clear-all\">\n<h3><tt>clear_all()</tt></h3>\n<p>Clears the entire cache. To be used when some event results in unknown\ninvalidations across this particular <tt>DataLoader</tt>. Returns itself for\nmethod chaining.</p>\n</div>\n<div id=\"prime-key-value\">\n<h3><tt>prime(key, value)</tt></h3>\n<p>Primes the cache with the provided key and value. If the key already\nexists, no change is made. (To forcefully prime the cache, clear the key\nfirst with <tt><span class=\"pre\">loader.clear(key).prime(key,</span> value)</tt>.) Returns itself for\nmethod chaining.</p>\n</div>\n</div>\n<div id=\"using-with-graphql\">\n<h2>Using with GraphQL</h2>\n<p>DataLoader pairs nicely well with\n<a href=\"https://github.com/graphql-python/graphene\" rel=\"nofollow\">GraphQL</a>. GraphQL fields\nare designed to be stand-alone functions. Without a caching or batching\nmechanism, it\u2019s easy for a naive GraphQL server to issue new database\nrequests each time a field is resolved.</p>\n<p>Consider the following GraphQL request:</p>\n<pre>{\n  me {\n    name\n    bestFriend {\n      name\n    }\n    friends(first: 5) {\n      name\n      bestFriend {\n        name\n      }\n    }\n  }\n}\n</pre>\n<p>Naively, if <tt>me</tt>, <tt>bestFriend</tt> and <tt>friends</tt> each need to request\nthe backend, there could be at most 13 database requests!</p>\n<p>When using DataLoader, we could define the <tt>User</tt> type using the\n<a href=\"examples/SQL.md\" rel=\"nofollow\">SQLite</a> example with clearer code and at most 4\ndatabase requests, and possibly fewer if there are cache hits.</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">User</span><span class=\"p\">(</span><span class=\"n\">graphene</span><span class=\"o\">.</span><span class=\"n\">ObjectType</span><span class=\"p\">):</span>\n    <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"n\">graphene</span><span class=\"o\">.</span><span class=\"n\">String</span><span class=\"p\">()</span>\n    <span class=\"n\">best_friend</span> <span class=\"o\">=</span> <span class=\"n\">graphene</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">User</span><span class=\"p\">)</span>\n    <span class=\"n\">friends</span> <span class=\"o\">=</span> <span class=\"n\">graphene</span><span class=\"o\">.</span><span class=\"n\">List</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">User</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">resolve_best_friend</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"p\">,</span> <span class=\"n\">info</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">user_loader</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">best_friend_id</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">resolve_friends</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"p\">,</span> <span class=\"n\">info</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">user_loader</span><span class=\"o\">.</span><span class=\"n\">load_many</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">friend_ids</span><span class=\"p\">)</span>\n</pre>\n</div>\n<div id=\"common-patterns\">\n<h2>Common Patterns</h2>\n<div id=\"creating-a-new-dataloader-per-request\">\n<h3>Creating a new DataLoader per request.</h3>\n<p>In many applications, a web server using DataLoader serves requests to\nmany different users with different access permissions. It may be\ndangerous to use one cache across many users, and is encouraged to\ncreate a new DataLoader per request:</p>\n<pre><span class=\"k\">def</span> <span class=\"nf\">create_loaders</span><span class=\"p\">(</span><span class=\"n\">auth_token</span><span class=\"p\">):</span>\n  <span class=\"k\">return</span> <span class=\"p\">{</span>\n    <span class=\"s1\">'users'</span><span class=\"p\">:</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">ids</span><span class=\"p\">:</span> <span class=\"n\">gen_users</span><span class=\"p\">(</span><span class=\"n\">auth_token</span><span class=\"p\">,</span> <span class=\"n\">ids</span><span class=\"p\">)),</span>\n    <span class=\"s1\">'cdn_urls'</span><span class=\"p\">:</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">raw_urls</span><span class=\"p\">:</span> <span class=\"n\">gen_cdn_urls</span><span class=\"p\">(</span><span class=\"n\">auth_token</span><span class=\"p\">,</span> <span class=\"n\">raw_urls</span><span class=\"p\">)),</span>\n    <span class=\"s1\">'stories'</span><span class=\"p\">:</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">keys</span><span class=\"p\">:</span> <span class=\"n\">gen_stories</span><span class=\"p\">(</span><span class=\"n\">auth_token</span><span class=\"p\">,</span> <span class=\"n\">keys</span><span class=\"p\">)),</span>\n  <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"c1\"># When handling an incoming web request:</span>\n<span class=\"n\">loaders</span> <span class=\"o\">=</span> <span class=\"n\">create_loaders</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">query</span><span class=\"o\">.</span><span class=\"n\">auth_token</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Then, within application logic:</span>\n<span class=\"n\">user</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">loaders</span><span class=\"o\">.</span><span class=\"n\">users</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"n\">pic</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">loaders</span><span class=\"o\">.</span><span class=\"n\">cdn_urls</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">user</span><span class=\"o\">.</span><span class=\"n\">raw_pic_url</span><span class=\"p\">)</span>\n</pre>\n<p>Creating an object where each key is a <tt>DataLoader</tt> is one common\npattern which provides a single value to pass around to code which needs\nto perform data loading, such as part of the <tt>root_value</tt> in a\n[graphql][] request.</p>\n</div>\n<div id=\"loading-by-alternative-keys\">\n<h3>Loading by alternative keys.</h3>\n<p>Occasionally, some kind of value can be accessed in multiple ways. For\nexample, perhaps a \u201cUser\u201d type can be loaded not only by an \u201cid\u201d but\nalso by a \u201cusername\u201d value. If the same user is loaded by both keys,\nthen it may be useful to fill both caches when a user is loaded from\neither source:</p>\n<pre><span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">user_by_id_batch_fn</span><span class=\"p\">(</span><span class=\"n\">ids</span><span class=\"p\">):</span>\n    <span class=\"n\">users</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">gen_users_by_id</span><span class=\"p\">(</span><span class=\"n\">ids</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">user</span> <span class=\"ow\">in</span> <span class=\"n\">users</span><span class=\"p\">:</span>\n        <span class=\"n\">username_loader</span><span class=\"o\">.</span><span class=\"n\">prime</span><span class=\"p\">(</span><span class=\"n\">user</span><span class=\"o\">.</span><span class=\"n\">username</span><span class=\"p\">,</span> <span class=\"n\">user</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">users</span>\n\n<span class=\"n\">user_by_id_loader</span> <span class=\"o\">=</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">user_by_id_batch_fn</span><span class=\"p\">)</span>\n\n<span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">username_batch_fn</span><span class=\"p\">(</span><span class=\"n\">names</span><span class=\"p\">):</span>\n    <span class=\"n\">users</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">gen_usernames</span><span class=\"p\">(</span><span class=\"n\">names</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">user</span> <span class=\"ow\">in</span> <span class=\"n\">users</span><span class=\"p\">:</span>\n        <span class=\"n\">user_by_id_loader</span><span class=\"o\">.</span><span class=\"n\">prime</span><span class=\"p\">(</span><span class=\"n\">user</span><span class=\"o\">.</span><span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">user</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">users</span>\n\n<span class=\"n\">username_loader</span> <span class=\"o\">=</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">username_batch_fn</span><span class=\"p\">)</span>\n</pre>\n</div>\n</div>\n<div id=\"custom-caches\">\n<h2>Custom Caches</h2>\n<p>DataLoader can optionaly be provided a custom dict instance to use as\nits memoization cache. More specifically, any object that implements the\nmethods <tt>get()</tt>, <tt>set()</tt>, <tt>delete()</tt> and <tt>clear()</tt> can be\nprovided. This allows for custom dicts which implement various <a href=\"https://en.wikipedia.org/wiki/Cache_algorithms\" rel=\"nofollow\">cache\nalgorithms</a> to be\nprovided. By default, DataLoader uses the standard\n<a href=\"https://docs.python.org/3/tutorial/datastructures.html#dictionaries\" rel=\"nofollow\">dict</a>\nwhich simply grows until the DataLoader is released. The default is\nappropriate when requests to your application are short-lived.</p>\n</div>\n<div id=\"video-source-code-walkthrough\">\n<h2>Video Source Code Walkthrough</h2>\n<p><strong>DataLoader Source Code Walkthrough (YouTube):</strong></p>\n</div>\n\n          </div>"}, "last_serial": 5799351, "releases": {"0.1.3": [{"comment_text": "", "digests": {"md5": "caeb6b622f9a854271f6e2c14cf249b1", "sha256": "d9d2a665b3602a0d000991e425ca82c975283765aa5b4ea8198368d0dadd59e0"}, "downloads": -1, "filename": "aiodataloader_next-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "caeb6b622f9a854271f6e2c14cf249b1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10746, "upload_time": "2019-09-08T13:10:19", "upload_time_iso_8601": "2019-09-08T13:10:19.240526Z", "url": "https://files.pythonhosted.org/packages/25/be/71234347f63d822a9dec0de4421152b914c7b1b51733756c75c03c759688/aiodataloader_next-0.1.3-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "761b2295192fa4296a360454cae82fe5", "sha256": "50b10700f29817ba95c166695f2b987c5ecee4502d192a50e92c09b999fd9dc4"}, "downloads": -1, "filename": "aiodataloader-next-0.1.3.tar.gz", "has_sig": false, "md5_digest": "761b2295192fa4296a360454cae82fe5", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13082, "upload_time": "2019-09-08T13:10:22", "upload_time_iso_8601": "2019-09-08T13:10:22.080128Z", "url": "https://files.pythonhosted.org/packages/f7/9b/23629f05fd1f1639afc04c79e1b5c04312271b8a6f0b0a6aed0ddd6e4a53/aiodataloader-next-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "af3e89e60565d2212733696a6dc27076", "sha256": "bc2b4ac9cbbed1a93f114c833f5b38cd87d940c153baf9e62adddd95aca71577"}, "downloads": -1, "filename": "aiodataloader_next-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "af3e89e60565d2212733696a6dc27076", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10775, "upload_time": "2019-09-08T13:53:45", "upload_time_iso_8601": "2019-09-08T13:53:45.770456Z", "url": "https://files.pythonhosted.org/packages/7d/34/5a8c6dfecccc4ce49616bd16b18f20566a1a79658de596119cb261f2573d/aiodataloader_next-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5d1ac83b9e1db36f6d786a2d2b9de60d", "sha256": "12547985a2b9ec2341257c32977c6f5d7722e4104d51cf348ee0776066c56228"}, "downloads": -1, "filename": "aiodataloader-next-0.1.4.tar.gz", "has_sig": false, "md5_digest": "5d1ac83b9e1db36f6d786a2d2b9de60d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13143, "upload_time": "2019-09-08T13:53:47", "upload_time_iso_8601": "2019-09-08T13:53:47.509923Z", "url": "https://files.pythonhosted.org/packages/2b/95/85b4ffdf7379dd125d36599c9b036f9da7ac184b37b3e5207cabada52872/aiodataloader-next-0.1.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "af3e89e60565d2212733696a6dc27076", "sha256": "bc2b4ac9cbbed1a93f114c833f5b38cd87d940c153baf9e62adddd95aca71577"}, "downloads": -1, "filename": "aiodataloader_next-0.1.4-py3-none-any.whl", "has_sig": false, "md5_digest": "af3e89e60565d2212733696a6dc27076", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 10775, "upload_time": "2019-09-08T13:53:45", "upload_time_iso_8601": "2019-09-08T13:53:45.770456Z", "url": "https://files.pythonhosted.org/packages/7d/34/5a8c6dfecccc4ce49616bd16b18f20566a1a79658de596119cb261f2573d/aiodataloader_next-0.1.4-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5d1ac83b9e1db36f6d786a2d2b9de60d", "sha256": "12547985a2b9ec2341257c32977c6f5d7722e4104d51cf348ee0776066c56228"}, "downloads": -1, "filename": "aiodataloader-next-0.1.4.tar.gz", "has_sig": false, "md5_digest": "5d1ac83b9e1db36f6d786a2d2b9de60d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13143, "upload_time": "2019-09-08T13:53:47", "upload_time_iso_8601": "2019-09-08T13:53:47.509923Z", "url": "https://files.pythonhosted.org/packages/2b/95/85b4ffdf7379dd125d36599c9b036f9da7ac184b37b3e5207cabada52872/aiodataloader-next-0.1.4.tar.gz", "yanked": false}], "timestamp": "Thu May  7 16:21:41 2020"}