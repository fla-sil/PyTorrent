{"info": {"author": "Michael Harms", "author_email": "michaelharms95@icloud.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Topic :: Scientific/Engineering", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Python Modules", "Typing :: Typed"], "description": "# comcrawl\n\n![GitHub Workflow Status](https://img.shields.io/github/workflow/status/michaelharms/comcrawl/CI)\n[![codecov](https://codecov.io/gh/michaelharms/comcrawl/branch/master/graph/badge.svg?token=FEw4KEcpRm)](https://codecov.io/gh/michaelharms/comcrawl)\n![GitHub](https://img.shields.io/github/license/michaelharms/comcrawl)\n\n_comcrawl_ is a python package for easily querying and downloading pages from [commoncrawl.org](https://commoncrawl.org).\n\n## Introduction\n\nI was inspired to make _comcrawl_ by reading this [article](https://www.bellingcat.com/resources/2015/08/13/using-python-to-mine-common-crawl/).\n\n**Note:** I made this for personal projects and for fun. Thus this package is intended for use in small to medium projects, because it is not optimized for handling gigabytes or terrabytes of data. You might want to check out [cdx-toolkit](https://pypi.org/project/cdx-toolkit/) or [cdx-index-client](https://github.com/ikreymer/cdx-index-client) in such cases.\n\n### What is Common Crawl?\n\nThe Common Crawl project is an _\"open repository of web crawl data that can be accessed and analyzed by anyone\"_.\nIt contains billions of web pages and is often used for NLP projects to gather large amounts of text data.\n\nCommon Crawl provides a [search index](https://index.commoncrawl.org), which you can use to search for certain URLs in their crawled data.\nEach search result contains a link and byte offset to a specific location in their [AWS S3 buckets](https://commoncrawl.s3.amazonaws.com/cc-index/collections/index.html) to download the page.\n\n### What does _comcrawl_ offer?\n\n_comcrawl_ simplifies this process of searching and downloading from Common Crawl by offering a simple API interface you can use in your python program.\n\n## Installation\n\n_comcrawl_ is available on PyPI.\n\nInstall it via pip by running the following command from your terminal:\n\n```\npip install comcrawl\n```\n\n## Usage\n\n### Basic\n\nThe HTML for each page will be available as a string in the 'html' key in each results dictionary after calling the `download` method.\n\n```python\nfrom comcrawl import IndexClient\n\nclient = IndexClient()\n\nclient.search(\"reddit.com/r/MachineLearning/*\")\nclient.download()\n\nfirst_page_html = client.results[0][\"html\"]\n```\n\n### Multithreading\n\nYou can leverage multithreading while searching or downloading by specifying the number of threads you want to use.\n\nPlease keep in mind to not overdo this, so you don't put too much stress on the Common Crawl servers (have a look at [Code of Conduct](#code-of-conduct)).\n\n```python\nfrom comcrawl import IndexClient\n\nclient = IndexClient()\n\nclient.search(\"reddit.com/r/MachineLearning/*\", threads=4)\nclient.download(threads=4)\n```\n\n### Removing duplicates & Saving\n\nYou can easily combine this package with the [pandas](https://github.com/pandas-dev/pandas) library, to filter out duplicate results and persist them to disk:\n\n```python\nfrom comcrawl import IndexClient\nimport pandas as pd\n\nclient = IndexClient()\nclient.search(\"reddit.com/r/MachineLearning/*\")\n\nclient.results = (pd.DataFrame(client.results)\n                  .sort_values(by=\"timestamp\")\n                  .drop_duplicates(\"urlkey\", keep=\"last\")\n                  .to_dict(\"records\"))\n\nclient.download()\n\npd.DataFrame(client.results).to_csv(\"results.csv\")\n```\n\nThe urlkey alone might not be sufficient here, so you might want to write a function to compute a custom id from the results' properties for the removal of duplicates.\n\n### Searching subsets of Indexes\n\nBy default, when instantiated, the `IndexClient` fetches a list of currently available Common Crawl indexes to search. You can also restrict the search to certain Common Crawl Indexes, by specifying them as a list.\n\n```python\nfrom comcrawl import IndexClient\n\nclient = IndexClient([\"2019-51\", \"2019-47\"])\nclient.search(\"reddit.com/r/MachineLearning/*\")\nclient.download()\n```\n\n### Logging HTTP requests\n\nWhen debugging your code, you can enable logging of all HTTP requests that are made.\n\n```python\nfrom comcrawl import IndexClient\n\nclient = IndexClient(verbose=True)\nclient.search(\"reddit.com/r/MachineLearning/*\")\nclient.download()\n```\n\n## Code of Conduct\n\nWhen accessing Common Crawl, please beware these guidelines posted by one of the Common Crawl maintainers:\n\nhttps://groups.google.com/forum/#!msg/common-crawl/3QmQjFA_3y4/vTbhGqIBBQAJ\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/michaelharms/comcrawl", "keywords": "commoncrawl,machine-learning,nlp,data,common-crawl,scraping,deep-learning,training-dataset", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "comcrawl", "package_url": "https://pypi.org/project/comcrawl/", "platform": "", "project_url": "https://pypi.org/project/comcrawl/", "project_urls": {"Homepage": "https://github.com/michaelharms/comcrawl", "Repository": "https://github.com/michaelharms/comcrawl"}, "release_url": "https://pypi.org/project/comcrawl/1.0.1/", "requires_dist": ["requests (>=2.22.0,<3.0.0)"], "requires_python": ">=3.6,<4.0", "summary": "A python utility for downloading Common Crawl data.", "version": "1.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>comcrawl</h1>\n<p><img alt=\"GitHub Workflow Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/d63991946501215639cebbf2bb188307908f4cdf/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f6d69636861656c6861726d732f636f6d637261776c2f4349\">\n<a href=\"https://codecov.io/gh/michaelharms/comcrawl\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/27aacb671c6d4dfbaeddded4f4db15d4d91ba7c6/68747470733a2f2f636f6465636f762e696f2f67682f6d69636861656c6861726d732f636f6d637261776c2f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d464577344b456370526d\"></a>\n<img alt=\"GitHub\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7c2ef5894f859a8e13b2f2281689daf6b099bd1c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d69636861656c6861726d732f636f6d637261776c\"></p>\n<p><em>comcrawl</em> is a python package for easily querying and downloading pages from <a href=\"https://commoncrawl.org\" rel=\"nofollow\">commoncrawl.org</a>.</p>\n<h2>Introduction</h2>\n<p>I was inspired to make <em>comcrawl</em> by reading this <a href=\"https://www.bellingcat.com/resources/2015/08/13/using-python-to-mine-common-crawl/\" rel=\"nofollow\">article</a>.</p>\n<p><strong>Note:</strong> I made this for personal projects and for fun. Thus this package is intended for use in small to medium projects, because it is not optimized for handling gigabytes or terrabytes of data. You might want to check out <a href=\"https://pypi.org/project/cdx-toolkit/\" rel=\"nofollow\">cdx-toolkit</a> or <a href=\"https://github.com/ikreymer/cdx-index-client\" rel=\"nofollow\">cdx-index-client</a> in such cases.</p>\n<h3>What is Common Crawl?</h3>\n<p>The Common Crawl project is an <em>\"open repository of web crawl data that can be accessed and analyzed by anyone\"</em>.\nIt contains billions of web pages and is often used for NLP projects to gather large amounts of text data.</p>\n<p>Common Crawl provides a <a href=\"https://index.commoncrawl.org\" rel=\"nofollow\">search index</a>, which you can use to search for certain URLs in their crawled data.\nEach search result contains a link and byte offset to a specific location in their <a href=\"https://commoncrawl.s3.amazonaws.com/cc-index/collections/index.html\" rel=\"nofollow\">AWS S3 buckets</a> to download the page.</p>\n<h3>What does <em>comcrawl</em> offer?</h3>\n<p><em>comcrawl</em> simplifies this process of searching and downloading from Common Crawl by offering a simple API interface you can use in your python program.</p>\n<h2>Installation</h2>\n<p><em>comcrawl</em> is available on PyPI.</p>\n<p>Install it via pip by running the following command from your terminal:</p>\n<pre><code>pip install comcrawl\n</code></pre>\n<h2>Usage</h2>\n<h3>Basic</h3>\n<p>The HTML for each page will be available as a string in the 'html' key in each results dictionary after calling the <code>download</code> method.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">comcrawl</span> <span class=\"kn\">import</span> <span class=\"n\">IndexClient</span>\n\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">IndexClient</span><span class=\"p\">()</span>\n\n<span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"p\">(</span><span class=\"s2\">\"reddit.com/r/MachineLearning/*\"</span><span class=\"p\">)</span>\n<span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">()</span>\n\n<span class=\"n\">first_page_html</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">results</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s2\">\"html\"</span><span class=\"p\">]</span>\n</pre>\n<h3>Multithreading</h3>\n<p>You can leverage multithreading while searching or downloading by specifying the number of threads you want to use.</p>\n<p>Please keep in mind to not overdo this, so you don't put too much stress on the Common Crawl servers (have a look at <a href=\"#code-of-conduct\" rel=\"nofollow\">Code of Conduct</a>).</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">comcrawl</span> <span class=\"kn\">import</span> <span class=\"n\">IndexClient</span>\n\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">IndexClient</span><span class=\"p\">()</span>\n\n<span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"p\">(</span><span class=\"s2\">\"reddit.com/r/MachineLearning/*\"</span><span class=\"p\">,</span> <span class=\"n\">threads</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">(</span><span class=\"n\">threads</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n</pre>\n<h3>Removing duplicates &amp; Saving</h3>\n<p>You can easily combine this package with the <a href=\"https://github.com/pandas-dev/pandas\" rel=\"nofollow\">pandas</a> library, to filter out duplicate results and persist them to disk:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">comcrawl</span> <span class=\"kn\">import</span> <span class=\"n\">IndexClient</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">IndexClient</span><span class=\"p\">()</span>\n<span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"p\">(</span><span class=\"s2\">\"reddit.com/r/MachineLearning/*\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">results</span><span class=\"p\">)</span>\n                  <span class=\"o\">.</span><span class=\"n\">sort_values</span><span class=\"p\">(</span><span class=\"n\">by</span><span class=\"o\">=</span><span class=\"s2\">\"timestamp\"</span><span class=\"p\">)</span>\n                  <span class=\"o\">.</span><span class=\"n\">drop_duplicates</span><span class=\"p\">(</span><span class=\"s2\">\"urlkey\"</span><span class=\"p\">,</span> <span class=\"n\">keep</span><span class=\"o\">=</span><span class=\"s2\">\"last\"</span><span class=\"p\">)</span>\n                  <span class=\"o\">.</span><span class=\"n\">to_dict</span><span class=\"p\">(</span><span class=\"s2\">\"records\"</span><span class=\"p\">))</span>\n\n<span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">()</span>\n\n<span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">results</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to_csv</span><span class=\"p\">(</span><span class=\"s2\">\"results.csv\"</span><span class=\"p\">)</span>\n</pre>\n<p>The urlkey alone might not be sufficient here, so you might want to write a function to compute a custom id from the results' properties for the removal of duplicates.</p>\n<h3>Searching subsets of Indexes</h3>\n<p>By default, when instantiated, the <code>IndexClient</code> fetches a list of currently available Common Crawl indexes to search. You can also restrict the search to certain Common Crawl Indexes, by specifying them as a list.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">comcrawl</span> <span class=\"kn\">import</span> <span class=\"n\">IndexClient</span>\n\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">IndexClient</span><span class=\"p\">([</span><span class=\"s2\">\"2019-51\"</span><span class=\"p\">,</span> <span class=\"s2\">\"2019-47\"</span><span class=\"p\">])</span>\n<span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"p\">(</span><span class=\"s2\">\"reddit.com/r/MachineLearning/*\"</span><span class=\"p\">)</span>\n<span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">()</span>\n</pre>\n<h3>Logging HTTP requests</h3>\n<p>When debugging your code, you can enable logging of all HTTP requests that are made.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">comcrawl</span> <span class=\"kn\">import</span> <span class=\"n\">IndexClient</span>\n\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">IndexClient</span><span class=\"p\">(</span><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"p\">(</span><span class=\"s2\">\"reddit.com/r/MachineLearning/*\"</span><span class=\"p\">)</span>\n<span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">()</span>\n</pre>\n<h2>Code of Conduct</h2>\n<p>When accessing Common Crawl, please beware these guidelines posted by one of the Common Crawl maintainers:</p>\n<p><a href=\"https://groups.google.com/forum/#!msg/common-crawl/3QmQjFA_3y4/vTbhGqIBBQAJ\" rel=\"nofollow\">https://groups.google.com/forum/#!msg/common-crawl/3QmQjFA_3y4/vTbhGqIBBQAJ</a></p>\n\n          </div>"}, "last_serial": 6772258, "releases": {"1.0.1": [{"comment_text": "", "digests": {"md5": "741a19873a51bac033fbfa92627d7c43", "sha256": "84571175a76aca64272a8b263d8943e4d97f56a021194d6b62ff8fc39c9880d1"}, "downloads": -1, "filename": "comcrawl-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "741a19873a51bac033fbfa92627d7c43", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4.0", "size": 9947, "upload_time": "2020-03-08T15:48:35", "upload_time_iso_8601": "2020-03-08T15:48:35.832312Z", "url": "https://files.pythonhosted.org/packages/9e/50/54e114158b84a4f438e222ec176a894ded5ee76b891cff7f43c0398161ce/comcrawl-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d8c3cc4ab93091cff6059fd9fa573129", "sha256": "940622df9adad502b3808c189e187ab5338dcd4189f524d63b4cf0b4719a23e8"}, "downloads": -1, "filename": "comcrawl-1.0.1.tar.gz", "has_sig": false, "md5_digest": "d8c3cc4ab93091cff6059fd9fa573129", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4.0", "size": 8865, "upload_time": "2020-03-08T15:48:37", "upload_time_iso_8601": "2020-03-08T15:48:37.756723Z", "url": "https://files.pythonhosted.org/packages/da/bc/d35a7043e1aa5a8bd928601227983dd5c53e2dd29bc44432193de1188012/comcrawl-1.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "741a19873a51bac033fbfa92627d7c43", "sha256": "84571175a76aca64272a8b263d8943e4d97f56a021194d6b62ff8fc39c9880d1"}, "downloads": -1, "filename": "comcrawl-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "741a19873a51bac033fbfa92627d7c43", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6,<4.0", "size": 9947, "upload_time": "2020-03-08T15:48:35", "upload_time_iso_8601": "2020-03-08T15:48:35.832312Z", "url": "https://files.pythonhosted.org/packages/9e/50/54e114158b84a4f438e222ec176a894ded5ee76b891cff7f43c0398161ce/comcrawl-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d8c3cc4ab93091cff6059fd9fa573129", "sha256": "940622df9adad502b3808c189e187ab5338dcd4189f524d63b4cf0b4719a23e8"}, "downloads": -1, "filename": "comcrawl-1.0.1.tar.gz", "has_sig": false, "md5_digest": "d8c3cc4ab93091cff6059fd9fa573129", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6,<4.0", "size": 8865, "upload_time": "2020-03-08T15:48:37", "upload_time_iso_8601": "2020-03-08T15:48:37.756723Z", "url": "https://files.pythonhosted.org/packages/da/bc/d35a7043e1aa5a8bd928601227983dd5c53e2dd29bc44432193de1188012/comcrawl-1.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:44:28 2020"}