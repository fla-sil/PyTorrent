{"info": {"author": "Gorka Eguileor", "author_email": "gorka@eguileor.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Information Technology", "License :: OSI Approved :: Apache Software License", "Natural Language :: English", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7"], "description": "# Ember CSI\n\n[![Docker build status](https://img.shields.io/docker/build/akrog/ember-csi.svg)](https://hub.docker.com/r/akrog/ember-csi/) [![Docker build](https://img.shields.io/docker/automated/akrog/ember-csi.svg)](https://hub.docker.com/r/akrog/ember-csi/builds/) [![PyPi](https://img.shields.io/pypi/v/ember_csi.svg)](https://pypi.python.org/pypi/ember_csi) [![PyVersion](https://img.shields.io/pypi/pyversions/ember_csi.svg)](https://pypi.python.org/pypi/ember_csi) [![License](https://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0)\n\n\nMulti-vendor CSI plugin driver supporting over 80 storage drivers in a single plugin to provide `block` and `mount` storage to Container Orchestration systems.\n\n* Free software: Apache Software License 2.0\n* Documentation: Pending\n\n\n## Features\n\nThis CSI driver is up to date with latest CSI specs including the [new snapshots feature](https://github.com/container-storage-interface/spec/pull/224) recently introduced.\n\nCurrently supported features are:\n\n- Create block volume\n- Creating snapshots\n- Creating a block volume from a snapshot\n- Delete block volume\n- Deleting snapshots\n- Listing volumes with pagination\n- Listing snapshots with pagination\n- Attaching volumes\n- Detaching volumes\n- Reporting storage capacity\n- Probing the node\n- Retrieving the plugin info\n\n\n## Runtime Dependencies\n\nThis driver requires that Cinder v11.0 (OSP-12/Pike) is already installed in the system, how this is accomplished is left to the installer, as there are multiple ways this can be accomplished:\n\n- From OSP repositories\n- From RDO repositories\n- From github\n- From other repositories\n\nAny other basic requirement is already handled by `ember-csi` when installing from PyPi.\n\nBesides the basic dependencies there are also some drivers that have additional requirements that must be met for proper operation of the driver and/or attachment/detachment operations, just like in Cinder.\n\nSome of these Python dependencies for the Controller servicer are:\n\n- DRBD: dbus and drbdmanage\n- HPE 3PAR: python-3parclient\n- Kaminario: krest\n- Pure: purestorage\n- Dell EMC VMAX, IBM DS8K: pyOpenSSL\n- HPE Lefthad: python-lefthandclient\n- Fujitsu Eternus DX: pywbem\n- IBM XIV: pyxcli\n- RBD: rados and rbd\n- Dell EMC VNX: storops\n- Violin: vmemclient\n- INFINIDAT: infinisdk, capacity, infy.dtypes.wwn, infi.dtypes.iqn\n\nOther backends may also require additional packages, for example LVM on CentOS/RHEL requires the `targetcli` package, so please check with your hardware vendor.\n\nBesides the Controller requirements there are usually requirements for the Node servicer needed to handle the attaching and detaching of volumes to the node based on the connection used to access the storage.  For example:\n\n- iSCSI: iscsi-initiator-tools and device-mapper-multipath\n- RBD/Ceph: ceph-common package\n\n\n## Installation\n\nFirst we need to install the Cinder Python package, for example to install from RDO on CentOS:\n\n```\n    $ sudo yum install -y centos-release-openstack-pike\n    $ sudo yum install -y openstack-cinder python-pip\n```\n\n\nThen we just need to install the `ember-csi` package:\n\n```\n    $ sudo pip install ember-csi\n```\n\n\nNow we should install any additional package required by our backend.\n\nFor iSCSI backends we'll want to install:\n\n```\n    $ sudo yum install iscsi-initiator-utils\n    $ sudo yum install device-mapper-multipath\n    $ sudo mpathconf --enable --with_multipathd y --user_friendly_names n --find_multipaths y\n```\n\n\nFor RBD we'll also need a specific package:\n\n```\n    $ sudo yum install ceph-common\n```\n\n\n## Configuration\n\nThe CSI driver is configured via environmental variables, any value that doesn't have a default is a required value.\n\n| Name                       | Role       | Description                                                   | Default                                                                                                                                                                                                                                                       | Example                                                                                                                                                                           |\n| -------------------------- | ---------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `CSI_ENDPOINT`             | all        | IP and port to bind the service                               | [::]:50051                                                                                                                                                                                                                                                    | 192.168.1.22:50050                                                                                                                                                                |\n| `CSI_MODE`                 | all        | Role the service should perform: controller, node, all        | all                                                                                                                                                                                                                                                           | controller                                                                                                                                                                        |\n| `X_CSI_SPEC_VERSION`       | all        | CSI Spec version to run. Supported v0.2 and v1.0              | v0.2.0                                                                                                                                                                                                                                                        | 0.2.0                                                                                                                                                                             |\n| `X_CSI_STORAGE_NW_IP`      | node       | IP address in the Node used to connect to the storage         | IP resolved from Node's fqdn                                                                                                                                                                                                                                  | 192.168.1.22                                                                                                                                                                      |\n| `X_CSI_NODE_ID`            | node       | ID used by this node to identify itself to the controller     | Node's fqdn                                                                                                                                                                                                                                                   | csi_test_node                                                                                                                                                                     |\n| `X_CSI_PERSISTENCE_CONFIG` | all        | Configuration of the `cinderlib` metadata persistence plugin. | {\"storage\": \"crd\", \"namespace\": \"default\"}                                                                                                                                                                                                                    | {\"storage\": \"db\", \"connection\": \"mysql+pymysql://root:stackdb@192.168.1.1/cinder?charset=utf8\"}                                                                                   |\n| `X_CSI_EMBER_CONFIG`       | all        | Global `Ember` and `cinderlib` configuration                  | {\"project_id\": \"ember-csi.io\", \"user_id\": \"ember-csi.io\", \"root_helper\": \"sudo\", \"request_multipath\": false, \"plugin_name\": \"\", \"file_locks_path\": \"/var/lib/ember-csi/locks\", \"name\": \"io.ember-csi\", \"grpc_workers\": 30, \"enable_probe\": false}             | {\"project_id\":\"k8s project\",\"user_id\":\"csi driver\",\"root_helper\":\"sudo\",\"plugin_name\": \"external-ceph\"}                                                              |\n| `X_CSI_BACKEND_CONFIG`     | controller | Driver configuration                                          |                                                                                                                                                                                                                                                               | {\"name\": \"rbd\", \"driver\": \"RBD\", \"rbd_user\": \"cinder\", \"rbd_pool\": \"volumes\", \"rbd_ceph_conf\": \"/etc/ceph/ceph.conf\", \"rbd_keyring_conf\": \"/etc/ceph/ceph.client.cinder.keyring\"} |\n| `X_CSI_DEFAULT_MOUNT_FS`   | node       | Default mount filesystem when missing in publish calls        | ext4                                                                                                                                                                                                                                                          | btrfs                                                                                                                                                                             |\n| `X_CSI_SYSTEM_FILES`       | all        | All required storage driver-specific files archived in tar, tar.gz or tar.bz2 format|                                                                                                                                                                                                                                         | /path/to/etc-ceph.tar.gz                                                                                                                                                          |\n| `X_CSI_DEBUG_MODE`         | all        | Debug mode (rpdb, pdb) to use. Disabled by default.           |                                                                                                                                                                                                                                                               | rpdb                                                                                                                                                                              |\n| `X_CSI_ABORT_DUPLICATES`   | all        | If we want to abort or queue (default) duplicated requests.   | false                                                                                                                                                                                                                                                         | true                                                                                                                                                                              |\n\nThe only role that has been tested at the moment is the default one, where Controller and Node servicer are executed in the same service (`CSI_MODE=all`), and other modes are expected to have issues at the moment.\n\nThe X_CSI_SYSTEM_FILES variable should point to a tar/tar.gz/tar.bz2 file accessible in the Ember CSI driver's filesystem. The contents of the archive will be extracted into '/'. A trusted user such as an operator/administrator with privileged access must create the archive before starting the driver.\n\ne.g.\n\n```\n$ tar cvf ceph-files.tar /etc/ceph/ceph.conf /etc/ceph/ceph.client.cinder.keyring\ntar: Removing leading `/' from member names\n/etc/ceph/ceph.conf\n/etc/ceph/ceph.client.cinder.keyring\n$ export X_CSI_SYSTEM_FILES=`pwd`/ceph-files.tar\n```\n\n## Starting the plugin\n\nOnce we have installed `ember-csi` and required dependencies (for the backend and for the connection type) we just have to run the `ember-csi` service with a user that can do passwordless sudo:\n\n```\n    $ ember-csi\n```\n\n\n## Testing the plugin\n\nThere are several examples of running the Ember CSI plugin in the `examples` directory both for a baremetal deployment and a containerized version of the driver.\n\nIn all cases we have to run the plugin first before we can test it, and for that we have to check the configuration provided as a test before starting the plugin.  By default all examples run the service on port 50051.\n\n\n### Baremetal\n\nFor example to test with the LVM driver on our development environment we can just run the following commands from the root of the `ember-csi` project:\n\n*Note*: The iscsi IP addresses are auto-assigned in the [lvm](examples/baremetal/lvm) env file. You may change these IP addresses if desired:\n\n```\n    $ cd tmp\n    $ sudo dd if=/dev/zero of=ember-volumes bs=1048576 seek=22527 count=1\n    $ lodevice=`sudo losetup --show -f ./ember-volumes`\n    $ sudo pvcreate $lodevice\n    $ sudo vgcreate ember-volumes $lodevice\n    $ sudo vgscan --cache\n    $ cd ../examples/baremetal\n    $ ./run.sh lvm\n    py27 develop-inst-nodeps: /home/geguileo/code/ember-csi\n    py27 installed: ...\n    ___ summary ___\n      py27: skipped tests\n      congratulations :)\n    Starting Ember CSI v0.0.2 (cinderlib: v0.2.1, cinder: v11.1.2.dev5, CSI spec: v0.2.0)\n    Supported filesystems are: fat, ext4dev, vfat, ext3, ext2, msdos, ext4, hfsplus, cramfs, xfs, ntfs, minix, btrfs\n    Running backend LVMVolumeDriver v3.0.0\n    Debugging is OFF\n    Now serving on [::]:50051...\n```\n\n\nThere is also an example of testing a Ceph cluster using a user called \"cinder\" and the \"volumes\" pool.  For the Ceph/RBD backend, due to a limitation in Cinder, we need to have both the credentials and the configuration in `/etc/ceph` for it to work:\n\n```\n    $ cd examples/baremetal\n    $ ./run.sh rbd\n    Starting Ember CSI v0.0.2 (cinderlib: v0.2.1, cinder: v11.1.2.dev5, CSI spec: v0.2.0)\n    Supported filesystems are: fat, ext4dev, vfat, ext3, ext2, msdos, ext4, hfsplus, cramfs, xfs, ntfs, minix, btrfs\n    Running backend LVMVolumeDriver v3.0.0\n    Debugging is OFF\n    Now serving on [::]:50051...\n```\n\n\nThere is also an XtremIO example that only requires the iSCSI connection packages.\n\n\n### Containerized\n\nThere is a sample `Dockerfile` included in the project that has been used to create the `akrog/ember-csi` container available in the docker hub.\n\nThere are two bash scripts, one for each example, that will run the CSI driver on a container, be aware that the container needs to run as privileged to mount the volumes.\n\nFor the RBD example we need to copy our \"ceph.conf\" and \"ceph.client.cinder.keyring\" files, assuming we are using the \"cinder\" user into the example/docker directory replacing the existing ones:\n\n```\n    $ cd examples/docker\n    $ ./rbd.sh\n    Starting Ember CSI v0.0.2 (cinderlib: v0.2.1, cinder: v11.1.0, CSI spec: v0.2.0)\n    Supported filesystems are: cramfs, minix, ext3, ext2, ext4, xfs, btrfs\n    Running backend LVMVolumeDriver v3.0.0\n    Debugging is ON with rpdb\n    Now serving on [::]:50051...\n```\n\n### CSC\n\nNow that we have the service running we can use the [CSC tool](https://github.com/rexray/gocsi/tree/master/csc) to run commands simulating the Container Orchestration system.\n\nDue to the recent changes in the CSI spec not all commands are available yet, so you won't be able to test the snapshot commands.\n\nChecking the plugin info:\n\n```\n    $ csc identity plugin-info -e tcp://127.0.0.1:50051\n    \"io.ember-csi\"      \"0.0.2\" \"cinder-driver\"=\"RBDDriver\"     \"cinder-driver-supported\"=\"True\"        \"cinder-driver-version\"=\"1.2.0\" \"cinder-version\"=\"11.1.0\"       \"cinderlib-version\"=\"0.2.1\"     \"persistence\"=\"DBPersistence\"\n```\n\nChecking the node id:\n\n```\n    $ csc node get-id -e tcp://127.0.0.1:50051\n    localhost.localdomain\n\n    $ hostname -f\n    localhost.localdomain\n```\n\nChecking the current backend capacity:\n\n```\n    $ csc controller get-capacity -e tcp://127.0.0.1:50051\n    24202140712\n```\n\nCreating a volume:\n\n```\n    $ csc controller create-volume --cap SINGLE_NODE_WRITER,block --req-bytes 2147483648 disk -e tcp://127.0.0.1:50051\n    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  2147483648\n```\n\nListing volumes:\n\n```\n    $ csc controller list-volumes -e tcp://127.0.0.1:50051\n    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  2147483648\n```\n\nStore the volume id for all the following calls:\n\n```\n    $ vol_id=`csc controller list-volumes -e tcp://127.0.0.1:50051|awk '{ print gensub(\"\\\"\",\"\",\"g\",$1)}'`\n```\n\nAttaching the volume to `tmp/mnt/publish` on baremetal as a block device:\n\n```\n    $ touch tmp/mnt/{staging,publish}\n\n    $ csc controller publish --cap SINGLE_NODE_WRITER,block --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  \"connection_info\"=\"{\\\"connector\\\": {\\\"initiator\\\": \\\"iqn.1994-05.com.redhat:aa532823bac9\\\", \\\"ip\\\": \\\"127.0.0.1\\\", \\\"platform\\\": \\\"x86_64\\\", \\\"host\\\": \\\"localhost.localdomain\\\", \\\"do_local_attach\\\": false, \\\"os_type\\\": \\\"linux2\\\", \\\"multipath\\\": false}, \\\"conn\\\": {\\\"driver_volume_type\\\": \\\"rbd\\\", \\\"data\\\": {\\\"secret_uuid\\\": null, \\\"volume_id\\\": \\\"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"auth_username\\\": \\\"cinder\\\", \\\"secret_type\\\": \\\"ceph\\\", \\\"name\\\": \\\"volumes/volume-5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"discard\\\": true, \\\"keyring\\\": \\\"[client.cinder]\\\\n\\\\tkey = AQCQPetaof03IxAAoHZJD6kGxiMQfLdn3QzdlQ==\\\\n\\\", \\\"cluster_name\\\": \\\"ceph\\\", \\\"hosts\\\": [\\\"192.168.1.22\\\"], \\\"auth_enabled\\\": true, \\\"ports\\\": [\\\"6789\\\"]}}}\"\n\n    $ csc node stage --pub-info connection_info=\"irrelevant\" --cap SINGLE_NODE_WRITER,block --staging-target-path `realpath tmp/mnt/staging` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node publish --cap SINGLE_NODE_WRITER,block --pub-info connection_info=\"irrelevant\" --staging-target-path `realpath tmp/mnt/staging` --target-path `realpath tmp/mnt/publish` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n```\n\nAttaching the volume to `tmp/mnt/publish` on container as a block device:\n\n```\n    $ touch tmp/mnt/{staging,publish}\n\n    $ csc controller publish --cap SINGLE_NODE_WRITER,block --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  \"connection_info\"=\"{\\\"connector\\\": {\\\"initiator\\\": \\\"iqn.1994-05.com.redhat:aa532823bac9\\\", \\\"ip\\\": \\\"127.0.0.1\\\", \\\"platform\\\": \\\"x86_64\\\", \\\"host\\\": \\\"localhost.localdomain\\\", \\\"do_local_attach\\\": false, \\\"os_type\\\": \\\"linux2\\\", \\\"multipath\\\": false}, \\\"conn\\\": {\\\"driver_volume_type\\\": \\\"rbd\\\", \\\"data\\\": {\\\"secret_uuid\\\": null, \\\"volume_id\\\": \\\"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"auth_username\\\": \\\"cinder\\\", \\\"secret_type\\\": \\\"ceph\\\", \\\"name\\\": \\\"volumes/volume-5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"discard\\\": true, \\\"keyring\\\": \\\"[client.cinder]\\\\n\\\\tkey = AQCQPetaof03IxAAoHZJD6kGxiMQfLdn3QzdlQ==\\\\n\\\", \\\"cluster_name\\\": \\\"ceph\\\", \\\"hosts\\\": [\\\"192.168.1.22\\\"], \\\"auth_enabled\\\": true, \\\"ports\\\": [\\\"6789\\\"]}}}\"\n\n    $ csc node stage --pub-info connection_info=\"irrelevant\" --cap SINGLE_NODE_WRITER,block --staging-target-path /mnt/staging $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node publish --cap SINGLE_NODE_WRITER,block --pub-info connection_info=\"irrelevant\" --staging-target-path /mnt/staging --target-path /mnt/publish $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n```\n\n\nDetaching the volume on baremetal:\n\n```\n    $ csc node unpublish --target-path `realpath tmp/mnt/publish` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node unstage --staging-target-path `realpath tmp/mnt/staging` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc controller unpublish --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n```\n\nDetaching the volume on container:\n\n```\n    $ csc node unpublish --target-path /mnt/publish $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node unstage --staging-target-path /tmp/mnt/staging $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc controller unpublish --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n```\n\nDeleting the volume:\n\n```\n    $ csc controller delete-volume $vol_id -e tcp://127.0.0.1:50051\n```\n\nIf we want to use the mount interface instead of the block one, we can also do it making sure we create directories instead of files and replacing the `block` word with `mount,ext4` if we want an `ext4` filesystem.\n\nFor example these would be the commands for the baremetal attach:\n\n```\n    $ mkdir tmp/mnt/{staging_dir,publish_dir}\n\n    $ csc controller publish --cap SINGLE_NODE_WRITER,mount,ext4 --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n\n    $ csc node stage --pub-info connection_info=\"irrelevant\" --cap SINGLE_NODE_WRITER,mount,ext4 --staging-target-path `realpath tmp/mnt/staging_dir` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node publish --pub-info connection_info=\"irrelevant\" --cap SINGLE_NODE_WRITER,mount,ext4 -staging-target-path `realpath tmp/mnt/staging_dir` --target-path `realpath tmp/mnt/publish_dir` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n```\n\n\n## Capable operational modes\n\nThe CSI spec defines a set of `AccessModes` that CSI drivers can support, such as single writer, single reader, multiple writers, single writer and multiple readers.\n\nThis CSI driver currently only supports `SINGLE_MODE_WRITER`, although it will also succeed with the `SINGLE_MODE_READER_ONLY` mode and mount it as read/write.\n\n\n## Debugging\n\nThe first tool for debugging is the log that displays detailed information on the driver code used by *ember-CSI*.  We can enable INFO or DEBUG logs using the `X_CSI_EMBER_CONFIG` environmental variable.\n\nTo enable logs, defaulting to INFO level, we must set the `disable_logs` key to `false`.  If we want them at DEBUG levels, we also need to set `debug` to `true`.\n\nFor baremetal, enablig DEBUG log levels can be done like this:\n\n```\n    export X_CSI_EMBER_CONFIG={\"project_id\":\"io.ember-csi\",\"user_id\":\"io.ember-csi\",\"root_helper\":\"sudo\",\"plugin_name\": \"io.ember-csi\",\"disable_logs\":false,\"debug\":true}\n\n```\n\nFor containers we can just add the environmental variable to a file and import into our run using `--env-file` or adding it to our command line with `-e`.\n\nIn both cases it should not have the `export` command:\n\n```\n    X_CSI_EMBER_CONFIG={\"project_id\":\"io.ember-csi\",\"user_id\":\"io.ember-csi\",\"root_helper\":\"sudo\",\"plugin_name\": \"io.ember-csi\",\"disable_logs\":false,\"debug\":true}\n\n```\n\nBesides this basic debugging level, the Ember CSI plugin also supports live debugging when run in the baremetal and when running as a container.\n\nThere are two mechanisms that can be used to debug the driver: with `pdb`, and with `rpdb`.\n\nThe difference between them is that `pdb` works with stdin and stdout, whereas `rpdb` opens port 4444 to accept remote connections for debugging.\n\nDebugging the Ember CSI plugin requires enabling debugging on the plugin before starting it, and then one it is running we have to turn it on.\n\nEnabling debugging is done using the `X_CSI_DEBUG_MODE` environmental variable.  Setting it to `pdb` or `rpdb` will enable debugging.  The plugin has this feature disabled by default, but our *latest* and *master* containers have it enabled by default with `rpdb`.\n\nOnce we have the plugin running with the debugging enable (we can see it in the start message) we can turn it on and off using the `SIGUSR1` signal, and the service will output the change with a *Debugging is ON* or *Debugging is OFF* message.\n\nAfter turning it *ON* the plugin will stop for debugging on the next GRPC request.  Going into interactive mode if using `pdb` or opening port 4444 if using `rpdb`.  When using `rpdb` we'll see the following message on the plugin: *pdb is running on 127.0.0.1:4444*\n\nSending the signal to toggle ON/OFF the debugging is quite easy.  For baremetal we can do:\n\n```\n    $ pkill -USR1 ember-csi\n```\n\nAnd for the container (assuming its named `ember-csi` like in the examples) we can do:\n\n```\n    $ docker kill -sUSR1 ember-csi\n```\n\nIf we are using `rpdb` then we'll have to connect to the port:\n\n```\n    $ nc 127.0.0.1 4444\n```\n\n## Troubleshooting\n\n### CSC commands timeout\n\nIf you have a slow backend or a slow data network connection, and you are creating mount volumes, then you may run into \"context deadline exceeded\" errors when running the node staging command on the volume.\n\nThis is just a 60 seconds timeout, and we can easily fix this by increasing allowed timeout for the command to complete.  For example to 5 minutes with `-t5m` or to 1 hour if we are manually debugging things on the server side with `-t1h`.\n\n### Staging fails in container using iSCSI\n\n\nWhen I try to stage a volume using a containerized *Node* I see the error \"ERROR root VolumeDeviceNotFound: Volume device not found at .\".\n\nTurning the DEBUG log levels shows me login errors:\n\n```\n    2018-07-03 11:14:57.258 1 WARNING os_brick.initiator.connectors.iscsi [req-0e77bf32-a29b-40d1-b359-9e115435a94a io.ember-csi io.ember-csi - - -] Failed to connect to iSCSI portal 192.168.1.1:3260.\n    2018-07-03 11:14:57.259 1 WARNING os_brick.initiator.connectors.iscsi [req-0e77bf32-a29b-40d1-b359-9e115435a94a io.ember-csi io.ember-csi - - -] Failed to login iSCSI target iqn.2008-05.com.something:smt00153500071-514f0c50023f6c01 on portal 192.168.1.1:3260 (exit code 12).: ProcessExecutionError: Unexpected error while running command.\n```\n\nAnd looking into the host's journal (where the `iscsid` daemon is running) I can see `Kmod` errors:\n\n```\n    Jul 03 13:15:02 think iscsid[9509]: Could not insert module . Kmod error -2\n```\n\nThis seems to be cause by some kind of incompatibility between the host and the container's iSCSI modules.  We currently don't have a solution other than using a CentOS 7 host system.\n\n## Support\n\nFor any questions or concerns please file an issue with the [ember-csi](https://github.com/akrog/ember-csi/issues) project or ping me on IRC (my handle is geguileo and I hang on the #openstack-cinder channel in Freenode).\n\n\n## TODO\n\nThere are many things that need to be done in this POC driver, and here's a non exhaustive list:\n\n- Support for NFS volumes\n- Support for Kubernetes CRDs as the persistence storage\n- Unit tests\n- Functional tests\n- Improve received parameters checking\n- Make driver more resilient\n- Test driver in Kubernetes\n- Review some of the returned error codes\n- Support volume attributes via volume types\n- Look into multi-attaching\n- Support read-only mode\n- Report capacity based on over provisioning values\n- Configure the private data location\n\n---\n# History\n\n\n## 0.9.0 (2019-06-04)\n\nBeta release with full CSI v0.2, v0.3, and v1.0 spec support.\n\n### Features\n\n- Multi-driver support on single container\n- Support for mount filesystems\n- Support for block\n- Topology support\n- Snapshot support\n- Liveness probe\n- CRD metadata persistence plugin\n- Multi-version support on single container\n- Aliases for configuration\n- Storage driver list tool\n- Support live debugging of running driver\n- Duplicated requests queuing support (for k8s)\n- Support of mocked probe\n- Configurable default mount filesystem\n\n### Bugs\n\n- Fix issues receiving duplicated RPC calls\n- Fix UUID warning\n- Check staging and publishing targets\n- Exit on binding error\n\n\n## 0.0.2 (2018-06-19)\n\n* Use cinderlib v0.2.1 instead of github branch\n\n\n## 0.0.1 (2018-05-18)\n\n* First release on PyPI.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/akrog/ember-csi", "keywords": "ember_csi", "license": "Apache Software License 2.0", "maintainer": "", "maintainer_email": "", "name": "ember-csi", "package_url": "https://pypi.org/project/ember-csi/", "platform": "", "project_url": "https://pypi.org/project/ember-csi/", "project_urls": {"Homepage": "https://github.com/akrog/ember-csi"}, "release_url": "https://pypi.org/project/ember-csi/0.9.0/", "requires_dist": ["cinderlib (>=0.9.0)", "grpcio (==1.15.0)", "protobuf (>=3.5.0.post1)", "kubernetes (>=7.0.0)"], "requires_python": "", "summary": "Multi-vendor CSI plugin supporting over 80 storage drivers", "version": "0.9.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Ember CSI</h1>\n<p><a href=\"https://hub.docker.com/r/akrog/ember-csi/\" rel=\"nofollow\"><img alt=\"Docker build status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/71587fb87e202d489ed4e9e08ef615f8f52e84c1/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6275696c642f616b726f672f656d6265722d6373692e737667\"></a> <a href=\"https://hub.docker.com/r/akrog/ember-csi/builds/\" rel=\"nofollow\"><img alt=\"Docker build\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f4bcf2309ecc9ae6116de1abf39cfe06751964a8/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f616b726f672f656d6265722d6373692e737667\"></a> <a href=\"https://pypi.python.org/pypi/ember_csi\" rel=\"nofollow\"><img alt=\"PyPi\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3c6c0270087e544642e2759407d9b6db01a0971d/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f656d6265725f6373692e737667\"></a> <a href=\"https://pypi.python.org/pypi/ember_csi\" rel=\"nofollow\"><img alt=\"PyVersion\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f923b4a7782e7e16cb377eb840712804354e2e32/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f656d6265725f6373692e737667\"></a> <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" rel=\"nofollow\"><img alt=\"License\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/382baecb6923f5cbcad5e0a48881f28a69c0bf4c/68747470733a2f2f696d672e736869656c64732e696f2f3a6c6963656e73652d6170616368652d626c75652e737667\"></a></p>\n<p>Multi-vendor CSI plugin driver supporting over 80 storage drivers in a single plugin to provide <code>block</code> and <code>mount</code> storage to Container Orchestration systems.</p>\n<ul>\n<li>Free software: Apache Software License 2.0</li>\n<li>Documentation: Pending</li>\n</ul>\n<h2>Features</h2>\n<p>This CSI driver is up to date with latest CSI specs including the <a href=\"https://github.com/container-storage-interface/spec/pull/224\" rel=\"nofollow\">new snapshots feature</a> recently introduced.</p>\n<p>Currently supported features are:</p>\n<ul>\n<li>Create block volume</li>\n<li>Creating snapshots</li>\n<li>Creating a block volume from a snapshot</li>\n<li>Delete block volume</li>\n<li>Deleting snapshots</li>\n<li>Listing volumes with pagination</li>\n<li>Listing snapshots with pagination</li>\n<li>Attaching volumes</li>\n<li>Detaching volumes</li>\n<li>Reporting storage capacity</li>\n<li>Probing the node</li>\n<li>Retrieving the plugin info</li>\n</ul>\n<h2>Runtime Dependencies</h2>\n<p>This driver requires that Cinder v11.0 (OSP-12/Pike) is already installed in the system, how this is accomplished is left to the installer, as there are multiple ways this can be accomplished:</p>\n<ul>\n<li>From OSP repositories</li>\n<li>From RDO repositories</li>\n<li>From github</li>\n<li>From other repositories</li>\n</ul>\n<p>Any other basic requirement is already handled by <code>ember-csi</code> when installing from PyPi.</p>\n<p>Besides the basic dependencies there are also some drivers that have additional requirements that must be met for proper operation of the driver and/or attachment/detachment operations, just like in Cinder.</p>\n<p>Some of these Python dependencies for the Controller servicer are:</p>\n<ul>\n<li>DRBD: dbus and drbdmanage</li>\n<li>HPE 3PAR: python-3parclient</li>\n<li>Kaminario: krest</li>\n<li>Pure: purestorage</li>\n<li>Dell EMC VMAX, IBM DS8K: pyOpenSSL</li>\n<li>HPE Lefthad: python-lefthandclient</li>\n<li>Fujitsu Eternus DX: pywbem</li>\n<li>IBM XIV: pyxcli</li>\n<li>RBD: rados and rbd</li>\n<li>Dell EMC VNX: storops</li>\n<li>Violin: vmemclient</li>\n<li>INFINIDAT: infinisdk, capacity, infy.dtypes.wwn, infi.dtypes.iqn</li>\n</ul>\n<p>Other backends may also require additional packages, for example LVM on CentOS/RHEL requires the <code>targetcli</code> package, so please check with your hardware vendor.</p>\n<p>Besides the Controller requirements there are usually requirements for the Node servicer needed to handle the attaching and detaching of volumes to the node based on the connection used to access the storage.  For example:</p>\n<ul>\n<li>iSCSI: iscsi-initiator-tools and device-mapper-multipath</li>\n<li>RBD/Ceph: ceph-common package</li>\n</ul>\n<h2>Installation</h2>\n<p>First we need to install the Cinder Python package, for example to install from RDO on CentOS:</p>\n<pre><code>    $ sudo yum install -y centos-release-openstack-pike\n    $ sudo yum install -y openstack-cinder python-pip\n</code></pre>\n<p>Then we just need to install the <code>ember-csi</code> package:</p>\n<pre><code>    $ sudo pip install ember-csi\n</code></pre>\n<p>Now we should install any additional package required by our backend.</p>\n<p>For iSCSI backends we'll want to install:</p>\n<pre><code>    $ sudo yum install iscsi-initiator-utils\n    $ sudo yum install device-mapper-multipath\n    $ sudo mpathconf --enable --with_multipathd y --user_friendly_names n --find_multipaths y\n</code></pre>\n<p>For RBD we'll also need a specific package:</p>\n<pre><code>    $ sudo yum install ceph-common\n</code></pre>\n<h2>Configuration</h2>\n<p>The CSI driver is configured via environmental variables, any value that doesn't have a default is a required value.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Role</th>\n<th>Description</th>\n<th>Default</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>CSI_ENDPOINT</code></td>\n<td>all</td>\n<td>IP and port to bind the service</td>\n<td>[::]:50051</td>\n<td>192.168.1.22:50050</td>\n</tr>\n<tr>\n<td><code>CSI_MODE</code></td>\n<td>all</td>\n<td>Role the service should perform: controller, node, all</td>\n<td>all</td>\n<td>controller</td>\n</tr>\n<tr>\n<td><code>X_CSI_SPEC_VERSION</code></td>\n<td>all</td>\n<td>CSI Spec version to run. Supported v0.2 and v1.0</td>\n<td>v0.2.0</td>\n<td>0.2.0</td>\n</tr>\n<tr>\n<td><code>X_CSI_STORAGE_NW_IP</code></td>\n<td>node</td>\n<td>IP address in the Node used to connect to the storage</td>\n<td>IP resolved from Node's fqdn</td>\n<td>192.168.1.22</td>\n</tr>\n<tr>\n<td><code>X_CSI_NODE_ID</code></td>\n<td>node</td>\n<td>ID used by this node to identify itself to the controller</td>\n<td>Node's fqdn</td>\n<td>csi_test_node</td>\n</tr>\n<tr>\n<td><code>X_CSI_PERSISTENCE_CONFIG</code></td>\n<td>all</td>\n<td>Configuration of the <code>cinderlib</code> metadata persistence plugin.</td>\n<td>{\"storage\": \"crd\", \"namespace\": \"default\"}</td>\n<td>{\"storage\": \"db\", \"connection\": \"mysql+pymysql://root:stackdb@192.168.1.1/cinder?charset=utf8\"}</td>\n</tr>\n<tr>\n<td><code>X_CSI_EMBER_CONFIG</code></td>\n<td>all</td>\n<td>Global <code>Ember</code> and <code>cinderlib</code> configuration</td>\n<td>{\"project_id\": \"ember-csi.io\", \"user_id\": \"ember-csi.io\", \"root_helper\": \"sudo\", \"request_multipath\": false, \"plugin_name\": \"\", \"file_locks_path\": \"/var/lib/ember-csi/locks\", \"name\": \"io.ember-csi\", \"grpc_workers\": 30, \"enable_probe\": false}</td>\n<td>{\"project_id\":\"k8s project\",\"user_id\":\"csi driver\",\"root_helper\":\"sudo\",\"plugin_name\": \"external-ceph\"}</td>\n</tr>\n<tr>\n<td><code>X_CSI_BACKEND_CONFIG</code></td>\n<td>controller</td>\n<td>Driver configuration</td>\n<td></td>\n<td>{\"name\": \"rbd\", \"driver\": \"RBD\", \"rbd_user\": \"cinder\", \"rbd_pool\": \"volumes\", \"rbd_ceph_conf\": \"/etc/ceph/ceph.conf\", \"rbd_keyring_conf\": \"/etc/ceph/ceph.client.cinder.keyring\"}</td>\n</tr>\n<tr>\n<td><code>X_CSI_DEFAULT_MOUNT_FS</code></td>\n<td>node</td>\n<td>Default mount filesystem when missing in publish calls</td>\n<td>ext4</td>\n<td>btrfs</td>\n</tr>\n<tr>\n<td><code>X_CSI_SYSTEM_FILES</code></td>\n<td>all</td>\n<td>All required storage driver-specific files archived in tar, tar.gz or tar.bz2 format</td>\n<td></td>\n<td>/path/to/etc-ceph.tar.gz</td>\n</tr>\n<tr>\n<td><code>X_CSI_DEBUG_MODE</code></td>\n<td>all</td>\n<td>Debug mode (rpdb, pdb) to use. Disabled by default.</td>\n<td></td>\n<td>rpdb</td>\n</tr>\n<tr>\n<td><code>X_CSI_ABORT_DUPLICATES</code></td>\n<td>all</td>\n<td>If we want to abort or queue (default) duplicated requests.</td>\n<td>false</td>\n<td>true</td>\n</tr></tbody></table>\n<p>The only role that has been tested at the moment is the default one, where Controller and Node servicer are executed in the same service (<code>CSI_MODE=all</code>), and other modes are expected to have issues at the moment.</p>\n<p>The X_CSI_SYSTEM_FILES variable should point to a tar/tar.gz/tar.bz2 file accessible in the Ember CSI driver's filesystem. The contents of the archive will be extracted into '/'. A trusted user such as an operator/administrator with privileged access must create the archive before starting the driver.</p>\n<p>e.g.</p>\n<pre><code>$ tar cvf ceph-files.tar /etc/ceph/ceph.conf /etc/ceph/ceph.client.cinder.keyring\ntar: Removing leading `/' from member names\n/etc/ceph/ceph.conf\n/etc/ceph/ceph.client.cinder.keyring\n$ export X_CSI_SYSTEM_FILES=`pwd`/ceph-files.tar\n</code></pre>\n<h2>Starting the plugin</h2>\n<p>Once we have installed <code>ember-csi</code> and required dependencies (for the backend and for the connection type) we just have to run the <code>ember-csi</code> service with a user that can do passwordless sudo:</p>\n<pre><code>    $ ember-csi\n</code></pre>\n<h2>Testing the plugin</h2>\n<p>There are several examples of running the Ember CSI plugin in the <code>examples</code> directory both for a baremetal deployment and a containerized version of the driver.</p>\n<p>In all cases we have to run the plugin first before we can test it, and for that we have to check the configuration provided as a test before starting the plugin.  By default all examples run the service on port 50051.</p>\n<h3>Baremetal</h3>\n<p>For example to test with the LVM driver on our development environment we can just run the following commands from the root of the <code>ember-csi</code> project:</p>\n<p><em>Note</em>: The iscsi IP addresses are auto-assigned in the <a href=\"examples/baremetal/lvm\" rel=\"nofollow\">lvm</a> env file. You may change these IP addresses if desired:</p>\n<pre><code>    $ cd tmp\n    $ sudo dd if=/dev/zero of=ember-volumes bs=1048576 seek=22527 count=1\n    $ lodevice=`sudo losetup --show -f ./ember-volumes`\n    $ sudo pvcreate $lodevice\n    $ sudo vgcreate ember-volumes $lodevice\n    $ sudo vgscan --cache\n    $ cd ../examples/baremetal\n    $ ./run.sh lvm\n    py27 develop-inst-nodeps: /home/geguileo/code/ember-csi\n    py27 installed: ...\n    ___ summary ___\n      py27: skipped tests\n      congratulations :)\n    Starting Ember CSI v0.0.2 (cinderlib: v0.2.1, cinder: v11.1.2.dev5, CSI spec: v0.2.0)\n    Supported filesystems are: fat, ext4dev, vfat, ext3, ext2, msdos, ext4, hfsplus, cramfs, xfs, ntfs, minix, btrfs\n    Running backend LVMVolumeDriver v3.0.0\n    Debugging is OFF\n    Now serving on [::]:50051...\n</code></pre>\n<p>There is also an example of testing a Ceph cluster using a user called \"cinder\" and the \"volumes\" pool.  For the Ceph/RBD backend, due to a limitation in Cinder, we need to have both the credentials and the configuration in <code>/etc/ceph</code> for it to work:</p>\n<pre><code>    $ cd examples/baremetal\n    $ ./run.sh rbd\n    Starting Ember CSI v0.0.2 (cinderlib: v0.2.1, cinder: v11.1.2.dev5, CSI spec: v0.2.0)\n    Supported filesystems are: fat, ext4dev, vfat, ext3, ext2, msdos, ext4, hfsplus, cramfs, xfs, ntfs, minix, btrfs\n    Running backend LVMVolumeDriver v3.0.0\n    Debugging is OFF\n    Now serving on [::]:50051...\n</code></pre>\n<p>There is also an XtremIO example that only requires the iSCSI connection packages.</p>\n<h3>Containerized</h3>\n<p>There is a sample <code>Dockerfile</code> included in the project that has been used to create the <code>akrog/ember-csi</code> container available in the docker hub.</p>\n<p>There are two bash scripts, one for each example, that will run the CSI driver on a container, be aware that the container needs to run as privileged to mount the volumes.</p>\n<p>For the RBD example we need to copy our \"ceph.conf\" and \"ceph.client.cinder.keyring\" files, assuming we are using the \"cinder\" user into the example/docker directory replacing the existing ones:</p>\n<pre><code>    $ cd examples/docker\n    $ ./rbd.sh\n    Starting Ember CSI v0.0.2 (cinderlib: v0.2.1, cinder: v11.1.0, CSI spec: v0.2.0)\n    Supported filesystems are: cramfs, minix, ext3, ext2, ext4, xfs, btrfs\n    Running backend LVMVolumeDriver v3.0.0\n    Debugging is ON with rpdb\n    Now serving on [::]:50051...\n</code></pre>\n<h3>CSC</h3>\n<p>Now that we have the service running we can use the <a href=\"https://github.com/rexray/gocsi/tree/master/csc\" rel=\"nofollow\">CSC tool</a> to run commands simulating the Container Orchestration system.</p>\n<p>Due to the recent changes in the CSI spec not all commands are available yet, so you won't be able to test the snapshot commands.</p>\n<p>Checking the plugin info:</p>\n<pre><code>    $ csc identity plugin-info -e tcp://127.0.0.1:50051\n    \"io.ember-csi\"      \"0.0.2\" \"cinder-driver\"=\"RBDDriver\"     \"cinder-driver-supported\"=\"True\"        \"cinder-driver-version\"=\"1.2.0\" \"cinder-version\"=\"11.1.0\"       \"cinderlib-version\"=\"0.2.1\"     \"persistence\"=\"DBPersistence\"\n</code></pre>\n<p>Checking the node id:</p>\n<pre><code>    $ csc node get-id -e tcp://127.0.0.1:50051\n    localhost.localdomain\n\n    $ hostname -f\n    localhost.localdomain\n</code></pre>\n<p>Checking the current backend capacity:</p>\n<pre><code>    $ csc controller get-capacity -e tcp://127.0.0.1:50051\n    24202140712\n</code></pre>\n<p>Creating a volume:</p>\n<pre><code>    $ csc controller create-volume --cap SINGLE_NODE_WRITER,block --req-bytes 2147483648 disk -e tcp://127.0.0.1:50051\n    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  2147483648\n</code></pre>\n<p>Listing volumes:</p>\n<pre><code>    $ csc controller list-volumes -e tcp://127.0.0.1:50051\n    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  2147483648\n</code></pre>\n<p>Store the volume id for all the following calls:</p>\n<pre><code>    $ vol_id=`csc controller list-volumes -e tcp://127.0.0.1:50051|awk '{ print gensub(\"\\\"\",\"\",\"g\",$1)}'`\n</code></pre>\n<p>Attaching the volume to <code>tmp/mnt/publish</code> on baremetal as a block device:</p>\n<pre><code>    $ touch tmp/mnt/{staging,publish}\n\n    $ csc controller publish --cap SINGLE_NODE_WRITER,block --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  \"connection_info\"=\"{\\\"connector\\\": {\\\"initiator\\\": \\\"iqn.1994-05.com.redhat:aa532823bac9\\\", \\\"ip\\\": \\\"127.0.0.1\\\", \\\"platform\\\": \\\"x86_64\\\", \\\"host\\\": \\\"localhost.localdomain\\\", \\\"do_local_attach\\\": false, \\\"os_type\\\": \\\"linux2\\\", \\\"multipath\\\": false}, \\\"conn\\\": {\\\"driver_volume_type\\\": \\\"rbd\\\", \\\"data\\\": {\\\"secret_uuid\\\": null, \\\"volume_id\\\": \\\"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"auth_username\\\": \\\"cinder\\\", \\\"secret_type\\\": \\\"ceph\\\", \\\"name\\\": \\\"volumes/volume-5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"discard\\\": true, \\\"keyring\\\": \\\"[client.cinder]\\\\n\\\\tkey = AQCQPetaof03IxAAoHZJD6kGxiMQfLdn3QzdlQ==\\\\n\\\", \\\"cluster_name\\\": \\\"ceph\\\", \\\"hosts\\\": [\\\"192.168.1.22\\\"], \\\"auth_enabled\\\": true, \\\"ports\\\": [\\\"6789\\\"]}}}\"\n\n    $ csc node stage --pub-info connection_info=\"irrelevant\" --cap SINGLE_NODE_WRITER,block --staging-target-path `realpath tmp/mnt/staging` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node publish --cap SINGLE_NODE_WRITER,block --pub-info connection_info=\"irrelevant\" --staging-target-path `realpath tmp/mnt/staging` --target-path `realpath tmp/mnt/publish` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n</code></pre>\n<p>Attaching the volume to <code>tmp/mnt/publish</code> on container as a block device:</p>\n<pre><code>    $ touch tmp/mnt/{staging,publish}\n\n    $ csc controller publish --cap SINGLE_NODE_WRITER,block --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n    \"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\"  \"connection_info\"=\"{\\\"connector\\\": {\\\"initiator\\\": \\\"iqn.1994-05.com.redhat:aa532823bac9\\\", \\\"ip\\\": \\\"127.0.0.1\\\", \\\"platform\\\": \\\"x86_64\\\", \\\"host\\\": \\\"localhost.localdomain\\\", \\\"do_local_attach\\\": false, \\\"os_type\\\": \\\"linux2\\\", \\\"multipath\\\": false}, \\\"conn\\\": {\\\"driver_volume_type\\\": \\\"rbd\\\", \\\"data\\\": {\\\"secret_uuid\\\": null, \\\"volume_id\\\": \\\"5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"auth_username\\\": \\\"cinder\\\", \\\"secret_type\\\": \\\"ceph\\\", \\\"name\\\": \\\"volumes/volume-5ee5fd7c-45cd-44cf-af7b-06081f680f2c\\\", \\\"discard\\\": true, \\\"keyring\\\": \\\"[client.cinder]\\\\n\\\\tkey = AQCQPetaof03IxAAoHZJD6kGxiMQfLdn3QzdlQ==\\\\n\\\", \\\"cluster_name\\\": \\\"ceph\\\", \\\"hosts\\\": [\\\"192.168.1.22\\\"], \\\"auth_enabled\\\": true, \\\"ports\\\": [\\\"6789\\\"]}}}\"\n\n    $ csc node stage --pub-info connection_info=\"irrelevant\" --cap SINGLE_NODE_WRITER,block --staging-target-path /mnt/staging $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node publish --cap SINGLE_NODE_WRITER,block --pub-info connection_info=\"irrelevant\" --staging-target-path /mnt/staging --target-path /mnt/publish $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n</code></pre>\n<p>Detaching the volume on baremetal:</p>\n<pre><code>    $ csc node unpublish --target-path `realpath tmp/mnt/publish` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node unstage --staging-target-path `realpath tmp/mnt/staging` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc controller unpublish --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n</code></pre>\n<p>Detaching the volume on container:</p>\n<pre><code>    $ csc node unpublish --target-path /mnt/publish $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node unstage --staging-target-path /tmp/mnt/staging $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc controller unpublish --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n</code></pre>\n<p>Deleting the volume:</p>\n<pre><code>    $ csc controller delete-volume $vol_id -e tcp://127.0.0.1:50051\n</code></pre>\n<p>If we want to use the mount interface instead of the block one, we can also do it making sure we create directories instead of files and replacing the <code>block</code> word with <code>mount,ext4</code> if we want an <code>ext4</code> filesystem.</p>\n<p>For example these would be the commands for the baremetal attach:</p>\n<pre><code>    $ mkdir tmp/mnt/{staging_dir,publish_dir}\n\n    $ csc controller publish --cap SINGLE_NODE_WRITER,mount,ext4 --node-id `hostname -f` $vol_id -e tcp://127.0.0.1:50051\n\n    $ csc node stage --pub-info connection_info=\"irrelevant\" --cap SINGLE_NODE_WRITER,mount,ext4 --staging-target-path `realpath tmp/mnt/staging_dir` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n\n    $ csc node publish --pub-info connection_info=\"irrelevant\" --cap SINGLE_NODE_WRITER,mount,ext4 -staging-target-path `realpath tmp/mnt/staging_dir` --target-path `realpath tmp/mnt/publish_dir` $vol_id -e tcp://127.0.0.1:50051\n    5ee5fd7c-45cd-44cf-af7b-06081f680f2c\n</code></pre>\n<h2>Capable operational modes</h2>\n<p>The CSI spec defines a set of <code>AccessModes</code> that CSI drivers can support, such as single writer, single reader, multiple writers, single writer and multiple readers.</p>\n<p>This CSI driver currently only supports <code>SINGLE_MODE_WRITER</code>, although it will also succeed with the <code>SINGLE_MODE_READER_ONLY</code> mode and mount it as read/write.</p>\n<h2>Debugging</h2>\n<p>The first tool for debugging is the log that displays detailed information on the driver code used by <em>ember-CSI</em>.  We can enable INFO or DEBUG logs using the <code>X_CSI_EMBER_CONFIG</code> environmental variable.</p>\n<p>To enable logs, defaulting to INFO level, we must set the <code>disable_logs</code> key to <code>false</code>.  If we want them at DEBUG levels, we also need to set <code>debug</code> to <code>true</code>.</p>\n<p>For baremetal, enablig DEBUG log levels can be done like this:</p>\n<pre><code>    export X_CSI_EMBER_CONFIG={\"project_id\":\"io.ember-csi\",\"user_id\":\"io.ember-csi\",\"root_helper\":\"sudo\",\"plugin_name\": \"io.ember-csi\",\"disable_logs\":false,\"debug\":true}\n\n</code></pre>\n<p>For containers we can just add the environmental variable to a file and import into our run using <code>--env-file</code> or adding it to our command line with <code>-e</code>.</p>\n<p>In both cases it should not have the <code>export</code> command:</p>\n<pre><code>    X_CSI_EMBER_CONFIG={\"project_id\":\"io.ember-csi\",\"user_id\":\"io.ember-csi\",\"root_helper\":\"sudo\",\"plugin_name\": \"io.ember-csi\",\"disable_logs\":false,\"debug\":true}\n\n</code></pre>\n<p>Besides this basic debugging level, the Ember CSI plugin also supports live debugging when run in the baremetal and when running as a container.</p>\n<p>There are two mechanisms that can be used to debug the driver: with <code>pdb</code>, and with <code>rpdb</code>.</p>\n<p>The difference between them is that <code>pdb</code> works with stdin and stdout, whereas <code>rpdb</code> opens port 4444 to accept remote connections for debugging.</p>\n<p>Debugging the Ember CSI plugin requires enabling debugging on the plugin before starting it, and then one it is running we have to turn it on.</p>\n<p>Enabling debugging is done using the <code>X_CSI_DEBUG_MODE</code> environmental variable.  Setting it to <code>pdb</code> or <code>rpdb</code> will enable debugging.  The plugin has this feature disabled by default, but our <em>latest</em> and <em>master</em> containers have it enabled by default with <code>rpdb</code>.</p>\n<p>Once we have the plugin running with the debugging enable (we can see it in the start message) we can turn it on and off using the <code>SIGUSR1</code> signal, and the service will output the change with a <em>Debugging is ON</em> or <em>Debugging is OFF</em> message.</p>\n<p>After turning it <em>ON</em> the plugin will stop for debugging on the next GRPC request.  Going into interactive mode if using <code>pdb</code> or opening port 4444 if using <code>rpdb</code>.  When using <code>rpdb</code> we'll see the following message on the plugin: <em>pdb is running on 127.0.0.1:4444</em></p>\n<p>Sending the signal to toggle ON/OFF the debugging is quite easy.  For baremetal we can do:</p>\n<pre><code>    $ pkill -USR1 ember-csi\n</code></pre>\n<p>And for the container (assuming its named <code>ember-csi</code> like in the examples) we can do:</p>\n<pre><code>    $ docker kill -sUSR1 ember-csi\n</code></pre>\n<p>If we are using <code>rpdb</code> then we'll have to connect to the port:</p>\n<pre><code>    $ nc 127.0.0.1 4444\n</code></pre>\n<h2>Troubleshooting</h2>\n<h3>CSC commands timeout</h3>\n<p>If you have a slow backend or a slow data network connection, and you are creating mount volumes, then you may run into \"context deadline exceeded\" errors when running the node staging command on the volume.</p>\n<p>This is just a 60 seconds timeout, and we can easily fix this by increasing allowed timeout for the command to complete.  For example to 5 minutes with <code>-t5m</code> or to 1 hour if we are manually debugging things on the server side with <code>-t1h</code>.</p>\n<h3>Staging fails in container using iSCSI</h3>\n<p>When I try to stage a volume using a containerized <em>Node</em> I see the error \"ERROR root VolumeDeviceNotFound: Volume device not found at .\".</p>\n<p>Turning the DEBUG log levels shows me login errors:</p>\n<pre><code>    2018-07-03 11:14:57.258 1 WARNING os_brick.initiator.connectors.iscsi [req-0e77bf32-a29b-40d1-b359-9e115435a94a io.ember-csi io.ember-csi - - -] Failed to connect to iSCSI portal 192.168.1.1:3260.\n    2018-07-03 11:14:57.259 1 WARNING os_brick.initiator.connectors.iscsi [req-0e77bf32-a29b-40d1-b359-9e115435a94a io.ember-csi io.ember-csi - - -] Failed to login iSCSI target iqn.2008-05.com.something:smt00153500071-514f0c50023f6c01 on portal 192.168.1.1:3260 (exit code 12).: ProcessExecutionError: Unexpected error while running command.\n</code></pre>\n<p>And looking into the host's journal (where the <code>iscsid</code> daemon is running) I can see <code>Kmod</code> errors:</p>\n<pre><code>    Jul 03 13:15:02 think iscsid[9509]: Could not insert module . Kmod error -2\n</code></pre>\n<p>This seems to be cause by some kind of incompatibility between the host and the container's iSCSI modules.  We currently don't have a solution other than using a CentOS 7 host system.</p>\n<h2>Support</h2>\n<p>For any questions or concerns please file an issue with the <a href=\"https://github.com/akrog/ember-csi/issues\" rel=\"nofollow\">ember-csi</a> project or ping me on IRC (my handle is geguileo and I hang on the #openstack-cinder channel in Freenode).</p>\n<h2>TODO</h2>\n<p>There are many things that need to be done in this POC driver, and here's a non exhaustive list:</p>\n<ul>\n<li>Support for NFS volumes</li>\n<li>Support for Kubernetes CRDs as the persistence storage</li>\n<li>Unit tests</li>\n<li>Functional tests</li>\n<li>Improve received parameters checking</li>\n<li>Make driver more resilient</li>\n<li>Test driver in Kubernetes</li>\n<li>Review some of the returned error codes</li>\n<li>Support volume attributes via volume types</li>\n<li>Look into multi-attaching</li>\n<li>Support read-only mode</li>\n<li>Report capacity based on over provisioning values</li>\n<li>Configure the private data location</li>\n</ul>\n<hr>\n<h1>History</h1>\n<h2>0.9.0 (2019-06-04)</h2>\n<p>Beta release with full CSI v0.2, v0.3, and v1.0 spec support.</p>\n<h3>Features</h3>\n<ul>\n<li>Multi-driver support on single container</li>\n<li>Support for mount filesystems</li>\n<li>Support for block</li>\n<li>Topology support</li>\n<li>Snapshot support</li>\n<li>Liveness probe</li>\n<li>CRD metadata persistence plugin</li>\n<li>Multi-version support on single container</li>\n<li>Aliases for configuration</li>\n<li>Storage driver list tool</li>\n<li>Support live debugging of running driver</li>\n<li>Duplicated requests queuing support (for k8s)</li>\n<li>Support of mocked probe</li>\n<li>Configurable default mount filesystem</li>\n</ul>\n<h3>Bugs</h3>\n<ul>\n<li>Fix issues receiving duplicated RPC calls</li>\n<li>Fix UUID warning</li>\n<li>Check staging and publishing targets</li>\n<li>Exit on binding error</li>\n</ul>\n<h2>0.0.2 (2018-06-19)</h2>\n<ul>\n<li>Use cinderlib v0.2.1 instead of github branch</li>\n</ul>\n<h2>0.0.1 (2018-05-18)</h2>\n<ul>\n<li>First release on PyPI.</li>\n</ul>\n\n          </div>"}, "last_serial": 5356863, "releases": {"0.9.0": [{"comment_text": "", "digests": {"md5": "3205d000eecafc37a1f707ab560683a5", "sha256": "1df41c3ab01c85b29ec49152752427e77129d11297451984f00b54904ae09b99"}, "downloads": -1, "filename": "ember_csi-0.9.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "3205d000eecafc37a1f707ab560683a5", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 104360, "upload_time": "2019-06-04T10:58:20", "upload_time_iso_8601": "2019-06-04T10:58:20.568027Z", "url": "https://files.pythonhosted.org/packages/5d/b6/21f889e39ca2559606eed18e65f19473783ef577f7810d7f9b14e0cbfbad/ember_csi-0.9.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bc70aed3eb2af0093e5e5b26d8c89260", "sha256": "7e5f89603ea6f365178b788e9f9e5806760b5a5298f01f306e92c153e3463f9c"}, "downloads": -1, "filename": "ember-csi-0.9.0.tar.gz", "has_sig": false, "md5_digest": "bc70aed3eb2af0093e5e5b26d8c89260", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 111497, "upload_time": "2019-06-04T10:58:22", "upload_time_iso_8601": "2019-06-04T10:58:22.895610Z", "url": "https://files.pythonhosted.org/packages/0f/ad/8c8732d9f61905e0613a464a9bbf1ff70798b248569d98b84d58c54b3895/ember-csi-0.9.0.tar.gz", "yanked": false}], "0.9.0rc1": [{"comment_text": "", "digests": {"md5": "3a4f5de997ec8940f29600bf0057e84a", "sha256": "1ce45e1524cf14908ab8d26567afe7a5e0579b25b9ec12b393970432394e8a49"}, "downloads": -1, "filename": "ember_csi-0.9.0rc1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "3a4f5de997ec8940f29600bf0057e84a", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 104419, "upload_time": "2019-05-20T09:03:09", "upload_time_iso_8601": "2019-05-20T09:03:09.444697Z", "url": "https://files.pythonhosted.org/packages/33/8c/8181d195a9a68ea20ca01b70c9b7f3b2f006228976000e62d2f5247d6166/ember_csi-0.9.0rc1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a455db426f0141a4b6f3e2eeaeb55412", "sha256": "ae860c4d581d634e23800f641db90dcb8b3df4af19fef2becd2f67a73546a219"}, "downloads": -1, "filename": "ember-csi-0.9.0rc1.tar.gz", "has_sig": false, "md5_digest": "a455db426f0141a4b6f3e2eeaeb55412", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 111487, "upload_time": "2019-05-20T09:03:12", "upload_time_iso_8601": "2019-05-20T09:03:12.492381Z", "url": "https://files.pythonhosted.org/packages/42/b3/2635f760a44d39c6cfefb3a41aaa19c2eb6285d11908d490dde727077530/ember-csi-0.9.0rc1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3205d000eecafc37a1f707ab560683a5", "sha256": "1df41c3ab01c85b29ec49152752427e77129d11297451984f00b54904ae09b99"}, "downloads": -1, "filename": "ember_csi-0.9.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "3205d000eecafc37a1f707ab560683a5", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 104360, "upload_time": "2019-06-04T10:58:20", "upload_time_iso_8601": "2019-06-04T10:58:20.568027Z", "url": "https://files.pythonhosted.org/packages/5d/b6/21f889e39ca2559606eed18e65f19473783ef577f7810d7f9b14e0cbfbad/ember_csi-0.9.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bc70aed3eb2af0093e5e5b26d8c89260", "sha256": "7e5f89603ea6f365178b788e9f9e5806760b5a5298f01f306e92c153e3463f9c"}, "downloads": -1, "filename": "ember-csi-0.9.0.tar.gz", "has_sig": false, "md5_digest": "bc70aed3eb2af0093e5e5b26d8c89260", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 111497, "upload_time": "2019-06-04T10:58:22", "upload_time_iso_8601": "2019-06-04T10:58:22.895610Z", "url": "https://files.pythonhosted.org/packages/0f/ad/8c8732d9f61905e0613a464a9bbf1ff70798b248569d98b84d58c54b3895/ember-csi-0.9.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:46:40 2020"}