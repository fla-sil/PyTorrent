{"info": {"author": "Christoph Boeddeker", "author_email": "", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Software Development :: Build Tools"], "description": "# dlp_mpi - Data-level parallelism with mpi for python\n\n<table>\n<tr>\n<th>\nRun an serial algorithm on multiple examples\n</th>\n<th>\nUse dlp_mpi to run the loop body in parallel\n</th>\n<th>\nUse dlp_mpi to run a function in parallel\n</th>\n</tr>\n<tr>\n<td>\n\n```python\n# python script.py\n\nimport time\n\n\nexamples = list(range(10))\nresults = []\n\n\n\n\n\n\n\nfor example in examples:\n\n    # Some heavy workload:\n    # CPU or IO\n    time.sleep(0.2)\n    result = example\n\n    # Remember the results\n    results.append(result)\n\n\n\n\n\n\n\n\n\n\n# Summarize your experiment\nprint(sum(results))\n```\n</td>\n<td>\n\n```python\n# mpiexec -np 8 python script.py\n\nimport time\nimport dlp_mpi\n\nexamples = list(range(10))\nresults = []\n\n\n\n\n\n\n\nfor example in dlp_mpi.split_managed(\n        examples):\n    # Some heavy workload:\n    # CPU or IO\n    time.sleep(0.2)\n    result = example\n\n    # Remember the results\n    results.append(result)\n\nresults = dlp_mpi.gather(results)\n\nif dlp_mpi.IS_MASTER:\n    results = [\n        result\n        for worker_results in results\n        for result in worker_results\n    ]\n\n    # Summarize your experiment\n    print(results)\n```\n</td>\n<td>\n\n```python\n# mpiexec -np 8 python script.py\n\nimport time\nimport dlp_mpi\n\nexamples = list(range(10))\nresults = []\n\ndef work_load(example):\n    # Some heavy workload:\n    # CPU or IO\n    time.sleep(0.2)\n    result = example\n\nfor result in dlp_mpi.map_unordered(\n        work_load, examples):\n\n\n\n\n\n    # Remember the results\n    results.append(result)\n\n\n\n\n\n\n\n\n\nif dlp_mpi.IS_MASTER:\n    # Summarize your experiment\n    print(results)\n```\n</td>\n</tr>\n</table>\n\nThis package uses `mpi4py` to provide utilities to parallize algorithms that are applied to multiple examples.\n\nThe core idea is: Start `N` processes and each process works on a subset of all examples.\nTo start the processes `mpiexec` can be used. Most HPC systems support MPI to scatter the workload across multiple hosts. For the command, look in the documentation for your HPC system and search for MPI launches.\n\nSince each process should operate on different examples, MPI provides the variables `RANK` and `SIZE`, where `SIZE` is the number of workers and `RANK` is a unique identifier from `0` to `SIZE - 1`.\nThe simplest way to improve the execution time is to process `examples[RANK::SIZE]` on each worker.\nThis is a round robin load balancing (`dlp_mpi.split_round_robin`).\nAn more advanced load balaning is `dlp_mpi.split_managed`, where one process manages the load and assigns a new task to a worker, once he finishes the last task.\n\nWhen in the end of a program all results should be summariesd or written in a single file, comunication between all processes is nessesary.\nFor this purpose `dlp_mpi.gather` (`mpi4py.MPI.COMM_WORLD.gather`) can be used. This function sends all data to the root process (For serialisation is `pickle` used).\n\nAs alternative to splitting the data, this package also provides a `map` style parallelization (see example in the beginning):\nThe function `dlp_mpi.map_unordered` calls `work_load` in parallel and executes the `for` body in serial.\nThe communication between the processes is only the `result` and the index to get the `i`th example from the examples. i.e.: The example aren't transferred between the processes.\n\n# Runtime\n\nWithout this package your code runs serial.\nThe execution time of the following code snippets will demonstrated, how it runs with this package.\nRegarding the color: The `examples = ...` is the setup code.\nTherefore it is blue in the code and the block that represents the execution time is also blue.\n\n![(Serial Worker)](doc/tikz_split_managed_serial.svg)\n\nThis simples way to paralize the workload (dark orange) is to do an round robin assignment of the load:\n`for example in dlp_mpi.split_round_robin(examples)`.\nThis function call is equivalent to `for example in examples[dlp_mpi.RANK::dlp_mpi.SIZE]`.\nSo there is zero comunications between the workers.\nOnly when it is nessesary to do some final work on the results of all data (e.g. calculating average metrics) a comunication is nessesary.\nThis is done with the `gather` function.\nThis functions returns the worker results in an list on the master process and the worker process gets a `None` return value.\nDepending on the workload the round robin assingment can be suboptimal.\nSee the example block diagramm.\nWorker 1 got tasks that are relative long.\nSo this worker used much more time than the others.\n\n![(Round Robin)](doc/tikz_split_managed_rr.svg)\n\nTo overcome the limitations of the round robin assingment, this package propose to use a manager to assign the work to the workers.\nThis optimizes the utilisation of the workers.\nOnce a worker finished an example, he request a new one from the manager and gets one assigned.\nNote: The comunication is only which example should be processed (i.e. the index of the example) not the example itself.\n\n![(Managed Split)](doc/tikz_split_managed_split.svg)\n\nAn alternative to splitting the iterator is to use a `map` function.\nThe function is the executed on the workers and the return value is send back to the manager.\nBe carefull, that the loop body is fast enough, otherwise it can be a bottleneck.\nWhen a worker sends a task to the manager, the manager sends back a new task and enter the for loop body. \nWhile the manager is in the loop body, he cannot react on requests of other workers, see the block diagramm:\n\n![(Managed Map)](doc/tikz_split_managed_map.svg)\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/fgnt/dlp_mpi", "keywords": "mpi", "license": "", "maintainer": "", "maintainer_email": "", "name": "dlp-mpi", "package_url": "https://pypi.org/project/dlp-mpi/", "platform": "", "project_url": "https://pypi.org/project/dlp-mpi/", "project_urls": {"Homepage": "https://github.com/fgnt/dlp_mpi"}, "release_url": "https://pypi.org/project/dlp-mpi/0.0.2/", "requires_dist": ["mpi4py"], "requires_python": ">=3.6, <4", "summary": "Data-level parallelism with mpi in python", "version": "0.0.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>dlp_mpi - Data-level parallelism with mpi for python</h1>\n<table>\n<tr>\n<th>\nRun an serial algorithm on multiple examples\n</th>\n<th>\nUse dlp_mpi to run the loop body in parallel\n</th>\n<th>\nUse dlp_mpi to run a function in parallel\n</th>\n</tr>\n<tr>\n<td>\n<pre><span class=\"c1\"># python script.py</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n\n\n<span class=\"n\">examples</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n\n\n\n\n\n\n<span class=\"k\">for</span> <span class=\"n\">example</span> <span class=\"ow\">in</span> <span class=\"n\">examples</span><span class=\"p\">:</span>\n\n    <span class=\"c1\"># Some heavy workload:</span>\n    <span class=\"c1\"># CPU or IO</span>\n    <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"mf\">0.2</span><span class=\"p\">)</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">example</span>\n\n    <span class=\"c1\"># Remember the results</span>\n    <span class=\"n\">results</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n\n\n\n\n\n\n\n\n\n\n<span class=\"c1\"># Summarize your experiment</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">))</span>\n</pre>\n</td>\n<td>\n<pre><span class=\"c1\"># mpiexec -np 8 python script.py</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n<span class=\"kn\">import</span> <span class=\"nn\">dlp_mpi</span>\n\n<span class=\"n\">examples</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n\n\n\n\n\n\n<span class=\"k\">for</span> <span class=\"n\">example</span> <span class=\"ow\">in</span> <span class=\"n\">dlp_mpi</span><span class=\"o\">.</span><span class=\"n\">split_managed</span><span class=\"p\">(</span>\n        <span class=\"n\">examples</span><span class=\"p\">):</span>\n    <span class=\"c1\"># Some heavy workload:</span>\n    <span class=\"c1\"># CPU or IO</span>\n    <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"mf\">0.2</span><span class=\"p\">)</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">example</span>\n\n    <span class=\"c1\"># Remember the results</span>\n    <span class=\"n\">results</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">dlp_mpi</span><span class=\"o\">.</span><span class=\"n\">gather</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">)</span>\n\n<span class=\"k\">if</span> <span class=\"n\">dlp_mpi</span><span class=\"o\">.</span><span class=\"n\">IS_MASTER</span><span class=\"p\">:</span>\n    <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n        <span class=\"n\">result</span>\n        <span class=\"k\">for</span> <span class=\"n\">worker_results</span> <span class=\"ow\">in</span> <span class=\"n\">results</span>\n        <span class=\"k\">for</span> <span class=\"n\">result</span> <span class=\"ow\">in</span> <span class=\"n\">worker_results</span>\n    <span class=\"p\">]</span>\n\n    <span class=\"c1\"># Summarize your experiment</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">)</span>\n</pre>\n</td>\n<td>\n<pre><span class=\"c1\"># mpiexec -np 8 python script.py</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n<span class=\"kn\">import</span> <span class=\"nn\">dlp_mpi</span>\n\n<span class=\"n\">examples</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">work_load</span><span class=\"p\">(</span><span class=\"n\">example</span><span class=\"p\">):</span>\n    <span class=\"c1\"># Some heavy workload:</span>\n    <span class=\"c1\"># CPU or IO</span>\n    <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"mf\">0.2</span><span class=\"p\">)</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">example</span>\n\n<span class=\"k\">for</span> <span class=\"n\">result</span> <span class=\"ow\">in</span> <span class=\"n\">dlp_mpi</span><span class=\"o\">.</span><span class=\"n\">map_unordered</span><span class=\"p\">(</span>\n        <span class=\"n\">work_load</span><span class=\"p\">,</span> <span class=\"n\">examples</span><span class=\"p\">):</span>\n\n\n\n\n\n    <span class=\"c1\"># Remember the results</span>\n    <span class=\"n\">results</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n\n\n\n\n\n\n\n\n\n<span class=\"k\">if</span> <span class=\"n\">dlp_mpi</span><span class=\"o\">.</span><span class=\"n\">IS_MASTER</span><span class=\"p\">:</span>\n    <span class=\"c1\"># Summarize your experiment</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">)</span>\n</pre>\n</td>\n</tr>\n</table>\n<p>This package uses <code>mpi4py</code> to provide utilities to parallize algorithms that are applied to multiple examples.</p>\n<p>The core idea is: Start <code>N</code> processes and each process works on a subset of all examples.\nTo start the processes <code>mpiexec</code> can be used. Most HPC systems support MPI to scatter the workload across multiple hosts. For the command, look in the documentation for your HPC system and search for MPI launches.</p>\n<p>Since each process should operate on different examples, MPI provides the variables <code>RANK</code> and <code>SIZE</code>, where <code>SIZE</code> is the number of workers and <code>RANK</code> is a unique identifier from <code>0</code> to <code>SIZE - 1</code>.\nThe simplest way to improve the execution time is to process <code>examples[RANK::SIZE]</code> on each worker.\nThis is a round robin load balancing (<code>dlp_mpi.split_round_robin</code>).\nAn more advanced load balaning is <code>dlp_mpi.split_managed</code>, where one process manages the load and assigns a new task to a worker, once he finishes the last task.</p>\n<p>When in the end of a program all results should be summariesd or written in a single file, comunication between all processes is nessesary.\nFor this purpose <code>dlp_mpi.gather</code> (<code>mpi4py.MPI.COMM_WORLD.gather</code>) can be used. This function sends all data to the root process (For serialisation is <code>pickle</code> used).</p>\n<p>As alternative to splitting the data, this package also provides a <code>map</code> style parallelization (see example in the beginning):\nThe function <code>dlp_mpi.map_unordered</code> calls <code>work_load</code> in parallel and executes the <code>for</code> body in serial.\nThe communication between the processes is only the <code>result</code> and the index to get the <code>i</code>th example from the examples. i.e.: The example aren't transferred between the processes.</p>\n<h1>Runtime</h1>\n<p>Without this package your code runs serial.\nThe execution time of the following code snippets will demonstrated, how it runs with this package.\nRegarding the color: The <code>examples = ...</code> is the setup code.\nTherefore it is blue in the code and the block that represents the execution time is also blue.</p>\n<p><img alt=\"(Serial Worker)\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/eece17eeceec37cf4ff54b2c4d421b97fea2ba80/646f632f74696b7a5f73706c69745f6d616e616765645f73657269616c2e737667\"></p>\n<p>This simples way to paralize the workload (dark orange) is to do an round robin assignment of the load:\n<code>for example in dlp_mpi.split_round_robin(examples)</code>.\nThis function call is equivalent to <code>for example in examples[dlp_mpi.RANK::dlp_mpi.SIZE]</code>.\nSo there is zero comunications between the workers.\nOnly when it is nessesary to do some final work on the results of all data (e.g. calculating average metrics) a comunication is nessesary.\nThis is done with the <code>gather</code> function.\nThis functions returns the worker results in an list on the master process and the worker process gets a <code>None</code> return value.\nDepending on the workload the round robin assingment can be suboptimal.\nSee the example block diagramm.\nWorker 1 got tasks that are relative long.\nSo this worker used much more time than the others.</p>\n<p><img alt=\"(Round Robin)\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b9ec3cb5ed375e40c37f519ed007a2fcc924621b/646f632f74696b7a5f73706c69745f6d616e616765645f72722e737667\"></p>\n<p>To overcome the limitations of the round robin assingment, this package propose to use a manager to assign the work to the workers.\nThis optimizes the utilisation of the workers.\nOnce a worker finished an example, he request a new one from the manager and gets one assigned.\nNote: The comunication is only which example should be processed (i.e. the index of the example) not the example itself.</p>\n<p><img alt=\"(Managed Split)\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8e5296d3b50520ac7ada60a600451e5bd1c83fc0/646f632f74696b7a5f73706c69745f6d616e616765645f73706c69742e737667\"></p>\n<p>An alternative to splitting the iterator is to use a <code>map</code> function.\nThe function is the executed on the workers and the return value is send back to the manager.\nBe carefull, that the loop body is fast enough, otherwise it can be a bottleneck.\nWhen a worker sends a task to the manager, the manager sends back a new task and enter the for loop body.\nWhile the manager is in the loop body, he cannot react on requests of other workers, see the block diagramm:</p>\n<p><img alt=\"(Managed Map)\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/f1e428540671ee3ed4107f1929a7185d4c4f34d2/646f632f74696b7a5f73706c69745f6d616e616765645f6d61702e737667\"></p>\n\n          </div>"}, "last_serial": 6045881, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "5953199b28a6e3afd813fb20e328bb64", "sha256": "b1304074c08eccd2ce84882809ff81156986fb3eec3e8fe18c53429bbfae2102"}, "downloads": -1, "filename": "dlp_mpi-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "5953199b28a6e3afd813fb20e328bb64", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, <4", "size": 11695, "upload_time": "2019-10-28T10:02:12", "upload_time_iso_8601": "2019-10-28T10:02:12.603669Z", "url": "https://files.pythonhosted.org/packages/d0/e2/741b63557278113e86fef8cee4644f6541ec9dd8e1fb715a121b7aa93850/dlp_mpi-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "37438c1ec34adb23a1a15988f20bff2d", "sha256": "dd174ada88a1703e09dd78fd178a00198bf54fe3d860366e037693a5b92f1ffb"}, "downloads": -1, "filename": "dlp_mpi-0.0.1.tar.gz", "has_sig": false, "md5_digest": "37438c1ec34adb23a1a15988f20bff2d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, <4", "size": 14393, "upload_time": "2019-10-28T10:02:14", "upload_time_iso_8601": "2019-10-28T10:02:14.607715Z", "url": "https://files.pythonhosted.org/packages/5a/68/9b338d930a0cd36a4f8dcc26e0fce9c6b96be87a84841b851cbebc986712/dlp_mpi-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "fb5647c8bdebfa91afecf132d64e9d28", "sha256": "6635708a758db61cea0bfa4d381ee507e2729a1710712cf365d48cb3d41f8a36"}, "downloads": -1, "filename": "dlp_mpi-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "fb5647c8bdebfa91afecf132d64e9d28", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6, <4", "size": 12022, "upload_time": "2019-10-29T08:23:45", "upload_time_iso_8601": "2019-10-29T08:23:45.850781Z", "url": "https://files.pythonhosted.org/packages/01/59/20d7b6c5c531a2007aea49664b70ebbfefc56477c594669e0a7e4162c9ee/dlp_mpi-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "574b0d1cbab516305627f50f99995005", "sha256": "dd092a839dacadeee241a71f80b7ef26baedc044271728396ac63710982d0bf2"}, "downloads": -1, "filename": "dlp_mpi-0.0.2.tar.gz", "has_sig": false, "md5_digest": "574b0d1cbab516305627f50f99995005", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6, <4", "size": 14549, "upload_time": "2019-10-29T08:23:48", "upload_time_iso_8601": "2019-10-29T08:23:48.231298Z", "url": "https://files.pythonhosted.org/packages/d3/9a/d89ba58e8ffbc2b6a79fc5787e8d655c00fb279bb8807138515c7f8574d0/dlp_mpi-0.0.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "fb5647c8bdebfa91afecf132d64e9d28", "sha256": "6635708a758db61cea0bfa4d381ee507e2729a1710712cf365d48cb3d41f8a36"}, "downloads": -1, "filename": "dlp_mpi-0.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "fb5647c8bdebfa91afecf132d64e9d28", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6, <4", "size": 12022, "upload_time": "2019-10-29T08:23:45", "upload_time_iso_8601": "2019-10-29T08:23:45.850781Z", "url": "https://files.pythonhosted.org/packages/01/59/20d7b6c5c531a2007aea49664b70ebbfefc56477c594669e0a7e4162c9ee/dlp_mpi-0.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "574b0d1cbab516305627f50f99995005", "sha256": "dd092a839dacadeee241a71f80b7ef26baedc044271728396ac63710982d0bf2"}, "downloads": -1, "filename": "dlp_mpi-0.0.2.tar.gz", "has_sig": false, "md5_digest": "574b0d1cbab516305627f50f99995005", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6, <4", "size": 14549, "upload_time": "2019-10-29T08:23:48", "upload_time_iso_8601": "2019-10-29T08:23:48.231298Z", "url": "https://files.pythonhosted.org/packages/d3/9a/d89ba58e8ffbc2b6a79fc5787e8d655c00fb279bb8807138515c7f8574d0/dlp_mpi-0.0.2.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:51:18 2020"}