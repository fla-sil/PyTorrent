{"info": {"author": "Ali Fadel", "author_email": "aliosm1997@gmail.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# Simple Representations\n\nThis library is based on the [Transformers](https://github.com/huggingface/transformers) library by HuggingFace. Using this library, you can quickly extract text representations from Transformer models. Only two lines of code are needed to initialize the required model and extract the text representations from it.\n\n# Table of contents\n\n* [Installation](#installation)\n\t* [With `pip`](#with-pip)\n\t* [From source](#from-source)\n* [Usage](#usage)\n\t* [Minimal Start](#minimal-start)\n\t* [Default Settings](#default-settings)\n\t* [Current Pretrained Models](#current-pretrained-models)\n* [Acknowledgements](#acknowledgements)\n\n## Installation\n\nThis repository is tested on Python 3.6.8 and PyTorch 1.2.0\n\n### With `pip`\n\nFirst you need to install PyTorch. Please refer to [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) regarding the specific install command for your platform.\n\nWhen PyTorch has been installed, Simple Representation can be installed using pip as follows:\n\n```\npip install simplerepresentation\n```\n\n### From source\n\nHere also, you first need to install PyTorch. Please refer to [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) regarding the specific install command for your platform.\n\nWhen PyTorch has been installed, you can install from source by cloning the repository and running:\n\n```\npip install .\n```\n\n## Usage\n\n### Minimal Start\n\nThe following example extracts the text representations from `BERT Base Uncased` model for the sentences `Hello Transformers!` and `It's very simple.`.\n\n```python\nfrom simplerepresentations import RepresentationModel\n\n\ndef load_data():\n\treturn ['Hello Transformers!', 'It\\'s very simple.']\n\n\nif __name__ == '__main__':\n\tmodel_type = 'bert'\n\tmodel_name = 'bert-base-uncased'\n\n\trepresentation_model = RepresentationModel(\n\t\tmodel_type=model_type,\n\t\tmodel_name=model_name,\n\t\tbatch_size=32,\n\t\tmax_seq_length=10, # truncate sentences to be less than or equal to 10 tokens\n\t\tcombination_method='cat', # concatenate the last `last_hidden_to_use` hidden states\n\t\tlast_hidden_to_use=4 # use the last 4 hidden states to build tokens representations\n\t)\n\n\ttext_a = load_data()\n\n\tall_sentences_representations, all_tokens_representations = representation_model(text_a=text_a)\n\n\tprint(all_sentences_representations.shape) # (2, 768) => (number of sentences, hidden size)\n\tprint(all_tokens_representations.shape) # (2, 10, 3072) => (number of sentences, number of tokens, hidden size)\n```\n\nYou can change the code in `load_data` function to load your own data from any source you want (e.g. a CSV file).\n\n### Default Settings\n\nThe default settings for `RepresentationModel` class are given below:\n\n#### batch_size (32): integer\nThe batch size will be used while extracting representations.\n\n#### max_seq_length (128): integer\nMaximum sequence length the model will support.\n\n#### last_hidden_to_use (1): integer\nThe number of the last hidden states that will be used to build the representations.\n\n#### combination_method ('sum'): string ('sum', 'cat')\nThe method that will be used to combine the `last_hidden_to_use`.\n\n#### use_cuda (True): boolean\nWhether to use `CUDA` or not.\n\n#### process_count (cpu_count() - 2 if cpu_count() > 2 else 1): integer\nNumber of CPU cores (processes) to use when converting examples to features. Default is (number of cores - 2) or 1 if (number of cores <= 2).\n\n#### chunksize (500): integer\nThe number of chunks that the examples will be divided to when converting them to features.\n\n### Current Pretrained Models\n\nYou can find the complete list of the current pretrained models from Transformers library [documentation](https://huggingface.co/transformers/pretrained_models.html).\n\n## Acknowledgements\n\nNone of this would have been possible without the hard work by the HuggingFace team in developing the [Transformers](https://github.com/huggingface/transformers) library.\n\nAlso, a lot of ideas used in this repository inspired from the [Simple Transformers](https://github.com/ThilinaRajapakse/simpletransformers) library.", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/AliOsm/simplerepresentations", "keywords": "NLP,natural language processing,deep learning,pytorch,transformer,BERT,XLM,XLNet,RoBERTa,DistilBERT,GPT,GPT-2,ALBERT,google,openai,CMU", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "simplerepresentations", "package_url": "https://pypi.org/project/simplerepresentations/", "platform": "", "project_url": "https://pypi.org/project/simplerepresentations/", "project_urls": {"Homepage": "https://github.com/AliOsm/simplerepresentations"}, "release_url": "https://pypi.org/project/simplerepresentations/0.0.4/", "requires_dist": null, "requires_python": ">=3.6", "summary": "Easy-to-use text representations extraction library based on the Transformers library.", "version": "0.0.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Simple Representations</h1>\n<p>This library is based on the <a href=\"https://github.com/huggingface/transformers\" rel=\"nofollow\">Transformers</a> library by HuggingFace. Using this library, you can quickly extract text representations from Transformer models. Only two lines of code are needed to initialize the required model and extract the text representations from it.</p>\n<h1>Table of contents</h1>\n<ul>\n<li><a href=\"#installation\" rel=\"nofollow\">Installation</a>\n<ul>\n<li><a href=\"#with-pip\" rel=\"nofollow\">With <code>pip</code></a></li>\n<li><a href=\"#from-source\" rel=\"nofollow\">From source</a></li>\n</ul>\n</li>\n<li><a href=\"#usage\" rel=\"nofollow\">Usage</a>\n<ul>\n<li><a href=\"#minimal-start\" rel=\"nofollow\">Minimal Start</a></li>\n<li><a href=\"#default-settings\" rel=\"nofollow\">Default Settings</a></li>\n<li><a href=\"#current-pretrained-models\" rel=\"nofollow\">Current Pretrained Models</a></li>\n</ul>\n</li>\n<li><a href=\"#acknowledgements\" rel=\"nofollow\">Acknowledgements</a></li>\n</ul>\n<h2>Installation</h2>\n<p>This repository is tested on Python 3.6.8 and PyTorch 1.2.0</p>\n<h3>With <code>pip</code></h3>\n<p>First you need to install PyTorch. Please refer to <a href=\"https://pytorch.org/get-started/locally/#start-locally\" rel=\"nofollow\">PyTorch installation page</a> regarding the specific install command for your platform.</p>\n<p>When PyTorch has been installed, Simple Representation can be installed using pip as follows:</p>\n<pre><code>pip install simplerepresentation\n</code></pre>\n<h3>From source</h3>\n<p>Here also, you first need to install PyTorch. Please refer to <a href=\"https://pytorch.org/get-started/locally/#start-locally\" rel=\"nofollow\">PyTorch installation page</a> regarding the specific install command for your platform.</p>\n<p>When PyTorch has been installed, you can install from source by cloning the repository and running:</p>\n<pre><code>pip install .\n</code></pre>\n<h2>Usage</h2>\n<h3>Minimal Start</h3>\n<p>The following example extracts the text representations from <code>BERT Base Uncased</code> model for the sentences <code>Hello Transformers!</code> and <code>It's very simple.</code>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">simplerepresentations</span> <span class=\"kn\">import</span> <span class=\"n\">RepresentationModel</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">load_data</span><span class=\"p\">():</span>\n\t<span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"s1\">'Hello Transformers!'</span><span class=\"p\">,</span> <span class=\"s1\">'It</span><span class=\"se\">\\'</span><span class=\"s1\">s very simple.'</span><span class=\"p\">]</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">'__main__'</span><span class=\"p\">:</span>\n\t<span class=\"n\">model_type</span> <span class=\"o\">=</span> <span class=\"s1\">'bert'</span>\n\t<span class=\"n\">model_name</span> <span class=\"o\">=</span> <span class=\"s1\">'bert-base-uncased'</span>\n\n\t<span class=\"n\">representation_model</span> <span class=\"o\">=</span> <span class=\"n\">RepresentationModel</span><span class=\"p\">(</span>\n\t\t<span class=\"n\">model_type</span><span class=\"o\">=</span><span class=\"n\">model_type</span><span class=\"p\">,</span>\n\t\t<span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"n\">model_name</span><span class=\"p\">,</span>\n\t\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span>\n\t\t<span class=\"n\">max_seq_length</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"c1\"># truncate sentences to be less than or equal to 10 tokens</span>\n\t\t<span class=\"n\">combination_method</span><span class=\"o\">=</span><span class=\"s1\">'cat'</span><span class=\"p\">,</span> <span class=\"c1\"># concatenate the last `last_hidden_to_use` hidden states</span>\n\t\t<span class=\"n\">last_hidden_to_use</span><span class=\"o\">=</span><span class=\"mi\">4</span> <span class=\"c1\"># use the last 4 hidden states to build tokens representations</span>\n\t<span class=\"p\">)</span>\n\n\t<span class=\"n\">text_a</span> <span class=\"o\">=</span> <span class=\"n\">load_data</span><span class=\"p\">()</span>\n\n\t<span class=\"n\">all_sentences_representations</span><span class=\"p\">,</span> <span class=\"n\">all_tokens_representations</span> <span class=\"o\">=</span> <span class=\"n\">representation_model</span><span class=\"p\">(</span><span class=\"n\">text_a</span><span class=\"o\">=</span><span class=\"n\">text_a</span><span class=\"p\">)</span>\n\n\t<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">all_sentences_representations</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># (2, 768) =&gt; (number of sentences, hidden size)</span>\n\t<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">all_tokens_representations</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># (2, 10, 3072) =&gt; (number of sentences, number of tokens, hidden size)</span>\n</pre>\n<p>You can change the code in <code>load_data</code> function to load your own data from any source you want (e.g. a CSV file).</p>\n<h3>Default Settings</h3>\n<p>The default settings for <code>RepresentationModel</code> class are given below:</p>\n<h4>batch_size (32): integer</h4>\n<p>The batch size will be used while extracting representations.</p>\n<h4>max_seq_length (128): integer</h4>\n<p>Maximum sequence length the model will support.</p>\n<h4>last_hidden_to_use (1): integer</h4>\n<p>The number of the last hidden states that will be used to build the representations.</p>\n<h4>combination_method ('sum'): string ('sum', 'cat')</h4>\n<p>The method that will be used to combine the <code>last_hidden_to_use</code>.</p>\n<h4>use_cuda (True): boolean</h4>\n<p>Whether to use <code>CUDA</code> or not.</p>\n<h4>process_count (cpu_count() - 2 if cpu_count() &gt; 2 else 1): integer</h4>\n<p>Number of CPU cores (processes) to use when converting examples to features. Default is (number of cores - 2) or 1 if (number of cores &lt;= 2).</p>\n<h4>chunksize (500): integer</h4>\n<p>The number of chunks that the examples will be divided to when converting them to features.</p>\n<h3>Current Pretrained Models</h3>\n<p>You can find the complete list of the current pretrained models from Transformers library <a href=\"https://huggingface.co/transformers/pretrained_models.html\" rel=\"nofollow\">documentation</a>.</p>\n<h2>Acknowledgements</h2>\n<p>None of this would have been possible without the hard work by the HuggingFace team in developing the <a href=\"https://github.com/huggingface/transformers\" rel=\"nofollow\">Transformers</a> library.</p>\n<p>Also, a lot of ideas used in this repository inspired from the <a href=\"https://github.com/ThilinaRajapakse/simpletransformers\" rel=\"nofollow\">Simple Transformers</a> library.</p>\n\n          </div>"}, "last_serial": 6391509, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "48913e71e4fe752703af6d3782569300", "sha256": "28da6f3fcbb6161e8a1c8597eee5df2c60f028a4294626a19704a30dc069fbb0"}, "downloads": -1, "filename": "simplerepresentations-0.0.1.tar.gz", "has_sig": false, "md5_digest": "48913e71e4fe752703af6d3782569300", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 7009, "upload_time": "2019-10-25T17:27:06", "upload_time_iso_8601": "2019-10-25T17:27:06.662529Z", "url": "https://files.pythonhosted.org/packages/bb/92/9b1c897408151a6cd00cd2ac1f1081fe6a1952ccede792dbe05c4c7731d6/simplerepresentations-0.0.1.tar.gz", "yanked": false}], "0.0.2": [{"comment_text": "", "digests": {"md5": "b724c281ffaaf9bcc4b07d9a3c807bf4", "sha256": "b8339ba899d8f8e0dbea87615ca1ffdf15703fe49b0d54131cc2a48bf19298e4"}, "downloads": -1, "filename": "simplerepresentations-0.0.2.tar.gz", "has_sig": false, "md5_digest": "b724c281ffaaf9bcc4b07d9a3c807bf4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 7697, "upload_time": "2019-12-14T16:28:48", "upload_time_iso_8601": "2019-12-14T16:28:48.553573Z", "url": "https://files.pythonhosted.org/packages/fa/86/a538ce39a1d4ed5883acfaaab3dc34e23a027b29d43eda7634d7863fa88e/simplerepresentations-0.0.2.tar.gz", "yanked": false}], "0.0.3": [{"comment_text": "", "digests": {"md5": "6c7ffb665bd5c52b419db85d94e2f08a", "sha256": "575023d12ea3114c7fe42c854b3b631e8349722d988b6be15ab88176acbce5c6"}, "downloads": -1, "filename": "simplerepresentations-0.0.3.tar.gz", "has_sig": false, "md5_digest": "6c7ffb665bd5c52b419db85d94e2f08a", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 7320, "upload_time": "2020-01-03T14:50:15", "upload_time_iso_8601": "2020-01-03T14:50:15.343546Z", "url": "https://files.pythonhosted.org/packages/19/8f/2530240ae8d346142144447aebe29ef55785d1ef56f279aecee7788e8a2d/simplerepresentations-0.0.3.tar.gz", "yanked": false}], "0.0.4": [{"comment_text": "", "digests": {"md5": "026ab4ada0f239fdfe5035b252056d79", "sha256": "bdc3b6a08cabb4f6966dd723e5e8dcd56336a5b049d53c44ad99b17b32e12092"}, "downloads": -1, "filename": "simplerepresentations-0.0.4.tar.gz", "has_sig": false, "md5_digest": "026ab4ada0f239fdfe5035b252056d79", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 7313, "upload_time": "2020-01-03T15:33:31", "upload_time_iso_8601": "2020-01-03T15:33:31.046729Z", "url": "https://files.pythonhosted.org/packages/b6/c7/6b3dbc9b94612307a1f360640da888d5599783b601b0944422e1ba560b58/simplerepresentations-0.0.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "026ab4ada0f239fdfe5035b252056d79", "sha256": "bdc3b6a08cabb4f6966dd723e5e8dcd56336a5b049d53c44ad99b17b32e12092"}, "downloads": -1, "filename": "simplerepresentations-0.0.4.tar.gz", "has_sig": false, "md5_digest": "026ab4ada0f239fdfe5035b252056d79", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 7313, "upload_time": "2020-01-03T15:33:31", "upload_time_iso_8601": "2020-01-03T15:33:31.046729Z", "url": "https://files.pythonhosted.org/packages/b6/c7/6b3dbc9b94612307a1f360640da888d5599783b601b0944422e1ba560b58/simplerepresentations-0.0.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:10:17 2020"}