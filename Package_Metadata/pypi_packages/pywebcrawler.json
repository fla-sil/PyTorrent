{"info": {"author": "Karthik E C", "author_email": "eckarthik39@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Python Web Crawler [![Build Status](https://travis-ci.org/eckarthik/PyWebCrawler.svg?branch=master)](https://travis-ci.org/eckarthik/PyWebCrawler)\n\nA web crawler written in Python to crawl a given website.\n\n# Features!\n\n  - Faster\n  - Ablility to specify the number of threads to use to crawl the given website\n  - Ability to use proxies to bypass IP restrictions\n  - Clear summary of all the urls that were crawled. View the crawled.txt file to see the complete list of all the links crawled\n  - Ability to specify delay between each HTTP Request\n  - Stop and resume crawler whenever you need\n  - Gather all the urls with their titles to a csv, incase if you are planning to create a search engine\n  - Search for specific text throughout the website\n  - Clear statistics about how many links ended up as Files,Timeout Errors,Connecrion Errors\n  - Crawl until you need. You can specify upto what level the crawler should crawl.\n  - Random browser user agents will be used while crawling.\n\n# Upcoming Features!\n\n  - Gather AWS Buckets,Emails,Phone Numbers etc\n  - Download all images\n\n### Dependencies\n\nThis tool uses a number of open source projects to work properly:\n\n* [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) - Parser to parse the HTML response of each request made.\n* [Requests](https://pypi.org/project/requests/) - To make GET requests to the URLs.\n\n### Usage\nIf you like to see the list of supported features, simply run\n![Usage Demo](https://i.ibb.co/8zVss64/Running-Main-Py.png)\n\n###### Specifying only to crawl for 3 levels\n![Depth Crawl](https://i.ibb.co/TTF8g2X/Running-Main-Py.png>)\n###### Search for specific text throughout the website\n![Text Search](https://i.ibb.co/q9trhVp/Running-Main-Py.png)\n###### Gather all the links along with their titles to a CSV file. A CSV file with the links and their titles will be created after the crawl completes\n![Gather Titles](https://i.ibb.co/6sDD2cC/Running-Main-Py.png)\n###### Use proxies to crawl the site. \n![Use Proxies](https://i.ibb.co/51SwP7m/Running-Main-Py.png)\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/eckarthik/PyWebCrawler", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "pywebcrawler", "package_url": "https://pypi.org/project/pywebcrawler/", "platform": "", "project_url": "https://pypi.org/project/pywebcrawler/", "project_urls": {"Homepage": "https://github.com/eckarthik/PyWebCrawler"}, "release_url": "https://pypi.org/project/pywebcrawler/0.0.1/", "requires_dist": ["requests", "bs4"], "requires_python": ">=3.6", "summary": "A fast web crawler to satisfy all your needs", "version": "0.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Python Web Crawler <a href=\"https://travis-ci.org/eckarthik/PyWebCrawler\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/42f892defd6e4946226877a57ff61eecd36fc258/68747470733a2f2f7472617669732d63692e6f72672f65636b61727468696b2f5079576562437261776c65722e7376673f6272616e63683d6d6173746572\"></a></h1>\n<p>A web crawler written in Python to crawl a given website.</p>\n<h1>Features!</h1>\n<ul>\n<li>Faster</li>\n<li>Ablility to specify the number of threads to use to crawl the given website</li>\n<li>Ability to use proxies to bypass IP restrictions</li>\n<li>Clear summary of all the urls that were crawled. View the crawled.txt file to see the complete list of all the links crawled</li>\n<li>Ability to specify delay between each HTTP Request</li>\n<li>Stop and resume crawler whenever you need</li>\n<li>Gather all the urls with their titles to a csv, incase if you are planning to create a search engine</li>\n<li>Search for specific text throughout the website</li>\n<li>Clear statistics about how many links ended up as Files,Timeout Errors,Connecrion Errors</li>\n<li>Crawl until you need. You can specify upto what level the crawler should crawl.</li>\n<li>Random browser user agents will be used while crawling.</li>\n</ul>\n<h1>Upcoming Features!</h1>\n<ul>\n<li>Gather AWS Buckets,Emails,Phone Numbers etc</li>\n<li>Download all images</li>\n</ul>\n<h3>Dependencies</h3>\n<p>This tool uses a number of open source projects to work properly:</p>\n<ul>\n<li><a href=\"https://pypi.org/project/beautifulsoup4/\" rel=\"nofollow\">BeautifulSoup</a> - Parser to parse the HTML response of each request made.</li>\n<li><a href=\"https://pypi.org/project/requests/\" rel=\"nofollow\">Requests</a> - To make GET requests to the URLs.</li>\n</ul>\n<h3>Usage</h3>\n<p>If you like to see the list of supported features, simply run\n<img alt=\"Usage Demo\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/7117b69b9cb8b0a7f473160c56545890dfbebe87/68747470733a2f2f692e6962622e636f2f387a56737336342f52756e6e696e672d4d61696e2d50792e706e67\"></p>\n<h6>Specifying only to crawl for 3 levels</h6>\n<p><img alt=\"Depth Crawl\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/48bfeeb72e941e95caf3551468590f0b8ad63d58/68747470733a2f2f692e6962622e636f2f545446386732582f52756e6e696e672d4d61696e2d50792e706e67253345\"></p>\n<h6>Search for specific text throughout the website</h6>\n<p><img alt=\"Text Search\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5acb8394f88d0584685415a7a20f6b9edbdfa9e0/68747470733a2f2f692e6962622e636f2f713974726856702f52756e6e696e672d4d61696e2d50792e706e67\"></p>\n<h6>Gather all the links along with their titles to a CSV file. A CSV file with the links and their titles will be created after the crawl completes</h6>\n<p><img alt=\"Gather Titles\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/cb1ce3515fafa110d3e54e1369b965c43460ed4d/68747470733a2f2f692e6962622e636f2f367344443263432f52756e6e696e672d4d61696e2d50792e706e67\"></p>\n<h6>Use proxies to crawl the site.</h6>\n<p><img alt=\"Use Proxies\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/75a01ccad59fb109be5ee08f37824e6af8439afe/68747470733a2f2f692e6962622e636f2f3531537750376d2f52756e6e696e672d4d61696e2d50792e706e67\"></p>\n\n          </div>"}, "last_serial": 6478523, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "3f6d412bf4f73e7e0d62a956d787df3a", "sha256": "9decb290e655c1bd8b851cdf95556162c8285bb404b459dabada1f0ac8c70d2a"}, "downloads": -1, "filename": "pywebcrawler-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "3f6d412bf4f73e7e0d62a956d787df3a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 17388, "upload_time": "2020-01-18T15:28:12", "upload_time_iso_8601": "2020-01-18T15:28:12.229614Z", "url": "https://files.pythonhosted.org/packages/31/38/877e41e197bf1aec6a73533b0fb7ae326ec5decf7a3d7ce4b6ad577070de/pywebcrawler-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9c1fb2f4442a859811673d719f1a276c", "sha256": "8746bb0d60b12be062757375718a4fd96d58619f1edcaddc8c5ac4ee67223e01"}, "downloads": -1, "filename": "pywebcrawler-0.0.1.tar.gz", "has_sig": false, "md5_digest": "9c1fb2f4442a859811673d719f1a276c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 8151, "upload_time": "2020-01-18T15:28:15", "upload_time_iso_8601": "2020-01-18T15:28:15.714296Z", "url": "https://files.pythonhosted.org/packages/78/10/a5debfdd4e9c6949fca4233e152fb3d5dbe9a52eb6c3918978c6919517d1/pywebcrawler-0.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3f6d412bf4f73e7e0d62a956d787df3a", "sha256": "9decb290e655c1bd8b851cdf95556162c8285bb404b459dabada1f0ac8c70d2a"}, "downloads": -1, "filename": "pywebcrawler-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "3f6d412bf4f73e7e0d62a956d787df3a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 17388, "upload_time": "2020-01-18T15:28:12", "upload_time_iso_8601": "2020-01-18T15:28:12.229614Z", "url": "https://files.pythonhosted.org/packages/31/38/877e41e197bf1aec6a73533b0fb7ae326ec5decf7a3d7ce4b6ad577070de/pywebcrawler-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9c1fb2f4442a859811673d719f1a276c", "sha256": "8746bb0d60b12be062757375718a4fd96d58619f1edcaddc8c5ac4ee67223e01"}, "downloads": -1, "filename": "pywebcrawler-0.0.1.tar.gz", "has_sig": false, "md5_digest": "9c1fb2f4442a859811673d719f1a276c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 8151, "upload_time": "2020-01-18T15:28:15", "upload_time_iso_8601": "2020-01-18T15:28:15.714296Z", "url": "https://files.pythonhosted.org/packages/78/10/a5debfdd4e9c6949fca4233e152fb3d5dbe9a52eb6c3918978c6919517d1/pywebcrawler-0.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:11:35 2020"}