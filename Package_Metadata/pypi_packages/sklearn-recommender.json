{"info": {"author": "Felix Geilert", "author_email": "", "bugtrack_url": null, "classifiers": [], "description": "# Sklearn Recommender\n\nThis library implements some common recommender functions based on the `Estimator` and `Transformer` interfaces from sklearn.\n\n## Getting Started\n\nInstall it through PyPi through:\n\n\n`pip install sklearn-recommender`\n\n\nInstall the library on your local distribution through:\n\n\n`pip install .`\n\n\n## Tutorial\n\nAll functions of the library are build around `Transformer` and `Estimator`, allowing them to be used through a `Pipeline` as well as with `GridSearchCV`.\nIn general the system assumes the input to be\n\n```python\nimport sklearn_recommender as skr\n```\n\n### Transformer\n\n**User-Item**\n\nUses a list of user-item interactions to create a user-item matrix that can also be used as input to the similarity transformer. This also supports binary interactions by setting the `binarize` flag in the constructor.\n\n```python\ntf = skr.transformer.UserItemTransformer(user_col='user_id', item_col='item_id', value_col='ranking', agg_fct='mean')\nuser_item = tf.transform(df_reviews)\n```\n\n**Similarity**\n\nCreates a similarity matrix based on the given input data. It assumes that each row in a matrix is single item and computes the similarity between them according to the features listed in the given `cols`. The resulting dataframe will have the `index_col` as index (or preserves the original index if `index_col` is not given).\nNote that `cols` can be a list of names or a tuple defining the position of the first and last column to use.\n\n```python\ntf = skr.transformer.SimilarityTransformer(cols=(2, -1), index_col='item_id', normalize=True)\nsim_mat = tf.transform(items)\n```\n\n**GloVe**\n\nBased on the [Global Vector for Word Embeddings](https://nlp.stanford.edu/projects/glove/), this implements a transform that create n-dimensional word embeddings based on a list of input texts.\nThe library comes with functions to download pre-trained models from the project website (note of caution: these models can take 3+GB of additional disk space). There are current two pre-trained models integrated: `'wikipedia'` (which only has 300-dimensional embeddings) and `'twitter'` (coming with 25, 50, 100 and 200 dimensional embeddings).\nThere are also multiple ways to create embeddings for the given text (as it spans more than one word):\n\n* `word` - generates a list of simple word embeddings (only recommended for single words)\n* `sent` - creates a document embedding by adding up all vectors and normalizing them\n* `sent-matrix` - creates a matrix with `max_feat` rows that contains the embeddings for the first `max_feat` words (if less words it is filled with random vectors according to distribution of vector space)\n* `centroid` - Takes all word embeddings in the given text and computes the `max_feat` centroids for the clusters of the vectors\n\n```python\n# optionally download the requried models\nskr.glove.download('twitter')\ntf = skr.glove.GloVeTransformer('twitter', 25, 'sent', tokenizer=skr.nlp.tokenize_clean)\n```\n\n### Recommender\n\n**Similarity**\n\nRecommendations are made based on the similarity of item. That requires the id of an item to be given and returns the n most similar candidates.\n\n```python\nrec = skr.recommender.SimilarityRecommender(5)\nrec.fit(sim_mat)\n# predict the 5 most similar items to the given items 5, 6 and 7 respectively\nrec.predict([5, 6, 7])\n```\n\n**Cross-Similarity**\n\nCollaborative-Filtering based approach. This uses the most similar items on one dimensions (e.g. most similar users to the given user) to predict the most relevant items along a different dimension (e.g. the items the most similar users interacted the most with).\n\n```python\nrec = skr.recommender.CrossSimilarityRecommender(5)\nrec.fit((user_item, sim_mat, ))\nrec.predict([10, 12])\n```\n\n### Helper Functions\n\nApart from the sklearn extensions, there are also various helper functions that help to prepare data for the training or reasoning process.\n\n**Train-Test Split:**\n\nTrain-Test split for user-item interactions.\n\n```python\ndf = ...\n# create a 30% size test set\ntrain_df, test_df = skr.train_test_split(df, split_size=0.3)\n```\n\n**NLP Functions:**\n\nIn combination with text embeddings, there are some functions to tokenize input words using functions from `nltk`.\n\n```python\ntokens = skr.nlp.tokenize_clean('Just a simple sample text.')\n```\n\n## Design Philosophy\n\nThe library is build on top of the sklearn interfaces to allow easy chaining of pipelines and expects pandas dataframes as inputs.\nThe general goal is to allow the quick and easy exploration of data relevant to recommender systems as well as the quick building of a baseline recommender.\n\n## Future Work\n\n* Implement sufficient test coverage\n* Add type tests to code (+ conversions to required datatypes between numpy and pandas)\n* Implement additional guarantees into the `train_test_split` (e.g. coverage of item ids)\n\n## License\n\nThe code is published under MIT License", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/felixnext/sklearn-recommender", "keywords": "recommender systems", "license": "MIT License", "maintainer": "", "maintainer_email": "", "name": "sklearn-recommender", "package_url": "https://pypi.org/project/sklearn-recommender/", "platform": "", "project_url": "https://pypi.org/project/sklearn-recommender/", "project_urls": {"Homepage": "https://github.com/felixnext/sklearn-recommender"}, "release_url": "https://pypi.org/project/sklearn-recommender/0.1.5/", "requires_dist": null, "requires_python": "", "summary": "Sklearn Extension to integration recommender functions", "version": "0.1.5", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Sklearn Recommender</h1>\n<p>This library implements some common recommender functions based on the <code>Estimator</code> and <code>Transformer</code> interfaces from sklearn.</p>\n<h2>Getting Started</h2>\n<p>Install it through PyPi through:</p>\n<p><code>pip install sklearn-recommender</code></p>\n<p>Install the library on your local distribution through:</p>\n<p><code>pip install .</code></p>\n<h2>Tutorial</h2>\n<p>All functions of the library are build around <code>Transformer</code> and <code>Estimator</code>, allowing them to be used through a <code>Pipeline</code> as well as with <code>GridSearchCV</code>.\nIn general the system assumes the input to be</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">sklearn_recommender</span> <span class=\"k\">as</span> <span class=\"nn\">skr</span>\n</pre>\n<h3>Transformer</h3>\n<p><strong>User-Item</strong></p>\n<p>Uses a list of user-item interactions to create a user-item matrix that can also be used as input to the similarity transformer. This also supports binary interactions by setting the <code>binarize</code> flag in the constructor.</p>\n<pre><span class=\"n\">tf</span> <span class=\"o\">=</span> <span class=\"n\">skr</span><span class=\"o\">.</span><span class=\"n\">transformer</span><span class=\"o\">.</span><span class=\"n\">UserItemTransformer</span><span class=\"p\">(</span><span class=\"n\">user_col</span><span class=\"o\">=</span><span class=\"s1\">'user_id'</span><span class=\"p\">,</span> <span class=\"n\">item_col</span><span class=\"o\">=</span><span class=\"s1\">'item_id'</span><span class=\"p\">,</span> <span class=\"n\">value_col</span><span class=\"o\">=</span><span class=\"s1\">'ranking'</span><span class=\"p\">,</span> <span class=\"n\">agg_fct</span><span class=\"o\">=</span><span class=\"s1\">'mean'</span><span class=\"p\">)</span>\n<span class=\"n\">user_item</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">df_reviews</span><span class=\"p\">)</span>\n</pre>\n<p><strong>Similarity</strong></p>\n<p>Creates a similarity matrix based on the given input data. It assumes that each row in a matrix is single item and computes the similarity between them according to the features listed in the given <code>cols</code>. The resulting dataframe will have the <code>index_col</code> as index (or preserves the original index if <code>index_col</code> is not given).\nNote that <code>cols</code> can be a list of names or a tuple defining the position of the first and last column to use.</p>\n<pre><span class=\"n\">tf</span> <span class=\"o\">=</span> <span class=\"n\">skr</span><span class=\"o\">.</span><span class=\"n\">transformer</span><span class=\"o\">.</span><span class=\"n\">SimilarityTransformer</span><span class=\"p\">(</span><span class=\"n\">cols</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">index_col</span><span class=\"o\">=</span><span class=\"s1\">'item_id'</span><span class=\"p\">,</span> <span class=\"n\">normalize</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">sim_mat</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">items</span><span class=\"p\">)</span>\n</pre>\n<p><strong>GloVe</strong></p>\n<p>Based on the <a href=\"https://nlp.stanford.edu/projects/glove/\" rel=\"nofollow\">Global Vector for Word Embeddings</a>, this implements a transform that create n-dimensional word embeddings based on a list of input texts.\nThe library comes with functions to download pre-trained models from the project website (note of caution: these models can take 3+GB of additional disk space). There are current two pre-trained models integrated: <code>'wikipedia'</code> (which only has 300-dimensional embeddings) and <code>'twitter'</code> (coming with 25, 50, 100 and 200 dimensional embeddings).\nThere are also multiple ways to create embeddings for the given text (as it spans more than one word):</p>\n<ul>\n<li><code>word</code> - generates a list of simple word embeddings (only recommended for single words)</li>\n<li><code>sent</code> - creates a document embedding by adding up all vectors and normalizing them</li>\n<li><code>sent-matrix</code> - creates a matrix with <code>max_feat</code> rows that contains the embeddings for the first <code>max_feat</code> words (if less words it is filled with random vectors according to distribution of vector space)</li>\n<li><code>centroid</code> - Takes all word embeddings in the given text and computes the <code>max_feat</code> centroids for the clusters of the vectors</li>\n</ul>\n<pre><span class=\"c1\"># optionally download the requried models</span>\n<span class=\"n\">skr</span><span class=\"o\">.</span><span class=\"n\">glove</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">(</span><span class=\"s1\">'twitter'</span><span class=\"p\">)</span>\n<span class=\"n\">tf</span> <span class=\"o\">=</span> <span class=\"n\">skr</span><span class=\"o\">.</span><span class=\"n\">glove</span><span class=\"o\">.</span><span class=\"n\">GloVeTransformer</span><span class=\"p\">(</span><span class=\"s1\">'twitter'</span><span class=\"p\">,</span> <span class=\"mi\">25</span><span class=\"p\">,</span> <span class=\"s1\">'sent'</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"n\">skr</span><span class=\"o\">.</span><span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">tokenize_clean</span><span class=\"p\">)</span>\n</pre>\n<h3>Recommender</h3>\n<p><strong>Similarity</strong></p>\n<p>Recommendations are made based on the similarity of item. That requires the id of an item to be given and returns the n most similar candidates.</p>\n<pre><span class=\"n\">rec</span> <span class=\"o\">=</span> <span class=\"n\">skr</span><span class=\"o\">.</span><span class=\"n\">recommender</span><span class=\"o\">.</span><span class=\"n\">SimilarityRecommender</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"n\">rec</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">sim_mat</span><span class=\"p\">)</span>\n<span class=\"c1\"># predict the 5 most similar items to the given items 5, 6 and 7 respectively</span>\n<span class=\"n\">rec</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">])</span>\n</pre>\n<p><strong>Cross-Similarity</strong></p>\n<p>Collaborative-Filtering based approach. This uses the most similar items on one dimensions (e.g. most similar users to the given user) to predict the most relevant items along a different dimension (e.g. the items the most similar users interacted the most with).</p>\n<pre><span class=\"n\">rec</span> <span class=\"o\">=</span> <span class=\"n\">skr</span><span class=\"o\">.</span><span class=\"n\">recommender</span><span class=\"o\">.</span><span class=\"n\">CrossSimilarityRecommender</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"n\">rec</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">((</span><span class=\"n\">user_item</span><span class=\"p\">,</span> <span class=\"n\">sim_mat</span><span class=\"p\">,</span> <span class=\"p\">))</span>\n<span class=\"n\">rec</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">12</span><span class=\"p\">])</span>\n</pre>\n<h3>Helper Functions</h3>\n<p>Apart from the sklearn extensions, there are also various helper functions that help to prepare data for the training or reasoning process.</p>\n<p><strong>Train-Test Split:</strong></p>\n<p>Train-Test split for user-item interactions.</p>\n<pre><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"o\">...</span>\n<span class=\"c1\"># create a 30% size test set</span>\n<span class=\"n\">train_df</span><span class=\"p\">,</span> <span class=\"n\">test_df</span> <span class=\"o\">=</span> <span class=\"n\">skr</span><span class=\"o\">.</span><span class=\"n\">train_test_split</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"n\">split_size</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">)</span>\n</pre>\n<p><strong>NLP Functions:</strong></p>\n<p>In combination with text embeddings, there are some functions to tokenize input words using functions from <code>nltk</code>.</p>\n<pre><span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"n\">skr</span><span class=\"o\">.</span><span class=\"n\">nlp</span><span class=\"o\">.</span><span class=\"n\">tokenize_clean</span><span class=\"p\">(</span><span class=\"s1\">'Just a simple sample text.'</span><span class=\"p\">)</span>\n</pre>\n<h2>Design Philosophy</h2>\n<p>The library is build on top of the sklearn interfaces to allow easy chaining of pipelines and expects pandas dataframes as inputs.\nThe general goal is to allow the quick and easy exploration of data relevant to recommender systems as well as the quick building of a baseline recommender.</p>\n<h2>Future Work</h2>\n<ul>\n<li>Implement sufficient test coverage</li>\n<li>Add type tests to code (+ conversions to required datatypes between numpy and pandas)</li>\n<li>Implement additional guarantees into the <code>train_test_split</code> (e.g. coverage of item ids)</li>\n</ul>\n<h2>License</h2>\n<p>The code is published under MIT License</p>\n\n          </div>"}, "last_serial": 5894749, "releases": {"0.1.2": [{"comment_text": "", "digests": {"md5": "18c1b549d6b95dc4b292a9a8609be8b7", "sha256": "20c19ada53d20c5ce54ff93a538c6d68abd9c6438e3426a77ebfbc39fe5163b9"}, "downloads": -1, "filename": "sklearn-recommender-0.1.2.tar.gz", "has_sig": false, "md5_digest": "18c1b549d6b95dc4b292a9a8609be8b7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16114, "upload_time": "2019-09-19T07:53:54", "upload_time_iso_8601": "2019-09-19T07:53:54.954779Z", "url": "https://files.pythonhosted.org/packages/bb/73/4303926f4ad7048f7d14346c0522d48b325f30c2636e1ea30fae97438fdd/sklearn-recommender-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "2a623e65f359dae9aa3dbb99ecb22fed", "sha256": "fc3191c8afa93530cd4822ac0864833362af4cf32605e4e98b3a69c2642a3d52"}, "downloads": -1, "filename": "sklearn-recommender-0.1.3.tar.gz", "has_sig": false, "md5_digest": "2a623e65f359dae9aa3dbb99ecb22fed", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16205, "upload_time": "2019-09-23T04:52:20", "upload_time_iso_8601": "2019-09-23T04:52:20.908228Z", "url": "https://files.pythonhosted.org/packages/41/91/5bcd41dca92b57072c639f6af841cddc9ca122cf61450127af9def1208d3/sklearn-recommender-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "b4891b88a03f56c8768ec4f69fa3e3de", "sha256": "608fcbb55f0ca6a64f9aa5fc6143ccbef11aac912c6de9b3f164744f4b1b8690"}, "downloads": -1, "filename": "sklearn-recommender-0.1.4.tar.gz", "has_sig": false, "md5_digest": "b4891b88a03f56c8768ec4f69fa3e3de", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16399, "upload_time": "2019-09-24T13:20:02", "upload_time_iso_8601": "2019-09-24T13:20:02.792345Z", "url": "https://files.pythonhosted.org/packages/77/ec/1b005f62f5c090c8a35534521f4976d8f1a7ac4c5e0c2680b79f01a9fbfc/sklearn-recommender-0.1.4.tar.gz", "yanked": false}], "0.1.5": [{"comment_text": "", "digests": {"md5": "af434222412948451c086a3f2de66480", "sha256": "4a090120477e6f0959fdef65aaab1ca357674b8072b96f0f5472f3bf7c4786c6"}, "downloads": -1, "filename": "sklearn-recommender-0.1.5.tar.gz", "has_sig": false, "md5_digest": "af434222412948451c086a3f2de66480", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16727, "upload_time": "2019-09-27T07:47:06", "upload_time_iso_8601": "2019-09-27T07:47:06.741143Z", "url": "https://files.pythonhosted.org/packages/98/03/db8aca53881d49ec4209c79c93759b1319383ad0f9d86dead377b6e368d5/sklearn-recommender-0.1.5.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "af434222412948451c086a3f2de66480", "sha256": "4a090120477e6f0959fdef65aaab1ca357674b8072b96f0f5472f3bf7c4786c6"}, "downloads": -1, "filename": "sklearn-recommender-0.1.5.tar.gz", "has_sig": false, "md5_digest": "af434222412948451c086a3f2de66480", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 16727, "upload_time": "2019-09-27T07:47:06", "upload_time_iso_8601": "2019-09-27T07:47:06.741143Z", "url": "https://files.pythonhosted.org/packages/98/03/db8aca53881d49ec4209c79c93759b1319383ad0f9d86dead377b6e368d5/sklearn-recommender-0.1.5.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:08:55 2020"}