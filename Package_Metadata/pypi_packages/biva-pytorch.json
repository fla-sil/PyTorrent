{"info": {"author": "Valentin Lievin", "author_email": "valentin.lievin@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# BIVA (PyTorch)\n\nOfficial PyTorch BIVA implementation (BIVA: A Very Deep Hierarchy of Latent Variables forGenerative Modeling) for binarized MNIST. The original Tensorflow implementation can be found [here](https://github.com/larsmaaloee/BIVA).\n\n**For the sake of clarity, this version slightly differs from the original Tensorflow implementation**\n\n**Coming soon: natural images architecture and experiment**\n\n\n## run the binary MNIST experiment\n\n```bash\npython run_deepvae.py\n```\n\n## Citation\n\n```\n@article{maale2019biva,\n    title={BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling},\n    author={Lars Maal\u00f8e and Marco Fraccaro and Valentin Li\u00e9vin and Ole Winther},\n    year={2019},\n    eprint={1902.02102},\n    archivePrefix={arXiv},\n    primaryClass={stat.ML}\n}\n```\n\n## Pip package\n\n### install requirements\n\n* `pytorch 1.3.0`\n* `torchvision`\n* `matplotlib`\n\n### install package\n\n```bash\npip install git+https://github.com/vlievin/biva-pytorch.git\n```\n\n### build deep VAEs\n\n```python\nimport torch\nfrom torch.distributions import Bernoulli\n\nfrom biva import DenseNormal, ConvNormal\nfrom biva import VAE, LVAE, BIVA\n\n# build a 2 layers VAE for binary images\n\n# define the stochastic layers\nz = [\n    {'N': 8, 'kernel': 5, 'block': ConvNormal},  # z1\n    {'N': 16, 'block': DenseNormal}  # z2\n]\n\n# define the intermediate layers\n# each stage defines the configuration of the blocks for q_(z_{l} | z_{l-1}) and p_(z_{l-1} | z_{l})\n# each stage is defined by a sequence of 3 resnet blocks\n# each block is degined by a tuple [filters, kernel, stride]\nstages = [\n    [[64, 3, 1], [64, 3, 1], [64, 3, 2]],\n    [[64, 3, 1], [64, 3, 1], [64, 3, 2]]\n]\n\n# build the model\nmodel = VAE(tensor_shp=(-1, 1, 28, 28), stages=stages, latents=z, dropout=0.5)\n\n# forward pass\nx = torch.empty((8, 1, 28, 28)).uniform_().bernoulli()\ndata = model(x)  # data = {'x_' : p(x|z), z \\sim q(z|x), 'kl': [kl_z1, kl_z2]}\n\n# sample from prior\ndata = model.sample_from_prior(N=16)  # data = {'x_' : p(x|z), z \\sim p(z)}\nsamples = Bernoulli(logits=data['x_']).sample()\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/vlievin/biva-pytorch", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "biva-pytorch", "package_url": "https://pypi.org/project/biva-pytorch/", "platform": "", "project_url": "https://pypi.org/project/biva-pytorch/", "project_urls": {"Homepage": "https://github.com/vlievin/biva-pytorch"}, "release_url": "https://pypi.org/project/biva-pytorch/0.1.3/", "requires_dist": null, "requires_python": "", "summary": "Official PyTorch BIVA implementation (BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling)", "version": "0.1.3", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>BIVA (PyTorch)</h1>\n<p>Official PyTorch BIVA implementation (BIVA: A Very Deep Hierarchy of Latent Variables forGenerative Modeling) for binarized MNIST. The original Tensorflow implementation can be found <a href=\"https://github.com/larsmaaloee/BIVA\" rel=\"nofollow\">here</a>.</p>\n<p><strong>For the sake of clarity, this version slightly differs from the original Tensorflow implementation</strong></p>\n<p><strong>Coming soon: natural images architecture and experiment</strong></p>\n<h2>run the binary MNIST experiment</h2>\n<pre>python run_deepvae.py\n</pre>\n<h2>Citation</h2>\n<pre><code>@article{maale2019biva,\n    title={BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling},\n    author={Lars Maal\u00f8e and Marco Fraccaro and Valentin Li\u00e9vin and Ole Winther},\n    year={2019},\n    eprint={1902.02102},\n    archivePrefix={arXiv},\n    primaryClass={stat.ML}\n}\n</code></pre>\n<h2>Pip package</h2>\n<h3>install requirements</h3>\n<ul>\n<li><code>pytorch 1.3.0</code></li>\n<li><code>torchvision</code></li>\n<li><code>matplotlib</code></li>\n</ul>\n<h3>install package</h3>\n<pre>pip install git+https://github.com/vlievin/biva-pytorch.git\n</pre>\n<h3>build deep VAEs</h3>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.distributions</span> <span class=\"kn\">import</span> <span class=\"n\">Bernoulli</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">biva</span> <span class=\"kn\">import</span> <span class=\"n\">DenseNormal</span><span class=\"p\">,</span> <span class=\"n\">ConvNormal</span>\n<span class=\"kn\">from</span> <span class=\"nn\">biva</span> <span class=\"kn\">import</span> <span class=\"n\">VAE</span><span class=\"p\">,</span> <span class=\"n\">LVAE</span><span class=\"p\">,</span> <span class=\"n\">BIVA</span>\n\n<span class=\"c1\"># build a 2 layers VAE for binary images</span>\n\n<span class=\"c1\"># define the stochastic layers</span>\n<span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span><span class=\"s1\">'N'</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"s1\">'kernel'</span><span class=\"p\">:</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"s1\">'block'</span><span class=\"p\">:</span> <span class=\"n\">ConvNormal</span><span class=\"p\">},</span>  <span class=\"c1\"># z1</span>\n    <span class=\"p\">{</span><span class=\"s1\">'N'</span><span class=\"p\">:</span> <span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"s1\">'block'</span><span class=\"p\">:</span> <span class=\"n\">DenseNormal</span><span class=\"p\">}</span>  <span class=\"c1\"># z2</span>\n<span class=\"p\">]</span>\n\n<span class=\"c1\"># define the intermediate layers</span>\n<span class=\"c1\"># each stage defines the configuration of the blocks for q_(z_{l} | z_{l-1}) and p_(z_{l-1} | z_{l})</span>\n<span class=\"c1\"># each stage is defined by a sequence of 3 resnet blocks</span>\n<span class=\"c1\"># each block is degined by a tuple [filters, kernel, stride]</span>\n<span class=\"n\">stages</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">[[</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">]],</span>\n    <span class=\"p\">[[</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">]]</span>\n<span class=\"p\">]</span>\n\n<span class=\"c1\"># build the model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">VAE</span><span class=\"p\">(</span><span class=\"n\">tensor_shp</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">),</span> <span class=\"n\">stages</span><span class=\"o\">=</span><span class=\"n\">stages</span><span class=\"p\">,</span> <span class=\"n\">latents</span><span class=\"o\">=</span><span class=\"n\">z</span><span class=\"p\">,</span> <span class=\"n\">dropout</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># forward pass</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">uniform_</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">bernoulli</span><span class=\"p\">()</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>  <span class=\"c1\"># data = {'x_' : p(x|z), z \\sim q(z|x), 'kl': [kl_z1, kl_z2]}</span>\n\n<span class=\"c1\"># sample from prior</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">sample_from_prior</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">)</span>  <span class=\"c1\"># data = {'x_' : p(x|z), z \\sim p(z)}</span>\n<span class=\"n\">samples</span> <span class=\"o\">=</span> <span class=\"n\">Bernoulli</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">'x_'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">()</span>\n</pre>\n\n          </div>"}, "last_serial": 6038213, "releases": {"0.1.2": [{"comment_text": "", "digests": {"md5": "8bd188f3b40f46d675ab74a2fb223cbd", "sha256": "dcb3c75be4259f59ec79106fe5947a880baca6ff3d611aadc94ddaa083bc44f8"}, "downloads": -1, "filename": "biva-pytorch-0.1.2.tar.gz", "has_sig": false, "md5_digest": "8bd188f3b40f46d675ab74a2fb223cbd", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20504, "upload_time": "2019-10-27T11:40:05", "upload_time_iso_8601": "2019-10-27T11:40:05.815686Z", "url": "https://files.pythonhosted.org/packages/f1/c7/b3e5817b429efdc52c10a7d80edd703d51f43204cf9bb4935e11d32ad74a/biva-pytorch-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "8ffd13e0f410ff967309b99f02656271", "sha256": "9c7891e7a8cfb9a2ecbf2b90855431c2d1f76ae5d273c11d0a565ad5ffa437ad"}, "downloads": -1, "filename": "biva-pytorch-0.1.3.tar.gz", "has_sig": false, "md5_digest": "8ffd13e0f410ff967309b99f02656271", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20903, "upload_time": "2019-10-27T19:55:21", "upload_time_iso_8601": "2019-10-27T19:55:21.075339Z", "url": "https://files.pythonhosted.org/packages/d4/bb/54a5df73d642141a7be6f984863dc04ba85c88c731f69b5556843efbe95b/biva-pytorch-0.1.3.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "8ffd13e0f410ff967309b99f02656271", "sha256": "9c7891e7a8cfb9a2ecbf2b90855431c2d1f76ae5d273c11d0a565ad5ffa437ad"}, "downloads": -1, "filename": "biva-pytorch-0.1.3.tar.gz", "has_sig": false, "md5_digest": "8ffd13e0f410ff967309b99f02656271", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 20903, "upload_time": "2019-10-27T19:55:21", "upload_time_iso_8601": "2019-10-27T19:55:21.075339Z", "url": "https://files.pythonhosted.org/packages/d4/bb/54a5df73d642141a7be6f984863dc04ba85c88c731f69b5556843efbe95b/biva-pytorch-0.1.3.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:37:12 2020"}