{"info": {"author": "haoxintong", "author_email": "haoxintongpku@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "Gluon Audio Toolkit\n===================\n\nGluon Audio is a toolkit providing deep learning based audio recognition\nalgorithm. The project is still under development, and only Chinese\nintroduction will be provided.\n\nGluonAR Introduction:\n---------------------\n\nGluonAR is based on MXnet-Gluon, if you are new to it, please check out\n`dmlc 60-minute crash course <http://gluon-crash-course.mxnet.io/>`__.\n\n\u867d\u7136\u540d\u5b57\u53ebGluonAR, \u4f46\u662f\u76ee\u524d\u4ee5\u53ca\u53ef\u4ee5\u9884\u89c1\u7684\u65f6\u95f4\u5185\u53ea\u6709Text-Independent\nSpeaker Recognition\u7684\u5185\u5bb9.\n\n\u5df2\u7ecf\u5b9e\u73b0\u7684feature: - \u4f7f\u7528ffmpeg\u7684pythonic binding\n``av``\\ \u548c\\ ``librosa``\\ \u505aaudio\u6570\u636e\u8bfb\u53d6 - \u6a21\u5757\u652f\u6301\\ ``Hybridize()``.\nforward\u9636\u6bb5\u4e0d\u4f7f\u7528pysound, librosa, scipy, \u6548\u7387\u66f4\u9ad8,\n\u63d0\u4f9b\u5feb\u901f\u8bad\u7ec3\u548cend-to-end\u90e8\u7f72\u7684\u80fd\u529b, \u5305\u62ec: -\n\u57fa\u4e8e\\ ``nd.contrib.fft``\\ \u7684\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362(\\ ``STFTBlock``)\u548cz-score\nblock, \u76f8\u6bd4\u4f7f\u7528numpy\u548cscipy\u9884\u5904\u7406\u540e\u8f7d\u5165GPU\u8bad\u7ec3\u6548\u7387\u63d0\u9ad812%. -\n``MelSpectrogram``, ``DCT1D``, ``MFCC``, ``PowerToDB`` -\n`1808.00158 <https://arxiv.org/abs/1808.00158>`__\\ \u4e2d\u63d0\u51fa\u7684\\ ``SincBlock``\n- gluon\u98ce\u683c\u7684VOX\u6570\u636e\u96c6\u8f7d\u5165 - \u7c7b\u4f3c\u4eba\u8138\u9a8c\u8bc1\u7684Speaker Verification -\n\u4f7f\u7528\u9891\u8c31\u56fe\u8bad\u7ec3\u58f0\u7eb9\u7279\u5f81\u7684\u4f8b\u5b50, \u5728VOX1\u4e0a\u76841:1\u9a8c\u8bc1acc: 0.941152+-0.004926\n\nexample:\n\n.. code:: python\n\n    import numpy as np\n    import mxnet as mx\n    import librosa as rosa\n    from gluonar.utils.viz import view_spec\n    from gluonar.nn.basic_blocks import STFTBlock\n\n    data = rosa.load(r\"resources/speaker_recognition/speaker0_0.m4a\", sr=16000)[0][:35840]\n    nd_data = mx.nd.array([data], ctx=mx.gpu())\n\n    stft = STFTBlock(35840, hop_length=160, win_length=400)\n    stft.initialize(ctx=mx.gpu())\n\n    # stft block forward\n    ret = stft(nd_data).asnumpy()[0][0]\n    spec = np.transpose(ret, (1, 0)) ** 2\n    view_spec(spec)\n\n    # stft in librosa \n    spec = rosa.stft(data, hop_length=160, win_length=400, window=\"hamming\")\n    spec = np.abs(spec) ** 2\n    view_spec(spec)\n\n\u8f93\u51fa:\n\n+-------------+-------------------+\n| STFTBlock   | STFT in librosa   |\n+=============+===================+\n+-------------+-------------------+\n\n\u66f4\u591a\u7684\u4f8b\u5b50\u8bf7\u53c2\u8003\\ ``examples/``.\n\nRequirements\n------------\n\nmxnet-1.5.0+, gluonfr, av, librosa, ...\n\n\u97f3\u9891\u5e93\u7684\u9009\u62e9\u4e3b\u8981\u8003\u8651\u6570\u636e\u8bfb\u53d6\u901f\u5ea6,\n\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u97f3\u9891\u7684\u89e3\u7801\u76f8\u6bd4\u56fe\u50cf\u89e3\u7801\u4f1a\u6d88\u8017\u66f4\u591a\u65f6\u95f4,\n\u5b9e\u9645\u6d4b\u8bd5librosa\u4ece\u78c1\u76d8\u52a0\u8f7d\u4e00\u4e2aaac\u7f16\u7801\u7684\u77ed\u97f3\u9891 \u8017\u65f6\u662fpyav\u76848\u500d\u5de6\u53f3.\n\n-  librosa\n   ``pip install librosa``\n-  ffmpeg\n\n   ::\n\n       # \u4e0b\u8f7dffmpeg\u6e90\u7801, \u8fdb\u5165\u6839\u76ee\u5f55\n       ./configure --extra-cflags=-fPIC --enable-shared\n       make -j\n       sudo make install\n\n-  pyav, \u9700\u8981\u5148\u5b89\u88c5ffmpeg\n   ``pip install av``\n-  | gluonfr\n   | ``pip install git+https://github.com/THUFutureLab/gluon-face.git@master``\n\nDatasets\n--------\n\nTIMIT\n~~~~~\n\nThe DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\nTraining and Test Data. Before using this dataset please follow the\ninstruction on `link <https://catalog.ldc.upenn.edu/LDC93S1>`__.\n\nA copy of this was uploaded to `Google Drive <https://goo.gl/l0sPwz>`__\nby @philipperemy `here <https://github.com/philipperemy/timit>`__.\n\nVoxCeleb\n~~~~~~~~\n\nVoxCeleb is an audio-visual dataset consisting of short clips of human\nspeech, extracted from interview videos uploaded to YouTube.\n\nFor more information, checkout this\n`page <http://www.robots.ox.ac.uk/~vgg/data/voxceleb/>`__.\n\nPretrained Models\n-----------------\n\nSpeaker Recognition\n~~~~~~~~~~~~~~~~~~~\n\nResNet18 training with VoxCeleb\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nDownload: `Baidu <https://pan.baidu.com/s/1Gkhi67oJSiSyAiYNTdPlTw>`__,\n`Google\nDrive <https://drive.google.com/open?id=1oEvSQrnNwYL4pRyQ8t87hRP3m22wuePz>`__\n\nI followed the ideas in paper **VoxCeleb2**\n`1806.05622 <https://arxiv.org/abs/1806.05622>`__ to train this model,\nthe differences between them:\n\n+-------+--------+--------+\n| -     | Res18  | Res34  |\n|       | in     | in     |\n|       | this   | paper  |\n|       | repo   |        |\n+=======+========+========+\n| Train | VoxCel | VoxCel |\n| ed    | eb2    | eb2    |\n| on    |        |        |\n+-------+--------+--------+\n| Input | 224x22 | 512x30 |\n| spec  | 4      | 0      |\n| size  |        |        |\n+-------+--------+--------+\n| Eval  | Random | Origin |\n| on    | 9500+  | al     |\n|       | pair   | VoxCel |\n|       | sample | eb1    |\n|       | s      | test   |\n|       | from   | set    |\n|       | VoxCel |        |\n|       | eb1    |        |\n|       | train  |        |\n|       | and    |        |\n|       | test   |        |\n|       | set    |        |\n+-------+--------+--------+\n| Metri | Accura | EER:   |\n| c     | cy:0.9 | 0.0504 |\n|       | 32656+ |        |\n|       | -0.005 |        |\n|       | 187    |        |\n+-------+--------+--------+\n| Frame | Mxnet  | Matcon |\n| work  | Gluon  | vnet   |\n+-------+--------+--------+\n| ROC   |        | -      |\n+-------+--------+--------+\n\nTODO\n----\n\n\u63a5\u4e0b\u6765\u4f1a\u6162\u6162\u8865\u5168\u4f7f\u7528mxnet gluon\u8bad\u7ec3\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7684\u5de5\u5177\u94fe, \u9884\u8ba1\u4f1a\u82b1\u8d85\u957f\u65f6\u95f4.\n\nDocs\n----\n\nGluonAR documentation is not available now.\n\nAuthors\n-------\n\n{ `haoxintong <https://github.com/haoxintong>`__ }\n\nDiscussion\n----------\n\nAny suggestions, please open an issue.\n\nContributes\n-----------\n\nThe final goal of this project is providing an easy using deep learning\nbased audio algorithm library like\n`pytorch-kaldi <https://github.com/mravanelli/pytorch-kaldi>`__.\n\nContribution is welcomed.\n\nReferences\n----------\n\n1. MXNet Documentation and Tutorials\n   https://zh.diveintodeeplearning.org/\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/haoxintong/gluon-audio", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "gluonar", "package_url": "https://pypi.org/project/gluonar/", "platform": "", "project_url": "https://pypi.org/project/gluonar/", "project_urls": {"Homepage": "https://github.com/haoxintong/gluon-audio"}, "release_url": "https://pypi.org/project/gluonar/0.1.0/", "requires_dist": null, "requires_python": "", "summary": "Gluon Audio Toolkit", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p>Gluon Audio is a toolkit providing deep learning based audio recognition\nalgorithm. The project is still under development, and only Chinese\nintroduction will be provided.</p>\n<div id=\"gluonar-introduction\">\n<h2>GluonAR Introduction:</h2>\n<p>GluonAR is based on MXnet-Gluon, if you are new to it, please check out\n<a href=\"http://gluon-crash-course.mxnet.io/\" rel=\"nofollow\">dmlc 60-minute crash course</a>.</p>\n<p>\u867d\u7136\u540d\u5b57\u53ebGluonAR, \u4f46\u662f\u76ee\u524d\u4ee5\u53ca\u53ef\u4ee5\u9884\u89c1\u7684\u65f6\u95f4\u5185\u53ea\u6709Text-Independent\nSpeaker Recognition\u7684\u5185\u5bb9.</p>\n<p>\u5df2\u7ecf\u5b9e\u73b0\u7684feature: - \u4f7f\u7528ffmpeg\u7684pythonic binding\n<tt>av</tt>\u548c<tt>librosa</tt>\u505aaudio\u6570\u636e\u8bfb\u53d6 - \u6a21\u5757\u652f\u6301<tt>Hybridize()</tt>.\nforward\u9636\u6bb5\u4e0d\u4f7f\u7528pysound, librosa, scipy, \u6548\u7387\u66f4\u9ad8,\n\u63d0\u4f9b\u5feb\u901f\u8bad\u7ec3\u548cend-to-end\u90e8\u7f72\u7684\u80fd\u529b, \u5305\u62ec: -\n\u57fa\u4e8e<tt>nd.contrib.fft</tt>\u7684\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362(<tt>STFTBlock</tt>)\u548cz-score\nblock, \u76f8\u6bd4\u4f7f\u7528numpy\u548cscipy\u9884\u5904\u7406\u540e\u8f7d\u5165GPU\u8bad\u7ec3\u6548\u7387\u63d0\u9ad812%. -\n<tt>MelSpectrogram</tt>, <tt>DCT1D</tt>, <tt>MFCC</tt>, <tt>PowerToDB</tt> -\n<a href=\"https://arxiv.org/abs/1808.00158\" rel=\"nofollow\">1808.00158</a>\u4e2d\u63d0\u51fa\u7684<tt>SincBlock</tt>\n- gluon\u98ce\u683c\u7684VOX\u6570\u636e\u96c6\u8f7d\u5165 - \u7c7b\u4f3c\u4eba\u8138\u9a8c\u8bc1\u7684Speaker Verification -\n\u4f7f\u7528\u9891\u8c31\u56fe\u8bad\u7ec3\u58f0\u7eb9\u7279\u5f81\u7684\u4f8b\u5b50, \u5728VOX1\u4e0a\u76841:1\u9a8c\u8bc1acc: 0.941152+-0.004926</p>\n<p>example:</p>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">mxnet</span> <span class=\"k\">as</span> <span class=\"nn\">mx</span>\n<span class=\"kn\">import</span> <span class=\"nn\">librosa</span> <span class=\"k\">as</span> <span class=\"nn\">rosa</span>\n<span class=\"kn\">from</span> <span class=\"nn\">gluonar.utils.viz</span> <span class=\"kn\">import</span> <span class=\"n\">view_spec</span>\n<span class=\"kn\">from</span> <span class=\"nn\">gluonar.nn.basic_blocks</span> <span class=\"kn\">import</span> <span class=\"n\">STFTBlock</span>\n\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">rosa</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s2\">\"resources/speaker_recognition/speaker0_0.m4a\"</span><span class=\"p\">,</span> <span class=\"n\">sr</span><span class=\"o\">=</span><span class=\"mi\">16000</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">][:</span><span class=\"mi\">35840</span><span class=\"p\">]</span>\n<span class=\"n\">nd_data</span> <span class=\"o\">=</span> <span class=\"n\">mx</span><span class=\"o\">.</span><span class=\"n\">nd</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">data</span><span class=\"p\">],</span> <span class=\"n\">ctx</span><span class=\"o\">=</span><span class=\"n\">mx</span><span class=\"o\">.</span><span class=\"n\">gpu</span><span class=\"p\">())</span>\n\n<span class=\"n\">stft</span> <span class=\"o\">=</span> <span class=\"n\">STFTBlock</span><span class=\"p\">(</span><span class=\"mi\">35840</span><span class=\"p\">,</span> <span class=\"n\">hop_length</span><span class=\"o\">=</span><span class=\"mi\">160</span><span class=\"p\">,</span> <span class=\"n\">win_length</span><span class=\"o\">=</span><span class=\"mi\">400</span><span class=\"p\">)</span>\n<span class=\"n\">stft</span><span class=\"o\">.</span><span class=\"n\">initialize</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"o\">=</span><span class=\"n\">mx</span><span class=\"o\">.</span><span class=\"n\">gpu</span><span class=\"p\">())</span>\n\n<span class=\"c1\"># stft block forward</span>\n<span class=\"n\">ret</span> <span class=\"o\">=</span> <span class=\"n\">stft</span><span class=\"p\">(</span><span class=\"n\">nd_data</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">asnumpy</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">spec</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"n\">ret</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span> <span class=\"o\">**</span> <span class=\"mi\">2</span>\n<span class=\"n\">view_spec</span><span class=\"p\">(</span><span class=\"n\">spec</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># stft in librosa</span>\n<span class=\"n\">spec</span> <span class=\"o\">=</span> <span class=\"n\">rosa</span><span class=\"o\">.</span><span class=\"n\">stft</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">hop_length</span><span class=\"o\">=</span><span class=\"mi\">160</span><span class=\"p\">,</span> <span class=\"n\">win_length</span><span class=\"o\">=</span><span class=\"mi\">400</span><span class=\"p\">,</span> <span class=\"n\">window</span><span class=\"o\">=</span><span class=\"s2\">\"hamming\"</span><span class=\"p\">)</span>\n<span class=\"n\">spec</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">spec</span><span class=\"p\">)</span> <span class=\"o\">**</span> <span class=\"mi\">2</span>\n<span class=\"n\">view_spec</span><span class=\"p\">(</span><span class=\"n\">spec</span><span class=\"p\">)</span>\n</pre>\n<p>\u8f93\u51fa:</p>\n<table>\n<colgroup>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th>STFTBlock</th>\n<th>STFT in librosa</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>\u00a0</td>\n<td>\u00a0</td>\n</tr>\n</tbody>\n</table>\n<p>\u66f4\u591a\u7684\u4f8b\u5b50\u8bf7\u53c2\u8003<tt>examples/</tt>.</p>\n</div>\n<div id=\"requirements\">\n<h2>Requirements</h2>\n<p>mxnet-1.5.0+, gluonfr, av, librosa, \u2026</p>\n<p>\u97f3\u9891\u5e93\u7684\u9009\u62e9\u4e3b\u8981\u8003\u8651\u6570\u636e\u8bfb\u53d6\u901f\u5ea6,\n\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u97f3\u9891\u7684\u89e3\u7801\u76f8\u6bd4\u56fe\u50cf\u89e3\u7801\u4f1a\u6d88\u8017\u66f4\u591a\u65f6\u95f4,\n\u5b9e\u9645\u6d4b\u8bd5librosa\u4ece\u78c1\u76d8\u52a0\u8f7d\u4e00\u4e2aaac\u7f16\u7801\u7684\u77ed\u97f3\u9891 \u8017\u65f6\u662fpyav\u76848\u500d\u5de6\u53f3.</p>\n<ul>\n<li><p>librosa\n<tt>pip install librosa</tt></p>\n</li>\n<li><p>ffmpeg</p>\n<pre># \u4e0b\u8f7dffmpeg\u6e90\u7801, \u8fdb\u5165\u6839\u76ee\u5f55\n./configure --extra-cflags=-fPIC --enable-shared\nmake -j\nsudo make install\n</pre>\n</li>\n<li><p>pyav, \u9700\u8981\u5148\u5b89\u88c5ffmpeg\n<tt>pip install av</tt></p>\n</li>\n<li><div>\n<div>gluonfr</div>\n<div><tt>pip install <span class=\"pre\">git+https://github.com/THUFutureLab/gluon-face.git@master</span></tt></div>\n</div>\n</li>\n</ul>\n</div>\n<div id=\"datasets\">\n<h2>Datasets</h2>\n<div id=\"timit\">\n<h3>TIMIT</h3>\n<p>The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\nTraining and Test Data. Before using this dataset please follow the\ninstruction on <a href=\"https://catalog.ldc.upenn.edu/LDC93S1\" rel=\"nofollow\">link</a>.</p>\n<p>A copy of this was uploaded to <a href=\"https://goo.gl/l0sPwz\" rel=\"nofollow\">Google Drive</a>\nby @philipperemy <a href=\"https://github.com/philipperemy/timit\" rel=\"nofollow\">here</a>.</p>\n</div>\n<div id=\"voxceleb\">\n<h3>VoxCeleb</h3>\n<p>VoxCeleb is an audio-visual dataset consisting of short clips of human\nspeech, extracted from interview videos uploaded to YouTube.</p>\n<p>For more information, checkout this\n<a href=\"http://www.robots.ox.ac.uk/~vgg/data/voxceleb/\" rel=\"nofollow\">page</a>.</p>\n</div>\n</div>\n<div id=\"pretrained-models\">\n<h2>Pretrained Models</h2>\n<h2 id=\"speaker-recognition\"><span class=\"section-subtitle\">Speaker Recognition</span></h2>\n<div id=\"resnet18-training-with-voxceleb\">\n<h3>ResNet18 training with VoxCeleb</h3>\n<p>Download: <a href=\"https://pan.baidu.com/s/1Gkhi67oJSiSyAiYNTdPlTw\" rel=\"nofollow\">Baidu</a>,\n<a href=\"https://drive.google.com/open?id=1oEvSQrnNwYL4pRyQ8t87hRP3m22wuePz\" rel=\"nofollow\">Google\nDrive</a></p>\n<p>I followed the ideas in paper <strong>VoxCeleb2</strong>\n<a href=\"https://arxiv.org/abs/1806.05622\" rel=\"nofollow\">1806.05622</a> to train this model,\nthe differences between them:</p>\n<table>\n<colgroup>\n<col>\n<col>\n<col>\n</colgroup>\n<thead>\n<tr><th><ul>\n<li>\n</ul>\n</th>\n<th>Res18\nin\nthis\nrepo</th>\n<th>Res34\nin\npaper</th>\n</tr>\n</thead>\n<tbody>\n<tr><td>Train\ned\non</td>\n<td>VoxCel\neb2</td>\n<td>VoxCel\neb2</td>\n</tr>\n<tr><td>Input\nspec\nsize</td>\n<td>224x22\n4</td>\n<td>512x30\n0</td>\n</tr>\n<tr><td>Eval\non</td>\n<td>Random\n9500+\npair\nsample\ns\nfrom\nVoxCel\neb1\ntrain\nand\ntest\nset</td>\n<td>Origin\nal\nVoxCel\neb1\ntest\nset</td>\n</tr>\n<tr><td>Metri\nc</td>\n<td>Accura\ncy:0.9\n32656+\n-0.005\n187</td>\n<td>EER:\n0.0504</td>\n</tr>\n<tr><td>Frame\nwork</td>\n<td>Mxnet\nGluon</td>\n<td>Matcon\nvnet</td>\n</tr>\n<tr><td>ROC</td>\n<td>\u00a0</td>\n<td><ul>\n<li>\n</ul>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<div id=\"todo\">\n<h2>TODO</h2>\n<p>\u63a5\u4e0b\u6765\u4f1a\u6162\u6162\u8865\u5168\u4f7f\u7528mxnet gluon\u8bad\u7ec3\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7684\u5de5\u5177\u94fe, \u9884\u8ba1\u4f1a\u82b1\u8d85\u957f\u65f6\u95f4.</p>\n</div>\n<div id=\"docs\">\n<h2>Docs</h2>\n<p>GluonAR documentation is not available now.</p>\n</div>\n<div id=\"authors\">\n<h2>Authors</h2>\n<p>{ <a href=\"https://github.com/haoxintong\" rel=\"nofollow\">haoxintong</a> }</p>\n</div>\n<div id=\"discussion\">\n<h2>Discussion</h2>\n<p>Any suggestions, please open an issue.</p>\n</div>\n<div id=\"contributes\">\n<h2>Contributes</h2>\n<p>The final goal of this project is providing an easy using deep learning\nbased audio algorithm library like\n<a href=\"https://github.com/mravanelli/pytorch-kaldi\" rel=\"nofollow\">pytorch-kaldi</a>.</p>\n<p>Contribution is welcomed.</p>\n</div>\n<div id=\"references\">\n<h2>References</h2>\n<ol>\n<li>MXNet Documentation and Tutorials\n<a href=\"https://zh.diveintodeeplearning.org/\" rel=\"nofollow\">https://zh.diveintodeeplearning.org/</a></li>\n</ol>\n</div>\n\n          </div>"}, "last_serial": 5385310, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "d8dd0a44fa65e46068a6d97ae497713d", "sha256": "0e38622857699094e231ecc9a680d1e20175d98ec63cfb3e63b02158a812a194"}, "downloads": -1, "filename": "gluonar-0.1.0.tar.gz", "has_sig": false, "md5_digest": "d8dd0a44fa65e46068a6d97ae497713d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15000, "upload_time": "2019-06-11T08:15:00", "upload_time_iso_8601": "2019-06-11T08:15:00.773540Z", "url": "https://files.pythonhosted.org/packages/e2/a1/33e1a0221c11e85382979dfd6d227f768c97ac25facc72efafa08328b089/gluonar-0.1.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d8dd0a44fa65e46068a6d97ae497713d", "sha256": "0e38622857699094e231ecc9a680d1e20175d98ec63cfb3e63b02158a812a194"}, "downloads": -1, "filename": "gluonar-0.1.0.tar.gz", "has_sig": false, "md5_digest": "d8dd0a44fa65e46068a6d97ae497713d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 15000, "upload_time": "2019-06-11T08:15:00", "upload_time_iso_8601": "2019-06-11T08:15:00.773540Z", "url": "https://files.pythonhosted.org/packages/e2/a1/33e1a0221c11e85382979dfd6d227f768c97ac25facc72efafa08328b089/gluonar-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:56:27 2020"}