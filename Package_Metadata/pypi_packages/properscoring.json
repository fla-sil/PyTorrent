{"info": {"author": "The Climate Corporation", "author_email": "eng@climate.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Topic :: Scientific/Engineering"], "description": "properscoring\n=============\n\n.. image:: https://travis-ci.org/TheClimateCorporation/properscoring.svg?branch=master\n    :target: https://travis-ci.org/TheClimateCorporation/properscoring\n\n`Proper scoring rules`_ for evaluating probabilistic forecasts in Python.\nEvaluation methods that are \"strictly proper\" cannot be artificially improved\nthrough hedging, which makes them fair methods for accessing the accuracy of\nprobabilistic forecasts. In particular, these rules are often used for\nevaluating weather forecasts.\n\n.. _Proper scoring rules: https://www.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf\n\nproperscoring runs on both Python 2 and 3. It requires NumPy (1.8 or\nlater) and SciPy (any recent version should be fine). Numba is optional,\nbut highly encouraged: it enables significant speedups (e.g., 20x faster)\nfor ``crps_ensemble`` and ``threshold_brier_score``.\n\nTo install, use pip: ``pip install properscoring``.\n\nExample: five ways to calculate CRPS\n------------------------------------\n\nThis library focuses on the closely related\n`Continuous Ranked Probability Score`_ (CRPS) and `Brier Score`_. We like\nthese scores because they are both interpretable (e.g., CRPS is a\ngeneralization of mean absolute error) and easily calculated from a finite\nnumber of samples of a probability distribution.\n\n.. _Continuous Ranked Probability Score: http://www.eumetcal.org/resources/ukmeteocal/verification/www/english/msg/ver_prob_forec/uos3b/uos3b_ko1.htm\n.. _Brier score: https://en.wikipedia.org/wiki/Brier_score\n\nWe will illustrate how to calculate CRPS against a forecast given by a\nGaussian random variable. To begin, import properscoring::\n\n    import numpy as np\n    import properscoring as ps\n    from scipy.stats import norm\n\nExact calculation using ``crps_gaussian`` (this is the fastest method)::\n\n    >>>> ps.crps_gaussian(0, mu=0, sig=1)\n    0.23369497725510913\n\nNumerical integration with ``crps_quadrature``::\n\n    >>> ps.crps_quadrature(0, norm)\n    array(0.23369497725510724)\n\nFrom a finite sample with ``crps_ensemble``::\n\n    >>> ensemble = np.random.RandomState(0).randn(1000)\n    >>> ps.crps_ensemble(0, ensemble)\n    0.2297109370729622\n\nWeighted by PDF values with ``crps_ensemble``::\n\n    >>> x = np.linspace(-5, 5, num=1000)\n    >>> ps.crps_ensemble(0, x, weights=norm.pdf(x))\n    0.23370047937569616\n\nBased on the `threshold decomposition`_ of CRPS with\n``threshold_brier_score``::\n\n    >>> threshold_scores = ps.threshold_brier_score(0, ensemble, threshold=x)\n    >>> (x[1] - x[0]) * threshold_scores.sum(axis=-1)\n    0.22973090090090081\n\n.. _threshold decomposition: https://www.stat.washington.edu/research/reports/2008/tr533.pdf\n\nIn this example, we only scored a single observation/forecast pair. But\nto reliably evaluate a forecast model, you need to average these scores across\nmany observations. Fortunately, all scoring rules in properscoring happily\naccept and return observations as multi-dimensional arrays::\n\n    >>> ps.crps_gaussian([-2, -1, 0, 1, 2], mu=0, sig=1)\n    array([ 1.45279182,  0.60244136,  0.23369498,  0.60244136,  1.45279182])\n\nOnce you calculate an average score, is often useful to normalize them\nrelative to a baseline forecast to calculate a so-called \"skill score\",\ndefined such that 0 indicates no improvement over the baseline and 1\nindicates a perfect forecast. For example, suppose that our baseline\nforecast is to always predict 0::\n\n    >>> obs = [-2, -1, 0, 1, 2]\n    >>> baseline_score = ps.crps_ensemble(obs, [0, 0, 0, 0, 0]).mean()\n    >>> forecast_score = ps.crps_gaussian(obs, mu=0, sig=1).mean()\n    >>> skill = (baseline_score - forecast_score) / baseline_score\n    >>> skill\n    0.27597311068630859\n\nA standard normal distribution was 28% better at predicting these five\nobservations.\n\nAPI\n---\n\nproperscoring contains optimized and extensively tested routines for\nscoring probability forecasts. These functions currently fall into two\ncategories:\n\n* Continuous Ranked Probability Score (CRPS):\n\n  - for an ensemble forecast: ``crps_ensemble``\n  - for a Gaussian distribution: ``crps_gaussian``\n  - for an arbitrary cumulative distribution function: ``crps_quadrature``\n\n* Brier score:\n\n  - for binary probability forecasts: ``brier_score``\n  - for threshold exceedances with an ensemble forecast: ``threshold_brier_score``\n\nAll functions are robust to missing values represented by the floating\npoint value ``NaN``.\n\nHistory\n-------\n\nThis library was written by researchers at The Climate Corporation. The\noriginal authors include Leon Barrett, Stephan Hoyer, Alex Kleeman and\nDrew O'Kane.\n\nLicense\n-------\n\nCopyright 2015 The Climate Corporation\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nContributions\n-------------\n\nOutside contributions (bug fixes or new features related to proper scoring\nrules) would be very welcome! Please open a GitHub issue to discuss your\nplans.", "description_content_type": null, "docs_url": null, "download_url": "UNKNOWN", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/TheClimateCorporation/properscoring", "keywords": null, "license": "Apache", "maintainer": null, "maintainer_email": null, "name": "properscoring", "package_url": "https://pypi.org/project/properscoring/", "platform": "UNKNOWN", "project_url": "https://pypi.org/project/properscoring/", "project_urls": {"Download": "UNKNOWN", "Homepage": "https://github.com/TheClimateCorporation/properscoring"}, "release_url": "https://pypi.org/project/properscoring/0.1/", "requires_dist": null, "requires_python": null, "summary": "Proper scoring rules in Python", "version": "0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <a href=\"https://travis-ci.org/TheClimateCorporation/properscoring\" rel=\"nofollow\"><img alt=\"https://travis-ci.org/TheClimateCorporation/properscoring.svg?branch=master\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1772b565e81396ef40f4217c03be49057c2b7066/68747470733a2f2f7472617669732d63692e6f72672f546865436c696d617465436f72706f726174696f6e2f70726f70657273636f72696e672e7376673f6272616e63683d6d6173746572\"></a>\n<p><a href=\"https://www.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf\" rel=\"nofollow\">Proper scoring rules</a> for evaluating probabilistic forecasts in Python.\nEvaluation methods that are \u201cstrictly proper\u201d cannot be artificially improved\nthrough hedging, which makes them fair methods for accessing the accuracy of\nprobabilistic forecasts. In particular, these rules are often used for\nevaluating weather forecasts.</p>\n<p>properscoring runs on both Python 2 and 3. It requires NumPy (1.8 or\nlater) and SciPy (any recent version should be fine). Numba is optional,\nbut highly encouraged: it enables significant speedups (e.g., 20x faster)\nfor <tt>crps_ensemble</tt> and <tt>threshold_brier_score</tt>.</p>\n<p>To install, use pip: <tt>pip install properscoring</tt>.</p>\n<div id=\"example-five-ways-to-calculate-crps\">\n<h2>Example: five ways to calculate CRPS</h2>\n<p>This library focuses on the closely related\n<a href=\"http://www.eumetcal.org/resources/ukmeteocal/verification/www/english/msg/ver_prob_forec/uos3b/uos3b_ko1.htm\" rel=\"nofollow\">Continuous Ranked Probability Score</a> (CRPS) and <a href=\"https://en.wikipedia.org/wiki/Brier_score\" rel=\"nofollow\">Brier Score</a>. We like\nthese scores because they are both interpretable (e.g., CRPS is a\ngeneralization of mean absolute error) and easily calculated from a finite\nnumber of samples of a probability distribution.</p>\n<p>We will illustrate how to calculate CRPS against a forecast given by a\nGaussian random variable. To begin, import properscoring:</p>\n<pre>import numpy as np\nimport properscoring as ps\nfrom scipy.stats import norm\n</pre>\n<p>Exact calculation using <tt>crps_gaussian</tt> (this is the fastest method):</p>\n<pre>&gt;&gt;&gt;&gt; ps.crps_gaussian(0, mu=0, sig=1)\n0.23369497725510913\n</pre>\n<p>Numerical integration with <tt>crps_quadrature</tt>:</p>\n<pre>&gt;&gt;&gt; ps.crps_quadrature(0, norm)\narray(0.23369497725510724)\n</pre>\n<p>From a finite sample with <tt>crps_ensemble</tt>:</p>\n<pre>&gt;&gt;&gt; ensemble = np.random.RandomState(0).randn(1000)\n&gt;&gt;&gt; ps.crps_ensemble(0, ensemble)\n0.2297109370729622\n</pre>\n<p>Weighted by PDF values with <tt>crps_ensemble</tt>:</p>\n<pre>&gt;&gt;&gt; x = np.linspace(-5, 5, num=1000)\n&gt;&gt;&gt; ps.crps_ensemble(0, x, weights=norm.pdf(x))\n0.23370047937569616\n</pre>\n<p>Based on the <a href=\"https://www.stat.washington.edu/research/reports/2008/tr533.pdf\" rel=\"nofollow\">threshold decomposition</a> of CRPS with\n<tt>threshold_brier_score</tt>:</p>\n<pre>&gt;&gt;&gt; threshold_scores = ps.threshold_brier_score(0, ensemble, threshold=x)\n&gt;&gt;&gt; (x[1] - x[0]) * threshold_scores.sum(axis=-1)\n0.22973090090090081\n</pre>\n<p>In this example, we only scored a single observation/forecast pair. But\nto reliably evaluate a forecast model, you need to average these scores across\nmany observations. Fortunately, all scoring rules in properscoring happily\naccept and return observations as multi-dimensional arrays:</p>\n<pre>&gt;&gt;&gt; ps.crps_gaussian([-2, -1, 0, 1, 2], mu=0, sig=1)\narray([ 1.45279182,  0.60244136,  0.23369498,  0.60244136,  1.45279182])\n</pre>\n<p>Once you calculate an average score, is often useful to normalize them\nrelative to a baseline forecast to calculate a so-called \u201cskill score\u201d,\ndefined such that 0 indicates no improvement over the baseline and 1\nindicates a perfect forecast. For example, suppose that our baseline\nforecast is to always predict 0:</p>\n<pre>&gt;&gt;&gt; obs = [-2, -1, 0, 1, 2]\n&gt;&gt;&gt; baseline_score = ps.crps_ensemble(obs, [0, 0, 0, 0, 0]).mean()\n&gt;&gt;&gt; forecast_score = ps.crps_gaussian(obs, mu=0, sig=1).mean()\n&gt;&gt;&gt; skill = (baseline_score - forecast_score) / baseline_score\n&gt;&gt;&gt; skill\n0.27597311068630859\n</pre>\n<p>A standard normal distribution was 28% better at predicting these five\nobservations.</p>\n</div>\n<div id=\"api\">\n<h2>API</h2>\n<p>properscoring contains optimized and extensively tested routines for\nscoring probability forecasts. These functions currently fall into two\ncategories:</p>\n<ul>\n<li>Continuous Ranked Probability Score (CRPS):<ul>\n<li>for an ensemble forecast: <tt>crps_ensemble</tt></li>\n<li>for a Gaussian distribution: <tt>crps_gaussian</tt></li>\n<li>for an arbitrary cumulative distribution function: <tt>crps_quadrature</tt></li>\n</ul>\n</li>\n<li>Brier score:<ul>\n<li>for binary probability forecasts: <tt>brier_score</tt></li>\n<li>for threshold exceedances with an ensemble forecast: <tt>threshold_brier_score</tt></li>\n</ul>\n</li>\n</ul>\n<p>All functions are robust to missing values represented by the floating\npoint value <tt>NaN</tt>.</p>\n</div>\n<div id=\"history\">\n<h2>History</h2>\n<p>This library was written by researchers at The Climate Corporation. The\noriginal authors include Leon Barrett, Stephan Hoyer, Alex Kleeman and\nDrew O\u2019Kane.</p>\n</div>\n<div id=\"license\">\n<h2>License</h2>\n<p>Copyright 2015 The Climate Corporation</p>\n<p>Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d);\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<p><a href=\"http://www.apache.org/licenses/LICENSE-2.0\" rel=\"nofollow\">http://www.apache.org/licenses/LICENSE-2.0</a></p>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \u201cAS IS\u201d BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>\n</div>\n<div id=\"contributions\">\n<h2>Contributions</h2>\n<p>Outside contributions (bug fixes or new features related to proper scoring\nrules) would be very welcome! Please open a GitHub issue to discuss your\nplans.</p>\n</div>\n\n          </div>"}, "last_serial": 1813624, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "d497364acd7bd51c54f692c7edff4180", "sha256": "f84d5b06c13549d0171ce52ad7b45c6f5726ac44b733d24af5c60654cbb821dc"}, "downloads": -1, "filename": "properscoring-0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "d497364acd7bd51c54f692c7edff4180", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 23427, "upload_time": "2015-11-12T19:54:24", "upload_time_iso_8601": "2015-11-12T19:54:24.578877Z", "url": "https://files.pythonhosted.org/packages/0a/ff/51706ba1a029d0f2df0322543793d3bf1383de9dc567d23886144cb21bef/properscoring-0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f1fe6cc96c24713a28886c2c0b47afd6", "sha256": "b0cc4963cc218b728d6c5f77b3259c8f835ae00e32e82678cdf6936049b93961"}, "downloads": -1, "filename": "properscoring-0.1.tar.gz", "has_sig": false, "md5_digest": "f1fe6cc96c24713a28886c2c0b47afd6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17848, "upload_time": "2015-11-12T19:54:29", "upload_time_iso_8601": "2015-11-12T19:54:29.615748Z", "url": "https://files.pythonhosted.org/packages/38/ac/513d2c8653ab6bc66c4502372e6e4e20ce6a136cde4c1ba9908ec36e34c1/properscoring-0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "d497364acd7bd51c54f692c7edff4180", "sha256": "f84d5b06c13549d0171ce52ad7b45c6f5726ac44b733d24af5c60654cbb821dc"}, "downloads": -1, "filename": "properscoring-0.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "d497364acd7bd51c54f692c7edff4180", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 23427, "upload_time": "2015-11-12T19:54:24", "upload_time_iso_8601": "2015-11-12T19:54:24.578877Z", "url": "https://files.pythonhosted.org/packages/0a/ff/51706ba1a029d0f2df0322543793d3bf1383de9dc567d23886144cb21bef/properscoring-0.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f1fe6cc96c24713a28886c2c0b47afd6", "sha256": "b0cc4963cc218b728d6c5f77b3259c8f835ae00e32e82678cdf6936049b93961"}, "downloads": -1, "filename": "properscoring-0.1.tar.gz", "has_sig": false, "md5_digest": "f1fe6cc96c24713a28886c2c0b47afd6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 17848, "upload_time": "2015-11-12T19:54:29", "upload_time_iso_8601": "2015-11-12T19:54:29.615748Z", "url": "https://files.pythonhosted.org/packages/38/ac/513d2c8653ab6bc66c4502372e6e4e20ce6a136cde4c1ba9908ec36e34c1/properscoring-0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:16:58 2020"}