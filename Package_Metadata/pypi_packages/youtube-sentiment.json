{"info": {"author": "Dillon Mabry", "author_email": "rapid.dev.solutions@gmail.com", "bugtrack_url": null, "classifiers": [], "description": "# Youtube Sentiment Helper\n[![Build Status](https://travis-ci.org/dillonmabry/youtube-sentiment-helper.svg?branch=master)](https://travis-ci.org/dillonmabry/youtube-sentiment-helper)\n[![Python 3.4](https://img.shields.io/badge/python-3.4-blue.svg)](https://www.python.org/downloads/release/python-340/)\n[![Python 3.5](https://img.shields.io/badge/python-3.5-blue.svg)](https://www.python.org/downloads/release/python-350/)\n[![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nDetermine sentiment of Youtube video per comment based analysis using Sci-kit by analyzing video comments based on positive/negative sentiment. \nHelper tool to make requests to a machine learning model in order to determine sentiment using the Youtube API.\n\n## Install Instructions\n```\npip install .\n```\nor PyPI (https://pypi.org/project/youtube-sentiment/)\n```\npip install youtube-sentiment\n```\n## How to Use\nCurrent usage:\n```\nimport youtube_sentiment as yt\nyt.video_summary(<Youtube API Key>, <Youtube Video ID>, <Max Pages of Comments>, <Sentiment Model>) \n```\nor\n```\npython main.py <Youtube API Key> <Youtube Video ID> <Max Pages of Comments> <Sentiment Model>\n```\nChoices for model selection are found under the included models for setup also under project path `./models`\n## Tests\n```\npython setup.py test\n```\n## To-Do\n- [X] Create API to use Youtube API V3 via REST to get comments for videos\n- [X] Create initial Python package\n- [X] Analyze existing sentiment analysis models to select and use\n- [X] Improve/enhance existing sentiment learning model\n- [ ] Create deep model for sentiment\n- [X] Utilize sentiment analysis to analyze Youtube video and provide analytics\n- [X] Finalize Python package for project\n- [ ] Fix any new bugs\n- [ ] Create web based portal\n\n## Models Available\n - lr_sentiment_basic (Basic Vectorizer/Logistic Regression model, 2 MB)\n - lr_sentiment_cv (Hypertuned TFIDF/Logistic Regression model with clean dataset, 60 MB)\n - *To-be-added* cnn_sentiment (Convolutional Neural Net model)\n - *To-be-added* cnn_sentiment (LTSM Neural Net model)\n\n## Traditional ML Model Creation\n\n*Why use Twitter sentiment as training?*\n\nTwitter comments/replies/tweets are the closest existing training set to Youtube comments that are the simplest to setup. A deep autoencoder could be used to generate comments for a larger dataset (over 100k) with Youtube-esque comments but then the reliability of classifying the data would be very tricky.\n\n**TLDR: It is the simplest and most effective to bootstrap for a traditional model**\n\n```python\n# Develop sentiment analysis classifier using traditional ML models\n# Pipeline modeling using the following guide: \n# https://ryan-cranfill.github.io/sentiment-pipeline-sklearn-1/\n# Data processing and cleaning guide:\n# https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90\n\n# Imports\nimport numpy as np\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, log_loss, confusion_matrix, auc, roc_curve\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import train_test_split\n```\n\n\n```python\n# Dataset of 1.6m Twitter tweets\ncolumns = ['sentiment', 'id', 'date', 'query_string', 'user', 'text']\ntrain = pd.read_csv('stanford_twitter_train.csv', encoding='latin-1', header=None, names=columns)\ntest = pd.read_csv('stanford_twitter_test.csv', encoding='latin-1', header=None, names=columns)\n```\n\n\n```python\n## Local helpers\n\n# AUC visualization\ndef show_roc(model, test, test_labels):\n    # Predict\n    probs = model.predict_proba(test)\n    preds = probs[:,1]\n    fpr, tpr, threshold = roc_curve(test_labels, preds)\n    roc_auc = auc(fpr, tpr)\n    # Chart\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n\n# Tweet cleanser\ntok = nltk.tokenize.WordPunctTokenizer()\npat1 = r'@[A-Za-z0-9_]+'\npat2 = r'https?://[^ ]+'\ncombined_pat = r'|'.join((pat1, pat2))\nwww_pat = r'www.[^ ]+'\nnegations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n                \"mustn't\":\"must not\"}\nneg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\ndef clean_tweet(text):\n    soup = BeautifulSoup(text, 'lxml')\n    souped = soup.get_text()\n    try:\n        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n    except:\n        bom_removed = souped\n    stripped = re.sub(combined_pat, '', bom_removed)\n    stripped = re.sub(www_pat, '', stripped)\n    lower_case = stripped.lower()\n    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n    # During the letters_only process two lines above, it has created unnecessay white spaces,\n    # I will tokenize and join together to remove unneccessary white spaces\n    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n    return (\" \".join(words)).strip()\n```\n\n\n```python\n# Data cleaning\ncleaned_tweets = []\nfor tweet in train['text']:                                                                 \n    cleaned_tweets.append(clean_tweet(tweet))\ncleaned_df = pd.DataFrame(cleaned_tweets, columns=['text'])\ncleaned_df['target'] = train.sentiment\ncleaned_df.target[cleaned_df.target == 4] = 1 # rename 4 to 1 as positive label\ncleaned_df = cleaned_df[cleaned_df.target != 2] # remove neutral labels\ncleaned_df = cleaned_df.dropna() # drop null records\ncleaned_df.to_csv('stanford_clean_twitter_train.csv',encoding='utf-8')\n```\n\n```python\n# Starting point from import\ncsv = 'stanford_clean_twitter_train.csv'\ndf = pd.read_csv(csv,index_col=0)\n```\n\n```python\n# Random shuffle and ensure no null records\ndf = df.sample(frac=1).reset_index(drop=True)\ndf = df.dropna() # drop null records\n```\n\n\n```python\nX, y = df.text[0:200000], df.target[0:200000] # Max data size 200k for memory purposes\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.10)\n```\n\n\n```python\n# Dataset shapes post-split\nprint(np.shape(X_train))\nprint(np.shape(X_test))\nprint(np.unique(y_train))\n```\n\n    (180000,)\n    (20000,)\n    [0 1]\n    \n\n\n```python\n# NLTK Twitter tokenizer best used for short comment-type text sets\nimport nltk\ntokenizer = nltk.casual.TweetTokenizer(preserve_case=False)\n```\n\n\n```python\n# Hyperparameter tuning (Simple model)\n#cvect = CountVectorizer(tokenizer=tokenizer.tokenize)\ntfidf = TfidfVectorizer()\nclf = LogisticRegression()\n\npipeline = Pipeline([\n        ('tfidf', tfidf),\n        ('clf', clf)\n    ])\n\nparameters = {\n    'tfidf__ngram_range': [(1,1), (1,2), (1,3)], # ngram range of tokenizer\n    'tfidf__norm': ['l1', 'l2', None], # term vector normalization\n    'tfidf__max_df': [0.25, 0.5, 1.0], # maximum document frequency for the CountVectorizer\n    'clf__C': np.logspace(-2, 0, 3) # C value for the LogisticRegression\n}\n\ngrid = GridSearchCV(pipeline, parameters, cv=3, verbose=1)\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipeline.steps])\nt0 = time.time()\ngrid.fit(X_train, y_train)\nprint(\"done in %0.3fs\" % (time.time() - t0))\nprint()\n\nprint(\"Best score: %0.3f\" % grid.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n```\n\n    Performing grid search...\n    pipeline: ['tfidf', 'clf']\n    Fitting 3 folds for each of 81 candidates, totalling 243 fits\n    \n\n    [Parallel(n_jobs=1)]: Done 243 out of 243 | elapsed: 52.7min finished\n    \n\n    done in 3186.295s\n    \n    Best score: 0.803\n    Best parameters set:\n    \tclf__C: 0.01\n    \ttfidf__max_df: 0.25\n    \ttfidf__ngram_range: (1, 3)\n    \ttfidf__norm: None\n    \n\n\n```python\n# Dump model from grid search cv\njoblib.dump(grid.best_estimator_, 'lr_sentiment_cv.pkl', compress=1)\n```\n\n\n\n\n    ['lr_sentiment_cv.pkl']\n\n\n\n\n```python\n# Starting point 2: Post-model load comparison\nlra = joblib.load('./Models/Stanford_Twitter_Models/lr_sentiment_cv.pkl') \nlrb = joblib.load('./Models/Twitter_Simple_Models/lr_sentiment_basic.pkl') \n```\n\n\n```python\n# Model performance indicators for basic model\ny_pred_basic = lrb.predict(X_test)\nprint(confusion_matrix(y_test, y_pred_basic))\nshow_roc(lrb, X_test, y_test) # AUC\n```\n\n    [[7562 2347]\n     [2181 7910]]\n    \n\n\n![basic_auc](https://user-images.githubusercontent.com/10522556/47269973-06dd1280-d533-11e8-8686-284702733082.png)\n\n\n\n```python\n# Model performance indicators for hypertuned model\ny_pred_hyper = lra.predict(X_test)\nprint(confusion_matrix(y_test, y_pred_hyper))\nshow_roc(lra, X_test, y_test) # AUC\n```\n\n    [[7861 2048]\n     [1863 8228]]\n    \n\n\n![cv_auc](https://user-images.githubusercontent.com/10522556/47269972-06dd1280-d533-11e8-99d6-a2b211f73185.png)\n\n\n\n```python\nprint(lrb.predict([\"terrible idea why was this even made\"]))\nprint(lrb.predict([\"that was the best movie ever\"]))\n```\n\n    [0]\n    [1]", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/dillonmabry/youtube-sentiment-helper", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "youtube-sentiment", "package_url": "https://pypi.org/project/youtube-sentiment/", "platform": "", "project_url": "https://pypi.org/project/youtube-sentiment/", "project_urls": {"Homepage": "https://github.com/dillonmabry/youtube-sentiment-helper"}, "release_url": "https://pypi.org/project/youtube-sentiment/0.3.0/", "requires_dist": null, "requires_python": "", "summary": "Analyze Youtube videos for general sentiment analysis", "version": "0.3.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Youtube Sentiment Helper</h1>\n<p><a href=\"https://travis-ci.org/dillonmabry/youtube-sentiment-helper\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a2227f520541c0e36c9e9a6746985ce22254fa69/68747470733a2f2f7472617669732d63692e6f72672f64696c6c6f6e6d616272792f796f75747562652d73656e74696d656e742d68656c7065722e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://www.python.org/downloads/release/python-340/\" rel=\"nofollow\"><img alt=\"Python 3.4\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b638aa780ddc0d9025fba6afb33e83b1ce2bc647/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e342d626c75652e737667\"></a>\n<a href=\"https://www.python.org/downloads/release/python-350/\" rel=\"nofollow\"><img alt=\"Python 3.5\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8963091336cb0312d5fefe404229cb9ec0fb8b25/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e352d626c75652e737667\"></a>\n<a href=\"https://www.python.org/downloads/release/python-360/\" rel=\"nofollow\"><img alt=\"Python 3.6\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/01419d339114693587408dd14856677a4789006a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d626c75652e737667\"></a>\n<a href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"><img alt=\"License: MIT\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8645b002dd7ec1b54275a80574942e7a318e03c6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667\"></a></p>\n<p>Determine sentiment of Youtube video per comment based analysis using Sci-kit by analyzing video comments based on positive/negative sentiment.\nHelper tool to make requests to a machine learning model in order to determine sentiment using the Youtube API.</p>\n<h2>Install Instructions</h2>\n<pre><code>pip install .\n</code></pre>\n<p>or PyPI (<a href=\"https://pypi.org/project/youtube-sentiment/\" rel=\"nofollow\">https://pypi.org/project/youtube-sentiment/</a>)</p>\n<pre><code>pip install youtube-sentiment\n</code></pre>\n<h2>How to Use</h2>\n<p>Current usage:</p>\n<pre><code>import youtube_sentiment as yt\nyt.video_summary(&lt;Youtube API Key&gt;, &lt;Youtube Video ID&gt;, &lt;Max Pages of Comments&gt;, &lt;Sentiment Model&gt;) \n</code></pre>\n<p>or</p>\n<pre><code>python main.py &lt;Youtube API Key&gt; &lt;Youtube Video ID&gt; &lt;Max Pages of Comments&gt; &lt;Sentiment Model&gt;\n</code></pre>\n<p>Choices for model selection are found under the included models for setup also under project path <code>./models</code></p>\n<h2>Tests</h2>\n<pre><code>python setup.py test\n</code></pre>\n<h2>To-Do</h2>\n<ul>\n<li>[X] Create API to use Youtube API V3 via REST to get comments for videos</li>\n<li>[X] Create initial Python package</li>\n<li>[X] Analyze existing sentiment analysis models to select and use</li>\n<li>[X] Improve/enhance existing sentiment learning model</li>\n<li>[ ] Create deep model for sentiment</li>\n<li>[X] Utilize sentiment analysis to analyze Youtube video and provide analytics</li>\n<li>[X] Finalize Python package for project</li>\n<li>[ ] Fix any new bugs</li>\n<li>[ ] Create web based portal</li>\n</ul>\n<h2>Models Available</h2>\n<ul>\n<li>lr_sentiment_basic (Basic Vectorizer/Logistic Regression model, 2 MB)</li>\n<li>lr_sentiment_cv (Hypertuned TFIDF/Logistic Regression model with clean dataset, 60 MB)</li>\n<li><em>To-be-added</em> cnn_sentiment (Convolutional Neural Net model)</li>\n<li><em>To-be-added</em> cnn_sentiment (LTSM Neural Net model)</li>\n</ul>\n<h2>Traditional ML Model Creation</h2>\n<p><em>Why use Twitter sentiment as training?</em></p>\n<p>Twitter comments/replies/tweets are the closest existing training set to Youtube comments that are the simplest to setup. A deep autoencoder could be used to generate comments for a larger dataset (over 100k) with Youtube-esque comments but then the reliability of classifying the data would be very tricky.</p>\n<p><strong>TLDR: It is the simplest and most effective to bootstrap for a traditional model</strong></p>\n<pre><span class=\"c1\"># Develop sentiment analysis classifier using traditional ML models</span>\n<span class=\"c1\"># Pipeline modeling using the following guide: </span>\n<span class=\"c1\"># https://ryan-cranfill.github.io/sentiment-pipeline-sklearn-1/</span>\n<span class=\"c1\"># Data processing and cleaning guide:</span>\n<span class=\"c1\"># https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90</span>\n\n<span class=\"c1\"># Imports</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">re</span>\n<span class=\"kn\">from</span> <span class=\"nn\">bs4</span> <span class=\"kn\">import</span> <span class=\"n\">BeautifulSoup</span>\n<span class=\"kn\">import</span> <span class=\"nn\">nltk</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_extraction.text</span> <span class=\"kn\">import</span> <span class=\"n\">TfidfVectorizer</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.linear_model</span> <span class=\"kn\">import</span> <span class=\"n\">LogisticRegression</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.metrics</span> <span class=\"kn\">import</span> <span class=\"n\">accuracy_score</span><span class=\"p\">,</span> <span class=\"n\">log_loss</span><span class=\"p\">,</span> <span class=\"n\">confusion_matrix</span><span class=\"p\">,</span> <span class=\"n\">auc</span><span class=\"p\">,</span> <span class=\"n\">roc_curve</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">GridSearchCV</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">FeatureUnion</span><span class=\"p\">,</span> <span class=\"n\">Pipeline</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.externals</span> <span class=\"kn\">import</span> <span class=\"n\">joblib</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">train_test_split</span>\n</pre>\n<pre><span class=\"c1\"># Dataset of 1.6m Twitter tweets</span>\n<span class=\"n\">columns</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'sentiment'</span><span class=\"p\">,</span> <span class=\"s1\">'id'</span><span class=\"p\">,</span> <span class=\"s1\">'date'</span><span class=\"p\">,</span> <span class=\"s1\">'query_string'</span><span class=\"p\">,</span> <span class=\"s1\">'user'</span><span class=\"p\">,</span> <span class=\"s1\">'text'</span><span class=\"p\">]</span>\n<span class=\"n\">train</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s1\">'stanford_twitter_train.csv'</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s1\">'latin-1'</span><span class=\"p\">,</span> <span class=\"n\">header</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">names</span><span class=\"o\">=</span><span class=\"n\">columns</span><span class=\"p\">)</span>\n<span class=\"n\">test</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s1\">'stanford_twitter_test.csv'</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s1\">'latin-1'</span><span class=\"p\">,</span> <span class=\"n\">header</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">names</span><span class=\"o\">=</span><span class=\"n\">columns</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"c1\">## Local helpers</span>\n\n<span class=\"c1\"># AUC visualization</span>\n<span class=\"k\">def</span> <span class=\"nf\">show_roc</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">test</span><span class=\"p\">,</span> <span class=\"n\">test_labels</span><span class=\"p\">):</span>\n    <span class=\"c1\"># Predict</span>\n    <span class=\"n\">probs</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict_proba</span><span class=\"p\">(</span><span class=\"n\">test</span><span class=\"p\">)</span>\n    <span class=\"n\">preds</span> <span class=\"o\">=</span> <span class=\"n\">probs</span><span class=\"p\">[:,</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">fpr</span><span class=\"p\">,</span> <span class=\"n\">tpr</span><span class=\"p\">,</span> <span class=\"n\">threshold</span> <span class=\"o\">=</span> <span class=\"n\">roc_curve</span><span class=\"p\">(</span><span class=\"n\">test_labels</span><span class=\"p\">,</span> <span class=\"n\">preds</span><span class=\"p\">)</span>\n    <span class=\"n\">roc_auc</span> <span class=\"o\">=</span> <span class=\"n\">auc</span><span class=\"p\">(</span><span class=\"n\">fpr</span><span class=\"p\">,</span> <span class=\"n\">tpr</span><span class=\"p\">)</span>\n    <span class=\"c1\"># Chart</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s1\">'Receiver Operating Characteristic'</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">fpr</span><span class=\"p\">,</span> <span class=\"n\">tpr</span><span class=\"p\">,</span> <span class=\"s1\">'b'</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"s1\">'AUC = </span><span class=\"si\">%0.2f</span><span class=\"s1\">'</span> <span class=\"o\">%</span> <span class=\"n\">roc_auc</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">loc</span> <span class=\"o\">=</span> <span class=\"s1\">'lower right'</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"s1\">'r--'</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylim</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">'True Positive Rate'</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">'False Positive Rate'</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Tweet cleanser</span>\n<span class=\"n\">tok</span> <span class=\"o\">=</span> <span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"o\">.</span><span class=\"n\">WordPunctTokenizer</span><span class=\"p\">()</span>\n<span class=\"n\">pat1</span> <span class=\"o\">=</span> <span class=\"sa\">r</span><span class=\"s1\">'@[A-Za-z0-9_]+'</span>\n<span class=\"n\">pat2</span> <span class=\"o\">=</span> <span class=\"sa\">r</span><span class=\"s1\">'https?://[^ ]+'</span>\n<span class=\"n\">combined_pat</span> <span class=\"o\">=</span> <span class=\"sa\">r</span><span class=\"s1\">'|'</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">((</span><span class=\"n\">pat1</span><span class=\"p\">,</span> <span class=\"n\">pat2</span><span class=\"p\">))</span>\n<span class=\"n\">www_pat</span> <span class=\"o\">=</span> <span class=\"sa\">r</span><span class=\"s1\">'www.[^ ]+'</span>\n<span class=\"n\">negations_dic</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">\"isn't\"</span><span class=\"p\">:</span><span class=\"s2\">\"is not\"</span><span class=\"p\">,</span> <span class=\"s2\">\"aren't\"</span><span class=\"p\">:</span><span class=\"s2\">\"are not\"</span><span class=\"p\">,</span> <span class=\"s2\">\"wasn't\"</span><span class=\"p\">:</span><span class=\"s2\">\"was not\"</span><span class=\"p\">,</span> <span class=\"s2\">\"weren't\"</span><span class=\"p\">:</span><span class=\"s2\">\"were not\"</span><span class=\"p\">,</span>\n                <span class=\"s2\">\"haven't\"</span><span class=\"p\">:</span><span class=\"s2\">\"have not\"</span><span class=\"p\">,</span><span class=\"s2\">\"hasn't\"</span><span class=\"p\">:</span><span class=\"s2\">\"has not\"</span><span class=\"p\">,</span><span class=\"s2\">\"hadn't\"</span><span class=\"p\">:</span><span class=\"s2\">\"had not\"</span><span class=\"p\">,</span><span class=\"s2\">\"won't\"</span><span class=\"p\">:</span><span class=\"s2\">\"will not\"</span><span class=\"p\">,</span>\n                <span class=\"s2\">\"wouldn't\"</span><span class=\"p\">:</span><span class=\"s2\">\"would not\"</span><span class=\"p\">,</span> <span class=\"s2\">\"don't\"</span><span class=\"p\">:</span><span class=\"s2\">\"do not\"</span><span class=\"p\">,</span> <span class=\"s2\">\"doesn't\"</span><span class=\"p\">:</span><span class=\"s2\">\"does not\"</span><span class=\"p\">,</span><span class=\"s2\">\"didn't\"</span><span class=\"p\">:</span><span class=\"s2\">\"did not\"</span><span class=\"p\">,</span>\n                <span class=\"s2\">\"can't\"</span><span class=\"p\">:</span><span class=\"s2\">\"can not\"</span><span class=\"p\">,</span><span class=\"s2\">\"couldn't\"</span><span class=\"p\">:</span><span class=\"s2\">\"could not\"</span><span class=\"p\">,</span><span class=\"s2\">\"shouldn't\"</span><span class=\"p\">:</span><span class=\"s2\">\"should not\"</span><span class=\"p\">,</span><span class=\"s2\">\"mightn't\"</span><span class=\"p\">:</span><span class=\"s2\">\"might not\"</span><span class=\"p\">,</span>\n                <span class=\"s2\">\"mustn't\"</span><span class=\"p\">:</span><span class=\"s2\">\"must not\"</span><span class=\"p\">}</span>\n<span class=\"n\">neg_pattern</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s1\">'\\b('</span> <span class=\"o\">+</span> <span class=\"s1\">'|'</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">negations_dic</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">())</span> <span class=\"o\">+</span> <span class=\"sa\">r</span><span class=\"s1\">')\\b'</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">clean_tweet</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">):</span>\n    <span class=\"n\">soup</span> <span class=\"o\">=</span> <span class=\"n\">BeautifulSoup</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"s1\">'lxml'</span><span class=\"p\">)</span>\n    <span class=\"n\">souped</span> <span class=\"o\">=</span> <span class=\"n\">soup</span><span class=\"o\">.</span><span class=\"n\">get_text</span><span class=\"p\">()</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">bom_removed</span> <span class=\"o\">=</span> <span class=\"n\">souped</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s2\">\"utf-8-sig\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"sa\">u</span><span class=\"s2\">\"</span><span class=\"se\">\\ufffd</span><span class=\"s2\">\"</span><span class=\"p\">,</span> <span class=\"s2\">\"?\"</span><span class=\"p\">)</span>\n    <span class=\"k\">except</span><span class=\"p\">:</span>\n        <span class=\"n\">bom_removed</span> <span class=\"o\">=</span> <span class=\"n\">souped</span>\n    <span class=\"n\">stripped</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">sub</span><span class=\"p\">(</span><span class=\"n\">combined_pat</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">,</span> <span class=\"n\">bom_removed</span><span class=\"p\">)</span>\n    <span class=\"n\">stripped</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">sub</span><span class=\"p\">(</span><span class=\"n\">www_pat</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">,</span> <span class=\"n\">stripped</span><span class=\"p\">)</span>\n    <span class=\"n\">lower_case</span> <span class=\"o\">=</span> <span class=\"n\">stripped</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span>\n    <span class=\"n\">neg_handled</span> <span class=\"o\">=</span> <span class=\"n\">neg_pattern</span><span class=\"o\">.</span><span class=\"n\">sub</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">negations_dic</span><span class=\"p\">[</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">group</span><span class=\"p\">()],</span> <span class=\"n\">lower_case</span><span class=\"p\">)</span>\n    <span class=\"n\">letters_only</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">sub</span><span class=\"p\">(</span><span class=\"s2\">\"[^a-zA-Z]\"</span><span class=\"p\">,</span> <span class=\"s2\">\" \"</span><span class=\"p\">,</span> <span class=\"n\">neg_handled</span><span class=\"p\">)</span>\n    <span class=\"c1\"># During the letters_only process two lines above, it has created unnecessay white spaces,</span>\n    <span class=\"c1\"># I will tokenize and join together to remove unneccessary white spaces</span>\n    <span class=\"n\">words</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">x</span> <span class=\"k\">for</span> <span class=\"n\">x</span>  <span class=\"ow\">in</span> <span class=\"n\">tok</span><span class=\"o\">.</span><span class=\"n\">tokenize</span><span class=\"p\">(</span><span class=\"n\">letters_only</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"s2\">\" \"</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">words</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span>\n</pre>\n<pre><span class=\"c1\"># Data cleaning</span>\n<span class=\"n\">cleaned_tweets</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">tweet</span> <span class=\"ow\">in</span> <span class=\"n\">train</span><span class=\"p\">[</span><span class=\"s1\">'text'</span><span class=\"p\">]:</span>                                                                 \n    <span class=\"n\">cleaned_tweets</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">clean_tweet</span><span class=\"p\">(</span><span class=\"n\">tweet</span><span class=\"p\">))</span>\n<span class=\"n\">cleaned_df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">cleaned_tweets</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'text'</span><span class=\"p\">])</span>\n<span class=\"n\">cleaned_df</span><span class=\"p\">[</span><span class=\"s1\">'target'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">sentiment</span>\n<span class=\"n\">cleaned_df</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">[</span><span class=\"n\">cleaned_df</span><span class=\"o\">.</span><span class=\"n\">target</span> <span class=\"o\">==</span> <span class=\"mi\">4</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"c1\"># rename 4 to 1 as positive label</span>\n<span class=\"n\">cleaned_df</span> <span class=\"o\">=</span> <span class=\"n\">cleaned_df</span><span class=\"p\">[</span><span class=\"n\">cleaned_df</span><span class=\"o\">.</span><span class=\"n\">target</span> <span class=\"o\">!=</span> <span class=\"mi\">2</span><span class=\"p\">]</span> <span class=\"c1\"># remove neutral labels</span>\n<span class=\"n\">cleaned_df</span> <span class=\"o\">=</span> <span class=\"n\">cleaned_df</span><span class=\"o\">.</span><span class=\"n\">dropna</span><span class=\"p\">()</span> <span class=\"c1\"># drop null records</span>\n<span class=\"n\">cleaned_df</span><span class=\"o\">.</span><span class=\"n\">to_csv</span><span class=\"p\">(</span><span class=\"s1\">'stanford_clean_twitter_train.csv'</span><span class=\"p\">,</span><span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s1\">'utf-8'</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"c1\"># Starting point from import</span>\n<span class=\"n\">csv</span> <span class=\"o\">=</span> <span class=\"s1\">'stanford_clean_twitter_train.csv'</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"n\">csv</span><span class=\"p\">,</span><span class=\"n\">index_col</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"c1\"># Random shuffle and ensure no null records</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"n\">frac</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reset_index</span><span class=\"p\">(</span><span class=\"n\">drop</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">dropna</span><span class=\"p\">()</span> <span class=\"c1\"># drop null records</span>\n</pre>\n<pre><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"mi\">200000</span><span class=\"p\">],</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"mi\">200000</span><span class=\"p\">]</span> <span class=\"c1\"># Max data size 200k for memory purposes</span>\n\n<span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">train_test_split</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">,</span> <span class=\"n\">test_size</span><span class=\"o\">=</span><span class=\"mf\">0.10</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"c1\"># Dataset shapes post-split</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">unique</span><span class=\"p\">(</span><span class=\"n\">y_train</span><span class=\"p\">))</span>\n</pre>\n<pre><code>(180000,)\n(20000,)\n[0 1]\n</code></pre>\n<pre><span class=\"c1\"># NLTK Twitter tokenizer best used for short comment-type text sets</span>\n<span class=\"kn\">import</span> <span class=\"nn\">nltk</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">casual</span><span class=\"o\">.</span><span class=\"n\">TweetTokenizer</span><span class=\"p\">(</span><span class=\"n\">preserve_case</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n</pre>\n<pre><span class=\"c1\"># Hyperparameter tuning (Simple model)</span>\n<span class=\"c1\">#cvect = CountVectorizer(tokenizer=tokenizer.tokenize)</span>\n<span class=\"n\">tfidf</span> <span class=\"o\">=</span> <span class=\"n\">TfidfVectorizer</span><span class=\"p\">()</span>\n<span class=\"n\">clf</span> <span class=\"o\">=</span> <span class=\"n\">LogisticRegression</span><span class=\"p\">()</span>\n\n<span class=\"n\">pipeline</span> <span class=\"o\">=</span> <span class=\"n\">Pipeline</span><span class=\"p\">([</span>\n        <span class=\"p\">(</span><span class=\"s1\">'tfidf'</span><span class=\"p\">,</span> <span class=\"n\">tfidf</span><span class=\"p\">),</span>\n        <span class=\"p\">(</span><span class=\"s1\">'clf'</span><span class=\"p\">,</span> <span class=\"n\">clf</span><span class=\"p\">)</span>\n    <span class=\"p\">])</span>\n\n<span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s1\">'tfidf__ngram_range'</span><span class=\"p\">:</span> <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">)],</span> <span class=\"c1\"># ngram range of tokenizer</span>\n    <span class=\"s1\">'tfidf__norm'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">'l1'</span><span class=\"p\">,</span> <span class=\"s1\">'l2'</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">],</span> <span class=\"c1\"># term vector normalization</span>\n    <span class=\"s1\">'tfidf__max_df'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.25</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">],</span> <span class=\"c1\"># maximum document frequency for the CountVectorizer</span>\n    <span class=\"s1\">'clf__C'</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">logspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span> <span class=\"c1\"># C value for the LogisticRegression</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"n\">GridSearchCV</span><span class=\"p\">(</span><span class=\"n\">pipeline</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">cv</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Performing grid search...\"</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"pipeline:\"</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"n\">name</span> <span class=\"k\">for</span> <span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"n\">pipeline</span><span class=\"o\">.</span><span class=\"n\">steps</span><span class=\"p\">])</span>\n<span class=\"n\">t0</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n<span class=\"n\">grid</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"done in </span><span class=\"si\">%0.3f</span><span class=\"s2\">s\"</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">t0</span><span class=\"p\">))</span>\n<span class=\"nb\">print</span><span class=\"p\">()</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Best score: </span><span class=\"si\">%0.3f</span><span class=\"s2\">\"</span> <span class=\"o\">%</span> <span class=\"n\">grid</span><span class=\"o\">.</span><span class=\"n\">best_score_</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Best parameters set:\"</span><span class=\"p\">)</span>\n<span class=\"n\">best_parameters</span> <span class=\"o\">=</span> <span class=\"n\">grid</span><span class=\"o\">.</span><span class=\"n\">best_estimator_</span><span class=\"o\">.</span><span class=\"n\">get_params</span><span class=\"p\">()</span>\n<span class=\"k\">for</span> <span class=\"n\">param_name</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">()):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"</span><span class=\"se\">\\t</span><span class=\"si\">%s</span><span class=\"s2\">: </span><span class=\"si\">%r</span><span class=\"s2\">\"</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">param_name</span><span class=\"p\">,</span> <span class=\"n\">best_parameters</span><span class=\"p\">[</span><span class=\"n\">param_name</span><span class=\"p\">]))</span>\n</pre>\n<pre><code>Performing grid search...\npipeline: ['tfidf', 'clf']\nFitting 3 folds for each of 81 candidates, totalling 243 fits\n\n\n[Parallel(n_jobs=1)]: Done 243 out of 243 | elapsed: 52.7min finished\n\n\ndone in 3186.295s\n\nBest score: 0.803\nBest parameters set:\n\tclf__C: 0.01\n\ttfidf__max_df: 0.25\n\ttfidf__ngram_range: (1, 3)\n\ttfidf__norm: None\n</code></pre>\n<pre><span class=\"c1\"># Dump model from grid search cv</span>\n<span class=\"n\">joblib</span><span class=\"o\">.</span><span class=\"n\">dump</span><span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"o\">.</span><span class=\"n\">best_estimator_</span><span class=\"p\">,</span> <span class=\"s1\">'lr_sentiment_cv.pkl'</span><span class=\"p\">,</span> <span class=\"n\">compress</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n</pre>\n<pre><code>['lr_sentiment_cv.pkl']\n</code></pre>\n<pre><span class=\"c1\"># Starting point 2: Post-model load comparison</span>\n<span class=\"n\">lra</span> <span class=\"o\">=</span> <span class=\"n\">joblib</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">'./Models/Stanford_Twitter_Models/lr_sentiment_cv.pkl'</span><span class=\"p\">)</span> \n<span class=\"n\">lrb</span> <span class=\"o\">=</span> <span class=\"n\">joblib</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">'./Models/Twitter_Simple_Models/lr_sentiment_basic.pkl'</span><span class=\"p\">)</span> \n</pre>\n<pre><span class=\"c1\"># Model performance indicators for basic model</span>\n<span class=\"n\">y_pred_basic</span> <span class=\"o\">=</span> <span class=\"n\">lrb</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">confusion_matrix</span><span class=\"p\">(</span><span class=\"n\">y_test</span><span class=\"p\">,</span> <span class=\"n\">y_pred_basic</span><span class=\"p\">))</span>\n<span class=\"n\">show_roc</span><span class=\"p\">(</span><span class=\"n\">lrb</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">)</span> <span class=\"c1\"># AUC</span>\n</pre>\n<pre><code>[[7562 2347]\n [2181 7910]]\n</code></pre>\n<p><img alt=\"basic_auc\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/95d0deb49c7d27945ec36ccd866f6e944301c9d5/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f31303532323535362f34373236393937332d30366464313238302d643533332d313165382d383638362d3238343730323733333038322e706e67\"></p>\n<pre><span class=\"c1\"># Model performance indicators for hypertuned model</span>\n<span class=\"n\">y_pred_hyper</span> <span class=\"o\">=</span> <span class=\"n\">lra</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">confusion_matrix</span><span class=\"p\">(</span><span class=\"n\">y_test</span><span class=\"p\">,</span> <span class=\"n\">y_pred_hyper</span><span class=\"p\">))</span>\n<span class=\"n\">show_roc</span><span class=\"p\">(</span><span class=\"n\">lra</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">)</span> <span class=\"c1\"># AUC</span>\n</pre>\n<pre><code>[[7861 2048]\n [1863 8228]]\n</code></pre>\n<p><img alt=\"cv_auc\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e161a4b6fff1978bcfb08bbe3977eece54d8a29c/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f31303532323535362f34373236393937322d30366464313238302d643533332d313165382d393964362d6132623231316637333138352e706e67\"></p>\n<pre><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">lrb</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([</span><span class=\"s2\">\"terrible idea why was this even made\"</span><span class=\"p\">]))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">lrb</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([</span><span class=\"s2\">\"that was the best movie ever\"</span><span class=\"p\">]))</span>\n</pre>\n<pre><code>[0]\n[1]\n</code></pre>\n\n          </div>"}, "last_serial": 4459745, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "ec700534dd80f37734bd509c61edc2f2", "sha256": "b2daa5dee42d20da305fe2e75d10144ded63c447f6c97b71ef2f3ef95612de84"}, "downloads": -1, "filename": "youtube_sentiment-0.1.tar.gz", "has_sig": false, "md5_digest": "ec700534dd80f37734bd509c61edc2f2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 58791331, "upload_time": "2018-10-30T02:49:16", "upload_time_iso_8601": "2018-10-30T02:49:16.261950Z", "url": "https://files.pythonhosted.org/packages/98/86/1d7e316e10f7eff63645080420c54251aa53dd3151f129f00f286aaa724f/youtube_sentiment-0.1.tar.gz", "yanked": false}], "0.2.1": [{"comment_text": "", "digests": {"md5": "82689fa8e081b37a15677f30fca9c37e", "sha256": "37650c118cc808945e90445620c6dc28bbf491f748d0615aa8be3772de1a843f"}, "downloads": -1, "filename": "youtube_sentiment-0.2.1.tar.gz", "has_sig": false, "md5_digest": "82689fa8e081b37a15677f30fca9c37e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 58796955, "upload_time": "2018-10-30T23:42:58", "upload_time_iso_8601": "2018-10-30T23:42:58.848893Z", "url": "https://files.pythonhosted.org/packages/0b/b5/e51426b729729ff7a9b131244dd1b7e086410f7877ad66a9364f2036facf/youtube_sentiment-0.2.1.tar.gz", "yanked": false}], "0.3.0": [{"comment_text": "", "digests": {"md5": "356986d8de0f3fb95666fecfaffa1536", "sha256": "428475343dcfb115ba9d1417bb1955f37fd5f3ca6c22cfd06eec25fe2f298ba0"}, "downloads": -1, "filename": "youtube_sentiment-0.3.0.tar.gz", "has_sig": false, "md5_digest": "356986d8de0f3fb95666fecfaffa1536", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 58797200, "upload_time": "2018-11-07T02:19:26", "upload_time_iso_8601": "2018-11-07T02:19:26.128393Z", "url": "https://files.pythonhosted.org/packages/bf/01/52bfd759ae90a800ad538f6f8b0d43da4b1d8d6dc70de04fabbcfe69e68d/youtube_sentiment-0.3.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "356986d8de0f3fb95666fecfaffa1536", "sha256": "428475343dcfb115ba9d1417bb1955f37fd5f3ca6c22cfd06eec25fe2f298ba0"}, "downloads": -1, "filename": "youtube_sentiment-0.3.0.tar.gz", "has_sig": false, "md5_digest": "356986d8de0f3fb95666fecfaffa1536", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 58797200, "upload_time": "2018-11-07T02:19:26", "upload_time_iso_8601": "2018-11-07T02:19:26.128393Z", "url": "https://files.pythonhosted.org/packages/bf/01/52bfd759ae90a800ad538f6f8b0d43da4b1d8d6dc70de04fabbcfe69e68d/youtube_sentiment-0.3.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:21:41 2020"}