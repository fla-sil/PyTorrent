{"info": {"author": "Hunter McGushion", "author_email": "hunter@mcgushion.com", "bugtrack_url": null, "classifiers": ["Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Information Technology", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Topic :: Desktop Environment :: File Managers", "Topic :: Education", "Topic :: Scientific/Engineering", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Scientific/Engineering :: Human Machine Interfaces", "Topic :: Scientific/Engineering :: Information Analysis", "Topic :: Scientific/Engineering :: Mathematics", "Topic :: Software Development", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Utilities"], "description": "HyperparameterHunter\n====================\n\n![HyperparameterHunter Overview](docs/media/overview.gif)\n\n[![Build Status](https://travis-ci.org/HunterMcGushion/hyperparameter_hunter.svg?branch=master)](https://travis-ci.org/HunterMcGushion/hyperparameter_hunter)\n[![Documentation Status](https://readthedocs.org/projects/hyperparameter-hunter/badge/?version=stable)](https://hyperparameter-hunter.readthedocs.io/en/stable/?badge=stable)\n[![Coverage Status](https://coveralls.io/repos/github/HunterMcGushion/hyperparameter_hunter/badge.svg?branch=master&service=github)](https://coveralls.io/github/HunterMcGushion/hyperparameter_hunter?branch=master&service=github)\n[![codecov](https://codecov.io/gh/HunterMcGushion/hyperparameter_hunter/branch/master/graph/badge.svg)](https://codecov.io/gh/HunterMcGushion/hyperparameter_hunter)\n[![Maintainability](https://api.codeclimate.com/v1/badges/ef0d004a10ede0b228cc/maintainability)](https://codeclimate.com/github/HunterMcGushion/hyperparameter_hunter/maintainability)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/1413b76fabe2400fab1958e70be593a2)](https://www.codacy.com/app/HunterMcGushion/hyperparameter_hunter?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=HunterMcGushion/hyperparameter_hunter&amp;utm_campaign=Badge_Grade)\n\n[![PyPI version](https://badge.fury.io/py/hyperparameter-hunter.svg)](https://badge.fury.io/py/hyperparameter-hunter)\n[![Downloads](https://pepy.tech/badge/hyperparameter-hunter)](https://pepy.tech/project/hyperparameter-hunter)\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=Q3EX3PQUV256G)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)\n\nAutomatically save and learn from Experiment results, leading to long-term, persistent optimization that remembers all your tests.\n\nHyperparameterHunter provides a wrapper for machine learning algorithms that saves all the important data. Simplify the experimentation and hyperparameter tuning process by letting HyperparameterHunter do the hard work\nof recording, organizing, and learning from your tests \u2014 all while using the same libraries you already do. Don't let any of your experiments go to waste, and start doing hyperparameter optimization the way it was meant to be.\n\n* **Installation:** `pip install hyperparameter-hunter`\n* **Source:** https://github.com/HunterMcGushion/hyperparameter_hunter\n* **Documentation:** [https://hyperparameter-hunter.readthedocs.io](https://hyperparameter-hunter.readthedocs.io/en/latest/index.html)\n\nFeatures\n--------\n* Automatically record Experiment results\n* Truly informed hyperparameter optimization that automatically uses past Experiments\n* Eliminate boilerplate code for cross-validation loops, predicting, and scoring\n* Stop worrying about keeping track of hyperparameters, scores, or re-running the same Experiments\n* Use the libraries and utilities you already love\n\nHow to Use HyperparameterHunter\n-------------------------------\nDon\u2019t think of HyperparameterHunter as another optimization library that you bring out only when its time to do hyperparameter optimization. Of course, it does optimization, but its better to view HyperparameterHunter as your own personal machine learning toolbox/assistant.\n\nThe idea is to start using HyperparameterHunter immediately. Run all of your benchmark/one-off experiments through it. \n\nThe more you use HyperparameterHunter, the better your results will be. If you just use it for optimization, sure, it\u2019ll do what you want, but that\u2019s missing the point of HyperparameterHunter.\n\nIf you\u2019ve been using it for experimentation and optimization along the entire course of your project, then when you decide to do hyperparameter optimization, HyperparameterHunter is already aware of all that you\u2019ve done, and that\u2019s when HyperparameterHunter does something remarkable. It doesn\u2019t start optimization from scratch like other libraries. It starts from all of the Experiments and previous optimization rounds you\u2019ve already run through it.\n\nGetting Started\n---------------\n\n### 1) Environment:\n\nSet up an Environment to organize Experiments and Optimization results.\n<br>\nAny Experiments or Optimization rounds we perform will use our active Environment.\n\n```python\nfrom hyperparameter_hunter import Environment, CVExperiment\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import StratifiedKFold\n\ndata = load_breast_cancer()\ndf = pd.DataFrame(data=data.data, columns=data.feature_names)\ndf['target'] = data.target\n\nenv = Environment(\n    train_dataset=df,  # Add holdout/test dataframes, too\n    results_path='path/to/results/directory',  # Where your result files will go\n    metrics=['roc_auc_score'],  # Callables, or strings referring to `sklearn.metrics`\n    cv_type=StratifiedKFold,  # Class, or string in `sklearn.model_selection`\n    cv_params=dict(n_splits=5, shuffle=True, random_state=32)\n)\n```\n\n### 2) Individual Experimentation:\n\nPerform Experiments with your favorite libraries simply by providing model initializers and hyperparameters\n<!-- Keras -->\n\n<details>\n<summary>Keras</summary>\n\n```python\n# Same format used by `keras.wrappers.scikit_learn`. Nothing new to learn\ndef build_fn(input_shape):  # `input_shape` calculated for you\n    model = Sequential([\n        Dense(100, kernel_initializer='uniform', input_shape=input_shape, activation='relu'),\n        Dropout(0.5),\n        Dense(1, kernel_initializer='uniform', activation='sigmoid')\n    ])  # All layer arguments saved (whether explicit or Keras default) for future use\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\nexperiment = CVExperiment(\n    model_initializer=KerasClassifier,\n    model_init_params=build_fn,  # We interpret your build_fn to save hyperparameters in a useful, readable format\n    model_extra_params=dict(\n        callbacks=[ReduceLROnPlateau(patience=5)],  # Use Keras callbacks\n        batch_size=32, epochs=10, verbose=0  # Fit/predict arguments\n    )\n)\n```\n\n</details>\n\n<!-- SKLearn -->\n<details>\n<summary>SKLearn</summary>\n\n```python\nexperiment = CVExperiment(\n    model_initializer=LinearSVC,  # (Or any of the dozens of other SK-Learn algorithms)\n    model_init_params=dict(penalty='l1', C=0.9)  # Default values used and recorded for kwargs not given\n)\n```\n</details>\n<!-- XGBoost -->\n<details open>\n<summary>XGBoost</summary>\n\n```python\nexperiment = CVExperiment(\n    model_initializer=XGBClassifier,\n    model_init_params=dict(objective='reg:linear', max_depth=3, n_estimators=100, subsample=0.5)\n)\n```\n</details>\n<!-- LightGBM -->\n<details>\n<summary>LightGBM</summary>\n\n```python\nexperiment = CVExperiment(\n    model_initializer=LGBMClassifier,\n    model_init_params=dict(boosting_type='gbdt', num_leaves=31, max_depth=-1, min_child_samples=5, subsample=0.5)\n)\n```\n</details>\n<!-- CatBoost -->\n<details>\n<summary>CatBoost</summary>\n\n```python\nexperiment = CVExperiment(\n    model_initializer=CatboostClassifier,\n    model_init_params=dict(iterations=500, learning_rate=0.01, depth=7, allow_writing_files=False),\n    model_extra_params=dict(fit=dict(verbose=True))  # Send kwargs to `fit` and other extra methods\n)\n```\n</details>\n<!-- RGF -->\n<details>\n<summary>RGF</summary>\n\n```python\nexperiment = CVExperiment(\n    model_initializer=RGFClassifier,\n    model_init_params=dict(max_leaf=1000, algorithm='RGF', min_samples_leaf=10)\n)\n```\n</details>\n\n### 3) Hyperparameter Optimization:\n\nJust like Experiments, but if you want to optimize a hyperparameter, use the classes imported below\n\n```python\nfrom hyperparameter_hunter import Real, Integer, Categorical\nfrom hyperparameter_hunter import optimization as opt\n```\n\n<!-- Keras -->\n<details>\n<summary>Keras</summary>\n\n```python\ndef build_fn(input_shape):\n    model = Sequential([\n        Dense(Integer(50, 150), input_shape=input_shape, activation='relu'),\n        Dropout(Real(0.2, 0.7)),\n        Dense(1, activation=Categorical(['sigmoid', 'softmax']))\n    ])\n    model.compile(\n        optimizer=Categorical(['adam', 'rmsprop', 'sgd', 'adadelta']),\n        loss='binary_crossentropy', metrics=['accuracy']\n    )\n    return model\n\noptimizer = opt.RandomForestOptPro(iterations=7)\noptimizer.forge_experiment(\n    model_initializer=KerasClassifier,\n    model_init_params=build_fn,\n    model_extra_params=dict(\n        callbacks=[ReduceLROnPlateau(patience=Integer(5, 10))],\n        batch_size=Categorical([32, 64]),\n        epochs=10, verbose=0\n    )\n)\noptimizer.go()\n```\n</details>\n\n<!-- SKLearn -->\n<details>\n<summary>SKLearn</summary>\n\n```python\noptimizer = opt.DummyOptPro(iterations=42)\noptimizer.forge_experiment(\n    model_initializer=AdaBoostClassifier,  # (Or any of the dozens of other SKLearn algorithms)\n    model_init_params=dict(\n        n_estimators=Integer(75, 150),\n        learning_rate=Real(0.8, 1.3),\n        algorithm='SAMME.R'\n    )\n)\noptimizer.go()\n```\n</details>\n<!-- XGBoost -->\n<details open>\n<summary>XGBoost</summary>\n\n```python\noptimizer = opt.BayesianOptPro(iterations=10)\noptimizer.forge_experiment(\n    model_initializer=XGBClassifier,\n    model_init_params=dict(\n        max_depth=Integer(low=2, high=20),\n        learning_rate=Real(0.0001, 0.5),\n        n_estimators=200,\n        subsample=0.5,\n        booster=Categorical(['gbtree', 'gblinear', 'dart']),\n    )\n)\noptimizer.go()\n```\n</details>\n<!-- LightGBM -->\n<details>\n<summary>LightGBM</summary>\n\n```python\noptimizer = opt.BayesianOptPro(iterations=100)\noptimizer.forge_experiment(\n    model_initializer=LGBMClassifier,\n    model_init_params=dict(\n        boosting_type=Categorical(['gbdt', 'dart']),\n        num_leaves=Integer(5, 20),\n        max_depth=-1,\n        min_child_samples=5,\n        subsample=0.5\n    )\n)\noptimizer.go()\n```\n</details>\n<!-- CatBoost -->\n<details>\n<summary>CatBoost</summary>\n\n```python\noptimizer = opt.GradientBoostedRegressionTreeOptPro(iterations=32)\noptimizer.forge_experiment(\n    model_initializer=CatBoostClassifier,\n    model_init_params=dict(\n        iterations=100,\n        eval_metric=Categorical(['Logloss', 'Accuracy', 'AUC']),\n        learning_rate=Real(low=0.0001, high=0.5),\n        depth=Integer(4, 7),\n        allow_writing_files=False\n    )\n)\noptimizer.go()\n```\n</details>\n<!-- RGF -->\n<details>\n<summary>RGF</summary>\n\n```python\noptimizer = opt.ExtraTreesOptPro(iterations=10)\noptimizer.forge_experiment(\n    model_initializer=RGFClassifier,\n    model_init_params=dict(\n        max_leaf=1000,\n        algorithm=Categorical(['RGF', 'RGF_Opt', 'RGF_Sib']),\n        l2=Real(0.01, 0.3),\n        normalize=Categorical([True, False]),\n        learning_rate=Real(0.3, 0.7),\n        loss=Categorical(['LS', 'Expo', 'Log', 'Abs'])\n    )\n)\noptimizer.go()\n```\n</details>\n\nOutput File Structure\n---------------------\nThis is a simple illustration of the file structure you can expect your `Experiment`s to generate. For an in-depth description of the directory structure and the contents of the various files, see the [File Structure Overview](https://hyperparameter-hunter.readthedocs.io/en/latest/file_structure_overview.html) section in the documentation. However, the essentials are as follows:\n\n1. An `Experiment` adds a file to each *HyperparameterHunterAssets/Experiments* subdirectory, named by `experiment_id`\n2. Each `Experiment` also adds an entry to *HyperparameterHunterAssets/Leaderboards/GlobalLeaderboard.csv*\n3. Customize which files are created via `Environment`'s `file_blacklist` and `do_full_save` kwargs (documented [here](https://hyperparameter-hunter.readthedocs.io/en/latest/api_essentials.html#environment))\n\n```\nHyperparameterHunterAssets\n|   Heartbeat.log\n|\n\u2514\u2500\u2500\u2500Experiments\n|   |\n|   \u2514\u2500\u2500\u2500Descriptions\n|   |   |   <Files describing Experiment results, conditions, etc.>.json\n|   |\n|   \u2514\u2500\u2500\u2500Predictions<OOF/Holdout/Test>\n|   |   |   <Files containing Experiment predictions for the indicated dataset>.csv\n|   |\n|   \u2514\u2500\u2500\u2500Heartbeats\n|   |   |   <Files containing the log produced by the Experiment>.log\n|   |\n|   \u2514\u2500\u2500\u2500ScriptBackups\n|       |   <Files containing a copy of the script that created the Experiment>.py\n|\n\u2514\u2500\u2500\u2500Leaderboards\n|   |   GlobalLeaderboard.csv\n|   |   <Other leaderboards>.csv\n|\n\u2514\u2500\u2500\u2500TestedKeys\n|   |   <Files named by Environment key, containing hyperparameter keys>.json\n|\n\u2514\u2500\u2500\u2500KeyAttributeLookup\n    |   <Files linking complex objects used in Experiments to their hashes>\n```\n\nInstallation\n------------\n\n```\npip install hyperparameter-hunter\n```\n\nIf you like being on the cutting-edge, and you want all the latest developments, run:\n\n```\npip install git+https://github.com/HunterMcGushion/hyperparameter_hunter.git\n```\n\nIf you want to contribute to HyperparameterHunter, [get started here](CONTRIBUTING.md).\n\nI Still Don't Get It\n--------------------\nThat's ok. Don't feel bad. It's a bit weird to wrap your head around. Here's an example that illustrates how everything is related:\n\n```python\nfrom hyperparameter_hunter import Environment, CVExperiment, BayesianOptPro, Integer\nfrom hyperparameter_hunter.utils.learning_utils import get_breast_cancer_data\nfrom xgboost import XGBClassifier\n\n# Start by creating an `Environment` - This is where you define how Experiments (and optimization) will be conducted\nenv = Environment(\n    train_dataset=get_breast_cancer_data(target='target'),\n    results_path='HyperparameterHunterAssets',\n    metrics=['roc_auc_score'],\n    cv_type='StratifiedKFold',\n    cv_params=dict(n_splits=10, shuffle=True, random_state=32),\n)\n\n# Now, conduct an `Experiment`\n# This tells HyperparameterHunter to use the settings in the active `Environment` to train a model with these hyperparameters\nexperiment = CVExperiment(\n    model_initializer=XGBClassifier,\n    model_init_params=dict(\n        objective='reg:linear',\n        max_depth=3\n    )\n)\n\n# That's it. No annoying boilerplate code to fit models and record results\n# Now, the `Environment`'s `results_path` directory will contain new files describing the Experiment just conducted\n\n# Time for the fun part. We'll set up some hyperparameter optimization by first defining the `OptPro` (Optimization Protocol) we want\noptimizer = BayesianOptPro(verbose=1)\n\n# Now we're going to say which hyperparameters we want to optimize.\n# Notice how this looks just like our `experiment` above\noptimizer.forge_experiment(\n    model_initializer=XGBClassifier,\n    model_init_params=dict(\n        objective='reg:linear',  # We're setting this as a constant guideline - Not one to optimize\n        max_depth=Integer(2, 10)  # Instead of using an int like the `experiment` above, we provide a space to search\n    )\n)\n# Notice that our range for `max_depth` includes the `max_depth=3` value we used in our `experiment` earlier\n\noptimizer.go()  # Now, we go\n\nassert experiment.experiment_id in [_[2] for _ in optimizer.similar_experiments]\n# Here we're verifying that the `experiment` we conducted first was found by `optimizer` and used as learning material\n# You can also see via the console that we found `experiment`'s saved files, and used it to start optimization\n\nlast_experiment_id = optimizer.current_experiment.experiment_id\n# Let's save the id of the experiment that was just conducted by `optimizer`\n\noptimizer.go()  # Now, we'll start up `optimizer` again...\n\n# And we can see that this second optimization round learned from both our first `experiment` and our first optimization round\nassert experiment.experiment_id in [_[2] for _ in optimizer.similar_experiments]\nassert last_experiment_id in [_[2] for _ in optimizer.similar_experiments]\n# It even did all this without us having to tell it what experiments to learn from\n\n# Now think about how much better your hyperparameter optimization will be when it learns from:\n# - All your past experiments, and\n# - All your past optimization rounds\n# And the best part: HyperparameterHunter figures out which experiments are compatible all on its own\n# You don't have to worry about telling it that KFold=5 is different from KFold=10,\n# Or that max_depth=12 is outside of max_depth=Integer(2, 10)\n```\n\nTested Libraries\n----------------\n* [Keras](https://github.com/HunterMcGushion/hyperparameter_hunter/blob/master/examples/lib_keras_example.py)\n* [scikit-learn](https://github.com/HunterMcGushion/hyperparameter_hunter/blob/master/examples/lib_sklearn_example.py)\n* [LightGBM](https://github.com/HunterMcGushion/hyperparameter_hunter/blob/master/examples/lib_lightgbm_example.py)\n* [CatBoost](https://github.com/HunterMcGushion/hyperparameter_hunter/blob/master/examples/lib_catboost_example.py)\n* [XGBoost](https://github.com/HunterMcGushion/hyperparameter_hunter/blob/master/examples/simple_experiment_example.py)\n* [rgf_python](https://github.com/HunterMcGushion/hyperparameter_hunter/blob/master/examples/lib_rgf_example.py)\n* ... More on the way\n\nGotchas/FAQs\n------------\nThese are some things that might \"getcha\"\n\n### General:\n- **Can't provide initial search points to `OptPro`?**\n   - This is intentional. If you want your optimization rounds to start with specific search points (that you haven't recorded yet), simply perform a `CVExperiment` before initializing your `OptPro`\n   - Assuming the two have the same guideline hyperparameters and the `Experiment` fits within the search space defined by your `OptPro`, the optimizer will locate and read in the results of the `Experiment`\n   - Keep in mind, you'll probably want to remove the `Experiment` after you've done it once, as the results have been saved. Leaving it there will just execute the same `Experiment` over and over again\n- **After changing things in my \"HyperparameterHunterAssets\" directory, everything stopped working**\n   - Yeah, don't do that. Especially not with \"Descriptions\", \"Leaderboards\", or \"TestedKeys\"\n   - HyperparameterHunter figures out what's going on by reading these files directly. \n   - Removing them, or changing their contents can break a lot of HyperparameterHunter's functionality\n\n### Keras:\n- **Can't find similar Experiments with simple Dense/Activation neural networks?**\n   - This is likely caused by switching between using a separate `Activation` layer, and providing a `Dense` layer with the `activation` kwarg\n   - Each layer is treated as its own little set of hyperparameters (as well as being a hyperparameter, itself), which means that as far as HyperparameterHunter is concerned, the following two examples are NOT equivalent:\n      - ```Dense(10, activation=\u2018sigmoid\u2019)```\n      - ```Dense(10); Activation(\u2018sigmoid\u2019)```\n   - We\u2019re working on this, but for now, the workaround is just to be consistent with how you add activations to your models\n      - Either use separate `Activation` layers, or provide `activation` kwargs to other layers, and stick with it!\n- **Can't optimize the `model.compile` arguments: `optimizer` and `optimizer_params` at the same time?**\n   - This happens because Keras\u2019 `optimizers` expect different arguments\n   - For example, when `optimizer=Categorical(['adam', 'rmsprop'])`, there are two different possible dicts of `optimizer_params`\n   - For now, you can only optimize `optimizer`, and `optimizer_params` separately\n   - A good way to do this might be to select a few optimizers you want to test, and don\u2019t provide an `optimizer_params` value. That way, each `optimizer` will use its default parameters\n      - Then you can select which `optimizer` was the best, and set `optimizer=<best optimizer>`, then move on to tuning `optimizer_params`, with arguments specific to the `optimizer` you selected\n\n### CatBoost:\n- **Can't find similar Experiments for CatBoost?**\n   - This may be happening because the default values for the kwargs expected in CatBoost\u2019s model `__init__` methods are defined somewhere else, and given placeholder values of `None` in their signatures\n   - Because of this, HyperparameterHunter assumes that the default value for an argument really is `None` if you don\u2019t explicitly provide a value for that argument\n   - This is obviously not the case, but I just can\u2019t seem to figure out where the actual default values used by CatBoost are located, so if anyone knows how to remedy this situation, I would love your help!\n\n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/HunterMcGushion/hyperparameter_hunter", "keywords": "hyperparameter tuning optimization machine learning artificial intelligence neural network keras scikit-learn xgboost catboost lightgbm rgf", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "hyperparameter-hunter", "package_url": "https://pypi.org/project/hyperparameter-hunter/", "platform": "", "project_url": "https://pypi.org/project/hyperparameter-hunter/", "project_urls": {"Homepage": "https://github.com/HunterMcGushion/hyperparameter_hunter"}, "release_url": "https://pypi.org/project/hyperparameter-hunter/3.0.0/", "requires_dist": ["dill", "nbconvert", "nbformat", "numpy", "pandas", "scikit-learn", "scikit-optimize", "scipy", "simplejson", "wrapt", "pre-commit ; extra == 'dev'", "numpydoc ; extra == 'docs'", "hyperparameter-hunter ; extra == 'docs'", "keras ; extra == 'docs'", "pytest (>=4.0) ; extra == 'travis'", "pytest-cov ; extra == 'travis'", "black ; extra == 'travis'", "hyperparameter-hunter ; extra == 'travis'", "keras ; extra == 'travis'", "tensorflow ; extra == 'travis'", "xgboost ; extra == 'travis'", "coveralls ; extra == 'travis'", "codecov ; extra == 'travis'"], "requires_python": "", "summary": "Easy hyperparameter optimization and automatic result saving across machine learning algorithms and libraries", "version": "3.0.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>HyperparameterHunter</h1>\n<p><img alt=\"HyperparameterHunter Overview\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/22451dcda65009084e168ff4ef28fc455fd2af01/646f63732f6d656469612f6f766572766965772e676966\"></p>\n<p><a href=\"https://travis-ci.org/HunterMcGushion/hyperparameter_hunter\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6aa2e6096c198d458364d3a694e2507e6a44091f/68747470733a2f2f7472617669732d63692e6f72672f48756e7465724d6347757368696f6e2f6879706572706172616d657465725f68756e7465722e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://hyperparameter-hunter.readthedocs.io/en/stable/?badge=stable\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/cbbee1fcd88f4753a325d2e6659bb25f47d699a4/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6879706572706172616d657465722d68756e7465722f62616467652f3f76657273696f6e3d737461626c65\"></a>\n<a href=\"https://coveralls.io/github/HunterMcGushion/hyperparameter_hunter?branch=master&amp;service=github\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1904f9bb94c539ae54cde6620ff3d17502db672f/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f48756e7465724d6347757368696f6e2f6879706572706172616d657465725f68756e7465722f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562\"></a>\n<a href=\"https://codecov.io/gh/HunterMcGushion/hyperparameter_hunter\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b41299d0e51e908530352300ca27bdea77f8bee2/68747470733a2f2f636f6465636f762e696f2f67682f48756e7465724d6347757368696f6e2f6879706572706172616d657465725f68756e7465722f6272616e63682f6d61737465722f67726170682f62616467652e737667\"></a>\n<a href=\"https://codeclimate.com/github/HunterMcGushion/hyperparameter_hunter/maintainability\" rel=\"nofollow\"><img alt=\"Maintainability\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a5420bf6234b14b905176ea5526850295fa19fc7/68747470733a2f2f6170692e636f6465636c696d6174652e636f6d2f76312f6261646765732f65663064303034613130656465306232323863632f6d61696e7461696e6162696c697479\"></a>\n<a href=\"https://www.codacy.com/app/HunterMcGushion/hyperparameter_hunter?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=HunterMcGushion/hyperparameter_hunter&amp;utm_campaign=Badge_Grade\" rel=\"nofollow\"><img alt=\"Codacy Badge\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/0c08e78cc56b2ae3aa5fea94f467bb60254ebd2a/68747470733a2f2f6170692e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3134313362373666616265323430306661623139353865373062653539336132\"></a></p>\n<p><a href=\"https://badge.fury.io/py/hyperparameter-hunter\" rel=\"nofollow\"><img alt=\"PyPI version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b580d19674ba2980a4b57b2f04a5f367652116a5/68747470733a2f2f62616467652e667572792e696f2f70792f6879706572706172616d657465722d68756e7465722e737667\"></a>\n<a href=\"https://pepy.tech/project/hyperparameter-hunter\" rel=\"nofollow\"><img alt=\"Downloads\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/e67957a63e25deab190d5894351169d65ed5578e/68747470733a2f2f706570792e746563682f62616467652f6879706572706172616d657465722d68756e746572\"></a>\n<a href=\"https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3EX3PQUV256G\" rel=\"nofollow\"><img alt=\"Donate\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1c6b48d4ac9b582dbc400aa2bd53b809f2f2b394/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d677265656e2e737667\"></a>\n<a href=\"https://github.com/ambv/black\" rel=\"nofollow\"><img alt=\"Code style: black\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fbfdc7754183ecf079bc71ddeabaf88f6cbc5c00/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667\"></a></p>\n<p>Automatically save and learn from Experiment results, leading to long-term, persistent optimization that remembers all your tests.</p>\n<p>HyperparameterHunter provides a wrapper for machine learning algorithms that saves all the important data. Simplify the experimentation and hyperparameter tuning process by letting HyperparameterHunter do the hard work\nof recording, organizing, and learning from your tests \u2014 all while using the same libraries you already do. Don't let any of your experiments go to waste, and start doing hyperparameter optimization the way it was meant to be.</p>\n<ul>\n<li><strong>Installation:</strong> <code>pip install hyperparameter-hunter</code></li>\n<li><strong>Source:</strong> <a href=\"https://github.com/HunterMcGushion/hyperparameter_hunter\" rel=\"nofollow\">https://github.com/HunterMcGushion/hyperparameter_hunter</a></li>\n<li><strong>Documentation:</strong> <a href=\"https://hyperparameter-hunter.readthedocs.io/en/latest/index.html\" rel=\"nofollow\">https://hyperparameter-hunter.readthedocs.io</a></li>\n</ul>\n<h2>Features</h2>\n<ul>\n<li>Automatically record Experiment results</li>\n<li>Truly informed hyperparameter optimization that automatically uses past Experiments</li>\n<li>Eliminate boilerplate code for cross-validation loops, predicting, and scoring</li>\n<li>Stop worrying about keeping track of hyperparameters, scores, or re-running the same Experiments</li>\n<li>Use the libraries and utilities you already love</li>\n</ul>\n<h2>How to Use HyperparameterHunter</h2>\n<p>Don\u2019t think of HyperparameterHunter as another optimization library that you bring out only when its time to do hyperparameter optimization. Of course, it does optimization, but its better to view HyperparameterHunter as your own personal machine learning toolbox/assistant.</p>\n<p>The idea is to start using HyperparameterHunter immediately. Run all of your benchmark/one-off experiments through it.</p>\n<p>The more you use HyperparameterHunter, the better your results will be. If you just use it for optimization, sure, it\u2019ll do what you want, but that\u2019s missing the point of HyperparameterHunter.</p>\n<p>If you\u2019ve been using it for experimentation and optimization along the entire course of your project, then when you decide to do hyperparameter optimization, HyperparameterHunter is already aware of all that you\u2019ve done, and that\u2019s when HyperparameterHunter does something remarkable. It doesn\u2019t start optimization from scratch like other libraries. It starts from all of the Experiments and previous optimization rounds you\u2019ve already run through it.</p>\n<h2>Getting Started</h2>\n<h3>1) Environment:</h3>\n<p>Set up an Environment to organize Experiments and Optimization results.\n<br>\nAny Experiments or Optimization rounds we perform will use our active Environment.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">hyperparameter_hunter</span> <span class=\"kn\">import</span> <span class=\"n\">Environment</span><span class=\"p\">,</span> <span class=\"n\">CVExperiment</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">load_breast_cancer</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">StratifiedKFold</span>\n\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">load_breast_cancer</span><span class=\"p\">()</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">feature_names</span><span class=\"p\">)</span>\n<span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s1\">'target'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">target</span>\n\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">Environment</span><span class=\"p\">(</span>\n    <span class=\"n\">train_dataset</span><span class=\"o\">=</span><span class=\"n\">df</span><span class=\"p\">,</span>  <span class=\"c1\"># Add holdout/test dataframes, too</span>\n    <span class=\"n\">results_path</span><span class=\"o\">=</span><span class=\"s1\">'path/to/results/directory'</span><span class=\"p\">,</span>  <span class=\"c1\"># Where your result files will go</span>\n    <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'roc_auc_score'</span><span class=\"p\">],</span>  <span class=\"c1\"># Callables, or strings referring to `sklearn.metrics`</span>\n    <span class=\"n\">cv_type</span><span class=\"o\">=</span><span class=\"n\">StratifiedKFold</span><span class=\"p\">,</span>  <span class=\"c1\"># Class, or string in `sklearn.model_selection`</span>\n    <span class=\"n\">cv_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">n_splits</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n</pre>\n<h3>2) Individual Experimentation:</h3>\n<p>Perform Experiments with your favorite libraries simply by providing model initializers and hyperparameters</p>\n\n<details>\n<summary>Keras</summary>\n<pre><span class=\"c1\"># Same format used by `keras.wrappers.scikit_learn`. Nothing new to learn</span>\n<span class=\"k\">def</span> <span class=\"nf\">build_fn</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"p\">):</span>  <span class=\"c1\"># `input_shape` calculated for you</span>\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">([</span>\n        <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">kernel_initializer</span><span class=\"o\">=</span><span class=\"s1\">'uniform'</span><span class=\"p\">,</span> <span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"n\">input_shape</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'relu'</span><span class=\"p\">),</span>\n        <span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"p\">),</span>\n        <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">kernel_initializer</span><span class=\"o\">=</span><span class=\"s1\">'uniform'</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'sigmoid'</span><span class=\"p\">)</span>\n    <span class=\"p\">])</span>  <span class=\"c1\"># All layer arguments saved (whether explicit or Keras default) for future use</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s1\">'adam'</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s1\">'binary_crossentropy'</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'accuracy'</span><span class=\"p\">])</span>\n    <span class=\"k\">return</span> <span class=\"n\">model</span>\n\n<span class=\"n\">experiment</span> <span class=\"o\">=</span> <span class=\"n\">CVExperiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">KerasClassifier</span><span class=\"p\">,</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"n\">build_fn</span><span class=\"p\">,</span>  <span class=\"c1\"># We interpret your build_fn to save hyperparameters in a useful, readable format</span>\n    <span class=\"n\">model_extra_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span>\n        <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">ReduceLROnPlateau</span><span class=\"p\">(</span><span class=\"n\">patience</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)],</span>  <span class=\"c1\"># Use Keras callbacks</span>\n        <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">0</span>  <span class=\"c1\"># Fit/predict arguments</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n</pre>\n</details>\n\n<details>\n<summary>SKLearn</summary>\n<pre><span class=\"n\">experiment</span> <span class=\"o\">=</span> <span class=\"n\">CVExperiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">LinearSVC</span><span class=\"p\">,</span>  <span class=\"c1\"># (Or any of the dozens of other SK-Learn algorithms)</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">penalty</span><span class=\"o\">=</span><span class=\"s1\">'l1'</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mf\">0.9</span><span class=\"p\">)</span>  <span class=\"c1\"># Default values used and recorded for kwargs not given</span>\n<span class=\"p\">)</span>\n</pre>\n</details>\n\n<details>\n<summary>XGBoost</summary>\n<pre><span class=\"n\">experiment</span> <span class=\"o\">=</span> <span class=\"n\">CVExperiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">XGBClassifier</span><span class=\"p\">,</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">objective</span><span class=\"o\">=</span><span class=\"s1\">'reg:linear'</span><span class=\"p\">,</span> <span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">subsample</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n</pre>\n</details>\n\n<details>\n<summary>LightGBM</summary>\n<pre><span class=\"n\">experiment</span> <span class=\"o\">=</span> <span class=\"n\">CVExperiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">LGBMClassifier</span><span class=\"p\">,</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">boosting_type</span><span class=\"o\">=</span><span class=\"s1\">'gbdt'</span><span class=\"p\">,</span> <span class=\"n\">num_leaves</span><span class=\"o\">=</span><span class=\"mi\">31</span><span class=\"p\">,</span> <span class=\"n\">max_depth</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">min_child_samples</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">subsample</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n</pre>\n</details>\n\n<details>\n<summary>CatBoost</summary>\n<pre><span class=\"n\">experiment</span> <span class=\"o\">=</span> <span class=\"n\">CVExperiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">CatboostClassifier</span><span class=\"p\">,</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">iterations</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"n\">depth</span><span class=\"o\">=</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"n\">allow_writing_files</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">),</span>\n    <span class=\"n\">model_extra_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">fit</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">))</span>  <span class=\"c1\"># Send kwargs to `fit` and other extra methods</span>\n<span class=\"p\">)</span>\n</pre>\n</details>\n\n<details>\n<summary>RGF</summary>\n<pre><span class=\"n\">experiment</span> <span class=\"o\">=</span> <span class=\"n\">CVExperiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">RGFClassifier</span><span class=\"p\">,</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">max_leaf</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"n\">algorithm</span><span class=\"o\">=</span><span class=\"s1\">'RGF'</span><span class=\"p\">,</span> <span class=\"n\">min_samples_leaf</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n</pre>\n</details>\n<h3>3) Hyperparameter Optimization:</h3>\n<p>Just like Experiments, but if you want to optimize a hyperparameter, use the classes imported below</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">hyperparameter_hunter</span> <span class=\"kn\">import</span> <span class=\"n\">Real</span><span class=\"p\">,</span> <span class=\"n\">Integer</span><span class=\"p\">,</span> <span class=\"n\">Categorical</span>\n<span class=\"kn\">from</span> <span class=\"nn\">hyperparameter_hunter</span> <span class=\"kn\">import</span> <span class=\"n\">optimization</span> <span class=\"k\">as</span> <span class=\"n\">opt</span>\n</pre>\n\n<details>\n<summary>Keras</summary>\n<pre><span class=\"k\">def</span> <span class=\"nf\">build_fn</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"p\">):</span>\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">([</span>\n        <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">150</span><span class=\"p\">),</span> <span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"n\">input_shape</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s1\">'relu'</span><span class=\"p\">),</span>\n        <span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.7</span><span class=\"p\">)),</span>\n        <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"n\">Categorical</span><span class=\"p\">([</span><span class=\"s1\">'sigmoid'</span><span class=\"p\">,</span> <span class=\"s1\">'softmax'</span><span class=\"p\">]))</span>\n    <span class=\"p\">])</span>\n    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span>\n        <span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"n\">Categorical</span><span class=\"p\">([</span><span class=\"s1\">'adam'</span><span class=\"p\">,</span> <span class=\"s1\">'rmsprop'</span><span class=\"p\">,</span> <span class=\"s1\">'sgd'</span><span class=\"p\">,</span> <span class=\"s1\">'adadelta'</span><span class=\"p\">]),</span>\n        <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s1\">'binary_crossentropy'</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'accuracy'</span><span class=\"p\">]</span>\n    <span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">model</span>\n\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">RandomForestOptPro</span><span class=\"p\">(</span><span class=\"n\">iterations</span><span class=\"o\">=</span><span class=\"mi\">7</span><span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">forge_experiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">KerasClassifier</span><span class=\"p\">,</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"n\">build_fn</span><span class=\"p\">,</span>\n    <span class=\"n\">model_extra_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span>\n        <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">ReduceLROnPlateau</span><span class=\"p\">(</span><span class=\"n\">patience</span><span class=\"o\">=</span><span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))],</span>\n        <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">Categorical</span><span class=\"p\">([</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">]),</span>\n        <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">0</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">go</span><span class=\"p\">()</span>\n</pre>\n</details>\n\n<details>\n<summary>SKLearn</summary>\n<pre><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">DummyOptPro</span><span class=\"p\">(</span><span class=\"n\">iterations</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">forge_experiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">AdaBoostClassifier</span><span class=\"p\">,</span>  <span class=\"c1\"># (Or any of the dozens of other SKLearn algorithms)</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span>\n        <span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">75</span><span class=\"p\">,</span> <span class=\"mi\">150</span><span class=\"p\">),</span>\n        <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">1.3</span><span class=\"p\">),</span>\n        <span class=\"n\">algorithm</span><span class=\"o\">=</span><span class=\"s1\">'SAMME.R'</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">go</span><span class=\"p\">()</span>\n</pre>\n</details>\n\n<details>\n<summary>XGBoost</summary>\n<pre><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">BayesianOptPro</span><span class=\"p\">(</span><span class=\"n\">iterations</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">forge_experiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">XGBClassifier</span><span class=\"p\">,</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span>\n        <span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"n\">low</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">high</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">),</span>\n        <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mf\">0.0001</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">),</span>\n        <span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span>\n        <span class=\"n\">subsample</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span>\n        <span class=\"n\">booster</span><span class=\"o\">=</span><span class=\"n\">Categorical</span><span class=\"p\">([</span><span class=\"s1\">'gbtree'</span><span class=\"p\">,</span> <span class=\"s1\">'gblinear'</span><span class=\"p\">,</span> <span class=\"s1\">'dart'</span><span class=\"p\">]),</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">go</span><span class=\"p\">()</span>\n</pre>\n</details>\n\n<details>\n<summary>LightGBM</summary>\n<pre><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">BayesianOptPro</span><span class=\"p\">(</span><span class=\"n\">iterations</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">forge_experiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">LGBMClassifier</span><span class=\"p\">,</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span>\n        <span class=\"n\">boosting_type</span><span class=\"o\">=</span><span class=\"n\">Categorical</span><span class=\"p\">([</span><span class=\"s1\">'gbdt'</span><span class=\"p\">,</span> <span class=\"s1\">'dart'</span><span class=\"p\">]),</span>\n        <span class=\"n\">num_leaves</span><span class=\"o\">=</span><span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">),</span>\n        <span class=\"n\">max_depth</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n        <span class=\"n\">min_child_samples</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n        <span class=\"n\">subsample</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">go</span><span class=\"p\">()</span>\n</pre>\n</details>\n\n<details>\n<summary>CatBoost</summary>\n<pre><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">GradientBoostedRegressionTreeOptPro</span><span class=\"p\">(</span><span class=\"n\">iterations</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">forge_experiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">CatBoostClassifier</span><span class=\"p\">,</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span>\n        <span class=\"n\">iterations</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n        <span class=\"n\">eval_metric</span><span class=\"o\">=</span><span class=\"n\">Categorical</span><span class=\"p\">([</span><span class=\"s1\">'Logloss'</span><span class=\"p\">,</span> <span class=\"s1\">'Accuracy'</span><span class=\"p\">,</span> <span class=\"s1\">'AUC'</span><span class=\"p\">]),</span>\n        <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"n\">low</span><span class=\"o\">=</span><span class=\"mf\">0.0001</span><span class=\"p\">,</span> <span class=\"n\">high</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">),</span>\n        <span class=\"n\">depth</span><span class=\"o\">=</span><span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">),</span>\n        <span class=\"n\">allow_writing_files</span><span class=\"o\">=</span><span class=\"kc\">False</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">go</span><span class=\"p\">()</span>\n</pre>\n</details>\n\n<details>\n<summary>RGF</summary>\n<pre><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">ExtraTreesOptPro</span><span class=\"p\">(</span><span class=\"n\">iterations</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">forge_experiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">RGFClassifier</span><span class=\"p\">,</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span>\n        <span class=\"n\">max_leaf</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span>\n        <span class=\"n\">algorithm</span><span class=\"o\">=</span><span class=\"n\">Categorical</span><span class=\"p\">([</span><span class=\"s1\">'RGF'</span><span class=\"p\">,</span> <span class=\"s1\">'RGF_Opt'</span><span class=\"p\">,</span> <span class=\"s1\">'RGF_Sib'</span><span class=\"p\">]),</span>\n        <span class=\"n\">l2</span><span class=\"o\">=</span><span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">),</span>\n        <span class=\"n\">normalize</span><span class=\"o\">=</span><span class=\"n\">Categorical</span><span class=\"p\">([</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"kc\">False</span><span class=\"p\">]),</span>\n        <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"n\">Real</span><span class=\"p\">(</span><span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"mf\">0.7</span><span class=\"p\">),</span>\n        <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"n\">Categorical</span><span class=\"p\">([</span><span class=\"s1\">'LS'</span><span class=\"p\">,</span> <span class=\"s1\">'Expo'</span><span class=\"p\">,</span> <span class=\"s1\">'Log'</span><span class=\"p\">,</span> <span class=\"s1\">'Abs'</span><span class=\"p\">])</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">go</span><span class=\"p\">()</span>\n</pre>\n</details>\n<h2>Output File Structure</h2>\n<p>This is a simple illustration of the file structure you can expect your <code>Experiment</code>s to generate. For an in-depth description of the directory structure and the contents of the various files, see the <a href=\"https://hyperparameter-hunter.readthedocs.io/en/latest/file_structure_overview.html\" rel=\"nofollow\">File Structure Overview</a> section in the documentation. However, the essentials are as follows:</p>\n<ol>\n<li>An <code>Experiment</code> adds a file to each <em>HyperparameterHunterAssets/Experiments</em> subdirectory, named by <code>experiment_id</code></li>\n<li>Each <code>Experiment</code> also adds an entry to <em>HyperparameterHunterAssets/Leaderboards/GlobalLeaderboard.csv</em></li>\n<li>Customize which files are created via <code>Environment</code>'s <code>file_blacklist</code> and <code>do_full_save</code> kwargs (documented <a href=\"https://hyperparameter-hunter.readthedocs.io/en/latest/api_essentials.html#environment\" rel=\"nofollow\">here</a>)</li>\n</ol>\n<pre><code>HyperparameterHunterAssets\n|   Heartbeat.log\n|\n\u2514\u2500\u2500\u2500Experiments\n|   |\n|   \u2514\u2500\u2500\u2500Descriptions\n|   |   |   &lt;Files describing Experiment results, conditions, etc.&gt;.json\n|   |\n|   \u2514\u2500\u2500\u2500Predictions&lt;OOF/Holdout/Test&gt;\n|   |   |   &lt;Files containing Experiment predictions for the indicated dataset&gt;.csv\n|   |\n|   \u2514\u2500\u2500\u2500Heartbeats\n|   |   |   &lt;Files containing the log produced by the Experiment&gt;.log\n|   |\n|   \u2514\u2500\u2500\u2500ScriptBackups\n|       |   &lt;Files containing a copy of the script that created the Experiment&gt;.py\n|\n\u2514\u2500\u2500\u2500Leaderboards\n|   |   GlobalLeaderboard.csv\n|   |   &lt;Other leaderboards&gt;.csv\n|\n\u2514\u2500\u2500\u2500TestedKeys\n|   |   &lt;Files named by Environment key, containing hyperparameter keys&gt;.json\n|\n\u2514\u2500\u2500\u2500KeyAttributeLookup\n    |   &lt;Files linking complex objects used in Experiments to their hashes&gt;\n</code></pre>\n<h2>Installation</h2>\n<pre><code>pip install hyperparameter-hunter\n</code></pre>\n<p>If you like being on the cutting-edge, and you want all the latest developments, run:</p>\n<pre><code>pip install git+https://github.com/HunterMcGushion/hyperparameter_hunter.git\n</code></pre>\n<p>If you want to contribute to HyperparameterHunter, <a href=\"CONTRIBUTING.md\" rel=\"nofollow\">get started here</a>.</p>\n<h2>I Still Don't Get It</h2>\n<p>That's ok. Don't feel bad. It's a bit weird to wrap your head around. Here's an example that illustrates how everything is related:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">hyperparameter_hunter</span> <span class=\"kn\">import</span> <span class=\"n\">Environment</span><span class=\"p\">,</span> <span class=\"n\">CVExperiment</span><span class=\"p\">,</span> <span class=\"n\">BayesianOptPro</span><span class=\"p\">,</span> <span class=\"n\">Integer</span>\n<span class=\"kn\">from</span> <span class=\"nn\">hyperparameter_hunter.utils.learning_utils</span> <span class=\"kn\">import</span> <span class=\"n\">get_breast_cancer_data</span>\n<span class=\"kn\">from</span> <span class=\"nn\">xgboost</span> <span class=\"kn\">import</span> <span class=\"n\">XGBClassifier</span>\n\n<span class=\"c1\"># Start by creating an `Environment` - This is where you define how Experiments (and optimization) will be conducted</span>\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">Environment</span><span class=\"p\">(</span>\n    <span class=\"n\">train_dataset</span><span class=\"o\">=</span><span class=\"n\">get_breast_cancer_data</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"o\">=</span><span class=\"s1\">'target'</span><span class=\"p\">),</span>\n    <span class=\"n\">results_path</span><span class=\"o\">=</span><span class=\"s1\">'HyperparameterHunterAssets'</span><span class=\"p\">,</span>\n    <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'roc_auc_score'</span><span class=\"p\">],</span>\n    <span class=\"n\">cv_type</span><span class=\"o\">=</span><span class=\"s1\">'StratifiedKFold'</span><span class=\"p\">,</span>\n    <span class=\"n\">cv_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">n_splits</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">),</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Now, conduct an `Experiment`</span>\n<span class=\"c1\"># This tells HyperparameterHunter to use the settings in the active `Environment` to train a model with these hyperparameters</span>\n<span class=\"n\">experiment</span> <span class=\"o\">=</span> <span class=\"n\">CVExperiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">XGBClassifier</span><span class=\"p\">,</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span>\n        <span class=\"n\">objective</span><span class=\"o\">=</span><span class=\"s1\">'reg:linear'</span><span class=\"p\">,</span>\n        <span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"mi\">3</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># That's it. No annoying boilerplate code to fit models and record results</span>\n<span class=\"c1\"># Now, the `Environment`'s `results_path` directory will contain new files describing the Experiment just conducted</span>\n\n<span class=\"c1\"># Time for the fun part. We'll set up some hyperparameter optimization by first defining the `OptPro` (Optimization Protocol) we want</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">BayesianOptPro</span><span class=\"p\">(</span><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Now we're going to say which hyperparameters we want to optimize.</span>\n<span class=\"c1\"># Notice how this looks just like our `experiment` above</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">forge_experiment</span><span class=\"p\">(</span>\n    <span class=\"n\">model_initializer</span><span class=\"o\">=</span><span class=\"n\">XGBClassifier</span><span class=\"p\">,</span>\n    <span class=\"n\">model_init_params</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span>\n        <span class=\"n\">objective</span><span class=\"o\">=</span><span class=\"s1\">'reg:linear'</span><span class=\"p\">,</span>  <span class=\"c1\"># We're setting this as a constant guideline - Not one to optimize</span>\n        <span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"n\">Integer</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>  <span class=\"c1\"># Instead of using an int like the `experiment` above, we provide a space to search</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">)</span>\n<span class=\"c1\"># Notice that our range for `max_depth` includes the `max_depth=3` value we used in our `experiment` earlier</span>\n\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">go</span><span class=\"p\">()</span>  <span class=\"c1\"># Now, we go</span>\n\n<span class=\"k\">assert</span> <span class=\"n\">experiment</span><span class=\"o\">.</span><span class=\"n\">experiment_id</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"n\">_</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">similar_experiments</span><span class=\"p\">]</span>\n<span class=\"c1\"># Here we're verifying that the `experiment` we conducted first was found by `optimizer` and used as learning material</span>\n<span class=\"c1\"># You can also see via the console that we found `experiment`'s saved files, and used it to start optimization</span>\n\n<span class=\"n\">last_experiment_id</span> <span class=\"o\">=</span> <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">current_experiment</span><span class=\"o\">.</span><span class=\"n\">experiment_id</span>\n<span class=\"c1\"># Let's save the id of the experiment that was just conducted by `optimizer`</span>\n\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">go</span><span class=\"p\">()</span>  <span class=\"c1\"># Now, we'll start up `optimizer` again...</span>\n\n<span class=\"c1\"># And we can see that this second optimization round learned from both our first `experiment` and our first optimization round</span>\n<span class=\"k\">assert</span> <span class=\"n\">experiment</span><span class=\"o\">.</span><span class=\"n\">experiment_id</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"n\">_</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">similar_experiments</span><span class=\"p\">]</span>\n<span class=\"k\">assert</span> <span class=\"n\">last_experiment_id</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"n\">_</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">similar_experiments</span><span class=\"p\">]</span>\n<span class=\"c1\"># It even did all this without us having to tell it what experiments to learn from</span>\n\n<span class=\"c1\"># Now think about how much better your hyperparameter optimization will be when it learns from:</span>\n<span class=\"c1\"># - All your past experiments, and</span>\n<span class=\"c1\"># - All your past optimization rounds</span>\n<span class=\"c1\"># And the best part: HyperparameterHunter figures out which experiments are compatible all on its own</span>\n<span class=\"c1\"># You don't have to worry about telling it that KFold=5 is different from KFold=10,</span>\n<span class=\"c1\"># Or that max_depth=12 is outside of max_depth=Integer(2, 10)</span>\n</pre>\n<h2>Tested Libraries</h2>\n<ul>\n<li><a href=\"https://github.com/HunterMcGushion/hyperparameter_hunter/blob/master/examples/lib_keras_example.py\" rel=\"nofollow\">Keras</a></li>\n<li><a href=\"https://github.com/HunterMcGushion/hyperparameter_hunter/blob/master/examples/lib_sklearn_example.py\" rel=\"nofollow\">scikit-learn</a></li>\n<li><a href=\"https://github.com/HunterMcGushion/hyperparameter_hunter/blob/master/examples/lib_lightgbm_example.py\" rel=\"nofollow\">LightGBM</a></li>\n<li><a href=\"https://github.com/HunterMcGushion/hyperparameter_hunter/blob/master/examples/lib_catboost_example.py\" rel=\"nofollow\">CatBoost</a></li>\n<li><a href=\"https://github.com/HunterMcGushion/hyperparameter_hunter/blob/master/examples/simple_experiment_example.py\" rel=\"nofollow\">XGBoost</a></li>\n<li><a href=\"https://github.com/HunterMcGushion/hyperparameter_hunter/blob/master/examples/lib_rgf_example.py\" rel=\"nofollow\">rgf_python</a></li>\n<li>... More on the way</li>\n</ul>\n<h2>Gotchas/FAQs</h2>\n<p>These are some things that might \"getcha\"</p>\n<h3>General:</h3>\n<ul>\n<li><strong>Can't provide initial search points to <code>OptPro</code>?</strong>\n<ul>\n<li>This is intentional. If you want your optimization rounds to start with specific search points (that you haven't recorded yet), simply perform a <code>CVExperiment</code> before initializing your <code>OptPro</code></li>\n<li>Assuming the two have the same guideline hyperparameters and the <code>Experiment</code> fits within the search space defined by your <code>OptPro</code>, the optimizer will locate and read in the results of the <code>Experiment</code></li>\n<li>Keep in mind, you'll probably want to remove the <code>Experiment</code> after you've done it once, as the results have been saved. Leaving it there will just execute the same <code>Experiment</code> over and over again</li>\n</ul>\n</li>\n<li><strong>After changing things in my \"HyperparameterHunterAssets\" directory, everything stopped working</strong>\n<ul>\n<li>Yeah, don't do that. Especially not with \"Descriptions\", \"Leaderboards\", or \"TestedKeys\"</li>\n<li>HyperparameterHunter figures out what's going on by reading these files directly.</li>\n<li>Removing them, or changing their contents can break a lot of HyperparameterHunter's functionality</li>\n</ul>\n</li>\n</ul>\n<h3>Keras:</h3>\n<ul>\n<li><strong>Can't find similar Experiments with simple Dense/Activation neural networks?</strong>\n<ul>\n<li>This is likely caused by switching between using a separate <code>Activation</code> layer, and providing a <code>Dense</code> layer with the <code>activation</code> kwarg</li>\n<li>Each layer is treated as its own little set of hyperparameters (as well as being a hyperparameter, itself), which means that as far as HyperparameterHunter is concerned, the following two examples are NOT equivalent:\n<ul>\n<li><code>Dense(10, activation=\u2018sigmoid\u2019)</code></li>\n<li><code>Dense(10); Activation(\u2018sigmoid\u2019)</code></li>\n</ul>\n</li>\n<li>We\u2019re working on this, but for now, the workaround is just to be consistent with how you add activations to your models\n<ul>\n<li>Either use separate <code>Activation</code> layers, or provide <code>activation</code> kwargs to other layers, and stick with it!</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Can't optimize the <code>model.compile</code> arguments: <code>optimizer</code> and <code>optimizer_params</code> at the same time?</strong>\n<ul>\n<li>This happens because Keras\u2019 <code>optimizers</code> expect different arguments</li>\n<li>For example, when <code>optimizer=Categorical(['adam', 'rmsprop'])</code>, there are two different possible dicts of <code>optimizer_params</code></li>\n<li>For now, you can only optimize <code>optimizer</code>, and <code>optimizer_params</code> separately</li>\n<li>A good way to do this might be to select a few optimizers you want to test, and don\u2019t provide an <code>optimizer_params</code> value. That way, each <code>optimizer</code> will use its default parameters\n<ul>\n<li>Then you can select which <code>optimizer</code> was the best, and set <code>optimizer=&lt;best optimizer&gt;</code>, then move on to tuning <code>optimizer_params</code>, with arguments specific to the <code>optimizer</code> you selected</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3>CatBoost:</h3>\n<ul>\n<li><strong>Can't find similar Experiments for CatBoost?</strong>\n<ul>\n<li>This may be happening because the default values for the kwargs expected in CatBoost\u2019s model <code>__init__</code> methods are defined somewhere else, and given placeholder values of <code>None</code> in their signatures</li>\n<li>Because of this, HyperparameterHunter assumes that the default value for an argument really is <code>None</code> if you don\u2019t explicitly provide a value for that argument</li>\n<li>This is obviously not the case, but I just can\u2019t seem to figure out where the actual default values used by CatBoost are located, so if anyone knows how to remedy this situation, I would love your help!</li>\n</ul>\n</li>\n</ul>\n\n          </div>"}, "last_serial": 5638471, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "395fc2cc91464cb7214583008b62e4e1", "sha256": "ed43365b4703858d0e436f17cb7d6940e1c096df7566aa397145b720d02e3183"}, "downloads": -1, "filename": "hyperparameter_hunter-0.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "395fc2cc91464cb7214583008b62e4e1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 108911, "upload_time": "2018-06-14T02:21:57", "upload_time_iso_8601": "2018-06-14T02:21:57.682390Z", "url": "https://files.pythonhosted.org/packages/4a/dc/d109ffc3e7e51ac9a8e3dad9e8dbfbdd6b7c13fee06aca122cc74d356cb4/hyperparameter_hunter-0.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4547b43847f5f99f648aafb1f325fa8b", "sha256": "8a2a2eba2a6eaf45a4225a85c64f89b2c4f48b98fe1e4dd1560987ae1b3d8139"}, "downloads": -1, "filename": "hyperparameter_hunter-0.0.1.tar.gz", "has_sig": false, "md5_digest": "4547b43847f5f99f648aafb1f325fa8b", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 88894, "upload_time": "2018-06-14T02:21:59", "upload_time_iso_8601": "2018-06-14T02:21:59.286704Z", "url": "https://files.pythonhosted.org/packages/ae/d2/adfb1cdd24c8c123bd78c3c67026ada07fba4d2d8c1010784fae1d202b4e/hyperparameter_hunter-0.0.1.tar.gz", "yanked": false}], "1.0.0": [{"comment_text": "", "digests": {"md5": "8936e091109f1075e4d67e0478989b26", "sha256": "937070cc893fc4b735137a82e973d90f5256c518b309fa4f613516bf7f270518"}, "downloads": -1, "filename": "hyperparameter_hunter-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "8936e091109f1075e4d67e0478989b26", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 134630, "upload_time": "2018-08-20T00:43:41", "upload_time_iso_8601": "2018-08-20T00:43:41.406310Z", "url": "https://files.pythonhosted.org/packages/27/fa/0a2bed1b0e1a7c28356020da18e39252ba10b09a82ce2131e5bc92bbde00/hyperparameter_hunter-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "11c4282f67627963cf5ce2c29edf2e34", "sha256": "d1022759fe2c543bb7ad606851569ca8f0c97bd009170f5ce2d5b1692bfc39ba"}, "downloads": -1, "filename": "hyperparameter_hunter-1.0.0.tar.gz", "has_sig": false, "md5_digest": "11c4282f67627963cf5ce2c29edf2e34", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 114571, "upload_time": "2018-08-20T00:43:43", "upload_time_iso_8601": "2018-08-20T00:43:43.274239Z", "url": "https://files.pythonhosted.org/packages/f4/04/6a77551f43357a138d0a9e712eef31bc0daad82956e59982093b2a242e1b/hyperparameter_hunter-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "4ef58b0b39bc9bf32553df8a74769273", "sha256": "2be14d13975d8121ec95bff96e45f89aa139759ebbd3dfc71a06bf6c16a29a02"}, "downloads": -1, "filename": "hyperparameter_hunter-1.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "4ef58b0b39bc9bf32553df8a74769273", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 134659, "upload_time": "2018-08-20T01:04:41", "upload_time_iso_8601": "2018-08-20T01:04:41.172877Z", "url": "https://files.pythonhosted.org/packages/ce/e5/28c3c3b436a1dd3dc6c5b950ae912aae86e66452b6cacc6e410c869b44c4/hyperparameter_hunter-1.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d8c3d4f2bda280ea1529fb4fa14c9079", "sha256": "ccde5b28bcd05359c0f95745172ee47f77d7a48190e131f3f106bc75bd23101f"}, "downloads": -1, "filename": "hyperparameter_hunter-1.0.1.tar.gz", "has_sig": false, "md5_digest": "d8c3d4f2bda280ea1529fb4fa14c9079", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 114697, "upload_time": "2018-08-20T01:04:42", "upload_time_iso_8601": "2018-08-20T01:04:42.900305Z", "url": "https://files.pythonhosted.org/packages/17/87/ebea71e3619339d2c217deecbbd38aa43f3b3a9888b8fcb125978aecc2c7/hyperparameter_hunter-1.0.1.tar.gz", "yanked": false}], "1.0.2": [{"comment_text": "", "digests": {"md5": "70e965a04537f74527b4ff5c85f920b9", "sha256": "1f146bfb81400c4c6a12018d92ba8b03016912698c5616a31db66ec02805ed2b"}, "downloads": -1, "filename": "hyperparameter_hunter-1.0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "70e965a04537f74527b4ff5c85f920b9", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 143446, "upload_time": "2018-08-27T03:51:44", "upload_time_iso_8601": "2018-08-27T03:51:44.148687Z", "url": "https://files.pythonhosted.org/packages/e0/76/dd67c9911f08baa6b565d483688c085f6b731853f3f87b409a8b51077447/hyperparameter_hunter-1.0.2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9a02f60fdaeb1facad8437e47269b944", "sha256": "dea6fcb93aad4386b846d6c3f945ccb5462fa418c6e4c05f8709ed9472bf65cc"}, "downloads": -1, "filename": "hyperparameter_hunter-1.0.2.tar.gz", "has_sig": false, "md5_digest": "9a02f60fdaeb1facad8437e47269b944", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 122163, "upload_time": "2018-08-27T03:51:46", "upload_time_iso_8601": "2018-08-27T03:51:46.317855Z", "url": "https://files.pythonhosted.org/packages/1e/45/a908befcd75eef95de2cf5cb709fae1c45b5c10299b77813b2b01266b6af/hyperparameter_hunter-1.0.2.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "8ef854d882e86a0eca1953d8fd2400f0", "sha256": "5283353f1a811e0078513177b7478e70a56cbeff62f85081bf2a7d88479a4b28"}, "downloads": -1, "filename": "hyperparameter_hunter-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "8ef854d882e86a0eca1953d8fd2400f0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 147060, "upload_time": "2018-10-05T05:28:48", "upload_time_iso_8601": "2018-10-05T05:28:48.418606Z", "url": "https://files.pythonhosted.org/packages/cb/81/4560c3d8f432b7c42d271e15dd116fd8d0accb6232dddd3cc5f4f8897ec7/hyperparameter_hunter-1.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "846382c91152160f5e703a7f7d4a8555", "sha256": "c25ffc33c7af87d4e5a36ffd51e401f4d4af09a2abed52c4ef991f0a49ad035e"}, "downloads": -1, "filename": "hyperparameter_hunter-1.1.0.tar.gz", "has_sig": false, "md5_digest": "846382c91152160f5e703a7f7d4a8555", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 126166, "upload_time": "2018-10-05T05:28:50", "upload_time_iso_8601": "2018-10-05T05:28:50.611110Z", "url": "https://files.pythonhosted.org/packages/58/1a/658a4db9b3e1693b002a6737d4fdd8f3cab653167bc11d28fc8f265b1793/hyperparameter_hunter-1.1.0.tar.gz", "yanked": false}], "2.0.0": [{"comment_text": "", "digests": {"md5": "4ed87ab48e29616f485090627a650fd3", "sha256": "b9e81a931625ccad814da28335f2fdd130618a62c26f841800e2990477c35506"}, "downloads": -1, "filename": "hyperparameter_hunter-2.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "4ed87ab48e29616f485090627a650fd3", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 156513, "upload_time": "2018-11-17T01:36:25", "upload_time_iso_8601": "2018-11-17T01:36:25.803868Z", "url": "https://files.pythonhosted.org/packages/dc/27/130b0e0bc58931090657617e24575d3336957f31a1a43930b4d5c137b5c5/hyperparameter_hunter-2.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0513216a9d0109447b36a0424e7fcb60", "sha256": "b26ba114c057825dfafbc0a3dd41d680e883b77b685372cbf01f60bf768bd6eb"}, "downloads": -1, "filename": "hyperparameter_hunter-2.0.0.tar.gz", "has_sig": false, "md5_digest": "0513216a9d0109447b36a0424e7fcb60", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 140588, "upload_time": "2018-11-17T01:36:28", "upload_time_iso_8601": "2018-11-17T01:36:28.219735Z", "url": "https://files.pythonhosted.org/packages/ae/ef/98b45c09e675282b176669f3636ae3b65af621dc74deab37c7f07ce7029c/hyperparameter_hunter-2.0.0.tar.gz", "yanked": false}], "2.0.1": [{"comment_text": "", "digests": {"md5": "9a52cb4bf83ff24319a23c86508e7742", "sha256": "ed84a52aeb2cf98154529a22957e268a76f5df163a298ce0b6f4ea5269c2543a"}, "downloads": -1, "filename": "hyperparameter_hunter-2.0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "9a52cb4bf83ff24319a23c86508e7742", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 157462, "upload_time": "2018-11-25T23:19:43", "upload_time_iso_8601": "2018-11-25T23:19:43.474572Z", "url": "https://files.pythonhosted.org/packages/dc/20/33ea4197ff0b123be8ebeb09a221a311e0441a64b6fdf114d042e6d4b1a5/hyperparameter_hunter-2.0.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "58f50ae7f08d2c505cc1f540bd87b63e", "sha256": "e1b8548858fb7548e8c182922e618ea3c6e224e3ccc4cc696c5128f18befe3fa"}, "downloads": -1, "filename": "hyperparameter_hunter-2.0.1.tar.gz", "has_sig": false, "md5_digest": "58f50ae7f08d2c505cc1f540bd87b63e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 141138, "upload_time": "2018-11-25T23:19:47", "upload_time_iso_8601": "2018-11-25T23:19:47.082476Z", "url": "https://files.pythonhosted.org/packages/23/04/cc48107df1cb1a99f48e401ff3ab0bd05c932498b5c5aa40556dd3f8d6d3/hyperparameter_hunter-2.0.1.tar.gz", "yanked": false}], "2.1.0": [{"comment_text": "", "digests": {"md5": "ab1275bf7e190f41d5cc09d99c049972", "sha256": "009714437ed51c1e22adf4243e2fe70e405cdef13ba05b42d6b038646c27057e"}, "downloads": -1, "filename": "hyperparameter_hunter-2.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "ab1275bf7e190f41d5cc09d99c049972", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 162285, "upload_time": "2019-01-16T05:33:36", "upload_time_iso_8601": "2019-01-16T05:33:36.550174Z", "url": "https://files.pythonhosted.org/packages/d9/e0/1eb58ee8b62ebd44591cc4c2ae7f0854c2e0384b7366dce7684d17d76d94/hyperparameter_hunter-2.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bf9177575c5267946f2ef7a29a0c76c6", "sha256": "906ebd24c59ffb0abc05f488c8eeb23c70dbc127659b9d1d338314a9fcff30d3"}, "downloads": -1, "filename": "hyperparameter_hunter-2.1.0.tar.gz", "has_sig": false, "md5_digest": "bf9177575c5267946f2ef7a29a0c76c6", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 146333, "upload_time": "2019-01-16T05:33:38", "upload_time_iso_8601": "2019-01-16T05:33:38.451286Z", "url": "https://files.pythonhosted.org/packages/71/3a/cbe0178749e75f498c6b35515ebb017eade332f1c4a76e2e41f0abc933dd/hyperparameter_hunter-2.1.0.tar.gz", "yanked": false}], "2.1.1": [{"comment_text": "", "digests": {"md5": "6b080dbbd3cc9c9a4628934240159100", "sha256": "170698c83ceb01ef6fcf8f06c89b3ee70928193e480479efa83a022dcfd3e369"}, "downloads": -1, "filename": "hyperparameter_hunter-2.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "6b080dbbd3cc9c9a4628934240159100", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 162286, "upload_time": "2019-01-16T05:57:25", "upload_time_iso_8601": "2019-01-16T05:57:25.993686Z", "url": "https://files.pythonhosted.org/packages/ce/3f/bb215cb919b1638c1ca49be23c167f0cccd04f2b852c98d99d12a18574f7/hyperparameter_hunter-2.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "5b827bffbb92e1b81e0728c2c96019f8", "sha256": "4ccdc3d6ecf9b6180173704e5f1c1a3d2223c9debd8a465b5e1e73c1d8ce2da6"}, "downloads": -1, "filename": "hyperparameter_hunter-2.1.1.tar.gz", "has_sig": false, "md5_digest": "5b827bffbb92e1b81e0728c2c96019f8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 146332, "upload_time": "2019-01-16T05:57:27", "upload_time_iso_8601": "2019-01-16T05:57:27.994332Z", "url": "https://files.pythonhosted.org/packages/8e/cf/88467a7b84d057036012de79ef358717cca7c0222e5060e5b3121da7c6bb/hyperparameter_hunter-2.1.1.tar.gz", "yanked": false}], "2.2.0": [{"comment_text": "", "digests": {"md5": "1f99f4cf038a99ace01c03b6ae7214cd", "sha256": "14f0022d707a8036a9c5e6ae364b2fbc21cbb58eb457c341458e2cafa74c3133"}, "downloads": -1, "filename": "hyperparameter_hunter-2.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "1f99f4cf038a99ace01c03b6ae7214cd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 166070, "upload_time": "2019-02-11T00:29:12", "upload_time_iso_8601": "2019-02-11T00:29:12.693803Z", "url": "https://files.pythonhosted.org/packages/6f/02/1d15f1cd8bf982781897fb5aa2b551c83c34e6d464654f71bd9352220895/hyperparameter_hunter-2.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7325610cd6f0df084018cd5d7b5893ea", "sha256": "f9e2358fd0dd8fc210f0aedd1a8e8a2545d94ef74ffb77bafe5a9480eb688bda"}, "downloads": -1, "filename": "hyperparameter_hunter-2.2.0.tar.gz", "has_sig": false, "md5_digest": "7325610cd6f0df084018cd5d7b5893ea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 150206, "upload_time": "2019-02-11T00:29:14", "upload_time_iso_8601": "2019-02-11T00:29:14.789605Z", "url": "https://files.pythonhosted.org/packages/01/75/44bb5b43fc4523e7a4a897b34dfcb23d911fd30b44cfa86d485a4cb04065/hyperparameter_hunter-2.2.0.tar.gz", "yanked": false}], "3.0.0": [{"comment_text": "", "digests": {"md5": "48cc0da3cc8efc68c747cf43ec9a4e0c", "sha256": "4fbe5c320cb29c6043f7532d14dcb05b42dc0d4c613dfa206b732302e985d2d8"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "48cc0da3cc8efc68c747cf43ec9a4e0c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 233342, "upload_time": "2019-08-06T08:52:20", "upload_time_iso_8601": "2019-08-06T08:52:20.160261Z", "url": "https://files.pythonhosted.org/packages/7a/e0/e6e73f1bb07cc738746a5a5bfdbc575b4123c8a6aa388b4d64e4880c802f/hyperparameter_hunter-3.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a029938f443d3930cbbcf84d7c03df8f", "sha256": "279baf496c31aa2542479e97db96dccd182d9ddc2e1a7da4ad7a570e83d135a2"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0.tar.gz", "has_sig": false, "md5_digest": "a029938f443d3930cbbcf84d7c03df8f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 208877, "upload_time": "2019-08-06T08:52:22", "upload_time_iso_8601": "2019-08-06T08:52:22.627252Z", "url": "https://files.pythonhosted.org/packages/1a/c8/9e703dba3866daf8255ac0fdb2b68c79d6e5567645d88aea59240c9c5966/hyperparameter_hunter-3.0.0.tar.gz", "yanked": false}], "3.0.0a0": [{"comment_text": "", "digests": {"md5": "118255f7ac80d6d8924aa62db38a39b1", "sha256": "afddd5c0ae4d375d52d563a10468f7f6e84821f96848bb19ef4cc05d3ea29160"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0a0-py3-none-any.whl", "has_sig": false, "md5_digest": "118255f7ac80d6d8924aa62db38a39b1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 191673, "upload_time": "2019-06-07T11:54:00", "upload_time_iso_8601": "2019-06-07T11:54:00.200798Z", "url": "https://files.pythonhosted.org/packages/1f/b1/686c10ee38e0c0614e940338aa7d4d280aa2df7214a6960e024854ae4997/hyperparameter_hunter-3.0.0a0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "345e160330adbd209a47c00169560e85", "sha256": "1b949082043accc4b39ff4184eea78e76aae498f60667631da6bfcf3d9a8712a"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0a0.tar.gz", "has_sig": false, "md5_digest": "345e160330adbd209a47c00169560e85", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 171811, "upload_time": "2019-06-07T11:54:03", "upload_time_iso_8601": "2019-06-07T11:54:03.293836Z", "url": "https://files.pythonhosted.org/packages/44/f7/81874830764a01d66eddec71b2503cb8015de439a9d2aee34da4c32c1589/hyperparameter_hunter-3.0.0a0.tar.gz", "yanked": false}], "3.0.0a1": [{"comment_text": "", "digests": {"md5": "b27f25e2f95b497d8a285c3fcb216798", "sha256": "faa29d3e0e25afc61577995f1c31c7f11e313c6eb0028fbc337dfd894bbe059b"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0a1-py3-none-any.whl", "has_sig": false, "md5_digest": "b27f25e2f95b497d8a285c3fcb216798", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 195157, "upload_time": "2019-06-08T06:02:04", "upload_time_iso_8601": "2019-06-08T06:02:04.116101Z", "url": "https://files.pythonhosted.org/packages/74/8b/b707f89ee6e18674db9e6c02808df974f401d08174d1e6d4cb3f28af76d7/hyperparameter_hunter-3.0.0a1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "b5d634029a07d07e12b93a933a510c8e", "sha256": "06706d5a9f95abd7304292a3bee8106107a8cc463939b3bc922dbba28d643bcc"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0a1.tar.gz", "has_sig": false, "md5_digest": "b5d634029a07d07e12b93a933a510c8e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 173710, "upload_time": "2019-06-08T06:02:06", "upload_time_iso_8601": "2019-06-08T06:02:06.481333Z", "url": "https://files.pythonhosted.org/packages/9b/52/888ee98059caf218a89c8f17657c7a2ae97aad141773eb1f27f057e6f68a/hyperparameter_hunter-3.0.0a1.tar.gz", "yanked": false}], "3.0.0a2": [{"comment_text": "", "digests": {"md5": "1fbc8737d036cb569a675d4f00bed1dd", "sha256": "d7b1c94b6023487d608048af05d546a79c20bc17455c3ea243da9e4d778a06e4"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0a2-py3-none-any.whl", "has_sig": false, "md5_digest": "1fbc8737d036cb569a675d4f00bed1dd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 196657, "upload_time": "2019-06-12T08:06:19", "upload_time_iso_8601": "2019-06-12T08:06:19.956049Z", "url": "https://files.pythonhosted.org/packages/39/98/4bb558f77b7dfd54b6ecb20fd0ab624977379d719780a46b1936ca95f279/hyperparameter_hunter-3.0.0a2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0db596b74dbe55f10d08c5af79c13688", "sha256": "1a95d25142d9bc95185fef536b62a577fd8f57532eeb1969983234a8937693c3"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0a2.tar.gz", "has_sig": false, "md5_digest": "0db596b74dbe55f10d08c5af79c13688", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 175125, "upload_time": "2019-06-12T08:06:21", "upload_time_iso_8601": "2019-06-12T08:06:21.957312Z", "url": "https://files.pythonhosted.org/packages/4e/f9/82b2f2456485859899225ecb563c225182490c73710e6fd1f0bb4ace5541/hyperparameter_hunter-3.0.0a2.tar.gz", "yanked": false}], "3.0.0b0": [{"comment_text": "", "digests": {"md5": "ed9d3e52fbc4024b3b669f5fb6f919a6", "sha256": "c5526254a0612161357667fa0a197b8d64d76b9d6244b3c7b0f091fd6e4d8da3"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0b0-py3-none-any.whl", "has_sig": false, "md5_digest": "ed9d3e52fbc4024b3b669f5fb6f919a6", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 223236, "upload_time": "2019-07-14T09:52:36", "upload_time_iso_8601": "2019-07-14T09:52:36.495288Z", "url": "https://files.pythonhosted.org/packages/64/59/703c42449a6085f717e5f9bff74814feb0347d093b77d071db4d1b6ffbd8/hyperparameter_hunter-3.0.0b0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "7bfa73c6c8f20b8ed01d9d61afe60ab9", "sha256": "b6efed48d33ace76a249ca1ece749f409a7c7307d90760a71aafae249cc3cd85"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0b0.tar.gz", "has_sig": false, "md5_digest": "7bfa73c6c8f20b8ed01d9d61afe60ab9", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 199030, "upload_time": "2019-07-14T09:52:38", "upload_time_iso_8601": "2019-07-14T09:52:38.648847Z", "url": "https://files.pythonhosted.org/packages/44/f6/bde4ab495c67d7ae120157d54fcad03885fb4b93a1cdf837ce9dfccc8788/hyperparameter_hunter-3.0.0b0.tar.gz", "yanked": false}], "3.0.0b1": [{"comment_text": "", "digests": {"md5": "4e259fb5b809a1ca2f2d4867568755e8", "sha256": "ca00e8094b6d6d2337d4890638e993e59615165961fda7cd188740cddd1980c9"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0b1-py3-none-any.whl", "has_sig": false, "md5_digest": "4e259fb5b809a1ca2f2d4867568755e8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 233035, "upload_time": "2019-08-06T06:01:58", "upload_time_iso_8601": "2019-08-06T06:01:58.974145Z", "url": "https://files.pythonhosted.org/packages/de/25/c08e82287dc5305f669118af7288b28179f3ffc5919307af81ca70f135a4/hyperparameter_hunter-3.0.0b1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "701b3caf167227378edb77c61b178e1d", "sha256": "16cc88396ead51543cfa4a3403528b236952c3693093ae322426c63d26e6c5db"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0b1.tar.gz", "has_sig": false, "md5_digest": "701b3caf167227378edb77c61b178e1d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 208684, "upload_time": "2019-08-06T06:02:01", "upload_time_iso_8601": "2019-08-06T06:02:01.141826Z", "url": "https://files.pythonhosted.org/packages/28/cb/4eb675ca1d0bcc362903dea6aeff8cd856c7fa026591f264cb33022318aa/hyperparameter_hunter-3.0.0b1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "48cc0da3cc8efc68c747cf43ec9a4e0c", "sha256": "4fbe5c320cb29c6043f7532d14dcb05b42dc0d4c613dfa206b732302e985d2d8"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "48cc0da3cc8efc68c747cf43ec9a4e0c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 233342, "upload_time": "2019-08-06T08:52:20", "upload_time_iso_8601": "2019-08-06T08:52:20.160261Z", "url": "https://files.pythonhosted.org/packages/7a/e0/e6e73f1bb07cc738746a5a5bfdbc575b4123c8a6aa388b4d64e4880c802f/hyperparameter_hunter-3.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a029938f443d3930cbbcf84d7c03df8f", "sha256": "279baf496c31aa2542479e97db96dccd182d9ddc2e1a7da4ad7a570e83d135a2"}, "downloads": -1, "filename": "hyperparameter_hunter-3.0.0.tar.gz", "has_sig": false, "md5_digest": "a029938f443d3930cbbcf84d7c03df8f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 208877, "upload_time": "2019-08-06T08:52:22", "upload_time_iso_8601": "2019-08-06T08:52:22.627252Z", "url": "https://files.pythonhosted.org/packages/1a/c8/9e703dba3866daf8255ac0fdb2b68c79d6e5567645d88aea59240c9c5966/hyperparameter_hunter-3.0.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:49:31 2020"}