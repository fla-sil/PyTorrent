{"info": {"author": "Raghavendra Kotikalapudi, Johannes Filter", "author_email": "ragha@outlook.com, hi@jfilter.de", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# Text Classification Keras [![Build Status](https://travis-ci.com/jfilter/text-classification-keras.svg?branch=master)](https://travis-ci.com/jfilter/text-classification-keras) [![PyPI](https://img.shields.io/pypi/v/text-classification-keras.svg)](https://pypi.org/project/text-classification-keras/) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/text-classification-keras.svg)](https://pypi.org/project/text-classification-keras/) [![Gitter](https://img.shields.io/gitter/room/text-classification-keras/Lobby.svg)](https://gitter.im/text-classification-keras/Lobby)\n\nA high-level text classification library implementing various well-established models. With a clean and extendable interface to implement custom architectures.\n\n## Quick start\n\n### Install\n\n```bash\npip install text-classification-keras[full]\n```\n\nThe `[full]` will additionally install [TensorFlow](https://github.com/tensorflow/tensorflow), [Spacy](https://github.com/explosion/spaCy), and [Deep Plots](https://github.com/jfilter/deep-plots). Choose this if you want to get started right away.\n\n### Usage\n\n```python\nfrom texcla import experiment, data\nfrom texcla.models import TokenModelFactory, YoonKimCNN\nfrom texcla.preprocessing import FastTextWikiTokenizer\n\n# input text\nX = ['some random text', 'another random text lala', 'peter', ...]\n\n# input labels\ny = ['a', 'b', 'a', ...]\n\n# use the special tokenizer used for constructing the embeddings\ntokenizer = FastTextWikiTokenizer()\n\n# preprocess data (once)\nexperiment.setup_data(X, y, tokenizer, 'data.bin', max_len=100)\n\n# load data\nds = data.Dataset.load('data.bin')\n\n# construct base\nfactory = TokenModelFactory(\n    ds.num_classes, ds.tokenizer.token_index, max_tokens=100,\n    embedding_type='fasttext.wiki.simple', embedding_dims=300)\n\n# choose a model\nword_encoder_model = YoonKimCNN()\n\n# build a model\nmodel = factory.build_model(\n    token_encoder_model=word_encoder_model, trainable_embeddings=False)\n\n# use experiment.train as wrapper for Keras.fit()\nexperiment.train(x=ds.X, y=ds.y, validation_split=0.1, model=model,\n    word_encoder_model=word_encoder_model)\n```\n\nCheck out more [examples](./examples).\n\n## API Documenation\n\n<https://github.io/jfilter/text-classification-keras/>\n\n## Advanced\n\n### Embeddings\n\nChoose a pre-trained word embedding by setting the embedding_type and the corresponding embedding dimensions. Set `embedding_type=None` to initialize the word embeddings randomly (but make sure to set `trainable_embeddings=True` so you actually train the embeddings).\n\n```python\nfactory = TokenModelFactory(embedding_type='fasttext.wiki.simple', embedding_dims=300)\n```\n\n#### FastText\n\nSeveral pre-trained [FastText](https://fasttext.cc/) embeddings are included. For now, we only have the word embeddings and not the n-gram features. All embedding have 300 dimensions.\n\n-   [English Vectors](https://fasttext.cc/docs/en/english-vectors.html): e.g. `fasttext.wn.1M.300d`, [check out all avaiable embeddings](https://github.com/jfilter/text-classification-keras/blob/master/texcla/embeddings.py#L19)\n-   [Multilang Vectors](https://fasttext.cc/docs/en/crawl-vectors.html): in the format `fasttext.cc.LANG_CODE` e.g. `fasttext.cc.en`\n-   [Wikipedia Vectors](https://fasttext.cc/docs/en/pretrained-vectors.html): in the format `fasttext.wiki.LANG_CODE` e.g. `fasttext.wiki.en`\n\n#### GloVe\n\nThe [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings are some kind of predecessor to FastText. In general choose FastText embeddings over GloVe. The dimension for the pre-trained embeddings varies.\n\n-   : e.g. `glove.6B.50d`, [check out all avaiable embeddings](https://github.com/jfilter/text-classification-keras/blob/master/texcla/embeddings.py#L19)\n\n### Tokenzation\n\n-   To work on token (or word) level, use a TokenTokenizer such e.g. `TwokenizeTokenizer` or `SpacyTokenizer`.\n-   To work on token and sentence level, use `SpacySentenceTokenizer`.\n-   To create an custom Tokenizer, extend `Tokenizer` and implement the `token_generator` method.\n\n#### Spacy\n\nYou may use [spaCy](https://spacy.io/) for the tokenization. See instructions on how to\n[download model](https://spacy.io/docs/usage/models#download) for your target language. E.g. for English:\n\n```bash\npython -m spacy download en\n```\n\n### Models\n\n#### Token-based Models\n\nWhen working on token level, use `TokenModelFactory`.\n\n```python\nfrom texcla.models import TokenModelFactory, YoonKimCNN\n\nfactory = TokenModelFactory(tokenizer.num_classes, tokenizer.token_index,\n    max_tokens=100, embedding_type='glove.6B.100d')\nword_encoder_model = YoonKimCNN()\nmodel = factory.build_model(token_encoder_model=word_encoder_model)\n```\n\nCurrently supported models include:\n\n-   [Yoon Kim CNN](https://arxiv.org/abs/1408.5882)\n-   [Stacked RNNs](https://arxiv.org/abs/1312.6026)\n-   [Attention (with/without context) based RNN encoders](https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf)\n\n`TokenModelFactory.build_model` uses the provided word encoder which is then classified via a [Dense](https://keras.io/layers/core/#dense) layer.\n\n#### Sentence-based Models\n\nWhen working on sentence level, use `SentenceModelFactory`.\n\n```python\n# Pad max sentences per doc to 500 and max words per sentence to 200.\n# Can also use `max_sents=None` to allow variable sized max_sents per mini-batch.\n\nfactory = SentenceModelFactory(10, tokenizer.token_index, max_sents=500,\n    max_tokens=200, embedding_type='glove.6B.100d')\nword_encoder_model = AttentionRNN()\nsentence_encoder_model = AttentionRNN()\n\n# Allows you to compose arbitrary word encoders followed by sentence encoder.\nmodel = factory.build_model(word_encoder_model, sentence_encoder_model)\n```\n\n-   [Hierarchical attention networks](http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)\n    (HANs) can be build by composing two attention based RNN models. This is useful when a document is very large.\n-   For smaller document a reasonable way to encode sentences is to average words within it. This can be done by using\n    `token_encoder_model=AveragingEncoder()`\n-   Mix and match encoders as you see fit for your problem.\n\n`SentenceModelFactory.build_model` created a tiered model where words within a sentence is first encoded using\n`word_encoder_model`. All such encodings per sentence is then encoded using `sentence_encoder_model`.\n\n## Related\n\n-   https://github.com/brightmart/text_classification\n-   https://github.com/allenai/allennlp\n-   https://github.com/facebookresearch/pytext\n-   https://docs.fast.ai/text.html\n-   https://github.com/dkpro/dkpro-tc\n\n## Contributing\n\nIf you have a **question**, found a **bug** or want to propose a new **feature**, have a look at the [issues page](https://github.com/jfilter/text-classification-keras/issues).\n\n**Pull requests** are especially welcomed when they fix bugs or improve the code quality.\n\n## Acknowledgements\n\nBuilt upon the work by Raghavendra Kotikalapudi: [keras-text](https://github.com/raghakot/keras-text).\n\n## Citation\n\nIf you find Text Classification Keras useful for an academic publication, then please use the following BibTeX to cite it:\n\n```tex\n@misc{raghakotfiltertexclakeras\n    title={Text Classification Keras},\n    author={Raghavendra Kotikalapudi, and Johannes Filter, and contributors},\n    year={2018},\n    publisher={GitHub},\n    howpublished={\\url{https://github.com/jfilter/text-classification-keras}},\n}\n```\n\n## License\n\nMIT.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/jfilter/text-classification-keras", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "text-classification-keras", "package_url": "https://pypi.org/project/text-classification-keras/", "platform": "", "project_url": "https://pypi.org/project/text-classification-keras/", "project_urls": {"Homepage": "https://github.com/jfilter/text-classification-keras"}, "release_url": "https://pypi.org/project/text-classification-keras/0.1.4/", "requires_dist": ["keras (==2.*)", "six (==1.*)", "scikit-learn (==0.*)", "joblib (==0.*)", "jsonpickle (==0.*)", "numpy (==1.*)", "spacy (==2.*) ; extra == 'full'", "deep-plots (==0.*) ; extra == 'full'", "tensorflow (==1.*) ; extra == 'full'"], "requires_python": "", "summary": "Text Classification Library for Keras", "version": "0.1.4", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Text Classification Keras <a href=\"https://travis-ci.com/jfilter/text-classification-keras\" rel=\"nofollow\"><img alt=\"Build Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/67a55276b999d09ed73d5d0271ad98823d206635/68747470733a2f2f7472617669732d63692e636f6d2f6a66696c7465722f746578742d636c617373696669636174696f6e2d6b657261732e7376673f6272616e63683d6d6173746572\"></a> <a href=\"https://pypi.org/project/text-classification-keras/\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/6056cb2595602e5e60027194e17def5ecd645b19/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f746578742d636c617373696669636174696f6e2d6b657261732e737667\"></a> <a href=\"https://pypi.org/project/text-classification-keras/\" rel=\"nofollow\"><img alt=\"PyPI - Python Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3c033139020ed20bb95252ab47ccd980e5937e59/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f746578742d636c617373696669636174696f6e2d6b657261732e737667\"></a> <a href=\"https://gitter.im/text-classification-keras/Lobby\" rel=\"nofollow\"><img alt=\"Gitter\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/628d97376eac92f3bff809869650aa6d421411e6/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f746578742d636c617373696669636174696f6e2d6b657261732f4c6f6262792e737667\"></a></h1>\n<p>A high-level text classification library implementing various well-established models. With a clean and extendable interface to implement custom architectures.</p>\n<h2>Quick start</h2>\n<h3>Install</h3>\n<pre>pip install text-classification-keras<span class=\"o\">[</span>full<span class=\"o\">]</span>\n</pre>\n<p>The <code>[full]</code> will additionally install <a href=\"https://github.com/tensorflow/tensorflow\" rel=\"nofollow\">TensorFlow</a>, <a href=\"https://github.com/explosion/spaCy\" rel=\"nofollow\">Spacy</a>, and <a href=\"https://github.com/jfilter/deep-plots\" rel=\"nofollow\">Deep Plots</a>. Choose this if you want to get started right away.</p>\n<h3>Usage</h3>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">texcla</span> <span class=\"kn\">import</span> <span class=\"n\">experiment</span><span class=\"p\">,</span> <span class=\"n\">data</span>\n<span class=\"kn\">from</span> <span class=\"nn\">texcla.models</span> <span class=\"kn\">import</span> <span class=\"n\">TokenModelFactory</span><span class=\"p\">,</span> <span class=\"n\">YoonKimCNN</span>\n<span class=\"kn\">from</span> <span class=\"nn\">texcla.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">FastTextWikiTokenizer</span>\n\n<span class=\"c1\"># input text</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'some random text'</span><span class=\"p\">,</span> <span class=\"s1\">'another random text lala'</span><span class=\"p\">,</span> <span class=\"s1\">'peter'</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># input labels</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'b'</span><span class=\"p\">,</span> <span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># use the special tokenizer used for constructing the embeddings</span>\n<span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">FastTextWikiTokenizer</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># preprocess data (once)</span>\n<span class=\"n\">experiment</span><span class=\"o\">.</span><span class=\"n\">setup_data</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"s1\">'data.bin'</span><span class=\"p\">,</span> <span class=\"n\">max_len</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># load data</span>\n<span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">'data.bin'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># construct base</span>\n<span class=\"n\">factory</span> <span class=\"o\">=</span> <span class=\"n\">TokenModelFactory</span><span class=\"p\">(</span>\n    <span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">num_classes</span><span class=\"p\">,</span> <span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">token_index</span><span class=\"p\">,</span> <span class=\"n\">max_tokens</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n    <span class=\"n\">embedding_type</span><span class=\"o\">=</span><span class=\"s1\">'fasttext.wiki.simple'</span><span class=\"p\">,</span> <span class=\"n\">embedding_dims</span><span class=\"o\">=</span><span class=\"mi\">300</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># choose a model</span>\n<span class=\"n\">word_encoder_model</span> <span class=\"o\">=</span> <span class=\"n\">YoonKimCNN</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># build a model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">factory</span><span class=\"o\">.</span><span class=\"n\">build_model</span><span class=\"p\">(</span>\n    <span class=\"n\">token_encoder_model</span><span class=\"o\">=</span><span class=\"n\">word_encoder_model</span><span class=\"p\">,</span> <span class=\"n\">trainable_embeddings</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># use experiment.train as wrapper for Keras.fit()</span>\n<span class=\"n\">experiment</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">validation_split</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span>\n    <span class=\"n\">word_encoder_model</span><span class=\"o\">=</span><span class=\"n\">word_encoder_model</span><span class=\"p\">)</span>\n</pre>\n<p>Check out more <a href=\"./examples\" rel=\"nofollow\">examples</a>.</p>\n<h2>API Documenation</h2>\n<p><a href=\"https://github.io/jfilter/text-classification-keras/\" rel=\"nofollow\">https://github.io/jfilter/text-classification-keras/</a></p>\n<h2>Advanced</h2>\n<h3>Embeddings</h3>\n<p>Choose a pre-trained word embedding by setting the embedding_type and the corresponding embedding dimensions. Set <code>embedding_type=None</code> to initialize the word embeddings randomly (but make sure to set <code>trainable_embeddings=True</code> so you actually train the embeddings).</p>\n<pre><span class=\"n\">factory</span> <span class=\"o\">=</span> <span class=\"n\">TokenModelFactory</span><span class=\"p\">(</span><span class=\"n\">embedding_type</span><span class=\"o\">=</span><span class=\"s1\">'fasttext.wiki.simple'</span><span class=\"p\">,</span> <span class=\"n\">embedding_dims</span><span class=\"o\">=</span><span class=\"mi\">300</span><span class=\"p\">)</span>\n</pre>\n<h4>FastText</h4>\n<p>Several pre-trained <a href=\"https://fasttext.cc/\" rel=\"nofollow\">FastText</a> embeddings are included. For now, we only have the word embeddings and not the n-gram features. All embedding have 300 dimensions.</p>\n<ul>\n<li><a href=\"https://fasttext.cc/docs/en/english-vectors.html\" rel=\"nofollow\">English Vectors</a>: e.g. <code>fasttext.wn.1M.300d</code>, <a href=\"https://github.com/jfilter/text-classification-keras/blob/master/texcla/embeddings.py#L19\" rel=\"nofollow\">check out all avaiable embeddings</a></li>\n<li><a href=\"https://fasttext.cc/docs/en/crawl-vectors.html\" rel=\"nofollow\">Multilang Vectors</a>: in the format <code>fasttext.cc.LANG_CODE</code> e.g. <code>fasttext.cc.en</code></li>\n<li><a href=\"https://fasttext.cc/docs/en/pretrained-vectors.html\" rel=\"nofollow\">Wikipedia Vectors</a>: in the format <code>fasttext.wiki.LANG_CODE</code> e.g. <code>fasttext.wiki.en</code></li>\n</ul>\n<h4>GloVe</h4>\n<p>The <a href=\"https://nlp.stanford.edu/projects/glove/\" rel=\"nofollow\">GloVe</a> embeddings are some kind of predecessor to FastText. In general choose FastText embeddings over GloVe. The dimension for the pre-trained embeddings varies.</p>\n<ul>\n<li>: e.g. <code>glove.6B.50d</code>, <a href=\"https://github.com/jfilter/text-classification-keras/blob/master/texcla/embeddings.py#L19\" rel=\"nofollow\">check out all avaiable embeddings</a></li>\n</ul>\n<h3>Tokenzation</h3>\n<ul>\n<li>To work on token (or word) level, use a TokenTokenizer such e.g. <code>TwokenizeTokenizer</code> or <code>SpacyTokenizer</code>.</li>\n<li>To work on token and sentence level, use <code>SpacySentenceTokenizer</code>.</li>\n<li>To create an custom Tokenizer, extend <code>Tokenizer</code> and implement the <code>token_generator</code> method.</li>\n</ul>\n<h4>Spacy</h4>\n<p>You may use <a href=\"https://spacy.io/\" rel=\"nofollow\">spaCy</a> for the tokenization. See instructions on how to\n<a href=\"https://spacy.io/docs/usage/models#download\" rel=\"nofollow\">download model</a> for your target language. E.g. for English:</p>\n<pre>python -m spacy download en\n</pre>\n<h3>Models</h3>\n<h4>Token-based Models</h4>\n<p>When working on token level, use <code>TokenModelFactory</code>.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">texcla.models</span> <span class=\"kn\">import</span> <span class=\"n\">TokenModelFactory</span><span class=\"p\">,</span> <span class=\"n\">YoonKimCNN</span>\n\n<span class=\"n\">factory</span> <span class=\"o\">=</span> <span class=\"n\">TokenModelFactory</span><span class=\"p\">(</span><span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">num_classes</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">token_index</span><span class=\"p\">,</span>\n    <span class=\"n\">max_tokens</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">embedding_type</span><span class=\"o\">=</span><span class=\"s1\">'glove.6B.100d'</span><span class=\"p\">)</span>\n<span class=\"n\">word_encoder_model</span> <span class=\"o\">=</span> <span class=\"n\">YoonKimCNN</span><span class=\"p\">()</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">factory</span><span class=\"o\">.</span><span class=\"n\">build_model</span><span class=\"p\">(</span><span class=\"n\">token_encoder_model</span><span class=\"o\">=</span><span class=\"n\">word_encoder_model</span><span class=\"p\">)</span>\n</pre>\n<p>Currently supported models include:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1408.5882\" rel=\"nofollow\">Yoon Kim CNN</a></li>\n<li><a href=\"https://arxiv.org/abs/1312.6026\" rel=\"nofollow\">Stacked RNNs</a></li>\n<li><a href=\"https://www.cs.cmu.edu/%7Ehovy/papers/16HLT-hierarchical-attention-networks.pdf\" rel=\"nofollow\">Attention (with/without context) based RNN encoders</a></li>\n</ul>\n<p><code>TokenModelFactory.build_model</code> uses the provided word encoder which is then classified via a <a href=\"https://keras.io/layers/core/#dense\" rel=\"nofollow\">Dense</a> layer.</p>\n<h4>Sentence-based Models</h4>\n<p>When working on sentence level, use <code>SentenceModelFactory</code>.</p>\n<pre><span class=\"c1\"># Pad max sentences per doc to 500 and max words per sentence to 200.</span>\n<span class=\"c1\"># Can also use `max_sents=None` to allow variable sized max_sents per mini-batch.</span>\n\n<span class=\"n\">factory</span> <span class=\"o\">=</span> <span class=\"n\">SentenceModelFactory</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">token_index</span><span class=\"p\">,</span> <span class=\"n\">max_sents</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">,</span>\n    <span class=\"n\">max_tokens</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span> <span class=\"n\">embedding_type</span><span class=\"o\">=</span><span class=\"s1\">'glove.6B.100d'</span><span class=\"p\">)</span>\n<span class=\"n\">word_encoder_model</span> <span class=\"o\">=</span> <span class=\"n\">AttentionRNN</span><span class=\"p\">()</span>\n<span class=\"n\">sentence_encoder_model</span> <span class=\"o\">=</span> <span class=\"n\">AttentionRNN</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Allows you to compose arbitrary word encoders followed by sentence encoder.</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">factory</span><span class=\"o\">.</span><span class=\"n\">build_model</span><span class=\"p\">(</span><span class=\"n\">word_encoder_model</span><span class=\"p\">,</span> <span class=\"n\">sentence_encoder_model</span><span class=\"p\">)</span>\n</pre>\n<ul>\n<li><a href=\"http://www.cs.cmu.edu/%7E./hovy/papers/16HLT-hierarchical-attention-networks.pdf\" rel=\"nofollow\">Hierarchical attention networks</a>\n(HANs) can be build by composing two attention based RNN models. This is useful when a document is very large.</li>\n<li>For smaller document a reasonable way to encode sentences is to average words within it. This can be done by using\n<code>token_encoder_model=AveragingEncoder()</code></li>\n<li>Mix and match encoders as you see fit for your problem.</li>\n</ul>\n<p><code>SentenceModelFactory.build_model</code> created a tiered model where words within a sentence is first encoded using\n<code>word_encoder_model</code>. All such encodings per sentence is then encoded using <code>sentence_encoder_model</code>.</p>\n<h2>Related</h2>\n<ul>\n<li><a href=\"https://github.com/brightmart/text_classification\" rel=\"nofollow\">https://github.com/brightmart/text_classification</a></li>\n<li><a href=\"https://github.com/allenai/allennlp\" rel=\"nofollow\">https://github.com/allenai/allennlp</a></li>\n<li><a href=\"https://github.com/facebookresearch/pytext\" rel=\"nofollow\">https://github.com/facebookresearch/pytext</a></li>\n<li><a href=\"https://docs.fast.ai/text.html\" rel=\"nofollow\">https://docs.fast.ai/text.html</a></li>\n<li><a href=\"https://github.com/dkpro/dkpro-tc\" rel=\"nofollow\">https://github.com/dkpro/dkpro-tc</a></li>\n</ul>\n<h2>Contributing</h2>\n<p>If you have a <strong>question</strong>, found a <strong>bug</strong> or want to propose a new <strong>feature</strong>, have a look at the <a href=\"https://github.com/jfilter/text-classification-keras/issues\" rel=\"nofollow\">issues page</a>.</p>\n<p><strong>Pull requests</strong> are especially welcomed when they fix bugs or improve the code quality.</p>\n<h2>Acknowledgements</h2>\n<p>Built upon the work by Raghavendra Kotikalapudi: <a href=\"https://github.com/raghakot/keras-text\" rel=\"nofollow\">keras-text</a>.</p>\n<h2>Citation</h2>\n<p>If you find Text Classification Keras useful for an academic publication, then please use the following BibTeX to cite it:</p>\n<pre>@misc<span class=\"nb\">{</span>raghakotfiltertexclakeras\n    title=<span class=\"nb\">{</span>Text Classification Keras<span class=\"nb\">}</span>,\n    author=<span class=\"nb\">{</span>Raghavendra Kotikalapudi, and Johannes Filter, and contributors<span class=\"nb\">}</span>,\n    year=<span class=\"nb\">{</span>2018<span class=\"nb\">}</span>,\n    publisher=<span class=\"nb\">{</span>GitHub<span class=\"nb\">}</span>,\n    howpublished=<span class=\"nb\">{</span><span class=\"k\">\\url</span><span class=\"nb\">{</span>https://github.com/jfilter/text-classification-keras<span class=\"nb\">}}</span>,\n<span class=\"nb\">}</span>\n</pre>\n<h2>License</h2>\n<p>MIT.</p>\n\n          </div>"}, "last_serial": 5200359, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "eefd96f6fdd562d590a7aeefc184b30c", "sha256": "abccb93005f7580003c163800bed464cd7225a5b8894dd1c37a7f8cbe887aaab"}, "downloads": -1, "filename": "text_classification_keras-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "eefd96f6fdd562d590a7aeefc184b30c", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 45978, "upload_time": "2018-08-01T13:52:34", "upload_time_iso_8601": "2018-08-01T13:52:34.554330Z", "url": "https://files.pythonhosted.org/packages/e7/83/0cb191006c208664d3cf3273acfdaf54047d21e15927cf26be323b3863d5/text_classification_keras-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6ec155c06e8f308890edaad31e93d6fa", "sha256": "8515277cd2d1209663005b97fe0abab6f36ca1e501954bbbfbd745f94523dee7"}, "downloads": -1, "filename": "text-classification-keras-0.1.0.tar.gz", "has_sig": false, "md5_digest": "6ec155c06e8f308890edaad31e93d6fa", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 38088, "upload_time": "2018-08-01T13:52:37", "upload_time_iso_8601": "2018-08-01T13:52:37.143789Z", "url": "https://files.pythonhosted.org/packages/87/7b/6c3543529c6f5621499ef01b45de871f2f932ba86c73c676ceafe73de0e5/text-classification-keras-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "1e9d369f772bb6d78a29c945d40ec493", "sha256": "8d4cec60614c65408969755b0caa1e152a8dfbe6fd0b2394c5f5bd8709eafb6b"}, "downloads": -1, "filename": "text_classification_keras-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "1e9d369f772bb6d78a29c945d40ec493", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 46178, "upload_time": "2018-09-12T22:44:38", "upload_time_iso_8601": "2018-09-12T22:44:38.109776Z", "url": "https://files.pythonhosted.org/packages/c2/1b/778dcab0ad3a0dac7d4116deb40e5582d9e196b825c228f97a1cb2180dd7/text_classification_keras-0.1.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "1cb05b2219e0a718eea1f20c300fae32", "sha256": "5a62febf19996d194d0edc60d73a0dcd32569337524245e78d3f86c7d8a407f3"}, "downloads": -1, "filename": "text_classification_keras-0.1.1-py3.7.egg", "has_sig": false, "md5_digest": "1cb05b2219e0a718eea1f20c300fae32", "packagetype": "bdist_egg", "python_version": "3.7", "requires_python": null, "size": 46661, "upload_time": "2018-11-01T13:05:44", "upload_time_iso_8601": "2018-11-01T13:05:44.045846Z", "url": "https://files.pythonhosted.org/packages/62/32/083d31cb458c34405942547c567d369bca843af9383098afc617e011fb9d/text_classification_keras-0.1.1-py3.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "d7cc412f2d0d9495c0d0f117be4761fc", "sha256": "8366aa50613842d9b656da38f979ffdcb19ab136b22d9f5758296b2451bd2585"}, "downloads": -1, "filename": "text-classification-keras-0.1.1.tar.gz", "has_sig": false, "md5_digest": "d7cc412f2d0d9495c0d0f117be4761fc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 38294, "upload_time": "2018-09-12T22:44:40", "upload_time_iso_8601": "2018-09-12T22:44:40.039840Z", "url": "https://files.pythonhosted.org/packages/2f/a5/5ad27bdb9d8b8c1244dcaaf40775280943dc1acceca9268cfc6ef1f7caf1/text-classification-keras-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "89d5ce9c0bcae55511bd7c51be0d5d17", "sha256": "206b75e233ff43cb06fb3610e0a22538ff398fa30ddfd40fb39dd1d68c744ea9"}, "downloads": -1, "filename": "text_classification_keras-0.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "89d5ce9c0bcae55511bd7c51be0d5d17", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 48747, "upload_time": "2018-11-01T13:05:40", "upload_time_iso_8601": "2018-11-01T13:05:40.427961Z", "url": "https://files.pythonhosted.org/packages/ae/ae/3e5704da0fbd90b2b3bc1553aa88811f90a5c27bb6f07bc6f5fc89ffda9d/text_classification_keras-0.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "91203d5e1095dee92a4055465f152f1c", "sha256": "492a50ff03617a04d9af65b95f5e6e15ec193ee2df029c6733146b4173a9b5d5"}, "downloads": -1, "filename": "text_classification_keras-0.1.2-py3.7.egg", "has_sig": false, "md5_digest": "91203d5e1095dee92a4055465f152f1c", "packagetype": "bdist_egg", "python_version": "3.7", "requires_python": null, "size": 46682, "upload_time": "2018-11-01T13:05:45", "upload_time_iso_8601": "2018-11-01T13:05:45.515632Z", "url": "https://files.pythonhosted.org/packages/46/da/e192d8f29159daee8f97e4ffd13d57be937ca303e39adcf30a85ca59edcb/text_classification_keras-0.1.2-py3.7.egg", "yanked": false}, {"comment_text": "", "digests": {"md5": "56c93ed1131045dd2cd1904f5b23035e", "sha256": "957ed5ae338f4f78f5ecabf7f133888897a57bec99402b9d1027ad23e185ec29"}, "downloads": -1, "filename": "text-classification-keras-0.1.2.tar.gz", "has_sig": false, "md5_digest": "56c93ed1131045dd2cd1904f5b23035e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 39701, "upload_time": "2018-11-01T13:05:42", "upload_time_iso_8601": "2018-11-01T13:05:42.765023Z", "url": "https://files.pythonhosted.org/packages/bd/11/d71cd71029cfa4885c7272c6c1456325f38cfa8f7e67b752bae537dba0c7/text-classification-keras-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "e0dc029a3fbb32b4eb5bf4e38ab7f892", "sha256": "d82eb06231c849831127522bea3cc1e65a37b446a5ed7fc1cf28cfd5c479264a"}, "downloads": -1, "filename": "text_classification_keras-0.1.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "e0dc029a3fbb32b4eb5bf4e38ab7f892", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 48742, "upload_time": "2018-12-14T15:50:56", "upload_time_iso_8601": "2018-12-14T15:50:56.735617Z", "url": "https://files.pythonhosted.org/packages/e1/bb/ae52983bc88af860e946446fc730c0a9b0488501b1a1a1787fa15b63ca9f/text_classification_keras-0.1.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "fd413abc0cdbf3657cd19f206ba1b9fc", "sha256": "56f12ca4cd3d99f46a410742e2aefaa72eae8bf5a393562e54794aeb413291f8"}, "downloads": -1, "filename": "text-classification-keras-0.1.3.tar.gz", "has_sig": false, "md5_digest": "fd413abc0cdbf3657cd19f206ba1b9fc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 40208, "upload_time": "2018-12-14T15:51:00", "upload_time_iso_8601": "2018-12-14T15:51:00.478106Z", "url": "https://files.pythonhosted.org/packages/ee/19/96d3a26818e89f3cf76978f23ae6df63c2d3cf6f4c7d77a43c471bd4227f/text-classification-keras-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "2fcae53bc0200aa3c20e0a80eab34d3b", "sha256": "8219e16304c4335ebcca0c1e6f7b121be0c2acb29f0aa25af4126feec1c89e51"}, "downloads": -1, "filename": "text_classification_keras-0.1.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "2fcae53bc0200aa3c20e0a80eab34d3b", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 50952, "upload_time": "2019-04-28T18:42:44", "upload_time_iso_8601": "2019-04-28T18:42:44.058779Z", "url": "https://files.pythonhosted.org/packages/10/4e/e37a46a62416881cb18cc164ec66c87ba288e7483df45302187e99c7762f/text_classification_keras-0.1.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "945859bdecfd224b95e37bef83b8dcac", "sha256": "10d4805b7b9d451dc9dbf3511fddd3cb8d4e14f5e2132a805d0636af1ddfbc26"}, "downloads": -1, "filename": "text-classification-keras-0.1.4.tar.gz", "has_sig": false, "md5_digest": "945859bdecfd224b95e37bef83b8dcac", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 42035, "upload_time": "2019-04-28T18:42:53", "upload_time_iso_8601": "2019-04-28T18:42:53.086781Z", "url": "https://files.pythonhosted.org/packages/af/c9/740d1446c409891996341feb894738d8212ffdf80df0cc18dbf5b95e22aa/text-classification-keras-0.1.4.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2fcae53bc0200aa3c20e0a80eab34d3b", "sha256": "8219e16304c4335ebcca0c1e6f7b121be0c2acb29f0aa25af4126feec1c89e51"}, "downloads": -1, "filename": "text_classification_keras-0.1.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "2fcae53bc0200aa3c20e0a80eab34d3b", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 50952, "upload_time": "2019-04-28T18:42:44", "upload_time_iso_8601": "2019-04-28T18:42:44.058779Z", "url": "https://files.pythonhosted.org/packages/10/4e/e37a46a62416881cb18cc164ec66c87ba288e7483df45302187e99c7762f/text_classification_keras-0.1.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "945859bdecfd224b95e37bef83b8dcac", "sha256": "10d4805b7b9d451dc9dbf3511fddd3cb8d4e14f5e2132a805d0636af1ddfbc26"}, "downloads": -1, "filename": "text-classification-keras-0.1.4.tar.gz", "has_sig": false, "md5_digest": "945859bdecfd224b95e37bef83b8dcac", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 42035, "upload_time": "2019-04-28T18:42:53", "upload_time_iso_8601": "2019-04-28T18:42:53.086781Z", "url": "https://files.pythonhosted.org/packages/af/c9/740d1446c409891996341feb894738d8212ffdf80df0cc18dbf5b95e22aa/text-classification-keras-0.1.4.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:55:05 2020"}