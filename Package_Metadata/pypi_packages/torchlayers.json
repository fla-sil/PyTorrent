{"info": {"author": "Szymon Maszke", "author_email": "szymon.maszke@protonmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 2 - Pre-Alpha", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: 3.8", "Topic :: Scientific/Engineering", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "![torchlayers Logo](https://github.com/szymonmaszke/torchlayers/blob/master/assets/banner.png)\n\n--------------------------------------------------------------------------------\n\n\n| Version | Docs | Tests | Coverage | Style | PyPI | Python | PyTorch | Docker |\n|---------|------|-------|----------|-------|------|--------|---------|--------|\n| [![Version](https://img.shields.io/static/v1?label=&message=0.1.1&color=377EF0&style=for-the-badge)](https://github.com/szymonmaszke/torchlayers/releases) | [![Documentation](https://img.shields.io/static/v1?label=&message=docs&color=EE4C2C&style=for-the-badge)](https://szymonmaszke.github.io/torchlayers/)  | ![Tests](https://github.com/szymonmaszke/torchlayers/workflows/test/badge.svg) | [![codecov](https://codecov.io/gh/szymonmaszke/torchlayers/branch/master/graph/badge.svg?token=GbZmdqbTWM)](https://codecov.io/gh/szymonmaszke/torchlayers) | [![codebeat badge](https://codebeat.co/badges/0e3d33b0-95a4-429c-8692-881a4ffeac6b)](https://codebeat.co/projects/github-com-szymonmaszke-torchlayers-master) | [![PyPI](https://img.shields.io/static/v1?label=&message=PyPI&color=377EF0&style=for-the-badge)](https://pypi.org/project/torchlayers/) | [![Python](https://img.shields.io/static/v1?label=&message=>=3.7&color=377EF0&style=for-the-badge&logo=python&logoColor=F8C63D)](https://www.python.org/) | [![PyTorch](https://img.shields.io/static/v1?label=&message=>=1.3.0&color=EE4C2C&style=for-the-badge)](https://pytorch.org/) | [![Docker](https://img.shields.io/static/v1?label=&message=docker&color=309cef&style=for-the-badge)](https://cloud.docker.com/u/szymonmaszke/repository/docker/szymonmaszke/torchlayers) |\n\n[__torchlayers__](https://szymonmaszke.github.io/torchlayers/) is a library based on [__PyTorch__](https://pytorch.org/)\nproviding __automatic shape and dimensionality inference of `torch.nn` layers__ + additional\nbuilding blocks featured in current SOTA architectures (e.g. [Efficient-Net](https://arxiv.org/abs/1905.11946)).\n\nAbove requires no user intervention (except single call to `torchlayers.build`)\nsimilarly to the one seen in [__Keras__](https://www.tensorflow.org/guide/keras).\n\n### Main functionalities:\n\n* __Shape inference__ for most of `torch.nn` module (__convolutional, recurrent, transformer, attention and linear layers__)\n* __Dimensionality inference__ (e.g. `torchlayers.Conv` working as `torch.nn.Conv1d/2d/3d` based on `input shape`)\n* __Shape inference of custom modules__ (see examples section)\n* __Additional [Keras-like](https://www.tensorflow.org/guide/keras) layers__ (e.g. `torchlayers.Reshape` or `torchlayers.StandardNormalNoise`)\n* __Additional SOTA layers__ mostly from ImageNet competitions\n(e.g. [PolyNet](https://arxiv.org/abs/1608.06993),\n[Squeeze-And-Excitation](https://arxiv.org/abs/1709.01507),\n[StochasticDepth](www.arxiv.org/abs/1512.03385>))\n* __Useful defaults__ (`\"same\"` padding and default `kernel_size=3` for `Conv`, dropout rates etc.)\n* __Zero overhead and [torchscript](https://pytorch.org/docs/stable/jit.html) support__\n\n# Examples\n\nFor full functionality please check [__torchlayers documentation__](https://img.shields.io/static/v1?label=&message=docs&color=EE4C2C&style=for-the-badge).\nBelow examples should introduce all necessary concepts you should know.\n\n## Simple convolutional image and text classifier\n\n* We will use single \"model\" for both tasks.\nFirstly let's define it using `torch.nn` and `torchlayers`:\n\n```python\nimport torch\nimport torchlayers\n\n# torch.nn and torchlayers can be mixed easily\nmodel = torch.nn.Sequential(\n    torchlayers.Conv(64),  # specify ONLY out_channels\n    torch.nn.ReLU(),  # use torch.nn wherever you wish\n    torchlayers.BatchNorm(),  # BatchNormNd inferred from input\n    torchlayers.Conv(128),  # Default kernel_size equal to 3\n    torchlayers.ReLU(),\n    torchlayers.Conv(256, kernel_size=11),  # \"same\" padding as default\n    torchlayers.GlobalMaxPool(),  # Known from Keras\n    torchlayers.Linear(10),  # Output for 10 classes\n)\n\nprint(model)\n```\n\nAbove would give you model's summary like this (__notice question marks for not yet inferred values__):\n\n```python\nSequential(\n  (0): Conv(in_channels=?, out_channels=64, kernel_size=3, stride=1, padding=same, dilation=1, groups=1, bias=True, padding_mode=zeros)\n  (1): ReLU()\n  (2): BatchNorm(num_features=?, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Conv(in_channels=?, out_channels=128, kernel_size=3, stride=1, padding=same, dilation=1, groups=1, bias=True, padding_mode=zeros)\n  (4): ReLU()\n  (5): Conv(in_channels=?, out_channels=256, kernel_size=11, stride=1, padding=same, dilation=1, groups=1, bias=True, padding_mode=zeros)\n  (6): GlobalMaxPool()\n  (7): Linear(in_features=?, out_features=10, bias=True)\n)\n```\n\n* Now you can __build__/instantiate your model with example input (in this case MNIST-like):\n\n```python\nmnist_model = torchlayers.build(model, torch.randn(1, 3, 28, 28))\n```\n\n* Or if it's text classification you are after, same model could be built with different\n`input shape` (e.g. for text classification using `300` dimensional pretrained embedding):\n\n```python\n# [batch, embedding, timesteps], first dimension > 1 for BatchNorm1d to work\ntext_model = torchlayers.build(model, torch.randn(2, 300, 1))\n```\n\n* Finally, you can `print` both models after instantiation, provided below side\nby-side for readability (__notice different dimenstionality, e.g. `Conv2d` vs `Conv1d` after `torchlayers.build`__):\n\n```python\n                # MNIST CLASSIFIER                TEXT CLASSIFIER\n\n                Sequential(                       Sequential(\n                  (0): Conv1d(300, 64)              (0): Conv2d(3, 64)\n                  (1): ReLU()                       (1): ReLU()\n                  (2): BatchNorm1d(64)              (2): BatchNorm2d(64)\n                  (3): Conv1d(64, 128)              (3): Conv2d(64, 128)\n                  (4): ReLU()                       (4): ReLU()\n                  (5): Conv1d(128, 256)             (5): Conv2d(128, 256)\n                  (6): GlobalMaxPool()              (6): GlobalMaxPool()\n                  (7): Linear(256, 10)              (7): Linear(256, 10)\n                )                                 )\n```\n\nAs you can see both modules \"compiled\" into original `pytorch` layers.\n\n## Custom modules with shape inference capabilities\n\nUser can define any module and make it shape inferable with `torchlayers.infer`\nfunction:\n\n```python\n # Class defined with in_features\n # It might be a good practice to use _ prefix and Impl as postfix\n # to differentiate from shape inferable version\nclass _MyLinearImpl(torch.nn.Module):\n    def __init__(self, in_features: int, out_features: int):\n        super().__init__()\n        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = torch.nn.Parameter(torch.randn(out_features))\n\n    def forward(self, inputs):\n        return torch.nn.functional.linear(inputs, self.weight, self.bias)\n\nMyLinear = torchlayers.infer(_MyLinearImpl)\n\n# Build and use just like any other layer in this library\nlayer =torchlayers.build(MyLinear(out_features=32), torch.randn(1, 64))\nlayer(torch.randn(1, 64))\n```\n\nBy default `inputs.shape[1]` will be used as `in_features` value\nduring initial `forward` pass. If you wish to use different `index` (e.g. to infer using\n`inputs.shape[3]`) use `MyLayer = torchlayers.infer(_MyLayerImpl, index=3)` as a decorator.\n\n## Autoencoder with inverted residual bottleneck and pixel shuffle\n\nPlease check code comments and [__documentation__](https://img.shields.io/static/v1?label=&message=docs&color=EE4C2C&style=for-the-badge)\nif needed. If you are unsure what autoencoder is you could see\n[__this example blog post__](https://towardsdatascience.com/auto-encoder-what-is-it-and-what-is-it-used-for-part-1-3e5c6f017726).\n\nBelow is a convolutional denoising autoencoder example for `ImageNet`-like images.\nThink of it like a demonstration of capabilities of different layers\nand building blocks provided by `torchlayers`.\n\n\n```python\n# Input - 3 x 256 x 256 for ImageNet reconstruction\nclass AutoEncoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = torchlayers.Sequential(\n            torchlayers.StandardNormalNoise(),  # Apply noise to input images\n            torchlayers.Conv(64, kernel_size=7),\n            torchlayers.activations.Swish(),  # Direct access to module .activations\n            torchlayers.InvertedResidualBottleneck(squeeze_excitation=False),\n            torchlayers.AvgPool(),  # shape 64 x 128 x 128, kernel_size=2 by default\n            torchlayers.HardSwish(),  # Access simply through torchlayers\n            torchlayers.SeparableConv(128),  # Up number of channels to 128\n            torchlayers.InvertedResidualBottleneck(),  # Default with squeeze excitation\n            torch.nn.ReLU(),\n            torchlayers.AvgPool(),  # shape 128 x 64 x 64, kernel_size=2 by default\n            torchlayers.DepthwiseConv(256),  # DepthwiseConv easier to use\n            # Pass input thrice through the same weights like in PolyNet\n            torchlayers.Poly(torchlayers.InvertedResidualBottleneck(), order=3),\n            torchlayers.ReLU(),  # all torch.nn can be accessed via torchlayers\n            torchlayers.MaxPool(),  # shape 256 x 32 x 32\n            torchlayers.Fire(out_channels=512),  # shape 512 x 32 x 32\n            torchlayers.SqueezeExcitation(hidden=64),\n            torchlayers.InvertedResidualBottleneck(),\n            torchlayers.MaxPool(),  # shape 512 x 16 x 16\n            torchlayers.InvertedResidualBottleneck(squeeze_excitation=False),\n            # Randomly switch off the last two layers with 0.5 probability\n            torchlayers.StochasticDepth(\n                torch.nn.Sequential(\n                    torchlayers.InvertedResidualBottleneck(squeeze_excitation=False),\n                    torchlayers.InvertedResidualBottleneck(squeeze_excitation=False),\n                ),\n                p=0.5,\n            ),\n            torchlayers.AvgPool(),  # shape 512 x 8 x 8\n        )\n\n        # This one is more \"standard\"\n        self.decoder = torchlayers.Sequential(\n            torchlayers.Poly(torchlayers.InvertedResidualBottleneck(), order=2),\n            # Has ICNR initialization by default after calling `build`\n            torchlayers.ConvPixelShuffle(out_channels=512, upscale_factor=2),\n            # Shape 512 x 16 x 16 after PixelShuffle\n            torchlayers.Poly(torchlayers.InvertedResidualBottleneck(), order=3),\n            torchlayers.ConvPixelShuffle(out_channels=256, upscale_factor=2),\n            # Shape 256 x 32 x 32\n            torchlayers.Poly(torchlayers.InvertedResidualBottleneck(), order=3),\n            torchlayers.ConvPixelShuffle(out_channels=128, upscale_factor=2),\n            # Shape 128 x 64 x 64\n            torchlayers.Poly(torchlayers.InvertedResidualBottleneck(), order=4),\n            torchlayers.ConvPixelShuffle(out_channels=64, upscale_factor=2),\n            # Shape 64 x 128 x 128\n            torchlayers.InvertedResidualBottleneck(),\n            torchlayers.Conv(256),\n            torchlayers.Dropout(),  # Defaults to 0.5 and Dropout2d for images\n            torchlayers.Swish(),\n            torchlayers.InstanceNorm(),\n            torchlayers.ConvPixelShuffle(out_channels=32, upscale_factor=2),\n            # Shape 32 x 256 x 256\n            torchlayers.Conv(16),\n            torchlayers.Swish(),\n            torchlayers.Conv(3),\n            # Shape 3 x 256 x 256\n        )\n\n    def forward(self, inputs):\n        return self.decoder(self.encoder(inputs))\n```\n\nNow one can instantiate the module and use it with `torch.nn.MSELoss` as per usual.\n\n```python\nautoencoder = torchlayers.build(AutoEncoder(), torch.randn(1, 3, 256, 256))\n```\n\n# Installation\n\n## [pip](<https://pypi.org/project/torchlayers/>)\n\n### Latest release:\n\n```shell\npip install --user torchlayers\n```\n\n### Nightly:\n\n```shell\npip install --user torchlayers-nightly\n```\n\n## [Docker](https://cloud.docker.com/repository/docker/szymonmaszke/torchlayers)\n\n__CPU standalone__ and various versions of __GPU enabled__ images are available\nat [dockerhub](https://cloud.docker.com/repository/docker/szymonmaszke/torchlayers).\n\nFor CPU quickstart, issue:\n\n```shell\ndocker pull szymonmaszke/torchlayers:18.04\n```\n\nNightly builds are also available, just prefix tag with `nightly_`. If you are going for `GPU` image make sure you have\n[nvidia/docker](https://github.com/NVIDIA/nvidia-docker) installed and it's runtime set.\n\n# Contributing\n\nIf you find issue or would like to see some functionality (or implement one), please [open new Issue](https://help.github.com/en/articles/creating-an-issue) or [create Pull Request](https://help.github.com/en/articles/creating-a-pull-request-from-a-fork).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/pypa/torchlayers", "keywords": "pytorch keras input inference automatic shape layers sota custom imagenet resnet efficientnet", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "torchlayers", "package_url": "https://pypi.org/project/torchlayers/", "platform": "", "project_url": "https://pypi.org/project/torchlayers/", "project_urls": {"Documentation": "https://szymonmaszke.github.io/torchlayers/#torchlayers", "Homepage": "https://github.com/pypa/torchlayers", "Issues": "https://github.com/szymonmaszke/torchlayers/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc", "Website": "https://szymonmaszke.github.io/torchlayers"}, "release_url": "https://pypi.org/project/torchlayers/0.1.1/", "requires_dist": ["torch (>=1.3.0)"], "requires_python": ">=3.7", "summary": "Input shape inference and SOTA custom layers for PyTorch.", "version": "0.1.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <p><img alt=\"torchlayers Logo\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/dd4c4c93cbb6270eea9812fb7af662715bd5b71d/68747470733a2f2f6769746875622e636f6d2f737a796d6f6e6d61737a6b652f746f7263686c61796572732f626c6f622f6d61737465722f6173736574732f62616e6e65722e706e67\"></p>\n<hr>\n<table>\n<thead>\n<tr>\n<th>Version</th>\n<th>Docs</th>\n<th>Tests</th>\n<th>Coverage</th>\n<th>Style</th>\n<th>PyPI</th>\n<th>Python</th>\n<th>PyTorch</th>\n<th>Docker</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/szymonmaszke/torchlayers/releases\" rel=\"nofollow\"><img alt=\"Version\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/3f6b5ce473a6927500ca9c3ac8e20df046106bb6/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d266d6573736167653d302e312e3126636f6c6f723d333737454630267374796c653d666f722d7468652d6261646765\"></a></td>\n<td><a href=\"https://szymonmaszke.github.io/torchlayers/\" rel=\"nofollow\"><img alt=\"Documentation\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/611e23036e6193a0da20bfe66381e1f1c7ff1a62/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d266d6573736167653d646f637326636f6c6f723d454534433243267374796c653d666f722d7468652d6261646765\"></a></td>\n<td><img alt=\"Tests\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2f586b05e49f920e9ea2e19723de70eb34af4e87/68747470733a2f2f6769746875622e636f6d2f737a796d6f6e6d61737a6b652f746f7263686c61796572732f776f726b666c6f77732f746573742f62616467652e737667\"></td>\n<td><a href=\"https://codecov.io/gh/szymonmaszke/torchlayers\" rel=\"nofollow\"><img alt=\"codecov\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ec31769c9cedd0eb2f5b8ffe329c4b437c50293c/68747470733a2f2f636f6465636f762e696f2f67682f737a796d6f6e6d61737a6b652f746f7263686c61796572732f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d47625a6d64716254574d\"></a></td>\n<td><a href=\"https://codebeat.co/projects/github-com-szymonmaszke-torchlayers-master\" rel=\"nofollow\"><img alt=\"codebeat badge\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fdc871012e063d5495e3712b98efe35f023b511c/68747470733a2f2f636f6465626561742e636f2f6261646765732f30653364333362302d393561342d343239632d383639322d383831613466666561633662\"></a></td>\n<td><a href=\"https://pypi.org/project/torchlayers/\" rel=\"nofollow\"><img alt=\"PyPI\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8215dcd37002b6dcc738f2b879bfe91408b52b43/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d266d6573736167653d5079504926636f6c6f723d333737454630267374796c653d666f722d7468652d6261646765\"></a></td>\n<td><a href=\"https://www.python.org/\" rel=\"nofollow\"><img alt=\"Python\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a30b1b56ad502322578a21315c4d7122638b8bf6/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d266d6573736167653d2533453d332e3726636f6c6f723d333737454630267374796c653d666f722d7468652d6261646765266c6f676f3d707974686f6e266c6f676f436f6c6f723d463843363344\"></a></td>\n<td><a href=\"https://pytorch.org/\" rel=\"nofollow\"><img alt=\"PyTorch\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/8fdfc6038c71671a2da2f77af4d8dbf8b3a4709f/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d266d6573736167653d2533453d312e332e3026636f6c6f723d454534433243267374796c653d666f722d7468652d6261646765\"></a></td>\n<td><a href=\"https://cloud.docker.com/u/szymonmaszke/repository/docker/szymonmaszke/torchlayers\" rel=\"nofollow\"><img alt=\"Docker\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/72872e5a621755bfa1dcadaa1447fb18ac0ad0e8/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d266d6573736167653d646f636b657226636f6c6f723d333039636566267374796c653d666f722d7468652d6261646765\"></a></td>\n</tr></tbody></table>\n<p><a href=\"https://szymonmaszke.github.io/torchlayers/\" rel=\"nofollow\"><strong>torchlayers</strong></a> is a library based on <a href=\"https://pytorch.org/\" rel=\"nofollow\"><strong>PyTorch</strong></a>\nproviding <strong>automatic shape and dimensionality inference of <code>torch.nn</code> layers</strong> + additional\nbuilding blocks featured in current SOTA architectures (e.g. <a href=\"https://arxiv.org/abs/1905.11946\" rel=\"nofollow\">Efficient-Net</a>).</p>\n<p>Above requires no user intervention (except single call to <code>torchlayers.build</code>)\nsimilarly to the one seen in <a href=\"https://www.tensorflow.org/guide/keras\" rel=\"nofollow\"><strong>Keras</strong></a>.</p>\n<h3>Main functionalities:</h3>\n<ul>\n<li><strong>Shape inference</strong> for most of <code>torch.nn</code> module (<strong>convolutional, recurrent, transformer, attention and linear layers</strong>)</li>\n<li><strong>Dimensionality inference</strong> (e.g. <code>torchlayers.Conv</code> working as <code>torch.nn.Conv1d/2d/3d</code> based on <code>input shape</code>)</li>\n<li><strong>Shape inference of custom modules</strong> (see examples section)</li>\n<li><strong>Additional <a href=\"https://www.tensorflow.org/guide/keras\" rel=\"nofollow\">Keras-like</a> layers</strong> (e.g. <code>torchlayers.Reshape</code> or <code>torchlayers.StandardNormalNoise</code>)</li>\n<li><strong>Additional SOTA layers</strong> mostly from ImageNet competitions\n(e.g. <a href=\"https://arxiv.org/abs/1608.06993\" rel=\"nofollow\">PolyNet</a>,\n<a href=\"https://arxiv.org/abs/1709.01507\" rel=\"nofollow\">Squeeze-And-Excitation</a>,\n<a href=\"www.arxiv.org/abs/1512.03385%3E\" rel=\"nofollow\">StochasticDepth</a>)</li>\n<li><strong>Useful defaults</strong> (<code>\"same\"</code> padding and default <code>kernel_size=3</code> for <code>Conv</code>, dropout rates etc.)</li>\n<li><strong>Zero overhead and <a href=\"https://pytorch.org/docs/stable/jit.html\" rel=\"nofollow\">torchscript</a> support</strong></li>\n</ul>\n<h1>Examples</h1>\n<p>For full functionality please check <a href=\"https://img.shields.io/static/v1?label=&amp;message=docs&amp;color=EE4C2C&amp;style=for-the-badge\" rel=\"nofollow\"><strong>torchlayers documentation</strong></a>.\nBelow examples should introduce all necessary concepts you should know.</p>\n<h2>Simple convolutional image and text classifier</h2>\n<ul>\n<li>We will use single \"model\" for both tasks.\nFirstly let's define it using <code>torch.nn</code> and <code>torchlayers</code>:</li>\n</ul>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchlayers</span>\n\n<span class=\"c1\"># torch.nn and torchlayers can be mixed easily</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n    <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"mi\">64</span><span class=\"p\">),</span>  <span class=\"c1\"># specify ONLY out_channels</span>\n    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>  <span class=\"c1\"># use torch.nn wherever you wish</span>\n    <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">BatchNorm</span><span class=\"p\">(),</span>  <span class=\"c1\"># BatchNormNd inferred from input</span>\n    <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">),</span>  <span class=\"c1\"># Default kernel_size equal to 3</span>\n    <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n    <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">11</span><span class=\"p\">),</span>  <span class=\"c1\"># \"same\" padding as default</span>\n    <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">GlobalMaxPool</span><span class=\"p\">(),</span>  <span class=\"c1\"># Known from Keras</span>\n    <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">),</span>  <span class=\"c1\"># Output for 10 classes</span>\n<span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n</pre>\n<p>Above would give you model's summary like this (<strong>notice question marks for not yet inferred values</strong>):</p>\n<pre><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n  <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">):</span> <span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"n\">in_channels</span><span class=\"o\">=</span><span class=\"err\">?</span><span class=\"p\">,</span> <span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"n\">same</span><span class=\"p\">,</span> <span class=\"n\">dilation</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">groups</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">padding_mode</span><span class=\"o\">=</span><span class=\"n\">zeros</span><span class=\"p\">)</span>\n  <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">):</span> <span class=\"n\">ReLU</span><span class=\"p\">()</span>\n  <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">):</span> <span class=\"n\">BatchNorm</span><span class=\"p\">(</span><span class=\"n\">num_features</span><span class=\"o\">=</span><span class=\"err\">?</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-05</span><span class=\"p\">,</span> <span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">affine</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">track_running_stats</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n  <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">):</span> <span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"n\">in_channels</span><span class=\"o\">=</span><span class=\"err\">?</span><span class=\"p\">,</span> <span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"n\">same</span><span class=\"p\">,</span> <span class=\"n\">dilation</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">groups</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">padding_mode</span><span class=\"o\">=</span><span class=\"n\">zeros</span><span class=\"p\">)</span>\n  <span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">):</span> <span class=\"n\">ReLU</span><span class=\"p\">()</span>\n  <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">):</span> <span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"n\">in_channels</span><span class=\"o\">=</span><span class=\"err\">?</span><span class=\"p\">,</span> <span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">11</span><span class=\"p\">,</span> <span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"n\">same</span><span class=\"p\">,</span> <span class=\"n\">dilation</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">groups</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">padding_mode</span><span class=\"o\">=</span><span class=\"n\">zeros</span><span class=\"p\">)</span>\n  <span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">):</span> <span class=\"n\">GlobalMaxPool</span><span class=\"p\">()</span>\n  <span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">):</span> <span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">in_features</span><span class=\"o\">=</span><span class=\"err\">?</span><span class=\"p\">,</span> <span class=\"n\">out_features</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n</pre>\n<ul>\n<li>Now you can <strong>build</strong>/instantiate your model with example input (in this case MNIST-like):</li>\n</ul>\n<pre><span class=\"n\">mnist_model</span> <span class=\"o\">=</span> <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">build</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">))</span>\n</pre>\n<ul>\n<li>Or if it's text classification you are after, same model could be built with different\n<code>input shape</code> (e.g. for text classification using <code>300</code> dimensional pretrained embedding):</li>\n</ul>\n<pre><span class=\"c1\"># [batch, embedding, timesteps], first dimension &gt; 1 for BatchNorm1d to work</span>\n<span class=\"n\">text_model</span> <span class=\"o\">=</span> <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">build</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">300</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n</pre>\n<ul>\n<li>Finally, you can <code>print</code> both models after instantiation, provided below side\nby-side for readability (<strong>notice different dimenstionality, e.g. <code>Conv2d</code> vs <code>Conv1d</code> after <code>torchlayers.build</code></strong>):</li>\n</ul>\n<pre>                <span class=\"c1\"># MNIST CLASSIFIER                TEXT CLASSIFIER</span>\n\n                <span class=\"n\">Sequential</span><span class=\"p\">(</span>                       <span class=\"n\">Sequential</span><span class=\"p\">(</span>\n                  <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">):</span> <span class=\"n\">Conv1d</span><span class=\"p\">(</span><span class=\"mi\">300</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>              <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">):</span> <span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>\n                  <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">):</span> <span class=\"n\">ReLU</span><span class=\"p\">()</span>                       <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">):</span> <span class=\"n\">ReLU</span><span class=\"p\">()</span>\n                  <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">):</span> <span class=\"n\">BatchNorm1d</span><span class=\"p\">(</span><span class=\"mi\">64</span><span class=\"p\">)</span>              <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">):</span> <span class=\"n\">BatchNorm2d</span><span class=\"p\">(</span><span class=\"mi\">64</span><span class=\"p\">)</span>\n                  <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">):</span> <span class=\"n\">Conv1d</span><span class=\"p\">(</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">)</span>              <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">):</span> <span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">)</span>\n                  <span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">):</span> <span class=\"n\">ReLU</span><span class=\"p\">()</span>                       <span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">):</span> <span class=\"n\">ReLU</span><span class=\"p\">()</span>\n                  <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">):</span> <span class=\"n\">Conv1d</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">)</span>             <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">):</span> <span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">)</span>\n                  <span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">):</span> <span class=\"n\">GlobalMaxPool</span><span class=\"p\">()</span>              <span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">):</span> <span class=\"n\">GlobalMaxPool</span><span class=\"p\">()</span>\n                  <span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">):</span> <span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>              <span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">):</span> <span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n                <span class=\"p\">)</span>                                 <span class=\"p\">)</span>\n</pre>\n<p>As you can see both modules \"compiled\" into original <code>pytorch</code> layers.</p>\n<h2>Custom modules with shape inference capabilities</h2>\n<p>User can define any module and make it shape inferable with <code>torchlayers.infer</code>\nfunction:</p>\n<pre> <span class=\"c1\"># Class defined with in_features</span>\n <span class=\"c1\"># It might be a good practice to use _ prefix and Impl as postfix</span>\n <span class=\"c1\"># to differentiate from shape inferable version</span>\n<span class=\"k\">class</span> <span class=\"nc\">_MyLinearImpl</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">in_features</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">out_features</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">weight</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Parameter</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">out_features</span><span class=\"p\">,</span> <span class=\"n\">in_features</span><span class=\"p\">))</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">bias</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Parameter</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">out_features</span><span class=\"p\">))</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">functional</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">bias</span><span class=\"p\">)</span>\n\n<span class=\"n\">MyLinear</span> <span class=\"o\">=</span> <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">infer</span><span class=\"p\">(</span><span class=\"n\">_MyLinearImpl</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Build and use just like any other layer in this library</span>\n<span class=\"n\">layer</span> <span class=\"o\">=</span><span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">build</span><span class=\"p\">(</span><span class=\"n\">MyLinear</span><span class=\"p\">(</span><span class=\"n\">out_features</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">),</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>\n<span class=\"n\">layer</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>\n</pre>\n<p>By default <code>inputs.shape[1]</code> will be used as <code>in_features</code> value\nduring initial <code>forward</code> pass. If you wish to use different <code>index</code> (e.g. to infer using\n<code>inputs.shape[3]</code>) use <code>MyLayer = torchlayers.infer(_MyLayerImpl, index=3)</code> as a decorator.</p>\n<h2>Autoencoder with inverted residual bottleneck and pixel shuffle</h2>\n<p>Please check code comments and <a href=\"https://img.shields.io/static/v1?label=&amp;message=docs&amp;color=EE4C2C&amp;style=for-the-badge\" rel=\"nofollow\"><strong>documentation</strong></a>\nif needed. If you are unsure what autoencoder is you could see\n<a href=\"https://towardsdatascience.com/auto-encoder-what-is-it-and-what-is-it-used-for-part-1-3e5c6f017726\" rel=\"nofollow\"><strong>this example blog post</strong></a>.</p>\n<p>Below is a convolutional denoising autoencoder example for <code>ImageNet</code>-like images.\nThink of it like a demonstration of capabilities of different layers\nand building blocks provided by <code>torchlayers</code>.</p>\n<pre><span class=\"c1\"># Input - 3 x 256 x 256 for ImageNet reconstruction</span>\n<span class=\"k\">class</span> <span class=\"nc\">AutoEncoder</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span> <span class=\"o\">=</span> <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">StandardNormalNoise</span><span class=\"p\">(),</span>  <span class=\"c1\"># Apply noise to input images</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">7</span><span class=\"p\">),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">activations</span><span class=\"o\">.</span><span class=\"n\">Swish</span><span class=\"p\">(),</span>  <span class=\"c1\"># Direct access to module .activations</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">InvertedResidualBottleneck</span><span class=\"p\">(</span><span class=\"n\">squeeze_excitation</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">AvgPool</span><span class=\"p\">(),</span>  <span class=\"c1\"># shape 64 x 128 x 128, kernel_size=2 by default</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">HardSwish</span><span class=\"p\">(),</span>  <span class=\"c1\"># Access simply through torchlayers</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">SeparableConv</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">),</span>  <span class=\"c1\"># Up number of channels to 128</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">InvertedResidualBottleneck</span><span class=\"p\">(),</span>  <span class=\"c1\"># Default with squeeze excitation</span>\n            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">AvgPool</span><span class=\"p\">(),</span>  <span class=\"c1\"># shape 128 x 64 x 64, kernel_size=2 by default</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">DepthwiseConv</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">),</span>  <span class=\"c1\"># DepthwiseConv easier to use</span>\n            <span class=\"c1\"># Pass input thrice through the same weights like in PolyNet</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Poly</span><span class=\"p\">(</span><span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">InvertedResidualBottleneck</span><span class=\"p\">(),</span> <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>  <span class=\"c1\"># all torch.nn can be accessed via torchlayers</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">MaxPool</span><span class=\"p\">(),</span>  <span class=\"c1\"># shape 256 x 32 x 32</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Fire</span><span class=\"p\">(</span><span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">),</span>  <span class=\"c1\"># shape 512 x 32 x 32</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">SqueezeExcitation</span><span class=\"p\">(</span><span class=\"n\">hidden</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">InvertedResidualBottleneck</span><span class=\"p\">(),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">MaxPool</span><span class=\"p\">(),</span>  <span class=\"c1\"># shape 512 x 16 x 16</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">InvertedResidualBottleneck</span><span class=\"p\">(</span><span class=\"n\">squeeze_excitation</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">),</span>\n            <span class=\"c1\"># Randomly switch off the last two layers with 0.5 probability</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">StochasticDepth</span><span class=\"p\">(</span>\n                <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n                    <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">InvertedResidualBottleneck</span><span class=\"p\">(</span><span class=\"n\">squeeze_excitation</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">),</span>\n                    <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">InvertedResidualBottleneck</span><span class=\"p\">(</span><span class=\"n\">squeeze_excitation</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">),</span>\n                <span class=\"p\">),</span>\n                <span class=\"n\">p</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span>\n            <span class=\"p\">),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">AvgPool</span><span class=\"p\">(),</span>  <span class=\"c1\"># shape 512 x 8 x 8</span>\n        <span class=\"p\">)</span>\n\n        <span class=\"c1\"># This one is more \"standard\"</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span> <span class=\"o\">=</span> <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Poly</span><span class=\"p\">(</span><span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">InvertedResidualBottleneck</span><span class=\"p\">(),</span> <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"c1\"># Has ICNR initialization by default after calling `build`</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">ConvPixelShuffle</span><span class=\"p\">(</span><span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">upscale_factor</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"c1\"># Shape 512 x 16 x 16 after PixelShuffle</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Poly</span><span class=\"p\">(</span><span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">InvertedResidualBottleneck</span><span class=\"p\">(),</span> <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">ConvPixelShuffle</span><span class=\"p\">(</span><span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">upscale_factor</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"c1\"># Shape 256 x 32 x 32</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Poly</span><span class=\"p\">(</span><span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">InvertedResidualBottleneck</span><span class=\"p\">(),</span> <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">ConvPixelShuffle</span><span class=\"p\">(</span><span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">upscale_factor</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"c1\"># Shape 128 x 64 x 64</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Poly</span><span class=\"p\">(</span><span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">InvertedResidualBottleneck</span><span class=\"p\">(),</span> <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">ConvPixelShuffle</span><span class=\"p\">(</span><span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">upscale_factor</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"c1\"># Shape 64 x 128 x 128</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">InvertedResidualBottleneck</span><span class=\"p\">(),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Dropout</span><span class=\"p\">(),</span>  <span class=\"c1\"># Defaults to 0.5 and Dropout2d for images</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Swish</span><span class=\"p\">(),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">InstanceNorm</span><span class=\"p\">(),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">ConvPixelShuffle</span><span class=\"p\">(</span><span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"n\">upscale_factor</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"c1\"># Shape 32 x 256 x 256</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Swish</span><span class=\"p\">(),</span>\n            <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n            <span class=\"c1\"># Shape 3 x 256 x 256</span>\n        <span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">inputs</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">decoder</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">encoder</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"p\">))</span>\n</pre>\n<p>Now one can instantiate the module and use it with <code>torch.nn.MSELoss</code> as per usual.</p>\n<pre><span class=\"n\">autoencoder</span> <span class=\"o\">=</span> <span class=\"n\">torchlayers</span><span class=\"o\">.</span><span class=\"n\">build</span><span class=\"p\">(</span><span class=\"n\">AutoEncoder</span><span class=\"p\">(),</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">))</span>\n</pre>\n<h1>Installation</h1>\n<h2><a href=\"https://pypi.org/project/torchlayers/\" rel=\"nofollow\">pip</a></h2>\n<h3>Latest release:</h3>\n<pre>pip install --user torchlayers\n</pre>\n<h3>Nightly:</h3>\n<pre>pip install --user torchlayers-nightly\n</pre>\n<h2><a href=\"https://cloud.docker.com/repository/docker/szymonmaszke/torchlayers\" rel=\"nofollow\">Docker</a></h2>\n<p><strong>CPU standalone</strong> and various versions of <strong>GPU enabled</strong> images are available\nat <a href=\"https://cloud.docker.com/repository/docker/szymonmaszke/torchlayers\" rel=\"nofollow\">dockerhub</a>.</p>\n<p>For CPU quickstart, issue:</p>\n<pre>docker pull szymonmaszke/torchlayers:18.04\n</pre>\n<p>Nightly builds are also available, just prefix tag with <code>nightly_</code>. If you are going for <code>GPU</code> image make sure you have\n<a href=\"https://github.com/NVIDIA/nvidia-docker\" rel=\"nofollow\">nvidia/docker</a> installed and it's runtime set.</p>\n<h1>Contributing</h1>\n<p>If you find issue or would like to see some functionality (or implement one), please <a href=\"https://help.github.com/en/articles/creating-an-issue\" rel=\"nofollow\">open new Issue</a> or <a href=\"https://help.github.com/en/articles/creating-a-pull-request-from-a-fork\" rel=\"nofollow\">create Pull Request</a>.</p>\n\n          </div>"}, "last_serial": 6963405, "releases": {"0.1.1": [{"comment_text": "", "digests": {"md5": "f709a61b36771384d6c6b16f64ca72aa", "sha256": "785dd215dd0ca774f776465781ba25bb285245f38ed6ca7dc4ce121012e5ffd2"}, "downloads": -1, "filename": "torchlayers-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "f709a61b36771384d6c6b16f64ca72aa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 36187, "upload_time": "2020-03-27T22:11:33", "upload_time_iso_8601": "2020-03-27T22:11:33.574042Z", "url": "https://files.pythonhosted.org/packages/c7/3e/991c901501dc56b5c19cc7eb8f99523f57cd687fd1dc8b321569aa4d3ec9/torchlayers-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "38fceba411a136071bba89b996cfae53", "sha256": "fbeb3ba773a74bd38154a9e1b90aa8fc6b9d505cdcba3f15dd311af2a4593565"}, "downloads": -1, "filename": "torchlayers-0.1.1.tar.gz", "has_sig": false, "md5_digest": "38fceba411a136071bba89b996cfae53", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 30538, "upload_time": "2020-03-27T22:11:34", "upload_time_iso_8601": "2020-03-27T22:11:34.902275Z", "url": "https://files.pythonhosted.org/packages/9f/e5/71dae7f930afbdccff07a57ba4dcb74ea3602aadc4d6578eb6843f85f61d/torchlayers-0.1.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "f709a61b36771384d6c6b16f64ca72aa", "sha256": "785dd215dd0ca774f776465781ba25bb285245f38ed6ca7dc4ce121012e5ffd2"}, "downloads": -1, "filename": "torchlayers-0.1.1-py3-none-any.whl", "has_sig": false, "md5_digest": "f709a61b36771384d6c6b16f64ca72aa", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 36187, "upload_time": "2020-03-27T22:11:33", "upload_time_iso_8601": "2020-03-27T22:11:33.574042Z", "url": "https://files.pythonhosted.org/packages/c7/3e/991c901501dc56b5c19cc7eb8f99523f57cd687fd1dc8b321569aa4d3ec9/torchlayers-0.1.1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "38fceba411a136071bba89b996cfae53", "sha256": "fbeb3ba773a74bd38154a9e1b90aa8fc6b9d505cdcba3f15dd311af2a4593565"}, "downloads": -1, "filename": "torchlayers-0.1.1.tar.gz", "has_sig": false, "md5_digest": "38fceba411a136071bba89b996cfae53", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 30538, "upload_time": "2020-03-27T22:11:34", "upload_time_iso_8601": "2020-03-27T22:11:34.902275Z", "url": "https://files.pythonhosted.org/packages/9f/e5/71dae7f930afbdccff07a57ba4dcb74ea3602aadc4d6578eb6843f85f61d/torchlayers-0.1.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 03:50:18 2020"}