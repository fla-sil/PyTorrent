{"info": {"author": "Somshubra Majumdar", "author_email": "titu1994@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "\n# TabNet for Tensorflow 2.0\nA Tensorflow 2.0 port for the paper [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442), whose original codebase is available at https://github.com/google-research/google-research/blob/master/tabnet.\n\n<img src=\"https://github.com/titu1994/tf-TabNet/blob/master/images/TabNet.png?raw=true\" height=100% width=100%>\n\nThe above image is obtained from the paper, where the model is built of blocks in two stages - one to attend to the input features and anither to construct the output of the model. \n\n# Differences from Paper\nThere are two major differences from the paper and the official implementation.\n\n1) This implementation offers a choice in the normalization method, between the regular `Batch Normalization` from the paper and `Group Normalization`.\n   - It has been observed that the paper uses very large batch sizes to stabilie Batch Normalization and obtain good generalization. An issue with this is computational cost. \n   - Therefore Group Normalization (with number of groups set as 1, aka Instance Normalization) offers a reasonable alternative which is independent of the batch size.\n   - One can set `num_groups` to 1 for `Instance Normalization` type behaviour, or to -1 for `Layer Normalization` type behaviour.\n\n2) This implementation does not strictly need feature columns as input. \n   - While this model was originally developed for tabulur data, there is no hard requirement for that to be the only type of input it accepts.\n   - By passing `feature_columns=None` and explicitly specifying the input dimensionality of the data (using `num_features`), we can get a semi-interpretable result from even image data (after flattening it into a long vector).\n\n# Installation\n\n - For latest release branch\n```bash\n$ pip install --upgrade tabnet\n```\n\n - For Master branch.\n```bash\n$ pip install git+https://github.com/titu1994/tf-TabNet.git\n```\n\nAs Tensorflow can be used with either a CPU or GPU, the package can be installed with the conditional requirements using `[cpu]` or `[gpu]` as follows.\n\n```bash\n$ pip install tabnet[cpu]\n$ pip install tabnet[gpu]\n```\n\n# Usage\n\nThe script `tabnet.py` can be imported to yield either the `TabNet` building block, or the `TabNetClassification` and `TabNetRegression` models, which add appropriate heads for the basic `TabNet` model. If the classification or regression head is to be customized, it is recommended to compose a new model with the `TabNet` as the base of the model.\n\n```python\nfrom tabnet import TabNet, TabNetClassifier\n\nmodel = TabNetClassifier(feature_list, num_classes, ...)\n```\n\n## Stacked TabNets\n\nRegular TabNets can be stacked into various layers, thereby reducing interpretability but improving model capacity.\n\n```python\nfrom tabnet import StackedTabNetClassifier\n\nmodel = TabNetClassifier(feature_list, num_classes, num_layers, ...)\n```\n\nAs the models use custom objects, it is necessary to import `custom_objects.py` in an evaluation only script.\n\n# Mask Visualization\nThe masks of the TabNet can be obtained by using the TabNet class properties\n - `feature_selection_masks`: Returns a list of 1 or more masks at intermediate decision steps. Number of masks = number of decision steps - 1\n - `aggregate_feature_selection_mask`: Returns a single tensor which is the average activation of the masks over that batch of training samples.\n\n These masks can be obtained as `TabNet.feature_selection_masks`. Since the `TabNetClassification` and `TabNetRegression` models are composed of `TabNet`, the masks can be obtained as `model.tabnet.*`\n\n ## Mask Generation must be in Eager Execution Mode\n Note: Due to autograph, the outputs of the model when using `fit()` or `predict()` Keras APIs will \n generally be graph based Tensors, not EagerTensors. Since the masks are generated inside the `Model.call()` method,\n it is necessary to force the model to behave in Eager execution mode, not in Graph mode.\n\n Therefore there are two ways to force the model into eager mode:\n\n 1) Get tensor data samples, and directly `call` the model using this data as below :\n\n ```python\nx, _ = next(iter(tf_dataset))  # Assuming it generates an (x, y) tuple.\n_ = model(x)  # This forces eager execution.\n ```\n\n 2) Or another choice is to build a seperate model (but here you will pass the `dynamic=True` flag to the model constructor),\n load the weights and parameters in this model, and call `model.predict(x)`. This should also force eager execution mode.\n\n ```python\nnew_model = TabNetClassification(..., dynamic=True)\nnew_model.load_weights('path/to/weights)')\n\nx, _ = next(iter(tf_dataset))  # Assuming it generates an (x, y) tuple.\nmodel.predict(x)\n ```\n\n ---\n\nAfter the model has been forced into Eager Execution mode, the masks can be visualized in Tensorboard as follows - \n```python\nwriter = tf.summary.create_file_writer(\"logs/\")\nwith writer.as_default():\n    for i, mask in enumerate(model.tabnet.feature_selection_masks):\n        print(\"Saving mask {} of shape {}\".format(i + 1, mask.shape))\n        tf.summary.image('mask_at_iter_{}'.format(i + 1), step=0, data=mask, max_outputs=1)\n        writer.flush()\n\n    agg_mask = model.tabnet.aggregate_feature_selection_mask\n    print(\"Saving aggregate mask of shape\", agg_mask.shape)\n    tf.summary.image(\"Aggregate Mask\", step=0, data=agg_mask, max_outputs=1)\n    writer.flush()\nwriter.close()\n```\n\n# Requirements\n- Tensorflow 2.0+ (1.14+ with V2 compat enabled may be sufficient for 1.x)\n- Tensorflow-datasets (Only required for evaluating `train_iris.py`)\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/titu1994/tf-TabNet", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/titu1994/tf-TabNet", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "tabnet", "package_url": "https://pypi.org/project/tabnet/", "platform": "", "project_url": "https://pypi.org/project/tabnet/", "project_urls": {"Download": "https://github.com/titu1994/tf-TabNet", "Homepage": "https://github.com/titu1994/tf-TabNet"}, "release_url": "https://pypi.org/project/tabnet/0.1.4.1/", "requires_dist": ["tensorflow ; extra == 'cpu'", "tensorflow-gpu ; extra == 'gpu'"], "requires_python": ">=3.0.0", "summary": "Tensorflow 2.0 implementation of TabNet of any configuration.", "version": "0.1.4.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>TabNet for Tensorflow 2.0</h1>\n<p>A Tensorflow 2.0 port for the paper <a href=\"https://arxiv.org/abs/1908.07442\" rel=\"nofollow\">TabNet: Attentive Interpretable Tabular Learning</a>, whose original codebase is available at <a href=\"https://github.com/google-research/google-research/blob/master/tabnet\" rel=\"nofollow\">https://github.com/google-research/google-research/blob/master/tabnet</a>.</p>\n<img height=\"100%\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/88a031698031ed731cf65dc79a2f0085dbce4670/68747470733a2f2f6769746875622e636f6d2f74697475313939342f74662d5461624e65742f626c6f622f6d61737465722f696d616765732f5461624e65742e706e673f7261773d74727565\" width=\"100%\">\n<p>The above image is obtained from the paper, where the model is built of blocks in two stages - one to attend to the input features and anither to construct the output of the model.</p>\n<h1>Differences from Paper</h1>\n<p>There are two major differences from the paper and the official implementation.</p>\n<ol>\n<li>\n<p>This implementation offers a choice in the normalization method, between the regular <code>Batch Normalization</code> from the paper and <code>Group Normalization</code>.</p>\n<ul>\n<li>It has been observed that the paper uses very large batch sizes to stabilie Batch Normalization and obtain good generalization. An issue with this is computational cost.</li>\n<li>Therefore Group Normalization (with number of groups set as 1, aka Instance Normalization) offers a reasonable alternative which is independent of the batch size.</li>\n<li>One can set <code>num_groups</code> to 1 for <code>Instance Normalization</code> type behaviour, or to -1 for <code>Layer Normalization</code> type behaviour.</li>\n</ul>\n</li>\n<li>\n<p>This implementation does not strictly need feature columns as input.</p>\n<ul>\n<li>While this model was originally developed for tabulur data, there is no hard requirement for that to be the only type of input it accepts.</li>\n<li>By passing <code>feature_columns=None</code> and explicitly specifying the input dimensionality of the data (using <code>num_features</code>), we can get a semi-interpretable result from even image data (after flattening it into a long vector).</li>\n</ul>\n</li>\n</ol>\n<h1>Installation</h1>\n<ul>\n<li>For latest release branch</li>\n</ul>\n<pre>$ pip install --upgrade tabnet\n</pre>\n<ul>\n<li>For Master branch.</li>\n</ul>\n<pre>$ pip install git+https://github.com/titu1994/tf-TabNet.git\n</pre>\n<p>As Tensorflow can be used with either a CPU or GPU, the package can be installed with the conditional requirements using <code>[cpu]</code> or <code>[gpu]</code> as follows.</p>\n<pre>$ pip install tabnet<span class=\"o\">[</span>cpu<span class=\"o\">]</span>\n$ pip install tabnet<span class=\"o\">[</span>gpu<span class=\"o\">]</span>\n</pre>\n<h1>Usage</h1>\n<p>The script <code>tabnet.py</code> can be imported to yield either the <code>TabNet</code> building block, or the <code>TabNetClassification</code> and <code>TabNetRegression</code> models, which add appropriate heads for the basic <code>TabNet</code> model. If the classification or regression head is to be customized, it is recommended to compose a new model with the <code>TabNet</code> as the base of the model.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tabnet</span> <span class=\"kn\">import</span> <span class=\"n\">TabNet</span><span class=\"p\">,</span> <span class=\"n\">TabNetClassifier</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">TabNetClassifier</span><span class=\"p\">(</span><span class=\"n\">feature_list</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">)</span>\n</pre>\n<h2>Stacked TabNets</h2>\n<p>Regular TabNets can be stacked into various layers, thereby reducing interpretability but improving model capacity.</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">tabnet</span> <span class=\"kn\">import</span> <span class=\"n\">StackedTabNetClassifier</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">TabNetClassifier</span><span class=\"p\">(</span><span class=\"n\">feature_list</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"p\">,</span> <span class=\"n\">num_layers</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">)</span>\n</pre>\n<p>As the models use custom objects, it is necessary to import <code>custom_objects.py</code> in an evaluation only script.</p>\n<h1>Mask Visualization</h1>\n<p>The masks of the TabNet can be obtained by using the TabNet class properties</p>\n<ul>\n<li><code>feature_selection_masks</code>: Returns a list of 1 or more masks at intermediate decision steps. Number of masks = number of decision steps - 1</li>\n<li><code>aggregate_feature_selection_mask</code>: Returns a single tensor which is the average activation of the masks over that batch of training samples.</li>\n</ul>\n<p>These masks can be obtained as <code>TabNet.feature_selection_masks</code>. Since the <code>TabNetClassification</code> and <code>TabNetRegression</code> models are composed of <code>TabNet</code>, the masks can be obtained as <code>model.tabnet.*</code></p>\n<h2>Mask Generation must be in Eager Execution Mode</h2>\n<p>Note: Due to autograph, the outputs of the model when using <code>fit()</code> or <code>predict()</code> Keras APIs will\ngenerally be graph based Tensors, not EagerTensors. Since the masks are generated inside the <code>Model.call()</code> method,\nit is necessary to force the model to behave in Eager execution mode, not in Graph mode.</p>\n<p>Therefore there are two ways to force the model into eager mode:</p>\n<ol>\n<li>Get tensor data samples, and directly <code>call</code> the model using this data as below :</li>\n</ol>\n<pre><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"nb\">iter</span><span class=\"p\">(</span><span class=\"n\">tf_dataset</span><span class=\"p\">))</span>  <span class=\"c1\"># Assuming it generates an (x, y) tuple.</span>\n<span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>  <span class=\"c1\"># This forces eager execution.</span>\n</pre>\n<ol>\n<li>Or another choice is to build a seperate model (but here you will pass the <code>dynamic=True</code> flag to the model constructor),\nload the weights and parameters in this model, and call <code>model.predict(x)</code>. This should also force eager execution mode.</li>\n</ol>\n<pre><span class=\"n\">new_model</span> <span class=\"o\">=</span> <span class=\"n\">TabNetClassification</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">dynamic</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">new_model</span><span class=\"o\">.</span><span class=\"n\">load_weights</span><span class=\"p\">(</span><span class=\"s1\">'path/to/weights)'</span><span class=\"p\">)</span>\n\n<span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"nb\">iter</span><span class=\"p\">(</span><span class=\"n\">tf_dataset</span><span class=\"p\">))</span>  <span class=\"c1\"># Assuming it generates an (x, y) tuple.</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n</pre>\n<hr>\n<p>After the model has been forced into Eager Execution mode, the masks can be visualized in Tensorboard as follows -</p>\n<pre><span class=\"n\">writer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"o\">.</span><span class=\"n\">create_file_writer</span><span class=\"p\">(</span><span class=\"s2\">\"logs/\"</span><span class=\"p\">)</span>\n<span class=\"k\">with</span> <span class=\"n\">writer</span><span class=\"o\">.</span><span class=\"n\">as_default</span><span class=\"p\">():</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">mask</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">tabnet</span><span class=\"o\">.</span><span class=\"n\">feature_selection_masks</span><span class=\"p\">):</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Saving mask </span><span class=\"si\">{}</span><span class=\"s2\"> of shape </span><span class=\"si\">{}</span><span class=\"s2\">\"</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">))</span>\n        <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"o\">.</span><span class=\"n\">image</span><span class=\"p\">(</span><span class=\"s1\">'mask_at_iter_</span><span class=\"si\">{}</span><span class=\"s1\">'</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">step</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">max_outputs</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"n\">writer</span><span class=\"o\">.</span><span class=\"n\">flush</span><span class=\"p\">()</span>\n\n    <span class=\"n\">agg_mask</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">tabnet</span><span class=\"o\">.</span><span class=\"n\">aggregate_feature_selection_mask</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">\"Saving aggregate mask of shape\"</span><span class=\"p\">,</span> <span class=\"n\">agg_mask</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"o\">.</span><span class=\"n\">image</span><span class=\"p\">(</span><span class=\"s2\">\"Aggregate Mask\"</span><span class=\"p\">,</span> <span class=\"n\">step</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">agg_mask</span><span class=\"p\">,</span> <span class=\"n\">max_outputs</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">writer</span><span class=\"o\">.</span><span class=\"n\">flush</span><span class=\"p\">()</span>\n<span class=\"n\">writer</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n</pre>\n<h1>Requirements</h1>\n<ul>\n<li>Tensorflow 2.0+ (1.14+ with V2 compat enabled may be sufficient for 1.x)</li>\n<li>Tensorflow-datasets (Only required for evaluating <code>train_iris.py</code>)</li>\n</ul>\n\n          </div>"}, "last_serial": 6214820, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "7d8703c96ff4097da99a48e0f90e9c69", "sha256": "5a7eed09b7fc3bca107b2d6fae8ac804d6bbb67913302922eb3b6cf763d4d16b"}, "downloads": -1, "filename": "tabnet-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "7d8703c96ff4097da99a48e0f90e9c69", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.0.0", "size": 12916, "upload_time": "2019-10-09T02:22:29", "upload_time_iso_8601": "2019-10-09T02:22:29.474474Z", "url": "https://files.pythonhosted.org/packages/e9/f3/86af359b9ad97067f15ceda274b01d35a6e93565c7b2e377d3b1ed5d651a/tabnet-0.1.0-py2.py3-none-any.whl", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "4ca72add46e56969397ef6f9f23622fb", "sha256": "11d9fcd59e4069376acc5a867e3c0cc6336c34d8441a94c4d81bd4c8134d6f84"}, "downloads": -1, "filename": "tabnet-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "4ca72add46e56969397ef6f9f23622fb", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.0.0", "size": 12954, "upload_time": "2019-10-09T02:35:31", "upload_time_iso_8601": "2019-10-09T02:35:31.993319Z", "url": "https://files.pythonhosted.org/packages/0c/0b/36ebbecb381e104bbcf7d95fe3058db13e072f1e8f0eeaa13d1837e6134a/tabnet-0.1.1-py2.py3-none-any.whl", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "8d35f7129edb2da0ea1a866df758d217", "sha256": "c81201ffa267db0ed7d7c8feb93d7c83753bdf639b70d88a9f818cdeae4ba34e"}, "downloads": -1, "filename": "tabnet-0.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "8d35f7129edb2da0ea1a866df758d217", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.0.0", "size": 12970, "upload_time": "2019-10-09T06:09:19", "upload_time_iso_8601": "2019-10-09T06:09:19.887748Z", "url": "https://files.pythonhosted.org/packages/d8/70/a3ecf48575f68f61d417e7d208928a0bd6b9859f38ac4d11336b3e1b5cbc/tabnet-0.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "0528239545423f9295bd305ac937c8fa", "sha256": "78fde0612d0c67ad3265eb053a5d91134aeddec401ea7c24d68c9a277c876ae4"}, "downloads": -1, "filename": "tabnet-0.1.2.tar.gz", "has_sig": false, "md5_digest": "0528239545423f9295bd305ac937c8fa", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0.0", "size": 14598, "upload_time": "2019-10-09T06:09:21", "upload_time_iso_8601": "2019-10-09T06:09:21.374620Z", "url": "https://files.pythonhosted.org/packages/55/1e/64fba7964460ca449eac6c33dc9ad91d52e253adf68a8b2fdfe28b5fd8c7/tabnet-0.1.2.tar.gz", "yanked": false}], "0.1.3": [{"comment_text": "", "digests": {"md5": "4640ba61d0e84f94b4287f49f88662c7", "sha256": "ff013ac049711f2654f5e42b10003036a68e0be212e247ff1a4d1e97156d033c"}, "downloads": -1, "filename": "tabnet-0.1.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "4640ba61d0e84f94b4287f49f88662c7", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.0.0", "size": 12974, "upload_time": "2019-10-10T23:32:32", "upload_time_iso_8601": "2019-10-10T23:32:32.917006Z", "url": "https://files.pythonhosted.org/packages/95/2e/fc30ad2f5aa88243daddb3a72efe1952e51fab4e5d342f3866a50fe1f8c7/tabnet-0.1.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2b454f1b9d0f59d49f40698b5aff3c9e", "sha256": "7d702fafc1bcec90917a1b2eb43bf128ffd16954eff354ef2dd45e6006800377"}, "downloads": -1, "filename": "tabnet-0.1.3.tar.gz", "has_sig": false, "md5_digest": "2b454f1b9d0f59d49f40698b5aff3c9e", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0.0", "size": 14603, "upload_time": "2019-10-10T23:32:34", "upload_time_iso_8601": "2019-10-10T23:32:34.704006Z", "url": "https://files.pythonhosted.org/packages/ea/4d/8b55493dffe885eb2065ab275d18c4c5cbe46549a718be86a3a71331e7df/tabnet-0.1.3.tar.gz", "yanked": false}], "0.1.4": [{"comment_text": "", "digests": {"md5": "9b039279959fd7fb9471ec0415793418", "sha256": "523a4d517a73bf672449f9a2a44dc2ae577ae9792f2dddff739bb3577b4dc31a"}, "downloads": -1, "filename": "tabnet-0.1.4-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "9b039279959fd7fb9471ec0415793418", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.0.0", "size": 16008, "upload_time": "2019-11-16T22:58:26", "upload_time_iso_8601": "2019-11-16T22:58:26.451501Z", "url": "https://files.pythonhosted.org/packages/e9/43/32e0b8d66574e15ac28d78f0cc59663712881b9fe583c6f6cd4bb5a40b4a/tabnet-0.1.4-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8b3c44a6d229b4a65439e79a73ba8656", "sha256": "f23bce852e403a5f724d8ebe08c741bfecaddea9da2eb1e43a60f2ba0f27d7e7"}, "downloads": -1, "filename": "tabnet-0.1.4.tar.gz", "has_sig": false, "md5_digest": "8b3c44a6d229b4a65439e79a73ba8656", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0.0", "size": 15670, "upload_time": "2019-11-16T22:58:28", "upload_time_iso_8601": "2019-11-16T22:58:28.296878Z", "url": "https://files.pythonhosted.org/packages/7d/c5/439ed5ce04397d5c443030cc9afb4a483ca2c715f9a13602109bb3ac7838/tabnet-0.1.4.tar.gz", "yanked": false}], "0.1.4.1": [{"comment_text": "", "digests": {"md5": "78d55854906fed8001e1e00dd4a3afef", "sha256": "93e1a065a4ad83c6aab4bcbaeaf81b97e8cced15df772d9abcc8455f09775d41"}, "downloads": -1, "filename": "tabnet-0.1.4.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "78d55854906fed8001e1e00dd4a3afef", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.0.0", "size": 16030, "upload_time": "2019-11-28T15:51:26", "upload_time_iso_8601": "2019-11-28T15:51:26.486539Z", "url": "https://files.pythonhosted.org/packages/13/22/851a15af4c79f5f73ffa05a71f84e7d0d65d404085cd312b2d0588a9e9ca/tabnet-0.1.4.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "66b792b5d4bc7544bdfe39768227f851", "sha256": "b53f41d5a6e6fab3806b1aa8f365dd517bbb431dfd781aad2a20183cc55f66ee"}, "downloads": -1, "filename": "tabnet-0.1.4.1.tar.gz", "has_sig": false, "md5_digest": "66b792b5d4bc7544bdfe39768227f851", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0.0", "size": 15682, "upload_time": "2019-11-28T15:51:29", "upload_time_iso_8601": "2019-11-28T15:51:29.466854Z", "url": "https://files.pythonhosted.org/packages/57/07/3289f593ce7747bb362e22fce6f196d20a9b96a8aa4ba696573a5bfb6028/tabnet-0.1.4.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "78d55854906fed8001e1e00dd4a3afef", "sha256": "93e1a065a4ad83c6aab4bcbaeaf81b97e8cced15df772d9abcc8455f09775d41"}, "downloads": -1, "filename": "tabnet-0.1.4.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "78d55854906fed8001e1e00dd4a3afef", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.0.0", "size": 16030, "upload_time": "2019-11-28T15:51:26", "upload_time_iso_8601": "2019-11-28T15:51:26.486539Z", "url": "https://files.pythonhosted.org/packages/13/22/851a15af4c79f5f73ffa05a71f84e7d0d65d404085cd312b2d0588a9e9ca/tabnet-0.1.4.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "66b792b5d4bc7544bdfe39768227f851", "sha256": "b53f41d5a6e6fab3806b1aa8f365dd517bbb431dfd781aad2a20183cc55f66ee"}, "downloads": -1, "filename": "tabnet-0.1.4.1.tar.gz", "has_sig": false, "md5_digest": "66b792b5d4bc7544bdfe39768227f851", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.0.0", "size": 15682, "upload_time": "2019-11-28T15:51:29", "upload_time_iso_8601": "2019-11-28T15:51:29.466854Z", "url": "https://files.pythonhosted.org/packages/57/07/3289f593ce7747bb362e22fce6f196d20a9b96a8aa4ba696573a5bfb6028/tabnet-0.1.4.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:58:36 2020"}