{"info": {"author": "Flax team", "author_email": "flax-dev@google.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# Flax: A neural network library for JAX designed for flexibility\n\n\n**NOTE**: Flax is being actively improved and has a growing community\nof researchers and engineers at Google who happily use Flax for their\ndaily research. Flax is in \"early release stage\" -- if that's your style,\nnow could be a good time to start using it.\nWe want to smooth out any rough edges so please report\nany issues, questions or concerns as\n[GitHub issues](https://github.com/google/flax/issues). Expect changes to the\nAPI, but we'll use deprecation warnings when we can, and keep\ntrack of them in our [Changelog](CHANGELOG.md).\n\nIn case you need to reach us directly, we're at flax-dev@google.com.\n\n\n## Quickstart\n\n**\u27f6 [Full documentation and API reference](https://flax.readthedocs.io/)**\n\n**\u27f6 [Annotated full end-to-end MNIST example](https://flax.readthedocs.io/en/latest/annotated_mnist.html)**\n\n**\u27f6 [The Flax Guide](https://flax.readthedocs.io/en/latest/notebooks/flax_guided_tour.html)** -- a guided walkthrough of the parts of Flax\n\n## Background: JAX\n\n[JAX](https://github.com/google/jax) is NumPy + autodiff + GPU/TPU\n\nIt allows for fast scientific computing and machine learning\nwith the normal NumPy API\n(+ additional APIs for special accelerator ops when needed)\n\nJAX comes with powerful primitives, which you can compose arbitrarily:\n\n* Autodiff (`jax.grad`): Efficient any-order gradients w.r.t any variables\n* JIT compilation (`jax.jit`): Trace any function \u27f6 fused accelerator ops\n* Vectorization (`jax.vmap`): Automatically batch code written for individual samples\n* Parallelization (`jax.pmap`): Automatically parallelize code across multiple accelerators (including across hosts, e.g. for TPU pods)\n\n## What is Flax?\n\nFlax is a high-performance neural network library for\nJAX that is **designed for flexibility**:\nTry new forms of training by forking an example and by modifying the training\nloop, not by adding features to a framework.\n\nFlax is being developed in close collaboration with the JAX team and \ncomes with everything you need to start your research, including:\n\n* **Common layers** (`flax.nn`): Dense, Conv, {Batch|Layer|Group} Norm, Attention, Pooling, {LSTM|GRU} Cell, Dropout\n\n* **Optimizers** (`flax.optim`): SGD, Momentum, Adam, LARS\n\n* **Utilities and patterns**: replicated training, serialization and checkpointing, metrics, prefetching on device\n\n* **Educational examples** that work out of the box: MNIST, LSTM seq2seq, Graph Neural Networks, Sequence Tagging\n\n* **HOWTO guides** -- diffs that add functionality to educational base exampless\n\n* **Fast, tuned large-scale end-to-end examples**: CIFAR10, ResNet on ImageNet, Transformer LM1b\n\n## Try Flax now by forking one of our starter examples\n\n### Image Classification\n\u27f6 [MNIST](examples/mnist) (also see [annotated version](https://flax.readthedocs.io/en/latest/annotated_mnist.html))\n\n\u27f6 [CIFAR-10](examples/cifar10) (Wide ResNet w/ and w/o Shake-Shake, PyramidNet w/ShakeDrop)\n\n\u27f6 [ResNet50 on ImageNet](examples/imagenet)\n\n### Transformer Models\n\u27f6 [Sequence tagging on Universal Dependencies](examples/nlp_seq)\n\n\u27f6 [LM1b language modeling](examples/lm1b) **([try on a TPU in Colab](https://colab.research.google.com/github/google/flax/blob/master/examples/lm1b/Colab_Language_Model.ipynb))**\n\n\u27f6 (work-in-progress) [WMT translation](https://github.com/google/flax/pull/133)\n\n### RNNs\n\u27f6 [LSTM text classifier on SST-2](examples/sst2)\n\n\u27f6 [LSTM seq2seq on number addition](examples/seq2seq)\n\n\n### Generative Models\n\u27f6 [Basic VAE](examples/vae)\n\n### Graph Neural Networks\n\u27f6 [Semi-supervised node classification on Zachary's karate club](examples/graph)\n\n## The Flax Module abstraction in a nutshell\n\nThe core of Flax is the Module abstraction. Modules allow you to write parameterized functions just as if you were writing a normal numpy function with JAX. The Module api allows you to declare parameters and use them directly with the JAX api\u2019s.\n\nModules are the one part of Flax with \"magic\" -- the magic is constrained, and enables a very ergonomic model construction style, where modules are defined in a single function with minimal boilerplate.\n\nA few things to know about Modules:\n\n1. Create a new module by subclassing `flax.nn.Module` and implementing the `apply` method.\n\n2. Within `apply`, call `self.param(name, shape, init_func)` to register a new parameter and returns its initial value.\n\n3. Apply submodules with `MySubModule(name=..., ...)` within `MyModule.apply`. Parameters of `MySubModule` are stored\nas a dictionary under the parameters `MyModule` and accessible via `self.get_param(name=...)`. This applies `MySubmodule` once --\nto re-use parameters, use [`Module.shared`](https://flax.readthedocs.io/en/latest/notebooks/flax_intro.html#Parameter-sharing)\n\n4. `MyModule.init(rng, ...)` is a pure function that calls `apply` in \"init mode\" and returns a nested Python dict of initialized parameter values\n\n5. `MyModule.call(params, ...)` is a pure function that calls `apply` in \"call mode\" and returns the output of the module.\n\nFor example you can define a learned linear transformation as follows:\n\n```py\nfrom flax import nn\nimport jax.numpy as jnp\n\nclass Linear(nn.Module):\n  def apply(self, x, num_features, kernel_init_fn):\n    input_features = x.shape[-1]\n    W = self.param('W', (input_features, num_features), kernel_init_fn)\n    return jnp.dot(x, W)\n```\n\nYou can also use `nn.module` as a function decorator to create a new module, as\nlong as you don't need access to `self` for creating parameters directly:\n\n```py\n@nn.module\ndef DenseLayer(x, features):\n  x = flax.nn.Dense(x, features)\n  x = flax.nn.relu(x)\n  return x\n```\n\n**\u27f6 Read more about Modules in the [Flax Guide](https://flax.readthedocs.io/en/latest/notebooks/flax_intro.html#Flax-Modules)**\n\n## A full ResNet implementation\n\n(from [examples/imagenet/models.py](examples/imagenet/models.py))\n\n```py\nclass ResidualBlock(nn.Module):\n  def apply(self, x, filters, strides=(1, 1), train=True, dtype=jnp.float32):\n    needs_projection = x.shape[-1] != filters * 4 or strides != (1, 1)\n    batch_norm = nn.BatchNorm.partial(\n        use_running_average=not train, momentum=0.9, epsilon=1e-5, dtype=dtype)\n    conv = nn.Conv.partial(bias=False, dtype=dtype)\n\n    residual = x\n    if needs_projection:\n      residual = conv(residual, filters * 4, (1, 1), strides, name='proj_conv')\n      residual = batch_norm(residual, name='proj_bn')\n\n    y = conv(x, filters, (1, 1), name='conv1')\n    y = batch_norm(y, name='bn1')\n    y = nn.relu(y)\n    y = conv(y, filters, (3, 3), strides, name='conv2')\n    y = batch_norm(y, name='bn2')\n    y = nn.relu(y)\n    y = conv(y, filters * 4, (1, 1), name='conv3')\n\n    y = batch_norm(y, name='bn3', scale_init=nn.initializers.zeros)\n    y = nn.relu(residual + y)\n    return y\n\n\nclass ResNet(nn.Module):\n  def apply(self, x, num_classes, num_filters=64, num_layers=50,\n            train=True, dtype=jnp.float32):\n    if num_layers not in _block_size_options:\n      raise ValueError('Please provide a valid number of layers')\n    block_sizes = _block_size_options[num_layers]\n    x = nn.Conv(\n        x, num_filters, (7, 7), (2, 2), padding=[(3, 3), (3, 3)],\n        bias=False, dtype=dtype, name='init_conv')\n    x = nn.BatchNorm(\n        x, use_running_average=not train, momentum=0.9,\n        epsilon=1e-5, dtype=dtype, name='init_bn')\n    x = nn.max_pool(x, (3, 3), strides=(2, 2), padding='SAME')\n    for i, block_size in enumerate(block_sizes):\n      for j in range(block_size):\n        strides = (2, 2) if i > 0 and j == 0 else (1, 1)\n        x = ResidualBlock(\n            x, num_filters * 2 ** i, strides=strides,\n            train=train, dtype=dtype)\n    x = jnp.mean(x, axis=(1, 2))\n    x = nn.Dense(x, num_classes)\n    x = nn.log_softmax(x)\n    return x\n```\n\n## Installation\n\nYou will need Python 3.6 or later.\n\nFor GPU support, first install `jaxlib`; please follow the\ninstructions in the [JAX\nreadme](https://github.com/google/jax/blob/master/README.md).  If they\nare not already installed, you will need to install\n[CUDA](https://developer.nvidia.com/cuda-downloads) and\n[CuDNN](https://developer.nvidia.com/cudnn) runtimes.\n\nThen install `flax` from PyPi:\n```\n> pip install flax\n```\n\n## TPU support\n\nWe currently have a [LM1b/Wikitext-2 language model with a Transformer architecture](https://colab.research.google.com/github/google/flax/blob/master/examples/lm1b/Colab_Language_Model.ipynb)\nthat's been tuned. You can run it directly via Colab.\n\nAt present, Cloud TPUs are network-attached, and Flax users typically feed in data from one or more additional VMs\n\nWhen working with large-scale input data, it is important to create large enough VMs with sufficient network bandwidth to avoid having the TPUs bottlenecked waiting for input\n\nTODO: Add an example for running on Google Cloud.\n\n## Getting involved\nWe welcome pull requests, in particular for those issues [marked as PR-ready](https://github.com/google/flax/issues?q=is%3Aopen+is%3Aissue+label%3A%22pull+requests+welcome%22). For other proposals, we ask that you first open an Issue to discuss your planned contribution.\n\n## Note\n\nThis is not an official Google product.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/google/flax", "keywords": "", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "flax", "package_url": "https://pypi.org/project/flax/", "platform": "", "project_url": "https://pypi.org/project/flax/", "project_urls": {"Homepage": "https://github.com/google/flax"}, "release_url": "https://pypi.org/project/flax/0.1.0/", "requires_dist": ["numpy (>=1.12)", "jax (>=0.1.59)", "matplotlib", "dataclasses", "msgpack", "pytest ; extra == 'testing'", "pytest-xdist ; extra == 'testing'", "tensorflow-datasets ; extra == 'testing'"], "requires_python": "", "summary": "Flax: A neural network library for JAX designed for flexibility", "version": "0.1.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Flax: A neural network library for JAX designed for flexibility</h1>\n<p><strong>NOTE</strong>: Flax is being actively improved and has a growing community\nof researchers and engineers at Google who happily use Flax for their\ndaily research. Flax is in \"early release stage\" -- if that's your style,\nnow could be a good time to start using it.\nWe want to smooth out any rough edges so please report\nany issues, questions or concerns as\n<a href=\"https://github.com/google/flax/issues\" rel=\"nofollow\">GitHub issues</a>. Expect changes to the\nAPI, but we'll use deprecation warnings when we can, and keep\ntrack of them in our <a href=\"CHANGELOG.md\" rel=\"nofollow\">Changelog</a>.</p>\n<p>In case you need to reach us directly, we're at <a href=\"mailto:flax-dev@google.com\">flax-dev@google.com</a>.</p>\n<h2>Quickstart</h2>\n<p><strong>\u27f6 <a href=\"https://flax.readthedocs.io/\" rel=\"nofollow\">Full documentation and API reference</a></strong></p>\n<p><strong>\u27f6 <a href=\"https://flax.readthedocs.io/en/latest/annotated_mnist.html\" rel=\"nofollow\">Annotated full end-to-end MNIST example</a></strong></p>\n<p><strong>\u27f6 <a href=\"https://flax.readthedocs.io/en/latest/notebooks/flax_guided_tour.html\" rel=\"nofollow\">The Flax Guide</a></strong> -- a guided walkthrough of the parts of Flax</p>\n<h2>Background: JAX</h2>\n<p><a href=\"https://github.com/google/jax\" rel=\"nofollow\">JAX</a> is NumPy + autodiff + GPU/TPU</p>\n<p>It allows for fast scientific computing and machine learning\nwith the normal NumPy API\n(+ additional APIs for special accelerator ops when needed)</p>\n<p>JAX comes with powerful primitives, which you can compose arbitrarily:</p>\n<ul>\n<li>Autodiff (<code>jax.grad</code>): Efficient any-order gradients w.r.t any variables</li>\n<li>JIT compilation (<code>jax.jit</code>): Trace any function \u27f6 fused accelerator ops</li>\n<li>Vectorization (<code>jax.vmap</code>): Automatically batch code written for individual samples</li>\n<li>Parallelization (<code>jax.pmap</code>): Automatically parallelize code across multiple accelerators (including across hosts, e.g. for TPU pods)</li>\n</ul>\n<h2>What is Flax?</h2>\n<p>Flax is a high-performance neural network library for\nJAX that is <strong>designed for flexibility</strong>:\nTry new forms of training by forking an example and by modifying the training\nloop, not by adding features to a framework.</p>\n<p>Flax is being developed in close collaboration with the JAX team and\ncomes with everything you need to start your research, including:</p>\n<ul>\n<li>\n<p><strong>Common layers</strong> (<code>flax.nn</code>): Dense, Conv, {Batch|Layer|Group} Norm, Attention, Pooling, {LSTM|GRU} Cell, Dropout</p>\n</li>\n<li>\n<p><strong>Optimizers</strong> (<code>flax.optim</code>): SGD, Momentum, Adam, LARS</p>\n</li>\n<li>\n<p><strong>Utilities and patterns</strong>: replicated training, serialization and checkpointing, metrics, prefetching on device</p>\n</li>\n<li>\n<p><strong>Educational examples</strong> that work out of the box: MNIST, LSTM seq2seq, Graph Neural Networks, Sequence Tagging</p>\n</li>\n<li>\n<p><strong>HOWTO guides</strong> -- diffs that add functionality to educational base exampless</p>\n</li>\n<li>\n<p><strong>Fast, tuned large-scale end-to-end examples</strong>: CIFAR10, ResNet on ImageNet, Transformer LM1b</p>\n</li>\n</ul>\n<h2>Try Flax now by forking one of our starter examples</h2>\n<h3>Image Classification</h3>\n<p>\u27f6 <a href=\"examples/mnist\" rel=\"nofollow\">MNIST</a> (also see <a href=\"https://flax.readthedocs.io/en/latest/annotated_mnist.html\" rel=\"nofollow\">annotated version</a>)</p>\n<p>\u27f6 <a href=\"examples/cifar10\" rel=\"nofollow\">CIFAR-10</a> (Wide ResNet w/ and w/o Shake-Shake, PyramidNet w/ShakeDrop)</p>\n<p>\u27f6 <a href=\"examples/imagenet\" rel=\"nofollow\">ResNet50 on ImageNet</a></p>\n<h3>Transformer Models</h3>\n<p>\u27f6 <a href=\"examples/nlp_seq\" rel=\"nofollow\">Sequence tagging on Universal Dependencies</a></p>\n<p>\u27f6 <a href=\"examples/lm1b\" rel=\"nofollow\">LM1b language modeling</a> <strong>(<a href=\"https://colab.research.google.com/github/google/flax/blob/master/examples/lm1b/Colab_Language_Model.ipynb\" rel=\"nofollow\">try on a TPU in Colab</a>)</strong></p>\n<p>\u27f6 (work-in-progress) <a href=\"https://github.com/google/flax/pull/133\" rel=\"nofollow\">WMT translation</a></p>\n<h3>RNNs</h3>\n<p>\u27f6 <a href=\"examples/sst2\" rel=\"nofollow\">LSTM text classifier on SST-2</a></p>\n<p>\u27f6 <a href=\"examples/seq2seq\" rel=\"nofollow\">LSTM seq2seq on number addition</a></p>\n<h3>Generative Models</h3>\n<p>\u27f6 <a href=\"examples/vae\" rel=\"nofollow\">Basic VAE</a></p>\n<h3>Graph Neural Networks</h3>\n<p>\u27f6 <a href=\"examples/graph\" rel=\"nofollow\">Semi-supervised node classification on Zachary's karate club</a></p>\n<h2>The Flax Module abstraction in a nutshell</h2>\n<p>The core of Flax is the Module abstraction. Modules allow you to write parameterized functions just as if you were writing a normal numpy function with JAX. The Module api allows you to declare parameters and use them directly with the JAX api\u2019s.</p>\n<p>Modules are the one part of Flax with \"magic\" -- the magic is constrained, and enables a very ergonomic model construction style, where modules are defined in a single function with minimal boilerplate.</p>\n<p>A few things to know about Modules:</p>\n<ol>\n<li>\n<p>Create a new module by subclassing <code>flax.nn.Module</code> and implementing the <code>apply</code> method.</p>\n</li>\n<li>\n<p>Within <code>apply</code>, call <code>self.param(name, shape, init_func)</code> to register a new parameter and returns its initial value.</p>\n</li>\n<li>\n<p>Apply submodules with <code>MySubModule(name=..., ...)</code> within <code>MyModule.apply</code>. Parameters of <code>MySubModule</code> are stored\nas a dictionary under the parameters <code>MyModule</code> and accessible via <code>self.get_param(name=...)</code>. This applies <code>MySubmodule</code> once --\nto re-use parameters, use <a href=\"https://flax.readthedocs.io/en/latest/notebooks/flax_intro.html#Parameter-sharing\" rel=\"nofollow\"><code>Module.shared</code></a></p>\n</li>\n<li>\n<p><code>MyModule.init(rng, ...)</code> is a pure function that calls <code>apply</code> in \"init mode\" and returns a nested Python dict of initialized parameter values</p>\n</li>\n<li>\n<p><code>MyModule.call(params, ...)</code> is a pure function that calls <code>apply</code> in \"call mode\" and returns the output of the module.</p>\n</li>\n</ol>\n<p>For example you can define a learned linear transformation as follows:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">flax</span> <span class=\"kn\">import</span> <span class=\"n\">nn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">jax.numpy</span> <span class=\"k\">as</span> <span class=\"nn\">jnp</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">Linear</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n  <span class=\"k\">def</span> <span class=\"nf\">apply</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">num_features</span><span class=\"p\">,</span> <span class=\"n\">kernel_init_fn</span><span class=\"p\">):</span>\n    <span class=\"n\">input_features</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">param</span><span class=\"p\">(</span><span class=\"s1\">'W'</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">input_features</span><span class=\"p\">,</span> <span class=\"n\">num_features</span><span class=\"p\">),</span> <span class=\"n\">kernel_init_fn</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">jnp</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">)</span>\n</pre>\n<p>You can also use <code>nn.module</code> as a function decorator to create a new module, as\nlong as you don't need access to <code>self</code> for creating parameters directly:</p>\n<pre><span class=\"nd\">@nn</span><span class=\"o\">.</span><span class=\"n\">module</span>\n<span class=\"k\">def</span> <span class=\"nf\">DenseLayer</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">features</span><span class=\"p\">):</span>\n  <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">flax</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">features</span><span class=\"p\">)</span>\n  <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">flax</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span> <span class=\"n\">x</span>\n</pre>\n<p><strong>\u27f6 Read more about Modules in the <a href=\"https://flax.readthedocs.io/en/latest/notebooks/flax_intro.html#Flax-Modules\" rel=\"nofollow\">Flax Guide</a></strong></p>\n<h2>A full ResNet implementation</h2>\n<p>(from <a href=\"examples/imagenet/models.py\" rel=\"nofollow\">examples/imagenet/models.py</a>)</p>\n<pre><span class=\"k\">class</span> <span class=\"nc\">ResidualBlock</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n  <span class=\"k\">def</span> <span class=\"nf\">apply</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">filters</span><span class=\"p\">,</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">jnp</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">):</span>\n    <span class=\"n\">needs_projection</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">!=</span> <span class=\"n\">filters</span> <span class=\"o\">*</span> <span class=\"mi\">4</span> <span class=\"ow\">or</span> <span class=\"n\">strides</span> <span class=\"o\">!=</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">batch_norm</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">BatchNorm</span><span class=\"o\">.</span><span class=\"n\">partial</span><span class=\"p\">(</span>\n        <span class=\"n\">use_running_average</span><span class=\"o\">=</span><span class=\"ow\">not</span> <span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"n\">epsilon</span><span class=\"o\">=</span><span class=\"mf\">1e-5</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n    <span class=\"n\">conv</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"o\">.</span><span class=\"n\">partial</span><span class=\"p\">(</span><span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n\n    <span class=\"n\">residual</span> <span class=\"o\">=</span> <span class=\"n\">x</span>\n    <span class=\"k\">if</span> <span class=\"n\">needs_projection</span><span class=\"p\">:</span>\n      <span class=\"n\">residual</span> <span class=\"o\">=</span> <span class=\"n\">conv</span><span class=\"p\">(</span><span class=\"n\">residual</span><span class=\"p\">,</span> <span class=\"n\">filters</span> <span class=\"o\">*</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'proj_conv'</span><span class=\"p\">)</span>\n      <span class=\"n\">residual</span> <span class=\"o\">=</span> <span class=\"n\">batch_norm</span><span class=\"p\">(</span><span class=\"n\">residual</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'proj_bn'</span><span class=\"p\">)</span>\n\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">conv</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">filters</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'conv1'</span><span class=\"p\">)</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">batch_norm</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'bn1'</span><span class=\"p\">)</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">conv</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">filters</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'conv2'</span><span class=\"p\">)</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">batch_norm</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'bn2'</span><span class=\"p\">)</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">conv</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">filters</span> <span class=\"o\">*</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'conv3'</span><span class=\"p\">)</span>\n\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">batch_norm</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'bn3'</span><span class=\"p\">,</span> <span class=\"n\">scale_init</span><span class=\"o\">=</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">initializers</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">)</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">residual</span> <span class=\"o\">+</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">y</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">ResNet</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n  <span class=\"k\">def</span> <span class=\"nf\">apply</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"p\">,</span> <span class=\"n\">num_filters</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">num_layers</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">,</span>\n            <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">jnp</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">num_layers</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">_block_size_options</span><span class=\"p\">:</span>\n      <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"s1\">'Please provide a valid number of layers'</span><span class=\"p\">)</span>\n    <span class=\"n\">block_sizes</span> <span class=\"o\">=</span> <span class=\"n\">_block_size_options</span><span class=\"p\">[</span><span class=\"n\">num_layers</span><span class=\"p\">]</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Conv</span><span class=\"p\">(</span>\n        <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">num_filters</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)],</span>\n        <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'init_conv'</span><span class=\"p\">)</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">BatchNorm</span><span class=\"p\">(</span>\n        <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">use_running_average</span><span class=\"o\">=</span><span class=\"ow\">not</span> <span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mf\">0.9</span><span class=\"p\">,</span>\n        <span class=\"n\">epsilon</span><span class=\"o\">=</span><span class=\"mf\">1e-5</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'init_bn'</span><span class=\"p\">)</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">max_pool</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">'SAME'</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">block_size</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">block_sizes</span><span class=\"p\">):</span>\n      <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">block_size</span><span class=\"p\">):</span>\n        <span class=\"n\">strides</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span> <span class=\"ow\">and</span> <span class=\"n\">j</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"k\">else</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">ResidualBlock</span><span class=\"p\">(</span>\n            <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">num_filters</span> <span class=\"o\">*</span> <span class=\"mi\">2</span> <span class=\"o\">**</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"n\">strides</span><span class=\"p\">,</span>\n            <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">jnp</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"p\">)</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">log_softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">x</span>\n</pre>\n<h2>Installation</h2>\n<p>You will need Python 3.6 or later.</p>\n<p>For GPU support, first install <code>jaxlib</code>; please follow the\ninstructions in the <a href=\"https://github.com/google/jax/blob/master/README.md\" rel=\"nofollow\">JAX\nreadme</a>.  If they\nare not already installed, you will need to install\n<a href=\"https://developer.nvidia.com/cuda-downloads\" rel=\"nofollow\">CUDA</a> and\n<a href=\"https://developer.nvidia.com/cudnn\" rel=\"nofollow\">CuDNN</a> runtimes.</p>\n<p>Then install <code>flax</code> from PyPi:</p>\n<pre><code>&gt; pip install flax\n</code></pre>\n<h2>TPU support</h2>\n<p>We currently have a <a href=\"https://colab.research.google.com/github/google/flax/blob/master/examples/lm1b/Colab_Language_Model.ipynb\" rel=\"nofollow\">LM1b/Wikitext-2 language model with a Transformer architecture</a>\nthat's been tuned. You can run it directly via Colab.</p>\n<p>At present, Cloud TPUs are network-attached, and Flax users typically feed in data from one or more additional VMs</p>\n<p>When working with large-scale input data, it is important to create large enough VMs with sufficient network bandwidth to avoid having the TPUs bottlenecked waiting for input</p>\n<p>TODO: Add an example for running on Google Cloud.</p>\n<h2>Getting involved</h2>\n<p>We welcome pull requests, in particular for those issues <a href=\"https://github.com/google/flax/issues?q=is%3Aopen+is%3Aissue+label%3A%22pull+requests+welcome%22\" rel=\"nofollow\">marked as PR-ready</a>. For other proposals, we ask that you first open an Issue to discuss your planned contribution.</p>\n<h2>Note</h2>\n<p>This is not an official Google product.</p>\n\n          </div>"}, "last_serial": 7140031, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "2c9bca997f32d96902655ea34900b7d1", "sha256": "2f5c21e8006e938fdf37b2748a802403fe13e90d4691e59531c5c2fa2cf5f0d0"}, "downloads": -1, "filename": "flax-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "2c9bca997f32d96902655ea34900b7d1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 77962, "upload_time": "2020-04-30T20:03:36", "upload_time_iso_8601": "2020-04-30T20:03:36.599166Z", "url": "https://files.pythonhosted.org/packages/13/f6/33d3a8e180787d4540421acf57d8a64175dc23dc29a9125c4f76977b0d75/flax-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "604050bc41adc22e99ec79eccb0e866a", "sha256": "94e6a05531009c44b000b9778603c6cceff43b8450e4059c22fd0dea93ea7b3b"}, "downloads": -1, "filename": "flax-0.1.0.tar.gz", "has_sig": false, "md5_digest": "604050bc41adc22e99ec79eccb0e866a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 51138, "upload_time": "2020-04-30T20:03:38", "upload_time_iso_8601": "2020-04-30T20:03:38.453275Z", "url": "https://files.pythonhosted.org/packages/6e/8d/e8afc69e4984c8513876948d7b462e21094211e7c62f2bfe3d51a951ede7/flax-0.1.0.tar.gz", "yanked": false}], "0.1.0rc1": [{"comment_text": "", "digests": {"md5": "19dafd44431cb1176469b67df2004e37", "sha256": "8c7cceaf0a1116aeb38d5a7f4b24d9baea5a9dddfa4fdf87802363ed2b887ea7"}, "downloads": -1, "filename": "flax-0.1.0rc1-py3-none-any.whl", "has_sig": false, "md5_digest": "19dafd44431cb1176469b67df2004e37", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 60168, "upload_time": "2020-03-17T10:21:38", "upload_time_iso_8601": "2020-03-17T10:21:38.652850Z", "url": "https://files.pythonhosted.org/packages/8c/5e/b4d781e8a1689ed1b71296be8eaf79e21d3e6e4e473a79b6362e65faed02/flax-0.1.0rc1-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "de4c79d3ef5c4a3c3695dc123ddc4d32", "sha256": "d88f24cd59e22fcfe4bfc037c88ecc34d3f7d9ce8db297c317421f7664d6b678"}, "downloads": -1, "filename": "flax-0.1.0rc1.tar.gz", "has_sig": false, "md5_digest": "de4c79d3ef5c4a3c3695dc123ddc4d32", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 47915, "upload_time": "2020-03-17T10:21:40", "upload_time_iso_8601": "2020-03-17T10:21:40.282211Z", "url": "https://files.pythonhosted.org/packages/3d/1b/f363177a24ec73bb28cc6425d85de8f4d589ab51871d9af1b8af4e7c1e2b/flax-0.1.0rc1.tar.gz", "yanked": false}], "0.1.0rc2": [{"comment_text": "", "digests": {"md5": "029a266dd17edafddb1a5e0b01378b2f", "sha256": "1d7b093b11efd7faee42262956ad3b62b7ad9c46f59671e6ba7fda5f2efb19f8"}, "downloads": -1, "filename": "flax-0.1.0rc2-py3-none-any.whl", "has_sig": false, "md5_digest": "029a266dd17edafddb1a5e0b01378b2f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 66196, "upload_time": "2020-03-18T10:51:34", "upload_time_iso_8601": "2020-03-18T10:51:34.133346Z", "url": "https://files.pythonhosted.org/packages/0e/8f/6d772aba2fa63aa6c1fd10d267324c44288c1c5705ac971c7d79cea219bb/flax-0.1.0rc2-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9780d569f5086cfc42a91382ca708e64", "sha256": "3cc797122d47ed69a28f93e81a66d316c5463e156f7bf2cc3bd1ef59cdcad93a"}, "downloads": -1, "filename": "flax-0.1.0rc2.tar.gz", "has_sig": false, "md5_digest": "9780d569f5086cfc42a91382ca708e64", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 51279, "upload_time": "2020-03-18T10:51:35", "upload_time_iso_8601": "2020-03-18T10:51:35.071793Z", "url": "https://files.pythonhosted.org/packages/91/30/955d6b6143c9d6b819ba85ebce654aae7a235017962fa8e8519110e5ac5f/flax-0.1.0rc2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "2c9bca997f32d96902655ea34900b7d1", "sha256": "2f5c21e8006e938fdf37b2748a802403fe13e90d4691e59531c5c2fa2cf5f0d0"}, "downloads": -1, "filename": "flax-0.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "2c9bca997f32d96902655ea34900b7d1", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 77962, "upload_time": "2020-04-30T20:03:36", "upload_time_iso_8601": "2020-04-30T20:03:36.599166Z", "url": "https://files.pythonhosted.org/packages/13/f6/33d3a8e180787d4540421acf57d8a64175dc23dc29a9125c4f76977b0d75/flax-0.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "604050bc41adc22e99ec79eccb0e866a", "sha256": "94e6a05531009c44b000b9778603c6cceff43b8450e4059c22fd0dea93ea7b3b"}, "downloads": -1, "filename": "flax-0.1.0.tar.gz", "has_sig": false, "md5_digest": "604050bc41adc22e99ec79eccb0e866a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 51138, "upload_time": "2020-04-30T20:03:38", "upload_time_iso_8601": "2020-04-30T20:03:38.453275Z", "url": "https://files.pythonhosted.org/packages/6e/8d/e8afc69e4984c8513876948d7b462e21094211e7c62f2bfe3d51a951ede7/flax-0.1.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 01:02:02 2020"}