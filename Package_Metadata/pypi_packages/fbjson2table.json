{"info": {"author": "Numbers", "author_email": "hi@numbersprotocol.io", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "# Abstract\n\n   This repository helps you parse JSON files in the Facebook archive to tables so that it is easier to analyze your own Facebook data. In this README, we will first introduce the steps to setup the environment and to use the tool. You may find more information about the reason why you may (or may not) need this tool.\n\n\n   The project was initiated during the collaboration of Spring App with [Bitmark Inc.](https://github.com/bitmark-inc). \n   If you are not a developer, have no idea how to use this tool but still hope to see your own Facebook data, you can download the Spring App from:\n   * Android Play Store [download](https://play.google.com/store/apps/details?id=com.bitmark.spring)\n   * iOS App Store [download](https://install.appcenter.ms/orgs/support-zzd0-28/apps/spring-inhouse/distribution_groups/users)\n\n# Setup and start using it\n\n## Requirement\n\n   pandas >= 0.24.1\n\n## Before you start\n\n   In order to analyze your Facebook data, you first need to get a copy of your data from Facebook. Please follow the [instruction](https://www.facebook.com/help/1701730696756992?helpref=hc_global_nav) provided by Facebook and download. [This post](https://www.wired.com/story/download-facebook-data-how-to-read/) also provides good instructions on how to download your own Facebook data and how it looks.\n\n\n   The downloaded data is a big zip archive with the size from a hundred MB to a few GB. Be patient and unzip the archive. Pick one **JSON file in the archive** as the `PATH_OF_JSON` and execute the sample code below. For example, you can use `$PATH_OF_ARCHIVE/ads/ads_interests.json` as `PATH_OF_JSON` to test the following Hello World sample.\n\n\nThe tree structure of the folder should look like\n\n\n```\n.\n\u251c about_you\n\u2502\u00a0\u00a0 \u251c face_recognition.json\n\u2502\u00a0\u00a0 \u251c friend_peer_group.json\n\u2502\u00a0\u00a0 \u2514 your_address_books.json\n\u251c ads\n\u2502\u00a0\u00a0 \u251c ads_interests.json\n\u2502\u00a0\u00a0 \u2514 advertisers_who_uploaded_a_contact_list_with_your_information.json\n\u251c apps_and_websites\n\u2502\u00a0\u00a0 \u251c apps_and_websites.json\n\u2502\u00a0\u00a0 \u2514 posts_from_apps_and_websites.json\n\u251c calls_and_messages\n\u2502\u00a0\u00a0 \u2514 no-data.txt\n\u251c comments\n\u2502\u00a0\u00a0 \u2514 comments.json\n\n...\n\n```\n\n## Setup environment\n\n### Install from PyPI\n\nUser-level installation\n\n```\n$ pip3 install --user fbjson2table\n```\n\nSystem-level installation\n\n```\n$ sudo pip3 install fbjson2table\n```\n\n### Install from Source\n\n1. Clone this repository\n\n    ```\n    $ git clone https://github.com/numbersprotocol/fb-json2table.git\n    ```\n\n1. Export PYTHPNPATH\n\n    ```\n    $ export PYTHONPATH=$PWD/fb-json2table/:$PYTHONPATH\n    ```\n\n    or you may use `virtualenv` to keep the dev environment clean\n\n    ```\n    $ git clone https://github.com/numbersprotocol/fb-json2table.git\n    $ pip3 install virtualenv\n    $ virtualenv -p python3 env\n    $ source env/bin/activate\n    $ (env) python3 setup.py bdist_wheel\n    $ (env) pip3 install dist/fbjson2table-1.0.0-py3-none-any.whl\n\n    # if you want to run example/examply.py\n    $ (env) pip3 install tabulate\n    ```\n\n## TL;DR (The Hello World example)\n\n```\nfrom fbjson2table.func_lib import parse_fb_json\nfrom fbjson2table.table_class import TempDFs\n\n\njson_content = parse_fb_json($PATH_OF_JSON)\ntemp_dfs = TempDFs(json_content)\n\nfor df in temp_dfs.df_list:\n    print(df)\n```\n\nthe example above turns JSON files in your `PATH_OF_JSON` into table-like DataFrame.\nand you will be able to start analyzing content in the `df`.\n\nPlease note! If your data contains special characters or non-English words, you will need to handle the encoding properly before you can analyze it.\n\n# More introduction about fb-json2table\n\nJSON is not friendly to data scientists, we love tables.\n\nThe purpose of this repository is to automate the parsing process of JSON files in the downloaded Facebook archive,\nturn those JSON files (not easy-to-analyze) to tables (easy-to-analyze) so that it can be easier for data scientists to analyze. \nThis repository is not only for data scientists, the ultimate goal is to reduce the entry barrier for anyone who wants to analyze their own data.\n\nActually, it can also be used to **turn any data with FB-LIKE JSON structure to table** as long as the data structure if similar.\n\n\nHere is an example of **FB-LIKE JSON structure**\n\n```\n[\n  {\n    feature_1: feature_1_of_record_1,\n    feature_2: feature_2_of_record_1,\n    ...\n   },\n   {\n    feature_1: feature_1_of_record_2,\n    feature_2: feature_2_of_record_2,\n    ...\n   },\n   ...\n]\n```\n\n[This simple document](https://github.com/numbersprotocol/fb-json2table/blob/master/dict_list_combination_to_table.txt) shows the logic behind the JSON to Table conversions. If you are not sure whether or not your data is *FB-LIKE*, it might give you some hint.\n\n## why Facebook json is not friendly to analyze?\n\nhere is an example of Facebook json: [example_facebook_json](https://github.com/numbersprotocol/fb-json2table/blob/master/example/example_facebook_json.json)\n\nWe can find that if we want to analyze the relationship between reaction type (\"LIKE\" or \"WOW\") and time by using python,\n\nwe have to write code like:\n\n```\ntimestamp = [x[\"timestamp\"] for x in example_json[\"reactions\"]]\nreaction_type = [x[\"data\"][0][\"reactions\"][\"reactions\"] for x in example_json[\"reactions\"]]\n```\n\nWe have to specify many keys in our code.\nFurthermore, make things more difficult is that, if one record does not have one feature, in Facebook json, it will not display instead of displaying \"null\"\n, and we do not really know how many features Facebook records. \n\nOr at least, I do not find a formal document writed kinds of data recorded by Facebook.\n\nTake a look of [example_json_content](https://github.com/numbersprotocol/fb-json2table/blob/master/example/example_json_content.json), this is example of `your_post.json`, we can\nfind that if one post does not have photo, that post will not have features of photo. If the json is too long for naked eyes, we may ignore some interesting \ndata recorded by Facebook!\n\nFinally, the most bothering and making automation almost impossible is that, the structure of Facebook json may change, and have changed! And the worst is that,\n\n#### !!!!Facebook will not notify you!!!!\n\nFor example, the data I download long time ago, I can find posts in \"posts/your_posts.json/\", and the content is like:\n\n```\n{\n  \"status_updates\": [\n    {\n      \"timestamp\": 1415339550,\n      ...\n```\n\nThe data I download recently, if I want to find posts, I should go to \"posts/your_posts_1.json/\", the filename have changed, and the content is like:\n\n```\n[\n  {\n    \"timestamp\": 1575375973,\n    ...\n```\n\nWe can find that in new structure, we do not have to and shoud not to specify \"status_updates\", and if we load new json into our old code, it will raise many\n\"KeyError\". Furthermore, there may be other changing in the json content.\n\n## the goal of this repo\n\n1. turn the Facebook json to table\n\n2. decrease the things should be speficfied\n\n3. make the code robust to changing of json structure, or make it easier to fix when json structure changes\n\n# How to use\n\n1. load json\n\n    you can load json by your own method, or use the function we write for Facebook json to handling mojibake.\n\n    ```\n    from fbjson2table.func_lib import parse_fb_json\n\n    json_content = parse_fb_json($PATH_OF_JSON)\n    ```\n\n2. feed it into \"TempDFs\", and take a look of \"TempDFs.df_list\" and \"TempDFs.table_name_list\",\n\n    ```\n    from tabulate import tabulate\n    from fbjson2table.table_class import TempDFs\n\n    temp_dfs = TempDFs(json_content)\n    for df, table_name in zip(temp_dfs.df_list, temp_dfs.table_name_list):\n        print(table_name, ':')\n        print(tabulate(df, headers='keys', tablefmt='psql'), '\\n')\n    ```\n\n    here is example of [json_content](https://github.com/numbersprotocol/fb-json2table/blob/master/example/example_json_content.json)\n\n    here is example of [TempDFs.df_list and TempDFs.table_name_list](https://github.com/numbersprotocol/fb-json2table/blob/master/example/example_df_list.txt)\n\n    #### explanation: \n\n    Every df has its own name, the first df is default named with \"temp\", for the follownig dfs will concat \"__DICT_KEY \" \n    as suffix.\n\n    Every df has id of its own \"depth(peeling)\", and all ids of connected upper layer. The id of first depth is always named \"id_0\", and the\n    following id is named with \"id_DICT_KEY_DEPTH\", example: \"id_attachment_1\".\n\n    With the ids, we can do the \"join\" operation. For example, if we want to put \"uri\" of \"media\" and \"timestamp\" of posts in same table,\n    the code will like:\n\n    ```\n    top_df = temp_posts_dfs[0].set_index(\"id_0\", drop=False)\n    append_df = temp_posts_df[4].set_index(\"id_0\", drop=False)\n\n    wanted_df = top_df.join(append_df) # What we want\n    ```\n\n    If you are lazy to find where is the data you want, and you confirm that the data is one-to-one relationship with \"top_df\",\n    you can use \"merge_one_to_one_sub_df.\"\n\n    example:\n\n    ```\n    one_to_one_df = temp_dfs.merge_one_to_one_sub_df(\n                    temp_dfs.df_list,\n                    temp_dfs.table_name_list,\n                    temp_dfs.id_column_names_list,\n                    start_peeling=0) # start_peeling is the index of df we want to set as \"top_df\" in df_list\n    ```\n\n    note: in the \"one_to_one_df\", all column names of sub dfs will concat its depth dict key as prefix. For example,\n    \"id_media_3\" => \"media_id_media_3\".\n\n## explanation of terms\n\n   #### depth(peeling)\n\n   In the json, every dict will add one depth(peeling). We count depth from 0.\n\n   For example, \n\n   `dummy_dict = {\"a\": \"b\", \"c\":{\"aa\": \"bb\", \"cc\": \"d\"}}`,\n\n   \"a\" is at depth 0, \"aa\" is at depth 1.\n\n   Because in normal method, if we want to get \"b\" or \"bb\", we should write `dummy_dict[\"a\"]` or `dummy_dict[\"c\"][\"aa\"]`.\n   We have to specify 1 or 2 keys, so the depth is 0 or 1.\n\n   #### top_df, sub_df\n\n   Sub_df are those dfs with table name containing the table name of the specifics. For example, \"temp__attachments__data\", \"temp__attachments__data__media\" and so on, are sub_dfs of \"temp__attachments\".\n\n   The sub_df can be viewd as one column but recording mutilple value of one df.\n\n   Take \"dummy_dict\" as example, this repo will turn it into,\n\n   ```\n   temp\n   id_0| a |\n   ----+---+\n     0 | b |      \n   ----+---+\n\n   temp_c(table name)\n   id_0|id_c_1| aa | cc |\n   ----+------+----+----+\n     0 |  0   | bb | d  |\n   ----+------+----+----+\n   ```\n\n   but it can be viewed as\n\n   ```\n   temp\n   id_0| a | c                      |\n   ----+---+------------------------+\n     0 | b |{\"aa\": \"bb\", \"cc\": \"d\"} |    \n   ----+---+------------------------+\n   ```\n\n   The \"temp_c\" is like something growing from \"temp\", so I call it sub_df.\n\n   Top_df refers to the base df when we want to merge sub_df.\n\n# Automation\n\n   ## TL;DR\n\n   For most case of Facebook json\n\n   1.\n\n   ```\n   from fbjson2table.func_lib import parse_fb_json\n   from fbjson2table.table_class import TempDFs\n\n\n   json_content = parse_fb_json($PATH_OF_JSON)\n   temp_dfs = TempDFs(json_content)\n   one_to_one_df, _ = temp_dfs.temp_to_wanted_df(\n                 wanted_columns=[]\n                 ) \n   ```\n\n   Take a look of one_to_one_df, and determine which columns we want.\n\n   ```\n   print(one_to_one_df.columns)\n   ```\n\n   2.\n\n   ```\n   wanted_columns = LIST_OF_WANTED_COLUMNS\n\n   df, top_id = temp_dfs.temp_to_wanted_df(\n            wanted_columns=wanted_columns\n        )\n   ```\n\n   Then \"df\" is what we want.\n\n   ## Take a look of `temp_to_wanted_df`\n\n   For the reason that Facebook may change json structure, we developed a series of methods to handle this problem.\n\n   The core concept is that, \"putting all things in one table, then from the table take what we want.\"\n\n   Take a look of: [link](https://github.com/numbersprotocol/fb-json2table/blob/3f5f4b8741727c8dd2aa3f4f95030e3f23d53554/fbjson2table/table_class.py#L529)\n\n   The first thing we do is `get_routed_dfs`.\n\n   => Because that \"df\" in \"df_list\" may diverge, we have to get the dfs have the same \"route\" of \"top_df\".\n\n   Second, we do `get_start_peeling`.\n\n   => We have to find the index of top_df in df_list for next step.\n\n   Then, we do `merge_one_to_one_sub_df`.\n\n   => Merge all one-to-one sub_dfs.\n\n   Finally, we do `get_wanted_columns`.\n\n   => Extract the columns what we want, and return NaN when the column do not exist.\n\n   In `temp_to_wanted_df`, there are four parameters we should specify, `temp_to_wanted_df`, `wanted_columns`,`route_by_table_name`, `start_by`, and `regex`.\n\n   So, how to input these value?\n\n   The `wanted_columns` is a list of names of columns we want, or we can input `[]`, it will return all columns.\n\n   The `route_by_table_name` is the last table name suffix of top_df. \n\n   It is be used when we want to choose one df as top_df, and that df is in a diverging branch of df_list.\n\n   Take [example_df_list](https://github.com/numbersprotocol/fb-json2table/blob/master/example/example_df_list.txt) for example,\n\n   If we plot the relationship of df_list:\n\n   ![Image of relation of df_list](https://github.com/numbersprotocol/fb-json2table/blob/master/images/example_df_list_relationship.png)\n\n   We can find there are two route in the picture. When we want to merge `temp__attachments__data__media` and its sub_dfs, we have to \n   extract them in df_list. Or `temp__data` may be a problem when we do the `join` operation.\n\n   When we set `route_by_table_name` eqauls `\"media\"`, we will get the route like:\n\n   ![Image of route media](https://github.com/numbersprotocol/fb-json2table/blob/master/images/route_of_temp__attachments__data__media.png)\n\n   The `start_by` is the unique column name of top_df in routed_df_list.\n\n   When we do `join` operation, we have to choose one df in df_list as our top_df. Take the [example_df_list](https://github.com/numbersprotocol/fb-json2table/blob/master/example/example_df_list.txt), if we want to set \"temp__attachments__data__media\" as top_df, we have to find the unique column name of it. For example, \"uri\". By the way, the \"title\" will not work because \"title\" also appear in \"temp.\"\n\n   The `regex` is whether to use \"regex\" when find `start_by`.\n\n   Thus, if we want get the \"creation_timestamp\", \"description\", \"title\", \"uri\", and \"upload_ip\" of photos of posts in one table, the whole process will be like:\n\n   ```\n   from fbjson2table.func_lib import parse_fb_json\n   from fbjson2table.table_class import TempDFs\n\n\n   json_content = parse_fb_json($PATH_OF_JSON)\n   temp_dfs = TempDFs(json_content)\n   one_to_one_df, _ = temp_dfs.temp_to_wanted_df(\n                      route_by_table_name='media',\n                      strat_by='uri'\n                      )\n   print(one_to_one_df.columns)\n   ```\n\n   get\n\n   ```\n   Index(['creation_timestamp', 'description', 'id_0', 'id_attachments_1',\n       'id_data_2', 'id_media_3', 'title', 'uri', 'media_metadata_id_0',\n       'media_metadata_id_attachments_1', 'media_metadata_id_data_2',\n       'media_metadata_id_media_metadata_4', 'photo_metadata_id_0',\n       'photo_metadata_id_attachments_1', 'photo_metadata_id_data_2',\n       'photo_metadata_id_media_metadata_4',\n       'photo_metadata_id_photo_metadata_5', 'photo_metadata_upload_ip'],\n      dtype='object')\n   ```\n\n   Then,\n\n   ```\n   wanted_columns = ['creation_timestamp', 'description', 'title', 'uri',\n                     'photo_metadata_upload_ip']\n   photo_df, _ = temp_dfs.temp_to_wanted_df(\n                          wanted_columns=wanted_columns,\n                          route_by_table_name='media',\n                          strat_by='uri'\n                      )\n   ```\n\n   The `photo_df` is what we want.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/numbersprotocol/fb-json2table", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "fbjson2table", "package_url": "https://pypi.org/project/fbjson2table/", "platform": "", "project_url": "https://pypi.org/project/fbjson2table/", "project_urls": {"Homepage": "https://github.com/numbersprotocol/fb-json2table"}, "release_url": "https://pypi.org/project/fbjson2table/1.2.0/", "requires_dist": ["pandas (>=0.24.1)"], "requires_python": ">=3.5", "summary": "Parse Facebook archive JSON files to tables", "version": "1.2.0", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Abstract</h1>\n<p>This repository helps you parse JSON files in the Facebook archive to tables so that it is easier to analyze your own Facebook data. In this README, we will first introduce the steps to setup the environment and to use the tool. You may find more information about the reason why you may (or may not) need this tool.</p>\n<p>The project was initiated during the collaboration of Spring App with <a href=\"https://github.com/bitmark-inc\" rel=\"nofollow\">Bitmark Inc.</a>.\nIf you are not a developer, have no idea how to use this tool but still hope to see your own Facebook data, you can download the Spring App from:</p>\n<ul>\n<li>Android Play Store <a href=\"https://play.google.com/store/apps/details?id=com.bitmark.spring\" rel=\"nofollow\">download</a></li>\n<li>iOS App Store <a href=\"https://install.appcenter.ms/orgs/support-zzd0-28/apps/spring-inhouse/distribution_groups/users\" rel=\"nofollow\">download</a></li>\n</ul>\n<h1>Setup and start using it</h1>\n<h2>Requirement</h2>\n<p>pandas &gt;= 0.24.1</p>\n<h2>Before you start</h2>\n<p>In order to analyze your Facebook data, you first need to get a copy of your data from Facebook. Please follow the <a href=\"https://www.facebook.com/help/1701730696756992?helpref=hc_global_nav\" rel=\"nofollow\">instruction</a> provided by Facebook and download. <a href=\"https://www.wired.com/story/download-facebook-data-how-to-read/\" rel=\"nofollow\">This post</a> also provides good instructions on how to download your own Facebook data and how it looks.</p>\n<p>The downloaded data is a big zip archive with the size from a hundred MB to a few GB. Be patient and unzip the archive. Pick one <strong>JSON file in the archive</strong> as the <code>PATH_OF_JSON</code> and execute the sample code below. For example, you can use <code>$PATH_OF_ARCHIVE/ads/ads_interests.json</code> as <code>PATH_OF_JSON</code> to test the following Hello World sample.</p>\n<p>The tree structure of the folder should look like</p>\n<pre><code>.\n\u251c about_you\n\u2502\u00a0\u00a0 \u251c face_recognition.json\n\u2502\u00a0\u00a0 \u251c friend_peer_group.json\n\u2502\u00a0\u00a0 \u2514 your_address_books.json\n\u251c ads\n\u2502\u00a0\u00a0 \u251c ads_interests.json\n\u2502\u00a0\u00a0 \u2514 advertisers_who_uploaded_a_contact_list_with_your_information.json\n\u251c apps_and_websites\n\u2502\u00a0\u00a0 \u251c apps_and_websites.json\n\u2502\u00a0\u00a0 \u2514 posts_from_apps_and_websites.json\n\u251c calls_and_messages\n\u2502\u00a0\u00a0 \u2514 no-data.txt\n\u251c comments\n\u2502\u00a0\u00a0 \u2514 comments.json\n\n...\n\n</code></pre>\n<h2>Setup environment</h2>\n<h3>Install from PyPI</h3>\n<p>User-level installation</p>\n<pre><code>$ pip3 install --user fbjson2table\n</code></pre>\n<p>System-level installation</p>\n<pre><code>$ sudo pip3 install fbjson2table\n</code></pre>\n<h3>Install from Source</h3>\n<ol>\n<li>\n<p>Clone this repository</p>\n<pre><code>$ git clone https://github.com/numbersprotocol/fb-json2table.git\n</code></pre>\n</li>\n<li>\n<p>Export PYTHPNPATH</p>\n<pre><code>$ export PYTHONPATH=$PWD/fb-json2table/:$PYTHONPATH\n</code></pre>\n<p>or you may use <code>virtualenv</code> to keep the dev environment clean</p>\n<pre><code>$ git clone https://github.com/numbersprotocol/fb-json2table.git\n$ pip3 install virtualenv\n$ virtualenv -p python3 env\n$ source env/bin/activate\n$ (env) python3 setup.py bdist_wheel\n$ (env) pip3 install dist/fbjson2table-1.0.0-py3-none-any.whl\n\n# if you want to run example/examply.py\n$ (env) pip3 install tabulate\n</code></pre>\n</li>\n</ol>\n<h2>TL;DR (The Hello World example)</h2>\n<pre><code>from fbjson2table.func_lib import parse_fb_json\nfrom fbjson2table.table_class import TempDFs\n\n\njson_content = parse_fb_json($PATH_OF_JSON)\ntemp_dfs = TempDFs(json_content)\n\nfor df in temp_dfs.df_list:\n    print(df)\n</code></pre>\n<p>the example above turns JSON files in your <code>PATH_OF_JSON</code> into table-like DataFrame.\nand you will be able to start analyzing content in the <code>df</code>.</p>\n<p>Please note! If your data contains special characters or non-English words, you will need to handle the encoding properly before you can analyze it.</p>\n<h1>More introduction about fb-json2table</h1>\n<p>JSON is not friendly to data scientists, we love tables.</p>\n<p>The purpose of this repository is to automate the parsing process of JSON files in the downloaded Facebook archive,\nturn those JSON files (not easy-to-analyze) to tables (easy-to-analyze) so that it can be easier for data scientists to analyze.\nThis repository is not only for data scientists, the ultimate goal is to reduce the entry barrier for anyone who wants to analyze their own data.</p>\n<p>Actually, it can also be used to <strong>turn any data with FB-LIKE JSON structure to table</strong> as long as the data structure if similar.</p>\n<p>Here is an example of <strong>FB-LIKE JSON structure</strong></p>\n<pre><code>[\n  {\n    feature_1: feature_1_of_record_1,\n    feature_2: feature_2_of_record_1,\n    ...\n   },\n   {\n    feature_1: feature_1_of_record_2,\n    feature_2: feature_2_of_record_2,\n    ...\n   },\n   ...\n]\n</code></pre>\n<p><a href=\"https://github.com/numbersprotocol/fb-json2table/blob/master/dict_list_combination_to_table.txt\" rel=\"nofollow\">This simple document</a> shows the logic behind the JSON to Table conversions. If you are not sure whether or not your data is <em>FB-LIKE</em>, it might give you some hint.</p>\n<h2>why Facebook json is not friendly to analyze?</h2>\n<p>here is an example of Facebook json: <a href=\"https://github.com/numbersprotocol/fb-json2table/blob/master/example/example_facebook_json.json\" rel=\"nofollow\">example_facebook_json</a></p>\n<p>We can find that if we want to analyze the relationship between reaction type (\"LIKE\" or \"WOW\") and time by using python,</p>\n<p>we have to write code like:</p>\n<pre><code>timestamp = [x[\"timestamp\"] for x in example_json[\"reactions\"]]\nreaction_type = [x[\"data\"][0][\"reactions\"][\"reactions\"] for x in example_json[\"reactions\"]]\n</code></pre>\n<p>We have to specify many keys in our code.\nFurthermore, make things more difficult is that, if one record does not have one feature, in Facebook json, it will not display instead of displaying \"null\"\n, and we do not really know how many features Facebook records.</p>\n<p>Or at least, I do not find a formal document writed kinds of data recorded by Facebook.</p>\n<p>Take a look of <a href=\"https://github.com/numbersprotocol/fb-json2table/blob/master/example/example_json_content.json\" rel=\"nofollow\">example_json_content</a>, this is example of <code>your_post.json</code>, we can\nfind that if one post does not have photo, that post will not have features of photo. If the json is too long for naked eyes, we may ignore some interesting\ndata recorded by Facebook!</p>\n<p>Finally, the most bothering and making automation almost impossible is that, the structure of Facebook json may change, and have changed! And the worst is that,</p>\n<h4>!!!!Facebook will not notify you!!!!</h4>\n<p>For example, the data I download long time ago, I can find posts in \"posts/your_posts.json/\", and the content is like:</p>\n<pre><code>{\n  \"status_updates\": [\n    {\n      \"timestamp\": 1415339550,\n      ...\n</code></pre>\n<p>The data I download recently, if I want to find posts, I should go to \"posts/your_posts_1.json/\", the filename have changed, and the content is like:</p>\n<pre><code>[\n  {\n    \"timestamp\": 1575375973,\n    ...\n</code></pre>\n<p>We can find that in new structure, we do not have to and shoud not to specify \"status_updates\", and if we load new json into our old code, it will raise many\n\"KeyError\". Furthermore, there may be other changing in the json content.</p>\n<h2>the goal of this repo</h2>\n<ol>\n<li>\n<p>turn the Facebook json to table</p>\n</li>\n<li>\n<p>decrease the things should be speficfied</p>\n</li>\n<li>\n<p>make the code robust to changing of json structure, or make it easier to fix when json structure changes</p>\n</li>\n</ol>\n<h1>How to use</h1>\n<ol>\n<li>\n<p>load json</p>\n<p>you can load json by your own method, or use the function we write for Facebook json to handling mojibake.</p>\n<pre><code>from fbjson2table.func_lib import parse_fb_json\n\njson_content = parse_fb_json($PATH_OF_JSON)\n</code></pre>\n</li>\n<li>\n<p>feed it into \"TempDFs\", and take a look of \"TempDFs.df_list\" and \"TempDFs.table_name_list\",</p>\n<pre><code>from tabulate import tabulate\nfrom fbjson2table.table_class import TempDFs\n\ntemp_dfs = TempDFs(json_content)\nfor df, table_name in zip(temp_dfs.df_list, temp_dfs.table_name_list):\n    print(table_name, ':')\n    print(tabulate(df, headers='keys', tablefmt='psql'), '\\n')\n</code></pre>\n<p>here is example of <a href=\"https://github.com/numbersprotocol/fb-json2table/blob/master/example/example_json_content.json\" rel=\"nofollow\">json_content</a></p>\n<p>here is example of <a href=\"https://github.com/numbersprotocol/fb-json2table/blob/master/example/example_df_list.txt\" rel=\"nofollow\">TempDFs.df_list and TempDFs.table_name_list</a></p>\n<h4>explanation:</h4>\n<p>Every df has its own name, the first df is default named with \"temp\", for the follownig dfs will concat \"__DICT_KEY \"\nas suffix.</p>\n<p>Every df has id of its own \"depth(peeling)\", and all ids of connected upper layer. The id of first depth is always named \"id_0\", and the\nfollowing id is named with \"id_DICT_KEY_DEPTH\", example: \"id_attachment_1\".</p>\n<p>With the ids, we can do the \"join\" operation. For example, if we want to put \"uri\" of \"media\" and \"timestamp\" of posts in same table,\nthe code will like:</p>\n<pre><code>top_df = temp_posts_dfs[0].set_index(\"id_0\", drop=False)\nappend_df = temp_posts_df[4].set_index(\"id_0\", drop=False)\n\nwanted_df = top_df.join(append_df) # What we want\n</code></pre>\n<p>If you are lazy to find where is the data you want, and you confirm that the data is one-to-one relationship with \"top_df\",\nyou can use \"merge_one_to_one_sub_df.\"</p>\n<p>example:</p>\n<pre><code>one_to_one_df = temp_dfs.merge_one_to_one_sub_df(\n                temp_dfs.df_list,\n                temp_dfs.table_name_list,\n                temp_dfs.id_column_names_list,\n                start_peeling=0) # start_peeling is the index of df we want to set as \"top_df\" in df_list\n</code></pre>\n<p>note: in the \"one_to_one_df\", all column names of sub dfs will concat its depth dict key as prefix. For example,\n\"id_media_3\" =&gt; \"media_id_media_3\".</p>\n</li>\n</ol>\n<h2>explanation of terms</h2>\n<h4>depth(peeling)</h4>\n<p>In the json, every dict will add one depth(peeling). We count depth from 0.</p>\n<p>For example,</p>\n<p><code>dummy_dict = {\"a\": \"b\", \"c\":{\"aa\": \"bb\", \"cc\": \"d\"}}</code>,</p>\n<p>\"a\" is at depth 0, \"aa\" is at depth 1.</p>\n<p>Because in normal method, if we want to get \"b\" or \"bb\", we should write <code>dummy_dict[\"a\"]</code> or <code>dummy_dict[\"c\"][\"aa\"]</code>.\nWe have to specify 1 or 2 keys, so the depth is 0 or 1.</p>\n<h4>top_df, sub_df</h4>\n<p>Sub_df are those dfs with table name containing the table name of the specifics. For example, \"temp__attachments__data\", \"temp__attachments__data__media\" and so on, are sub_dfs of \"temp__attachments\".</p>\n<p>The sub_df can be viewd as one column but recording mutilple value of one df.</p>\n<p>Take \"dummy_dict\" as example, this repo will turn it into,</p>\n<pre><code>temp\nid_0| a |\n----+---+\n  0 | b |      \n----+---+\n\ntemp_c(table name)\nid_0|id_c_1| aa | cc |\n----+------+----+----+\n  0 |  0   | bb | d  |\n----+------+----+----+\n</code></pre>\n<p>but it can be viewed as</p>\n<pre><code>temp\nid_0| a | c                      |\n----+---+------------------------+\n  0 | b |{\"aa\": \"bb\", \"cc\": \"d\"} |    \n----+---+------------------------+\n</code></pre>\n<p>The \"temp_c\" is like something growing from \"temp\", so I call it sub_df.</p>\n<p>Top_df refers to the base df when we want to merge sub_df.</p>\n<h1>Automation</h1>\n<h2>TL;DR</h2>\n<p>For most case of Facebook json</p>\n<ol>\n<li>\n</ol>\n<pre><code>from fbjson2table.func_lib import parse_fb_json\nfrom fbjson2table.table_class import TempDFs\n\n\njson_content = parse_fb_json($PATH_OF_JSON)\ntemp_dfs = TempDFs(json_content)\none_to_one_df, _ = temp_dfs.temp_to_wanted_df(\n              wanted_columns=[]\n              ) \n</code></pre>\n<p>Take a look of one_to_one_df, and determine which columns we want.</p>\n<pre><code>print(one_to_one_df.columns)\n</code></pre>\n<ol>\n<li>\n</ol>\n<pre><code>wanted_columns = LIST_OF_WANTED_COLUMNS\n\ndf, top_id = temp_dfs.temp_to_wanted_df(\n         wanted_columns=wanted_columns\n     )\n</code></pre>\n<p>Then \"df\" is what we want.</p>\n<h2>Take a look of <code>temp_to_wanted_df</code></h2>\n<p>For the reason that Facebook may change json structure, we developed a series of methods to handle this problem.</p>\n<p>The core concept is that, \"putting all things in one table, then from the table take what we want.\"</p>\n<p>Take a look of: <a href=\"https://github.com/numbersprotocol/fb-json2table/blob/3f5f4b8741727c8dd2aa3f4f95030e3f23d53554/fbjson2table/table_class.py#L529\" rel=\"nofollow\">link</a></p>\n<p>The first thing we do is <code>get_routed_dfs</code>.</p>\n<p>=&gt; Because that \"df\" in \"df_list\" may diverge, we have to get the dfs have the same \"route\" of \"top_df\".</p>\n<p>Second, we do <code>get_start_peeling</code>.</p>\n<p>=&gt; We have to find the index of top_df in df_list for next step.</p>\n<p>Then, we do <code>merge_one_to_one_sub_df</code>.</p>\n<p>=&gt; Merge all one-to-one sub_dfs.</p>\n<p>Finally, we do <code>get_wanted_columns</code>.</p>\n<p>=&gt; Extract the columns what we want, and return NaN when the column do not exist.</p>\n<p>In <code>temp_to_wanted_df</code>, there are four parameters we should specify, <code>temp_to_wanted_df</code>, <code>wanted_columns</code>,<code>route_by_table_name</code>, <code>start_by</code>, and <code>regex</code>.</p>\n<p>So, how to input these value?</p>\n<p>The <code>wanted_columns</code> is a list of names of columns we want, or we can input <code>[]</code>, it will return all columns.</p>\n<p>The <code>route_by_table_name</code> is the last table name suffix of top_df.</p>\n<p>It is be used when we want to choose one df as top_df, and that df is in a diverging branch of df_list.</p>\n<p>Take <a href=\"https://github.com/numbersprotocol/fb-json2table/blob/master/example/example_df_list.txt\" rel=\"nofollow\">example_df_list</a> for example,</p>\n<p>If we plot the relationship of df_list:</p>\n<p><img alt=\"Image of relation of df_list\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/236649fffe8b95036b0dd141dcbdd6d628b2460b/68747470733a2f2f6769746875622e636f6d2f6e756d6265727370726f746f636f6c2f66622d6a736f6e327461626c652f626c6f622f6d61737465722f696d616765732f6578616d706c655f64665f6c6973745f72656c6174696f6e736869702e706e67\"></p>\n<p>We can find there are two route in the picture. When we want to merge <code>temp__attachments__data__media</code> and its sub_dfs, we have to\nextract them in df_list. Or <code>temp__data</code> may be a problem when we do the <code>join</code> operation.</p>\n<p>When we set <code>route_by_table_name</code> eqauls <code>\"media\"</code>, we will get the route like:</p>\n<p><img alt=\"Image of route media\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/534341fae5014478cc24023b401abf49476191fa/68747470733a2f2f6769746875622e636f6d2f6e756d6265727370726f746f636f6c2f66622d6a736f6e327461626c652f626c6f622f6d61737465722f696d616765732f726f7574655f6f665f74656d705f5f6174746163686d656e74735f5f646174615f5f6d656469612e706e67\"></p>\n<p>The <code>start_by</code> is the unique column name of top_df in routed_df_list.</p>\n<p>When we do <code>join</code> operation, we have to choose one df in df_list as our top_df. Take the <a href=\"https://github.com/numbersprotocol/fb-json2table/blob/master/example/example_df_list.txt\" rel=\"nofollow\">example_df_list</a>, if we want to set \"temp__attachments__data__media\" as top_df, we have to find the unique column name of it. For example, \"uri\". By the way, the \"title\" will not work because \"title\" also appear in \"temp.\"</p>\n<p>The <code>regex</code> is whether to use \"regex\" when find <code>start_by</code>.</p>\n<p>Thus, if we want get the \"creation_timestamp\", \"description\", \"title\", \"uri\", and \"upload_ip\" of photos of posts in one table, the whole process will be like:</p>\n<pre><code>from fbjson2table.func_lib import parse_fb_json\nfrom fbjson2table.table_class import TempDFs\n\n\njson_content = parse_fb_json($PATH_OF_JSON)\ntemp_dfs = TempDFs(json_content)\none_to_one_df, _ = temp_dfs.temp_to_wanted_df(\n                   route_by_table_name='media',\n                   strat_by='uri'\n                   )\nprint(one_to_one_df.columns)\n</code></pre>\n<p>get</p>\n<pre><code>Index(['creation_timestamp', 'description', 'id_0', 'id_attachments_1',\n    'id_data_2', 'id_media_3', 'title', 'uri', 'media_metadata_id_0',\n    'media_metadata_id_attachments_1', 'media_metadata_id_data_2',\n    'media_metadata_id_media_metadata_4', 'photo_metadata_id_0',\n    'photo_metadata_id_attachments_1', 'photo_metadata_id_data_2',\n    'photo_metadata_id_media_metadata_4',\n    'photo_metadata_id_photo_metadata_5', 'photo_metadata_upload_ip'],\n   dtype='object')\n</code></pre>\n<p>Then,</p>\n<pre><code>wanted_columns = ['creation_timestamp', 'description', 'title', 'uri',\n                  'photo_metadata_upload_ip']\nphoto_df, _ = temp_dfs.temp_to_wanted_df(\n                       wanted_columns=wanted_columns,\n                       route_by_table_name='media',\n                       strat_by='uri'\n                   )\n</code></pre>\n<p>The <code>photo_df</code> is what we want.</p>\n\n          </div>"}, "last_serial": 6596805, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "1c479871d20ec768664551b6cf522d67", "sha256": "387477782a86fcd6d149662bbffa41b90248fa7c098b33fa7bb18133c662dd08"}, "downloads": -1, "filename": "fbjson2table-1.0.0-py3-none-any.whl", "has_sig": false, "md5_digest": "1c479871d20ec768664551b6cf522d67", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 21284, "upload_time": "2020-01-30T10:18:27", "upload_time_iso_8601": "2020-01-30T10:18:27.714350Z", "url": "https://files.pythonhosted.org/packages/f8/76/e9948ba9ca38a5486840e835541135c310a1013490b8ef2d086cc2ad134e/fbjson2table-1.0.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "f6a92c078157d5e5139ba09331198931", "sha256": "1a867ec186504cc74787376e597223368f1efbcd7a1eaca679823c51dd3baafb"}, "downloads": -1, "filename": "fbjson2table-1.0.0.tar.gz", "has_sig": false, "md5_digest": "f6a92c078157d5e5139ba09331198931", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 7813, "upload_time": "2020-01-30T10:18:30", "upload_time_iso_8601": "2020-01-30T10:18:30.038558Z", "url": "https://files.pythonhosted.org/packages/f3/23/af9d11c295e23ff54c29a6729639ddfb5e0637de314e73d4455696c3a461/fbjson2table-1.0.0.tar.gz", "yanked": false}], "1.1.0": [{"comment_text": "", "digests": {"md5": "5abcb2856cc0b5358c2539a02401893e", "sha256": "6113c3e9cce8d681e93f73d5c3288452f0584de67dbec2477ab46a75c774412f"}, "downloads": -1, "filename": "fbjson2table-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "5abcb2856cc0b5358c2539a02401893e", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 27296, "upload_time": "2020-02-08T10:07:21", "upload_time_iso_8601": "2020-02-08T10:07:21.861063Z", "url": "https://files.pythonhosted.org/packages/71/69/b53d0706035c16cf640829c610f406d1b1c26ba052cda31192dc47f2aeaa/fbjson2table-1.1.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "8935f3b99ae09e77877febc5bf73b8b8", "sha256": "4dfb3c117f2ef9636147d456412722fa815a461b0b7d882107e5ab1d61aa4b27"}, "downloads": -1, "filename": "fbjson2table-1.1.0.tar.gz", "has_sig": false, "md5_digest": "8935f3b99ae09e77877febc5bf73b8b8", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 16203, "upload_time": "2020-02-08T10:07:23", "upload_time_iso_8601": "2020-02-08T10:07:23.610170Z", "url": "https://files.pythonhosted.org/packages/be/fb/ff690f9c41d3cbdf92c7d6ea876fbfca3fddd762fe955488c1f890a02cb2/fbjson2table-1.1.0.tar.gz", "yanked": false}], "1.2.0": [{"comment_text": "", "digests": {"md5": "3f3af04115614379ee314e922dee005f", "sha256": "04e23124367b16e91540f506f8af2a793f988707afc06a6031eed7d19111ffc9"}, "downloads": -1, "filename": "fbjson2table-1.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3f3af04115614379ee314e922dee005f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 28493, "upload_time": "2020-02-09T09:05:41", "upload_time_iso_8601": "2020-02-09T09:05:41.265592Z", "url": "https://files.pythonhosted.org/packages/64/47/f521570f2c02661c330c55a209dd1f99c8da0c87fec7e5d3b600fb38bd9d/fbjson2table-1.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2a8cb5cc0eb26a25fcdbb6e5487951d2", "sha256": "a8e0de0349dc2117a65e9729ca1f24902fb8d5ae5d324cf1336c20bb24392973"}, "downloads": -1, "filename": "fbjson2table-1.2.0.tar.gz", "has_sig": false, "md5_digest": "2a8cb5cc0eb26a25fcdbb6e5487951d2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 18621, "upload_time": "2020-02-09T09:05:43", "upload_time_iso_8601": "2020-02-09T09:05:43.043174Z", "url": "https://files.pythonhosted.org/packages/22/4f/617f11d287a3d347db366a2d067a862bd30cf799068596b924a9e85ec4bf/fbjson2table-1.2.0.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "3f3af04115614379ee314e922dee005f", "sha256": "04e23124367b16e91540f506f8af2a793f988707afc06a6031eed7d19111ffc9"}, "downloads": -1, "filename": "fbjson2table-1.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "3f3af04115614379ee314e922dee005f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.5", "size": 28493, "upload_time": "2020-02-09T09:05:41", "upload_time_iso_8601": "2020-02-09T09:05:41.265592Z", "url": "https://files.pythonhosted.org/packages/64/47/f521570f2c02661c330c55a209dd1f99c8da0c87fec7e5d3b600fb38bd9d/fbjson2table-1.2.0-py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "2a8cb5cc0eb26a25fcdbb6e5487951d2", "sha256": "a8e0de0349dc2117a65e9729ca1f24902fb8d5ae5d324cf1336c20bb24392973"}, "downloads": -1, "filename": "fbjson2table-1.2.0.tar.gz", "has_sig": false, "md5_digest": "2a8cb5cc0eb26a25fcdbb6e5487951d2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.5", "size": 18621, "upload_time": "2020-02-09T09:05:43", "upload_time_iso_8601": "2020-02-09T09:05:43.043174Z", "url": "https://files.pythonhosted.org/packages/22/4f/617f11d287a3d347db366a2d067a862bd30cf799068596b924a9e85ec4bf/fbjson2table-1.2.0.tar.gz", "yanked": false}], "timestamp": "Fri May  8 00:43:16 2020"}