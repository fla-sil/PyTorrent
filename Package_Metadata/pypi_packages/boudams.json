{"info": {"author": "Thibault Cl\u00e9rice", "author_email": "", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.6", "Topic :: Text Processing :: Linguistic"], "description": "\n# Le Boucher d'Amsterdam\n\nBoudams, or \"Le boucher d'Amsterdam\", is a deep-learning tool built for tokenizing Latin or Medieval French languages.\n\n## How to cite\n\nAn article has been published about this work : https://hal.archives-ouvertes.fr/hal-02154122v1\n\n```text\n@unpublished{clerice:hal-02154122,\n  TITLE = {{Evaluating Deep Learning Methods for Word Segmentation of Scripta Continua Texts in Old French and Latin}},\n  AUTHOR = {Cl{\\'e}rice, Thibault},\n  URL = {https://hal.archives-ouvertes.fr/hal-02154122},\n  NOTE = {working paper or preprint},\n  YEAR = {2019},\n  MONTH = Jun,\n  KEYWORDS = {convolutional network ; scripta continua ; tokenization ; Old French ; word segmentation},\n  PDF = {https://hal.archives-ouvertes.fr/hal-02154122/file/Evaluating_Deep_Learning_Methods_for_Tokenization_of_Scripta_Continua_in_Old_French_and_Latin%284%29.pdf},\n  HAL_ID = {hal-02154122},\n  HAL_VERSION = {v1},\n}\n\n```\n\n## How to\n\nInstall the usual way you install python stuff: `python setup.py install` (**Python >= 3.6**)).\n\nThe config file can be kickstarted using `boudams template config.json`, we recommend using the following settings :\n\n- `linear-conv-no-pos` for the model, as it is not limited by the input size;\n- `normalize` and `lower` to `True` depending on your dataset size.\n\nThe initial dataset is pretty small but if you want to build with your own, it's fairly simple : you need data in the \nfollowing shape : `\"samesentence<TAB>same sentence\"` where the first element is the same than the second but with no\nspace and they are separated by tabs (`\\t`, marked here as `<TAB>`).\n\n\n```json\n{\n    \"name\": \"model\",\n    \"max_sentence_size\": 150,\n    \"network\": {\n        \"emb_enc_dim\": 256,\n        \"enc_n_layers\": 10,\n        \"enc_kernel_size\": 3,\n        \"enc_dropout\": 0.25\n    },\n    \"model\": \"linear-conv-no-pos\",\n    \"learner\": {\n        \"lr_grace_periode\": 2,\n        \"lr_patience\": 2,\n        \"lr\": 0.0001\n    },\n    \"label_encoder\": {\n        \"normalize\": true,\n        \"lower\": true\n    },\n    \"datasets\": {\n        \"test\": \"./test.tsv\",\n        \"train\": \"./train.tsv\",\n        \"dev\": \"./dev.tsv\",\n        \"random\": true\n    }\n}\n```\n\nThe best architecture I find for medieval French was Conv to Linear without POS using the following setup:\n\n```json\n{\n    \"network\": {\n        \"emb_enc_dim\": 256,\n        \"enc_n_layers\": 10,\n        \"enc_kernel_size\": 5,\n        \"enc_dropout\": 0.25\n    },\n    \"model\": \"linear-conv-no-pos\",\n    \"batch_size\": 64,\n    \"learner\": {\n        \"lr_grace_periode\": 2,\n        \"lr_patience\": 2,\n        \"lr\": 0.00005,\n        \"lr_factor\": 0.5\n    }\n}\n```\n\n\n## Credits\n\nInspirations, bits of code and source for being able to understand how Seq2Seq words or write my own Torch module come \nboth from [Ben Trevett](https://github.com/bentrevett/pytorch-seq2seq) and [Enrique Manjavacas](https://github.com/emanjavacas/pie). \n\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/ponteineptique/boudams", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "boudams", "package_url": "https://pypi.org/project/boudams/", "platform": "", "project_url": "https://pypi.org/project/boudams/", "project_urls": {"Homepage": "https://github.com/ponteineptique/boudams"}, "release_url": "https://pypi.org/project/boudams/0.1.2/", "requires_dist": ["torch (>=1.0.1.post2)", "tqdm (>=4.31.1)", "unidecode (>=1.0.23)", "leven (==1.0.4)", "click (<=8.0,>=7.0)", "regex (==2019.5.25)", "mufidecode", "matplotlib", "sklearn", "numpy"], "requires_python": ">=3.6.0", "summary": "A framework and toolkit for automatic segmentation", "version": "0.1.2", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>Le Boucher d'Amsterdam</h1>\n<p>Boudams, or \"Le boucher d'Amsterdam\", is a deep-learning tool built for tokenizing Latin or Medieval French languages.</p>\n<h2>How to cite</h2>\n<p>An article has been published about this work : <a href=\"https://hal.archives-ouvertes.fr/hal-02154122v1\" rel=\"nofollow\">https://hal.archives-ouvertes.fr/hal-02154122v1</a></p>\n<pre>@unpublished{clerice:hal-02154122,\n  TITLE = {{Evaluating Deep Learning Methods for Word Segmentation of Scripta Continua Texts in Old French and Latin}},\n  AUTHOR = {Cl{\\'e}rice, Thibault},\n  URL = {https://hal.archives-ouvertes.fr/hal-02154122},\n  NOTE = {working paper or preprint},\n  YEAR = {2019},\n  MONTH = Jun,\n  KEYWORDS = {convolutional network ; scripta continua ; tokenization ; Old French ; word segmentation},\n  PDF = {https://hal.archives-ouvertes.fr/hal-02154122/file/Evaluating_Deep_Learning_Methods_for_Tokenization_of_Scripta_Continua_in_Old_French_and_Latin%284%29.pdf},\n  HAL_ID = {hal-02154122},\n  HAL_VERSION = {v1},\n}\n</pre>\n<h2>How to</h2>\n<p>Install the usual way you install python stuff: <code>python setup.py install</code> (<strong>Python &gt;= 3.6</strong>)).</p>\n<p>The config file can be kickstarted using <code>boudams template config.json</code>, we recommend using the following settings :</p>\n<ul>\n<li><code>linear-conv-no-pos</code> for the model, as it is not limited by the input size;</li>\n<li><code>normalize</code> and <code>lower</code> to <code>True</code> depending on your dataset size.</li>\n</ul>\n<p>The initial dataset is pretty small but if you want to build with your own, it's fairly simple : you need data in the\nfollowing shape : <code>\"samesentence&lt;TAB&gt;same sentence\"</code> where the first element is the same than the second but with no\nspace and they are separated by tabs (<code>\\t</code>, marked here as <code>&lt;TAB&gt;</code>).</p>\n<pre><span class=\"p\">{</span>\n    <span class=\"nt\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"model\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"max_sentence_size\"</span><span class=\"p\">:</span> <span class=\"mi\">150</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"network\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"emb_enc_dim\"</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"enc_n_layers\"</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"enc_kernel_size\"</span><span class=\"p\">:</span> <span class=\"mi\">3</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"enc_dropout\"</span><span class=\"p\">:</span> <span class=\"mf\">0.25</span>\n    <span class=\"p\">},</span>\n    <span class=\"nt\">\"model\"</span><span class=\"p\">:</span> <span class=\"s2\">\"linear-conv-no-pos\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"learner\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"lr_grace_periode\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"lr_patience\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"lr\"</span><span class=\"p\">:</span> <span class=\"mf\">0.0001</span>\n    <span class=\"p\">},</span>\n    <span class=\"nt\">\"label_encoder\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"normalize\"</span><span class=\"p\">:</span> <span class=\"kc\">true</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"lower\"</span><span class=\"p\">:</span> <span class=\"kc\">true</span>\n    <span class=\"p\">},</span>\n    <span class=\"nt\">\"datasets\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"test\"</span><span class=\"p\">:</span> <span class=\"s2\">\"./test.tsv\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"train\"</span><span class=\"p\">:</span> <span class=\"s2\">\"./train.tsv\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"dev\"</span><span class=\"p\">:</span> <span class=\"s2\">\"./dev.tsv\"</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"random\"</span><span class=\"p\">:</span> <span class=\"kc\">true</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</pre>\n<p>The best architecture I find for medieval French was Conv to Linear without POS using the following setup:</p>\n<pre><span class=\"p\">{</span>\n    <span class=\"nt\">\"network\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"emb_enc_dim\"</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"enc_n_layers\"</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"enc_kernel_size\"</span><span class=\"p\">:</span> <span class=\"mi\">5</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"enc_dropout\"</span><span class=\"p\">:</span> <span class=\"mf\">0.25</span>\n    <span class=\"p\">},</span>\n    <span class=\"nt\">\"model\"</span><span class=\"p\">:</span> <span class=\"s2\">\"linear-conv-no-pos\"</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"batch_size\"</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span>\n    <span class=\"nt\">\"learner\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"nt\">\"lr_grace_periode\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"lr_patience\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"lr\"</span><span class=\"p\">:</span> <span class=\"mf\">0.00005</span><span class=\"p\">,</span>\n        <span class=\"nt\">\"lr_factor\"</span><span class=\"p\">:</span> <span class=\"mf\">0.5</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</pre>\n<h2>Credits</h2>\n<p>Inspirations, bits of code and source for being able to understand how Seq2Seq words or write my own Torch module come\nboth from <a href=\"https://github.com/bentrevett/pytorch-seq2seq\" rel=\"nofollow\">Ben Trevett</a> and <a href=\"https://github.com/emanjavacas/pie\" rel=\"nofollow\">Enrique Manjavacas</a>.</p>\n\n          </div>"}, "last_serial": 6776165, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "599f48648ae4fefab4d5af38d43fde98", "sha256": "939726ac993eb910786a9599f7b714d4773bda12272f67b5ed0b278911780642"}, "downloads": -1, "filename": "boudams-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "599f48648ae4fefab4d5af38d43fde98", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 3624, "upload_time": "2019-10-12T09:36:33", "upload_time_iso_8601": "2019-10-12T09:36:33.035704Z", "url": "https://files.pythonhosted.org/packages/9e/6e/6fb5c75f8b14a6e1fb868d7c2790d4dfb587988cafc0152a195ba99e4990/boudams-0.1.0-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "bcbef31a04ff697bfac7c8c11632c6d7", "sha256": "fac7046fb4ea673e0214c668555b9e98446d6cdaab61363d73207f2154fce311"}, "downloads": -1, "filename": "boudams-0.1.0.tar.gz", "has_sig": false, "md5_digest": "bcbef31a04ff697bfac7c8c11632c6d7", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 3978, "upload_time": "2019-10-12T09:36:35", "upload_time_iso_8601": "2019-10-12T09:36:35.339123Z", "url": "https://files.pythonhosted.org/packages/a2/39/5e9a04e601004ca6942aefa1872852264187d67f0f28025a88c8a0574500/boudams-0.1.0.tar.gz", "yanked": false}], "0.1.1": [{"comment_text": "", "digests": {"md5": "4a4e0194dd48385a5db67ec7de3a6227", "sha256": "f4cb96edc607c078b8b179cd45c8e33f220eb83cbff1ab2285573d506d10f694"}, "downloads": -1, "filename": "boudams-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "4a4e0194dd48385a5db67ec7de3a6227", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 3640, "upload_time": "2020-03-06T17:04:43", "upload_time_iso_8601": "2020-03-06T17:04:43.895511Z", "url": "https://files.pythonhosted.org/packages/6b/1f/c791a7db91529758d37991d2d25e1aced802612b784eaf91f79511173037/boudams-0.1.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "27a5994b9a51659856a7cca1af798e88", "sha256": "8cd4343e2647eda369c911fc3924afdb0e22e0c9f420ffe53fcbbf6f38d3a6af"}, "downloads": -1, "filename": "boudams-0.1.1.tar.gz", "has_sig": false, "md5_digest": "27a5994b9a51659856a7cca1af798e88", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 3963, "upload_time": "2020-03-06T17:04:45", "upload_time_iso_8601": "2020-03-06T17:04:45.802505Z", "url": "https://files.pythonhosted.org/packages/a3/15/418572850e712fd20bfef54844a507e1ce14c6241f0852abe492819d718c/boudams-0.1.1.tar.gz", "yanked": false}], "0.1.2": [{"comment_text": "", "digests": {"md5": "1ffecc732c2f13aa57b1e4fb59303798", "sha256": "bc43cbaee160571337ad1d0f610deac6e0b5797fe181e6d325c11196232ae7af"}, "downloads": -1, "filename": "boudams-0.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "1ffecc732c2f13aa57b1e4fb59303798", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 33878, "upload_time": "2020-03-09T10:37:17", "upload_time_iso_8601": "2020-03-09T10:37:17.450124Z", "url": "https://files.pythonhosted.org/packages/3a/a5/7ffce290e36a3152b23b22157aeac2bcca9d1d5d6f76428361c244cd3647/boudams-0.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6b823a7f726f4928b535c6ca0f04bd12", "sha256": "188bed46f9fb12e17947abaa17cc87b935a9fcacf9d19767aee2ce4529b5d3b4"}, "downloads": -1, "filename": "boudams-0.1.2.tar.gz", "has_sig": false, "md5_digest": "6b823a7f726f4928b535c6ca0f04bd12", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 27476, "upload_time": "2020-03-09T10:37:18", "upload_time_iso_8601": "2020-03-09T10:37:18.963708Z", "url": "https://files.pythonhosted.org/packages/c7/51/633b1249c92accd24e0f4695f6f594f513947c55d3daf0bd1243fa574cf1/boudams-0.1.2.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "1ffecc732c2f13aa57b1e4fb59303798", "sha256": "bc43cbaee160571337ad1d0f610deac6e0b5797fe181e6d325c11196232ae7af"}, "downloads": -1, "filename": "boudams-0.1.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "1ffecc732c2f13aa57b1e4fb59303798", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=3.6.0", "size": 33878, "upload_time": "2020-03-09T10:37:17", "upload_time_iso_8601": "2020-03-09T10:37:17.450124Z", "url": "https://files.pythonhosted.org/packages/3a/a5/7ffce290e36a3152b23b22157aeac2bcca9d1d5d6f76428361c244cd3647/boudams-0.1.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "6b823a7f726f4928b535c6ca0f04bd12", "sha256": "188bed46f9fb12e17947abaa17cc87b935a9fcacf9d19767aee2ce4529b5d3b4"}, "downloads": -1, "filename": "boudams-0.1.2.tar.gz", "has_sig": false, "md5_digest": "6b823a7f726f4928b535c6ca0f04bd12", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6.0", "size": 27476, "upload_time": "2020-03-09T10:37:18", "upload_time_iso_8601": "2020-03-09T10:37:18.963708Z", "url": "https://files.pythonhosted.org/packages/c7/51/633b1249c92accd24e0f4695f6f594f513947c55d3daf0bd1243fa574cf1/boudams-0.1.2.tar.gz", "yanked": false}], "timestamp": "Thu May  7 22:36:31 2020"}