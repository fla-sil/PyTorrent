{"info": {"author": "Rolando Espinoza", "author_email": "rolando@rmax.io", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5"], "description": "============\nScrapy-Redis\n============\n\n.. image:: https://readthedocs.org/projects/scrapy-redis/badge/?version=latest\n        :target: https://readthedocs.org/projects/scrapy-redis/?badge=latest\n        :alt: Documentation Status\n\n.. image:: https://img.shields.io/pypi/v/scrapy-redis.svg\n        :target: https://pypi.python.org/pypi/scrapy-redis\n\n.. image:: https://img.shields.io/pypi/pyversions/scrapy-redis.svg\n        :target: https://pypi.python.org/pypi/scrapy-redis\n\n.. image:: https://img.shields.io/travis/rolando/scrapy-redis.svg\n        :target: https://travis-ci.org/rolando/scrapy-redis\n\n.. image:: https://codecov.io/github/rolando/scrapy-redis/coverage.svg?branch=master\n    :alt: Coverage Status\n    :target: https://codecov.io/github/rolando/scrapy-redis\n\n.. image:: https://landscape.io/github/rolando/scrapy-redis/master/landscape.svg?style=flat\n    :target: https://landscape.io/github/rolando/scrapy-redis/master\n    :alt: Code Quality Status\n\n.. image:: https://requires.io/github/rolando/scrapy-redis/requirements.svg?branch=master\n    :alt: Requirements Status\n    :target: https://requires.io/github/rolando/scrapy-redis/requirements/?branch=master\n\nRedis-based components for Scrapy.\n\n* Free software: MIT license\n* Documentation: https://scrapy-redis.readthedocs.org.\n* Python versions: 2.7, 3.4+\n\nFeatures\n--------\n\n* Distributed crawling/scraping\n\n    You can start multiple spider instances that share a single redis queue.\n    Best suitable for broad multi-domain crawls.\n\n* Distributed post-processing\n\n    Scraped items gets pushed into a redis queued meaning that you can start as\n    many as needed post-processing processes sharing the items queue.\n\n* Scrapy plug-and-play components\n\n    Scheduler + Duplication Filter, Item Pipeline, Base Spiders.\n\nRequirements\n------------\n\n* Python 2.7, 3.4 or 3.5\n* Redis >= 2.8\n* ``Scrapy`` >= 1.0\n* ``redis-py`` >= 2.10\n\nUsage\n-----\n\nUse the following settings in your project:\n\n.. code-block:: python\n\n  # Enables scheduling storing requests queue in redis.\n  SCHEDULER = \"scrapy_redis.scheduler.Scheduler\"\n\n  # Ensure all spiders share same duplicates filter through redis.\n  DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\"\n\n  # Default requests serializer is pickle, but it can be changed to any module\n  # with loads and dumps functions. Note that pickle is not compatible between\n  # python versions.\n  # Caveat: In python 3.x, the serializer must return strings keys and support\n  # bytes as values. Because of this reason the json or msgpack module will not\n  # work by default. In python 2.x there is no such issue and you can use\n  # 'json' or 'msgpack' as serializers.\n  #SCHEDULER_SERIALIZER = \"scrapy_redis.picklecompat\"\n\n  # Don't cleanup redis queues, allows to pause/resume crawls.\n  #SCHEDULER_PERSIST = True\n\n  # Schedule requests using a priority queue. (default)\n  #SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'\n\n  # Alternative queues.\n  #SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue'\n  #SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue'\n\n  # Max idle time to prevent the spider from being closed when distributed crawling.\n  # This only works if queue class is SpiderQueue or SpiderStack,\n  # and may also block the same time when your spider start at the first time (because the queue is empty).\n  #SCHEDULER_IDLE_BEFORE_CLOSE = 10\n\n  # Store scraped item in redis for post-processing.\n  ITEM_PIPELINES = {\n      'scrapy_redis.pipelines.RedisPipeline': 300\n  }\n\n  # The item pipeline serializes and stores the items in this redis key.\n  #REDIS_ITEMS_KEY = '%(spider)s:items'\n\n  # The items serializer is by default ScrapyJSONEncoder. You can use any\n  # importable path to a callable object.\n  #REDIS_ITEMS_SERIALIZER = 'json.dumps'\n\n  # Specify the host and port to use when connecting to Redis (optional).\n  #REDIS_HOST = 'localhost'\n  #REDIS_PORT = 6379\n\n  # Specify the full Redis URL for connecting (optional).\n  # If set, this takes precedence over the REDIS_HOST and REDIS_PORT settings.\n  #REDIS_URL = 'redis://user:pass@hostname:9001'\n\n  # Custom redis client parameters (i.e.: socket timeout, etc.)\n  #REDIS_PARAMS  = {}\n  # Use custom redis client class.\n  #REDIS_PARAMS['redis_cls'] = 'myproject.RedisClient'\n\n  # If True, it uses redis' ``spop`` operation. This could be useful if you\n  # want to avoid duplicates in your start urls list. In this cases, urls must\n  # be added via ``sadd`` command or you will get a type error from redis.\n  #REDIS_START_URLS_AS_SET = False\n\n  # Default start urls key for RedisSpider and RedisCrawlSpider.\n  #REDIS_START_URLS_KEY = '%(name)s:start_urls'\n\n  # Use other encoding than utf-8 for redis.\n  #REDIS_ENCODING = 'latin1'\n\n.. note::\n\n  Version 0.3 changed the requests serialization from `marshal` to `cPickle`,\n  therefore persisted requests using version 0.2 will not able to work on 0.3.\n\n\nRunning the example project\n---------------------------\n\nThis example illustrates how to share a spider's requests queue\nacross multiple spider instances, highly suitable for broad crawls.\n\n1. Setup scrapy_redis package in your PYTHONPATH\n\n2. Run the crawler for first time then stop it::\n\n    $ cd example-project\n    $ scrapy crawl dmoz\n    ... [dmoz] ...\n    ^C\n\n3. Run the crawler again to resume stopped crawling::\n\n    $ scrapy crawl dmoz\n    ... [dmoz] DEBUG: Resuming crawl (9019 requests scheduled)\n\n4. Start one or more additional scrapy crawlers::\n\n    $ scrapy crawl dmoz\n    ... [dmoz] DEBUG: Resuming crawl (8712 requests scheduled)\n\n5. Start one or more post-processing workers::\n\n    $ python process_items.py dmoz:items -v\n    ...\n    Processing: Kilani Giftware (http://www.dmoz.org/Computers/Shopping/Gifts/)\n    Processing: NinjaGizmos.com (http://www.dmoz.org/Computers/Shopping/Gifts/)\n    ...\n\n\nFeeding a Spider from Redis\n---------------------------\n\nThe class `scrapy_redis.spiders.RedisSpider` enables a spider to read the\nurls from redis. The urls in the redis queue will be processed one\nafter another, if the first request yields more requests, the spider\nwill process those requests before fetching another url from redis.\n\nFor example, create a file `myspider.py` with the code below:\n\n.. code-block:: python\n\n    from scrapy_redis.spiders import RedisSpider\n\n    class MySpider(RedisSpider):\n        name = 'myspider'\n\n        def parse(self, response):\n            # do stuff\n            pass\n\n\nThen:\n\n1. run the spider::\n\n    scrapy runspider myspider.py\n\n2. push urls to redis::\n\n    redis-cli lpush myspider:start_urls http://google.com\n\n\n.. note::\n\n    These spiders rely on the spider idle signal to fetch start urls, hence it\n    may have a few seconds of delay between the time you push a new url and the\n    spider starts crawling it.\n\n=======\nHistory\n=======\n\n\n0.6.8 (2017-02-14)\n------------------\n* Fixed automated release due to not matching registered email.\n\n0.6.7 (2016-12-27)\n------------------\n* Fixes bad formatting in logging message.\n\n0.6.6 (2016-12-20)\n------------------\n* Fixes wrong message on dupefilter duplicates.\n\n0.6.5 (2016-12-19)\n------------------\n* Fixed typo in default settings.\n\n\n0.6.4 (2016-12-18)\n------------------\n* Fixed data decoding in Python 3.x.\n* Added ``REDIS_ENCODING`` setting (default ``utf-8``).\n* Default to ``CONCURRENT_REQUESTS`` value for ``REDIS_START_URLS_BATCH_SIZE``.\n* Renamed queue classes to a proper naming conventiong (backwards compatible).\n\n0.6.3 (2016-07-03)\n------------------\n* Added ``REDIS_START_URLS_KEY`` setting.\n* Fixed spider method ``from_crawler`` signature.\n\n0.6.2 (2016-06-26)\n------------------\n* Support ``redis_cls`` parameter in ``REDIS_PARAMS`` setting.\n* Python 3.x compatibility fixed.\n* Added ``SCHEDULER_SERIALIZER`` setting.\n\n0.6.1 (2016-06-25)\n------------------\n* **Backwards incompatible change:** Require explicit ``DUPEFILTER_CLASS``\n  setting.\n* Added ``SCHEDULER_FLUSH_ON_START`` setting.\n* Added ``REDIS_START_URLS_AS_SET`` setting.\n* Added ``REDIS_ITEMS_KEY`` setting.\n* Added ``REDIS_ITEMS_SERIALIZER`` setting.\n* Added ``REDIS_PARAMS`` setting.\n* Added ``REDIS_START_URLS_BATCH_SIZE`` spider attribute to read start urls\n  in batches.\n* Added ``RedisCrawlSpider``.\n\n0.6.0 (2015-07-05)\n------------------\n* Updated code to be compatible with Scrapy 1.0.\n* Added `-a domain=...` option for example spiders.\n\n0.5.0 (2013-09-02)\n------------------\n* Added `REDIS_URL` setting to support Redis connection string.\n* Added `SCHEDULER_IDLE_BEFORE_CLOSE` setting to prevent the spider closing too\n  quickly when the queue is empty. Default value is zero keeping the previous\n  behavior.\n* Schedule preemptively requests on item scraped.\n* This version is the latest release compatible with Scrapy 0.24.x.\n\n0.4.0 (2013-04-19)\n------------------\n* Added `RedisSpider` and `RedisMixin` classes as building blocks for spiders\n  to be fed through a redis queue.\n* Added redis queue stats.\n* Let the encoder handle the item as it comes instead converting it to a dict.\n\n0.3.0 (2013-02-18)\n------------------\n* Added support for different queue classes.\n* Changed requests serialization from `marshal` to `cPickle`.\n\n0.2.0 (2013-02-17)\n------------------\n* Improved backward compatibility.\n* Added example project.\n\n0.1.0 (2011-09-01)\n------------------\n* First release on PyPI.\n\n", "description_content_type": null, "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/rolando/scrapy-redis", "keywords": "scrapy-redis", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "scrapy-redis", "package_url": "https://pypi.org/project/scrapy-redis/", "platform": "", "project_url": "https://pypi.org/project/scrapy-redis/", "project_urls": {"Homepage": "https://github.com/rolando/scrapy-redis"}, "release_url": "https://pypi.org/project/scrapy-redis/0.6.8/", "requires_dist": ["Scrapy (>=1.0)", "redis (>=2.10)", "six (>=1.5.2)"], "requires_python": "", "summary": "Redis-based components for Scrapy.", "version": "0.6.8", "yanked": false, "html_description": "<div class=\"project-description\">\n            <div id=\"scrapy-redis\">\n<h2>Scrapy-Redis</h2>\n<a href=\"https://readthedocs.org/projects/scrapy-redis/?badge=latest\" rel=\"nofollow\"><img alt=\"Documentation Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c304410079d29068ffe85e74765eb05f7f2cc241/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7363726170792d72656469732f62616467652f3f76657273696f6e3d6c6174657374\"></a>\n<a href=\"https://pypi.python.org/pypi/scrapy-redis\" rel=\"nofollow\"><img alt=\"https://img.shields.io/pypi/v/scrapy-redis.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ca40186922b44da6c826c1ec591447fa18ea951b/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7363726170792d72656469732e737667\"></a>\n<a href=\"https://pypi.python.org/pypi/scrapy-redis\" rel=\"nofollow\"><img alt=\"https://img.shields.io/pypi/pyversions/scrapy-redis.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/fd5da332ac5cb2fe09c09214623cf2fa7bece5dd/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7363726170792d72656469732e737667\"></a>\n<a href=\"https://travis-ci.org/rolando/scrapy-redis\" rel=\"nofollow\"><img alt=\"https://img.shields.io/travis/rolando/scrapy-redis.svg\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c62c4feae6bd3ec541a91d3aa4bfdb1e8c6967bb/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f726f6c616e646f2f7363726170792d72656469732e737667\"></a>\n<a href=\"https://codecov.io/github/rolando/scrapy-redis\" rel=\"nofollow\"><img alt=\"Coverage Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/c85ba2331caefd353e39edc58fb2101ad9ca5e04/68747470733a2f2f636f6465636f762e696f2f6769746875622f726f6c616e646f2f7363726170792d72656469732f636f7665726167652e7376673f6272616e63683d6d6173746572\"></a>\n<a href=\"https://landscape.io/github/rolando/scrapy-redis/master\" rel=\"nofollow\"><img alt=\"Code Quality Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/1030a7845d26d969f12351da2e13ef22c0c03c0e/68747470733a2f2f6c616e6473636170652e696f2f6769746875622f726f6c616e646f2f7363726170792d72656469732f6d61737465722f6c616e6473636170652e7376673f7374796c653d666c6174\"></a>\n<a href=\"https://requires.io/github/rolando/scrapy-redis/requirements/?branch=master\" rel=\"nofollow\"><img alt=\"Requirements Status\" src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/66c104f3be95ddac09ebb06fd4ec79424e6a40bd/68747470733a2f2f72657175697265732e696f2f6769746875622f726f6c616e646f2f7363726170792d72656469732f726571756972656d656e74732e7376673f6272616e63683d6d6173746572\"></a>\n<p>Redis-based components for Scrapy.</p>\n<ul>\n<li>Free software: MIT license</li>\n<li>Documentation: <a href=\"https://scrapy-redis.readthedocs.org\" rel=\"nofollow\">https://scrapy-redis.readthedocs.org</a>.</li>\n<li>Python versions: 2.7, 3.4+</li>\n</ul>\n<div id=\"features\">\n<h3>Features</h3>\n<ul>\n<li><p>Distributed crawling/scraping</p>\n<blockquote>\n<p>You can start multiple spider instances that share a single redis queue.\nBest suitable for broad multi-domain crawls.</p>\n</blockquote>\n</li>\n<li><p>Distributed post-processing</p>\n<blockquote>\n<p>Scraped items gets pushed into a redis queued meaning that you can start as\nmany as needed post-processing processes sharing the items queue.</p>\n</blockquote>\n</li>\n<li><p>Scrapy plug-and-play components</p>\n<blockquote>\n<p>Scheduler + Duplication Filter, Item Pipeline, Base Spiders.</p>\n</blockquote>\n</li>\n</ul>\n</div>\n<div id=\"requirements\">\n<h3>Requirements</h3>\n<ul>\n<li>Python 2.7, 3.4 or 3.5</li>\n<li>Redis &gt;= 2.8</li>\n<li><tt>Scrapy</tt> &gt;= 1.0</li>\n<li><tt><span class=\"pre\">redis-py</span></tt> &gt;= 2.10</li>\n</ul>\n</div>\n<div id=\"usage\">\n<h3>Usage</h3>\n<p>Use the following settings in your project:</p>\n<pre><span class=\"c1\"># Enables scheduling storing requests queue in redis.</span>\n<span class=\"n\">SCHEDULER</span> <span class=\"o\">=</span> <span class=\"s2\">\"scrapy_redis.scheduler.Scheduler\"</span>\n\n<span class=\"c1\"># Ensure all spiders share same duplicates filter through redis.</span>\n<span class=\"n\">DUPEFILTER_CLASS</span> <span class=\"o\">=</span> <span class=\"s2\">\"scrapy_redis.dupefilter.RFPDupeFilter\"</span>\n\n<span class=\"c1\"># Default requests serializer is pickle, but it can be changed to any module</span>\n<span class=\"c1\"># with loads and dumps functions. Note that pickle is not compatible between</span>\n<span class=\"c1\"># python versions.</span>\n<span class=\"c1\"># Caveat: In python 3.x, the serializer must return strings keys and support</span>\n<span class=\"c1\"># bytes as values. Because of this reason the json or msgpack module will not</span>\n<span class=\"c1\"># work by default. In python 2.x there is no such issue and you can use</span>\n<span class=\"c1\"># 'json' or 'msgpack' as serializers.</span>\n<span class=\"c1\">#SCHEDULER_SERIALIZER = \"scrapy_redis.picklecompat\"</span>\n\n<span class=\"c1\"># Don't cleanup redis queues, allows to pause/resume crawls.</span>\n<span class=\"c1\">#SCHEDULER_PERSIST = True</span>\n\n<span class=\"c1\"># Schedule requests using a priority queue. (default)</span>\n<span class=\"c1\">#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'</span>\n\n<span class=\"c1\"># Alternative queues.</span>\n<span class=\"c1\">#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue'</span>\n<span class=\"c1\">#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue'</span>\n\n<span class=\"c1\"># Max idle time to prevent the spider from being closed when distributed crawling.</span>\n<span class=\"c1\"># This only works if queue class is SpiderQueue or SpiderStack,</span>\n<span class=\"c1\"># and may also block the same time when your spider start at the first time (because the queue is empty).</span>\n<span class=\"c1\">#SCHEDULER_IDLE_BEFORE_CLOSE = 10</span>\n\n<span class=\"c1\"># Store scraped item in redis for post-processing.</span>\n<span class=\"n\">ITEM_PIPELINES</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s1\">'scrapy_redis.pipelines.RedisPipeline'</span><span class=\"p\">:</span> <span class=\"mi\">300</span>\n<span class=\"p\">}</span>\n\n<span class=\"c1\"># The item pipeline serializes and stores the items in this redis key.</span>\n<span class=\"c1\">#REDIS_ITEMS_KEY = '%(spider)s:items'</span>\n\n<span class=\"c1\"># The items serializer is by default ScrapyJSONEncoder. You can use any</span>\n<span class=\"c1\"># importable path to a callable object.</span>\n<span class=\"c1\">#REDIS_ITEMS_SERIALIZER = 'json.dumps'</span>\n\n<span class=\"c1\"># Specify the host and port to use when connecting to Redis (optional).</span>\n<span class=\"c1\">#REDIS_HOST = 'localhost'</span>\n<span class=\"c1\">#REDIS_PORT = 6379</span>\n\n<span class=\"c1\"># Specify the full Redis URL for connecting (optional).</span>\n<span class=\"c1\"># If set, this takes precedence over the REDIS_HOST and REDIS_PORT settings.</span>\n<span class=\"c1\">#REDIS_URL = 'redis://user:pass@hostname:9001'</span>\n\n<span class=\"c1\"># Custom redis client parameters (i.e.: socket timeout, etc.)</span>\n<span class=\"c1\">#REDIS_PARAMS  = {}</span>\n<span class=\"c1\"># Use custom redis client class.</span>\n<span class=\"c1\">#REDIS_PARAMS['redis_cls'] = 'myproject.RedisClient'</span>\n\n<span class=\"c1\"># If True, it uses redis' ``spop`` operation. This could be useful if you</span>\n<span class=\"c1\"># want to avoid duplicates in your start urls list. In this cases, urls must</span>\n<span class=\"c1\"># be added via ``sadd`` command or you will get a type error from redis.</span>\n<span class=\"c1\">#REDIS_START_URLS_AS_SET = False</span>\n\n<span class=\"c1\"># Default start urls key for RedisSpider and RedisCrawlSpider.</span>\n<span class=\"c1\">#REDIS_START_URLS_KEY = '%(name)s:start_urls'</span>\n\n<span class=\"c1\"># Use other encoding than utf-8 for redis.</span>\n<span class=\"c1\">#REDIS_ENCODING = 'latin1'</span>\n</pre>\n<div>\n<p>Note</p>\n<p>Version 0.3 changed the requests serialization from <cite>marshal</cite> to <cite>cPickle</cite>,\ntherefore persisted requests using version 0.2 will not able to work on 0.3.</p>\n</div>\n</div>\n<div id=\"running-the-example-project\">\n<h3>Running the example project</h3>\n<p>This example illustrates how to share a spider\u2019s requests queue\nacross multiple spider instances, highly suitable for broad crawls.</p>\n<ol>\n<li><p>Setup scrapy_redis package in your PYTHONPATH</p>\n</li>\n<li><p>Run the crawler for first time then stop it:</p>\n<pre>$ cd example-project\n$ scrapy crawl dmoz\n... [dmoz] ...\n^C\n</pre>\n</li>\n<li><p>Run the crawler again to resume stopped crawling:</p>\n<pre>$ scrapy crawl dmoz\n... [dmoz] DEBUG: Resuming crawl (9019 requests scheduled)\n</pre>\n</li>\n<li><p>Start one or more additional scrapy crawlers:</p>\n<pre>$ scrapy crawl dmoz\n... [dmoz] DEBUG: Resuming crawl (8712 requests scheduled)\n</pre>\n</li>\n<li><p>Start one or more post-processing workers:</p>\n<pre>$ python process_items.py dmoz:items -v\n...\nProcessing: Kilani Giftware (http://www.dmoz.org/Computers/Shopping/Gifts/)\nProcessing: NinjaGizmos.com (http://www.dmoz.org/Computers/Shopping/Gifts/)\n...\n</pre>\n</li>\n</ol>\n</div>\n<div id=\"feeding-a-spider-from-redis\">\n<h3>Feeding a Spider from Redis</h3>\n<p>The class <cite>scrapy_redis.spiders.RedisSpider</cite> enables a spider to read the\nurls from redis. The urls in the redis queue will be processed one\nafter another, if the first request yields more requests, the spider\nwill process those requests before fetching another url from redis.</p>\n<p>For example, create a file <cite>myspider.py</cite> with the code below:</p>\n<pre><span class=\"kn\">from</span> <span class=\"nn\">scrapy_redis.spiders</span> <span class=\"kn\">import</span> <span class=\"n\">RedisSpider</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">MySpider</span><span class=\"p\">(</span><span class=\"n\">RedisSpider</span><span class=\"p\">):</span>\n    <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s1\">'myspider'</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">response</span><span class=\"p\">):</span>\n        <span class=\"c1\"># do stuff</span>\n        <span class=\"k\">pass</span>\n</pre>\n<p>Then:</p>\n<ol>\n<li><p>run the spider:</p>\n<pre>scrapy runspider myspider.py\n</pre>\n</li>\n<li><p>push urls to redis:</p>\n<pre>redis-cli lpush myspider:start_urls http://google.com\n</pre>\n</li>\n</ol>\n<div>\n<p>Note</p>\n<p>These spiders rely on the spider idle signal to fetch start urls, hence it\nmay have a few seconds of delay between the time you push a new url and the\nspider starts crawling it.</p>\n</div>\n</div>\n</div>\n<div id=\"history\">\n<h2>History</h2>\n<div id=\"id1\">\n<h3>0.6.8 (2017-02-14)</h3>\n<ul>\n<li>Fixed automated release due to not matching registered email.</li>\n</ul>\n</div>\n<div id=\"id2\">\n<h3>0.6.7 (2016-12-27)</h3>\n<ul>\n<li>Fixes bad formatting in logging message.</li>\n</ul>\n</div>\n<div id=\"id3\">\n<h3>0.6.6 (2016-12-20)</h3>\n<ul>\n<li>Fixes wrong message on dupefilter duplicates.</li>\n</ul>\n</div>\n<div id=\"id4\">\n<h3>0.6.5 (2016-12-19)</h3>\n<ul>\n<li>Fixed typo in default settings.</li>\n</ul>\n</div>\n<div id=\"id5\">\n<h3>0.6.4 (2016-12-18)</h3>\n<ul>\n<li>Fixed data decoding in Python 3.x.</li>\n<li>Added <tt>REDIS_ENCODING</tt> setting (default <tt><span class=\"pre\">utf-8</span></tt>).</li>\n<li>Default to <tt>CONCURRENT_REQUESTS</tt> value for <tt>REDIS_START_URLS_BATCH_SIZE</tt>.</li>\n<li>Renamed queue classes to a proper naming conventiong (backwards compatible).</li>\n</ul>\n</div>\n<div id=\"id6\">\n<h3>0.6.3 (2016-07-03)</h3>\n<ul>\n<li>Added <tt>REDIS_START_URLS_KEY</tt> setting.</li>\n<li>Fixed spider method <tt>from_crawler</tt> signature.</li>\n</ul>\n</div>\n<div id=\"id7\">\n<h3>0.6.2 (2016-06-26)</h3>\n<ul>\n<li>Support <tt>redis_cls</tt> parameter in <tt>REDIS_PARAMS</tt> setting.</li>\n<li>Python 3.x compatibility fixed.</li>\n<li>Added <tt>SCHEDULER_SERIALIZER</tt> setting.</li>\n</ul>\n</div>\n<div id=\"id8\">\n<h3>0.6.1 (2016-06-25)</h3>\n<ul>\n<li><strong>Backwards incompatible change:</strong> Require explicit <tt>DUPEFILTER_CLASS</tt>\nsetting.</li>\n<li>Added <tt>SCHEDULER_FLUSH_ON_START</tt> setting.</li>\n<li>Added <tt>REDIS_START_URLS_AS_SET</tt> setting.</li>\n<li>Added <tt>REDIS_ITEMS_KEY</tt> setting.</li>\n<li>Added <tt>REDIS_ITEMS_SERIALIZER</tt> setting.</li>\n<li>Added <tt>REDIS_PARAMS</tt> setting.</li>\n<li>Added <tt>REDIS_START_URLS_BATCH_SIZE</tt> spider attribute to read start urls\nin batches.</li>\n<li>Added <tt>RedisCrawlSpider</tt>.</li>\n</ul>\n</div>\n<div id=\"id9\">\n<h3>0.6.0 (2015-07-05)</h3>\n<ul>\n<li>Updated code to be compatible with Scrapy 1.0.</li>\n<li>Added <cite>-a domain=\u2026</cite> option for example spiders.</li>\n</ul>\n</div>\n<div id=\"id10\">\n<h3>0.5.0 (2013-09-02)</h3>\n<ul>\n<li>Added <cite>REDIS_URL</cite> setting to support Redis connection string.</li>\n<li>Added <cite>SCHEDULER_IDLE_BEFORE_CLOSE</cite> setting to prevent the spider closing too\nquickly when the queue is empty. Default value is zero keeping the previous\nbehavior.</li>\n<li>Schedule preemptively requests on item scraped.</li>\n<li>This version is the latest release compatible with Scrapy 0.24.x.</li>\n</ul>\n</div>\n<div id=\"id11\">\n<h3>0.4.0 (2013-04-19)</h3>\n<ul>\n<li>Added <cite>RedisSpider</cite> and <cite>RedisMixin</cite> classes as building blocks for spiders\nto be fed through a redis queue.</li>\n<li>Added redis queue stats.</li>\n<li>Let the encoder handle the item as it comes instead converting it to a dict.</li>\n</ul>\n</div>\n<div id=\"id12\">\n<h3>0.3.0 (2013-02-18)</h3>\n<ul>\n<li>Added support for different queue classes.</li>\n<li>Changed requests serialization from <cite>marshal</cite> to <cite>cPickle</cite>.</li>\n</ul>\n</div>\n<div id=\"id13\">\n<h3>0.2.0 (2013-02-17)</h3>\n<ul>\n<li>Improved backward compatibility.</li>\n<li>Added example project.</li>\n</ul>\n</div>\n<div id=\"id14\">\n<h3>0.1.0 (2011-09-01)</h3>\n<ul>\n<li>First release on PyPI.</li>\n</ul>\n</div>\n</div>\n\n          </div>"}, "last_serial": 2640054, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "674226e38a31757a6bed74ca72ea48f4", "sha256": "cbc4f8963a804d93de02671f479d6bbe57d81a5ddab98998efd9ce3c8f7c82ed"}, "downloads": -1, "filename": "scrapy-redis-0.1.tar.gz", "has_sig": false, "md5_digest": "674226e38a31757a6bed74ca72ea48f4", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 2994, "upload_time": "2011-08-29T06:57:34", "upload_time_iso_8601": "2011-08-29T06:57:34.165158Z", "url": "https://files.pythonhosted.org/packages/ee/27/a7bedf75e32e7111c89b931fec2e391582be04ab869e40c37865f03f80dc/scrapy-redis-0.1.tar.gz", "yanked": false}], "0.2": [{"comment_text": "", "digests": {"md5": "9f2eb76734042c420d3823d03c1b961e", "sha256": "1e0446e50081b5c0a9b9e897db6325ef6d374ebcfbbeffe1fd1250020e7c2c5c"}, "downloads": -1, "filename": "scrapy-redis-0.2.tar.gz", "has_sig": false, "md5_digest": "9f2eb76734042c420d3823d03c1b961e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3343, "upload_time": "2013-02-17T14:44:34", "upload_time_iso_8601": "2013-02-17T14:44:34.533692Z", "url": "https://files.pythonhosted.org/packages/d5/5e/a068c572a6967b955e59f1921526c251173a842f4feacf86e8c6b92ceb5e/scrapy-redis-0.2.tar.gz", "yanked": false}], "0.3": [{"comment_text": "", "digests": {"md5": "7abd77528c568c887e8da4456eafe877", "sha256": "85aa174c6050f4f34927204a04ea031b151fb203d67b0520f5d8d88b023b6f3c"}, "downloads": -1, "filename": "scrapy-redis-0.3.tar.gz", "has_sig": false, "md5_digest": "7abd77528c568c887e8da4456eafe877", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5491, "upload_time": "2013-02-17T17:55:04", "upload_time_iso_8601": "2013-02-17T17:55:04.835436Z", "url": "https://files.pythonhosted.org/packages/4a/a5/4f2ed2a2590c8a6be8c2130f76f4f172ad58137173877e1e087ff687ebf8/scrapy-redis-0.3.tar.gz", "yanked": false}], "0.4": [{"comment_text": "", "digests": {"md5": "2ccb0a32709f2c97d252c8964f6164f0", "sha256": "82237da37807278901f74d5e5540e843eb56885a78a8ab010bbab741cd7a8193"}, "downloads": -1, "filename": "scrapy-redis-0.4.tar.gz", "has_sig": false, "md5_digest": "2ccb0a32709f2c97d252c8964f6164f0", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 6761, "upload_time": "2013-04-19T18:08:33", "upload_time_iso_8601": "2013-04-19T18:08:33.312836Z", "url": "https://files.pythonhosted.org/packages/5f/ef/093126e2b8725674ce2329d67307ffd625ed0a740370520826700081b743/scrapy-redis-0.4.tar.gz", "yanked": false}], "0.5": [{"comment_text": "", "digests": {"md5": "e988d46411f38bfbe50c556df5438578", "sha256": "aed58b93179d4f2de609398f77c69cbe0670c0919be127ef55ab62d62f41365a"}, "downloads": -1, "filename": "scrapy-redis-0.5.tar.gz", "has_sig": false, "md5_digest": "e988d46411f38bfbe50c556df5438578", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 9852, "upload_time": "2013-09-02T16:46:46", "upload_time_iso_8601": "2013-09-02T16:46:46.052520Z", "url": "https://files.pythonhosted.org/packages/94/d4/7ff6d8593175b2fad6e0a5791f5201df478c2a12f852865f3b67cd5fdbb7/scrapy-redis-0.5.tar.gz", "yanked": false}], "0.5.2": [{"comment_text": "", "digests": {"md5": "9fe3a7656648eac4b4f5f8ff5db743e7", "sha256": "d18a000581f86c2fcb2ff98933fb702aa42c7fd401c623d77380485ec1f582d5"}, "downloads": -1, "filename": "scrapy-redis-0.5.2.tar.gz", "has_sig": false, "md5_digest": "9fe3a7656648eac4b4f5f8ff5db743e7", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 7905, "upload_time": "2013-10-01T12:58:45", "upload_time_iso_8601": "2013-10-01T12:58:45.756977Z", "url": "https://files.pythonhosted.org/packages/11/e8/733d232547da1fa0e0e93b3f1ca17b8d59a0ba2f3c9d6fe251da28959c02/scrapy-redis-0.5.2.tar.gz", "yanked": false}], "0.5.3": [{"comment_text": "", "digests": {"md5": "c561d0f6a7e34a84cb2dfa86a3d0b701", "sha256": "388fedd27115b48743e09b6832f605c8138bb3841713193763874e637e356ef3"}, "downloads": -1, "filename": "scrapy-redis-0.5.3.tar.gz", "has_sig": false, "md5_digest": "c561d0f6a7e34a84cb2dfa86a3d0b701", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10193, "upload_time": "2015-07-06T00:32:00", "upload_time_iso_8601": "2015-07-06T00:32:00.731355Z", "url": "https://files.pythonhosted.org/packages/84/7f/a8f302915b873c4574cf702327db6b95403251b8d224a95cd7643aaa82c6/scrapy-redis-0.5.3.tar.gz", "yanked": false}], "0.6.0": [{"comment_text": "", "digests": {"md5": "7551f9e4be72e2a9bd46b8e49004f7bc", "sha256": "4f00eebc21350ae29c70262db586592a08c342e4bce97213bee8a5b08c11ed16"}, "downloads": -1, "filename": "scrapy-redis-0.6.0.tar.gz", "has_sig": false, "md5_digest": "7551f9e4be72e2a9bd46b8e49004f7bc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 10265, "upload_time": "2015-07-06T02:10:34", "upload_time_iso_8601": "2015-07-06T02:10:34.288689Z", "url": "https://files.pythonhosted.org/packages/fb/32/de6cca657aa6a59b87a241301e7206ea98738174c4a6aa2ee0ae0fe4e1d2/scrapy-redis-0.6.0.tar.gz", "yanked": false}], "0.6.1": [{"comment_text": "", "digests": {"md5": "ea95b71d3dac484effa1010a1be93791", "sha256": "5122ef7bfc5d53239a49518d7c2385db33b54aa3b706d533a22a9cdece177c98"}, "downloads": -1, "filename": "scrapy_redis-0.6.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "ea95b71d3dac484effa1010a1be93791", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 14373, "upload_time": "2016-06-25T05:42:44", "upload_time_iso_8601": "2016-06-25T05:42:44.099531Z", "url": "https://files.pythonhosted.org/packages/66/63/bb4d0e033998fe54e5e78c84f39dbdd16ebbc60dfb5e9169ee0b6592364b/scrapy_redis-0.6.1-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "4823d10635998e8128dcb143b1d1ad6f", "sha256": "14c717e108e2bf23056e9c14f7d8c039f2dd97b38bd0cc18b076f6b6531c0206"}, "downloads": -1, "filename": "scrapy-redis-0.6.1.tar.gz", "has_sig": false, "md5_digest": "4823d10635998e8128dcb143b1d1ad6f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 28944, "upload_time": "2016-06-25T05:42:48", "upload_time_iso_8601": "2016-06-25T05:42:48.838571Z", "url": "https://files.pythonhosted.org/packages/29/3e/a56abaf56cdadc66297586f49ab4c0bc0eef0775809386ac8bbdc44e7f36/scrapy-redis-0.6.1.tar.gz", "yanked": false}], "0.6.2": [{"comment_text": "", "digests": {"md5": "97159e16c91d4b372fddd6b5ddedf04d", "sha256": "3b9646b8af68c5bad9eb40b81e9af11e3a734134757cae7365f2e41010101521"}, "downloads": -1, "filename": "scrapy_redis-0.6.2-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "97159e16c91d4b372fddd6b5ddedf04d", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 16670, "upload_time": "2016-06-25T22:53:29", "upload_time_iso_8601": "2016-06-25T22:53:29.934786Z", "url": "https://files.pythonhosted.org/packages/a8/f3/645efa52bbb09089edc50b4bb917ae6079d4d281756340e19d5d59b5dfcd/scrapy_redis-0.6.2-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "d59329e1029320bc9c976a74e3b93e81", "sha256": "9e244602b6d308e3f996943fb623b266855cf27913a50147048a822c665ea690"}, "downloads": -1, "filename": "scrapy-redis-0.6.2.tar.gz", "has_sig": false, "md5_digest": "d59329e1029320bc9c976a74e3b93e81", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 32595, "upload_time": "2016-06-25T22:53:34", "upload_time_iso_8601": "2016-06-25T22:53:34.990386Z", "url": "https://files.pythonhosted.org/packages/aa/47/538adcd373034bf0f276a8689c24d9a27058516d525fbfe26f9b1553c918/scrapy-redis-0.6.2.tar.gz", "yanked": false}], "0.6.3": [{"comment_text": "", "digests": {"md5": "c190d8bf4e35fa6e3779ebc9ffccc664", "sha256": "b83c98febeb1aeab8749e71bce42acc1a83894e6e389b4eccf03402f480a6c99"}, "downloads": -1, "filename": "scrapy_redis-0.6.3-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c190d8bf4e35fa6e3779ebc9ffccc664", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 17342, "upload_time": "2016-07-03T21:22:25", "upload_time_iso_8601": "2016-07-03T21:22:25.859591Z", "url": "https://files.pythonhosted.org/packages/06/30/a86932977398ab1de92721d5c70ff7beb85fd8b1e87842ce8816d2623556/scrapy_redis-0.6.3-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "9375be2daea45e360bb73c0ded5f8dea", "sha256": "2b10ccd1386207b58e4396bcdcef7cfdfbfa6bac74776399f7ce3682f946c1c8"}, "downloads": -1, "filename": "scrapy-redis-0.6.3.tar.gz", "has_sig": false, "md5_digest": "9375be2daea45e360bb73c0ded5f8dea", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 34548, "upload_time": "2016-07-03T21:22:30", "upload_time_iso_8601": "2016-07-03T21:22:30.730771Z", "url": "https://files.pythonhosted.org/packages/be/09/7415e61309e55c307c6ee68ad217a76b5d2e55b36e8a132561be920e2f5d/scrapy-redis-0.6.3.tar.gz", "yanked": false}], "0.6.8": [{"comment_text": "", "digests": {"md5": "c4d25dadccd2e439c79ccbc3a485147a", "sha256": "1508dfb29141a7eb562e303a12d2733fd490dee866f7fabe3bee3e5c5bef6df8"}, "downloads": -1, "filename": "scrapy_redis-0.6.8-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c4d25dadccd2e439c79ccbc3a485147a", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 19341, "upload_time": "2017-02-14T04:15:14", "upload_time_iso_8601": "2017-02-14T04:15:14.164096Z", "url": "https://files.pythonhosted.org/packages/00/91/bbc84cb0b95c361e9066d6ec115fd387142c07cabc69c5620761afa36874/scrapy_redis-0.6.8-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a69045d7b6a66221df564f5e7752dacc", "sha256": "b6991f2c0b2a72cd211296756acd03b9a7695fc437f5adbc3770d2e035979707"}, "downloads": -1, "filename": "scrapy-redis-0.6.8.tar.gz", "has_sig": false, "md5_digest": "a69045d7b6a66221df564f5e7752dacc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 36392, "upload_time": "2017-02-14T04:15:16", "upload_time_iso_8601": "2017-02-14T04:15:16.964310Z", "url": "https://files.pythonhosted.org/packages/fa/a6/233d9a9ecd82e898b4ef465ae62a668e3a0a33f356f78f938faa12a7d83e/scrapy-redis-0.6.8.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c4d25dadccd2e439c79ccbc3a485147a", "sha256": "1508dfb29141a7eb562e303a12d2733fd490dee866f7fabe3bee3e5c5bef6df8"}, "downloads": -1, "filename": "scrapy_redis-0.6.8-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c4d25dadccd2e439c79ccbc3a485147a", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 19341, "upload_time": "2017-02-14T04:15:14", "upload_time_iso_8601": "2017-02-14T04:15:14.164096Z", "url": "https://files.pythonhosted.org/packages/00/91/bbc84cb0b95c361e9066d6ec115fd387142c07cabc69c5620761afa36874/scrapy_redis-0.6.8-py2.py3-none-any.whl", "yanked": false}, {"comment_text": "", "digests": {"md5": "a69045d7b6a66221df564f5e7752dacc", "sha256": "b6991f2c0b2a72cd211296756acd03b9a7695fc437f5adbc3770d2e035979707"}, "downloads": -1, "filename": "scrapy-redis-0.6.8.tar.gz", "has_sig": false, "md5_digest": "a69045d7b6a66221df564f5e7752dacc", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 36392, "upload_time": "2017-02-14T04:15:16", "upload_time_iso_8601": "2017-02-14T04:15:16.964310Z", "url": "https://files.pythonhosted.org/packages/fa/a6/233d9a9ecd82e898b4ef465ae62a668e3a0a33f356f78f938faa12a7d83e/scrapy-redis-0.6.8.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:56:42 2020"}