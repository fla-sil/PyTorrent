{"info": {"author": "Shkarupa Alex", "author_email": "shkarupa.alex@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "Intended Audience :: Education", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 2", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development", "Topic :: Software Development :: Libraries", "Topic :: Software Development :: Libraries :: Python Modules"], "description": "# tfwn\n\n[Weight Normalization](https://arxiv.org/abs/1602.07868) layer wrapper for TensorFlow-Keras API.\n\nInspired by [Sean Morgan](https://github.com/tensorflow/tensorflow/pull/21276) implementation, but:\n- No data initialization (only eager mode was implemented in original pull request).\n- Code refactoring\n- More tests\n- CIFAR10 example from original paper reimplemented\n\n## Examples\nUnfortunately I couldn't reproduce parer results on CIFAR10 with batch size 100.\nAs you can see there is no much difference in accuracy.\n\n<img src=\"https://github.com/shkarupa-alex/tfwn/raw/master/examples/cifar10_accuracy_100.png\">\n<img src=\"https://github.com/shkarupa-alex/tfwn/raw/master/examples/cifar10_loss_100.png\">\n\n\nBut with much smaller batch size model with weight normalization is much better then regular one.\n\n<img src=\"https://github.com/shkarupa-alex/tfwn/raw/master/examples/cifar10_accuracy_16.png\">\n<img src=\"https://github.com/shkarupa-alex/tfwn/raw/master/examples/cifar10_loss_16.png\">\n\n\n## How to use\n```python\nimport tensorflow as tf\nfrom tfwn import WeightNorm\n\n\ndense_wn = WeightNorm(tf.keras.layers.Dense(3))\nout = dense_wn(input)\n```\n\n\n## References\n### Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks\nTim Salimans, and Diederik P. Kingma.\n\n```\n@inproceedings{Salimans2016WeightNorm,\n  title={Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},\n  author={Tim Salimans and Diederik P. Kingma},\n  booktitle={Neural Information Processing Systems 2016},\n  year={2016}\n}\n```", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/shkarupa-alex/tfwn", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "tfwn", "package_url": "https://pypi.org/project/tfwn/", "platform": "", "project_url": "https://pypi.org/project/tfwn/", "project_urls": {"Homepage": "https://github.com/shkarupa-alex/tfwn"}, "release_url": "https://pypi.org/project/tfwn/1.0.1/", "requires_dist": null, "requires_python": "", "summary": "Weight normalization layer for TensorFlow", "version": "1.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <h1>tfwn</h1>\n<p><a href=\"https://arxiv.org/abs/1602.07868\" rel=\"nofollow\">Weight Normalization</a> layer wrapper for TensorFlow-Keras API.</p>\n<p>Inspired by <a href=\"https://github.com/tensorflow/tensorflow/pull/21276\" rel=\"nofollow\">Sean Morgan</a> implementation, but:</p>\n<ul>\n<li>No data initialization (only eager mode was implemented in original pull request).</li>\n<li>Code refactoring</li>\n<li>More tests</li>\n<li>CIFAR10 example from original paper reimplemented</li>\n</ul>\n<h2>Examples</h2>\n<p>Unfortunately I couldn't reproduce parer results on CIFAR10 with batch size 100.\nAs you can see there is no much difference in accuracy.</p>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/ed48a7cb13c0aa730f2bb46c4d630316f0040184/68747470733a2f2f6769746875622e636f6d2f73686b61727570612d616c65782f7466776e2f7261772f6d61737465722f6578616d706c65732f636966617231305f61636375726163795f3130302e706e67\">\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/5c35be40e96fdc85a8b17660b4c19a22c0b0b0d6/68747470733a2f2f6769746875622e636f6d2f73686b61727570612d616c65782f7466776e2f7261772f6d61737465722f6578616d706c65732f636966617231305f6c6f73735f3130302e706e67\">\n<p>But with much smaller batch size model with weight normalization is much better then regular one.</p>\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/eabb8e7e72a4dd2efabf8222a82df25f3453e7e3/68747470733a2f2f6769746875622e636f6d2f73686b61727570612d616c65782f7466776e2f7261772f6d61737465722f6578616d706c65732f636966617231305f61636375726163795f31362e706e67\">\n<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/b1f26fa65c616459bef321aab701212583b00974/68747470733a2f2f6769746875622e636f6d2f73686b61727570612d616c65782f7466776e2f7261772f6d61737465722f6578616d706c65732f636966617231305f6c6f73735f31362e706e67\">\n<h2>How to use</h2>\n<pre><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"nn\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tfwn</span> <span class=\"kn\">import</span> <span class=\"n\">WeightNorm</span>\n\n\n<span class=\"n\">dense_wn</span> <span class=\"o\">=</span> <span class=\"n\">WeightNorm</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">))</span>\n<span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"n\">dense_wn</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n</pre>\n<h2>References</h2>\n<h3>Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</h3>\n<p>Tim Salimans, and Diederik P. Kingma.</p>\n<pre><code>@inproceedings{Salimans2016WeightNorm,\n  title={Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},\n  author={Tim Salimans and Diederik P. Kingma},\n  booktitle={Neural Information Processing Systems 2016},\n  year={2016}\n}\n</code></pre>\n\n          </div>"}, "last_serial": 4818822, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "b7d6b054facd6ad5489ac45f93dce87a", "sha256": "ca5fdead2f4098605f08a93b457c636d30403a6019c70f42e8b16420cfcbfc3e"}, "downloads": -1, "filename": "tfwn-1.0.0.tar.gz", "has_sig": false, "md5_digest": "b7d6b054facd6ad5489ac45f93dce87a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5023, "upload_time": "2019-02-14T05:09:09", "upload_time_iso_8601": "2019-02-14T05:09:09.459720Z", "url": "https://files.pythonhosted.org/packages/38/16/b77093c6772feb919c12c34df00611df776c8da1123c213ce1885f4e7df7/tfwn-1.0.0.tar.gz", "yanked": false}], "1.0.1": [{"comment_text": "", "digests": {"md5": "e6f4c8dc45948c0fc8d2dc9109f8a4cb", "sha256": "4572f44092a7ab4a2a2c452f57b5090de626d500d879b3e4c7cfbdaf5c13e0a5"}, "downloads": -1, "filename": "tfwn-1.0.1.tar.gz", "has_sig": false, "md5_digest": "e6f4c8dc45948c0fc8d2dc9109f8a4cb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5009, "upload_time": "2019-02-14T05:15:07", "upload_time_iso_8601": "2019-02-14T05:15:07.293281Z", "url": "https://files.pythonhosted.org/packages/bd/5b/98ac0c3b972f2550c5ce50c320ad65b14d70c38b5ec75f14bb504070ca0c/tfwn-1.0.1.tar.gz", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "e6f4c8dc45948c0fc8d2dc9109f8a4cb", "sha256": "4572f44092a7ab4a2a2c452f57b5090de626d500d879b3e4c7cfbdaf5c13e0a5"}, "downloads": -1, "filename": "tfwn-1.0.1.tar.gz", "has_sig": false, "md5_digest": "e6f4c8dc45948c0fc8d2dc9109f8a4cb", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 5009, "upload_time": "2019-02-14T05:15:07", "upload_time_iso_8601": "2019-02-14T05:15:07.293281Z", "url": "https://files.pythonhosted.org/packages/bd/5b/98ac0c3b972f2550c5ce50c320ad65b14d70c38b5ec75f14bb504070ca0c/tfwn-1.0.1.tar.gz", "yanked": false}], "timestamp": "Fri May  8 02:54:19 2020"}