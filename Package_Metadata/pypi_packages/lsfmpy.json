{"info": {"author": "Sylwia Bednarek, Piotr Majka", "author_email": "s.bednarek@nencki.gov.pl", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Environment :: Console", "Intended Audience :: Science/Research", "License :: OSI Approved :: GNU General Public License v3 (GPLv3)", "Natural Language :: English", "Operating System :: POSIX :: Linux", "Programming Language :: Python :: 2.7", "Topic :: Scientific/Engineering"], "description": "lsfmpy python library\n=====================\n\n\nFunctionality:\n--------------\n\n1. Writing:\n\n* a series of tiffs, a  stack of images in a single tiff or a nifti file (with RAS orientation) can be compressed to hdf5\n\n  * channel name is required\n  * a json file with metadata is required\n  * an xml file for big data viewer compatibility will be created (and recreated every time a new channel is added)\n  * user can specify how much memory should be used (for file reading, in GB) at once\n  * deafult 2 GB\n  * does not take into account memory for processing this amount of data\n  * an affine from a file (itk compatible) can be written for a specified channel\n\n\n2. Exporting:\n\n* 3D images\n\n  * reoriented from original orientation to RAS\n  * resampled (down- or upsampled) to desired resolution\n  * from desired level of internal pyramid of resolutions\n  * works for single compononet images, multicomponent images, segmentation\n  * affine and displacement fields can be specified for transformation of image during export. the order, type and inversion of transforms must be given. many transforms can be chained (a composite transform will be created and executed)\n\n  * if channel segmentation (from registration) is present in the hdf file, it can be used to specify which region (anatomical structure) should be exported (a bounding box), this also can be transformed to a reference space during export\n\n  * a subset of image volume, in the form of a cuboid, with origin specified in the output space (i.e. reference space if transformations are required) and of given physical size and resolution (in mm) can also be exported\n  * if a grid (number of chunks in each direction in output space) and overlap are also given when exporting a subset of image volume, appropriate grid of chunks will be exported (doesn't work with transformations)\n  * if grid = 0,0,0 whole image volume will be exported as a grid of chunks (origin parameter is ignored)\n\n\nslices\n\n* slices along a given (0,1,2) axis can be exported from pre-resampled levels, with\tgiven range\n\n* if range is None, all slices will be exported\n\n* roi can be specified via ox,oy,sx,sy (origin and size) in pixel coordinates\n\n\nExport functions can either write data to specified path name, or if output_path isNone, will return the data (or generator for multiple chunks/slices)\n\n\nviewing:\n\n* hdf5 files can be viewed with BigDataViewer plugin for Fiji via accompanying xml file\n\n\ninfo:\n\n* informations about the data written to file (or for specified channel) can be printed\n\n\nmetadata:\n\n* arbitrary metadata (as a key-value pair) can be added to file\n\nprovenance:\n\n* all operations on hdf5 file, along with command line, will be logged (unix systems)\n\ninteractive:\n\n* console commands with -- --interactive will return data in IPython notebook\n\ndump_metadata:\n\n* some metadata can be recovered from some of the tiff/nifti files, and a json file will be created, it needs to be inspected, and missing values must be filled in\n\nh5_external:\n* hdf5 files can be repacked, compressed or decompressed with this utility (repacking is useful for recovering space after deletion of large amounts of data that \totherwise will not be freed)\n\n", "description_content_type": "", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/Neuroinflab/lsfm_image_server", "keywords": "", "license": "GPLv3", "maintainer": "", "maintainer_email": "", "name": "lsfmpy", "package_url": "https://pypi.org/project/lsfmpy/", "platform": "", "project_url": "https://pypi.org/project/lsfmpy/", "project_urls": {"Homepage": "https://github.com/Neuroinflab/lsfm_image_server"}, "release_url": "https://pypi.org/project/lsfmpy/0.0.1/", "requires_dist": ["SimpleITK", "enum34", "fire", "h5py", "h5py-cache", "imageio", "lxml", "nibabel", "numpy", "scipy", "tifffile (==0.12.1)"], "requires_python": "", "summary": "HDF5-based software for storing and managing voluminous 3D imaging data", "version": "0.0.1", "yanked": false, "html_description": "<div class=\"project-description\">\n            <ol>\n<li>Writing:</li>\n</ol>\n<ul>\n<li>a series of tiffs, a  stack of images in a single tiff or a nifti file (with RAS orientation) can be compressed to hdf5<ul>\n<li>channel name is required</li>\n<li>a json file with metadata is required</li>\n<li>an xml file for big data viewer compatibility will be created (and recreated every time a new channel is added)</li>\n<li>user can specify how much memory should be used (for file reading, in GB) at once</li>\n<li>deafult 2 GB</li>\n<li>does not take into account memory for processing this amount of data</li>\n<li>an affine from a file (itk compatible) can be written for a specified channel</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li>Exporting:</li>\n</ol>\n<ul>\n<li>3D images<ul>\n<li>reoriented from original orientation to RAS</li>\n<li>resampled (down- or upsampled) to desired resolution</li>\n<li>from desired level of internal pyramid of resolutions</li>\n<li>works for single compononet images, multicomponent images, segmentation</li>\n<li>affine and displacement fields can be specified for transformation of image during export. the order, type and inversion of transforms must be given. many transforms can be chained (a composite transform will be created and executed)</li>\n<li>if channel segmentation (from registration) is present in the hdf file, it can be used to specify which region (anatomical structure) should be exported (a bounding box), this also can be transformed to a reference space during export</li>\n<li>a subset of image volume, in the form of a cuboid, with origin specified in the output space (i.e. reference space if transformations are required) and of given physical size and resolution (in mm) can also be exported</li>\n<li>if a grid (number of chunks in each direction in output space) and overlap are also given when exporting a subset of image volume, appropriate grid of chunks will be exported (doesn\u2019t work with transformations)</li>\n<li>if grid = 0,0,0 whole image volume will be exported as a grid of chunks (origin parameter is ignored)</li>\n</ul>\n</li>\n</ul>\n<p>slices</p>\n<ul>\n<li>slices along a given (0,1,2) axis can be exported from pre-resampled levels, with     given range</li>\n<li>if range is None, all slices will be exported</li>\n<li>roi can be specified via ox,oy,sx,sy (origin and size) in pixel coordinates</li>\n</ul>\n<p>Export functions can either write data to specified path name, or if output_path isNone, will return the data (or generator for multiple chunks/slices)</p>\n<p>viewing:</p>\n<ul>\n<li>hdf5 files can be viewed with BigDataViewer plugin for Fiji via accompanying xml file</li>\n</ul>\n<p>info:</p>\n<ul>\n<li>informations about the data written to file (or for specified channel) can be printed</li>\n</ul>\n<p>metadata:</p>\n<ul>\n<li>arbitrary metadata (as a key-value pair) can be added to file</li>\n</ul>\n<p>provenance:</p>\n<ul>\n<li>all operations on hdf5 file, along with command line, will be logged (unix systems)</li>\n</ul>\n<p>interactive:</p>\n<ul>\n<li>console commands with \u2013 \u2013interactive will return data in IPython notebook</li>\n</ul>\n<p>dump_metadata:</p>\n<ul>\n<li>some metadata can be recovered from some of the tiff/nifti files, and a json file will be created, it needs to be inspected, and missing values must be filled in</li>\n</ul>\n<p>h5_external:\n* hdf5 files can be repacked, compressed or decompressed with this utility (repacking is useful for recovering space after deletion of large amounts of data that       otherwise will not be freed)</p>\n\n          </div>"}, "last_serial": 4611593, "releases": {"0.0.1": [{"comment_text": "", "digests": {"md5": "c4a3a8f105d388c72a05ecfe56d6ebf0", "sha256": "b005b12590718401a7f32b9f9ad3f7d18bf142d2c5b7d4425dcce3b3c8a22bfe"}, "downloads": -1, "filename": "lsfmpy-0.0.1-py2-none-any.whl", "has_sig": false, "md5_digest": "c4a3a8f105d388c72a05ecfe56d6ebf0", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 92300, "upload_time": "2018-12-18T10:07:46", "upload_time_iso_8601": "2018-12-18T10:07:46.498795Z", "url": "https://files.pythonhosted.org/packages/e6/d7/cf0af64cae7156af5945da3ce63d098a2c1eaae0cb19e2e915cea8ca5dab/lsfmpy-0.0.1-py2-none-any.whl", "yanked": false}]}, "urls": [{"comment_text": "", "digests": {"md5": "c4a3a8f105d388c72a05ecfe56d6ebf0", "sha256": "b005b12590718401a7f32b9f9ad3f7d18bf142d2c5b7d4425dcce3b3c8a22bfe"}, "downloads": -1, "filename": "lsfmpy-0.0.1-py2-none-any.whl", "has_sig": false, "md5_digest": "c4a3a8f105d388c72a05ecfe56d6ebf0", "packagetype": "bdist_wheel", "python_version": "py2", "requires_python": null, "size": 92300, "upload_time": "2018-12-18T10:07:46", "upload_time_iso_8601": "2018-12-18T10:07:46.498795Z", "url": "https://files.pythonhosted.org/packages/e6/d7/cf0af64cae7156af5945da3ce63d098a2c1eaae0cb19e2e915cea8ca5dab/lsfmpy-0.0.1-py2-none-any.whl", "yanked": false}], "timestamp": "Fri May  8 00:43:29 2020"}